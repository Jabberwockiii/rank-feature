[
  {
    "title": "From Sparse to Soft Mixtures of Experts",
    "link": "https://arxiv.org/pdf/2308.00951.pdf",
    "upvote": "19",
    "text": "From Sparse to Soft Mixtures of Experts\nJoan Puigcerver\u2217\nCarlos Riquelme\u2217\nBasil Mustafa\nNeil Houlsby\nGoogle DeepMind\nAbstract\nSparse mixture of expert architectures (MoEs) scale model capacity without large increases in training\nor inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token\ndropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose\nSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the\nbenefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations\nof all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of\nthe (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual\nrecognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens\nChoice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5\u00d7 lower inference cost (5.7\u00d7\nlower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE\nalso scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40\u00d7 more parameters than\nViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better.\n1\nIntroduction\nLarger Transformers improve performance at increased computational cost. Recent studies suggest that model\nsize and training data must be scaled together to optimally use any given training compute budget (Kaplan\net al., 2020; Hoffmann et al., 2022; Zhai et al., 2022a). A promising alternative that allows to scale models\nin size without paying their full computational cost is sparse mixtures of experts (MoEs). Recently, a\nnumber of successful approaches have proposed ways to sparsely activate token paths across the network\nin language (Lepikhin et al., 2020; Fedus et al., 2022), vision (Riquelme et al., 2021), and multimodal\nmodels (Mustafa et al., 2022).\nAt the core of sparse MoE Transformers lies a discrete optimization problem: deciding which modules should\nbe applied to each input token. These modules are commonly referred to as experts and are usually MLPs.\nMany techniques have been devised to find good token-to-expert matches: linear programs (Lewis et al.,\n2021), reinforcement learning (Bengio et al., 2015), deterministic fixed rules (Roller et al., 2021), optimal\ntransport (Liu et al., 2022), greedy top-k experts per token (Shazeer et al., 2017), or greedy top-k tokens\nper expert (Zhou et al., 2022). In many cases, heuristic auxiliary losses are required to balance utilization\nof experts and minimize unassigned tokens. These challenges can be exacerbated in out-of-distribution\nscenarios: small inference batch sizes, novel inputs, or in transfer learning.\nWe introduce a new approach, Soft MoE, that overcomes many of these challenges. Rather than employing a\nsparse and discrete router that tries to find a good hard assignment between tokens and experts, Soft MoEs\ninstead perform a soft assignment by mixing tokens. In particular, we compute several weighted averages\nof all tokens\u2014with weights depending on both tokens and experts\u2014and then we process each weighted\naverage by its corresponding expert.\nSoft MoE models avoid most of the challenges mentioned above which are caused by the discrete procedure\nat the core of sparse MoEs. Popular sparse MoE algorithms learn some router parameters, and the source\nof gradients is usually two-fold: post-multiplication of expert outputs with the selected routing scores, and\n\u2217Equal contribution. The order was decided by a coin toss.\n1\narXiv:2308.00951v1  [cs.LG]  2 Aug 2023\nSparse MoE\n...\n...\n...\nExpert 1\nExpert 2\nExpert n\nAssign\nSlots\nSoft MoE\n...\n...\n...\nExpert 1\nExpert 2\nExpert n\nWeighted average\n0.01\n0.01\n0.01\n0.01\n0.02\n0.09\n0.09\n0.30\n0.40\n0.01\n0.03\n0.01\n0.01\nSlots\nFigure 1: Main differences between Sparse and Soft MoE layers. While the router in Sparse MoE layers\n(left) learns to assign individual input tokens to each of the available slots, in Soft MoE layers (right) each slot\nis the result of a (different) weighted average of all the input tokens. Learning to make discrete assignments\nintroduces several optimization and implementation issues that Soft MoE sidesteps.\nauxiliary losses that enforce some desired behaviour and also depend on the routing scores. It has been\nobserved that these mechanisms are often no better than random fixed routing (Roller et al., 2021). Soft MoE\nsidesteps this issue as every routing (or mixing) parameter is directly updated based on every single input\ntoken. Soft routing can provide stability while training a router; (Mustafa et al., 2022) observed that during\ntraining large fractions of input tokens can simultaneously change discrete routes through the network,\nleading to training challenges. Further, hard routing can be challenging with many experts, with most works\ntraining with just a few dozen. We show that Soft MoE scales to thousands of experts, and it is balanced\nby construction. Finally, there are no batch-effects at inference, where one input can affect routing (due to\nlimited expert capacity), and hence prediction, for other inputs.\nSoft MoE L/16 beats ViT H/14 on upstream, fewshot and finetuning while requiring almost half the training\ntime, and being 2\u00d7 faster at inference. Moreover, Soft MoE B/16 matches ViT H/14 on fewshot and finetuning\nand outperforms it on upstream metrics after a comparable amount of training. Remarkably, Soft MoE B/16\nis 5.7\u00d7 faster at inference despite having 5.5\u00d7 the number of parameters of ViT H/14. Section 4 demonstrates\nSoft MoE\u2019s potential to extend to other tasks: we train a contrastive model text tower against the frozen vision\ntower, showing that representations learned via soft routing preserve their benefits for image-text alignment.\n2\nSoft Mixture of Experts\n2.1\nAlgorithm description\nThe Soft MoE routing algorithm is depicted in Figure 2. We denote the inputs tokens for one sequence by\nX \u2208 Rm\u00d7d, where m is the number of tokens and d is their dimension. Each MoE layer uses a set of n expert\nfunctions1 applied on individual tokens, namely {fi : Rd \u2192 Rd}1:n. Each expert will process p slots, and each\nslot has a corresponding d-dimensional vector of parameters. We denote these parameters by \u03a6 \u2208 Rd\u00d7(n\u00b7p).\nIn particular, the input slots \u02dcX \u2208 R(n\u00b7p)\u00d7d are the result of convex combinations of all the m input tokens, X:\nDij =\nexp((X\u03a6)ij)\nPm\ni\u2032=1 exp((X\u03a6)i\u2032j)\n\u02dcX = D\u22a4X.\n(1)\nNotice that D, which we call the dispatch weights, is simply the result of applying a softmax over the columns\nof X\u03a6. Then, as mentioned above, the corresponding expert function is applied on each slot (i.e. on rows of\n1In practice, all experts apply the same function with different parameters, usually an MLP.\n2\nSoft MoE\nWeighting\nLogits\nExpert 1\nExpert 2\nExpert E\nSlot 1\nSlot 2\nSlot 3\nSlot 4\nSlot S\nSlot 1\nSlot 2\nSlot 3\nSlot 4\nSlot S\nslots S\ninput tokens N\nToken 1\nToken 2\nToken 3\nToken N\nDispatch\nWeights\nsoftmax per slot\n(normalized per column)\nCombine\nWeights\nsoftmax per token\n(normalized per row)\nToken 1\nToken 2\nToken 3\nToken N\nPer-slot\nLearnable\nParameters\n...\n...\n...\n...\n...\nslot linear\ncombination\noutput linear\ncombination\nSoft MoE Layer\nFigure 2: The Soft MoE routing algorithm. Soft MoE first computes scores or logits for every pair of input\ntoken and slot, based on some learnable per-slot parameters. These logits are then normalized per slot\n(columns) and every slot computes a linear combination of all the input tokens based on these weights (in\ngreen). Each expert (an MLP in this work) then processes its slots (e.g. 2 slots per expert, in this diagram).\nFinally, the same original logits are normalized per token (i.e. by row) and used to combine all the slot\noutputs, for every input token (in blue). Dashed boxes represent learnable parameters.\n\u02dcX) to obtain the output slots:\n\u02dcYi = f\u230ai/p\u230b( \u02dcXi).\n(2)\nFinally, the output tokens Y are computed as a convex combination of all (n \u00b7 p) output slots, \u02dcY, whose\nweights are computed similarly as before:\nCij =\nexp((X\u03a6)ij)\nPn\u00b7p\nj\u2032=1 exp((X\u03a6)ij\u2032)\nY = C \u02dcY.\n(3)\nWe refer to C as the combine weights, and it is the result of applying a softmax over the rows of X\u03a6.\nFollowing the usual design for Sparse MoEs (Lepikhin et al., 2020; Fedus et al., 2022; Riquelme et al., 2021;\nZhou et al., 2022), we replace a subset of the Transformer\u2019s MLP blocks with Soft MoE blocks. In particular,\nwe typically replace the second half of MLP blocks. The total number of slots is a key hyperparameter of\nSoft MoE layers because the time complexity depends on the number of slots rather than on the number of\nexperts. For example, one can set the number of slots equal to the input sequence length to match the FLOPs\nof the equivalent dense Transformer.\n1 def\nsoft_moe_layer (X, Phi , experts):\n2\n# Compute\nthe\ndispatch\nand\ncombine\nweights.\n3\nlogits = jnp.einsum(\u2019md ,dnp ->mnp \u2019, X, Phi)\n4\nD = jax.nn.softmax(logits , axis =(0 ,))\n5\nC = jax.nn.softmax(logits , axis =(1, 2))\n6\n# The\ninput\nslots\nare a weighted\naverage\nof all the\ninput\ntokens ,\n7\n# given by the\ndispatch\nweights.\n8\nXs = jnp.einsum(\u2019md ,mnp ->npd\u2019, X, D)\n9\n# Apply\nthe\ncorresponding\nexpert\nfunction\nto each\ninput\nslot.\n10\nYs = jnp.stack ([\n11\nf_i(Xs[i, :, :]) for i, f_i in\nenumerate(experts)],\n12\naxis =0)\n13\n# The\noutput\ntokens\nare a weighted\naverage\nof all the\noutput\nslots ,\n14\n# given by the\ncombine\nweights.\n15\nY = jnp.einsum(\u2019npd ,mnp ->md\u2019, Ys , C)\n16\nreturn Y\nAlgorithm 1: Simple JAX (Bradbury et al., 2018) implementation of a Soft MoE layer. Full code is available at\nhttps://github.com/google-research/vmoe.\n3\n2.2\nProperties of Soft MoEs and connections with Sparse MoEs\nFully differentiable\nAt the core of all Sparse MoE algorithms there is an assignment problem between\ntokens and experts, which is usually subject to some specific capacity and balance constraints. Different\nalgorithms relax the problem or approximate the solution in different ways: the Top-k or \u201cToken Choice\u201d\nrouter (Shazeer et al., 2017; Lepikhin et al., 2020; Riquelme et al., 2021), for instance, selects the top-k-\nscored experts for each token, while there are slots available in such expert (i.e. the expert has not filled\nits capacity). The \u201cExpert Choice\u201d router (Zhou et al., 2022) selects the top-capacity-scored tokens for each\nexpert. Other works suggest more advanced (and often costly) algorithms to compute the assignments, such\nas approaches based on Linear Programming algorithms (Lewis et al., 2021), Optimal Transport (Liu et al.,\n2022; Clark et al., 2022) or Reinforcement Learning (Clark et al., 2022). Nevertheless virtually all of these\napproaches are discrete in nature, and thus non-differentiable. In contrast, all operations in Soft MoE layers\nare continuous and fully differentiable. Indeed, we can interpret the weighted averages with softmax scores\nas soft assignments \u2013which motivates our algorithm\u2019s name\u2013 rather than the hard assignments that Sparse\nMoE methods typically use.\nNo token dropping and expert unbalance\nThe classical routing mechanisms mentioned above tend to\nsuffer from issues such as \u201ctoken dropping\u201d (i.e. some tokens are not assigned to any expert), or \u201cexpert\nunbalance\u201d (i.e. some experts receive far more tokens than others). Unfortunately, performance can be\nseverely impacted as a consequence. For instance, the popular Top-k or \u201cToken Choice\u201d router (Shazeer et al.,\n2017) suffers from both, while the \u201cExpert Choice\u201d router (Zhou et al., 2022) only suffers from the former\n(see Appendix B for some experiments regarding dropping in both cases). Soft MoEs are basically immune\nto token dropping and expert unbalance since every slot is filled with a weighted average of all tokens. All\nweights are (in theory) strictly positive thanks to the softmax (see Section 5 for detailed experiments).\nFast\nThe total number of slots is the main hyperparameter that determines the cost of a Soft MoE layer.\nEvery input applies such number of MLPs. The total number of experts is irrelevant in this calculation: few\nexperts with many slots per expert or many experts with few slots per expert will have matching costs if\nthe total number of slots is identical. The only constraint we must meet is that the number of slots has to be\ngreater or equal to the number of experts (as each expert must process at least one slot). The main advantage\nof Soft MoE is completely avoiding sort or top-k operations which are slow and typically not well suited for\nhardware accelerators. As a result, Soft MoE is significantly faster than most sparse MoEs (Figure 6). See\nSection 2.3 for time complexity details.\nFeatures of both sparse and dense\nThe sparsity in Sparse MoEs comes from the fact that expert parameters\nare only applied to a subset of the input tokens. However, Soft MoEs are not technically sparse, since every\nslot is a weighted average of all the input tokens. Every input token fractionally activates all the model\nparameters. Likewise, all output tokens are fractionally dependent on all slots (and experts). Finally, notice\nalso that Soft MoEs are not Dense MoEs, where every expert processes all input tokens, since every expert\nonly processes a subset of the slots.\nPer-sequence determinism\nUnder capacity constraints, all Sparse MoE approaches route tokens in groups of\na fixed size and enforce (or encourage) balance within the group. When groups contain tokens from different\nsequences or inputs, these tokens often compete against each other for available spots in expert buffers. As a\nconsequence, the model is no longer deterministic at the sequence-level, but only at the batch-level, as some\ninput sequences may affect the final prediction for other inputs. Models using larger groups tend to provide\nmore freedom to the routing algorithm and usually perform better, while their computational cost is also\nhigher. On the other hand, when groups contain tokens from a single sequence, the model is forced to use\nevery expert on every input sequence. This may lead to more generalist experts. Moreover, changing the\ngroup size between training and inference can be problematic due to the potential distributional shift in\ntoken-to-expert assignments. We explore these aspects in Section 3.5.\n4\nSoft MoE gracefully sidesteps all these challenges. Since it combines all tokens in each input sequence, we\njust set the group size to be a single sequence. Every expert does handle tokens from every input, maybe\nsomewhat limiting the amount of high-level specialization. Yet, this also implies that it is per-example\ndeterministic and fast, while typical instances of Sparse MoEs are not.\n2.3\nImplementation\nTime complexity\nAssume the per-token cost of a single expert function is O(k). The time complexity of a\nSoft MoE layer is then O(mnpd + npk). By choosing p = O(m/n) slots per expert, i.e. the number of tokens\nover the number of experts, the cost reduces to O(m2d + mk). Given that each expert function has its own set\nof parameters, increasing the number of experts n and scaling p accordingly, allows us to increase the total\nnumber of parameters without any impact on the time complexity. Moreover, when the cost of applying an\nexpert is large, the mk term dominates over m2d, and the overall cost of a Soft MoE layer becomes comparable\nto that of applying a single expert on all the input tokens. Finally, even when m2d is not dominated, this is the\nsame as the (single-headed) self-attention cost, thus it does not become a bottleneck in Transformer models.\nNormalization\nIn Transformers, MoE layers are typically used to replace the feedforward layer in each\nencoder block. Thus, when using pre-normalization as most modern Transformer architectures (Domhan,\n2018; Xiong et al., 2020; Riquelme et al., 2021; Fedus et al., 2022), the inputs to the MoE layer are \u201clayer\nnormalized\u201d. This causes stability issues when scaling the model dimension d, since the softmax approaches\na one-hot vector as d \u2192 \u221e (see Appendix E). Thus, in Line 3 of algorithm 1 we replace X and Phi with\nl2_normalize(X, axis=1) and scale * l2_normalize(Phi, axis=0), respectively; where scale is a trainable scalar,\nand l2_normalize normalizes the corresponding axis to have unit (L2) norm, as Algorithm 2 shows.\n1 def\nl2_normalize (x, axis , eps =1e-6):\n2\nnorm = jnp.sqrt(jnp.square(x).sum(axis=axis , keepdims=True))\n3\nreturn x * jnp.reciprocal(norm + eps)\nAlgorithm 2: JAX implementation of the L2 normalization used in Soft MoE layers.\nFor relatively small values of d (e.g. the model dimension used for ViT models up to ViT-H, that use d \u2264 1280),\nthe normalization has little impact on the model\u2019s quality. However, with the proposed normalization in the\nSoft MoE layer, we can eventually make the model dimension bigger and/or increase the learning rate (see\nAppendix E). Accordingly, we use it in all our experiments.\nDistributed model\nWhen the number of experts increases significantly, it is not possible to fit the entire\nmodel in memory on a single device, especially during training or when using MoEs on top of large model\nbackbones. In these cases, we employ the standard techniques to distribute the model across many devices,\nas in (Lepikhin et al., 2020; Riquelme et al., 2021; Fedus et al., 2022) and other works training large MoE\nmodels. Distributing the model typically adds an overhead in the cost of the model, which is not captured by\nthe time complexity analysis based on FLOPs that we derived above. In order to account for this difference,\nin all of our experiments we measure not only the FLOPs, but also the wall-clock time in TPUv3-chip-hours.\n2.4\nConnections with other methods\nMany existing works merge, mix or fuse input tokens to reduce the input sequence length (Jaegle et al., 2021;\nRyoo et al., 2021; Renggli et al., 2022; Wang et al., 2022), typically using attention-like weighted averages\nwith fixed keys, to try to alleviate the quadratic cost of self-attention with respect to the sequence length.\nAlthough our dispatch and combine weights are computed in a similar fashion to these approaches, our goal\nis not to reduce the sequence length (while it is possible), and we actually recover the original sequence\nlength after weighting the experts\u2019 outputs with the combine weights, at the end of each Soft MoE layer.\n5\nMulti-headed attention also shows some similarities with Soft MoE, beyond the use of softmax in weighted\naverages: the h different heads can be interpreted as different (linear) experts. The distinction is that, if m is\nthe sequence length and each input token has dimensionality d, each of the h heads processes m vectors of\nsize d/h. The m resulting vectors are combined using different weights for each of the m\u2032 output tokens (i.e.\nthe attention weights), on each head independently, and then the resulting (d/h)-dimensional vectors from\neach head are concatenated into one of dimension d. Our experts are non-linear and combine vectors of size\nd, at the input and output of such experts.\nFinally, there are also connections with other MoE works that use a weighted combination of the experts\nparameters, rather than doing a sparse routing of the examples (Yang et al., 2019; Tian et al., 2020; Muqeeth\net al., 2023). These approaches are also fully differentiable, although they can have a much higher cost, since\n1) they must average the parameters of the experts, which can become a time and/or memory bottleneck\nwhen experts with many parameters are used; and 2) they cannot take advantage of vectorized operations as\nbroadly as Soft (and Sparse) MoEs, since every input uses a different weighted combination of the parameters. We\nrecommend the \u201ccomputational cost\u201d discussion in (Muqeeth et al., 2023) that addresses these issues.\n2.5\nCurrent limitations\nAuto-regressive decoding\nOne of the key aspects of Soft MoE consists in smartly merging all tokens in\nthe input. This makes the use of Soft MoEs in auto-regressive decoders difficult, since causality between\npast and future tokens has to be preserved during training. Although causal masks used in attention layers\ncould be used, one must be careful to not introduce any correlation between token and slot indices, since this\nwould bias which token indices each expert is trained on. The use of Soft MoE in auto-regressive decoders is\na promising research avenue that we leave for future works.\nLazy experts & memory consumption\nWe extensively show in Section 3 that one slot per expert tends to\nbe the optimal choice. In other words, rather than feeding one expert with two slots (or mixes of tokens),\nit is more effective from a performance standpoint to use two experts with one slot each. We hypothesize\nsame-expert slots tend to somewhat align and provide small informational gains, and single experts may lack\nthe flexibility to accommodate very different slot projections. We show this in Appendix H. Consequently,\nSoft MoE tends to leverage a large number of experts and \u2013while its cost is still similar to the dense backbone\u2013\nthe memory requirements of the model can grow large.\n3\nImage Classification Experiments\nWe present three types of experiments on image classification:\nTraining Pareto frontiers. First, in Section 3.3 we systematically compare dense ViT models at the Small,\nBase, Large and Huge sizes with their sparse counterparts based on the most common routing techniques\n(Tokens Choice, Experts Choice) and Soft MoE routing. We study the training FLOPs versus performance\nand training time versus performance plots to conclude that Soft MoE dominates all other approaches.\nInference-time optimized models. Second, in Section 3.4, we present longer training runs (\u201covertraining\u201d).\nRelative to ViT, Soft MoE brings large improvements in terms of inference speed (small models: S, B) and\nabsolute performance (large models: L, H).\nModel ablations. Third, in Section 3.5 we investigate some of the central aspects of Soft MoE routing (such\nas number of experts, slots per expert, etc), and compare their behavior with other routing algorithms. We\npresent the optimal configurations for Soft MoE and the source of the improvement benefits.\n6\n3.1\nTraining and evaluation data\nWe pretrain our models on JFT-4B (Sun et al., 2017), a proprietary dataset whose latest version contains\nmore than 4B images, covering more than 29k classes. During pretraining, we typically evaluate the models\non two metrics: upstream validation precision-at-1 on JFT-4B, and ImageNet 10-shot accuracy. The latter is\ncomputed by freezing the model weights and replacing the head with a new one that is only trained on a\ndataset containing 10 images per class from ImageNet-1k (Deng et al., 2009). Finally, we provide the accuracy\non the validation set of ImageNet-1k after finetuning on the training set of ImageNet-1k (1.3 million images).\n3.2\nSparse routing algorithms\nWe compare to the following popular MoE routing algorithms:\nTokens Choice. Every token selects the top-K experts with the highest routing score for the token (Shazeer et al.,\n2017). Increasing K typically leads to better performance at the expense of extra computational cost. Batch\nPriority Routing (BPR) (Riquelme et al., 2021) significantly improves the model performance, especially in\nthe case of K = 1 (see Appendix, Table 8). Accordingly we use Top-K routing with BPR and K \u2208 {1, 2}. We\nalso optimize the number of experts (Appendix, Figure 15).\nExperts Choice. Alternatively, experts can select the top-C tokens in terms of routing scores (Zhou et al.,\n2022). In this case, C is the buffer size, and we typically set E \u00b7 C = c \u00b7 T where E is the number of experts,\nT is the total number of tokens in the group, and c is the capacity multiplier. When c = 1, all tokens can\nbe served via the union of experts. Note that in this type of routing, it is common that some tokens are\nsimultaneously selected by several experts whereas some other tokens are not selected at all. Figure 14\nillustrates this phenomenon. We experiment with c = 0.5, c = 1 and c = 2.\n3.3\nTraining Pareto-optimal models\nWe train VIT-S/8, VIT-S/16, VIT-S/32, VIT-B/16, VIT-B/32, VIT-L/16, VIT-L/32 and VIT-H/14 models, and\ntheir sparse counterparts. We consider three routing algorithms for the sparse models: Token Choice, Expert\nChoice, and Soft MoE. In each case, we train several model variants (different K, C and number of experts\nwhere it corresponds). In total, we train 106 models. The models are trained for 300k steps with batch size\n4096 in all cases, and inverse square root learning rate decay.\nFigure 3a and Figure 3b show the results for models in each class that lie on their respective training cost\n/ performance Pareto frontiers. On both metrics, Soft MoE strongly outperforms dense and other sparse\napproaches for any given FLOPs or time budget. Table 9 in Appendix I list all the models, with their\nparameters, performance and costs, and are shown in Figure 22. Figures 23 to 25 in Appendix F compare\nSoft MoE individually to Dense, Token Choice and Expert Choice models respectively.\n3.4\nLong training runs\nIn addition to shorter runs and ablations, we trained a number of models for much longer (a few million\nsteps) to test the Soft MoE capabilities at larger computational scales. We present two experiments.\nFirst, in Section 3.4.1, we trained ViT and Soft MoE of different sizes ranging from Small to Huge for 4M\nsteps. Figure 4 and Table 2 show the results. Second, in Section 3.4.2, we kept training some of the previous\nSoft MoE models beyond the optimal point suggested by standard dense scaling laws. Sparse models can\nleverage the extra capacity to steadily improve their performance, leading to very strong Soft MoE models\nthat are notably cheap at inference.\n7\n101\n102\nTotal Training TPUv3-days\n0.44\n0.46\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58\nJFT-4B Precision-at-1\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nExperts Choice\nTokens Choice\nDense\n(a) JFT-4B Precision-at-1\n101\n102\nTotal Training TPUv3-days\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nImageNet 10-shot Accuracy\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nExperts Choice\nTokens Choice\nDense\n(b) ImageNet 10-shot Accuracy\nFigure 3: Pareto Models. Soft MoE dominates both ViTs (dense) and popular MoEs (Experts Choice,Tokens\nChoice) on the training cost / performance Pareto frontier. Each point is a model trained for 300k steps, and\nlarger marker sizes indicate larger models: S/32, S/16, B/32, B/16, L/16 and H/14. Cost is shown both in\nterms of FLOPS and realized TPU-v3 training time. MoE runs include different configuration; for clarity,\nonly models on their respective Pareto frontier are displayed. Figure 22 in Appendix F shows all models.\n3.4.1\nComparison with large-scale Vision Transformers\nWe trained a number of Soft MoEs on JFT, following a comparable setting to that used by Zhai et al. (2022a).\nWe replace the last half of the blocks in ViT S/16, B/16, L/16, and H/14 with Soft MoE layers with 128 experts,\nusing one slot per expert. We train models ranging from 1B to 54B parameters. Large Soft MoE models incur\nin a small wall-clock time overhead compared to their dense counterparts due to the extra data transfers\nrequired by model parallelism. All variants were trained for 4M steps, except for H/14s which was trained\nfor 2M steps for cost reasons.\nFigure 4 shows the JFT-4B precision, ImageNet 10-shot accuracy, and the ImageNet finetuning accuracy for\nSoft MoE and ViT versus training cost in ExaFLOPS. Table 2 contains all the results, and Figure 19 shows\nperformance versus core-hours. Soft MoE models widely outperform Vision Transformer models for a given\ncompute budget. For instance, the Soft MoE S/16 performs better than ViT B/16 on JFT and 10-shot ImageNet,\nand it also improves finetuning scores on the full ImageNet data, even though its training (and inference) cost\nis significantly smaller. Similarly, Soft MoE B/16 outperforms ViT L/16 upstream, and only lags 0.5 behind\nafter finetuning while being 3x faster and requiring almost 4x fewer FLOPs. Finally, the Soft MoE L/16 model\noutperforms the dense H/14 one while again being around 3x faster to train and serve at inference.\n3.4.2\nSoft MoEs optimized for inference\nEncouraged by the fact that Soft MoEs with smaller backbones can match the quality of larger Vision\nTransformers, we continue training the small backbones to obtain models of higher quality at very low\ninference cost. Even after additional (over) training, the overall training time with respect to larger ViT\nmodels is comparable. For these long runs, we observe that longer cooldowns (period where the learning\nrate is decreased linearly to zero (Zhai et al., 2022a)) work well for Soft MoE. Therefore, we increase the\ncooldown from 50k steps (used elsewhere) to up to 500k steps. Figure 5 presents these models.\nWe now summarize our main results. Soft MoE B/16 trained for 1k TPUv3 days outperforms ViT H/14\ntrained on a similar time budget (see Table 1, ViT H/14, 1M steps) while being 10\u00d7 cheaper at inference in\nFLOPs and 5.7\u00d7 in wall-clock time, and it almost matches the ViT H/14 model performance even if we double\nViT-H/14\u2019s training budget (2M steps and 2039.8 train days for ViT H/14 versus 1011.4 days for Soft MoE\nB/16). Soft MoE L/16 beats all models substantially while being almost 2\u00d7 faster at inference than ViT H/14.\n8\n103\nTotal Training ExaFLOPs\n67%\n73%\n77%\n81%\n84%\nImageNet 10-shot Accuracy\nS/16\nB/16\nL/16\nH/14\n103\nTotal Training ExaFLOPs\n51%\n54%\n57%\n59%\nJFT-4B Precision-at-1\nS/16\nB/16\nL/16\nH/14\n103\nTotal Training ExaFLOPs\n84%\n85%\n87%\n88%\n89%\nImageNet Finetune Accuracy\nS/16\nB/16\nL/16H/14\nSoft MoE\nDense\nFigure 4: Long runs. Soft MoE and ViT models trained for 4 million steps with batch size 4096 (H/14 models\ntrained for 2 million steps instead). Equivalent model classes (S/16, B/16, L/16, H/14) have similar training\ncosts, but Soft MoE outperforms ViT on all metrics. We show ImageNet 10-shot (left), JFT precision at 1\n(middle) and ImageNet accuracy after finetuning (right), versus total training FLOPs. See Table 2. We report\ntraining wall-clock time in Figure 19.\nTable 1: Training and finetuning results for Soft MoE and dense models. Finetuning performed on ImageNet\nat 384 resolution. Steps used for linear cooldown indicated in parentheses, these are included in the total\ntrain steps count. Results are plotted in Figure 5.\nModel\nParams Train steps Train days & exaFLOP Eval Ms/img & GFLOP/img JFT P@1 IN/10shot IN/ft\nViT S/16\n33M\n4M (50k)\n153.5\n227.1\n0.5\n9.2\n51.3\n67.6\n84.0\nViT B/16\n108M\n4M (50k)\n410.1\n864.1\n1.3\n35.1\n56.2\n76.8\n86.6\nViT L/16\n333M\n4M (50k)\n1290.1\n3025.4\n4.9\n122.9\n59.8\n81.5\n88.5\nViT H/14\n669M\n2M (50k)\n2039.8\n4120.3\n8.6\n334.2\n59.7\n83.3\n88.9\nSoft MoE S/14 256E\n1.8B 10M (50k)\n494.7\n814.2\n0.9\n13.2\n60.1\n80.6\n87.5\nSoft MoE B/16 128E\n3.7B 9M (500k)\n1011.4\n1769.5\n1.5\n32.0\n62.4\n82.9\n88.5\nSoft MoE L/16 128E\n13.1B 4M (500k)\n1355.4\n2734.1\n4.8\n111.1\n63.0\n84.3\n89.2\n3.5\nSoft MoE Ablations\nHere we establish the optimal configurations for Soft MoE models by exploring the following:\nOptimal number of slots per expert. One or two slots per expert work best. We demonstrate this by fixing the\ntotal number of slots (which determines the compute cost of the model), and changing the number of experts,\ni.e. the slots per expert (Figure 6).\nOptimal number of experts. Roughly the same number of experts as input tokens work best when using one\nslot per expert. The model is then similarly expensive in terms of FLOPs as its dense equivalent. To show\nthis, we increase the number of experts and train models for the same amount of time, and find the best\nperforming model (Figure 8).\nArchitectural/algorithmic ablations. To disentangle the source of the benefits, we compare Soft MoE to a number\nof ablated versions: route token i deterministically to expert i, fixed uniform dispatch/combine weights, and\nothers (TablTable 3).\nMoE layers placement. An additional ablation regarding where to place MoE layers is presented in Appendix D.\n9\n101\n102\nEvaluation cost (GFLOP/img)\n70%\n75%\n80%\n85%\nImageNet 10-shot Accuracy\nS/16\nB/16\nL/16\nH/14\nS/14\nB/16\n10.4\u00d7\n101\n102\nEvaluation cost (GFLOP/img)\n52%\n55%\n57%\n60%\n62%\n65%\nJFT-4B Precision-at-1\nS/16\nB/16\nL/16\nH/14\n10.4\u00d7\n103\nTraining cost (exaFLOP)\n70%\n75%\n80%\n85%\nImageNet 10-shot Accuracy\nS/16\nB/16\nL/16\nH/14\n103\nTraining cost (exaFLOP)\n52%\n55%\n57%\n60%\n62%\n65%\nJFT-4B Precision-at-1\nS/16\nB/16\nL/16\nH/14\nS/14\nB/16 L/16\nViT\nSoft MoE\nSoft MoE (long)\n100\n101\nEvaluation time (TPUv3 ms/img)\n70%\n75%\n80%\n85%\nImageNet 10-shot Accuracy\nS/16\nB/16\nL/16\nH/14\nS/14\nB/16\n5.7\u00d7\n100\n101\nEvaluation time (TPUv3 ms/img)\n52%\n55%\n57%\n60%\n62%\n65%\nJFT-4B Precision-at-1\nS/16\nB/16\nL/16\nH/14\n5.7\u00d7\n103\nTraining time (TPUv3 days)\n70%\n75%\n80%\n85%\nImageNet 10-shot Accuracy\nS/16\nB/16\nL/16\nH/14\n103\nTraining time (TPUv3 days)\n52%\n55%\n57%\n60%\n62%\n65%\nJFT-4B Precision-at-1\nS/16\nB/16\nL/16\nH/14\nS/14\nB/16L/16\nFigure 5: Soft MoE optimized for inference. These plots show the quality on JFT-4B (Precision-at-1)\nand ImageNet (10-shot Accuracy) achieved by different models with different training and inference cost\n(measured both in TPUv3 time and FLOPs). Red and light blue curves correspond (respectively) to ViT and\nSoft MoE S/16, S/14, B/16, L/16 and H/14 trained for 4 million steps (except H/14, that was trained for 2\nmillion steps), following a recipe similar to (Zhai et al., 2022a). Dark blue curves correspond to Soft MoE\nS/14, B/16, L/16 trained for additional steps as detailed in Table 1. We observe that the overtrained Soft MoE\nB/16 is better than the best ViT model (H/14) while using 10\u00d7 less computation (5.7\u00d7 time). Soft MoE L/16\nis the most performant model requiring one third of the inference FLOPs (one half of the time). Detailed\nresults in Tables 1 and 2.\n3.5.1\nNumber of Experts and Slots per Expert\nWhen applying Soft MoE to a given architecture and input sequence length, one must decide how many\nexperts and how many slots per expert to use. The total number of slots determines the amount of work\n(FLOPs) applied in the MoE layer (ignoring the small the routing cost). If the total number of slots is greater\nthan the number of input tokens, the model will require more FLOPs than dense Transformers: more \u201ctokens\u201d\nwill be processed in the MoE layer. Conversely, if the number of slots is lower than the original number of\ntokens, Soft MoE will save some compute relative to dense Transformers.\nUnless stated otherwise, the following experiments use a ViT-S/16 backbone trained for 300k steps with\nbatch size 4096. The MoEs have expert layers in their last six of twelve blocks.\nOptimal number of slots per expert. In this experiment the total amount of compute is fixed, and we\ncompare different configurations. Specifically, we fix the total number of slots to 4096, and we train models\nwith different number of experts. MoE algorithms are often unable to scale well to a large number of experts\n(over 100). The model sizes range from just 38M (with 2 experts) to 9.7B parameters (when using 4096\nexperts). Figure 6 (and Figure 26) shows the results in terms of pre-training precision (left) and the few-shot\nevaluation (middle). In the case of Experts Choice and Tokens Choice MoEs, the size of the union of all\nexpert buffers is also 4096 per input image. We just vary the number of experts keeping the total number of\ntokens processed across the union of experts constant, as for Soft MoE. For the Sparse MoEs (Experts/Tokens\nChoice), there is an implementation detail: The \u201cgroup size\u201d is the subset of the batch that is routed together.\nAll tokens in a group compete to be selected by each expert. This can range from one image/group to the\nentire batch/group; the latter is more flexible, but increases computational overhead in routing (sorting the\n10\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n0.515\n0.520\n0.525\n0.530\n0.535\n0.540\n0.545\nJFT-4B Precision-at-1\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n0.70\n0.71\n0.72\n0.73\n0.74\n0.75\nImageNet 10-shot Accuracy\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n1.00\n1.25\n1.50\n1.75\n2.00\n(normalized) Train Step Time\n128\n64\n32\n16\n8\n4\n2\n1\nSlots / Expert\n128\n64\n32\n16\n8\n4\n2\n1\nSlots / Expert\n128\n64\n32\n16\n8\n4\n2\n1\nSlots / Expert\nSoft MoE\nExperts Choice\nTokens Choice\nFigure 6: Performance (left, center), and training step time (right) as a function of number of experts, for\nmodels with a fixed number of slots (Soft MoE) or expert buffer capacity (Sparse MoEs) on a ViT-S/16\nbackbone with MoEs in the last two layers. Soft MoE achieves much better scaling with more experts, while\ncost is roughly constant. However, with Experts and Tokens Choice routers, having too many experts not only\nhurts performance but also significantly increases the cost (Tokens Choice reaches 3.9x with 4096 experts).\nitems). In Figure 6, we use group size eight. Figure 20, Appendix, shows other options.\nFigure 6 shows that Soft MoE scales with increased experts. The best configurations are 2048 and 4096\nexperts, at one/two slots per experts, respectively. In contrast, Experts Choice and Tokens Choice do not\nscale well with the number of experts, and performance degrades after 512 experts. In addition, Figure 6,\nright, shows the step time for each model. Due to sorting leading to increased computational overhead, the\nSparse MoE\u2019s step time increases substantially with more experts, which is not the case for Soft MoE.\nOptimal number of experts. From the previous analysis, we set the number of slots per expert to one. The\nnext question is how many experts to use. Here, the cost of models are not matched: more experts will\nincrease cost (through more slots). Figure 7 shows that, both for Soft MoE and Experts Choice, more experts\ndo better (up to 1024).\nNext, we match the total training time for each model by adjusting the number of training steps (Figure 8).\nAt this scale (ViT-S), the optimal number of experts for a given training budget is around 128 or 256 experts.\nThe number of input tokens is 196, this corresponds to the minimum number of experts that does not lead to\na strong token bottleneck (many fewer than 196 slots) in the MoE layer. For any number of experts, Soft MoE\noutperforms Experts Choice. Both models have the same capacity, but Experts Choice is significantly more\nexpensive, especially with large group size.\nMore slots per expert. Appendix C explores how Soft MoE behaves when increasing the number of slots per\nexpert. Appendix H looks at the (strong) correlation between the learned slot parameters in this case.\n3.5.2\nAlgorithmic Ablations: Identity & Uniform Routing\nSoft MoE relies on learning how to mix tokens for each expert. To understand the impact of finding useful\nlinear combinations of input tokens, we ablate this aspect by testing some natural choices:\nIdentity routing. Tokens are not mixed: the first token goes to first expert, second token goes to second expert,\netc.\nUniform Mixing. Every slot mixes all input tokens in the same way: by uniformly averaging them, both for\ndispatching and combining. In this case, we must independently and randomly initialize every expert as\n11\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.46\n0.48\n0.50\n0.52\n0.54\nJFT-4B Precision-at-1\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nImageNet 10-shot Accuracy\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n100\n200\n300\n400\n500\n600\nTrain step time in milliseconds\nSoft-MoE\nExperts Choice (gs=1 img)\nExperts Choice (gs=32 img)\nFigure 7: Performance (left, center) and step time (right) for models trained with increased experts and\none slot (or token) per expert for a fixed number of steps (300k). The performance of all models improves\nas their capacity increases. However, the cost of Experts Choice grows faster than that of Soft MoE, especially\nwhen the group size is larger (gs= 32).\notherwise the additional capacity coming from different experts will not be used (we end up with copies).\nSoft / Uniform. We learn to mix tokens to create the slots (dispatch weights), but we uniformly average all\nexpert outputs. This implies every input token is identically updated before the residual connection.\nUniform / Soft. All slots are filled with the uniform average of the input tokens. We learn to mix the expert\noutput tokens depending on the input tokens.\nTable 3 summarizes our results, and Appendix A contains further details. Learning to mix tokens for\ndispatching and for combining tokens after expert processing seems essential to perform well, and dispatch\nmixing is slightly more important than the combine mixing. Dense underperform all variants.\n4\nContrastive learning experiments\nWe test whether the learned representations are also significantly better when used for other tasks. In this\nsection we explore a popular paradigm, image-language contrastive learning. We follow the approach in\nZhai et al. (2022b) where the image tower is pre-trained on an image classification task, and then frozen\nwhile training the text encoder on a dataset of image-text pairs.\nWe re-use the models trained on JFT in the previous section and compare their performance on a number\nof downstream applications. For contrastive learning we train on WebLI (Chen et al., 2022), a proprietary\ndataset consisting of 10B images and their ALT texts crawled from the internet. The image encoder is frozen,\nwhile the text encoder is trained from scratch.\nTable 4 shows our results. Overall, the gaps we observed on image classification are preserved in this setting.\nFor instance, Soft MoE-L/16 outperforms ViT-L/16 by more than 1% and 2% on Imagenet and Cifar-100\nzero-shot, respectively. Retrieval numbers are generally modest.\n5\nModel Inspection\nIn this section, we take a look at various aspects of the routing the model learns.\n12\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.48\n0.50\n0.52\n0.54\n0.56\nJFT-4B Precision-at-1\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\nImageNet 10-shot Accuracy\nSoft MoE\nExperts Choice (gs=1 img)\nExperts Choice (gs=8 img)\nDense\nFigure 8: Performance of models trained with increasing experts (one slot/token per expert), with matched\ntraining duration. The total number of steps in each case is computed to match the total training time of 300k\nsteps for 1024-expert Experts Choice with 32 images per group. For context, the dashed line corresponds to\nDense ViT-S/16. Here, Soft MoE outperforms Experts Choice at all capacities, and the optimum point is at\naround 512 experts.\nTokens contributions to slots. While there is no dropping in Soft MoE, it is still possible that some tokens\ncontribute little to all slots if their logits are much lower than those of other tokens. We would like to see if\nsome tokens contribute to slots in a disproportionate manner. Figure 9 (left) shows the distribution across\ntokens for the total weight each token provides to slots (i.e. summed over all slots). This was computed\nover a batch with 256 images with 196 tokens each on a Soft MoE S/16 finetuned on ImageNet. We see\nthere is a heavy tail of tokens that provide a stronger total contribution to slots, and the shape is somewhat\nsimilar across layers. Around 2-5% of the tokens provide a summed weight above 2. Also, between 15% and\n20% of the tokens only contribute up to 0.25 in total weight. The last layer is slightly different, where token\ncontribution is softer tailed. Appendix G further explores this.\nExperts contributions to outputs. Similarly, we would like to understand how much different slots end up\ncontributing to the output tokens. We focus on the case of one slot per expert. We can approximate the total\ncontribution of each expert (equivalently, slot) by averaging their corresponding coefficients in the linear\ncombinations for all output tokens in a batch. Figure 9 (center) shows such (normalized) importance across\nexperts for different MoE layers. We see that, depending on the layer, some experts can impact output tokens\nbetween 3x and 14x more than others.\nNumber of input tokens per slot. For each slot, Figure 9 (right) shows how many input tokens are required to\nachieve a certain cumulative weight in its linear combination. The distribution varies significantly across slots.\nFor a few slots the top 20-25 tokens account for 90% of the slot weight, while for other slots the distribution is\nmore uniform and many tokens contribute to fill in the slot. In general, we see that slots tend to mix a large\nnumber of tokens unlike in standard Sparse MoEs.\nVisual inspection. In order to provide some intuition regarding how slots average input tokens, Figure 10\ngraphically shows the linear combinations for 8 different slots for the image shown in Figure 1. We shade\npatches inversely proportionally to their weight in the slots; note that all tokens representations are eventually\ncombined into a single one (with hidden dimension h) before being passed to the expert (unlike in our plot,\nwhere they are arranged in the usual way). These plots correspond to a Soft MoE S/16 with 128 experts and\none slot per expert, and we handpicked 8 out of the 128 slots to highlight how different slots tend to focus on\ndifferent elements of the image.\n13\nTable 2: Training and finetuning results for Soft MoE and dense models. Finetuning results on ImageNet\nat 384 resolution. We use one slot per expert and did not increase this number during finetuning, thus\nSoft MoEs become cheaper than ViT, as the number of input tokens grows to 576 (patch size 16x16) and 752\n(patch size 14x14) but the number slots is fixed to a much smaller number (either 128 or 256).\nModel\nParams Train steps Train days & exaFLOP Eval Ms/img & GFLOP/img JFT P@1 IN/10s IN/ft\nViT S/16\n33M\n4M (50k)\n153.5\n227.1\n0.5\n9.2\n51.3\n67.6\n84.0\nSoft MoE S/16 128E\n933M\n4M (50k)\n175.1\n211.9\n0.7\n8.6\n58.1\n78.8\n86.8\nSoft MoE S/16 128E\n933M\n10M (50k)\n437.7\n529.8\n0.7\n8.6\n59.2\n79.8\n87.1\nSoft MoE S/14 256E\n1.8B\n4M (50k)\n197.9\n325.7\n0.9\n13.2\n58.9\n80.0\n87.2\nSoft MoE S/14 256E\n1.8B 10M (500k)\n494.7\n814.2\n0.9\n13.2\n60.9\n80.7\n87.7\nViT B/16\n108M\n4M (50k)\n410.1\n864.1\n1.3\n35.1\n56.2\n76.8\n86.6\nSoft MoE B/16 128E\n3.7B\n4M (50k)\n449.5\n786.4\n1.5\n32.0\n60.0\n82.0\n88.0\nViT L/16\n333M\n4M (50k)\n1290.1\n3025.4\n4.9\n122.9\n59.8\n81.5\n88.5\nSoft MoE L/16 128E\n13.1B\n1M (50k)\n338.9\n683.5\n4.8\n111.1\n60.2\n82.9\n88.4\nSoft MoE L/16 128E\n13.1B\n2M (50k)\n677.7\n1367.0\n4.8\n111.1\n61.3\n83.3\n88.9\nSoft MoE L/16 128E\n13.1B\n4M (50k)\n1355.4\n2734.1\n4.8\n111.1\n61.3\n83.7\n88.9\nViT H/14\n669M\n1M (50k)\n1019.9\n2060.2\n8.6\n334.2\n58.8\n82.7\n88.6\nViT H/14\n669M\n2M (50k)\n2039.8\n4120.3\n8.6\n334.2\n59.7\n83.3\n88.9\nSoft MoE H/14 128E\n27.3B\n1M (50k)\n1112.7\n1754.6\n8.8\n284.6\n61.0\n83.7\n88.9\nSoft MoE H/14 128E\n27.3B\n2M (50k)\n2225.4\n3509.2\n8.8\n284.6\n61.7\n84.2\n89.1\nSoft MoE H/14 256E\n54.1B\n1M (50k)\n1276.9\n2110.1\n10.9\n342.4\n60.8\n83.6\n88.9\nSoft MoE H/14 256E\n54.1B\n2M (50k)\n2553.7\n4220.3\n10.9\n342.4\n62.1\n84.3\n89.1\nTable 3: Algorithmic ablation on an S/14 backbone trained for 300k steps (with 256 experts).\nMethod\nExperts\nMixing\nLearned Dispatch\nLearned Combine\nJFT p@1\nIN/10shot\nSoft MoE\n\u2713\n\u2713\n\u2713\n\u2713\n54.3%\n74.8%\nSoft / Uniform\n\u2713\n\u2713\n\u2713\n53.6%\n72.0%\nUniform / Soft\n\u2713\n\u2713\n\u2713\n52.6%\n71.8%\nUniform\n\u2713\n\u2713\n51.8%\n70.0%\nIdentity\n\u2713\n51.5%\n69.1%\nDense ViT\n48.3%\n62.3%\nTable 4: LIT-style evaluation with a ViT-g text tower trained for 18B input images (\u223c 5 epochs).\nModel\nExperts IN/0shot Cifar100/0shot Pet/0shot Coco Img2Text Coco Text2Img\nViT-S/16\n\u2013\n74.2%\n56.6%\n94.8%\n53.6%\n37.0%\nSoft MoE-S/16\n128\n81.2%\n67.2%\n96.6%\n56.0%\n39.0%\nSoft MoE-S/14\n256\n82.0%\n75.1%\n97.1%\n56.5%\n39.4%\nViT-B/16\n\u2013\n79.6%\n71.0%\n96.4%\n58.2%\n41.5%\nSoft MoE-B/16\n128\n82.5%\n74.4%\n97.6%\n58.3%\n41.6%\nViT-L/16\n\u2013\n82.7%\n77.5%\n97.1%\n60.7%\n43.3%\nSoft MoE-L/16\n128\n83.8%\n79.9%\n97.3%\n60.9%\n43.4%\nSouped Soft MoE-L/16\n128\n84.3%\n81.3%\n97.2%\n61.1%\n44.5%\nViT-H/14\n\u2013\n83.8%\n84.7%\n97.5%\n62.7%\n45.2%\nSoft MoE-H/14\n256\n84.6%\n86.3%\n97.4%\n61.0%\n44.8%\n14\n0.25\n1\n2\n3\n4\n5\nper-token sum of its dispatch weights\n0.05\n0.10\n0.15\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.85\n0.90\n0.95\n1.00\ncumulative percentage of tokens\nMoE Layer 6\nMoE Layer 7\nMoE Layer 8\nMoE Layer 9\nMoE Layer 10\nMoE Layer 11\n0\n20\n40\n60\n80\n100\n120\n(sorted) expert id\n1\n2\n4\n6\n8\n10\n12\n14\n(normalized) average combine weight across tokens\nMoE Layer 6\nMoE Layer 7\nMoE Layer 8\nMoE Layer 9\nMoE Layer 10\nMoE Layer 11\n0\n25\n50\n75\n100\n125\n150\n175\n200\ntop-k tokens for slot s\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncumulative weight for top-k tokens\nFigure 9: (Left) Distribution of summed dispatch weights per token for different MoE layers. For instance,\nin layer 11, the dispatch weights for 90-95% of the input tokens summed over all the slots are at most 1. Only\na tiny fraction of tokens contribute to slots by summing more than 3. (Middle) Distribution of combine\nweights per slot (or expert, as we use one slot per expert) summed across all input tokens. We normalize the\nsum by its minimum value across experts. (Right) Each curve corresponds to one slot. Dispatch weights\nfrom all tokens to each slot add up to 1. Distribution of how many inputs tokens are needed to achieve a\ncertain fraction of the total weight for the slot.\n6\nDiscussion\nSparse models can face infrastructural challenges which may have slowed down their broad adoption. Since\nthese models were originally conceived to unlock massive model sizes, they tend to be distributed and most\nrouting algorithms require additional communication costs: additional activations, gradients, or expert\nparameters are sent across devices. This is also true for Soft MoEs, where the experts may also be distributed.\nHowever, modern dense models are now sufficiently large that they are also distributed, thus closing the gap\nin this axis. In addition, the benefits of sparsity shine at small model scales, both in prior work (Riquelme\net al., 2021) and with Soft MoE, fitting with the current needs of the industry for faster inference.\nWe presented Soft MoE, a new sparse Transformer architecture that avoids the discrete token-to-expert\nassignment problem that is common in sparse mixture of experts models. By merging input tokens into linear\ncombinations before dispatching them to experts, we are able to train a fast and fully-differentiable model.\nWe perform extensive image-classification and image-language contrastive learning experiments comparing\nthe performance of dense models and several sparse methods (Tokens Choice, Experts Choice, Soft MoE).\nThese experiments suggest Soft MoE is surprisingly effective and strongly outperforms the other approaches\nwhile often being computationally cheaper. How to deal with causal masking for language decoders is an\nexciting and impactful research direction for future work.\nAcknowledgements\nWe thank Rodolphe Jenatton, who provided extremely valuable feedback on an earlier version of this\nmanuscript; Ilya Tolstikhin, who suggested the \u201cIdentity router\u201d used in Appendix A (or \u201cLiquid router\u201d, as\nhe dubbed it); and the rest of Google DeepMind folks for providing a supportive research environment, very\nespecially to our colleagues in Europe.\n15\nSlot / Expert 1\nSlot / Expert 2\nSlot / Expert 3\nSlot / Expert 4\nSlot / Expert 5\nSlot / Expert 6\nSlot / Expert 7\nSlot / Expert 8\nFigure 10: Linear combinations for 8 slots when using input image in Figure 1. Model is Soft MoE S/16 with\n128 experts and one slot per expert, and it was finetuned on ImageNet. We show results for the first MoE\nlayer (seventh block). The selected slots (among 128) are cherry-picked to highlight differences across slots.\n16\nReferences\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural\nnetworks for faster models. arXiv preprint arXiv:1511.06297, 2015.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George\nNecula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. JAX: composable transformations\nof Python+ NumPy programs, 2018.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.\narXiv preprint arXiv:2209.06794, 2022.\nAidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan\nDamoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language\nmodels. In International Conference on Machine Learning, pages 4057\u20134086. PMLR, 2022.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\nTobias Domhan. How much attention do you need? a granular analysis of neural machine translation\narchitectures. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 1799\u20131808, 2018.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.\nIn Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256.\nJMLR Workshop and Conference Proceedings, 2010.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-\nlevel performance on imagenet classification. In Proceedings of the IEEE international conference on computer\nvision, pages 1026\u20131034, 2015.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver:\nGeneral perception with iterative attention. In International conference on machine learning, pages 4651\u20134664.\nPMLR, 2021.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\nG\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural\nnetworks. Advances in neural information processing systems, 30, 2017.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\nand automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying\ntraining of large, sparse models. In International Conference on Machine Learning, pages 6265\u20136274. PMLR,\n2021.\nTianlin Liu, Joan Puigcerver, and Mathieu Blondel. Sparsity-constrained optimal transport. arXiv preprint\narXiv:2209.15466, 2022.\n17\nMohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing, 2023.\nBasil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal con-\ntrastive learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770,\n2022.\nCedric Renggli, Andr\u00e9 Susano Pinto, Neil Houlsby, Basil Mustafa, Joan Puigcerver, and Carlos Riquelme.\nLearning to merge tokens in vision transformers. arXiv preprint arXiv:2202.12015, 2022.\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto,\nDaniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural\nInformation Processing Systems, 34:8583\u20138595, 2021.\nStephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in\nNeural Information Processing Systems, 34:17555\u201317566, 2021.\nMichael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner:\nWhat can 8 learned tokens do for images and videos? arXiv preprint arXiv:2106.11297, 2021.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness\nof data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages\n843\u2013852, 2017.\nZhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages\n282\u2013298. Springer, 2020.\nYikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe Wang. Multimodal token\nfusion for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 12186\u201312195, June 2022.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\nLiwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International\nConference on Machine Learning, pages 10524\u201310533. PMLR, 2020.\nBrandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized\nconvolutions for efficient inference. Advances in Neural Information Processing Systems, 32, 2019.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113, 2022a.\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas\nBeyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18123\u201318133, 2022b.\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James\nLaudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing\nSystems, 35:7103\u20137114, 2022.\n18\nA\nSoft vs. Uniform vs. Identity dispatch and combine weights\nIn this section, we compare Soft MoE (i.e. the algorithm that uses the dispatch and combine weights computed\nby Soft MoE in eq. (1) and eq. (3)) with different \u201cfixed routing\u201d alternatives, where neither the expert\nselected nor the weight of the convex combinations depend on the content of the tokens.\nWe consider the following simple modifications of Soft MoE:\nIdentity. The first token in the sequence is processed by the first expert, the second token by the second\nexpert, and so on in a round robin fashion. When the sequence length is the same as the number of slots and\nexperts, this is equivalent to replacing the matrix D in eq. (1) (resp. C in eq. (3)) with an identity matrix.\nUniform. Every input slot is filled with a uniform average of all input tokens, and every output token is a\nuniform average of all output slots. This is equivalent to replacing the matrix D from eq. (1) with values 1\nm\nin all elements, and a matrix C from eq. (3) with values\n1\nnp in all elements. We randomly and independently\ninitialize every expert.\nUniform / Soft. Every input slot is filled with a uniform average of all input tokens, but we keep the definition\nof C from eq. (3).\nSoft / Uniform. Every output token is a uniform average of all output slots, but we keep the definition of D\nin eq. (1).\nFigure 11 and Table 3 shows the results from this experiment, training a S/14 backbone model with MoEs on\nthe last 6 layers. Since the sequence length is 256, we choose 256 experts and slots (i.e. 1 slot per expert),\nso that the matrices D and C are squared. As shown in the figure, Soft MoE is far better than all the other\nalternatives. For context, we also add the dense ViT S/14 to the comparison.\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.35\n0.40\n0.45\n0.50\n0.55\nJFT-4B Precision-at-1\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nImageNet 10shot Accuracy\nIdentity\nUniform\nUniform / Soft\nSoft / Uniform\nSoft\nDense\nFigure 11: Soft MoE compared against different \u201cfixed routing\u201d strategies. Identity processes the i-th token\nwith the i-th expert; Uniform replaces both the dispatch and combine matrices with uniform averages; Uniform\n/ Soft replaces the dispatch weights with a uniform average, but the combine weights are computed as in\nSoft MoE; Soft / Uniform does the opposite replacement; and Soft uses the algorithm we present in Section 2.\n19\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nImageNet 10-shot Accuracy\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.46\n0.48\n0.50\n0.52\n0.54\nJFT-4B Precision-at-1\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMaximum Dropping Ratio across Layers\nExperts Choose C=1\nTokens Choose K=1 C=1\nFigure 12: S/14. Performance and amount of token dropping for increasing experts for Experts Choose\n(C = 1) and Tokens Choose (K = 1 and C = 1).\nB\nToken Dropping\nIn this appendix, we briefly explore token dropping for the Experts Choose and Tokens Choose algorithms.\nFor Tokens Choose, each token selects K experts. When experts are full, some tokens assigned to that expert\nwill not be processed. A token is \u201cdropped\u201d when none of its choices go through, and no expert at all\nprocesses the token. Expert Choose algorithms lead to an uneven amount of processing per token: some\ninput tokens are selected by many experts, while some others are not selected by any. We usually define\nthe number of tokens to be processed by each expert in a way that the combined capacity of all experts\ncorresponds to the number of input tokens (or a multiple C of them). If we use a multiplier C higher than\none (say, 2x or 3x), the amount of dropping will decrease but we will pay an increased computational cost.\nThus, we mainly explore the K = 1 and C = 1 setup, where there is no slack in the buffers.\nIn all cases to follow we see a common trend: fixing everything constant, increasing the number of experts\nleads to more and more dropping both in Experts Choose and Tokens Choose.\nFigure 12 compares Experts Choose and Tokens Choose with the same multiplier C = 1. This is the cheapest\nsetup where every token could be assigned to an expert with balanced routing. We see that in both cases the\namount of dropping quickly grows with the number of experts. Moreover, even though Experts Choose has\nhigher levels of dropping (especially for large number of experts), it is still more performant than Tokens\nChoose. Note there is a fundamental difference: when Tokens Choose drops a token, the model wastes that\namount of potential compute. On the other hand, for Experts Choose dropping just means some other token\ngot that spot in the expert buffer, thus the model just transferred compute from one unlucky token to another\nlucky one.\nIn this setup, for a small number of experts (16-32) it is common to observe a \u223c 15% rate of dropping. On the\nother hand, we also experimented with a large number of experts (100-1000) where each expert selects very\nfew tokens. In this case, the dropping rate for Experts Choose can grow above 40-50% in some layers: most\nexperts select the very same tokens. Tokens Choose seems to completely drop up to \u223c25% of the tokens.\nIn Figures 13 and 14 we study how much a little bit of buffer slack (C = 1.125) can help in terms of\nperformance and dropping to Experts Choose and Tokens Choose, respectively. Both plots are similar: the\namount of dropping goes down around \u223c5% and performance slightly increases when the number of experts\nis large. Note that the step time also increases in these cases.\nFinally, Figure 15 shows the effect of Batch Priority Routing (Riquelme et al., 2021) for Tokens Choose. By\nsmartly selecting which tokens to drop we do not only uniformly reduce the amount of dropping, but we\nsignificantly bump up performance.\n20\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nImageNet 10-shot Accuracy\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.46\n0.48\n0.50\n0.52\n0.54\nJFT-4B Precision-at-1\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMaximum Dropping Ratio across Layers\nTokens Choose K=1 C=1\nTokens Choose K=1 C=1.125\nFigure 13: S/14. Performance and amount of token dropping for increasing experts for Tokens Choose with\ntight buffers (K = 1 and C = 1) and some amount of buffer slack (K = 1 and C = 1.125).\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nImageNet 10-shot Accuracy\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.46\n0.48\n0.50\n0.52\n0.54\nJFT-4B Precision-at-1\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMaximum Dropping Ratio across Layers\nExperts Choose C=1\nExperts Choose C=1.125\nFigure 14: S/14. Performance and amount of token dropping for increasing experts for Experts Choose with\ntight buffers (C = 1) and slightly larger buffers (C = 1.125).\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nImageNet 10-shot Accuracy\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.46\n0.48\n0.50\n0.52\n0.54\nJFT-4B Precision-at-1\n4\n8\n16\n32\n64\n128\n256\n512\n1024\ntotal number of experts\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMaximum Dropping Ratio across Layers\nTokens Choose K=1 C=1\nTokens Choose K=1 C=1 No BPR\nFigure 15: S/14. Performance and amount of token dropping for increasing experts with and without BPR\nfor Tokens Choose.\n21\n1\n2\n3\n4\n8\n16\n32\nslots per expert\n0.49\n0.50\n0.51\n0.52\n0.53\nJFT-4B Precision-at-1\n1\n2\n3\n4\n8\n16\n32\nslots per expert\n0.64\n0.66\n0.68\n0.70\n0.72\nImageNet 10-shot Accuracy\n1\n2\n3\n4\n8\n16\n32\nslots per expert\n100\n125\n150\n175\n200\n225\n250\nTrain step time in milliseconds\nSoft-MoE\nExperts Choice (gs = 1 img)\nExperts Choice (gs = 8 img)\nFigure 16: Performance (left, center) and step time (right) of models with 32 experts, but increased slots,\nall trained for the same number of steps (300k). Increasing the number of slots per expert only increases\nperformance of Soft MoE a small amount, while increasing cost substantially.\nC\nSoft MoE Increasing Slots\nIn this section we explore the following question: for a fixed number of experts, how much does Soft MoE\nrouting benefit from having additional slots per expert? Figure 16 shows results for Soft MoE S/16 with\n32 experts. We also show Experts Choice with group sizes of one and eight images. When increasing the\nnumber of slots, the performance grows only modestly, while cost increases quickly. Experts Choice benefits\nmuch more from increased slots, catching up at a large group size, but at a very large cost.\n22\nD\nSparse Layers Placement\nSoft MoE does unlock the effective use of a large number of experts. An important design choice for sparse\nmodels is the number and location of sparse layers, together with the number of experts per layer. Unfortu-\nnately, the large number of degrees of freedom in these choices has usually made thorough ablations and\noptimization unfeasible. In this section, we provide the results of a simple experiment that can help better\ndesign the configuration of sparse models. We fix a total number of experts (E = 512) with one slot per\nexpert, thus leading to matched number of parameters (note in this case FLOPs may vary greatly depending\non the number of sparse layers). Then, for an S/16 backbone architecture, we distribute those experts in\nvarious ways (all in one layer, half of them in two layers, etc) and compare their performance after 300k\ntraining steps. Table 5 shows the results. Again, we observe that a number of experts close to the number\nof input tokens (there are 196 tokens, given the 16x16 patch size for 224x224 images) split over the last few\nlayers works best. Moreover, note these models are indeed cheaper than those in the comparison with 512 or\n256 experts per layer. Table 6 offers results for Tokens Choose routing with K = 1 and BPR Riquelme et al.\n(2021). In this case, all algorithms use a comparable FLOPs count (ignoring slightly increasing routing costs\nwith more experts). Results are essentially similar, thus suggesting optimal expert placement (including\nexpert count and location) may not strongly depend on the routing algorithm.\nTable 5: Expert placing ablation with a Soft MoE S/16 with 12 layers (indexed from 0 to 11).\nSparse Layers\nExperts per Layer\nTotal Experts\nIN/10shot\nJFT prec1\n11\n512\n512\n70.0%\n51.5%\n10\n512\n512\n70.1%\n52.0%\n10, 11\n256\n512\n71.7%\n52.2%\n5, 11\n256\n512\n70.4%\n52.1%\n8, 9, 10, 11\n128\n512\n72.8%\n53.2%\n2, 5, 8, 11\n128\n512\n71.1%\n52.5%\n4:11\n64\n512\n72.1%\n53.1%\n1:4, 8:11\n64\n512\n70.5%\n52.1%\nTable 6: Expert placing ablation with a V-MoE S/16 Tokens Choose K = 1 with 12 layers (indexed as 0:11).\nSparse Layers\nExperts per Layer\nTotal Experts\nIN/10shot\nJFT prec1\n11\n512\n512\n64.4%\n50.1%\n10\n512\n512\n67.2%\n51.9%\n10, 11\n256\n512\n68.6%\n51.3%\n5, 11\n256\n512\n65.3%\n50.6%\n8, 9, 10, 11\n128\n512\n69.1%\n52.3%\n2, 5, 8, 11\n128\n512\n67.3%\n51.1%\n4:11\n64\n512\n69.9%\n52.2%\n1:4, 8:11\n64\n512\n68.0%\n51.2%\n23\nTable 7: Expert placing ablation with a V-MoE S/16 Experts Choose C = 1 with 12 layers (indexed as 0:11).\nSparse Layers\nExperts per Layer\nTotal Experts\nIN/10shot\nJFT prec1\n11\n512\n512\n65.3%\n50.3%\n10\n512\n512\n66.5%\n51.7%\n10, 11\n256\n512\n68.8%\n51.8%\n5, 11\n256\n512\n65.9%\n51.1%\n8, 9, 10, 11\n128\n512\n69.4%\n52.2%\n2, 5, 8, 11\n128\n512\n68.0%\n51.7%\n4:11\n64\n512\n69.0%\n52.2%\n1:4, 8:11\n64\n512\n67.4%\n51.1%\n24\nE\nThe collapse of softmax layers applied after layer normalization\nE.1\nTheoretical analysis\nA softmax layer with parameters \u0398 \u2208 Rn\u00d7d transforms a vector x \u2208 Rd into the vector softmax(\u0398x) \u2208 Rn,\nwith elements:\nsoftmax(\u0398x)i =\nexp((\u0398x)i)\nPn\nj=1 exp((\u0398x)j) =\nexp(Pd\nk=1 \u03b8ikxk)\nPn\nj=1 exp(Pd\nk=1 \u03b8jkxk)\n(4)\nLayer normalization applies the following operation on x \u2208 Rd.\nLN(x)i = \u03b1i\nxi \u2212 \u00b5(x)\n\u03c3(x)\n+ \u03b2i;\nwhere \u00b5(x) = 1\nd\nd\nX\ni=1\nxi and \u03c3(x) =\nv\nu\nu\nt1\nd\nd\nX\ni=1\n(xi \u2212 \u00b5(xi))2\n(5)\nNotice that LN(x) = LN(x \u2212 \u00b5(x)), thus we can rewrite LayerNorm with respect to the centered vector\n\u02dcx = x \u2212 \u00b5(x), and the centered vector scaled to have unit norm \u02c6xi =\n\u02dcxi\n\u2225\u02dcx\u2225:\nLN(\u02dcx)i = \u03b1i\n\u02dcxi\nq\n1\nd\nPd\nj=1 \u02dcx2\nj\n+ \u03b2i =\n\u221a\nd\u03b1i\n\u02dcxi\n\u2225\u02dcx\u2225 + \u03b2i =\n\u221a\nd\u03b1i\u02c6xi + \u03b2i\n(6)\nWhen a softmax layer is applied to the outputs of layer normalization, the outputs of the softmax are given\nby the equation:\nsoftmax(\u0398LN(x))i =\nexp(Pd\nk=1 \u03b8ik(\n\u221a\nd\u03b1k\u02c6xk + \u03b2k))\nPn\nj=1 exp(Pd\nk=1 \u03b8jk(\n\u221a\nd\u03b1k\u02c6xk + \u03b2k))\n(7)\nBy setting \u03d1i = Pd\nk=1 \u03b8ik\u03b1k\u02c6xk, and \u03b4i = Pd\nk=1 \u03b8ik\u03b2k, the previous equation can be rewritten as:\nsoftmax(\u0398LN(x))i =\nexp(\n\u221a\nd\u03d1i + \u03b4i)\nPn\nj=1 exp(\n\u221a\nd\u03d1j + \u03b4j)\n(8)\nDefine m = maxi\u2208[n]\n\u221a\nd\u03d1i \u2212 \u03b4i, M = {i \u2208 [n] :\n\u221a\nd\u03d1i \u2212 \u03b4i = m}. Then, the following equality holds:\nsoftmax(\u0398LN(x))i =\nexp(\n\u221a\nd\u03d1i + \u03b4i \u2212 m)\nPn\nj=1 exp(\n\u221a\nd\u03d1j + \u03b4j \u2212 m)\n(9)\nGiven that limd\u2192\u221e exp(\n\u221a\nd\u03d1i + \u03b4i \u2212 m) =\n(\n1 : i \u2208 M\n0 : i /\u2208 M\nthe output of the softmax tends to:\nlim\nd\u2192\u221e softmax(\u0398LN(x))i =\n(\n1\n|M|\ni \u2208 M\n0\ni /\u2208 M\n(10)\nIn particular, when the maximum is only achieved by one of the components (i.e. |M| = 1), the softmax\ncollapses to a one-hot vector (a vector with all elements equal to 0 except for one).\nE.2\nEmpirical analysis\nThe previous theoretical analysis assumes that the parameters of the softmax layer are constants, or more\nspecifically that they do not depend on d. One might argue that using modern parameter initialization\ntechniques, which take into account\n1\n\u221a\nd in the standard deviation of the initialization Glorot and Bengio\n(2010); He et al. (2015); Klambauer et al. (2017), might fix this issue. We found that they don\u2019t (in particular,\nwe use the initialization from Glorot and Bengio (2010)).\n25\nFigure 17 shows different metric curves during the training of a small SoftMoE model with different model\ndimensions. The model dimensions are those corresponding to different standard backbones: S (384), B\n(768), L (1024), H (1280) and G (1664). The rest of the architecture parameters are fixed: 6 layers (3 dense\nlayers followed by 3 MoE layers with 256 experts), 14x14 patches, and a MLP dimension of 1536. As the\nmodel dimension d increases, the figure shows that, if the inputs to the softmax in the SoftMoE layers are not\nnormalized, the average maximum values of the dispatch and combine weights tend to grow (especially the\nformer). When d is big enough, the ImageNet 10shot accuracy is significantly worse than that achieved by\nproperly normalizing the inputs.\nIn the previous experiment, we trained our model with a linear decay schedule and a peak value of 10\u22123. In\naddition, we also found that applying the softmax layer directly on the output of layer normalization is also\nvery sensible to the learning rate\u2019s configuration. Once again, our recipe suggested in Section 2.3 gives equal\nor better quality, and is generally more stable. Figure 18 shows different metric curves during the training\nof the same small SoftMoE model as before, with a model dimension of d = 1664, using an inverse square\nroot learning rate schedule, with a fixed timescale of 105, a linear warmup phase of 105 steps, and a linear\ncooldown of 5 \u00b7 105 steps, varying the peak learning rate value. In this figure, similarly to the results from the\nprevious experiment, the average maximum values of the dispatch and combine weights grows to values\napproaching 1.0 (indicating a collapse in the softmax layers to a one-hot vector), when the inputs to the\nsoftmax in the SoftMoE layers are not normalized, which eventually severely hurts the accuracy of the model.\nHowever, using the normalization in Section 2.3 gives better accuracy and makes the model less sensible to\nthe choice of the peak value of the learning rate.\n26\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nImageNet 10shot Accuracy\nd = 384\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 768\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1024\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1280\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1664\nNormalization\nNo\nYes\n(a) ImageNet 10shot accuracy.\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAverage maximum dispatch weight\nd = 384\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 768\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1024\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1280\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1664\nNormalization\nNo\nYes\n(b) Average value of the maximum dispatch weight per slot.\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage maximum combine weight\nd = 384\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 768\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1024\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1280\n0\n1\n2\n3\nTrain steps\n\u00d7105\nd = 1664\nNormalization\nNo\nYes\n(c) Average value of the maximum combine weight per token.\nFigure 17: Training plots of the ImageNet 10shot accuracy (top), the average value of the maximum dispatch\nweight per slot (middle) and the average value of the maximum combine weight per token (bottom) for\ndifferent model dimensions d. Observe that maximum values of the combine and (especially) the dispatch\nweights grow as the model dimension grows during training, as our theoretical analysis predicted. Although\nthe ImageNet 10shot accuracy is similar for small model dimensions, applying the softmax layer directly on\nthe output of layer normalization, without any further re-normalization, hurts the accuracy as the model\ndimension d grows. By normalizing the inputs to the softmax as suggested in Section 2.3 improves the\nperformance for large values of d.\n27\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nImageNet 10shot Accuracy\nLearning rate = 0.0003\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.0006\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.001\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.003\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.006\nNormalization\nNo\nYes\n(a) ImageNet 10shot accuracy.\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.0\n0.2\n0.4\n0.6\n0.8\nAverage maximum dispatch weight\nLearning rate = 0.0003\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.0006\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.001\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.003\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.006\nNormalization\nNo\nYes\n(b) Average value of the maximum dispatch weight per slot.\n0\n1\n2\n3\nTrain steps\n\u00d7105\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage maximum combine weight\nLearning rate = 0.0003\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.0006\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.001\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.003\n0\n1\n2\n3\nTrain steps\n\u00d7105\nLearning rate = 0.006\nNormalization\nNo\nYes\n(c) Average value of the maximum combine weight per token.\nFigure 18: Training plots of the ImageNet 10shot accuracy (top), the average value of the maximum dispatch\nweight per slot (middle) and the average value of the maximum combine weight per token (bottom) for\ndifferent peak values of the learning rate, using a model dimension of d = 1664 (i.e. that of a G backbone).\n28\nF\nAdditional Results\nTable 8: Comparison between Top-K with and without BPR.\nModel\nNumber of Experts\nK\nBPR\nJFT prec@1\nIN/10shot\nV-MoE S/16\n32\n1\nNo\n50.1%\n64.5%\nV-MoE S/16\n32\n1\nYes\n51.2%\n68.9%\nV-MoE S/16\n32\n2\nNo\n52.5%\n71.0%\nV-MoE S/16\n32\n2\nYes\n52.8%\n71.4%\nV-MoE S/16\n64\n1\nNo\n50.0%\n64.4%\nV-MoE S/16\n64\n1\nYes\n51.5%\n69.1%\nV-MoE S/16\n64\n2\nNo\n52.9%\n70.9%\nV-MoE S/16\n64\n2\nYes\n52.9%\n71.4%\n103\nTotal Training TPUv3-days\n67%\n73%\n77%\n81%\n84%\nImageNet 10-shot Accuracy\nS/16\nB/16\nL/16\nH/14\n103\nTotal Training TPUv3-days\n51%\n54%\n57%\n59%\nJFT-4B Precision-at-1\nS/16\nB/16\nL/16\nH/14\n103\nTotal Training TPUv3-days\n84%\n85%\n87%\n88%\n89%\nImageNet Finetune Accuracy\nS/16\nB/16\nL/16\nH/14\nSoft MoE\nDense\nFigure 19: Long runs. Soft MoE and ViT models trained for 4 million steps with batch size 4096 (H/14\nmodels trained for 2 million steps instead). Equivalent model classes (S/16, B/16, L/16, H/14) have similar\ntraining costs, but Soft MoE outperforms ViT on all metrics. We show ImageNet 10-shot (left), JFT precision\nat 1 (middle) and ImageNet accuracy after finetuning (right), versus total training FLOPs. See Table 2. We\nreport training FLOPs in Figure 4.\n29\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n0.515\n0.520\n0.525\n0.530\n0.535\n0.540\n0.545\nJFT-4B Precision-at-1\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n0.69\n0.70\n0.71\n0.72\n0.73\n0.74\n0.75\nImageNet 10-shot Accuracy\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n1.00\n1.25\n1.50\n1.75\n2.00\n2.50\n(normalized) Train Step Time\nSoft MoE\nExperts Choice (gs=1 img)\nExperts Choice (gs=8 img)\nExperts Choice (gs=32 img)\nFigure 20: JFT precision-at-1, ImageNet 10-shot accuracy, and normalized training step time when increasing\nthe total number of experts while keeping the total amount of slots fixed. Soft MoE achieves consistently\nbetter results with more experts, whereas cost is kept roughly constant. Adding too many experts to Experts\nChoice hurt performance and significantly increases the cost. Experts Choice can perform well with many\nexperts if we increase the group size up to 32 images per group. The normalized train step time is computed\nwith respect to Soft MoE with 32 experts. Experts Choice with 32 images per group and 4096 experts requires\nmore than 2.5x its cost.\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n0.515\n0.520\n0.525\n0.530\n0.535\n0.540\n0.545\nJFT-4B Precision-at-1\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n0.70\n0.71\n0.72\n0.73\n0.74\n0.75\nImageNet 10-shot Accuracy\n32\n64\n128 256 512 1024 2048 4096\nNumber of Experts\n1.00\n1.25\n1.50\n1.75\n2.00\n2.50\n3.00\n3.50\n4.00\n(normalized) Train Step Time\nSoft MoE\nTokens Choice (gs=8 img)\nTokens Choice (gs=16 img)\nFigure 21: JFT precision-at-1, ImageNet 10-shot accuracy, and normalized training step time when increasing\nthe total number of experts while keeping the total amount of slots fixed. Soft MoE achieves consistently\nbetter results with more experts, whereas cost is kept roughly constant. Adding too many experts to Tokens\nChoice hurt performance and significantly increases the cost. Even with a large group size (16 images),\nTokens Choice struggles to perform well with a few thousand experts. The normalized train step time is\ncomputed with respect to Soft MoE with 32 experts. Tokens Choice with 8 or 16 images per group and 4096\nexperts requires almost 4x its cost.\n30\n101\n102\nTotal Training TPUv3-days\n0.44\n0.46\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58\nJFT-4B Precision-at-1\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nExperts Choice\nTokens Choice\nDense\n(a) JFT-4B Precision-at-1\n101\n102\nTotal Training TPUv3-days\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nImageNet 10-shot Accuracy\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nExperts Choice\nTokens Choice\nDense\n(b) ImageNet 10-shot Accuracy\nFigure 22: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k steps). The size of the\nmarker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent different\nmethods: Soft MoE (blue), Sparse MoEs with Experts Choice (orange) and Tokens Choice routing (green),\nand a Dense (red) model. MoE runs include different configurations.\n101\n102\nTotal Training TPUv3-days\n0.44\n0.46\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58\nJFT-4B Precision-at-1\n101\n102\nTotal Training ExaFLOP\nSoft MoE\nDense\n(a) JFT-4B Precision-at-1\n101\n102\nTotal Training TPUv3-days\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nImageNet 10-shot Accuracy\n101\n102\nTotal Training ExaFLOP\nSoft MoE\nDense\n(b) ImageNet 10-shot Accuracy\nFigure 23: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The\nsize of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent\ndifferent methods: Soft MoE (blue) and Dense (red) models. MoE runs include different configurations. We\nonly show the runs that are not dominated by another model using the same method (S/8 and L/32 were\nalways dominated).\n31\n101\n102\nTotal Training TPUv3-days\n0.46\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58\nJFT-4B Precision-at-1\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nExperts Choice\n(a) JFT-4B Precision-at-1\n101\n102\nTotal Training TPUv3-days\n0.60\n0.65\n0.70\n0.75\n0.80\nImageNet 10-shot Accuracy\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nExperts Choice\n(b) ImageNet 10-shot Accuracy\nFigure 24: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The\nsize of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent\ndifferent methods: Soft MoE (blue) and Sparse MoEs with Experts Choice (orange) models. MoE runs\ninclude different configurations. We only show the runs that are not dominated by another model using the\nsame method (S/8 and L/32 were always dominated).\n101\n102\nTotal Training TPUv3-days\n0.46\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58\nJFT-4B Precision-at-1\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nTokens Choice\n(a) JFT-4B Precision-at-1\n101\n102\nTotal Training TPUv3-days\n0.60\n0.65\n0.70\n0.75\n0.80\nImageNet 10-shot Accuracy\n101\n102\n103\nTotal Training ExaFLOP\nSoft MoE\nTokens Choice\n(b) ImageNet 10-shot Accuracy\nFigure 25: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The\nsize of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent\ndifferent methods: Soft MoE (blue) and Sparse MoEs with Tokens Choice (green) models. MoE runs include\ndifferent configurations. We only show the runs that are not dominated by another model using the same\nmethod (S/8 and L/32 were always dominated).\n32\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n2048\n4096\nNumber of Experts\n0.48\n0.49\n0.50\n0.51\n0.52\n0.53\n0.54\nJFT-4B Precision-at-1\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n2048\n4096\nNumber of Experts\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\nImageNet 10-shot Accuracy\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n2048\n4096\nNumber of Experts\n0.90\n0.95\n1.00\n1.05\n1.10\n1.15\n(normalized) Train Step Time\nSoft MoE\nFigure 26: JFT Precision-at-1, ImageNet 10-shot Accuracy, and normalized Training Step time when\nincreasing the total number of experts while keeping the total amount of slots fixed (4096). Soft MoE\nachieves consistently better results with more experts, whereas cost is kept roughly constant (same FLOPs\nbut communication costs vary due to higher topologies needed for larger models). The normalized train step\ntime is computed with respect to Soft MoE with 32 experts. Model sizes range from 38M (2 experts) to 9.7B\nparameters (4096 experts).\n33\nG\nAdditional analysis\nG.1\nCumulative sum of dispatch and combine weights\nFigure 27 shows the distribution over slots of the cumulative sum (over tokens) of their corresponding\ndispatch weights. For each slot we compute the cumulative sum of the dispatch weights over tokens sorted in\ndecreasing order. This indicates how many tokens are necessary to cover a given percentage of the total mass\nof the weighted average. We compute this cumulative sum for all slots over all the 50 000 ImageNet validation\nimages, across all layers of the Soft MoE H/16 model after finetuning. In the plot, we represent with a solid\nline the average (over all slots and images) cumulative sum, and the different colored areas represent the\ncentral 60%, 80%, 90%, 95% and 99% of the distribution (from darker to lighter colors) of cumulative sums.\nThis tells us, for instance, how uniform is the weighted average over tokens used to compute each input\nslot. In particular, each slot in the last two layers is close to a uniform average of all the tokens (a completely\nuniform average would be represented by a straight line). This tells us that in these layers, every expert\nprocesses roughly the same inputs, at least after the model is trained. However, this weighted average is far\nfrom uniform in the rest of the layers, meaning that there are tokens that contribute far more than others. For\nexample, in layer 28, a few tens of tokens already cover 80% of the weighted average mass. Finally, given\nthe width of the colored areas, we can also see that there\u2019s a significant difference on the weighted averages\ndepending on the slot, across all layers (except maybe the last two). This indicates that the dispatch weights\nvary across different slots and images.\nSimilarly, Figure 28 shows the corresponding plots for the cumulative sum of the combine weights. In this\ncase, for each output token we compute the cumulative sum of the combine weights over slots sorted in\ndecreasing order. Notice that, although the dispatch weights in the last two layers were almost uniform, the\ncombine weights are not. This indicates that some slots (and thus, experts) are more important than others\nin computing the output tokens, and thus their corresponding expert parameters are not redundant. Of\ncourse, the identity of the \u201cimportant\u201d slots may vary depending on the input token.\n34\n0\n20\n40\n60\n80\n100\nCumulative dispatch weights (%)\nBlock 16\nBlock 17\nBlock 18\nBlock 19\n0\n20\n40\n60\n80\n100\nCumulative dispatch weights (%)\nBlock 20\nBlock 21\nBlock 22\nBlock 23\n0\n20\n40\n60\n80\n100\nCumulative dispatch weights (%)\nBlock 24\nBlock 25\nBlock 26\nBlock 27\n0\n250\n500\n750\nNumber of tokens\n0\n20\n40\n60\n80\n100\nCumulative dispatch weights (%)\nBlock 28\n0\n250\n500\n750\nNumber of tokens\nBlock 29\n0\n250\n500\n750\nNumber of tokens\nBlock 30\n0\n250\n500\n750\nNumber of tokens\nBlock 31\nFigure 27: Distribution of the cumulative sum of dispatch weights. For each input slot, we compute the\ncumulative sum of its corresponding dispatch weights (sorted by decreasing value). This indicates over\nhow many input tokens a certain cumulative weight is distributed over. The line in each plot represents the\naverage computed over all slots and ImageNet validation images of the given block in the SoftMoE H/14\nmodel. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker\nto lighter, better seen in color).\n35\n0\n20\n40\n60\n80\n100\nCumulative combine weights (%)\nBlock 16\nBlock 17\nBlock 18\nBlock 19\n0\n20\n40\n60\n80\n100\nCumulative combine weights (%)\nBlock 20\nBlock 21\nBlock 22\nBlock 23\n0\n20\n40\n60\n80\n100\nCumulative combine weights (%)\nBlock 24\nBlock 25\nBlock 26\nBlock 27\n0\n100\n200\nNumber of slots\n0\n20\n40\n60\n80\n100\nCumulative combine weights (%)\nBlock 28\n0\n100\n200\nNumber of slots\nBlock 29\n0\n100\n200\nNumber of slots\nBlock 30\n0\n100\n200\nNumber of slots\nBlock 31\nFigure 28: Distribution of the cumulative sum of combine weights. For each output token, we compute\nthe cumulative sum of its corresponding combine weights (sorted by decreasing value). This indicates over\nhow many output slots a certain cumulative weight is distributed over. The line in each plot represents the\naverage computed over all tokens and ImageNet validation images of the given block in the SoftMoE H/14\nmodel. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker\nto lighter, better seen in color).\n36\nH\nSlot Correlation\nIn this section we explore the correlation between the different slot parameters that Soft MoE learns, and its\nrelationship with the number of slots per expert. Figures 29 to 31 show for each of 6 layers in a Soft MoE\nS/16 the inner product between each pair of (normalized) slot parameter vectors.\nWhile Figure 29 shows no clear relationship between slots from different experts (as each expert only has\none slot), we observe in Figures 30 and 31 how consecutive slots (corresponding to the same expert) are\nextremely aligned. This confirms our hypothesis that adding more slots to experts does not work very well\nas these slots end up aligning their value, and computing somewhat similar linear combinations. Therefore,\nthese projections do not add too much useful information to the different tokens to be processed by the\nexperts (in the extreme, these slots would be identical).\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\nSoft MoE Layer 6\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\nSoft MoE Layer 7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\nSoft MoE Layer 8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\nSoft MoE Layer 9\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\nSoft MoE Layer 10\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\n1\n4\n8\n12\n16\n20\n24\n28\n32\nslots\nSoft MoE Layer 11\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 29: Soft MoE S/16 with 1 slot per expert.\n37\n0\n20\n40\n60\n80\n100\n120\nslots\n0\n20\n40\n60\n80\n100\n120\nslots\nSoft MoE Layer 6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nslots\n0\n20\n40\n60\n80\n100\n120\nslots\nSoft MoE Layer 7\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nslots\n0\n20\n40\n60\n80\n100\n120\nslots\nSoft MoE Layer 8\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nslots\n0\n20\n40\n60\n80\n100\n120\nslots\nSoft MoE Layer 9\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nslots\n0\n20\n40\n60\n80\n100\n120\nslots\nSoft MoE Layer 10\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nslots\n0\n20\n40\n60\n80\n100\n120\nslots\nSoft MoE Layer 11\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 30: Soft MoE S/16 with 4 slots per expert.\n38\n0\n100\n200\n300\n400\n500\nSlots\n0\n100\n200\n300\n400\n500\nSlots\nSoft MoE Layer 6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\nSlots\n0\n100\n200\n300\n400\n500\nSlots\nSoft MoE Layer 7\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\nSlots\n0\n100\n200\n300\n400\n500\nSlots\nSoft MoE Layer 8\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\nSlots\n0\n100\n200\n300\n400\n500\nSlots\nSoft MoE Layer 9\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\nSlots\n0\n100\n200\n300\n400\n500\nSlots\nSoft MoE Layer 10\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\nSlots\n0\n100\n200\n300\n400\n500\nSlots\nSoft MoE Layer 11\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 31: Soft MoE S/16 with 16 slots per expert.\n39\nI\nPareto Models\nTable 9: Model runs from Section 3.3 (shown in Pareto plot) trained for 300k steps on JFT with inverse square\nroot decay and 50k steps cooldown. We trained dense and MoE (Soft MoE, Tokens Choice, Experts Choice)\nmodels with sizes S/32, S/16, S/8, B/32, B/16, L/32, L/16 and H/14. Sorted by increasing training TPUv3 days.\nRef Model Routing\nExperts Group Size K\nC JFT P@1 IN/10shot Train exaFLOP Train Days\n1\nS/32\nDense\n\u2013\n\u2013 \u2013\n\u2013\n42.8\n51.0\n4.2\n6.3\n2\nS/32\nExperts Choice\n32\n392 \u2013\n0.5\n45.5\n56.4\n3.7\n6.9\n3\nS/32\nSoft MoE\n32\n49 \u2013\n\u2013\n48.2\n62.3\n3.8\n7.0\n4\nS/32\nExperts Choice\n32\n49 \u2013\n0.5\n44.3\n54.8\n3.8\n7.0\n5\nS/32\nExperts Choice\n32\n392 \u2013\n1.0\n46.6\n58.2\n4.4\n7.6\n6\nS/32\nExperts Choice\n64\n392 \u2013\n1.0\n46.7\n58.4\n4.4\n7.8\n7\nS/32\nSoft MoE\n64\n49 \u2013\n\u2013\n49.4\n65.1\n4.6\n7.8\n8\nS/32\nExperts Choice\n64\n49 \u2013\n1.0\n45.4\n56.3\n4.6\n7.9\n9\nS/32\nExperts Choice\n32\n49 \u2013\n1.0\n45.5\n56.9\n4.6\n7.9\n10\nS/32\nTokens Choice\n32\n392 1\n1.0\n46.3\n58.7\n4.4\n8.0\n11\nS/32\nTokens Choice\n64\n392 1\n1.0\n47.1\n59.4\n4.4\n8.1\n12\nS/32\nTokens Choice\n32\n392 2\n1.0\n47.4\n60.3\n6.0\n9.3\n13\nS/32\nTokens Choice\n64\n392 2\n1.0\n48.0\n61.9\n6.0\n9.8\n14\nB/32\nDense\n\u2013\n\u2013 \u2013\n\u2013\n48.0\n63.4\n16.0\n11.7\n15\nB/32\nSoft MoE\n32\n49 \u2013\n\u2013\n50.9\n69.7\n14.3\n11.7\n16\nS/32\nTokens Choice\n64\n392 2\n2.0\n48.9\n63.9\n9.0\n12.0\n17\nS/32\nTokens Choice\n32\n392 2\n2.0\n48.5\n62.8\n9.1\n12.1\n18\nS/16\nSoft MoE\n16\n196 \u2013\n\u2013\n50.9\n68.1\n12.4\n14.2\n19\nS/16\nSoft MoE\n32\n196 \u2013\n\u2013\n52.0\n70.8\n12.9\n14.8\n20\nS/16\nDense\n\u2013\n\u2013 \u2013\n\u2013\n47.9\n60.8\n17.0\n15.3\n21\nS/16\nSoft MoE\n64\n196 \u2013\n\u2013\n52.9\n72.3\n13.9\n15.7\n22\nS/16\nSoft MoE\n128\n196 \u2013\n\u2013\n53.6\n73.0\n15.9\n17.6\n23\nB/32\nExperts Choice\n32\n392 \u2013\n0.5\n49.0\n64.3\n13.7\n18.0\n24\nB/32\nExperts Choice\n32\n49 \u2013\n0.5\n47.8\n62.4\n14.3\n18.2\n25\nS/16\nExperts Choice\n128\n196 \u2013\n0.5\n50.2\n66.7\n15.8\n18.5\n26\nS/16\nExperts Choice\n32\n196 \u2013\n1.0\n50.5\n67.4\n17.5\n18.8\n27\nS/16\nExperts Choice\n128\n1568 \u2013\n0.5\n51.4\n67.7\n16.8\n19.7\n28\nB/32\nExperts Choice\n32\n392 \u2013\n1.0\n49.9\n66.0\n16.5\n19.7\n29\nB/32\nExperts Choice\n64\n392 \u2013\n1.0\n49.9\n65.5\n16.5\n19.8\n30\nB/32\nTokens Choice\n32\n392 1\n1.0\n49.7\n66.2\n16.5\n20.0\n31\nB/32\nTokens Choice\n64\n392 1\n1.0\n49.8\n65.6\n16.5\n20.2\n32\nB/32\nSoft MoE\n64\n49 \u2013\n\u2013\n51.8\n70.7\n17.8\n20.3\n33\nB/32\nExperts Choice\n64\n49 \u2013\n1.0\n48.6\n64.0\n17.7\n20.3\n34\nB/32\nExperts Choice\n32\n49 \u2013\n1.0\n48.4\n63.8\n17.7\n20.5\n35\nS/16\nExperts Choice\n32\n1568 \u2013\n1.0\n51.3\n68.7\n21.5\n21.5\n36\nS/16\nSoft MoE\n256\n196 \u2013\n\u2013\n53.8\n73.7\n19.9\n22.1\n37\nS/16\nTokens Choice\n32\n1568 1\n1.0\n51.2\n68.9\n21.5\n23.2\n38\nS/16\nExperts Choice\n256\n196 \u2013\n1.0\n50.7\n67.7\n19.8\n23.3\n39\nS/16\nExperts Choice\n32\n196 \u2013\n2.0\n51.0\n68.3\n23.1\n23.5\n40\nB/32\nTokens Choice\n32\n392 2\n1.0\n50.2\n67.4\n22.0\n23.6\n41\nB/32\nTokens Choice\n64\n392 2\n1.0\n50.8\n68.0\n22.1\n23.8\n42\nS/16\nTokens Choice\n64\n1568 1\n1.0\n51.5\n69.1\n21.3\n24.9\n43\nS/16\nExperts Choice\n256\n1568 \u2013\n1.0\n52.3\n69.7\n21.7\n25.5\n44\nS/16\nExperts Choice\n32\n1568 \u2013\n2.0\n52.4\n70.3\n31.0\n27.8\n45\nS/16\nTokens Choice\n32\n1568 2\n1.0\n52.1\n70.3\n31.0\n30.0\n46\nB/32\nTokens Choice\n64\n392 2\n2.0\n51.2\n70.0\n33.2\n30.4\n40\nTable 9: Model runs from Section 3.3 (shown in Pareto plot) trained for 300k steps on JFT with inverse square\nroot decay and 50k steps cooldown. We trained dense and MoE (Soft MoE, Tokens Choice, Experts Choice)\nmodels with sizes S/32, S/16, S/8, B/32, B/16, L/32, L/16 and H/14. Sorted by increasing training TPUv3 days.\nRef Model Routing\nExperts Group Size K\nC JFT P@1 IN/10shot Train exaFLOP Train Days\n47\nB/32\nTokens Choice\n32\n392 2\n2.0\n51.0\n69.5\n33.6\n31.1\n48\nS/16\nTokens Choice\n64\n1568 2\n1.0\n52.3\n70.4\n31.1\n32.0\n49\nS/16\nTokens Choice\n32\n1568 2\n2.0\n52.8\n71.4\n50.0\n42.5\n50\nS/16\nTokens Choice\n64\n1568 2\n2.0\n52.9\n71.4\n50.1\n45.1\n51\nB/16\nDense\n\u2013\n\u2013 \u2013\n\u2013\n52.0\n71.8\n64.8\n45.2\n52\nB/16\nSoft MoE\n128\n196 \u2013\n\u2013\n55.3\n77.0\n59.0\n46.8\n53\nB/16\nExperts Choice\n128\n1568 \u2013\n0.5\n53.7\n73.0\n59.0\n48.2\n54\nB/16\nExperts Choice\n32\n196 \u2013\n1.0\n53.3\n73.0\n65.6\n51.0\n55\nB/16\nExperts Choice\n128\n196 \u2013\n0.5\n52.5\n72.2\n58.8\n52.6\n56\nL/32\nDense\n\u2013\n\u2013 \u2013\n\u2013\n51.3\n70.9\n55.9\n54.9\n57\nL/32\nExperts Choice\n32\n392 \u2013\n0.5\n52.3\n71.2\n47.4\n55.2\n58\nL/32\nExperts Choice\n32\n49 \u2013\n0.5\n51.1\n70.6\n49.8\n55.7\n59\nL/32\nSoft MoE\n32\n49 \u2013\n\u2013\n53.5\n75.0\n49.8\n56.0\n60\nB/16\nExperts Choice\n32\n1568 \u2013\n1.0\n54.2\n74.5\n73.6\n56.2\n61\nB/16\nTokens Choice\n32\n1568 1\n1.0\n53.7\n74.4\n73.6\n57.8\n62\nB/16\nExperts Choice\n256\n196 \u2013\n1.0\n52.7\n72.7\n73.4\n58.1\n63\nB/16\nSoft MoE\n256\n196 \u2013\n\u2013\n55.8\n78.0\n73.7\n58.2\n64\nB/16\nTokens Choice\n64\n1568 1\n1.0\n54.0\n74.8\n73.2\n58.7\n65\nL/32\nExperts Choice\n64\n392 \u2013\n1.0\n52.7\n72.1\n56.9\n60.4\n66\nB/16\nExperts Choice\n256\n1568 \u2013\n1.0\n53.9\n73.5\n73.8\n60.5\n67\nL/32\nExperts Choice\n32\n392 \u2013\n1.0\n52.7\n71.7\n56.8\n60.6\n68\nL/32\nTokens Choice\n64\n392 1\n1.0\n51.9\n71.4\n56.9\n61.0\n69\nL/32\nTokens Choice\n32\n392 1\n1.0\n52.3\n71.7\n57.1\n61.6\n70\nL/32\nExperts Choice\n64\n49 \u2013\n1.0\n51.1\n70.7\n61.6\n62.6\n71\nL/32\nSoft MoE\n64\n49 \u2013\n\u2013\n54.0\n75.2\n61.7\n62.8\n72\nL/32\nExperts Choice\n32\n49 \u2013\n1.0\n51.4\n70.3\n61.5\n63.2\n73\nB/16\nExperts Choice\n32\n196 \u2013\n2.0\n53.1\n73.9\n86.8\n64.2\n74\nL/32\nTokens Choice\n32\n392 2\n1.0\n51.5\n70.7\n76.0\n72.2\n75\nB/16\nExperts Choice\n32\n1568 \u2013\n2.0\n54.6\n75.6\n102.9\n72.5\n76\nL/32\nTokens Choice\n64\n392 2\n1.0\n52.0\n71.8\n76.0\n72.5\n77\nB/16\nTokens Choice\n32\n1568 2\n1.0\n53.9\n74.7\n102.9\n74.7\n78\nB/16\nSoft MoE\n512\n196 \u2013\n\u2013\n56.1\n78.5\n103.1\n76.5\n79\nB/16\nTokens Choice\n64\n1568 2\n1.0\n54.3\n74.8\n103.0\n76.5\n80\nS/8\nDense\n\u2013\n\u2013 \u2013\n\u2013\n49.9\n66.7\n82.7\n77.7\n81\nS/8\nSoft MoE\n512\n784 \u2013\n\u2013\n56.1\n78.0\n85.6\n88.5\n82\nS/8\nExperts Choice\n32\n784 \u2013\n1.0\n52.9\n72.6\n91.3\n93.0\n83\nL/32\nTokens Choice\n64\n392 2\n2.0\n52.9\n72.9\n114.3\n93.2\n84\nL/32\nTokens Choice\n32\n392 2\n2.0\n52.5\n72.5\n115.7\n95.8\n85\nL/16\nDense\n\u2013\n\u2013 \u2013\n\u2013\n54.8\n77.8\n226.9\n100.9\n86\nL/16\nExperts Choice\n128\n196 \u2013\n0.5\n54.0\n76.7\n204.6\n104.9\n87\nL/16\nSoft MoE\n128\n196 \u2013\n\u2013\n57.2\n80.3\n205.0\n106.0\n88\nB/16\nTokens Choice\n32\n1568 2\n2.0\n54.4\n75.7\n161.4\n108.4\n89\nB/16\nTokens Choice\n64\n1568 2\n2.0\n54.8\n76.0\n161.5\n110.5\n90\nL/16\nExperts Choice\n32\n196 \u2013\n1.0\n55.1\n77.5\n228.6\n113.6\n91\nL/16\nTokens Choice\n32\n1568 1\n1.0\n55.9\n78.5\n250.4\n125.1\n92\nL/16\nTokens Choice\n64\n1568 1\n1.0\n56.2\n78.6\n248.8\n125.7\n93\nS/8\nExperts Choice\n32\n6272 \u2013\n1.0\n53.6\n73.4\n160.6\n126.6\n94\nS/8\nExperts Choice\n512\n784 \u2013\n1.0\n53.4\n72.4\n104.1\n129.0\n41\nTable 9: Model runs from Section 3.3 (shown in Pareto plot) trained for 300k steps on JFT with inverse square\nroot decay and 50k steps cooldown. We trained dense and MoE (Soft MoE, Tokens Choice, Experts Choice)\nmodels with sizes S/32, S/16, S/8, B/32, B/16, L/32, L/16 and H/14. Sorted by increasing training TPUv3 days.\nRef Model Routing\nExperts Group Size K\nC JFT P@1 IN/10shot Train exaFLOP Train Days\n95\nL/16\nSoft MoE\n256\n196 \u2013\n\u2013\n57.4\n80.2\n256.0\n129.6\n96\nS/8\nTokens Choice\n32\n6272 1\n1.0\n53.8\n73.7\n162.5\n129.8\n97\nL/16\nExperts Choice\n256\n196 \u2013\n1.0\n54.1\n76.7\n255.2\n130.1\n98\nL/16\nExperts Choice\n32\n196 \u2013\n2.0\n55.2\n77.8\n301.0\n140.3\n99\nS/8\nExperts Choice\n512\n6272 \u2013\n1.0\n54.8\n74.6\n149.3\n161.9\n100 S/8\nTokens Choice\n32\n6272 2\n1.0\n54.2\n74.6\n243.4\n166.6\n101 H/14\nSoft MoE\n128\n256 \u2013\n\u2013\n58.0\n81.6\n599.2\n170.5\n102 H/14\nDense\n\u2013\n\u2013 \u2013\n\u2013\n56.5\n80.1\n680.5\n196.2\n103 H/14\nExperts Choice\n64\n2048 \u2013 1.25\n57.3\n80.4\n855.9\n210.9\n104 L/16\nTokens Choice\n32\n1568 2\n2.0\n53.5\n74.6\n534.5\n218.5\n105 L/16\nTokens Choice\n64\n1568 2\n2.0\n53.3\n73.3\n535.1\n226.9\n106 H/14\nTokens Choice\n64\n2048 1 1.25\n56.7\n79.8\n857.0\n230.7\n107 S/8\nTokens Choice\n32\n6272 2\n2.0\n54.1\n74.8\n424.4\n255.4\n42\n"
  },
  {
    "title": "ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation",
    "link": "https://arxiv.org/pdf/2308.00906.pdf",
    "upvote": "12",
    "text": "ImageBrush: Learning Visual In-Context Instructions\nfor Exemplar-Based Image Manipulation\nYasheng Sun\u2217\nYifan Yang*\nHouwen Peng\nTokyo Institute of Technology\nMicrosoft\nMicrosoft\nsun.y.aj@m.titech.ac.jp\nyifanyang@microsoft.com\nhouwen.peng@microsoft.com\nYifei Shen\nYuqing Yang\nHan Hu\nMicrosoft\nMicrosoft\nMicrosoft\nyifeishen@microsoft.com\nyuqing.yang@microsoft.com\nhanhu@microsoft.com\nLili Qiu\u2020\nHideki Koike\nMicrosoft\nTokyo Institute of Technology\nliliqiu@microsoft.com\nkoike@c.titech.ac.jp\nInstruction\nQuery Image\nOutput\nInstruction\nQuery Image\nOutput\nInstruction\nQuery Image\nOutput\nFigure 1: Demo results of the proposed ImageBrush framework on various image manipulation tasks. By\nproviding a pair of task-specific examples and a new query image that share a similar context, ImageBrush\naccurately identifies the underlying task and generates the desired output.\nAbstract\nWhile language-guided image manipulation has made remarkable progress, the\nchallenge of how to instruct the manipulation process faithfully reflecting human\nintentions persists. An accurate and comprehensive description of a manipulation\ntask using natural language is laborious and sometimes even impossible, primarily\ndue to the inherent uncertainty and ambiguity present in linguistic expressions. Is\nit feasible to accomplish image manipulation without resorting to external cross-\nmodal language information? If this possibility exists, the inherent modality gap\nwould be effortlessly eliminated. In this paper, we propose a novel manipulation\nmethodology, dubbed ImageBrush, that learns visual instructions for more accurate\nimage editing. Our key idea is to employ a pair of transformation images as visual\ninstructions, which not only precisely captures human intention but also facilitates\naccessibility in real-world scenarios. Capturing visual instructions is particularly\nchallenging because it involves extracting the underlying intentions solely from\nvisual demonstrations and then applying this operation to a new image. To address\n\u2217Equal contribution\n\u2020Corresponding Author\nPreprint. Under review.\narXiv:2308.00906v1  [cs.CV]  2 Aug 2023\nthis challenge, we formulate visual instruction learning as a diffusion-based inpaint-\ning problem, where the contextual information is fully exploited through an iterative\nprocess of generation. A visual prompting encoder is carefully devised to enhance\nthe model\u2019s capacity in uncovering human intent behind the visual instructions.\nExtensive experiments show that our method generates engaging manipulation\nresults conforming to the transformations entailed in demonstrations. Moreover,\nour model exhibits robust generalization capabilities on various downstream tasks\nsuch as pose transfer, image translation and video inpainting.\n1\nIntroduction\nImage manipulation has experienced a remarkable transformation in recent years [49, 14, 36, 71,\n21, 29] . The pursuit of instructing manipulation process to align with human intent has garnered\nsignificant attention. Language, as a fundamental element of human communication, is extensively\nstudied to guide the manipulation towards intended direction [16, 35, 6, 57, 33, 60, 5]. Despite the\nuniversal nature of language-based instructions, there are cases where they fall short in expressing\ncertain world concepts. This limitation necessitates additional efforts and experience in magic prompt\nengineering, as highlighted in [64]. To compensate linguistic ambiguity, some studies [65, 32, 66]\nattempt to introduce visual guidance to the manipulation process. However, these approaches heavily\nrely on cross-modal alignment, which may not always be perfectly matched, thereby resulting in\nlimited adaptability to user instructions.\nIs it conceivable to accomplish image manipulation exclusively through visual instructions? If such a\ncapability were attainable, it would not only mitigate the aforementioned cross-modality disparity\nbut also introduce a novel form of interaction for image manipulation. Taking inspiration from the\nexceptional in-context capabilities by large language models [38, 41, 7] like ChatGPT and GPT-4, we\npropose to conceptualize the image manipulation task as a visual prompting process. This approach\nutilizes paired examples to establish visual context, while employing the target image as a query.\nRecent works [63, 4] reveal the visual in-context ability through a simple Mask Image Modeling [15]\nscheme. But these researches focus on understanding tasks such as detection and segmentation while\nonly very limited context tasks are studied. Therefore, the in-context potential for image manipulation,\nwhere numerous editing choices are involved, is still a promising avenue for exploration.\nIn contrast to language, which primarily communicates abstract concepts, exemplar-based visual\ninstruction explicitly concretizes manipulation operations within the visual domain. In this way,\nthe network could directly leverage their textures, which eases the hallucination difficulty for some\ninexpressible concepts (e.g., artist style or convoluted object appearance). On the other hand, the\npairwise cases directly exhibit transformation relationship from the visual perspective, thereby\nfacilitating better semantic correspondence learning.\nIn this paper, we propose an Exemplar-Based Image Manipulation framework, ImageBrush, which\nachieves adaptive image manipulation under the instruction of a pair of exemplar demonstrations.\nSpecifically, no external information from another modality is needed in our work. The key is to devise\na generative model that tackles both pairwise visual instruction understanding and image synthesis\nin an unified manner. To establish the intra-correlations among exemplar transformations and their\ninter-correlation with the query image, we utilize a grid-like image as model input that concatenates\na manipulation example and a target query as [4, 63, 17]. The in-context image manipulation is\naccomplished by inpainting the lower-right picture. Unlike their approaches forwarding once for\nfinal result, we adopt the diffusion process to iteratively enhance in-context learning and refine the\nsynthesized image. This practice not only provide stable training objectives [32], but also mimics the\nbehavior of a painter [9] who progressively fills and tweaks the details.\nAlthough the aforementioned formulation is capable of handling correlation modeling and image\ngeneration in a single stroke, it places a significant computational burden on the network, due to the\nintricate concepts and processes involved. To address this challenge, we delicately design a visual\nprompting encoder that maximizes the utilization of contextual information, thereby alleviating the\ncomplexity of learning. Specifically, 1) we extract the features of each image and further process\nthem through a transformer module for effective feature exchanging and integration. The obtained\nfeatures are then injected to a diffusion network with cross-attention to augment its understanding\ncapability. 2) Inspired by recent advances of visual prompting [52, 73], we design a bounding box\nincorporation module to take user interactivity into consideration. Different from prior research [52]\n2\nwhich focused on exploring the influence of visual prompt on reasoning ability, our study attempts to\nutilize it to improve generation conformity by better comprehending the human intent.\nOur main contributions can be summarized as follows: 1) We introduce a novel image manipulation\nprotocol that enables the accomplishment of numerous operations through an in-context approach.\nMoreover, we developed a metric to assess the quality of the manipulations performed. 2) We deli-\ncately devise a diffusion-based generative framework coupled with a hybrid context injection strategy\nto facilitate better correlation reasoning. 3) Extensive experiments demonstrate our approach gener-\nates compelling manipulation results aligned with human intent and exhibits robust generalization\nabilities on various downstream tasks, paving the way for future vision foundation models.\n2\nRelated Work\nLanguage-Guided Image Manipulation.\nThe domain of generating images from textual input [46,\n48, 51] has experienced extraordinary advancements, primarily driven by the powerful architecture\nof diffusion models [19, 48, 53, 54]. Given rich generative priors in text-guided models, numerous\nstudies [33, 11, 37, 2, 2] have proposed their adaptation for image manipulation tasks. To guide the\nediting process towards the intended direction, researchers have employed CLIP [45] to fine-tune\ndiffusion models. Although these methods demonstrate impressive performance, they often require\nexpensive fine-tuning procedures [24, 58, 25, 5]. Recent approaches [16, 35] have introduced cross-\nattention injection techniques to facilitate the editing of desired semantic regions on spatial feature\nmaps. Subsequent works further improved upon this technique by incorporating semantic loss [28] or\nconstraining the plugged feature with attention loss [57].\nImage Translation.\nImage translation aims to convert an image from one domain to another\nwhile preserving domain-irrelevant characteristics. Early studies [67, 30, 23, 62, 39] have employed\nconditional Generative Adversarial Networks (GANs) to ensure that the translated outputs conform to\nthe distribution of the target domain. However, these approaches typically requires training a specific\nnetwork for each translation task and relies on collections of instance images from both the source\nand target domains. Other approaches [1, 47, 55, 56, 59] have explored exploiting domain knowledge\nfrom pre-trained GANs or leveraging augmentation techniques with a single image to achieve image\ntranslation with limited data. Another slew of studies [70, 3, 61, 43, 22] focus on exemplar-based\nimage translation due to their flexibility and superior image quality. While the majority of these\napproaches learn style transfer from a reference image, CoCosNet [72] proposes capturing fine\nstructures from the exemplar image through dense correspondence learning.\n3\nMethod\nIn this section, we will discuss the details of our proposed Exemplar-Based Image Manipulation\nFramework, ImageBrush. The primary objective of this framework is to develop a model capable of\nperforming various image editing tasks by interpreting visual prompts as instructions. To achieve this,\nthe model must possess the ability to comprehend the underlying human intent from contextual visual\ninstructions and apply the desired operations to a new image. The entire pipeline is depicted in Fig. 2,\nwhere we employ a diffusion-based inpainting strategy to facilitate unified context learning and image\nsynthesis. To further augment the model\u2019s reasoning capabilities, we meticulously design a visual\nprompt encoding module aimed at deciphering the human intent underlying the visual instruction.\n3.1\nExemplar-Based Image Manipulation\nProblem Formulation.\nGiven a pair of manipulated examples {E, E\u2032} and a query image I, our\ntraining objective is to generate an edited image I\u2032 that adheres to the underlying instructions provided\nby the examples. Accomplishing this objective necessitates a comprehensive understanding of the\nintra-relationships between the examples as well as their inter-correlations with the new image.\nUnlike text, images are not inherently sequential and contain a multitude of semantic information\ndispersed across their spatial dimensions.\nTherefore, we propose a solution in the form of \"Progressive In-Painting\". A common approach is\nto utilize cross-attention to incorporate the demonstration examples {E, E\u2032} and query image I as\ngeneral context like previous work [66]. However, we observed that capturing detailed information\namong visual instructions with cross-attention injection is challenging. To overcome this, we propose\nlearning their low-level context using a self-attention mechanism. Specifically, we concatenate a\n3\n\ud835\udc65!\n\ud835\udc65\"#$\n\ud835\udc65%\nTransformer \ud835\udc3f!\nTransformer \ud835\udc3f\"\n\u00b7\u00b7\u00b7\nVisual Prompt Design\n\ud835\udc38\u2032\n\ud835\udc38\n\ud835\udc3c\nVisual\nEncoder\nLanguage Description\nInterface Design\nBounding Box Drawing\nGrounding \nDINO\n\u00b7\u00b7\u00b7\n\u201cTurning \npomegranates \ninto tomatoes\u201d\nDenoising U-Net \ud835\udf16&\nResidual Block\nSelf Attention\nCross Attention\n\ud835\udc38\u2032\n\ud835\udc38\n\ud835\udc3c\n\ud835\udc3c\u2032\n\ud835\udc38\u2032\n\ud835\udc38\n\ud835\udc3c\nBbox\nEncoder\nBbox\nCoordinates\nVisual\nEncoder\n\ud835\udc3c\u2032\n\ud835\udc52'\n\ud835\udc52(\n\ud835\udc52)\nPrompt\nEncoder\nFigure 2: Illustration of ImageBrush. We introduce a novel and intuitive way of interacting with images. Users\ncan easily manipulate images by providing a pair of examples and a query image as prompts to our system. If\nusers wish to convey more precise instructions, they have the option to inform the model about their areas of\ninterest through manual bounding box annotations or by using language to automatically generate them.\nblank image M to the visual instructions and compose a grid-like image {E, E\u2032, I, M} as illustrated\nin Fig. 2. The objective is to iteratively recover {E, E\u2032, I, I\u2032} from this grid-like image.\nPreliminary on Diffusion Model.\nAfter an extensive examination of recent generative models,\nwe have determined that the diffusion model [48] aligns with our requirements. This model stands\nout due to its stable training objective and exceptional ability to generate high-quality images. It\noperates by iteratively denoising Gaussian noise to produce the image x0. Typically, the diffusion\nmodel assumes a Markov process [44] wherein Gaussian noises are gradually added to a clean image\nx0 based on the following equation:\nxt = \u221a\u03b1tx0 +\n\u221a\n1 \u2212 \u03b1t\u03f5,\n(1)\nwhere \u03f5 \u223c N(0, I) represents the additive Gaussian noise, t denotes the time step and \u03b1t is scalar\nfunctions of t. Our training objective is to devise a neural network \u03f5\u03b8(xt, t, c) to predict the added\nnoise \u03f5. Empirically, a simple mean-squared error is leveraged as the loss function:\nLsimple := E\u03f5\u223cN (0,I),x0,c\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, c)\u22252\n2\ni\n,\n(2)\nwhere \u03b8 represents the learnable parameters of our diffusion model, and c denotes the conditional\ninput to the model, which can take the form of another image [50], a class label [20], or text [25].\nBy incorporating this condition, the classifier-free guidance [46] adjusts the original predicted noise\n\u03f5\u03b8(xt, \u2205) towards the guidance signal \u03f5\u03b8(xt, c), formulated as\n\u02c6\u03f5\u03b8(xt, c) = \u03f5\u03b8(xt, \u2205) + w(\u03f5\u03b8(xt, c) \u2212 \u03f5\u03b8(xt, \u2205)).\n(3)\nThe w is the guidance scale, determining the degree to which the denoised image aligns with the\nprovided condition c.\nContext Learning by Progressive Denoising.\nThe overall generation process is visualized in Fig.2.\nGiven the denoised result xt\u22121 = Grid({E, E\u2032, I, I\u2032}t\u22121) from the previous time step t, our objective\nis to refine this grid-like image based on the contextual description provided in the visual instructions.\nRather than directly operating in the pixel space, our model diffuses in the latent space of a pre-trained\nvariational autoencoder, following a similar protocol to Latent Diffusion Models (LDM)[48]. This\ndesign choice reduces the computational resources required during inference and enhances the quality\nof image generation. Specifically, for an image xt, the diffusion process removes the added Gaussian\nnoise from its encoded latent input zt = E(xt). At the end of the diffusion process, the latent variable\nz0 is decoded to obtain the final generated image x0 = D(z0). The encoder E and decoder D are\nadapted from Autoencoder-KL [48], and their weights are fixed in our implementation.\n4\nUnlike previous studies that rely on external semantic information [29, 6], here we focus on establish-\ning spatial correspondence within image channels. We introduce a UNet-like network architecture,\nprominently composed of self-attention blocks, as illustrated in Fig. 2. This design enables our model\nto attentively process features within each channel and effectively capture their interdependencies.\n3.2\nPrompt Design for Visual In-Context Instruction Learning\nHowever, relying only on universal correspondence modeling along the spatial channel may not\nbe sufficient for comprehending abstract and complex concepts, which often require reassembling\nfeatures from various aspects at multiple levels. To address this issue, we propose an additional\nprompt learning module to enable the model to capture high-level semantics without compromising\nthe synthesis process of the major UNet architecture.\nContextual Exploitation of Visual Prompt.\nGiven the visual prompt vp = {E, E\u2032, I}, we aim to\nexploit their high-level semantic relationships. To achieve this, a visual prompt module comprising\ntwo components is carefully devised, which entails a shared visual encoder ev and a prompt encoder\nep as illustrated in Fig. 2. For an arbitrary image I \u2208 RH\u00d7W \u00d73 within the visual prompt, we extract its\ntokenized feature representation fimg using the Visual Transformer (ViT)-based backbone ev. These\ntokenized features are then fed into the bi-directional transformer ep, which effectively exploits the\ncontextual information. The resulting feature representation fc encapsulates the high-level semantic\nchanges and correlations among the examples.\nUsing the visual prompt, we can integrate high-level semantic information into the UNet architecture\nby employing the classifier-free guidance strategy as discussed in Section 3.1. This is achieved by\ninjecting the contextual feature into specific layers of the main network through cross-attention.\n\u03d5l\u22121 = \u03d5l\u22121 + Conv(\u03d5l\u22121)\n(4)\n\u03d5l\u22121 = \u03d5l\u22121 + SelfAttn(\u03d5l\u22121)\n(5)\n\u03d5l = \u03d5l\u22121 + CrossAttn(fc)\n(6)\nwhere \u03d5l denotes the input feature of the l-th block, and fc represents the context feature. Specifically,\nwe select the middle blocks of the UNet architecture, as these blocks have been shown in recent\nstudies to be responsible for processing high-level semantic information [60].\nInterface Design for Region of User Interest.\nRecent advancements in visual instruction systems\nhave demonstrated impressive interactivity by incorporating human prompts [27]. For example, a\nsimple annotation like a red circle on an image can redirect the attention of a pre-trained visual-\nlanguage model to a specific region [52]. To enhance our model\u2019s ability to capture human intent, we\npropose the inclusion of an interface that allows individuals to emphasize their focal points. This\ninterface will enable our model to better understand and interpret human intent, leading to more\neffective comprehension of instructions.\nTo allow users to specify their area of interest in the examples {E, E\u2032}, we provide the flexibility\nof using bounding boxes. We found that directly drawing rectangles on the example image led\nto difficulties in the network\u2019s understanding of the intention. As a result, we decided to directly\ninject a bounding box into the architecture. For each bounding box Bi = [\u03b1i\nmin, \u03b2i\nmin, \u03b1i\nmax, \u03b2i\nmax]\nwhere \u03b1i\nmin, \u03b2i\nmin are the coordinates of the top-left corner and \u03b1i\nmax, \u03b2i\nmax are the coordinates of\nthe bottom-right corner, we encode it into a token representation using a bounding box encoder eb.\nFormally, this can be expressed as follows:\nf i\nb = eb(Fourier(Bi))\n(7)\nwhere Fourier is the Fourier embedding [34]. Each bounding box feature is stacked together as\nfb, and combined with the previous tokenized image features fimg to form fg = [fimg; fb]. This\ngrounded image feature is then passed through ep for enhanced contextual exploitation. In cases\nwhere a user does not specify a bounding box, we employ a default bounding box that covers the\nentire image.\nTo simulate user input of region of interest, which can be laborious to annotate with bounding\nboxes, we use an automatic tool, GroundingDINO [31], to label the focused region based on textual\ninstruction. This approach not only reduces the burden of manual labling but also provides users\nwith the flexibility to opt for an automatic tool that leverages language descriptions to enhance their\nintentions, in cases where they are unwilling to draw a bounding box. During the training process,\nwe randomly remove a portion of the bounding boxes to ensure that our model takes full advantage\nof the visual in-context instructions.\n5\nTable 1: Quantitative Comparison on In-the-wild Dataset.\nExemplar-Based Image Translation\nPose Transfer\nInpainting\nDataset\nScannet[12]\nLRW(Edge)[10]\nLRW(Mask)[10]\nUBC-Fashion[69]\nDAVIS[42]\nTSAM[74]\n-\n-\n-\n-\n86.84\nCoCosNet[72]\n19.49\n15.44\n14.25\n38.61\n-\nImageBrush\n9.18\n9.67\n8.95\n12.99\n18.70\n1 We report the widely used FID [18] value here.\n4\nExperiments\n4.1\nExperimental Settings\nDatasets.\nOur work leverages four widely used in-the-wild video datasets - Scannet [12], LRW [10],\nUBCFashion [69], and DAVIS [42] - as well as a synthetic dataset that involves numerous image\nediting operations. The Scannet [12] dataset is a large-scale collection of indoor scenes covering\nvarious indoor environments, such as apartments, offices, and hotels. It comprises over 1,500 scenes\nwith rich annotations, of which 1,201 scenes lie in training split, 312 scenes are in the validation set.\nNo overlapping physical location has been seen during training. The LRW [10] dataset is designed\nfor lip reading, containing over 1000 utterances of 500 different words with a duration of 1-second\nvideo. We adopt 80 percent of their test videos for training and 20 percent for evaluation. The\nUBC-Fashion [69] dataset comprises 600 videos covering a diverse range of clothing categories.\nThis dataset includes 500 videos in the training set and 100 videos in the testing set. No same\nindividuals has been seen during training. The DAVIS [42] (Densely Annotated VIdeo Segmentation)\ndataset is a popular benchmark dataset for video object segmentation tasks. It comprises a total of\n150 videos, of which 90 are densely annotated for training and 60 for validation. Similar to previous\nworks [74], we trained our network on the 60 videos in the validation set and evaluate its performance\non the original 90 training videos. The synthetic image manipulation dataset is created using image\ncaptions of LAION Improved Aesthetics 6.5+ through Stable Diffusion by [6]. It comprises over\n310k editing instructions, each with its corresponding editing pairs. Out of these editing instructions,\n260k have more than two editing pairs. We conduct experiment on this set, reserving 10k operations\nfor model validation.\nImplementation Details.\nIn our approach, all input images have a size of 256\u00d7256 pixels and are\nconcatenated to a 512\u00d7512 image as input to the UNet architecture. The UNet architecture, adapted\nfrom Stable Diffusion, consists of 32 blocks, where the cross-attention of both upper and lower blocks\nis replaced with self-attention. We use cross-attention to incorporate the features of the visual prompt\nmodule into its two middle blocks. The visual prompt\u2019s shared backbone, denoted as ev, follows the\narchitecture of EVA-02 [13]. The prompt encoder ep comprises five layers and has a latent dimension\nof 768. For the bounding box encoder, denoted as eb, we adopt a simple MLP following [29]. During\ntraining, we set the classifier-free scale for the encoded instruction to 7.5 and the dropout ratio to\n0.05. Our implementation utilizes PyTorch [40] and is trained on 24 Tesla V100-32G GPUs for 14K\niterations using the AdamW [26] optimizer. The learning rate is set to 1e-6, and the batch size is set\nto 288.\nComparison Methods.\nTo thoroughly evaluate the effectiveness of our approach, we compare it\nagainst other state-of-art models tailored for specific tasks, including: SDEdit [33], a widely-used\nstochastic differential equation (SDE)-based image editing method; Instruct-Pix2pix [6], a cutting-\nedge language-instructed image manipulation method; CoCosNet [72], an approach that employs a\ncarefully designed correspondence learning architecture to achieve exemplar-based image translation;\nand TSAM [74], a model that incorporates a feature alignment module for video inpainting.\n4.2\nQuantitative Evaluation\nEvaluation Metrics.\nIn the image manipulation task, it is crucial for the edited image to align with\nthe intended direction while preserving the instruction-invariant elements in their original form. To\nassess the degree of agreement between the edited image and the provided instructions, we utilize\na cosine similarity metric referred to as CLIP Direction Similarity in the CLIP space. In order to\nmeasure the level of consistency with original image, we utilize the cosine similarity of the CLIP\nimage embedding, CLIP Image Similarity, following the protocol presented in [6]. Additionally, for\nother generation tasks such as exemplar-based image translation, pose transfer, and video in-painting,\n6\nFigure 3: Quantitative Comparison on Image Editing. We compare our approach with the representative\ntext-instruction method in terms of direction consistency and similarity consistency.\nwe employ the Fr\u00e9chet Inception Score (FID) [18] as an evaluation metric. The FID score allows us\nto evaluate the dissimilarity between the distributions of synthesized images and real images.\nImage Generation Paradigm.\nIn the image manipulation task, a pair of transformed images is\nutilized as visual instructions to edit a target image within the same instruction context. For other tasks,\nwe conduct experiments in a cross-frame prediction setting, where we select two temporally close\nframes and use one of them as an example to predict the other. Specifically, we employ one frame\nalong with its corresponding label (e.g., edge, semantic segmentation, keypoints, or masked image)\nas a contextual example, and take the label from another frame as a query to recover that frame. To\nobtain the label information, we extract keypoints from the UBC-Fashion dataset using OpenPose [8],\nand for the LRW dataset, we utilize a face parsing network [68] to generate segmentation labels.\nTo ensure that the selected frames belong to the same context, we restrict the optical flow between\nthem to a specific range, maintaining consistency for both training and testing. It is important to note\nthat, for fair comparison, we always utilize the first frame of each video as a reference during the\nevaluation of the video inpainting task.\nEvaluation Results.\nThe comparison results for image manipulation are presented in Fig. 3. We\nobserve that our results exhibit higher image consistency for the same directional similarity values\nand higher directional similarity values for the same image similarity value. One possible reason is\nthe ability of visual instructions to express certain concepts without a modality gap, which allows our\nmethod to better align the edited images.\nTo demonstrate the versatility of our model, we conducted experiments using in-the-wild videos that\nencompassed diverse real-world contexts. The results for various downstream tasks are presented\nin Table 1, showing the superior performance of our approach across all datasets. It is noteworthy\nthat our model achieves these results using a single model, distinguishing it from other methods that\nrequire task-specific networks.\n4.3\nQualitative Evaluation\nWe provide a qualitative comparison with SDEdit [33] and Instruct-Pix2pix [6] in Figure 4. In the case\nof the SDEdit model, we attempted to input both the edit instruction and the output caption, referred\nto as SDEdit-E and SDEdit-OC, respectively. In contrast to language-guided approaches, our method\ndemonstrates superior fidelity to the provided examples, particularly in capturing text-ambiguous\nconcepts such as holistic style, editing extent, and intricate local object details.\nAdditionally, we provide a qualitative analysis on a real-world dataset in Figure 5. Compared to\nCocosNet [72], our approach demonstrates superior visual quality in preserving object structures,\nas seen in the cabinet and table. It also exhibits higher consistency with the reference appearance,\nparticularly noticeable in the woman\u2019s hair and dress. Furthermore, our method achieves improved\nshape consistency with the query image, as observed in the mouth. When compared to TSAM [74], our\napproach yields superior results with enhanced clarity in object shapes and more realistic background\nfilling. These improvements are achieved by effectively leveraging visual cues from the examples.\n4.4\nFurther Analysis\nNovel Evaluation Metric.\nExemplar-based image manipulation is a novel task, and the evaluation\nmetric in the literature is not suitable for this task. Therefore, we present an evaluation metric\n7\nQuery Image\nInstruct-Pix2pix\nOurs\nEdit Instruction\nThe picture is of a street in Paris\nThe tiger cub is a snow leopard\nTurn it into a snowy landscape\nTurn the bridge into a bridge made of ice\nSDEdit (OC)\nSDEdit (E)\nFigure 4: Qualitative Comparison on Image Manipulation. In contrast to language-guided approaches, our\nediting results guided by visual instruction exhibit better compliance with the provided examples.\nOthers\nGT\nOurs\nScannet\nUBC-Fashion\nLRW\nDAVIS\nInstruction & Query\nFigure 5: Qualitative Comparison on In-the-wild Dataset. We conduct experiments on various downstream\ntasks including exemplar-based image translation, pose transfer and inpainting.\nthat requires neither ground-truth image nor human evaluation. Specifically, our model, denoted\nas \u03a6\u03b8, takes the examples {E, E\u2032}, and the source image I to produce a manipulated output I\u2032.\nThis procedure is presented as I\u2032 = \u03a6\u03b8(I|E\u2032 \u2192 E). The goal is to let the output image I\u2032 abide\nby instruction induced from examples E and E\u2032 while preserving the instruction-invariant content\nof input image I. We define prompt fidelity to assess the model\u2019s ability to follow the instruction.\n8\n!\n!\u2032\n#\u2032\n#\n\u03a6\"(#!|!! \u2192 !)\n\u03a6\"(!|#! \u2192 #)\n!\n!\u2032\n#\u2032\n#\n\u03a6\"(#!|!! \u2192 !)\n\u03a6\"(!|#! \u2192 #)\nFigure 6: Case Study in Terms of Prompt Fidelity and Image Fidelity.\nTable 2: Ablation Study on Image Manipulation. Here we evaluate on our proposed metric.\nw/o Diffusion\nw/o Cross-Attention\nw/o Interest Region\nFull Model\nPrompt Fidelity\n78.65\n39.44\n24.29\n23.97\nImage Fidelity\n82.33\n41.51\n25.74\n24.43\nAccording to the symmetry of {E, E\u2032} and {I, I\u2032}, if its operation strictly follows the instruction, the\nmodel should manipulate E to E\u2032 by using I \u2192 I\u2032 as the prompt (see pictures labeled with a dashed\nyellow tag in Fig. 6). We express the prompt fidelity as follows\n\u2206prompt = FID(E\u2032, \u03a6\u03b8(E|I \u2192 I\u2032)).\n(8)\nOn the other hand, to evaluate the extent to which the model preserves its content, we introduce an\nimage fidelity metric. If I\u2032 maintains the content of I, the manipulation should be invertible. That is\nto say, by using E\u2032 \u2192 E as the prompt, the model should reconstruct I from I\u2032 (see pictures labeled\nwith a dashed blue tag in Fig. 6). Therefore, we define the image fidelity as\n\u2206image = FID(I, \u03a6\u03b8(I\u2032|E\u2032 \u2192 E)).\n(9)\nAblation Study.\nWe performed ablation studies on three crucial components of our method,\nnamely the diffusion process for context exploitation, the vision prompt design, and the injection\nof human-interest area. Specifically, we conducted experiments on our model by (1) replacing the\ndiffusion process with masked image modeling, (2) removing the cross-attention feature injection\nobtained from vision prompt module, and (3) deactivating the input of the human interest region.\nSpecifically, we implemented the masked image modeling using the MAE-VQGAN architecture\nfollowing the best setting of [4]. The numerical results on image manipulation task are shown in\nTable 2. The results demonstrate the importance of the diffusion process. Without it, the model is\nunable to progressively comprehend the visual instruction and refine its synthesis, resulting in inferior\ngeneration results. When we exclude the feature integration from the visual prompt module, the\nmodel struggles to understand high-level semantics, leading to trivial generation results on these\ninstructions. Furthermore, removing the region of interest interface results in a slight decrease in\nperformance, highlighting its effectiveness in capturing human intent.\nCase Study.\nIn Figure 6, we present two cases that showcase our model\u2019s performance in terms of\nimage and prompt fidelity. The first case, depicted on the left, highlights our model\u2019s capability to\nutilize its predicted result I\u2032 to reconstruct I by transforming the image back to a dark appearance.\nWith predicted I\u2032, our model also successfully edits the corresponding example from E to E\u2032, making\nthe example image colorful. On the right side of the figure, we present a case where our model\nencounters difficulty in capturing the change in the background, specifically the disappearance of the\nholographic element.\n5\nConclusion\nIn this article, we present an innovative way of interacting with images, Image Manipulation by\nVisual Instruction. Within this paradigm, we propose a framework, ImageBrush, and incorporate\ncarefully designed visual prompt strategy. Through extensive experimentation, we demonstrate the\n9\neffectiveness of our visual instruction-driven approach in various image editing tasks and real-world\nscenarios.\nLimitation and Future Work. 1) The proposed model may face challenges when there is a substantial\ndisparity between the given instruction and the query image. 2) The current model also encounters\ndifficulties in handling intricate details, such as subtle background changes or adding small objects. 3)\nFuture work can delve into a broader range of datasets and tasks, paving the way for future generative\nvisual foundation models.\nEthical Consideration. Like all the image generation models, our model can potentially be exploited\nfor malicious purposes such as the synthesis of deceptive or harmful content. we are committed to\nlimiting the use of our model strictly to research purposes.\nReferences\n[1] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle: A residual-based stylegan encoder via iterative\nrefinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2021.\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski.\nBlended latent diffusion.\narXiv preprint\narXiv:2206.02779, 2022.\n[3] Aayush Bansal, Yaser Sheikh, and Deva Ramanan. Shapes and context: In-the-wild image synthesis &\nmanipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 2317\u20132326, 2019.\n[4] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\nvia image inpainting.\nIn S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh\n(eds.), Advances in Neural Information Processing Systems, volume 35, pp. 25005\u201325017. Curran As-\nsociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\n9f09f316a3eaf59d9ced5ffaefe97e0f-Paper-Conference.pdf.\n[5] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven\nlayered image and video editing. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XV, pp. 707\u2013723. Springer, 2022.\n[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2023.\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[8] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using\npart affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n7291\u20137299, 2017.\n[9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 11315\u201311325, 2022.\n[10] J. S. Chung and A. Zisserman. Lip reading in the wild. In Asian Conference on Computer Vision, 2016.\n[11] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based\nsemantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.\n[12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner.\nScannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern\nRecognition (CVPR), IEEE, 2017.\n[13] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual\nrepresentation for neon genesis. arXiv preprint arXiv:2303.11331, 2023.\n[14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion,\n2022. URL https://arxiv.org/abs/2208.01618.\n10\n[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 16000\u201316009, June 2022.\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-\nprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.\n[17] Aaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian Curless, and David H Salesin. Image analogies. In\nProceedings of the 28th annual conference on Computer graphics and interactive techniques, pp. 327\u2013340,\n2001.\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. Von Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.\ncc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf.\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840\u20136851, 2020.\n[20] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23\n(47):1\u201333, 2022. URL http://jmlr.org/papers/v23/21-0635.html.\n[21] Lianghua Huang, Di Chen, Yu Liu, Shen Yujun, Deli Zhao, and Zhou Jingren. Composer: Creative and\ncontrollable image synthesis with composable conditions. 2023.\n[22] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image\ntranslation. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\n[23] P. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial\nnetworks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5967\u2013\n5976, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society. doi: 10.1109/CVPR.2017.632. URL\nhttps://doi.ieeecomputersociety.org/10.1109/CVPR.2017.632.\n[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal\nIrani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276,\n2022.\n[25] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for\nrobust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 2426\u20132435, 2022.\n[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,\n2014.\n[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything.\narXiv:2304.02643, 2023.\n[28] Gihyun Kwon and Jong Chul Ye. Diffusion-based image translation using disentangled style and content\nrepresentation. arXiv preprint arXiv:2209.15264, 2022.\n[29] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. Gligen: Open-set grounded text-to-image generation. CVPR, 2023.\n[30] Ming-Yu Liu, Thomas Breuel, and Jan Kautz.\nUnsupervised image-to-image translation net-\nworks.\nIn I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-\nciates, Inc., 2017.\nURL https://proceedings.neurips.cc/paper_files/paper/2017/file/\ndc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf.\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023.\n[32] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal latent diffusion\nfor joint subject and text conditional image generation. arXiv preprint arXiv:2303.09319, 2023.\n11\n[33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSDEdit: Guided image synthesis and editing with stochastic differential equations. In International\nConference on Learning Representations, 2022.\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.\nNerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65\n(1):99\u2013106, 2021.\n[35] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing\nreal images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\n[36] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023.\n[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35,\npp. 27730\u201327744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n[39] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-\nadaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2019.\n[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang,\nZach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\nand Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. Proceedings\nof the 33rd International Conference on Neural Information Processing Systems, 2019.\n[41] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4.\narXiv preprint arXiv:2304.03277, 2023.\n[42] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark\ndataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern\nRecognition, 2016.\n[43] Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun. Semi-parametric image synthesis. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n[44] Lawrence R Rabiner. A tutorial on hidden markov models and selected applications in speech recognition.\nProceedings of the IEEE, 77(2):257\u2013286, 1989.\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\n[46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[47] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-\nOr. Encoding in style: A stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 2287\u20132296, June 2021.\n[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10684\u201310695, 2022.\n[49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n12\n[50] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet,\nand Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Confer-\nence Proceedings, SIGGRAPH \u201922, New York, NY, USA, 2022. Association for Computing Machinery.\nISBN 9781450393379. doi: 10.1145/3528233.3530757. URL https://doi.org/10.1145/3528233.\n3530757.\n[51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022.\n[52] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle?\nvisual prompt engineering for vlms. arXiv preprint arXiv:2304.06712, 2023.\n[53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[54] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019.\n[55] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for\nstylegan image manipulation. ACM Trans. Graph., 40(4), jul 2021. ISSN 0730-0301. doi: 10.1145/\n3450626.3459838. URL https://doi.org/10.1145/3450626.3459838.\n[56] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance\ntransfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n10748\u201310757, 2022.\n[57] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-\ndriven image-to-image translation. Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023.\n[58] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by\nfine tuning an image generation model on a single image. arXiv preprint arXiv:2210.09477, 2022.\n[59] Yael Vinker, Eliahu Horwitz, Nir Zabari, and Yedid Hoshen. Image shape manipulation from a single\naugmented training sample. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 13769\u201313778, 2021.\n[60] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in\ntext-to-image generation. arXiv preprint arXiv:2303.09522, 2023.\n[61] Miao Wang, Guo-Ye Yang, Ruilong Li, Run-Ze Liang, Song-Hai Zhang, Peter M Hall, and Shi-Min Hu.\nExample-guided style-consistent image synthesis from semantic labeling. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 1495\u20131504, 2019.\n[62] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-\nresolution image synthesis and semantic manipulation with conditional gans. 2018 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 8798\u20138807, 2017.\n[63] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\ngeneralist painter for in-context visual learning. Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023.\n[64] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard\nprompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint\narXiv:2302.03668, 2023.\n[65] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text,\nimages and variations all in one diffusion model. 2022. URL https://arxiv.org/abs/2211.08332.\n[66] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen.\nPaint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227,\n2022.\n[67] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-to-image\ntranslation. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2868\u20132876, 2017.\ndoi: 10.1109/ICCV.2017.310.\n13\n[68] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, and Nong Sang. Bisenet v2:\nBilateral network with guided aggregation for real-time semantic segmentation. Int. J. Comput. Vision,\n129(11):3051\u20133068, nov 2021. ISSN 0920-5691. doi: 10.1007/s11263-021-01515-2. URL https:\n//doi.org/10.1007/s11263-021-01515-2.\n[69] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network\nfor pose-guided human video generation. In British Machine Vision Conference, 2019.\n[70] Bo Zhang, Mingming He, Jing Liao, Pedro V Sander, Lu Yuan, Amine Bermak, and Dong Chen. Deep\nexemplar-based video colorization. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 8052\u20138061, 2019.\n[71] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023.\n[72] Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. Cross-domain correspondence learning for\nexemplar-based image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 5143\u20135153, 2020.\n[73] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning?\narXiv preprint arXiv:2301.13670, 2023.\n[74] Xueyan Zou, Linjie Yang, Ding Liu, and Yong Jae Lee. Progressive temporal feature alignment network\nfor video inpainting. In CVPR, 2021.\nAppendices\nA\nAnalysis of In-Context Instruction with Multiple Examples\nOur approach can be naturally extended to include multiple examples. Specifically, given a series of examples\n{E1, E\u2032\n1, . . . , En, E\u2032\nn, I}, where n represents the number of support examples, our objective is to generate\nI\u2032. In our main paper, we primarily focused on the special case where n = 1. When dealing with multiple\nexamples, we could also establish their spatial correspondence by directly concatenating them as input to\nour UNet architecture. Specifically, we create a grid xt\u22121 = Grid({E1, E\u2032\n1, . . . , En, E\u2032\nn, I}t\u22121) that can\naccommodate up to eight examples following [4]. To facilitate contextual learning, we extend the input length of\nour prompt encoder ep, and incorporate tokenized representations of these collections of examples as input to it.\nIn this way, our framework is able to handle cases that involve multiple examples.\nBelow we discuss the impact of these examples on our model\u2019s final performance by varying their numbers and\norders.\nInstruction with 3 Support Examples\nFigure 7: Analysis of In-Context Instruction with Multiple Examples.\n14\nA.1\nNumber of In-Context Examples.\nIn our dataset, which typically consists of 4 examples, we examine the influence of the number of in-context\nexamples by varying it from 1 to 3, as illustrated in Figure 7. We evaluate this variation using our proposed\nmetrics: prompt fidelity \u2206prompt and image fidelity \u2206image, which assess instruction adherence and content\nconsistency, respectively. The results demonstrate that increasing the number of support examples enhances\nperformance, potentially by reducing task ambiguity.\nFurthermore, we present additional instruction results in Figure 9, showcasing the impact of different numbers of\nin-context examples. It is clear that as the number of examples increases, the model becomes more confident in\ncomprehending the task description. This is particularly evident in the second row, where the synthesized floor\nbecomes smoother and the rainbow appears more distinct. Similarly, in the third row, the wormhole becomes\ncomplete. However, for simpler tasks that can be adequately induced with just one example (as demonstrated in\nthe last row), increasing the number of examples does not lead to further performance improvements.\nA.2\nOrder of In-Context Examples.\nWe have also conducted an investigation into the impact of the order of support examples by shuffling them.\nThrough empirical analysis, we found no evidence suggesting that the order of the examples has any impact\non performance. This finding aligns with our initial expectation, as there is no sequential logic present in our\nexamples.\nB\nCase Study on Visual Prompt User Interface\nIn our work, we have developed a human interface to further enhance our model\u2019s ability to understand human\nintent. Since manual labeling of human-intended bounding boxes is not available, we utilize language-grounded\nboxes [31] to train the network in our experiments. In Figure 8, we demonstrate the difference when the bounding\nbox is utilized during inference. We can observe that when the crown area is further emphasized by bounding\nbox, the synthesized crown exhibits a more consistent style with the provided example. Additionally, the dress\nbefore the chest area is better preserved.\nFigure 8: Analysis of Visual Prompt User Interface. On the left side, we illustrate the usual case, while on the\nright side, we present a scenario where a bounding box (indicated by the yellow box) is grounded by DINO as\nthe model input.\nC\nMore Visual Instruction Results\nIn this section, we present additional visual instruction results, where the synthesized result is highlighted within\nthe red box. These results demonstrate the effectiveness of our approach in handling diverse manipulation\ntypes, including style transfer, object swapping, and composite operations, as showcased in Fig. 10 and Fig. 11.\nFurthermore, our method demonstrates its versatility across real-world datasets, successfully tackling various\ndownstream tasks, such as image translation, pose translation, and inpainting in Fig. 12 and Fig. 13. Importantly,\nit is worth noting that all the presented results are generated by a single model.\n15\nFigure 9: Qualitative Analysis on Number of In-Context Examples.\n16\nFigure 10: Image Manipulation Results by Visual Instruction.\n17\nFigure 11: Image Manipulation Results by Visual Instruction.\n18\nFigure 12: Instruction Results on In-the-wild Dataset.\n19\nFigure 13: Instruction Results on In-the-wild Dataset.\n20\n"
  },
  {
    "title": "ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders",
    "link": "https://arxiv.org/pdf/2308.01317.pdf",
    "upvote": "12",
    "text": "ELIXR: Towards a general purpose X-ray arti\u016fcial\nintelligence\nsystem\nthrough\nalignment\nof\nlarge\nlanguage models and radiology vision encoders\nShawn Xu1,*, Lin Yang1,*, Christopher Kelly1,*, Marcin Sieniek1, Timo Kohlberger1, Ma\u0178in Ma1,\nWei-Hung Weng1, Atilla P. Kiraly1, Sahar Kazemzadeh1, Zakkai Melamed1, Jungyeon Park1, Patricia\nStrachan1, Yun Liu1, Chuck Lau2, Preeti Singh1, Christina Chen1, Mozziyar Etemadi3, Sreenivasa\nRaju Kalidindi4, Yossi Matias1, Katherine Chou1, Greg S. Corrado1, Shravya She\u017ay1, Daniel Tse1,\nShruthi Prabhakara1, Daniel Golden1, Rory Pilgrim1, Krish Eswaran1,\u2021, Andrew Sellergren1,\u2021\n1 Google LLC, Mountain View, CA, USA\n2 Work done at Google via Advanced Clinical, Dee\u0176ield, IL, USA\n3 No\u0178hwestern Medicine, Chicago, IL, USA\n4 Apollo Radiology International, Hyderabad, India\nAddress correspondence to: Andrew Sellergren (asellerg@google.com), Google LLC, 1600\nAmphitheatre Parkway, Mountain View, CA 94043, USA\n* Equal contributions.\n\u2021 Equal leadership.\nAbstract\nBackground: A\u0178i\u0171cial intelligence systems for medical imaging have traditionally focused on\nhighly speci\u0171c tasks and have generalized inconsistently to new problems. The combination of\nlarge language models (LLMs) and vision encoders o\u0168ers the potential to address some of these\nchallenges. In this work, we present an approach that enables e\u016bcient training of multimodal\nmodels using routinely collected medical images and their associated text repo\u0178s, and adds the\nability to pe\u0176orm a diverse range of tasks with rich expressive outputs. This approach unlocks\nthe potential for a new generation of medical AI applications, suppo\u0178ing work\u0174ows including\nhigh pe\u0176ormance zero-shot and data-e\u016bcient classi\u0171cation, semantic search, visual question\nanswering (VQA), and radiology repo\u0178 quality assurance (QA).\nMethods: Our approach, which we call Embeddings for Language/Image-aligned X-Rays, or\n\u201cELIXR\u201d, leverages a language-aligned image encoder \u201cgra\u0175ed\u201d via an adapter onto a \u0171xed\nLLM, PaLM 2, to pe\u0176orm a broad range of tasks. We train this lightweight adapter architecture\nusing images paired with corresponding free-text radiology repo\u0178s from the MIMIC-CXR\ndataset. Evaluation of zero-shot and data-e\u016bcient classi\u0171cation was pe\u0176ormed using the public\nCheXpe\u0178 and ChestX-ray14 datasets, as well as a private dataset from \u0171ve hospitals in India.\nSemantic search was evaluated across four themes using the MIMIC-CXR test set. VQA was\nevaluated using the VQA-RAD benchmark and the MIMIC-CXR test set. LLM output for repo\u0178\nQA was evaluated on the MIMIC-CXR test set by a board-ce\u0178i\u0171ed thoracic radiologist.\nResults: ELIXR achieved state-of-the-a\u0178 pe\u0176ormance on zero-shot chest X-ray (CXR)\nclassi\u0171cation (mean AUC of 0.850 across 13 \u0171ndings), data-e\u016bcient CXR classi\u0171cation (mean\nAUCs of 0.893 and 0.898 across \u0171ve \u0171ndings (atelectasis, cardiomegaly, consolidation, pleural\ne\u0168usion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images) training\ndata), and semantic search (0.76 normalized discounted cumulative gain (NDCG) across\nnineteen queries, including pe\u0176ect retrieval on twelve of them). Compared to existing\ndata-e\u016bcient methods including supervised contrastive learning (SupCon), ELIXR required two\norders of magnitude less data to reach similar pe\u0176ormance. ELIXR also showed promise on CXR\nvision-language tasks, demonstrating overall accuracies of 58.7% and 62.5% on visual question\nanswering and repo\u0178 quality assurance tasks, respectively. These results suggest that ELIXR is a\nrobust and versatile approach to CXR AI.\nConclusion: LLM-aligned multimodal models can unlock the value of chest X-rays paired with\nradiology repo\u0178s to solve a variety of previously challenging tasks.\nKeywords: a\u0178i\u0171cial intelligence, medical imaging, deep learning, natural language processing,\nchest X-ray, multimodal fusion, CLIP, BLIP-2\nIntroduction\nThe past decade has witnessed dramatic advances in a\u0178i\u0171cial intelligence (AI) in medical\nimaging. Numerous deep learning systems have been developed that can achieve expe\u0178-level\npe\u0176ormance across a range of medical tasks1. However, clinical and technical limitations have\nresulted in an implementation gap that has impeded the impact of AI in real world health\napplications at scale2. Key challenges include the signi\u0171cant cost of curating high quality\ntraining datasets, restriction of AI development to narrow, highly speci\u0171c tasks, di\u016bculty in\nprocessing multimodal data, and limited interpretability that has hampered e\u0168ective human-AI\ninteraction3.\nUntil recently, AI systems for medical imaging have been largely built using vision-only models,\nincluding convolutional neural networks (CNNs) and vision transformers4. Using a traditional fully\nsupervised\napproach,\ntraining\nCNNs\nand\nvision\ntransformers\nis\nan\nexpensive\nand\ntime-consuming process that requires large quantities of expe\u0178ly annotated data5. In addition,\nsuch networks are usually limited to pe\u0176orming discrete tasks, such as image classi\u0171cation,\nobject detection, and segmentation. On the input side, these networks also take in only images,\nusually of just one modality. In contrast, healthcare work\u0174ows are typically multimodal in nature,\nwith clinicians leveraging a diverse array of inputs (e.g. clinical notes, images, investigations)\nwhen making diagnoses and treatment decisions.\nLarge language models (LLMs) are pa\u0178 of a new generation of versatile transformer-based6 AI\nmodels that are trained on massive datasets and demonstrate previously unseen abilities to\ngeneralize to a range of tasks, despite requiring very li\u017ale task-speci\u0171c data7,8. The multimodal\ncombination of vision models and LLMs presents a range of exciting possibilities including\nzero-shot image-to-text generation that can follow natural language instructions9\u201311. In medical\nimaging, these advances o\u0168er the potential to address limitations of vision-only models by\nenabling model training using ubiquitous medical images that have paired free text repo\u0178s,\nadding capabilities to carry out a diverse range of tasks, accurately coping with the long-tail of\ndiagnoses, enabling true multimodal inference, and presenting new options for expressive\nhuman-computer interaction7.\nIn this paper, we present a lightweight vision-language adapter model called ELIXR (Embeddings\nfor Language/Image-aligned X-Rays), which builds upon prior work9\u201311 to combine or \u201cgra\u0175\u201d a\nvision encoder with a frozen LLM to pe\u0176orm a wide range of vision-language tasks broadly\nrelevant to medical imaging. Our case study in this work focuses on chest X-rays (CXRs) due to\nthe wide availability of image-text paired data, but the methods are applicable to other image\nmodalities. We describe the following key advantages of ELIXR:\n1.\nELIXR\nachieves\nstate-of-the-a\u0178\npe\u0176ormance\nfor\nzero-shot\nclassi\u0171cation,\ndata-e\u016bcient classi\u0171cation, and semantic search of thoracic conditions across a\nrange of datasets. These factors may enable a new class of models that can address\nthe long-tails of diagnoses in the medical domain, and provide a path for more broadly\nuseful AI tools in the diagnostic work\u0174ow.\n2.\nAdapting an image encoder to an LLM using ELIXR is a fast and resource-e\u016bcient\nmethod of training compared to full \u0171netuning of an LLM, leveraging a modest-sized\nfrozen LLM and a data-e\u016bcient frozen vision encoder. Building models on top of ELIXR\ncan be done rapidly to prototype new use cases, adapt to distribution shi\u0175s with a small\namount of new training data, or use alternative publicly available LLMs.\n3.\nELIXR\u2019s synthesis of imaging and text unlocks a new generation of medical AI\napplications. In this study we demonstrate semantic search, visual question answering\n(VQA), and radiology repo\u0178 quality assurance (QA), but there are countless potential\napplications across the medical domain that can be addressed using the proposed\nmultimodal framework.\n4. ELIXR is trained using paired CXR and free text radiology repo\u0178s - data that are\nubiquitous in healthcare. The training process does not require expensive manual label\ncuration by expe\u0178s. Such an approach unlocks the value of routinely collected medical\ndata to develop AI systems at far greater scale and at lower overall cost than previously\npossible.\nMethods\nELIXR system\nWe trained the ELIXR system in two stages (ELIXR-C and ELIXR-B).\nFirst, we trained the ELIXR-C model using Contrastive Language\u2013Image Pre-training (CLIP)9\n(Figure 1a). This uses radiology repo\u0178s to align our previously published pre-trained supervised\ncontrastive learning-based (SupCon) vision-only CXR model12 with a T5 language encoder13. CLIP\nuses a contrastive loss function, which encourages the model to bring the representations of an\nimage and its associated text (in this case, the radiology repo\u0178) closer together in a\nhigh-dimensional space, while simultaneously pushing apa\u0178 representations of mismatched\nimages and text.\nSecond, we trained an LLM-aligned adapter network ELIXR-B (Figure 1b), based on the\nBootstrapping Language-Image Pre-training 2 (BLIP-2) architecture10. ELIXR-B is built directly\nupon ELIXR-C, where it aims to extract location-aware features from the unpooled spatial\nELIXR-C image embedding space and map them to the LLM\u2019s language token space. In this\nwork, we used PaLM 2-S as the LLM14. By serving as an adapter between the image encoder and\nthe LLM, ELIXR-B passes information between vision and language encoders via an a\u017aention\nmechanism, and allows us to leverage the existing knowledge and reasoning abilities of the LLM\nto interpret the images and pe\u0176orm various vision-language tasks (e.g. captioning, VQA). For\ncomputation and data e\u016bciency, we keep both ELIXR-C and PaLM 2-S frozen, and only train the\nadapter between them. This can be thought of as a way of gra\u0175ing an image encoder onto an\nLLM. More speci\u0171cally, following the BLIP-2 architecture10, there are two phases to ELIXR-B\ntraining: vision-language representation learning (phase 1) and vision-language generative\nlearning (phase 2). In the \u0171rst phase, the vision-language model (the Q-Former) is trained to\nunderstand and represent both CXRs and repo\u0178s in a shared embedding space by jointly\nemploying three di\u0168erent tasks: a) image-text contrastive learning (ITC), b) image-grounded text\ngeneration (ITG), and c) image-text matching (ITM). Standard contrastive loss is applied for\nimage-text contrastive learning; image-grounded text generation is modeled as a classi\u0171cation\nproblem (i.e. which token in the vocabulary should be chosen at each output position) and\noptimized by cross-entropy loss; image-text matching is modeled as a binary classi\u0171cation\nproblem (image-text matched/unmatched) and optimized by cross-entropy loss. This results in a\nmodel that can extract key information from the image embeddings and align it with the repo\u0178\ntext embedding space. In the second phase, a multilayer perceptron (MLP) that connects the\nQ-Former with the LLM, and the Q-Former itself are fu\u0178her trained to generate the impressions\nsection of radiology repo\u0178s based upon the image embeddings from ELIXR-B using the LLM.\nThe language modeling (standard cross-entropy) loss is used to guide the training. The result is\nthat the Q-Former is able to produce LLM-aligned tokens based on the image and feed the\nmost useful information to the LLM, while removing irrelevant visual information.\nFigure 1: Architecture of ELIXR. (a) Training and inference of ELIXR-C. (b) Training and\ninference of ELIXR-B. ELIXR-B is trained in two phases. In the \u0171rst phase, the model bootstraps\nvision-language representation in the Q-Former with three learning objectives (image-text\ncontrastive learning (ITC), image-grounded text generation (ITG), image-text matching (ITM)\nlosses) to learn from embeddings from a frozen image encoder. In the second phase, the model\nbootstraps vision-to-language generation from a frozen large language model. The purple text\nboxes represent the learned (unfrozen) components in the training step. Details of the VQA\ninference are fu\u0178her described in the relevant section.\nDatasets\nWe included more than one million CXR images from six datasets in this study as described in\nTable 1: \u0171ve hospitals from India (IND1), a hospital from Illinois, USA (US) (US1), Beth Israel\nDeaconess Medical Center in Massachuse\u017as, USA (MIMIC-CXR), National Institutes of Health\n(NIH) Clinical Center in Maryland, USA (CXR-14), Stanford Health Care in California, USA\n(CheXpe\u0178),\nand\nthe\nVQA-RAD\ndataset\nfrom the National Library of Medicine MedPix\npla\u0179orm15\u201321.\nData from IND1, US1, and the MIMIC-CXR train set were used for training ELIXR-C, while only\ndata from the MIMIC-CXR train set were used for training ELIXR-B. Details of datasets and the\nevaluation tasks in which they were used appear in Table 1. Labels for IND1 and US1 are as\npreviously described12.\nDataset\nIND1\nUS1\nMIMIC-CXR\nCXR-14\nCheXpe\u0178\nVQA-RAD\n(Chest-only)\nDataset usage\nDevelopment\n(train/validation\nsets)\nELIXR-C\nELIXR-C\nELIXR-C and\nELIXR-B\nEvaluation\n(test set)\nData-e\u016bcient\nand zero-shot\nAll tasks\nData-e\u016bcient\nand zero-shot\nData-e\u016bcient\nand zero-shot\nVQA\nDataset and patient statistics\nDataset origin\nFive hospitals in\nIndia\nAn AMC in\nIllinois, USA\nAMC in\nMassachuse\u017as,\nUSA\nNIH, Maryland,\nUSA\nAMC in\nCalifornia, USA\nNIH, Maryland,\nUSA\nNumber of\npatients\n348,335\n12,988\n60,523\n30,805\n65,654\n107\nAge (IQR)\n35-58\n48-71\n43-72\n34-59\nN/A\nN/A\nSex\nFemale:\n133,833 (38.5%)\nMale:\n214,334 (61.5%)\nUnknown:\n168 (< 0.1%)\nFemale:\n6,779 (52.2%)\nMale:\n6,209 (48.8%)\nFemale: 31,610\n(52.2%)\nMale: 28,913\n(48.8%)\nFemale:\n14,175 (46.0%)\nMale:\n16,630 (54.0%)\nN/A\nN/A\nImage and \u0171nding statistics\nNumber of\nimages\n485,082\n165,182\n243,324\n104,278\n223,648\n107\nView (AP/PA)\nAP:\n79,958 (16.5%)\nPA:\n625,735 (83.5%)\nAP:\n108,822 (65.9%)\nPA:\n24,269 (14.7%)\nUnknown:\n32,091 (19.4%)\nAP:\n147,169 (60.4%)\nPA:\n96,155 (39.6%)\nAP:\n44,811 (40.0%)\nPA:\n67,305 (60.0%)\nN/A\nN/A\nAirspace opacity\n43,629 (9.0%)\n15,309 (10.1%)*\n54,769 (22.5%)\n3,485 (3.3%)\n94,328 (30.2%)\nN/A\nFracture\n5,200 (1.1%)\n5,760 (3.8%)*\n4,781 (2.0%)\n546 (0.5%)\n7,436 (2.4%)\nN/A\nPneumothorax\n1,657 (0.3%)\n7,202 (4.8%)*\n11,235 (4.6%)\n5,302 (5.1%)\n17,700 (5.7%)\nN/A\nConsolidation\n15,144 (3.1%)\n6,315 (4.2%)*\n11,525 (4.7%)\n4,667 (4.5%)\n13,015 (4.2%)\nN/A\nPleural e\u0168usion\n1,228 (0.3%)\n33,280 (22.0%*)\n57,721 (23.7%)\n13,317 (12.8%)\n76,963 (24.6%)\nN/A\nPulmonary edema\n1,136 (0.2%)\n34,301 (22.7%)*\n29,331 (12.1%)\n2,303 (2.2%)\n49,717 (15.9%)\nN/A\nAtelectasis\n15,929 (3.3%)\n49,293 (32.6%)*\n48,790 (20.1%)\n11,559 (11.1%)\n29,795 (9.5%)\nN/A\nCardiomegaly\n1,115 (0.2%)\n17,001 (11.3%)*\n47,673 (19.6%)\n2,776 (2.7%)\n23,451 (7.5%)\nN/A\nSuppo\u0178 Devices\n29,698 (6.1%)*\n97,463 (64.6%)*\n73,294 (30.1%)\nN/A\n107,014 (56.2%)\nN/A\nEnlarged\ncardiomediastinum\n349 (0.1%)*\n421 (0.3%)*\n7,657 (3.1%)\nN/A\n9,273 (4.9%)\nN/A\nLung lesion\n7,713 (1.6%)*\n1351 (0.9%)*\n6,632 (2.7%)\nN/A\n7,022 (3.7%)\nN/A\nPleural other\n19,301 (4.0%)*\n1807 (1.2%)*\n2,083 (0.9%)\nN/A\n2,493 (1.3%)\nN/A\nPneumonia\n54 (0.0%)*\n29,816 (19.7%)*\n17,222 (7.1%)\n1,255 (1.2%)\n4,657 (2.4%)\nN/A\nTable 1: Descriptive statistics of the datasets used in the study. *: estimated from radiology\nreports\nEvaluation\nWe demonstrated the utility of ELIXR on \u0171ve CXR-related tasks: (1) zero-shot classi\u0171cation, (2)\ndata-e\u016bcient classi\u0171cation, (3) semantic search, (4) VQA, and (5) repo\u0178 QA (Table 2). Zero-shot\nand data-e\u016bcient image classi\u0171cation as well as semantic search were pe\u0176ormed using\nELIXR-C and ELIXR-B phase 1 (language-aligned image embeddings), while VQA and quality\nassurance were pe\u0176ormed using ELIXR-B phase 2, which combined these embeddings with the\n\u0171xed PaLM 2-S LLM14.\nTask\nInput\nModel output\nMetric\nELIXR versions\nused\nZero-shot\nclassi\u0171cation\nCXR image\nPositive prompt(s)\nNegative prompt(s)\nClassi\u0171cation score\nAUC\nELIXR-C and\nELIXR-B phase 1\nData-e\u016bcient\nclassi\u0171cation\nFor training small nonlinear\nclassi\u0171ers on embeddings:\nvariable amount of CXR\nimages with their\ncorresponding annotations\nFor inference: CXR image\nClassi\u0171cation score\nAUC\nELIXR-C and\nELIXR-B phase 1\nSemantic\nsearch\nText description of search\nterm\nTop-5 CXR images from\nMIMIC-CXR test set that\nare related to the\ndescription\nNDCG@5\nPrecision\nELIXR-C and\nELIXR-B phase 1\nVisual question\nanswering\n(VQA)\nCXR image\nQuestions about the image\nAnswers to given\nquestions based on the\nimage\nAccuracy\nELIXR-B phase 2\nRepo\u0178 quality\nassurance (QA)\nCXR image\nNormal/altered radiology\nrepo\u0178\nDecision about accuracy\nof repo\u0178, along with rich\ntext explanation.\nAccuracy\nELIXR-B phase 2\nTable 2: Downstream chest X-ray tasks that are evaluated in this study. ELIXR-C takes\nimages and/or text as input and outputs embeddings; ELIXR-B takes images and/or text as input\nand outputs embeddings and/or text. Therefore, for text output tasks such as VQA and QA, only\nELIXR-B is used.\nClassi\u016fcation\nFor zero-shot and data-e\u016bcient classi\u0171cation of CXR \u0171ndings, area under the receiver\noperating characteristic curve (AUC) for each \u0171nding class is repo\u0178ed since the tasks are all\nbinary classi\u0171cations. We compared ELIXR-C and ELIXR-B pe\u0176ormance with SupCon and\nprevious SOTA models CheXzero and ConVIRT as baselines22,23. We also evaluated the e\u0168ect of\nvarying the training dataset size.\nZero-shot classi\u0171cation\nTo pe\u0176orm zero-shot classi\u0171cation using ELIXR-C, we adopted a prompting strategy described\npreviously23. Note that because ELIXR-C was pretrained on images and repo\u0178s that include the\n\u0171ndings being classi\u0171ed, \u201czero-shot\u201d refers more to open-ended classi\u0171cation without using\nexplicit labels during training than it does to never having observed these \u0171ndings during\ntraining. The positive and negative text prompts were passed through the text encoder to obtain\ntheir text embeddings. These text embeddings were each average pooled and normalized,\nresulting in one representative embedding for each prompt. A cosine similarity was then\ncalculated between the image embedding and these two text embeddings. A so\u0175max of the\ncosine similarity for the positive prompt was used to produce a classi\u0171cation score so that AUC\ncan be calculated. When there are multiple prompts for the positive or negative case, the mean\nof the cosine similarities of these prompts was taken before computing the so\u0175max. Details of\nthe prompts are listed in Supplementary table 1.\nFor ELIXR-B, the image as well as positive and negative text prompts were passed through the\nQ-Former to get the output image query tokens and the sequence of text embeddings. The \u0171rst\nin the sequence of text embeddings is the single special classi\u0171cation token ([CLS]). The\ncosine similarity is calculated between this classi\u0171cation token and each of the output image\nquery tokens. The highest cosine similarity for the positive prompt and the highest cosine\nsimilarity for the negative prompt are passed to the so\u0175max function to get the \u0171nal\nclassi\u0171cation score.\nData-e\u016bcient classi\u0171cation\nTo pe\u0176orm data-e\u016bcient classi\u0171cation, we followed the same procedure as in Sellergren et al12:\na nonlinear classi\u0171er, an MLP consisting of two layers of 512 and 256 neurons, was trained on top\nof the frozen image encoder. We adopted a learning rate of 0.2, batch size of 512, and trained\nfor 300 epochs with the layer-wise adaptive rate scaling (LARS) optimizer. To make results more\ndirectly comparable to CheXzero and ConVIRT, we also trained linear classi\u0171ers for the 1% and\n10% training data samples.\nStatistical analysis\nFor data-e\u016bcient classi\u0171cation, the AUC was averaged across 10 repeats of randomly\nsubsampled training sets. To obtain an overall summary, the AUCs were (also) averaged across\nall tasks and all datasets (CheXpe\u0178, CXR-14, and IND1) for each training set size. 95%\ncon\u0171dence intervals were calculated using twice the standard error across the 10 randomly\ndrawn train set samples, and hypothesis testing was based on comparing the model closest to\nthe mean pe\u0176ormance across the 10. For the zero-shot models, 95% con\u0171dence intervals were\nestimated and AUCs were compared using the DeLong method24.\nFor semantic search, con\u0171dence intervals were calculated by bootstrapping with 1,000 samples.\np-values were calculated from two-sided permutation tests with 1,000 iterations.\nSemantic search\nFor semantic search (also known as text-image retrieval), we provided queries across four\ntopics, including queries for single \u0171ndings, laterality-speci\u0171c, severity-speci\u0171c, and nuanced\nfeatures. For each query, the analysis focused on the top \u0171ve retrieved images based on model\npredictions. There were seven single \u0171nding queries, three laterality queries, four severity\nqueries, and \u0171ve nuanced queries. We compared ELIXR-C and ELIXR-B against the current\nstate-of-the-a\u0178 model MedCLIP25 using the publicly available code and model checkpoints. The\nMIMIC-CXR test set served as the data pool for retrieval. The full list of 19 queries is provided in\nSupplementary table 2.\nFor semantic search using ELIXR-C, we computed the cosine similarity between the query text\nembedding and the image embeddings from the data pool. The top \u0171ve images with the highest\ncosine similarity were retrieved. For semantic search using ELIXR-B, we adopted the two-stage\nmethod in BLIP-210. In the \u0171rst stage, we retrieved 128 images with the highest cosine similarity\nsimilar to CLIP as the candidates. In the second stage, the image-text matching score (i.e. the\nmatched class probability) was computed to rerank these 128 candidates. The top \u0171ve images\nwith the highest image-text matching score from these 128 candidates were then returned.\nFor evaluation, a board-ce\u0178i\u0171ed thoracic radiologist (CL) scored the semantic search results as\nfollows: 0 = irrelevant or factually incorrect, 1 = close \u0171t to the query, but not the one intended\n(e.g., retrieved \u201cbilateral e\u0168usion\u201d for \u201cright e\u0168usion\u201d query), and 2 = reasonable match.\nPrecision at \u0171ve (precision@5) and normalized discounted cumulative gain at 5 (NDCG@5) were\ncalculated to evaluate the quality of retrieval26. For the ideal rank normalization, we assumed the\ndata pool contained at least \u0171ve matched queries, and thus we set the relevance scores to be all\n2s for the ideal relevance. Precision for cases that had reasonable match (score = 2) and\nprecision for at least somewhat \u0171t (score \u22651) were also measured.\nTo establish the context of the di\u016bculties of the retrieval, we estimated the total count of\nimages with corresponding pathologies. To do so, we adopted an LLM-based approach to\ndetect mislabeling within the MIMIC-CXR test set27,28. Candidates for correction were \u0171rst\nidenti\u0171ed by a keyword search on the radiology repo\u0178s. Next, a medically tuned LLM\n(Med-PaLM 229) was applied to ensure that the label was consistent with the repo\u0178, and a\nboard-ce\u0178i\u0171ed thoracic radiologist (CL) adjudicated cases where the LLM results di\u0168ered from\nthe ground truth in MIMIC-CXR. Details are listed in Supplementary table 3 and Supplementary\n\u0171gure 1.\nVisual question answering\nWe used two di\u0168erent datasets for VQA evaluation of ELIXR-B: a subset of the MIMIC-CXR test\nset, which is from the same domain as a pa\u0178 of the training dataset, and the subset of chest\nx-ray cases (IMAGEORGAN == \u201cCHEST\u201d AND ANSWER != \u201cultrasound\u201d) in the VQA-RAD\ndataset17, from a di\u0168erent domain than the training dataset. A small held-out tuning set\nconsisting only of \u201cyes\u201d or \u201cno\u201d answers from VQA-RAD was used for model checkpoint\nselection. Speci\u0171cally, we used our own test and tuning splits for the 793 VQA-RAD CXR\nquestion and answer pairs, since the original test set (column Phrase_type/QID_para ==\n\u201ctest_freeform\u201d or \u201ctest_para\u201d) shares images with the development set (QID_para ==\n\u201cfreeform\u201d or \u201cpara\u201d). By contrast, the images in our test set (584 questions on 73 images)\nare disjoint from the tune set images, and thus were unseen by the model until evaluation time.\nMoreover, we did not train the model on any VQA-RAD question and answer pairs, but only used\nit for checkpoint selection based on our smaller tuning set of 209 VQA pairs across 25 images.\nWe compared ELIXR-B against the SOTA MedVInT30 model using the publicly available code and\na model checkpoint that, like ours, was not \u0171netuned on VQA-RAD.\nThe second VQA dataset is a subset of cases with \u0171ndings from MIMIC-CXR test set, as labeled\nin the MIMIC-CXR-JPG project27,28, with eight cases for each for the following \u0171ndings: \u201cNo\nFinding\u201d,\n\u201cPneumothorax\u201d,\n\u201cPleural\nE\u0168usion\u201d,\n\u201cEdema\u201d,\n\u201cConsolidation\nOR\nPneumonia\u201d\n(collected as a single category), and \u201cLung Lesion\u201d. See the repo\u0178 quality assurance section\nbelow for fu\u0178her details on the case selection. For each case with a \u0171nding present (with \u0171nding\npresence con\u0171rmed by a board-ce\u0178i\u0171ed thoracic radiologist (CL)), we queried the model with a\nset of \u0171nding-speci\u0171c questions, covering its presence, location, and its severity, size or type,\nwhere applicable. See Supplementary table 4 for the complete question catalog. In addition, we\nasked two additional \u0171nding-independent questions per case, and three \u0171nding-independent\nquestions for cases without any of the above \u0171ndings (category \u201c\u0171nding-independent\u201d in the\nSupplementary table 4).\nSince both phases of ELIXR-B were trained to generate the impression section of a radiology\nrepo\u0178, but neither was tuned to follow instructions, we utilized both the impression generation\nresults from phase 1 (same ITG setup as BLIP-2,10) and LLM-aligned tokens from phase 2 to\nfacilitate the VQA use case. Speci\u0171cally, a\u0175er generating the impression with phase 1, we then\nran inference for the phase 2 model, using the following dialog prompt for the PaLM 2-S LLM\nwherein the LLM-aligned tokens from phase 2 inference were placed at the beginning of the\nprompt at ({aligned\nLLM\ntokens}), the phase 1-generated impression was fed to\n({impression}), and the speci\u0171c question ({question}) was added a\u0175erwards:\n{aligned LLM tokens}\n[Bot]\nI'm a helpful Chest X-ray assistant, I can help you interpret\nthe above image.\n[User] What are the findings?\n[Bot] {impression}\n[User]\nBased\non\nthe\nabove\nchest\nx-ray,\nthe\nfindings,\nand/or\nyour\nmedical knowledge, answer the following question: {question}\n[Bot]\nIn terms of answer grading, model answers that could programmatically be mapped to \u201cyes\u201d or\n\u201cno\u201d and compared against \u201cyes\u201d-or-\u201cno\u201d-expected answers were automatically graded with\n1.0 for a match, and 0.0 otherwise. A board-ce\u0178i\u0171ed thoracic radiologist (CL) graded the\nremaining model answers using correctness scores 0.0, 0.5 or 1.0. See Supplementary table 5\nfor the full scoring rubric.\nFor the MIMIC-CXR test set samples, the same radiologist using the same scoring rubric\nevaluated all 216 answers generated by the model based on their assessment directly of the CXR\nimage and original radiology repo\u0178.\nSensitivity, speci\u0171city and accuracy values were calculated from the average radiologist grades\non respective subsets where the condition was positive, negative or both.\nReport quality assurance\nIn the repo\u0178 quality assurance task, we simulated the situation where a radiology repo\u0178\ncontained errors and used ELIXR-B to identify and suggest corrections to these errors. Errors\nthat we evaluated included swapping laterality of a \u0171nding (\u201cle\u0175\u201d to \u201cright\u201d or vice versa),\nadding an erroneous \u0171nding that was not clearly present in the image, or omi\u017aing a major\n\u0171nding that was present in the image.\nTo evaluate ELIXR\u2019s pe\u0176ormance, we \u0171rst identi\u0171ed a subset of cases with \u0171ndings from the\nMIMIC-CXR test set, as labeled in the MIMIC-CXR-JPG project, that would be most relevant for\nevaluating quality assurance: \u201cNo Finding\u201d, \u201cPneumothorax\u201d, \u201cPleural E\u0168usion\u201d, \u201cEdema\u201d,\n\u201cConsolidation OR Pneumonia\u201d (collected as a single category), and \u201cLung Lesion\u201d. We\nrandomly selected eight cases per \u0171nding. For each case we de\u0171ned the \u201cprimary \u0171nding\u201d as\nthe \u0171nding that was used in the query that yielded that given case, even if other \u0171ndings,\nincluding more clinically signi\u0171cant \u0171ndings, were present in the case. For example, if we\nsearched for cases with pleural e\u0168usion and discovered a case with both pleural e\u0168usion and\npulmonary edema present, the primary \u0171nding for that case was considered to be pleural\ne\u0168usion. We \u0171ltered cases to ensure that (1) an impression section was present in the repo\u0178\n(true for approximately 83% of cases) and (2) the primary \u0171nding was present unambiguously in\nthe image, based on the impression of a board-ce\u0178i\u0171ed thoracic radiologist (CL).\nWithin the set of eight cases for each \u0171nding category, we made an alteration per case as\nfollows: two cases we le\u0175 with unaltered impression sections (the \u201ccontrol\u201d cases), two cases\nhad the primary \u0171nding\u2019s laterality swapped, two cases had the primary \u0171nding removed, and\ntwo cases had an erroneous \u0171nding added. For each alteration, we made fu\u0178her minimal\nmodi\u0171cations to the impression text as needed to ensure that it was internally consistent (e.g., if\nwe added an extraneous \u0171nding of \u201cmoderate pulmonary edema,\u201d we con\u0171rmed that the rest of\nthe repo\u0178 was clinically consistent with this alteration). A summary of these alterations appears\nin Supplementary table 6. For \u201dEdema\u201d, because pulmonary edema tends to be a bilateral\n\u0171nding, we did not include any cases with swapped laterality and instead included three control\ncases, three cases where an extraneous \u0171nding was added, and two cases where the primary\n\u0171nding of \u201dEdema\u201d was removed, still resulting in eight cases total. For the cases labeled as \"No\nFinding\", modi\u0171cations like laterality swapping and \u0171nding removal were not feasible. Therefore,\nwe chose four control cases and four cases in which we introduced a false \u0171nding, providing us\nwith a total of eight cases. A\u0175er making these alterations, we had 48 total cases, each of which\nhad an associated CXR image and an impression section that was either unaltered (control) or\naltered according to the above process.\nTo generate the model outputs for evaluation, we \u0171rst ran ELIXR-B phase 1 inference on the\nimage to produce a repo\u0178 with ELIXR\u2019s \u0171ndings. We then fed the image, the generated repo\u0178,\nand a series of prompts that covered the main possible set of \u0171ndings in chest x-rays to\nELIXR-B phase 2.\nFor each prompt, the Q-Former-encoded image embeddings preceded the text hard prompt as\ninput into PaLM 2-S, and the control or altered impressions section was inlined in place of the\nvariable {altered_impression}. ELIXR provided an assessment as to whether each \u0171nding existed,\nand if so, assessed the laterality in order to suppo\u0178 the laterality swap detection task. The\nprompts were as follows.\n1. If\nthere's\nan\nendotracheal\ntube (ET tube) in the chest x-ray, tell me\nwhether\nit's\nmal-positioned\nor\nwell-positioned.\nIf there's no ET tube,\nrespond 'no'\n2. Is there any evidence of pneumothorax in the chest x-ray? If so, on which\nside(s)?\n3. Are there any signs of pleural effusion present in the x-ray? If so, on\nwhich side(s)?\n4. Are there any visible signs of pulmonary edema in the CXR? If so, on which\nside(s)?\n5. Are\nthere\nany\nsigns\nof\npneumonia\nor\nlung\ninfection?\nIf\nso,\non\nwhich\nside(s)?\n6. Are there any signs of consolidation or lung infection in this patient's\nchest x-ray? If so, on which side(s)?\n7. Are there any signs of atelectasis in the lungs? If so, on which side(s)?\n8. Are there any signs of fibrosis in the lungs? If so, describe it\n9. Are there signs suggestive of a nodule or mass in this patient's chest\nx-ray? If so, on which side(s)?\n10.Is the cardiac silhouette size normal or enlarged?\n11.Is a hiatal hernia present? If so, on which side(s)?\n12.Are there any signs of acute skeletal fracture? If so, where?\nThis initial pa\u0178 of the work\u0174ow was essentially comprehensive VQA. We concatenated these\nresponses into a single piece of text to constitute ELIXR\u2019s comprehensive \u0171ndings.\nWe then fed the concatenated questions and answers into a non-image-aligned LLM,\nMed-PaLM 229, to determine whether there were any missing \u0171ndings, erroneously added\n\u0171ndings, or laterality swaps. The two prompts to do this were as follows. Note that while\nMIMIC-CXR contains full repo\u0178s for most cases, we altered and evaluated only the impression\nsection of the repo\u0178s.\nYou\nare\nan\nexpert\nradiologist.\nThese\nare\nyour\nresponses\nto\na\ncomprehensive\nassessment of a patient's chest x-ray (CXR).\nASSESSMENT: {questions and ELIXR answers separated by new lines}.\nA\nradiology resident has written the following radiology report for the same\nCXR. RESIDENT'S REPORT: {altered_impression}.\nAre there any findings that you mark positive or abnormal in your assessment but\nthat the resident either marks absent/negative or simply does not mention? If\nso, what are the findings?\nYou\nare\nan\nexpert\nradiologist.\nThese\nare\nyour\nresponses\nto\na\ncomprehensive\nassessment of a patient's chest x-ray (CXR).\nASSESSMENT: {questions and ELIXR answers separated by new lines}.\nA\nradiology resident has written the following radiology report for the same\nCXR. RESIDENT'S REPORT: {altered_impression}.\nAre there any findings that you mark negative or normal in your assessment but\nthat the resident marks positive/abnormal in his report? If so, what are the\nfindings?\nLLM responses were graded by a board-ce\u0178i\u0171ed thoracic radiologist (CL) according to the\nrubric in Supplementary table 7. If the LLM correctly described the alteration (or identi\u0171ed an\nunaltered, control repo\u0178 as being correct and complete), the LLM output was scored as correct;\nif the LLM failed to identify the alteration, even if it gave an otherwise correct response, the\noutput was scored as incorrect.\nResults\nELIXR\ndemonstrates\nstate-of-the-art\nzero-shot\nclassi\u016fcation\nperformance comparable to fully supervised SupCon classi\u016fers\ntrained on as many as 224,000 examples\nELIXR-B and ELIXR-C demonstrated zero-shot classi\u0171cation pe\u0176ormance on \u0171ve \u0171ndings\n(\u201catelectasis\u201d, \u201ccardiomegaly\u201d, \u201cconsolidation\u201d, \u201cpleural e\u0168usion\u201d, and \u201cpulmonary edema\u201d)\nthat was comparable to SupCon\u2019s classi\u0171cation pe\u0176ormance when trained on the entirety of the\nCheXpe\u0178 train set (~224,000 examples). Note that although ELIXR\u2019s vision encoder was\ninitialized from a SupCon checkpoint, CheXpe\u0178 was not used for pretraining SupCon at all, only\nfor training small downstream classi\u0171ers. Thus, CheXpe\u0178 is completely held out from ELIXR-B\nand ELIXR-C zero-shot.\nAcross the 13 \u0171ndings (excluding \u201cNo Finding\u201d) from the CheXpe\u0178 test set, ELIXR-C and\nELIXR-B both surpassed the state-of-the-a\u0178 zero-shot pe\u0176ormance from CheXzero23. Figure 2\nshows the details of the pe\u0176ormance comparison for zero-shot classi\u0171cation. Positive and\nnegative texts for prompt tuning in zero-shot classi\u0171cation are listed in Supplementary table 1.\nFigure 2: ELIXR demonstrated state-of-the-a\u0178 zero-shot classi\u0171cation pe\u0176ormance\ncomparable to label-e\u016bcient method supervised contrastive learning (SupCon). AUCs\nand 95% con\u0171dence intervals for zero-shot classi\u0171cation for ELIXR-B, ELIXR-C, and CheXzero\nensemble23 across 11 \u0171ndings (including only \u0171ndings with >5 positives and excluding \u201cNo\nFinding\u201d label) as well as SupCon fully-supervised (trained on the entire 224K examples of\nCheXpe\u0178) classi\u0171cation for \u0171ve \u0171ndings on the CheXpe\u0178 test dataset. Full results with p-values\nare available in Supplementary table 8.\nELIXR-B\nand\nELIXR-C\nboth\nset\na\nnew\nstate-of-the-a\u0178 for data-e\u016bcient linear probe\nclassi\u0171cation on CheXpe\u0178\u2019s \u0171ve main \u0171ndings (\u201catelectasis\u201d, \u201ccardiomegaly\u201d, \u201cconsolidation\u201d,\n\u201cpleural e\u0168usion\u201d, \u201cpulmonary edema\u201d) using 1% and 10% of the train set, outpe\u0176orming even\nthe fully-\u0171netuned ConVIRT22. ELIXR-B and ELIXR-C also both demonstrated data-e\u016bcient\npe\u0176ormance\nsuperior\nto\nSupCon\n(Figure\n3)\nor,\nto\nput\nit another way, demonstrated\ndata-e\u016bcient pe\u0176ormance equivalent to SupCon using roughly two orders of magnitude less\ntraining data (e.g. ELIXR-B and ELIXR-C 64-shot pe\u0176ormance was noninferior to SupCon\n4096-shot pe\u0176ormance; see Supplementary tables 9 and 10 for p-values). Table 3 shows a\nsummary of comparisons between ELIXR and the SOTA for zero-shot and data-e\u016bcient.\nFigure 3: E\u0168ect of using ELIXR-C, ELIXR-B, and supervised contrastive learning (SupCon)\nfor data-e\u016bcient classi\u0171cation. The repo\u0178ed pe\u0176ormance is averaged across 2 datasets\n(CheXpe\u0178 and Chest X-ray14) and seven \u0171ndings: atelectasis, cardiomegaly, airspace opacity,\nfracture, pneumothorax, consolidation, pleural e\u0168usion, and pulmonary edema. Both ELIXR-C\nand ELIXR-B demonstrate superior pe\u0176ormance compared to SupCon at matching dataset\nsizes, or, put another way, demonstrate pe\u0176ormance on par with SupCon with two orders of\nmagnitude less data (red and blue lines are translated two grid lines to the le\u0175 from the black\nline). Detailed per-dataset and per-\u0171nding graphs are available in Supplementary \u0171gure 2.\nDelong\u2019s test results are available in Supplementary tables 9, 10.\nMean AUC CheXpe\u0178 test\n(5 main \u0171ndings)\nMean AUC CheXpe\u0178 test\n(13 \u0171ndings)\nZero shot\nCheXzero\n0.889\n0.838\nELIXR-C\n0.851\n0.850\nELIXR-B\n0.837\n0.846\n1% training data\nConVIRT linear\n0.859\n--\nConVIRT \u0171netune\n0.870\n--\nELIXR-C linear\n0.887\n--\nELIXR-B linear\n0.893\n--\n10% training data\nConVIRT linear\n0.868\n--\nConVIRT \u0171netune\n0.881\n--\nELIXR-C linear\n0.889\n--\nELIXR-B linear\n0.898\n--\nTable\n3:\nComparison\nof\nELIXR\nagainst\nstate-of-the-a\u0178\nmodels,\nConVIRT\nand\nCheXzero22,23. ELIXR sets a new state of the a\u0178, as measured by mean AUC, for zero-shot\nclassi\u0171cation of 13 \u0171ndings in CheXpe\u0178 and data-e\u016bcient classi\u0171cation (1% and 10% training\ndata) of 5 main \u0171ndings in CheXpe\u0178.\nELIXR enables state-of-the-art semantic search for \u016fndings using\nlaterality-speci\u016fc, severity-based, and nuanced terminology\nELIXR-B outpe\u0176ormed both ELIXR-C and the state-of-the-a\u0178 MedCLIP25 on the retrieval quality\nof top-5 retrieved images. For each query group, we computed the average metrics across the\nqueries. NDCG@5 and Precision@5 of ELIXR-B were consistently be\u017aer than ELIXR-C across all\nquery groups. ELIXR-B scored higher than MedCLIP on NDCG@5 for all query groups and on\nPrecision@5 (score = 2) for three out of four query groups (Table 4).\nPrecision@5\n(score=2)\nPrecision@5\n(score=1)\nNDCG@5\nFindings\nMedCLIP\n0.29\n0.29\n0.25\nELIXR-C\n0.63\n0.66\n0.66\nELIXR-B\n0.74\n0.74\n0.74\nLaterality\nMedCLIP\n0.77\n1\n0.76\nELIXR-C\n0.73\n0.8\n0.83\nELIXR-B\n0.93\n0.93\n0.94\nSeverity\nMedCLIP\n0.63\n0.75\n0.66\nELIXR-C\n0.35\n0.7\n0.53\nELIXR-B\n0.55\n0.7\n0.68\nNuanced\nMedCLIP\n0.54\n0.68\n0.54\nELIXR-C\n0.6\n0.84\n0.73\nELIXR-B\n0.64\n0.84\n0.74\nTotal\nMedCLIP\n0.50 [0.34-0.64]\n0.60 [0.40-0.77]\n0.49 [0.31-0.63]\nELIXR-C\n0.66 [0.52-0.78],\n0.154\n0.74 [0.60-0.86],\np=0.296\n0.68 [0.53-0.81],\np=0.0912\nELIXR-B\n0.75 [0.57-0.88],\np=0.047\n0.79 [0.63-0.91],\np=0.143\n0.76 [0.59-0.89],\np=0.0234\nTable 4: Quantitative analysis of CXR semantic search using ELIXR-C and ELIXR-B.\nELIXR-C demonstrated the highest NDCG@5 scores across all four query groups and the\nhighest Precision scores across three of four query groups as compared to ELIXR-B and the\nstate-of-the-a\u0178 MedCLIP. Con\u0171dence intervals were calculated from bootstrapping; p-values\nwere calculated from permutation tests between ELIXR-B and MedCLIP or ELIXR-C and\nMedCLIP.\nOn twelve out of nineteen queries, ELIXR-B demonstrated pe\u0176ect retrieval, including for\nlaterality-speci\u0171c queries like \u201cright pleural e\u0168usion,\u201d severity-speci\u0171c queries like \u201cmoderate\ncardiomegaly,\u201d and nuanced queries like \u201cnasogastric tip reaches stomach.\u201d Notably, we found\nboth ELIXR-C and ELIXR-B pe\u0176ormed worse on fracture- and pneumothorax-related queries\nthan queries related to other \u0171ndings. This could be pa\u0178ially due to the low prevalence of these\ntwo pathologies in the MIMIC test set (4% for fracture and 3% for pneumothorax) compared to\n>10% prevalence for other pathologies. See Supplementary table 2 for a complete list of\nper-query scores.\nFinding (Both Correct)\nNuanced (Both Correct)\nLaterality (Both Correct)\nFinding (Both Incorrect)\nFigure 4: Demonstration of semantic search using ELIXR. Three of four examples here (top\nle\u0175, top right, bo\u017aom le\u0175) are correct for both images (scores of 2) while one example is\nincorrect for both images (scores of 0, bo\u017aom right).\nIn some cases, retrieval improved when using more speci\u0171c queries, e.g. adding a laterality or\nseverity modi\u0171er to a general \u0171nding. For example, ELIXR-C and ELIXR-B scored 0.723 and 0.83\nfor \u201cle\u0175 pneumothorax\u201d as compared to 0.131 and 0.214 for \u201cpneumothorax.\u201d These results\npoint to the sensitivity of these models to prompting style.\nELIXR supports visual question answering and quality assurance\nfor radiology reports\nOn more challenging text-generation tasks, ELIXR-B demonstrated overall accuracies of 58.7%\nand 54.8% on two visual question answering (VQA) datasets, VQA-RAD (CXR-only questions)\n(Table 5) and MIMIC-CXR test (Table 6), as well as 62.5% on repo\u0178 quality assurance (QA) on\nMIMIC-CXR test (Tables 7, 8). Notably, ELIXR-B surpassed the accuracy of the SOTA model\nMedVInT which wasn\u2019t \u0171netuned on VQA-RAD. A summary of VQA results appears in Tables 5\nand 6. Quality assurance results appear strati\u0171ed by alteration type in Table 7 and by primary\n\u0171nding in Table 8. Figure 5 shows a selection of example cases for both visual question\nanswering and quality assurance.\nAnswer type\n(A_TYPE)\nMed-VInT\nw/ \u0171netuning\nAccuracy**\nMed-VInT\nw/o \u0171netuning\nAccuracy\nELIXR-B\nAccuracy\nELIXR-B\nSensitivity*\nELIXR-B\nSpeci\u0171city*\nboth\n81.6% (451)\n27.9% (574)\n58.7% (574)\nN/A\nN/A\nclosed\n86.8% (272)\n28.2% (379)\n69.3% (379)\n42.6% (222)\n87.8% (195)\nopen\n73.7% (179)\n27.1% (195)\n37.9% (195)\nN/A\nN/A\nTable 5: VQA results of ELIXR-B on our VQA-RAD test set using semantic matching.\nNumber of total questions and answers in brackets. For comparison with the SOTA method, we\nprovide results from MedVInT30 both before and a\u0175er \u0171netuning on VQA-RAD to show the\nbene\u0171ts it provides. *On the subset of expected that could be programmatically mapped to\n\u201cyes\u201d or \u201cno\u201d. **Results from MedVInT for VQA-RAD \u0171netuning are on all image modalities (not\njust chest X-ray) and from the o\u016bcial test split.\nVQA, Presence, Laterality,\nSeverity (Correct)\nVQA, Presence, Laterality and\nSeverity (Correct)\nVQA, Presence, Laterality and\nSeverity (Incorrect)\nQA, Remove Major Finding\n(Correct)\nQA, Swap Laterality\n(Correct)\nQA, Remove Major Finding\n(Incorrect)\nFigure 5: Qualitative results for visual question answering and quality assurance\ninference (from MIMIC test dataset).\nQuestion type\nAccuracy\nall\n54.8% (217)\npresence\n64.5% (148)\nlocation\n41.0% (39)\nsize, severity or type\n25.0% (30)\nTable 6: Accuracy of ELIXR-B\u2019s VQA answers on a subset of 48 MIMIC cases using\nexpe\u0178-graded semantic matches. Number of questions & answers noted in brackets. Nine\nquestions were marked as non-gradable by the expe\u0178 due to insu\u016bcient information or\nnon-relevance (e.g. question about severity despite condition not being present).\nAlteration type\nNumber of\ncases\nOverall model score\n(percent correct)\nControl\n15\n53.3%\nSwap laterality\n8\n87.5%\nAdd major \u0171nding\n15\n60.0%\nRemove major \u0171nding\n9\n50.0%\nTotal\n48\n60.4%\nTable 7: Summary statistics of repo\u0178 quality assurance results, strati\u0171ed by alteration\ntype.\nPrimary \u0171nding\nNumber of\ncases\nOverall model score\n(percent correct)\nNo \u0171nding\n8\n100%\nPneumothorax\n8\n25%\nPleural E\u0168usion\n8\n62.5%\nEdema\n8\n62.5%\nConsolidation or Pneumonia\n8\n75%\nLung Lesion\n8\n37.5%\nTotal\n48\n60.4%\nTable 8: Summary statistics of quality assurance results, strati\u0171ed by primary \u0171nding.\nIt is impo\u0178ant to note that the results we repo\u0178 on VQA-RAD are not directly comparable to\nthose repo\u0178ed in the literature, for the following reasons: (1) we only used a CXR subset of\nVQA-RAD, since ELIXR currently is limited to this modality, (2) we opted for a more di\u016bcult split\nthan the o\u016bcial development/test split in which the same image never appears across splits, (3)\nwe refrained from training on the development set and only used it for checkpoint selection. The\ne\u0168ect of (2) and (3) appears to be large: as Table 5 shows, MedVInT\u2019s pe\u0176ormance increases\nfrom 27.1% to 73.7% on open-ended accuracy and from 28.2% to 86.8% on close-ended\naccuracy a\u0175er \u0171netuning on VQA-RAD. Moor and Huang et al31 noted this data leakage in the\no\u016bcial VQA-RAD splits, as well.\nDiscussion\nIn\nthis\nstudy,\nwe\ndeveloped\nand\nevaluated\nELIXR,\na multimodal model that gra\u0175s a\nlanguage-aligned vision encoder onto a frozen LLM. The model was trained using CXR images\npaired with their free-text radiology repo\u0178s, without the requirement for expensive expe\u0178 data\ncuration.\nThe model achieved state-of-the-a\u0178 pe\u0176ormance for zero-shot classi\u0171cation,\ndata-e\u016bcient classi\u0171cation, and semantic search tasks, while also demonstrating potential in\nvisual question answering and radiology repo\u0178 quality assurance. The modular architecture has\nthe advantage of being easily adaptable to other tasks, with the ability to swap in di\u0168erent vision\nencoders and base LLMs as required.\nThe ELIXR architecture is data and computationally e\u016bcient. Previously we demonstrated that\nsmall nonlinear classi\u0171ers trained on a frozen SupCon vision encoder can outpe\u0176orm fully\nsupervised models in low-data se\u017aings12. With ELIXR, we have improved upon the data e\u016bciency\nof supervised contrastive learning by two orders of magnitude. This o\u0168ers the potential to train\nhighly accurate models that are capable of addressing the long tail of diagnoses (including rare\ndiseases), with only a fraction of the requirement for expe\u0178-curated training data. The process\nof prototyping a model for a new task also becomes simpli\u0171ed and more widely accessible,\nrequiring only the design of positive and negative prompts using natural language without a\nrequirement for machine learning expe\u0178ise. We hope that these approaches will enable a wider\nrange of researchers to engage in a broader array of diagnostic research questions and will\nallow medical professionals to develop models for underserved populations or understudied\ndiseases.\nBy leveraging a pre-trained frozen vision encoder and a frozen LLM, we were able to train ELIXR\nin a highly compute-e\u016bcient manner. Backpropagation of gradients during the second stage of\ntraining is only pe\u0176ormed for the Q-Former and MLP components, which are orders of\nmagnitude smaller than the vision encoder and frozen LLM. In comparison to \u0171netuning the\nvision encoder and/or LLM, this approach to training can be easily used by others who do not\nhave access to substantial compute hardware. Fu\u0178hermore, when adapting to a new task, or\nwhen newer generations of vision encoders and LLMs become available, it is straigh\u0179orward and\nrelatively inexpensive to train a new ELIXR model to take advantage of these advances.\nLLMs o\u0168er a deep natural language understanding that enables a new range of possibilities for\nrich interaction between clinicians and AI. In this study, we demonstrated early promising\ncapabilities in semantic search, vision question answering, and repo\u0178 quality assurance\u2013all\ntasks that cannot be achieved easily using traditional vision-only models.\nSemantic search unlocks the ability to search within an image for features of interest using free\ntext prompts. ELIXR demonstrated high retrieval precision across a broad range of queries,\nincluding pe\u0176ect retrieval of \u0171ve out of seven \u0171ndings-related queries (e.g. \u201ccentral venous\ncatheter\u201d), two out of three laterality-speci\u0171c queries (e.g. \u201cright pleural e\u0168usion\u201d), two out of\nfour severity-related queries (e.g. \u201cmoderate cardiomegaly\u201d), and three out of \u0171ve nuanced\nqueries (e.g. \u201cnasogastric tube reaches stomach\u201d). This capability could be used by researchers\nto identify images for study datasets, by clinicians to search for speci\u0171c images of interest from\na patient\u2019s historical record, by educators to \u0171nd examples for teaching purposes, as well as in\nmany other applications.\nELIXR also enables rich human-AI interaction through visual question answering. We benchmark\nour VQA pe\u0176ormance on the chest X-ray subset of the VQA-RAD dataset, yielding accuracy of\n69.1% across closed type questions, and 37.9% across open type questions. In contrast to\nothers\u2019 work, we do not train on VQA-RAD, as the standard VQA-RAD data splits exhibit an\noverlap with images in both train/test that risk a\u0178i\u0171cially in\u0174ating pe\u0176ormance31. In addition, we\nrepo\u0178 accuracy based on assessment by a board-ce\u0178i\u0171ed thoracic radiologist rather than\nunigram matching (BLEU-1), since BLEU-1 does not comprehensively re\u0174ect the quality of VQA.\nFinally, we demonstrate the ability of ELIXR to use its understanding of CXRs to check for errors\nin wri\u017aen radiology repo\u0178s. We envision that such a capability could have utility in the hospital\nse\u017aing, potentially acting as an advanced multimodal \u201cspell check\u201d to ale\u0178 radiologists to\nsuspected inconsistencies in their repo\u0178s, improving both quality and consistency of care. It\ncould also act as an \u201cAI Mentor\u201d, evaluating and mentoring more junior radiologists, including in\nse\u017aings where resource limitations result in less oppo\u0178unity for oversight by senior radiologists.\nVQA and QA were heavily dependent on the speci\u0171c prompts used, which were in turn a\u0168ected\nby the LLM used by ELIXR. With fu\u0178her work into prompt engineering, it is expected that one\ncould make the QA more speci\u0171c, with fewer false positive repo\u0178s. This high speci\u0171city is likely\nto be needed in a clinical environment, especially where the error rate is far lower than in this\nenriched simulated set.\nThere are a number of limitations to our work. Firstly, through employing a frozen LLM and vision\nencoder, we achieve our goals of training e\u016bciency, but this might be at the expense of overall\npe\u0176ormance (although some works suggest otherwise32). Secondly, ELIXR inherits the current\nset of wider challenges of LLMs including fragility to changes in prompts, hallucinations, and\nunwarranted con\u0171dence in its answers, even when wrong33. We expect that advances in future\ngenerations of LLMs will directly lead to improved pe\u0176ormance when incorporated into ELIXR.\nThirdly, a lack of established, robust benchmarks makes it challenging to compare pe\u0176ormance\nand establish state-of-the-a\u0178. Fou\u0178hly, we note that the MIMIC CXRs used were highly complex\nin an intensive care se\u017aing, containing multiple \u0171ndings, which added more complexity than is\ntypical compared to routine hospital X-rays. A non-intensive care CXR dataset that re\u0174ects\nroutine wider hospital practice would be valuable to evaluate in future work. Finally, we selected\ndi\u0168erent stages of ELIXR for di\u0168erent tasks based upon a pe\u0176ormance versus e\u016bciency\ntrade-o\u0168. For example, on zero-shot learning tasks, the bene\u0171ts of ELIXR-B over ELIXR-C were\nlimited. In contrast, VQA and repo\u0178 QA tasks are more text dependent, and bene\u0171ted from\nlonger training using larger amounts of text data. We found that some categories of \u0171ndings\nwere challenging across all tasks\u2013for example pneumothorax and fractures\u2013where the di\u016bculty\nof these cases is only pa\u0178ly explained by their low prevalence and noisy reference standards in\nthe training datasets5.\nIn future work, we hope to explore ELIXR\u2019s pe\u0176ormance with di\u0168erent general purpose and\nmedically specialized LLMs (e.g. Med-PaLM 2). We are also excited by the possibility of\nextending\nthese\nmethods\nto\nother\nimaging\nmodalities\nsuch\nas\nmusculoskeletal X-ray,\nmammography, computed tomography (CT), and also beyond radiology in an e\u0168o\u0178 we are\ncalling Medical Information Adapters. It is also likely that temporal information can be\nincorporated\ninto\nthis\narchitecture, widening the range of potential real world clinical\napplications.\nConclusion\nIn this study, we developed and evaluated an e\u016bcient vision-language multimodal model for\nmedical imaging that is trained using only medical images paired with free-text radiology repo\u0178s\nobtained from routine clinical practice. The method is highly compute and data e\u016bcient to train,\nand we demonstrated promising pe\u0176ormance across a range of multimodal radiology tasks. This\nwork is an initial step towards a general purpose X-ray a\u0178i\u0171cial intelligence system.\nAcknowledgements\nWe acknowledge Yuan Liu and Karan Singhal from Google Research for their critical feedback on\nthe manuscript. We thank the NIH Clinical Center for making the ChestX-ray14 dataset publicly\navailable, the MIT Laboratory for Computational Physiology for making the MIMIC-CXR dataset\npublicly available, and the Stanford ML Group for making the CheXpe\u0178 dataset publicly\navailable. We also thank the Google Research team for so\u0175ware and hardware infrastructure\nsuppo\u0178.\nAuthor contributions\nS.X., L.Y., S.S., D.T., S.P., C.K., D.G., R.P., A.S. contributed to the conception of the study and\nstudy design; M.S., P.S., C.C., C.K., D.G., R.P., A.S. contributed to acquisition of the data; S.X., L.Y.,\nM.S., T.K., M.M., W-H.W., A.K., S.K., Z.M., Y.L., C.L., S.P., C.K., D.G., R.P., A.S. contributed to\nanalysis and interpretation of the data; P.S., Y.L., P.S., M.E., S.R.K., Y.M., K.C., G.C., S.S., D.T., K.E.\nprovided strategic guidance; L.Y., W-H.W., P.S., Y.L., S.P., C.K., D.G., R.P., A.S. contributed to\npaper organisation and team logistics; S.X., L.Y., M.S., T.K., M.M., W-H.W., A.K., S.K., Z.M., Y.L.,\nD.T., C.K., D.G., R.P., A.S. contributed to dra\u0175ing and revising the manuscript.\nReferences\n1.\nTopol, E. J. High-pe\u0176ormance medicine: the convergence of human and a\u0178i\u0171cial\nintelligence. Nat. Med. 25, 44\u201356 (2019).\n2.\nChen, J. H. & Asch, S. M. Machine Learning and Prediction in Medicine - Beyond the Peak of\nIn\u0174ated Expectations. N. Engl. J. Med. 376, 2507\u20132509 (2017).\n3.\nKelly, C. J., Ka\u0178hikesalingam, A., Suleyman, M., Corrado, G. & King, D. Key challenges for\ndelivering clinical impact with a\u0178i\u0171cial intelligence. BMC Med. 17, 195 (2019).\n4.\nRajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and medicine. Nat. Med. 28,\n31\u201338 (2022).\n5.\nDuggan, G. E., Reicher, J. J., Liu, Y., Tse, D. & She\u017ay, S. Improving reference standards for\nvalidation of AI-based radiography. Br. J. Radiol. 94, 20210435 (2021).\n6.\nVaswani, A. et al. A\u017aention Is All You Need. Adv. Neural Inf. Process. Syst. (2017).\n7.\nMoor, M. et al. Foundation models for generalist medical a\u0178i\u0171cial intelligence. Nature 616,\n259\u2013265 (2023).\n8.\nBrown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst.\n(2020).\n9.\nRadford, A. et al. Learning transferable visual models from natural language supervision. in\nInternational conference on machine learning 8748\u20138763 (PMLR, 2021).\n10. Li, J., Li, D., Savarese, S. & Hoi, S. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301. 12597 (2023).\n11.\nDai, W. et al. Instructblip: Towards general-purpose vision-language models with instruction\ntuning. arXiv preprint arXiv:2305. 06500 (2023).\n12.\nSellergren, A. B. et al. Simpli\u0171ed Transfer Learning for Chest Radiography Models Using Less\nData. Radiology 305, 454\u2013465 (2022).\n13.\nRa\u0168el, C. et al. Exploring the limits of transfer learning with a uni\u0171ed text-to-text\ntransformer. J. Mach. Learn. Res. 21, 5485\u20135551 (2020).\n14. Anil, R. et al. Palm 2 technical repo\u0178. arXiv preprint arXiv:2305. 10403 (2023).\n15. Irvin, J. et al. Chexpe\u0178: A large chest radiograph dataset with unce\u0178ainty labels and expe\u0178\ncomparison. in Proceedings of the AAAI conference on a\u0178i\u0171cial intelligence vol. 33 590\u2013597\n(2019).\n16. Johnson, A. E. W. et al. MIMIC-CXR, a de-identi\u0171ed publicly available database of chest\nradiographs with free-text repo\u0178s. Sci Data 6, 317 (2019).\n17.\nLau, J. J., Gayen, S., Ben Abacha, A. & Demner-Fushman, D. A dataset of clinically\ngenerated visual questions and answers about radiology images. Sci Data 5, 180251 (2018).\n18.\nWang, X. et al. ChestX-Ray8: Hospital-scale chest X-ray database and benchmarks on\nweakly-supervised classi\u0171cation and localization of common thorax diseases. in 2017 IEEE\nConference on Computer Vision and Pa\u017aern Recognition (CVPR) (IEEE, 2017).\ndoi:10.1109/cvpr.2017.369.\n19. Goldberger, A. L. et al. PhysioBank, PhysioToolkit, and PhysioNet: components of a new\nresearch resource for complex physiologic signals. Circulation 101, E215\u201320 (2000).\n20. Johnson, A. E. W., Pollard, T. J., Mark, R. G., Berkowitz, S. J. & Horng, S. MIMIC-CXR\nDatabase (version 2.0.0). PhysioNet h\u017aps://doi.org/10.13026/C2JT1Q (2019)\ndoi:10.13026/C2JT1Q.\n21.\nLau, J. J., Gayen, S., Demner, D. & Ben Abacha, A. Visual Question Answering in Radiology\n(VQA-RAD). OSFHOME h\u017aps://doi.org/10.17605/OSF.IO/89KPS (2019)\ndoi:10.17605/OSF.IO/89KPS.\n22. Zhang, Y., Jiang, H., Miura, Y., Manning, C. D. & Langlotz, C. P. Contrastive learning of\nmedical visual representations from paired images and text. in Machine Learning for\nHealthcare Conference 2\u201325 (PMLR, 2022).\n23. Tiu, E. et al. Expe\u0178-level detection of pathologies from unannotated chest X-ray images via\nself-supervised learning. Nat Biomed Eng 6, 1399\u20131406 (2022).\n24. DeLong, E. R., DeLong, D. M. & Clarke-Pearson, D. L. Comparing the areas under two or\nmore correlated receiver operating characteristic curves: a nonparametric approach.\nBiometrics 44, 837\u2013845 (1988).\n25. Wang, Z., Wu, Z., Agarwal, D. & Sun, J. Medclip: Contrastive learning from unpaired medical\nimages and text. arXiv preprint arXiv:2210. 10163.\n26. Wang, Y., Wang, L., Li, Y., He, D. & Liu, T.-Y. A theoretical analysis of NDCG type ranking\nmeasures. in Conference on learning theory 25\u201354 (PMLR, 2013).\n27. Johnson, A. E. W. et al. MIMIC-CXR-JPG - chest radiographs with structured labels (version\n2.0.0). PhysioNet h\u017aps://doi.org/10.13026/8360-t248 (2019) doi:10.13026/8360-t248.\n28. Johnson, A. E. W. et al. MIMIC-CXR-JPG, a large publicly available database of labeled chest\nradiographs. arXiv preprint arXiv:1901. 07042 (2019).\n29. Singhal, K. et al. Towards Expe\u0178-Level Medical Question Answering with Large Language\nModels. arXiv [cs.CL] (2023).\n30. Zhang, X. et al. PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering.\narXiv [cs.CV] (2023).\n31.\nMoor, M. et al. Med-Flamingo: a Multimodal Medical Few-shot Learner. arXiv [cs.CV] (2023).\n32. Zhai, X. et al. LiT: Zero-Shot Transfer with Locked-image text Tuning. in 2022 IEEE/CVF\nConference on Computer Vision and Pa\u017aern Recognition (CVPR) (IEEE, 2022).\ndoi:10.1109/cvpr52688.2022.01759.\n33. Ji, Z. et al. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55,\n1\u201338 (2023).\n34. Tan, M. & Le, Q. V. E\u016bcientNet: Rethinking Model Scaling for Convolutional Neural Networks.\narXiv [cs.LG] (2019) doi:10.48550/ARXIV.1905.11946.\n35. Kudo, T. & Richardson, J. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for Neural Text Processing. in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing: System Demonstrations\n(Association for Computational Linguistics, 2018). doi:10.18653/v1/d18-2012.\nSupplementary materials\nSupplementary text\n\u25cf\nAdditional ELIXR training details\nSupplementary tables\n\u25cf\nSupplementary table 1: Prompts used for zero-shot classi\u0171cation. We adopt the\nprompting strategy developed by CheXzero (Tiu et al. 2022).\n\u25cf\nSupplementary table 2: Queries used for semantic search and the ELIXR pe\u0176ormance\nusing normalized discounted cumulative gain (NDCG).\n\u25cf\nSupplementary table 3: Prompts used for MIMIC-CXR ground truthing.\n\u25cf\nSupplementary table 4: Questions used for visual question answering (VQA) evaluation\non MIMIC-CXR test cases.\n\u25cf\nSupplementary table 5: Visual question answering evaluation scoring rubric.\n\u25cf\nSupplementary\ntable\n6:\nMethodology\nfor\naltering\nimpressions\nfor\nrepo\u0178 quality\nassurance evaluation.\n\u25cf\nSupplementary table 7: Scoring rubric for quality assurance large language model (LLM)\noutput.\n\u25cf\nSupplementary table 8: Zero-shot classi\u0171cation using ELIXR-C and ELIXR-B across 13\n\u0171ndings in the CheXpe\u0178 test set.\n\u25cf\nSupplementary table 9: Data-e\u016bcient classi\u0171cation pe\u0176ormance comparison between\nELIXR-B and ELIXR-C.\n\u25cf\nSupplementary table 10: Data-e\u016bcient classi\u0171cation pe\u0176ormance comparison between\nELIXR-B and supervised contrastive learning (SupCon) by sample size.\nSupplementary \u0171gures\n\u25cf\nSupplementary \u0171gure 1: Suspect positive or negative ground truth MIMIC labels as\nidenti\u0171ed by the query to Med-PaLM 2 truthed by a board ce\u0178i\u0171ed radiologist.\n\u25cf\nSupplementary \u0171gure 2: Data-e\u016bcient classi\u0171cation pe\u0176ormance between models\nper-dataset and per-\u0171nding using di\u0168erent training set sample sizes.\nSupplementary Text\nAdditional ELIXR training details\nELIXR-C\nFor the \u0171rst stage of training using the CLIP loss, E\u016bcientNet-L234 was used as the vision\nencoder, initialized from the SupCon checkpoint used in Sellergren et al12. The small variant of T5\nwas used for the text encoder, initialized from scratch13. The T5 text encoder used a\nSentencePiece model35 with a vocabulary size of 32,000 pretrained on PubMed abstracts. For\ndata augmentation, horizontal \u0174ipping and random rotation up to 15 degrees were applied.\nImages were resized to 1280x1280 pixels. A dimension of 128 for the projection head and a \u0171xed\ntemperature of 0.07 were used. Preprocessing on the radiology repo\u0178s was done to select, in\norder of preference, the impression section, followed by the \u0171ndings section. The input text\nsequence was truncated at 128 tokens. A batch size of 64 was split across 128 TPUv3 cores using\nspatial pa\u0178itioning. Stochastic gradient descent (SGD) constant learning rate was set to 0.0001\nwith a momentum of 0.98. The model was trained for roughly 80,000 steps. Training was done\nin TensorFlow.\nELIXR-B\nIn the \u0171rst phase of ELIXR-B training, the embeddings from ELIXR-C were precomputed for all\ndatasets and spatially pooled from 40x40x1376 to 8x8x1376. We used 32 BLIP-2 query tokens\nand a max text sequence length of 128. Q-Former weights were initialized from BERT-base. A\nbatch size of 128 was split across 8 TPUv3 cores. Constant learning rate of 1e-5 was used with\nthe Adam optimizer (beta1 of 0.98, beta2 of 0.999, epsilon of 1e-8). Training was done in Jax.\nCheckpoints were selected using zero-shot pe\u0176ormance on CheXpe\u0178 validation as well as the\nvalidation losses. More speci\u0171cally, the checkpoint that pe\u0176orms the best on zero-shot AUC,\nimage-text matching loss, and contrastive loss was used for zero/data-e\u016bcient and semantic\nsearch, while the checkpoint that pe\u0176orms the best on image-grounded text generation was\nused for initial checkpoint for phase 2 training.\nIn the second phase of ELIXR-B training, 32 replicas of PaLM2 S were launched as inference\nservers, each on 8 TPUv3 cores. When called for given inputs (the output query tokens from the\nQ-Former plus any additional text tokens), the server provides back the gradients (based on the\nlanguage modeling loss) which can be backpropagated through the Q-Former. For this phase, a\nbatch size of 32 was used, 1 per inference server replica. Constant learning rate of 5e-4 was\nused with the Adam optimizer (beta1 of 0.98, beta2 of 0.999, epsilon of 1e-8).\nSupplementary tables\nSupplementary\ntable\n1:\nPrompts\nused\nfor zero-shot classi\u0171cation. We adopt the\nprompting strategy developed by CheXzero23.\nCondition\nPositive prompts\nNegative prompts\nEnlarged\ncardiomediastinum\nwidened cardiomediastinum\nno acute cardiopulmonary process\ncardiomediastinal silhoue\u017ae is normal\nCardiomegaly\nmild cardiomegaly\nmoderate cardiomegaly\nsevere cardiomegaly\nhea\u0178 size is normal\nno acute cardiopulmonary process\nnormal study\nLung lesion\nlytic lesion\ncavitary lesion\nparenchymal lesion\nno acute cardiopulmonary process\nAirspace opacity\nbilateral opacities\nbasal opacity\nno focal opacity\nlung volumes are normal\nEdema\nmild pulmonary edema\nmoderate pulmonary edema\nsevere pulmonary edema\nno pulmonary edema\nno acute cardiopulmonary process\nnormal study\nConsolidation\nsuggestive of consolidation\nnormal study\nPneumonia\nsuggestive of pneumonia\nlungs are clear\nno acute cardiopulmonary process\nAtelectasis\nplate atelectasis\nsubsegmental atelectasis\nlungs are clear\nno acute cardiopulmonary process\nnormal study\nPneumothorax\napical pneumothorax\nno pneumothorax\nPleural e\u0168usion\nle\u0175 pleural e\u0168usion\nright pleural e\u0168usion\nbilateral pleural e\u0168usions\nno acute cardiopulmonary process\nnormal study\nPleural other\nblunting of costophrenic angle\npleural thickening\nno acute cardiopulmonary process\nFracture\nrib fractures\nno acute cardiopulmonary process\nSuppo\u0178 Devices\nmonitoring and suppo\u0178 devices\nNG tube\nET tube\ncatheter\nPIC line\nno acute cardiopulmonary process\nSupplementary table 2: Queries used for semantic search and the ELIXR pe\u0176ormance\nusing normalized discounted cumulative gain at \u0171ve (NDCG@5).\nTopic\nQueries\nNDCG@5\nELIXR-C\nNDCG@5\nELIXR-B\nNDCG@5\nMedCLIP\nSingle\n\u0171nding\nPneumothorax\n0.131\n0.214\n0.0\nPneumonia\n0.927\n1.0\n0.0\nCentral venous catheter\n1.0\n1.0\n0.447\nNasogastric tube\n0.869\n1.0\n0.699\nEndotracheal tube\n1.0\n1.0\n0.146\nFracture\n0\n0\n0.301\nPacemaker\n0.723\n1.0\n0.146\nLaterality\nLe\u0175 Pneumothorax\n0.723\n0.83\n0.5\nRight pleural e\u0168usion\n1.0\n1.0\n1.0\nLe\u0175 Pneumonia\n0.769\n1.0\n0.765\nSeverity\nSmall pleural e\u0168usion\n0.616\n0.446\n0.777\nLarge Pneumothorax\n0.066\n0.254\n0.0\nModerate Pulmonary edema\n0.5\n1.0\n0.927\nModerate Cardiomegaly\n0.927\n1.0\n0.934\nNuanced\nfeatures\nSmall right pleural e\u0168usion, no le\u0175 pleural e\u0168usion\n0.68\n1.0\n0\nMild right Pneumonia\n0.786\n0.449\n0.764\nLoculated pleural e\u0168usion\n0.478\n0.246\n0.488\nMultiple focal lung opacities\n1.0\n1.0\n0.746\nNasogastric tube reaches stomach\n0.701\n1.0\n0.699\nSupplementary table 3: Prompts used for MIMIC-CXR ground truthing. Keywords for\nselecting candidates for MIMIC-CXR repo\u0178s followed by prompts used to determine if the given\ncondition exists. These were used to select candidates for con\u0171rmation by a board ce\u0178i\u0171ed\nthoracic radiologist (CL).\nCondition\nKey Words\natelectasis\natelectasis\ncardiomegaly\ncardiomegaly\ncardiac silhoue\u017ae\ncatheter\ncentral venous catheter\ncentral line\npicc\nfracture\nfracture\nacute fracture\nfx\nhilar enlargement\nhilar enlargement\nlung opacity\nlung opacity\nlung opacities\nin\u0171ltrate\npneumonia\natelectasis\nconsolidation\nairspace opacity\nairspace opacities\nmediastinal widening\nabnormal mediastinal widening\nwidened mediastinum\nwidened\nnodule\nlung nodule\nnodule\nnodular opacity\npleural e\u0168usion\npleural e\u0168usion\npneumonia\npneumonia\npneumothorax\npneumothorax\npulmonary edema\npulmonary edema\npulmonary vascular congestion\ntube\nendotracheal tube\nenteric tube\nng\nog\nfeeding tube\net tube\nPrompt used with above text on Med-PaLM 2\nAll key words except cardiac silhoue\u017ae\nYou\nare\na\nhelpful\nmedical\nknowledge\nassistant.\nProvide\nuseful,\ncomplete,\nconcise,\nand\nscienti\u0171cally-grounded queries to radiology repo\u0178s.\nDoes this repo\u0178 mention that the patient has a {keyword}? Repo\u0178:{description}\ncardiac silhoue\u017ae\nYou\nare\na\nhelpful\nmedical\nknowledge\nassistant.\nProvide\nuseful,\ncomplete,\nconcise,\nand\nscienti\u0171cally-grounded queries to radiology repo\u0178s.\nDoes this repo\u0178 mention that the patient has a cardiac silhoue\u017ae that is enlarged? Repo\u0178:{description}\nSupplementary table 4: Questions used for visual question answering evaluation on\nMIMIC-CXR test cases. For each case, the model was queried with a \u0171ndings-speci\u0171c set of\nquestions, depending on the corresponding label in the MIMIC CXR JPG dataset being set to 1.0,\nwith questions covering the condition\u2019s presence, location, and severity, size or type. An\nadditional two \u0171ndings-independent questions were asked per case, covering the presence of\nother conditions. For cases with none of the listed \u0171ve MIMIC labels being set to 1.0, three\nlabel-independent questions were asked.\nFinding\nQuestions the model was queried with\nQuestion type\nPneumothorax\nIs a pneumothorax present ?\nIs a pneumothorax present in this image ?\nPresence\nWhere is pneumothorax present ?\nLocation\nWhat is the size of pneumothorax, if present ?\nSize/Severity/Type\nPleural E\u0168usion\nDoes the patient have a pleural e\u0168usion ?\nIs a pleural e\u0168usion present in this image ?\nPresence\nWhat is the location of the pleural e\u0168usion, if present ?\nLocation\nWhat is the size of pleural e\u0168usion, if present ?\nSize/Severity/Type\nEdema\nIs a pulmonary edema present in this image ?\nIs there evidence of a pulmonary edema ?\nPresence\nWhere is pulmonary edema present ?\nLocation\nWhat is the severity of the pulmonary edema, if present ?\nWhat is the type of the pulmonary edema, if present ?\nSize/Severity/Type\nConsolidation OR\nPneumonia\nIs a lung consolidation or pneumonia present in this image ?\nPresence\nWhat\nis\nthe\nlocation\nof\nthe\nlung\nconsolidation\nor\npneumonia, if present ?\nLocation\nLung Lesion\nAre lung nodules or a mass present?\nDoes the patient have lung nodules or a mass ?\nPresence\nWhere are lung nodules or a mass located ?\nLocation\nFinding-\nindependent\nWhat abnormalities are seen within the lungs ?\nDoes the patient have cardiomegaly ?\nWhat is the pathology ?\nIs atelectasis present in this image ?\nIs a pneumothorax present ?\nIs a pneumothorax present in this image ?\nDoes the patient have a pleural e\u0168usion ?\nIs a pleural e\u0168usion present in this image ?\nIs a pulmonary edema present in this image ?\nIs there evidence of a pulmonary edema ?\nIs a lung consolidation or pneumonia present in this image ?\nIs pneumonia present ?\nAre lung nodules or a mass present?\nDoes the patient have lung nodules or a mass ?\nPresence\nDoes the patient have lung opacity ?\nIs a radiopaque foreign body or pacemaker present ?\nAre there any \u0171ndings ?\nIs a hiatal hernia present ?\nAre there any signs of interstitial \u0171brosis in the lungs ?\nAre there any notable abnormalities in the imaged upper\nabdomen ?\nIs there evidence of an endotracheal tube ?\nIs a central venous catheter present ?\nIs there evidence of a nasogastric or orogastric tube ?\nIs a hilar enlargement present in this image ?\nIs a mediastinal widening present in this image ?\nDoes the patient have a skeletal fracture ?\nSupplementary table 5: Visual question answering evaluation scoring rubric. De\u0171nition of\ncorrectness scores used for assessing model answers against expected answers (for VQA-RAD)\nor the CXR image and/or repo\u0178 (for MIMIC test set) in visual question answering by radiologist.\nSummary\nScore Details\nScore\nCorrect\nLLM-provided answer either:\n\u25cf\nis an exact match to the ground truth answer.\n\u25cf\ncontains additional and still accurate information above what\nthe ground truth speci\u0171es\n\u25cf\nis another diagnosis consistent with the image that also\nanswers the question. e.g. the associated image shows two\npotential diagnoses and the predicted answer provides the\ndiagnosis not present in the ground truth, and the question\ndid not specify which diagnosis it was looking for.\n\u25cf\nis a rephrasing of the ground truth answer such that a\npatient\u2019s diagnosis would not di\u0168er in treatment from the\nground truth. e.g. a predicted answer of \u201clobe collapse\u201d with\nground truth of \u201cpneumothorax\u201d would be counted as a\ncorrect prediction.\n1.0\nPa\u0178ially\nCorrect\nLLM-provided answer has correct information but is in \u0174awed in one\nof the following ways:\n\u25cf\nthe answer omits some key details (e.g. laterality, location)\nasked by the question and present in the ground truth.\n\u25cf\nthe answer provides incorrect key details, but the underlying\ncondition is still accurate.\n0.5\nIncorrect\nLLM-provided answer is either:\n\u25cf\nentirely inaccurate.\n\u25cf\ndoes not respond to the question or appears to answer a\ndi\u0168erent one.\n\u25cf\nis missing an answer.\n\u25cf\nis internally inconsistent.\n0.0\nAmbiguous\nLLM-provided answer cannot be compared to the ground truth. e.g. a\nground-truth answer of \u201cunsure\u201d or \u201cambiguous\u201d does not provide\nenough information to compare against.\nN/A\nSupplementary table 6: Methodology for altering impressions for repo\u0178 quality assurance\nevaluation. Here we describe the speci\u0171c alterations that were made to cases associated with\neach \u0171nding for the quality assurance task.\nAlteration\nPrimary Finding\nAlteration rule\nNo change (control)\nAll\nNo change\nChange laterality\nAll except No Finding\nand Edema\nIf the laterality of the primary \u0171nding is le\u0175-sided, replace\nto indicate right-sided or vice versa. Do not change\nlaterality of any other \u0171nding. Cases with bilateral primary\n\u0171ndings are excluded from this alteration category.\nRemove major \u0171nding\nAll except No Finding\nRemove all references to the primary \u0171nding and indicate\neither that there is \u201cno evidence\u201d of the \u0171nding or that the\n\u0171nding \u201chas resolved,\u201d whichever is more appropriate in\nthe context of the remaining impression. Retain content\nthat relates to other \u0171ndings. If no \u0171ndings remain, change\nimpression to read, \u201cNo acute cardiopulmonary process.\u201d\nAdd major \u0171nding\nNo Finding\nAdd\n\u201cMedium\nright\npleural\ne\u0168usion.\u201d\nRemove\nany\nsentences indicating the repo\u0178 is normal/clear.\nPneumothorax\nAdd: \u201cMalpositioned endotracheal tube.\u201d\nPleural E\u0168usion\nAdd: \u201cApproximately 1 cm nodule in mid right lung.\u201d\nEdema\nAdd: \u201cSeveral acute displaced rib fractures.\u201d\nConsolidation\nor\nPneumonia\nAdd: \u201cLarge le\u0175 pleural e\u0168usion.\u201d\nLung Lesion\nAdd: \u201cModerate right pneumothorax.\u201d\nSupplementary table 7: Scoring rubric for quality assurance large language model (LLM)\noutput. Here we show the detailed scoring rubric that was used by the board-ce\u0178i\u0171ed thoracic\nradiologist (CL) to assess the LLM output.\nImpression type being analyzed\nLLM assessment by board-certi\u016fed thoracic\nradiologist (CL)\nScore\nControl (no change)\nLLM suggests that impression is correct\n1\nControl (no change)\nLLM suggests a problem with the impression\nwhere the suggestion is correct or possibly\ncorrect but not important for case management\n(e.g., \u201creport failed to characterize heart size\u201d)\n1\nControl (no change)\nLLM suggests a problem with the impression\nwhere the suggestion is correct and possibly\nimportant for case management\nExclude\nas\nthis\nwas not a\nvalid\ncontrol\nControl (no change)\nLLM suggests a problem with the impression\nwhere the suggestion is incorrect\n0\nRemove primary \u016fnding\nLLM suggests that impression is correct\n0\nRemove primary \u016fnding\nLLM does not identify the primary \u016fnding that\nwas\nremoved\n(even\nif\nit\nmakes\nany\nother\nsuggestion that is correct or possibly correct)\n0\nRemove primary \u016fnding\nLLM\nidenti\u016fes\nthe primary \u016fnding that was\nremoved\n1\nAdding incorrect \u016fnding\nLLM suggests that impression is correct\n0\nAdding incorrect \u016fnding\nLLM does not correctly identify the added \u016fnding\n(even if it makes any other suggestion that is\ncorrect or possibly correct)\n0\nAdding incorrect \u016fnding\nLLM correctly identi\u016fes the added \u016fnding\n1\nSwap primary \u016fnding laterality\nLLM suggests that impression is correct\n0\nSwap primary \u016fnding laterality\nLLM response does not identify the problem with\nthe laterality of the primary \u016fnding (even if it\nmakes any other suggestion that is correct or\npossibly correct)\n0\nSwap primary \u016fnding laterality\nLLM identi\u016fes a problem with the laterality of the\nprimary \u016fnding\n1\nSupplementary table 8: Zero-shot classi\u0171cation using ELIXR-C and ELIXR-B across 13\n\u0171ndings in the CheXpe\u0178 test set. Area under curves (AUCs) with 95% con\u0171dence intervals\nare repo\u0178ed. the \u201cNo Finding\u201d label is excluded.\nZero-shot\nELIXR-C AUC\n[95% CI]\nELIXR-B AUC\n[95% CI]\nDi\u0168erence ELIXR-B vs.\nELIXR-C [95% CI],\np-value\nLargest SupCon\ndata-e\u016bcient sample\nsize for which ELIXR-B\nis noninferior [95%\nCI], p-value\nCheXpe\u0178\nAtelectasis\n0.754\n[0.714-0.795]\n0.798\n[0.759-0.838]\n0.042 [0.00, 0.084]\np=0.05272\n224316, -0.004\n[-0.045, 0.036]\np=0.84095\nE\u0168usion\n0.930\n[0.908-0.951]\n0.873\n[0.841-0.903]\n-0.059 [-0.086,\n-0.032] p=0.00001\n4096, 0.020 [-0.006,\n0.047] p=0.13340\nCardiomegaly\n0.891\n[0.862-0.919]\n0.892\n[0.861-0.921]\n0.002 [-0.023, 0.026]\np=0.88496\n224316, -0.088\n[-0.120, -0.056]\np<1e-5\nConsolidation\n0.875\n[0.819-0.922]\n0.742\n[0.680-0.804]\n-0.133 [-0.199,\n-0.067] p=0.00008\n512, 0.015 [-0.067,\n0.096] p=0.72400\nPulmonary\nEdema\n0.880\n[0.843-0.913]\n0.915\n[0.881-0.942]\n0.033 [0.015, 0.052]\np=0.00033\n224316, -0.021\n[-0.046, 0.005]\np=0.10897\nEnlarged\nCardiomediasti\nnum\n0.800\n[0.763-0.837]\n0.837\n[0.803-0.869]\n0.035 [0.013, 0.056]\np=0.00195\nN/A\nPleural Other\n0.729\n[0.467-1.000]\n0.490\n[0.151-0.780]\n-0.239 [-0.622, 0.143]\np=0.22031\nN/A\nPneumothorax\n0.800\n[0.630-0.932]\n0.846\n[0.656-0.980]\n0.047 [-0.067, 0.160]\np=0.42131\nN/A\nSuppo\u0178\nDevices\n0.894\n[0.865-0.919]\n0.865\n[0.835-0.892]\n-0.027 [-0.047,\n-0.007] p=0.00712\nN/A\nAirspace\nOpacity\n0.888\n[0.857-0.915]\n0.908\n[0.882-0.931]\n0.021 [0.000, 0.041]\np=0.04913\nN/A\nLung Lesion\n0.747\n[0.659-0.838]\n0.798\n[0.671-0.914]\n0.049 [-0.064, 0.163]\np=0.39248\nN/A\nPneumonia\n0.881\n[0.833-0.930]\n0.828\n[0.734-0.914]\n-0.054 [-0.140,\n0.033] p=0.22251\nN/A\nFracture\n0.637\n[0.444-0.876]\n0.395\n[0.173-0.679]\n-0.242 [-0.637, 0.153]\np=0.22950\nN/A\nSupplementary table 9: Data-e\u016bcient classi\u0171cation pe\u0176ormance comparison between\nELIXR-B and ELIXR-C. (a) CheXpe\u0178, (b) CXR-14. Delong\u2019s test results comparing data-e\u016bcient\npe\u0176ormance of ELIXR-B vs. ELIXR-C at matching dataset sizes for CheXpe\u0178 and Chest X-ray14.\nBold for ELIXR-B\u2019s outpe\u0176ormance being statistically signi\u0171cant. Red if ELIXR-C outpe\u0176ormed\nELIXR-B.\n(a)\nCheXpe\u0178\nDi\u0168erence between ELIXR-B and ELIXR-C\nSample size\n64\n512\n4096\n32768\n224316\nAtelectasis\n0.057 [-0.006,\n0.119] p=0.07509\n0.045 [0.005,\n0.085]\np=0.02870\n0.021 [-0.015,\n0.057]\np=0.24958\n0.020 [-0.013,\n0.053] p=0.23971\n-0.001 [-0.027,\n0.026]\np=0.95679\nCardiomegaly\n0.098 [0.052,\n0.144]\np=0.00003\n0.040 [0.006,\n0.074] p=0.02128\n0.032 [-0.001,\n0.066]\np=0.05489\n0.019 [0.000,\n0.038]\np=0.04469\n0.012 [-0.012,\n0.035]\np=0.32533\nConsolidation\n0.042 [-0.075,\n0.159] p=0.48560\n0.025 [-0.066,\n0.117] p=0.59087\n-0.002 [-0.045,\n0.041]\np=0.91520\n0.005 [-0.036,\n0.047]\np=0.79792\n0.029 [-0.009,\n0.068]\np=0.13381\nE\u0168usion\n0.001 [-0.023,\n0.025] p=0.95793\n-0.001 [-0.020,\n0.018] p=0.89187\n0.002 [-0.013,\n0.017]\np=0.79330\n0.007 [-0.009,\n0.022]\np=0.41349\n-0.004 [-0.017,\n0.009]\np=0.55608\nPulmonary edema\n-0.006 [-0.053,\n0.041] p=0.81119\n0.000 [-0.029,\n0.029] p=0.99372\n0.007 [-0.017,\n0.031]\np=0.57298\n0.005 [-0.013,\n0.022] p=0.61078\n0.006 [-0.017,\n0.029]\np=0.59233\n(b)\nCXR-14\nDi\u0168erence between ELIXR-B and ELIXR-C\nSample size\n64\n512\n4096\n32768\n68801\n674533\nAirspace opacity\n0.035 [0.023,\n0.047] p<1e-5\n0.019 [0.010,\n0.029]\np=0.00007\n0.016 [0.009,\n0.022] p<1e-5\n0.010 [0.004,\n0.016]\np=0.00086\n-0.001\n[-0.007,\n0.005]\np=0.77917\nConsolidation\n0.015 [0.001,\n0.030]\np=0.03916\n0.010 [-0.001,\n0.021]\np=0.06217\n0.005 [-0.001,\n0.012]\np=0.09394\n0.006 [0.000,\n0.013]\np=0.04882\n0.005\n[-0.002,\n0.013]\np=0.12789\nE\u0168usion\n-0.009\n[-0.014,\n-0.003]\np=0.00178\n-0.000\n[-0.005, 0.005]\np=0.99721\n-0.003\n[-0.006, 0.001]\np=0.15106\n-0.002\n[-0.005,\n0.000]\np=0.10230\n-0.004\n[-0.006,\n-0.001]\np=0.01593\nFracture\n0.004 [-0.081,\n0.090]\np=0.92169\n-0.004 [-0.076,\n0.068]\np=0.91167\n0.012 [-0.067,\n0.091]\np=0.76612\n-0.016 [-0.084,\n0.052]\np=0.64405\n-0.035\n[-0.099,\n0.028]\np=0.27689\nPneumothorax\n0.067 [0.032,\n0.102]\np=0.00021\n-0.030 [-0.059,\n-0.001]\np=0.04248\n-0.012 [-0.036,\n0.012]\np=0.31538\n0.007 [-0.005,\n0.020]\np=0.24105\n0.007\n[-0.006,\n0.020]\np=0.30800\nPulmonary edema\n-0.005\n[-0.028, 0.017]\np=0.63723\n0.019 [0.006,\n0.033]\np=0.00459\n-0.007 [-0.016,\n0.002]\np=0.13283\n-0.001 [-0.007,\n0.004]\np=0.68002\n-0.000\n[-0.006,\n0.006]\np=0.99304\nSupplementary table 10: Data-e\u016bcient classi\u0171cation pe\u0176ormance comparison between\nELIXR-B and supervised contrastive learning (SupCon) by sample size. (a) CheXpe\u0178, (b)\nCXR-14. AUC di\u0168erences between SupCon and ELIXR-B pretraining by sample sizes. Values in\neach cell represent the di\u0168erence in AUC, with 95% CIs in square braces, and p-values. Bold\nindicates where the ELIXR-B approach pe\u0176orms noninferior (at a margin of 0.05) to SupCon at a\nsmaller sample size.\n(a)\nCheXpe\u0178\nSample Size\nELIXR-B\n64\n512\n4096\n32768\n224316\nSample size\nSupCon\nAtelectasis\n224316\n-0.048 [-0.094,\n-0.002]\np=0.04094\n-0.051 [-0.088,\n-0.015]\np=0.00613\n-0.013 [-0.048,\n0.023]\np=0.47990\n0.007 [-0.009,\n0.022]\np=0.39980\n32768\n-0.025 [-0.076,\n0.026]\np=0.33903\n-0.028 [-0.073,\n0.017]\np=0.21884\n0.010 [-0.035,\n0.056]\np=0.65029\n0.034 [0.002,\n0.066]\np=0.04019\n4096\n-0.011 [-0.058,\n0.037]\np=0.66183\n-0.014 [-0.054,\n0.026]\np=0.49807\n0.044 [0.012,\n0.076]\np=0.00665\n0.048 [0.019,\n0.077] p=0.00114\n512\n0.039 [-0.003,\n0.081]\np=0.07230\n0.074 [0.034,\n0.114] p=0.00032\n0.093 [0.053,\n0.134] p<1e-5\n0.098 [0.057,\n0.138] p<1e-5\n64\n0.121 [0.066,\n0.177]\np=0.00002\n0.160 [0.108,\n0.212] p<1e-5\n0.179 [0.126,\n0.232] p<1e-5\n0.183 [0.132,\n0.235] p<1e-5\nCardiomegaly\n224316\n-0.057 [-0.111,\n-0.003]\np=0.03936\n-0.018 [-0.062,\n0.027]\np=0.43275\n-0.011 [-0.047,\n0.025]\np=0.55874\n-0.004 [-0.027,\n0.020]\np=0.74519\n32768\n-0.038 [-0.094,\n0.017] p=0.17618\n0.001 [-0.043,\n0.045]\np=0.97401\n0.008 [-0.026,\n0.042]\np=0.65056\n0.029 [0.008,\n0.051]\np=0.00653\n4096\n-0.018 [-0.066,\n0.029]\np=0.45020\n0.021 [-0.013,\n0.054]\np=0.22596\n0.035 [-0.002,\n0.071]\np=0.06508\n0.049 [0.016,\n0.083]\np=0.00371\n512\n0.013 [-0.034,\n0.061]\np=0.58649\n0.059 [0.020,\n0.099]\np=0.00338\n0.066 [0.022,\n0.110] p=0.00317\n0.081 [0.038,\n0.124]\np=0.00023\n64\n0.123 [0.068,\n0.178] p=0.00001\n0.130 [0.083,\n0.177] p<1e-5\n0.137 [0.082,\n0.192] p<1e-5\n0.152 [0.099,\n0.205] p<1e-5\nConsolidation\n224316\n-0.095 [-0.179,\n-0.011]\np=0.02671\n-0.039 [-0.105,\n0.027]\np=0.25086\n0.001 [-0.056,\n0.058]\np=0.97076\n0.012 [-0.021,\n0.044]\np=0.48487\n32768\n-0.075 [-0.155,\n0.006]\n-0.018 [-0.084,\n0.048]\n0.021 [-0.034,\n0.077]\n0.033 [-0.009,\n0.074] p=0.12735\np=0.06871\np=0.58336\np=0.44896\n4096\n-0.058 [-0.146,\n0.030]\np=0.19550\n-0.002 [-0.060,\n0.056]\np=0.94469\n0.048 [-0.007,\n0.104] p=0.08743\n0.049 [-0.000,\n0.098]\np=0.05150\n512\n0.011 [-0.096,\n0.117]\np=0.84267\n0.107 [0.033,\n0.180]\np=0.00450\n0.117 [0.036,\n0.199]\np=0.00494\n0.118 [0.036,\n0.200]\np=0.00465\n64\n0.175 [0.081,\n0.268]\np=0.00026\n0.214 [0.134,\n0.295] p<1e-5\n0.225 [0.139,\n0.311] p<1e-5\n0.226 [0.142,\n0.309] p<1e-5\nPleural e\u0168usion\n224316\n-0.078 [-0.115,\n-0.042]\np=0.00002\n-0.044 [-0.070,\n-0.018]\np=0.00095\n0.002 [-0.019,\n0.023]\np=0.83881\n0.001 [-0.014,\n0.017]\np=0.89296\n32768\n-0.071 [-0.107,\n-0.035]\np=0.00011\n-0.037 [-0.063,\n-0.011]\np=0.00583\n0.009 [-0.011,\n0.030]\np=0.36040\n0.012 [-0.001,\n0.026]\np=0.07923\n4096\n-0.061 [-0.093,\n-0.029]\np=0.00021\n-0.026 [-0.048,\n-0.005]\np=0.01582\n0.019 [0.004,\n0.033] p=0.01237\n0.022 [0.003,\n0.042]\np=0.02590\n512\n-0.025 [-0.059,\n0.010]\np=0.16249\n0.056 [0.027,\n0.084]\np=0.00012\n0.055 [0.027,\n0.082]\np=0.00009\n0.059 [0.027,\n0.091]\np=0.00032\n64\n0.077 [0.038,\n0.115]\np=0.00009\n0.123 [0.085,\n0.161] p<1e-5\n0.122 [0.082,\n0.161] p<1e-5\n0.126 [0.084,\n0.167] p<1e-5\nPulmonary\nedema\n224316\n-0.066 [-0.107,\n-0.026]\np=0.00130\n-0.036 [-0.069,\n-0.003]\np=0.03056\n-0.011 [-0.031,\n0.009]\np=0.28688\n-0.003 [-0.019,\n0.014]\np=0.74628\n32768\n-0.048 [-0.087,\n-0.010]\np=0.01442\n-0.018 [-0.054,\n0.018]\np=0.33455\n0.007 [-0.018,\n0.032]\np=0.57217\n0.029 [0.006,\n0.051] p=0.01272\n4096\n-0.032 [-0.068,\n0.005]\np=0.09393\n-0.001 [-0.038,\n0.036]\np=0.94866\n0.032 [0.009,\n0.055]\np=0.00608\n0.045 [0.018,\n0.073] p=0.00114\n512\n-0.006 [-0.040,\n0.029]\np=0.75042\n0.050 [0.018,\n0.082]\np=0.00200\n0.058 [0.025,\n0.091]\np=0.00049\n0.071 [0.038,\n0.105]\np=0.00003\n64\n0.049 [0.008,\n0.089]\np=0.01800\n0.074 [0.039,\n0.109]\np=0.00003\n0.082 [0.046,\n0.118] p<1e-5\n0.095 [0.058,\n0.133] p<1e-5\n(b)\nCXR-14\nSample Size\nELIXR-B\n64\n512\n4096\n32768\n68801\n674533\nSample size\nSupCon\nAirspace\nopacity\n674533\n-0.047\n[-0.058,\n-0.035] p<1e-5\n-0.041\n[-0.054,\n-0.029] p<1e-5\n-0.033\n[-0.043,\n-0.024] p<1e-5\n-0.009\n[-0.015,\n-0.003]\np=0.00445\n32768\n-0.028\n[-0.039,\n-0.017] p<1e-5\n-0.023\n[-0.036,\n-0.010]\np=0.00039\n-0.015 [-0.022,\n-0.008]\np=0.00003\n0.027 [0.019,\n0.036] p<1e-5\n4096\n0.010\n[-0.001,\n0.022]\np=0.08267\n0.015 [0.002,\n0.028]\np=0.02128\n0.048 [0.036,\n0.059] p<1e-5\n0.066 [0.053,\n0.078] p<1e-5\n512\n0.018 [0.013,\n0.024] p<1e-5\n0.031 [0.019,\n0.044] p<1e-5\n0.056 [0.043,\n0.069] p<1e-5\n0.074 [0.060,\n0.087] p<1e-5\n64\n0.041 [0.027,\n0.055] p<1e-5\n0.049 [0.036,\n0.062] p<1e-5\n0.073 [0.059,\n0.088] p<1e-5\n0.091 [0.076,\n0.107] p<1e-5\nFracture\n674533\n-0.057\n[-0.141,\n0.027]\np=0.18287\n-0.051\n[-0.138,\n0.036]\np=0.25343\n-0.021\n[-0.093,\n0.052]\np=0.57805\n0.003\n[-0.070,\n0.076]\np=0.92861\n32768\n-0.059\n[-0.134,\n0.016]\np=0.12386\n-0.053\n[-0.128,\n0.023]\np=0.17258\n-0.022\n[-0.104,\n0.060]\np=0.59567\n0.014 [-0.056,\n0.083]\np=0.69461\n4096\n-0.014\n[-0.070,\n0.043]\np=0.63317\n-0.007\n[-0.100,\n0.085]\np=0.87417\n0.047 [-0.022,\n0.116]\np=0.18368\n0.059 [-0.016,\n0.133]\np=0.12069\n512\n-0.006\n[-0.087,\n0.074]\np=0.87682\n0.030 [-0.048,\n0.109]\np=0.45057\n0.054 [-0.029,\n0.137]\np=0.20016\n0.066 [-0.020,\n0.153]\np=0.13367\n64\n0.028 [-0.064,\n0.121]\np=0.54877\n0.058 [-0.046,\n0.163]\np=0.27171\n0.082 [-0.008,\n0.173]\np=0.07281\n0.095 [-0.003,\n0.193]\np=0.05868\nPneumothorax\n674533\n-0.232 [-0.272,\n-0.191] p<1e-5\n-0.114 [-0.149,\n-0.079] p<1e-5\n-0.104 [-0.133,\n-0.075] p<1e-5\n-0.039\n[-0.062,\n-0.016]\np=0.00107\n32768\n-0.172 [-0.209,\n-0.134] p<1e-5\n-0.054\n[-0.093,\n-0.015]\np=0.00676\n-0.045\n[-0.075,\n-0.014]\np=0.00428\n0.066 [0.044,\n0.088] p<1e-5\n4096\n-0.092 [-0.131,\n-0.053] p<1e-5\n0.026 [-0.012,\n0.064]\np=0.18491\n0.101 [0.073,\n0.130] p<1e-5\n0.146 [0.114,\n0.178] p<1e-5\n512\n0.010\n[-0.038,\n0.059]\np=0.67914\n0.137 [0.097,\n0.178] p<1e-5\n0.203 [0.160,\n0.246] p<1e-5\n0.248 [0.209,\n0.287] p<1e-5\n64\n0.178 [0.126,\n0.231] p<1e-5\n0.188 [0.148,\n0.227] p<1e-5\n0.253 [0.204,\n0.303] p<1e-5\n0.298 [0.256,\n0.341] p<1e-5\nConsolidation\n68801\n-0.051 [-0.061,\n-0.041] p<1e-5\n-0.021\n[-0.030,\n-0.012] p<1e-5\n-0.010 [-0.016,\n-0.004]\np=0.00222\n0.004\n[-0.000,\n0.008]\np=0.06665\n32768\n-0.050\n[-0.060,\n-0.039] p<1e-5\n-0.019 [-0.027,\n-0.012] p<1e-5\n-0.009\n[-0.014,\n-0.003]\np=0.00274\n0.004\n[-0.000,\n0.009]\np=0.08012\n4096\n-0.032\n[-0.042,\n-0.022] p<1e-5\n-0.002\n[-0.009,\n0.005]\np=0.60796\n0.023 [0.016,\n0.030] p<1e-5\n0.022 [0.015,\n0.028] p<1e-5\n512\n-0.014 [-0.019,\n-0.009]\np<1e-5\n0.027 [0.018,\n0.036] p<1e-5\n0.041 [0.031,\n0.050] p<1e-5\n0.040 [0.030,\n0.049] p<1e-5\n64\n0.070 [0.056,\n0.083] p<1e-5\n0.080 [0.069,\n0.092] p<1e-5\n0.094 [0.081,\n0.107] p<1e-5\n0.093 [0.080,\n0.106] p<1e-5\nPleural e\u0168usion\n68801\n-0.090\n[-0.097,\n-0.083] p<1e-5\n-0.073\n[-0.079,\n-0.067] p<1e-5\n-0.033\n[-0.038,\n-0.028] p<1e-5\n-0.004\n[-0.006,\n-0.001]\np=0.00524\n32768\n-0.082\n[-0.089,\n-0.075] p<1e-5\n-0.065 [-0.071,\n-0.059] p<1e-5\n-0.025\n[-0.030,\n-0.021] p<1e-5\n0.011 [0.008,\n0.014] p<1e-5\n4096\n-0.051 [-0.057,\n-0.045] p<1e-5\n-0.033\n[-0.039,\n-0.028] p<1e-5\n0.036 [0.031,\n0.041] p<1e-5\n0.043 [0.038,\n0.047] p<1e-5\n512\n0.001\n[-0.006,\n0.007]\np=0.82577\n0.058 [0.052,\n0.064] p<1e-5\n0.087 [0.080,\n0.094] p<1e-5\n0.094 [0.087,\n0.101] p<1e-5\n64\n0.046 [0.041,\n0.051] p<1e-5\n0.086 [0.079,\n0.093] p<1e-5\n0.115 [0.108,\n0.123] p<1e-5\n0.122 [0.115,\n0.129] p<1e-5\nPulmonary\nedema\n68801\n-0.142 [-0.157,\n-0.127] p<1e-5\n-0.090 [-0.101,\n-0.078] p<1e-5\n-0.021 [-0.028,\n-0.014] p<1e-5\n-0.003\n[-0.007,\n0.002]\np=0.24640\n32768\n-0.132 [-0.146,\n-0.117] p<1e-5\n-0.079\n[-0.090,\n-0.068] p<1e-5\n-0.010 [-0.017,\n-0.003]\np=0.00367\n0.013 [0.008,\n0.017] p<1e-5\n4096\n-0.100 [-0.117,\n-0.048\n0.039 [0.030, 0.044 [0.035,\n-0.084] p<1e-5\n[-0.062,\n-0.034] p<1e-5\n0.048] p<1e-5 0.053] p<1e-5\n512\n-0.030\n[-0.050,\n-0.010]\np=0.00342\n0.091 [0.076,\n0.106] p<1e-5\n0.110 [0.095,\n0.124] p<1e-5\n0.114 [0.099,\n0.129] p<1e-5\n64\n0.053 [0.036,\n0.069] p<1e-5\n0.121 [0.107,\n0.136] p<1e-5\n0.140 [0.124,\n0.155] p<1e-5\n0.144 [0.129,\n0.159] p<1e-5\nSupplementary \u016fgures\nSupplementary \u0171gure 1: Suspect positive or negative ground truth MIMIC labels as\nidenti\u0171ed by the query to Med-PaLM 2 truthed by a board ce\u0178i\u0171ed radiologist. Green\nrepresents correctly identi\u0171ed errors or lack of labels in MIMIC labels by Med-PaLM 2. Blue\nrepresents correct MIMIC labels. Red represents errors identi\u0171ed in both MIMIC and MedPalm2\nbased labels while gray represents indeterminate labels. A total of 1568 labels were \u0174agged, of\nwhich 1092 were modi\u0171ed.\nSupplementary \u0171gure 2: Data-e\u016bcient classi\u0171cation pe\u0176ormance between models\nper-dataset and per-\u0171nding using di\u0168erent training set sample size. Evaluation on (a)\nCheXpe\u0178, (b) CXR-14. Signi\u0171cance test results are available in Supplementary tables 9, 10.\n(a)\n(b)\n"
  },
  {
    "title": "Revisiting DETR Pre-training for Object Detection",
    "link": "https://arxiv.org/pdf/2308.01300.pdf",
    "upvote": "7",
    "text": "Revisiting DETR Pre-training for Object Detection\nYan Ma1 \u00b7 Weicong Liang2 \u00b7 Bohan Chen3 \u00b7 Yiduo Hao6 \u00b7\nBojian Hou4 Xiangyu Yue5 \u00b7 Chao Zhang2 \u00b7 Yuhui Yuan6\nAbstract Motivated by the remarkable achievements of\nDETR-based approaches on COCO object detection and\nsegmentation benchmarks, recent endeavors have been di-\nrected towards elevating their performance through self-\nsupervised pre-training of Transformers while preserving\na frozen backbone. Noteworthy advancements in accuracy\nhave been documented in certain studies. Our investiga-\ntion delved deeply into a representative approach, DETReg,\nand its performance assessment in the context of emerging\nmodels like H-Deformable-DETR. Regrettably, DETReg\nproves inadequate in enhancing the performance of robust\nDETR-based models under full data conditions. To dissect\nthe underlying causes, we conduct extensive experiments\non COCO and PASCAL VOC probing elements such as\nthe selection of pre-training datasets and strategies for pre-\ntraining target generation. By contrast, we employ an op-\ntimized approach named Simple Self-training which leads\nto marked enhancements through the combination of an im-\nproved box predictor and the Objects365 benchmark. The\nculmination of these endeavors results in a remarkable AP\nscore of 59.3% on the COCO val set, outperforming H-\nDeformable-DETR + Swin-L without pre-training by 1.4%.\nMoreover, a series of synthetic pre-training datasets, gen-\nerated by merging contemporary image-to-text(LLaVA) and\ntext-to-image (SDXL) models, significantly amplifies object\ndetection capabilities.\nKeywords Object detection, DETR, Pre-training\n1 Introduction\nRecently, the DETR-based approaches (Carion et al., 2020;\nJia et al., 2023; Li et al., 2023; Zhang et al., 2022;\nZhu et al., 2020) have achieved significant progress and\n1University of Toronto\n2Peking University\n3Xi\u2019an Jiaotong-\nLiverpool\nUniversity\n4University\nof\nPennsylvania\n5CUHK\n6Microsoft Research Asia\n\u0000 yuhui.yuan@microsoft.com\npushed the frontier on both object detection and segmen-\ntation tasks. For example, DINO-DETR (Zhang et al.,\n2022), H-Deformable-DETR (Jia et al., 2023), and Group-\nDETRv2 (Chen et al., 2022) have set new state-of-the-art\nobject detection performance on COCO benchmark. Mask-\nDINO (Li et al., 2023) further extends DINO-DETR and es-\ntablishes the best results across COCO instance segmenta-\ntion and panoptic segmentation tasks. To some degree, this\nis the first time that end-to-end transformer approaches can\nachieve an even better performance than the conventional\nheavily tuned strong detectors (Li et al., 2021; Liu et al.,\n2022b) based on convolution, e.g., Cascade Mask-RCNN\nand HTC++.\nDespite the great success of these DETR-based ap-\nproaches, they still choose a randomly initialized Trans-\nformer and thus fail to unleash the potential of a fully pre-\ntrained detection architecture like (Wei et al., 2021), which\nalready verifies the benefits of aligning the pre-training\narchitecture with the downstream architecture. Figure 1a\nand 1b illustrate the distribution of the number of parameters\nand GFLOPs within a standard Deformable-DETR network\nbased on ResNet50 backbone. We can see that the Trans-\nformer encoder and decoder occupy 34% of the parameters\nand 65% of the GFLOPs, which means there exists much\nroom for improvement along the path of performing pre-\ntraining on the Transformer part within DETR.\nSeveral recent works have improved DETR-based ob-\nject detection models by performing self-supervised pre-\ntraining on the Transformer encoder and decoder while\nfreezing the backbone. For example, UP-DETR (Dai et al.,\n2021) pre-trains Transformer to detect random patches in\nan image, DETReg (Bar et al., 2022) pre-trains Transformer\nto match object locations and features with priors gener-\nated from Selective Search algorithm, and most recently,\nSiamese DETR locates the target boxes with the query fea-\ntures extracted from a different view\u2019s corresponding box.\nHowever, these works utilize either the vanilla DETR net-\nwork (AP=42.1% in terms of object detection performance\narXiv:2308.01300v2  [cs.CV]  1 Dec 2023\n2\nMa et al.\nBackbone\n51%\nEncoder\n17%\nDecoder\n17%\nOther\n15%\n(a) #parameters\nBackbone\n30%\nEncoder\n60%\nDecoder\n5%\nOther\n5%\n(b) #GFLOPs\nDETR\nDeformable-DETR\nH-Deformable-DETR\n40\n50\n60\n42.1\n45.2\n49.6\n43.7\n45.5\n49.5\nAP (%)\nBaseline\nDETReg\n(c) COCO object detection results of DETReg.\nFig. 1: The distribution of the number of parameters and\nGFLOPs within Deformable-DETR network with a ResNet50\nbackbone, and the pre-training performance of DETReg. As shown\nin (a) and (b), we can see that around 34% parameters and 65%\nGFLOPs are distributed in the randomly initialized Transformer en-\ncoder and decoder. According to (c), DETReg only improves the\nvanilla DETR and Deformable-DETR by +1.6% and +0.3% while\nshowing no gains over the stronger H-Deformable-DETR.\non COCO) or the Deformable-DETR variant (AP=45.2%).\nTheir results fall significantly short when pre-training on\nthe latest much stronger DETR model like H-Deformable-\nDETR (Jia et al., 2023) (AP=49.6%). In Figure 1c, we\npresent the object detection results of different DETR mod-\nels on COCO under two conditions: without pre-training\nof the Transformer component (referred to as the baseline)\nand with pre-training using the DETReg method. In both\ncases, the backbones of these models are ResNet50 initial-\nized with SwAV (Caron et al., 2020). Notably, in the case\nof the H-Deformable-DETR, the utilization of the DETReg\npre-training actually leads to a performance decrease rather\nthan an improvement.\nIn this work, we first take a closer look at how much\nself-supervised pre-training methods, exemplified by DE-\nTReg, can improve over the increasingly potent DETR mod-\nels on COCO object detection benchmark. Our investiga-\ntion unveils a significant limitation in the efficacy of DE-\nTReg when applied to fortified DETR networks bolstered\nby improvements like the SwAV pre-trained backbone, de-\nformable techniques in Deformable-DETR, and the hybrid\nmatching scheme in H-Deformable-DETR. We pinpoint the\ncrux of the issue as originating from unreliable box pro-\nposals generated by unsupervised methods like Selective\nSearch, which contribute to noisy localization targets, and\nthe weak semantic information provided through feature\nreconstruction which is not an efficient classification tar-\nget either. These drawbacks make the self-supervised pre-\ntraining methods ineffective when applied to an already\nstrong DETR model.\nTo fix this, we propose to use a COCO object de-\ntector to get more accurate pseudo-boxes with informa-\ntive pseudo-class labels. Extensive ablation experiments un-\nderscore the impact of three pivotal factors: the choice\nof pre-training datasets (ImageNet vs. Objects365), local-\nization pre-training targets (Selective Search proposals vs.\npseudo-box predictions), and classification pre-training tar-\ngets (object-embedding vs. pseudo-class predictions).\nOur findings reveal that a Simple Self-training scheme,\nemploying pseudo-box and pseudo-class predictions as\npre-training targets, outperforms the DETReg approach in\nvarious settings. Notably, this simple design yields dis-\ncernible pre-training enhancements even for the state-of-the-\nart DETR network without accessing the pre-training bench-\nmark\u2019s ground-truth label. For example, with a ResNet50\nbackbone and the Objects365 pre-training dataset, Simple\nSelf-training elevates DETReg\u2019s COCO object detection re-\nsults on H-Deformable-DETR by 3.6%. Furthermore, a re-\nmarkable performance is observed with the Swin-L back-\nbone, yielding competitive results 59.3%.\nAdditionally, we delve into an exploration of contem-\nporary image-to-text and text-to-image generation models,\naiming to create a sequence of synthetic datasets for ob-\nject detection pre-training. Empirically, our observations\nyield encouraging outcomes, as pre-training with these\nsynthetic datasets demonstrates commendable performance\neven when compared against the widely adopted Objects365\nbenchmark, which entails substantial annotation costs. In\ngeneral, our efforts are poised to provide a more authentic\nassessment of the progress in the formidable task of DETR\npre-training.\n2 Related Work\nDETR for object detection.\nSince the emergence of\nDETR (Carion et al., 2020) as the first fully end-to-end\nobject detector, many works have extended DETR with\nnovel techniques to achieve state-of-the-art results on var-\nious vision tasks. To accelerate the convergence of the origi-\nnal DETR, Deformable-DETR (Zhu et al., 2020) proposes\na novel multi-scale deformable self/cross-attention to fo-\ncus on a sparse set of important sampling points around a\nreference point. Furthermore, based on DAB-DETR (Liu\net al., 2022a) with a different query formulation, DINO-\nDETR (Zhang et al., 2022) introduces a query denoising\nscheme and sets new records on object detection tasks. Be-\nsides, to address the training efficiency bottleneck caused by\none-to-one matching in DETR, H-Deformable-DETR (Jia\nRevisiting DETR Pre-training for Object Detection\n3\net al., 2023) and Group-DETR (Chen et al., 2022) propose\nto train with more queries in the transformer decoder with\nan additional one-to-many matching scheme, which helps to\nachieve even faster convergence and better performance.\nSelf-supervised pre-training.\nSelf-supervised learning\n(SSL) has achieved remarkable results in image classifi-\ncation methods such as MoCo (He et al., 2020), Sim-\nCLR (Chen et al., 2020), and BYOL (Grill et al., 2020).\nHowever, SSL on object detection has shown limited trans-\nferability. To overcome this challenge, many works have\nproposed pretext tasks that leverage region or pixel local-\nization cues to enhance the pre-training signals. For exam-\nple, InsLoc (Yang et al., 2021a) uses contrastive learning\non foreground patches to learn instance localization. Uni-\nVIP (Li et al., 2022) exploits scene similarity, scene-instance\ncorrelation, and instance discrimination to capture semantic\naffinity. CP2 (Wang et al., 2022) employs pixel-wise con-\ntrastive learning to facilitate both image-level and pixel-\nlevel representation learning. Unlike most of these meth-\nods that aim to improve conventional object detectors such\nas Faster R-CNN or Cascade R-CNN, we focus on design-\ning an effective pre-training scheme for the state-of-the-art\nDETR-based detector.\nDETR pre-training.\nDETR typically relies on a super-\nvised pre-trained backbone on ImageNet and random ini-\ntialization of the transformer encoder and decoder. Some\nrecent works have explored pre-training the transformer\ncomponent of DETR for enhanced object detection perfor-\nmance. For example, UP-DETR (Dai et al., 2021) introduces\nan unsupervised pretext task to detect and reconstruct ran-\ndom patches of the input. DETReg (Bar et al., 2022) re-\nfines the pretext task by using unsupervised region proposals\nfrom Selective Search (Uijlings et al., 2013) instead of ran-\ndom patches and also reconstructs the object embeddings of\nthese regions from its SwAV (Caron et al., 2020) backbone\nto learn invariant representations. Siamese DETR (Huang\net al., 2023) employs a siamese self-supervised learning ap-\nproach to pre-train DETR in a symmetric pipeline where\neach branch takes one view as input and aims to locate\nand discriminate the corresponding regions from another\nview. However, these pre-training methods only yield minor\nimprovements to a strong DETR variant like Deformable-\nDETR.\nSelf-training.\nSelf-training is a powerful technique for\nimproving various computer vision tasks, such as image\nclassification (Li et al., 2023; Sahito et al., 2022), object de-\ntection (Vandeghen et al., 2022; Yang et al., 2021b), and\nsegmentation (Zhu et al., 2021). A common self-training\nmethod is NoisyStudent (Xie et al., 2020), which trains a\nteacher model on labeled data and uses it to generate pseudo-\nlabels on unlabeled images. These pseudo-labels are then\nused to train a student model, and this process is repeated\nto obtain better models by updating the teacher model with\nthe previous student model. The ASTOD (Vandeghen et al.,\n2022) framework applies an iterative self-training process\nfor object detection, using multiple image views to produce\nhigh-quality pseudo-labels. ST++(Yang et al., 2022) is a re-\ncent self-training algorithm for segmentation tasks, which\nuses confidence scores to filter out incorrect pseudo-labels.\n(Zoph et al., 2020) has demonstrated that self-training out-\nperforms traditional pre-training methods in various scenar-\nios, including low and high data regimes, and can even suc-\nceed when pre-training methods fail. Unlike these complex\nself-training schemes that use an iterative approach to refine\npseudo-labels, we propose a Simple Self-training scheme\nthat generates pseudo-labels only once by keeping a fixed\nnumber of the most confident predictions.\n3 Approach\nIn this work, we focus on studying how to perform pre-\ntraining over the Transformer encoder and decoder parts\nwithin DETR for object detection tasks following (Bar et al.,\n2022; Dai et al., 2021). The goal of DETR pre-training is to\ndesign an effective pretext task that can make the best use\nof a large-scale unlabeled dataset that has no ground-truth\nbounding box annotations.\n3.1 Formulation\nThe conventional DETR model has three components,\nthe backbone extracting the image feature, the encoder en-\nhancing the feature with a self-attention mechanism, and the\ndecoder turning query inputs into object class and location\npredictions through cross-attention with image features. The\nexisting self-supervised pre-training methods share a simi-\nlar scheme that optimizes the encoder and decoder network\nparameters on the pre-training dataset while freezing a pre-\ntrained backbone. After pre-training, all three components\nare tuned together on the downstream dataset. The pipeline\nis illustrated in Figure 2.\nPreliminary.\nIn the following article, we formulate the\ngeneral self-supervised pre-training process as several equa-\ntions. We use f\u03b8B, f\u03b8E, f\u03b8D to represent the backbone, Trans-\nformer encoder, and Transformer decoder within a DETR\nnetwork parameterized by \u03b8B, \u03b8E, and \u03b8D. The input im-\nages from the pre-training and downstream dataset are de-\nnoted as X = {x1, \u00b7 \u00b7 \u00b7 , xN} and X = {x1, \u00b7 \u00b7 \u00b7 , xM} re-\nspectively, where N\u226bM. The ground-truth label of down-\nstream data is Y={y1, \u00b7 \u00b7 \u00b7 , yM|yi=(ci, bi)}, where ci is\nthe category label and bi is the box location label. Typi-\ncally, the domain-specific pre-training data labels are lack-\n4\nMa et al.\nBackbone\nTransformer Decoder\nTransformer Encoder\nImageNet\n(Self-)Supervised\nPre-trained\nBackbone\nTransformer Decoder\nTransformer Encoder\nRandom\nInitialized\n Detection\nSelf-Supervised\nPre-trained\nPre-training\nDataset\nPseudo-\nLabels\nDownstream\nDataset\nGround-truth\nLabels\nFig. 2: The overall framework of self-supervised pre-training scheme. There are two steps to pre-train the DETR network. In the first step, we\nfreeze the backbone and pre-train a randomly initialized Transformer encoder and decoder with the well-designed pre-training target on a large-\nscale pre-training benchmark. In the second step, we initialize the encoder and decoder with pre-trained weights and fine-tune all the parameters\nof the DETR network on the downstream dataset supervised by ground-truth labels.\ning and most works choose to generate the pseudo-labels,\ni.e., Y = {y1, \u00b7 \u00b7 \u00b7 , yN} instead.\nPre-train.\nWe illustrate the mathematical formulations of\nthe DETR pre-training with Equation 1 and 2. Specifically,\nthe pre-training input xi is forwarded through the back-\nbone f\u03b8B, encoder f\u03b8E, and decoder f\u03b8D to get the predic-\ntion zi. Here \u03b8B, \u03b8E, \u03b8D represent the learnable parameters\nfor the three network components respectively. \u03b8B is initial-\nized with SwAV (Caron et al., 2020) self-supervised pre-\ntraining method and frozen during pre-training. \u03b8E and \u03b8D\nare randomly initialized and then optimized to minimize the\npre-training loss Lpre(\u00b7), which is calculated with network\noutput zi and pre-training target yi.\nzi = f\u03b8D(f\u03b8E(f\u03b8B(xi)), Q),\n(1)\nb\u03b8D, b\u03b8E, bQ = argmin\n\u03b8D,\u03b8E,Q\nN\nX\ni=1\nLpre(zi, yi),\n(2)\nwhere Q = {q1, \u00b7 \u00b7 \u00b7 , qk} represents the learnable object\nquery of decoder and will also be jointly optimized with\nthe encoder/decoder parameters. b\u03b8D, b\u03b8E, bQ represent the de-\ncoder parameters, encoder parameters, and object query af-\nter pre-training. In the following section 3.2, we will illus-\ntrate the formulation of Lpre in different methods.\nFine-tune.\nWe obtain the optimized encoder and decoder\nparameter b\u03b8E, b\u03b8D during pre-training. Then we tune the same\nnetwork on the downstream data xi. Here, we initialize the\nbackbone, encoder and decoder parameter with \u03b8B, b\u03b8E, b\u03b8D,\nand denote the network output as zi. All parameters of the\nthree components and learnable query Q are optimized to\nminimize the downstream loss Lds(\u00b7) between zi and down-\nstream label yi.\nzi = fb\u03b8D(fb\u03b8E(f\u03b8B(xi)), bQ),\n(3)\ne\u03b8D, e\u03b8E, e\u03b8B, eQ = argmin\nb\u03b8D,b\u03b8E,\u03b8B,bQ\nM\nX\ni=1\nLds(zi, yi),\n(4)\nwhere e\u03b8D, e\u03b8E, e\u03b8B, eQ are optimized decoder, encoder, back-\nbone parameters, and object query after downstream fine-\ntuning.\n3.2 Instantiations\nAssume the target of the i-th pre-training input can be\ndenoted as yi = {yi1, \u00b7 \u00b7 \u00b7 , yim}, where m is the number\nof objects in each target. The network output consists of k\nbounding box predictions, which is the same as the number\nof object queries. We denote the corresponding prediction\nas zi = {zi1, \u00b7 \u00b7 \u00b7 , zik}. Typically, the number of targets in\nyi is less than 30, while we set our DETR network to out-\nput 100 or 300 predictions, so m < k. Thus we pad the\ntargets with no-object category \u2205 following DETR (Car-\nion et al., 2020) to be of size k. Then, DETR performs\none-to-one alignment via Hungarian bipartite matching al-\ngorithm (Kuhn, 1955) over yi and zi. We illustrate the math-\nematical formulation in Equation 5, which computes the op-\ntimal label assignment for each prediction by minimizing\nthe matching cost function Lmatch(\u00b7):\n\u03c3i = argmin\n\u03c3i\u2208\u03a3k\nk\nX\nj=1\nLmatch(yij, zi\u03c3i(j)),\n(5)\nwhere \u03a3k represents all permutations over k elements and\n\u03c3i(j) maps the targeted box j to the most similar pre-\ndicted box within the i-th input. The matching cost function\nLmatch(\u00b7) measures the predictions from two aspects includ-\ning the localization accuracy and classification accuracy fol-\nlowing DETR (Carion et al., 2020).\nMost self-supervised pre-training methods differentiate\nthrough the design of pretext tasks, which results in differ-\nent structures for the pre-training target yi and implementa-\ntions of the pre-training loss Lpre. A good pretext task design\ncan improve the final prediction performance. In the follow-\ning, we first introduce the instantiation of a representative\nmethod called DETReg (Bar et al., 2022). Then, we pro-\npose two more effective pre-training schemes: DETReg +\nPseudo-box and Simple Self-training. Both methods focus\non enhancing the localization and classification pre-training\ntarget quality. We compare the pre-training pipeline of three\nmethods in Figure 3.\nDETReg.\nDETReg uses an unsupervised region proposal\nmethod named Selective Search (ss) to generate the tar-\nRevisiting DETR Pre-training for Object Detection\n5\nget boxes. The j-th \u201cbox proposal\u201d for the i-th input is\ndenoted as b\nss\nij\n\u2208 [0, 1]4. We select the top k Selective\nSearch box proposals {b\nss\ni1, \u00b7 \u00b7 \u00b7 , b\nss\nik} and pair them with\nthe binary category target padded to the size of network\nquery number k (k > k) {pss\ni1, \u00b7 \u00b7 \u00b7 , pss\nik|pss\ni1, \u00b7 \u00b7 \u00b7 , pss\nik =\n1, pss\ni(k+1), \u00b7 \u00b7 \u00b7 , pss\nik = 0}, where pss\nij = 1 indicates the el-\nement is a box proposal while pss\nij = 0 indicates a padded\n\u2205. To compensate for the lack of semantic information in\nthe binary category, the DETReg network incorporates an-\nother object embedding reconstruction branch to predict the\nobject embeddings {f i1, \u00b7 \u00b7 \u00b7 , f ik|f ij \u2208 Rd} of detected\nboxes, which is supervised by the target object descriptor\n{f\nswav\ni1 , \u00b7 \u00b7 \u00b7 , f\nswav\nik } with f\nswav\nij\nindicating the object embed-\nding extracted from the image patch in the j-th box proposal\non the i-th input with a fixed SwAV backbone. Therefore,\nthe pre-training target and network prediction are denoted\nas Equation 6:\nyij = (pss\nij , b\nss\nij , f\nswav\nij\n),\nzij = (pij, bij, f ij).\n(6)\nThe pre-training loss is the sum of binary classification\nloss Lbin\ncls (\u00b7), box loss Lbox(\u00b7), and embedding loss Lemb(\u00b7)\nthrough all k outputs as below:\nLpre(yi, zi) =\nk\nX\nj=1\n\u03bbcLbin\ncls (pss\nij , pi\u03c3i(j))\n+ \u03bbb1{pss\nij \u0338=0}Lbox(b\nss\nij , bi\u03c3i(j))\n+ \u03bbeLemb(f\nswav\nij\n, f i\u03c3i(j)),\n(7)\nwhere Lbin\ncls (\u00b7) is the binary classification loss which can be\nimplemented as Cross Entropy Loss or Focal Loss. Lbox(\u00b7)\nis the sum of L1 and GIoU Loss, and Lemb(\u00b7) is the L1 Loss.\n\u03bbc, \u03bbb, and \u03bbe are loss coefficients and \u03c3i(j) maps the target\nbox j to the assigned predicted box \u03c3i(j) with lowest cost\nwithin the i-th input.\nDETReg + Pseudo-box.\nThe unsupervised box propos-\nals like Selective Search boxes are of very low quality.\nTo handle this, we employ two off-the-shelf well-trained\nCOCO object detectors to predict the pseudo-boxes for the\npre-training data to replace the Selective Search proposals.\nSpecifically, we replace the (pss\nij , b\nss\nij ) in Equation 6 and 7\nwith (ppseudo\nij\n, b\npseudo\nij\n). We use H-Deformable-DETR with\nResNet50 or Swin-L backbone as our detector network. We\nfirst train them on COCO, then use the trained detector to\npredict pseudo-boxes on the pre-training dataset, and the\ntop 30 predictions are selected as k.\nSimple Self-training.\nWe further replace the binary cat-\negory target ppseudo\nij\nwith category predictions cpseudo\nij\n\u2208\n{\u2205, c1, \u00b7 \u00b7 \u00b7 , cn} of aforementioned COCO object detectors\nas the classification target and remove f\nswav\nij\nsince we already\nhave detailed class information. Due to that the detector is\nLocalization method\nAP\nAP50 AP75 APS APM APL AR@10 AR@30 AR@100\nSelective Search\n0.5\n1.6\n0.2\n0.2\n0.3\n1.2\n3.7\n8.3\n15.5\nH-Deformable-DETR + R50\n28.4\n40.4\n30.2 12.7 26.7 43.1\n26.5\n37.4\n47.7\nH-Deformable-DETR + Swin-L 30.7 41.3\n33.0 15.2 29.0 44.9\n28.1\n38.5\n47.4\nTable 1: Objects356 AP and AR score for Selective Search box\nproposals, and pseudo-box predictions of H-Deformable-DETR-based\nCOCO detectors with R50 and Swin-L backbone.\ntrained on COCO and the pseudo-category labels it predicts\nare the 80 COCO categories, the binary classification turns\ninto a multiclass classification. The formulation is shown be-\nlow:\nyij = (cpseudo\nij\n, b\npseudo\nij\n),\nzij = (cij, bij),\n(8)\nLpre(yi, zi) =\nk\nX\nj=1\n\u03bbcLmul\ncls (cpseudo\nij\n, ci\u03c3i(j))\n+ \u03bbb1{cpseudo\nij\n\u0338=\u2205}Lbox(b\npseudo\nij\n, bi\u03c3i(j)),\n(9)\nwhere Lmul\ncls (\u00b7) is the multiclass classification loss.\n3.3 Discussion\nWe utilize ImageNet and Objects365 as the two pre-\ntraining benchmarks. To display the quality of Selective\nSearch proposals and pseudo-boxes generated by two off-\nthe-shelf COCO object detectors, we report their boxes\u2019 Av-\nerage Precision and Average Recall on Objects365 valida-\ntion set in Table 1. As can be seen, pseudo-boxes generated\nby COCO object detectors are far more accurate than Selec-\ntive Search boxes. We also visualize their box proposals in\nFigure 4.\nUnlike the conventional self-training scheme (Xie et al.,\n2020; Zoph et al., 2020) that relies on applying complicated\naugmentation strategy to boost the quality of pseudo-labels,\nadjusting NMS threshold carefully, and re-generating more\naccurate pseudo-labels based on the fine-tuned models in an\niterative manner, our Simple Self-training method directly\ngenerate the pseudo-labels for one time without those tricks,\nresulting in a much simpler approach.\n4 Experiment\n4.1 Implementation Details\nDatasets.\nOur object detection network is pre-trained on\nthe ImageNet or Objects365 (Shao et al., 2019) bench-\nmark, then fine-tuned on COCO train2017 and evaluated\non COCO val2017, or fine-tuned on PASCAL VOC train-\nval07+12 and evaluated on PASCAL VOC test2007. For the\npre-training benchmarks, ImageNet has 1.2 Million images\n6\nMa et al.\nBackbone\nTransformer\nPre-training\nDataset\nObject embed\nBox proposals\nCrop\nlocation\nhave object\nTarget embed\nBinary-class\nBoxes\nBackbone\nSelective\nSearch\n(a) DETReg\nBackbone\nTransformer\nPre-training\nDataset\nObject embed\nPseudo-boxes\nCrop\nlocation\nhave object\nTarget embed\nBinary-class\nBoxes\nBackbone\nCOCO detector\n(b) DETReg+pseudo-box\nImageNet\n(Self-)Supervised\nPre-trained\nRandom Initialized\nBackbone\nTransformer\nPre-training\nDataset\nMulti-class\nPseudo-boxes\nBoxes\nPseudo-classes\nCOCO detector\nCOCO Supervised\nPre-trained\n(c) Simple Self-training\nFig. 3: The pre-training pipelines of DETReg, DETReg+pseudo-box, and Simple Self-training. In DETReg and DETReg+pseudo-box, we\nuse an extra frozen backbone branch to get the target object embeddings from the image crops. The binary-class outputs of the Transformer predict\nwhether the detected boxes contain an object.\nGround-Truth\nSelective Search\nH-Def-DETR + R50\nH-Def-DETR + Swin-L\nFig. 4: Qualitative comparisons of the top 30 generated bounding\nboxes of different methods on Objects365. The methods include Se-\nlective Search and trained H-Deformable-DETR detectors with R50 or\nSwin-L backbones.\nwhich mostly contain one object since the dataset is cre-\nated for classification. Objects365 is a large-scale dataset for\nobject detection with 2 Million images. The image scene\nis more complicated with around 15 ground-truth bound-\ning boxes per image on average. We use Objects365 as the\ndefault pre-training benchmark for all experiments in sec-\ntions 4.2 and 4.4, as its complex scenes bring better pre-\ntraining performance for the Simple Self-training approach.\nArchitectures.\nWe use two kinds of DETR backbones in-\ncluding ResNet50 which is self-supervised pre-trained by\nSwAV on ImageNet and Swin-L which is supervised pre-\ntrained on ImageNet. We pre-train three DETR-based ar-\nchitectures in Section 4.3 including vanilla DETR (Car-\nion et al., 2020), Deformable-DETR (Zhu et al., 2020),\nand H-Deformable-DETR (Jia et al., 2023), which is a\nrecent state-of-the-art object detector based on a combi-\nnation of an improved Deformable-DETR and an effec-\ntive hybrid matching scheme. The Transformer module\nin those architectures is composed of 6 encoder layers\nand 6 decoder layers. The vanilla DETR and Deformable-\nDETR are plain without tricks, while H-Deformable-DETR\nis improved with iterative bounding box refinement, two-\nstage (Zhu et al., 2020), mixed query selection, and look\nforward twice scheme (Zhang et al., 2022). By default, we\nuse H-Deformable-DETR with ResNet50 backbone for the\nablation study.\nTraining.\nWe pre-train the network on ImageNet for 5\nepochs following DETReg or on Objects365 for 3 epochs to\nensure the same iteration number according to their different\ndataset sizes. For fine-tuning, we train for 150 epochs with\nRevisiting DETR Pre-training for Object Detection\n7\nMethod\nFramework Backbone #epoch AP AP50 AP75 APS APM APL\nSwin (Liu et al., 2021)\nHTC\nSwin-L\n36\n57.1 75.6 62.5 42.4 60.7 71.1\nGroup-DETR (Chen et al., 2022)\nDETR\nSwin-L\n36\n58.4\n-\n-\n41.0 62.5 73.9\nDINO-DETR (Zhang et al., 2022)\nDETR\nSwin-L\n36\n58.5 77.0 64.1 41.5 62.3 74.0\nH-Deformable-DETR (Jia et al., 2023)\nDETR\nSwin-L\n36\n57.9 76.9 63.7 42.4 61.9 73.4\nOurs (pre-trained H-Deformable-DETR)\nDETR\nSwin-L\n24\n59.3 77.9 65.1 44.1 62.9 73.6\nTable 2: System-level comparisons with the state-of-the-art DETR-\nbased single-scale evaluation results on COCO val set.\nMethod\nDETR model\nPretrain #query #epoch AP AP50 AP75 APS APM APL\nfrom scratch\nDETR\n-\n100\n150\n40.3 61.3\n42.2 18.2 44.6 60.5\nDETReg\nDETR\nImageNet\n100\n150\n40.2 60.7\n42.3 17.6 44.3 59.6\nours\nDETR\nImageNet\n100\n150\n41.9 62.7\n44.0 20.7 46.0 62.8\nfrom scratch\nDDETR-MS\n-\n300\n50\n45.2 64.2\n49.4 27.2 49.3 59.1\nDETReg\nDDETR-MS\nImageNet\n300\n50\n43.5 61.4\n47.3 24.2 47.1 58.7\nours\nDDETR-MS\nImageNet\n300\n50\n46.0 64.4\n50.0 26.6 49.8 61.5\nfrom scratch H-DDETR-MS\n-\n300\n12\n49.6 67.5\n54.1 31.9 53.3 64.1\nDETReg\nH-DDETR-MS ImageNet\n300\n12\n49.5 66.8\n53.9 30.5 53.5 63.6\nours\nH-DDETR-MS ImageNet\n300\n12\n51.6 69.4\n56.4 35.0 55.3 66.8\nTable 3: Comparisons with self-supervised pre-training method DE-\nTReg on the COCO downstream benchmark.\nMethod\nDETR model\nPretrain #query #epoch AP AP50 AP75 APS APM APL\nfrom scratch\nDETR\n-\n100\n150\n56.3 80.3\n60.6 10.2 36.0 65.9\nDETReg\nDETR\nImageNet\n100\n150\n60.9 82.0\n65.9 15.1 40.8 69.8\nours\nDETR\nImageNet\n100\n150\n63.5 83.8\n68.6 22.5 44.3 72.1\nfrom scratch\nDDETR-MS\n-\n300\n50\n61.1 83.1\n68.0 25.5 47.4 67.7\nDETReg\nDDETR-MS\nImageNet\n300\n50\n63.6 82.6\n70.2 27.5 49.7 70.2\nours\nDDETR-MS\nImageNet\n300\n50\n67.8 85.4\n75.5 30.9 54.7 74.4\nfrom scratch H-DDETR-MS\n-\n300\n12\n63.8 82.4\n70.0 26.5 50.0 70.4\nDETReg\nH-DDETR-MS ImageNet\n300\n12\n67.7 84.5\n74.9 35.1 55.1 74.7\nours\nH-DDETR-MS ImageNet\n300\n12\n71.6 87.0\n79.2 33.1 60.3 78.2\nTable 4: Comparisons with self-supervised pre-training method DE-\nTReg on the PASCAL VOC downstream benchmark.\nMethod\nPre-training dataset\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nDETReg\nImageNet\n49.5\n66.8\n53.9\n30.5\n53.5\n63.6\nO365\n49.2\n66.5\n53.6\n31.4\n53.2\n63.5\nDETReg+pseudo-box\nImageNet\n50.9\n68.3\n55.7\n33.6\n54.6\n64.9\nO365\n52.0\n69.6\n56.7\n36.1\n55.9\n65.3\nSimple Self-training\nImageNet\n51.6\n69.4\n56.4\n35.0\n55.3\n66.8\nO365\n52.8\n70.9\n57.6\n37.0\n56.6\n67.3\nTable 5: Effect of pre-training dataset choices.\nvanilla DETR, 50 epochs with Deformable-DETR, and 12\nepochs with H-Deformable-DETR, or 24 epochs with H-\nDeformable-DETR in Section 4.2 for better performance.\nThe learning rate drops at 120/150, 40/50, 11/12, and\n20/24 respectively. The batch size for pre-training and fine-\ntuning are both 16.\nMetrics.\nWe measure the object detection accuracy for the\ntop 100 detected bounding boxes. Specifically, we compute\nAP, AP50 and AP75 as the average precision when using\nIoU thresholds across the range of 0.50 to 0.95, and exactly\nMethod\nLocalization target\nClassification target\nAP AP50AP75APS APM APL\nfrom scratch\n-\n-\n49.6 67.5 54.1 31.9 53.3 64.1\nDETReg\nSelective Search\nObject-embedding loss 49.2 66.5 53.6 31.4 53.2 63.5\nDETReg+pseudo-boxPseudo-box prediction Object-embedding loss 52.0 69.6 56.7 36.1 55.9 65.3\nSimple Self-training Pseudo-box predictionPseudo-class prediction 52.8 70.9 57.6 37.0 56.6 67.3\nSupervised\nGround-truth\nGround-truth\n53.2 71.5 58.1 37.3 57.0 67.4\nTable 6: Fine-tuning results on COCO after pre-training with differ-\nent methods using various localization and classification pre-training\ntargets on Objects365.\nMethod\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nfrom scratch\n63.8\n82.4\n70.0\n26.5\n50.0\n70.4\nDETReg\n67.7\n84.7\n74.1\n34.8\n55.9\n74.3\nDETReg+pseudo-box\n71.6\n87.0\n79.1\n36.1\n59.0\n77.9\nSimple Self-training\n71.6\n87.9\n79.7\n33.5\n60.2\n78.7\nSupervised\n72.6\n88.0\n80.7\n37.6\n62.6\n78.6\nTable 7: Fine-tuning results on PASCAL VOC after pre-training with\ndifferent methods on Objects365.\n#pseudo-box\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\n30\n52.0\n69.6\n56.7\n36.1\n55.9\n65.3\n60\n51.6\n69.1\n56.6\n34.8\n55.4\n65.5\n100\n51.5\n68.9\n56.3\n34.9\n54.7\n65.4\nTable 8: Ablation experiments on the number of pseudo-boxes for the\nDETReg+pseudo-box method.\nMethod\nEncoder\nDecoder\nAP\nAP50\nAP75\nAPS\nAPM\nAPL\nDETReg+pseudo-box\n\u2713\n\u2713\n52.0\n69.6\n56.7\n36.1\n55.9\n65.3\n\u2713\n49.4\n67.1\n53.5\n32.0\n53.2\n63.2\n\u2713\n51.5\n69.6\n56.1\n35.4\n55.3\n65.5\nSimple Self-training\n\u2713\n\u2713\n52.8\n70.9\n57.6\n37.0\n56.6\n67.3\n\u2713\n50.2\n68.2\n54.3\n32.4\n54.1\n63.6\n\u2713\n51.8\n69.6\n56.4\n34.9\n55.4\n66.6\nTable 9: Effect of Transformer encoder or decoder pre-training.\nof 0.50 or 0.75; and also APS, APM, APL as the AP for\nsmall, medium, large bounding boxes.\n4.2 Comparison to the State-of-the-art\nTable 2 shows the object detection result on COCO val-\nidation set of H-Deformable-DETR network pre-trained on\nObjects365 benchmark with our method in comparison with\nother state-of-the-art object detection systems. Our Simple\nSelf-training approach significantly boosts the performance\nof H-Deformable-DETR from 57.9% to 59.3% with fewer\ntraining epochs. We expect our approach to achieve better\nresults with bigger batch size and epoch number, for in-\nstance, batch size of 256 and epoch of 60 are used for the\nself-supervised pre-training in Siamese DETR (Huang et al.,\n2023).\n4.3 Results on different DETR architectures\nAs shown in Table 3 and Table 4, we display the results\nof DETReg and Simple Self-training with different DETR\narchitectures on the COCO and PASCAL VOC benchmarks.\n8\nMa et al.\n5%\n10%\n25%\n50%\n0\n5\n10\n15\n20\n+3.0\n+3.1\n+1.7\n+1.0\n+9.9\n+8.5\n+4.2\n+2.1\n+18.9\n+15.2\n+8.3\n+3.1\nLabeled Data (%)\nAP (%)\nProportion of agents in households\nDETReg\nDETReg+pseudo box\nsimple self-training\nFig. 5: Ablation experiments on low-data regimes. The value shows\nthe performance improvement of three pre-training schemes compared\nto the from scratch baseline.\nThe line of from scratch shows the baseline results with-\nout pre-training as the ResNet50 backbone is initialized with\nSwAV and the Transformer is randomly initialized. The re-\nsults show that with the reported experiment setting, the DE-\nTReg pre-training fails to bring improvement to the from\nscratch baseline on the COCO benchmark, while can get\nsmall gains on the PASCAL VOC benchmark. Our Sim-\nple Self-training can effectively improve the baseline per-\nformance for all three DETR architectures on both bench-\nmarks.\n4.4 Ablation Experiments and Analysis\nChoice of pre-training dataset.\nWe also investigate the\nimpact of pre-training datasets with the H-Deformable-\nDETR architecture in Table 5. Compared to ImageNet, pre-\ntraining with the Objects365 benchmark yields better per-\nformance with the DETReg+pseudo-box and Simple Self-\ntraining approach, which is not the case with the DE-\nTReg approach. As DETReg+pseudo-box and Simple Self-\ntraining employ accurate pseudo-boxes of COCO detec-\ntors as the pre-training targets, they can benefit from a\nmore complex image scene that contains richer objects like\nObjects365, while Selective Search\u2019s chaotic proposals on\nObjects365 may not be better localization targets than its\nproposals on ImageNet. It has been proved that ImageNet\nis a good benchmark for pre-training general representa-\ntion ability, which can be transferred to multiple down-\nstream tasks. However, for pre-training a specific detec-\ntion network, a large-scale object detection benchmark like\nObjects365 is more helpful if the pseudo-box has good qual-\nity. Therefore, we use Objects365 as the default pre-training\nbenchmark for the following studies.\nPre-training methods.\nWe present the downstream re-\nsults on the COCO benchmark of the from scratch and dif-\nferent pre-training methods in Table 6 and results on the\nPASCAL VOC benchmark in Table 7. All methods ex-\ncept from scratch are pre-trained on the Objects365 bench-\nmark. The middle three pre-training methods do not uti-\nlize Objects365 ground-truth labels, while the last is su-\npervised by ground-truth and thereby serves as an upper\nbound. The difference between the three unsupervised pre-\ntraining pipelines is illustrated in Figure 3. As shown in\nthe Table 6 and 7, the DETReg+pseudo-box method builds\nupon DETReg and improves its localization targets by utiliz-\ning more accurate COCO detector pseudo-boxes, leading to\nsignificant improvement. The Simple Self-training method\ndiscards the object-embedding loss and instead supervises\nthe multi-class classification head with the class predic-\ntions of COCO detectors, resulting in further performance\ngains. For the supervised method, we replace the pseudo-\nbox and pseudo-class targets in the Simple Self-training with\nground-truth and achieve an upper-bound performance that\nis only slightly better than our Simple Self-training strategy.\nThis step-by-step comparison demonstrates how we can pro-\ngressively improve the pre-training performance by intro-\nducing better localization and classification targets. Addi-\ntionally, we observe that better localization pre-training tar-\ngets are more impactful than better classification targets for\nobject detection tasks.\nPseudo-box Number.\nIn Table 8, we ablate with the num-\nber of pseudo-boxes in the DETReg + pseudo-box method.\nWe observe that using more than 30 pseudo-boxes for pre-\ntraining does not improve the performance, despite more\npseudo-boxes exhibiting higher recall on the ground-truth\n(as shown in Table 1, where AR@10, 30, 100 means AR\nwith 10, 30, 100 proposed boxes) and providing more su-\npervision signals. A possible explanation is that each Ob-\njects365 image contains approximately 15 box annotations,\nand the predictions beyond the top 30 may have low confi-\ndence and less meaningful information, as a result of incor-\nporating noise into the pseudo-box target.\nEncoder/Decoder Pre-training.\nWe evaluate the impor-\ntance of Transformer encoder and decoder pre-training\nin the DETReg+pseudo-box and Simple Self-training ap-\nproaches in Table 9. We first report the performance of us-\ning both the encoder and decoder pre-trained parameters,\nthen we report the results of only loading the encoder or\ndecoder pre-trained parameters and random initializing the\nother part. In both pre-training approaches, we observe that\nthe encoder pre-training contributes more than the decoder\npre-training, which is reasonable considering the high ratio\nof encoder GFLOPs shown in 1b.\nFine-tuning dataset size.\nWe investigate the effectiveness\nof three pre-training schemes compared to training from\nscratch when only a limited amount of data is available for\nRevisiting DETR Pre-training for Object Detection\n9\nText prompt\nGernerative model\nPretraining dataset\nLocalization target\nClassification target\nCOCO\nPASCAL VOC\nAP\nAP50\nAP75\nAP\nAP50\nAP75\n-\n-\nO365\nPseudo-box prediction\nPseudo-class prediction\n52.8\n70.9\n57.6\n71.6\n87.9\n79.7\nCOCO captions\nControlNet\nControl-COCO 2M\nGround-truth\nGround-truth\n51.1\n69.2\n55.8\n71.7\n87.8\n79.2\nCOCO captions\nControlNet\nControl-COCO 2M\nPseudo-box prediction\nPseudo-class prediction\n52.6\n70.6\n57.5\n72.0\n87.8\n80.4\nLLaVA captions\nControlNet\nLLaVAControl-COCO 2M\nGround-truth\nGround-truth\n50.7\n69.6\n55.4\n71.6\n87.5\n79.5\nLLaVA captions\nControlNet\nLLaVAControl-COCO 2M\nPseudo-box prediction\nPseudo-class prediction\n52.9\n70.8\n57.9\n72.3\n87.7\n80.4\nCOCO captions\nSDXL\nSDXL-COCO 2M\nPseudo-box prediction\nPseudo-class prediction\n52.5\n70.7\n57.3\n72.1\n87.6\n79.7\nLLaVA captions\nSDXL\nSDXL-COCO 2M\nPseudo-box prediction\nPseudo-class prediction\n52.9\n71.0\n58.0\n72.0\n87.6\n80.1\nTable 10: Evaluation results of pre-training with synthetic images similar to COCO generated by text-to-image generative models ControlNet\nand SDXL. The text prompts given to the generative models are COCO ground-truth captions (represented as COCO captions) or the generated\ncaptions by the large multimodal model LLaVA based on COCO images (represented as LLaVA captions).\nText prompt\nGernerative model\nPretraining dataset\nLocalization target\nClassification target\nCOCO\nPASCAL VOC\nAP\nAP50\nAP75\nAP\nAP50\nAP75\nLLaVA captions\nControlNet\nLLaVAControl-O365 2M\nPseudo-box prediction\nPseudo-class prediction\n52.4\n70.5\n57.2\n71.8\n87.6\n79.8\nLLaVA captions\nSDXL\nSDXL-O365 2M\nPseudo-box prediction\nPseudo-class prediction\n52.6\n70.6\n57.6\n71.6\n87.4\n79.3\nTable 11: Evaluation results of pre-training with synthetic images similar to Objects365 generated by ControlNet and SDXL. Since Objects365\ndoes not have ground-truth captions, the text prompts given to the generative models are generated captions by LLaVA based on Objects365\nimages (represented as LLaVA captions).\nfine-tuning in Figure 5. Specifically, we fine-tune the pre-\ntrained network on 5%, 10%, 25%, and 50% of the COCO\ntraining set and evaluate it on the full COCO validation set.\nAll three pre-training schemes greatly speed up the conver-\ngence. We observe that DETReg only yields slightly higher\nperformance than random initialization. The Simple Self-\ntraining approach remains the most effective, particularly\nwhen only a very small amount of data (5%) is available.\nQualitative analysis.\nWithout fine-tuning, we visualize\nthe discriminability scores (Zong et al., 2022) of the pre-\ntrained encoder in Figure 6 to investigate what the encoder\nhas learned in pre-training. From the figures, we can see that\nDETReg\u2019s feature discriminability is seriously disturbed by\nthe background. However, when we utilize improved local-\nization and classification targets in the DETReg+pseudo-\nbox and Simple Self-training approach, finer details are\ncaptured. Notably, the Simple Self-training method demon-\nstrates performance that is almost on par with pre-training\nusing ground-truth.\nWe also visualize the deformable cross-attention of the\npre-trained decoder in Figure 7. The colored dots in the\nimage represent the sampling points from all resolution\nscales, where the color indicates the attention weights, with\na lighter color indicating higher attention. As random ini-\ntialization shows, the initial key points are sampled radially\nfrom the center to the edge. All pre-training methods learn\nto scatter the sampling points across the entire object of in-\nterest with different patterns, while the Simple Self-training\npre-trained decoder can sample key points from an accurate\nrange of objects and distribute attention weight more effec-\ntively.\n4.5 Results with synthetic data generated by T2I\nLast, we investigate the effectiveness of pre-training\nwith synthetic data, which is generated using recent large-\nscale text-to-image generation models. Specifically, we\nleverage two representative text-to-image models, Control-\nNet (Zhang and Agrawala, 2023) and SDXL (Podell et al.,\n2023), to generate images. These models take original cap-\ntions from the COCO dataset or captions generated by\nLLaVA (Liu et al., 2023) as prompts for image synthesis.\nControlNet uses predicted depth maps from DPT (Ranftl\net al., 2021) as conditional input to generate images that\nmatch both the depth maps and captions. On the other hand,\nSDXL generates images solely based on the provided cap-\ntions without any additional conditions. We create a syn-\nthetic dataset comprising 2.3 Million generated images. Fig-\nure 8 displays some examples.\nUpon analyzing the images produced by ControlNet, we\nfind that they closely resemble the layout of the original\n10\nMa et al.\nImage\nRandom initialization\nDETReg\nDETReg+ Pseudo-box\nSimple Self-training\nSupervised-pretraining\nFig. 6: Visualizations of discriminability scores in the encoder on COCO val images.\nRandom initialization\nDETReg\nDETReg+ Pseudo-box\nSimple Self-training\nSupervised-pretraining\nFig. 7: Visualizations of deformable cross-attention based on the last Transformer decoder layer on COCO val images.\nimages due to the conditioning on depth maps. This char-\nacteristic allows us to use COCO ground-truth data to su-\npervise the pretraining process when using synthetic images\ngenerated by ControlNet. Additionally, we also explore the\nSimple Self-training approach on the synthetic data by pre-\ntraining with pseudo-box and pseudo-class predictions that\nare generated by trained COCO detectors. The process in-\nvolves pre-training the H-Deformable-DETR model with\nsynthetic images for 3 epochs, followed by fine-tuning on\nCOCO or PASCAL VOC benchmarks for 12 epochs. The\nresults of this evaluation are presented in Table 10. Inter-\nestingly, pre-training with the synthetic dataset generated\nbased on COCO demonstrates comparable improvements\nto pre-training with Objects365 real data using the Simple\nSelf-training scheme. This outcome indicates that text-to-\nimage synthesis is an effective method for scaling up the\noriginal dataset for pre-training. Furthermore, the results on\nthe PASCAL VOC benchmark showcase the generalization\nability of pre-training with synthetic data generated based\non COCO.\nTable 11 shows the results of pre-training with the syn-\nthetic data generated based on Objects365 by first caption-\ning Objects365 image with LLaVA and then synthesizing\nnew images from the caption. They are not as good as pre-\ntraining with COCO-based synthetic data on both down-\nstream benchmarks.\n5 Conclusion\nWe investigate the effectiveness of DETReg, a representa-\ntive self-supervised pre-training approach for DETR, across\nRevisiting DETR Pre-training for Object Detection\n11\nthree distinct DETR architectures. Our findings, unfortu-\nnately, do not reveal any performance enhancements of DE-\nTReg in recent architectures, thereby challenging the valid-\nity of previous conclusions. In response, we reevaluate cru-\ncial design aspects, including pre-training targets for local-\nization and classification. As a result of this analysis, we in-\ntroduce several impactful enhancements and a Simple Self-\ntraining scheme that significantly boosts performance across\nstrong DETR architectures. Additionally, we leverage the\npowerful text-to-image generative models to construct syn-\nthetic datasets for pre-training purposes. Remarkably, our\napproach yields improvements on par with the achievements\nof pre-training with Objects365. Moving forward, we plan\nto extend DETR pre-training to encompass a broader spec-\ntrum of vision tasks, such as instance segmentation and pose\nestimation. We hope our work can stimulate the research\ncommunity to reassess the actual capacity of existing self-\nsupervised pre-training methods when employed in the con-\ntext of strong DETR models and advance the progress on\nthis challenging task.\n12\nMa et al.\nOriginal images\nCOCO captions\n+ ControlNet\nLLaVA captions\n+ ControlNet\nCOCO captions\n+ SDXL\nLLaVA captions\n+ SDXL\nFig. 8: Examples of synthetic images using different captions and generative models. The original images are sampled from COCO train set.\nRevisiting DETR Pre-training for Object Detection\n13\nData availability statement\nThe author confirmed that the data supporting the findings\nof this study are available within the article. Raw data that\nsupport the findings of this study and the generated synthetic\ndataset are available from the corresponding author, upon\nreasonable request.\nReferences\nBar A, Wang X, Kantorov V, Reed CJ, Herzig R, Chechik G, Rohrbach\nA, Darrell T, Globerson A (2022) Detreg: Unsupervised pretraining\nwith region priors for object detection. In: CVPR, pp 14605\u201314615\nCarion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko\nS (2020) End-to-end object detection with transformers. In: ECCV,\nSpringer, pp 213\u2013229\nCaron M, Misra I, Mairal J, Goyal P, Bojanowski P, Joulin A (2020)\nUnsupervised learning of visual features by contrasting cluster as-\nsignments. NeurIPS 33:9912\u20139924\nChen Q, Wang J, Han C, Zhang S, Li Z, Chen X, Chen J, Wang X, Han\nS, Zhang G, et al. (2022) Group detr v2: Strong object detector with\nencoder-decoder pretraining. arXiv preprint arXiv:221103594\nChen T, Kornblith S, Norouzi M, Hinton G (2020) A simple framework\nfor contrastive learning of visual representations. In: ICML, PMLR,\npp 1597\u20131607\nDai Z, Cai B, Lin Y, Chen J (2021) Up-detr: Unsupervised pre-training\nfor object detection with transformers. In: CVPR, pp 1601\u20131610\nGrill JB, Strub F, Altch\u00e9 F, Tallec C, Richemond P, Buchatskaya E,\nDoersch C, Avila Pires B, Guo Z, Gheshlaghi Azar M, et al. (2020)\nBootstrap your own latent-a new approach to self-supervised learn-\ning. NeurIPS 33:21271\u201321284\nHe K, Fan H, Wu Y, Xie S, Girshick R (2020) Momentum contrast for\nunsupervised visual representation learning. In: CVPR, pp 9729\u2013\n9738\nHuang G, Li W, Teng J, Wang K, Chen Z, Shao J, Loy CC, Sheng L\n(2023) Siamese detr. In: CVPR, pp 15722\u201315731\nJia D, Yuan Y, He H, Wu X, Yu H, Lin W, Sun L, Zhang C, Hu H\n(2023) Detrs with hybrid matching. In: CVPR, pp 19702\u201319712\nKuhn HW (1955) The hungarian method for the assignment problem.\nNaval research logistics quarterly 2(1-2):83\u201397\nLi F, Zhang H, Xu H, Liu S, Zhang L, Ni LM, Shum HY (2023) Mask\ndino: Towards a unified transformer-based framework for object de-\ntection and segmentation. In: CVPR, pp 3041\u20133050\nLi Y, Wu CY, Fan H, Mangalam K, Xiong B, Malik J, Feichtenhofer\nC (2021) Improved multiscale vision transformers for classification\nand detection. arXiv preprint arXiv:211201526\nLi Z, Zhu Y, Yang F, Li W, Zhao C, Chen Y, Chen Z, Xie J, Wu L, Zhao\nR, et al. (2022) Univip: A unified framework for self-supervised\nvisual pre-training. In: CVPR, pp 14627\u201314636\nLiu H, Li C, Wu Q, Lee YJ (2023) Visual instruction tuning\nLiu S, Li F, Zhang H, Yang X, Qi X, Su H, Zhu J, Zhang L (2022a)\nDab-detr: Dynamic anchor boxes are better queries for detr. arXiv\npreprint arXiv:220112329\nLiu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B (2021) Swin\ntransformer: Hierarchical vision transformer using shifted windows.\nIn: ICCV, pp 10012\u201310022\nLiu Z, Hu H, Lin Y, Yao Z, Xie Z, Wei Y, Ning J, Cao Y, Zhang Z,\nDong L, et al. (2022b) Swin transformer v2: Scaling up capacity\nand resolution. In: CVPR, pp 12009\u201312019\nPodell D, English Z, Lacey K, Blattmann A, Dockhorn T, M\u00fcller J,\nPenna J, Rombach R (2023) Sdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis. 2307.01952\nRanftl R, Bochkovskiy A, Koltun V (2021) Vision transformers for\ndense prediction. In: ICCV, pp 12179\u201312188\nSahito A, Frank E, Pfahringer B (2022) Better self-training for im-\nage classification through self-supervision. In: AJCAI, Springer, pp\n645\u2013657\nShao S, Li Z, Zhang T, Peng C, Yu G, Zhang X, Li J, Sun J (2019)\nObjects365: A large-scale, high-quality dataset for object detection.\nIn: ICCV, pp 8430\u20138439\nUijlings JR, Van De Sande KE, Gevers T, Smeulders AW (2013) Se-\nlective search for object recognition. IJCV 104(2):154\u2013171\nVandeghen R, Louppe G, Van Droogenbroeck M (2022) Adaptive self-\ntraining for object detection. arXiv preprint arXiv:221205911\nWang F, Wang H, Wei C, Yuille A, Shen W (2022) Cp 2: Copy-\npaste contrastive pretraining for semantic segmentation. In: ECCV,\nSpringer, pp 499\u2013515\nWei F, Gao Y, Wu Z, Hu H, Lin S (2021) Aligning pretraining for\ndetection via object-level contrastive learning. NeurIPS 34:22682\u2013\n22694\nXie Q, Luong MT, Hovy E, Le QV (2020) Self-training with noisy stu-\ndent improves imagenet classification. In: CVPR, pp 10687\u201310698\nYang C, Wu Z, Zhou B, Lin S (2021a) Instance localization for self-\nsupervised detection pretraining. In: CVPR, pp 3987\u20133996\nYang L, Zhuo W, Qi L, Shi Y, Gao Y (2022) St++: Make self-training\nwork better for semi-supervised semantic segmentation. In: CVPR,\npp 4268\u20134277\nYang Q, Wei X, Wang B, Hua XS, Zhang L (2021b) Interactive self-\ntraining with mean teachers for semi-supervised object detection.\nIn: CVPR, pp 5941\u20135950\nZhang H, Li F, Liu S, Zhang L, Su H, Zhu J, Ni LM, Shum HY (2022)\nDino: Detr with improved denoising anchor boxes for end-to-end\nobject detection. arXiv preprint arXiv:220303605\nZhang L, Agrawala M (2023) Adding conditional control to text-to-\nimage diffusion models. arXiv preprint arXiv:230205543\nZhu X, Su W, Lu L, Li B, Wang X, Dai J (2020) Deformable detr:\nDeformable transformers for end-to-end object detection. In: ICLR\nZhu Y, Zhang Z, Wu C, Zhang Z, He T, Zhang H, Manmatha R, Li\nM, Smola AJ (2021) Improving semantic segmentation via efficient\nself-training. PAMI\nZong Z, Song G, Liu Y (2022) Detrs with collaborative hybrid assign-\nments training. arXiv preprint arXiv:221112860\nZoph B, Ghiasi G, Lin TY, Cui Y, Liu H, Cubuk ED, Le Q (2020)\nRethinking pre-training and self-training. NeurIPS 33:3833\u20133845\n"
  },
  {
    "title": "More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes",
    "link": "https://arxiv.org/pdf/2308.01313.pdf",
    "upvote": "7",
    "text": "Published as a conference paper at ICLR 2024\nPERCEPTIONCLIP: VISUAL CLASSIFICATION BY IN-\nFERRING AND CONDITIONING ON CONTEXTS\nBang An1\u2217\nSicheng Zhu1\u2217\nMichael-Andrei Panaitescu-Liess1\nChaithanya Kumar Mummadi2\nFurong Huang1\n1University of Maryland, College Park\n2Bosch Center for Artificial Intelligence\nABSTRACT\nVision-language models like CLIP are widely used in zero-shot image classification\ndue to their ability to understand various visual concepts and natural language\ndescriptions. However, how to fully leverage CLIP\u2019s unprecedented human-like un-\nderstanding capabilities to achieve better performance is still an open question. This\npaper draws inspiration from the human visual perception process: when classifying\nan object, humans first infer contextual attributes (e.g., background and orientation)\nwhich help separate the foreground object from the background, and then classify\nthe object based on this information. Inspired by it, we observe that providing CLIP\nwith contextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably infer\nthe attributes from an image. With these observations, we propose a training-free,\ntwo-step zero-shot classification method PerceptionCLIP. Given an image, it first\ninfers contextual attributes (e.g., background) and then performs object classifica-\ntion conditioning on them. Our experiments show that PerceptionCLIP achieves\nbetter generalization, group robustness, and interpretability. Our code is available\nat https://github.com/umd-huang-lab/perceptionCLIP.\nFigure 1: (Left): CLIP co-relates natural language descriptions of contextual attributes with visual\ncues (orientation: upside-down). (Center): Unlike CLIP\u2019s standard zero-shot inference that uses\nfixed template(s) for class name retrieval, our method first infers contextual attributes (background:\non the grass) using CLIP and then let CLIP predicts the class conditioned on the inferred contextual\nattributes. Here, background and orientation are both examples of contextual attributes. (Right):\nGrad-CAM visualization illustrates that our method focuses more on core features (on the dog) and is\nless distracted by spurious features (grass background) when performing the object classification.\n1\nINTRODUCTION\nCLIP (Contrastive Language-Image Pretraining by Radford et al. (2021)) is a foundational Vision-\nLanguage Model (VLM) that connects the fields of vision and natural language. By pretraining on\n400 million image-caption pairs, CLIP can associate various visual concepts with their corresponding\nnatural language descriptions, making it the foundation for numerous other vision-language models\n\u2217Equal contribution\n1\narXiv:2308.01313v3  [cs.CV]  18 Mar 2024\nPublished as a conference paper at ICLR 2024\n(Zhu et al., 2023; Liu et al., 2023; Dai et al., 2023; Li et al., 2023b), diffusion models (Ramesh\net al., 2022; Rombach et al., 2022), and semantic segmentation models (Kirillov et al., 2023). This\nremarkable understanding capability of CLIP is significant for zero-shot classification (Larochelle\net al., 2008), enabling open-ended image classification via natural language without training. This\ncapability also addresses many challenging tasks with limited or no downstream data, such as model\ndeployment in the wild (Li et al., 2023a), medical image classification (Wang et al., 2022) and satellite\nobject recognition (Ramaswamy et al., 2023).\nAlthough CLIP shows strong potential in zero-shot classification, current methods treat image classi-\nfication as a text retrieval task and lack systematic investigation into the text prompts used. This leads\nto sub-optimal generalization (Radford et al., 2021), reliance on spurious features (Yang et al., 2023),\nbiased predictions (Agarwal et al., 2021; Chuang et al., 2023), and lack of interpretability (Zhou\net al., 2022b; Menon & Vondrick, 2022). For example, Radford et al. (2021) uses a basic template\n\"a photo of a {class name}\" to identify the most relevant class for an image, much less informative\nthan the image captions used during pretraining (see examples in Table 9). Another method, prompt\nensembling (Radford et al., 2021), employs 80 crafted templates for better generalization. Neverthe-\nless, it remains unclear whether these templates are optimal and why they are effective. By treating\nzero-shot classification simply as a class name retrieval problem, these methods potentially waste the\ncapability of CLIP to understand both class-specific features and class-independent attributes such as\nbackground and orientation (referred to as contextual attributes in this paper).\nGiven CLIP\u2019s unprecedented human-like vision and language understanding, a natural idea is to\ndraw inspiration from human visual perception. Classic neuroscience (Kandel et al., 2013) describes\nhuman visual perception as a three-tiered, context-dependent process: first discerning basic visual\nattributes like color and orientation, then analyzing scene layout and distinguishing foreground from\nbackground, and finally recognizing objects (see details in Appendix C). For example, when humans\nclassify objects in images, we unconsciously acquire contextual attributes like the background and\norientation, and in the case of an upside-down image (Figure 1 left), we first infer that the image is\nrotated and then calibrate our classification accordingly. This hierarchical and context-dependent\nprocess contrasts with existing classification methods, which overlook contextual attributes.\nBuilding on this insight, we propose a zero-shot classification method called PerceptionCLIP,\nwhich emulates a crucial part of human visual perception \u2014 inferring and conditioning on the\ncontextual attributes \u2014 resulting in improved generalization, reduced reliance on spurious features,\nbetter group robustness, and interpretability. Our contributions are as follows:\n\u25b7 (1) We prepare CLIP for perception by structuring CLIP-understandable text prompts with\ncontextual attributes and introducing an attribute-aware CLIP score to approximate essential\nconditional probabilities for perception emulation.\n\u25b7 (2) Through two proof-of-concept investigations, we reveal that conditioning on ground-truth\ncontextual attributes improves CLIP\u2019s zero-shot classification and mitigates reliance on spurious\nfeatures. Moreover, CLIP has the ability to infer contextual attributes by itself.\n\u25b7 (3) Based on the observations, we propose PerceptionCLIP. Given an image, as shown in\nFigure 1, it first employs CLIP to infer contextual attributes. Then, it uses CLIP to infer the class\nconditioned on the attributes by incorporating the descriptions of the inferred attributes into the\nprompt. This two-step inference resembles the concept of chain-of-thoughts in language models.\n\u25b7 (4) We empirically demonstrate that PerceptionCLIP excels in both standard generalization\nand group robustness, exhibiting improved interpretability. For generalization, it consistently\noutperforms baselines that use simple templates and prompt ensembles on 11 datasets. For\nexample, it provides a near 5% accuracy gain on the EuroSAT dataset. For group robustness, it\nreduces the gap between average accuracy and worst group accuracy by 19% on the Waterbirds\ndataset and 7% on the CelebA dataset with ViT-L/14, showing less reliance on spurious features.\n2\nRELATED WORK\nDue to CLIP\u2019s ability to understand finer-grained visual concepts beyond classes, some work also\nleverages external knowledge to augment prompts. For example, Menon & Vondrick (2022); Pratt\net al. (2022); Mao et al. (2022); Feng et al. (2023) use large language models to generate class-specific\ndescriptions, resulting in prompts like \"a photo of a hen, which has two legs\". Novack et al. (2023)\n2\nPublished as a conference paper at ICLR 2024\nuse class hierarchies to generate sub-classes for each parent class and aggregate model predictions\non all sub-classes to get a final prediction. Udandarao et al. (2023) use class names to retrieve and\nmaintain some auxiliary data to help downstream classification. In contrast, our method addresses\nclass-independent attributes (i.e., contextual attributes) such as background and orientation, whose\ncomprehension by CLIP is not well-known. These attributes are also combinatorial, potentially\ncovering more aspects of an image than class-specific attributes. Moreover, we can still leverage\ncontextual attributes (e.g., gender, age) when class-specific attributes are hard to articulate, as in the\nhair-color classification tasks on CelebA. We defer more related work to Appendix A.\n3\nPRELIMINARIES\nNotation. We use uppercase letters to denote random variables and lowercase letters to denote their\nrealizations. For a random variable Z, we use pZ(z) to denote its probability mass or density function,\nand omit the subscript Z when the function\u2019s meaning can be inferred from the input notation z.\nPretraining of CLIP. CLIP is pretrained on web-scale image-caption pairs, using a contrastive loss\nto learn good image and text representations in a shared space, aiming to correctly associate images\nand their textual descriptions. The captions in the pretraining data (as shown in Table 9) typically\ndescribe not only the object\u2019s class but also contextual attributes like color, style, and background.\nZero-shot classification. After pretraining, Radford et al. (2021) use a universal prompt template,\nrepresented by an annotation function \u03b1(y) = \"a photo of a {class name of y}\", that takes the class\nindex y as the input and outputs a text that only describes the class. For any image x in the image\nspace X and y in the class set Y, the CLIP model serves as a score function CLIP1 : Y \u00d7 X \u2192 R via\nCLIP1(y; x)\n\u225c\n\u27e8\u03d5I(x),\n\u03d5T (\u03b1(y))\u27e9,\n(1)\ncomputing a similarity score (within [\u22121, 1]) between the image and text through inner products of\ntheir representations produced by image encoder \u03d5I and the text encoder \u03d5T . The subscript \u20181\u2019 in\n\u2018CLIP1\u2019 indicates that only one textual template is used. Then, given an image x, the method predicts\nthe class \u02c6y \u2208 Y as the one with the highest CLIP1 score, \u02c6y = arg maxy\u2208Y CLIP1(y; x).\nIn addition, Radford et al. (2021) propose prompt ensembling, which ensembles 80 manually-designed\ntemplates {\u03b1i}80\ni=1, such as \u2018a bad photo of a {class name of y}\u2019 and \u2018a sculpture of a {class name of\ny}\u2019, and replace CLIP1 with the following CLIP80 score for inference. Prompt ensembling involves\nsome contextual attributes in the templates, but it is ad-hoc and lacks a systematic analysis.\nCLIP80(y; x)\n\u225c\n*\n\u03d5I(x),\n1\n80\nP80\ni=1 \u03d5T (\u03b1i(y))\n\r\r\r 1\n80\nP80\ni=1 \u03d5T (\u03b1i(y))\n\r\r\r\n+\n.\n(2)\n4\nPREPARING CLIP FOR PERCEPTION\n4.1\nSTRUCTURING AND DESCRIBING CONTEXTUAL ATTRIBUTES\nFigure 2: Illustration of contextual attributes, their\nsymbolic discrete values, and the possible textual\ndescriptions mapped by the annotation function.\nContextual attributes as generative factors.\nWe consider contextual attributes as generative\nfactors that contribute to the data generation pro-\ncess. Specifically, let Y denote the underlying\nobject class (e.g., dog) that takes values in the\nclass set Y. Let each Zi (1 \u2264 i \u2264 m) denote a\ncertain contextual attribute of the object (e.g.,\norientation) that takes values in the contextual\nattribute set Zi (e.g., {upright, upside-down, ro-\ntated}) and is causally independent (Pearl, 2009)\nof the object class Y . Then, we consider an im-\nage X to be generated as Y \u2192 X \u2190 {Zi}m\ni=1.\nTextual descriptions for contextual attributes. While CLIP requires semantic text, generative\nfactors are often symbolized discrete values, thus creating a gap. It is negligible for the objects\u2019\nclasses since class names are descriptions with no ambiguities. However, the textual descriptions\n3\nPublished as a conference paper at ICLR 2024\nFigure 3: Evaluating CLIP scores on ImageNet with different transformations altering the contextual\nattributes. The attribute-aware CLIP score gives higher scores for correctly matched image-attribute\npairs (green) while giving lower scores for mismatched pairs (grey) and random pairs (blue), confirm-\ning CLIP\u2019s understanding of our contextual attribute descriptions. CLIP score measures the similarity\nbetween images and contextual attributes, while the original CLIP score (orange) is attribute-agnostic.\nof the contextual attributes are vague. Taking upright images as an example, people may use terms\nlike \"upright,\" \"upstanding,\" or no description since it is a common direction. To bridge this gap\nand translate discrete values into CLIP-readable text, we introduce a specific annotation function\n\u03b1 : Z \u2192 P(text), which maps a symbolic discrete value in Z to a distribution over natural language\ntextual descriptions. Figure 2 illustrates some examples. An ideal annotation function models people\u2019s\npreferences when captioning images. We form the final image description using the concatenation\noperation \u2295. This operation results in a new description distribution \u03b1(y)\u2295\u03b1(z1)\u2295\u03b1(z2)\u2295... where\nattributes\u2019 descriptions are concatenated together and separated by commas. For example, when\ny, z1, z2 represent \"dog,\" \"upright,\" and \"bright\" respectively, the concatenation \u03b1(y)\u2295\u03b1(z1)\u2295\u03b1(z2)\nyields the description \"a photo of a dog, upright, bright,\" or \"a photo of a dog, sunny,\" etc.\n4.2\nCONNECTING CONDITIONAL PROBABILITIES WITH CLIP SCORE\nAttribute-aware CLIP score. Existing CLIP score is agnostic of contextual attributes and thus cannot\napproximate conditional probabilities that are attribute-dependent. Therefore, we define a new score\nfunction CLIP : Y \u00d7 Z1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Zm \u00d7 X \u2192 R:\nCLIP(y, z1, . . . , zm; x)\n\u225c\n\u001c\n\u03d5I(x),\nE \u03d5T\n\u0000\u03b1(y) \u2295 \u03b1(z1) \u2295 \u00b7 \u00b7 \u00b7 \u2295 \u03b1(zm)\n\u0001\n\u2225E \u03d5T\n\u0000\u03b1(y) \u2295 \u03b1(z1) \u2295 \u00b7 \u00b7 \u00b7 \u2295 \u03b1(zm)\n\u0001\n\u2225\n\u001d\n.\n(3)\nIt takes contextual attributes zis as additional inputs, describes them internally alongside the class\nthrough the annotation function \u03b1(zi), and calculates the similarity with the image in the embedding\nspace. The expectation is taken over the randomness of the descriptions of contextual attributes. The\ndefined CLIP score captures the contextual attributes and behaves like an energy function (LeCun\net al., 2006): it is high for correctly matched image-attribute pairs while low for mismatched ones.\nMore formally, when (y\u2217, z\u2217\n1, . . . , z\u2217\nm) are the ground-truth class and attributes that generate image\nx\u2217 whereas (y, z1, . . . , zm) are some arbitrary class and attributes,\nCLIP(y\u2217, z\u2217\n1, . . . , z\u2217\nm) \u2265 CLIP(y, z1, . . . , zm),\n\u2200 y \u2208 Y,\n\u2200 zi \u2208 Zi,\n\u2200 1 \u2264 i \u2264 m.\n(4)\nFigure 3 and 5 empirically verified this property (see Appendix E.1 for details). Given the pretraining\nprocess, this observation is not surprising since it encourages high scores for correctly matched\nimage-caption pairs where the caption describes not only the class but also the contextual attributes.\nTable 1: Conditional probabilities. x, y, and z\ndenote image, class, and contextual attributes. z\ndenotes (z1, . . . , zm) for simplicity.\nProbability\nApproximation\np(y, z|x)\neCLIP(y,z;x)\nP\ny\nP\nz eCLIP(y,z;x)\np(y|x, z)\neCLIP(y,z;x)\nP\ny eCLIP(y,z;x)\np(z|x)\nP\ny eCLIP(y,z;x)\nP\nz\nP\ny eCLIP(y,z;x) or\neCLIP(z;x)\nP\nz eCLIP(z;x)\nApproximating conditional probabilities. With\nthe energy-function-like CLIP score, we approx-\nimate the conditional probabilities.\nSpecifi-\ncally (in Table 1 and Appendix D), we ap-\nproximate (1) the joint conditional probability\np(y, z1, . . . , zm|x), which measures the likeli-\nhood of an object class and some contextual at-\ntributes occurring together given the image, requir-\ning only exponentiation and normalization. Based\non it, we derive the rest two using the law of to-\ntal probability. (2) the conditional probability\np(y|z1, . . . , zm, x), which measures the proba-\nbility of an object class given both the image and\n4\nPublished as a conference paper at ICLR 2024\nTable 2: Classification accuracy (%) on ImageNet. We apply the left-side image transformations to\nalter the corresponding attributes. Different methods condition on different values of the contextual\nattributes. Conditioning on correct or self-inferred attribute values improves accuracy the most.\nContextual attribute\nAccuracy\nw/o z\nw/ random z\nw/ wrong z\nw/ correct z\nw/ self-infer z\nvertical flip\n51.17\n52.02 (\u21910.85)\n52.19 (\u21911.02)\n52.48 (\u21911.31)\n52.54 (\u21911.37)\n90\u00b0 rotation\n57.02\n58.38 (\u21911.36)\n58.23 (\u21911.21)\n58.75 (\u21911.73)\n58.30 (\u21911.28)\nelastic-transform\n48.66\n48.45 (\u21930.21)\n48.75 (\u21910.09)\n48.89 (\u21910.23)\n49.00 (\u21910.34)\ncolor-invert\n35.29\n36.12 (\u21910.83)\n35.89 (\u21910.60)\n36.72 (\u21911.43)\n36.80 (\u21911.51)\nsolarize\n49.79\n49.74 (\u21930.05)\n50.20 (\u21910.41)\n50.49 (\u21910.70)\n50.54 (\u21910.75)\nblur\n38.86\n39.65 (\u21910.79)\n39.21 (\u21910.35)\n39.92 (\u21911.06)\n39.80 (\u21910.94)\ngrayscale\n59.51\n59.67 (\u21910.16)\n59.48 (\u21930.03)\n59.98 (\u21910.47)\n60.04 (\u21910.53)\nbright\n60.81\n62.04 (\u21911.23)\n60.94 (\u21910.13)\n61.41 (\u21910.60)\n61.28 (\u21910.47)\nnoise\n14.16\n14.88 (\u21910.72)\n14.75 (\u21910.59)\n15.66 (\u21911.50)\n15.68 (\u21911.52)\nsnow\n33.09\n32.94 (\u21930.15)\n33.56 (\u21910.47)\n34.50 (\u21911.41)\n34.33 (\u21911.24)\nfrost\n31.08\n31.91 (\u21910.83)\n31.76 (\u21910.68)\n32.63 (\u21911.55)\n32.81 (\u21911.73)\nfog\n37.61\n38.40 (\u21910.79)\n38.00 (\u21910.39)\n39.31 (\u21911.70)\n39.34 (\u21911.73)\njpeg\n33.67\n34.80 (\u21911.13)\n35.11 (\u21911.45)\n35.39 (\u21911.72)\n35.47 (\u21911.80)\naverage\n-\n\u21910.64\n\u21910.57\n\u21911.16\n\u21911.17\nthe contextual attributes, which is our main inference objective. (3) the conditional probability\np(z1, . . . , zm|x), measures the likelihood of some contextual attributes given the image and is used\nfor inferring the contextual attributes. We provide two approximations, referred to as ClassAttr (left)\nand PureAttr (right). The textual description corresponding to CLIP(y, z; x) in ClassAttr is \"a photo\nof a {class name of y}, {description of z},\" while the description corresponding to CLIP(z; x) in\nPureAttr is \"a photo of an object, {description of z}\" with a word like \"object\" substituting all classes.\n5\nCONTEXTUAL ATTRIBUTES ARE HELPFUL AND INFERABLE\nThis section presents proof-of-concept experiments showing that emulating human perception through\nconditional inference on contextual attributes improves zero-shot classification. Additionally, such\nimprovement does not require ground-truth attributes, as CLIP itself can infer attributes reasonably.\n5.1\nCONDITIONING ON CONTEXTUAL ATTRIBUTES IS HELPFUL\nWe first evaluate if conditioning on the ground-truth contextual attributes improves the zero-shot\nclassification accuracy. Given an image x, the most likely class is \u02c6y = arg maxy p(y|x, z\u2217) with:\narg max\ny\np(y|x, z\u2217) = arg max\ny\neCLIP(y,z\u2217;x)\nP\ny eCLIP(y,z\u2217;x) = arg max\ny\nCLIP(y, z\u2217; x),\n(5)\nwhere the second equality holds because P\ny eCLIP(y,z;x) is a constant of y and exponential function is\nmonotonic. Intuitively, we classify an image by considering the combinations of all possible classes\nwith the ground-truth contextual attributes and identify the class that yields the highest CLIP score.\nConditioning on ground-truth contextual attributes improves classification accuracy. We\ncompare the following four methods in zero-shot classification, where the last two are for ablation:\nConditioning on\nCalculation\nPrompt example\nNo contextual attributes\narg maxy CLIP1(y; x)\na photo of a {class name of y}.\nGround-truth attribute values\narg maxy CLIP(y, z\u2217; x)\na photo of a {class name of y}, upside-down.\nWrong attribute values\narg maxy CLIP(y, zwrong; x)\na photo of a {class name of y}, upright.\nRandom attribute values\narg maxy CLIP(y, zrandom; x)\na photo of a {class name of y}, iaYo5n0Dli7.\nWe evaluate these methods on ImageNet dataset. Similar to Figure 3, we alter easily observable\nand adjustable attributes such as orientation through image transformations (e.g., vertical flipping).\nThese new attributes become part of the modified images\u2019 generation process, for which we have\nground-truth annotations. Table 2 shows that compared to not using contextual attributes, conditioning\n5\nPublished as a conference paper at ICLR 2024\nFigure 4: Images of a leopard and a waterbird, core and spurious features, and Grad-CAM heatmaps\nusing no, incorrect, and ground-truth contextual attributes (with text below images). The bar shows\ncore vs. spurious ratio in the heatmap. Visualization shows that classification conditioned on correct\ncontextual attributes enforces CLIP\u2019s focus on core features.\non ground-truth contextual attributes improves classification accuracy notably. As an ablation study,\nconditioning on wrong or randomly generated contextual attributes does not yield similar benefits.\nConditioning on ground-truth contextual attributes mitigates the reliance on spurious features.\nContextual attributes like background (e.g., grass) may exhibit spurious correlations with the class\n(e.g., dog). Classifiers relying on these contextual attributes, also known as spurious features, usually\nperform poorly. We investigate whether classification conditioned on the known spurious features\ncan enforce CLIP\u2019s focus on the object (i.e., core features). As shown in Figure 4, we isolate the\nbackground from the core region and employ Grad-CAM (Selvaraju et al., 2017) to identify which\nregion the model focuses on during classification. Specifically, the gradients on pixels with respect\nto p(y\u2217|x, z\u2217), the likelihood of the correct class conditioned on the known background given the\nimage, yields the saliency heatmap. Figure 1 and 4 illustrate that CLIP may rely on spurious features,\nhowever, conditioning on correct contextual attributes reduces such reliance and enforces the model\nto focus on core features, resulting in a more interpretable and reasonable perception (see more results\nin Appendix E.3). Since image embedding captures both object and background, we suspect that\nspecifying an image\u2019s background to CLIP minimizes background influence, potentially sharpening\nthe focus on object features for a better image and text matching in the embedding space.\n5.2\nCONTEXTUAL ATTRIBUTES ARE INFERABLE\nThe above results highlight the advantages of leveraging CLIP\u2019s understanding of contextual attributes.\nHowever, manually annotating the attributes is impractical. We now investigate whether CLIP can\ninfer contextual attributes. To infer z, we calculate arg maxz p(z|x) using one of the two approxima-\ntions in Table 1, where the ClassAttr option yields arg maxz p(z|x) = arg maxz\nP\ny eCLIP(y,z;x),\nand the PureAttr option yields arg maxz p(z|x) = arg maxz CLIP(z; x).\nCLIP can infer contextual attributes. Different from the setting in Section 5.1, we randomly apply\ntransformations to only half of the images in ImageNet. Therefore, inferring each attribute is a binary\nclassification task with a random guessing accuracy of 50%. Table 3 shows that the average accuracy\nis around 74% for both methods, indicating that CLIP can reasonably infer contextual attributes, with\nsome attributes being easier to infer than others. CLIP\u2019s understanding of contextual attributes may\noriginate from the numerous captions during the pre-training stage. Moreover, inferring contextual\nattributes could be easier than determining the object class. Therefore, we may bootstrap CLIP\u2019s\ninference by conditioning on the contextual attributes inferred by itself which is verified in Table 2.\nTable 3: Inference accuracy (%) of two contextual attribute inference methods on ImageNet.\nAttribute\nvflip\nrotation\nelastic\ninvert\nsolarize\nblur\ngray\nbright\nnoise\nsnow\nfrost\nfog\njpeg\nAvg\nClassAttr\n76.30\n68.65\n72.03\n78.67\n74.67\n62.91\n84.67\n56.98\n66.00\n86.56\n82.39\n89.11\n66.66\n74.28\nPureAttr\n77.31\n66.01\n60.00\n80.61\n88.79\n59.26\n74.26\n58.94\n67.16\n86.56\n78.23\n93.95\n68.71\n73.83\n6\nPublished as a conference paper at ICLR 2024\n6\nPerceptionCLIP: EMULATING HUMAN PERCEPTION\nBuilding on the above observations, we propose PerceptionCLIP, a two-step zero-shot classifi-\ncation method for CLIP. It emulates the human perception process by first inferring the contextual\nattributes and then inferring the class conditioning on the contextual attributes. The pseudocode of\nPerceptionCLIP is outlined in Algorithm 1.\nAlgorithm 1: PerceptionCLIP\nRequire : class Y , contextual attributes {Z1, . . . , Zm}, CLIP score (with annotation function \u03b1),\ntemperature hyperparameter \u03c4\nInput\n: image x\nOutput : predicted class \u02c6y\nStep 1: infer the distribution of contextual attribute values\n\u02c6p(z1, . . . , zm|x) \u2190\nP\ny eCLIP(y,z1,...,zm;x)/\u03c4\nP\ny\nP\nz1,...,zm eCLIP(y,z1,...,zm;x)/\u03c4 or\neCLIP(z1,...,zm;x)/\u03c4\nP\nz1,...,zm eCLIP(z1,...,zm;x)/\u03c4\nStep 2: infer the class\np(y|x, z1, . . . , zm) \u2190\neCLIP(y,z1,...,zm;x)\nP\ny eCLIP(y,z1,...,zm;x)\n\u02c6y \u2190 arg maxy p(y|x) = arg maxy\nP\nz1,...,zm p(y|x, z1, . . . , zm)\u02c6p(z1, . . . , zm|x).\nStep one: PerceptionCLIP estimates the distribution of contextual attributes given an image. Rather\nthan selecting the most probable attribute value, we estimate the entire distribution to accommodate\nCLIP\u2019s inherent uncertainty. In addition, we introduce a temperature hyperparameter \u03c4 to intervene\nin the estimation. A temperature \u03c4 greater than 1 smoothens CLIP\u2019s estimation, implying less trust\nin its predictions. The two-step nature also allows for other interventions, such as truncating top k\npredicted values (i.e., beam search), which we leave for future work.\nStep two: PerceptionCLIP first approximates the class distribution conditioning on each possible\ncontextual attributes\u2019 value. Then, it uses the estimated distribution of contextual attributes to\ncalculate the weighted sum of these class distributions, marginalizing out the contextual attributes.\nFinally, it selects the most probable class y as the predicted output.\nSimplifying into a single step. It can be seen from Algorithm 1 that setting the temperature to 1 and\nignoring constant terms yields \u02c6y \u2190 arg maxy\nP\nz1,...,zm eCLIP(y,z1,...,zm;x), essentially simplifying\nthe two-step algorithm into a single step. Intuitively, for each possible class, it sums the exponentiated\nCLIP scores calculated over each contextual attribute value, resulting in an aggregated score for the\nclass. Then, it selects the class with the highest aggregated score.\nSingle-step vs. prompt ensembling. This single-step approach, as a special case of our method,\ncoincides with the prompt ensembling method if we aggregate over some randomly selected attributes\n(as in 80 templates) instead of all contextual attribute combinations. This coincidence explains the\neffectiveness of prompt ensembling - it undergoes an implicit perception process. Nevertheless, our\nexperiments show that constructing diverse and systematic prompts using our contextual attribute\ncombinations is superior to ad-hoc template selections in prompt ensembling.\nTwo-step vs. single-step. The one-step method is simpler to implement but lacks two key features.\nIt disallows human intervention when inferring contextual attributes. Our experiments indicate that\nCLIP does not always infer contextual attributes correctly, whereas human intervention can leverage\nour prior knowledge to adjust its estimation. Second, the one-step method prevents us from knowing\nthe inferred contextual attributes, which could have improved the interpretability of the results.\nConstructing contextual attributes. The set of possible contextual attributes is at the core of\nPerceptionCLIP. We construct it with two approaches: 1) We manually construct essential attributes\nthat may be generative factors in the image generation process, especially those causing spurious\ncorrelations. This is particularly effective when we know of the dataset. For instance, for the CelebA\ndataset, we consider gender, age, and race as the attributes. 2) We leverage the in-context learning of\nlarge language models for semi-automated construction (shown in Appendix F.4).\n7\nPublished as a conference paper at ICLR 2024\nTable 4: Zero-shot classification accuracy on five datasets using ViT-B/16. The best result in each\ncolumn is highlighted in bold, while the next three highest values are underlined.\nAttributes\nImageNet\nImageNetV2\nImageNet-R\nImageNet-A\nImageNet-Sketch\nsingle template\n66.72\n60.85\n73.99\n47.80\n46.16\n80 templates\n68.32\n61.93\n77.71\n49.95\n48.26\nsingle attribute\nbackground\n67.98\n61.65\n75.87\n49.85\n47.08\nillumination\n67.47\n61.48\n75.37\n48.90\n46.67\norientation\n67.28\n61.11\n74.51\n48.47\n46.87\nquality\n68.18\n61.65\n76.23\n50.36\n47.40\nquantity\n67.64\n61.46\n75.37\n50.04\n46.59\nperspective\n67.90\n61.27\n75.00\n49.61\n46.84\nart\n67.53\n61.11\n77.16\n49.48\n47.96\nmedium\n67.58\n61.31\n76.67\n49.62\n47.37\ncondition\n68.39\n61.69\n75.74\n49.54\n47.41\ncolor-scheme\n66.89\n60.70\n74.47\n48.14\n47.03\ntool\n67.42\n61.02\n76.72\n48.88\n48.19\ncomposition of top 2 attributes\n68.52\n62.28\n77.78\n50.88\n48.46\ncomposition of top 3 attributes\n68.80\n62.22\n78.14\n51.15\n48.92\ncomposition of top 4 attributes\n68.71\n62.32\n78.38\n51.39\n49.10\nTable 5: Classification accuracy of ViT-B/16 on different data domains with PerceptionCLIP.\nCUB200\nEuroSAT\nPlaces365\nFlowers102\nFood101\nOxford Pets\nsimple template\n56.07\n51.44\n38.93\n67.73\n88.24\n88.25\ndomain template\n56.32\n54.94\n38.93\n70.99\n88.72\n89.04\n+ Z\n57.08\n59.23\n40.92\n72.86\n89.19\n90.38\n7\nEXPERIMENTS\n7.1\nZERO-SHOT GENERALIZATION\nSettings. We test PerceptionCLIP on ImageNet (Deng et al., 2009) and its out-of-distribution\ndatasets, including ImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a),\nImageNet-A (Hendrycks et al., 2021b), and ImageNet-Sketch (Wang et al., 2019). We also test\non different data domains (e.g., satellite images), including CUB200 (Wah et al., 2011), Eu-\nroSAT (Helber et al., 2019), Places365 (Zhou et al., 2017), Flowers102 (Nilsback & Zisserman, 2008),\nFood101 (Bossard et al., 2014), and Oxford Pets (Parkhi et al., 2012). For natural images, we compile\na set of possible contextual attributes. Each attribute has multiple possible values, and each value has\nmultiple possible descriptions with uniform possibilities to simulate the unknown distribution (details\nin Appendix E.4). For the dataset in a specific domain, we use domain-specific contextual attributes,\nfor example, image source for EuroSAT, cuisine for Food101, species for Oxford Pets. We use our\ntwo-step method (ClassAttr) with the temperature as a hyperparameter (details in Appendix E.4).\nUsing a single attribute. Table 4 shows that compared to using the simple template \"a photo of a\n{class name},\" considering almost any single contextual attribute improves the accuracy, some even\nsurpassing the use of 80 templates. We also observe that the most influential contextual attributes\nvary for different datasets, potentially attributable to different data generation processes. For example,\nall images in ImageNet-Sketch are sketches, making tool and art crucial contextual attributes for\nimage generation. This also indicates that PerceptionCLIP works the best when the considered\ncontextual attributes cover the generation process of the dataset.\nUsing multiple attributes. Table 4 also presents the results considering multiple contextual attributes.\nPerceptionCLIP, using the two most effective attributes, can already outperform prompt ensem-\nbling using 80 templates across all datasets. As the number of attributes considered increases, the\nclassification accuracy gradually improves. We also test our method on different domains of data in\nTable 5. The domain templates provided in Radford et al. (2021) already describe the domain in text\nprompt (e.g., \"a centered satellite photo of {class name}\") where the domain is a known contextual\nattribute. As expected, specifying it improves accuracy. PerceptionCLIP considers more contextual\nattributes and further improves zero-shot classification accuracy. For instance, by considering image\nsource and condition for the EuroSAT dataset, PerceptionCLIP achieves a near 5% gain in accuracy.\n8\nPublished as a conference paper at ICLR 2024\nAblation studies in Appendix E.4 demonstrate that substituting contextual attributes with random\nstrings markedly reduces performance, highlighting their critical role in our method\u2019s effectiveness.\nTable 6: Intervening in inferring contextual\nattributes improves zero-shot classification.\nWithout\nWith intervention\nintervention\nClassAtrr\nPureAttr\nImageNet\n68.59%\n68.70%\n68.72%\nImageNetV2\n62.10%\n62.31%\n62.32%\nImageNet-R\n78.12%\n78.38%\n78.27%\nImageNet-A\n51.17%\n51.39%\n51.22%\nImageNet-Sketch\n49.03%\n49.10%\n49.10%\nIntervening in attributes inference. In Table 6, we\nevaluate the effectiveness of the intervention. We\nset temperature \u03c4 = 3 and consider the top four\nattributes. Results show that intervening in infer-\nring contextual attributes achieves modest but consis-\ntent performance gains across datasets. In practice,\nwe find that setting the temperature to 3 or 5 usu-\nally yields better performance, which also confirms\nthat CLIP cannot perfectly infer contextual attributes.\nOne can also search for the best temperature with a validation set when applicable.\n7.2\nGROUP ROBUSTNESS\nGroup robustness is a critical measure of a model\u2019s bias. It measures the ability to perform consistently\nacross different subgroups within a dataset (Liu et al., 2021). We evaluate the group robustness of\nPerceptionCLIP through bird type classification on the Waterbirds dataset (Sagawa et al., 2020)\nand hair color classification on the CelebA (Liu et al., 2015) dataset. In both datasets, each image\nhas an underlying group attribute unknown to the model. These group attributes are background in\nWaterbirds and gender in CelebA. They both spuriously correlate with the class but do not causally\ndetermine the class. To evaluate the worst-group accuracy, we group images by their classes and\nattributes, then assess each group\u2019s accuracy following Sagawa et al. (2020). Table 7 and 8 show\nthat when the text prompts only describe the class and ignore contextual attributes (first row), such\nas \"a photo of a {landbird/waterbird}\" and \"a photo of a celebrity with {dark hair/blond hair},\"\nCLIP exhibits biased accuracy, with a significant discrepancy between average accuracy and the\nworst-group accuracy. This bias arises because CLIP overly relies on spurious features, such as\nassociating images with a water background to the waterbird class, instead of focusing on the bird. As\nshown in Figure 4, conditioning on group attributes such as background helps reduce CLIP\u2019s reliance\non spurious features, making the model less biased. Results in Table 7 and 8 also confirm that by\nconsidering background (with values in {on land, on water}) for Waterbird dataset, and gender (with\nvalues in {female, male}) for CelebA dataset, PerceptionCLIP reduces the accuracy gap in most\ncases. By incorporating more values (e.g., in forest) into the attribute background+, or considering\nmore contextual attributes like age and race, the group robustness can be further improved.\nTable 7: Average accuracy and worst group accuracy on the Waterbirds dataset.\nRN50\nViT-B/32\nViT-B/16\nViT-L/14\nAvg \u2191\nWorst \u2191\nGap\u2193\nAvg \u2191\nWorst \u2191\nGap \u2193\nAvg \u2191\nWorst \u2191\nGap \u2193\nAvg \u2191\nWorst \u2191\nGap \u2193\nwithout Z\n90.47\n16.07\n74.40\n87.34\n47.28\n40.06\n87.34\n26.79\n60.56\n90.55\n44.64\n45.91\nZ={background}\n88.78\n16.07\n72.71\n89.80\n66.07\n23.73\n82.98\n16.07\n66.91\n86.44\n44.94\n41.51\nZ={background+}\n90.32\n35.71\n54.61\n78.60\n60.33\n18.28\n85.80\n41.07\n44.73\n87.74\n61.12\n26.62\nTable 8: Average accuracy and worst group accuracy on the CelebA dataset.\nRN50\nViT-B/32\nViT-B/16\nViT-L/14\nAvg \u2191\nWorst \u2191\nGap\u2193\nAvg \u2191\nWorst \u2191\nGap \u2193\nAvg \u2191\nWorst \u2191\nGap \u2193\nAvg \u2191\nWorst \u2191\nGap \u2193\nwithout Z\n81.05\n73.87\n7.19\n80.73\n75.82\n4.91\n75.16\n62.01\n13.16\n86.98\n77.36\n9.61\nZ={gender}\n85.10\n80.44\n4.65\n79.89\n76.70\n3.19\n75.27\n65.13\n10.14\n80.30\n74.31\n5.99\nZ={gender, age}\n87.71\n84.98\n2.74\n82.82\n78.06\n4.76\n75.81\n65.52\n10.29\n82.26\n79.06\n3.21\nZ={gender, age, race}\n85.55\n82.51\n3.05\n82.02\n75.94\n6.09\n77.17\n69.18\n7.99\n83.04\n80.84\n2.20\n8\nCONCLUSION\nThis paper proposes PerceptionCLIP, a zero-shot classification method for CLIP that emulates\nhuman visual perception. By doing classification conditioned on self-inferred contextual attributes, it\nachieves improved generalization, less reliance on spurious features, and improved group robustness.\nOne limitation of our method is its sensitivity to text descriptions. Although using a distribution\nof descriptions alleviates this sensitivity, it is an intrinsic problem of CLIP itself. Future work may\novercome this limitation by using advanced vision-language models. Another future direction is\napplying this technique to pre-training and fine-tuning stages (see more in Appendix G).\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENT\nAn, Zhu, Panaitescu-Liess and Huang are supported by National Science Foundation NSF-IIS-\n2147276 FAI, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-\nAFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, DOD-\nDARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception\n(GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships.\nREFERENCES\nSandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, and Miles\nBrundage. Evaluating clip: towards characterization of broader capabilities and downstream\nimplications. arXiv preprint arXiv:2108.02818, 2021.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative compo-\nnents with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part VI 13, pp. 446\u2013461. Springer, 2014.\nChing-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka. Debiasing\nvision-language models via biased prompts. arXiv preprint arXiv:2302.00070, 2023.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. (arXiv:2305.06500), Jun 2023. URL http://arxiv.org/\nabs/2305.06500. arXiv:2305.06500 [cs].\nTimoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need\nregisters. arXiv preprint arXiv:2309.16588, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009.\nMohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor Guilherme Turrisi da Costa,\nCees G. M. Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning for\nimage-language model generalization, 2023.\nLijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training\nwith language rewrites. arXiv preprint arXiv:2305.20088, 2023.\nZhili Feng, Anna Bair, and J. Zico Kolter. Leveraging multiple descriptive features for robust few-shot\nimage learning. ArXiv, abs/2307.04317, 2023. URL https://api.semanticscholar.\norg/CorpusID:259501400.\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset\nand deep learning benchmark for land use and land cover classification. IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.\nDan Hendrycks and Thomas G Dietterich. Benchmarking neural network robustness to common\ncorruptions and surface variations. arXiv preprint arXiv:1807.01697, 2018.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 8340\u20138349, 2021a.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-\nsarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 15262\u201315271, 2021b.\nTony Huang, Jack Chu, and Fangyun Wei. Unsupervised prompt learning for vision-language models.\narXiv preprint arXiv:2204.03649, 2022.\n10\nPublished as a conference paper at ICLR 2024\nNeel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli,\nBrian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neftune: Noisy\nembeddings improve instruction finetuning. arXiv preprint arXiv:2310.05914, 2023.\nEric R Kandel, James H Schwartz, Thomas M Jessell, Steven Siegelbaum, A James Hudspeth, Sarah\nMack, et al. Principles of neural science, Fifth Edition, volume 4. McGraw-hill New York, 2013.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick.\nSegment anything. (arXiv:2304.02643), Apr 2023. doi: 10.48550/arXiv.2304.02643. URL\nhttp://arxiv.org/abs/2304.02643. arXiv:2304.02643 [cs].\nHugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In AAAI,\nvolume 1, pp. 3, 2008.\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based\nlearning. Predicting structured data, 1(0), 2006.\nHanting Li, Hongjing Niu, Zhaoqing Zhu, and Feng Zhao. Cliper: A unified vision-language\nframework for in-the-wild facial expression recognition. ArXiv, abs/2303.00193, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. (arXiv:2301.12597), Jun\n2023b. URL http://arxiv.org/abs/2301.12597. arXiv:2301.12597 [cs].\nLiunian Harold Li, Zi-Yi Dou, Nanyun Peng, and Kai-Wei Chang. Desco: Learning object recognition\nwith rich language descriptions. (arXiv:2306.14060), Jun 2023c. doi: 10.48550/arXiv.2306.14060.\nURL http://arxiv.org/abs/2306.14060. arXiv:2306.14060 [cs].\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,\nPercy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training\ngroup information. In International Conference on Machine Learning, pp. 6781\u20136792. PMLR,\n2021.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tun-\ning.\n(arXiv:2304.08485), Apr 2023.\nURL http://arxiv.org/abs/2304.08485.\narXiv:2304.08485 [cs].\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), December 2015.\nChengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Eric Wang, and\nCarl Vondrick. Doubly right object recognition: A why prompt for visual rationales. ArXiv,\nabs/2212.06202, 2022.\nCristina Menghini, Andrew Delworth, and Stephen H. Bach. Enhancing clip with clip: Exploring\npseudolabeling for limited-label prompt tuning. (arXiv:2306.01669), Jun 2023. doi: 10.48550/\narXiv.2306.01669. URL http://arxiv.org/abs/2306.01669. arXiv:2306.01669 [cs].\nSachit Menon and Carl Vondrick. Visual classification via description from large language models.\narXiv preprint arXiv:2210.07183, 2022.\nM. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Mateusz Kozinski, Horst Possegger, Rogerio Feris,\nand Horst Bischof. Lafter: Label-free tuning of zero-shot classifier using language and unlabeled\nimage collections. (arXiv:2305.18287), May 2023. URL http://arxiv.org/abs/2305.\n18287. arXiv:2305.18287 [cs].\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number\nof classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pp.\n722\u2013729. IEEE, 2008.\nZachary Novack, S. Garg, Julian McAuley, and Zachary Chase Lipton. Chils: Zero-shot image\nclassification with hierarchical label sets. ArXiv, abs/2302.02551, 2023.\n11\nPublished as a conference paper at ICLR 2024\nOpenAI. Gpt-4 technical report, 2023.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012\nIEEE conference on computer vision and pattern recognition, pp. 3498\u20133505. IEEE, 2012.\nJudea Pearl. Causal inference in statistics: An overview. 2009.\nSarah Pratt, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized\nprompts for zero-shot image classification. arXiv preprint arXiv:2209.03320, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nVikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B. Adcock, Laurens van der Maaten,\nDeepti Ghadiyaram, and Olga Russakovsky. Geode: a geographically diverse evaluation dataset\nfor object recognition. (arXiv:2301.02560), Apr 2023. doi: 10.48550/arXiv.2301.02560. URL\nhttp://arxiv.org/abs/2301.02560. arXiv:2301.02560 [cs].\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. (arXiv:2204.06125), Apr 2022. doi: 10.48550/\narXiv.2204.06125. URL http://arxiv.org/abs/2204.06125. arXiv:2204.06125 [cs].\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers\ngeneralize to imagenet? In International conference on machine learning, pp. 5389\u20135400. PMLR,\n2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models.\npp. 10684\u201310695, 2022.\nURL https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_\nHigh-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_\nCVPR_2022_paper.html.\nKarsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep\nAkata. Waffling around for performance: Visual classification with random words and broad\nconcepts, 2023.\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust\nneural networks. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=ryxGuJrFvS.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-\nization. In Proceedings of the IEEE international conference on computer vision, pp. 618\u2013626,\n2017.\nManli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei\nXiao. Test-time prompt tuning for zero-shot generalization in vision-language models. arXiv\npreprint arXiv:2209.07511, 2022.\nVishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of\nvision-language models. (arXiv:2211.16198), Jul 2023. doi: 10.48550/arXiv.2211.16198. URL\nhttp://arxiv.org/abs/2211.16198. arXiv:2211.16198 [cs].\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011.\n12\nPublished as a conference paper at ICLR 2024\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations\nby penalizing local predictive power. Advances in Neural Information Processing Systems, 32,\n2019.\nZifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from\nunpaired medical images and text. ArXiv, abs/2210.10163, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nYu Yang, Besmira Nushi, Hamid Palangi, and Baharan Mirzasoleiman. Mitigating spurious correla-\ntions in multi-modal models during fine-tuning. ArXiv, abs/2304.03916, 2023.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and\nwhy vision-language models behave like bags-of-words, and what to do about it? In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=KRLUvxh8uaX.\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10\nmillion image database for scene recognition. IEEE transactions on pattern analysis and machine\nintelligence, 40(6):1452\u20131464, 2017.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for\nvision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 16816\u201316825, 2022a.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\nlanguage models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022b.\nBeier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for\nprompt tuning. arXiv preprint arXiv:2205.14865, 2022.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. (arXiv:2304.10592), Apr\n2023. URL http://arxiv.org/abs/2304.10592. arXiv:2304.10592 [cs].\n13\nPublished as a conference paper at ICLR 2024\nAPPENDIX\nA\nEXTENDED RELATED WORK\nDescriptive prompts with external knowledge. Due to CLIP\u2019s ability to understand finer-grained\nvisual concepts beyond classes (e.g., body parts and components), some work leverages external\nknowledge to augment prompts with additional visual concepts to improve CLIP\u2019s zero-shot classifi-\ncation. For example, Menon & Vondrick (2022); Pratt et al. (2022); Mao et al. (2022); Feng et al.\n(2023) use large language models (LLMs) like GPT-3 to generate class-specific descriptions for each\nclass and incorporate them into prompts, resulting in prompts like \"a photo of a hen, which has\ntwo legs\". Novack et al. (2023) use class hierarchies (existing or by querying GPT-3) to generate\nsub-classes for each parent class and aggregate model predictions on all sub-classes to get a final\nprediction. Udandarao et al. (2023) use class names to retrieve and maintain some auxiliary data\nto help downstream classification. In contrast, our method addresses class-independent attributes\n(i.e., contextual attributes) such as background and orientation, whose comprehension by CLIP is not\nwell-known. These attributes are also combinatorial, potentially covering more aspects of an image\nthan class-specific attributes. Moreover, we can still leverage contextual attributes (e.g., gender, age)\nwhen class-specific attributes are hard to articulate, as in the hair-color classification tasks on CelebA.\nWe also find that specifying spurious contextual attributes reduces distractions from their spurious\ncorrelations.\nDoes CLIP truly understand descriptive prompts? Some work investigates a seemingly obvious\nquestion: do these descriptive prompts play a role in CLIP\u2019s prediction? Roth et al. (2023) show that\nreplacing class-specific descriptions in prior work with random words or even meaningless characters\ncan achieve similar performance, resembling the effect of noise augmentation or randomized smooth-\ning. Li et al. (2023c) find that GLIP (a similar VLM as CLIP), often disregards contextual information\nin the prompts and relies heavily on class names in object detection. Addressing these findings, we\nablate our method and show that random attributes or meaningless characters yield approximately\nhalf the benefit compared to using correct or self-inferred attributes, indicating that our method\u2019s\neffectiveness stems from the proper use of contextual attributes instead of noise augmentation. Roth\net al. (2023) also show that appending high-level class-independent descriptions (e.g., \"food\" for\nFood101, \"place\" for Places365) to prompts helps classification, which aligns with our findings.\nPrompt tuning. Another line of work that modifies prompts to improve CLIP\u2019s classification is\nprompt tuning, which optimizes the prefix characters of the prompts. Typical prompt tuning methods\nrequire labeled (Zhou et al., 2022b;a; Zhu et al., 2022; Derakhshani et al., 2023) or unlabeled\ndownstream data (Huang et al., 2022; Mirza et al., 2023; Menghini et al., 2023), making them\nfall outside our scope of zero-shot (data-free) classification. They are also prone to overfitting the\ntraining dataset, whereas our method relies on general image attributes (e.g, illumination) shared by\ncommon datasets. On the other hand, Shu et al. (2022) use test-time prompt tuning that applies to\nzero-shot classification. Specifically, they generate multiple views for each test image and optimize\nthe prompt to minimize the entropy of the model\u2019s prediction on these views. This method introduces\nseveral hyperparameters that require tuning on a labeled proxy validation set. In contrast, our\nmethod, depending on implementation, introduces either no additional hyperparameters or only one\n(temperature). Furthermore, our method is training-free and can work in the black-box setting.\nReasoning and chain-of-thoughts. The inference process of our method resembles the reasoning\nor chain-of-thoughts in prompting LLMs (Wei et al., 2022; Yao et al., 2023), where the model is\nprompted to give some intermediate step results and then conditioning on them to give final results.\nHowever, CLIP itself cannot do step-wise reasoning out of the box, so our method manually prompts\nit through the reasoning process.\nB\nIMAGE CAPTION EXAMPLES\nIn the pertaining stage, the human-written caption for each image typically describes the visual object,\nencompassing its class and a few contextual attributes. We show some caption examples in Table 9,\nchosen from a similar dataset LAION-400M (Schuhmann et al., 2021), since the original pretraining\ndataset of CLIP is not made public. We can see that those captions not only describe class but also\ncontextual attributes like color, style, and background.\n14\nPublished as a conference paper at ICLR 2024\nTable 9: Image caption examples from LAION-400M (comparable to CLIP\u2019s pretraining dataset).\nCaption #1\nMen\u2019s Classics Round Bracelets Watch in Grey\nCaption #2\nstock photo of gremlins - 3 d cartoon cute green gremlin monster - JPG\nCaption #3\nMedium Size of Chair: Fabulous Mid Century Modern Chair Adalyn Accent In Red:\nC\nHUMAN VISUAL PERCEPTION\nThe classic neuroscience textbook Kandel et al. (2013) offers a modern view of human visual\nperception, presenting a significant difference from current zero-shot classification methods:\n\"The brain analyzes a visual scene at three levels: low, intermediate, and high. At\nthe lowest level, visual attributes such as local contrast, orientation, color, and\nmovement are discriminated. The intermediate level involves analysis of the layout\nof scenes and of surface properties, parsing the visual image into surfaces and\nglobal contours, and distinguishing foreground from background. The highest level\ninvolves object recognition.\"\n\"... the perceptual interpretation we make of any visual object depends not just on\nthe properties of the stimulus but also on its context, on other features in the visual\nfield.\"\nThis perception process is hierarchical, cascaded, and context-dependent, differing from current\nzero-shot classification methods, which overlook contextual attributes. In this paper, we propose\nPerceptionCLIP to mimic human perception process.\nD\nAPPROXIMATING CONDITIONAL PROBABILITIES\nWith the energy-function-like CLIP score\nCLIP(y, z1, . . . , zm; x)\n\u225c\n\u001c\n\u03d5I(x),\nE \u03d5T\n\u0000\u03b1(y) \u2295 \u03b1(z1) \u2295 \u00b7 \u00b7 \u00b7 \u2295 \u03b1(zm)\n\u0001\n\u2225E \u03d5T\n\u0000\u03b1(y) \u2295 \u03b1(z1) \u2295 \u00b7 \u00b7 \u00b7 \u2295 \u03b1(zm)\n\u0001\n\u2225\n\u001d\n,\n(6)\nwe first approximate the joint conditional probability p(y, z1, . . . , zm|x). It measures the likelihood\nof an object class and some contextual attributes occurring together given the image as\np(y, z1, . . . , zm|x) \u225c\neCLIP(y,z1,...,zm;x)\nP\ny\nP\nz eCLIP(y,z1,...,zm;x)\n(7)\nwhich is essentially the normalization of the exponential of the CLIP score. Then, we derive the\nconditional probability p(y|z1, . . . , zm, x), which measures the probability of an object class given\nboth the image and the contextual attributes as\np(y|x, z1, . . . , zm) = p(y, z1, . . . , zm|x)\np(z1, . . . , zm|x)\n(8)\n=\np(y, z1, . . . , zm|x)\nP\ny p(y, z1, . . . , zm|x)\n(9)\n=\neCLIP(y,z1,...,zm;x)\nP\ny eCLIP(y,z1,...,zm;x)\n(10)\nusing the definition of joint probability and the rules of conditional probability. Next, we approximate\nthe conditional probability p(z1, . . . , zm|x), which measures the likelihood of some contextual\nattributes given the image as\np(z1, . . . , zm|x) =\nX\ny\np(y, z1, . . . , zm|x)\n(11)\n=\nP\ny eCLIP(y,z1,...,zm;x)\nP\nz\nP\ny eCLIP(y,z1,...,zm;x).\n(12)\n15\nPublished as a conference paper at ICLR 2024\nIt sums up the probabilities of contextual attributes appearing in each class to give a total probability\nof them appearing in the image. We named this method ClassAttr. Another simplified way, named\nPureAttr, ignores the classes and use\np(z1, . . . , zm|x) \u2248\neCLIP(z1,...,zm;x)\nP\nz eCLIP(z1,...,zm;x)\n(13)\nto do the estimation. Here, we only consider the contextual attributes in the CLIP score with\ndescriptions like \"a photo of an object, {description of z}\" where we use a word like \"object\" instead\nof a particular class, making the CLIP score class-agnostic. In our experiments, the first version\noccasionally outperformed the second, although the performance of the two is generally similar.\nE\nEXPERIMENTAL DETAILS\nTable 10: Summary of descriptions for different attributes used in Figure 3, Table 2 and Table 3.\nz\u2217 denotes the correct value of the contextual attribute, and zwrong denotes the wrong value of the\ncontextual attribute. Ideally, each attribute has a distribution of text descriptions. Here, we use three\ndescriptions and use the averaged text embeddings of them to calculate the CLIP score.\nAttribute\n\u03b1(y) \u2295 \u03b1(z\u2217)\n\u03b1(y) \u2295 \u03b1(zwrong)\nvertical flip\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, upside-down.\"\n\"a photo of a {y}, upright.\"\n\"a photo of a {y}, the photo is upside-down.\"\n\"a photo of a {y}, the photo is upright.\"\n90\u00b0 rotation\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, rotated.\"\n\"a photo of a {y}, upright.\"\n\"a photo of a {y}, the photo is rotated.\"\n\"a photo of a {y}, the photo is upright.\"\nelastic-transform\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, with distortion.\"\n\"a photo of a {y}, normal.\"\n\"a photo of a {y}, the photo is distorted.\"\n\"a photo of a {y}, the photo is normal.\"\ncolor-invert\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, color-inverted.\"\n\"a photo of a {y}, normal.\"\n\"a photo of a {y}, the photo is color-inverted.\"\n\"a photo of a {y}, the photo is normal.\"\nsolarize\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, solarized.\"\n\"a photo of a {y}, normal.\"\n\"a photo of a {y}, the photo is solarized.\"\n\"a photo of a {y}, the photo is normal.\"\nblur\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, blurred.\"\n\"a photo of a {y}, clear.\"\n\"a photo of a {y}, the photo is blurred.\"\n\"a photo of a {y}, the photo is clear.\"\ngrayscale\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, grayscale.\"\n\"a photo of a {y}, colorful.\"\n\"a photo of a {y}, the photo is in black and white.\"\n\"a photo of a {y}, the photo is colorful.\"\nbright\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, bright.\"\n\"a photo of a {y}, dark.\"\n\"a photo of a {y}, the photo is bright.\"\n\"a photo of a {y}, the photo is dark.\"\nnoise\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, with noise.\"\n\"a photo of a {y}, clear.\"\n\"a photo of a {y}, the photo has noise.\"\n\"a photo of a {y}, the photo is clear.\"\nsnow\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, in the snow.\"\n\"a photo of a {y}, clear.\"\n\"a photo of a {y}, the photo is in the snow.\"\n\"a photo of a {y}, the photo is clear.\"\nfrost\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, in the frost.\"\n\"a photo of a {y}, clear.\"\n\"a photo of a {y}, the photo is in the frost.\"\n\"a photo of a {y}, the photo is clear.\"\nfog\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, in the fog.\"\n\"a photo of a {y}, clear.\"\n\"a photo of a {y}, the photo is in the fog.\"\n\"a photo of a {y}, the photo is clear.\"\njpeg\n\"a photo of a {y}.\"\n\"a photo of a {y}.\"\n\"a photo of a {y}, in jpeg format.\"\n\"a photo of a {y}, in high resolution.\"\n\"a photo of a {y}, the photo is in jpeg format.\"\n\"a photo of a {y}, the photo is in high resolution.\"\nE.1\nDETAILS ON THE EVALUATION IN FIGURE 3 AND TABLE 2\nWe do evaluation on the ImageNet dataset. Due to the lack of annotated contextual attributes, we\nconsider some easily observable and adjustable attributes, including image orientation, illumination,\netc. We first examine and confirm that most ImageNet images share the same attribute values,\nincluding upright orientation, natural illumination, and standard image quality. However, these\ndefault values are too trivial, making their textual descriptions unlikely to appear in the captions of the\n16\nPublished as a conference paper at ICLR 2024\nTable 11: Similarity score and classification accuracy on ImageNet test set. We apply a composition\nof two transformation functions on images, and use the composition of attributes\u2019 descriptions for\ntext.\nAttributes\nSimilarity (CLIP score \u00d7 100)\nw/o z\nw/ random z\nw/ wrong z\nw/ correct z\nvertical flip + color-invert\n25.39\n28.23 (\u21912.84)\n26.28 (\u21910.88)\n30.26 (\u21914.86)\ngrayscale + elastic-transform\n26.66\n30.55 (\u21913.89)\n26.48 (\u21930.19)\n32.15 (\u21915.49)\nAttributes\nAccuracy (%)\nw/o z\nw/ random z\nw/ wrong z\nw/ correct z\nvertical flip + color-invert\n19.44\n20.88 (\u21911.44)\n20.01 (\u21910.57)\n21.32 (\u21911.88)\ngrayscale + elastic-transform\n29.79\n30.49 (\u21910.70)\n30.14 (\u21910.35)\n30.59 (\u21910.80)\npretraining data. Therefore, we then alter these attribute values through certain image transformations\n(e.g., vertical flipping), thus making the new attribute values have non-trivial descriptions. These new\nattribute values become part of the modified images\u2019 data generation process, for which we have\nground-truth annotations.\nContextual attributes and their descriptions.\nWe separately apply thirteen image transformation\nfunctions (e.g., vertical flip) to all the ImageNet test images. Note that the last five transformations\n(i.e., noise, snow, frost, fog, jpeg) are tested directly on the ImageNet-C dataset (Hendrycks &\nDietterich, 2018), which contains the same images as in the ImageNet test set while images are\ncorrupted with certain transformations. We use relatively strong strengths in those transformations,\nensuring nonnegligible generative factors. This is also why CLIP has degraded performance on these\ncorrupted data. Table 10 shows the descriptions we used in this evaluation. When we ignore the\ncontextual attribute, we use a simple template, \"a photo of a {class name}.\". When considering the\ncontextual attribute, we test the cases using the correct attribute value (e.g., upside-down) with \"a\nphoto of a {class name}, upside-down.\" and the wrong attribute value (e.g., upright) with \"a photo of a\n{class name}, upright.\", respectively. For each contextual value, we use three descriptions to simulate\nthe distribution of descriptions and average their embeddings as the text embedding to calculate the\nCLIP score.\nRandomized descriptions.\nTo validate the effectiveness of contextual attributes, we also compare\nwith the cases where we use random descriptions as proposed in Roth et al. (2023). We replace\nevery word in \u03b1(z\u2217) with random characters while keeping the word length unchanged. For example,\n\u03b1(y) \u2295 \u03b1(zrandom) of vertical flip contains three descriptions: \"a photo of a {y}.\", \"a photo of a {y},\niaYo5n0Dli7.\", \"a photo of a {y}, 8g2, Me5tx, q1, 6Ud2ole94Ir.\"\nAdditional results on the composition of contextual attributes.\nWe perform the same evaluation\nwhile considering more than one contextual attribute. Table 11 shows the results. We draw the same\nconclusion that correct contextual attribute values lead to better image-text alignment and higher\nclassification accuracy.\nAblation on the increased similarities.\nThe CLIP score measures the similarity between the\nimage and the text prompt. In Figure 3, we observe that correctly matched image and attribute pairs\nprovide higher CLIP scores for the ground-truth class than mismatched pairs using wrong or random\ncontextual attributes. It indicates that incorporating correct contextual attributes yields the greatest\nbenefit for the ground-truth class. Additionally, we delve into the impact of including these correct\nattributes on both the correct and incorrect classes. We calculate the increase in CLIP scores for class\ny with \u2206CLIP(y) \u225c CLIP(y, z\u2217; x) \u2212 CLIP1(y; x), and the increase in prediction probabilities for\nclass y with \u2206p(y) \u225c p(y|x, z\u2217) \u2212 p(y|x).\nIn Figure 5 (left), we compare the \u2206CLIP(y\u2217) and \u2206CLIP(ywrong), where the latter one is the\naverage increase of the Top-K wrong classes. As expected, incorporating ground-truth attributes\ninto text prompts results in increased scores for both correct and incorrect classes, and the correct\nclass benefits more from this enhancement, as the accurate description of the class and the attribute,\nachieves a better alignment with the corresponding image. Figure 3 and 5 together validate that the\n17\nPublished as a conference paper at ICLR 2024\nFigure 5: The increase in (left) CLIP scores and the (right) prediction probabilities by incorporating\nthe descriptions of the correct contextual attribute into the text prompts. We compare the increased\nCLIP scores and prediction probabilities for the ground-truth class y\u2217, the Top-5 and Top-10 wrong\nclasses. (left) Incorporating ground-truth attributes into text prompts results in increased CLIP scores\nfor both correct and incorrect classes. This improvement is attributed to the enhanced alignment of\nthe text prompts with the images, addressing previously overlooked contextual attributes. Notably, the\nCLIP score of the correct class benefits more from this enhancement for all the attributes considered.\nThis is because the accurate description of the class, combined with the contextual attributes, achieves\na more precise alignment with the corresponding image. (right) Therefore, the model is more likely\nto predict the correct class after being provided with the correct context description in the prompt.\nCLIP model understands the contextual attributes, and describing correct class and attributes yields\nhigher similarity scores as described in Equation 4.\nIn Figure 5 (right), we further compare the increase in the prediction probabilities for \u2206p(y) and\n\u2206p(ywrong) where the probability is calculated by applying softmax on CLIP scores and is used for\nthe final classification. By incorporating correct context, the prediction probability of the correct class\nincreased significantly, while the wrong classes got unchanged or even decreased probabilities. The\npredicted probability for the correct class increases by an average of 1.5%. The predicted probabilities\nfor top-5 and top-10 wrong classes decrease by an average of 0.07% and 0.05%. Such findings also\nexplain the increased accuracy in Table 2 when incorporating the correct contextual attributes.\nE.2\nDETAILS ON THE EVALUATION IN TABLE 3\nIn Table 3, we test whether CLIP can infer underlying contextual attributes by itself. In this experiment,\nwe only apply transformations to half of the images from the ImageNet test set and use descriptions\nshown in Table 10. The task is to predict the correct contextual attribute\u2019s value, which is a binary\nclassification task. For example, half images are upright, and half images are upside-down, and\nthe goal is to classify the orientation of the images by CLIP. We evaluate two methods with two\napproximations of p(z|x) in Table 1. Note that we do not intervene in the inference in this experiment.\nE.3\nDETAILS ON THE VISUALIZATION\nWe consider some spatially separable spurious attributes (also known as spurious features), such\nas backgrounds, and annotate core regions and spurious regions with the help of Segment Anything\n(Kirillov et al., 2023). Then, we use Grad-CAM to generate a heatmap for salient pixels. first, we\nuse a function that computes CLIP\u2019s similarity score for each class CLIP(y, z\u2217; x) and apply softmax\non top of these values. Then, we only consider the scalar value corresponding to the ground-truth\nclass which is essentially the conditional probability p(y\u2217|x, z\u2217). We compute the gradients using the\nlayer before the final attention block of ViT-L/14 as suggested in a popular explainability library.1\nIntuitively, the regions where the salient pixels are located are the regions the model pays attention\n1https://github.com/jacobgil/pytorch-grad-cam\n18\nPublished as a conference paper at ICLR 2024\nTable 13: Summary of contextual attributes and their value descriptions used in ImageNet-related\ndatasets.\nAttributes\nValues\norientation\nupright, upside-down, rotated\nbackground\nothers, natural, urban, indoor\nquality\nnormal, good, bad, low res, pixelated, jpeg corrupted, blurry, clean, dirty\nillumination\nnormal, bright, dark\nquantity\nothers, many, one, large, small\nperspective\nnormal, close up, cropped, obscured\nart\nnon-art, others, sculpture, rendering, graffiti, tattoo, embroidery, paper art, sketch, cartoon\nmedium\nothers, video game, plastic, toy\ncondition\nnormal, cool, nice, weird\ncolor-scheme\nnormal, black and white\ntool\nothers, pencil, pen, digital tool\nto when making predictions, and we hope that the model focuses as much as possible on regions of\ncore features (i.e., features with causal correlation to classes). Note that, adding a spurious attribute\u2019s\ndescription in this evaluation won\u2019t really make the model look at it when classifying because all the\ndescriptions (for all classes) will contain that attribute.\nTo compute the ratio between the usage of core and spurious regions in prediction, we: (1) run\nSegment Anything (Kirillov et al., 2023) and select a segmentation mask for the core region of each\nimage (e.g., the bird or the leopard), then consider the rest (non-core) regions as spurious; (2) use the\nmasks to identify the core and spurious pixel values from the Grad-CAMs and compute the mean\npixel value for both of these regions; (3) normalize the numbers and show them as two percentages in\na bar plot for each Grad-CAM.\nTable 12: The average saliency (%) of\nthe core feature and the spurious feature\nevaluated on the Waterbirds test set.\nCore (\u2191)\nSpurious (\u2193)\nno context\n62.8\n37.2\nwrong context\n62.6\n37.4\nrandom context\n62.3\n37.7\ncorrect context\n66.3\n33.7\nIn Table 12, we quantitatively evaluate the model reliance\non core feature and spurious feature. We use ViT-B/32\nas the image encoder and evaluate it on the Waterbirds\ntest set. The dataset contains images of land birds and\nwater birds on land or on the water. We use the same\nmethod introduced above to calculate the ratio of core\nversus spurious through Grad-CAM. We use \"on land\"\nand \"on water\" to describe the context (e.g., background).\nWe compare the correct context with no context, wrong\ncontext, and random context where the random context is\nthe random string that replaces the correct context while keeping the description length unchanged.\nResults in Table 12 also indicate that, by incorporating the correct context, the model relies more on\nthe core feature when doing the classification.\nE.4\nDETAILS ON THE EXPERIMENTS IN SECTION 7\nIn Table 4, we test PerceptionCLIP on ImageNet and its OOD datasets. We first use GPT-4 to\nsummarize the contextual attributes involved in the 80 hand-crafted templates (Radford et al., 2021),\nthen add three contextual attributes (orientation, background, tool) to the testing bed. Table 13 shows\nthe values of every contextual attribute. We use multiple descriptions to describe every attribute value,\nand use their average text embedding of the full sentence in the implementation. When considering\na single attribute, we use a main template, \"a photo of a {class name}\" and concatenate it with the\ndescription of each attribute value. When considering the composition of attributes, we generate\ncombinations from the values across all attributes. Such simple concatenation of descriptions works\nwell, probably because the pre-trained CLIP model behaves like a bag-of-words (Yuksekgonul et al.,\n2023). Future works could explore better ways of composing text prompts.\nTable 14 and 15 list the contextual attributes used in Table 5, 7 and 8. Attributes and their values\nare manually designed based on our priors of datasets. Experiments on Waterbirds and CelebA are\nconducted on their training set.\n19\nPublished as a conference paper at ICLR 2024\nTable 14: Datasets, domain templates and contextual attributes used in Table 5\nDataset\nDomain Template\nAttributes\nCUB200\n\"a photo of a {y}, a type of bird\"\nsize, background, condition\nEuroSAT\n\"a centered satellite photo of {y}\"\ncondition, source\nPlaces365\n\"a photo of a {y}\"\nbackground, quality, condition\nFlowers102\n\"a photo of a {y}, a type of flower\"\nbackground, illumination, quality, condition\nFood101\n\"a photo of a {y}, a type of food\"\ncuisines, condition\nOxford Pets\n\"a photo of a {y}, a type of pet\"\nspecies, background, pose, interaction\nTable 15: Domain templates, contextual attributes and their descriptions used in Table 7 and Table 8\nDataset\nDomain Template\nAttributes\nValues\nWaterbirds\n\"a photo of a {y}\"\nbackground\non land, on water\nbackground+\n+ in forest, in sky, on street, on grass, on tree,\nwith flowers, on beach, with human, on a branch\nCelebA\n\"a photo of a celebrity with {y}\"\ngender\nfemale, male\nage\nyoung, old\nrace\nwhite skin, dark skin, asian\nAll the above experiments use ClassAttr version of PerceptionCLIP and the intervention by setting\na temperature \u03c4 in the first step (i.e., inferring contextual attributes). We found that mildly smoothing\nthe estimation by setting \u03c4 to be 3 or 5 usually has the best performance. When we do not have a\ngood prior of the temperature, just setting it to 1 can also have relatively good results. The reported\nnumbers in our experiments use a temperature selected from {1,3,5,10} that performs the best on the\nparticular dataset.\nAblation studies. In Table 4 and 5, by incorporating contextual attributes, PerceptionCLIP\nimproves the zero-shot classification accuracy in all cases. Adding descriptions of contextual\nattributes to text prompts has two effects: 1) it introduces more tokens to the text prompt, 2) and\nthe tokens describe the contextual attributes. To figure out which effect causes the improvement, we\nconduct ablation studies by replacing the descriptions of contextual attributes with the same-length\nrandom strings. In Table 16, we do ablation studies on the best attribute composition for all datasets.\nIn Table 17, we keep the domain template but randomize other contextual attributes. For every dataset,\nwe run 5 times with random seeds and report the mean and variance. Results show that adding more\ntokens can improve the accuracy marginally in most cases, but can also decrease the accuracy as in\nthe case of EuroSAT and Oxford Pets. The improvement brought by adding random tokens might\nbe a result of augmentation (Jain et al., 2023) or register (Darcet et al., 2023). More importantly,\nthere is a significant performance gap between using the random strings and the descriptions of\ncontextual attributes, suggesting that the improvement provided by our method primarily stems from\nthe incorporation of contextual attributes.\nTable 16: Ablation study on ImageNet and related datasets.\nAttributes\nImageNet\nImageNetV2\nImageNet-R\nImageNet-A\nImageNet-Sketch\nCLIP\n66.72\n60.85\n73.99\n47.80\n46.16\nPerceptionCLIP\n68.80\n62.32\n78.38\n51.39\n49.10\nablation w/ random\n67.59 \u00b1 0.27\n61.27 \u00b1 0.11\n75.53 \u00b1 0.28\n49.74 \u00b1 0.37\n47.63 \u00b1 0.23\nF\nADDITIONAL RESULTS AND ANALYSIS\nF.1\nCOMPUTATIONAL COMPLEXITY.\nSimilar to the implementation of prompt ensembling, we pre-compute the embeddings of all class and\ncontextual attribute combinations, and then use these pre-computed embeddings in each inference\nprocess. Since we use the average of text embeddings when there are multiple descriptions for\n20\nPublished as a conference paper at ICLR 2024\nTable 17: Ablation study on different data domains.\nCUB200\nEuroSAT\nPlaces365\nFlowers102\nFood101\nOxford Pets\ndomain template\n56.32\n54.94\n38.93\n70.99\n88.72\n89.04\n+ Z\n57.08\n59.23\n40.92\n72.86\n89.19\n90.38\n+ random\n56.68 \u00b1 0.17\n53.58 \u00b1 2.34\n39.98 \u00b1 0.37\n71.41 \u00b1 0.45\n88.89 \u00b1 0.08\n88.35 \u00b1 0.45\nTable 18: Performance of PerceptionCLIP using two order types in the attribute concatenation.\nOrder\nImageNet\nImageNetV2\nImageNet-R\nImageNet-A\nImageNet-Sketch\nforward\n68.71\n62.32\n78.25\n51.39\n48.97\nbackward\n68.71\n62.15\n78.38\n51.21\n49.10\none value, our method needs multiple forward passes to get the text embeddings, causing a longer\npreparation time. Since these computations are one-time, the time complexity during inference is\nunaffected by the number of contextual attributes. Compared to the basic method, which stores\nO(|Y|) embedding vectors, this implementation needs to store O(|Y|\u00d7|Z1|\u00d7\u00b7 \u00b7 \u00b7\u00d7|Zm|) embedding\nvectors. The space complexity limits the number of contextual attributes considered in practice.\nWe will consider using beam search to only reserve top-k attributes to reduce the space storage\nrequirement in future work.\nF.2\nAN ANALYSIS OF THE ORDER IN ATTRIBUTE COMBINATION\nWhen considering multiple contextual attributes, we concatenate their textual descriptions. An\ninteresting question is whether their order in the text affects the performance. In Table 18, we\ntest two order types when combining the top four attributes in the ImageNet experiments. The\nforward direction ordering attributes from the most powerful to the last. The backward direction\ndoes the opposite ordering. Unfortunately, we do not observe a good way of ordering consistently\noutperforming others. We suspect that it is due to CLIP\u2019s sensitivity to the text, and the direct\nconcatenation may not be the best way of combining attributes to approximate the distributions of\ncaptions in the pertaining stage.\nF.3\nMORE VISUALIZATIONS\nWe show more visualizations in Figure 6 and 7. Figure 6 shows images from the ImageNet dataset\nwith the ground-truth class leopard. Figure 7 shows images from the Waterbirds dataset with the\nground-truth class waterbird. Grad-CAMs show that CLIP relies more on core features when\nconditioned on the correct contextual attributes (e.g., background) for classification. The reliance on\ncore features also improves model interpretability.\n21\nPublished as a conference paper at ICLR 2024\nFigure 6: Leopard images from ImageNet dataset. Visualization of the original image, the regions\nof core and spurious features, and the Grad-CAMs obtained using no, incorrect, and ground-truth\ncontextual attributes.\n22\nPublished as a conference paper at ICLR 2024\nFigure 7: Waterbird images from Waterbirds dataset. Visualization of the original image, the regions\nof core and spurious features, and the Grad-CAMs obtained using no, incorrect, and ground-truth\ncontextual attributes.\n23\nPublished as a conference paper at ICLR 2024\nF.4\nDISCOVERING CONTEXTUAL ATTRIBUTES BY LLMS\nIn this section, we provide an example of how to use GPT-4 (OpenAI, 2023) to generate contextual\nattributes and their descriptions automatically. We take EuroSAT dataset as an example. There are\nthree steps:\n1. Given a dataset (e.g., EuroSAT) with a specific domain, we retrieve similar images (e.g.,\nsatellite images) from a large image+text dataset LAION-400M 2.\n2. We crawl the captions and randomly sample a limited number of these captions (e.g., 200).\n3. We provide GPT-4 with the captions and the information of the dataset, and ask it to extract\ncontextual attributes using Prompt 1.\nTable 19 shows the contextual attributes discovered by GPT from captions. Adding those attributes to\nthe domain template, we improve the accuracy from 51.44% to 59.20% (with intervention \u03c4 = 5),\nwhich is comparable to manually designed ones. However, we found that the attributes identified by\nGPT are not always appropriate, possibly because of the gap between the retrieved images and our\ndataset. Future work could involve using image-based searches to find more similar images rather\nthan relying on language-based searches.\nPrompt 1: An example prompt for discovering contextual attributes and their descriptions from\nexample captions for EuroSAT dataset. Here we use one description for every attribute value for\nsimplicity.\nYou are a helpful assistant who helps me summarize my text captions. I\nhave a dataset of image-caption pairs, where each caption briefly\ndescribes the image. I need to extract some attributes from these\ntextual descriptions that contribute to the data generation process\nof the image.\nFor example, from the three descriptions [\"A black dog on the green grass\n\", \"A red car on the road in a bright environment\", \"A white\nrefrigerator\"], you can summarize the \"background\", \"color\", \"\nillumination\" as three attributes, with possible values [\"grass field\n\", \"road\", \" \"], [\"black\", \"green\", \"red\", \"white\", \" \"], [\"bright\",\n\"dark\", \" \"] respectively. Note that they each have an empty value\nbecause the human annotators may choose not to mention them in the\ncaptions.\nNote:\n1. The number of potential values for each factor should not exceed 3, so\nyou should extract the most representative values.\n2. You should summarize at most 5 attributes, covering the most\nrepresentative attributes in the provided captions.\n3. I have a list of labels for these images, namely [\u2019annual crop land\u2019,\n\u2019forest\u2019, \u2019brushland or shrubland\u2019, \u2019highway or road\u2019, \u2019industrial\nbuildings or commercial buildings\u2019, \u2019pasture land\u2019, \u2019permanent crop\nland\u2019, \u2019residential buildings or homes or apartments\u2019, \u2019river\u2019, \u2019lake\nor sea\u2019,]. The attributes you summarized should not overlap with the\nconcepts of these labels, and the values you summarized should not\ninclude any of these labels. For example, since \"river\" is in my\nlabel set, your summarized values should not include \"river\" for any\nattributes.\n4. The set of all values for all attributes you summarized should not\noverlap.\nI need your summary to have the following format:\nsummerized_factors = {\n\"background\": [\n\"\",\n\"grass\",\n2https://github.com/rom1504/clip-retrieval\n24\nPublished as a conference paper at ICLR 2024\n\"road\",\n],\n\"color\": [\n\"black\",\n\"green\",\n\"red\",\n\"white\",\n],\n\"illumination\": [\n\"bright\",\n\"dark\",\n\"\"\n]\n}\nHere are the captions:\n//200 captions\nTable 19: Contextual attributes and their value descriptions for EuroSAT generated by GPT-4.\nAttributes\nValue Descriptions\nsource\n\"\", \"Yandex satellite\", \"NASA\", \"Google Maps\"\ngeographical feature\n\"\", \"island\", \"ul.\", \"street\"\nimage type\n\"\", \"satellite\", \"aerial\", \"map\"\nnatural phenomenon\n\"\", \"hurricane\", \"earthquake\", \"deforestation\"\nstructure type\n\"\", \"residential\", \"commercial\", \"fortress\"\nG\nIMPACT, LIMITATION AND FUTURE WORK\nIn this paper, we propose PerceptionCLIP, a zero-shot classification method for CLIP that emulates\nthe human visual perception. By doing class inference conditioned on self-inferred contextual\nattributes, it achieves improved generalization, less reliance on spurious features, and improved\ninterpretability. Along the path of proposing PerceptionCLIP, we show CLIP\u2019s understanding of\nobject attributes beyond common category features. Our work indicates that CLIP, as a model capable\nof communicating with humans via natural language, can achieve things that traditional classifiers\nfind challenging. Hence, it still has great potential in zero-shot classification and even broader tasks\nlike image generation. Furthermore, this capability complements the study of neuroscience, enabling\na better transition of the latter\u2019s research findings into practical use.\nLimitations. One limitation of PerceptionCLIP is its sensitivity to text description perturbations:\nusing different synonyms to describe the same attribute sometimes has non-trivial effects on the\nresults. Although using more descriptions to describe an attribute value (Figure 2) alleviates this\nsensitivity, this issue is more intrinsic to CLIP and still persists. Future work may overcome this\nlimitation by replacing CLIP with other vision-language models or improving CLIP\u2019s sensitivity\nto textual perturbations (e.g., through training-time text augmentation (Fan et al., 2023)). Another\nlimitation of PerceptionCLIP is the need to design a set of contextual attributes. While this process\nprovides a way to integrate human prior knowledge, it also requires additional effort, especially\nwhen we aim to cover many attributes. Currently, we use caption retrieval from the LAION-400M\ndataset and the in-context learning ability of large language models to semi-automate the construction\nprocess. In the future, our goal is to automate this process fully. In our paper, we show that directly\nconcatenating multiple attributes\u2019 descriptions is a simple and effective way to generate the image\u2019s\ndescription. Future work can explore more effective and efficient approaches for it.\nEthical Statement. In this paper, we use the CelebA dataset, employing descriptors of gender and\nrace to enhance classification accuracy. We acknowledge the sensitivity of these attributes in societal\nand ethical contexts. Our use of gender and race is strictly as example contextual attributes within\nour analytical framework, and not as endorsements of any form of racial or gender-based bias. The\n25\nPublished as a conference paper at ICLR 2024\ninclusion of these attributes is solely for the purpose of exploring and improving the performance of\nzero-shot classification, without attributing any significance beyond their technical utility. We are\ncommitted to maintaining ethical principles in our research and upholding respect and diversity in all\naspects of our work.\n26\n"
  }
]