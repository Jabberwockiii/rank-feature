[
  {
    "title": "Improving Text Embeddings with Large Language Models",
    "link": "https://arxiv.org/pdf/2401.00368.pdf",
    "upvote": "69",
    "text": "Improving Text Embeddings with\nLarge Language Models\nLiang Wang\u2217, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei\nMicrosoft Corporation\nhttps://aka.ms/GeneralAI\nAbstract\nIn this paper, we introduce a novel and simple method for obtaining high-quality\ntext embeddings using only synthetic data and less than 1k training steps. Unlike\nexisting methods that often depend on multi-stage intermediate pre-training with\nbillions of weakly-supervised text pairs, followed by fine-tuning with a few labeled\ndatasets, our method does not require building complex training pipelines or relying\non manually collected datasets that are often constrained by task diversity and\nlanguage coverage. We leverage proprietary LLMs to generate diverse synthetic\ndata for hundreds of thousands of text embedding tasks across nearly 100 languages.\nWe then fine-tune open-source decoder-only LLMs on the synthetic data using\nstandard contrastive loss. Experiments demonstrate that our method achieves\nstrong performance on highly competitive text embedding benchmarks without\nusing any labeled data. Furthermore, when fine-tuned with a mixture of synthetic\nand labeled data, our model sets new state-of-the-art results on the BEIR and\nMTEB benchmarks.\n1\nIntroduction\nText embeddings are vector representations of natural language that encode its semantic information.\nThey are widely used in various natural language processing (NLP) tasks, such as information\nretrieval (IR), question answering, semantic textual similarity, bitext mining, item recommendation,\netc. In the field of IR, the first-stage retrieval often relies on text embeddings to efficiently recall\na small set of candidate documents from a large-scale corpus using approximate nearest neighbor\nsearch techniques. Embedding-based retrieval is also a crucial component of retrieval-augmented\ngeneration (RAG) [21], which is an emerging paradigm that enables large language models (LLMs)\nto access dynamic external knowledge without modifying the model parameters. Source attribution\nof generated text is another important application of text embeddings [14] that can improve the\ninterpretability and trustworthiness of LLMs.\nPrevious studies have demonstrated that weighted average of pre-trained word embeddings [35, 1]\nis a strong baseline for measuring semantic similarity. However, these methods fail to capture the\nrich contextual information of natural language. With the advent of pre-trained language models\n[11], Sentence-BERT [37] and SimCSE [13] have been proposed to learn text embeddings by fine-\ntuning BERT on natural language inference (NLI) datasets. To further enhance the performance and\nrobustness of text embeddings, state-of-the-art methods like E5 [46] and BGE [48] employ a more\ncomplex multi-stage training paradigm that first pre-trains on billions of weakly-supervised text pairs,\nand then fine-tunes on several labeled datasets.\nExisting multi-stage approaches suffer from several drawbacks. Firstly, they entail a complex\nmulti-stage training pipeline that demands substantial engineering efforts to curate large amounts\n\u2217Correspondence to {wangliang,nanya,fuwei}@microsoft.com\nTechnical Report.\narXiv:2401.00368v2  [cs.CL]  19 Jan 2024\nof relevance pairs. Secondly, they rely on manually collected datasets that are often constrained by\nthe diversity of tasks and the coverage of languages. For instance, Instructor [40] is only trained on\ninstructions from 330 English datasets, whereas BGE [48] only focuses on high-resource languages\nsuch as English and Chinese. Moreover, most existing methods employ BERT-style encoders as the\nbackbone, neglecting the recent advances of training better LLMs and related techniques such as\ncontext length extension [38].\nIn this paper, we propose a novel method for text embeddings that leverages LLMs to overcome the\nlimitations of existing approaches. We use proprietary LLMs to generate synthetic data for a diverse\nrange of text embedding tasks in 93 languages, covering hundreds of thousands of embedding tasks.\nSpecifically, we use a two-step prompting strategy that first prompts the LLMs to brainstorm a pool\nof candidate tasks, and then prompts the LLMs to generate data conditioned on a given task from the\npool. To cover various application scenarios, we design multiple prompt templates for each task type\nand combine the generated data from different templates to boost diversity. For the text embedding\nmodels, we opt for fine-tuning powerful open-source LLMs rather than small BERT-style models.\nSince LLMs such as Mistral [19] have been extensively pre-trained on web-scale data, contrastive\npre-training offers little additional benefit.\nWe demonstrate that Mistral-7B, when fine-tuned solely on synthetic data, attains competitive\nperformance on the BEIR [42] and MTEB [28] benchmarks. This is particularly intriguing considering\nthat this setting does not involve any labeled data. When fine-tuned on a mixture of synthetic and\nlabeled data, our model achieves new state-of-the-art results, surpassing previous methods by a\nsignificant margin (+2%). The entire training process requires less than 1k steps.\nMoreover, we empirically validate that our model can effectively perform personalized passkey\nretrieval for inputs up to 32k tokens by altering the rotation base of the position embeddings,\nextending the context length beyond the conventional 512 token limit. Regarding its multilinguality,\nour model excels on high-resource languages. However, for low-resource languages, there is still\nroom for improvement as current open-source LLMs are not adequately pre-trained on them.\n2\nRelated Work\nText Embeddings are continuous low-dimensional representations of text and have been extensively\napplied to various downstream tasks such as information retrieval, question answering, and retrieval-\naugmented generation (RAG). Early work on text embeddings includes latent semantic indexing [10]\nand weighted average of word embeddings [25]. More recent methods exploit supervision from\nnatural language inference [3] and labeled query-document pairs, such as the MS-MARCO passage\nranking dataset [5], to train text embeddings [37, 6, 13]. However, labeled data are often limited in\nterms of task diversity and language coverage. To address this challenge, methods like Contriever [18],\nOpenAI Embeddings [30], E5 [46], and BGE [48] adopt a multi-stage training paradigm. They first\npre-train on large-scale weakly-supervised text pairs using contrastive loss and then fine-tune on\nsmall-scale but high-quality datasets. In this paper, we demonstrate that it is possible to obtain\nstate-of-the-art text embeddings with single-stage training.\nSynthetic Data Synthetic data generation is a widely studied topic in information retrieval research,\nwith various methods proposed to enhance retrieval systems with artificially created data. For instance,\nDoc2query [33], InPars [2], and Promptagator [8] generate synthetic queries for unlabeled documents,\nwhich are then leveraged for document expansion or model training. GPL [45] employs a cross-\nencoder to produce pseudo-labels for query-document pairs. Similarly, Query2doc [47] generates\npseudo-documents for query expansion by few-shot prompting LLMs. Unlike these methods, our\napproach does not rely on any unlabeled documents or queries and thus can generate more diverse\nsynthetic data.\nAnother related line of work focuses on knowledge distillation from black-box LLMs by training on\nsynthetic data generated from them. DINO [39] generates synthetic text pairs for semantic textual\nsimilarity. Unnatural Instructions [16] is a synthetic instruction following dataset by prompting\nexisting LLMs. Orca [29] and Phi [15] propose to train better small language models by using\nhigh-quality synthetic data from GPT-3.5/4 [34].\nLarge Language Models With the popularization of ChatGPT, large language models (LLMs) have\ndemonstrated remarkable capabilities in instruction following and few-shot in-context learning [4].\n2\nHowever, the most advanced LLMs such as GPT-4 [34] are proprietary and have little technical\ndetails disclosed. To bridge the gap between proprietary and open-source LLMs, several notable\nefforts have been made, such as LLaMA-2 [44] and Mistral [19] models. A major limitation of LLMs\nis that they lack awareness of recent events and private knowledge. This issue can be partly mitigated\nby augmenting LLMs with information retrieved from external sources, a technique known as\nretrieval-augmented generation (RAG). On the other hand, LLMs can also serve as foundation models\nto enhance text embeddings. RepLLaMA [24] proposes to fine-tune LLaMA-2 with bi-encoder\narchitecture for ad-hoc retrieval. SGPT [27], GTR [32], and Udever [51] demonstrate the scaling law\nof text embeddings empirically, but their performance still falls behind small bidirectional encoders\nsuch as E5 [46] and BGE [48]. In this paper, we present a novel approach to train state-of-the-art text\nembeddings by exploiting the latest advances of LLMs and synthetic data.\nYou have been assigned a retrieval task: {task}\nYour mission is to write one text retrieval example for this task in JSON format. The JSON object must \ncontain the following keys:\n  - \"user_query\": a string, a random user search query specified by the retrieval task.\n  - \"positive_document\": a string, a relevant document for the user query.\n  - \"hard_negative_document\": a string, a hard negative document that only  appears relevant to the query.\nPlease adhere to the following guidelines:\n  - The \"user_query\" should be {query_type}, {query_length}, {clarity}, and diverse in topic.\n  - All documents should be at least {num_words} words long.\n  - Both the query and documents should be in {language}.\n  \u2026 (omitted some for space)\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n{\"user_query\": \"How to use Microsoft Power BI for data analysis\",\n\"positive_document\": \"Microsoft Power BI is a sophisticated tool that requires time and practice to \nmaster. In this tutorial, we'll show you how to navigate Power BI \u2026 (omitted) \",\n\u201chard_negative_document\u201d: \u201cExcel is an incredibly powerful tool for managing and analyzing large \namounts of data. Our tutorial series focuses on how you\u2026(omitted)\u201d } \nBrainstorm a list of potentially useful text retrieval tasks.\nHere are a few examples for your reference:\n    - Provided a scientific claim as query, retrieve documents that help verify or refute the claim.\n    - Search for documents that answers a FAQ-style query on children's nutrition.\nPlease adhere to the following guidelines:\n    - Specify what the query is, and what the desired documents are.\n    - Each retrieval task should cover a wide range of queries, and should not be too specific.\nYour output should always be a python list of strings only, with about 20 elements, and each element \ncorresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be \ncreative!\n[\"Retrieve company's financial reports for a given stock ticker symbol.\",\n\"Given a book name as a query, retrieve reviews, ratings and summaries of that book.\",\n\"Search for scientific research papers supporting a medical diagnosis for a specified disease.\u201c\n\u2026 (omitted for space)]\nnew session\nFigure 1: An example two-step prompt template for generating synthetic data with GPT-4. We first\nprompt GPT-4 to brainstorm a list of potential retrieval tasks, and then generate (query, positive, hard\nnegative) triplets for each task. \u201c{...}\u201d denotes a placeholder that will be replaced by sampling from a\npredefined set of values. Full prompts are available in Appendix C.\n3\n3\nMethod\n3.1\nSynthetic Data Generation\nUtilizing synthetic data generated by advanced LLMs such as GPT-4 presents a compelling oppor-\ntunity, especially in terms of enhancing diversity across a multitude of tasks and languages. Such\ndiversity is essential for developing robust text embeddings that can perform well across different\ntasks, be it semantic retrieval, textual similarity, or clustering.\nTo generate diverse synthetic data, we propose a simple taxonomy that categorizes embedding tasks\ninto several groups, and then apply different prompt templates to each group.\nAsymmetric Tasks This category comprises tasks where the query and document are semantically\nrelated but are not paraphrases of each other. Depending on the length of the query and document, we\nfurther divide asymmetric tasks into four subgroups: short-long match, long-short match, short-short\nmatch, and long-long match. For instance, short-long match tasks involve a short query and a long\ndocument, which is a typical scenario in commercial search engines. For each subgroup, we design a\ntwo-step prompt template that first prompts LLMs brainstorm a list of tasks, and then generates a\nconcrete example conditioned on the task definition. In Figure 1, we show an example prompt for\nthe short-long match subgroup. The outputs from GPT-4 are mostly coherent and of high quality. In\nour preliminary experiments, we also attempted to generate the task definition and query-document\npairs using a single prompt, but the data diversity was not as satisfactory as the proposed two-step\napproach.\nSymmetric Tasks Symmetric tasks involve queries and documents that have similar semantic\nmeanings but different surface forms. We examine two application scenarios: monolingual semantic\ntextual similarity (STS) and bitext retrieval. We design two distinct prompt templates for each\nscenario, tailored to their specific objectives. Since the task definition is straightforward, we omit the\nbrainstorming step for symmetric tasks.\nTo further boost the diversity of the prompts and thus the synthetic data, we incorporate several\nplaceholders in each prompt template, whose values are randomly sampled at runtime. For example,\nin Figure 1, the value of \u201c{query_length}\u201d is sampled from the set \u201c{less than 5 words, 5-10 words,\nat least 10 words}\u201d.\nTo generate multilingual data, we sample the value of \u201c{language}\u201d from the language list of XLM-\nR [7], giving more weight to high-resource languages. Any generated data that does not conform to\nthe predefined JSON format are discarded during the parsing process. We also remove duplicates\nbased on exact string matching.\n3.2\nTraining\nGiven a relevant query-document pair (q+, d+), we first apply the following instruction template to\nthe original query q+ to generate a new one q+\ninst:\nq+\ninst = Instruct: {task_definition} \\n Query: {q+}\n(1)\nwhere \u201c{task_definition}\u201d is a placeholder for a one-sentence description of the embedding task. For\ngenerated synthetic data, we use the outputs from the brainstorming step. For other datasets, such as\nMS-MARCO, we manually craft the task definitions and apply them to all the queries in the dataset.\nWe do not modify the document side with any instruction prefix. In this way, the document index can\nbe prebuilt, and we can customize the task to perform by changing only the query side.\nGiven a pretrained LLM, we append an [EOS] token to the end of the query and document, and then\nfeed them into the LLM to obtain the query and document embeddings (hq+\ninst, hd+) by taking the last\nlayer [EOS] vector. To train the embedding model, we adopt the standard InfoNCE loss L over the\nin-batch negatives and hard negatives:\nmin L = \u2212 log\n\u03d5(q+\ninst, d+)\n\u03d5(q+\ninst, d+) +\nX\nni\u2208N\n(\u03d5(q+\ninst, ni))\n(2)\nwhere N denotes the set of all negatives, and \u03d5(q, d) is a function that computes the matching score\nbetween query q and document d. In this paper, we adopt the temperature-scaled cosine similarity\n4\nfunction as follows:\n\u03d5(q, d) = exp(1\n\u03c4 cos(hq, hd))\n(3)\n\u03c4 is a temperature hyper-parameter, which is fixed to 0.02 in our experiments.\n4\nExperiments\n4.1\nStatistics of the Synthetic Data\nshort-long\n167k\nlong-short\n122k\nshort-short\n13k\nlong-long\n17k\nbitext\n89k\nsts\n99k\ndistribution of task types\nEnglish\n43.1%\nPolish\n3.0%\nJapanese\n2.9%\nItalian\n2.9%\nRussian\n2.9%\nIndonesian\n2.9%\nGerman\n2.9%\nPersian\n2.9%\nSpanish\n2.8%\nChinese\n2.8%\nFrench\n2.8%\nPortuguese\n2.8%\nDutch\n2.8%\nArabic\n2.7%\nOthers\n19.8%\ndistribution of languages\nFigure 2: Task type and language statistics of the generated synthetic data (see Section 3.1 for task\ntype definitions). The \u201cOthers\u201d category contains the remaining languages from the XLM-R language\nlist.\nFigure 2 presents the statistics of our generated synthetic data. We manage to generate 500k examples\nwith 150k unique instructions using Azure OpenAI Service 2, among which 25% are generated by\nGPT-35-Turbo and others are generated by GPT-4. The total token consumption is about 180M. The\npredominant language is English, with coverage extending to a total of 93 languages. For the bottom\n75 low-resource languages, there are about 1k examples per language on average.\nIn terms of data quality, we find that a portion of GPT-35-Turbo outputs do not strictly follow the\nguidelines specified in the prompt templates. Nevertheless, the overall quality remains acceptable,\nand preliminary experiments have demonstrated the benefits of incorporating this data subset.\n4.2\nModel Fine-tuning and Evaluation\nThe pretrained Mistral-7b [19] checkpoint is fine-tuned for 1 epoch using the loss in Equation 2. We\nfollow the training recipe from RankLLaMA [24] and utilize LoRA [17] with rank 16. To further\nreduce GPU memory requirement, techniques including gradient checkpointing, mixed precision\ntraining, and DeepSpeed ZeRO-3 are applied.\nFor the training data, we utilize both the generated synthetic data and a collection of 13 public datasets,\nyielding approximately 1.8M examples after sampling. More details are available in Appendix A. To\nprovide a fair comparison with some previous work, we also report results when the only labeled\nsupervision is the MS-MARCO passage ranking [5] dataset.\nWe evaluate the trained model on the MTEB benchmark [28]. Note that the retrieval category in\nMTEB corresponds to the 15 publicly available datasets in the BEIR benchmark [42]. Evaluation\nof one model takes about 3 days on 8 V100 GPUs due to the need to encode a large number of\ndocuments. Although our model can accommodate sequence length beyond 512, we only evaluate on\nthe first 512 tokens for efficiency. Official metrics are reported for each category. For more details\nabout the evaluation protocol, please refer to the original papers [28, 42].\n2https://oai.azure.com/\n5\n4.3\nMain Results\nTable 1: Results on the MTEB benchmark [28] (56 datasets in the English subset). The numbers are\naveraged for each category. Please refer to Table 15 for the scores per dataset.\n# of datasets \u2192\nClass.\nClust.\nPairClass.\nRerank\nRetr.\nSTS\nSumm.\nAvg\n12\n11\n3\n4\n15\n10\n1\n56\nUnsupervised Models\nGlove [35]\n57.3\n27.7\n70.9\n43.3\n21.6\n61.9\n28.9\n42.0\nSimCSEbert-unsup [13]\n62.5\n29.0\n70.3\n46.5\n20.3\n74.3\n31.2\n45.5\nSupervised Models\nSimCSEbert-sup [13]\n67.3\n33.4\n73.7\n47.5\n21.8\n79.1\n23.3\n48.7\nContriever [18]\n66.7\n41.1\n82.5\n53.1\n41.9\n76.5\n30.4\n56.0\nGTRxxl [32]\n67.4\n42.4\n86.1\n56.7\n48.5\n78.4\n30.6\n59.0\nSentence-T5xxl [31]\n73.4\n43.7\n85.1\n56.4\n42.2\n82.6\n30.1\n59.5\nE5large-v2 [46]\n75.2\n44.5\n86.0\n56.6\n50.6\n82.1\n30.2\n62.3\nGTElarge [23]\n73.3\n46.8\n85.0\n59.1\n52.2\n83.4\n31.7\n63.1\nBGElarge-en-v1.5 [48]\n76.0\n46.1\n87.1\n60.0\n54.3\n83.1\n31.6\n64.2\nOurs\nE5mistral-7b + full data\n78.5\n50.3\n88.3\n60.2\n56.9\n84.6\n31.4\n66.6\nw/ synthetic data only\n78.2\n50.5\n86.0\n59.0\n46.9\n81.2\n31.9\n63.1\nw/ synthetic + msmarco\n78.3\n49.9\n87.1\n59.5\n52.2\n81.2\n32.7\n64.5\nIn Table 1, our model \u201cE5mistral-7b + full data\u201d attains the highest average score on the MTEB\nbenchmark, outperforming the previous state-of-the-art model by 2.4 points. In the \u201cw/ synthetic data\nonly\u201d setting, no labeled data is used for training, and yet the performance remains quite competitive.\nWe posit that generative language modeling and text embeddings are the two sides of the same coin,\nwith both tasks requiring the model to have a deep understanding of the natural language. Given an\nembedding task definition, a truly robust LLM should be able to generate training data on its own and\nthen be transformed into an embedding model through light-weight fine-tuning. Our experiments\nshed light on the potential of this direction, and more research is needed to fully explore it.\nTable 2: Comparison with commercial models and the model that tops the MTEB leaderboard (as\nof 2023-12-22). For the commercial models listed here, little details are available on their model\narchitectures and training data.\nModel\nBEIR Retrieval (15 datasets)\nMTEB Average (56 datasets)\nOpenAI Ada-002\n49.3\n61.0\nCohere-embed-english-v3.0\n55.0\n64.5\nvoyage-lite-01-instruct\n55.6\n64.5\nUAE-Large-V1 [22]\n54.7\n64.6\nE5mistral-7b + full data\n56.9\n66.6\nIn Table 2, we also present a comparison with several commercial text embedding models. However,\ndue to the lack of transparency and documentation about these models, a fair comparison is not\nfeasible. We focus especially on the retrieval performance on the BEIR benchmark, since RAG is\nan emerging technique to enhance LLM with external knowledge and proprietary data. As Table 2\nshows, our model outperforms the current commercial models by a significant margin.\n4.4\nMultilingual Retrieval\nTo assess the multilingual capabilities of our model, we conduct an evaluation on the MIRACL\ndataset [53], which comprises human-annotated queries and relevance judgments across 18 languages.\nAs shown in Table 3, our model surpasses mE5large on high-resource languages, notably on English.\nNevertheless, for low-resource languages, our model remains suboptimal compared to mE5base.\nWe attribute this to the fact that Mistral-7B is predominantly pre-trained on English data, and we\nanticipate that future multilingual LLMs will leverage our method to bridge this gap.\n6\nTable 3: nDCG@10 on the dev set of the MIRACL dataset for both high-resource and low-resource\nlanguages. We select the 4 high-resource languages and the 4 low-resource languages according to\nthe number of candidate documents. The numbers for BM25 and mDPR come from Zhang et al.\n[53]. For the complete results on all 18 languages, please see Table 5.\nHigh-resource Languages\nLow-resource Languages\nen\nfr\nes\nru\nte\nhi\nbn\nsw\nBM25 [53]\n35.1\n18.3\n31.9\n33.4\n49.4\n45.8\n50.8\n38.3\nmDPR [53]\n39.4\n43.5\n47.8\n40.7\n35.6\n38.3\n44.3\n29.9\nmE5base [46]\n51.2\n49.7\n51.5\n61.5\n75.2\n58.4\n70.2\n71.1\nmE5large [46]\n52.9\n54.5\n52.9\n67.4\n84.6\n62.0\n75.9\n74.9\nE5mistral-7b + full data\n57.3\n55.2\n52.2\n67.7\n73.9\n52.1\n70.3\n68.4\n5\nAnalysis\n5.1\nIs Contrastive Pre-training Necessary?\nRetrieval\nClassification\nMTEB All\n20\n30\n40\n50\n60\n70\n80\n90\nPerformance\n+8.2\n+4.3\n+5.7\nXLM-R-large + full data\noriginal\nw/ cont. pre-train\nRetrieval\nClassification\nMTEB All\n20\n30\n40\n50\n60\n70\n80\n90\nPerformance\n+0.0\n+0.2\n+0.1\nE5-mistral-7b + full data\noriginal\nw/ cont. pre-train\nFigure 3: Effects of contrastive pre-training. Detailed numbers are in Appendix Table 6.\nWeakly-supervised contrastive pre-training is one of the key factors behind the success of existing\ntext embedding models. For instance, Contriever [18] treats random cropped spans as positive pairs\nfor pre-training, while E5 [46] and BGE [48] collect and filter text pairs from various sources.\nThis section re-evaluates the necessity of contrastive pre-training for LLMs, particularly those that\nhave been pre-trained on trillions of tokens. Figure 3 shows that contrastive pre-training benefits\nXLM-Rlarge, enhancing its retrieval performance by 8.2 points when fine-tuned on the same data,\nwhich aligns with prior findings. However, for Mistral-7B based models, contrastive pre-training\nhas negligible impact on the model quality. This implies that extensive auto-regressive pre-training\nenables LLMs to acquire good text representations, and only minimal fine-tuning is required to\ntransform them into effective embedding models.\n5.2\nExtending to Long Text Embeddings\nQuery: what is the pass key for Malayah Graves?\nDoc1: <prefix filler> Malayah Graves's pass key is 123. Remember it. 123 is the pass key for Malayah Graves. <suffix filler>\nDoc2: <prefix filler> Cesar McLean's pass key is 456. Remember it. 456 is the pass key for Cesar McLean. <suffix filler>\n\u2026\u2026\nFigure 4: Illustration of the personalized passkey retrieval task adapted from Mohtashami and Jaggi\n[26]. The \u201c<prefix filler>\u201d and \u201c<suffix filler>\u201d are repeats of \u201cThe grass is green. The sky is blue.\nThe sun is yellow. Here we go. There and back again.\u201d In addition, each document has a unique\nperson name and a random passkey inserted at a random position. The task is to retrieve the document\nthat contains the given person\u2019s passkey from 100 candidates.\nExisting evaluation datasets for text embedding models are typically short, to evaluate the long-\ncontext capability of our model, we introduce a novel synthetic task called personalized passkey\n7\n256\n512\n1k\n2k\n4k\n8k\n16k\n32k\nContext Length\n0\n20\n40\n60\n80\n100\nTop1 Accuracy\nwindow 4k, base 10^4\nwindow 32k, base 10^4\nwindow 32k, base 10^5\nwindow 32k, base 10^6\nFigure 5: Accuracy of personalized passkey retrieval as a function of input context length. For each\ncontext length, we randomly generate 50 queries and compute the top 1 accuracy.\nTable 4: Results on the MTEB benchmark with various hyperparameters. The first row corresponds\nto the default setting, which employs last-token pooling, LoRA rank 16, and natural language\ninstructions. Unless otherwise stated, all models are trained on the synthetic and MS-MARCO\npassage ranking data.\nDatasets\nClass.\nClust.\nPairClass.\nRerank\nRetr.\nSTS\nSumm.\nAvg\nE5mistral-7b\n78.3\n49.9\n87.1\n59.5\n52.2\n81.2\n32.7\n64.5\nw/ LLaMA-2 7b init.\n76.2\n48.1\n85.1\n58.9\n49.6\n81.2\n30.8\n62.9-1.6\nw/ msmarco data only\n71.6\n47.1\n86.1\n58.8\n54.4\n79.5\n31.7\n62.7-1.8\npooling type\nw/ mean pool\n77.0\n48.9\n86.1\n59.2\n52.4\n81.4\n30.8\n64.1-0.4\nw/ weighted mean\n77.0\n49.0\n86.1\n59.2\n52.0\n81.4\n30.2\n64.0-0.5\nLoRA rank\nw/ r=8\n78.4\n50.3\n87.1\n59.3\n53.0\n81.0\n31.7\n64.8+0.3\nw/ r=32\n78.4\n50.3\n87.4\n59.5\n52.2\n81.2\n30.6\n64.6+0.1\ninstruction type\nw/o instruction\n72.3\n47.1\n82.6\n56.3\n48.2\n76.7\n30.7\n60.3-4.2\nw/ task type prefix\n71.1\n46.5\n79.7\n54.0\n52.7\n73.8\n30.0\n60.3-4.2\nretrieval, which is illustrated in Figure 4. This task requires encoding the passkey information in a\nlong context into the embeddings. We compare the performance of different variants by changing the\nsliding window size and the RoPE rotation base [41] in Figure 5. The results show that the default\nconfiguration with 4k sliding window attains 100% accuracy within 4k tokens, but the accuracy\ndeteriorates quickly as the context length grows. Naively extending the sliding window size to 32k\nresults in worse performance. By changing the RoPE rotation base to 105, the model can achieve\nover 90% accuracy within 32k tokens. However, this entails a minor trade-off in performance for\nshorter contexts. A potential avenue for future research is to efficiently adapt the model to longer\ncontexts through lightweight post-training [54].\n5.3\nAnalysis of Training Hyperparameters\nTable 4 presents the results under different configurations. We notice that the Mistral-7B initialization\nholds an advantage over LLaMA-2 7B, in line with the findings from Mistral-7B technical report [19].\nThe choice of pooling types and LoRA ranks does not affect the overall performance substantially,\nhence we adhere to the default setting despite the marginal superiority of LoRA rank 8. On the other\nhand, the way of adding instructions has a considerable impact on the performance. We conjecture\nthat natural language instructions better inform the model regarding the embedding task at hand, and\nthus enable the model to generate more discriminative embeddings. Our framework also provides a\nway to customize the behavior of text embeddings through instructions without the need to fine-tune\nthe model or re-built document index.\n8\n6\nConclusion\nThis paper shows that the quality of text embeddings can be substantially enhanced by exploiting\nLLMs. We prompt proprietary LLMs such as GPT-4 to generate diverse synthetic data with instruc-\ntions in many languages. Combined with the strong language understanding capability of the Mistral\nmodel, we establish new state-of-the-art results for nearly all task categories on the competitive MTEB\nbenchmark. The training process is much more streamlined and efficient than existing multi-stage\napproaches, thereby obviating the need for intermediate pre-training.\nFor future work, we aim to further improve the multilingual performance of our model and explore\nthe possibility of using open-source LLMs to generate synthetic data. We also intend to investigate\nways to improve the inference efficiency and lower the storage cost for LLM based text embeddings.\nReferences\n[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence\nembeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL\nhttps://openreview.net/forum?id=SyK00v5xx.\n[2] Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. Inpars:\nUnsupervised dataset generation for information retrieval. Proceedings of the 45th International\nACM SIGIR Conference on Research and Development in Information Retrieval, 2022.\n[3] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal,\n2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:\n//aclanthology.org/D15-1075.\n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learn-\ners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\nHsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n[5] Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh\nTiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated\nmachine reading comprehension dataset. ArXiv preprint, abs/1611.09268, 2016. URL https:\n//arxiv.org/abs/1611.09268.\n[6] Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. Super-\nvised learning of universal sentence representations from natural language inference data. In\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,\npages 670\u2013680, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi:\n10.18653/v1/D17-1070. URL https://aclanthology.org/D17-1070.\n[7] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-\nzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\nUnsupervised cross-lingual representation learning at scale. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online,\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL\nhttps://aclanthology.org/2020.acl-main.747.\n9\n[8] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu,\nKeith Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. In\nThe Eleventh International Conference on Learning Representations, 2022.\n[9] DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. Quora question\npairs, 2017. URL https://kaggle.com/competitions/quora-question-pairs.\n[10] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard\nHarshman. Indexing by latent semantic analysis. Journal of the American society for information\nscience, 41(6):391\u2013407, 1990.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis,\nMinnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nURL https://aclanthology.org/N19-1423.\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.\nELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 3558\u20133567, Florence, Italy, 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.\norg/P19-1346.\n[13] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of\nsentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6894\u20136910, Online and Punta Cana, Dominican Republic, 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL\nhttps://aclanthology.org/2021.emnlp-main.552.\n[14] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to\ngenerate text with citations. ArXiv preprint, abs/2305.14627, 2023. URL https://arxiv.\norg/abs/2305.14627.\n[15] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allison Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Piero C. Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil\nSalim, S. Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tau-\nman Kalai, Yin Tat Lee, and Yuan-Fang Li. Textbooks are all you need. ArXiv preprint,\nabs/2306.11644, 2023. URL https://arxiv.org/abs/2306.11644.\n[16] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor. ArXiv preprint, abs/2212.09689, 2022. URL\nhttps://arxiv.org/abs/2212.09689.\n[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.\n[18] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive\nlearning. ArXiv preprint, abs/2112.09118, 2021. URL https://arxiv.org/abs/2112.\n09118.\n[19] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. ArXiv preprint, abs/2310.06825, 2023. URL https://arxiv.org/\nabs/2310.06825.\n[20] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 6769\u20136781, Online, 2020. Association for Computational Linguistics. doi: 10.\n10\n18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.\n550.\n[21] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian\nRiedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP\ntasks. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\nHsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n6b493230205f780e1bc26945df7481e5-Abstract.html.\n[22] Xianming Li and Jing Li. Angle-optimized text embeddings. ArXiv preprint, abs/2309.12871,\n2023. URL https://arxiv.org/abs/2309.12871.\n[23] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.\nTowards general text embeddings with multi-stage contrastive learning.\nArXiv preprint,\nabs/2308.03281, 2023. URL https://arxiv.org/abs/2308.03281.\n[24] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for\nmulti-stage text retrieval. ArXiv preprint, abs/2310.08319, 2023. URL https://arxiv.org/\nabs/2310.08319.\n[25] Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word\nrepresentations in vector space. In ICLR, 2013.\n[26] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. ArXiv preprint, abs/2305.16300, 2023. URL https://arxiv.org/\nabs/2305.16300.\n[27] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. ArXiv preprint,\nabs/2202.08904, 2022. URL https://arxiv.org/abs/2202.08904.\n[28] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text em-\nbedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 2014\u20132037, Dubrovnik, Croatia, 2023. Association\nfor Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.148.\n[29] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Hassan Awadallah. Orca: Progressive learning from complex explanation traces of\ngpt-4. ArXiv preprint, abs/2306.02707, 2023. URL https://arxiv.org/abs/2306.02707.\n[30] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek,\nQiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav\nShyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr,\nFelipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov,\nJoanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive\npre-training. ArXiv preprint, abs/2201.10005, 2022. URL https://arxiv.org/abs/2201.\n10005.\n[31] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and\nYinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 1864\u20131874,\nDublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nfindings-acl.146. URL https://aclanthology.org/2022.findings-acl.146.\n[32] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 9844\u20139855, Abu Dhabi, United Arab Emirates, 2022. Association for\nComputational Linguistics. URL https://aclanthology.org/2022.emnlp-main.669.\n11\n[33] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query\nprediction. ArXiv preprint, abs/1904.08375, 2019. URL https://arxiv.org/abs/1904.\n08375.\n[34] OpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.\norg/abs/2303.08774.\n[35] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for\nword representation. In Proceedings of the 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar, 2014. Association for\nComputational Linguistics. doi: 10.3115/v1/D14-1162. URL https://aclanthology.org/\nD14-1162.\n[36] Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, QiaoQiao She, Jing Liu, Hua Wu, and Haifeng\nWang. DuReader-retrieval: A large-scale Chinese benchmark for passage retrieval from web\nsearch engine. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 5326\u20135338, Abu Dhabi, United Arab Emirates, 2022. Association for\nComputational Linguistics. URL https://aclanthology.org/2022.emnlp-main.357.\n[37] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese\nBERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410.\n[38] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton,\nManish P Bhatt, Cristian Cant\u00f3n Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u2019efossez,\nJade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom,\nand Gabriel Synnaeve.\nCode llama: Open foundation models for code.\nArXiv preprint,\nabs/2308.12950, 2023. URL https://arxiv.org/abs/2308.12950.\n[39] Timo Schick and Hinrich Sch\u00fctze. Generating datasets with pretrained language models. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npages 6943\u20136951, 2021.\n[40] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau\nYih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-\nfinetuned text embeddings. In Findings of the Association for Computational Linguistics:\nACL 2023, pages 1102\u20131121, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.findings-acl.71. URL https://aclanthology.org/\n2023.findings-acl.71.\n[41] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\n[42] Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych.\nBeir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack (Round 2), 2021.\n[43] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\nlarge-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana,\n2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https:\n//aclanthology.org/N18-1074.\n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023. URL https:\n//arxiv.org/abs/2307.09288.\n12\n[45] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. GPL: Generative pseudo\nlabeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2345\u20132360, Seattle, United States, 2022. Association\nfor Computational Linguistics.\ndoi: 10.18653/v1/2022.naacl-main.168.\nURL https://\naclanthology.org/2022.naacl-main.168.\n[46] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan\nMajumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.\nArXiv preprint, abs/2212.03533, 2022. URL https://arxiv.org/abs/2212.03533.\n[47] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language\nmodels. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pages 9414\u20139423, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.585. URL https://aclanthology.org/\n2023.emnlp-main.585.\n[48] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-pack: Packaged resources\nto advance general chinese embedding. ArXiv preprint, abs/2309.07597, 2023. URL https:\n//arxiv.org/abs/2309.07597.\n[49] Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu,\nXiangsheng Li, Haitao Li, Yiqun Liu, et al. T2ranking: A large-scale chinese benchmark for\npassage ranking. ArXiv preprint, abs/2304.03679, 2023. URL https://arxiv.org/abs/\n2304.03679.\n[50] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 2369\u20132380, Brussels, Belgium, 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\n[51] Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min\nZhang. Language models are universal embedders. ArXiv preprint, abs/2310.08232, 2023. URL\nhttps://arxiv.org/abs/2310.08232.\n[52] Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. TyDi: A multi-lingual benchmark\nfor dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning,\npages 127\u2013137, Punta Cana, Dominican Republic, 2021. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2021.mrl-1.12. URL https://aclanthology.org/2021.mrl-1.12.\n[53] Xinyu Crystina Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-\nHermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Miracl: A mul-\ntilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for\nComputational Linguistics, 11:1114\u20131131, 2023.\n[54] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose:\nEfficient context window extension of llms via positional skip-wise training. ArXiv preprint,\nabs/2309.10400, 2023. URL https://arxiv.org/abs/2309.10400.\nA\nImplementation Details\nBaseline Models For results with mE5base and mE5large, we use the public checkpoints available at\nhttps://huggingface.co/intfloat/multilingual-e5-base and https://huggingface.\nco/intfloat/multilingual-e5-large respectively. For experiments in Table 4, we follow the\nSGPT [27] paper for the implementation of weighted mean pooling. For the \u201cw/ task type prefix\u201d\nsetting, we prepend \u201cclassify: \u201d for the long-short matching subgroup, and \u201cquery: \u201d for other\nasymmetric tasks. No prefix is added for symmetric tasks.\nTraining Data For the \u201cE5mistral-7b + full data\u201d setting, our training data comprises generated\nsynthetic data, ELI5 [12](sample ratio 0.1), HotpotQA [50], FEVER [43], MIRACL [53], MS-\nMARCO passage ranking (sample ratio 0.5) and document ranking (sample ratio 0.2) [5], NQ [20],\n13\nNLI [13], SQuAD [20], TriviaQA [20], Quora Duplicate Questions [9](sample ratio 0.1), Mr-\nTyDi [52], DuReader [36], and T2Ranking [49](sample ratio 0.5) datasets. We only include the\ntraining set of each dataset. For the datasets without hard negatives, we use mE5base to mine top 100\nhard negatives. After sampling, we obtain approximately 1.8 million examples. The entire training\nprocess takes fewer than 1k steps to complete.\nHyperparameters for Fine-tuning When fine-tuning Mistral-7b, the batch size is set to 2048 and\nthe learning rate is 10\u22124 with 100 step warmup and linear decay. The weight decay is 0.1. We add\n1 hard negative for each query-document pair. The fine-tuning process takes roughly 18 hours on\n32 V100 GPUs with a maximum sequence length 512. We add LoRA adapters to all linear layers,\nresulting in a total of 42M trainable parameters. Our implementation is based on the HuggingFace\nPEFT library at https://github.com/huggingface/peft.\nThe model and dataset release information is available at https://github.com/microsoft/\nunilm/tree/master/e5.\nB\nTest Set Contamination Analysis\nTo assess the test set contamination on all the datasets in the MTEB benchmark, we perform a string\nmatch based analysis between the test set and our training set, disregarding differences in character\ncase and spacing. We categorize the train-test overlaps into three types:\n\u2022 Low entropy texts. These are texts such as \u201ci need a coffee\u201d and \u201cwhat does that mean\u201d,\nwhich are not considered as contamination because they are common expressions that can\noccur in various contexts.\n\u2022 Question overlap. We identify 4 test set questions in the DBPedia dataset that also appear\nin the TriviaQA training set. Given that they constitute a minor portion of the test set, their\nimpact on the overall performance is insignificant.\n\u2022 Retrieval corpus overlap. Several retrieval datasets share the same retrieval corpus. For\ninstance, the DBPedia, NQ, and TriviaQA datasets all use Wikipedia passages, even though\ntheir query sets are different. This is a standard evaluation practice in the field of information\nretrieval, and we do not regard it as contamination.\nIn summary, we did not detect substantial contamination risks that could alter the main findings of\nthis paper.\nAnother aspect to consider is the possibility of test set contamination in the training data of Mistral-\n7B and GPT-4. However, since the training data of these models is not publicly accessible, it is\nchallenging to estimate the degree of such contamination. Given their widespread use in the research\ncommunity, we believe it is still a valid comparison if other works also employ these models.\nTable 5: nDCG@10 and Recall@100 on the dev set of the MIRACL dataset for all 18 languages.\nnDCG@10\nRecall@100\nBM25\nmDPR\nmE5base\nmE5large\nE5mistral-7b full\nBM25\nmDPR\nmE5base\nmE5large\nE5mistral-7b full\nar\n48.1\n49.9\n71.6\n76.0\n73.3\n88.9\n84.1\n95.9\n97.3\n96.0\nbn\n50.8\n44.3\n70.2\n75.9\n70.3\n90.9\n81.9\n96.6\n98.2\n96.0\nen\n35.1\n39.4\n51.2\n52.9\n57.3\n81.9\n76.8\n86.4\n87.6\n90.2\nes\n31.9\n47.8\n51.5\n52.9\n52.2\n70.2\n86.4\n88.6\n89.1\n87.5\nfa\n33.3\n48.0\n57.4\n59.0\n52.1\n73.1\n89.8\n91.2\n92.9\n88.0\nfi\n55.1\n47.2\n74.4\n77.8\n74.7\n89.1\n78.8\n96.9\n98.1\n96.7\nfr\n18.3\n43.5\n49.7\n54.5\n55.2\n65.3\n91.5\n90.0\n90.6\n92.8\nhi\n45.8\n38.3\n58.4\n62.0\n52.1\n86.8\n77.6\n92.6\n93.9\n89.9\nid\n44.9\n27.2\n51.1\n52.9\n52.7\n90.4\n57.3\n87.4\n87.9\n88.4\nja\n36.9\n43.9\n64.7\n70.6\n66.8\n80.5\n82.5\n96.0\n97.1\n95.1\nko\n41.9\n41.9\n62.2\n66.5\n61.8\n78.3\n73.7\n91.6\n93.4\n89.4\nru\n33.4\n40.7\n61.5\n67.4\n67.7\n66.1\n79.7\n92.7\n95.5\n95.0\nsw\n38.3\n29.9\n71.1\n74.9\n68.4\n70.1\n61.6\n95.6\n96.7\n95.5\nte\n49.4\n35.6\n75.2\n84.6\n73.9\n83.1\n76.2\n98.0\n99.2\n95.1\nth\n48.4\n35.8\n75.2\n80.2\n74.0\n88.7\n67.8\n98.0\n98.9\n96.5\nzh\n18.0\n51.2\n51.5\n56.0\n54.0\n56.0\n94.4\n92.1\n93.3\n90.1\nAvg\n39.3\n41.5\n62.3\n66.5\n62.9\n78.7\n78.8\n93.1\n94.3\n92.6\n14\nTable 6: Detailed results for the effects of contrastive pre-training. For the \u201cE5mistral-7b w/ cont.\npre-train\u201d setting, we pre-train Mistral-7B following the mE5 recipe for 10k steps.\nDatasets\nClass.\nClust.\nPairClass.\nRerank\nRetr.\nSTS\nSumm.\nAvg\nXLM-Rlarge + full data\n72.9\n38.7\n84.5\n53.8\n42.0\n82.3\n29.7\n58.0\nw/ cont. pre-train\n77.2\n47.3\n85.5\n58.6\n50.2\n84.4\n30.7\n63.7\nE5mistral-7b + full data\n78.5\n50.3\n88.3\n60.2\n56.9\n84.6\n31.4\n66.6\nw/ cont. pre-train\n78.7\n50.1\n87.7\n60.9\n56.9\n84.9\n30.2\n66.7\nTable 7: Prompt template for the short-long matching subgroup. For placeholders, \u201c{query_type}\u201d \u2208\n{extremely long-tail, long-tail, common}, \u201c{query_length}\u201d \u2208 {less than 5 words, 5 to 15 words, at\nleast 10 words}, \u201c{difficulty}\u201d \u2208 {high school, college, PhD}, \u201c{clarity}\u201d \u2208 {clear, understandable\nwith some effort, ambiguous}, \u201c{num_words}\u201d \u2208 {50, 100, 200, 300, 400, 500}.\nBrainstorm a list of potentially useful text retrieval tasks.\nHere are a few examples for your reference:\n- Retrieve relevant documents for a short keyword web search query that asks for weather information.\n- Search for documents that answers a FAQ-style query on children\u2019s nutrition.\nPlease adhere to the following guidelines:\n- Specify what the query is, and what the desired documents are.\n- Each retrieval task should cover a wide range of queries, and should not be too specific.\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds\nto a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be creative!\nYou have been assigned a retrieval task: {task}\nYour mission is to write one text retrieval example for this task in JSON format. The JSON object must\ncontain the following keys:\n- \"user_query\": a string, a random user search query specified by the retrieval task.\n- \"positive_document\": a string, a relevant document for the user query.\n- \"hard_negative_document\": a string, a hard negative document that only appears relevant to the query.\nPlease adhere to the following guidelines:\n- The \"user_query\" should be {query_type}, {query_length}, {clarity}, and diverse in topic.\n- All documents must be created independent of the query. Avoid copying the query verbatim. It\u2019s acceptable\nif some parts of the \"positive_document\" are not topically related to the query.\n- All documents should be at least {num_words} words long.\n- The \"hard_negative_document\" contains some useful information, but it should be less useful or comprehen-\nsive compared to the \"positive_document\".\n- Both the query and documents should be in {language}.\n- Do not provide any explanation in any document on why it is relevant or not relevant to the query.\n- Both the query and documents require {difficulty} level education to understand.\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\nC\nPrompts for Synthetic Data Generation\nFor asymmetric tasks, we list the four prompt templates in Table 7, 8, 9, and 10. For symmetric\ntasks, the prompts templates are available in Table 11 and 12. To generate multilingual data, we\nsample the value of \u201c{language}\u201d from the language list of XLM-R [7] with higher probability for\nhigh-resource languages. When prompting GPT-4/3.5, we set the temperature to 1.0 and the top-p to\n1.0, which is higher than the default setting to encourage more diversity.\nD\nInstructions for Training and Evaluation\nWe manually write instructions for training datasets, as listed in Table 13. For evaluation datasets, the\ninstructions are listed in Table 14.\n15\nTable 8: Prompt template for the long-short matching subgroup. For placeholders, \u201c{num_words}\u201d\n\u2208 {\"less than 10\", \"at least 10\", \"at least 50\", \"at least 100\", \"at least 200\"}, \u201c{difficulty}\u201d \u2208 {high\nschool, college, PhD}, \u201c{clarity}\u201d \u2208 {clear, understandable with some effort, ambiguous}.\nBrainstorm a list of potentially useful text classification tasks.\nPlease adhere to the following guidelines:\n- Tasks should cover a diverse range of domains and task types.\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds\nto a distinct text classification task in one sentence. Do not explain yourself or output anything else. Be\ncreative!\nYou have been assigned a text classification task: {task}\nYour mission is to write one text classification example for this task in JSON format. The JSON object must\ncontain the following keys:\n- \"input_text\": a string, the input text specified by the classification task.\n- \"label\": a string, the correct label of the input text.\n- \"misleading_label\": a string, an incorrect label that is related to the task.\nPlease adhere to the following guidelines:\n- The \"input_text\" should be {num_words} words and diverse in expression.\n- The \"misleading_label\" must be a valid label for the given task, but not as appropriate as the \"label\" for the\n\"input_text\".\n- The values for all fields should be in {language}.\n- Avoid including the values of the \"label\" and \"misleading_label\" fields in the \"input_text\", that would make\nthe task too easy.\n- The \"input_text\" is {clarity} and requires {difficulty} level education to comprehend.\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\nTable 9: Prompt template for the short-short matching subgroup. We do not generate negative\ndocuments as the matching task is already reasonably difficult.\nBrainstorm a list of text matching tasks where both the queries and the groundtruth documents are very short\n(one or two sentences, even a short phrase).\nHere are a few examples:\n- Given a scientific paper title, retrieve the title of papers that cite the given paper.\n- Match a word with its definition.\n- Provided a notable person\u2019s name, identify their occupation or achievement.\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds\nto a distinct task in one sentence. Do not explain yourself or output anything else. Be creative!\nYou have been assigned a text matching task: {task}\nYour mission is to write one example for this task in JSON format. The JSON object must contain the\nfollowing keys:\n- \"input\": a string, a random input specified by the task.\n- \"positive_document\": a string, a relevant document for the \"input\" according to the task.\nPlease adhere to the following guidelines:\n- The values of all fields should be in {language}.\n- Both the \"input\" and \"positive_document\" should be very short (a sentence or a phrase), avoid substantial\nword overlaps, otherwise the task would be too easy.\n- The \"input\" and \"positive_document\" should be independent of each other.\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n16\nTable 10: Prompt template for the long-long matching subgroup. We do not generate negative\ndocuments for latency reasons.\nBrainstorm a list of text matching tasks where the queries are long documents.\nHere are a few examples:\n- Given a document that supports a debatable argument, find another document that contains opposite\narguments.\n- Provided a lengthy business proposal, retrieve competitive business strategies in the same industry.\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds\nto a distinct task in one sentence. Do not explain yourself or output anything else. Be creative!\nYou have been assigned a text matching task: {task}\nYour mission is to write one example for this task in JSON format. The JSON object must contain the\nfollowing keys:\n- \"input\": a string, a random input specified by the task.\n- \"positive_document\": a string, a relevant document for the \"input\" according to the task.\nPlease adhere to the following guidelines:\n- The values of all fields should be in {language}.\n- Both the \"input\" and \"positive_document\" should be long documents (at least 300 words), avoid substantial\nword overlaps, otherwise the task would be too easy.\n- The \"input\" and \"positive_document\" should be independent of each other.\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\nTable 11: Prompt template for monolingual STS. For placeholders, \u201c{high_score}\u201d \u2208 {4, 4.5, 5},\n\u201c{low_score}\u201d \u2208 {2.5, 3, 3.5}, \u201c{unit}\u201d \u2208 {sentence, phrase, passage}, \u201c{difficulty}\u201d \u2208 {elementary\nschool, high school, college}.\nWrite a {unit} triple with varying semantic similarity scores in JSON format. The semantic similarity score\nranges from 1 to 5, with 1 denotes least similar and 5 denotes most similar.\nPlease adhere to the following guidelines:\n- The keys in JSON are \"S1\", \"S2\", and \"S3\", the values are all strings in {language}, do not add any other\nkeys.\n- There should be some word overlaps between all three {unit}s.\n- The similarity score between S1 and S2 should be {high_score}.\n- The similarity score between S1 and S3 should be {low_score}.\n- The {unit}s require {difficulty} level education to understand and should be diverse in terms of topic and\nlength.\nYour output must always be a JSON object only with three keys \"S1\", \"S2\" and \"S3\", do not explain yourself\nor output anything else. Be creative!\n17\nTable 12: Prompt template for bitext retrieval. For placeholders, \u201c{high_score}\u201d \u2208 {4, 4.5, 5},\n\u201c{low_score}\u201d \u2208 {1.5, 2, 2.5}, \u201c{unit}\u201d \u2208 {sentence, phrase, passage}, \u201c{difficulty}\u201d \u2208 {elementary\nschool, high school, college}.\nWrite a {unit} triple with one {unit} in {src_lang} and two {unit}s in {tgt_lang} with varying translation\nqualities in JSON format.\nThe triple is denotes as (\"S1\", \"S2\", \"S3\"). The translation quality score ranges from 1 to 5, with higher\nscores are better.\nPlease adhere to the following guidelines:\n- The values of \"S1\" is a string in {src_lang}, the value of \"S2\" and \"S3\" are strings in {tgt_lang}.\n- There should be some word overlaps between \"S2\" and \"S3\".\n- The translation quality score of \"S2\" with respect to \"S1\" should be {high_score}.\n- The translation quality score of \"S3\" with respect to \"S1\" should be {low_score}.\n- \"S3\" should be grammatical and fluent, but contain some keyword or number translation errors, or miss\nsome information, or contain some redundant information.\n- \"S1\" requires {difficulty} level education to understand and should be diverse in terms of topic and length.\nYour output must always be a JSON object only with three keys \"S1\", \"S2\" and \"S3\", do not explain yourself\nor output anything else. Be creative!\nTable 13: Instructions for each training dataset.\nDataset\nInstruction\nELI5\nProvided a user question, retrieve the highest voted answers on Reddit ELI5 forum\nHotpotQA\nGiven a multi-hop question, retrieve documents that can help answer the question\nFEVER\nGiven a claim, retrieve documents that support or refute the claim\nMIRACL / MrTyDi / NQ\n/ SQuAD / TriviaQA\nGiven a question, retrieve Wikipedia passages that answer the question\nRetrieve Wikipedia passages that answer the question\nNLI\nGiven a premise, retrieve a hypothesis that is entailed by the premise\nRetrieve semantically similar text\nMS-MARCO\nGiven a web search query, retrieve relevant passages that answer the query\nGiven a web search query, retrieve relevant documents that answer the query\nQuora Duplicates\nGiven a question, retrieve questions that are semantically equivalent to the given\nquestion\nFind questions that have the same meaning as the input question\nDuReader / T2Ranking\nGiven a Chinese search query, retrieve web passages that answer the question\n18\nTable 14: Instructions used for evaluation on the MTEB benchmark. \u201cSTS*\u201d indicates we use the\nsame instructions for all the STS tasks.\nTask Name\nInstruction\nAmazonCounterfactualClassif.\nClassify a given Amazon customer review text as either counterfactual or not-\ncounterfactual\nAmazonPolarityClassification\nClassify Amazon reviews into positive or negative sentiment\nAmazonReviewsClassification\nClassify the given Amazon review into its appropriate rating category\nBanking77Classification\nGiven a online banking query, find the corresponding intents\nEmotionClassification\nClassify the emotion expressed in the given Twitter message into one of the six\nemotions: anger, fear, joy, love, sadness, and surprise\nImdbClassification\nClassify the sentiment expressed in the given movie review text from the IMDB dataset\nMassiveIntentClassification\nGiven a user utterance as query, find the user intents\nMassiveScenarioClassification\nGiven a user utterance as query, find the user scenarios\nMTOPDomainClassification\nClassify the intent domain of the given utterance in task-oriented conversation\nMTOPIntentClassification\nClassify the intent of the given utterance in task-oriented conversation\nToxicConversationsClassif.\nClassify the given comments as either toxic or not toxic\nTweetSentimentClassification\nClassify the sentiment of a given tweet as either positive, negative, or neutral\nArxivClusteringP2P\nIdentify the main and secondary category of Arxiv papers based on the titles and\nabstracts\nArxivClusteringS2S\nIdentify the main and secondary category of Arxiv papers based on the titles\nBiorxivClusteringP2P\nIdentify the main category of Biorxiv papers based on the titles and abstracts\nBiorxivClusteringS2S\nIdentify the main category of Biorxiv papers based on the titles\nMedrxivClusteringP2P\nIdentify the main category of Medrxiv papers based on the titles and abstracts\nMedrxivClusteringS2S\nIdentify the main category of Medrxiv papers based on the titles\nRedditClustering\nIdentify the topic or theme of Reddit posts based on the titles\nRedditClusteringP2P\nIdentify the topic or theme of Reddit posts based on the titles and posts\nStackExchangeClustering\nIdentify the topic or theme of StackExchange posts based on the titles\nStackExchangeClusteringP2P\nIdentify the topic or theme of StackExchange posts based on the given paragraphs\nTwentyNewsgroupsClustering\nIdentify the topic or theme of the given news articles\nSprintDuplicateQuestions\nRetrieve duplicate questions from Sprint forum\nTwitterSemEval2015\nRetrieve tweets that are semantically similar to the given tweet\nTwitterURLCorpus\nRetrieve tweets that are semantically similar to the given tweet\nAskUbuntuDupQuestions\nRetrieve duplicate questions from AskUbuntu forum\nMindSmallReranking\nRetrieve relevant news articles based on user browsing history\nSciDocsRR\nGiven a title of a scientific paper, retrieve the titles of other relevant papers\nStackOverflowDupQuestions\nRetrieve duplicate questions from StackOverflow forum\nArguAna\nGiven a claim, find documents that refute the claim\nClimateFEVER\nGiven a claim about climate change, retrieve documents that support or refute the claim\nCQADupstackRetrieval\nGiven a question, retrieve detailed question descriptions from Stackexchange that are\nduplicates to the given question\nDBPedia\nGiven a query, retrieve relevant entity descriptions from DBPedia\nFEVER\nGiven a claim, retrieve documents that support or refute the claim\nFiQA2018\nGiven a financial question, retrieve user replies that best answer the question\nHotpotQA\nGiven a multi-hop question, retrieve documents that can help answer the question\nMSMARCO\nGiven a web search query, retrieve relevant passages that answer the query\nNFCorpus\nGiven a question, retrieve relevant documents that best answer the question\nNQ\nGiven a question, retrieve Wikipedia passages that answer the question\nQuoraRetrieval\nGiven a question, retrieve questions that are semantically equivalent to the given\nquestion\nSCIDOCS\nGiven a scientific paper title, retrieve paper abstracts that are cited by the given paper\nSciFact\nGiven a scientific claim, retrieve documents that support or refute the claim\nTouche2020\nGiven a question, retrieve detailed and persuasive arguments that answer the question\nTRECCOVID\nGiven a query on COVID-19, retrieve documents that answer the query\nSTS*\nRetrieve semantically similar text.\nBUCC/Tatoeba\nRetrieve parallel sentences.\nSummEval\nGiven a news summary, retrieve other semantically similar summaries\n19\nTable 15: Results for each dataset in the MTEB benchmark. The evaluation metrics and detailed\nbaseline results are available in the original paper [28].\nDataset\nw/ synthetic only\nw/ synthetic + msmarco\nw/o synthetic data\nfull data\nBIOSSES\n84.2\n81.0\n85.4\n85.5\nSICK-R\n78.6\n78.5\n81.7\n82.6\nSTS12\n75.8\n74.7\n77.9\n79.7\nSTS13\n84.3\n85.3\n88.0\n88.4\nSTS14\n80.9\n81.2\n83.7\n84.5\nSTS15\n86.2\n86.8\n89.5\n90.4\nSTS16\n85.0\n85.3\n86.5\n87.7\nSTS17\n87.3\n87.7\n91.0\n91.8\nSTS22\n66.0\n67.1\n66.2\n67.0\nSTSBenchmark\n83.5\n84.0\n87.8\n88.6\nSummEval\n31.9\n32.7\n31.9\n31.4\nSprintDuplicateQuestions\n93.5\n95.8\n96.0\n95.7\nTwitterSemEval2015\n78.0\n78.5\n81.7\n81.6\nTwitterURLCorpus\n86.5\n86.9\n87.7\n87.8\nAmazonCounterfactualClass.\n79.6\n79.9\n77.2\n78.7\nAmazonPolarityClassification\n95.8\n95.9\n93.9\n95.9\nAmazonReviewsClassification\n56.9\n55.5\n48.2\n55.8\nBanking77Classification\n86.2\n87.0\n88.8\n88.2\nEmotionClassification\n49.2\n47.6\n51.0\n49.8\nImdbClassification\n94.8\n94.9\n89.0\n94.8\nMassiveIntentClassification\n79.8\n79.9\n79.6\n80.6\nMassiveScenarioClassification\n81.7\n82.4\n82.3\n82.4\nMTOPDomainClassification\n95.6\n95.9\n95.7\n96.1\nMTOPIntentClassification\n84.9\n85.9\n83.4\n86.1\nToxicConversationsClassification\n70.2\n70.8\n70.9\n69.6\nTweetSentimentExtractionClass.\n63.5\n63.4\n61.6\n63.7\nAskUbuntuDupQuestions\n64.3\n65.3\n67.4\n67.0\nMindSmallReranking\n33.1\n32.8\n32.5\n32.6\nSciDocsRR\n86.0\n86.0\n85.7\n86.3\nStackOverflowDupQuestions\n52.5\n53.7\n55.9\n54.9\nArxivClusteringP2P\n51.4\n51.2\n47.8\n50.5\nArxivClusteringS2S\n46.5\n44.9\n44.6\n45.5\nBiorxivClusteringP2P\n44.5\n43.3\n36.9\n43.5\nBiorxivClusteringS2S\n40.9\n40.1\n37.0\n40.2\nMedrxivClusteringP2P\n40.5\n39.9\n32.6\n38.2\nMedrxivClusteringS2S\n38.0\n37.9\n32.8\n37.5\nRedditClustering\n56.3\n55.9\n63.1\n57.7\nRedditClusteringP2P\n66.3\n64.8\n66.4\n66.5\nStackExchangeClustering\n72.9\n72.7\n74.5\n73.1\nStackExchangeClusteringP2P\n46.1\n45.6\n34.3\n45.9\nTwentyNewsgroupsClustering\n52.2\n52.5\n55.6\n54.3\nArguAna\n52.2\n42.7\n62.5\n61.9\nClimateFEVER\n21.1\n28.8\n25.2\n38.4\nCQADupstackAndroidRetrieval\n40.8\n36.0\n44.5\n43.0\nDBPedia\n42.0\n43.7\n47.7\n48.9\nFEVER\n72.5\n83.5\n73.1\n87.8\nFiQA2018\n38.1\n48.4\n54.5\n56.6\nHotpotQA\n48.1\n64.0\n75.6\n75.7\nMSMARCO\n25.7\n45.0\n42.9\n43.1\nNFCorpus\n35.5\n40.0\n35.3\n38.6\nNQ\n53.3\n63.5\n57.3\n63.5\nQuoraRetrieval\n75.0\n79.5\n89.5\n89.6\nSCIDOCS\n20.6\n15.8\n19.0\n16.3\nSciFact\n71.5\n71.9\n74.7\n76.4\nTouche2020\n25.4\n32.5\n19.1\n26.4\nTRECCOVID\n82.3\n87.3\n70.8\n87.2\nAverage\n63.1\n64.5\n64.6\n66.6\n20\n"
  },
  {
    "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
    "link": "https://arxiv.org/pdf/2401.00448.pdf",
    "upvote": "25",
    "text": "Beyond Chinchilla-Optimal:\nAccounting for Inference in Language Model Scaling\nLaws\nNikhil Sardana\u2217\nMosaicML\nnikhil@mosaicml.com\nJonathan Frankle\nMosaicML\njonathan@mosaicml.com\nAbstract\nLarge language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular DeepMind Chinchilla scaling\nlaws, neglect to include the cost of inference. We modify the Chinchilla scaling\nlaws to calculate the optimal LLM parameter count and pre-training data size to\ntrain and deploy a model of a given quality and inference demand. We conduct our\nanalysis both in terms of a compute budget and real-world costs and find that LLM\nresearchers expecting reasonably large inference demand (~1B requests) should\ntrain models smaller and longer than Chinchilla-optimal.\n1\nIntroduction\nLarge language models (LLM) have substantial training and inference compute and energy costs\n[6, 12]. Training computation costs are primarily determined by the size of the model and the amount\nof data it sees during training [4]. State-of-the-art models have tens of billions of parameters and are\ntrained on trillions of tokens [17]. Inference costs depend on the size of the model and the volume\nof user queries over the lifetime of the model. This volume can be significant: Demand for popular\nmodels can exceed billions of tokens per day [11, 15].\nAccounting for both training and inference, how does one minimize the cost required to produce and\nserve a high quality model?\nSignificant prior research has been conducted to find scaling laws, empirical formulas that estimate\nhow changes in model and training data size impact model quality [5, 4]. Hoffmann et al. [4] is\nperhaps the most influential of these works; finding that to scale language models most efficiently,\nparameters and tokens should grow approximately equally. The authors applied this scaling law to\ntrain a 70B parameter model, Chinchilla, that outperformed much larger, more expensive models,\nincluding GPT-3. Subsequent LLMs have been trained following the Chinchilla scaling laws [2, 9].\nHowever, the Chinchilla scaling laws only account for the computational costs of training. By\ncontrast, the LLaMA and LLaMA-2 family of models were trained on 1-2 trillion tokens, far more\ndata than the Chinchilla scaling laws would deem \u201coptimal\u201d [16, 17]. Since inference costs are\nlower for smaller models, the extra training compute required to train a LLaMA-style model over a\nChinchilla-style model of equivalent quality pays off after enough inference requests.\nPrior work has discussed the training-inference compute trade-off [16, 17, 18, 1, 19]. Touvron et al.\n[16] cites the lower inference cost of smaller models as inspiration for the LLaMA series. De Vries\n[1] calculates the compute overhead of training longer than Chinchilla, but does not discuss quantify\n\u2217Corresponding author.\n3rd NeurIPS Workshop on Efficient Natural Language and Speech Processing (ENLSP), 2023.\narXiv:2401.00448v1  [cs.LG]  31 Dec 2023\n(a)\n(b)\n(c)\nFigure 1: Ratios of (a) total FLOPs, (b) model parameters, and (c) pre-training tokens, for optimal\nmodels estimated via our method vs. Chinchilla-style models. For each point (x, y) in the figures, we\ncompute the Chinchilla model parameter count and training data required to reach the loss y, and the\nnumber of combined FLOPs required to train and run inference for x tokens using the Chinchilla\nmodel. Then, we compute the same values (total FLOPs, parameter count, training data size) for the\ncompute-optimal models returned by our method, and plot the ratios.\ncompute savings from inference. Recently, Villalobos and Atkinson [19] discusses this trade-off in\nmore detail, but shows the shift in scaling laws for only a single particular number of inferences.\nIn this paper, we modify Chinchilla scaling laws to account for inference costs, calculating the\noptimal parameter and training token counts\u2014both in terms of compute and dollar costs\u2014to train\nand deploy a model of any given quality and inference demand. Our principled derivation estimates\nthat LLM practitioners expecting significant demand (~109 inference requests) should train models\nsubstantially smaller and longer than Chinchilla-optimal.\n2\nComputational Optimality\nWe seek to minimize the computational costs of a model of a given quality and inference demand.\nWe closely follow the methodology in Hoffmann et al. [4] (henceforth referred to as \u201cthe Chinchilla\npaper\u201d), using pre-training cross-entropy loss as a proxy for quality, and floating-point operations\n(FLOPs) as our unit of computational cost.\nWe model our pre-training loss L(N, Dtr) in terms of the number of parameters, N, and pre-training\ntokens, Dtr, according to the Chinchilla paper\u2019s third scaling law:\nL(N, Dtr) \u225c E + A\nN \u03b1 + B\nD\u03b2\ntr\n(1)\nThe Chinchilla paper derived the parametric loss function in Eq. 1 and fit values for A, B, E, \u03b1, and\n\u03b2 from the authors\u2019 empirical training results. The best-fit values for these constants depend on the\nexact dataset and model architecture; however, the Chinchilla paper found largely consistent results\nacross the MassiveText, Github [13], and C4 [14] datasets, and subsequent work has replicated these\nscaling laws on other internet corpora and transformer variants [2]. Thus, we use the constant values\nfrom the Chinchilla paper in our analysis.\nAdditionally, we assume that conditioned on pre-training loss, inference demand is independent of\nmodel size and token count. In other words, models of equivalent quality but different parameter\ncounts will see the same requests.2\nLet TFLOPs(N, D) and IFLOPs(N, D) be the number of FLOPs required to train and run inference,\nrespectively, on a model with N parameters for D tokens. Denote the number of tokens (input +\noutput) of a single inference request i as D(i)\ninf . Let Dinf = P\ni D(i)\ninf be the sum of all tokens over all\ninference requests.\n2In practice, smaller models of equivalent quality may have greater demand since they can have lower\ninference latency.\n2\nFormally, we are interested in minimizing the sum of our training and inference FLOPs under the\nconstraint L(N, Dtr) = \u2113:\nN \u2217(\u2113, Dinf), D\u2217\ntr(\u2113, Dinf) =\narg min\nN,Dtr|L(N,Dtr)=\u2113\nTFLOPs(N, Dtr) +\nX\ni\nIFLOPs(N, D(i)\ninf ).\n(2)\nN \u2217 and D\u2217\ntr are functions that describe the optimal parameters and pre-training tokens, respectively,\nthat minimize total training and inference compute. The pre-training loss constraint ensures that we\nminimize compute for a given quality.\nWe use the standard approximation of FLOPs for transformer models with N parameters: 6N per\ntraining token and 2N per inference token [5]. Thus, our objective simplifies to:\nN \u2217(\u2113, Dinf), D\u2217\ntr(\u2113, Dinf) =\narg min\nN,Dtr|L(N,Dtr)=\u2113\n6NDtr + 2NDinf.\n(3)\nWe note that this is the \u201cconverse\u201d of the Chinchilla optimization problem. In the Chinchilla paper,\nthe authors assumed a fixed compute budget and found N \u2217 and D\u2217\ntr that minimized pre-training loss.\nOur objective is to fix pre-training loss and find N \u2217 and D\u2217\ntr that minimize compute costs.\nCrucially, our total computational cost depends on the inference demand over the lifetime of the\nmodel, but our model\u2019s parameter count and data size are determined prior to training. Thus, our\nanalysis is predicated on the assumption that LLM practitioners can estimate their inference demand\nprior to training.\nWithout inference (Dinf = 0), the optimization problem in Eq. 3 can be solved analytically. Unfortu-\nnately, accounting for inference (Dinf > 0), determining N \u2217 and D\u2217\ntr analytically as functions of \u2113\nand Dinf is intractable (we defer our proof to Appendix A). Instead, we computationally solve for N \u2217\nand D\u2217\ntr across a range of values of \u2113 and Dinf using the Newton root-finding method. In practice, this\nmethod converges for relevant inputs and we are able to determine optimal parameter/token counts.\nIn Figure 1, we show how our inference-adjusted model\u2019s FLOP counts, parameters, and pre-training\ntokens compare to Chinchilla-style models across a range of loss values and inference demands.\nWhen inference usage is significantly less than the number of pre-training tokens, Chinchilla models\nare essentially compute-optimal. However, as demand increases, inference costs becomes a significant\nfactor. For a Chinchilla-7B-quality model with an inference demand of 1011 tokens, our formula\nsuggests the compute-optimal method is to train a 6B parameter model on 1.18\u00d7 the original data.\nFor higher quality (i.e. larger and longer) models, the volume of inference demand required to shift\nthe scaling law increases: An LLM developer that expects a 30B-Chinchilla-quality model will see\n1013 tokens during inference can reduce their total FLOPs by 28% by training a 13.6B model on\n2.84\u00d7 the data. We provide additional results in Sec. B.1 in the Appendix.\n3\nEstimating Real-World Cost Optimality\nOptimizing purely for minimum FLOPs has significant drawbacks which limit the applicability of\nour analysis in Section 2 to real-world deployments. The real-world cost of an inference request of\n3D tokens is generally different than the cost to train on D tokens. For instance, inference hardware\nutilization can be much lower than training utilization, since small batch size computation can result\nin low Model FLOPs Utilization (MFU). MFU can be as low as ~1% for inference [12] but is typically\n40-60% during training [7]. Utilization is also different for input tokens vs. output tokens \u2014 since\ninput tokens (prompts) are typically processed in a single forward pass, utilization is typically near\ntraining levels. By contrast, during generation, output tokens must be produced sequentially, resulting\nin low utilization due to memory bandwidth constraints. Another complicating factor is that inference\noperations can sometimes be cheaper than training FLOPs, since models can be quantized before\ninference time, turning 16- or 32-bit floating-point operations into 4- or 8-bit integer operations which\nrun more efficiently on the same hardware. Quantization can also enable LLMs to fit on GPUs with\nless VRAM, so training and inference may occur on different hardware altogether [3].\nTo estimate the real-world cost of inference, we modify Eq. 2 to account for hardware utilization:\nMFUtr, MFUinp, and MFUout are our training, inference input, and inference output MFUs, respec-\ntively. In addition, we add parameters for training and inference cost per FLOP, Ctr and Cinf. Our\n3\n(a)\n(b)\n(c)\nFigure 2: Ratios of (a) total cost, (b) model parameters, and (c) pre-training tokens, for cost-optimal\nmodels via our real-world estimation method vs. Chinchilla-style models. Results in this figure\nare shown with the following settings: training with 50% MFU, inference input with 50% MFU,\ngeneration with 1% MFU. Inference requests have 70 input tokens and 215 output tokens each,\naligning with averages from real-world data [21]. To mimic a realistic scenario, we calculate costs\nassuming training occurs on A100-80GB and inference occurs on A100-40GB accelerators after\nINT8 quantization (see Sec. B.3 for details).\nnew objective is:\nN \u2217(\u2113, Dinp, Dout)\u2217(\u2113, Dinp, Dout) =\narg min\nN,D|L(N,Dtr)=\u2113\n\"\nCtr\nMFUtr\nTFLOPs(N, Dtr)\n(4)\n+\nX\ni\nCinf\nMFUinp\nIFLOPs(N, D(i)\ninp) +\nX\ni\nCinf\nMFUout\nIFLOPs(N, D(i)\nout)\n#\n.\n(5)\nWe again use the approximations for FLOPs for transformer models, reducing the above equation to:\nN \u2217(\u2113, Dinp, Dout), D\u2217\ntr(\u2113, Dinp, Dout) =\narg min\nN,Dtr|L(N,Dtr)=\u2113\n6NDtrCtr\nMFUtr\n+ 2NCinf\n\u0014\nDinp\nMFUinp\n+\nDout\nMFUout\n\u0015\n(6)\nEq. 6 is a simplified model of real-world costs: we leave aside latency requirements and assume\nMFU and cost per FLOP do not depend on model size, configuration, or sequence length. Still, our\napproximation is flexible enough to account for heterogeneous hardware utilization and costs.\nIn Figure 2, we show how inference-adjusted cost-optimal models compare to Chinchilla-style\nmodels, assuming typical training and inference hardware costs and MFU. For a 30B-Chinchilla-\nquality model, LLM practitioners expecting 1.5B inference requests can reduce costs by 17% by\ninstead training a 16B model on 3.35T tokens. In Sec. B.2, we show further results for various\nconfigurations.\nComparing our compute-optimal analysis in Fig. 1 to our real-world cost analysis in Fig. 2, we see\nthat for the same inference demand of 2T tokens (7.02B requests), a Chinchilla-70B model requires\nonly 1.3% extra FLOPs compared to an equal-quality compute-optimal model, but costs 36% more\nthan a cost-optimal model. This difference is attributable to the 50\u00d7 lower MFU of each inference\noutput token compared to training, which our FLOP-based analysis in Sec. 2 fails to capture.\n4\nConclusion\nIn this work, we modify the Chinchilla scaling laws to account for both the computational and\nreal-world costs of inference. As inference demand approaches pre-training data size, the additional\ncost pushes the optimal parameters-to-tokens ratio towards smaller and longer-trained models.\nWe make strong assumptions about the Chinchilla scaling laws and our analysis only applies insofar\nas these laws hold true. Further work is needed to experimentally validate our formulas and determine\nif scaling laws apply in the extreme ranges, where pre-training tokens exceed model parameters by\norders of magnitudes.\n4\nAcknowledgements\nWe thank Sasha Doubov for helpful discussions and Daya Khudia, Mihir Patel, and Linden Li for\ntheir feedback on the manuscript.\nReferences\n[1] H. De Vries. Go smol or go home, 2023. URL https://www.harmdevries.com/post/\nmodel-size-vs-compute-overhead/.\n[2] N. Dey, G. Gosal, Zhiming, Chen, H. Khachane, W. Marshall, R. Pathria, M. Tom, and\nJ. Hestness. Cerebras-gpt: Open compute-optimal language models trained on the cerebras\nwafer-scale cluster, 2023.\n[3] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers, 2023.\n[4] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre.\nTraining compute-optimal large language models, 2022.\n[5] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,\nJ. Wu, and D. Amodei. Scaling laws for neural language models, 2020.\n[6] W. Knight.\nOpenai\u2019s\nceo says the age\nof giant ai\nmodels is already\nover.\nWired,\n2023.\nISSN\n1059-1028.\nURL\nhttps://www.wired.com/story/\nopenai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/.\n[7] V. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro.\nReducing activation recomputation in large transformer models, 2022.\n[8] L. Labs.\nGpu cloud - vms for deep learning.\nhttps://lambdalabs.com/service/\ngpu-cloud, 2023. Accessed 2023-10-02.\n[9] N. Muennighoff, A. M. Rush, B. Barak, T. L. Scao, A. Piktus, N. Tazi, S. Pyysalo, T. Wolf, and\nC. Raffel. Scaling data-constrained language models, 2023.\n[10] NVIDIA.\nNvidia\na100\ndatasheet,\n2021.\nURL\nhttps://www.\nnvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/\nnvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf.\n[11] OpenAI and A. Pilipiszyn.\nGpt-3 powers the next generation of apps, Mar 2021.\nURL\nhttps://openai.com/blog/gpt-3-apps.\n[12] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,\nS. Agrawal, and J. Dean. Efficiently scaling transformer inference, 2022.\n[13] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson,\nR. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den\nDriessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang,\nJ. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar,\nE. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L.\nLi, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B.\nLespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong,\nD. Toyama, C. de Masson d\u2019Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark,\nD. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger,\nI. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub,\nJ. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models:\nMethods, analysis & insights from training gopher, 2022.\n[14] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n5\n[15] N. Shazeer and D. d. Freitas.\nIntroducing character, Dec 2022.\nURL https://blog.\ncharacter.ai/introducing-character/.\n[16] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and\nefficient foundation language models, 2023.\n[17] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[18] J.\nTow,\nD.\nM.\nMarco\nBellagente,\nand\nC.\nR.\nRuiz.\nTechnical\nreport\nfor\nstablelm-3b-4e1t.\nhttps://stability.wandb.io/stability-llm/\nstable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=\nu3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo,\n2023.\nAccessed 02-10-2023.\n[19] P. Villalobos and D. Atkinson. Trading off compute in training and inference, 2023. URL https:\n//epochai.org/blog/trading-off-compute-in-training-and-inference.\nAc-\ncessed: 2023-9-26.\n[20] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and\nefficient post-training quantization for large language models, 2023.\n[21] L. Zheng, W.-L. Chiang, Y. Sheng, T. Li, S. Zhuang, Z. Wu, Y. Zhuang, Z. Li, Z. Lin, E. P.\nXing, J. E. Gonzalez, I. Stoica, and H. Zhang. Lmsys-chat-1m: A large-scale real-world llm\nconversation dataset, 2023.\n6\nA\nNo Analytic Solution for Inference-Compute Optimality\nIn this section, we prove that it is not possible to analytically derive the optimal model size and\npre-training token count according to the third Chinchilla law, after accounting for the computational\ncost of inference. Conditioned on model quality, we assume that inference demand does not depend\non model size and can be estimated prior to training.\nTheorem A.1. Given a fixed model quality and inference demand, there exists no analytic solution\nfor the compute-optimal model size and pre-training token count according to the third Chinchilla\nlaw, after accounting for the computational cost of inference.\nProof. By Eq. 3, the overall compute cost in FLOPs for training a model with N parameters on Dtr\ntokens and running inference on Dinf tokens is given by C(N, Dtr, Dinf) = 6NDtr + 2NDinf.\nWe seek the minimum overall compute budget to train and deploy a model of a given quality and\ninference demand. Formally, we optimize the objective:\nmin C(N, Dtr, Dinf)\n(7)\nsubject to the constraint L(N, Dtr) = E +\nA\nN \u03b1 +\nB\nD\u03b2\ntr = \u2113.\nThis constraint, from the third Chinchilla law, ensures we are minimizing compute while fixing\nmodel quality (pre-training loss). A = 406.4, B = 410.7, E = 1.69, \u03b1 = 0.336, and \u03b2 = 0.283 are\nconstants determined empirically by Hoffmann et al. [4].3\nWe solve this optimization problem via the method of Lagrange multipliers. The gradients are:\n\u2207C(N, Dtr) = (6D + 2Dinf)\u02c6i + 6N\u02c6j\n(8)\n\u2207L(N, Dtr) = \u2212\u03b1AN \u2212\u03b1\u22121\u02c6i \u2212 \u03b2BD\u2212\u03b2\u22121\ntr\n\u02c6j\n(9)\nWe have three equations and three variables (Dtr, N, \u03bb), where \u03bb is our Lagrange multiplier:\n6Dtr + 2Dinf = \u2212\u03bb\u03b1AN \u2212\u03b1\u22121\n6N = \u2212\u03bb\u03b2BD\u2212\u03b2\u22121\ntr\nE + A\nN \u03b1 + B\nD\u03b2\ntr\n= \u2113\n(10)\nWith some algebraic manipulation, we can eliminate \u03bb and write\nA\nN \u03b1 in terms of Dtr:\nA\nN \u03b1 = 3\u03b2BD\u2212\u03b2\ntr\n+ Dinf\u03b2BD\u2212\u03b2\u22121\ntr\n3\u03b1\n(11)\nWe are left to solve the following equation for Dtr:\n0 = (E \u2212 \u2113) +\nh\u03b2B\n\u03b1 + B\ni\nD\u2212\u03b2\ntr\n+ Dinf\u03b2B\n3\u03b1\nD\u2212\u03b2\u22121\ntr\n(12)\nThus, determining Dtr as a function of Dinf and \u2113 involves finding the roots of equations of the form\nax\u22121.283 + 756.6x\u22120.283 + c = 0 for arbitrary a and c > 0, which is not possible in general.\nB\nFurther Results\nB.1\nCompute-Optimal Results\nWe present further results from our analysis in Sec. 2. In Table 1, we show the computational cost (in\nFLOPs) to train and run inference for Chinchilla-style models of various sizes and inference demands.\nWe then calculate the compute-optimal model configuration to reach the same quality (equal loss)\nand run inference, and note the overall compute reduction.\n3The Chinchilla paper reports \u03b1 = 0.34 and \u03b2 = 0.28. However, these are rounded values; to better fit the\nresults reported in Table A.3 of Hoffmann et al. [4], we use \u03b1 = 0.336 and \u03b2 = 0.283, as in De Vries [1].\n7\nTable 1: Compute-Optimal vs. Chinchilla-style Models for Selected Configurations.\nChinchilla Model\nCompute-Optimal Model\nInference\nTrain\nTraining\nTraining\nFLOP\nTokens\nLoss\nParams\nTokens\nFLOPs\nParams\nTokens\nFLOPs\nReduction\n50B\n2.53\n1B\n27.4B\n2.64e20\n6.33M\n46.8B\n2.41e20\n9.1%\n200B\n2.13\n7B\n276B\n1.44e22\n5.4B\n367B\n1.40e22\n2.6%\n1T\n2.05\n13B\n577B\n7.10e22\n8.32B\n967B\n6.49e22\n8.5%\n5T\n1.96\n30B\n1.56T\n5.80e23\n16.4B\n3.27T\n4.86e23\n16%\n10T\n1.89\n70B\n4.26T\n3.19e24\n41.6B\n7.92T\n2.81e24\n12%\nTable 2: Cost-Optimal vs. Chinchilla-style Models for Selected Configurations.\nChinchilla Model\nCost-Optimal Model\nInference\nTrain\nTraining\nTotal\nTraining\nTotal\nCost\nRequests\nLoss\nParams\nTokens\nCost ($)\nParams\nTokens\nCost ($)\nSavings\n175M\n2.53\n1B\n27.4B\n3.77K\n327M\n152B\n1.89K\n50%\n702M\n2.13\n7B\n276B\n124K\n2.90B\n929B\n81.8K\n34%\n3.51B\n2.05\n13B\n577B\n987K\n430B\n3.1T\n500K\n49%\n17.5B\n1.96\n30B\n1.56T\n10.8M\n8.58B\n12.1T\n4.52M\n58%\n35.1B\n1.89\n70B\n4.26T\n51.5M\n21.5B\n27T\n23.8M\n54%\nB.2\nCost-Optimal Results\nWe show additional results from our cost-optimality analysis in Sec. 3. In Table 2, we show the total\ntraining plus inference costs for Chinchilla models of various sizes at different levels of inference\ndemands. We then calculate costs for equivalent-quality (i.e. same pre-training loss) cost-optimal\nmodels and show the overall savings. We use the same settings from Figure 2, designed to mimic a\ntypical real-world deployment: training and inference input at 50% MFU, generation at 1% MFU\n[7, 12]. Each inference request has 70 input tokens and 215 output tokens, in accordance with\naverages from the LMSYS-Chat dataset of 1M inference requests from Zheng et al. [21]. Costs are\ncalculated assuming training and inference on A100-80GB and A100-40GB accelerators, respectively.\nWe further assume the model parameters are quantized to eight-bit integers prior to inference, which\nis commonly done with no quality reduction [20]. All costs are reported in US dollars.\nB.3\nGPU Details\nGPU pricing varies based on vendor and fluctuates over time. At the time of writing, an A100-40GB\ncosts USD $1.10/hr and an A100-80GB costs $1.50/hr on Lambda Labs [8]. We use these values in\nour cost analysis in Sec. 3 and in Table 2. Both variants have a peak performance of 3.12 \u00d7 1014\ndense FP16/BF16 operations and 6.24 \u00d7 1014 INT8 operations per second [10].\n8\n"
  },
  {
    "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models",
    "link": "https://arxiv.org/pdf/2401.00788.pdf",
    "upvote": "21",
    "text": "ASTRAIOS: Parameter-Efficient Instruction Tuning\nCode Large Language Models\nTerry Yue Zhuo 1,2\nArmel Zebaze 3\u2217\nNitchakarn Suppattarachai 1\nLeandro von Werra 3\nHarm de Vries 4\nQian Liu 5\nNiklas Muennighoff 6\n1 Monash University\n2 CSIRO\u2019s Data61\n3 Hugging Face\n4 ServiceNow Research\n5 Sea AI Lab\n6 Contextual AI\nterry.zhuo@monash.edu\n\u0087 https://github.com/bigcode-project/astraios\nAbstract\nThe high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods. How-\never, it remains unclear which methods provide the best cost-performance trade-off\nat different model scales. We introduce ASTRAIOS, a suite of 28 instruction-tuned\nOctoCoder models using 7 tuning methods and 4 model sizes up to 16 billion\nparameters. Through investigations across 5 tasks and 8 different datasets encom-\npassing both code comprehension and code generation tasks, we find that FFT\ngenerally leads to the best downstream performance across all scales, and PEFT\nmethods differ significantly in their efficacy based on the model scale. LoRA\nusually offers the most favorable trade-off between cost and performance. Further\ninvestigation into the effects of these methods on both model robustness and code\nsecurity reveals that larger models tend to demonstrate reduced robustness and\nless security. At last, we explore the relationships among updated parameters,\ncross-entropy loss, and task performance. We find that the tuning effectiveness\nobserved in small models generalizes well to larger models, and the validation loss\nin instruction tuning can be a reliable indicator of overall downstream performance.\n1B\n3B\n7B\n16B\n0\n5\n10\n15\n20\n25\n30\n  (IA)3\n 0.01%\n LoRA\n 0.21%\nP\nTuning\n 0.82%\nAdapterP\n 1.46%\nParallel\n 1.52%\nAdapterH\n 2.87%\n  FFT\n 100%\nFigure 1: Mean task performance of ASTRAIOS models across 5 representative tasks and 8 datasets.\nWe indicate the average percentage of total parameters updated for each PEFT method.\n\u2217The work was partially done at Hugging Face.\narXiv:2401.00788v1  [cs.CL]  1 Jan 2024\n1\nIntroduction\nLarge language models (LLMs) (Zhao et al., 2023) trained on Code (Code LLMs) have shown\nstrong performance on various software engineering tasks (Hou et al., 2023). There are three main\nmodel paradigms: (A) Code LLMs for code completion (Nijkamp et al., 2022; Fried et al., 2022; Li\net al., 2023); (B) Task-specific fine-tuned Code LLMs for a single task (Hou et al., 2023); and (C)\nInstruction-tuned (Ouyang et al., 2022) Code LLMs that excel at following human instructions and\ngeneralizing well on unseen tasks (Wang et al., 2023b; Muennighoff et al., 2023c). Recent instruction-\ntuned Code LLMs, including WizardCoder (Luo et al., 2023) and OctoCoder (Muennighoff et al.,\n2023a), have achieved state-of-the-art performance on various tasks without task-specific fine-tuning.\nHowever, with the increasing parameters of Code LLMs, it becomes more expensive to perform full-\nparameter fine-tuning (FFT) to obtain instruction-tuned models. In practise, to save computational\ncost, parameter-efficient fine-tuning (PEFT) have been applied. This training strategy aims to\nachieve comparable performance to FFT by updating fewer parameters. While there are many PEFT\nmethods (Ding et al., 2022), the predominant PEFT method is still LoRA, which is proposed in\n2021 (Hu et al., 2021). However, there is no empirical evidence showing LoRA remains the best for\ninstruction-tuned code LLMs. In this paper, we investigate instruction-tuned code LLMs with the\nfollowing research question: what are the best PEFT methods for Code LLMs?\nExisting analysis on PEFT methods presents several opportunities for further exploration: (1) Beyond\nTask-Specific LLMs. Most prior works (Zhou et al., 2022; Ding et al., 2023) only focus on the\nmodel paradigm (B), where the selected base models are fine-tuned on specific downstream tasks.\nWhile these studies provide insights into PEFT methods on task-specific LLMs, the transferability\nof their findings to the instruction tuning paradigm is unclear. (2) Diverse Domains. Studies on\nPEFT methods tend to evaluate in the predominant domains like vision (Sung et al., 2022; He\net al., 2023) and text (Houlsby et al., 2019; He et al., 2021), leaving other domains like code\nunderexplored. (3) Inclusive PEFT Methods. Prior investigations on PEFT mainly consider a\nlimited number of methods, such as adapter-based tuning (Houlsby et al., 2019) or reparametric\ntuning (Aghajanyan et al., 2021), which does not capture the full breadth of available methods. (4)\nMultidimensional Evaluation. Previous works only consider limited evaluation on representative\ndownstream tasks (Chen et al., 2022; Fu et al., 2023; Ding et al., 2023). We argue that other evaluation\ndimensions like model robustness (Han et al., 2021) and output code safety (Weidinger et al., 2021;\nZhuo et al., 2023b; Pearce et al., 2022; Dakhel et al., 2023) are also important, especially in the era\nof LLM agents (Ouyang et al., 2022; Xie et al., 2023). (5) Scalability. Most prior PEFT work has\nonly explored LLMs with insufficient scales of model sizes and training time, which makes their\nscalability questionable (Lester et al., 2021; Chen et al., 2022; Hu et al., 2023).\nTo explore these identified opportunities further, we introduce ASTRAIOS, a suite of 28 instruction-\ntuned Code LLMs, which are fine-tuned with 7 tuning methods based on the StarCoder (Li et al.,\n2023) base models (1B, 3B, 7B, 16B). We instruction-tune the models based on the open-source\ndataset, CommitPackFT from OctoPack (Muennighoff et al., 2023a), to balance their downstream\ncapabilities. We utilize PEFT configurations with Hugging Face\u2019s best practices (Mangrulkar et al.,\n2022) and integrate a few PEFT methods from recent frameworks (Hu et al., 2023). We first inspect\nthe scalability of different tuning methods through the lens of cross-entropy loss during instruction\ntuning. Specifically, we assess the scales of model size and training time. Our main evaluation\nfocuses on 5 representative code tasks, including clone detection (Svajlenko and Roy, 2021), defect\ndetection (Zhou et al., 2019), code synthesis (Muennighoff et al., 2023a), code repair Muennighoff\net al. (2023a), and code explain (Muennighoff et al., 2023a). We further study the tuning methods\nfrom two aspects: model robustness (Wang et al., 2023a) and code security (Pearce et al., 2022). We\nassess how well models can generate code based on the perturbed examples and how vulnerable the\ngenerated code can be.\nThe main experimental results can be found in Figure 1, where we observe that FFT generally leads\nto the best downstream performance across all scales. In addition, we find that PEFT methods differ\nsignificantly in their efficacy depending on the model scale. At 16B parameters, Parallel Adapter (He\net al., 2021) and LoRA (Hu et al., 2021) are the most competitive methods with FFT. Meanwhile, at\n1B parameters, they are both slightly outperformed by P-Tuning and (IA)3. Thus, the choice of the\nPEFT method should be considered along with the model scale at hand. Nevertheless, LoRA usually\noffers the most favourable trade-off between cost and performance.\n2\n1B\n3B\n7B\n16B\n(IA)3\nLoRA\nP-Tuning\nAdapterP\nParallel\nAdapterH\n0.0221\n0.0170\n0.0119\n0.0080\n0.3145\n0.2417\n0.1699\n0.1144\n1.1002\n0.7786\n0.6840\n0.7258\n2.1650\n1.6769\n1.1878\n0.8044\n2.2972\n1.7518\n1.2245\n0.8210\n4.2383\n3.2985\n2.3477\n1.5959\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nUpdated Parameter Ratio\nFigure 2: Percentage (%) of total parameters updated for each PEFT method in ASTRAIOS models.\nMeanwhile, we also observe that larger PEFT Code LLMs perform better on code generation tasks\nwhile they do not show such patterns on code comprehension tasks like clone detection and defect\ndetection. In addition, increasing model size improves generation task performance but exhibits\nvulnerabilities to adversarial examples and biases towards insecure code. Additionally, we investigate\nthe relationships among updated parameters, cross-entropy loss, and task performance. We find that\nthe final loss of small PEFT models can be extrapolated to the larger ones. We also observe strong\ncorrelations between final loss and overall downstream task performance. Although the instruction\ndataset we choose is general and is not directly correlated with the benchmark downstream tasks,\nwe suggest that the performance on such general data can serve as a proxy for the downstream\nperformance.\n2\nThe ASTRAIOS Suite and Benchmark\nIn this section, we document our model choices, training configurations, and evaluations in detail for\neasy reproducing our experimental results in this paper.\n2.1\nModel\nBase Model\nThere are many Code LLMs available that could be a suitable base model. However,\nsome of them are not fully open such as Code-Llama (Roziere et al., 2023), where the training data\nis not discussed. To maximize transparency, we select the StarCoder series as our base models.\nConcretely, four model scales including 1B, 3B, 7B and 16B parameters are selected.\nPEFT Model\nWe focus on three kinds of PEFT methods (Ding et al., 2022): (1) Adapter-based\nTuning (Houlsby et al., 2019): An early approach, which injects small-scale neural modules as\nadapters to LLMs and only tune these adapters for model adaptation. (2) Prompt-based Tuning (Li\nTable 1: Summary of tuning methods and the trainable parameters of different model scales.\nType\nName\n1B\n3B\n7B\n16B\nLow-Rank\nLoRA (Hu et al., 2021)\n3,588,096\n7,372,800\n12,472,320\n17,776,640\nPrompt\nP-Tuning (Liu et al., 2023)\n12,650,496\n23,882,496\n50,466,816\n113,448,960\nAdapter\n(IA)3 (Liu et al., 2022)\n251,904\n516,096\n870,912\n1,239,040\nAdapterH (Houlsby et al., 2019)\n50,331,648\n103,809,024\n176,160,768\n251,658,240\nAdapterP (Pfeiffer et al., 2020)\n25,165,824\n51,904,512\n88,080,384\n125,829,120\nParallel (He et al., 2021)\n26,738,688\n54,263,808\n90,832,896\n128,450,560\nFFT\nFFT\n1,137,207,296\n3,043,311,104\n7,327,263,232\n15,517,456,384\n3\nand Liang, 2021): It wraps the original input with additional context introducing virtual task-specific\ntokens without adding layers of modules like adapters. (3) Intrinsic-rank-based Tuning (Aghajanyan\net al., 2021): A representative method is LoRA, which assumes that the change of weights during\nmodel tuning has a low rank and thus low-rank changes to the matrices suffice. For all methods,\nwe utilize the implementations in the open-source PEFT library2 (Mangrulkar et al., 2022) and the\nLLM-Adapters work (Hu et al., 2023) built on top of it. We benchmark 6 PEFT methods, including\n4 adapter-based, 1 prompt-based, and 1 intrinsic-rank-based tuning methods as shown in Table 1.\nWe also include FFT for each model size. The ratio of updated parameters of each PEFT method is\npresented in Figure 2.\n2.2\nInstruction Tuning\nDataset\nFollowing previous work, we select the dataset CommitPackFT+OASST from Oc-\ntoPack (Muennighoff et al., 2023a) as the instruction tuning dataset, which helps StarCoder to\nachieve superior performance. We note that there could be other choices by utilizing other datasets\n(e.g., the publicly available dataset CodeAlpaca (Chaudhary, 2023)) . However, they usually focus on\na certain aspect of code-related tasks and lack generality to different tasks.\nConfiguration\nWe train all models with a sequence length of 2048 tokens, with the batch size as 1,\nthe warm-up step as 12, and the global steps as 200. We set the learning rate as 1 \u00d7 10\u22124 for PEFT\nmodels and 1 \u00d7 10\u22126 FFT models with a cosine scheduler in both cases. For PEFT methods, we use\n8-bit-quantized models during training (Dettmers et al., 2022).\n2.3\nEvaluation\nCode Comprehension\nTo evaluate code comprehension, we select two representative tasks: clone\ndetection and defect detection. Clone detection aims to identify segments of code that are either exact\nduplicates or structurally similar with variations in identifiers, literals, types, layout, and comments,\nor even more broadly similar in terms of functionality. Defect detection targets for identifying bugs,\nvulnerabilities, or antipatterns in code. We select two widely-used datasets from CodeXGLUE\nbenchmark Lu et al. (2021): BigCloneBench (Svajlenko and Roy, 2021) and Devign (Zhou et al.,\n2019). As the original BigCloneBench and Devign are designed to evaluate classification models, we\nprepend additional instructions to prompt the instruction-tuned models to complete such tasks. We\nfollow the evaluation settings of CodeXGLUE and use F1 and Accuracy for BigClone and Devign,\nrespectively. Due to the non-trivial number of test examples in these two datasets, we sample 2,000\nfrom each to save costs. As BigCloneBench and Devign are in the binary classification tasks, we use\ntemperature 0 for model inference to get deterministic outputs.\nCode Generation\nWe use HumanEvalPack (Muennighoff et al., 2023a), a benchmark recently\nproposed that enables easy evaluation of instruction-tuned Code LLMs. The benchmark is structured\naround three core tasks in code generation, each designed to test different capabilities of the model.\nThe first task, Code Synthesis, involves the model in synthesizing functional code given a function\nwith a docstring detailing the desired code behavior. The second task, Code Repair, challenges the\nmodel to identify and fix a subtle bug in an otherwise correct code function, using provided unit tests\nas a guide. The third and final task, Code Explanation, requires the model to generate a clear and\nconcise explanation for a correctly written code function. For the evaluation on HumanEvalPack, we\nuse its Python and Java splits and compute Pass@1 for each task. We use temperature 0.2 and sample\n20 outputs per test example.\nModel Robustness\nEvaluating the robustness of code generation models is crucial in understanding\ntheir real-world applicability and reliability. Models that can maintain high-performance levels\ndespite variations and perturbations in input data are more likely to be effective in diverse and\ndynamic coding environments (Bielik and Vechev, 2020; Henkel et al., 2022; Wang et al., 2023a).\nMotivated by such model behaviors, we utilize ReCode (Wang et al., 2023a), a benchmark framework\ndesigned to assess the robustness of Code LLMs. We use HumanEval (Chen et al., 2021) as the\nbase dataset and curated it to mimic natural variations while preserving the semantic integrity of\nthe original inputs. The perturbations cover a range of transformations (Zhuo et al., 2023c) on code\n2https://github.com/huggingface/peft\n4\nformat, function, variable names, code syntax, and docstrings. These transformations are not arbitrary\nbut represent changes occurring naturally in coding practices. The quality of the perturbed data in\nReCode is verified through human evaluation and objective similarity scores, ensuring the relevance\nand reliability of the dataset for robustness assessment. We use temperature 0.2 and 20 samples per\ntest example for the generation. To compute the level of model robustness, we adopt Robust Pass@k\n(RP@k) from ReCode and also compute Robust Change@k (RC@k) as follows:\nRP@k := Ex\n\"\n1 \u2212 n \u2212 rcs(x)\n\u0000n\nk\n\u0001\n#\n(1)\nRC@k := |Pass@k \u2212 Robust Pass@k|\n(2)\nCode Security\nOne limitation of Code LLMs is their tendency to generate code with potential\nsecurity vulnerabilities, as various studies have highlighted (Dakhel et al., 2023; Asare et al., 2023).\nIn our work, we aim to empirically examine how PEFT methods can influence the security aspects\nof Code LLM outputs. We utilize the \u201cAsleep at the Keyboard\u201d (AATK) benchmark (Pearce et al.,\n2022), which includes 89 security-centric scenarios, to provide a comprehensive evaluation across\nthree distinct dimensions: Diversity of Weakness (DoW), encompassing 18 unique vulnerability\nclasses from the MITRE Common Weakness Enumeration (CWE) taxonomy, sourced from the 2021\nCWE Top 25 Most Dangerous Software Weaknesses; Diversity of Prompt (DoP), assessing responses\nto different prompts within the SQL injection vulnerability class; and Diversity of Domain (DoD),\ninvolving scenarios in Verilog, a hardware description language. Our analysis predominantly focuses\non the DoW axis, comprising 54 scenarios\u201325 in C and 29 in Python\u2013covering 18 CWEs. This focus\nis due to the automatic evaluation challenges associated with the other two dimensions. After filtering\nout scenarios that lack an automated test, we thoroughly examine 40 scenarios, including 23 in C and\n17 in Python. We use temperature 0.2 and 20 samples per test example for the generation.\n3\nPreliminary Study: Cross-Entropy Loss\nCross-entropy loss has been used as the principal performance metric in training LLMs for NLP\ntasks (Brown et al., 2020; Hernandez et al., 2021; Zhang et al., 2022b). Most studies on modeling\nloss focus on either pre-training (Kaplan et al., 2020) or FFT (Chung et al., 2022). Previous studies\nhave consistent findings on loss (Kaplan et al., 2020; Hoffmann et al., 2022; Aghajanyan et al., 2023):\nThe final loss tends to decrease when the training computation (e.g., model sizes, training data and\ntraining time) increases. These observations indicate that more training time and more trainable\nmodel parameters can lead to better alignment with the tuning data. However, there is no systematic\ninvestigation for PEFT, especially for Code LLMs. Based on the updated parameters for each tuning\nmethod in Table 1, we hypothesize that each PEFT method has a similar trend to previous findings of\nloss. Inspired by Kaplan et al. (2020), we study the loss change for instruction tuning Code LLMs,\nvarying two factors: (1) Model Size (1B - 16B); and (2) Training Time (measured in global step,\nmaximum 200 steps). Due to the limited budget, We do not study how the amount of training data\nmay affect the loss.\n1B\n3B\n7B\n16B\nModel Size\n0.9\n1.5\nTrain Loss\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\n1B\n3B\n7B\n16B\nModel Size\n1\n1.5\nTest Loss\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nFigure 3: Final loss across model sizes.\n5\nModel Size Scaling\nWe present the results of final loss in Figure 3 when varying the model size\nfrom 1B to 16B. Our first observation is that train and test loss are well aligned, indicating that\nthe models trained on the selected tuning methods are not overfitted. The second observation is\nthat both train and test loss also strictly decrease when the model size increases. Although these\nobservations are aligned with the aforementioned observations (Kaplan et al., 2020; Hoffmann et al.,\n2022), they show the different scales of loss change, suggesting different tuning methods may require\ndifferent levels of power. Compared to other tuning methods, FFT demonstrates a slightly better loss\nperformance than PEFT methods like LoRA and Parallel Adapter. As we notice that heavier PEFT\nmethods (which update more parameters) tend to have a better final loss, we hypothesize that more\ntrainable parameters in the model may result in a smaller loss, regardless of how the parameters are\nupdated during training.\n25\n50\n75\n100\n125\n150\n175\n200\nGlobal Step\n1.4\n2\nTest Loss\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\n1B ASTRAIOS models.\n25\n50\n75\n100\n125\n150\n175\n200\nGlobal Step\n1.4\n2\nTest Loss\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\n3B ASTRAIOS models.\n25\n50\n75\n100\n125\n150\n175\n200\nGlobal Step\n1.2\n6\nTest Loss\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\n7B ASTRAIOS models.\n25\n50\n75\n100\n125\n150\n175\n200\nGlobal Step\n1.2\n5\nTest Loss\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\n16B ASTRAIOS models.\nFigure 4: Test loss of ASTRAIOS models across training time measured by Global Step.\nTraining Time Scaling\nWe show the changes in test loss on the ASTRAIOS when varying the\ntraining time in Figure 4. We notice that the loss continues decreasing when the model is trained\nlonger. Although the loss changes of (IA)3 are consistently insignificant. Notably, the loss of P-\nTuning decreases drastically to 50 steps but behaves similarly to other prompt-based methods. In\nterms of tuning stability, we observe that P-tuning is more unstable than other methods, where the\nloss change appears to be a non-monotonic pattern. When comparing FFT against PEFT methods, we\nfind that FFT tends to decrease even after 200 steps, while PEFT methods do not show a decreasing\ntrend clearly. We hypothesize that it may be due to the number of updated parameters, where FFT\nupdates the full parameters in the model.\n4\nMain Results: Task Performance\nAs cross-entropy loss only indicates how well Code LLMs can be aligned with the training data, it\ngreatly depends on the specific training content and may not serve as a reliable proxy of performance\n6\nTable 2: Results of ASTRAIOS models on Defect Detection and Clone Detection. The best perfor-\nmance is highlighted in bold. The second best performance is underlined.\nMethod\nDefect Detection\nClone Detection\n1B\n3B\n7B\n16B\n1B\n3B\n7B\n16B\nLoRA\n44.15\n44.90\n49.05\n31.95\n9.30\n12.05\n14.10\n8.80\nP-Tuning\n53.70\n27.75\n40.55\n11.00\n19.27\n23.52\n13.35\n3.24\nAdapterH\n45.75\n45.80\n46.25\n41.75\n8.59\n8.17\n12.05\n8.18\nAdapterP\n45.55\n46.05\n46.85\n27.35\n8.88\n8.63\n12.05\n9.00\nParallel\n34.50\n33.50\n52.55\n42.30\n9.55\n8.94\n10.16\n17.21\n(IA)3\n53.90\n33.55\n37.20\n23.70\n8.28\n11.76\n23.19\n8.13\nFFT\n50.80\n44.20\n48.30\n43.65\n8.34\n12.68\n8.04\n12.62\n1B\n3B\n7B\n16B\nModel Size\n10\n20\n30\n40\n50\nAccuracy\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nFigure 5: Accuracy results of ASTRAIOS models\non Defect Detection.\n1B\n3B\n7B\n16B\nModel Size\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nF1\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nFigure 6: F1 results of ASTRAIOS models on\nClone Detection.\non various tasks of source code. Therefore, We seek to examine how well selective PEFT methods\ncontribute to task performance in this section. To benchmark the performance, we leverage the\nrepresentative code downstream tasks: (1) Defect Detection, (2) Code Clone, (3) Code Synthesis,\n(4) Code Repair and (5) Code Explanation. For the first two code comprehension tasks, there is no\nexisting study stating that the larger code LLMs result in a better understanding of code. We are the\nfirst to study this aspect when varying the model sizes. Regarding the latter three code generation\ntasks, previous power-law studies (Kaplan et al., 2020; Hoffmann et al., 2022) have shown that\nincreasing model sizes can also lead to better task performance on generation tasks. We further\nvalidate this finding on the PEFT settings.\nCode Comprehension\nTable 2 shows the results of the two code comprehension tasks when varying\nthe model sizes. Surprisingly, as shown in Figures 5 and 6, the results of both tasks are not well\naligned with the patterns we observe on code generation tasks. All tuning methods consistently\nbehave like the inverse scaling, which has been discussed in McKenzie et al. (2023). We hypothesize\nthat Code LLMs have not seen enough task-specific training data and cannot generalize to those\nunseen tasks (Yadlowsky et al., 2023). As ASTRAIOS models are pre-trained on various source\ncode from GitHub repositories for next token prediction and fine-tuned on GitHub commits for code\nrefinement, they may not have a profound understanding of defects and cloned code.\nCode Generation\nTable 3 demonstrates the performance on three different code generation tasks\non the Python and Java splits in HumanEvalPack. Over the six benchmarks, we first observe that FFT\nresults in consistent gains when the model parameters increase. When examining the PEFT methods,\nWe find they can also provide reasonable performance scalability similar to FFT. Therefore, the lower\ntest loss may lead to better performance across various downstream generation tasks for Code LLMs.\nHowever, we notice that the benefit of base model sizes may also differ from tasks and languages.\nFor instance, 1B and 3B models typically underperform in code repair compared to code synthesis.\nWhen the model parameters expand to 7B and 16B, their performance across these tasks becomes\nmore comparable.\n7\nTable 3: Pass@1 results of ASTRAIOS models on HumanEvalPack Python and Java splits. The best\nperformance is highlighted in bold. The second best performance is underlined.\nMethod\nCode Synthesis\nCode Repair\nCode Explanation\n1B\n3B\n7B\n16B\n1B\n3B\n7B\n16B\n1B\n3B\n7B\n16B\nPython\nLoRA\n17.26\n25.37\n32.01\n38.08\n3.29\n11.16\n21.74\n27.50\n20.49\n22.53\n25.34\n30.52\nP-Tuning\n15.79\n24.33\n29.39\n35.58\n1.86\n13.69\n20.34\n18.72\n9.48\n11.92\n14.60\n15.43\nAdapterH\n15.70\n23.87\n28.26\n33.29\n3.14\n15.55\n22.50\n22.28\n17.77\n22.35\n24.24\n26.07\nAdapterP\n17.04\n24.76\n30.67\n34.97\n3.69\n12.87\n19.54\n26.46\n16.07\n24.05\n22.87\n30.67\nParallel\n15.98\n26.65\n28.81\n35.88\n4.91\n8.11\n16.13\n26.43\n19.70\n23.14\n23.93\n31.10\n(IA)3\n16.13\n25.34\n30.52\n36.80\n2.01\n14.05\n17.07\n23.60\n9.51\n11.86\n14.30\n16.19\nFFT\n16.95\n25.21\n32.38\n38.47\n3.26\n14.45\n21.40\n29.88\n15.37\n23.45\n26.13\n30.85\nJava\nLoRA\n2.84\n16.52\n24.27\n40.33\n3.72\n5.06\n13.60\n30.35\n7.07\n14.33\n14.70\n16.86\nP-Tuning\n10.67\n14.73\n20.73\n37.19\n0.00\n7.53\n11.74\n22.25\n6.07\n9.79\n17.32\n13.02\nAdapterH\n8.99\n13.45\n17.53\n33.41\n0.12\n6.89\n14.70\n24.91\n6.74\n9.57\n13.99\n14.85\nAdapterP\n10.46\n16.77\n21.28\n33.68\n3.66\n6.52\n15.40\n32.07\n6.65\n11.62\n14.15\n16.28\nParallel\n9.60\n15.91\n21.59\n38.56\n0.49\n5.09\n8.87\n29.39\n7.62\n12.16\n14.51\n17.93\n(IA)3\n10.34\n16.46\n21.95\n39.91\n2.87\n4.54\n13.02\n25.30\n6.13\n13.99\n17.04\n15.85\nFFT\n10.18\n17.04\n23.87\n41.16\n0.00\n5.61\n16.10\n32.47\n7.16\n13.60\n15.12\n16.62\nOverall Performance\nTo compare the overall task performance of different tuning methods, we\ncompute the mean cumulative scores for each tuning method per model size. We present the rankings\nin Figure 1. We show that FFT remains the best regarding overall task performance, while LoRA and\nParallel Adapter are comparable to FFT. However, there is still a huge performance gap between most\nPEFT methods and FFT, suggesting that they cannot guarantee optimal performance. Regarding the\ntuning efficiency, we use updated parameters as the metric to summarize two more findings. Firstly,\n(IA)3 is efficient enough to perform reasonably by updating much fewer parameters than the other\nPEFT methods. Secondly, we notice that AdapterP always performs better than AdapterH, even\nthough AdapterH updates more model parameters. The counter-intuitive observation indicates that\nAdapterH may not be worth deploying in real-world practice.\n5\nFurther Analysis\nIn this section, we further study two aspects of Code LLMs beyond task performance. Specifically, we\nhighlight the importance of model robustness and generated code security, which indicate real-world\npracticality. We tend to understand the trend of model behavior across tuning methods and model\nsizes.\nTable 4: RP@1 and RC@1 results of ASTRAIOS models on ReCode. The best performance is\nhighlighted in bold. The second best performance is underlined.\nMethod\nFormat\nFunction\nSyntax\nDocstring\n1B\n3B\n7B\n16B\n1B\n3B\n7B\n16B\n1B\n3B\n7B\n16B\n1B\n3B\n7B\n16B\nRobust Pass\nLoRA\n28.05\n35.98\n43.29\n51.22\n12.80\n15.24\n23.78\n29.27\n8.54\n13.41\n15.85\n18.29\n10.98\n15.24\n17.68\n20.73\nP-Tuning\n18.29\n29.88\n39.63\n48.78\n7.32\n15.85\n21.34\n23.78\n6.71\n11.59\n14.02\n17.68\n6.71\n14.63\n18.29\n21.34\nAdapterH\n10.98\n34.15\n40.24\n46.95\n4.88\n14.02\n17.07\n23.78\n7.32\n11.59\n12.20\n15.85\n6.10\n12.80\n14.63\n17.68\nAdapterP\n9.76\n35.37\n43.90\n50.00\n1.22\n15.85\n21.34\n26.22\n4.88\n12.20\n14.63\n18.29\n3.05\n15.24\n19.51\n20.12\nParallel\n26.22\n32.32\n42.68\n50.00\n10.37\n11.59\n21.95\n26.83\n7.93\n12.80\n14.63\n17.07\n8.54\n15.24\n17.68\n21.95\n(IA)3\n26.83\n33.54\n42.07\n50.61\n12.80\n17.07\n21.34\n26.83\n7.93\n12.20\n14.63\n17.07\n10.37\n15.85\n18.90\n22.56\nFFT\n20.12\n35.37\n45.73\n53.05\n5.49\n15.85\n21.34\n30.49\n7.32\n14.63\n15.85\n19.51\n6.10\n14.02\n18.90\n22.56\nRobust Change\nLoRA\n10.98\n14.63\n15.24\n15.85\n4.27\n6.10\n4.27\n6.10\n8.54\n7.93\n12.20\n17.07\n6.10\n6.10\n10.37\n14.63\nP-Tuning\n6.10\n9.76\n12.80\n17.68\n4.88\n4.27\n5.49\n7.32\n5.49\n8.54\n12.80\n13.41\n5.49\n5.49\n8.54\n9.76\nAdapterH\n0.61\n15.85\n15.85\n15.85\n5.49\n4.27\n7.32\n7.32\n3.05\n6.71\n12.20\n15.24\n4.27\n5.49\n9.76\n13.41\nAdapterP\n3.66\n14.63\n17.68\n15.85\n4.88\n4.88\n4.88\n7.93\n1.22\n8.54\n11.59\n15.85\n3.05\n5.49\n6.71\n14.02\nParallel\n12.20\n11.59\n15.85\n15.24\n3.66\n9.15\n4.88\n7.93\n6.10\n7.93\n12.20\n17.68\n5.49\n5.49\n9.15\n12.80\n(IA)3\n10.98\n12.80\n14.02\n14.63\n3.05\n3.66\n6.71\n9.15\n7.93\n8.54\n13.41\n18.90\n5.49\n4.88\n9.15\n13.41\nFFT\n7.32\n14.02\n17.68\n15.24\n7.32\n5.49\n6.71\n7.32\n5.49\n6.71\n12.20\n18.29\n6.71\n7.32\n9.15\n15.24\n8\n5.1\nModel Robustness\nWhile the performance on downstream tasks is essential, we argue that the evaluation of model\nrobustness is also necessary to characterize different tuning methods systematically. We therefore\nconsider benchmarking the robustness of code synthesis, one of the most representative downstream\ntasks of source code.\nTable 4 reports each tuning method\u2019s worst-case RP@1 and RC@1 of each perturbation category.\nAmong the four types of perturbation, all models perform the worst on syntax transformation,\nconfirming the findings in Wang et al. (2023a). Furthermore, RP@1 per tuning method increases\nwhen the model size is scaled up, indicating the generation capability is consistently improved. We\nnoticed that FFT may not perform better than other PEFT methods on smaller models, such as 1B\nand 3B. However, it results in the best RP@1 on larger models like 16B. By comparing different\nmodel sizes, we observe that RC@1 consistently increases when the model gets bigger, indicating\nthat larger models will be less robust.\n1B\n3B\n7B\n16B\n0\n10\n20\n30\n40\n50\nMean RC@1\nAdapterH\n 2.87%\nParallel\n 1.52%\nAdapterP\n 1.46%\nP\nTuning\n 0.82%\n LoRA\n 0.21%\n  (IA)3\n 0.01%\n  FFT\n 100%\nFigure 7: Mean RC@1 of ASTRAIOS on ReCode. Lower RC@1 indicates better robustness. We\nindicate the percentage of total parameters updated for each PEFT method.\nTo rank among the tuning methods through the lens of robustness, we compute the mean RC@1\nsimilar to Section 4 and illustrate in Figure 7. We observe that FFT and LoRA do not show strong\nrobustness. Instead, adapter-based tuning seems more robust while having comparable performance\nto FFT, which is similar to what Han et al. (2021) have found in NLP tasks.\nTable 5: Valid and Insecure rates of ASTRAIOS models on AATK benchmark. We note that the\ninsecure rate is calculated based on the valid programs. The best performance is highlighted in bold.\nThe second best performance is underlined.\nMethod\nValid% (\u2191)\nInsecure% (\u2193)\n1B\n3B\n7B\n16B\n1B\n3B\n7B\n16B\nLoRA\n85.9\n89.1\n75.9\n87.1\n23.1\n26.2\n20.9\n35.0\nP-Tuning\n70.1\n68.6\n86.8\n82.0\n32.8\n25.9\n28.1\n34.5\nAdapterH\n84.5\n90.9\n87.5\n86.8\n29.0\n26.0\n31.9\n34.1\nAdapterP\n83.9\n92.1\n82.8\n86.3\n31.7\n25.2\n26.6\n37.8\nParallel\n88.9\n94.1\n70.0\n86.0\n30.2\n19.3\n22.3\n32.6\n(IA)3\n78.0\n62.1\n77.4\n86.6\n34.8\n25.2\n23.1\n30.4\nFFT\n82.9\n93.6\n80.1\n84.1\n22.6\n27.4\n21.2\n38.3\n9\n5.2\nCode Security\nPrevious studies (Dakhel et al., 2023; Asare et al., 2023). have shown that Code LLMs can generate\ncode with security vulnerabilities, which can be exploited by malicious users. However, few studies\nhave studied different tuning methods from the output security perspective. In this experiment, we\nintend to understand how tuning methods affect the capability to generate secure code on AATK\nbenchmark.\nWe follow the original setting in Pearce et al. (2022) and compute the valid and insecure rates, which\nare illustrated in Table 5. When comparing the valid rate of PEFT methods, it does not show better\nperformance when the model size increases, indicating that current models may not learn the program\nvalidity intrinsically. However, we observe that the changes in the insecure rate show that larger\nmodels are more likely to generate insecure code. This observation suggests that the growth of\nlearning capability can result in learning more data, including insecure programs. The study on the\ninsecure rate among tuning methods further shows that FFT and LoRA are still better than the other\ntuning methods regarding the security level. While the other methods have a similar insecure rate,\nP-Tuning may have more chances to generate less secure programs, which may not be suitable for\ndeploying in security-sensitive scenarios.\n6\nDiscussion\nIn this section, we seek to conduct a preliminary analysis of the performance of Code LLMs through\nthe lens of updated parameters. Specifically, we ask two questions: (1) What is the relationship\nbetween the updated parameters and cross-entropy loss?; and (2) Can we utilize the performance of\nloss to predict the task performance of Code LLMs?.\n106\n107\n108\n109\n1010\nNumber of Updated Parameters\n1.0\n1.1\n1.2\n1.3\n1.4\nFinal Train Loss\n1B\n3B\n7B\n16B\nAdapterH\nAdapterP\n(IA)3\nLoRA\nParallel\nP-Tuning\nFFT\n106\n107\n108\n109\n1010\nNumber of Updated Parameters\n1.1\n1.2\n1.3\n1.4\n1.5\nFinal Test Loss\n1B\n3B\n7B\n16B\nAdapterH\nAdapterP\n(IA)3\nLoRA\nParallel\nP-Tuning\nFFT\nFigure 8: Relationships between cross-entropy loss and the number of updated parameters.\nLoss of small models can be projected to larger ones.\nThe relationship between the updated\nparameters of ASTRAIOS models and their final loss is analyzed in Figure 8. Our analysis does not\nreveal a consistent pattern across different model sizes when it comes to the correlation between\nmodel loss and updated parameters. However, an interesting finding is the consistency in relative loss\nperformance across different model sizes when comparing various tuning methods. This consistency\nsuggests that the improvements achieved by each tuning method are likely to be similar regardless of\nthe model\u2019s size. Therefore, the loss observed in smaller models, when tuned with different methods,\ncan be a useful predictor for the performance of the larger models.\nInstruct-tuning loss is a strong predictor of downstream performance.\nAssuming that the model\nhas been instruction-tuned already but not yet done for the evaluation, we seek to understand if we\ncan utilize such loss to predict its performance on downstream tasks. Despite our instruction data\nbeing derived from general sources like GitHub commits and broad NLP domains, which are not\ndirectly aligned with the downstream tasks discussed in Section 4, we find some strong correlations.\n10\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nTask Performance\n1.0\n1.1\n1.2\n1.3\n1.4\nFinal Train Loss\nAdapterH\nAdapterP\n(IA)3\nLoRA\nParallel\nP-Tuning\nFFT\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nTask Performance\n1.1\n1.2\n1.3\n1.4\n1.5\nFinal Test Loss\nAdapterH\nAdapterP\n(IA)3\nLoRA\nParallel\nP-Tuning\nFFT\nFigure 9: Relationships between cross-entropy loss and overall task performance.\nMotivated by the aforementioned scenario, we aggregate all the data points of mean task performance\nand their corresponding final loss in Figure 9. We observe that the models with lower loss generally\nhave better overall performance on downstream tasks. Specifically, the pattern is stronger on test loss\nthan on train loss. We explain by the fact that the models do not learn to fit the test split and can\npresent a more accurate determination of their actual performance. Our observation suggests that\ngeneral instruction data can work as a good proxy of downstream tasks in Code LLMs, similar to the\nprior findings in NLP (Anil et al., 2023; Wei et al., 2023).\n7\nRelated Work\nCode Large Language Models\nMany base Code LLMs have been proposed recently (Chen et al.,\n2021; Nijkamp et al., 2022; Fried et al., 2022; Allal et al., 2023; Zheng et al., 2023; Li et al., 2023;\nRoziere et al., 2023) mostly targeting code completion. With the help of these base Code LLMs, there\nhave been extensive studies fine-tuning task-specific Code LLMs to perform software engineering\ntasks like automatic program repair (Xia and Zhang, 2023; Xia et al., 2023a), code translation (Pan\net al., 2023) and code summarization (Wang et al., 2023b, 2022a). Later, a series of works has been\nproposed for instruction-tuning the base Code LLMs (Luo et al., 2023; Shen et al., 2023; Muennighoff\net al., 2023a; Bai et al., 2023), aiming to enhance the generalization capabilities of these models on\ndiverse tasks. As fine-tuning Code LLMs with full parameters is costly, most models have been tuned\nwith LoRA (Hu et al., 2021), a parameter-efficient tuning method. In this work, we seek to answer\nhow good LoRA is and if there are other comparable tuning methods.\nModel Analysis Across Scales\nUnderstanding why and how neural models behave is crucial\nfor developing more advanced ones. Existing studies have investigated predictable patterns in the\nbehavior of trained language models across scales (Kaplan et al., 2020; Henighan et al., 2020;\nHernandez et al., 2021; Hoffmann et al., 2022; Wei et al., 2022; Muennighoff et al., 2023b; Xia et al.,\n2023b) and their learning dynamics (McGrath et al., 2022; Tirumala et al., 2022; Biderman et al.,\n2023). However, they either focus on pre-training or task-specific full-parameter fine-tuning. There is\nno attempt to understand the mechanism of parameter-efficient instruction tuning. In this paper, we\nwork on this perspective and analyze Code LLMs (Wan et al., 2022; Troshin and Chirkova, 2022;\nZhuo et al., 2023a).\n8\nConclusion\nThis work studies the parameter-efficient instruction-tuning of Code LLMs. We introduce a model\nsuite consisting of 28 instruction-tuned OctoCoder across scales and PEFT methods. We characterize\nthe tuning methods on representative downstream tasks, model robustness, and output security,\nhighlighting the importance of understanding these models via comprehensive evaluation. We also\ndiscuss the relationships among updated parameters, cross-entropy loss, and task performance. We\n11\nhope these analyses will inspire further follow-up work on understanding the mechanism of tuning\nmethods and developing new approaches.\nAcknowledgements\nWe thank Monash University and Hugging Face for providing compute instances. We are extremely\ngrateful to Cristian Rojas for help on the initial exploration, Zhensu Sun for the discussion, Dmitry\nAbulkhanov for the paper review, Brendan Dolan-Gavitt for providing the evaluation script of \u201cAsleep\nAt The Keyboard\u201d benchmark, the BigCode community for providing the base models (Li et al.,\n2023) and instruction tuning data (Muennighoff et al., 2023a) from GitHub commits, and Mangrulkar\net al. (2022); Hu et al. (2023) for implementing PEFT methods.\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021. Intrinsic Dimensionality Explains\nthe Effectiveness of Language Model Fine-Tuning. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 7319\u20137328.\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang,\nStephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. 2023. Scaling laws for generative\nmixed-modal language models. arXiv preprint arXiv:2301.03728.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. SantaCoder:\ndon\u2019t reach for the stars! arXiv preprint arXiv:2301.03988.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report.\narXiv preprint arXiv:2305.10403.\nOwura Asare, Meiyappan Nagappan, and N Asokan. 2023. Is github\u2019s copilot as bad as humans at\nintroducing vulnerabilities in code? Empirical Software Engineering, 28(6):1\u201324.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von\nWerra. 2022. A framework for the evaluation of code generation models. https://github.com/\nbigcode-project/bigcode-evaluation-harness.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,\net al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In\nInternational Conference on Machine Learning, pages 2397\u20132430. PMLR.\nPavol Bielik and Martin Vechev. 2020. Adversarial robustness for code. In International Conference\non Machine Learning, pages 896\u2013907. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models\nare few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nSahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation.\nhttps://github.com/sahil280114/codealpaca.\nGuanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shangsong Liang. 2022. Revisiting Parameter-\nEfficient Tuning: Are We Really There Yet? In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 2612\u20132626.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating\nlarge language models trained on code. arXiv preprint arXiv:2107.03374.\n12\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416.\nArghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C Desmarais,\nand Zhen Ming Jack Jiang. 2023. Github copilot ai pair programmer: Asset or liability? Journal\nof Systems and Software, 203:111734.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Gpt3. int8 (): 8-bit matrix\nmultiplication for transformers at scale. Advances in Neural Information Processing Systems,\n35:30318\u201330332.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin\nChen, Chi-Min Chan, Weize Chen, et al. 2022. Delta tuning: A comprehensive study of parameter\nefficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin\nChen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale\npre-trained language models. Nature Machine Intelligence, 5(3):220\u2013235.\nAli Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Reza-\ngholizadeh. 2022. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint\narXiv:2212.10650.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott\nYih, Luke Zettlemoyer, and Mike Lewis. 2022. InCoder: A Generative Model for Code Infilling\nand Synthesis. In The Eleventh International Conference on Learning Representations.\nZihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. 2023. On\nthe effectiveness of parameter-efficient fine-tuning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, pages 12799\u201312807.\nWenjuan Han, Bo Pang, and Ying Nian Wu. 2021. Robust Transfer Learning with Pretrained\nLanguage Models through Adapters. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 854\u2013861.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021.\nTowards a Unified View of Parameter-Efficient Transfer Learning. In International Conference on\nLearning Representations.\nXuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, and Xin Eric Wang. 2023. Parameter-\nefficient model adaptation for vision transformers. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, pages 817\u2013825.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo\nJun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scaling laws for autoregressive\ngenerative modeling. arXiv preprint arXiv:2010.14701.\nJordan Henkel, Goutham Ramakrishnan, Zi Wang, Aws Albarghouthi, Somesh Jha, and Thomas Reps.\n2022. Semantic robustness of models of source code. In 2022 IEEE International Conference on\nSoftware Analysis, Evolution and Reengineering (SANER), pages 526\u2013537. IEEE.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. Scaling laws for\ntransfer. arXiv preprint arXiv:2102.01293.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556.\nXinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John\nGrundy, and Haoyu Wang. 2023. Large language models for software engineering: A systematic\nliterature review. arXiv preprint arXiv:2308.10620.\n13\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning\nfor NLP. In International Conference on Machine Learning, pages 2790\u20132799. PMLR.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference\non Learning Representations.\nZhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and\nSoujanya Poria. 2023. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of\nLarge Language Models. arXiv preprint arXiv:2304.01933.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Compacter: Efficient\nlow-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems,\n34:1022\u20131035.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient\nPrompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 3045\u20133059.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. StarCoder: may the source be\nwith you! arXiv preprint arXiv:2305.06161.\nXiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npages 4582\u20134597.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and\nColin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context\nlearning. Advances in Neural Information Processing Systems, 35:1950\u20131965.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023.\nGPT understands, too. AI Open.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. CodeXGLUE: A Machine Learning\nBenchmark Dataset for Code Understanding and Generation. In Thirty-fifth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 1).\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models\nwith Evol-Instruct. arXiv preprint arXiv:2306.08568.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin\nBossan. 2022.\nPEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods.\nhttps://\ngithub.com/huggingface/peft.\nThomas McGrath, Andrei Kapishnikov, Nenad Toma\u0161ev, Adam Pearce, Martin Wattenberg, Demis\nHassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik. 2022. Acquisition of chess knowledge\nin alphazero. Proceedings of the National Academy of Sciences, 119(47):e2206625119.\nIan R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,\nEuan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. 2023. Inverse Scaling: When Bigger\nIsn\u2019t Better. arXiv preprint arXiv:2306.09479.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam\nSingh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. 2023a. Octopack: Instruction\ntuning code large language models. arXiv preprint arXiv:2308.07124.\n14\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane\nTazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023b. Scaling Data-Constrained Language\nModels. arXiv preprint arXiv:2305.16264.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven\nLe Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,\nEdward Raff, and Colin Raffel. 2023c. Crosslingual Generalization through Multitask Finetuning.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 15991\u201316111, Toronto, Canada. Association for Computational\nLinguistics.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and\nCaiming Xiong. 2022. CodeGen: An Open Large Language Model for Code with Multi-Turn\nProgram Synthesis. In The Eleventh International Conference on Learning Representations.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744.\nRangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele\nMerler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2023. Un-\nderstanding the Effectiveness of Large Language Models in Code Translation. arXiv preprint\narXiv:2308.03109.\nHammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. 2022.\nAsleep at the keyboard? assessing the security of github copilot\u2019s code contributions. In 2022\nIEEE Symposium on Security and Privacy (SP), pages 754\u2013768. IEEE.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models.\nAdvances in Neural Information Processing Systems, 34:11054\u201311070.\nJonas Pfeiffer, Ivan Vuli\u00b4c, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 7654\u20137673.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for\ncode. arXiv preprint arXiv:2308.12950.\nBo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu,\nJichuan Ji, Jingyang Zhao, et al. 2023. Pangu-coder2: Boosting large language models for code\nwith ranking feedback. arXiv preprint arXiv:2307.14936.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Vl-adapter: Parameter-efficient transfer learning\nfor vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5227\u20135237.\nJeffrey Svajlenko and Chanchal K Roy. 2021. Bigclonebench. Code Clone Analysis: Research, Tools,\nand Practices, pages 93\u2013105.\nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization\nwithout overfitting: Analyzing the training dynamics of large language models. Advances in Neural\nInformation Processing Systems, 35:38274\u201338290.\nSergey Troshin and Nadezhda Chirkova. 2022. Probing Pretrained Models of Source Codes. In\nProceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP, pages 371\u2013383.\n15\nYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022. What do they\ncapture? a structural analysis of pre-trained language models for source code. In Proceedings of\nthe 44th International Conference on Software Engineering, pages 2377\u20132388.\nChaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R Lyu.\n2022a. No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence. In\nProceedings of the 30th ACM Joint European Software Engineering Conference and Symposium\non the Foundations of Software Engineering, pages 382\u2013394.\nShiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar,\nSamson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan,\nDan Roth, and Bing Xiang. 2023a. ReCode: Robustness Evaluation of Code Generation Models.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 13818\u201313843, Toronto, Canada. Association for Computational\nLinguistics.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi.\n2023b. Codet5+: Open code large language models for code understanding and generation. arXiv\npreprint arXiv:2305.07922.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified\nPre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696\u20138708.\nZhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. 2022b.\nMultitask Prompt Tuning Enables Parameter-Efficient Transfer Learning. In The Eleventh Interna-\ntional Conference on Learning Representations.\nJason Wei, Najoung Kim, Yi Tay, and Quoc V Le. 2022. Inverse scaling can become u-shaped. arXiv\npreprint arXiv:2211.02011.\nTianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng\nCheng, Weiwei L\u00fc, Rui Hu, et al. 2023. Skywork: A more open bilingual foundation model. arXiv\npreprint arXiv:2310.19341.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm\nfrom language models. arXiv preprint arXiv:2112.04359.\nChunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023a. Automated program repair in the\nera of large pre-trained language models. In Proceedings of the 45th International Conference on\nSoftware Engineering (ICSE 2023). Association for Computing Machinery.\nChunqiu Steven Xia and Lingming Zhang. 2023. Conversational automated program repair. arXiv\npreprint arXiv:2301.13246.\nMengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen,\nLuke Zettlemoyer, and Veselin Stoyanov. 2023b. Training Trajectories of Language Models Across\nScales. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 13711\u201313738, Toronto, Canada. Association for Computational\nLinguistics.\nTianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning\nZhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong,\nand Tao Yu. 2023. OpenAgents: An Open Platform for Language Agents in the Wild. CoRR,\nabs/2310.10634.\nSteve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni. 2023. Pretraining Data Mixtures Enable\nNarrow Model Selection Capabilities in Transformer Models. arXiv preprint arXiv:2311.00871.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple Parameter-efficient\nFine-tuning for Transformer-based Masked Language-models. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1\u20139.\n16\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and\nTuo Zhao. 2022a. Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. In The\nEleventh International Conference on Learning Representations.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022b. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv\npreprint arXiv:2303.18223.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang,\nAndi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with\nmultilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, pages 5673\u20135684.\nXin Zhou, Ruotian Ma, Yicheng Zou, Xuanting Chen, Tao Gui, Qi Zhang, Xuan-Jing Huang, Rui\nXie, and Wei Wu. 2022. Making parameter-efficient tuning more efficient: A unified framework\nfor classification tasks. In Proceedings of the 29th International Conference on Computational\nLinguistics, pages 7053\u20137064.\nYaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vul-\nnerability identification by learning comprehensive program semantics via graph neural networks.\nAdvances in neural information processing systems, 32.\nTerry Yue Zhuo, Xiaoning Du, Zhenchang Xing, Jiamou Sun, Haowei Quan, Li Li, and Liming Zhu.\n2023a. Pop Quiz! Do Pre-trained Code Models Possess Knowledge of Correct API Names? arXiv\npreprint arXiv:2309.07804.\nTerry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023b. Red teaming chatgpt\nvia jailbreaking: Bias, robustness, reliability and toxicity. arXiv preprint arXiv:2301.12867, pages\n12\u20132.\nTerry Yue Zhuo, Zhou Yang, Zhensu Sun, Yufei Wang, Li Li, Xiaoning Du, Zhenchang Xing, and\nDavid Lo. 2023c. Data Augmentation Approaches for Source Code Models: A Survey. arXiv\npreprint arXiv:2305.19915.\n17\nA\nWhat is ASTRAIOS?\nASTRAIOS is a suite of 28 instruction-tuned StarCoder models, employing 7 different PEFT methods\nacross 4 model sizes, with up to 16B parameters. Named after the Greek Titan god of the stars,\nASTRAIOS, this model collection represents a vast array of \u201cstars\u201d, each model illuminating a path\nto understanding the cost-performance trade-offs in Code LLMs. Through extensive testing across\nvarious tasks and datasets, ASTRAIOS evaluates the efficacy of fine-tuning methods with an emphasis\non understanding their performance implications at different model scales, robustness, and security\naspects. The suite serves as a celestial guide in the Code LLM universe, helping to chart the most\nefficient and effective methods for model fine-tuning.\nB\nArtifacts\nName\nPublic Link\nBase Models\nStarCoderBase 1B\nhttps://huggingface.co/bigcode/starcoderbase-1b\nStarCoderBase 3B\nhttps://huggingface.co/bigcode/starcoderbase-3b\nStarCoderBase 7B\nhttps://huggingface.co/bigcode/starcoderbase-7b\nStarCoderBase\nhttps://huggingface.co/bigcode/starcoderbase\nInstruction Tuning Data\nCommitPackFT + OASST\nhttps://huggingface.co/datasets/bigcode/guanaco-commits\nOriginal PEFT Implementation\nLoRA\nhttps://github.com/huggingface/peft\nP-Tuning\nhttps://github.com/huggingface/peft\nAdapterH\nhttps://github.com/AGI-Edgerunners/LLM-Adapters\nAdapterP\nhttps://github.com/AGI-Edgerunners/LLM-Adapters\nParallel\nhttps://github.com/AGI-Edgerunners/LLM-Adapters\n(IA)3\nhttps://github.com/huggingface/peft\nPrompt\nhttps://github.com/huggingface/peft\nAdaLoRA\nhttps://github.com/huggingface/peft\nEvaluation Framework\nCode Generation LM Evaluation Harness\nhttps://github.com/bigcode-project/bigcode-evaluation-harness\nAstraios Models\nAstraios LoRA 1B\nhttps://huggingface.co/bigcode/astraios-1b-lora\nAstraios P-Tuning 1B\nhttps://huggingface.co/bigcode/astraios-1b-ptuning\nAstraios AdapterH 1B\nhttps://huggingface.co/bigcode/astraios-1b-adapterh\nAstraios AdapterP 1B\nhttps://huggingface.co/bigcode/astraios-1b-adapterp\nAstraios Parallel 1B\nhttps://huggingface.co/bigcode/astraios-1b-parallel\nAstraios (IA)3 1B\nhttps://huggingface.co/bigcode/astraios-1b-ia3\nAstraios LoRA 3B\nhttps://huggingface.co/bigcode/astraios-3b-lora\nAstraios P-Tuning 3B\nhttps://huggingface.co/bigcode/astraios-3b-ptuning\nAstraios AdapterH 3B\nhttps://huggingface.co/bigcode/astraios-3b-adapterh\nAstraios AdapterP 3B\nhttps://huggingface.co/bigcode/astraios-3b-adapterp\nAstraios Parallel 3B\nhttps://huggingface.co/bigcode/astraios-3b-parallel\nAstraios (IA)3 3B\nhttps://huggingface.co/bigcode/astraios-3b-ia3\nAstraios LoRA 7B\nhttps://huggingface.co/bigcode/astraios-7b-lora\nAstraios P-Tuning 7B\nhttps://huggingface.co/bigcode/astraios-7b-ptuning\nAstraios AdapterH 7B\nhttps://huggingface.co/bigcode/astraios-7b-adapterh\nAstraios AdapterP 7B\nhttps://huggingface.co/bigcode/astraios-7b-adapterp\nAstraios Parallel 7B\nhttps://huggingface.co/bigcode/astraios-7b-parallel\nAstraios (IA)3 7B\nhttps://huggingface.co/bigcode/astraios-7b-ia3\nAstraios LoRA 16B\nhttps://huggingface.co/bigcode/astraios-lora\nAstraios P-Tuning 16B\nhttps://huggingface.co/bigcode/astraios-ptuning\nAstraios AdapterH 16B\nhttps://huggingface.co/bigcode/astraios-adapterh\nAstraios AdapterP 16B\nhttps://huggingface.co/bigcode/astraios-adapterp\nAstraios Parallel 16B\nhttps://huggingface.co/bigcode/astraios-parallel\nAstraios (IA)3 16B\nhttps://huggingface.co/bigcode/astraios-ia3\nTable 6: Used and produced artifacts.\n18\nC\nContributions\nTerry Yue Zhuo trained 1B and 3B models, conducted most evaluations, analyzed the results, wrote\nthe paper and led the project. Armel Zebaze trained 7B and 15B models, evaluated 15B models\non Code Synthesis and Code Repair, analyzed the results and helped edit the paper. Nitchakarn\nSuppattarachai evaluated two comprehension tasks. Niklas Muennighoff advised on the experiments\nand helped with plotting. Niklas Muennighoff, Qian Liu, Harm de Vries and Leandro von Werra\nprovided suggestions and helped edit the paper.\nD\nInstruction Tuning\nAll the instruction tuning experiments have been conducted on A100 80G GPUs. For all PEFT\nstrategies, we use the 8-bit quantized base models for training. For FFT, we use the original base\nmodels without quantization.\nLoRA\nWe use the attention dimension of 8, the alpha parameter of 16, dropout probability of 0.05,\nand target modules of \"[c_proj, c_attn, q_attn]\". We keep the other hyperparameters as default.\nP-Tuning\nWe use the 30 virtual tokens and remain the other hyperparameters as default.\nAdapterH\nWe use target modules of \"[c_fc, mlp.c_proj]\". We keep the other hyperparameters as\ndefault.\nAdapterP\nWe use target modules of \" [mlp.c_proj]\". We keep the other hyperparameters as default.\nParallel\nWe use target modules of \"[c_fc, mlp.c_proj]\". We keep the other hyperparameters as\ndefault.\n(IA)3\nWe target modules of \"c_attn, mlp.c_proj]\" and feedforward modules of \" [mlp.c_proj]\".\nPrompt (Lester et al., 2021)\nWe use the 30 virtual tokens and keep the other hyperparameters as\ndefault.\nAdaLoRA (Zhang et al., 2022a)\nWe use the target average rank of the incremental matrix of 8, the\ninitial rank for each incremental matrix of 12, 200 steps of initial fine-tuning warmup, 1000 step of\nfinal fine-tuning, the alpha parameter of 16, dropout probability of 0.05, the time interval between two\nbudget allocations of 10, EMA for sensitivity smoothing of 0.85, EMA for uncertainty quantification\nof 0.85, and target modules of \"[c_proj, c_attn, q_attn]\". We keep the other hyperparameters as\ndefault.\nE\nEvaluation Setup\nDevign\nWe generate the outputs with a max length of 512 tokens in the style of greedy decoding.\nAll other parameters are defaulted in Ben Allal et al. (2022). For the one-shot example, we randomly\nsample from the train set.\nBigCloneBench\nWe generate the outputs with a max length of 512 tokens in the style of greedy\ndecoding. All other parameters are defaulted in Ben Allal et al. (2022). For the one-shot example, we\nrandomly sample from the train set.\nHumanEvalPack\nWe generate 20 outputs per example with a max length of 2048 tokens and a\ntemperature of 0.2. All other parameters are defaulted in Ben Allal et al. (2022).\nReCode\nWe generate the outputs with a max length of 1024 tokens in the style of greedy decoding.\nAll other parameters are defaulted in Ben Allal et al. (2022).\n19\nAsleep At The Keyboard\nWe generate 20 outputs per example with a max length of 1024 tokens\nand a temperature of 0.2. All other parameters are defaulted in Ben Allal et al. (2022).\nF\nFailure of Scaling\n25\n50\n75\n100\n125\n150\n175\n200\nGlobal Step\n1.4\n2\nTest Loss\nAdaLoRA\nAdaLoRA-Ablation\nPrompt\nPrompt-Ablation\n1B model.\n25\n50\n75\n100\n125\n150\n175\n200\nGlobal Step\n1.4\n2\nTest Loss\nAdaLoRA\nAdaLoRA-Ablation\nPrompt\nPrompt-Ablation\n3B models.\nFigure 10: Test loss of selected models across training time measured by Global Step.\n1B\n3B\nModel Size\n1\n2\nTrain Loss\nAdaLoRA\nAdaLoRA-Ablation\nPrompt\nPrompt-Ablation\n1B\n3B\nModel Size\n1\n2\nTest Loss\nAdaLoRA\nAdaLoRA-Ablation\nPrompt\nPrompt-Ablation\nFigure 11: Final loss across model sizes.\nDuring the initial experiment, we also train the models with Prompt Tuning (Lester et al., 2021) and\nAdaLoRA (Zhang et al., 2022a). Although the loss continues decreasing when the training time\nincreases, we observe the phenomenon of model size scales in contrast to Section 2.2. As shown\nin Figure 11, the final loss of these two tuning strategies consistently increases as the model size\nincreases, which is contrary to what we observe for other PEFT methods. In the new version of\nLLM-Adapter (Hu et al., 2023), we notice that the learning rate has been specifically mentioned. For\nPrompt Tuning, the authors use 3 \u00d7 10\u22122 instead of 3 \u00d7 10\u22124, which is used in their other selected\nPEFT strategies. Therefore, we hypothesize that some tuning strategies may require a much higher\nlearning rate to achieve optimal performance. We further try a few learning rates on training 1B\nand 3B StarCoderBase models and find that 3 \u00d7 10\u22122 works well for Prompt Tuning. In addition,\n3 \u00d7 10\u22122 and 1 \u00d7 10\u22123 also work much better for AdaLoRA. With the new set of learning rates,\nwe find that these tuning strategies are aligned with our findings in Section 3. Different from the\nconclusion of Kaplan et al. (2020) that the choice of learning rate schedule is mostly irrelevant in\nlanguage model pre-training, we suggest that hyperparameters of learning rate schedule may matter a\nlot for scaling parameter-efficient language model on fine-tuning.\n20\nG\nVisualization on HumanEvalPack\n1B\n3B\n7B\n16B\nModel Size\n15\n20\n25\n30\n35\nPass@1\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nPython Code Synthesize\n1B\n3B\n7B\n16B\nModel Size\n5\n10\n15\n20\n25\n30\n35\n40\nPass@1\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nJava Code Synthesize\n1B\n3B\n7B\n16B\nModel Size\n5\n10\n15\n20\n25\n30\nPass@1\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nPython Code Repair\n1B\n3B\n7B\n16B\nModel Size\n0\n5\n10\n15\n20\n25\n30\nPass@1\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nJava Code Repair\n1B\n3B\n7B\n16B\nModel Size\n10\n15\n20\n25\n30\nPass@1\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nPython Code Explain\n1B\n3B\n7B\n16B\nModel Size\n6\n8\n10\n12\n14\n16\n18\nPass@1\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nJava Code Explain\nFigure 12: Pass@1 results of ASTRAIOS models on HumanEvalPack.\n21\nH\nMitigating Inverse Scaling\n1B\n3B\n7B\n16B\nModel Size\n30\n35\n40\n45\n50\n55\nAccuracy\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nFigure 13: Results on Defect Detection with\n1-shot demonstration.\n1B\n3B\n7B\n16B\nModel Size\n0\n5\n10\n15\n20\n25\nAccuracy\n(IA)3\nAdapterH\nAdapterP\nFFT\nLoRA\nP-Tuning\nParallel\nFigure 14: Results on Clone Detection with\n1-shot demonstration.\nWe have attempted to see if the inverse-scaling-like patterns in code comprehension tasks can\nbe mitigated and more aligned with scaling laws. As Wei et al. (2022) have shown that 1-shot\ndemonstrations can make all inverse scaling tasks U-shaped or flat, we try to see if 1-shot examples\ncan help with defection detection and clone detection. To select the 1-shot examples, we randomly\nsample a fixed sample from the train set of each benchmark. We re-evaluate all ASTRAIOS models on\nthe two tasks and present the results in Figures 13 and 14. For defect detection, all PEFT strategies\nbecome flatter than the previous patterns, which is similar to what Wei et al. (2022) observe. However,\nfor clone detection, the patterns of some tuning strategies like LoRA and FFT do not turn flat.\nAlthough the performances of LoRA and FFT have been scaling up to 7B, they decrease at 15B. We\nhypothesize that our size scaling is still not significant enough to represent an increasing pattern after\n15B for LoRA and FFT with 1-shot demonstrations.\nI\nFurther Discussion\nWe further measure the correlations among final loss in Section 3, overall task performance in\nSection 4, and numbers of updated parameters via three metrics, Kendall (\u03c4), Pearson (rp), and\nSpearman (rs) coefficients. Kendall coefficient measures the ordinal association and is robust against\noutliers, making it useful for non-normal data distributions. Pearson\u2019s coefficient assesses linear\ncorrelation, which is ideal for normal data distributions with expected linear relationships. Spearman\u2019s\ncoefficient, like Kendall coefficient, is a non-parametric measure that assesses rank correlation, useful\nfor identifying monotonic but non-linear relationships.\nTable 7: Correlations between trainable parameters and final loss. p-values are provided in gray.\nModel Size\nTrain Loss\nTest Loss\n\u03c4\nrp\nrs\n\u03c4\nrp\nrs\n1B\n.4286\n.3113\n.6071\n.3333\n.3358\n.4643\n3B\n.5238\n.3433\n.7143\n.2381\n.3835\n.4286\n7B\n.5238\n.3555\n.7143\n.2381\n.4091\n.4286\n16B\n.5238\n.3524\n.7143\n.2381\n.3986\n.4286\nOverall\n.4339 (.00)\n.3328 (.08)\n.5616 (.00)\n.3598 (.01)\n.3308 (.09)\n.4953 (.01)\nWe compute the correlations between updated parameters of ASTRAIOS models and the final loss\nof corresponding models in Table 7. From the table, we first observe that the updated parameters\nare more correlated to the final train loss than the test loss. However, they all imply that there is a\nmoderated correlation, which can be used for cross-entropy loss in model training. We also observe\nthat when we aggregate all statistics across model sizes, the correlations may slightly decrease.\nWe compute the correlations between the model loss and their mean downstream scores calculated in\nSection 4. We show the results in Table 8, where we compute correlations for each model size and\n22\nTable 8: Correlations between final loss and overall task performance. p-values are provided in gray.\nModel Size\nTrain Loss\nTest Loss\n\u03c4\nrp\nrs\n\u03c4\nrp\nrs\n1B\n-.2381\n-.4319\n-.285\n.04\n-.4328\n-.0357\n3B\n.5238\n.7819\n.7143\n.8095\n.7859\n.9286\n7B\n.5238\n.7165\n.6786\n.8095\n.8230\n.9286\n16B\n.3333\n.8096\n.5000\n.8095\n.9211\n.8929\nOverall\n.7302 (.00)\n.9027 (.00)\n.9201 (.00)\n.8466 (.00)\n.9277 (.00)\n.9579 (.00)\nthe final aggregated statistics. Our observation on the size-level correlations indicates that the task\nperformance of 1B models is hard to align with the final loss, while bigger models tend to be much\nmore correlated to both train and test loss. We explain the hypothesis that 1B models do not have\nenough capability to learn instructions. When aggregating the data points, we find that correlations\nare much stronger than the size-level prediction. The strong correlations imply that model loss on\nthe general instruction data can work as a good proxy of downstream tasks in Code LLMs. When\ncomparing the correlations on train loss to the test loss, we observe the correlations are stronger\non the latter one. This can be explained by the fact that models tend to FFT on the training data,\nwhere the loss on the train split can not generalize well on the unseen tasks and data. Moreover,\nwe also ask: What is the relationship between the downstream task performance and the updated\nparameters? Therefore, We investigate the correlation between tuned parameters and cumulative\nscores. The correlations are 0.3016 (.02), 0.4128 (.03) and 0.4138 (.03) for Kendall, Pearson and\nSpearman correlations, respectively. We draw the conclusion \u2013 Possible.\nJ\nLimitations and Future Work\nExperiment Noise\nWe observe that our empirical results are based solely on a single run of each\ntask, due to budget constraints that prevent us from tuning and evaluating the same Code LLMs\nmultiple times. Although the single evaluation approach limits the breadth of our results and may\nintroduce unexpected experiment noise, it provides a preliminary insight into the performance and\npotential of PEFT in different scenarios. Future investigations with multiple runs are necessary to\nestablish more robust conclusions and understand the variance and reliability of our results.\nFair Evaluation\nTo compare different PEFT strategies fairly, we have used the same training\nconfigurations described in Section 2.2. However, as we find that some PEFT strategies like Prompt\nTuning may be sensitive to the training hyperparameters in Section 3, the consistent configurations\ncan be unfair. On the other hand, finding the optimal hyperparameters for each PEFT strategy is\nimpractical and can cost more than training with FFT. A more efficient approach is to reuse the\nhyperparameters in previous work, which motivates us to adopt the default settings in the PEFT\nlibrary and LLM-Adapter framework. Meanwhile, we believe there may be other practical approaches\nto benchmark PEFT strategies, encouraging the community to investigate further.\nPEFT Strategy\nWe notice that there are many more PEFT strategies (Karimi Mahabadi et al., 2021;\nZaken et al., 2022; Wang et al., 2022b; Edalati et al., 2022) have been proposed recently. Due to the\nlimited computation budget, we do not include them all in our ASTRAIOS model suite. However, we\nhave publicly made all our source code, data, and models available. We encourage future development\nin analyzing PEFT strategies on Code LLMs, which helps design more efficient PEFT strategies.\nData Scaling\nOne limitation of our work is that we do not verify the validity of data scaling on\nPEFT strategies. However, this factor has been well-studied in various works (Kaplan et al., 2020;\nHoffmann et al., 2022; Muennighoff et al., 2023b) for model pre-training and fine-tuning. As we find\nthat the performance of PEFT on Code LLMs monotonically increases when scaling up the model\nsize and training time, these selected PEFT strategies are likely aligned with the previous findings of\ndata scaling. We recommend further verification on this aspect.\nModel Architecture\nAnother limitation of our study is that we do not vary the model architecture\nof Code LLMs. It is possible that some findings may not generalize to other encoder-decoder Code\n23\nLLMs like CodeT5 (Wang et al., 2021) and CodeT5+ (Wang et al., 2023b). However, as StarCoder is\nbuilt upon the enhanced GPT-2 (Radford et al.) architecture, we believe that our observations can be\ntransferred to other GPT-based LLMs.\nScaling Parameter-Constrained Language Models\nAlthough we demonstrate the possibility of\npredicting the final loss based on the updated parameters and vice versa, we note that a scaling law\ngenerally needs more than 100 models and their final loss. Ideally, the training experiments should\nbe consistent with different PEFT strategies, meaning that training hundreds of models is needed.\nFurthermore, task performance is hard to predict, as there is much more noise in the downstream\ntasks than the final loss. We foresee that predicting such overall performance is very challenging.\nK\nPrompts\nThe prompting format can significantly impact performance. In the spirit of true few-shot learn-\ning (Perez et al., 2021), we do not optimize prompts and go with the format provided by the respective\nmodel authors or the most intuitive format if none is provided. For each task not designed for\nevaluating instruction-tuned Code LLMs, we define an instruction. The instruction is to ensure that\nmodels behave correctly and that their outputs can be parsed effortlessly.\nQuestion: {context}\nIs there a defect in the Code, and respond to YES or NO.\nAnswer:\nFigure 15: Prompt for Devign.\nQuestion: Code 1: {context_1}\n.\nCode 2: {context_2}\nIs there a clone relation between the Code1 and Code2, and respond to YES or NO.\nAnswer:\nFigure 16: Prompt for BigCloneBench.\nQuestion: {instruction}\n{context}\nAnswer:\n{function_start}\nFigure 17: Prompt for HumanEvalPack.\n24\nQuestion: Create a Python script for this problem.\nAnswer: {function_start}\nFigure 18: Prompt for Code Completion on ReCode.\nQuestion: Create a script for this problem.\nAnswer: {function_start}\nFigure 19: Prompt for Asleep At The Keyboard.\nL\nTimeline\nSep/2023\nExperiment Design; Model Training; Model Evaluation.\nOct/2023\nModel Training; Evaluation Discussion; Model Evaluation.\nNov/2023\nModel Evaluation; Result Discussion; Paper Writing.\nDec/2023\nPaper Finalization; Codebase Construction.\n25\n"
  },
  {
    "title": "COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training",
    "link": "https://arxiv.org/pdf/2401.00849.pdf",
    "upvote": "14",
    "text": "COSMO: COntrastive Streamlined MultimOdal Model with\nInterleaved Pre-Training\nAlex Jinpeng Wang1\nLinjie Li2\nKevin Qinghong Lin1\nJianfeng Wang2\nKevin Lin2\nZhengyuan Yang2\nLijuan Wang2\nMike Zheng Shou 1\n1Show Lab, National University of Singapore\n2Microsoft Azure AI\nhttp://fingerrec.github.io/cosmo\nAbstract\nIn the evolution of Vision-Language Pre-training, shift-\ning from short-text comprehension to encompassing ex-\ntended textual contexts is pivotal.\nRecent autoregressive\nvision-language models like [2, 14], leveraging the long-\ncontext capability of Large Language Models, have ex-\ncelled in few-shot text generation tasks but face challenges\nin alignment tasks. Addressing this gap, we introduce the\ncontrastive loss into text generation models, presenting the\nCOntrastive-Streamlined MultimOdal framework (CosMo),\nstrategically partitioning the language model into dedicated\nunimodal text processing and adept multimodal data han-\ndling components. CosMo, our unified framework, merges\nunimodal and multimodal elements, enhancing model per-\nformance for tasks involving textual and visual data while\nnotably reducing learnable parameters.\nHowever, these\nmodels demand extensive long-text datasets, yet the avail-\nability of high-quality long-text video datasets remains lim-\nited.\nTo bridge this gap, this work introduces Howto-\nInterlink7M, an inaugural interleaved video-text dataset\nfeaturing comprehensive captions, marking a significant\nstep forward. Demonstrating its impact, we illustrate how\nHowto-Interlink7M enhances model performance in image-\ntext tasks. With 34% learnable parameters and utilizing\n72% of the available data, our model demonstrates signif-\nicant superiority over OpenFlamingo [3]. For instance, in\nthe 4-shot flickr captioning task, performance notably im-\nproves from 57.2% to 65.1%. The contributions of CosMo\nand Howto-Interlink7M are underscored by notable perfor-\nmance gains across 14 diverse downstream datasets encom-\npassing both image-text and video-text tasks.\n1. Introduction\nThe emergence of Large Language Models (LLMs) [35, 47,\n61] has significantly propelled the development of multi-\nFigure 1.\nAdvancements in Vision-Language Pre-training\n(VLP) have transitioned towards accommodating long-form\ntext inputs.\n(a).\nEarlier studies emphasized short, paired\nimage/video text correlations, exemplified by works such as\nCLIP [39] and GiT [51].\n(b).\nPresent research emphasizes\nin-context learning strategies, showcased by approaches like\nFlamingo [2] and Palm-E [14]. LLMs\u2019 exceptional text-processing\nenables effortless integration of lengthy documents, showcasing\nrobust few-shot learning sans extensive fine-tuning.\nmodal learning paradigms. A notable advantage lies in the\ncapacity of LLMs to effectively process extensively lengthy\ntextual inputs, with strong reasoning capabilities [7, 56].\nThis ability represents a significant stride forward in the do-\nmain of Natural Language Processing, underscoring the po-\ntential of LLMs in addressing complex, multi-dimensional\ndata. The success of LLMs has spurred considerable inter-\nests and efforts in leveraging it for multi modalities.\nIn-context learning [6, 12] provides a possible pathway\nfor models to accept long text inputs in the realm of multi-\nmodal learning.\nRecent advancements in employing in-\ncontext learning within multi-modal LLMs have catalyzed\nthe development of Unified Models with Emerging Capa-\nbilities, exemplified by Flamingo [2] and PALM-E [14],\nshowcased in Figure 1. These unified frameworks offer the\nremarkable ability to address numerous downstream tasks\nwithout fine-tuning. This capability is partly attributed to\narXiv:2401.00849v1  [cs.CV]  1 Jan 2024\ntheir architectural design, which supports the utilization of\nmultiple image-text pairs as input and organizes the data\ninto an \u201cinterleaved\u201d format. While these models have ex-\nhibited remarkable success in tasks such as Visual Question\nAnswering (VQA) and Captioning, the architecture pro-\nposed by Flamingo [2] is not optimally suited for classifica-\ntion tasks as its inherent design focuses on text generation\nrather than classification.\nHigh-quality interleaved data is required to enable mod-\nels with multimodal in-context learning. However, the ma-\njority of publicly available datasets, such as CC3M [44],\nLAION400M [43], and DataComp1B [42], predominantly\nconsist of short image-text pairs. Recent efforts by CM3\ndataset [1], MMC4 [63] and Obelics [25] have introduced\nthree publicly accessible interleaved datasets, based on web\ndocuments. However, web documents can be noisy, as the\nimages on the same page might not be highly correlated (see\nFigure 4). Compared to web documents, videos, naturally\nencompass highly-correlated image sequences. In response,\ninitiatives like InternVid [53] and Video Chapters [58] have\nproposed longer captions generated by language models or\nuser-annotated chapters. Despite these advancements, the\navailability of interleaved video-text data, demonstrating re-\nlationships across different clips, remains deficient.\nTo address these challenges, this paper introduces a\nnovel architecture capable of processing four distinct types\nof inputs, including interleaved data, aiming to rectify\nthe limitations observed in Flamingo.\nOur approach in-\nvolves dividing the LLM into two segments: the first seg-\nment specializes as a text encoder, while the second part\nis used for multimodal fusion.\nAdditionally, we present\nHowto-Interlink7M, a high-quality interleaved video-text\ndataset derived from Howto100M [34], by leveraging GPT-\n4 [35].\nCosMo is evaluated across a total of 14 image-\ntext and video-text benchmarks, achieving superior per-\nformance compared to Open-Flamingo [3], while utiliz-\ning fewer samples from the same public datasets. Our re-\nsults further show that high-quality video-text data from\nour Howto-Interlink7M further enhances the performance\nof CosMo, even helps on image-text tasks.\nOur key contributions include: (i). We introduce a novel\narchitecture CosMo for interleaved data pre-training, lever-\naging an additional contrastive loss. With only 34% learn-\nable parameters, our method outperform [3] clearly. (ii).\nWe introduce Howto-Interlink7M, a noteworthy addition to\nlong text multi-modality datasets. (iii). We show that top-\ntier interleaved video-text data boosts model performance\nin various image-text and video-text tasks.\n2. Related Work\nVision-Language Pretraining.\nThe evolution of Lan-\nguage Modeling has significantly impacted vision-language\npre-training methodologies.\nTraditional approaches such\nas OSCAR [29], ViLT [23] and UNITER [10], built\nupon BERT [11] language architectures, have demonstrated\nprowess in downstream tasks without requiring extensive\nfine-tuning. The focus, however, pivots towards harness-\ning larger language models [28, 48].\nFor instance, the\nFlamingo [2] has escalated from a 1.4B to a staggering 70B\nparameter language model, showcasing robust performance\nacross various downstream tasks.\nNotably, Flamingo\u2019s\narchitecture is adept at handling interleaved image/video\ntext datasets through specialized input formats and cross-\nattention mechanisms.\nYet, its performance falls behind\ncontrastive models in classification tasks, a limitation com-\npared with other works like CLIP [39] and CoCa [59],\nwhich thrive on contrastive learning paradigms.\nTo capitalize on these insights, we adopt Flamingo\u2019s\ninput design and incorporate the contrastive loss within\nmiddle-level LLM representations, to further enhance the\nalignment between visual and text representations.\nInterleaved Data for Multi-modality Learning.\nAc-\nquiring manually annotated vision-language datasets is\nprohibitively expensive, leading to relatively small-scale\ndatasets (often smaller than 100k instances) such as\nCOCO [31] and Visual Genome [24]. Traditionally, vision-\nlanguage datasets are mainly composed of image-text pairs\nfrom Internet, such as CC3M [44] and WIT in CLIP [39].\nThe text captions in these web-crawled datasets, are mostly\nalt-text descriptions, which are mostly short in length, hence\nless descriptive.\nAn innovative shift was introduced by\nFlamingo [2], pioneering the concept of long-form image-\ntext pairs. Recent advancements from Flamingo [2] and\nCM3 [1] emphasized the significance of training on en-\ntire multimodal webpages, presenting interleaved images\nand text as a cohesive sequence. These interleaved datasets\ninherently encompass multiple image and text pairs, con-\ntributing to the evolving landscape of multimodal learning.\nThe MMC4 dataset [63] notably stands as the initial pub-\nlicly available work directly addressing multi-image/multi-\nsentence interleaved data. However, MMC4 is missing a\ncrucial detail, the exact placements of images within the\ndocument structure, which is addressed in obelics [25].\nExisting research has predominantly concentrated on ex-\ntracting highly correlated image-text data from noisy web\ndocuments.\nIn contrast, our work introduces a pioneer-\ning interleaved video-text dataset, marking a departure from\nsolely image-text modalities.\n3. Howto-Interlink7M\nMotivation.\nThe\neffectiveness\nof\nvideo-language\npre-training often hinges on the availability of high-\nquality annotated captions.\nExisting datasets such as\nHowto100M [34] and YT-Temporal [60] predominantly\nrely on YouTube videos with texts generated by Automatic\nFigure 2. Comparing Conventional Video-Text Datasets with Our Howto-Interlink7M. Left: Conventional video-text datasets typically\ncontain brief ASR captions describing videos. Right: In Howto-Interlink7M, to include more details and improved video coherence, videos\nare segmented into shots. Following this, the GPT-4 model [35] is employed to annotate each shot based on historical context and detailed\nannotations include ASR, captions, and dense captions. We highlight hard object labels as well as connectives between clips.\nDatasets\nVideos\nClips\nDocs\nToken\nSim.\nHowTo100M [34]\n1.22M\n136.6M\n-\n30\n0.22\nHowto-Interlink7M\n1M\n7M\n1M\n55\n0.32\nTable 1.\nData statistics of Howto-Interlink7M. The last two\ncolumns are average token length and CLIP [39] image-text simi-\nlarity score for each clip, respectively.\nSpeech Recognition (ASR). However, these ASR-generated\nannotations suffer from weak alignments with the video\ncontent. As observed in Howto100M, only 51% of clips\nfeature objects or actions mentioned in their captions are\nvisibly depicted in the sampled clips [34]. This limitation\nposes a significant challenge to the vision-language pre-\ntraining community. To address this issue, we introduce\nHowto-Interlink7M, a novel interleaved video-text dataset\naiming to provide high-quality annotations for improved\nvideo-language understanding.\nGeneration Pipeline.\nTo construct Howto-Interlink7M,\nwe start with the publicly available HowTo100M dataset,\nbut undertake a distinctive approach. Diverging from the\noriginal dataset, we segment the original videos into shots\nusing a shot detector, specifically KTS [38]. For each shot,\nwe employ off-the-shelf caption models like BLIP2 [28]\nto generate concise descriptions. Furthermore, we employ\nGRIT [54] to produce dense region captions with bound-\ning box, enhancing the captions with rich descriptions and\nposition details. For simplicity, we name all these infor-\nmation as detailed annotation. For first clip of video, we\nleverage GPT-4 [35] model to generate comprehensive sum-\nmaries from detailed annotation only. Importantly, subse-\nquent clips\u2019 captions are conditioned on the context of pre-\nceding clips, maintaining narrative continuity. We provide\nexplicit instructions to GPT-4, emphasizing the preservation\nof ASR information to retain the nouns and actions. This\napproach strengthens the interconnection between individ-\nual clips, fostering a more coherent caption.\nMethod\nLanguage model\nVision Model\n#CE\nCosMo-2B\nOPT-IML1.8B [21]\nOpen-Clip ViT-L/14 [20]\n6\nCosMo-3.4B\nRedPajama-3B [47]\nOpen-Clip ViT-L/14 [20]\n4\nCosMo-8.1B\nMistral7B [22]\nOpen-Clip ViT-L/14 [20]\n4\nTable 2. Architecture details of the CosMo. #CE is short for the\nnumber of Cross Attention layers.\nAnalysis.\nIn Figure 2, we showcase an example illustrat-\ning the comprehensive and detailed nature of the generated\nparagraphs. Notably, our observations indicate the preser-\nvation of specific details across different clips, including the\nnames of actors and mentioned objects like food. Moreover,\nwe conduct a comparative analysis between the original\nHowTo100M [34] dataset and our Howto-Interlink7M, as\npresented in Table 1. The generated captions exhibit greater\nlength and improved alignment with the video content, as\nevidenced by the higher CLIP similarity scores.\n4. CosMo\nIn this section, we introduce CosMo, short for COntrastive-\nStreamlined MultimOdal framework, which strategically\npartitioning a LLM into dedicated unimodal text processing\nand adept multimodal data handling components. CosMo\nadds an additional contrastive loss to the language model\nloss in Flamingo [2, 3] baselines, supporting both clas-\nsification and generation tasks.\nWhile ALBEF [27] and\nCoCa [59] also integrate contrastive loss, our emphasis\nlies in developing LLMs specialized in in-context learn-\ning and handling extensive interleaved data sequences, set-\nting our work apart from these methods. Additionally, we\nstreamline our model architecture, reducing the number of\nlearnable parameters for improved computational efficiency\nwhile maintaining efficacy in multi-modality learning.\n4.1. Overall Architecture\nAs shown in Figure 3, CosMo consists of two components:\na visual encoder and a pre-trained LLM. The visual encoder,\nbased on the Vision Transformer (ViT) [13] from Open-\nCLIP [20], remains consistent across our experiments. For\nFigure 3. An introduction to CosMo: This model handles both\nimage/video text pairs and inter-level image/video text pairs. The\nLarge Language Model is divided into two parts to compute con-\ntrastive loss and language modeling loss.\nthe language model, we primarily employ the OPT [61]\nmodel, partitioned into two segments to accommodate di-\nverse tasks while reducing overall parameters.\nInput Representation as Document.\nCosMo accepts\nfour types of data:\nimage-text, video-text, interleaved\nimage-text,\nand interleaved video-text,\nall processed\ninto a document-style text sequence format.\nThis for-\nmat encapsulates visual and textual information, struc-\ntured as \u201c<s><Visual 1>Text1 <EOC><Visual 2>Text2\n<EOC>\u201d, with <s> marking the document start and\n<EOC> denoting the end of each text caption. The <Vi-\nsual> token represents the presence of an image or video.\nTo effectively handle longer documents while con-\nstrained with a fixed GPU memory budget, we implement a\nrandom sampling strategy to gather inputs with a fixed num-\nber of tokens. We first determine the position of the <Visual\n> token, and randomly select one image as the anchor. Sub-\nsequently, we introduce a slight random shift to the image\nposition, and then sample 128 tokens subsequently. The\nimages or videos corresponding to the <Visual > tokens\nserves as inputs to the visual encoder.\nUni-modality Feature Extraction.\nTo mitigate catas-\ntrophic forgetting, inspired by recent works [2, 15], we\nfreeze both the LLMs and the vision encoder. Images or\nvideos are directly fed into the frozen vision encoder, while\ndocuments are input into the first half layers of the LLM.\nLightweight Multi-modal Fusion.\nThen, we leverage vi-\nsual tokens to condition the frozen language model block\nthrough gated cross-attention layers, which share a similar\ndesign with Flamingo [2] and CoCa [59].\nThis strategy\neffectively integrates visual information for precise next-\ntoken prediction. However, a key difference from previous\nFigure 4. Instances of Low-Similarity Images: In datasets like\nMMC4 [63], based on raw website content, there are often incon-\ngruent images that don\u2019t align with the accompanying text, leading\nto training instability.\nmethods is that we introduce bottlenecks [18, 46] in input\nand output feature channels, resulting in a substantial re-\nduction in learnable parameters. Specifically, we compress\nthe feature channel dimension to one half at the beginning\nand raise it again at last. Moreover, cross-attention layers\nare strategically introduced at regular intervals, specifically\nevery 2 blocks for CosMo-2B, and 4 for CosMo-3.4B.\nLoss:\nFor document feature x and vision feature z, the\nmodel predicts the next word y and the loss is computed as:\np(y|x) = \u2212\ni=T\nX\ni=1\nlog p(y|x, z),\n(1)\nwhere T is the length of the input text. The final loss func-\ntion is composed of both language modeling loss (Ll) and\ncontrastive loss (Lc):\nL =\ni=N\nX\ni=1\n\u03bb1Ll + \u03bb2Lc,\n(2)\nwhere N is the number of data types (3 by default).\nTo this end, our CosMo is trained to adeptly handle in-\nterleaved text and visual sequences, naturally enabling its\napplication in in-context few-shot learning scenarios.\n4.2. Enhancing Image-Text Alignment\nThe Vision Encoder generally stems from the CLIP [39]\nmodel, meticulously trained with contrastive loss to pre-\nserve rich and discriminative information. Preceiver resam-\npler [2] obtains spatio-temporal features from the Vision\nEncoder, yielding a fixed number of visual tokens. But the\nrisk of missing discriminative details persists, potentially\nleaving certain information unclear.\nContrarily, our approach extends this process by incor-\nporating a learnable query to globally attend to all tokens,\nincluding an additional learnable query for Text Fusion Lay-\ners. This modification allows us to attain a more compre-\nhensive understanding of the entire token set. Subsequently,\nthe model employs a projection head to unify visual and text\nembeddings into the same dimensionality. The training ob-\njective centers around optimizing the contrastive loss.\nData Type\nDataset\nSample\nImage-Text\nCC3M [44]\n0.75M\nSBU [36]\n0.3M\nLAION400M [43]\n30M\nDataComp1B [42]\n65.6M\nInterlevel Image-Text\nMMC4 [63]\n30M\nVideo-Text\nWebVid [4]\n2.5M\nInterlevel Video-Text\nHowto-Interlink7M\n0.9M\nTotal\n-\n130M\nTable 3. Statistics of the Pre-training Dataset: We extract a sub-\nset from the complete dataset using clustering and filtering.\n4.3. Interleaved Data Preprocessing\nImage-text\nSimilarity\nMatrix.\nIn\nthe\ninterleaved\nMMC4 [63] dataset, each document (typically a website)\ncontains a text list and an image list extracted from the\ndocument. Also, this dataset provides pairwise image-text\nsimilarity computed using the CLIP [39] model. Due to the\nabsence of image positions within the document, during\npre-training, Open-Flamingo [3] select the matching index\nutilizing Optimal Transport [49] from the similarity matrix.\nData Filtering.\nThe MMC4 dataset comprises numerous\nimages with low similarity scores, which may include lo-\ngos, failed image downloads, or images completely unre-\nlated to the accompanying text, as shown in Figure 4. Train-\ning models directly on such data often results in gradient ex-\nplosions due to the inherent irrelevance between images and\ntext. To mitigate this issue, MMC4 employs a simple filter-\ning using a threshold of similarity score 0.24, computed by\nCLIP ViT-L/14. However, this approach discards a signifi-\ncant number of samples, reducing the dataset\u2019s diversity.\nTo overcome these limitations, we implement the follow-\ning strategies: (i.) Similarity Distribution: Unlike pre-\nvious methods, we introduce a disturbance matrix with a\nnormal distribution having a standard deviation within the\nrange (\u22120.04, 0.04) for the alignment score matrix. Fur-\nthermore, we clamp the min and max value to -0.08 and\n0.08, correspondingly. We then utilize optimal transport\nfor index matching. This approach allows images to match\nwith different texts within the same document. (ii.) Pre-\nserving Document Context: For images with a similar-\nity score below 0.20, we leverage a pre-trained captioning\nmodel to generate pseudo-captions to replace the matched\ntexts within the document. This operation significantly di-\nversifies the sampled documents and enhances the training\nstability. Refer to Section 6.1 for comprehensive analysis.\n4.4. Training Details\nPre-training Data: Conventional datasets often suffer from\nnoise and duplication issues. Following the approach out-\nlined in TLDR [50], we opt to sample a refined subset from\nFigure 5.\nLAION400M [43] and similar large datasets\ncommonly suffer from redundancy.\nClustering and uniform\ndistance-based sampling help alleviate this issue.\ncommonly used image-text pre-training datasets. Table 3\ndetails the specific datasets sampled for our model train-\ning. Further analysis on our data sampling methodology is\nshown in Section 5.1.\nTraining Configuration:\nAll models are trained using\nAdamW [32] optimizer with a cosine learning rate schedule,\ninitialized at 5e-4, and trained for 20 epochs. The warm-up\nratio is set to 0.03. Gradient accumulation steps align with\nthe data type, defaulting to 3 for our CosMo. We employ\nDeepSpeed [40] fp16 precision for model training, executed\nacross 128 NVIDIA V100 GPUs.\n5. Experiments\nOur primary objective is to develop models capable of swift\nadaptation to a wide array of diverse and challenging tasks.\nTo this end, we evaluate our model against numerous well-\nestablished image-text [16, 17, 31, 33, 37, 45] and video-\ntext [9, 26, 30, 52, 57, 62] benchmarks, to assess the few-\nshot in-context learning performance of CosMo.\n5.1. Pre-training Data Selection\nOur experimental setup draws inspiration from TLDR [50],\nwhere we leverage a subset of the SBU [36], CC3M [44],\nCC12M [8], LAION400M [43], and Data-comp 1B [42]\ndatasets. To obtain this subset, we begin by filtering out half\nof the data with low similarity scores. Subsequently, we em-\nploy K-means clustering and uniformly sample data from\neach cluster, resulting in a total of 100 million data points.\nWe compute the centroid of each cluster and uniform sam-\nple data according to the distance from the mcenter. An\nillustrative example of this clustering process is shown in\nFigure 5. For interleaved data, we utilize 30 million data\npoints from MMC4 [63] following a filtering process by re-\nmoving samples with too small images or no images. Also\nthe data filtering in Section 4.3.\nAblated setting\nOriginal\nChanged\nParameter\nIter.\nCaptioning (CIDER)\nVQA\nAverage\nValue\nValue\nTime\nCOCO\nFLICKR\nok-vqa\ntextvqa vizwiz\nvqav2\nBaseline\n683M/2.32G\n4.9s\n71.3\n50.9\n18.5\n15.0\n33.6\n38.4\n38.0\ni.\nContrastive\nLoss\nwo ContrastiveSingle GPU\n683M/2.32G\n5.1s\n76.0\n53.0\n20.9\n16.7\n35.5\n41.0\n40.5\nAll GPUS\n683M/2.32G\n6.1s\n73.1\n52.0\n20.4\n14.0\n26.4\n39.4\n39.7\nii.\nTraining\nData\nAll Data\nw/o Video-text\n683M/2.32G\n4.6s\n68.8\n51.6\n16.2\n14.1\n30.7\n35.3\n36.1\nw/o Interleaved\n683M/2.32G\n3.3s\n42.1\n26.0\n9.5\n6.1\n9.4\n27.2\n20.1\nw/o Image-text\n683M/2.32G\n4.5s\n44.1\n26.3\n9.6\n4.1\n6.3\n32.7\n18.5\niii.\nDataloader\nSampling\nRound-robin\nMin\n683M/2.32G\n4.9s\n73.2\n56.9\n22.1\n15.8\n33.7\n42.0\n40.6\nMax\n683M/2.32G\n4.9s\n73.2\n53.2\n19.0\n16.4\n20.0\n41.0\n37.1\niv.\nVision\nEncoder\nViT-L\nViT-B\n617M/1.98G\n4.2s\n64.7\n45.9\n19.3\n13.5\n21.4\n39.0\n34.0\nViT-G\n700M/3.28G\n7.5s\n72.1\n54.1\n20.6\n19.7\n34.0\n41.5\n40.3\nv. Layer Inter\n1\n2\n462M/2.10G\n4.5s\n71.4\n53.8\n22.3\n16.4\n28.5\n42.3\n39.1\n4\n352M/1.99G\n3.9s\n70.4\n51.3\n20.8\n15.8\n30.7\n41.8\n38.5\nvi.\nCompress\nRatio\n1\n2\n444M/2.08G\n4.3s\n71.5\n54.9\n22.1\n14.7\n31.7\n41.1\n39.3\n4\n325M/1.97G\n4.2s\n72.0\n48.0\n20.8\n12.8\n32.2\n41.3\n37.9\nvii.\nInterleaved\nLength\n64\n128\n683M/2.32G\n6.5s\n73.1\n53.5\n22.8\n15.7\n33.4\n40.8\n39.9\n192\n683M/2.32G\n9.3s\n72.8\n54.5\n22.5\n14.9\n33.4\n42.0\n40.0\nTable 4. Ablation Studies of CosMo on an 18M Subset. Our focus is primarily on presenting the 8-shot results for comparison, with the\nbaseline result positioned in the first row. We highlight the distinction between learnable and all parameters. For the captioning assessment,\nwe report CIDER, and \u2019Iter.\u2019 abbreviates Iteration. Text in indigo denotes our default setting.\n5.2. Ablation Study\nIn Table 4, we report our ablation results using our model\npre-trained on a 18M subset (12M Interleaved, 4M Image\nand 2M Video-text). The average score is computed by av-\neraging the scores across all datasets.\nSignificance of Contrastive Loss:\nAs seen in section\n(i) of Table 4, the contrastive loss plays a critical role in\nthe success of our approach.\nWe observe that using the\ncontrastive loss on both single and multiple GPUs consis-\ntently improves model performance over the baseline. Yet,\nour further investigations indicate that aggregating all con-\ntrastive losses across nodes does not provide commensurate\nbenefits. This can be attributed to the model\u2019s strong fo-\ncus on the contrastive loss, which can affect the learning\nof language modeling loss. In addition, employing the con-\ntrastive loss on all GPUs introduces a significant lag in train-\ning speed (from 4.9s to 6.1s), which increases computation\nexpenses. As a result, by default we use the contrastive loss\nover batch on each GPU.\nAll Data are important:\nAs demonstrated in section (ii)\nof Table 4, the selection of appropriate training data is im-\nportant to the model performance. Notably, the removal\nof the interleaved image-text dataset results in huge perfor-\nmance degradation in terms of the average score (from 38.0\nto 20.1), and omitting the conventional image-text pairs\nsimilarly affect the performance. Additionally, the exclu-\nsion of paired video-text datasets has a detrimental impact\non a majority of downstream tasks.\nIn light of these findings, we employ gradient accumula-\ntion, and the number of accumulation steps is set to match\nthe number of data types. This ensures that each iteration\nencompasses all data types, allowing us to leverage the full\nspectrum of data available for training. We also present var-\nious dataloader sampling strategies in row (iii) of Table 4.\nOur findings indicate that the \u201cminimum\u201d strategy outper-\nforms both \u201cmaximum\u201d and \u201cround-robin.\u201d\nThis under-\nscores the importance to ensure balanced quantities across\neach type of training data.\nVisual Encoder Size:\nIn section (iv) of Table 4, we evalu-\nate the effects of altering the size of the visual encoder. Our\nobservations reveals a general trend that larger visual en-\ncoders tend to produce slightly better results. However, this\nimprovement is offset by a corresponding increase in the\nnumber of model parameters and computational demands.\nNotably, the time required per iteration increases from 1.2\nseconds to 2 seconds when using larger visual encoders.\nConsidering this trade-off between performance gains and\ncomputational costs, we have chosen to adopt the larger vi-\nsual encoder size as the default configuration.\nLightweighting the Model:\nIn our pursuit of a more\nlightweight and efficient network, we focus on reducing the\nlearnable parameters by minimizing the number of cross-\nattention layers and compressing its associated parameters.\nThe results are presented in sections (v) and (vi). Sur-\nprisingly, reducing the number of learnable parameters does\nnot generally lead to a performance decrease. It even re-\nsults in enhanced performance, especially when employing\na layer interval of 2 and a compression ratio of 2, which we\nincorporate into our final framework.\nMethod\nSamples Para.\n#Tokens\u2193Shots\nCaptioning (CIDER)\nVQA\nClassification\nAvg \u2191\nCOCO\nFLICKR\nok-vqa\ntextvqa\nvizwiz\nvqav2\nhatefulmemes\nFlamingo-3B [2]\n2.1B\n1.4B/3.2B 256\n0\n73.0\n60.6\n41.2\n30.1\n28.9\n49.2\n53.7\n48.1\n4\n85.0\n72.0\n43.3\n32.7\n34.0\n53.2\n53.6\n53.4\n32\n99.0\n71.2\n45.9\n30.6\n45.5\n57.1\n56.3\n57.9\nOpen-\nFlamingo(3B) [3]\n180M\n1B/3B\n256\n0\n74.9\n52.3\n28.2\n24.2\n23.7\n44.6\n51.2\n42.7\n4\n77.3\n57.2\n30.3\n27.0\n27.0\n45.8\n50.6\n45.0\n32\n93.0\n61.1\n31.0\n28.3\n39.8\n47.0\n50.2\n50.0\nCosMo-2B\n130M\n340M/1.9B128\n0\n79.9\n51.3\n28.3\n21.9\n24.7\n46.7\n51.2\n43.4\n4\n91.7\n61.2\n27.3\n23.4\n30.0\n45.5\n50.6\n47.1\n32\n95.4\n64.7\n26.8\n25.6\n42.3\n43.9\n50.9\n49.9\nCosMo-\n2B+Howto-\nInterlink7M\n131M\n340M/1.9B128\n0\n81.3\n52.8\n25.5\n27.7\n27.4\n45.3\n50.5\n44.4\n4\n97.9\n65.1\n28.7\n25.8\n30.3\n47.8\n51.8\n49.6\n32\n100.1\n63.8\n25.8\n25.9\n42.1\n44.6\n49.8\n50.3\nOpen-\nFlamingo(4B) [3]\n180M\n1.3B/4B\n256\n0\n76.7\n53.6\n30.7\n21.8\n18.8\n45.1\n52.3\n42.7\n4\n81.8\n60.7\n35.1\n25.9\n26.6\n49.0\n51.5\n47.2\n32\n95.1\n56.9\n26.4\n14.1\n23.1\n43.0\n52.2\n44.4\nCosMo-3.4B\n130M\n405M/2.9B128\n0\n78.7\n55.4\n32.5\n23.4\n20.9\n46.2\n50.6\n44.0\n4\n91.4\n63.4\n35.3\n21.0\n28.4\n48.8\n52.7\n48.7\n32\n101.4\n67.4\n30.4\n20.8\n35.4\n47.8\n51.3\n50.6\nFlamingo(9B) [2]\n2.1B\n1.8B/9.3B 256\n0\n79.4\n61.5\n44.7\n31.8\n28.8\n51.8\n57.0\n50.7\n4\n93.1\n72.6\n49.3\n33.6\n34.9\n56.3\n62.7\n57.5\n32\n106.3\n72.8\n51.0\n32.8\n44.0\n60.4\n63.5\n61.5\nOpen-\nflamingo(9B) [3]\n180M\n1.9B/9B\n256\n0\n74.9\n52.3\n28.2\n24.2\n23.7\n44.6\n51.6\n42.8\n4\n77.3\n57.2\n30.3\n27.0\n27.0\n45.8\n54.0\n45.5\n32\n93.0\n61.1\n31.0\n28.3\n39.8\n47.0\n50.2\n50.1\nCosMo-8.1B\n180M\n514M/8.1B128\n0\n77.5\n58.2\n32.7\n23.5\n22.5\n47.2\n57.1\n45.5\n4\n85.5\n63.4\n33.7\n26.8\n30.5\n47.7\n58.9\n50.0\n32\n103.5\n67.7\n34.0\n28.9\n41.3\n49.2\n62.0\n55.2\nTable 5. Comparison Across Various Scales. By employing only 128 tokens in pre-training, our computational costs are significantly\nlower compared to related approaches. \u2019ILLength\u2019 abbreviates Interlevel Token Length. It\u2019s noteworthy that CosMo-3.4B and Open-\nFlamingo (4B) utilize the same LLM (RedPajama-3B) and visual encoder (Clip ViT-L/14). Rows with color indicate our own method.\nInterleaved Length Ablation:\nIn section (vii) of Table 4,\nwe explore the impact of varying the interleaved sequence\nlength within the range of 64 to 192. In general, longer se-\nquences lead to improved results. However, it\u2019s worth not-\ning that longer input sequences also introduce higher com-\nputational demands and can significantly slow down train-\ning due to increased GPU memory consumption.\nIn contrast to previous works [2, 3], which employ a se-\nquence length of 256, we opt for a sequence length of 128\nto expedite the training process, within a limited computa-\ntion budget. It is important to note that our model has the\npotential for further enhancement with the use of longer se-\nquences if budget allows.\n5.3. Few-shot Evaluation on Vision-Language Tasks\nResults on Image-Text Tasks.\nTable 5 presents a com-\nparative analysis with related works, demonstrating our\nstate-of-the-art performance.\nOur CosMo outperforms\nOpen-Flamingo [3] across nearly all benchmarks, and\nthis is achieved with a substantially reduced sample size\n(130M vs. 180M) and fewer parameters. When using the\nsame RedPajama-3B [47] model as the language model,\nour model also outperforms Open-Flamingo substantially.\nFurthermore, the incorporation of our proposed Howto-\nInterlink7M dataset leads to even better results, underscor-\ning the strength of the high-quality data. It is worth not-\ning that, during pre-training, our model is trained with only\nthree frames, yet it delivers strong results even when evalu-\nated with 32 shots.\n6. Training Details\nResults on Video-Text Tasks.\nWe test our model on\nvideo captioning and video question-answering tasks.\nWhen using an additional video dataset, we consistently ob-\nserve enhanced model performance across various aspects.\nHowever, it is important to note a significant limitation\nin the current MSVD and MSRVTT datasets: their ground\ntruth quality is compromised due to being generated via\nrule-based methods rather than human annotation. Conse-\nquently, there are instances where our model generates more\ndetailed and accurate predictions, which may not align with\nthe ground truth. For example, while the ground truth is\n\u201dCarve\u201d, our model predicts \u201dCarving sculpture\u201d. To ad-\ndress this challenge, we propose an alternative approach\nfor evaluating VQA performance. This involves evaluat-\ning text similarity using a pre-trained Language model from\nMethod\nShots\nCaptioning (CIDER)\nOpen-ended VQA\nAverage\nTVC\nMSVD\nMSRVTT\nYouCook2\nVATEX\nMSRVTT\nMSVD\nTGIF\nFlamingo-3B [2]\n0\n-\n-\n-\n55.8\n40.1\n11.0(-)\n27.5(-)\n-\nN/A\n4\n-\n-\n-\n64.6\n50.0\n14.9(-)\n33.0(-)\n-\nN/A\n32\n-\n-\n-\n76.7\n59.2\n25.6(-)\n42.6(-)\n-\nN/A\nCosMo-2B\n0\n6.0\n61.4\n31.4\n17.9\n33.5\n1.0(18.5)\n4.8(32.5)\n20.5(41.3)\n22.0\n4\n6.7\n64.2\n34.2\n21.5\n37.8\n2.3(21.4)\n8.7(34.2)\n23.2(45.2)\n24.8\n32\n8.2\n72.0\n37.5\n31.5\n36.8\n8.2(23.3)\n11.5(33.4)\n28.2(46.6)\n32.3\nCosMo-2B+Howto-\nInterlink7M\n0\n6.7\n80.2\n33.1\n19.2\n35.5\n5.1(20.3)\n15.2(34.3)\n21.5(43.3)\n27.1\n4\n8.3\n82.7\n42.3\n28.4\n39.9\n8.4(22.5)\n18.7 (35.2)\n24.3(45.8)\n31.6\n32\n12.2\n85.3\n53.5\n33.5\n42.3\n9.1(24.3)\n20.3(35.5)\n30.4(49.3)\n35.8\nTable 6. Video-text Task Comparison. We assess these datasets using open-ended generation. Parentheses indicate evaluation via text\nsimilarity matching using Language Models. Except for YouCook2, we only utilize 3 frames per video.\nFigure 6. The clip image-to-text similarity score distribution of\nMMC4. Left is all pairs and the right is matched pairs.\nNLTK [5]. The results of our evaluations are presented in\nthe brackets of Table 6. Our findings indicate that, across\na majority of the evaluated downstream tasks, our model\nconsistently delivers robust performance.\n6.1. Analysis on Interleaved Data\nVisualization of Interleaved Similarity Scores.\nWe vi-\nsualize the distribution of image-text similarity scores. The\nmajority of similarity scores fall within the range of 0.2 to\n0.4. By varying the threshold from 0.18 to 0.30 while main-\ntaining a dataset subset of 18 million, as consistent with our\nablation setting, we present the results in Table 7.\nWhen the similarity threshold is below 0.22, frequent oc-\ncurrences of gradient explosions hinder the successful train-\ning of our model. This issue predominantly arises from\nthe disruptive influence of noisy data, significantly impair-\ning training stability. Furthermore, large thresholds (e.g.,\n0.28 and 0.30) yield suboptimal results on both COCO and\nFLICKR30K, partially due to a substantial dropout of in-\nterleaved samples. To address this, we experiment with re-\nplacing low-similarity captions with those generated from\na pre-trained CosMo, resulting in more stable training. We\nhave noted that while replacing noisy captions contributes\npositively to performance enhancement, the correction of\naccurate pairs notably results in a significant decrease.\nAnalysis on Interleaved Data Sampling.\nOur empirical\nfindings underscore that a larger number of shots signifi-\ncantly improves outcomes in the experiments conducted on\nThresh\n0.18\n0.20\n0.24\n0.28\n0.30\nCOCO CIDER\nN/A\nN/A\n72.3\n64.7\n53.5\nFLICKR CIDER\nN/A\nN/A\n52.6\n44.3\n36.2\nCOCO CIDER\n65.4\n67.2\n73.2\n63.3\n45.7\nFLICKR CIDER\n42.3\n48.4\n54.3\n42.1\n32.2\nTable 7. The selection of similarity thresh score of interleaved\naffects the result. N/A means the model fail into gradient ex-\nplosion. The second line means we replace the noisy data with\ngenerated caption from pretrained CosMo.\nSubset\n0\n4\n8\n16\n32\nRandom Selection\n63.6\n66.5\n71.3\n64.7\n60.5\nMinimum Frames\n65.4\n63.2\n52.4\n43.5\n45.2\nMaximum Frames\n64.1\n66.8\n73.5\n75.5\n75.0\nTable 8. Data quality highly affects the multiple-shots ability.\ninterleaved frames. The predominant origin of most sam-\nples lies within raw images. To address potential bias and\npreserve the maximum number of frames, we implement a\nstrategic approach: replacing captions with pre-trained data\nfor images displaying exceptionally low similarity, account-\ning for approximately 5% of the dataset. This procedural\nadjustment yields a marginal yet discernible enhancement\nin the model\u2019s overall performance.\nAnalysis of Interleaved Subsets:\nIn this experiment, we\nclosely examine three unique subsets derived from the\nMMC4 dataset:\n\u2022 A randomly sampled 4 million subset.\n\u2022 4 million samples with the highest count of images.\n\u2022 4m samples that containing only a single frame.\nTo mitigate the potential issue of over-sampling docu-\nments with multiple frames, we limit the model training to\na single epoch. The detailed results, as presented in Table 8,\nreveal a significant performance gap, particularly favoring\nthe subset enriched with the most frames. This notable dis-\nparity suggests that the effectiveness in few-shot learning\nlargely stems from the integration of interleaved data com-\nMethod\nVTAB\nRetrieval\nAverage over 38 Datasets\nCosMo-2B\n31.5\n25.4\n32.7\nCosMo-3.4B\n33.2\n26.3\n35.8\nTable 9. Zero-shot performance for alignment tasks in [42].\nbined with the strategic use of LLMs.\n6.2. Zero-shot Alignment Tasks\nIn this experiment, we include zero-shot image classifica-\ntion and retrieval task. We utilize the DataComp [42] eval-\nuation pipeline to test our model\u2019s capabilities.\nSpecif-\nically, the model\u2019s performance is evaluated across 38\ndatasetswithout any training. The result is shown in Table 9.\n7. Conclusion and Limitations\nIn this work, we present a refined architecture aimed at in-\ncorporating contrastive loss into an existing autoregressive\nmulti-modality model, CosMo, tailored specifically for in-\ncontext learning across multiple modalities. But in-context\nlearning require high-quality interleaved data and there still\nno interleaved video-text dataset available. To address this\ngap, we introduce Howto-Interlink7M, a pioneering inter-\nleaved video-text dataset featuring comprehensive captions,\nmarking a significant stride in this field\u2019s advancement.\nFurthermore, the potential advantages of employing pre-\ntraining models in more downstream tasks, particularly in\nlong text tasks, warrant further exploration. Our ongoing\nendeavor involves the forthcoming release of our trained\nmodels and the dataset, aiming to foster extensive research\nin this domain.\nReferences\n[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir\nKarpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Man-\ndar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal\nmasked multimodal model of the internet.\narXiv preprint\narXiv:2201.07520, 2022. 2\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 1, 2, 3, 4, 7, 8\n[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. Openflamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\n1, 2, 3, 5, 7\n[4] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 5, 12\n[5] Steven Bird, Ewan Klein, and Edward Loper. Natural lan-\nguage processing with Python: analyzing text with the natu-\nral language toolkit. \u201d O\u2019Reilly Media, Inc.\u201d, 2009. 8, 12\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 1\n[7] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scal-\ning transformer to 1m tokens and beyond with rmt. arXiv\npreprint arXiv:2304.11062, 2023. 1\n[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3558\u20133568, 2021. 5\n[9] David Chen and William B Dolan. Collecting highly paral-\nlel data for paraphrase evaluation. In Proceedings of the 49th\nannual meeting of the association for computational linguis-\ntics: human language technologies, pages 190\u2013200, 2011.\n5\n[10] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning. In European\nconference on computer vision, pages 104\u2013120. Springer,\n2020. 2\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 2\n[12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang\nSui.\nA survey for in-context learning.\narXiv preprint\narXiv:2301.00234, 2022. 1\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3\n[14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha\nChowdhery,\nBrian\nIchter,\nAyzaan\nWahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-\ne: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023. 1\n[15] Constantin Eichenberg, Sidney Black, Samuel Weinbach,\nLetitia Parcalabescu, and Anette Frank. Magma\u2013multimodal\naugmentation of generative models through adapter-based\nfinetuning. arXiv preprint arXiv:2112.05253, 2021. 4\n[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904\u20136913, 2017. 5\n[17] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 5\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 4\n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 13, 14\n[20] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n3\n[21] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor\nMihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu\nWang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling\nlanguage model instruction meta learning through the lens of\ngeneralization. arXiv preprint arXiv:2212.12017, 2022. 3,\n13\n[22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, et al.\nMistral 7b.\narXiv preprint\narXiv:2310.06825, 2023. 3, 12, 13\n[23] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\nand-language transformer without convolution or region su-\npervision. In International Conference on Machine Learn-\ning, pages 5583\u20135594. PMLR, 2021. 2\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123:32\u201373, 2017. 2\n[25] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nObelics: An open web-scale filtered dataset of interleaved\nimage-text documents.\nIn Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Bench-\nmarks Track, 2023. 2\n[26] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr:\nA large-scale dataset for video-subtitle moment retrieval. In\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI\n16, pages 447\u2013463. Springer, 2020. 5\n[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. Advances in neural infor-\nmation processing systems, 34:9694\u20139705, 2021. 3\n[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 2, 3, 14\n[29] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al.\nOscar: Object-semantics aligned pre-training\nfor vision-language tasks. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part XXX 16, pages 121\u2013137. Springer,\n2020. 2\n[30] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault,\nLarry Goldberg, Alejandro Jaimes, and Jiebo Luo.\nTgif:\nA new dataset and benchmark on animated gif description.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 4641\u20134650, 2016. 5, 12\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 2, 5\n[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[33] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE/cvf conference on computer vision and pattern\nrecognition, pages 3195\u20133204, 2019. 5\n[34] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowto100m: Learning a text-video embedding by watching\nhundred million narrated video clips. In Proceedings of the\nIEEE/CVF international conference on computer vision,\npages 2630\u20132640, 2019. 2, 3\n[35] OpenAI. Gpt-4 technical report. 2023. 1, 2, 3\n[36] Vicente Ordonez,\nGirish Kulkarni,\nand Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. Advances in neural information processing sys-\ntems, 24, 2011. 5, 12, 14\n[37] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models.\nIn Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2641\u20132649, 2015. 5\n[38] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and\nCordelia Schmid. Category-specific video summarization.\nIn Computer Vision\u2013ECCV 2014: 13th European Confer-\nence, Zurich, Switzerland, September 6-12, 2014, Proceed-\nings, Part VI 13, pages 540\u2013555. Springer, 2014. 3\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1, 2, 3, 4, 5\n[40] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. Deepspeed: System optimizations enable train-\ning deep learning models with over 100 billion parame-\nters. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pages\n3505\u20133506, 2020. 5\n[41] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt\nSchiele. A dataset for movie description. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 3202\u20133212, 2015. 12\n[42] Alex Fang Samir Yitzhak Gadre, Gabriel Ilharco. Datacomp:\nIn search of the next generation of multimodal datasets.\narXiv preprint arXiv:2304.14108, 2023. 2, 5, 9, 12, 14\n[43] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nand Kaczmarczyk.\nLaion-400m:\nOpen dataset of clip-\nfiltered 400 million image-text pairs.\narXiv preprint\narXiv:2111.02114, 2021. 2, 5, 12, 14\n[44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018. 2, 5, 12, 14\n[45] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317\u20138326, 2019. 5\n[46] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1\u20139, 2015.\n4\n[47] Together.xyz. Releasing 3b and 7b redpajama incite family\nof models including base, instruction-tuned and chat models.\nhttps://www. together.xyz/blog/redpajama-models-v1, 2023.\n1, 3, 7, 13\n[48] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot\nlearning with frozen language models. Advances in Neural\nInformation Processing Systems, 34:200\u2013212, 2021. 2\n[49] C\u00b4edric Villani. Topics in optimal transportation. American\nMathematical Soc., 2021. 5\n[50] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao\nZhang, Stan Weixian Lei, and Mike Zheng Shou. Too large;\ndata reduction for vision-language pre-training. ICCV, 2023.\n5\n[51] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\nGit: A generative image-to-text transformer for vision and\nlanguage. arXiv preprint arXiv:2205.14100, 2022. 1, 14\n[52] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang. Vatex: A large-scale, high-\nquality multilingual dataset for video-and-language research.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 4581\u20134591, 2019. 5\n[53] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,\nXin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei\nLiu, et al.\nInternvid: A large-scale video-text dataset for\nmultimodal understanding and generation.\narXiv preprint\narXiv:2307.06942, 2023. 2\n[54] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,\nZicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-\nerative region-to-text transformer for object understanding.\narXiv preprint arXiv:2212.00280, 2022. 3\n[55] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Chris-\ntian Szegedy.\nMemorizing transformers.\narXiv preprint\narXiv:2203.08913, 2022. 14\n[56] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Chris-\ntian Szegedy.\nMemorizing transformers.\narXiv preprint\narXiv:2203.08913, 2022. 1\n[57] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5288\u20135296, 2016. 5\n[58] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and\nCordelia Schmid. Vidchapters-7m: Video chapters at scale.\narXiv preprint arXiv:2309.13952, 2023. 2\n[59] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. arXiv preprint\narXiv:2205.01917, 2022. 2, 3, 4\n[60] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Mer-\nlot: Multimodal neural script knowledge models. Advances\nin Neural Information Processing Systems, 34:23634\u201323651,\n2021. 2\n[61] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 1, 4\n[62] Luowei Zhou, Chenliang Xu, and Jason Corso.\nTowards\nautomatic learning of procedures from web instructional\nvideos. In Proceedings of the AAAI Conference on Artificial\nIntelligence, 2018. 5\n[63] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak\nGadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal\nc4: An open, billion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023. 2, 4, 5, 12\nAppendix\nA. Scaling up the Language Model\nA.1. Data and Comparison\nOur endeavor in this experimental phase aimed at a signif-\nicant upscaling of our language model, transitioning from\nthe previously employed 2B and 3.4B configurations to a\nmore expansive 8B setting.\nTo ensure a fair and consistent comparison across mod-\nels, we augmented the data sample size within the Data-\nComp subset from 130M to 180M, as outlined in the Data-\nComp [42] dataset. The associated data statistics, presented\ncomprehensively in Table 10, showcase a substantial aug-\nmentation, specifically involving a 60M increase in the sam-\nple count within the Datacomp [42] subset.\nOur efforts weren\u2019t solely confined to data augmenta-\ntion; we also incorporated a larger Mistral-7B language\nmodel [22] to achieve an overall parameter count of 8.1B.\nThe comparison between our method and Open-flamingo,\nutilizing equivalent-scale pre-training data, consistently\ndemonstrates the superior performance of our approach\nacross diverse tasks.\nB. Training Methodology Details\nB.1. Hyperparameters\nIn this section, we delineate the crucial training specifics\nnecessary for replication.\nThe experimentation encom-\npassed three variations in model size. Notably, larger mod-\nels necessitated smaller batch sizes owing to GPU memory\nconstraints. Our approach utilized deepspeed zero-stage 2\nwith fp16, while aligning gradient accumulation steps with\nthe data type count. Detailed results in Table 12.\nB.2. Handling Exceptions\nGiven the utilization of at least 8 nodes (64 GPUs) in our\ntraining setup, some challenges emerged during multi-node\ntraining. We encountered and addressed various issues:\nSkip Last Batch: Random sampling occasionally re-\nsulted in unstable batches, leading to markedly high con-\ntrastive loss and language model instability.\nLoss Upper Bound: To counter such occurrences, we\nimplemented a moment value mechanism to track con-\ntrastive loss and language modeling loss.\nIf the loss of\nbatch surpassed the moment value by a certain threshold,\nwe scaled down the loss, fostering more stable training.\nNAN Error Mitigation: Periodically, the model en-\ncountered NAN errors without the possibility of recovery.\nTo manage such instances, an automated job restart mecha-\nnism utilizing NCCL was employed, ensuring continuity in\ntraining despite potential failure points.\nData Type\nDataset\nSample\nImage-Text\nCC3M [44]\n0.75M\nSBU [36]\n0.3M\nLAION400M [43]\n30M\nDataComp1B [42]\n116M\nInterlevel Image-Text\nMMC4 [63]\n30M\nVideo-Text\nWebVid [4]\n2.5M\nInterlevel Video-Text\nHowto-Interlink7M\n0.9M\nTotal\n-\n180M\nTable 10.\nStatistics of the Pre-training Dataset for CosMo-\n8.1B: Subsets are from the complete sets using clustering and fil-\ntering.\nMethod\nTGIF-MC\nLSMDC-MC\nCosMo-2B\n47.2\n45.3\nCosMo-3.4B\n50.3\n46.6\nCosMo-8.1B\n53.4\n50.2\nTable 11. Results on Video Multiple-Choice task on TGIF [30]\naction split and LSMDC [41] val set with three frames.\nThese strategies were pivotal in mitigating challenges in-\nherent in multi-node training setups, ensuring a more stable\nand reliable training process despite the complexities intro-\nduced by distributed computing.\nC. Video-based Multiple-choice Tasks\nIn this experiment, we introduce an additional set of video\ntasks. Unlike open-ended video question answering, the\nmultiple-choice task involves selecting an answer from a\npool of candidates.\nPrecisely, our evaluation involves TGIF-MC [30] and\nLSMDC-MC [41] as benchmark datasets. To assess these\nmultiple-choice tasks without any fine-tuning, we propose a\nsimilarity-based matching approach. Initially, we generate\npotential answers and then measure the maximum similarity\nagainst all candidates. The most similar candidate, identi-\nfied using a Language Model from NLTK [5], is selected.\nIf the index matches the correct index, we mark it as a cor-\nrect answer. Our findings indicate that our model demon-\nstrates improved performance with larger model sizes in\nthese tasks.\nD. Extended Results: Multi-shot Details\nIn these experiments, we present CosMo\u2019s performance\nacross varying shot counts, ranging from 0 to 32 shots. For\nthese ablation studies, our training utilized 64 interleaved\ntokens within an 18M subset. For the Validation accuracy,\nwe report the top1 and top5 accuracy.\nCosMo-2B\nCosMo-3.4B\nCosMo-8.1B\nModel\nLanguage Model Backbone\nOPT-IML-1.8B [21]\nRedpajama-3B [47]\nMistral-7B [22]\nVision Model Backbone\nopenai/clip\n-vit-large\n-patch14\nopenai/clip\n-vit-large\n-patch14\nlaion/CLIP-ViT\n-H-14-laion2B\n-s32B-b79K\nCross-Layer Interval\n4\n4\n4\nTraining\nSequence Length\n128\n128\n128\nEffective Batch Size\n6144\n3072\n1536\nMax Training Steps\n200K\n200K\n500K\nWeight Decay\n0.1\n0.1\n0.1\nOptimizer\nadamw(0.9, 0.999)\nadamw(0.9, 0.999)\nadamw(0.9, 0.999)\nGradient Clipping\n1.0\n1.0\n1.0\nLearning Rate\nInitial Max\n5e-5\n5e-5\n3e-5\nDecay Schedule\nCosine\nCosine\nCosine\nLinear warmup Steps\n500\n500\n500\nTable 12. The hyperparameters used in pre-training for three distinct CosMovariations. The learning rate and batch size is smaller\nfor CosMo-8.1Bsine the GPU memory limitation is 32GB.\nMethod\nVal\nAcc1\nVal\nAcc5\nShots\nCaptioning (CIDER)\nVQA\nClassification\nRetrieval(t2v,v2t)\nAverage\nCOCO FLICKR\nok-\nvqa\ntextvqa vizwiz vqav2 hatefulmemes\nCOCO\nFLICKR\n(wo Retrieval)\n112,\n682.52M\n/2.32G,\n4d2h54m\n62.229 78.74\n0\n60.5\n41.1\n14.6\n17.9\n9.1\n22.3\n48.3\n22.8/21.2\n50.4/40.5\n29.3\n4\n67.3\n49.5\n21.6\n16.2\n20.5\n42.0\n48.1\n-\n-\n33.2\n8\n72.3\n55.3\n20.8\n16.0\n24.7\n42.6\n48.6\n-\n-\n37.6\n16\n67.8\n49.3\n15.5\n12.4\n28.7\n32.2\n52.5\n-\n-\n32.3\n32\n65.2\n38.5\n13.9\n12.6\n31.8\n28.0\n51.9\n-\n-\n31.6\n121,\n682.52M\n/2.32G,\n4d1h13m\n62.329 78.2\n0\n61.4\n42.2\n18.2\n19.2\n10.8\n28.7\n52.1\n22.0/19.1\n47.6/41.5\n30.5\n4\n68.5\n51.6\n20.8\n16.4\n23.5\n40.6\n50.6\n-\n-\n36.3\n8\n73.7\n54.3\n21.9\n16.3\n30.3\n39.9\n54.7\n-\n-\n38.8\n16\n69.0\n52.6\n15.9\n12.6\n37.4\n30.3\n53.0\n-\n-\n36.3\n32\n66.4\n47.3\n13.8\n13.2\n39.8\n26.3\n49.5\n-\n-\n34.2\n211,\n682.52M\n/2.32G,\n4d1h13m\n61.01\n77.3\n0\n66.4\n45.7\n17.9\n19.2\n9.5\n36.2\n49.0\n22.2/20.8\n48.8/43.8\n32.8\n4\n71.0\n52.4\n21.5\n16.3\n22.5\n42.0\n50.3\n-\n-\n34.1\n8\n76.5\n56.3\n19.6\n16.9\n29.4\n40.0\n48.9\n-\n-\n38.2\n16\n70.6\n51.3\n15.9\n13.0\n35.5\n33.0\n50.1\n-\n-\n33.7\n32\n65.7\n39.4\n12.9\n14.6\n40.0\n29.1\n50.7\n-\n-\n32.6\nTable 13. Ablation Study on Data Weights: Interleaved data emerges as a more pivotal contributor compared to other data types.\nD.1. Data Weight Ablation\nThis experiment delves into the data weight distribution\namong Image-text, Interleaved Image-text, and Video-Text.\nPrior to computing all losses with gradient accumulation,\nwe applied corresponding data type weights. Table 13 illus-\ntrates that interleaved image-text pairs notably influence\ndownstream accuracy. Driven by this observations, the data\nweight for interleaved data is 2 as default.\nD.2. PEFT method vs. Cross Attention\nParameter-efficient fine-tuning methods like LORA [19] are\nprevalent in LLM. The inquiry here is whether it is feasible\nto introduce LORA for model fine-tuning.\nTo this end, we introduce the LORA into text decoder.\nSpecifically, we keep the original cross attention but bring\nLORA additionaly. We show the result in Table 14. We find\nthe PEFT method bring negative gains for cross-attention\nlayers based method which already introduce a large num-\nber of trainable parameters.\nD.3. Learning Rate Ablation\nThis experiment involved varying the learning rate from 3e-\n4 to 3e-6, and the reported results in Table 15 reveal a cru-\ncial finding. Extremely small learning rates significantly\nimpair downstream task performance. For instance, a drop\nMethod\nVal\nAcc1\nVal\nAcc5\nShots\nCaptioning (CIDER)\nVQA\nClassification\nRetrieval(t2v,v2t)\nAverage\nCOCO FLICKR\nok-\nvqa\ntextvqa vizwiz vqav2 hatefulmemes\nCOCO\nFLICKR\n(wo Retrieval)\nCross-\nattention\nLayer\n61.402 77.342\n0\n63.6\n42.9\n10.9\n14.9\n6.5\n11.5\n50.3\n22.2/20.1\n49.8/40.1\n28.6\n4\n66.5\n48.3\n20.5\n17.1\n27.8\n41.3\n48.7\n-\n-\n38.6\n8\n71.3\n52.6\n20.9\n15.0\n33.6\n42.3\n49.5\n-\n-\n40.7\n16\n64.7\n48.2\n15.2\n12.5\n37.6\n32.6\n50.2\n-\n-\n37.3\n32\n60.5\n33.6\n13.8\n11.5\n40.3\n26.9\n49.7\n-\n-\n33.7\nLORA [19]61.33\n77.94\n0\n69.6\n49.0\n15.0\n17.9\n8.4\n30.1\n51.7\n22.3/19.8\n49.4/45.2\n31.5\n4\n68.7\n52.0\n18.2\n16.5\n19.3\n40.0\n51.4\n-\n-\n33.4\n8\n73.3\n54.7\n19.0\n15.9\n24.6\n39.9\n49.7\n-\n-\n36.8\n16\n71.9\n52.0\n16.7\n12.4\n31.8\n33.6\n47.3\n-\n-\n33.4\n32\n72.9\n43.5\n13.4\n12.5\n34.1\n31.2\n48.8\n-\n-\n33.3\nTable 14. LORA Ablation Study: The introduction of LORA does not significantly enhance the cross-attention within the interleaved\nframework.\nMethod\nVal\nAcc1\nVal\nAcc5\nShots\nCaptioning (CIDER)\nVQA\nClassification\nRetrieval(t2v,v2t)\nAverage\nCOCO FLICKR\nok-\nvqa\ntextvqa vizwiz vqav2 hatefulmemes\nCOCO\nFLICKR\n(wo Retrieval)\n3e-4\n682.52M\n/2.32G,\n3d7h12m\n61.402 77.342\n0\n63.6\n42.9\n10.9\n14.9\n6.5\n11.5\n50.3\n22.2/20.1\n49.8/40.1\n28.6\n4\n66.5\n48.3\n20.5\n17.1\n27.8\n41.3\n48.7\n-\n-\n38.6\n8\n71.3\n52.6\n20.9\n15.0\n33.6\n42.3\n49.5\n-\n-\n40.7\n16\n64.7\n48.2\n15.2\n12.5\n37.6\n32.6\n50.2\n-\n-\n37.3\n32\n60.5\n33.6\n13.8\n11.5\n40.3\n26.9\n49.7\n-\n-\n33.7\n3e-5,\n682.52M\n/2.32G,\n3d7h12m\n55.78\n72.96\n0\n37.8\n21.6\n21.8\n12.3\n8.4\n42.2\n48.0\n1.6/1.8\n2.7/4.1\n24.0\n4\n42.4\n22.0\n16.0\n7.1\n30.0\n37.8\n49.2\n-\n-\n25.9\n8\n25.0\n16.7\n14.7\n5.5\n35.8\n32.4\n49.1\n-\n-\n21.9\n16\n12.4\n9.9\n1.9\n3.0\n40.8\n24.8\n51.5\n-\n-\n15.5\n32\n10.7\n8.5\n6.9\n3.5\n42.4\n24.1\n52.2\n-\n-\n16.0\n3e-6,\n682.52M\n/2.32G,\n3d12h29m\n49.70\n69.70\n0\n3.6\n2.5\n20.6\n5.9\n6.7\n37.9\n48.7\n3.5/3.4\n6.7/8.3\n12.9\n4\n4.6\n3.5\n14.2\n4.4\n30.0\n34.7\n45.3\n-\n-\n15.2\n8\n6.7\n5.3\n14.3\n4.8\n35.7\n34.9\n47.5\n-\n-\n17.1\n16\n8.0\n6.4\n13.0\n4.7\n40.8\n32.7\n49.1\n-\n-\n17.8\n32\n8.0\n5.7\n5.3\n3.3\n42.4\n27.8\n50.0\n-\n-\n15.4\nTable 15. Learning rate ablation.\nfrom 40.7 to 21.9 accuracy on 8 shots was observed.\nAs default, we use 5e-4 for CosMo-2Band CosMo-3.4B.\nFor CosMo-8.1B, we use learning rate 3e-4.\nD.4. Learning Rate Schedule\nExperimenting with four distinct learning rate sched-\nules\u2014Cosine, Constant, Cosine-w-restart, and Inverse\nSqrt\u2014unveiled substantial variations in final results, as\ndemonstrated in Table 16. Generally, the Cosine schedule\nyields the best results across most cases, thus adopted as the\ndefault scheduler due to its consistent performance.\nE. Others\nE.1. Data Selection Strategy for Image-Text\nThe CC3M [44], SBU [36], LAION400M [43], and Data-\nComp1B [42] datasets are widely accessible and commonly\nused as pre-training data. We include these datasets to fa-\ncilitate easy replication. However, our observations suggest\nthat while these image-text datasets are valuable, their im-\npact on downstream performance is not as significant as that\nof interlevel image-text datasets.\nTo illustrate, utilizing the entire 130M data solely from\nDataComp [42] resulted in minimal changes in perfor-\nmance. Furthermore, unlike previous methods like GiT [51]\nand BLip2 [28], we opted to exclude COCO and Visual\nGenome datasets during pre-training due to potential over-\nlaps with downstream data. This strategic exclusion was\nimplemented to minimize potential redundancy and maxi-\nmize the distinctiveness of the learned representations.\nE.2. Exploring Longer Sequences with Memory\nBank\nManaging long text input data holds significant impor-\ntance in NLP. A recent approach, the Memorizing Trans-\nformer [55], introduces a memory bank to store segment to-\nkens from the entire document. This technique enables the\nmodel to handle lengthy texts, surpassing even 10K tokens.\nFollowing the principles of the Memorizing Trans-\nformer, our objective was to augment the interleaved data\nMethod\nVal\nAcc1\nVal\nAcc5\nShots\nCaptioning (CIDER)\nVQA\nClassification\nRetrieval(t2v,v2t)\nAverage\nCOCO FLICKR\nok-\nvqa\ntextvqa vizwiz vqav2 hatefulmemes\nCOCO\nFLICKR\n(wo Retrieval)\nCosine\n682.52M\n/2.32G,\n3d7h12m\n61.402 77.342\n0\n63.6\n42.9\n10.9\n14.9\n6.5\n11.5\n50.3\n22.2/20.1\n49.8/40.1\n28.6\n4\n66.5\n48.3\n20.5\n17.1\n27.8\n41.3\n48.7\n-\n-\n38.6\n8\n71.3\n52.6\n20.9\n15.0\n33.6\n42.3\n49.5\n-\n-\n40.7\n16\n64.7\n48.2\n15.2\n12.5\n37.6\n32.6\n50.2\n-\n-\n37.3\n32\n60.5\n33.6\n13.8\n11.5\n40.3\n26.9\n49.7\n-\n-\n33.7\nConstant(1e-4),\n682.52M\n/2.32G,\n2d21h40m\n57.00\n73.95\n0\n35.5\n30.3\n22.2\n14.6\n9.3\n42.9\n46.4\n3.4/3.8\n8.2/9.0\n26.0\n4\n52.3\n52.3\n19.9\n10.0\n23.8\n41.2\n50.2\n-\n-\n33.3\n8\n46.4\n46.4\n17.6\n7.8\n30.2\n38.7\n47.8\n-\n-\n31.1\n16\n38.6\n38.6\n9.1\n5.7\n32.6\n35.2\n52.9\n-\n-\n26.6\n32\n24.2\n24.2\n6.3\n4.2\n35.3\n24.8\n48.6\n-\n-\n19.8\nCosine-w-\nrestart,\n682.52M\n/2.32G,\n3d12h29m\n57.31\n74.577\n0\n36.1\n34.3\n19.0\n15.2\n6.9\n30.3\n52.0\n12.9/11.8\n34.0/26.5\n23.6\n4\n58.1\n42.1\n19.0\n14.7\n20.2\n42.7\n51.0\n-\n-\n32.8\n8\n51.7\n38.9\n11.8\n10.7\n26.8\n34.2\n52.8\n-\n-\n29.0\n16\n50.1\n28.1\n5.1\n7.2\n30.4\n34.1\n49.9\n-\n-\n25.8\n32\n21.5\n4.5\n2.7\n3.7\n36.2\n25.3\n50.5\n-\n-\n15.7\nINVERSE\nSQRT,\n682.52M\n/2.32G,\n3d12h29m\n54.26\n73.17\n0\n28.3\n15.7\n14.6\n6.6\n4.8\n25.5\n50.9\n1.8/2.2\n4.4/4.7\n15.9\n4\n24.8\n11.0\n8.3\n3.7\n25.5\n30.8\n54.5\n-\n-\n17.3\n8\n15.7\n11.0\n5.1\n2.7\n28.4\n26.0\n48.5\n-\n-\n14.8\n16\n12.0\n10.6\n1.7\n1.6\n29.1\n24.8\n50.8\n-\n-\n13.5\n32\n12.6\n8.6\n1.0\n1.7\n23.9\n23.3\n46.4\n-\n-\n11.9\nTable 16. Learning rate schedule ablation.\nMethod\nVal\nAcc1\nVal\nAcc5\nShots\nCaptioning (CIDER)\nVQA\nClassification\nRetrieval(t2v,v2t)\nAverage\nCOCO FLICKR\nok-\nvqa\ntextvqa vizwiz vqav2 hatefulmemes\nCOCO\nFLICKR\n(wo Retrieval)\nNo Memory,\n682.52M\n/2.32G,\n2d13h16m\n60.85\n78.04\n0\n65.6\n46.2\n15.8\n18.3\n7.9\n34.0\n47.7\n23.0/20.0\n51.5/43.3\n33.5\n4\n69.9\n53.7\n16.0\n14.1\n11.6\n39.0\n48.9\n-\n-\n36.3\n8\n74.4\n56.2\n18.5\n15.3\n16.4\n42.0\n44.6\n-\n-\n38.4\n16\n70.9\n53.6\n14.4\n10.3\n20.6\n31.6\n48.6\n-\n-\n35.8\n32\n70.0\n45.5\n11.9\n12.1\n27.6\n29.2\n47.9\n-\n-\n34.9\nRetrieval Mem-\nory,\n682.52M\n/2.32G,\n3d7h12m\n63.012 79.319\n0\n66.3\n45.8\n17.0\n16.3\n12.2\n25.3\n49.3\n26.4/24.3\n55.5/46.4\n33.2\n4\n68.3\n54.2\n15.3\n13.2\n14.4\n35.0\n40.8\n-\n-\n35.8\n8\n72.3\n55.6\n19.3\n13.3\n17.8\n40.3\n49.3\n-\n-\n36.5\n16\n73.4\n52.4\n10.3\n13.4\n18.2\n35.3\n50.3\n-\n-\n36.2\n32\n70.7\n47.3\n9.9\n14.3\n30.6\n31.2\n49.8\n-\n-\n36.2\nTable 17. Memory bank ablation.\nlength. The results, detailed in Table 17, yielded intriguing\nobservations:\ni. Improved Validation Results: Notably, the valida-\ntion accuracy exhibited significant enhancement, e.g., from\n63.012 to 60.85. ii. Increased Computational Cost: Intro-\nducing longer sequences incurred substantially higher com-\nputational demands.\niii.\nLimited Downstream Perfor-\nmance Gain: Surprisingly, the performance gains on down-\nstream tasks were relatively restricted and occasionally even\nnegative.\nConsequently, based on these findings, the Memorizing\nTransformer approach was not included in our final model\nconfiguration. However, this doesn\u2019t preclude the consid-\neration of alternative methods to bolster the model\u2019s ability\nto process longer texts. There\u2019s potential to explore differ-\nent approaches aimed at enhancing the model\u2019s handling of\nextended sequences.\nF. Training Progression\nIn this section, we explore the training curves for three\nscales of CosMo. Our findings reveal that larger Language\nLarge Models (LLMs) show a reduced Language Model\n(LM) loss, particularly in processing interlevel image-text\ndata, which underscores their proficiency in word predic-\ntion tasks.\nHowever, a notable aspect was the slower convergence\nin contrastive loss for CosMocompared to larger models\nlike CosMo-2Band CosMo-3.4B, attributed to its smaller\nbatch size. Additionally, hardware constraints were evident\nduring training CosMo-8.1Bon 32GB Tesla V100 GPUs,\nwhere the maximum batch size of 1536 impacted the con-\nvergence rate of contrastive loss.\nThese observations highlight the intricate balance be-\ntween model size, batch size, and hardware limitations in\nthe efficient training of LLMs.\nFigure 7. Generally, Larger Language Models exhibit lower Language Modeling (LM) loss. However, the convergence of contrastive\nloss tends to be slower owing to the smaller batch size.\n"
  },
  {
    "title": "GeoGalactica: A Scientific Large Language Model in Geoscience",
    "link": "https://arxiv.org/pdf/2401.00434.pdf",
    "upvote": "8",
    "text": "GE\nGALACTICA: A SCIENTIFIC LARGE LANGUAGE MODEL\nIN GEOSCIENCE\nZhouhan Lin1,\u2217, Cheng Deng1, Le Zhou1, Tianhang Zhang1, Yi Xu1, Yutong Xu1,\nZhongmou He1,2, Yuanyuan Shi1, Beiya Dai1, Yunchong Song1, Boyi Zeng1, Qiyuan Chen1,\nTao Shi1, Tianyu Huang3, Yiwei Xu4, Shu Wang5, Luoyi Fu1, Weinan Zhang1,\nJunxian He6, Chao Ma3, Yunqiang Zhu5, Xinbing Wang1, Chenghu Zhou1,5\n1Shanghai Jiao Tong University\n2University of Michigan\n3Chengdu University of Technology\n4 Nanjing Institute of Geology and Palaeontology, CAS\n5Institute of Geographical Science and Natural Resources Research, CAS\n6The Hong Kong University of Science and Technology\nlin.zhouhan@gmail.com, davendw@sjtu.edu.cn, junxianh@cse.ust.hk, zhouch@lreis.ac.cn\nABSTRACT\nLarge language models (LLMs) have achieved huge success for their general knowledge and ability to\nsolve a wide spectrum of tasks in natural language processing (NLP). Due to their impressive abilities,\nLLMs have shed light on potential inter-discipline applications to foster scientific discoveries of a\nspecific domain by using artificial intelligence (AI for science, AI4S). In the meantime, utilizing NLP\ntechniques in geoscience research and practice is wide and convoluted, contributing from knowledge\nextraction and document classification to question answering and knowledge discovery. In this work,\nwe take the initial step to leverage LLM for science, through a rather straightforward approach. We\ntry to specialize an open-sourced LLM into geoscience, by further pre-training the model with a\nvast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with\nour custom collected instruction tuning dataset. These efforts result in a model GEOGALACTICA\nconsisting of 30 billion parameters. To our best knowledge, it is the largest language model for the\ngeoscience domain. More specifically, GEOGALACTICA is from further pre-training of Galactica \u2013 a\ntop-performing LLM trained with a large number of scientific documents. We train GEOGALACTICA\nover a geoscience-related text corpus containing 65 billion tokens curated from extensive data sources\nin the big science project Deep-time Digital Earth (DDE), preserving as the largest geoscience-specific\ntext corpus. Then we fine-tune the model with 1 million pairs of instruction-tuning data consisting of\nquestions that demand professional geoscience knowledge to answer. We validate GEOGALACTICA\non various geoscience examinations and geoscience-related open-domain questions evaluated by a\ngroup of senior geoscientists. GEOGALACTICA demonstrates the state-of-the-art performance in\na diverse range of NLP tasks in geoscience, as well as revealing the potential of using geoscience-\nrelated tools. In this technical report, we will illustrate in detail all aspects of GEOGALACTICA,\nincluding data collection, data cleaning, base model selection, pre-training, SFT, and evaluation. We\nopen-source our data curation tools and the checkpoints of GEOGALACTICA during the first 3/4 of\npre-training in https://github.com/geobrain-ai/geogalactica\u00a7.\nKeywords Geoscience Language Model \u00b7 Generative AI \u00b7 Academic Language Model\n*Zhouhan Lin is the corresponding author (lin.zhouhan@gmail.com).\n\u2020Version: v1 (major update on December 30, 2023).\n\u2021For detailed author contributions, please refer to Appendix L.\n\u00a7For all the checkpoints during the 3/4 pre-training can be accessed on geobrain-ai/geogalactica-ckpt. One can apply for the\ndownload links for further research and investigation.\narXiv:2401.00434v1  [cs.CL]  31 Dec 2023\nGeoGalactica\nA PREPRINT\nContents\n1\nIntroduction\n5\n2\nRelated Work\n6\n2.1\nMachine Learning in Geoscience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nNatural Language Processing in Geoscience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nDomain-specific Large Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3\nPreliminary and Vocabulary\n8\n4\nData Collection and Cleaning\n9\n4.1\nThe Customized Pre-training dataset: GeoCoprus . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.2\nThe Customized SFT dataset: GeoSignal Version 2 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n4.2.1\nDomain General Natural Language Instruction\n. . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.2.2\nRestructured Knowledge-intensive Instruction . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.2.3\nSelf-Instruct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n5\nTraining\n14\n5.1\nFurther Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n5.2\nSupervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n5.3\nTool Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n6\nEvaluation\n19\n6.1\nAutomatic Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6.1.1\nGeoBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6.1.2\nMMLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6.2\nHuman Evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n6.2.1\nNoun Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n6.2.2\nBeginner Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n6.2.3\nIntermediate Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n6.2.4\nAdvanced Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n6.2.5\nKnowledge-based associative judgment question . . . . . . . . . . . . . . . . . . . . . . . .\n24\n6.2.6\nResearch Paper Titling Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n6.2.7\nGeoscience Research Functionality\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n7\ndiscussion\n26\n7.1\nThe Necessity of Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n7.2\nThe Necessity of Further Pre-training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n7.3\nCarbon Emissions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n7.4\nTowards Unified Foundation Model in Geoscience . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n8\nConclusion\n27\n2\nGeoGalactica\nA PREPRINT\nA Appendix: Progression of geoscience with AI\n33\nB Appendix: GeoCorpus\n33\nC Appendix: GeoSignal V2 Curation\n34\nC.1\nMinDat\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nC.2\nUSGS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nC.3\nNGDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nC.4\nFossil Ontology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nC.5\nFossil calibrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nD Appendix: Prompts\n41\nE Appendix: Training setup\n46\nF\nAppendix: Model Card\n47\nG Appendix: Evaluation\n48\nG.1\nOpen-ended Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nG.1.1\nNoun Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nG.1.2\nBeginner Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nG.1.3\nIntermediate Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nG.1.4\nAdvanced Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nG.2\nFunctional Tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nG.2.1\nKnowledge-based associative judgment question. . . . . . . . . . . . . . . . . . . . . . . . .\n49\nG.2.2\nResearch Paper Proposition Task.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\nG.2.3\nGeoscience Research Functionality\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\nH Generation Examples\n51\nH.1\nNoun Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\nH.2\nBeginner Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\nH.3\nIntermediate Level Q&A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\nH.4 Advanced Level Q&A\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\nH.5\nKnowledge-based associative judgment question\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\nH.6\nResearch Paper Titling Task\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nH.7\nGeoscience Research Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nI\nAppendix: Tool Learning Use cases\n56\nJ\nAppendix: GEOGALACTICA Generation\n58\nJ.1\nExample Research Papers Written by GEOGALACTICA . . . . . . . . . . . . . . . . . . . . . . . . .\n58\nJ.2\nExample Opinions Written by GEOGALACTICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\nJ.3\nExample Summary of Scientific Articles Written by GEOGALACTICA . . . . . . . . . . . . . . . . .\n60\n3\nGeoGalactica\nA PREPRINT\nK Appendix: Lessons and Progresses\n61\nK.1\nPhase 1: Prepare for Training on HPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\nK.2\nPhase 2: Training on HPC\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\nK.3\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nL\nMembership and Contributions\n67\nL.1\nData preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nL.2\nModel Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nL.3\nModel Evaluation and Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nL.4\nManuscript Writing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nL.5\nProject Management\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nL.6\nEvaluation Team\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nL.7\nIllustration in Arts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nL.8\nHPC Sponsor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nFigure 1: The overview of the processing, construction, components, and applications of GEOGALACTICA.\n4\nGeoGalactica\nA PREPRINT\n1\nIntroduction\nThe rapid advancement of Large Language Models (LLMs) has ushered in a transformative era in natural language\nprocessing (NLP), where these models have exhibited remarkable capabilities across a wide spectrum of tasks and\ndomains. These advanced AI models have demonstrated their prowess in handling diverse natural language tasks,\nincluding reading comprehension, open-ended question answering, code generation, etc. Their ability to harness vast\namounts of general knowledge and apply it to solve specific challenges has sparked interest in exploring their potential\napplications in various scientific disciplines. In this context, the intersection of artificial intelligence (AI) and science,\noften referred to as AI for Science (AI4S), has emerged as a promising frontier for driving scientific discoveries and\ninnovations.\nWithin the realm of AI4S, one particularly intriguing avenue is the integration of NLP techniques into geoscience\nresearch and practice. Geoscience is a comprehensive discipline encompassing fields such as geophysics, geology,\nmeteorology, environmental science, etc., with a primary focus on unraveling the complexities of natural processes and\nphenomena on Earth. Traditionally, geoscientists have relied on theoretical and empirical approaches to advance their\nunderstanding of the Earth\u2019s systems. However, the sheer volume of data generated in contemporary geoscience research\nnecessitates new strategies and tools for knowledge discovery. The integration of computer science methodologies and\nAI technologies into geoscience has thus emerged as a transformative paradigm, offering the potential to accelerate\nscientific progress and address pressing global challenges effectively. In an era characterized by global challenges\nsuch as climate change and natural disaster mitigation, the need for efficient data acquisition, information sharing, and\nknowledge dissemination in geoscience has never been more critical.\nIn the field of geoscience, domain-specific geoscientific knowledge is usually presented in various forms of text data,\nsuch as scientific literature, textbooks, patents, industry standards, etc., which traditionally require the utilization of\nknowledge systems [1], knowledge graphs[2], or semantic models [3] to extract a structured form of these knowledge.\nMore broadly, applying NLP techniques for geoscience use cases has been widely accepted [4], ranging from less\ncomplex tasks such as document classification [5], topic modeling [6], and entity recognition[7, 8], to more complex\ntasks such as knowledge graph construction [9], question answering [10] and summarization [11].\nWhile general domain LLMs like Galactica [12], LLaMA [13], and GLM [14] have achieved impressive performance\nacross various NLP tasks, they lack the domain-specific knowledge required for geoscience applications. These\nmodels have been trained on general datasets that lack authoritative geoscience-related data, limiting their adequacy in\naddressing the unique challenges posed by the geoscience domain. Although our recent attempt to adapt the LLaMA-7B\nmodel for geoscience using geoscience-specific data, i.e. the K2[15] model, has shown promising results, this primitive\nattempt is constrained by its model size and data scale, which consequently may not fully capture the complexity of\ngeoscientific terminology and concepts. However, training a larger LLM comes with new technical challenges, since\nmany aspects of the process become fundamentally different as the model scales up. For example, the stability of\ntraining will become more vulnerable, and the training data needs to be scaled up accordingly, resulting in a more\nsystematic way of managing different data sources, etc.\nTherefore, tailoring a general, larger LLM for the scientific domain of geoscience with a more systematically designed\ndataset and training pipeline is imperative in this era of LLMs. In response to these necessities, this work presents a\nsignificant step forward in the development of the model as well as the set of toolchains around it.\nLeveraging the vast amount of resources of scientific literature, particularly the data resources collected for the Deep-\ntime Digital Earth (DDE) project, we can create, organize, and manage a large and comprehensive geoscience dataset\ntargeted for all stages in large language model training. In particular, we have introduced GAKG [2], Deep Literature1,\nGSO2, and other platforms as carriers and repositories of geoscience text knowledge. These concerted efforts have\nnot only allowed us to accumulate a comprehensive geoscience data archive but also have served as foundations for\nconstructing an extensive instruction-tuning dataset for geoscience-related questions, GeoSignal-v2, which has been\nemployed in supervised fine-tuning (SFT). In addition, we have developed and customized a series of data-cleaning\ntools that allow us to automatically convert various forms of raw data, such as PDF files, forms, equations, knowledge\ngraphs, etc., into clean texts suited as training corpus for large language models. To our best knowledge, our collected\ncorpus has become the largest geoscience dataset.\nwe have then successfully further pre-trained a language model with 30B parameters, with Galactica-30B [12] as its base\nmodel. The resulting model is thus named as GEOGALACTICA, empowering various academic tasks in the geoscience\nfield. With its 30 billion parameters, this model represents the culmination of further pre-training and supervised\nfine-tuning, making it the largest language model dedicated to the geoscience domain. Our experimental findings\n1https://ddescholar.acemap.info/\n2https://gso.acemap.info/\n5\nGeoGalactica\nA PREPRINT\ndemonstrate that, compared to models of equivalent scale, GEOGALACTICA exhibits exceptional performance on\nGeoBenchmark [15]. Regarding human evaluation, our model showcases impressive competence in geoscience-related\ntasks when compared with 5 general language models (ChatGPT3, Yiyan4, Qianwen5, MOSS6, ChatGLM7).\nMoreover, since our GEOGALACTICA model provides a unified representation space and computational approach for\ndiverse geological data described in various textual formats, it holds tremendous potential in narrowing the technological\ngap between different earth science tasks.\nIn the subsequent sections of this technical report, we will provide a detailed description of the data collection and\ncleaning processes, base model selection, pre-training, supervised fine-tuning, and extensive evaluations in the creation\nof GEOGALACTICA. Additionally, we are committed to promoting open science by making our data curation tools\nand pre-training checkpoints available to the research community through our GitHub repositories8.\nBroad Contribution\nIn addition to establishing the academic mega-model in geoscience, our goal is to contribute to a broader research com-\nmunity. Specifically, the experiences documented in this paper provide evidence for further community understanding\nof several open questions in the literature.\n1. A Domain-specific LLM: Our construction of GEOGALACTICA, following in the footsteps of our previous work\nK2 [15], represents a geoscience LLM that focuses on interacting with humans and generating contents on highly\nprofessional academic topics.\n2. A Toolchain for Data Cleaning: A high-quality training dataset is crucial for successfully training large language\nmodels. Therefore, our contribution to the community includes developing an efficient academic data preprocessing\ntoolchain to construct a clean training corpus from PDF documents 9\n3. Primitive Explorations to Use Tools: As for training GEOGALACTICA to use tools, we also construct a set of\nsupervised data Geotools for training GEOGALACTICA to use tools. We also open-source the codes and data on\nGithub.10\n4. Training Details and pre-training Checkpoints: We conducted model training on the accelerator hardware\nprovided by the Advanced Computing East China Sub-center. We will describe in detail the pre-training and SFT\nprocesses in the remainder of this paper. In addition, we are releasing the training checkpoints during the first 3/4 of\nthe pre-training process on Hugging Face.11\n5. Model and data analysis process: In building a domain-specific LLM, the model and the data should be effectively\nevaluated and analyzed. We provide a set of analysis and visualization methods for the SFT data and the weights of\nthe GEOGALACTICA, open-sourced on Github.12\nIn summary, we aim to contribute to the research community by developing the GEOGALACTICA model and providing\ninsights and tools related to data construction, training processes, and evaluation strategies. The organization of the\npaper can be seen in the contents section listed above.\n2\nRelated Work\n2.1\nMachine Learning in Geoscience\nWith the advancement of artificial intelligence, utilizing machine learning, natural language processing, and recent\nlarge-scale model techniques to tackle critical problems in geoscience has become a crucial direction. Various subtasks\nin geoscience involve significant data collected from sensors, making them suitable for end-to-end learning using\nmachine learning approaches. Some studies model multiple aspects of seismic signals using deep learning models to\n3https://chat.openai.com/\n4https://yiyan.baidu.com/\n5https://qianwen.aliyun.com/\n6https://moss.fastnlp.top/\n7https://chatglm.cn/\n8The list of related tools, data, and codes can be found in https://github.com/geobrain-ai/geogalactica\n9The toolchain is open-sourced on Github repos: https://github.com/Acemap/pdf_parser and https://github.com/\ndavendw49/sciparser. In addition, an online demo of this toolchain can be found at https://sciparser.acemap.info/.\n10https://github.com/zthang/geotools\n11https://huggingface.co/geobrain-ai/geogalactica\n12https://github.com/dbylynn/GeoGalactica_Analysis\n6\nGeoGalactica\nA PREPRINT\nextract information relevant to earthquake prediction. Among them, [16] uses supervised learning with end-to-end\ntraining, while [17, 18] employs self-supervised learning to obtain models applied to downstream tasks. [19, 20] utilize\nmachine learning to explore the latent correlations among different rock properties for rock type prediction. Beyond\nrelatively straightforward classification tasks, there are numerous works applying machine learning to address more\ncomplex scenarios in geoscience, such as calculating wellhead flow rate [21], capturing and storing carbon [22], and\npredicting the condition of SPBM tunnels [23]. Additionally, machine learning is introduced to evaluate the real-world\nenvironment: [24] explores the use of Few-Shot Learning (FSL) methods to enhance the accuracy of high-resolution\npre-stack seismic inversion, and [25] employs various machine learning techniques and ensemble methods to improve\nlandslide hazard prediction, demonstrating their high practical value. Machine learning is also being used to aid\ngeoscience exploration, [26] attempts to use machine learning to do data-driven modeling of solid earth science, [27]\nattempts to use machine learning to reveal the link between fast and slow earthquakes, [28] uses machine learning to\nreveal the impact of aerosols on climate impact.\n2.2\nNatural Language Processing in Geoscience\nIn addition to the diverse and heterogeneous data collected from various sensors, the field of geoscience also encompasses\na significant amount of text data with standardized formats. The application of natural language processing (NLP) in\nearth science has witnessed remarkable progress. [29, 6] embed different sources of textual information into a unified\nspace, [29] employs joint training of language models with text and points of interest (POI) for POI retrieval, while [6]\nintegrates geological attribute information into the textual representation space to enable better knowledge extraction.\n[30, 31] enhance language models with knowledge graph techniques, where [30] constructs a knowledge graph on\ngeological text to discover ore-forming environments, and [31] proposes an automatic entity and relation extraction\napproach via three-level extraction to build a geological knowledge graph from extracted information in geological\nreports. [32] combines retrieval techniques with language models creates an integrated solution incorporating contextual\nretrieval and the GeoBERT model. [33] focuses on various language irregularities encountered in natural language\ntexts, introducing the NeuroSPE model for spatial extraction using neural networks. NLP techniques provide a unified\nrepresentation space and computational approach for diverse geological data described in various textual formats,\nnarrowing the technological gap between different earth science tasks.\n2.3\nDomain-specific Large Language Model\nThe recent emergence of large-scale language models marks a significant step towards unified information processing\nin geoscience. These models are pre-trained on vast amounts of text data and efficiently compress all input data.\nCurrently, in addition to earth science, various domains have seen the development of domain-specific pre-trained\nmodels trained on domain-specific corpora. [34, 35, 36, 12, 37, 38] performs large-scale pre-training on domain-specific\ntexts and has resulted in foundational models equipped with domain knowledge, while [39, 40, 41] fine-tuning these\nbase models using domain-specific data, achieving models tailored to specific downstream tasks at a lower cost. These\nworks have made significant strides in developing domain-specific LLMs through dedicated data integration and model\ntraining efforts. Recently, [42, 43, 44] explored the use of prompt engineering to unlock the potential of models\nwithout additional training, offering the possibility of unifying various geoscience tasks and further reducing the cost of\nemploying large models in domain applications. In the field of geoscience, the exploration of large models is still in its\nearly stages. [15] collected a substantial amount of high-quality data from earth science Wikipedia and research papers,\nand further fine-tuned the base model, leading to impressive scientific competence and knowledge in earth science.\nFor the first time, our work utilizes a large corpus of earth science papers and textbooks, which were cleaned using a\ndedicated toolchain for constructing large-scale earth science models, ensuring data quality. Furthermore, our work\ncompletes the entire process of \u201cfurther pre-training, supervised fine-tuning, augmented learning\u201d for large foundation\nmodels for geoscience, bringing the largest scale and highest quality proprietary language models to the geoscience\nfield. This will unlock tremendous possibilities for future research conducted by earth science researchers.\nWe have outlined the progression of geoscience research with the use of cutting-edge AI techniques, including neural\nnetwork (NN), K-nearest neighbor (KNN), recurrent neural network (RNN), convolutional neural network (CNN),\nbackpropagation (BP), reinforcement learning (RL), support vector machine (SVM), long-short term memory (LSTM),\ngraph convolutional neural network (GCN), Transformers, BERT, ChatGPT, and large language model (LLM). [45] The\ninvestigation reveals that the time intervals between AI technology advancements and their application in geoscience\nhave significantly shortened, indicating an increasing reliance on advanced AI technology in the field of geoscience.\nThe illustration is presented in Figure 2, and detailed information about the progression is shown in Appendix A.\n7\nGeoGalactica\nA PREPRINT\nFigure 2: The progression illustration of geoscience research with the use of cutting-edge AI techniques. The textboxes\nin PaleTurquoise show the techniques from computer science, The textboxes, in Bisque show the research that probably\nthe first time geoscientists used the techniques.\n3\nPreliminary and Vocabulary\nTo facilitate understanding of our work and the overview of our model, here are some key terms and their corresponding\nexplanations that will be widely used in the narrative of this article.\nVocab\nStage and illustration\nGalactica-30B\nThe vanilla Galactica model\nGeoGalactica-FP\nCheckpoint after pre-training over geoscience data (Further Pre-train)\nGeoGalactica-Alpaca\nApplying supervised fine-tuning with Alpaca data on top of GeoGalactica-FP\nGeoGalactica-GeoSignal\nApplying supervised fine-tuning with GeoSignal data on top of the first checkpoint (GeoGalactica-FP)\nGeoGalactica\nApplying supervised fine-tuning following training recipe of K2 on top of the first checkpoint (GeoGalactica-FT)\nGeoCorpus\nGeoscience text corpus for pre-training\nGeoSignal\nSupervised fine-tuning data for geoscience\nGeoBench\nBenchmarks for evaluating the performance of the geoscience LLM\nTable 1: Vocabulary for this technical report.\nHere we list the terms widely used in this report:\n\u2022 DDE. An International Union of Geological Sciences (IUGS) \u201cBig Science\u201d program that seeks to harmonize global\ndeep-time Earth data with new protocols, platforms, and programs.\n\u2022 Sciparser. A PDF parsing toolkit for preparing text corpus to transfer PDF to Markdown.\n\u2022 GeoTools. A set of supervised instruction data for training GEOGALACTICA to use tools.\n\u2022 K2. The first-ever geoscience large language model trained by firstly further pre-training LLaMA on collected\nand cleaned geoscience literature, including geoscience open-access papers and Wikipedia pages, and secondly\nfine-tuning with knowledge-intensive instruction tuning data (GeoSignal).\n\u2022 DDE Scholar. DDE Scholar is a literature platform, aiming to construct a knowledge information system for\ngeoscience scholars, which, step-by-step goes through knowledge arrangement, knowledge mining and knowledge\ndiscovery.\n\u2022 GAKG. GAKG [2] is a multimodal Geoscience Academic Knowledge Graph (GAKG) framework by fusing papers\u2019\nillustrations, text, and bibliometric data.\n\u2022 DeepShovel. DeepShovel [46] is an Online Collaborative Platform for Data Extraction in Geoscience Literature\nwith AI Assistance.\n8\nGeoGalactica\nA PREPRINT\n\u2022 GSO. Similar to WordNet, GeoScience Ontology, (GSO) is a hierarchical tree of geological terms contains a vast\namount of synonyms and word explanations, providing valuable geoscience connections between terms.\n\u2022 Acemap. AceMap is a platform displaying relationships among academic publications and a search engine for\nacademic literature reviews.\n\u2022 DataExpo. A one-stop dataset service for geoscience research.\nFinally, we share the model card in Appendix F.\n4\nData Collection and Cleaning\nThe training corpus of Galactica primarily consists of literature related to computer science and biochemistry rather\nthan earth science. This suggests that Galactica may lack sufficient knowledge in the field of geoscience. To address\nthis, we have collected approximately six million research papers specifically focused on earth science. These papers\nwere carefully selected by professional experts in the field. Furthermore, we have expanded the GeoSignal dataset based\non K2 to better support natural language processing tasks in earth science research. This expanded dataset was used for\nfine-tuning the model after further pre-training. In the following sections, we will provide a detailed explanation of how\nour dataset was constructed.\n4.1\nThe Customized Pre-training dataset: GeoCoprus\nAccording to our long-term collection efforts on geoscience papers, with the research fields subfields of geology and\ngeography, through Acemap, we have accumulated a total of 5,980,293 papers.\nDuring this process, we commenced our data collection in early 2020 by gathering a list of journals in geoscience\nfrom LetPub 13. We identified the corresponding publishers\u2019 websites using the journal names and ISSN to collect\nopen-access data through web page parsing. This process continued towards 2023 when we collaborated with\nexperts in various sub-disciplines of geoscience, we collect paper from high-quality SCI journals in the mathematical\ngeosciences, metamorphic petrology, geochronology,geomagnetism and paleomagnetism, geomorphology, tectonics,\nstratigraphy, hydrogeology, geophysical, geothermics, igneous and geochemistry, surficial geochemistry, geological\nmapping, sedimentological, petroleum geology, paleontology, paleogeography, and mineralogy. In total, we integrated a\nrecommended list of 849 DDE-recommended journals(Appendix B shows the distribution of the collected papers in\ngeoscience).\nWe employed the journal name list to search for journal information and their publishers\u2019 websites. Through web page\nscraping, we collected HTML pages and subsequently conducted data parsing to extract metadata from papers. For\nopen-access articles, we matched them with the parsed DOI and the corresponding PDF from Sci-Hub14. If no PDF\nwas available, we downloaded it based on the URL.\nThroughout this process, we adhered to the network conventions of information acquisition. When faced with obstacles\nsuch as anti-scraping measures like 5-second shields, JavaScript encryption, IP proxy bans, and account logins, we\nconstrained our actions to ensure the compliance of our data. Moreover, data security remained our utmost priority\nduring this process; thus, we refrained from publicly disclosing the data obtained during this stage.\nIn conclusion, we obtained a total of 6,611,063 papers. Our data collection system operated through a distributed\ninformation fusion mechanism, utilizing an 8-workstation k8s cluster. Data collection was conducted using Scrapy-\nRedis 15 framework. Additionally, we implemented compression techniques for HTML data to address challenges\nrelated to large-scale data storage.\nFurthermore, we have leveraged the copyrights obtained from the publishers we have been collaborating with over\nthe years to parse and anonymize the PDFs of these articles, creating a dataset of textual data. Additionally, referring\nto [47, 48], we have reason to believe that the inclusion of program code in the model\u2019s pre-training, alongside the text,\ncan significantly enhance the reasoning capabilities of the LLM model.\nTherefore, after collecting datasets from Acemap and ArXiv, we incorporated the training dataset from Codedata.\nFinally, our overall training corpus is detailed in Table 2, totaling 78B. The data from a specific source is concatenated\ninto a single record. After tokenization, we then split it according to a block size of 2048, with each instance ending\nwith the tokenizer.eos token. For each training batch, the proportion of geoscience papers to the other two datasets is\n8 : 1 : 1.\n13https://letpub.com.cn/\n14https://sci-hub.se/\n15https://github.com/rmax/scrapy-redis\n9\nGeoGalactica\nA PREPRINT\nDataset\n#blockNum\n#tokenNum\n#itemNum\n#tokenSize\n#batchRatio\nGeoCorpus\n25,743,070\n52,721,798,004\n5,548,479\n98.21G\n80%\nArXiv\n6,691,886\n13,704,981,558\n742,835\n25.53G\n10%\nCodedata\n6,066,725\n12,424,652,670\n3,456,887\n23.14G\n10%\nTotal\n38,501,681\n78,851,432,232\n9,748,201\n146.88G\n-\nTable 2: Data distribution of the corpus used for training GEOGALACTICA\nWe utilized data processing and enhancement tools based on DeepShovel and K2 during this process. With the help\nof Grobid [49] and pdffigure2 [50], we provided a comprehensive parsing solution for extracting text, images, tables,\nformulas, and other data types from research paper PDFs. This was further enhanced by DeepShovel for parsing tables\nand formulas, resulting in the development of the SciParser tool. We plan to open-source this tool and share it on\nGitHub.\nWithin PDF documents, there are various types of data, including text, images, tables, and formulas, all organized\nwithin the articles\u2019 hierarchical structure and page layout. Data preprocessing is necessary to extract and ensure the\nreadability of such content. It entails utilizing a PDF parsing tool to perform an initial parsing of the PDF document,\nresulting in a parsing file that contains various information from the document. However, the readability of this file\nis often poor, and it may have a significant amount of redundant information. Subsequently, the parsing file needs to\nundergo data cleansing, extracting the desired text, images, tables, formulas, and other data, and converting it into\nMarkdown format for further processing, analysis, or display purposes.\nCurrently, we are utilizing Grobid16 as our PDF parsing tool. Grobid can accurately extract text from PDF documents\nand perform structured extraction of articles. It provides an outline of the text, forming an XML structure that allows\nfor restoring the original PDF layout. Additionally, Grobid enables precise localization of images, tables, formulas, and\nother data types. With the provided bounding boxes, we can obtain the corresponding images using the PyMuPDF\ntool 17. Further leveraging the OCR recognition integrated into DeepShovel [46], we can convert tables, formulas,\nand other elements into Markdown format. The parsing process is completed by writing all the parsed content into a\nmarkdown file for further use. Throughout the entire process, for tables, we utilize the DeepShovel PDF Table Parser 18.\nThis tool ensures the completeness and accuracy of the table content while preserving the table structure, making it\nconvenient to reconstruct tables using Markdown. As for formulas, we employ an improved version of Latex-OCR 19\nfor the recognition, converting the parsing results into the string format. We open-source our PDF parsing solution on\nGitHub 20.\nTokenization is a crucial component of text corpus construction. To aid language models in comprehending academic\npapers, we utilize dedicated tokens for different types of special data. Finally, we use special tokens similar to the\noriginal Galactica paper [12] to unify various forms of text extracted from various sources into one standard protocol.\nBelow is an explanation of our special tokens.\n\u2022 Figures: We use the special tokens [START_FIGURE] and [END_FIGURE] to mark the captions of figures in the\npaper.\n\u2022 Tables: The special tokens [START_TABLE] and [END_TABLE] are employed to identify the position of tables\nwithin paragraphs. During this process, we convert tables from the PDF into Markdown format.\n\u2022 References: We use the special tokens [START_REF] and [END_REF] to annotate citations. The title of the article\nis placed within these special tokens to enhance readability.\n\u2022 Formulas: For mathematical content or formulas, we employ regular expressions and rule-based methods to filter\nand clean irregular formulas parsed from the PDF. Additionally, we use the special tokens [START_FORMULA]\nand [END_FORMULA] to capture them.\nAnd the dedicated tokens for these different types of special data are shown in Figure 3 (We use the one in [15])\n16Footnote for Gribid.\n17https://github.com/pymupdf/PyMuPDF\n18https://github.com/ShaoZhang0115/Table-Extraction-for-Geoscience-Literature\n19https://github.com/lukas-blecher/LaTeX-OCR\n20https://github.com/Acemap/pdf_parser\n10\nGeoGalactica\nA PREPRINT\nFigure 3: Tokenization processed text. A. shows an example of a figure marker, we only choose to preserve the captions;\nB. shows an example of a table marker, we transfer the tables into the form of Markdown; C. shows the tokenization of\nthe citations, we replace the reference numbers into reference papers\u2019 title to preserve the readability of the text corpus;\nD. shows an example of the special tokens for formulas.\n4.2\nThe Customized SFT dataset: GeoSignal Version 2\nThrough extensive investigation and research, we have thoroughly explored natural language processing tasks specifically\ntailored to geoscience. In this endeavor, we have identified a set of tasks that cater to the unique requirements of\ngeoscience applications. However, during this process, we have observed numerous unsupervised signals within these\ntasks that have yet to be fully harnessed and summarized.\n\u2022 Geoscience Knowledge Graph: Named entity recognition (NER) for temporal scales, rock types, etc., relation\nextraction (RE) for linking knowledge points, text-to-graph transformation, and knowledge discovery through\nreasoning\n\u2022 Academic Applications: Keyword extraction, summarization, and information retrieval.\n\u2022 General Applications: Question and Answering (Q&A), conversations related to geoscience education, and text\nclassification.\n\u2022 Geographical Applications: Point of Interest (POI) queries and multimodal Q&A.\nHowever, the supervised signals for these tasks can be reconstructed using professional geoscience data websites. Based\non the data scheme provided by K2, we further elaborate on the entire data construction process. In this process, we\nhave built three categories of data:\n11\nGeoGalactica\nA PREPRINT\n1. Literature-related data can be used to construct general natural language instructions, enabling the model to possess\nbasic semantic reasoning abilities.\n2. Geoscience-related data, which is used to build a knowledge-intensive instruction dataset, allowing the model to\nunderstand and comprehend the specific forms of natural language tasks in the field of geoscience.\n3. Self-instruction-related data, following the examples of Alpaca [51] and Baize [52], we have distilled geoscience-\nrelated data from ChatGPT and invited geoscience experts to annotate it. This data is used to construct high-quality\ngeoscience question-answering datasets.\n4.2.1\nDomain General Natural Language Instruction\nFor the general instruction learning data, we have integrated four platforms constructed under the initiative of DDE,\nsupported by Acemap, and reconstructed the data accordingly.\nFigure 4: Four platforms that contribute most to our GeoSignal.\nReferring to RST [53] and K2 [15], we restructure the signals from various geoscience-related platforms. The following\nparagraphs will provide a detailed explanation for each platform and the illustrations for restructured domain-general\nnatural language instruction.\nDeep Literature and DataExpo.\nThis two platforms can be understood as collections of papers and datasets.\nTherefore, the Related Paper (with abstract) and Reference resolution of Deep Literature, as well as the Reference\nresolution of DDE DataExpo, serve as excellent datasets for establishing referential relationships.\nUsing the text processing tool mentioned earlier, we explicitly employ a multi-threaded Grobid to process all PDF\npapers and convert them into an intermediate XML format. Within the converted XML, we identify the bibl_id of\nin-text citations and then locate the corresponding reference paper titles in the XML\u2019s reference section.\nGSO.\nSimilar to WordNet, the hierarchical tree of geological terms contains a vast amount of synonyms and word\nexplanations, providing valuable supervised signals. As a result, we traverse all the nouns in GSO, extract all the\nsynonyms for each term, and combine them with the term itself to create a set. We then construct all possible pairs of\n(term, synonym_term) and add them to a list of results.\nFor the word description, we traverse all the nouns of GSO, extract the definition of the respective noun to serve as\nthe description and create signal pairs (word, description). Additionally, there is also a specialized geology dictionary,\nwhich includes a dataset of categorized geology terms. The original data is in PDF format, and we convert it into JSON\nformat through PDF parsing. In this process, we first use a parsing tool to convert the PDF into a docx format, and then\nuse a data processing script to convert its content into JSON format. Subsequently, we proceed with content processing,\nremoving hyphens at line breaks, and merging multiple definitions of a single term. GSO use two geoscience dictionary.\nFor the geology dictionary, each entry consists of a \"name\" and \"description\". For the geography knowledge dictionary,\nincludes one more \"attribute\" field.\nGAKG.\nGAKG is rich in images, tables, and other elements from geology papers. Meanwhile, the text describing\nthese images and tables, as well as their captions, can serve as excellent sequence-to-sequence (seq2seq) supervised data.\nRegarding the papers and their graphical information, four types of binary pairs can be generated. During this process,\nwe transform the original text of the paper, tables, and illustrations in PNG format along with their corresponding\ncaptions, including table numbers and contents, into the target data format: (illustration caption, illustration content),\n(illustration caption, referring sentence), (table caption, table content), (table caption, referring sentence). For detailed\ninformation regarding this specific aspect, please refer to the Appendix.\n12\nGeoGalactica\nA PREPRINT\nOur approach to handling this is as follows:\n1. The captions and contents of tables and illustrations are stored in separate JSON files within their respective folders\nand can be extracted from there.\n2. The referring sentences, on the other hand, need to be retrieved from the original text of the paper by referencing the\ntable/illustration numbers mentioned in their captions.\nSpecifically, we search for the keywords \u201cfig\u201d (or variations like \u201cFig\u201d and \u201cFIG\u201d) and \u201ctable\u201d (or \u201cTable\u201d and \u201cTABLE\u201d)\nin the original text and identify the associated numbers (i.e., \u201ci\u201d) immediately following them. We then search for\ncomplete sentences between two periods preceding and following these numbers.\nOur program handles some unexpected scenarios, such as excluding cases like \u201cFig11\u201d or \u201cFig12\u201d when searching for\n\u201cFig1,\u201d and partially excluding cases where the confusion in numbering arises from referring to tables/illustrations from\nother papers. We also consider disorders caused by the dot used in English sentences and abbreviations, among other\ncases.\nHowever, there are still a few limitations to this method:\n1. When the keywords \"fig\" or \"table\" appear at the end of a sentence, our program includes both that sentence and the\nsubsequent one as the corresponding referring sentence.\n2. There might be instances where figures/tables from other papers are referenced. Our program can identify such cases\nif:\n1. The figure/table numbers are more significant than the current paper\u2019s total number of figures/tables.\n2. The word \"of\" appears close after \"Fig\" in the text.\nIn scenarios where it is difficult to discern whether a referenced figure/table belongs to another paper, we prioritize data\nquality. If we encounter any unmatched or garbled text, or the text is concise, we will discard that particular supervisory\nsignal.\nWikipedia.\nWikipedia contains a lot of crowd-sourcing information for geoscience. Consequently, we have also\nincorporated geoscience data from the Wikipedia page. To retrieve the information, we utilized web scraping techniques\nand relevant libraries.\nFor the article\u2019s title, we used the Wikipedia library in Python 21, which supports accessing sections of a Wikipedia page.\nEach section\u2019s title, text, and sub-sections form a nested structure. By recursively traversing each page, we obtained a\nlist of triplets comprising each section\u2019s level, title, and paragraph. The triplets are structured as (level, title, paragraph),\nwhere the level indicates the depth of nesting, the title represents the section\u2019s title, and the paragraph contains the\ncorresponding text content.\nTo retrieve the \u201cSummary & Abstract\u201d of the article, we utilize the Wikipedia library in Python to access the abstract of\nthe corresponding Wikipedia page directly. We then concatenate the paragraphs from the abovementioned sections to\nform the full text. Finally, we output the tuple (full text, abstract).\nTo extract the Entity mentioned in the article, we use the requests library and the BeautifulSoup 22 library to scrape the\nWikipedia page directly. We retrieve the text from all tags labeled \"p\" and \"ul\" and treat them as paragraphs. Next,\nwithin these paragraph tags, we search for tags labeled \"a\" with a href attribute starting with \"/wiki/\". These represent\nthe highlighted blue hyperlinked sections within the text. We collect these entities and output the tuple (paragraph,\nentities).\n4.2.2\nRestructured Knowledge-intensive Instruction\nIn our work of building restructured knowledge-intensive instruction data, we begin by searching for authoritative\nwebsites related to paleontology, dinosaurs, fossils, rocks, and other fields within geoscience. We then filter these\nwebsites, specifically selecting those with structured data available for extraction.\nFor the websites that can be structured, we perform corresponding restructured processing like K2 [15]. Taking the\nprovided image as an example, we match the structured data on the website using Key-Value pairs and create natural\nInstruction and Response pairs.\n21https://github.com/goldsmith/Wikipedia\n22https://www.crummy.com/software/BeautifulSoup/\n13\nGeoGalactica\nA PREPRINT\nDisciplines\nWebsites\nWebsites Intro.\nDinosaur\nhttps://dinoanimals.com/dinosaurdatabase/\nA comprehensive Dinosaur Database, offering a detailed catalog of dinosaurs.\nFossil\nhttps://fossilcalibrations.org/\nA specialized resource offering a curated collection of fossil calibrations.\nFossil\nhttp://fossil-ontology.com/\nA multi-dimensional scientific database of fossil specimens.\nMineral\nhttps://rruff.info/\nA dedicated resource for the study and identification of minerals.\nMineral\nhttps://zh.mindat.org/\nA comprehensive online mineralogical database.\nSedimentary\nhttps://mrdata.usgs.gov/\nA system with interactive maps and data for analyzing mineral resources on a regional and global scale.\nEarthquake\nhttps://www.usgs.gov/\nA website collecting all the earthquake world wide.\nHazard\nhttps://public.opendatasoft.com/explore/\nA platform for exploring various datasets sorted by their modification date.\nTable 3: Knowledge Intensive Data Sources.\nFigure 5: An example for illustrating the construction of restructured knowledge-intensive instruction data.\n4.2.3\nSelf-Instruct\nAccording to Alpaca [51] and Baize [52], using problem seeds to generate answer from ChatGPT 23 is an appropriate\nway to build instruction tuning data. In geoscience scenarios, we generate 1000 questions per subject under the\ngeoscience, and we put the problem seeds on GEOGALACTICA\u2019s Github Repo.\nIn terms of overall data collection, the total amount is as follows. And we select a certain proportion of data to be\nincluded in our supervised fine-tuning process. In the final version, after further manual verification and cleaning, we\nchoose to use a dataset of 100K samples as GeoSignal Version 2 for instructional data during the supervised fine-tuning.\nThe detailed statistic of the instruction tuning data is shown in Table 4.\n5\nTraining\nTaking the lessons from GLM-130B [14], we design the frameworks and plans of the GEOGALACTICA. The following\nare the details of our progress.\n5.1\nFurther Pre-training\nAfter the initial pre-training by Meta AI, the model Galactica can undergo additional training on a geoscience-specific\ndataset. We hope this fine-tunes the model\u2019s understanding and generation capabilities in particular domains or styles.\nWe utilize a supercomputing cluster based on the Hygon DCU architecture, combined with the Megatron-LM frame-\nwork [54], to further pre-train our models. The computing cluster consists of 512 nodes, with each node equipped with\na 32-core CPU, 128GB of memory, and 4 DCU acceleration cards, each with 16GB of memory, resulting in a total of\n2048 acceleration cards, where each acceleration card is equivalent to approximately 0.2 times the computing power of\nan NVIDIA A100 GPU. The Megatron-LM framework employs 3D parallelism strategies, including pipeline-parallel,\nmodel-parallel, and data-parallel, to maximize GPU performance while reducing communication overhead. Given the\nfour acceleration cards per node, we set the model parallel size to 4 for optimal model-parallel efficiency. Additionally,\nin the case of a mini-batch size of 1, we set the pipeline-parallel size to 16 to fully utilize the memory resources.\nWe preprocess all training text data by performing tokenization. The tokenized results of each document are then\nconcatenated using an end-of-sentence (eos) marker. Subsequently, we crop the concatenated sequences into fixed\n23In the data curation process and experiments throughout this paper, we use the 2023 March version of ChatGPT and 2023 March\nversion of GPT-4 unless otherwise specified\n14\nGeoGalactica\nA PREPRINT\nSignals\ntuples\n#NumofSamples\nDDE Scholar\nTitle (with Abstract)\n(abstract; title)\n2,690,569\nAbstract (with Publications Fulltext)\n(fulltext; abstract)\n2,601,879\nCategory (with abstract)\n(abstract; category)\n12,321,212\nRelated Paper (with abstract)\n(source abstract; target abstract; reference sentence)\n40,047,777\nOne Sentence Summary (with abstract)\n(abstract; question; answer)\n2,690,569\nReference resolution\n(sentence; pronoun.; reference item) [including citation]\n2,329,820\nDDE DataExpo\nTitle\n(abstract; title)\n216,036\nSummary & Abstract\n(fulltext; abstract)\n216,036\nGAKG\nGAKG\nPrincipal Concepts\n(sentence; entity; types)\n3,892,102\nRelations\n(abstract; sentence; head entity; relation; tail entity)\n30,123\nPaper table caption\n(table caption; refering sentence)\n2,772,166\nPaper illustration caption\n(illustration caption; refering sentence)\n9,128,604\nPaper table content\n(table caption; table content)\n2,772,166\nPaper illustration content\n(illustration caption; illustration content)\n9,128,604\nGSO\nFactual knowledge\n(sentence; facts; improper statement)\n114,392\nTaxonomy\n(upper term; term)\n112,298\nSynonyms\n(term; synonym term)\n23,018\nWord description\n(word; description; source)\n110,209\nGA-Dialogue\nFuture content and Previous content\n(corrupted text; corrupted positions; target spans)\n5,434\nGeoOpenData\ndinosaur\nFactual knowledge\n(property; property value)\n11,348\nfossilcalibrations\nFactual knowledge\n(property; property value)\n1,749\nfossilontology\nFactual knowledge\n(property; property value)\n3,210\nmindat\nFactual knowledge\n(property; property value)\n51,291\nngdb\nFactual knowledge\n(property; property value)\n148,212\nopendatasoft\nFactual knowledge\n(property; property value)\n37,823\nrruff\nFactual knowledge\n(property; property value)\n32,778\nusgsearthquake\nFactual knowledge\n(property; property value)\n37,284\nWordNet\nSynonyms\n(term; synonym term)\n6,408\nWord description\n(word; description; source)\n27,123\nWikipedia\nTitle\n(term; abstract)\n3,033,595\nSummary & Abstract\n(fulltext; abstract)\n753,920\nEntity mentions\n(paragraph; entities)\n3,688,926\nRelation\n(text; subject; property; object)\n630,210\nIODP\nTitle\n(abstract; title)\n2,839\nSummary & Abstract\n(fulltext; abstract)\n2,638\nTable 4: GeoSignal Statistics Table.\nlengths of 2048, resulting in 30 million training samples, corresponding to 7324 training steps. Before formally starting\nthe training, we conduct a preliminary experimental analysis of node failures and save checkpoints at intervals of 100\nsteps. We initiate the pre-training process after transforming the initial checkpoint format into the format Megatron-LM\nrequires. Ultimately, after running for 16 days, the computing cluster completes the further pre-training of the model\nat a speed of 3 minutes per step. Due to the frequent occurrence of node failures, the actual training takes nearly a\nmonth to complete. After the pre-training, we convert the checkpoints into the Hugging Face format for subsequent\napplications.\nChallenge in further pre-training\n1. Over-fitting: Further pre-training may increase the risk of overfitting, especially when the training data is relatively\nlimited compared to the original Galactica pre-training data (refer to Section 4).\n2. Catastrophic forgetting: In Further pre-train, ensuring that the training on the initial pre-training data is not\nforgotten is crucial. Sudden increases in the loss of new data sources can lead to the loss of knowledge acquired\nfrom the Galactica pre-training. It is essential to address how to effectively transfer higher-level language abilities\n15\nGeoGalactica\nA PREPRINT\nto specific tasks and prevent the loss of the model\u2019s generality obtained during the initial pre-training during the\nfine-tuning process.\n3. Stability and Convergence: Further pre-training models may be more prone to training instability and convergence\ndifficulties. During the training process, more sophisticated optimization techniques and strategies may be required\nto ensure that the model converges smoothly to an appropriate state.\nParameters transformation from Galactica to Megatron GPT-2\nSince Galactica belongs to the OPT model, we\nreferred to and modified the code available on Hugging Face for converting HF GPT-2 to Megatron GPT-2. The\nconversion parameters can be adjusted based on the actual scale of pipeline parallelism (PP), model parallelism (MP),\nand data parallelism (DP) during runtime.\nTraining detail\n* Training Setup: In this study, we utilized a supercomputing cluster based on the hygon DCU architecture and\ncombined it with the Megatron-LM framework for further pre-training of the model. The computing cluster consisted\nof 512 nodes, with each node equipped with a 32-core CPU, 128GB of memory, and 4 DCU accelerator cards\nwith 16GB of VRAM, totaling 2048 accelerator cards, each of which is equivalent to approximately 0.2 times the\ncomputational power of an NVIDIA A100.\n* Parallel Configuration: The Megatron-LM framework employed 3D parallelism techniques, including pipeline\nparallelism, model parallelism, and data parallelism, to maximize GPU performance and minimize communication\noverhead. Since each node had 4 accelerator cards, we set the model parallel size to 4 to achieve optimal parallel\nefficiency. Additionally, in cases where the mini-batch size was 1, we set the pipeline-parallel size to 16 to fully\nutilize the VRAM resources.\n* Data Preprocessing: We performed tokenization on all training text data, and the tokenized results of each document\nwere concatenated using the <eos> marker. Subsequently, we cropped the concatenated tokens into fixed lengths of\n2048, resulting in 30 million training samples, corresponding to 7324 training steps.\n* Checkpoints: Before formally starting the training process, we analyzed the node failure patterns through preliminary\nexperiments and saved checkpoints at intervals of 100 steps.\n* Hyperparameter Selection: We conducted extensive experiments for hyperparameter selection in Further pre-train.\nRegarding learning rate scheduling, initial experiments showed that directly adopting a maximum learning rate of\n1e \u2212 4 from the Galactica-30B model led to a rapid increase in loss after a certain number of steps, resulting in\ntraining failure. Hence, we observed the gradient norm curve during the training warm-up phase and selected the\nlearning rate corresponding to the minimum gradient norm, which was 1e \u2212 5, as the actual maximum learning rate\nfor training, which remained constant throughout the entire training process. For the training warm-up, we employed\na linear training warm-up strategy and tested different training warm-up steps, and the optimum result was achieved\nwith 100 training warm-up steps. Regarding other hyperparameters, we opted for the Adam optimizer with a \u03b21 of\n0.9, a \u03b22 of 0.95, a weight decay rate of 0.1, and epsilon of 1e \u2212 8. To balance effectiveness and efficiency, we set\nthe global batch size to 4096 and utilized checkpoint activations to save VRAM. Additionally, we set the gradient\nclip threshold to 1.0 and a dropout rate of 0.1.\nFor a better understanding of our training, we list the hyperparameters of the model and the configured setting of the\ntraining in Appendix E.\nTraining curves\nWe share the curves of the training loss and gradient normalization as Figure 6 and Figure 7. We\nobserved that the training loss quickly dropped from about 1.60 to 1.40 during the first 300 steps and then smoothly\ndecreased from 1.40 to 1.32 in the subsequent steps. Although the gradient normalization showed several spikes, sharply\nincreasing from 0.1 to approximately 0.3~4.8, the model exhibited no signs of saturation after further pre-training on 60\nbillion tokens. This demonstrates the stability of the entire further pre-training process.\nThe bottleneck of the training\n\u2022 The embedding layer is not treated as a stand-alone component in the training process. Instead, it is combined with\nthe first transformer layer. As a result, the VRAM usage on certain cards is 60% higher than on others, leading to\ndecreased training efficiency. This is because a larger PP value is required to accommodate the entire model, which\nincreases communication overhead.\n\u2022 Due to some bugs in Megatron, the \"continue pre-train\" function cannot utilize distributed optimizers. This results\nin each DP group or model replica having a complete copy of the optimizer state, significantly increasing VRAM\nusage.\n16\nGeoGalactica\nA PREPRINT\nFigure 6: Training curve during the further pre-training.\nFigure 7: Training curve of the first 500 steps during the further pre-training.\n5.2\nSupervised Fine-Tuning\nAfter pre-training, LLMs can be supervised fine-tuning (SFT) on a smaller, more targeted dataset under human\nsupervision. This process adapts the model to specific tasks or improves its performance in certain areas.\nWe employed SFT to enhance the geoscientific reasoning performance of our large-scale models on specific geoscientific\ntasks. This process is essential to effectively transfer advanced language capabilities to geoscientific-specific tasks and\npreserve the model\u2019s generalization acquired during pre-training.\nWe utilized two major frameworks, Huggingface and DeepSpeed, during this stage to facilitate our training work. This\naimed to accomplish instruction fine-tuning and model prediction tasks. In the training process, the Hygon DCU cluster\nremained our primary resource. Compared to the pre-training stage, SFT truncation only took advantage of 128 nodes\nand their accompanying 512 DCUs. We continued to employ the learning rate schedule used during pre-training, where\nthe maximum learning rate was set to 1e \u2212 5, combined with a linear warm-up consisting of 100 warm-up steps. For\nthe optimizer, we still selected the Adam optimizer, with \u03b21 and \u03b22 set to 0.9 and 0.999, respectively. Additionally, a\nweight decay of 0.05 and \u03f5 value of 1e-8 was chosen to better adapt to the required fine-tuning tasks.\nConsidering the enormous scale of the model, we utilized the DeepSpeed ZeRO3 technique for memory optimization,\nalong with the gradient checkpoint method, to further reduce memory pressure. The maximum input sequence length\nwas limited to 512 in this process to avoid unnecessary computational overhead. However, due to the limitations of\nDeepSpeed, the global batch size had to be no smaller than the number of accelerator cards. Therefore, we opted\nfor a larger global batch size of 512. Regarding the settings of other parameters, we followed the default values of\nthe Huggingface trainer framework. For the subsequent training, we used the Alpaca dataset and conducted training\nfor three epochs, which only took about one day to obtain the final SFT model. This training process, supported by\nMegatron-LM, supported our research work.\n17\nGeoGalactica\nA PREPRINT\nFigure 8: Training curve during the SFT on dataset Alpaca.\nFigure 9: Training curve during the SFT on Geosignal.\nFollowing the recipe proposed by K2 [15] and a similar experience in RadiologyGPT [55], we did the SFT in two stages.\nFor the first stage, we aligned the model with humans via Alpaca instruction tuning data, while using the GeoSignal v2\nin the second stage. The training curve of SFT on Alpaca is Figure 8 whole the SFT on GeoSignal is Figure 9\nMoreover, we compare the variety of the instruction tuning data of Dolly and GeoSignal in Figure 11, showing that the\ngeneral domain instructions dataset has less variety than the knowledge-intensive instructions dataset.\n5.3\nTool Learning\nIn addition, LLMs can be designed to interact with and learn from various tools, such as browsers, databases, or other\nsoftware interfaces. This allows the model to perform more complex tasks that require external information or specific\nfunctionalities.\nWe leveraged the ToolBench dataset [56], an open-source resource, to enable geoscientific large-scale models to leverage\ntool API capabilities. We sampled five types of tool QA data from ToolBench, namely arxiv, bing_search, database,\nweather, and wolframalpha, and supplemented it with our collected geo_search data, resulting in approximately 10k\ntraining samples. We open-source this dataset on https://github.com/zthang/geotools.\nDuring the SFT stage, we trained the models together with training data such as alpaca. As the training samples\nfrom the tool data tend to be longer, we set the max_length to be 2048. During training, we only calculate the loss\nand backpropagate the gradients for the part of the API call, specifically the thought, action, action input, and the\ncorresponding tokens for the final answer.\nOnce the model is trained, we specify the tool\u2019s description and corresponding API parameter instructions in the prompt.\nFor a given question, we first let the model output the related API call (thought, action, action input) to obtain the results\nreturned by the external tool. These results are then used as observations and fed back into the model, generating a new\nset of thought, action, and action input for the next iteration (if further tool calls are required). This process continues\nuntil the model gathers enough information and outputs the final answer.\n18\nGeoGalactica\nA PREPRINT\nFigure 10: Training curve during the tools SFT.\n(a)\n(b)\nFigure 11: Variety of the instruction tuning data in Dolly and GeoSignal.\nHere are two naive examples of how the Galactica-30b models use the tool. The detailed examples are shown in the\nAppendix:\n\u2022 Example 1:\nQuestion: \"What is the weather in New York 3M years ago?\"\nThought: \"weather\"\nAction: \"geo_search\"\nAction Input: \"New York, Weather, 3M years\"\n\u2022 Example 2\nQuestion: \"What is the definition of plate tectonics?\"\nThought: \"arxiv\"\nAction: \"search\"\nAction Input: \"query: plate tectonics\"\n6\nEvaluation\nOnce we have completed the model\u2019s training, we proceed to examine its grasp of scientific and geoscientific knowledge.\nWe have divided the evaluation into two parts.\n19\nGeoGalactica\nA PREPRINT\n\u2022 The first part involves automated evaluation using the GeoBench provided by K2. This enables us to assess the\nmodel\u2019s performance in handling geoscientific tasks. Additionally, to examine if the newly learned knowledge has\naffected the pre-existing ability, we conducted MMLU (Minimal Meaningful Learning Units) tests. These tests are\ncompared against the original Galactica model.\n\u2022 The second part encompasses manual evaluation, where we carefully selected several subtasks from geoscience. For\nthis evaluation, we invited 10 researchers specializing in geoscience to participate in voting and scoring. Ultimately,\nwe compare the model\u2019s performance with five other large-scale platforms in open testing.\nBy conducting these evaluations, we aim to comprehensively assess the model\u2019s abilities and compare its performance\nagainst automated benchmarks and human assessments, ensuring its competence in scientific and geoscientific domains.\n6.1\nAutomatic Evaluation\n6.1.1\nGeoBench\nGeoBench, proposed by [15] is a benchmarking tool specifically designed to evaluate and test the geoscientific\nunderstanding and capabilities of LLMs. It focuses on assessing how well LLMs can process and generate responses\ninvolving geographic and geological information.\nBaselines\nNPEE\nAPTest\nRandom\n27.1\n20.0\nGal-6.7B\n25.7\n29.9\nLLaMA-7B\n21.6\n27.6\nK2-7B\n39.9\n29.3\nChatGPT\n48.8\n20.0\nGal-30B\n41.2\n38.5\nGalAlp-30B\n42.6\n44.1\nGEOGALACTICA-30B\n46.6\n36.9\nTable 5: comparison among baselines on Objective tasks in GeoBench.\nThrough testing our models on GeoBench, we have observed that larger and more academic models outperform\nbenchmarks like NPEE, which are inclined toward academic research. However, they do not perform well in benchmarks\nlike AP Study, which lean more towards foundational education. This difference may be caused by training materials\nthat guide the model to contemplate more advanced knowledge. The training data consists of academic research\nachievements, namely papers, which may result in a deviation from and lack of basic knowledge. This is an area we\nintend to focus on for improvement in the future.\nIt is worth noting that Galactica, with 30 billion parameters, often fails to outperform Llama, with 7 billion parameters,\nin general benchmark tasks. However, in our GeoBench, we have successfully developed GEOGALACTICA, which\nbuilds upon Galactica, surpassing K2, built upon Llama.\n6.1.2\nMMLU\nThe MMLU has been divided into math and non-math sections by Galactica, and we have been following their reports\nclosely. From the results (Shown in Table 6), it is evident that after processing 6 million geoscience-related literature\ndocuments, specific skills of the model, such as algebra, biology, chemistry, and mathematics, have shown improvement.\nThis phenomenon appears to be linked to papers focusing on mathematical geology, biological geoscience, and chemical\ngeology, highlighting the interdisciplinary nature of geoscience. Surprisingly, machine learning has experienced\nsignificant enhancement, likely due to the inclusion of GitHub code in our corpus. In summary, subjects closely related\nto geoscience, including those logically connected to geology and its subfields, have shown notable progress. However,\ndisciplines like physics indicate that the original Galactica outperforms our GEOGALACTICA and subjects unrelated\nto geosciences, such as medical genetics, medicine, and electrical engineering, have shown a decline in performance.\nIt is noteworthy that GEOGALACTICA and the original Galactica are generally at a similar stage regarding average\nperformance in math-related subjects within the MMLU.\nAfter assessing the mathematical subject, we examined the results of the subjects that were excluded. Overall,\nGEOGALACTICA performs slightly better than the original Galactica in the average of non-math-related subjects in\nMMLU. Interestingly, subjects like global facts, US History, and World History have significantly improved compared\nto the original Galactica. This phenomenon can be attributed to the fact that many aspects of history, such as significant\n20\nGeoGalactica\nA PREPRINT\nSubject\nGEOGALACTICA30B\nGAL 30B\nGalAlp 30B\nAbstract Algebra\n0.300\n0.250\n0.320\nAstronomy\n0.461\n0.500\n0.474\nCollege Biology\n0.576\n0.576\n0.514\nCollege Chemistry\n0.370\n0.320\n0.350\nCollege Computer Science\n0.400\n0.410\n0.370\nCollege Mathematics\n0.320\n0.350\n0.350\nCollege Medicine\n0.480\n0.520\n0.445\nCollege Physics\n0.284\n0.333\n0.294\nEconometrics\n0.377\n0.368\n0.368\nElectrical Engineering\n0.538\n0.579\n0.503\nElementary Mathematics\n0.328\n0.310\n0.288\nFormal Logic\n0.302\n0.270\n0.278\nHigh School Biology\n0.565\n0.561\n0.535\nHigh School Chemistry\n0.360\n0.399\n0.355\nHigh School Computer Science\n0.500\n0.480\n0.510\nHigh School Mathematics\n0.311\n0.256\n0.304\nHigh School Physics\n0.298\n0.364\n0.325\nHigh School Statistics\n0.333\n0.352\n0.319\nMachine Learning\n0.411\n0.339\n0.366\nMedical Genetics\n0.550\n0.580\n0.520\nAverage\n0.4032\n0.40585\n0.3894\nTable 6: We report the results of the three models in math.\ndiscoveries and political knowledge, are closely intertwined with geoscience. This underscores the significance of\ngeoscience, which can profoundly influence global progress.\nFurthermore, in conceptual physics, learning from geoscience papers has led to a better understanding of the model.\nThis suggests that several concepts in geoscience do not align with the knowledge taught in colleges and high schools.\nConsequently, models struggle to apply this related knowledge when solving problems at the college and high school\nlevels.\nObservation on ablation\nFortunately, we came across Galpaca-30B on Hugging Face 24, which significantly reduced\nthe carbon emissions from our finetuning experiments. This model utilized Alpaca\u2019s instructions to learn from the\ndataset and was applied to SFT on Galactica-30B. Upon horizontal comparison, Galpaca-30B performed notably worse\nthan the original Galactica and GEOGALACTICA in the majority of disciplines. This indicates that instruction learning in\nthe general domain can significantly impact the performance of specialized domain models during practical evaluations.\n6.2\nHuman Evaluation\nIn this part, we have selected five open models to evaluate together with our GEOGALACTICA model. These models\ninclude:\n1. MOSS, an open-source tool-augmented conversational language model, was released by Qiu Xipeng\u2019s team from the\nSchool of Computer Science at Fudan University as a ChatGPT-like model.\n2. Qwen is a chatbot developed by Alibaba Cloud, a technology company under the Alibaba Group. Alibaba announced\nits intention to open Tongyi Qianwen to the public, indicating its readiness for the market and reflecting China\u2019s\ngrowing focus on AI technology.\n3. ChatGPT is an AI language model developed by OpenAI, known for its ability to generate human-like text based on\nprompts, facilitate engaging conversations, answer questions, and perform a wide range of language-related tasks. 25\n4. Yiyan, also known as Ernie Bot, is an AI chatbot service product developed by Baidu. It has been under development\nsince 2019 and is based on a large language model named \"Ernie 4.0\", which was announced on October 17, 2023\n24https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b\n25We use the 2023 March version of ChatGPT.\n21\nGeoGalactica\nA PREPRINT\nSubject\nGeoGal 30B\nGal 30B\nGalAlp 30B\nAnatomy\n0.496\n0.541\n0.533\nBusiness Ethics\n0.430\n0.420\n0.470\nClinical Knowledge\n0.532\n0.555\n0.491\nComputer Security\n0.600\n0.650\n0.620\nConceptual Physics\n0.481\n0.434\n0.417\nGlobal Facts\n0.390\n0.300\n0.340\nHigh School European History\n0.533\n0.606\n0.491\nHigh School Geography\n0.581\n0.540\n0.515\nHigh: School Gov & Politis\n0.534\n0.565\n0.461\nHigh School Macroeconomics\n0.408\n0.405\n0.367\nHigh School Microeconomics\n0.424\n0.458\n0.424\nHigh School Psychology\n0.613\n0.628\n0.556\nHigh School US History\n0.436\n0.352\n0.319\nHigh School World History\n0.620\n0.456\n0.446\nHuman Aging\n0.552\n0.552\n0.511\nHuman Sexuality\n0.511\n0.565\n0.481\nInternational Law\n0.612\n0.644\n0.554\nJurisprudence\n0.491\n0.472\n0.444\nLogical Fallacies\n0.423\n0.472\n0.442\nManagement\n0.573\n0.602\n0.515\nMarketing\n0.641\n0.705\n0.607\nMiscellaneous\n0.522\n0.501\n0.470\nMoral Disputes\n0.480\n0.462\n0.468\nMoral Scenarios\n0.238\n0.244\n0.245\nNutrition\n0.536\n0.520\n0.448\nPhilosophy\n0.444\n0.492\n0.431\nPrehistory\n0.503\n0.522\n0.435\nProfessional Accounting\n0.344\n0.312\n0.319\nProfessional Iaw\n0.326\n0.326\n0.327\nProfessional Medicine\n0.438\n0.449\n0.379\nProfessional Psychology\n0.472\n0.505\n0.449\nPublic Relations\n0.473\n0.445\n0.455\nSecurity Studies\n0.424\n0.408\n0.322\nSociology\n0.537\n0.547\n0.483\nUS Foreign Policy\n0.550\n0.510\n0.540\nVirology\n0.434\n0.422\n0.410\nWorld Religion\n0.421\n0.427\n0.380\nAverage\n0.487\n0.486\n0.448\nTable 7: We report the results of the three models in social sciences.\n5. ChatGLM is an open bilingual language model developed by Tsinghua University. It is optimized for Chinese\nconversation and is based on the General Language Model architecture.\nFor the selected projects, our evaluation is designed as follows:\nWe refer to K2\u2019s Human Evaluation and define the evaluation metrics for open-ended questions: scientificity, correctness,\nand coherence (score range is [1, 2, 3]). The specific explanations are as follows:\n\u2022 Scientificity: It represents whether the generated content appears as something that a geoscience professional would\nsay. A score of 1 indicates not good, 2 indicates acceptable, and 3 indicates very good.\n\u2022 Correctness: From the perspective of a geoscience expert, whether the model convinces you and if the information\nobtained is correct. A score of 1 indicates incorrect, 2 indicates possibly right, and 3 shows correct.\n\u2022 Coherence: This metric is used to evaluate the consistency and coherence of the model, i.e., whether the text\nconsistently discusses a specific topic and reads smoothly. A score of 1 indicates not good, 2 indicates acceptable,\nand 3 indicates very good.\nBased on this, the cumulative score can be calculated. Additionally, for the functional questions of the large model, the\nevaluation metric is relative ranking. Participants in the evaluation will receive replies from all six models on the same\n22\nGeoGalactica\nA PREPRINT\nCategory\nProblem\nPrompt\nSkills\nOpen-ended\nNoun Definition\nWhat is carbonate rock?\nKnowledge\nOpen-ended\nBeginner Level Q&A\nHow many continents are there in the world?\nKnowledge\nOpen-ended\nIntermediate Level Q&A\nWould the Ohio train derailment leading to vinyl chloride leakage\naffect the ecological environment around the Great Lakes based on\nocean currents or air dispersion?\nAnalysis\nOpen-ended\nAdvanced Level Q&A\nHow did dinosaurs become extinct?\nDiscovery\nFunctional\nConfirmation of Geoscience\nKnowledge System\nIs carbonate rock a type of limestone?\nJudgment\nFunctional\nGeoscience Paper Titling\nThis is the abstract of my paper.\nCan you help me come up with a title?\nSummarization\nFunctional\nPaper Summary\nThis is my passage.\nCan you help me summarize the passage?\nSummarization\nFunctional\nSpeech Writing\nPlease help me write a speech based on my topic.\nWriting\nFunctional\nPre-requisite\nKnowledge Recommendations\nThis is my article.\nCan you recommend some prerequisite knowledge points?\nInformation extraction\nTable 8: Tasks we designed in human evaluation parts.\ninput, and our expert judges will rate these models in the order of 1, 2, 3, 4, 5, and 6. Finally, the total ranking of each\nmodel will be calculated. In this part, we invite 10 geoscience practical people, including 6 students and 4 teachers.\n(The contribution of the human evaluation is shown in Appendix L).\nOpen-ended Tasks\nFor open-ended questions, the general large-scale model uses the interface output provided\nby ChatALL 26 for consistency. Our large model interacts through our UI interface, where higher metric scores are\npreferred.\n6.2.1\nNoun Definition\nIn our geoscience entrance exam question set, we randomly selected 20 geoscience vocabulary terms to evaluate the\nmodel\u2019s understanding of domain-specific terminology, the whole terms are in subsubsection G.1.1.\nScientificity\nCorrectness\nCoherence\nMOSS\n291\n302\n351\nQianwen\n419\n435\n435\nChatGPT\n337\n351\n357\nYiyan\n236\n276\n305\nChatGLM\n278\n291\n347\nGEOGALACTICA\n339\n361\n393\nTable 9: Comparison on Noun Definition tasks.\nIn this task, our model has demonstrated remarkable vocabulary proficiency, but what indeed astonishes us is its\nexceptional ability to handle scientific questions and professional respond beyond what other models can achieve. One\nexample is shown in subsection H.1.\n6.2.2\nBeginner Level Q&A\nWe selected 10 easy questions from the high school geoscience Olympiad in China and had them translated into English\nby professional translators. The questions are shown in subsubsection G.1.2. One example is shown in subsection H.2.\nOverall, our model ranks third when considering all aspects, but ChatGPT outperforms other models significantly in\nthis category of questions.\n6.2.3\nIntermediate Level Q&A\nWe have selected 10 moderately difficult questions from Chegg and SaveMyExam, which require a certain level of geo-\nscience knowledge training. The questions are shown in subsubsection G.1.3. One example is shown in subsection H.3.\n26https://github.com/sunner/ChatALL\n23\nGeoGalactica\nA PREPRINT\nScientificity\nCorrectness\nCoherence\nMOSS\n116\n120\n147\nQianwen\n191\n177\n207\nChatGPT\n219\n214\n225\nYiyan\n176\n174\n187\nChatGLM\n160\n156\n184\nGEOGALACTICA\n176\n173\n202\nTable 10: Comparison on Beginner Level Q&A.\nScientificity\nCorrectness\nCoherence\nMOSS\n143\n154\n178\nQianwen\n178\n180\n193\nChatGPT\n210\n206\n207\nYiyan\n180\n186\n189\nChatGLM\n161\n163\n179\nGEOGALACTICA\n162\n169\n171\nTable 11: Comparison on Intermediate Level Q&A.\nOverall, our model ranks tied for fourth place, but ChatGPT outperforms other models significantly in this category of\nquestions. The results are shown in Table 11.\n6.2.4\nAdvanced Level Q&A\nWe have selected 9 highly difficult questions from the urgent geoscience problems proposed by the Institute of\nGeography, Chinese Academy of Sciences. These questions require extensive training in geoscience knowledge as well\nas the ability to reason through scientific research. The questions are shown in subsubsection G.1.4. One example is\nshown in subsection H.4.\nScientificity\nCorrectness\nCoherence\nMOSS\n166\n173\n194\nQianwen\n202\n199\n209\nChatGPT\n137\n133\n181\nYiyan\n190\n192\n200\nChatGLM\n172\n171\n194\nGEOGALACTICA\n185\n187\n206\nTable 12: Comparison on Advanced Level Q&A.\nOverall, according to Table 12 our model ranks third, but ChatGPT seems to lack sufficient capability to handle these\ntypes of questions.\nFunctional Tasks\nWhen it comes to the evaluation of functional questions, we have chosen to apply GEOGALACTICA\nto scientific research literature. GEOGALACTICA is dedicated to facilitating the comprehension and interpretation of\nscientific research literature. When external information input is not required, we utilize the consistent output provided\nby the interface of ChatALL. In terms of overall evaluation, since it involves ranking, lower scores are preferred.\n6.2.5\nKnowledge-based associative judgment question\nTo determine the presence or absence of knowledge system relationships, the questions are derived from the Knowledge\ntrees in GSO. The questions are shown in subsubsection G.2.1. One example is shown in subsection H.5.\nOverall, our model ranks fifth, indicating that there is still significant room for improvement in handling these logical\nquestions. Further advancements can be achieved by constructing CoT-type data and injecting more expertise into the\nmodel.\n24\nGeoGalactica\nA PREPRINT\nModels\nSum of Rank\nMOSS\n579\nQianwen\n557\nChatGPT\n600\nYiyan\n570\nChatGLM\n752\nGEOGALACTICA\n725\nTable 13: Comparison on knowledge-based associative judgment question.\n6.2.6\nResearch Paper Titling Task\nIn this phase, we randomly selected abstracts from 20 geoscience research papers and inputted them into the model,\nasking it to generate a title. This task showcases the model\u2019s understanding of knowledge points and familiarity with\nthe field. The questions are shown in subsubsection G.2.2. One example is shown in subsection H.6.\nModels\nSum of Rank\nMOSS\n805\nQianwen\n426\nChatGPT\n326\nYiyan\n561\nChatGLM\n440\nGEOGALACTICA\n451\nTable 14: Comparison on research paper titling task.\nOverall, our model ranks fourth, with no clear distinction between the ChatGLM, Qianwen, and our model in terms of\nperformance on this task.\n6.2.7\nGeoscience Research Functionality\nTo ensure fairness when incorporating external research papers for evaluation, we employ our own PDF parsing\nsolution to interpret the papers. We then use the consistent output provided by the ChatALL interface. As for our\nGEOGALACTICA, we utilize our UI interface for interactions and obtain outputs accordingly.\nMore specifically, when it comes to interpreting scientific literature, we often inquire about the following aspects:\n1. Can you help me write a speech based on the content of the article?\n2. Can you help me summarize the article?\n3. Could you please recommend some prerequisite knowledge points?\nThese three scenarios are all closely related to us as researchers. We have assessed five papers, which cover various\ndomains of Earth sciences and are written in different styles. The papers are listed in subsubsection G.2.3. One example\nis shown in subsection H.7. After the evaluation, our ranking is as follows:\nWriting\nSummary\nExtraction\nMOSS\n114\n164\n115\nQianwen\n135\n185\n232\nChatGPT\n62\n86\n51\nYiyan\n178\n139\n160\nChatGLM\n106\n168\n169\nGEOGALACTICA\n135\n100\n212\nTable 15: Comparison on geoscience research functionality.\nOverall, considering the summation of the three scores, ChatGPT and MOSS are better than GEOGALACTICA, and\nGEOGALACTICA is tied third with ChatGLM. Our model demonstrates excellent summarization skills, thanks to its\ncomprehensive incorporation of knowledge in the field of Earth sciences. The same principle applies to the task of\nextracting key information, for which we still need to gather a certain amount of expert thinking data.\n25\nGeoGalactica\nA PREPRINT\nFigure 12: The user interface for evaluating the GEOGALACTICA.\n7\ndiscussion\n7.1\nThe Necessity of Pre-training\nInitiating training for a domain-specific language model from scratch in the field of geoscience is a complex decision\nthat requires careful consideration of multiple factors. Here, we discuss some thoughts on why we did not consider\ntraining from scratch:\n1. Geoscience data is relatively limited, and training a high-quality model from scratch requires sufficient data support.\nThe availability of geoscience data is constrained, potentially leading to the issue of data scarcity when training from\nscratch.\n2. Training a large-scale language model from scratch demands substantial computational resources and time. As we\nlack sufficient resources and time, employing pre-trained models and conducting transfer learning may yield more\ncost-effective results.\n3. Using pre-trained models and conducting transfer learning has shown promising results within a relatively short\ntimeframe. Thus, further training can be practical. Refer to K2. Therefore, while training from scratch would enable\nmodels to comprehend and capture domain-specific terminology, concepts, and relationships in geoscience, as well\nas train from the outset on geoscience data to better adapt to domain-specific language and knowledge, we opted for\na strategy that involves further pre-training due to cost, time, and data considerations.\n7.2\nThe Necessity of Further Pre-training\nThus, in our perspective, employing more general-purpose models for transfer learning and further pre-training in the\nfield of geoscience can be meaningful because:\n1. Geoscience encompasses various specialized domains, such as geology, meteorology, and environmental science.\nFurther pre-training can enhance the model\u2019s understanding and capture of these domain-specific concepts, terms,\nand relationships through training on geoscience-related textual data. Additionally, geoscience often involves many\n26\nGeoGalactica\nA PREPRINT\ndomain-specific terms and contextual information that may not be commonly found in everyday language. The model\ncan better comprehend and contextualize these terms through further pre-training, thereby improving performance in\ngeoscience texts.\n2. Geoscience texts may rely on specific geographical backgrounds, spatial and temporal relationships, and regional\ninformation. Further pre-training can assist the model in better understanding these contextual dependencies, leading\nto more accurate information processing and generation. Further pre-training can enhance model performance for\ntasks such as text classification, information extraction, and generating geological reports. The model can learn more\ntask-specific feature representations by training on relevant domain-specific data.\n3. During this process, we observed the alleviation of data scarcity in geoscience. In certain domains within geoscience,\nscarce data may limit training samples. However, further pre-training enables the model to learn general language\nabilities from a larger-scale dataset and subsequently fine-tune on a smaller amount of domain-specific data,\nmitigating the impact of data scarcity.\nIn conclusion, further pre-training can enable large language models to better adapt to the characteristics and require-\nments of the geoscience field, leading to enhanced performance in geoscience text processing and task execution.\n7.3\nCarbon Emissions\nDuring our cumulative training, 1,488,137.26 DCU hours were consumed, resulting in cumulative carbon emissions of\n212 tCO2eq calculated by Equation 1. Our work provides a foundational model for subsequent geoscience researchers\nto fine-tune their smaller models, potentially reducing carbon emissions in their future work.\nCEmission(kg) = DCU hours \u2217 TDP (kW) \u2217 CIntensity (kg/kWh)\n(1)\n7.4\nTowards Unified Foundation Model in Geoscience\nThe application of artificial intelligence in geosciences demonstrates vast prospects. In terms of geoscientific literature\nanalysis, AGI systems especially the unified foundation model can assist researchers in identifying the frequency of\nspecific vocabulary while addressing any ambiguities, thereby enhancing the accuracy of literature comprehension.\nFurthermore, AGI can integrate dispersed geoscientific knowledge by analyzing extensive literature uncovering novel\ncorrelations and trends, thus providing new perspectives and directions for geoscience research. Additionally, AGI\nsystems can aid in geoscience education, offering personalized content and teaching methods to facilitate students\u2019 ease\nof learning and understanding of geoscientific knowledge.\nOn the other hand, the application of a unified foundation model in the geoscience domain extends beyond academic\nresearch to practical uses such as geological hazard warnings, resource exploration, and environmental protection.\nRegarding geological hazard warnings, AGI can utilize big data and models to provide accurate predictions and\nassessments, helping to mitigate the damages caused by natural disasters. Concurrently, a unified foundation model\nplays a crucial role in underground resource exploration, improving efficiency and accuracy in exploration endeavors.\nIn the realm of environmental protection, a unified foundation model aids in the real-time monitoring of environmental\nconditions through the analysis of remote sensing data, supporting decision-making processes for environmental\nconservation.\nIn the future, with sufficient abundant data and computing powers, and other feasibility of achieving unified foundation\nmodels in the field of geoscience, the future of AGI in Geoscience can be expected. In the future, the unified foundation\nmodel will continue to play a role in advancing frontiers in geoscience research. It can assist scientists in conducting\nlarge-scale data analysis, unraveling complex phenomena such as internal Earth structures, plate tectonics, and crustal\nevolution, thereby providing deeper scientific comprehension. AGI also contributes to environmental monitoring and\nprotection, aeromagnetic data interpretation, water resource management, carbon capture, and other domains. By\nanalyzing hydrological data and geological information, a unified foundation model can predict groundwater resources\u2019\ndistribution and sustainable utilization, providing scientific foundations for water resource management. In carbon\ncapture, a unified foundation model can assist researchers in selecting suitable geological storage layers and sealing\nrocks, thereby driving the development of carbon reduction technologies. Overall, AGI accelerates the accumulation of\nscientific knowledge in geosciences and offers unprecedented support in addressing global challenges, providing robust\nintelligent assistance for humanity\u2019s future sustainable development.\n8\nConclusion\nIn conclusion, the utility of NLP in geoscience research and practice is vast, and large language models (LLMs) have\nshown great success in various NLP domains. However, specialized LLMs in geoscience are scarce. We introduce\n27\nGeoGalactica\nA PREPRINT\nGEOGALACTICA, a 30B parameters language model designed explicitly for geoscience applications. Through training\non a comprehensive geoscience academic dataset and fine-tuning with geoscience-knowledge intensive instruction\npairs, GEOGALACTICA outperforms existing models in geoscience NLP tasks. Our validation with senior geoscientists\nconfirms its effectiveness. The release of GEOGALACTICA and our training experience aims to contribute to the\nadvancement of unified foundation models in geoscience.\nAcknowledgement\nThis project is conducted under the DDE initiative. The computation resource was supported by the Advanced\nComputing East China Sub-center. This work is supported by NSF China (No.62020106005, 61960206002, 42050105,\n62061146002, 62106143), National Key Technologies R&D Program (No. 2022YFB3904201), Shanghai Pilot Program\nfor Basic Research - Shanghai Jiao Tong University. The second author would like to thank Wu Wen Jun Honorary\nDoctoral Scholarship, AI Institute, Shanghai Jiao Tong University.\n28\nGeoGalactica\nA PREPRINT\nReferences\n[1] Shu Wang, Yunqiang Zhu, Yanmin Qi, Zhiwei Hou, Kai Sun, Weirong Li, Lei Hu, Jie Yang, and Hairong Lv. A\nunified framework of temporal information expression in geosciences knowledge system. Geoscience Frontiers,\n2022.\n[2] Cheng Deng, Yuting Jia, Hui Xu, Chong Zhang, Jingyao Tang, Luoyi Fu, Weinan Zhang, Haisong Zhang, Xinbing\nWang, and Cheng Zhou. Gakg: A multimodal geoscience academic knowledge graph. Proceedings of the 30th\nACM International Conference on Information & Knowledge Management, 2021.\n[3] Rahul Ramachandran, Muthukumaran Ramasubramanian, Pravesh Koirala, Iksha Gurung, and Manil Maskey.\nLanguage model for earth science: Exploring potential downstream applications as well as current challenges.\nIGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium, pages 4015\u20134018, 2022.\n[4] Hao Zhang and Jin-Jian Xu. When geoscience meets foundation models: Towards general geoscience artificial\nintelligence system. arXiv preprint arXiv:2309.06799, 2023.\n[5] Qinjun Qiu, Zhong Xie, Liang Wu, and Wenjia Li. Geoscience keyphrase extraction algorithm using enhanced\nword embedding. Expert Systems with Applications, 125:157\u2013169, 2019.\n[6] Christopher JM Lawley, Michael G Gadd, Mohammad Parsa, Graham W Lederer, Garth E Graham, and Arianne\nFord. Applications of natural language processing to geoscience text data and prospectivity modeling. Natural\nResources Research, pages 1\u201325, 2023.\n[7] Qinjun Qiu, Zhong Xie, Liang Wu, and Liufeng Tao. Automatic spatiotemporal and semantic information\nextraction from unstructured geoscience reports using text mining techniques. Earth Science Informatics, 13:1393\u2013\n1410, 2020.\n[8] Qinjun Qiu, Zhong Xie, Liang Wu, and Wenjia Li. Dgeosegmenter: A dictionary-based chinese word segmenter\nfor the geoscience domain. Computers & geosciences, 121:1\u201311, 2018.\n[9] Chengbin Wang, Xiaogang Ma, Jianguo Chen, and Jingwen Chen. Information extraction and knowledge graph\nconstruction from geoscience literature. Computers & geosciences, 112:112\u2013120, 2018.\n[10] Cheng Deng, Bo Tong, Luoyi Fu, Jiaxin Ding, Dexing Cao, Xinbing Wang, and Chenghu Zhou. Pk-chat: Pointer\nnetwork guided knowledge driven generative dialogue model. arXiv preprint arXiv:2304.00592, 2023.\n[11] Kai Ma, Miao Tian, Yongjian Tan, Xuejing Xie, and Qinjun Qiu. What is this article about? generative\nsummarization with the bert model in the geosciences domain. Earth Science Informatics, pages 1\u201316, 2022.\n[12] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S. Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. ArXiv, abs/2211.09085,\n2022.\n[13] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u2019elien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.\n[14] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong,\nand Jie Tang. Glm-130b: An open bilingual pre-trained model. ArXiv, abs/2210.02414, 2022.\n[15] Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Le Zhou, Luoyi Fu, Weinan Zhang,\nXinbing Wang, Cheng Zhou, Zhouhan Lin, and Junxian He. Learning a foundation language model for geoscience\nknowledge understanding and utilization. ArXiv, abs/2306.05064, 2023.\n[16] Fanchun Meng, Tao Ren, Zhenxian Liu, and Zhida Zhong. Toward earthquake early warning: A convolutional\nneural network for repaid earthquake magnitude estimation. Artificial Intelligence in Geosciences, 2023.\n[17] Yifeng Fei, Hanpeng Cai, Junhui Yang, Jiandong Liang, and Guang Hu. Unsupervised pre-stack seismic facies\nanalysis constrained by spatial continuity. Artificial Intelligence in Geosciences, 2023.\n[18] Qingkai Kong, Andrea Chiang, Ana C Aguiar, M Giselle Fern\u00e1ndez-Godino, Stephen C Myers, and Donald D\nLucas. Deep convolutional autoencoders as generic feature extractors in seismological applications. Artificial\nintelligence in geosciences, 2:96\u2013106, 2021.\n[19] Priyadarshi Chinmoy Kumar and Kalachand Sain. Machine learning elucidates the anatomy of buried carbonate\nreef from seismic reflection data. Artificial Intelligence in Geosciences, 4:59\u201367, 2023.\n[20] Mazahir Hussain, Shuang Liu, Umar Ashraf, Muhammad Ali, Wakeel Hussain, Nafees Ali, and Aqsa Anees.\nApplication of machine learning for lithofacies prediction and cluster analysis approach to identify rock type.\nEnergies, 15(12):4501, 2022.\n29\nGeoGalactica\nA PREPRINT\n[21] Reda Abdel Azim. A new correlation for calculating wellhead oil flow rate using artificial neural network.\nArtificial Intelligence in Geosciences, 3:1\u20137, 2022.\n[22] Peiyi Yao, Ziwang Yu, Yanjun Zhang, and Tianfu Xu. Application of machine learning in carbon capture and\nstorage: An in-depth insight from the perspective of geoscience. Fuel, 333:126296, 2023.\n[23] Deming Xu, Yusheng Wang, Jingqi Huang, Sijin Liu, Shujun Xu, and Kun Zhou. Prediction of geology condition\nfor slurry pressure balanced shield tunnel with super-large diameter by machine learning algorithms. Tunnelling\nand Underground Space Technology, 131:104852, 2023.\n[24] Ting Chen, Yaojun Wang, Hanpeng Cai, Gang Yu, and Guangmin Hu. High resolution pre-stack seismic inversion\nusing few-shot learning. Artificial Intelligence in Geosciences, 3:203\u2013208, 2022.\n[25] Anik Saha and Sunil Saha. Integrating the artificial intelligence and hybrid machine learning algorithms for\nimproving the accuracy of spatial prediction of landslide hazards in kurseong himalayan region. Artificial\nIntelligence in Geosciences, 3:14\u201327, 2022.\n[26] Karianne J. Bergen, Paul A. Johnson, Maarten V. de Hoop, and Gregory C. Beroza. Machine learning for\ndata-driven discovery in solid earth geoscience. Science, 363(6433), mar 2019.\n[27] Claudia Hulbert, Bertrand Rouet-Leduc, Paul A. Johnson, Christopher X. Ren, Jacques Rivi\u00e8re, David C. Bolton,\nand Chris Marone. Similarity of fast and slow earthquakes illuminated by machine learning. Nature Geoscience,\n12(1):69\u201374, dec 2018.\n[28] Ying Chen, Jim Haywood, Yu Wang, Florent Malavelle, George Jordan, Daniel Partridge, Jonathan Fieldsend,\nJohannes De Leeuw, Anja Schmidt, Nayeong Cho, Lazaros Oreopoulos, Steven Platnick, Daniel Grosvenor, Paul\nField, and Ulrike Lohmann. Machine learning reveals climate forcing from aerosols is dominated by increased\ncloud cover. Nature Geoscience, 15(8):609\u2013614, aug 2022.\n[29] Xiao Liu, Juan Hu, Qi Shen, and Huan Chen. Geo-bert pre-training model for query rewriting in poi search. In\nConference on Empirical Methods in Natural Language Processing, 2021.\n[30] Qinjun Qiu, Kai Ma, Hairong Lv, Liufeng Tao, and Zhong Xie. Construction and application of a knowledge\ngraph for iron deposits using text mining analytics and a deep learning algorithm. Mathematical Geosciences,\n55(3):423\u2013456, 2023.\n[31] Bin Wang, Liang Wu, Zhong Xie, Qinjun Qiu, Yuan Zhou, Kai Ma, and Liufeng Tao. Understanding geological\nreports based on knowledge graphs using a deep learning approach. Computers & Geosciences, 168:105229,\n2022.\n[32] Huseyin Denli, HassanJaved Chughtai, Brian Hughes, Robert Gistri, and Peng Xu. Geoscience language processing\nfor exploration. Day 3 Wed, November 17, 2021, 2021.\n[33] Qinjun Qiu, Zhong Xie, Kai Ma, Liufeng Tao, and Shiyu Zheng. Neurospe: A neuro-net spatial relation extractor\nfor natural language text fusing gazetteers and pretrained models. Transactions in GIS.\n[34] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. In Conference\non Empirical Methods in Natural Language Processing, 2019.\n[35] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing.\nACM Transactions on Computing for Healthcare (HEALTH), 3(1):1\u201323, 2021.\n[36] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur,\nDavid Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. ArXiv, abs/2303.17564,\n2023.\n[37] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: Generative\npre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics, 2022.\n[38] Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit,\nClara Grazian, W. Zhang, Imran Razzak, and Bram Hoex. Darwin series: Domain specific large language models\nfor natural science. ArXiv, abs/2308.13565, 2023.\n[39] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\n36:1234 \u2013 1240, 2019.\n[40] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital\nreadmission. ArXiv, abs/1904.05342, 2019.\n[41] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. Legal-\nbert: The muppets straight out of law school. ArXiv, abs/2010.02559, 2020.\n30\nGeoGalactica\nA PREPRINT\n[42] Yifan Zhang, Cheng Wei, Shangyou Wu, Zhengting He, and Wenhao Yu. Geogpt: Understanding and processing\ngeospatial tasks through an autonomous gpt. arXiv preprint arXiv:2307.07930, 2023.\n[43] Chong Ma, Zihao Wu, Jiaqi Wang, Shaochen Xu, Yaonai Wei, Zhengliang Liu, Lei Guo, Xiaoyan Cai, Shu Zhang,\nTuo Zhang, et al. Impressiongpt: an iterative optimizing framework for radiology report summarization with\nchatgpt. arXiv preprint arXiv:2304.08448, 2023.\n[44] Zhiyuan Peng, Xuyang Wu, and Yi Fang. Soft prompt tuning for augmenting dense retrieval with large language\nmodels. arXiv preprint arXiv:2307.08303, 2023.\n[45] wikipedia. History of artificial neural networks. 2023.\n[46] Shao Zhang, Yuting Jia, Hui Xu, Ying Wen, Dakuo Wang, and Xinbing Wang. Deepshovel: An online collaborative\nplatform for data extraction in geoscience literature with ai assistance. arXiv preprint arXiv:2202.10163, 2022.\n[47] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha\nChowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought\ncan solve them. arXiv preprint arXiv:2210.09261, 2022.\n[48] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language\nmodels. arXiv preprint arXiv:2210.03493, 2022.\n[49] Grobid: A machine learning software for extracting information from scholarly documents. https://github.\ncom/kermitt2/grobid, 2008\u20132023.\n[50] Christopher Clark and Santosh Kumar Divvala. Pdffigures 2.0: Mining figures from research papers (jcdl\u201916).\n143\u2013152. Google Scholar Google Scholar Digital Library Digital Library, 2016.\n[51] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/\ntatsu-lab/stanford_alpaca, 2023.\n[52] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-\nefficient tuning on self-chat data. ArXiv, abs/2304.01196, 2023.\n[53] Weizhe Yuan and Pengfei Liu. restructured pre-training. ArXiv, abs/2206.11147, 2022.\n[54] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism.\narXiv preprint\narXiv:1909.08053, 2019.\n[55] Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen,\nSekeun Kim, et al. Radiology-gpt: A large language model for radiology. arXiv preprint arXiv:2306.08666, 2023.\n[56] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill\nQian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong\nSun. Toolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv, abs/2307.16789, 2023.\n[57] Michael D. McCormack. Neural computing in geophysics. Geophysics, 10:11\u201315, 1991.\n[58] Dave Hale. Methods to compute fault images, extract fault surfaces, and estimate fault throws from 3d seismic\nimages. Geophysics, 78, 2013.\n[59] Anders U. Waldeland and Anne H. Schistad Solberg. Salt classification using deep learning. 2017.\n[60] Li-Xin Wang and Jerry M. Mendel. Adaptive minimum prediction-error deconvolution and source wavelet\nestimation using hopfield neural networks. Geophysics, 57:670\u2013679, 1992.\n[61] Yusuf Nasir and Louis J. Durlofsky. Deep reinforcement learning for optimal well control in subsurface systems\nwith uncertain geology. J. Comput. Phys., 477:111945, 2022.\n[62] Pablo Guill\u00e9n, Germ\u00e1n Larraz\u00e1bal, Gladys Gonzalez, Dainis Boumber, and Ricardo Vilalta. Supervised learning\nto detect salt body. Seg Technical Program Expanded Abstracts, 2015.\n[63] Heidi Anderson Kuzma. A support vector machine for avo interpretation. Seg Technical Program Expanded\nAbstracts, pages 181\u2013184, 2003.\n[64] Henri Blondelle, A. Juneja, J. Micaelli, and Philip Neri. Machine learning can extract the information needed for\nmodelling and data analysing from unstructured documents. 2017.\n[65] L. Y. Zheng, A. Albayrak, W. L. Teng, M. G. Khayat, and L. Pham. Using Transformer Networks and Knowledge\nGraphs in Earth Science Literature to Synthesize Mass Information for Transdisciplinary Research. In AGU Fall\nMeeting Abstracts, volume 2020, pages IN030\u201304, December 2020.\n31\nGeoGalactica\nA PREPRINT\n[66] Danfeng Hong, Lianru Gao, Jing Yao, Bing Zhang, Antonio J. Plaza, and Jocelyn Chanussot. Graph convolutional\nnetworks for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing, 59:5966\u2013\n5978, 2020.\n[67] Prasanna Koirala, Muthukumaran Ramasubramanian, Iksha Gurung, Manil Maskey, and Rahul Ramachandran.\nBERT-E: An Earth Science Specific Language Model for Domain-Specific Downstream Tasks. In AGU Fall\nMeeting Abstracts, volume 2021, pages IN15B\u201306, December 2021.\n[68] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand\nKorthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and\nMatei A. Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. SC21:\nInternational Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201314, 2021.\n32\nGeoGalactica\nA PREPRINT\nA\nAppendix: Progression of geoscience with AI\nHere we show the progression of geoscience research with the use of cutting-edge AI techniques summarized by [45].\nMethods\nIn CS\nIn Geoscience\nGap\nNN\n1951\n1991 Neural computing in geophysics [57]\n40\nPerceptron\n1958\n-\nKNN\n1967\n2013 Methods to compute fault images, extract fault surfaces, and estimate fault throws from 3D seismic images [58]\n46\nCNN/BP\n1980/1989\n2017 Salt classification using deep learning [59]\n37\nRNN\n1982\n1992 Adaptive minimum prediction-error deconvolution and source wavelet estimation using Hopfield neural networks [60]\n10\nBP\n1986\n-\nRL\n1989\n2022 Deep reinforcement learning for optimal well control in subsurface systems with uncertain geology [61]\n33\nRandom Forest\n1995\n2015 Supervised learning to detect salt body [62]\n20\nSVM\n1995\n2003 A support vector machine for avo interpretation [63]\n8\nLSTM\n1997\n2017 Machine learning can extract the information needed for modelling and data analysing from unstructured documents [64]\n20\nTransformer\n2017\n2020 Using Transformer Networks and Knowledge Graphs in Earth Science Literature to Synthesize Mass Information for Transdisciplinary Research [65]\n3\nGCN\n2017\n2020 Graph Convolutional Networks for Hyperspectral Image Classification [66]\n3\nBERT\n2018\n2021 BERT-E: An Earth Science Specific Language Model for Domain-Specific Downstream Tasks [67]\n3\nChatGPT/LLM\n2022\n2023 K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization [15]\n1\nTable 16: The progression of geoscience research with the use of cutting-edge AI techniques.\nB\nAppendix: GeoCorpus\nHere we show the distribution of the collected papers from top-10 amounts journals in geoscience.\nFigure 13: Distribution of the collected papers from top-10 amounts journals in geoscience.\n33\nGeoGalactica\nA PREPRINT\nC\nAppendix: GeoSignal V2 Curation\nBelow, we will provide a detailed explanation of how we obtain useful supervision signals for geoscience tasks from\nwebsites like MinDat, USGS, NGDB, Fossil Ontology, and Fossil calibrations.\nC.1\nMinDat\nFigure 14: Mines in MinDat.\nThis website provides a list of all minerals in alphabetical order, and it is possible to obtain the collection of all minerals\nstarting with a certain letter from this page. (As shown in Figure 14) After scraping the HTML page, the following\nPython spider logic can be used to retrieve the URLs of all minerals starting with letters A-Z: (as shown in Figure 15)\nfind(id=\"mainwrap\") \u2192 find(id=\"mineralindex\")\nTo extract the relevant information for each mineral, we can create a dictionary for each mineral containing the following\ninformation:\n\u2022 Mineral Name. Key: Mineral, Value: Name of Mineral.\n\u2022 Physical Properties. Extract information on the physical properties of the mineral, including Colour, Lustre, Specific\nGravity, Crystal System, Hardness, Name.\n\u2022 Chemical Properties. Extract the chemical elements present in the mineral as Chemical Element and flatten them.\nFor example, Abellaite should be Na, Pb, C, O, H.\n\u2022 Type and Occurrence. Extract information on the Type and Occurrence of the mineral, including Type locality\n(which will be a separate URL in the format shown below), General Appearance, Place of Conservation.\n\u2022 References. Extract and include the References in the dictionary, with value as a list.\nNot all minerals will have all the above information. For minerals with missing information, we will still create a\ndictionary entry but the value will be \"No corresponding information\". For example, the dictionary for Abellaite will be\nillustrated in Figure 16.\n34\nGeoGalactica\nA PREPRINT\nFigure 15: Mines in MinDat.\nFigure 16: Signals in MinDat.\n35\nGeoGalactica\nA PREPRINT\nC.2\nUSGS\nFigure 17: USGS collections.\nFigure 18: USGS signals.\nWhen the webpage is opened, it displays important earthquake information from 1900 to 2023 around the world, and the\ninformation can be organized by year. For a given year \"x\", the corresponding webpage for significant earthquakes in that\nyear can be accessed through the URL: https://earthquake.usgs.gov/earthquakes/browse/significant.\nphp?year=x\n36\nGeoGalactica\nA PREPRINT\nFigure 19: NGDB collections.\nFor example, the webpage for significant earthquakes in the year 2023 can be accessed through the URL: https:\n//earthquake.usgs.gov/earthquakes/browse/significant.php?year=2023\nThe webpage for the year 2002 can be accessed through the URL: https://earthquake.usgs.gov/earthquakes/\nbrowse/significant.php?year=2002\nTherefore, using this method, all significant earthquake information pages from 1900 to 2023 can be obtained.\nFor each specific webpage corresponding to a given year, the list of earthquake URLs can be obtained using the function\n\"find(class=\u2019eqitem\u2019)\".\n\u2022 Basic Information. The basic information that needs to be obtained is the name, with the key value set as \"Name\"\nand the value as the name obtained from the webpage title.\n\u2022 Origin. The information needed for this section includes: Location, Origin Time, Minimum Distance, and Azimuthal\nGap.\n\u2022 Moment Tensor. The information needed for this section includes: Moment, Magnitude, Depth, and Percent DC.\n\u2022 Post ShakeAlert. The information needed for this section includes: Messages Issued, Magnitude Estimates, and\nNearby Cities. Since there may be multiple nearby cities, a list will be created to store them as the value for this\nspecific dictionary entry.\nFinally, we obtain the signals in USGS, specifying which signals are retrieved and providing details on how they are\nprocessed.\nC.3\nNGDB\nThe webpage provides a list of URLs containing information about all the sedimentary rocks. Figure 19\n37\nGeoGalactica\nA PREPRINT\nFigure 20: Mines in NGDB.\nFigure 21: NGDB final signals.\nInformation Acquisition:\nFor each type of sedimentary rock, corresponding information needs to be extracted. The extracted information for\neach type of sedimentary rock will be combined into one dictionary, and each dictionary will be a list for each type\nof sedimentary rock. In practice, it is possible that some sedimentary rocks may lack information. In this case, a\ncorresponding dictionary should still be created, with the value set to \"No corresponding information.\" The final data\nwill be stored in a JSON file. The information that needs to be extracted for each type of sedimentary rock is as follows,\ntaking 62M110 as an example.\nThe information needs to be extracted from \"About the sample\" and \"Chemical analysis\" sections (marked in red in the\nfigure).\n\u2022 About the sample. The following information needs to be extracted from this section: Lab ID, Submitter, Date\nsubmitted, State, Country, Location (this part is obtained by synthesizing the data from Original Latitude and Original\nLongitude), Location Precision, and Source.\n\u2022 Chemical analysis This section requires obtaining the major fossil elements of the sedimentary rock, by only\nrecording chemical elements with a composition greater than or equal to 1%.\n38\nGeoGalactica\nA PREPRINT\nFinally, we obtain the signals in NGDB, with details on the nature of these signals and the processing steps involved.\nC.4\nFossil Ontology\nFigure 22: Fossil ontology signals.\nThe webpage displays a list of all fossils, so the list can be directly organized. The final data format will be a list, where\neach element in the list is a dictionary representing a type of fossil, and the data will be saved in a JSON file. Taking\nTetrastigma shantungensis as an example, below shows the data structure:\n\u2022 Basic Information of the Fossil. Basic Information of the Fossil This part only requires the information of the\nbiological lineage to which the fossil belongs, with key value \"Lineage\".\n\u2022 Geological Information of the Fossil. This part requires information about the age of the fossil, the location of the\nfossil, and GPS information. The key values are \"Age\", \"Locality\", and \"GPS\", respectively.\n\u2022 Relevant Research Information of the Fossil. This part requires information about the collection points and\nreference sources of the fossil, with key values \"House\" and \"References\", respectively. Among them, there are\nmany References, which are organized as a list as the value.\nFinally, we obtain the signals in Fossil Ontology.\nC.5\nFossil calibrations\nStarting from the aforementioned URLs, all the specific webpage information about 220 fossil types is included. After\ngrabbing the HTML, please organize and save it accordingly.\nTo search, we use \"find(class=\u2019listed-calibrations\u2019)\", as shown in the following image:\n39\nGeoGalactica\nA PREPRINT\nFigure 23: Fossil calibrations collections.\nAfter performing the above search, it is possible to obtain the URLs for all the fossils, and then extract information\nabout each of them one by one.\nThe final data format will be a list, where each element in the list is a dictionary representing a type of fossil, and the\ndata will be saved in a JSON file. Taking the fossil Sessilia as an example, below shows the information to be extracted\nand the format:\n\u2022 Basic Information. This section contains two parts: the species to which the fossil belongs, and the biological\nlineage of the species. The key value for species name is \"Name\", and the value is the name of the species to which\nthe fossil belongs; The key value for biological lineage is \"Lineage\", and the value is the biological lineage of the\nspecies.\n\u2022 Age Information. This part includes the earliest and latest time of the fossil, with key values \"Minimum age\" and\n\"Maximum age\", respectively.\n\u2022 Key Fossil Information. This part mainly concerns the key fossils used to determine the node period, including the\nlocation and geological age, with key values \"Locality\" and \"Geological age\" respectively.\n\u2022 References. This part mainly includes the calibration and reference sources of the fossil information, with key\nvalues \"Calibration\" and \"Reference\", respectively.\nFinally, we obtain the signals in Fossil Calibrations.\nFigure 24: Fossil calibrations signals.\n40\nGeoGalactica\nA PREPRINT\nD\nAppendix: Prompts\nBelow, we will list the prompt words we use for constructing Instruction tuning data.\nddescholar.abstract.title\n1. Input: What is a suitable title for this geoscience paragraph: [input]\nOutput: The title can be \u2019[output]\u2019\n2. Input: Provide a fitting title for this geoscience text: [input]\nOutput: One possible title can be \u2019[output]\u2019\n3. Input: Suggest a title for this geoscience passage: [input]\nOutput: \u2019[output]\u2019 can be a suitable title.\n4. Input: What would be an appropriate title for this geoscience paragraph: [input]\nOutput: Here is a title for you to consider: [output].\n5. Input: What should be the title of this geoscience text considering its content: [input]\nOutput: \u2019[output]\u2019 is one option.\nddescholar.abstract.keyword\n1. Input: Extract the main keywords from this geoscience paragraph: [input]\nOutput: The main keywords in the paragraph can be listed as follows: [output]\n2. Input: Identify the key terms in this geoscience passage: [input]\nOutput:The key terms in the passage can be identified as: [output]\n3. Input: What are the most important keywords in this geoscience text: [input]\nOutput: The most important keywords in the text are: [output]\n4. Input: What are the fields of study connected to this geoscience text: [input]\nOutput:The fields of study connected to this text include: [output]\n5. Input: Determine the fields of research related to this geoscience paragraph: [input]\nOutput: The related fields of research for this paragraph are: [output]\nddescholar.reference.resolution\n1. Input: When you see the sentence like [input], can you recommend some related papers for me?\nOutput: Alright, I have found a relevant paper for you: \\n[output]\n2. Input: Based on the content of this sentence [input], could you suggest a few relevant papers?\nOutput: Okay, I\u2019ve got the related paper right here: \\n[output]\n3. Input: Considering the information in this sentence [input], what papers would you recommend?\nOutput: Sure, here\u2019s the associated paper you\u2019re looking for: \\n[output]\n4. Input: Given the context of this sentence [input], can you provide a list of related papers?\nOutput: No problem, I have the corresponding paper for you: \\n[output]\n5. Input: After reading this sentence [input], what are some papers you would recommend?\nOutput: Certainly, I\u2019m showing you the related paper now: \\n[output]\ngso.wikipedia.title\n1. Input: Can you explain the concept of [title] in relation to [keyword]?\nOutput: Certainly, when discussing [title] in the context of [keyword], [paragraph]\n2. Input: Please share information about [title] concerning [keyword].\nOutput: Of course, regarding [title] and its connection to [keyword], [paragraph]\n3. Input: What does the [title] section cover about the topic [keyword]?\nOutput: In the [title] section about [keyword], we have: [paragraph]\n4. Input: Please describe [title] as it pertains to [keyword].\nOutput: Certainly, in terms of [title] and its relationship to [keyword], [paragraph]\n5. Input: Can you elaborate on [title] in connection with [keyword]?\nOutput: Of course, when discussing [title] and its connection to [keyword], [paragraph]\n41\nGeoGalactica\nA PREPRINT\ngso.wikipedia.entity\n1. Input: Can you identify the key terms related to earth science mentioned in this passage? The passage: [input]\nOutput: Sure, the terms related to earth science mentioned in the passage include: [output]\n2. Input: What are the terms connected to earth science present in this passage? The passage: [input]\nOutput: The terms connected to earth science in the passage are: [output]\n3. Input: Please list the terms associated with earth science found in the given passage. The passage: [input]\nOutput: The terms associated with earth science in the passage are as follows: [output]\n4. Input: Can you help me find the terms relevant to earth science in this passage? The passage: [input]\nOutput: Sure, the terms relevant to earth science in the passage are: [output]\n5. Input: Identify the terms in the passage that are related to earth science. The passage: [input]\nOutput: The terms in the passage related to earth science are: [output]\ngso.wordnet.description\n1. Input: Can you provide a description for the word \u2019[input]\u2019?\nOutput: Sure, the description for the word \u2019[input]\u2019 is: [output]\n2. Input: What does the word \u2019[input]\u2019 mean?\nOutput: The meaning of the word \u2019[input]\u2019 is: [output]\n3. Input: Please explain the meaning of the term \u2019[input]\u2019.\nOutput: The term \u2019[input]\u2019 means: [output]\n4. Input: What can you tell me about the meaning of the word \u2019[input]\u2019?\nOutput: The word \u2019[input]\u2019 has the following meaning: [output]\n5. Input: Help me understand the definition of the term \u2019[input]\u2019.\nOutput: The definition of the term \u2019[input]\u2019 is: [output]\ngso.wordnet.synonym\n1. Input: Can you provide a synonym for the word \u2019[input]\u2019?\nOutput: Sure, a synonym for the word \u2019[input]\u2019 is: [output]\n2. Input: What is a synonym of the word \u2019[input]\u2019?\nOutput: A synonym of the word \u2019[input]\u2019 is: [output]\n3. Input: Please give me a synonym for the term \u2019[input]\u2019.\nOutput: A synonym for the term \u2019[input]\u2019 is: [output]\n4. Input: What would be a synonym for the word \u2019[input]\u2019?\nOutput: A synonym for the word \u2019[input]\u2019 is: [output]\n5. Input: Help me find a synonym for the term \u2019[input]\u2019.\nOutput: A synonym for the term \u2019[input]\u2019 is: [output]\ngso.dictionary.definition\n1. Input: Can you provide the specialized definition of the term \u2019[input]\u2019?\nOutput: Sure, in the context of geoscience, the term \u2019[input]\u2019 is defined as: [output]\n2. Input: What does the term \u2019[input]\u2019 mean in a specialized context?\nOutput: In the field of geoscience, the term \u2019[input]\u2019 means: [output]\n3. Input: Please explain the meaning of the specialized term \u2019[input]\u2019.\nOutput: The specialized term \u2019[input]\u2019, in the context of geoscience, means: [output]\n4. Input: Can you suggest the meaning of the term \u2019[input]\u2019 in a specific field?\nOutput: In the field of geoscience, the meaning of the term \u2019[input]\u2019 is: [output]\n5. Input: What would be the definition of the word \u2019[input]\u2019 in a specialized context?\nOutput: In the context of geoscience, the definition of the word \u2019[input]\u2019 is: [output]\ngso.dictionary.synonym\n1. Input: Can you provide a synonym for the word \u2019[input]\u2019?\nOutput: Sure, you might [output]\n42\nGeoGalactica\nA PREPRINT\n2. Input: What is a synonym of the word \u2019[input]\u2019?\nOutput: You can [output]\n3. Input: Please give me a synonym for the term \u2019[input]\u2019.\nOutput: Of course, you should [output]\n4. Input: What would be a synonym for the word \u2019[input]\u2019?\nOutput: One can [output]\n5. Input: What\u2019s a synonym for the word \u2019[input]\u2019?\nOutput: You can [output]\ngso.dictionary.classification\n1. Input: Identify the scientific discipline associated with the term \u2019[input]\u2019:\nOutput: [output]\n2. Input: In which scientific discipline would you find the term \u2019[input]\u2019?\nOutput: [output]\n3. Input: The term \u2019[input]\u2019 is primarily associated with which of these disciplines?\nOutput: [output]\n4. Input: In the realm of Earth sciences, which discipline is most closely linked to the term \u2019[input]\u2019?\nOutput: [output]\n5. Input: Which scientific discipline is most relevant when discussing the term \u2019[input]\u2019?\nOutput: [output]\ngso.taxonomy.hyponymy.child\n1. Input: What are some subfields or specific topics that fall under the broader concept of [parent]?\nOutput: Some subfields or specific topics that fall under the broader concept of [parent] in geoscience include\n[child].\n2. Input: What are some specific examples of [parent] within geoscience?\nOutput: Some specific examples of [parent] within geoscience include [child].\n3. Input: What are some narrower categories or subdisciplines within [parent]?\nOutput: Some narrower categories or subdisciplines within [parent] in geoscience include [child].\n4. Input: What are some specific types or varieties of [parent] in geoscience?\nOutput: Some specific types or varieties of [parent] in geoscience include [child].\n5. Input: What are some specific techniques or methodologies used to study [parent] in geoscience?\nOutput: Some specific techniques or methodologies used to study [parent] in geoscience include [child].\ngso.taxonomy.hyponymy.parent\n1. Input: What is the overarching category that [child] belongs to?\nOutput: It can be classified under [parent], which is a broad field of study within geoscience.\n2. Input: What are the subfields that fall under [parent]?\nOutput: Some subfields that are part of [parent] include [child], among others.\n3. Input: What are some related concepts to [parent] in geoscience?\nOutput: Some related concepts to [parent] in geoscience are [child], among others.\n4. Input: What is the specific category that [child] belongs to within geoscience?\nOutput: The specific category that [child] belongs to within geoscience is [parent].\n5. Input: What are the different branches of geoscience that [child] is part of?\nOutput: Some branches of geoscience that [child] is part of include [parent], among others.\ngso.taxonomy.hyponymy.judgment.parent\n1. Input: Is [child] a child concept of [parent] in geoscience?\nOutput: [Answer]\n2. Input: Does [child] fall under the broader concept of [parent] in geoscience?\nOutput: [Answer]\n43\nGeoGalactica\nA PREPRINT\n3. Input: Is [child] a narrower category or subdiscipline within [parent] in geoscience?\nOutput: [Answer]\n4. Input: Are [child] specific types or varieties of [parent] in geoscience?\nOutput: [Answer]\n5. Input: Do scientists use [child] as a technique or methodology to study [parent] in geoscience?\nOutput: [Answer]\ngso.taxonomy.hyponymy.judgment.child\n1. Input: Is [parent] the broader concept that encompasses [output] in geoscience?\nOutput: [Answer]\n2. Input: Does [parent] serve as the parent category for [output] in geoscience?\nOutput: [Answer]\n3. Input: Is [parent] a higher-level concept that includes [output] in geoscience?\nOutput: [Answer]\n4. Input: Does [parent] have a broader field of study than [child] within geoscience?\nOutput: [Answer]\n5. Input: Is [parent] a common parent concept of [child] within geoscience?\nOutput: [Answer]\nmetaearth\n1. Input: What\u2019s the [key] of the [object]?\nOutput: The [object]\u2019s [key] is [value].\n2. Input: Can you tell me the [key] of the [object]?\nOutput: Sure, the [object]\u2019s [key] is [value].\n3. Input: What is the [object]\u2019s [key]?\nOutput: The [key] of the [object] is [value].\n4. Input: What would be the [key] of the [object]?\nOutput: The [object]\u2019s [key] is [value].\n5. Input: I\u2019d like to know the [key] of the [object].\nOutput: The [object] has a [key] of [value].\ngakg.qa\n1. Input: What\u2019s the [key] of the paper according to the abstract: \\n[object]?\nOutput: The paper\u2019s [key] is [value].\n2. Input: Can you tell me the [key] of the paper according to the abstract: \\n[object]?\nOutput: Sure, the paper\u2019s [key] is [value].\n3. Input: What is the paper\u2019s [key]? According to the abstract: \\n[object]\nOutput: The [key] of the paper is [value].\n4. Input: Please inform me of the paper\u2019s [key]. According to the abstract: \\n[object]\nOutput: The [key] associated with the paper is [value].\n5. Input: I\u2019d like to know the [key] of the paper According to the abstract: \\n[object].\nOutput: The paper has a [key] of [value].\nner\n1. Input: Please extract the named entities in: [input].\nOutput: The named entities in the passage can be listed as follows: \\n[output]\n2. Input: What are the named entities mentioned in: [input]?\nOutput: The named entities mentioned in the passage are: \\n[output]\n3. Input: Can you identify the named entities in: [input]?\nOutput: Sure, the named entities in the passage are: \\n[output]\n44\nGeoGalactica\nA PREPRINT\n4. Input: What are the geological terms or concepts mentioned in: [input]?\nOutput: The geological terms or concepts mentioned in the passage are: \\n [output]\n5. Input: What are the names of any geological formations mentioned in: [input]?\nOutput: The names of the geological formations mentioned in the passage are: \\n [output]\ngakg.illustration\n1. Input: What kind of illustration when you see these content? [input]\nOutput: [output]\n2. Input: What type of illustration comes to mind when you observe the following content? [input]\nOutput: [output]\n3. Input: \"What sort of illustration is associated with this content? [input]\nOutput: [output]\n4. Input: What kind of illustration can you imagine when presented with this content? [input]\nOutput: [output]\n5. Input: When examining the following content, what type of illustration would you associate with it? [input]\nOutput: [output]\ngakg.table\n1. Input: What kind of table when you see these elements? [input]\nOutput: [output]\n2. Input: What type of table comes to mind when you observe the following content? [input]\nOutput: [output]\n3. Input: What sort of table is associated with this elements? [input]\nOutput: [output]\n4. Input: What kind of table can you imagine when presented with this content? [input]\nOutput: [output]\n5. Input: Upon seeing the content, what kind of table do you think of? [input]\nOutput: [output]\n45\nGeoGalactica\nA PREPRINT\nE\nAppendix: Training setup\n\u2022 Model setup (30B parameters)\n\u2013 num layers: 48\n\u2013 num attention heads: 56\n\u2013 hidden size: 7168\n\u2013 max position embeddings: 2048\n\u2013 layernorm epsilon: 1e \u2212 5\n\u2022 Regularization Setup\n\u2013 optimizer: Adam\n\u2013 attention dropout: 0.1\n\u2013 hidden dropout: 0.1\n\u2013 weight decay: 0.1\n\u2013 clip-grad: 1.0\n\u2013 adam \u03b21: 0.9\n\u2013 adam \u03b22: 0.95\n\u2013 adam \u03f5: 1e \u2212 8\n\u2022 Training Setup\n\u2013 micro-batch-size: 1\n\u2013 global-batch-size: 4096\n\u2013 recompute-activations: True (gradient checkpointing)\n\u2013 train-samples: 30M (60B token)\n\u2013 disable-bias-linear: True (turn off the bias of nn.linear)\n\u2013 seed: 42\n\u2013 save-interval: 100\n\u2022 Learning Rate Setup\n\u2013 lr-decay-style: linear\n\u2013 lr-warmup-steps: 100\n\u2013 lr: 1e \u2212 5\n\u2013 min-lr: 1e \u2212 7\n\u2022 Mixed Precision Setup\n\u2013 FP16: False\n\u2013 BF16: False\n\u2022 Parallel Configuration\n\u2013 tensor-model-parallel-size: 4\n\u2013 pipeline-model-parallel-size: 16\n\u2013 distributed-backend: NCCL\n\u2013 sequence-parallel: True\nNotices: With a model parallel size (TP) of 4 and a pipeline parallel size (PP) of 16, it can be considered that a 30B\nmodel with 48 layers is divided into 16 parts with 3 layers in each part, and each of the 4 accelerator cards in a node is\nresponsible for processing 3 continuous layer.\n46\nGeoGalactica\nA PREPRINT\nF\nAppendix: Model Card\nOur model is based on Galactica, a standard GPT2 structure with 48 transformer blocks and 56 attention heads per\nlayer, with a hidden dim of 7,168. The parameters and tokenizer of our model are initialized from the hugging face\nrelease checkpoint of Galactica-30B, and the maximum input length is 2048 and no bias settings are followed. Although\nGalactica-30B is an fp16 model, to ensure the stability of the training process, we used fp32 to train and save model\nparameters.\nModel Card - GEOGALACTICA\nModel Details\n\u2022 Developed by: Shanghai Jiao Tong University and Deep-time Digital Earth Science Center.\n\u2022 Shared by: Shanghai Jiao Tong University and GeoBRAIN.ai.\n\u2022 Model type: Further pre-train and Supervised Fine-tuning.\n\u2022 Language(s) (NLP): English.\n\u2022 License: Apache License 2.0.\n\u2022 Further pre-train from model: Galactica [12].\nModel Sources\n\u2022 Repository: https://github.com/geobrain-ai/geogalactica\n\u2022 Paper: GEOGALACTICA: A Scientific Large Language Model in Geoscience\nIntended Use\n\u2022 Research Assistance: Providing support in academic and industrial research by summarizing current scientific\nliterature, suggesting hypotheses, and identifying gaps in existing research.\n\u2022 Educational Tool: Serving as an educational resource for students and professionals in geosciences, offering\nexplanations of complex concepts and providing interactive learning experiences.\n\u2022 Collaboration and Communication: Facilitating collaboration among geoscientists by providing a platform for\nsharing data and insights, and helping in communicating complex geoscientific information to non-experts.\nEthical Considerations\n\u2022 This model inherits from Galactica [12], and in the training corpus, we have conducted sufficient data\ngovernance to ensure that the training data embodies geographical community, transparency, inclusiveness,\nrespect for privacy, and topic neutrality.\nTraining Data\n\u2022 Further pre-train: A geoscience-related text corpus containing 65 billion tokens curated from extensive data\nsources in the big science project Deep-time Digital Earth (DDE), preserving as the largest geoscience-specific\ntext corpus.\n\u2022 Supervised Fine-tuning: daven3/geosignal.\n\u2022 Tool-Augmented Learning: zthang/geotools.\nEvaluation Data\n\u2022 MMLU: Massive Multitask Language Understanding, a large-scale research initiative aimed at improving\nlanguage models\u2019 understanding and reasoning abilities across a diverse range of subjects and tasks.\n\u2022 GeoBench: The benchmark mentioned in K2 [15]. The data can be access on\n[daven3/geobench](https://huggingface.co/datasets/daven3/geobench).\n\u2022 Human Evaluation: Selected questions.\nModel Card Contact\nGEOGALACTICA is a research preview intended for non-commercial use only. Please contact us if you find any\nissues. For details, you can email via davendw@sjtu.edu.cn.\nFigure 25: Model Card for GEOGALACTICA.\n47\nGeoGalactica\nA PREPRINT\nG\nAppendix: Evaluation\nG.1\nOpen-ended Tasks\nG.1.1\nNoun Definition\nFor a better understanding of the words, we added the definitions from the Geology Dictionary.\n1 Physical weathering and chemical weathering are processes that break down rocks and minerals through mechanical\nand chemical means, respectively.\n2 Sedimentary differentiation (the sorting and separation of sediments during the formation of sedimentary rocks).\n3 A continental margin (the boundary between a continent and an ocean, characterized by various geological features).\n4 Seafloor spreading (the process where new oceanic crust is formed at mid-ocean ridges as tectonic plates move\napart).\n5 Stratum occurrence (relates to the presence and distribution of rock layers in a particular geological context).\n6 Normal fault (a type of fault where the hanging wall moves downward relative to the footwall).\n7 Plate tectonics (the theory that describes the movement and interaction of Earth\u2019s lithospheric plates).\n8 Continental margin (the boundary between a continent and an ocean, characterized by various geological features).\n9 The Yanshan Movement (refers to a tectonic event in China that resulted in significant geological changes).\n10 Continental margin (the boundary between a continent and an ocean, characterized by various geological features).\n11 A united paleocontinent (a reconstructed ancient landmass formed by merging the continents\u2019 positions from the\ndistant past).\n12 Earth resources (natural materials and substances that are valuable to humans for various purposes).\n13 Reverse fault (a type of fault where the hanging wall moves upward relative to the footwall).\n14 Ediacara fauna (refers to a group of early, soft-bodied, and mostly extinct organisms from the Ediacaran Period).\n15 Fingerfacies fossil (represents a specific type of fossil that provides information about sedimentary environments and\nconditions).\n16 Walther\u2019s Law (describes the vertical succession of sedimentary rock layers that were originally deposited in lateral\nproximity).\n17 Vertical accumulation (is the process of sediment layers accumulating on top of each other over time).\n18 The original levelness principle (suggests that sedimentary layers were initially deposited horizontally).\n19 The stratum overlap principle (relates to the idea that younger sedimentary layers can cover or overlap older ones).\n20 Lateral accumulation (refers to the process of sedimentary material accumulating horizontally, typically in a\ndepositional environment).\nG.1.2\nBeginner Level Q&A\n1 How does a cloud fill up with water?\n2 How does diffraction make a tree\u2019s shadow blurry?\n3 How does trash in the ocean disappear?\n4 How does water dowsing work?\n5 How does wind create all the ocean currents?\n6 If I jump, will the entire earth move a little bit?\n7 If I were able to dig a hole from the U.S. through the center of the earth, what part of China would I end up in?\n8 Is a quadruple rainbow possible?\n9 What causes the water going down a drain to swirl clockwise in the northern hemisphere and counter-clockwise in\nthe southern hemisphere?\n10 What keeps the continents floating on a sea of molten rock?\n48\nGeoGalactica\nA PREPRINT\nG.1.3\nIntermediate Level Q&A\n1 How does the movement of tectonic plates contribute to the formation of earthquakes and volcanic activity?\n2 What are the main factors that influence the formation and intensity of hurricanes in the Atlantic Ocean?\n3 How does the process of erosion shape the landscape and contribute to the formation of features such as canyons and\nvalleys?\n4 What are the primary mechanisms responsible for the formation and movement of glaciers?\n5 How do ocean currents influence the distribution of marine organisms and impact the productivity of marine\necosystems?\n6 What factors contribute to the formation and intensity of tornadoes in regions prone to severe weather events?\n7 How do geological processes such as weathering and sedimentation contribute to the formation and transformation\nof soil?\n8 What role does the Earth\u2019s magnetic field play in protecting the planet from harmful solar radiation?\n9 How do variations in atmospheric pressure and temperature contribute to the formation and behavior of weather\nsystems?\n10 What are the primary processes responsible for the formation and transformation of different types of rocks, such as\nigneous, sedimentary, and metamorphic rocks?\nG.1.4\nAdvanced Level Q&A\n1 How did Earth and other planets form? Were planets formed in situ?\n2 Was there ever a collision of the Earth with another planet Theia, giving birth to our satellite?\n3 What is the long-term heat balance of Earth?\n4 What made plate tectonics a dominant process only on Earth?\n5 How inherent to planetary evolution is the development of life conditions?\n6 As planets age and cool off, their internal and surface processes coevolve, chemically and mechanically, shaping the\natmospheric composition. What are the chemical composition and mechanical properties of rocks in the Earth\u2019s\nmantle at the extreme pressure and temperature they undergo?\n7 What are the dynamic processes in the Earth\u2019s interior that accommodate and fuel plate tectonics?\n8 How does the geomagnetic field link to the iron convection properties at the deep Earth?\n9 Are intraplate hotspots made by deep sources of uprising materials (mantle plumes) coming from the deepest Earth\u2019s\nmantle?\"\nG.2\nFunctional Tasks\nG.2.1\nKnowledge-based associative judgment question.\n1 What is the specific category that magnetostratigraphy belongs to within geoscience?\n2 What is the overarching category that magnetic polarity stratigraphy belongs to?\n3 What are the subfields that fall under magnetic polarity stratigraphy?\n4 What are some related concepts to geomagnetic polarity in geoscience?\n5 What are some related concepts to geomagnetic polarity in geoscience?\n6 What is the specific category that transitional polarity belongs to within geoscience?\n7 What are the subfields that fall under magnetic polarity stratigraphy?\n8 What are the subfields that fall under magnetostratigraphic polarity units?\n9 What are the subfields that fall under magnetostratigraphic polarity units?\n10 What are the subfields that fall under magnetostratigraphic polarity units?\n49\nGeoGalactica\nA PREPRINT\nG.2.2\nResearch Paper Proposition Task.\nHere are two examples, and we will release the whole data on Github:\n\u2022 Abstract: The Wenchuan Earthquake on 12 May 2008 triggered a large number of geo-hazards including landslides,\nslope collapses and debris flows. Field investigations and remote-sensing interpretation identified 11,308 geo-\nhazards in 16 seriously damaged counties in Sichuan Province, southwest China. The paper reports an analysis of\nthe distribution of these geo-hazards, particularly the earthquake-triggered landslides. Not surprisingly, the most\nsignificant geo-hazards were related to the main fault and on the hanging-wall side, although some occurred in deeply\nincised river gorges further away from the main rupture zone. Due to the high seismic intensity of the earthquake,\nmost of the large landslides moved at high speed and for considerable distances.\nTitle: Analysis of the geo-hazards triggered by the 12 May 2008 Wenchuan Earthquake, China\n\u2022 Abstract: The Modern-Era Retrospective Analysis for Research and Applications-2 (MERRA2) version of the\nGoddard Earth Observing System-5 (GEOS-5) atmospheric general circulation model (AGCM) is currently in\nuse in the NASA Global Modeling and Assimilation Office (GMAO) at a wide range of resolutions for a variety\nof applications. Details of the changes in parameterizations after the version in the original MERRA reanalysis\nare presented here. Results of a series of atmosphere-only sensitivity studies are shown to demonstrate changes\nin simulated climate associated with specific changes in physical parameterizations, and the impact of the newly\nimplemented resolution-aware behavior on simulations at different resolutions is demonstrated. The GEOS-5 AGCM\npresented here is the model used as part of the GMAO MERRA2 reanalysis, global mesoscale simulations at 10 km\nresolution through 1.5 km resolution, the real-time numerical weather prediction system, and for atmosphere-only,\ncoupled ocean-atmosphere and coupled atmosphere-chemistry simulations. The seasonal mean climate of the\nMERRA2 version of the GEOS-5 AGCM represents a substantial improvement over the simulated climate of the\nMERRA version at all resolutions and for all applications. Fundamental improvements in simulated climate are\nassociated with the increased re-evaporation of frozen precipitation and cloud condensate, resulting in a wetter\natmosphere. Improvements in simulated climate are also shown to be attributable to changes in the background\ngravity wave drag, and to upgrades in the relationship between the ocean surface stress and the ocean roughness.\nThe series of resolution-aware parameters related to the moist physics was shown to result in improvements at\nhigher resolutions and result in AGCM simulations that exhibit seamless behavior across different resolutions and\napplications..\nTitle: GMD - Development of the GEOS-5 atmospheric general circulation model: evolution from MERRA to\nMERRA2\nG.2.3\nGeoscience Research Functionality\nHere we share the five papers used to evaluate the geoscience research functionality of the LLMs in the citation form of\nMLA:\n1 Zheng, Yadong, et al. \"A challenge to the concept of slip-lines in extrusion tectonics.\" Geoscience Frontiers 2.1\n(2011): 23-34.\n2 Fedo, Christopher M., H. Wayne Nesbitt, and Grant M. Young. \"Unraveling the effects of potassium metasomatism\nin sedimentary rocks and paleosols, with implications for paleo weathering conditions and provenance.\" Geology\n23.10 (1995): 921-924.\n3 Owens, Jeremy D., et al. \"Iron isotope and trace metal records of iron cycling in the proto-North Atlantic during the\nCenomanian-Turonian oceanic anoxic event (OAE-2).\" Paleoceanography 27.3 (2012).\n4 Percival, L. M. E., et al. \"Determining the style and provenance of magmatic activity during the Early Aptian\nOceanic Anoxic Event (OAE 1a).\" Global and Planetary Change 200 (2021): 103461.\n5 Jenkyns, Hugh C. \"Transient cooling episodes during Cretaceous Oceanic Anoxic Events with special reference to\nOAE 1a (Early Aptian).\" Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering\nSciences 376.2130 (2018): 20170073.\n50\nGeoGalactica\nA PREPRINT\nH\nGeneration Examples\nH.1\nNoun Definition\nWe present an example with prompt \u201dWhat is physical weathering and chemical weathering\u00bf\u2018 as follows:\n\u2022 ChatGPT:\nPhysical weathering and chemical weathering are two processes that contribute to the breakdown and alteration\nof rocks and minerals on Earth\u2019s surface. Let\u2019s explore each of them:\n1. Physical Weathering: Physical weathering, also known as mechanical weathering, refers to the disintegration\nof rocks and minerals into smaller fragments without changing their chemical composition. It is primarily\ncaused by physical forces and environmental factors.\n2. Chemical Weathering: Chemical weathering involves the alteration or decomposition of rocks and minerals\nthrough chemical reactions. It occurs when minerals in rocks come into contact with water, atmospheric gases,\nor other substances that cause them to dissolve, decompose, or undergo chemical changes.\nIt\u2019s important to note that physical and chemical weathering often occur simultaneously and can complement\neach other, accelerating the overall weathering process. Additionally, the type and rate of weathering are\ninfluenced by factors such as climate, rock composition, topography, and the presence of vegetation or\norganisms.\n\u2022 GEOGALACTICA\nPhysical weathering and chemical weathering are two of the main processes that cause rocks and minerals to\nbreak down and change over time.\nPhysical weathering refers to the breakdown of rocks and minerals due to the action of natural forces such as\nwind, water, ice, and gravity. These forces can cause rocks to crack, crumble, and disintegrate into smaller\npieces. For example, when water flows over the surface of a rock, it can erode and wear away the rock\u2019s surface,\ncausing it to break apart.\nChemical weathering, on the other hand, refers to the breakdown of rocks and minerals due to the action of\nchemical reactions. These reactions occur when water, air, and other chemicals interact with the rocks and\nminerals, causing them to dissolve, decompose, or react with each other. For example, when carbon dioxide in\nthe air dissolves in water, it forms carbonic acid, which can dissolve calcium carbonate in limestone, causing it\nto break down into calcium and bicarbonate ions.\nIn summary, physical weathering refers to the breakdown of rocks and minerals due to the action of natural\nforces, while chemical weathering refers to the breakdown of rocks and minerals due to the action of chemical\nreactions. Both processes play an important role in shaping the Earth\u2019s landscape over time.\nH.2\nBeginner Level Q&A\nWe present an example with prompt \u201dHow does a cloud fill up with water\u00bf\u2018 as follows:\n\u2022 ChatGPT:\n\"A cloud forms and \"\"fills up\"\" with water through a process known as condensation. It involves the trans-\nformation of water vapor into liquid water droplets or ice crystals, which are the visible components of\nclouds.\nHere\u2019s a simplified explanation of the process:\n1. Evaporation: The sun\u2019s heat causes water from various sources like oceans, lakes, and rivers to evaporate,\nturning it into water vapor. This water vapor is essentially invisible.\n2. Moisture in the air: As the water evaporates, it rises into the atmosphere and mixes with the surrounding air.\nThe capacity of the air to hold water vapor depends on factors such as temperature and humidity.\n3. Cooling and saturation: As the warm, moist air rises higher into the atmosphere, it encounters cooler\ntemperatures. Cooler air cannot hold as much moisture as warm air, so the water vapor begins to condense into\ntiny water droplets or ice crystals around microscopic particles in the air, such as dust, salt, or pollution. This\nprocess is called saturation.\n51\nGeoGalactica\nA PREPRINT\n4. Cloud formation: The condensed water droplets or ice crystals cluster together to form visible clouds. Clouds\nconsist of countless tiny droplets or ice crystals suspended in the air, which give them their characteristic\nappearance.\nThe process of cloud formation is influenced by various factors such as air temperature, humidity levels, and\nthe presence of condensation nuclei (particles that water vapor can condense onto). Different types of clouds,\nsuch as cumulus, stratus, or cirrus, form under specific atmospheric conditions and have distinct appearances.\nIt\u2019s important to note that while clouds contain water, they don\u2019t \"\"fill up\"\" like a container. The amount of\nwater vapor in the atmosphere continuously changes as a result of various meteorological processes, including\nevaporation, condensation, and precipitation. Clouds form, evolve, and dissipate as these processes occur within\nthe atmosphere.\"\n\u2022 GEOGALACTICA\nA cloud fills up with water when water vapor in the air cools and condenses into tiny droplets or ice crystals.\nThis happens when the temperature of the air drops below the dew point or frost point, which is the temperature\nat which the air can hold the maximum amount of water vapor.\nWhen the temperature drops, the water molecules in the air slow down and collide with each other more\nfrequently. This causes the water vapor to condense into tiny droplets or ice crystals, forming clouds. The more\nwater vapor in the air, the more likely it is that the temperature will drop low enough for condensation to occur.\nClouds can also form when warm, moist air rises and cools. As the air rises, it expands and cools, causing the\nwater vapor in the air to condense into tiny droplets or ice crystals. This process is known as adiabatic cooling.\nIn summary, a cloud fills up with water when water vapor in the air cools and condenses into tiny droplets or\nice crystals. This can happen when the temperature of the air drops below the dew point or frost point, or when\nwarm, moist air rises and cools.\nH.3\nIntermediate Level Q&A\nWe present an example with prompt \u201dHow does the movement of tectonic plates contribute to the formation of\nearthquakes and volcanic activity\u00bf\u2018 as follows:\n\u2022 ChatGPT:\n\"The movement of tectonic plates is a key factor in the formation of earthquakes and volcanic activity. The\nEarth\u2019s lithosphere, which consists of several large and numerous smaller tectonic plates, is divided into\nrigid pieces that float and move on the underlying semi-fluid asthenosphere. There are three main types of\nplate boundaries where interactions between these plates occur: divergent boundaries, convergent boundaries,\nand transform boundaries. At divergent boundaries, tectonic plates move away from each other, creating a\ngap or rift. This movement results in the upwelling of magma from the mantle, forming new crust. As the\nmagma rises to the surface, it can lead to volcanic activity, with molten rock erupting to form volcanoes.\nHowever, the volcanic activity at divergent boundaries is generally less explosive compared to other types of\nvolcanic activity. At convergent boundaries, tectonic plates collide with each other. There are three types of\nconvergent boundaries: oceanic-oceanic, oceanic-continental, and continental-continental. When an oceanic\nplate converges with another oceanic plate or a continental plate, the denser oceanic plate is usually subducted\n(forced beneath) into the mantle. The subduction process can generate intense pressure and friction, resulting\nin the release of seismic energy and causing earthquakes. Additionally, as the subducted plate sinks into the\nmantle, it can melt, generating magma that rises to the surface and forms volcanoes. These volcanoes can be\nexplosive and often create volcanic arcs, such as the Pacific Ring of Fire. At transform boundaries, tectonic\nplates slide past each other horizontally. The friction between the plates can cause them to become locked,\npreventing smooth movement. As the plates continue to push against each other, stress builds up along the\nlocked boundary. Eventually, the stress overcomes the friction, leading to a sudden release of energy, resulting\nin earthquakes. Transform boundaries are primarily characterized by seismic activity rather than volcanic\nactivity. In summary, the movement of tectonic plates at divergent boundaries leads to volcanic activity, while\nat convergent boundaries, it results in both earthquakes and volcanic activity. Transform boundaries primarily\ncontribute to seismic activity. These interactions between tectonic plates play a fundamental role in shaping\nEarth\u2019s surface and are responsible for the occurrence of earthquakes and the formation of volcanic features\naround the world.\"\n\u2022 GEOGALACTICA\n52\nGeoGalactica\nA PREPRINT\n\"The movement of tectonic plates is the driving force behind the formation of earthquakes and volcanic activity\non Earth. Tectonic plates are large, rigid sections of the Earth\u2019s crust that move slowly over the surface of the\nplanet. The movement of these plates is caused by convection currents in the Earth\u2019s mantle, which is the layer\nbeneath the crust and above the core.\nWhen two tectonic plates move towards each other, they may collide, causing one plate to be forced under\nthe other. This process is known as subduction, and it is responsible for the formation of many of the world\u2019s\nmountain ranges, such as the Himalayas and the Andes. When two plates collide, the pressure can build up\nuntil it is released in a sudden, violent event known as an earthquake.\nOn the other hand, when two plates move in opposite directions, one may be forced upwards, causing the\nEarth\u2019s crust to stretch and thin. This process is known as rifting, and it is responsible for the formation of\nmany of the world\u2019s rift valleys, such as the Great Rift Valley in Africa. When the crust is stretched too far, it\ncan break apart, leading to the formation of new ocean basins.\nVolcanic activity is also closely linked to the movement of tectonic plates. When two plates move towards each\nother, the pressure can build up until it is released in a sudden, violent event known as an earthquake. This can\ncause the Earth\u2019s crust to crack.\"\nH.4\nAdvanced Level Q&A\nWe present an example with prompt \u201dHow did Earth and other planets form? Were planets formed in situ\u00bf\u2018 as follows:\n\u2022 ChatGPT:\n\"The formation of planets depends on several factors, including the gravitational pull of nearby celestial bodies,\nthe temperature and pressure of the surrounding environment, and the availability of necessary resources for\nplanet formation. Planets may form in situ (i.e., within their own gravitational field) or through the interaction\nwith other celestial bodies. Some planets may form from dust particles in space, while others may form around\nexisting celestial bodies such as comets or asteroids. The process of planet formation involves the accumulation\nof material over time until it reaches a critical mass where it can no longer be held back by gravity. This\ncritical mass determines the final size and composition of the planet. Overall, the process of planet formation is\ncomplex and influenced by various factors.\"\n\u2022 GEOGALACTICA\n\"The formation of Earth and other planets is a complex process that is still not fully understood. However, there\nare several theories and models that attempt to explain the formation of our solar system and its planets.\nOne of the most widely accepted theories is the \"\"nebular hypothesis,\"\" which suggests that the solar system\nformed from a rotating cloud of gas and dust called the solar nebula. This cloud collapsed under its own gravity,\nforming the sun at its center. As the cloud collapsed, it spun faster and faster, forming a disk of gas and dust\naround the sun. This disk is what we now call the solar system, and it is from this disk that the planets formed.\nOne of the main challenges in understanding the formation of the solar system and its planets is the wide range\nof sizes and compositions of the planets. For example, Earth is a rocky planet with a solid surface, while Jupiter\nis a gas giant with a thick atmosphere. Scientists have developed several models to explain the formation of\nthese different types of planets.\nOne of the most popular models for the formation of rocky planets like Earth is the \"\"core accretion model.\"\"\nThis model suggests that the first step in the formation of a rocky planet is the formation of a solid core. This\ncore is made up of small solid particles, such as dust and rocks, that collide and stick together. As the core\ngrows larger, it attracts more gas and dust from the solar nebula, forming a larger planet.\"\nH.5\nKnowledge-based associative judgment question\nWe present an example with prompt \u201dWhat are the subfields that fall under magnetic polarity stratigraphy\u00bf\u2018 as follows:\n\u2022 ChatGPT:\n\"There are four main subfields that fall under magnetic polarity stratigraphy: Stratigraphic magnetic polarity\n(SMP): SMP looks at the relative orientation of rocks and sediments along a given layer. It determines the\ndirection of flow of the rocks and sediments in that layer. Tectonic magnetic polarity (TMP): TMP looks at the\nrelationship between rocks and sediments in relation to the tectonic plate boundaries. It determines the relative\nmovements of the plates and the rocks and sediments on either side. Paleomagnetism (PM): PM looks at the\n53\nGeoGalactica\nA PREPRINT\nmagnetic susceptibility of rocks and sediments. It determines the type of rock or sediment and the strength\nof the magnetic signal. Applied geomagnetism (AG): AG looks at the application of magnetic principles in\ndifferent fields, such as geology, petrography, and mineralogy. It applies the principles of electromagnetic\ninduction to solve problems in various fields. Overall, magnetic polarity stratigraphy falls under three main\nsubfields: stratigraphic magnetic polarity (SMP), tectonic magnetic polarity (TMP), and applied geomagnetism\n(AG). These subfields combine to provide a comprehensive approach to analyzing the magnetic properties of\nrocks and sediments, and to interpreting their relationships with the underlying tectonic plate boundaries.\"\n\u2022 GEOGALACTICA\nMagnetic polarity stratigraphy, also known as magnetostratigraphy, is a subfield of geophysics and geology\nthat uses the polarity of the Earth\u2019s magnetic field to date and correlate sedimentary rock layers. The Earth\u2019s\nmagnetic field is generated by the movement of molten iron in the Earth\u2019s outer core, and the polarity of the\nmagnetic field can be either positive or negative. The polarity of the Earth\u2019s magnetic field has changed over\ntime, and these changes can be used to correlate sedimentary rock layers from one location to another.\nThere are several subfields that fall under magnetic polarity stratigraphy, including:\n1. Magnetostratigraphy: This subfield uses the polarity of the Earth\u2019s magnetic field to date and correlate\nsedimentary rock layers.\n2. Biostratigraphy: This subfield uses the distribution of fossils in sedimentary rock layers to date and correlate\nsedimentary rock layers.\n3. Lithostratigraphy: This subfield uses the physical characteristics of sedimentary rock layers to date and\ncorrelate sedimentary rock layers.\n4. Chemostratigraphy: This subfield uses the chemical composition of sedimentary rock layers to date and\ncorrelate sedimentary rock layers.\n5. Magneto-biostratigraphy: This subfield combines magnetostratigraphy and biostratigraphy to date and\ncorrelate sedimentary rock layers.\n6. Magneto-lithostratigraphy: This subfield combines magnetostratigraphy and lithostratigraphy to date and\ncorrelate sedimentary rock layers.\nH.6\nResearch Paper Titling Task\nWe present an example with instruction \u201dPlease help me to title the passages.\u201c ,and input \u201dThe Wenchuan Earthquake\non 12 May 2008 triggered a large number of geo-hazards including landslides, slope collapses and debris flows. Field\ninvestigations and remote-sensing interpretation identified 11,308 geo-hazards in 16 seriously damaged counties in\nSichuan Province, south west China. The paper reports an analysis of the distribution of these geo-hazards, particularly\nthe earthquake-triggered landslides. Not surprisingly, the most significant geo-hazards were related to the main fault\nand on the hanging-wall side, although some occurred in deeply incised river gorges further away from the main\nrupture zone. Due to the high seismic intensity of the earthquake, most of the large landslides moved at high speed and\nfor considerable distances.\u201c as follows:\n\u2022 Original title:\nAnalysis of the geo-hazards triggered by the 12 May 2008 Wenchuan Earthquake, China\n\u2022 ChatGPT:\nSpatial Distribution and Characteristics of Geo-Hazards Triggered by the Wenchuan Earthquake: Focus on\nLandslides\n\u2022 GEOGALACTICA\nAnalysis of the Distribution of Geo-Hazards Triggered by the 2008 Wenchuan Earthquake\nH.7\nGeoscience Research Functionality\nWe present an example with instruction \u201dCan you help me summarize the article\u00bf\u2018 over the paper \u201dA challenge to the\nconcept of slip-lines in extrusion tectonics\u201c as follows:\n\u2022 ChatGPT:\nThe article discusses wide-open V-shaped conjugate strike-slip faults in Asia and their association with extrusion\ntectonics. It highlights critical problems with the tectonic model based on the slip-line theory of plasticity.\n54\nGeoGalactica\nA PREPRINT\nAccording to the theory, the angles between conjugate slip-lines should be normal to each other, but in reality,\nthey are always greater than 90 degrees (on average around 110 degrees) in the direction of contraction.\nAnother issue is the failure of the slip-line theory to explain cases where the extrusion rate is significantly\nhigher than the indent rate, as observed in the Anatolian area in the eastern Mediterranean. The article proposes\nthat these problems can be addressed by considering the Maximum-Effective-Moment (MEM) Criterion. The\nMEM Criterion predicts that shear zones\u2019 orientations should theoretically be at an angle of 54.7 degrees (or\npractically around 55 degrees with the contractional direction), which fundamentally controls the orientations\nof the strike-slip faults involved in extrusion tectonics.\nThe concept of escape or extrusion tectonics, where India acts as an indentor and Tibet and neighboring areas act\nas a plastic body, is widely accepted to explain the tectonic patterns in various regions. However, the slip-line\ntheory on which this model is based is found to be incompatible with field observations and experiments. The\nangles between conjugate ductile shear zones, known as \"\"V-shaped conjugate faults,\"\" are consistently obtuse\nin nature, contrary to the slip-line theory\u2019s predictions.\nThese wide-open V-shaped conjugate strike-slip faults are prevalent in central Tibet, the eastern Alps, western\nTurkey, eastern Afghanistan, western Mongolia, Southeast Asia, and the Gulf of Thailand. They typically lie at\nan angle of 60 to 75 degrees from the maximum compressive stress direction, with a conjugate angle ranging\nfrom 120 to 145 degrees in the s1 direction. Due to their obtuse angles, they can be mistakenly identified as\nnormal conjugate faults with acute angles in the s1-direction. The article suggests using the term \"\"wide-open\"\"\nto distinguish this type of conjugate fault.\nIn summary, the article focuses on the wide conjugate angles observed in wide-open V-shaped conjugate\nstrike-slip faults in Asia, which are associated with extrusion tectonics. It highlights the limitations of the\nslip-line theory and proposes the use of the MEM Criterion to explain the fault orientations. The article also\ndiscusses the prevalence of these faults in various regions and suggests adding the term \"\"wide-open\"\" to\ndescribe their distinct characteristics.\n\u2022 GEOGALACTICA\nThe passage discusses the issue of wide-open V-shaped conjugate strike-slip faults in Asia, which are typically\nrelated to extrusion tectonics. However, the tectonic model based on the slip-line theory of plasticity has critical\nproblems associated with it. According to the theory of plasticity, the conjugate sets of slip-lines in plane\ndeformation should be normal to each other, but in reality, the angles between the conjugate strike-slip faults in\nthe eastern Mediterranean, Tibet-middle Asia, China, and the Indochina Peninsular regions are always wider\nthan 90 degrees in the direction of contraction. Additionally, the slip-line theory fails to explain how, in some\ncases, such as in the Anatolian area in the eastern Mediterranean, the extrusion rate is much higher than the\nindent rate.\n55\nGeoGalactica\nA PREPRINT\nI\nAppendix: Tool Learning Use cases\nFigure 26 shows the training instruction and prompt, Figure 27 and Figure 28 show the examples of using tool function\nwith GEOGALACTICA.\nPrompts for Tool Learning in GEOGALACTICA\nAnswer the following questions as best you can. In this level, you are calling the tools in natural language format,\nsince the tools are actually an intelligent agent like you, but they expert only in one area. Several things to\nremember.\n(1) Remember to follow the format of passing natural language as the Action Input.\n(2) DO NOT use your imagination, only use concrete information given by the tools.\n(3) If the observation contains images or urls which has useful information, YOU MUST INCLUDE ALL\nUSEFUL IMAGES and links in your Answer and Final Answers using format ![img](url). BUT DO NOT provide\nany imaginary links.\n(4) The information in your Final Answer should include ALL the information returned by the tools.\n(5) If a user\u2019s query is a language other than English, please translate it to English without tools, and translate it\nback to the source language in Final Answer. You have access to the following tools (Only use these tools we\nprovide you):\nGeo_search: \u00a8Perform Geoscience paper Search on AceMap Search engine.\nUse search_geoscience_paper(input: str, page: int) to get search results according to the input string and page\nindex (index begin from 1).\nget_arxiv_article_information: Run Arxiv search and get the article meta information. Your input should be a\njson (args json schema): \u00a8query\u00a8: string, The Action to trigger this API should be get_arxiv_article_information and\nthe input parameters should be a json dict string.\nsearch_top3: Search key words, return top 3 search results. Your input should be a json (args json schema):\n\u00a8key_words\u00a8: string, The Action to trigger this API should be search_top3 and the input parameters should be a json\ndict string. Pay attention to the type of parameters. load_page_index: Load page detail of the search result indexed\nas \u2019idx\u2019, and return the content of the page. Your input should be a json (args json schema): \u00efdx\u00a8: string, The\nAction to trigger this API should be load_page_index and the input parameters should be a json dict string.\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search_geoscience_paper, execute_code]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\nFigure 26: Prompts for Tool Learning in GEOGALACTICA.\nExample 1 Question: How do geologists study fossils?\nThought:I need to use the search_geoscience_paper action to find information about how geologists study fossils.\nAction: search_geoscience_paper\nAction Input: \"input\": \"how do geologists study fossils\", \"page\": 1.\nObservation: To establish the age of a rock or a fossil, researchers use some type of clock to determine the date it\nwas formed. Geologists commonly use radiometric dating methods, based on the natural radioactive decay of\ncertain elements such as potassium and carbon, as reliable clocks to date ancient events.\nThought: I now know the final answer.\nFinal Answer: Geologists use radiometric dating methods to establish the age of rocks and fossils.\nFigure 27: Respond example of the using search_geoscience_paper tool with GEOGALACTICA.\n56\nGeoGalactica\nA PREPRINT\nExample 2 Question: Introduce some arxiv paper about carbonate rock to me.\nThought: I need to use the get_arxiv_article_information API to search for papers related to carbonate rock.\nAction: get_arxiv_article_information\nAction Input: \"query\": \"carbonate rock\"\nObservation: Title: Multifractal analysis of the pore space of real and simulated sedimentary rocks.\nSummary: It is well known that sedimentary rocks having same porosity can have very different pore size\ndistribution. The pore distribution determines many characteristics of the rock among which, its transport property\nis often the most useful. Multifractal analysis is a powerful tool that is increasingly used to characterize the pore\nspace. In this study we have done multifractal analysis of pore distribution on sedimentary rocks simulated using\nthe Relaxed Bidisperse Ballistic Model (RBBDM). The RBBDM can generate a 3 \u2212 D structure of sedimentary\nrocks of variable porosity by tuning the fraction p of particles of two different sizes. We have also done\nmultifractal analysis on two samples of real sedimentary rock to compare with the simulation studies. One sample,\nan oolitic limestone is of high porosity (40%)while the other is a reefal carbonate of low porosity around 7%.\n2 \u2212 D sections of X-ray micro-tomographs of the real rocks were stacked sequentially to reconstruct the real rock\nspecimens. Both samples show a multifractal character, but we show that RBBDM gives a very realistic\nrepresentation of a typical high porosity sedimentary rock.\nTitle: Geochemistry of silicate-rich rocks can curtail spreading of carbon dioxide in subsurface aquifers\nSummary: Pools of carbon dioxide are found in natural geological accumulations and in engineered storage in\nsaline aquifers. It has been thought that once this CO2 dissolves in the formation water, making it denser,\nconvection streams will transport it efficiently to depth, but this may not be so. Here, we assess theoretically and\nexperimentally the impact of natural chemical reactions between the dissolved CO2 and the rock formation on the\nconvection streams in the subsurface. We show that, while in carbonate rocks the streaming of dissolved carbon\ndioxide persists, the chemical interactions in silicate-rich rocks may curb this transport drastically and even inhibit\nit altogether. These results challenge our view of carbon sequestration and dissolution rates in the subsurface,\nsuggesting that pooled carbon dioxide may remain in the shallower regions of the formation for hundreds to\nthousands of years. The deeper regions of the reservoir can remain virtually carbon free.\nThought: I now know the final answer.\nFinal Answer: The get_arxiv_article_information API returned two papers related to carbonate rock. The first\npaper is about multifractal analysis of the pore space of real and simulated sedimentary rocks, while the second\npaper discusses the geochemistry of silicate-rich rocks and its impact on the convection streams in the subsurface.\nFigure 28: Respond example of the using get_arxiv_article_information tool with GEOGALACTICA.\n57\nGeoGalactica\nA PREPRINT\nJ\nAppendix: GEOGALACTICA Generation\nJ.1\nExample Research Papers Written by GEOGALACTICA\n\u2022 Please write a paper on the study of Dinosaur Fossils.\nAs an AI, I do not have my own personal opinion, but I can provide information and facts about the topic. Plate\ntectonics is the scientific theory that explains the movement of large sections of Earth\u2019s crust, called plates.\nThese plates are made of rigid, solid rock that move slowly over the surface of our planet. The movement is\ncaused by the convection currents in the mantle, the layer of hot, molten rock beneath our crust. As the plates\nmove, they can collide with each other, causing mountain ranges to form, or move apart, creating new ocean\nbasins. This movement can also cause earthquakes, volcanic eruptions, and changes in climate. Overall, plate\ntectonics\n\u2022 Please write a paper on the Climate Change study.\nClimate change is one of the most pressing environmental issues of our time. It is caused by the release of\ngreenhouse gases, such as carbon dioxide, methane, and nitrous oxide, into the atmosphere. These gases trap\nheat from the sun and cause the Earth\u2019s temperature to rise, leading to changes in weather patterns, rising\nsea levels, more frequent extreme weather events and loss of biodiversity. Climate change has far-reaching\nconsequences for our planet and its inhabitants, making it essential that we understand its causes and effects.\nIn this paper, we will explore the study of climate change, including its history, causes, effects and potential\nsolutions. We will also look at the role that scientists and researchers play in understanding and addressing this\nglobal issue.\n\u2022 Please write a paper on the research of Urbanization and Social Change.\nUrbanization is one of the most significant social, economic, and cultural transformations of our time. It is\nthe process by which people move from rural areas to urban centers, resulting in the growth and expansion\nof cities and towns. This process has been ongoing for centuries, but the pace and scale of urbanization\nhas accelerated dramatically in recent decades, driven by factors such as population growth, globalization,\ntechnological advancements and economic development. As a result, the world is now more urbanized than\never before, with more than half the global population now living in urban areas. However, urbanization is not\nwithout its challenges, as it is often associated with social inequality, poverty, environmental degradation and\nother forms of social change. In this paper, we will explore the various ways in which urbanization and social\nchanges are intertwined, examining both the positive and negative impacts of this process on society. We will\nalso discuss the role of government, NGOs and the private sector in addressing these challenges and promoting\nequitable and sustainable urbanization.\nFigure 29: Generation examples of GEOGALACTICA on paper writing.\n58\nGeoGalactica\nA PREPRINT\nJ.2\nExample Opinions Written by GEOGALACTICA\n\u2022 Earth plate movement: The outer part of the Earth is divided into several large plates, which influence\nthe shape and seismic activity of the crust in a way that is constantly moving and colliding.\nAs an AI, I do not have my own personal opinion, but I can provide information and facts about the topic. Plate\ntectonics is the scientific theory that explains the movement of large sections of Earth\u2019s crust, called plates.\nThese plates are made of rigid, solid rock that move slowly over the surface of our planet. The movement is\ncaused by the convection currents in the mantle, the layer of hot, molten rock beneath our crust. As the plates\nmove, they can collide with each other, causing mountain ranges to form, or move apart, creating new ocean\nbasins. This movement can also cause earthquakes, volcanic eruptions, and changes in climate. Overall, plate\ntectonics...\n\u2022 Earth\u2019s water cycle: The cycle of water in the Earth, including evaporation, condensation, precipitation,\nsurface runoff, and groundwater.\nAs an AI, I don\u2019t have my own personal opinion, but I can provide information and facts about the topic. The\nEarth\u2019s Water Cycle is a natural process that involves the continuous movement and transformation of liquid\nwater, water vapor and ice on, above and below the surface of the planet. Water evaporates from the ocean,\nlakes and land surfaces, then rises into the atmosphere where it cools and condenses into clouds. When the\nclouds become heavy with water droplets or ice crystals, they fall back to the ground as precipitation such as\nrain, snow, hail or sleet. Some of this precipitation flows into rivers, streams and lakes, while the rest recharges\nthe groundwater or runs off into oceans. This cycle is essential for life on Earth as it helps regulate the climate,\nsupports plant growth and sustains the water supply for humans, animals and other living organisms. However\n\u2022 Earth\u2019s climate changes: The Earth\u2019s climate changes over time, either naturally or as a result of human\nactivities.\nAs an AI, I do not have my own personal opinions, but I can provide information and facts about the topic.\nClimate change refers to the long-term changes in weather patterns, temperature, and precipitation that occur\non Earth. These changes are caused by a combination of natural factors, such as variations in solar activity and\nvolcanic eruptions, as well as human activity, including the burning of fossil fuels and deforestation. According\nto scientific evidence, human-caused climate change is causing global temperatures to rise, leading to more\nfrequent and intense heatwaves, droughts, hurricanes and other extreme weather events. It is important to reduce\nour carbon emissions and transition to renewable energy sources in order to slow down and potentially reverse\nthese changes. However, it is essential that we continue to research and understand the complex interactions\nbetween the Earth and its climate, so we can make informed decisions and take action to protect\nFigure 30: Generation examples of GEOGALACTICA on proposing scientific opinions.\n59\nGeoGalactica\nA PREPRINT\nJ.3\nExample Summary of Scientific Articles Written by GEOGALACTICA\n\u2022 Earthquake is a way for the earth to release energy. Earthquake will produce ground vibration, strong\nvibration will cause reservoir dam break, mountain collapse, surface rupture, sand liquefaction, soft soil\nearthquake subsidence and ground settlement, resulting in earthquake disaster. Earthquake disaster has\nthe characteristics of strong suddenness, great destructiveness, wide disaster scope, deep social impact,\ndifficult defense, serious secondary disasters and so on, which is the first group disaster. For cities, due\nto the characteristics of dense population, numerous buildings, extensive infrastructure, concentrated\nwealth and multiple functions, once an earthquake occurs, it often produces huge disasters and extremely\nheavy losses.\nThe article discusses the causes, effects, and consequences of earthquakes, as well as the damage they can\ncause to cities. Earthquakes are caused by the release of energy in the Earth\u2019s crust and can result in a range\nof damages, including the collapse of mountains, the breaking of reservoir dams, ground surface ruptures,\nliquefaction of sand, subsidence of soft soils and settlements, all of which can lead to significant loss of life and\nproperty. Cities, with their high population densities, large numbers of buildings and infrastructure and diverse\neconomic activities, are particularly vulnerable to damage from earthquakes.\n\u2022 The exploitation and utilization of groundwater resources meet the diversified needs of human beings for\nwater resources, and achieve huge social and economic benefits. But at the same time, the development\nand utilization of groundwater resources also have a certain impact on the ecological environment,\nland salinization and desertification are serious, water pollution is serious, and a variety of negative\nenvironmental effects appear, seriously restricting the ecological sustainable development. In order to\nmeet people\u2019s water needs, groundwater has been over-exploited in the past time, which has caused\nfrequent geological environmental problems, and has begun to affect people\u2019s living environment. In\nthis paper, the impact of groundwater exploitation on environmental geology is discussed, and the\ncountermeasures are analyzed.\nThis scientific article discusses the importance and benefits of using groundwater as a source of water for\nhumans, but also acknowledges the negative impacts that can result from its overuse. The article highlights\nthe consequences of overusing groundwater, such as land degradation, pollution and ecological damage, all\nof which can negatively impact the environment and human life. It is important to balance the need for\ngroundwater with its potential negative effects, to ensure that it is used in a sustainable and responsible manner\n\u2022 Genetic engineering, as an important branch of biotechnology, has made remarkable progress. By\nprecisely editing and modifying an organism\u2019s genome, we open up entirely new possibilities for changing\nlives. This article will delve into the new chapter of genetic engineering, explore its application prospects\nin medicine, agriculture, environmental protection and other fields, and show its far-reaching impact on\nhuman society.\nThis scientific article discusses the use and potential of Genetic Engineering in the field of agriculture. It\nhighlights the ability of this technology to precisely edit and modify the genomes of organisms, allowing for\nthe development of new products and methods that have the potential to greatly impact human life. The article\nexplores the application of these techniques in fields such as medicine and agriculture and discusses their\npotential benefits and drawbacks. Overall, the article emphasizes the\nFigure 31: Generation examples of GEOGALACTICA on summarizing scientific papers.\n60\nGeoGalactica\nA PREPRINT\nK\nAppendix: Lessons and Progresses\nK.1\nPhase 1: Prepare for Training on HPC\nDebugging and Resolving Initialization Issues with Megatron Optimizer\nDuring the development and debugging\nphase, we encountered specific issues with the initialization of the Megatron Optimizer. It was observed that in the\nsecond step, the loss would increase to the level of randomly initialized models. We eliminated the possibility of errors\nin parameter conversion, as the same phenomenon persisted even with the use of Megatron\u2019s openly released models\nfor further pre-training. However, when we initialized the model from scratch, trained it for several steps, saved a\ncheckpoint, and then loaded this checkpoint to resume training, the behavior normalized. We noted that the main\ndifference between further pre-training and resuming training lies in the optimizer: in further pre-training, the optimizer\nis re-initialized, whereas in resuming training, it is imported from the checkpoint. This led us to hypothesize that there\nmight be an issue with the optimizer\u2019s initialization in the original code. Finally, we referred to a pull request 27 on\nGithub, which helped us identify and resolve this problem.\nDebugging and Resolving bugs of operators on ROCm\nOur primary training cluster is based on ROCm chips.\nDuring the development and debugging phase, we encountered another issue related to bugs in operators on ROCm.\nInitially, our tests on the ROCm cluster showed that training in FP16 usually worked well, producing relatively normal\nresults. However, the outcomes for BF16 and FP32 were inaccurate, characterized by enormous gradients and losses.\nInterestingly, we couldn\u2019t replicate this phenomenon in our local CUDA cluster, which operated using the same code,\nhyper-parameters, and checkpoints. Despite our numerous attempts, we found no solution until we came across a\nGithub report 28 that led us to identify and resolve the issue: a bug in the compilation of the LayerNorm operator\u2019s\nsource code in the ROCm environment. This discovery helped explain the observed behavior: FP16, with its smaller\nrange of numerical values compared to BF16 and FP32, was less likely to exhibit such large gradients or losses, even\nthough the accuracy in all these cases was affected by the bug.\nSelection of Training Accuracy on ROCm\nThere are three types of training accuracies: FP16, BF16, and FP32.\nFP16, though relatively unstable, often grapples with numerical overflow. FP32 offers the best accuracy and stability\nbut is the least efficient. BF16 strikes a balance between stability and efficiency, but its support is limited to a few\ndevices. Initially, we preferred using BF16 in our ROCm cluster because FP16 presented challenges, as shown in our\nlater experiments and evidenced in the chronicles of OPT training, and FP32 was significantly slower and rarely used in\nLarge Language Model (LLM) training. However, our ROCm chips were not fully compatible with BF16; its BF16\ncomputations largely depended on FP32 processing units in the hardware layer, lacking dedicated processing units,\nwhich meant no significant speedup over FP32. Considering these factors, we chose to use FP32 for further pre-training\nand supervised fine-tuning.\nParallel Parameter Selection\nIn general, accelerating approaches in parallel computing involve parameters such as\ntensor parallelism (TP), pipeline parallelism (PP), and data parallelism (DP), each of which significantly impacts\ntraining efficiency. To determine the most effective parallel parameters, we conducted several timing experiments\nvarying TP size, PP size, DP size, and mini batch size. These parameters are interconnected in a way that satisfies the\nformula TP \u00d7 PP \u00d7 DP \u00d7 mini_batchsize \u00d7 gradient_accumulation = global_batchsize.\n\u2022 We initiated our experiments with\nTP = 8, PP = 12, DP = 2, mini_batchsize = 2, global_batchsize = 1024, seq_len = 2048. Under these\ninitial settings, processing one batch took approximately 7.5 minutes, which was relatively slow.\n\u2022 Our first optimization, based on suggestions from [68], involved adjusting TP from 8 to 4. We tested\nTP = 4, PP = 24, DP = 2, but this configuration resulted in an Out-Of-Memory (OOM) error.\n\u2022 Subsequently, we experimented with TP = 4, PP = 48, DP = 1, while maintaining mini_batchsize = 2.\nHowever, this too led to an OOM error.\n\u2022 Finally, we reduced the mini_batchsize from 2 to 1. With TP = 4, PP = 48, DP = 1, and\nmini_batchsize = 1, we achieved success. One batch took approximately 264 seconds (4.4 minutes), which met\nour expectations.\nIn this section, we summarize our findings on efficient parallel parameter configuration, focusing on achieving\nmaximum speed with minimal VRAM usage. The optimal setup we\u2019ve identified involves setting Tensor Parallelism\n27https://github.com/NVIDIA/Megatron-LM/pull/240/\n28https://github.com/microsoft/Megatron-DeepSpeed/pull/96\n61\nGeoGalactica\nA PREPRINT\nsize to the number of GPUs per node and Pipeline Parallelism size to the number of layers in the model. This approach\nis efficient because, on one hand, tensor parallelism incurs significant communication overhead, which is minimized by\naligning it with the number of GPUs per node. On the other hand, a Large Language Model (LLM) can be most\neffectively divided into a number of parts equal to its layer count, resulting in the lowest VRAM usage under these\nconditions.\nK.2\nPhase 2: Training on HPC\nBased on prior experiments and accumulated experiences, we started formal model training. We encountered several\nfailures (as shown in Figure 32) but eventually found reasonable hyperparameters that achieved stable further\npre-training for 17 days.\nFigure 32: Training curve during the entire work of further pre-training.\nDuring this process, we encountered two types of hardware failures that affected training:\n1. The faulty node produces incorrect result: The hardware of some nodes is faulty, but no error is reported during the\ntraining program running, and the program is not terminated, resulting in incorrect calculation results This was the\ndirect cause of bewildering problems (such as disappearing gradients) encountered in some training that we could\nnot reproduce. We finally found this faulty node through repeated screening to avoid this problem.\n2. Random node crashes: Some nodes are offline due to overheating and other reasons, and the training is interrupted.\nThis problem can be solved by restarting the training.\nTo find out the best setup of the model, we do several try runs, for each try, we setup several experiments:\nFigure 33: 1st Try.\n1st Try\nWe used FP16 to train the models at first, with a maximum learning rate of 1e \u2212 4 and a warm-up step of\n1000 steps, each consisting of 1024 samples. The results show that the model can maintain relatively stable training\n62\nGeoGalactica\nA PREPRINT\nduring 0~500 steps. In the course of 500~1000 steps, the model grad norm tends to be unstable. After a lot of struggle\nand further investigation, we believe that FP16 was the root cause of the instability of the training, so we decided to use\nFP32 in the later tries. (Shown in Figure 33)\n2nd Try\nWe decided to use FP32 this time, and conducted three experiments to make the training stable.\nIn the first experiment, we used the same hyper-parameters as the 1st try except for using FP32. As shown in\nFigure 34, while the loss appeared to be as expected during the initial training of the model, the gradient norm was very\nlarge, on the order of 1e9, and the model ultimately failed to converge.\nFigure 34: 2nd Try, Experiment #1.\nIn the second experiment, we increased the micro batch size from 1 to 2 based on the first experiment, because we\naccidentally discovered during performance testing that increasing the micro batch size can decrease the gradient norm.\nWe found that this setting restored the gradient norm to a normal level, starting at around 1.7 and quickly dropping to a\nlevel around 0.2 and maintaining that level. However, since each step took over 100 seconds and the warm-up steps\nwere quite long, the experiment took a long time to complete. So, we conducted a third experiment to test whether the\nunstable issue persists even when the learning rate reaches its maximum.\nIn the third experiment, we changed the warm-up step from 1000 to 183 steps, which is the setting used in Galactica\u2019s\nexperiment. As shown in Figure 35, The results showed that the model\u2019s gradient norm and loss fluctuated when the\nlearning rate was increased to 1e \u2212 4 during warm-up. However, it appeared to recover on its own for now, although it\nis unclear whether it will further impact the model. Through these experiments, we believe that the setting of the\nlearning rate scheduler is the key point to making the training stable: how large is the learning rate and how fast to\nreach to the maximum of learning rate need to be carefully decided.\nFigure 35: 2nd Try, Experiment #3.\n63\nGeoGalactica\nA PREPRINT\n3rd Try\nWe found that in the previous try run, as mentioned in the beginning, there were faulty nodes in the cluster\nand the incorrect calculation results were output, so we mainly re-ran the experiments in the 2nd try run for this try run\nand conducted two experiments to finally find the best training setup and finish the training.\nIn the first experiment, we first set the parameter global-batch-size as 4096, which leads to 7,324 steps in total.\nBesides, the maximum learning rate is 1e \u2212 4 with linear warmup steps 1000. The curves of the first 800 steps are\nshown as Figure 36.\n(a)\n(b)\nFigure 36: 3rd Try, Experiment #1, curves of 0~800 steps.\nIf we take a look at the curve of grad-norm in Figure 33 between 100 steps and 500 steps, we can find that the\ngrad-norm has a trend of growing up. For this reason, we scale up the curve in 600~800 steps as shown in Figure 37,\nand we can find that it has a minimum grad-norm when learning rate nears 1e \u2212 5.\nFigure 37: 3rd Try, Experiment #1, grad-norm curve of 600~800 steps.\nUnfortunately, the training crashed due to a spike in next hundreds of steps. However, based on the content of\nFigure 37, we believe that 1e \u2212 5 may be appropriate as the maximum learning rate instead of 1e \u2212 4.\nIn the second experiment, we continue from the previous experiment, and we began training from the 100-step\ncheckpoint with a fixed learning rate of 1e \u2212 5 and a global batch size of 4096. The resulting Figure 38 from steps 100\nto 500 show that the gradient norm has been fairly stable and there is no overall upward trend.\n64\nGeoGalactica\nA PREPRINT\n(a)\n(b)\nFigure 38: 3rd Try, Experiment #2, curves of 100~500 steps.\nAfter nearly three weeks of stable pre-training, there have been no abnormal occurrences. Please see the attached image\nfor details. (Shown in Figure 39)\n(a)\n(b)\nFigure 39: 3rd Try, Experiment #2, curves of first 75% training steps (7324 total).\nK.3\nSummary\nWe have attempted to implement Megatron-LM [54] 29, Megatron-Deepspeed 30 and Huggingface-Deepspeed 31\napproaches on the ROCm cluster with 2048 Hygon DCUs. We present a comparative analysis of the three techniques\nbased on their distinctive features as follows:\nMegatron-LM (Original approach)\n\u2022 High performance: Supports 4D parallelism (Tensor Parallelism + Pipeline Parallelism + Data Parallelism +\nSequence Parallelism), with a 30B model taking approximately 47 seconds per step on FP16.\n\u2022 Supports FP32: 30B model takes approximately 90 seconds per step.\n\u2022 Moderate difficulty level for parameter conversion: Scripts for bidirectional parameter conversion are available for\nreference and modification.\n29https://github.com/NVIDIA/Megatron-LM\n30https://github.com/microsoft/Megatron-DeepSpeed\n31https://huggingface.co/docs/transformers/main_classes/deepspeed\n65\nGeoGalactica\nA PREPRINT\n\u2022 More suitable for GPU clusters with larger VRAM.\n\u2022 Only compatible with specific architectures of models (such as GPT and OPT).\n\u2022 High VRAM usage: Optimizer requires significant VRAM.\n\u2022 Poor user-friendliness and numerous bugs: The official maintenance could be more satisfactory, and some bugs must\nbe fixed manually to obtain accurate results.\nMegatron-Deepspeed\n\u2022 High performance: 3D parallelism + ZeRO optimizer (Tensor Parallelism + Pipeline Parallelism + Data Parallelism\n+ ZeRO optimizer) achieves similar performance as Megatron-LM, with a 30B model taking approximately 47\nseconds per step on FP16.\n\u2022 Good scalability and low VRAM usage: Suitable for models of any size and number of GPUs and can support\nmodels as large as 120B.\n\u2022 Well-maintained repository with a relatively mature ecosystem and various products such as GLM, BLOOM, and\nmore.\n\u2022 Only compatible with specific architectures of models (such as GPT and OPT).\n\u2022 Difficult parameter conversion: No bidirectional parameter conversion script is available, and the conversion can\nonly be done in one direction towards the Huggingface model.\n\u2022 Does not support FP32: FP32 tends to strangely overflow, while FP16 is stable in our attempts to train the model,\nwhich might be an issue with the framework.\nHuggingface-Deepspeed\n\u2022 Supports three stages of the ZeRO optimizer with minimal VRAM usage: a 30B model on a 1024-card cluster\nrequires only 2G of VRAM on each card.\n\u2022 Suitable for any model (such as Llama, Alpaca, etc.).\n\u2022 No parameter conversion is necessary.\n\u2022 Potentially more suitable for clusters with fewer GPUs.\n\u2022 Poor data loading performance: The original dataloader of Huggingface is less efficient for large-scale datasets than\nMegatron\u2019s.\n\u2022 Lower performance and limited parallelism: performance is only half as fast as Megatron-LM or\nMegatron-Deepspeed (30B model takes approximately 120 seconds per step). In addition, each card must\nindependently complete the calculation of a mini-batch, and GBS cannot be smaller than the number of cards.\n66\nGeoGalactica\nA PREPRINT\nL\nMembership and Contributions\nThe GeoGalactica project was conceived in October 2022, with data collection and construction completed in March\n2023. The pre-training phase was accomplished on May 30, 2023, followed by the supervised fine-tuning component\non June 14. The model evaluation and application phases are finalized on June 17. Throughout this entire process, we\nencountered various technological and engineering challenges.\nThe magnitude of the data engineering and model training tasks would not have been possible without the collaborative\nefforts of multiple teams, specifically the data team, K2 team, architecture team, and model team from the Acemap32\nand LUMIA33 group in Shanghai Jiao Tong University and the team from the Institute of Geographical Science and\nNatural Resources Research, Chinese Academy of Sciences. The detailed contributions are as follows.\nL.1\nData preparation\n\u2022 Developing Data Cleaning Standard: Zhouhan Lin.\n\u2022 Data Source Selection and Mixing: Zhouhan Lin, Junxian He.\n\u2022 Pretrain Data Preparation: Cheng Deng, Ziwei He, Boyi Zeng, Tao Shi.\n\u2022 Supervised Fine-Tuning Data (GeoSignal) Preparation: Cheng Deng, Yutong Xu, Tianhang Zhang, Zhongmou\nHe, Yuanyuan Shi.\n\u2022 PDF Parsing: Cheng Deng.\n\u2022 Academic Data Cleaning: Cheng Deng, Zhongmou He.\n\u2022 Instruction Data for Tool Learning: Tianhang Zhang, Cheng Deng, Yutong Xu.\n\u2022 GeoBench: Qiyuan Chen, Tianyu Huang, Cheng Deng.\n\u2022 Files Transportation & HPC File Management: Cheng Deng, Yi Xu, Tianhang Zhang.\nL.2\nModel Training\n\u2022 Base Model Selection: Junxian He.\n\u2022 Further Pre-training: Zhouhan Lin, Le Zhou, Yi Xu, Cheng Deng.\n\u2022 Supervised Fine-tuning: Junxian He, Zhouhan Lin, Cheng Deng, Tianhang Zhang, Boyi Zeng.\n\u2022 Tool Learning: Tianhang Zhang.\n\u2022 Trial and error of training: Zhouhan Lin, Le Zhou, Yi Xu, Cheng Deng.\n\u2022 Model Performance Validation: Zhouhan Lin, Cheng Deng, Le Zhou.\nL.3\nModel Evaluation and Application\n\u2022 Evaluation Framework & proposal: Cheng Deng, Zhouhan Lin.\n\u2022 GeoBench Evaluation: Cheng Deng, Zhongmou He.\n\u2022 MMLU Evaluation: Tianhang Zhang, Cheng Deng.\n\u2022 Inference Acceleration: Cheng Deng, Le Zhou.\n\u2022 Demo and API: Cheng Deng, Beiya Dai, Tianhang Zhang.\nL.4\nManuscript Writing\n\u2022 Cheng Deng, Zhouhan Lin, and Yi Xu wrote the main paper, and Yutong Xu, Zhongmou He, Yuanyuan Shi,\nYuncong Song, Tianhang Zhang, and Le Zhou wrote the Appendix.\nL.5\nProject Management\n\u2022 Student Leaders: Cheng Deng.\n\u2022 Technical Advisors: Zhouhan Lin, Junxian He, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou.\n32https://www.acemap.info/\n33https://github.com/LUMIA-Group\n67\nGeoGalactica\nA PREPRINT\nL.6\nEvaluation Team\nWe invite several professional evaluators, from several geoscience-related institutes and schools.\n\u2022 Chengdu University of Technology: Lei Zhang, Han Wang, Yangfan Liu, Tianyu Huang.\n\u2022 Nanjing Institute of Geology and Palaeontology, CAS: Yiwei Xu.\n\u2022 Institute of Geographical Science and Natural Resources Research, CAS: Shu Wang, Yunqiang Zhu, Chenghu\nZhou.\n\u2022 Shanghai Jiao Tong University: Kun Wei.\n\u2022 University of Waterloo: Shengde Yu.\nL.7\nIllustration in Arts\n\u2022 We invite a research assistant and a student from School of Design of Shanghai Jiao Tong University to help us\nfinish the illustrations in arts (e.g. Figure 1). They are Qiyuan Chen and Yuanyuan Wu.\nL.8\nHPC Sponsor\n\u2022 GPU Sponsor: The Advanced Computing East China Sub-center provided this project\u2019s computation resource.\n68\n"
  },
  {
    "title": "Boosting Large Language Model for Speech Synthesis: An Empirical Study",
    "link": "https://arxiv.org/pdf/2401.00246.pdf",
    "upvote": "7",
    "text": "Boosting Large Language Model for Speech Synthesis:\nAn Empirical Study\nHongkun Hao1\u2217, Long Zhou2, Shujie Liu2, Jinyu Li2, Shujie Hu2, Rui Wang1, Furu Wei2\n1Shanghai Jiao Tong University\n2Microsoft Corporation\nAbstract\nLarge language models (LLMs) have made significant advancements in natural\nlanguage processing and are concurrently extending the language ability to other\nmodalities, such as speech and vision. Nevertheless, most of the previous work\nfocuses on prompting LLMs with perception abilities like auditory comprehension,\nand the effective approach for augmenting LLMs with speech synthesis capabil-\nities remains ambiguous. In this paper, we conduct a comprehensive empirical\nexploration of boosting LLMs with the ability to generate speech, by combining\npre-trained LLM LLaMA/OPT and text-to-speech synthesis model VALL-E. We\ncompare three integration methods between LLMs and speech synthesis models,\nincluding directly fine-tuned LLMs, superposed layers of LLMs and VALL-E, and\ncoupled LLMs and VALL-E using LLMs as a powerful text encoder. Experimen-\ntal results show that, using LoRA method to fine-tune LLMs directly to boost\nthe speech synthesis capability does not work well, and superposed LLMs and\nVALL-E can improve the quality of generated speech both in speaker similarity and\nword error rate (WER). Among these three methods, coupled methods leveraging\nLLMs as the text encoder can achieve the best performance, making it outperform\noriginal speech synthesis models with a consistently better speaker similarity and a\nsignificant (10.9%) WER reduction.\n1\nIntroduction\nThe emergence of large language models (LLMs), such as ChatGPT [OpenAI, 2023a] and LLaMA\n[Touvron et al., 2023], has revolutionized most traditional natural language processing (NLP) tasks,\nlike text summarization and dialogue system. The powerful language generation capabilities of\nLLMs have prompted exploration into their applications in other modalities, e.g., speech and vision\n[OpenAI, 2023b, Huang et al., 2023, Zhang et al., 2023b]. For example, GPT-4V [OpenAI, 2023b]\nenables users to instruct GPT-4 to analyze image inputs they provided. Video-LLaMA [Zhang et al.,\n2023b] empowers LLM with the ability to comprehend both visual and auditory content present in\nvideo. These multi-modal LLMs provide the potential to enhance the impact of text-only systems by\nintegrating new interfaces and functionalities, allowing them to handle new tasks and deliver fresh\nexperiences to users.\nRegarding the application of LLMs to speech, the majority of earlier research primarily concentrates\non aligning speech representation with the LLM input space [Wu et al., 2023, Fathullah et al., 2023,\nShu et al., 2023, Tang et al., 2023]. For instance, Speech-LLaMA [Wu et al., 2023] proposes an effec-\ntive method to accomplish speech-to-text tasks by leveraging Connectionist Temporal Classification\n(CTC) [Graves et al., 2006] model and audio encoder to map the compressed acoustic features to the\ncontinuous semantic space of the LLM. LLaSM [Shu et al., 2023] takes advantage of a well-trained\nWhisper encoder to encode the speech signals into hidden states, and utilizes a modal adaptor to align\n\u2217Work was done during internship at Microsoft Research Asia.\narXiv:2401.00246v1  [cs.CL]  30 Dec 2023\nthe above output hidden states with the input text embedding of LLMs. Compared to understanding\nspeech, enabling LLMs to generate speech is considerably more challenging, given that speech is a\ncontinuous signal significantly deviating from the output space of LLMs. To enable speech generation\nability, existing works such as SpeechGPT [Zhang et al., 2023a] and AudioPaLM [Rubenstein et al.,\n2023] employ the approach of directly fine-tuning a pre-trained LLM, which requires substantial\ncomputational resources and time. How to effectively enhance LLMs with the capabilities for speech\nsynthesis remains a relatively unexplored area.\nTo better understand this task, we are going to answer two questions: 1) Can the codec codes be\ntreated by LLMs simply as a kind of language similar to other natural languages? 2) What kind of\ninformation can LLMs provide to improve the quality of synthesized speech? In order to answer\nthese two questions, in this paper, we propose and compare several integration approaches to enable\nthe LLMs with speech synthesis capability. In this study, we focus on zero-shot text-to-speech\n(TTS) tasks following the state-of-the-art model VALL-E [Wang et al., 2023a], which mainly uses an\nauto-regressive (AR) Transformer decoder model to predict the discrete token of speech depending\non the corresponding textual tokens. To enhance the speech generation of LLMs, we first discretize\nthe continuous speech into multi-layer discrete codec codes via audio compression model Encodec\n[D\u00e9fossez et al., 2022], and expand the vocabulary of LLMs with the vocabulary of codec codes, e.g.,\n1024 tokens. We design three combination strategies to achieve the first-layer codec code prediction\nwith LLM, like the AR model in VALL-E, as follows:\n\u2022 Directly Fine-tuned LLMs. We directly fine-tune large language models via paired text\nand codec codes from speech recognition dataset, with full parameters or partial parameters\n(LoRA [Hu et al., 2021]), as shown in Figure 1(a).\n\u2022 Superposed LLMs and VALL-E. Figure 1(b) illustrates this strategy that we superimpose\nthe two models into one model. In this method, we use the large language model to encode\nboth textual tokens and acoustic tokens, and then we feed them into the codec language\nmodel VALL-E.\n\u2022 Coupled LLMs and VALL-E. As shown in Figure 1(c), we use an additional text-based\nlarge language model to encode the text sequence and then input them into the VALL-E AR\nmodel. The coupled method differs from the aforementioned superposed approach as it does\nnot utilize LLMs to model codec codes.\nAfter that, we can use the non-autoregressive (NAR) model of VALL-E to generate the codec codes\nof the rest quantizers, and utilize the Encodec decoder to recover the waveform of the speech. Models\nare trained on 44.5K hours Multilingual Librispeech English data and 960 hours LibriSpeech data\nand evaluated on LibriSpeech dev-clean, dev-other, test-clean, and test-other datasets. Experimental\nresults demonstrate that coupled LLMs and VALL-E can achieve the best performance among\nbaseline and our methods. Additionally, we perform thorough analyses of various facets of our\napproach, examining the impact of model size, the benefits of continuous pre-training, the effect\nof the pre-trained VALL-E, and a comparative evaluation of LoRA versus complete fine-tuning for\nVALL-E. Based on the results, we can draw conclusions as follows:\n\u2022 Codec codes can not be simply treated as another language since the results of directly fine-\ntuned LLM are not promising. The reason could be that, the sequence length of codec codes\nis much longer than the length of corresponding text, and also the information provided by\ncodec codes is much more fine-grained and more diverse than that of text.\n\u2022 While LLMs with LoRA may not excel at generating codec codes, they can serve as a\nunified encoder for processing both text and codec codes. The outputs generated by LLMs\ncan provide valuable representation for a codec language model (e.g., VALL-E) to produce\nmore accurate codec codes.\n\u2022 LLM can be used as a powerful text encoder alone that can model pertinent and extensive\ncontent information, which is instrumental for VALL-E to generate speech of superior\nquality and enhanced robustness. The structure using LLM as a text encoder, coupled with a\ndedicated decoder module such as VALL-E, achieves the best performance.\n2\n2\nRelated Work\nOur work is typically based on LLMs, which have made significant breakthroughs in natural language\nprocessing, outperform previous state-of-the-art models in extensive NLP tasks, and inspire the\ninstruction-following capability to achieve the unseen tasks [Ouyang et al., 2022, OpenAI, 2023a,\nTouvron et al., 2023, Anil et al., 2023]. The advent of ChatGPT [Ouyang et al., 2022] marks a\ntransformative era in the field of artificial intelligence. By leveraging vast training datasets and\nextensive parameter configurations, in conjunction with instruction tuning and RLHF algorithm, it\nraises amazing emergent abilities and becomes the best artificial intelligence assistant with natural\nlanguage as an interface.\nGiven that the world\u2019s information not only includes text but also encompasses mediums such as\nspeech and images, it is a natural idea to expand from uni-modal text-based large language models to\nmulti-modal LLMs [Tang et al., 2023, Huang et al., 2023, Zhang et al., 2023b, Driess et al., 2023,\nMoon et al., 2023, Chu et al., 2023]. Most of this work focuses on enhancing the perceptual field of\nLLMs, enabling them to specifically understand auditory and visual capabilities. For example, LLaVA\n[Liu et al., 2023] combines a vision encoder and LLM into an end-to-end model for general-purpose\nvisual-language understanding with impressive chat capabilities. Speech-LLaMA [Wu et al., 2023]\nand SALMONN [Tang et al., 2023] try to perceive and understand all kinds of audio inputs with an\nadditional audio encoder. Different from the above work, the goal of our work is to boost LLMs to\ngenerate speech instead of understanding speech.\nOur work is also related to that large audio generative models [Borsos et al., 2022, Wang et al.,\n2023a, Zhang et al., 2023c, Rubenstein et al., 2023, Zhang et al., 2023a, Chen et al., 2023]. VALL-E\n[Chen et al., 2023] is a novel and state-of-the-art zero-shot text-to-speech model, which contains\nan autogressive (AR) Transformer model and a non-autoregrressive (NAR) Transformer model to\npredict the first-layer quantized codes and rest-layer quantized codes separately. Our work follows\nthe framework of VALL-E AR architecture to synthesize the speech with augmented LLMs. Besides,\nSpeechGPT [Zhang et al., 2023a] and AudioPaLM [Rubenstein et al., 2023] convert speech into\ndiscrete hidden units and continually pre-train LLMs with hidden unit corpus. LauraGPT [Chen et al.,\n2023] also fully fine-tunes LLMs with discrete codec codes of speech, to enable speech generation\nability. However, no work has explored the use of existing speech synthesis models (e.g., VALL-E) to\nempower the speech generation capabilities of LLMs. This paper focuses on empirically investigating\nand comparing different methods of endowing LLMs with speech synthesis capabilities.\n3\nMethodology\nIn this section, we will first introduce the core model components in the proposed framework in\nsubsection 3.1, including large language model, speech compression model, and codec language\nmodel, then present the three integration strategies for LLMs and VALL-E in subsection 3.2.\n3.1\nModel Components\nThere are three core components in our framework including a large language model (i.e., OPT\n[Zhang et al., 2022] or LLaMA [Touvron et al., 2023]), a speech compression model (i.e., Encodec\n[D\u00e9fossez et al., 2022]), and a codec language model (i.e., VALL-E [Wang et al., 2023a]). The large\nlanguage model is employed to model textual tokens, with the option to include acoustic tokens as\nwell. Meanwhile, the speech compression model is tasked with transforming continuous speech into\ndiscrete codec codes and subsequently reconstructing speech from these codes. Additionally, the\ncodec language model is used to generate codec codes conditioning on the representation of textual\ntokens.\nLarge Language Model\nWe conduct extensive experiments utilizing various pre-trained large\nlanguage models including OPT [Zhang et al., 2022] models with different sizes including 125M,\n350M, and 1.3B, and the LLaMA-7B [Touvron et al., 2023] model. These decoder-only models\nwill be adapted using either full fine-tuning or parameter-efficient fine-tuning methods such as Low-\nrank Adaptation (LoRA) [Hu et al., 2021]. The OPT-125M/350M/1.3B model is a 12/24/24-layer\nTransformer decoder with an attention dimension of 768/1024/2048, respectively. The LLaMA-7B\nmodel is a 32-layer Transformer decoder with an attention dimension of 4096.\n3\nLarge Language Model (LLaMA)\nAcoustic \nPrompt\nText \nPrompt\n3-second enrolled \nrecording\nText for synthesis\nPersonalized \nSpeech\nTokenizer\nAudio Codec \nEncoder\nAudio Codec \nDecoder\n(a) Method A: Directly Fine-tuned LLM\nLarge Language Model (LLaMA)\nAcoustic \nPrompt\nText \nPrompt\n3-second enrolled \nrecording\nText for synthesis\nPersonalized \nSpeech\nTokenizer\nAudio Codec \nEncoder\nAudio Codec \nDecoder\nCodec Language Model (VALL-E)\nLoRA\nLoRA\n(b) Method B: Superposed LLM and VALL-E\nLarge Language Model \n(LLaMA)\nAcoustic \nPrompt\nText \nPrompt\n3-second enrolled \nrecording\nText for synthesis\nPersonalized \nSpeech\nTokenizer\nAudio Codec \nEncoder\nAudio Codec \nDecoder\nCodec Language Model (VALL-E)\nLoRA\nLoRA\nTextual tokens\nFrozen\nCodec codes\nTrainable\n(c) Method C: Coupled LLM and VALL-E\nFigure 1: Overview of the proposed different integration methods. (a) Method A: Directly fine-tuned\nLLMs where LLMs are trained for predicting codec codes with an expanded vocabulary. (b) Method\nB: Superposed LLMs and VALL-E, where both LLMs and VALL-E are used to model textual tokens\nand acoustic tokens successively. (c) Method C: Coupled LLMs and VALL-E, where the better text\nrepresentation provided by LLM is regarded as the textual input of VALL-E.\nSpeech Compression Model\nTo enable the LLM with speech generation ability, we utilize an\nexternal speech compression model EnCodec [D\u00e9fossez et al., 2022] to convert continuous speech\ninto discrete codec codes. EnCodec model is a convolution-based encoder-decoder network with\nresidual vector quantization (RVQ) method. It first tokenizes speech data into L-layer acoustic tokens\nusing EnCodec encoder and RVQ module, and then recovers the speech waveform from all acoustic\ntokens using EnCodec decoder. In this paper, we adapt EnCodec with 6 kbps bandwidth and L=8\ntokens for each frame.\nCodec Language Model\nThe neural codec language model VALL-E [Wang et al., 2023a] treats text-\nto-speech synthesis as a language model task, like GPT, and employs acoustic tokens (audio codec\ncodes) as an intermediate representation of original speech. According to textual representations,\nVALL-E generates the codec code sequences (8 codes for each frame), from which final waveforms\ncan be recovered by an audio compression decoder (e.g., Encodec). VALL-E contains two key\nmodules, the auto-regressive (AR) codec language model and the non-autoregressive (NAR) codec\nlanguage model. The former is responsible for predicting the acoustic tokens of the first codec code\nfor each frame based on the semantic tokens in an auto-regressive manner, and the latter is used to\ngenerate the other 7-layer codes according to the sequence of the first-layer codes in parallel with the\n4\nlayer-level iterative generation method. In this work, we follow the VALL-E AR model, which is\nidentical to the model architecture of LLMs, to augment LLMs with speech synthesis ability.\n3.2\nIntegration Strategies\nWe propose three methods to boost large language models with speech synthesis capability. Figure 1\nillustrates the different methods, including directly fine-tuned LLMs (Method A), superposed LLMs\nand VALL-E (Method B), and coupled LLMs and VALL-E (Method C). Initially, we propose to\ndirectly fine-tune LLMs in Method A to determine if acoustic tokens can be integrated into LLMs by\ntreating them as a novel language. Furthermore, through Method B, we assess the capability of LLMs\nto encode both acoustic and textual tokens into a unified continuous embedding space, enhancing the\nperformance of VALL-E in text-to-speech tasks. Finally, in Method C, we explore the potential of\nleveraging only the text encoding proficiency of LLMs to improve TTS outcomes without regarding\nacoustic tokens as a new language.\nMethod A: Directly Fine-tuned LLMs\nIn order to verify whether acoustic tokens can be incorpo-\nrated into LLMs by simply regarding it as a new language, enabling the joint training of both acoustic\nand textual tokens, the most straightforward approach involves fine-tuning language models directly\nwith TTS training data by either full fine-tuning or parameter-efficient fine-tuning, as shown in Figure\n1(a). Through training on TTS data, we also augment large language models with speech synthesis\nability at the same time. In practice, we found that using parameter-efficient fine-tuning methods such\nas LoRA in this way is less effective and results in relatively poor performance. We speculate that\nthis is because large language models do not have the ability to generate codec codes inherently and it\nis more difficult for LLMs to generate speech than understand speech signals. Therefore, we directly\nfully fine-tune LLMs as one kind of approach that endows LLMs with speech synthesis ability.\nMethod B: Superposed LLMs and VALL-E\nInspired by the observation of Method A introduced\nabove, we aim to further explore the suitability of LLMs for encoding both acoustic tokens and\ntextual tokens into continuous embedding space so that this representation can be used by VALL-E to\nperform TTS tasks better. As shown in Figure 1(b), in this approach, we superpose the pre-trained\nLLMs and VALL-E models to promote the speech generation ability of LLMs. Both textual tokens\nand acoustic tokens are encoded by LLM, and are sent to the codec language model to predict the\nfirst-layer codec code. Besides, a linear projection layer is added between LLM and codec language\nmodel to bridge the dimension gap between them.\nMethod C: Coupled LLMs and VALL-E\nGiven the distinct roles and strengths of LLMs and\nVALL-E, it would be interesting to investigate the effect of only utilizing the text encoding ability\nof LLMs, instead of treating acoustic tokens as a new language in previous methods, to promote\nTTS performance of VALL-E. Therefore, another natural idea is to take full use of the advantages\nof LLMs and VALL-E, and cascade the pre-trained LLMs and VALL-E into an end-to-end model.\nLLMs excel at encoding and generating text, while VALL-E specializes in producing speech tokens\nbased on textual tokens. Hence, in this text-to-speech framework, we first use LLMs to encode text\nand get better text representation, then feed it to VALL-E as text input, as shown in Figure 1(c). In\nthis method, we also incorporate a linear projection layer between the LLM and the codec language\nmodel to reconcile the disparity in dimensions.\n4\nExperiments\n4.1\nExperiment Setup\nDataset:\nPre-trained models are fine-tuned on two ASR datasets, which can also be used to\ntrain TTS tasks as VALL-E (X) [Wang et al., 2023a, Zhang et al., 2023c]. Specifically, we use\nLibriSpeech (LS, 960 hours) [Panayotov et al., 2015] and the English part of Multilingual LibriSpeech\n(MLS) [Pratap et al., 2020]1. The Multilingual LibriSpeech is a 50K-hour ASR corpus including\n8 languages derived from read audiobooks of LibriVox, where English accounts for about 44.5K\nhours predominately. We evaluate our proposed methods on the LibriSpeech dev-clean, dev-other,\n1We do not use Librilight [Kahn et al., 2020] data like VALL-E, due to its lack of ground-truth transcriptions\nrequired for tokenization using large language model\u2019s tokenizer.\n5\ntest-clean, and test-other datasets. We use the samples that range in duration from 4 to 20 seconds\nfrom these datasets2. Following Wang et al. [2023a], we use the first 3 seconds of the ground-truth\nspeech as prompts for each sample synthesis. Each experiment is conducted thrice, with the average\nscore being reported.\nData Preprocessing:\nTo unify the training of speech and text modalities, we transform both into\ndiscrete tokens. In our approach, ASR data transcriptions are tokenized into subwords (semantic\ntokens) with the tokenizer from large language models. Meanwhile, speech data are quantized into\nacoustic tokens using the EnCodec, which operates at a 6 kbps bandwidth and a downsampling ratio\nof 320, producing 8 acoustic tokens per frame and 75 frames per second of audio. We concatenate\nthe semantic tokens and corresponding acoustic tokens to form a cohesive training sample.\n4.2\nTraining Details\nFor Method A, we employ both LoRA and full fine-tuning techniques to train OPT models. However,\ndue to computational resource limitations, we exclusively utilize LoRA for training the LLaMA-\n7B model. Additionally, we augment the LLMs\u2019 vocabulary with acoustic tokens, specifically\nincorporating 1024 Encodec tokens in our configuration. In Method B, we introduce LoRA parameters\nto LLM and codec language model respectively. The LLM is initialized with either a pre-trained OPT-\n350M or LLaMA-7B, while the codec language model is initialized with a pre-trained VALL-E. We\nalso expand the vocabulary of LLM with acoustic tokens like Method A. Besides, the input acoustic\nand textual embeddings from VALL-E are omitted, as the LLM now provides the representations for\nboth acoustic and textual tokens. Similarly, in Method C we also add LoRA parameters to pre-trained\nLLM and pre-trained VALL-E respectively, and discard the textual token embedding of VALL-E.\nWe fix the LoRA parameter to R = 64 for adjusting self-attention parameters. Consequently, using\nMethod A for LoRA training yields approximately 14M trainable parameters for OPT-350M and\n71M for LLaMA-7B. In contrast, Method B incorporates codec code embedding, LoRA, and linear\nprojection, resulting in around 21M trainable parameters for OPT-350M and 82M for LLaMA-7B.\nMeanwhile, Method C reduces the count of trainable parameters to 20M for OPT-350M and 78M\nfor LLaMA-7B, as it does not utilize codec code embedding for the LLMs. Our models are trained\nusing the Adam optimizer with \u03b21 = 0.9 and \u03b22 = 0.98 [Kingma and Ba, 2015]. All models are\ntrained on TTS tasks for 400K steps on 32 V100 GPUs with a batch size of 100 seconds per GPU.\nThe maximum learning rate is 5 \u00d7 10\u22124 with a warm-up step of 40K. We follow the configuration of\nVALL-E to train our non-autoregressive language model as introduced in Section 3.1.\n4.3\nEvaluation Metrics\nWe use the automatic evaluation metrics, including the word error rate (WER), speaker similarity\n(SS), and speech naturalness (SN) to evaluate the generated speech for simplicity and convenience.\nThe WER score is obtained by an open-source Conformer Transducer model3, ranging from 0 to\n100. The lower the WER, the more accurate the generated speech is. Given generated and prompt\nspeech utterances, the SS is measured by an automatic speaker verification (ASV) WavLM [Chen\net al., 2022] model4, ranging from -1 to 1. The larger the SS, the more similar the speakers of the two\nutterances are. SN score of generated speech is measured by the open-source NISQA5 [Mittag and\nM\u00f6ller, 2020]. Since we mainly use LoRA to fine-tune LLMs, the original textual processing ability\nof LLMs will not be affected when performing NLP tasks without LoRA parameters, therefore NLP\ntasks are not evaluated in this paper.\n4.4\nInference Strategies\nAfter training, we use sampling methods for our models to generate the acoustic tokens of the\nfirst layer codec codes. Specifically, we use top-p [Holtzman et al., 2020] sampling with p = 1.0\n2Note that VALL-E (X)\u2019s evaluation set contains audio samples ranging from 4 to 10 seconds in length.\nGiven that the audio durations within the MLS dataset span 10 to 20 seconds, our model demonstrates the\ncapability to perform speech synthesis tasks over extended periods.\n3https://github.com/NVIDIA/NeMo/\n4https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_\nverification\n5https://github.com/gabrielmittag/NISQA\n6\nand temperature is 1.0. We adopt three different strategies to choose sampled sequences following\nprevious work [Wang et al., 2023b].\n\u2022 Strategy I performs only one synthesis inference for one text, and then the sampled acoustic\nsequence is chosen as the final result.\n\u2022 Strategy II conducts five synthesis inferences for a single text, selecting the utterance that\nyields the highest speaker similarity score.\n\u2022 Strategy III also performs five synthesis inferences for a given text and selects the utterance\nthat exhibits the lowest word error rate.\n4.5\nMain Results\nWe synthesize the English speech of corresponding text prompted by a 3s English speech utterance on\nselected samples of dev-clean, dev-other, test-clean, and test-other datasets, where Table 1 shows the\nresults of dev-clean and others are shown in Appendix A. As summarized in Table 1, we replicate the\nVALL-E baseline using parameters identical to those of Wang et al. [2023a], while the proposed three\nmethods are validated using both LLaMA-7B and OPT-350M models. We apply the three inference\nstrategies outlined in Section 4.4, evaluating their performance using the metrics of word error rate\n(WER), sentence similarity (SS), and speaker naturalness (SN), as introduced in Section 4.3.\nAccording to the experimental results, we can draw three conclusions: (1) Directly fine-tuning LLMs\nby LoRA performs worse than the VALL-E baseline model. Although full fine-tuning can mitigate\nthe problem and achieve comparable performance with VALL-E, it needs massive computational\nresources for large models. (2) Method B, when employed with both the OPT-350M or LLaMA-\n7B models, surpasses the VALL-E baseline in terms of WER, SS, and SN, which demonstrates\nthat augmenting LLM with VALL-E can address the above challenge with LoRA methods, given\nthat LLMs are capable of encoding both acoustic and textual tokens and VALL-E shares a portion\nof the burden for speech synthesis in LLMs. (3) By fully leveraging the respective strengths of\nboth components, Method C achieves the best performance among the proposed methods, which\nsignificantly outperforms VALL-E on word error rate, speaker similarity, and speech naturalness.\nCompared to the VALL-E, the word error rate of Method C with LLaMA-7B is relatively decreased\nby 10.9%, 14.3%, and 6.9% under inference Strategy I, II, and III respectively, the speaker similarity\nis relatively improved by 0.02, 0.03, and 0.03, and the speech naturalness is improved by 0.03, 0.02,\nand 0.02 respectively.\nMethods\nLLMs\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nVALL-E\n-\n4.39\n0.52\n3.26\n4.27\n0.58\n3.28\n1.31\n0.56\n3.27\nA\nOPT-350M\n10.28\n0.49\n3.20\n9.74\n0.53\n3.21\n3.97\n0.51\n3.20\nOPT-350M FT\u2217\n4.21\n0.53\n3.28\n4.08\n0.60\n3.29\n1.28\n0.58\n3.28\nLLaMA-7B\n9.61\n0.49\n3.20\n9.19\n0.54\n3.21\n3.63\n0.51\n3.21\nB\nOPT-350M\n4.12\n0.53\n3.28\n3.94\n0.61\n3.29\n1.25\n0.57\n3.29\nLLaMA-7B\n4.05\n0.53\n3.29\n3.82\n0.61\n3.30\n1.23\n0.58\n3.29\nC\nOPT-350M\n3.99\n0.54\n3.30\n3.72\n0.61\n3.29\n1.26\n0.59\n3.30\nLLaMA-7B\n3.91\n0.54\n3.29\n3.66\n0.61\n3.30\n1.22\n0.59\n3.29\nTable 1: Main evaluation results on LibriSpeech dev-clean dataset. FT\u2217 means full fine-tuning, and\nother models adopt LoRA techniques. VALL-E is the text-to-speech baseline, Method A/B/C are\nintroduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.\n4.6\nAnalysis\nTo facilitate a clearer comprehension of our method, we conduct detailed analyses and ablation\nstudies in this section.\nEffect of Model Size\nThe capacity of a large language model is significantly influenced by its\nparameter number. Consequently, we explore the impact of varying model sizes within the OPT\n7\nframework through direct full fine-tuning (referred to as Method A in Table 1), examining models\nwith 125M, 350M, and 1.3B parameters. Additionally, we establish baselines by training these\nmodels from scratch. We conduct this experiment on the dev-clean dataset, the results of which are\ndepicted in Figure 2. The comparison between the two curves illustrates the effectiveness of using\npre-trained LLMs. The largest OPT model with 1.3B parameters achieves the best performance\noverall compared to 125M and 350M. This finding suggests that increasing the model size could be a\nviable strategy for enhancing speech synthesis capabilities.\n125M\n350M\n1.3B\n3.8\n4.0\n4.2\n4.4\n4.6\n4.8\n5.0\nWER\nStrategy I\n125M\n350M\n1.3B\n3.6\n3.8\n4.0\n4.2\n4.4\n4.6\n4.8\nStrategy II\n125M\n350M\n1.3B\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\nStrategy III\nTrain From Scratch\nFull Fine-tune\nFigure 2: WER results of using different model sizes in Method A under three inference strategies\nintroduced in Section 4.4. The overall results including speaker similarity and speech naturalness are\nsummarized in Appendix B.\nEffect of Continual Pre-training\nSince unlabeled speech data is more common than paired\nspeech-text data, we also investigate the way of taking advantage of massive unlabeled speech data\nto promote the speech synthesis performance of LLMs. Specifically, inspired by the next token\nprediction pre-training objective of decoder-only language models like GPT, we use EnCodec codes\nof the LibriLight [Kahn et al., 2020] dataset to continually pre-train large language models, so that\nthey can adapt to speech modality better. Then we use paired speech-text data to fine-tune continually\npre-trained models and compare them with those that have not been continually pre-trained. Table\n2 shows the comparison results of (1) training from scratch, (2) directly full fine-tuning, and (3)\ncontinually pre-training and then full fine-tuning, on large (MLS+LS) and small (LS) datasets. The\nexperimental results on Method A with OPT-350M as LLM show that the continual pre-training\nmethod achieves significant WER reduction than methods of full fine-tuning and training from scratch\non the small fine-tuning dataset, and obtains comparable performance on the large fine-tuning dataset.\nData\nMethod\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nMLS+LS\nTrain From Scratch\n4.33\n0.52\n3.26\n4.10\n0.59\n3.28\n1.30\n0.56\n3.27\nFull Fine-tune\n4.21\n0.53\n3.28\n4.08\n0.60\n3.29\n1.28\n0.58\n3.28\nPre-train+Fine-tune\n4.19\n0.53\n3.28\n4.03\n0.60\n3.29\n1.26\n0.58\n3.28\nLS\nTrain From Scratch\n5.71\n0.51\n3.26\n5.11\n0.58\n3.28\n1.97\n0.55\n3.28\nFull Fine-tune\n5.65\n0.50\n3.26\n5.10\n0.57\n3.27\n1.99\n0.53\n3.28\nPre-train+Fine-tune\n5.47\n0.51\n3.26\n4.99\n0.58\n3.29\n1.91\n0.55\n3.30\nTable 2: Effect of continual pre-training on dev-clean set with Method A and OPT-350M. MLS+LS\nmeans that the fine-tuning data are Multilingual LibriSpeech and LibriSpeech, and LS means Lib-\nrispeech only.\nEffect of Pre-trained VALL-E\nTo validate the benefits of employing the pre-trained codec language\nmodel VALL-E, we undertake an ablation study focusing on the impact of random initialization\nversus pre-trained initialization. Specifically, we fully fine-tune the randomly initialized VALL-E\nbut use LoRA to fine-tune the VALL-E initialized with pre-trained weights. Table 3 delineates the\nperformance disparity between models with Method B that begin with random weights and those\ninitialized with pre-trained VALL-E. The results clearly indicate that initializing with pre-trained\n8\nVALL-E results in fewer trainable parameters and significantly surpasses random initialization across\nvarious inference strategies and evaluation criteria.\nLLMs\nVALL-E\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nOPT-350M\nRandomly (FT\u2217)\n4.31\n0.52\n3.27\n4.09\n0.59\n3.28\n1.36\n0.56\n3.27\nPre-trained\n4.12\n0.53\n3.28\n3.94\n0.61\n3.29\n1.25\n0.57\n3.29\nLLaMA-7B\nRandomly (FT\u2217)\n4.27\n0.52\n3.27\n4.11\n0.59\n3.28\n1.32\n0.56\n3.28\nPre-trained\n4.05\n0.53\n3.29\n3.82\n0.61\n3.30\n1.23\n0.58\n3.29\nTable 3: Effect of pre-trained VALL-E on dev-clean set with method B, where VALL-E is either\nrandomly initialized or is leveraged as a pre-trained model. FT\u2217 means full fine-tuning, and models\nwith pre-trained VALL-E adopt LoRA techniques.\nLoRA vs. Full Fine-tuning in VALL-E\nThe previous section has demonstrated that pre-trained\nVALL-E enhanced with LoRA outperforms a randomly initialized version of VALL-E. Besides,\nthe main results also indicate that fully fine-tuning OPT-350M yields better results than applying\nLoRA techniques. Since the model size of VALL-E is relatively small compared to that of LLMs,\nwe are now keen to investigate the peak performance achievable by substituting LoRA with full\nfine-tuning in VALL-E. Table 4 presents a comparison of performance between LoRA fine-tuning\nand full fine-tuning approaches for VALL-E, revealing that full fine-tuning can indeed lead to further\nenhancements in performance.\nLLMs\nVALL-E\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nOPT-350M\nLoRA\n3.99\n0.54\n3.30\n3.72\n0.61\n3.29\n1.26\n0.59\n3.30\nFull Fine-tune\n3.97\n0.54\n3.31\n3.64\n0.61\n3.30\n1.25\n0.59\n3.31\nLLaMA-7B\nLoRA\n3.91\n0.54\n3.29\n3.66\n0.61\n3.30\n1.22\n0.59\n3.29\nFull Fine-tune\n3.90\n0.54\n3.31\n3.46\n0.61\n3.31\n1.20\n0.59\n3.31\nTable 4: Comparison of LoRA and full fine-tuning of VALL-E on dev-clean set with Method C.\n5\nConclusion\nIn this study, we explore various strategies for incorporating speech synthesis capabilities into large\nlanguage models (LLMs). Our findings show that simply fine-tuning LLMs with LoRA fails to\nmatch the performance of the baseline, indicating the challenge of enhancing LLMs with speech\nsynthesis capabilities. Further investigation demonstrates that LLMs augmented with a pre-trained\ntext-to-speech synthesis model can surpass the performance of the baseline VALL-E model. In\nparticular, by leveraging the respective strengths of LLMs and VALL-E, the coupled LLM and\nVALL-E method achieves the highest performance among the methods evaluated. Moreover, we\nconduct comprehensive analyses to better understand the proposed LLMs augmented with speech\nsynthesis ability.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nZal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,\nOlivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a language\nmodeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022.\n9\nQian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen\nWang, Siqi Zheng, et al. Lauragpt: Listen, attend, understand, and regenerate audio with gpt. arXiv\npreprint arXiv:2310.04673, 2023.\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training\nfor full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):\n1505\u20131518, 2022.\nYunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and\nJingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale\naudio-language models. arXiv preprint arXiv:2311.07919, 2023.\nAlexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378, 2023.\nYassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo,\nWenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. Prompting large language models with\nspeech recognition abilities. arXiv preprint arXiv:2307.11795, 2023.\nAlex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. Connectionist temporal\nclassification: labelling unsegmented sequence data with recurrent neural networks. In ICML,\npages 369\u2013376, 2006.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/\nforum?id=rygGQyrFvH.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023.\nJ. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazar\u00e9, J. Karadayi, V. Liptchinsky,\nR. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux.\nLibri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7669\u2013\n7673, 2020. doi: 10.1109/ICASSP40776.2020.9052942.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\nhttp://arxiv.org/abs/1412.6980.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\nGabriel Mittag and Sebastian M\u00f6ller.\nDeep Learning Based Assessment of Synthetic Speech\nNaturalness. In Proc. Interspeech 2020, pages 1748\u20131752, 2020. doi: 10.21437/Interspeech.\n2020-2382.\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain,\nChun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and\nscalable any-modality augmented language model. arXiv preprint arXiv:2309.16058, 2023.\n10\nOpenAI.\nGpt-4 technical report.\nArXiv, abs/2303.08774, 2023a.\nURL https://api.\nsemanticscholar.org/CorpusID:257532815.\nOpenAI. Gpt-4v(ision) system card. 2023b. URL https://cdn.openai.com/papers/GPTV_\nSystem_Card.pdf.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus\nbased on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 5206\u20135210, 2015. doi: 10.1109/ICASSP.2015.7178964.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A\nLarge-Scale Multilingual Dataset for Speech Research. In Proc. Interspeech 2020, pages 2757\u2013\n2761, 2020. doi: 10.21437/Interspeech.2020-2826.\nPaul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal\u00e1n Borsos,\nF\u00e9lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.\nAudiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925,\n2023.\nYu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and\nYemin Shi. Llasm: Large language and speech model. arXiv preprint arXiv:2308.15930, 2023.\nChangli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and\nChao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint\narXiv:2310.13289, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language\nmodels are zero-shot text to speech synthesizers. axXiv preprint arXiv:2301.02111, 2023a. URL\nhttps://arxiv.org/abs/2301.02111.\nTianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu\nLi, and Furu Wei. Viola: Unified codec language models for speech recognition, synthesis, and\ntranslation. arXiv preprint arXiv:2305.16107, 2023b.\nJian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu,\nBo Ren, Linquan Liu, et al. On decoder-only architecture for speech-to-text and large language\nmodel integration. arXiv preprint arXiv:2307.03917, 2023.\nDong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.\nSpeechgpt: Empowering large language models with intrinsic cross-modal conversational abilities.\narXiv preprint arXiv:2305.11000, 2023a.\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\nmodel for video understanding. arXiv preprint arXiv:2306.02858, 2023b.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\nZiqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Speak foreign languages with\nyour own voice: Cross-lingual neural codec language modeling. axXiv preprint arXiv:2303.03926,\n2023c. URL https://arxiv.org/abs/2303.03926.\n11\nA\nMain Results of dev-other, test-clean, test-other\nThroughout this section, we list the main results of dev-other, test-clean, and test-other in Table 5,\n6, and 7 respectively. All these three tables show the same trend as in Table 1, which can further\nconsolidate the conclusions summarized in Section 4.5.\nMethods\nLLMs\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nVALL-E\n-\n7.65\n0.47\n3.07\n7.44\n0.55\n3.08\n2.53\n0.53\n3.08\nA\nOPT-350M\n14.76\n0.41\n2.97\n13.93\n0.50\n3.00\n4.48\n0.47\n3.03\nOPT-350M FT\u2217\n7.39\n0.47\n3.08\n7.00\n0.56\n3.09\n2.45\n0.54\n3.08\nLLaMA-7B\n14.43\n0.41\n2.98\n13.21\n0.51\n3.00\n4.41\n0.46\n3.02\nB\nOPT-350M\n6.90\n0.49\n3.08\n6.76\n0.57\n3.09\n2.35\n0.54\n3.08\nLLaMA-7B\n6.99\n0.49\n3.08\n6.81\n0.58\n3.08\n2.31\n0.55\n3.08\nC\nOPT-350M\n6.94\n0.50\n3.08\n6.75\n0.57\n3.09\n2.33\n0.54\n3.10\nLLaMA-7B\n6.85\n0.50\n3.09\n6.60\n0.58\n3.10\n2.31\n0.54\n3.10\nTable 5: Main evaluation results on LibriSpeech dev-other dataset. FT\u2217 means full fine-tuning, and\nthe other models adapt LoRA techniques. VALL-E is the text-to-speech baseline, Method A/B/C are\nintroduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.\nMethods\nLLMs\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nVALL-E\n-\n4.52\n0.51\n3.31\n4.33\n0.58\n3.31\n1.31\n0.56\n3.31\nA\nOPT-350M\n10.66\n0.47\n3.21\n10.01\n0.54\n3.21\n4.01\n0.51\n3.21\nOPT-350M FT\u2217\n4.26\n0.51\n3.31\n3.98\n0.59\n3.31\n1.23\n0.57\n3.31\nLLaMA-7B\n10.49\n0.47\n3.20\n9.72\n0.55\n3.21\n3.95\n0.52\n3.22\nB\nOPT-350M\n3.91\n0.53\n3.30\n3.54\n0.60\n3.31\n1.17\n0.57\n3.31\nLLaMA-7B\n3.86\n0.53\n3.30\n3.44\n0.60\n3.32\n1.16\n0.57\n3.31\nC\nOPT-350M\n3.89\n0.52\n3.32\n3.54\n0.60\n3.33\n1.25\n0.58\n3.34\nLLaMA-7B\n3.71\n0.53\n3.31\n3.43\n0.60\n3.32\n1.16\n0.58\n3.32\nTable 6: Main evaluation results on LibriSpeech test-clean dataset. FT\u2217 means full fine-tuning, and\nthe other models adapt LoRA techniques. VALL-E is the text-to-speech baseline, Method A/B/C are\nintroduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.\nMethods\nLLMs\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nVALL-E\n-\n8.45\n0.46\n3.08\n8.09\n0.54\n3.09\n2.98\n0.50\n3.09\nA\nOPT-350M\n14.42\n0.41\n2.99\n13.79\n0.51\n3.01\n4.51\n0.47\n3.03\nOPT-350M FT\u2217\n8.23\n0.46\n3.09\n7.86\n0.54\n3.10\n2.86\n0.50\n3.10\nLLaMA-7B\n14.17\n0.42\n3.00\n13.59\n0.51\n3.02\n4.36\n0.47\n3.03\nB\nOPT-350M\n7.68\n0.47\n3.10\n7.53\n0.55\n3.10\n2.79\n0.50\n3.10\nLLaMA-7B\n7.67\n0.47\n3.10\n7.40\n0.55\n3.10\n2.76\n0.51\n3.11\nC\nOPT-350M\n7.78\n0.47\n3.10\n7.42\n0.56\n3.10\n2.76\n0.51\n3.11\nLLaMA-7B\n7.62\n0.48\n3.10\n7.14\n0.56\n3.11\n2.71\n0.52\n3.11\nTable 7: Main evaluation results on LibriSpeech test-other dataset. FT\u2217 means full fine-tuning, and\nthe other models adapt LoRA techniques. VALL-E is the text-to-speech baseline, Method A/B/C are\nintroduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.\n12\nB\nEffect of Model Size: Detailed Results\nTable 8 shows the detailed word error rate, speaker similarity, and speech naturalness results of using\ndifferent model sizes in Method A under three inference strategies introduced in Section 4.4.\nMethods\nModel\nStrategy I\nStrategy II\nStrategy III\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nWER\u2193\nSS\u2191\nSN\u2191\nTrain From Scratch\nOPT-125M\n4.94\n0.51\n3.26\n4.51\n0.58\n3.26\n1.35\n0.56\n3.27\nOPT-350M\n4.33\n0.52\n3.26\n4.10\n0.59\n3.28\n1.30\n0.56\n3.27\nOPT-1.3B\n4.17\n0.52\n3.27\n3.82\n0.59\n3.27\n1.25\n0.58\n3.28\nFull Fine-tune\nOPT-125M\n4.63\n0.53\n3.26\n4.17\n0.60\n3.28\n1.30\n0.58\n3.28\nOPT-350M\n4.21\n0.53\n3.28\n4.08\n0.60\n3.29\n1.28\n0.58\n3.28\nOPT-1.3B\n4.01\n0.53\n3.28\n3.77\n0.60\n3.29\n1.21\n0.59\n3.30\nTable 8: WER, SS, and SN results of using different model sizes in Method A under three inference\nstrategies.\n13\n"
  },
  {
    "title": "Unicron: Economizing Self-Healing LLM Training at Scale",
    "link": "https://arxiv.org/pdf/2401.00134.pdf",
    "upvote": "7",
    "text": "Unicron: Economizing Self-Healing LLM Training at Scale\nTao He1, Xue Li1, Zhibin Wang1,2, Kun Qian1, Jingbo Xu1, Wenyuan Yu1, Jingren Zhou1\n1Alibaba Group, 2Nanjing University\nunicron@alibaba-inc.com\nAbstract\nTraining large-scale language models is increasingly criti-\ncal in various domains, but it is hindered by frequent failures,\nleading to significant time and economic costs. Current fail-\nure recovery methods in cloud-based settings inadequately\naddress the diverse and complex scenarios that arise, focusing\nnarrowly on erasing downtime for individual tasks without\nconsidering the overall cost impact on a cluster.\nWe introduce Unicron, a workload manager designed for\nefficient self-healing in large-scale language model training.\nUnicron optimizes the training process by minimizing failure-\nrelated costs across multiple concurrent tasks within a cluster.\nIts key features include in-band error detection for real-time\nerror identification without extra overhead, a dynamic cost-\naware plan generation mechanism for optimal reconfiguration,\nand an efficient transition strategy to reduce downtime during\nstate changes. Deployed on a 128-GPU distributed cluster,\nUnicron demonstrates up to a 1.9\u00d7 improvement in training\nefficiency over state-of-the-art methods, significantly reducing\nfailure recovery costs and enhancing the reliability of large-\nscale language model training.\n1\nIntroduction\nLarge language models (LLMs) like ChatGPT [37], BERT [8],\nBLOOM [43], Llama [47], and Llama-2 [48] are widely used\nin various real-world applications [5,8,20,39], drawing signif-\nicant attention from both academia and industry for their role\nin advancing natural language processing and AI. These com-\nprehensive models, often comprising billions of parameters,\nare trained on large-scale GPU clusters [40,44]. To facilitate\ntheir training on thousands of GPUs, distributed frameworks\nlike Megatron-LM (Megatron) [34,44] and DeepSpeed [41]\nhave been developed, offering efficient parallelization and\noptimization.\nWith advanced models demanding extensive computational\ncapabilities, cloud platforms provide a practical and cost-\neffective approach to Large Language Model (LLM) train-\ning by enabling the straightforward provisioning of GPU-\nrich clusters as required. Notable cloud providers such as\nfinish successfully (56.6%)\nNCCL timeout (10.1%)\nconnection refused (2.1%)\nlink flapping (3.9%)\nconnection reset (1.0%)\nother network errors (4.0%)\nECC errors (5.0%)\ninvalid DMA mapping (2.1%)\nNVLink errors (4.5%)\nGPU driver errors (0.6%)\ntask hang (3.1%)\nillegal memory access (2.1%)\nCUDA errors (1.6%)\nothers (3.3%)\nFigure 1: Distribution of task termination statistics.\nAmazon Web Services [2], Google Cloud Platform [14], Mi-\ncrosoft Azure [28], and Alibaba Cloud [7] provide special-\nized services tailored to distributed model training. However,\ntraining failures remain a common challenge, primarily due\nto the considerable volume of deployed resources and ex-\ntended training durations, as underscored in several recent\nstudies [19,43,49,50,54]. Our analysis of one of the major\ncloud platforms, referred to as Alibaba Cloud for clarity in\nthis paper, echoes this observation. We found that the fail-\nure rates for LLM training tasks can skyrocket to 43.4% for\nthe top 5% most resource-intensive tasks, as illustrated in\nFigure 1.\nFigure 2 delineates the divergent paths of recovery in the\nevent of errors during a GPT-3 training exercise on Alibaba\nCloud, leveraging a fleet of 256 NVIDIA H800 GPUs within\nthe Megatron framework. For transient faults, which con-\nstitute 73% of all errors and are typically remediable by\nrestarting the system, the recovery trajectory may involve\na system hang lasting up to 30 minutes \u2013 stemming from the\nall-reduce communication timeout. This delay is followed\nby task termination and a succession of steps including a\n9-minute wait for task resubmission, a 14-minute span for\nenvironment and CUDA configuration, and a final 15-minute\nrecomputation phase. Collectively, this results in a downtime\nof 68 minutes. Conversely, hardware faults \u2013 which precipitate\nthe need for node drainage in 37% of cases \u2013 set off a more\n1\narXiv:2401.00134v1  [cs.DC]  30 Dec 2023\nhealthy\ninterrupted\nhealthy\nhealthy\ninterrupted\nsub-healthy\nerror occurs\ntask restart\ndrain faulty node\nrepair node\nhealthy\nrejoin the cluster\nerror occurs\nhanging\n30mins\nlaunching\n9mins\ninitial.\n14mins\nre-comp.\n15mins\nrequire manual investigation\nseveral hours to days\nSeveral hours to days\nFigure 2: Training process with manual failure recovery.\nlabor-intensive recovery entailing manual fault identification,\nnode drainage, down-scaling of Megatron\u2019s configuration, and\ncheckpoint realignment before resumption of training. This\nmanual labor can extend the \u2018interrupted\u2019 state for several\nhours to days, relegating the system to a \u2018sub-healthy\u2019 state of\ncompromised capacity. The scarcity and high cost of GPU re-\nsources exacerbate the financial stakes of these failures, as any\ninefficiency or idle time translates directly into considerable\neconomic loss by squandering invaluable training time.\nDespite these challenges, a range of methods have been\ndeveloped to mitigate the impacts of training failures. How-\never, existing methods often fail to provide comprehen-\nsive solutions, as they primarily focus on individual as-\npects of LLM training failure handling. For instance, some\nstudies [9, 31, 35, 49, 50] concentrate on the checkpoint-\ning process with the aim of reducing interruptions dura-\ntions. Others propose elastic training and scheduling strate-\ngies [3,16,19,24,27,45,51,52], while additional works [3,46]\nexplore the use of redundant computation or hot spares to pre-\nvent task interruptions. However, these solutions typically\noverlook the complexities of failures, which require an all-\nencompassing recovery strategy \u2013 from quick error detection\nto swift recovery to a healthy state and seamless transitions\nbetween task running states when nodes are draining or join-\ning. Consequently, there is a significant gap in the current\nmethodologies which necessitates a more holistic approach.\nMoreover, systems specialized for training resilience such\nas Oobleck [19], Bamboo [46], and Varuna [3] operate at a\nfraction of Megatron\u2019s efficiency, leading to a scenario where\nresources are expended but not effectively utilized for training,\nas demonstrated in Figure 3a. This inefficiency becomes even\nmore pronounced when considering the throughput losses due\nto failures. As Figure 3b reveals, a mere 2% downtime can\nlead to throughput losses that are threefold or greater than the\noptimal scenario (compared with their own respective imple-\nmentations). Such discrepancies indicate a misalignment in\nfault recovery objectives: the essence lies not in sustaining\ntraining processes through failures but in economizing the\nentire training to minimize lost throughput.\nUnicron. To address existing limitations in failure recovery\nduring LLM training on cloud platforms, we introduce Uni-\ncron, a distributed workload manager built with Megatron.\nUnicron is designed to enhance the training process by adopt-\ning a comprehensive strategy that focuses on minimizing the\ntotal cost of failures. This is achieved through a combination\nof efficient error detection, seamless transitions in system\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTheoretical\nOobleck\nBamboo\nMegatron\nVaruna\n8\n16\n32\n64GPUs\n0\n20\n40\n60\n80\nThroughput (samples/s)\n(a) Throughput of training with-\nout failures.\nSystems\n0\n5\n10\n15\n20\n25\n30\nFLOP/s loss ratio(%)\n(b) The FLOP/s reduction caused\nby failures.\nFigure 3: Throughput and FLOP/s reduction of training the\nGPT-3 7B model on a cluster of 64 GPUs for 7 days with 10\nnode fault errors occurred during the period. (a) The through-\nput is the number of samples the system can process per\nsecond. (b) The theoretical reduction is the ratio of hardware\nresources due to the unavailability during failures. For each\nsystem, the reduction is the percentage of lost FLOP/s com-\npared with the ideal FLOP/s it can achieved assuming no\nfailure happens.\nstates, and effective management of sub-optimal conditions.\nUnicron\u2019s key features include inheriting all optimization\ntechniques from Megatron, ensuring efficient training task ex-\necution. It preserves strict optimizer semantics, guaranteeing\nexact parameter updates without resorting to asynchronous\nor approximate methods during recovery from faults. The\nsystem\u2019s non-intrusive design facilitates easy adaptation to ex-\nisting workloads and seamless integration with future updates\nto Megatron. Furthermore, Unicron\u2019s self-healing capabilities\nenable it to efficiently detect and recover from a variety of fail-\nures. Its multi-tasking approach maximizes overall resource\nutilization across multiple tasks within a cluster, embodying\neconomic efficiency in large-scale LLM training. Our key\ncontributions can be outlined as follows:\n\u2022 We strategized the architectural design of Unicron (Sec-\ntion 3), integrating it with Megatron [34] for its high per-\nformance training capabilities and inserting new features to\nstreamline failure recovery.\n\u2022 We developed efficient error detection and handling ap-\nproaches (Section 4), enabling Unicron to promptly identify\nfailures during execution and initiate suitable corrective\nactions tailored to the particular failure causes.\n\u2022 We formulated a cost-aware plan generation mechanism\n(Section 5), aiding Unicron in configuring the most optimal\nplan. This mechanism is informed by a model considering\nthe multiplicity of tasks within the cluster.\n\u2022 We introduced a transition strategy (Section 6), which min-\nimizes system transition durations by exploiting partial\nresults from ongoing training iterations and leveraging the\nnearest principle for state migration.\n\u2022 We conducted extensive experiments, using a myriad of\nrepresentative training tasks on a cluster comprised of 128\nGPUs (Section 7). These experiments revealed that Uni-\ncron markedly reduces costs associated with recovery from\nfailures resulting in an enhancement of up to 1.9\u00d7 in overall\ntraining efficiency.\n2\n2\nBackground and Opportunities\nThis section provides a comprehensive overview of the archi-\ntecture underpinning distributed LLM training, delves into\nthe prevailing statistics on system failures, and evaluates the\ncurrent strategies implemented for failure recovery. Further-\nmore, it elucidates the opportunities that these challenges\npresent, laying the groundwork for transformative approaches\nin resilience and efficiency within this domain.\n2.1\nLLM Training\nTraining Frameworks. The scale of data and computa-\ntional intensity required for training Large Language Mod-\nels (LLMs) necessitates the adoption of distributed training\nframeworks. Recent benchmarks, such as the MLPerf re-\nsults [29], showcase the formidable capabilities of NVIDIA\u2019s\naccelerated computing platforms. Specifically, leveraging\n10,752 NVIDIA H100 Tensor Core GPUs coupled with\nQuantum-2 InfiniBand networking, NVIDIA has achieved\nthe impressive feat of training a GPT-3 model with 175 bil-\nlion parameters over 3.7 trillion tokens in just eight days [30].\nThe cornerstone of this achievement is Megatron [44] \u2013 a\nrobust transformer architecture by NVIDIA that integrates a\nslew of advanced optimizations, ensuring over 50% of FLOP/s\nutilization of available computational resources, a benchmark\nsubstantiated by Figure 3a. This blend of performance and\nefficiency positions Megatron as a venerated choice for LLM\ntraining [10,13,43].\nParallelism Approaches. The quest for efficiency in dis-\ntributed training has given rise to various parallelism strate-\ngies. Data Parallelism (DP) [17, 21, 25, 41] distributes the\nworkload evenly across multiple workers, each processing\na distinct subset of data. In contrast, Pipeline Parallelism\n(PP) [12, 15, 22, 32, 33, 41] slices the model into sequential\nstages, optimizing the process via micro-batches. Tensor Par-\nallelism (TP) [34,44], another variant, opts for vertical model\npartitioning. The fusion of DP, PP, and TP\u2014termed \u20183D Paral-\nlelism\u2019\u2014along with additional techniques like sequence par-\nallelism [23,26] and gradient checkpointing [6,18], enhances\nthe adaptability and scalability of LLM training, affording a\ndegree of natural elasticity in managing resources within the\nconstraints of memory limitations.\n2.2\nFailure Statistics\nIn the domain of LLM training, failures are an unwelcome\nyet common occurrence, often arising from the immense\ncomputational loads and extended training periods required.\nThe root causes are varied, encompassing complexities in\nnetwork topology and inconsistencies in hardware reliabil-\nity [42]. An examination of the failure patterns on the Alibaba\nCloud platform has revealed that the most resource-intensive\ntasks\u2014representing the top 5% \u2013 exhibit a startling 43.4%\nrate of abnormal terminations, underscoring the critical need\nfor more resilient training systems.\nContrary to what one might expect, these failures are not\ncontinuous but sporadic. The bulk of GPUs perform without\nissues for the majority of the time. In a typical set of 128\nGPUs, failure frequencies range from once to seven times\nweekly, equating to an average time between failures of over\na day. This pattern of infrequent yet impactful disruptions mir-\nrors trends reported in Meta\u2019s training of the OPT model [54].\nFailure Recovery Cost. The recovery from such failures, as\ndepicted in Figure 2, is a multi-stage process beginning with\nerror detection and the determination of a viable configuration\nfor resumption. The subsequent steps involve transitioning\nto this new configuration and then continuing training, po-\ntentially at a reduced efficiency. This process incurs distinct\ncosts at each phase\u2014whether managed manually or automat-\nically\u2014including the time lost during error detection, the\neffort required to reconfigure the system, and the diminished\nthroughput in less-than-ideal operational states. These costs,\nwhich collectively impact the overall efficiency and output of\nthe training process, can be formalized as follows:\nCrecovery = Cdetection +Ctransition +Csub-healthy\n(1)\n2.3\nRelated Work\nError Detection. Contemporary cloud services offer funda-\nmental monitoring tools that allow for basic oversight of sys-\ntem operations [42]. Advanced research efforts are pushing\nthese boundaries, advocating for more integrated and sophisti-\ncated monitoring solutions to preemptively catch errors before\nthey escalate [49,50].\nCheckpointing. Checkpointing, a technique that periodically\nsaves the training state, serves as a pivotal recovery mech-\nanism in deep learning frameworks. The focus of recent\nresearch has been to streamline checkpointing\u2014enhancing\nits efficiency to mitigate the time lost to recomputation\nand accelerate the recovery process in the event of a fail-\nure [1,9,31,35,38,49]. Despite its benefits, the complexity of\nfailures within distributed training systems often transcends\nwhat checkpointing alone can address.\nElasticity. In the face of failures, some systems employ elas-\nticity \u2013 adjusting parallelism settings to prevent training in-\nterruptions [3,16,19,24,27,45,51,52]. While this adaptabil-\nity is advantageous for maintaining operational continuity,\nit may introduce additional overhead and potentially reduce\nthroughput. Moreover, the complexity of integrating such\nelastic designs into high-performance systems like Megatron\noften poses significant challenges.\nRedundant Computation and Hot Spares. Other strategies\ninvolve employing redundant computation or allocating hot\nspares to preempt failures [3,46]. While these methods aim to\nprovide a buffer against disruptions, they come with a signif-\nicant economic and resource cost, highlighting the need for\nmore efficient solutions.\nWorkload Managers. Workload managers SLURM [53] and\nKubernetes [4] are pivotal in orchestrating general-purpose\ncomputing, offering a suite of functionalities that include\n3\n32\n40\n48\n56\n64GPUs\n0\n40\n80\nFLOP/s ratio (%)\n7B\n13B\n70B\n175B\n32\n40\n48\n56\n64GPUs\n0\n40\n80\nFLOP/s ratio (%)\n32\n40\n48\n56\n64GPUs\n4\n8\n12\nAchieved aggr. \n petaFLOP/s\nFigure 4: Achieved FLOP/s ratio and aggregate FLOP/s for\ntraining varying-sized GPT-3 models using Megatron.\ncluster management, task queuing, scheduling, and efficient\nresource allocation. These platforms typically handle tasks as\nblack boxes \u2013 opaque entities with predefined static configu-\nrations \u2013 submitting them into a queue for execution without\nintimate knowledge of their inner workings. While they are\nequipped with mechanisms like hot spares and automated re-\ntries for basic failure handling, they lack the bespoke features\nnecessary for the specialized domain of LLM training. As a\nresult, when failures occur, the intricate task of diagnosis and\nrecovery largely reverts to the LLM training tasks themselves.\n2.4\nOpportunities\nWe have identified three key opportunities not yet fully real-\nized by existing solutions, which could significantly enhance\nthe management of LLM training failures.\nO1: Cost-Effectiveness in Failure Management. In LLM\ntraining, failures are not just operational setbacks; they entail\nexcessive manual interventions and wasted training time, caus-\ning significant inconvenience as highlighted in Sections 1 and\n2.2. Indeed, around 73% of such failures can be rectified by\nsimply restarting the training processes. Although this process\ntakes about 68 minutes under default settings, sophisticated\ncheckpointing methods like GEMINI [49] offer potential re-\nductions in downtime. Despite the mean time between failures\nbeing relatively infrequent \u2013 ranging from a day to a week\nfor a 128 GPU cluster \u2013 the economic implications cannot\nbe overlooked. Resilient training systems [3,19,46], which\nshow considerably less efficiency compared to Megatron in\nnormal conditions (as shown in Figure 3a), are not economi-\ncally sustainable. Relying on such systems is akin to keeping\na significant portion of the training cluster idle, which is im-\npractical, especially in larger clusters. Additionally, the use of\nhot spares and redundancy, while helpful for continuity, must\nbe balanced against their economic impact to avoid resource\nwastage. This situation raises a critical question: how can we\nalleviate the pain points associated with failures while simul-\ntaneously maximizing economic efficiency and minimizing\nresource wastage?\nO2: Inherent Elasticity with Non-Linear Performance.\nThe concept of 3D parallelism introduces a remarkable degree\nof flexibility and elasticity into LLM training. This framework\nallows for adjustments in parallelism across three distinct\ndimensions while preserving the integrity of the training se-\nmantics. However, this elasticity is a double-edged sword. It\ndemands meticulous management to optimize resource uti-\nlization and enhance performance. For instance, Figure 4\nillustrates the variations in achieved aggregate FLOP/s and its\nratio to the theoretical peak FLOP/s of a healthy system (set-\ntings detailed in Section 7.4). A notable trend is the non-linear,\nand sometimes non-monotonic, relationship between the num-\nber of GPUs and the achieved aggregate FLOP/s. Adding a\nseemingly small number of GPUs, say 8 to a 48 GPU cluster,\ncan lead to performance dips due to the inability to directly\ntranslate the optimal configuration from a 48 GPU setup to a\n56 GPU one, mainly owing to memory constraints. Addition-\nally, tasks operating at different scales may exhibit varying\nlevels of resource utilization. This phenomenon underscores\na critical insight: simply maximizing the use of available re-\nsources doesn\u2019t guarantee peak performance. It also implies\nthat altering parallelism settings can significantly impact the\nefficiency of each GPU in the cluster. The question then be-\ncomes: how can we harness this inherent elasticity of LLM\ntraining to ensure resilient operations while simultaneously\nachieving high performance and efficient GPU utilization?\nO3: Rethinking from a Workload Manager\u2019s Perspective.\nWorkload managers, positioned uniquely at the helm of cluster\noperations, can revolutionize the approach to LLM training\nfailures. These systems, such as SLURM and Kubernetes,\nare not just for queueing and managing tasks; they provide a\nglobal perspective of the entire cluster, coupled with detailed\nnode-specific information through agents. This comprehen-\nsive oversight allows for a holistic management approach,\ntranscending the traditional method of treating training tasks\nas isolated, black-box entities in a queue. Such managers\ncan seamlessly integrate with advanced training systems like\nMegatron, enhancing the efficacy of existing tools for check-\npointing and monitoring. The real transformative potential lies\nin leveraging the inherent elasticity of LLM training tasks. By\nno longer confining these tasks to static configurations within\nqueues, a workload manager can dynamically select optimal\nconfigurations, efficiently managing resources in response to\nreal-time conditions. This strategy extends to handling the\nintricacies of failure recovery, guiding the system from detect-\ning and addressing issues to smoothly transitioning back into\nMegatron\u2019s standard training process. This leads us to a cru-\ncial consideration: can we innovate a workload manager that\nnot only supports self-healing LLM training at scale but also\nadopts a holistic approach, maximizing economic efficiency\nthroughout the process?\n3\nSystem Design\nOur Answer: Unicron is designed as a distributed workload\nmanager built with Megatron, to enhance the training process\nby achieving economic efficiency and self-healing capabilities.\nThe architecture of Unicron is depicted in Figure 5. The non-\nintrusive integration of Unicron with Megatron guarantees the\npreservation of both existing techniques and the future func-\ntionalities of Megatron, thereby ensuring efficient training task\nexecution and maintaining strict semantics preservation. Fur-\n4\nCPU\nnode1\nMegatron\nMonitoring thread\nPlan \nGenerator\nUnicron Coordinator\nMegatron-training\nNodes\nin & out\nFailure \nHandler\nStatus Monitor\n(Distributed KV)\nCheckpoint\nPersistent Store\nIn-memory checkpoints\nData \ufb02ow\nControl \ufb02ow\nStatus check/report\nUnicron Agent\n\u2026\n\u2026\n\u2026\nGPU\nTask set\nFigure 5: The system architecture of Unicron.\nthermore, Unicron introduces additional components, specifi-\ncally the Unicron agent and the Unicron coordinator, as well\nas several key techniques to enable efficient self-healing in the\nevent of various failures during LLM training. Additionally,\nUnicron considers concurrent training tasks within the cluster,\nwith the goal of improving overall resource utilization. The\ncombination of these features ensures that Unicron maintains\neconomic efficiency while effectively achieving self-healing\ncapabilities. Next, we will discuss the key components and\ntechniques employed by Unicron in detail.\n3.1\nUnicron Agent\nThe Unicron agent is a crucial component, performing several\nkey tasks associated with a single machine in the cluster.\nError Detection. The Unicron agent establishes a persistent\nconnection with the coordinator, which helps to ensure the\ncontinuous availability and responsiveness of the node. Be-\nsides, it assigns a dedicated monitoring thread on the CPU\nfor each GPU. These monitoring threads closely observe the\nstatus of the GPU, capturing important events and detecting\nany irregularities or exceptions. This fine-grained monitoring\nhelps in identifying potential failures or issues promptly.\nRecovery Actions Execution. The Unicron agent plays a\nsignificant role in executing recovery actions inside a machine,\nguided by the Unicron coordinator and following a transition\nstrategy. These actions aim to swiftly transition the system\nfrom its current state to a new state, ensuring the system is\nrapidly restored to a functional state.\nCheckpointing. Recognizing the necessity of checkpoint-\ning for failure recovery as a bottom-up solution, Unicron in-\ncorporates a checkpointing mechanism. The Unicron agent\nmanages the checkpointing workflow, handling in-memory\ncheckpoints following the methodology proposed by GEM-\nINI [49], and asynchronously transfers these checkpoints to\na remote persistent storage. This hierarchical checkpointing\napproach guarantees the ability to restore training progress\nin the event of failures. It is noteworthy that our framework\noperates independently of research aimed specifically at opti-\nmizing the checkpointing process. Advanced mechanisms can\nbe integrated into Unicron to reduce the overhead associated\nwith checkpointing, as our focus lies elsewhere.\n3.2\nUnicron Coordinator\nThe Unicron coordinator leverages the information collected\nby the agents to make informed decisions and coordinate\nactions across the entire cluster. Its responsibilities include:\nConsolidation of Process Status. The Unicron coordina-\ntor utilizes a distributed key-value store (implemented using\netcd [11]) referred to as the status monitor, to consolidate and\nstore the process statuses reported by the monitoring threads\nfrom each agent, allowing for a comprehensive view of the\nsystem\u2019s health.\nError Handling. Upon detecting abnormal status from the\nstatus monitor, the Unicron coordinator assesses the situation\nto determine the correct response. It then directs the agent to\nimplement the necessary actions, ensuring that these actions\nare well-coordinated and synchronously executed across the\nworkers involved.\nReconfiguration Plan Generation. The Unicron coordinator\nplays a crucial role in developing an optimal reconfiguration\nplan when necessary, such as in cases of node faults, node\nintegration, task finished, or task launched within the cluster.\nThis plan is guided by a comprehensive model that considers\nall ongoing tasks within the cluster, with the primary objective\nof minimizing any potential decline in training efficiency.\nTraining Task Management. The Unicron coordinator is\nresponsible for managing the training tasks within the cluster.\nIt keeps a watch on the status of each task, coordinates task\nsubmission and termination with the cloud service, utilizing a\ntask set for tracking. Each task is associated with the minimum\ncomputational requirements, as well as a weight assigned by\nthe user to model its priority. The weights can represent task\npriority or the unit price / quota the users willing to pay for\neach unit of useful FLOP/s. These weights and computational\nrequirements serve as essential inputs for the coordinator to\ndetermine the optimal reconfiguration plan for distributing\navailable resources among the tasks.\nOther External Interactions. In addition to its primary re-\nsponsibilities, the Unicron coordinator also handles other ex-\nternal interactions. These interactions encompass a range of\nactivities such as alerting maintenance personnel of failure in-\ncidents, acknowledging node recovery updates, incorporating\nnewly provisioned nodes from the cloud service, and others.\n3.3\nKey Techniques\nUnicron incorporates several key techniques to address the\nthree types of costs related to failure recovery. Firstly, the cost\nof detecting errors, denoted as Cdetection, is managed by the\nerror detection module described in Section 4.1. This module\nleverages four distinct detection methods to ensure failures\nare identified promptly and accurately, while avoiding extra\noverhead on the training process. Next, the actions taken to\nrespond to failures, including reattempting in-place, restarting\nthe training process, and reconfiguring the cluster, contribute\nto the transition cost, Ctransition. Unicron seeks to minimize\nthis cost through the adoption of a rapid transition strategy,\n5\nwhich is explained in Section 6. Lastly, the cost of sub-healthy,\nreferred to as Csub-healthy, is associated with reduced resource\nutilization and training efficiency due to sub-optimal config-\nurations. This cost can have a prolonged impact on overall\nsystem performance. To mitigate it, Unicron utilizes a formu-\nlated model designed to determine the most effective configu-\nration plan. The development and solution of this model are\ndiscussed in detail in Section 5.\n4\nError Detection and Handling\nIn this section, we present the design for error detection and\nhandling strategies in Unicron. Effective error detection meth-\nods are essential for minimizing detection costs, while tailored\nhandling strategies ensure appropriate measures for failure\nrecovery. Table 1 summarizes the four detection methods em-\nployed by Unicron, along with our subjective classification\nof error statuses based on their severity levels. The severity\nlevels represent the impact of the error on the training process,\nranging from SEV1 (most severe) to SEV3 (least severe), and\nare used to determine the appropriate handling strategy.\nTable 1: Detection methods and severity levels of errors.\nDetection method\nError status\nSeverity\nNode health monitoring\nLost connection\nSEV1\nProcess supervision\nExited abnormally\nSEV2\nException propagation\nConnection refused/reset\nSEV3\nIllegal memory access\nSEV2\nECC errors\nSEV1\nInvalid DMA mapping\nSEV1\nCUDA errors\nSEV2\nNVLink errors\nSEV1\nGPU driver errors\nSEV1\nOther network errors\nSEV3\nOther software errors\nSEV2\nOnline statistical\nmonitoring\nNCCL timeout\nSEV3\nLink flapping\nSEV3\nTask hang\nSEV2\nOther software errors\nSEV2\n4.1\nError Detection\nUnicron utilizes in-band error detection by continuously mon-\nitoring the real-time status of each training processes. This is\ndone through the monitoring threads of the agent, which track\ntraining progress, exceptions and communication timeouts.\nThe agent is launched at the beginning of the training process\nand operates concurrently with it. It enables the system to\npromptly identify any irregularities or exceptions. Compared\nto other solutions that rely on out-of-band monitoring like\ncloud monitoring services, this method provides a significant\nadvantage in terms of efficiency and accuracy. Furthermore,\nthe monitoring threads operate on the CPU, ensuring that they\nintroduce no extra load on the GPU, which carries out the\nprimary training workload.\nNode Health Monitoring. The persistent connection main-\ntained between the Unicron agent and the Unicron coordinator\nguarantees node availability. If this connection is lost, the\nnode is marked as unavailable, and a SEV1 failure is trig-\nIteration (120 iterations)\n0\n5\n10\n15\nIteration time (seconds)\n1.1\u00d7 avg.\n3\u00d7 avg.\nOutliers\nNormal\nFigure 6: Completion time per iteration.\ngered.\nProcess Supervision. The Unicron agent has a monitoring\nthread for each GPU to watch the training process. Should a\nprocess terminate unexpectedly, this thread signals a SEV2\nfailure to the coordinator for resolution.\nException Propagation. Exception handling is crucial for\nthe prompt detection of incidental errors, such as ECC errors,\nNVLink errors, CUDA errors, and others. These errors are\nimmediately identified once the GPU issues an exception,\nwhich is then captured by the monitoring thread and reported\nto the coordinator.\nOnline Statistical Monitoring. Unicron leverages online sta-\ntistical monitoring to detect certain errors like NCCL timeout,\nTCP timeout, and task hangs, where notifications are delayed.\nFor instance, a delay of up to 30 minutes may occur before a\nNCCL timeout error is raised, as shown in Figure 2. Although\nthe timeout threshold can be adjusted, it is challenging to\ndetermine the appropriate value, as it is highly dependent on\nthe system configuration and the workload.\nThe monitoring threads implement online statistical mon-\nitoring to detect these errors. Under normal training condi-\ntions, which proceed periodically, the statistics of iterations\nshould reveal a relative consistency when the system is set to\na particular configuration, as Figure 6 shows with green dots\nrepresenting iteration completion times for training a GPT-3\n175B model on 256 NVIDIA H800 GPUs. While minor fluc-\ntuations may occur due to network variations and congestion,\nthey typically stay within a reasonable margin indicated by\nthe blue line (i.e., 1.1\u00d7 the average iteration time). The red\ndots illustrate instances where a network switch has been\ndeliberately turned off, leading to a marked increase in com-\npletion time; yet, the training process manages to persist. If\nthe waiting time surpasses the threshold denoted by the grey\nline, this confirms a failure, requiring immediate recovery\nmeasures. Empirical evidence suggests that setting the failure\nthreshold at 3\u00d7 the average iteration time, achieves a practical\nbalance between efficiency and accuracy.\n4.2\nError Handling\nWhen any abnormal status is detected, the Unicron coordina-\ntor is notified and proceeds to take appropriate actions. The\nfirst step in handling a failure is to classify the collected status\nbased on its severity level, as outlined in Table 1. This classifi-\ncation (indicated by \u2460 to \u2462) helps determine the appropriate\nrecovery strategy, which corresponds to one of three specific\n6\ntraining\nhandling\nreattempt in-place\nrestart process\nrecon\ufb01gure cluster\nSEV3 failure\u2460 \nSEV2 failure\u2461\nSEV1 failure\u2462\nfailed\nfailed\nsucceed\nsucceed\ncon\ufb01guration 0\ncon\ufb01guration 1\n\u00d78\nfaulted/unused GPU\ncon\ufb01guration 2\n\u2026\ntask launched\u2465\nself-healing\nnode join\n   task launched\ntask \ufb01nished\ntask \ufb01nished\u2464\nTask 0\n\u00d732\nTask 1\n\u00d796\nTask 0\n\u00d756\nTask 1\n\u00d764\nTask 1\n\u00d7128\nGPU\nnode join\u2463\nFigure 7: The error handling workflow in Unicron.\nactions, following the guidance of Figure 7. Additionally, Fig-\nure 7 includes other triggers (indicated by \u2463 to \u2465), to ensure\nthe smooth operation of the training process.\nReattempt In-place. The initial attempt to mitigate a failure\ninvolves retrying the operation where it failed, assuming there\nare no indications of underlying software or hardware issues,\nwhich is classified as SEV3 failure \u2460. This method is effec-\ntive for addressing issues like temporary connection problems\n(e.g., link flapping or connection refused/reset). If the reat-\ntempt succeeds, the training process immediately proceeds as\nnormal. Otherwise, the issue is upgraded to a SEV2 failure.\nRestart Process. SEV2 failures \u2461, such as CUDA errors or\nillegal memory accesses, are resolved by restarting the train-\ning process on the affected node. The system configuration,\nincluding the number of GPUs, parallelism settings, and pro-\ncess ranks, remains unchanged. And the training states are\nrecovered from either other data parallel replicas or the latest\ncheckpoint, as detailed in Section 6. If the restart attempt fails,\nthe severity of the failure is upgraded to SEV1.\nReconfigure Cluster. In the event of a SEV1 failure \u2462, the\nUnicron coordinator isolates the failed node and initiates a\nreconfiguration procedure. This happens when the coordina-\ntor receives notifications of GPU node faults or if the system\nfails to recover from a SEV2 failure. When a previously failed\nand unused node is successfully recovered and becomes avail-\nable again, or when a new node is allocated, it can join \u2463 the\nongoing training process. In such cases, the Unicron coordi-\nnator initiates the cluster to enter the reconfiguration process,\nwhich allows the system to adapt and incorporate the addi-\ntional available resources. Additionally, the reconfiguration\nprocess can also be triggered when an existing task is finished\n\u2464 or a new task is launched \u2465. This is because the optimal\nconfiguration plan may differ from the current configuration,\nrequiring the reallocation of resources to achieve the global\noptimal training efficiency of the cluster.\n5\nOptimal Reconfiguration Plan Generation\nThis section introduces a method for generating an optimal re-\nconfiguration plan to efficiently distribute tasks across GPUs\nwithin a distributed training cluster. The formulation of this\nplan is critical as it directly influences the sub-healthy cost\nCsub-healthy. To capture these costs, we define the metric of\nWAF, representing the weighted achieved aggregate FLOP/s,\nand formulate an optimization problem to minimize the costs\nincurred during sub-healthy and transition states.\n5.1\nModel Formulation\nWith n workers available in the cluster for distributed train-\ning and m tasks to be trained, our goal is to fully utilize the\ncomputation capacity of the resources while meeting the re-\nquirement of each running task. A GPU card is considered as\na worker by default within the scope of this problem. Before\ndelving into our model, we first define the metric to measure\nthe training efficiency of a task.\nWAF. The WAF of a task measures the weighted achieved\naggregate FLOP per second. Formally, we define a function\nF : N \u00d7 N \u2192 R, where F(t,x) represents the WAF of task t\nwhen x workers assigned to it.\nAs implied by the name, the most important part of WAF is\nthe achieved aggregate FLOP/s, denoted as T(t,x), for a given\ntask t with x workers. Notice the T(t,x) reflects the optimal\nperformance of the task, which is obtained by tuning the com-\nplex combination of parallelism hyperparameters and other\noptimization settings. To address this, we rely on calibrating\ntasks on the given GPU cluster and leverage automatic execu-\ntion plan generation techniques [55] to estimate the optimal\nparallelism settings and associated T(t,x).\nIn addition to the achieved aggregate FLOP/s, we further\nintegrate two additional factors to model the minimum compu-\ntational requirements and task priorities. Requirement condi-\ntion (Tnecessary(t)) represents the minimum required resources\nfor a task given. A task will only be scheduled if the available\nresources can satisfy this requirement. Task weight (w(t)) is\nused to model the priority of a task. Tasks with higher prior-\nity are assigned higher weights. By default, we set w(t) = 1\nfor all tasks. This weight factor allows the user to adjust the\nrelative importance of different tasks in the problem, with the\nrecommended values for w(t) range between 0.5 and 2.0.\nAccordingly, the WAF is defined as follows:\nF(t,x) =\n(\nw(t)\u00b7T(t,x)\nif (t,x) \u22a2 Tnecessary(t),\n0\notherwise.\n(2)\nHere, F(t,x) is calculated as the product of the task\u2019s weight,\nw(t), and its achieved aggregate FLOP/s, T(t,x), when (t,x)\nsatisfies the necessary requirements, Tnecessary(t). Otherwise,\nif it falls short of this threshold, the WAF of the task is con-\nsidered to be zero.\nOptimization Objective. The formulation of the optimization\nproblem for plan generation is centered around a trade-off:\nmaximizing the WAF of the cluster post-reconfiguration while\nminimizing the impact on GPUs during the transition.\n7\nargmax\nx\u2032\n1,...,x\u2032m\nm\n\u2211\ni=1\nG(ti,x\u2032\ni),\nwhere\nG(ti,x\u2032\ni) =F(ti,x\u2032\ni)\u00b7Drunning(n\u2032)\n\u2212F(ti,xi)\u00b71(ti,xi \u2192 x\u2032\ni)\u00b7Dtransition,\nsubject to\nm\n\u2211\ni=1\nx\u2032\ni \u2264 n\u2032.\n(3)\nPrime notation (\u2019) differentiates states before and after re-\nconfiguration. The subscript i denotes the task identifier, ti\nrepresents the i-th task, and xi the number of workers initially\nassigned. The constraint ensures that the workers assigned\nacross all tasks do not exceed the total available in the clus-\nter. The goal is to maximize the cluster\u2019s cumulative reward,\nrepresented by the sum of individual task rewards G(ti,x\u2032\ni).\nThe primary term, G(ti,x\u2032\ni), reflects the reward when the\ncluster is operational post-reconfiguration, calculated as the\nWAF of task ti with x\u2032\ni workers, multiplied by the expected\nrun duration Drunning(n\u2032). This duration is contingent on the\noperational condition of GPUs and the cluster size; a larger\npool of GPUs implies a higher likelihood of failure, potentially\nshortening the run duration.\nThe penalty term, F(ti,xi) \u00b7 1(ti,xi \u2192 x\u2032\ni) \u00b7 Dtransition, cap-\ntures the WAF loss during transition, where Dtransition is the\nestimated duration of this period. The indicator function\n1(ti,xi \u2192 x\u2032\ni) activates when there is a change in the num-\nber of workers assigned to task ti or if a worker fault occurs:\n1(ti,xi \u2192 x\u2032\ni) =\n(\n1\nif xi \u0338= x\u2032\ni or a worker of ti faults,\n0\notherwise.\n(4)\nThis term discourages frequent reconfiguration, especially\nfor healthy tasks, by factoring in the WAF that could have\nbeen achieved by unaffected GPUs.\nThe significance of each term is context-dependent. In sta-\nble clusters with fewer faults, maximizing WAF takes prece-\ndence, focusing on the reward during healthy operation. Con-\nversely, in larger or less reliable clusters where faults are\nmore common, the penalty term becomes more critical, em-\nphasizing the need to limit the scope of reconfigurations and\nmaintain as many tasks running as possible.\n5.2\nSolving Algorithm\nThis problem can be solved through the dynamic program-\nming algorithm, in which we define the state as S(i, j), repre-\nsenting the maximum value of the first i tasks with j workers.\nAccordingly, the state transition equation is given by:\nS(i, j) =\nj\nmax\nk=0 {S(i\u22121, j \u2212k)+G(ti,k)}\n(5)\nThis equation implies that the maximal reward of the first\ni tasks with j workers can be transited from the maximal\nreward of the first i\u22121 tasks with j \u2212k workers plus G(ti,k),\nwhere k denotes the number of workers assigned to the i-th\ntask. We set S(0, j) = 0 for j > 0, to initialize the states. The\noptimal value of Equation 3 can be obtained from S(m,n\u2032).\nTo obtain the optimal assignment strategy specifically, we can\ntrace back the state transition process.\nComplexity. The time complexity of this algorithm is\nO(mn2), where m represents the number of tasks and n repre-\nsents the cluster size. In practice, both m and n remain moder-\nate in size, making this algorithm efficient to execute. Further-\nmore, the Unicron coordinator can pre-compute and prepare\nthe lookup table for the dynamic programming algorithm in\nadvance, taking into account potential failure scenarios of\nany task or joining node. This allows for one-step advance-\nment from the current configuration. Once reconfiguration\nis required, the Unicron coordinator can directly retrieve the\noptimal assignment strategy from the lookup table, thereby\nreducing the time complexity to O(1).\n6\nTransition Strategy\nIn this section, we delve into the implementation of transi-\ntioning a training task to a new configuration. We first review\nthe training process of an iteration in Megatron to identify the\nmaximal possible partial results that can be reused. Next, we\npresent the adaptive resumption process according to failure\nscenarios. This resumption can leverage the partial results\nto finish the current iteration. Finally, we discuss the tran-\nsition process for a task that is instructed to reconfigure. In\nthis process, we aim to minimize the state migration cost by\nleveraging the nearest available states.\n6.1\nIterations in Megatron\nWe first review the training process of an iteration in Mega-\ntron, which is depicted in Figure 8. In an iteration, a global-\nbatch of samples will be forward passed through the model,\nand the gradients will be accumulated until the completion of\nthe backward pass. For distributed training, a global-batch is\nfurther partitioned into multiple micro-batches, e.g., 8 micro-\nbatches in the figure.\nCorresponding to the parallelism of the model, a rank in\ndistributed data parallelism (DP) is responsible for a subset of\nthe micro-batches. With the DP degree of DP, and B micro-\nbatches in a global-batch, each DP rank is responsible for k =\nB/DP micro-batches. Therefore, we utilize two dimension of\nindexes to denote the j-th micro-batch in the i-th DP rank, i.e.,\ngradi,j. The aggregated gradient for the global-batch, denoted\nas grad, is computed as follows:\ngrad =\nDP\n\u2211\ni=1\n|{z}\nall-reduce\nk\n\u2211\nj=1\ngradi, j\n|\n{z\n}\naccumulation\n(6)\nWithin a rank of DP, a rank of PP handles a subset of the\nlayers in the model, a.k.a., stage. As shown in the figure, the\nmicro-batch 1 is distributed to the DP 1, it first forward passes\nthrough the PP 1 then the PP 2, and the backward pass reverses\n8\nDP 1\nall-reduce between data parallel ranks\nScenario #2\nDP 2\nScenario #1\n1\nPP 2\nPP 1\n1\n1\n2\n1\n2\n2\n3\n3\n4\n4\n3\n2\n4\n3\n4\nForward\n5\nPP 2\nPP 1\n5\n5\n6\n5\n6\n6\n7\n7\n8\n8\n7\n6\n8\n7\n8\nBackward\nFigure 8: Timeline of data and pipeline parallelism.\nthe order. The PP pipeline processes the micro-batches within\na DP rank and accumulate the gradients until the completion\nof the backward pass for the last micro-batch. Subsequently,\nthe parameters are updated by averaging the gradients from\nall DP ranks through the all-reduce operation. Notice, that\nthe tensor parallelism (TP) can be further employed within\neach PP rank, which is not shown in the figure as it does not\nimpact our strategy.\n6.2\nResuming from a Failed Iteration\nFailures can disrupt training at any point, but leveraging par-\ntial results from completed micro-batches within the current\nglobal-batch can mitigate recomputation costs. Thanks to\nthe gradient accumulation in micro-batch iterations in the\ndistributed data parallelism, as shown in Equation 6, a state-\ndriven approach has been devised to resume training from a\nfailed global-batch iteration.\nFailures may occur in two scenarios: prior to the\nall-reduce operation (scenario #1) or subsequent to it (sce-\nnario #2), as depicted in Figure 8. Unicron uses a micro-batch\niteration scheduler that monitors and synchronizes progress,\nenabling the training process resume seamlessly from the\npoint of failure. It also redistributes micro-batches from the\nfailed DP rank to others in a round-robin fashion, ensuring\nthe integrity of the training process.\nScenario #1. Before initiating the all-reduce, each DP rank\nmanages its own accumulated gradients and independently\nperforms forward and backward passes for micro-batches as-\nsigned to it. Unicron tracks each DP rank\u2019s progress through\nforward and backward passes. In the event of a failure, the\nresumption process consists of the following steps: 1) pausing\nthe training process, 2) re-establishing the network connection\namong healthy workers, 3) redistributing the micro-batches\nowned by the failed DP rank, and 4) resuming the training\nprocess. After redistributing, each remaining DP rank now\nowns k\u2032 = k +k/(DP\u22121) micro-batches. Consequently, the\naggregated gradient for the global-batch is computed as fol-\nlows:\ngrad =\nx\u22121\n\u2211\ni=1\ni\u0338=x\n\u0012 k\n\u2211\nj=1\ngradi, j\n|\n{z\n}\ncurrent\n+\nk\u2032\n\u2211\nj=k+1\ngradi, j\n|\n{z\n}\nredistributed\n\u0013\n(7)\nScenario #2. In the event of a failure occurring after the\nall-reduce operation started, the response depends on\nwhether the failed worker\u2019s gradients have been reduced, due\nto pipeline parallelism inside each DP rank. 1) If the worker\u2019s\ngradients were already reduced, the worker can be omitted, as\nthe aggregated gradient is already accounted for by the other\nDP replicas. Training proceeds uninterrupted. 2) Conversely,\nif the gradients from the failed worker were not yet reduced,\nUnicron ensures semantic correctness by redistributing the\nfailed DP rank\u2019s micro-batches to remaining DP ranks, simi-\nlar to scenario #1, and recomputing the aggregated gradient.\nUnfortunately, different from scenario #1 where no gradient\nhas been reduced, there are partial gradients (segmented on\nthe stage/layer) that have already been reduced and we need to\ndistinguish them from the rest unreduced gradients. Accord-\ningly, when recomputing the redistributed micro-batches, we\nonly need to recompute the unreduced gradients and ensure\nthe reduced gradients are not overwritten. It\u2019s worth noting\nthat the all-reduce operations solely take place at the end\nof a global-batch iteration and occupy a minor fraction (< 2%\nwhen training the GPT-3 175B model using 128 GPUs) of\nthe iteration time. Therefore, the likelihood of a failure occur-\nring after the initiation of the all-reduce operation remains\nrelatively low.\n6.3\nTransitioning to the New Configuration\nOnce the parameter updates for the ongoing iteration are\ncompleted, or the problematic nodes have been fixed and\nreintegrated into the cluster. Unicron instructs the workers to\ntransition to the new configuration planned by the plan gener-\nator. Next, we investigate the major problem in transitioning\na given task: how to migrate the training states to the new\nconfiguration.\nAccording to Equation 4, there are two kinds of tasks re-\nquiring transitioning: 1) a worker of the task is faulted, and\n2) the task is instructed to scale in or out. Regarding the\nfirst case, Unicron follows the nearest principle to minimize\nthe state migration cost. Unicron initially attempts to request\nthe state of the healthy rank of DP, since the state is already\nreplicated in each DP rank. If replication cannot be fulfilled,\nUnicron resorts to loading the necessary data from the hi-\nerarchical checkpoints [49]. Experience from GEMINI sug-\ngests that it is highly likely that the in-memory checkpoint\nis available, avoiding slower access from remote storage and\nenabling quick resumption of training. In contrast, if the in-\nvolved task is the one that runs normally, complete parameters\nand optimizer states must already be present in the cluster.\nUnicron then requests the workers to proactively replicate the\nstate from existing workers. Notice, that different workers\nissue replication requests simultaneously to minimize their\nown transition period.\n7\nEvaluation\nIn this section, we evaluate Unicron\u2019s performance using a\nrange of workloads and failure scenarios. We first conduct\nmicro-benchmarks to measure Unicron\u2019s error detection time,\ntransition time, the effective throughput and the WAF metric.\n9\nThese tests showcase how effectively Unicron\u2019s techniques\ntackle failure recovery challenges. Additionally, we compare\nthe overall training efficiency of Unicron\u2019s with that of estab-\nlished baselines under diverse failure traces, in a multi-task\nenvironment over a specified period.\nTable 2: The time to detect different kinds of failures.\nCase\nMethod\nUnicron\nw/o Unicron\n1\nNode health monitoring\n5.6 seconds\n5.7 seconds\n2\nProcess supervision\n1.8 seconds\nDtimeout\n3\nException propagation\n0.3 seconds\nDtimeout\n4\nOnline statistical monitoring\n3\u00d7Diter\nDtimeout\nDiter: the average time of one training iteration, typically within 1 minute.\nDtimeout: the timeout threshold of Megatron, 30 minutes by default.\n7.1\nExperimental Setup\nPlatform. All experiments are conducted on the Alibaba\nCloud platform. For the evaluation, we utilize a total of 16\ninstances, with each instance equipped with 8 NVIDIA A800\n(80GB) GPUs, 96 CPU cores, and 1,600 GB of CPU mem-\nory. The 8 GPUs inside each instance are interconnected\nthrough NVSwitch, and each instance is connected to others\nvia four 200Gbps Ethernet NICs. We use Alibaba Cloud\u2019s\ncloud filesystem service as the remote persistent storage for\ncheckpointing, which supports a maximum throughput of 20\nGB/s. For the implementation of Unicron, the software ver-\nsions used is Megatron v23.08, PyTorch v1.12.1 and CUDA\nv11.3.\nWorkloads.\nIn our evaluation, we utilize the GPT-3\nmodel [36] as the primary workload, due to its status as one\nof the largest and most popular models in the LLM domain.\nTo represent different scales, we vary the size of the model\nparameters, specifically considering model sizes of 1.3 billion,\n7 billion, 13 billion, 70 billion, and 175 billion parameters.\nBaselines. For comparison, we have selected several repre-\nsentative systems as baselines. These baselines are chosen\nbased on their availability and their ability on failure recov-\nery. The first is Megatron (v23.08) without any additional\noptimizations introduced by Unicron. This baseline represents\nthe solution of terminating the training process and restarting\nfrom the last persistent checkpoint when resources are re-\ncovered, without support for training with reduced resources.\nThe second baseline is Oobleck [19], which is a state-of-the-\nart framework that adopts dynamic reconfiguration to enable\nresilient distributed training with guaranteed fault tolerance.\nVaruna [3] is included as another baseline, which enables\nasynchronous checkpointing and dynamic reconfiguration\nbased on job morphing for fast recovery. Additionally, we\nalso evaluate Bamboo [46], as the state-of-the-art solution for\nfault-tolerant distributed training through redundant computa-\ntion. For Oobleck, Varuna and Bamboo, the latest open-source\nversions provided by the respective authors have been utilized.\n7.2\nError Detection Efficiency\nTo evaluate the error detection efficiency of Unicron, we sim-\nulate four failure cases, by killing a node (case 1), killing a\n15.0\n16.0\n17.0\n18.0\nUnicron\nMegatron\nOobleck\nBamboo\nVaruna\n16\n24\n32\n40\n48\n56\n64GPUs\n0.0\n1.0\nTransition time (minutes)\nFigure 9: The transition time under failures.\nprocess (case 2), throwing an exception (case 3) during train-\ning, and triggering a performance degradation (case 4). These\ncases cover the most common types of failures that can occur\nduring training, and are detected by the four detection meth-\nods implemented in Unicron. The experiments are performed\nusing different tasks and cluster sizes, and the detection time\nremains relatively consistent for each case, except for case 4.\nIn case 4, the detection time was influenced by the average\nduration of one training iteration (Diter).\nWe compare the detection time of Unicron with the baseline\napproach, which is Megatron without integrating Unicron. For\nthe baseline approach, the detection time is measured as the\nduration from the occurrence of the failure to the termination\nof the training task. The results in Table 2 demonstrate that the\ndetection time of Unicron aligns with the baseline approach\nfor case 1, while is significantly shorter for the remaining\nthree cases.\n7.3\nTransition Efficiency\nFigure 9 presents the transition time of Unicron compared\nto the baselines when a SEV1 failure is detected during the\ntraining of the GPT-3 7B model on clusters of varying scales 1.\nMissing bars indicate that the training task cannot be success-\nfully launched for certain baselines at the specified cluster\nsize. The transition time refers to the duration from detecting\nthe failure to resuming training, which includes recovering\nthe training task based on a new configuration (with a reduced\nnumber of nodes) and recomputing lost progress.\nFor Megatron and Varuna, the transition time required\nis considerably long because the training task needs to be\nrestarted from the last checkpoint, and the progress since the\nlast checkpoint must be recomputed 2 Oobleck and Bamboo\ncan reduce the transition time by enabling dynamic recon-\nfiguration, which eliminates the need to load the checkpoint\nand restart from it. However, their transition time still ex-\nceeds that of Unicron. Unicron further decreases the transition\ntime by utilizing the transition technique, which maximizes\nthe reuse of partial results from ongoing training iterations,\nthereby minimizing the loss caused by failures. Besides, Uni-\ncron is able to maintain a relatively stable transition time\n1As Megatron lacks support for dynamic reconfiguration, our testing of\nMegatron includes the use of a hot spare node that substitutes for the failed\nnode. Consequently, the time Megatron spends waiting for resources is not\nfactored into the transition time measurement.\n2Average recomputation time is 15mins for 30mins checkpoint intervals.\n10\n40\n48\n56 64GPUs\nUnicron\nMegatron\nCase#1\nCase#2\nCase#3\nCase#4\nCase#5\n0\n1\n2\n3\n4\nWAF\n1e16\nequally\nweighted\nsized\nUnicron\n8\n16\n24\n32\n40\n48\n56 64GPUs\n0\n20\n40\n60\n80\nThroughput (samples/s)\n(a) Training throughput (single task).\n1.7B\n7B\n13B 70B 175B\n0\n10\n20\n30\n40\n50\n60\n70\nAchieved FLOP/s Ratio(%)\n(b) FLOP/s ratio (single task).\nCase#1\nCase#2\nCase#3\nCase#4\nCase#5\n0\n1\n2\n3\n4\nWAF\n1e16\n(c) WAF (multiple tasks).\nFigure 10: Training throughput, achieved FLOP/s ratio and WAF of Unicron and baselines under different workloads.\nacross different cluster sizes by efficiently migrating training\nstates between nodes, leveraging the characteristics of the\nparallelism approaches.\n7.4\nTraining Throughput and WAF\nIn this subsection, we evaluate the training efficiency of Uni-\ncron and the baselines in terms of training throughput and\nWAF, considering scenarios with a single training task as well\nas multiple tasks on a given-size cluster, without any failures.\nComparison with Baselines (single task). The experiment\ninvolves training the GPT-3 model of different scales on clus-\nters of varying sizes. Megatron is selected as the baseline\nbecause it exhibits significantly higher training throughput\ncompared to other baselines like Oobleck, Varuna, and Bam-\nboo. To ensure a fair comparison, Megatron is configured\nwith optimal system settings including DP, PP, TP, and oth-\ners, which are identical to those used for Unicron. Figure 10a\npresents the training throughput of training the GPT-3 7B\nmodel, measured in samples per second. We observe that\nUnicron performs on par with Megatron as it introduces no\nadditional overhead to the normal training process and allows\nall the optimizations implemented in Megatron to be fully\nutilized. Figure 10b further validates this claim by comparing\nthe achieved FLOP/s ratio of Unicron with that of Megatron,\nbased on testing the GPT-3 model of varying sizes on a 64-\nGPU cluster.\nTable 3: The tested cases in multi-task experiments.\nCase\nTask 1\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\n1\nS.\n7B\n7B\n7B\n7B\n7B\n7B\nW.\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2\nS.\n1.3B\n1.3B\n1.3B\n7B\n7B\n13B\nW.\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3\nS.\n7B\n7B\n7B\n7B\n7B\n7B\nW.\n0.5\n0.8\n1.1\n1.4\n1.7\n2.0\n4\nS.\n1.3B\n1.3B\n1.3B\n7B\n7B\n13B\nW.\n0.5\n0.8\n1.1\n1.4\n1.7\n2.0\n5\nS.\n1.3B\n1.3B\n1.3B\n7B\n7B\n13B\nW.\n2.0\n1.7\n1.4\n1.1\n0.8\n0.5\nS. denotes the model size, and W. denotes the weight of the task.\nComparison with Baselines (multiple tasks). Figure 10c\ncompares Unicron with various baselines when training six\ntasks on a 128-GPU cluster, measured in the WAF metric of\nthe cluster. Table 3 summarizes the settings for the five tested\ncases, which involve training multiple tasks with different\nsizes and priorities. Since other systems do not support gener-\nating configuration plans for multiple tasks simultaneously,\nwe implemented several baseline strategies for comparison.\nThe first baseline, denoted as \u201cequally\u201d, evenly allocates com-\nputing resources among all tasks. The second baseline, de-\nnoted as \u201cweighted\u201d, allocates computing resources based on\nthe weight assigned to each task. The third baseline, denoted\nas \u201csized\u201d, allocates computing resources based on the model\nsize. From the figure, it is evident that Unicron consistently\nachieves the highest WAF across all five cases compared to\nthe baselines. This outcome highlights the effectiveness of\nthe configuration plan generated by Unicron in maximizing\nWAF, thereby optimizing overall utilization and efficiency.\n7.5\nOverall Training Efficiency\nLastly, we evaluate and compare Unicron\u2019s overall training\nefficiency with baselines in various failure trace scenarios.\nTraces. We collected a failure trace referred to as trace-a in\nFigure 11a from real-world logs we collected, as presented\nin Figure 1. Note that failure occurrences are considered\nindependently for each GPU or node. The trace spans a 8-\nweeks period, including 10 SEV1 failures and 33 failures of\nother types. On the trace, the x-axis denotes the timeline of\nthe training process, while the y-axis reflects the count of\navailable GPUs in the cluster at any given time. It is worth\nmentioning that solely SEV1 failures lead to a decrease in the\ncount of available GPUs, while other types of failures (SEV2\nand SEV3) do not affect this count. Regarding SEV1 failures,\nthe time taken for a node to recover and become available once\nmore is determined by a uniform random selection, varying\nfrom 1 to 7 days.\nAdditionally, Figure 11d depicts another trace (referred\nto as trace-b), which is generated by amplifying the failure\nfrequency of trace-a by a factor of 20\u00d7. The occurrence of\nfailures during the training process is simulated using a Pois-\nson distribution, which allows for the possibility of multiple\nfailures happening within a given time interval. trace-b is\ndesigned to simulate a scenario where failures occur more\nfrequently, thus providing a rigorous test of the systems\u2019 self-\nhealing capabilities under extreme scenarios.\nIt spans a duration of 7 day and records 26 SEV1 fail-\n11\n49\n56d\nUnicron\nMegatron\nOobleck\nBamboo\nVaruna\n7\n14\n21\n28\n35\n42\n49 56d\n64\n80\n96\n112\n128\n144\n#GPUs\nSEV2/3\nSEV1\nNode rejoin\n(a) Unicron trace-a.\n7\n14\n21\n28\n35\n42\n49 56d\n0\n2\n4\nWAF\n1e16\n(b) WAF (trace-a).\n7\n14\n21\n28\n35\n42\n49 56d\n0\n2\n4\nAccumulated WAF\n1e19\n(c) Accumulated WAF (trace-a).\n1\n2\n3\n4\n5\n6\n7d\n64\n80\n96\n112\n128\n144\n#GPUs\nSEV2/3\nSEV1\nNode rejoin\n(d) Unicron trace-b.\n1\n2\n3\n4\n5\n6\n7d\n0\n2\n4\nWAF\n1e16\n(e) WAF (trace-b).\n1\n2\n3\n4\n5\n6\n7d\n0\n2\n4\n6\nAccumulated WAF\n1e18\n(f) Accumulated WAF (trace-b).\nFigure 11: Overall training efficiency (in WAF and accumulated WAF) of Unicron and baselines under different failure traces.\nures and 80 other failures. Accordingly, repaired nodes are\nre-joining the cluster at a similar rate to maintain a stable\nresource pool. This particular trace serves to replicate an\nenvironment in which failures occur more frequently, thus\nproviding a rigorous test of the system\u2019s self-healing capabili-\nties. In our simulation, SEV1 failures are induced by killing\nthe agent process on the affected node, while SEV2 and SEV3\nfailures are triggered by either raising an exception or by\nkilling/stalling the training process.\nWorkloads and Baselines. For the evaluation, the workload\nused is the Case#5 from Table 3. This workload involves\nthe concurrent training of multiple tasks on a cluster with\n128 GPUs. These tasks vary in size and priority, simulating\nreal-world scenarios where different training jobs are running\nsimultaneously. Given that the baseline systems we compare\nagainst do not possess the capability to generate configu-\nration plans for multiple tasks, we assign the same initial\nconfiguration plan designated for Unicron to these baselines,\nwhich is considered optimal according to our proposed model.\nHowever, in the event of a failure, the baseline methods only\nreconfigure the task directly impacted. Moreover, should a\nnode recover, these methods give precedence to reconfiguring\nthe task that was first affected. In contrast, Unicron features\na critical advantage over the baseline approaches: it can also\nreconfigure other tasks if it proves beneficial to do so.\nComparison. In Figure 11, we present a comparison analysis\nof the overall training efficiency between Unicron and the\nbaselines. The efficiency metric employed is the total WAF of\nall tasks within the cluster, measured at successive time points.\nThe accumulated WAF is also reported to illustrate the overall\ntraining efficiency throughout the evaluation period.\nThe results unequivocally demonstrate that Unicron consis-\ntently achieves the highest performance in accumulated WAF.\nCompared with baselines, on trace-a, Unicron outperforms\nMegatron by 1.2\u00d7, Bamboo by 4.6\u00d7, Oobleck by 3.7\u00d7, and\nVaruna by 4.8\u00d7 in terms of accumulated WAF. It should\nbe noted that Megatron achieves higher performance than\nBamboo, Oobleck, and Varuna, which is expected given its\nintegration of various techniques to ensure training efficiency.\nConversely, Bamboo, Oobleck, and Varuna, with their primary\nfocus on fault tolerance, do not prioritize training efficiency\noptimizations, resulting in lower performance.\nUnder trace-b, when failure frequency raising, Unicron out-\nperforms Megatron by 1.9\u00d7, Bamboo by 4.8\u00d7, Oobleck by\n3.8\u00d7, and Varuna by 5.8\u00d7 in terms of accumulated WAF.\nAll systems experience diminished performance under this\ntrace due to the increased frequency of failures. Megatron\nsuffers the most significant reduction as the recovery costs\nare exacerbated by the frequent failures, and it lacks specific\noptimizations to mitigate these additional expenses. Unicron,\nbeing built on top of Megatron, is able to fully leverage its\noptimizations while introducing additional optimizations for\nefficient self-healing. As a result, Unicron achieves significant\nimprovements in performance compared to other baselines\nunder this trace. These outcome underscores the effective-\nness of Unicron in achieving high performance under diverse\nscenarios, validating its practical applicability.\n8\nConclusion\nThis paper introduces the Unicron system as a holistic ap-\nproach to address the challenges of failure recovery in training\nlarge-scale language models. By incorporating in-band error\ndetection, a dynamic cost-aware plan generation mechanism,\nand a transition strategy, Unicron minimizes the overall cost\nof failures across multiple tasks within a cluster. As a result,\nUnicron demonstrates a notable increase in overall training\nefficiency, with performance gains reaching up to 1.9\u00d7 that\nof state-of-the-art solutions.\n12\nReferences\n[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-\njay Ghemawat, Geoffrey Irving, Michael Isard, et al.\n{TensorFlow}: a system for {Large-Scale} machine\nlearning. In 12th USENIX symposium on operating\nsystems design and implementation (OSDI 16), pages\n265\u2013283, 2016.\n[2] Amazon Web Services. Amazon Web Services. https:\n//aws.amazon.com/.\n[3] Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ra-\nmachandran Ramjee, and Nipun Kwatra. Varuna: scal-\nable, low-cost training of massive deep learning models.\nIn Proceedings of the Seventeenth European Conference\non Computer Systems, pages 472\u2013487, 2022.\n[4] David Bernstein. Containers and cloud: From lxc to\ndocker to kubernetes. IEEE Cloud Computing, 1(3):81\u2013\n84, 2014.\n[5] Siyuan Chen, Mengyue Wu, Kenny Q Zhu, Kunyao Lan,\nZhiling Zhang, and Lyuchun Cui. Llm-empowered chat-\nbots for psychiatrist and patient simulation: Application\nand evaluation. arXiv preprint arXiv:2305.13614, 2023.\n[6] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. Training deep nets with sublinear memory\ncost. arXiv preprint arXiv:1604.06174, 2016.\n[7] Alibaba Cloud. Alibaba cloud: Reliable and secure\ncloud computing services. https://www.alibabaclo\nud.com/.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\n[9] Assaf Eisenman, Kiran Kumar Matam, Steven Ingram,\nDheevatsa Mudigere, Raghuraman Krishnamoorthi, Kr-\nishnakumar Nair, Misha Smelyanskiy, and Murali An-\nnavaram. {Check-N-Run}: A checkpointing system for\ntraining deep learning recommendation models. In 19th\nUSENIX Symposium on Networked Systems Design and\nImplementation (NSDI 22), pages 929\u2013943, 2022.\n[10] Jinze Bai et al. Qwen technical report, 2023.\n[11] etcd. etcd. https://etcd.io/.\n[12] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu\nWang, Zhen Zheng, Chuan Wu, Guoping Long, Jun\nYang, Lixue Xia, et al. Dapple: A pipelined data paral-\nlel approach for training large models. In Proceedings\nof the 26th ACM SIGPLAN Symposium on Principles\nand Practice of Parallel Programming, pages 431\u2013445,\n2021.\n[13] glm. glm. https://keg.cs.tsinghua.edu.cn/yux\niao/papers/slides-2023-ijcai-llm-glm-130-c\nhatglm-agentbench.pdf.\n[14] Google. Google cloud platform. https://cloud.go\nogle.com/.\n[15] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan\nFirat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan\nNgiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Effi-\ncient training of giant neural networks using pipeline\nparallelism. Advances in neural information processing\nsystems, 32, 2019.\n[16] Changho Hwang, Taehyun Kim, Sunghyun Kim, Jinwoo\nShin, and KyoungSoo Park. Elastic resource sharing\nfor distributed deep learning. In 18th USENIX Sympo-\nsium on Networked Systems Design and Implementation\n(NSDI 21), pages 721\u2013739. USENIX Association, April\n2021.\n[17] Changho Hwang, Taehyun Kim, Sunghyun Kim, Jinwoo\nShin, and KyoungSoo Park. Elastic resource sharing\nfor distributed deep learning. In 18th USENIX Sympo-\nsium on Networked Systems Design and Implementation\n(NSDI 21), pages 721\u2013739, 2021.\n[18] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gho-\nlami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and\nIon Stoica.\nCheckmate: Breaking the memory wall\nwith optimal tensor rematerialization. Proceedings of\nMachine Learning and Systems, 2:497\u2013511, 2020.\n[19] Insu Jang, Zhenning Yang, Zhen Zhang, Xin Jin, and\nMosharaf Chowdhury. Oobleck: Resilient distributed\ntraining of large models using pipeline templates. In\nProceedings of the 29th Symposium on Operating Sys-\ntems Principles, pages 382\u2013395, 2023.\n[20] Andreas Jungherr. Using chatgpt and other large lan-\nguage model (llm) applications for academic paper as-\nsignments. 2023.\n[21] Can Karakus, Rahul Huilgol, Fei Wu, Anirudh Subra-\nmanian, Cade Daniel, Derya Cavdar, Teng Xu, Haohan\nChen, Arash Rahnama, and Luis Quintela. Amazon\nsagemaker model parallelism: A general and flexible\nframework for large model training.\narXiv preprint\narXiv:2111.05972, 2021.\n[22] Taebum Kim, Hyoungjoo Kim, Gyeong-In Yu, and\nByung-Gon Chun. Bpipe: Memory-balanced pipeline\nparallelism for training large language models. 2023.\n13\n[23] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym,\nLawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. Reducing activation\nrecomputation in large transformer models. Proceed-\nings of Machine Learning and Systems, 5, 2023.\n[24] Jiamin Li, Hong Xu, Yibo Zhu, Zherui Liu, Chuanxiong\nGuo, and Cong Wang. Lyra: Elastic scheduling for deep\nlearning clusters. In Proceedings of the Eighteenth Eu-\nropean Conference on Computer Systems, EuroSys \u201923,\npages 835\u2013850, New York, NY, USA, 2023. Association\nfor Computing Machinery.\n[25] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar,\nPieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith,\nBrian Vaughan, Pritam Damania, et al. Pytorch dis-\ntributed: Experiences on accelerating data parallel train-\ning. arXiv preprint arXiv:2006.15704, 2020.\n[26] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yong-\nbin Li, and Yang You. Sequence parallelism: Long se-\nquence training from system perspective. arXiv preprint\narXiv:2105.13120, 2021.\n[27] Luo Mai, Guo Li, Marcel Wagenl\u00e4nder, Konstantinos\nFertakis, Andrei-Octavian Brabete, and Peter Pietzuch.\nKungFu: Making training in distributed machine learn-\ning adaptive. In 14th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 20), pages\n937\u2013954. USENIX Association, November 2020.\n[28] Microsoft Azure. Microsoft Azure. https://azure.\nmicrosoft.com/.\n[29] MLPerf. MLPerf. https://www.mlperf.org/.\n[30] MLPerf v3.1 NVIDIA Submission.\nMLPerf v3.1\nNVIDIA Submission. https://github.com/mlc\nommons/training_results_v3.1/tree/main/NVI\nDIA.\n[31] Jayashree\nMohan, Amar Phanishayee, and Vijay\nChidambaram.\n{CheckFreq}:\nFrequent,{Fine-\nGrained}{DNN} checkpointing.\nIn 19th USENIX\nConference on File and Storage Technologies (FAST\n21), pages 203\u2013216, 2021.\n[32] Deepak Narayanan, Aaron Harlap, Amar Phanishayee,\nVivek Seshadri, Nikhil R Devanur, Gregory R Ganger,\nPhillip B Gibbons, and Matei Zaharia. Pipedream: Gen-\neralized pipeline parallelism for dnn training. In Pro-\nceedings of the 27th ACM Symposium on Operating\nSystems Principles, pages 1\u201315, 2019.\n[33] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie\nChen, and Matei Zaharia. Memory-efficient pipeline-\nparallel dnn training. In International Conference on\nMachine Learning, pages 7937\u20137947. PMLR, 2021.\n[34] Deepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Korthikanti,\nDmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer,\nBryan Catanzaro, et al. Efficient large-scale language\nmodel training on gpu clusters using megatron-lm. In\nProceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Anal-\nysis, pages 1\u201315, 2021.\n[35] Bogdan Nicolae, Jiali Li, Justin M Wozniak, George\nBosilca, Matthieu Dorier, and Franck Cappello. Deep-\nfreeze: Towards scalable asynchronous checkpointing\nof deep learning models. In 2020 20th IEEE/ACM In-\nternational Symposium on Cluster, Cloud and Internet\nComputing (CCGRID), pages 172\u2013181. IEEE, 2020.\n[36] OpenAI. Language models are few-shot learners. http\ns://openai.com/blog/gpt-3-apps, 2020.\n[37] OpenAI. Chatgpt: Language models for task-oriented\ndialogue. https://openai.com/blog/chatgpt/,\n2021.\n[38] Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py-\ntorch: An imperative style, high-performance deep learn-\ning library. Advances in neural information processing\nsystems, 32, 2019.\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language mod-\nels are unsupervised multitask learners. OpenAI blog,\n1(8):9, 2019.\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a unified text-to-text transformer. The Journal\nof Machine Learning Research, 21(1):5485\u20135551, 2020.\n[41] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. Deepspeed: System optimizations enable\ntraining deep learning models with over 100 billion pa-\nrameters. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery &\nData Mining, pages 3505\u20133506, 2020.\n[42] SageMaker. Sagemaker. https://docs.aws.amazon.\ncom/sagemaker/index.html.\n[43] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9,\nAlexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\nGall\u00e9, et al.\nBloom:\nA 176b-parameter open-\naccess multilingual language model. arXiv preprint\narXiv:2211.05100, 2022.\n14\n[44] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter lan-\nguage models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\n[45] Dharma\nShukla,\nMuthian\nSivathanu,\nSrinidhi\nViswanatha, Bhargav\nGulavani, Rimma\nNehme,\nAmey Agrawal, Chen Chen, Nipun Kwatra, Ramachan-\ndran Ramjee, Pankaj Sharma, Atul Katiyar, Vipul\nModi, Vaibhav Sharma, Abhishek Singh, Shreshth\nSinghal, Kaustubh Welankar, Lu Xun, Ravi Anupindi,\nKarthik Elangovan, Hasibur Rahman, Zhou Lin, Rahul\nSeetharaman, Cheng\nXu, Eddie\nAilijiang, Suresh\nKrishnappa, and Mark Russinovich.\nSingularity:\nPlanet-scale, preemptive and elastic scheduling of ai\nworkloads, 2022.\n[46] John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yi-\nfan Qiao, Zhihao Jia, Minjia Zhang, Ravi Netravali, and\nGuoqing Harry Xu. Bamboo: Making preemptible in-\nstances resilient for affordable training of large {DNNs}.\nIn 20th USENIX Symposium on Networked Systems De-\nsign and Implementation (NSDI 23), pages 497\u2013513,\n2023.\n[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[49] Zhuang Wang, Zhen Jia, Shuai Zheng, Zhen Zhang, Xin-\nwei Fu, TS Eugene Ng, and Yida Wang. Gemini: Fast\nfailure recovery in distributed training with in-memory\ncheckpoints. In Proceedings of the 29th Symposium on\nOperating Systems Principles, pages 364\u2013381, 2023.\n[50] Baodong Wu, Lei Xia, Qingping Li, Kangyu Li,\nXu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen,\nand Shigang Li. Transom: An efficient fault-tolerant sys-\ntem for training llms. arXiv preprint arXiv:2310.10046,\n2023.\n[51] Yidi Wu, Kaihao Ma, Xiao Yan, Zhi Liu, Zhenkun\nCai, Yuzhen Huang, James Cheng, Han Yuan, and Fan\nYu. Elastic deep learning in multi-tenant gpu clusters.\nIEEE Transactions on Parallel and Distributed Systems,\n33(1):144\u2013158, 2022.\n[52] Lei Xie, Jidong Zhai, Baodong Wu, Yuanbo Wang,\nXingcheng Zhang, Peng Sun, and Shengen Yan. Elan:\nTowards generic and efficient elastic training for deep\nlearning. In 2020 IEEE 40th International Conference\non Distributed Computing Systems (ICDCS), pages 78\u2013\n88, 2020.\n[53] Andy B Yoo, Matt A Jette, and Mark Grondona. Slurm:\nSimple linux utility for resource management.\nJob\nScheduling Strategies for Parallel Processing, pages 44\u2013\n60, 2003.\n[54] Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt:\nOpen pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068, 2022.\n[55] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao\nZhuang, Zhifeng Chen, Yanping Huang, Yida Wang,\nYuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa:\nAutomating inter-and {Intra-Operator} parallelism for\ndistributed deep learning.\nIn 16th USENIX Sympo-\nsium on Operating Systems Design and Implementation\n(OSDI 22), pages 559\u2013578, 2022.\n15\n"
  },
  {
    "title": "SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity",
    "link": "https://arxiv.org/pdf/2401.00604.pdf",
    "upvote": "4",
    "text": "Preprint\nSTEINDREAMER: VARIANCE REDUCTION FOR TEXT-\nTO-3D SCORE DISTILLATION VIA STEIN IDENTITY\nPeihao Wang1\u2217, Zhiwen Fan1, Dejia Xu1, Dilin Wang2, Sreyas Mohan2, Forrest Iandola2,\nRakesh Ranjan2, Yilei Li2, Qiang Liu1, Zhangyang Wang1, Vikas Chandra2\n1The University of Texas at Austin, 2Meta Reality Labs\n{peihaowang, zhiwenfan, dejia, atlaswang}@utexas.edu, lqiang@cs.utexas.edu\n{wdilin, sreyasmohan, fni, rakeshr, yileil, vchandra}@meta.com\nABSTRACT\nScore distillation has emerged as one of the most prevalent approaches for text-\nto-3D asset synthesis. Essentially, score distillation updates 3D parameters by\nlifting and back-propagating scores averaged over different views. In this paper,\nwe reveal that the gradient estimation in score distillation is inherent to high\nvariance. Through the lens of variance reduction, the effectiveness of SDS and\nVSD can be interpreted as applications of various control variates to the Monte\nCarlo estimator of the distilled score. Motivated by this rethinking and based\non Stein\u2019s identity, we propose a more general solution to reduce variance for\nscore distillation, termed Stein Score Distillation (SSD). SSD incorporates control\nvariates constructed by Stein identity, allowing for arbitrary baseline functions.\nThis enables us to include flexible guidance priors and network architectures to\nexplicitly optimize for variance reduction. In our experiments, the overall pipeline,\ndubbed SteinDreamer, is implemented by instantiating the control variate with a\nmonocular depth estimator. The results suggest that SSD can effectively reduce\nthe distillation variance and consistently improve visual quality for both object-\nand scene-level generation. Moreover, we demonstrate that SteinDreamer achieves\nfaster convergence than existing methods due to more stable gradient updates.\nProject page: vita-group.github.io/SteinDreamer/.\n1\nINTRODUCTION\nThere have been recent significant advancements in text-to-image generation, driven by diffusion\nmodels. Notable examples include Nichol et al. (2021); Ramesh et al. (2021; 2022); Rombach et al.\n(2022) and Sohl-Dickstein et al. (2015); Ho et al. (2020); Song & Ermon (2019); Song et al. (2020);\nDhariwal & Nichol (2021). These developments have sparked growing interest in the realm of\ntext-guided 3D generation. This emerging field aims to automate and accelerate 3D asset creation in\nthe applications of virtual reality, movies, and gaming. However, 3D synthesis poses significantly\ngreater challenges. Directly training generative models using 3D data, as explored in works by (Wu\net al., 2016; Yang et al., 2019; Cai et al., 2020; Nichol et al., 2022; Jun & Nichol, 2023; Chan et al.,\n2022; Shue et al., 2022), faces practical hurdles due to the scarcity of high-quality and diverse data.\nMoreover, the inherent complexity of generative modeling with 3D representations adds an extra\nlayer of intricacy to this endeavor.\nIn recent times, techniques based on score distillation (Poole et al., 2022; Wang et al., 2023b),\nexemplified by DreamFusion and ProlificDreamer, have gained prominence. These methods have\ngarnered attention for their ability to effectively bypass the need for 3D data by leveraging a 2D\ndiffusion model for 3D generation. In particular, Poole et al. (2022) introduces Score Distillation\nSampling (SDS), which optimizes a differentiable 3D representation, such as NeRF (Mildenhall et al.,\n2020), by lifting and back-propagating image scores from a pre-trained text-to-image diffusion model.\nAmong its subsequent works (Lin et al., 2023; Wang et al., 2023a; Chen et al., 2023; Metzer et al.,\n2023), ProlificDreamer stands out for significantly enhancing the generation quality through derived\nVariational Score Distillation (VSD) (Wang et al., 2023b) . VSD introduces an additional score for\nrendered images to improve parameter updates.\n\u2217Work done during an internship with Meta.\n1\narXiv:2401.00604v1  [cs.CV]  31 Dec 2023\nPreprint\nHowever, it is widely recognized that gradient obtained through score distillation techniques tend to\nbe noisy and unstable due to the high uncertainty in the denoising process and the small batch size\nlimited by computational constraints. Consequently, this leads to slow convergence and suboptimal\nsolutions. In this paper, we address this issue by proposing a unified variance reduction approach.\nWe reveal that both the noise term in SDS and the extra score function introduced by VSD have zero\nmeans, and thus can be regarded as control variates. The update of VSD is equivalent to the update\nof SSD in expectation. However, the gradient variance is smaller in VSD due to the effect of a better\nimplementation the control variate.\nBuilding on these insights, we present a more flexible control variate for score distillation, leveraging\nStein identity (Stein, 1972; Chen, 1975; Gorham & Mackey, 2015), dubbed Stein Score Distillation\n(SSD). Stein\u2019s identity, given by Ex\u223cp[\u2207 log p(x) \u00b7 f(x)\u22a4 + \u2207xf(x)] = 0 for any distribution p and\nfunction f satisfying mild regularity conditions (Stein, 1972; Gorham & Mackey, 2015; Liu et al.,\n2016). This formulation establishes a broader class of control variates due to its zero means, providing\nflexibility in optimizing function f for variance reduction. Specifically, our Stein Score Distillation\n(SSD) frames the distillation update as a combination of the score estimation from a pre-trained\ndiffusion model and a control variate derived from Stein\u2019s identity. The first term aligns with with\nthat in SDS and VSD, serving to maximize the likelihood of the rendered image. The second control\nvariate is tailored to specifically reduce gradient variance. Importantly, our construction allows us\nto incorporate arbitrary prior knowledge and network architectures in f, facilitating the design of\ncontrol variates highly correlated with the lifted image score, leading to a significant reduction in\ngradient variance.\nWe integrate our proposed SSD into a text-to-3D generation pipeline, coined as SteinDreamer.\nThrough extensive experiments, we demonstrate that SteinDreamer can consistently mitigate vari-\nance issues within the score distillation process. For both 3D object and scene-level generation,\nSteinDreamer outperforms DreamFusion and ProlificDreamer by providing detailed textures, precise\ngeometries, and effective alleviation of the Janus (Hong et al., 2023) and ghostly (Warburg et al.,\n2023) artifacts. Lastly, it\u2019s worth noting that SteinDreamer, with its reduced variance, accelerates the\nconvergence of 3D generation, reducing the number of iterations required by 14%-22%.\n2\nPRELIMINARIES\n2.1\nSCORE DISTILLATION\nDiffusion models, as demonstrated by various works (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song\n& Ermon, 2019; Song et al., 2020), have proven to be highly effective in text-to-image generation.\nBuild upon the success of 2D diffusion models, Poole et al. (2022); Wang et al. (2023a); Lin et al.\n(2023); Chen et al. (2023); Tsalicoglou et al. (2023); Metzer et al. (2023); Wang et al. (2023b);\nHuang et al. (2023) demonstrate the feasibility of using a 2D generative model to create 3D asserts.\nAmong these works, score distillation techniques play a central role by providing a way to guide a\ndifferentiable 3D representation using a pre-trained text-to-image diffusion model.\nEssentially, score distillation lifts and back-propagates signals estimated from a 2D prior to update a\ndifferentiable 3D representation, such as NeRF (Mildenhall et al., 2020), via the chain rule (Wang\net al., 2023a). There are primarily two types of distillation schemes: Score Distillation Sampling\n(SDS) (Poole et al., 2022) and Variational Score Distillation (VSD) (Wang et al., 2023b):\nScore Distillation Sampling.\nThe main idea of SDS is to utilize the denoising score matching\nloss to optimize a 3D representation that semantically matches a given text prompt y based on\nits multi-view projection, using the score function of a 2D image distribution \u2207 log pt. By taking\nderivatives with respect to 3D parameters \u03b8 and dropping the Jacobian matrix of the score function to\nsimplify the computation, SDS yields the following update to optimize \u03b8 1:\n\u2206SDS = Et,c,\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(\u03b1tg(\u03b8, c) + \u03c3t\u03f5|y) \u2212 \u03f5)\n\u0015\n,\n(1)\nwhere the expectation of t is taken over a uniform distribution U[0, T], and \u03b1t, \u03c3t > 0 are time-\ndependent diffusion coefficients. And c is taken over some camera distribution pc defined on\n1By default, Jacobian matrices are transposed.\n2\nPreprint\nSO(3) \u00d7 R3, g(\u03b8, c) renders a 2D view from \u03b8 given c. In this work, we follow DreamFusion and\nparameterize \u03b8 as a NeRF. In this case, g(\u03b8, c) renders a view by casting each pixel on the image\nplane to a ray using camera pose c with volume rendering. Meanwhile, \u2207 log pt can be surrogated by\na noise estimator in a pre-trained diffusion model.\nVariational Score Distillation.\nProlificDreamer introduced a new variant of score distillation,\nvariational score distillation (VSD) (Wang et al., 2023b), through the lens of particle-based variational\ninference (Liu & Wang, 2016; Liu, 2017; Detommaso et al., 2018). ProlificDreamer also minimizes\nthe KL divergence between pt(x) and the image distribution rendered from a 3D representation \u03b8. It\nachieves this by deriving the following update rule through Wasserstein gradient flow:\n\u2206V SD = Et,c,\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(x|y) \u2212 \u03c3t\u2207 log qt(x|c))\n\u0015\n,\n(2)\nwhere x = \u03b1tg(\u03b8, c) + \u03c3t\u03f5 is the noisy observation of the rendered image, sampled by a random\ncamera pose c. Notably, there emerges a new score function of probability density function qt(x|c),\nwhich characterizes the conditional distribution of noisy rendered images given the camera pose c.\nWhile \u2207 log pt can be approximated in a similar manner using an off-the-shelf diffusion model \u03f5\u03d5,\n\u2207 log qt is not readily available. The solution provided by Wang et al. (2023b) is to fine-tune a pre-\ntrained diffusion model using the rendered images. The approach results an alternating optimization\nparadigm between \u03b8 with Eq. 2 and \u03c8 with score matching:\nmin\n\u03c8 Et,c,\u03f5\u223cN (0,I)\n\u0002\n\u03c9(t)\u2225\u03f5\u03c8(\u03b1tg(\u03b8, c) + \u03c3t\u03f5, t, c, y) \u2212 \u03f5\u22252\n2\n\u0003\n,\n(3)\nwhere \u03f5\u03c8(x, t, c, y) is a diffusion model additionally conditioned on the camera pose c. \u03c8 is\ninitialized with a pre-trained one and parameterized by LoRA (Hu et al., 2021).\n2.2\nCONTROL VARIATE\nMeanwhile, in this work, we introduce control variate to the context of score distillation. Control\nvariate is a widely utilized technique to reduce variance for Monte Carlo estimator in various fields,\nincluding physical simulation (Davies et al., 2004), graphical rendering (Kajiya, 1986; M\u00a8uller et al.,\n2020), network science (Meyn, 2008; Chen et al., 2017), and reinforcement learning (Williams,\n1992; Sutton et al., 1998; 1999; Liu et al., 2017). Suppose we want to estimate the expectation\nEx\u223cq(x)[f(x)] for some function f : RD \u2192 RD via Monte Carlo samples {xi \u2208 RD}N\ni=1: \u2206 =\n1\nN\nPN\ni=1 f(xi). The estimator \u2206 is supposed to have large variance when N is small. Consider we\nhave control variate as a function h : RD \u2192 RD with analytic mean under q(x), which without loss\nof generality, can be assumed to have zero mean. Then we can construct an unbiased estimator by\nadding term \u03be =\n1\nN\nPN\ni=1 h(xi): \u2206\u2020 =\n1\nN\nPN\ni=1(f(xi) + \u00b5 \u2299 h(xi)), where u \u2208 RD is a group\nof reweighting coefficients and \u2299 denotes element-wise multiplication. The resultant estimator has\nvariance for the i-th entry:\nVar\nh\n\u2206\u2020\ni\ni\n= Var [\u2206i] + \u00b52\ni Var [\u03bei] + 2\u00b5i E\n\u0002\n\u2206\u03be\u22a4\u0003\nii ,\n(4)\nwhere it is possible to reduce Var[\u2206\u2020\ni] by selecting h and u properly. To maximize variance reduction,\nu is chosen as u\u2217\ni = \u2212 E[\u2206\u03be\u22a4]ii/Var[\u03bei], leading to Var[\u2206\u2020\ni] = (1 \u2212 Corr(\u2206i, \u03bei)2)Var[\u2206i],\nwhere Corr(\u00b7, \u00b7) denotes the correlation coefficient. This signifies that higher correlation between\nfunctions f and h, then more variance can be reduced.\n3\nRETHINKING SDS AND VSD: A CONTROL VARIATE PERSPECTIVE\nIn this section, we reveal that the variance of update estimation may play a key role in score distillation.\nAt first glance, SDS and VSD differ in their formulation and implementation. However, our first\ntheoretical finding reveals that SDS and (single-particle) VSD are equivalent in their expectation, i.e.,\n\u2206SDS = \u2206V SD. We formally illustrate this observation below.\nAs a warm-up, we inspect SDS via the following rewriting.\n\u2206SDS = Et,c,\u03f5\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(x|y)\n\u0015\n|\n{z\n}\nf(t,\u03b8,x,c)\n\u2212 Et,c,\u03f5\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03f5\n\u0015\n|\n{z\n}\nhSDS(t,\u03b8,x,c)\n,\n(5)\n3\nPreprint\nTraining Steps (1e3)\nLog Variance\nSDS\nVSD\na 3D model of an adorable cottage with a thatched roof\nFigure 1: Variance comparison between SDS and VSD. We monitor the variance of \u2206SDS and \u2206V SD for\nevery 100 training step. We show that variance level is highly correlated to the performance of score distillation.\nwhere x = \u03b1tg(\u03b8, c) + \u03c3t\u03f5. The second term E[hSDS(t, \u03b8, x, c)] = 0 simply because it is the\nexpectation of a zero-mean Gaussian vector. For VSD, we follow a similar derivation and obtain:\n\u2206V SD = Et,c,\u03f5\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(x|y)\n\u0015\n|\n{z\n}\nf(t,\u03b8,x,c)\n\u2212 Et,c,\u03f5\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log qt(x|c)\n\u0015\n|\n{z\n}\nhV SD(t,\u03b8,x,c)\n,\n(6)\nwhere once again the second term E[hV SD(t, \u03b8, x, c)] = 0. This can be proven by showing that\nqt(x|c) turns out to be a zero-mean Gaussian distribution or applying the inverse chain rule followed\nby the fact that the first-order moment of the score function is constantly zero. Moreover, the first term\nE[f(t, \u03b8, x, c)] of both SDS and VSD equals to \u2212\u2207\u03b8 Et [DKL(qt(x|c)\u2225pt(x|y))]. This implies that\nSDS and VSD are equivalently minimizing the distribution discrepancy between the noisy rendered\nimage distribution and the Gaussian perturbed true image distribution, as a gradient descent algorithm.\nWe defer the full derivation to Appendix A.\nHowever, in most scenarios, empirical evidence indicates that VSD consistently outperforms SDS,\ndespite both methods aiming to minimize the same underlying objective. To explain this paradox,\nwe posit that the underlying source of their performance disparities is attributed to the variance of\nstochastic simulation of the expected updates suggested by SDS and VSD. The numerical evaluation\nof Eq. 1 and Eq. 2 typically relies on Monte Carlo estimation over a mini-batch. Unfortunately,\nrendering a full view from NeRF and performing inference with diffusion models are computationally\ndemanding processes, leading to constraints on the number of rendered views that can be utilized\nwithin a single optimization step, often limited to just one, as observed in previous work (Poole\net al., 2022). Additionally, the term related to the score function within the expectation undergoes\na denoising procedure, notorious for its instability and high uncertainty, especially when t is large.\nHence, despite SDS and VSD having identical means, we argue that the variance of their numerical\nestimation significantly differs. We empirically validate this hypothesis in Fig. 1, where we visualize\nthe variance of \u2206SDS and \u2206V SD during the training process. We observe that VSD yields results\nwith richer textures, and in the meanwhile, achieves lower level variance compared to SDS.\nTo gain insight into the variance disparity between SDS and VSD, we connect SDS and VSD via\nthe concept of control variates. As introduced in Sec. 2.2, a control variate is a zero-mean random\nvariable capable of reducing the variance of Monte Carlo estimator when incorporated into the\nsimulated examples. Notably, both hSDS(t, \u03b8, x, c) and hV SD(t, \u03b8, x, c) can be regarded as control\nvariates, as confirmed by Eq. 5 and Eq. 6 due to their zero means. Consequently, SDS and VSD\ncan be interpreted as Monte Carlo estimators of the gradient of the KL divergence, integrated with\ndifferent control variates. As demonstrated in Sec. 2.2, control variate with higher correlation to the\nestimated variable leads to larger variance reduction. VSD exhibits lower variance primarily because\n\u2207 log qt(x|c) in control variate hV SD is fine-tuned from \u2207 log pt(x|c), and thus resulting in higher\ncorrelation compared to the pure Gaussian noises in hSDS.\n4\nSTEIN SCORE DISTILLATION\nHaving revealed that variance control is one of the key knobs to improve the performance of score dis-\ntillation, we extend the family of control variates that can be used for score distillation in this section.\n4\nPreprint\nNoises\nUNet \nBaseline Function\nCamera pose\nStein Formula\nRender\n3D representation \nImage\nBackward\nFigure 2: Pipeline of SteinDreamer. We incorporate control variates constructed by Stein\u2019s identity into a\nscore distillation pipeline, allowing for arbitrary baseline functions. In practice, we implement the baseline\nfunctions with a monocular depth estimator or normal estimator.\n4.1\nSTEIN CONTROL VARIATES FOR SCORE DISTILLATION\nOur main inspiration is drawn from Oates et al. (2017); Liu (2017); Roeder et al. (2017) that Stein\u2019s\nidentity can be served as a powerful and flexible tool to construct zero-mean random variables. We\nconsider Stein\u2019s identity associated with any conditional probability p(x|\u03b8, c) as below:\nEx\u223cp(x|\u03b8,c) [\u2207 log p(x|\u03b8, c)\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c)] = 0,\n(7)\nwhere \u03d5(t, \u03b8, x, c) is referred to as the baseline function, which can be arbitrary scalar-value function\nsatisfying regularity conditions (Stein, 1972; Gorham & Mackey, 2015; Liu et al., 2016). By plugging\nqt(x|\u03b8, c) into Eq. 7, we can construct our control variate as follows:\nhSSD(t, \u03b8, c, x) = \u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0012\n\u03f5\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c)\n\u0013\n,\n(8)\nwhere x = \u03b1tg(\u03b8, c) + \u03c3t\u03f5 and \u03f5 \u223c N(0, I). Additional details and derivations are provided in\nAppendix A. The advantage of hSSD lies in its flexibility to define an infinite class of control variates,\ncharacterized by arbitrary baseline function \u03d5(t, \u03b8, x, c).\n4.2\nVARIANCE MINIMIZATION VIA STEIN SCORE DISTILLATION\nWe propose to adopt hSSD as the control variate for score distillation. In addition to hSSD, we\nintroduce a group of learnable weights \u00b5 \u2208 RD to facilitate optimal variance reduction following the\nstandard scheme introduced in Sec. 2.2. Altogether, we present the following update rule, termed as\nStein Score Distillation (SSD):\n\u2206SSD = Et,c,\u03f5\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(x|y) + \u00b5 \u2299 [\u03f5\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c)])\n\u0015\n. (9)\nHere \u03d5(t, \u03b8, x, c) can be instantiated using any neural network architecture taking 3D parameters,\nnoisy rendered image, and camera pose as the input.\nIn our experiments, we employ a pre-trained monocular depth estimator, MiDAS (Ranftl et al., 2020;\n2021), coupled with domain-specific loss functions to construct \u03d5(t, \u03b8, x, c), as a handy yet effective\nchoice. Specifically:\n\u03d5(t, x, \u03b8, c) = \u2212\u2113(\u03b1(\u03b8, c), MiDAS(x)).\n(10)\nHere MiDAS(\u00b7) can estimate either depth or normal map from noisy observation x. And \u03b1(\u00b7, \u00b7)\nis chosen as the corresponding depth or normal renderer of the 3D representation \u03b8, and \u2113(\u00b7, \u00b7) is\nthe Pearson correlation loss when estimating depth map or cosine similarity loss when considering\nnormal map.\nAs introduced in Sec. 2.2, there exists a closed-form \u00b5 that maximizes the variance reduction.\nHowever, it assumes the correlation between the control variate and the random variable of interest is\n5\nPreprint\nknown. Instead, we propose to directly optimize variance by adjusting \u00b5 to minimize the second-order\nmoment of Eq. 9 since its first-order moment is independent of \u00b5:\nmin\n\u00b5 Et,c,\u03f5\n\"\r\r\r\r\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(x|y) + \u00b5 \u2299 [\u03f5\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c)])\n\r\r\r\r\n2\n2\n#\n, (11)\nwhich essentially imposes a penalty on the gradient norm of \u03b8. We alternate between optimizing \u03b8\nand \u00b5 using SSD gradient in Eq. 9 and the objective function in Eq. 11, respectively. We refer to our\ncomplete text-to-3D framework as SteinDreamer, and its optimization paradigm is illustrated in Fig. 2.\nSpecifically, during each optimization iteration, SteinDreamer performs the following steps: 1) renders\nRGB map and depth/normal map from a random view of \u03b8, 2) perturbs the RGB map and obtains the\nscore estimation using a pre-trained diffusion model and monocular depth/normal prediction from\na pre-trained MiDAS, 3) computes \u03d5 via Eq. 10 and its gradient via auto-differentiation to form\ncontrol variate hSSD, 4) weights the control variate by \u00b5 and combine it with the diffusion score\n\u2207 log pt(x|y), 5) back-propagates \u2206SSD through the chain rule to update 3D parameters \u03b8. In the\nother fold, SteinDreamer keeps \u03b8 frozen and optimizes \u00b5 to minimize the \u21132 norm of the update\nsignals on \u03b8 according to Eq. 11.\n4.3\nDISCUSSION\nIn this section, we discuss a few merits of our Stein score distillation (SSD) in comparison to SDS\nand VSD. First, SDS is a special case of SSD when taking \u03d5(t, \u03b8, x, c) = \u22121. This observation\nsuggests the potential for SSD to provide a lower variance in gradient estimation due to its broader\nrange in representing control variates. As demonstrated in Oates et al. (2017), an optimal control\nvariate can be constructed using Stein\u2019s identity by carefully selecting \u03d5, achieving a zero-variance\nestimator. The key advantage of SSD lies in its flexibility in choosing the baseline function \u03d5, which\ncan directly condition and operate on all relevant variables. Furthermore, the expressive power of\nSSD surpasses that of VSD, in which \u2207 log qt(x|c) implicitly conditions on \u03b8 through x and c.\nKim et al. (2023) proposes Collaborative Score Distillation (CSD) to sample latent parameters via\nStein Variational Gradient Descent (SVGD). While both methods are grounded in Stein\u2019s method,\nthe underlying principles significantly differ. In CSD, the SVGD-based update takes the form of the\nStein discrepancy: max\u03d5\u2208F Ex\u223cq(x)[\u03d5(x)\u2207 log p(x) + \u2207x\u03d5(x)], where \u03d5(x) is often interpreted\nas an update direction constrained by a function class F (RBF kernel space in (Kim et al., 2023)).\nIn contrast, our update rule appends a zero-mean random variable via the Stein identity after the\nraw gradient of the KL divergence (Eq. 9), where \u03d5(x) typically represents a pre-defined baseline\nfunction. The potential rationale behind CSD to reducing variance lies in introducing the RBF kernel\nas a prior to constrain the solution space by modeling pairwise relations between data samples. Our\nSSD is centered around constructing a more general control variate that correlates with the random\nvariable of interest, featuring zero mean but variance reduction.\n5\nEXPERIMENTS\nWe conduct experiments for both object-level and scene-level text-to-3d generation. The text prompts\nutilized in the experiments are originally from ProlificDreamer. We mainly compare against the\nseminal works SDS from DreamFusion and VSD from ProlificDreamer. For a fair comparison, we\nutilize the open-source threestudio 2 as a unified benchmarking implementation. We thoroughly\ntest our proposed SteinDreamer with both depth estimator and normal estimator priors. All training\nhyper-parameters are kept the same with ProlificDreamer. For simplicity, we evaluate VSD with the\nparticle number set to one.\n5.1\nRESULTS AND ANALYSIS\nObject Centric Generation.\nWe put our qualitative results in Fig. 3a and Fig. 3b for SteinDreamer\nwith Depth or normal prior, respectively. Compared with SDS, our SteinDreamer presents novel\nviews with less over-saturation and over-smoothing artifacts. When comparing with VSD, not\n2https://github.com/threestudio-project/threestudio\n6\nPreprint\nA blue tulip\nA Car made out of sushi\nA lion fish\nSDS\nVSD\nSSD\n(a) Qualitative comparisons between SteinDreamer w/ depth estimator and existing methods.\nA matte painting of a castle made of cheesecake surrounded by a moat mode of ice cream\nA plush dragon toy\nMichelangelo style statue of dog reading news on a cellphone\nSDS\nVSD\nSSD\n(b) Qualitative comparisons between SteinDreamer w/ normal estimator and existing methods.\nFigure 3: Object-level comparisons. Compared to existing methods, our SteinDreamer delivers\nsmoother geometry, more detailed texture, and fewer floater artifacts.\nonly does our SteinDreamer generate smoother geometry, but also delivers sharper textures without\ncontamination of floaters. Additionally, it\u2019s worth noting that our SteinDreamer also alleviates the\nJanus problem. As shown in the dog statue case, there is only one face produced in our SteinDreamer\u2019s\noutput, while previous methods suffer from the multi-face issue. We further monitor the variance\n7\nPreprint\nA DSLR photo of a hamburger inside a restaurant\nInside of a smart home realistic detailed photo 4k\nSDS\nVSD\nSSD\n(a) Qualitative comparisons between SteinDreamer w/ depth estimator and existing methods.\nA DSLR photo of an icecream sundae inside a shopping mall\nSmall lavender isometric room soft lighting unreal engine render voxels\nSDS\nVSD\nSSD\n(b) Qualitative comparisons between SteinDreamer w/ normal estimator and existing methods.\nFigure 4: Scene-level comparisons between DreamFusion, ProlificDreamer, and SteinDreamer.\nCompared to existing methods, SteinDreamer presents more realistic textures with better details.\nfor all the demonstrated examples during the training stage in Fig. 5. Variance is estimated by 120\nrandomly sampled tuples of time steps, camera poses, and Gaussian noises. It is clear that our\nSteinDreamer consistently has lower variance than compared baselines throughout the course of\ntraining. Quantitative comparison can be found in Appendix C.2.\nLarge Scene Generation.\nWe further investigate the performance of our method and the\ncomparison baselines on a more challenging scenario for scene generation. We provide detailed\ncomparisons for 360\u00b0scene-level generation in Fig. 4a and Fig. 4b. SDS delivers blurry results with\nunrealistic colors and textures. The results from VSD suffer from the noisy background, and we\nalso observe that the VSD loss can diverge in the texture refining process (Fig. 4a ). In comparison,\nwe observe that results generated by SteinDreamer are much sharper in appearance and enjoy better\ndetails. More results are deferred to Appendix C.5.\n5.2\nABLATION STUDIES\nTo further validate the effectiveness of our proposed components, we conduct ablation studies on\nwhether or not to employ Eq. 11 to minimize second-order moment. The alternative candidate is to\nfix \u00b5 as all-one vector during training. As shown in Fig. 6, when explicitly minimizing the variance,\nwe reach cleaner results with better high-frequency signals. The results when optimizing without\n8\nPreprint\nLog Variance\nLog Variance\nTraining Steps (1e3)\nTraining Steps (1e3)\nTraining Steps (1e3)\nFigure 5: Variance comparison of \u2206SDS, \u2206V SD, and \u2206SSD. We visualize how the variance of the\ninvestigated three methods for every 1,000 steps. The variance decays as the training converges while \u2206SSD\nconsistently achieves lower variance throughout the whole process.\nSmall lavender isometric room, soft lighting, unreal engine render, voxels\nw/ Var. Minimization\nw/o Var. Minimization\nFigure 6: Ablation study on explicit variance\nminimization. We study the effect of turning\non/off the optimization step for \u00b5 with respect\nto loss Eq. 11.\nTraining Steps (1e3)\nCLIP Distance\n22%\n14%\na lionfish\nFigure 7: Convergence speed comparison.\nWith the help of more stable gradient updates,\nSteinDreamer accelerates the training process\nby 14%-22%.\nvariance minimization, on the other hand, turned out to generate blurry geometry and noisy textures.\nIt is worth mentioning that excessive variance reduction may smoothen out some necessary details,\nespecially in the background regions, as the left-hand side result contains more detailed background\ntextures than the right-hand side one.\n5.3\nCONVERGENCE SPEED\nWe also study the convergence speed of our methods as well as compared baselines. Specifically,\nwe use the average CLIP distance (Xu et al., 2022) between the rendered images and the input text\nprompts as the quality metric. During the training process, we render the 3D assets into multi-view\nimages every 1,000 training steps. In each training step, the diffusion model is inference twice\nthrough the classifier-free guidance, which is the same protocol in all compared methods. In Fig. 7,\nwe profile the training steps needed for each approach to reach 0.75 CLIP distance as a desirable\nthreshold. We observe that the proposed SteinDreamer can effectively attain rapid and superior\nconvergence, saving 14%-22% calls of diffusion models. This means that lower variance in our\ndistillation process can speed up convergence. Our SteinDreamer utilizes fewer number of score\nfunction evaluations to achieve distilled 3D results that are more aligned with the text prompts.\nMoreover, since SteinDreamer avoids inferencing and fine-tuning another diffusion model, each\niteration of SSD is approximately 30% faster than VSD (see Tab. 2).\n9\nPreprint\n6\nCONCLUSIONS\nIn this work, we present SteinDreamer, revealing a more general solution to reduce variance for\nscore distillation. Our Stein Score Distillation (SSD) incorporates control variates through Stein\nidentity, admitting arbitrary baseline functions conditioned on all relevant variables with any guidance\npriors. The experimental results suggest that SSD can effectively reduce the distillation variance and\nconsistently improve visual quality for both object- and scene-level generations. We also showcase\nthat SSD achieves faster and better convergence than existing methods with the help of more stable\ngradient updates.\nACKNOWLEDGMENTS\nP Wang sincerely appreciates insightful discussions with Zhaoyang Lv, Xiaoyu Xiang, Amit Kumar,\nJinhui Xiong, and Varun Nagaraja. P Wang also thanks Ruisi Cai for helping plot figures. Any\nstatements, opinions, findings, and conclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reflect the views of their employers or the supporting\nentities.\nREFERENCES\nRuojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and\nBharath Hariharan. Learning gradient fields for shape generation. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pp.\n364\u2013381. Springer, 2020.\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d\ngenerative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 16123\u201316133, 2022.\nJianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with\nvariance reduction. arXiv preprint arXiv:1710.10568, 2017.\nLouis HY Chen. Poisson approximation for dependent trials. The Annals of Probability, 3(3):\n534\u2013545, 1975.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023.\nChristine TH Davies, E Follana, A Gray, GP Lepage, Q Mason, M Nobes, J Shigemitsu, HD Trottier,\nM Wingate, C Aubin, et al. High-precision lattice qcd confronts experiment. Physical Review\nLetters, 92(2):022001, 2004.\nGianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, and Robert Scheichl. A\nstein variational newton method. Advances in Neural Information Processing Systems, 31, 2018.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021.\nGuillaume Garrigos and Robert M Gower. Handbook of convergence theorems for (stochastic)\ngradient methods. arXiv preprint arXiv:2301.11235, 2023.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nJackson Gorham and Lester Mackey. Measuring sample quality with stein\u2019s method. Advances in\nneural information processing systems, 28, 2015.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\n10\nPreprint\nLukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. Text2room:\nExtracting textured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989,\n2023.\nSusung Hong, Donghoon Ahn, and Seungryong Kim. Debiasing scores and prompts of 2d diffusion\nfor robust text-to-3d generation. arXiv preprint arXiv:2303.15413, 2023.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nYukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dreamtime: An\nimproved optimization strategy for text-to-3d content creation. arXiv preprint arXiv:2306.12422,\n2023.\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 867\u2013876, 2022.\nHeewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023.\nJames T Kajiya. The rendering equation. In Proceedings of the 13th annual conference on Computer\ngraphics and interactive techniques, pp. 143\u2013150, 1986.\nSubin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin.\nCollaborative score distillation for consistent visual editing. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 300\u2013309, 2023.\nHao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-depedent control\nvariates for policy optimization via stein\u2019s identity. arXiv preprint arXiv:1710.11198, 2017.\nQiang Liu. Stein variational gradient descent as gradient flow. Advances in neural information\nprocessing systems, 30, 2017.\nQiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference\nalgorithm. Advances in neural information processing systems, 29, 2016.\nQiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests.\nIn International conference on machine learning, pp. 276\u2013284. PMLR, 2016.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023.\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 12663\u201312673, 2023.\nSean Meyn. Control techniques for complex networks. Cambridge University Press, 2008.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European\nconference on computer vision, pp. 405\u2013421. Springer, 2020.\nThomas M\u00a8uller, Fabrice Rousselle, Alexander Keller, and Jan Nov\u00b4ak. Neural control variates. ACM\nTransactions on Graphics (TOG), 39(6):1\u201319, 2020.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics\nprimitives with a multiresolution hash encoding. arXiv preprint arXiv:2201.05989, 2022.\n11\nPreprint\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\nChris J Oates, Mark Girolami, and Nicolas Chopin. Control functionals for monte carlo integration.\nJournal of the Royal Statistical Society Series B: Statistical Methodology, 79(3):695\u2013718, 2017.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pp. 8821\u20138831. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nRen\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards\nrobust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE\ntransactions on pattern analysis and machine intelligence, 44(3):1623\u20131637, 2020.\nRen\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\nIn Proceedings of the IEEE/CVF international conference on computer vision, pp. 12179\u201312188,\n2021.\nGeoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-variance\ngradient estimators for variational inference. Advances in Neural Information Processing Systems,\n30, 2017.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nJ Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d\nneural field generation using triplane diffusion. arXiv preprint arXiv:2211.16677, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\npp. 2256\u20132265. PMLR, 2015.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\nCharles Stein. A bound for the error in the normal approximation to the distribution of a sum of\ndependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical\nStatistics and Probability, Volume 2: Probability Theory, volume 6, pp. 583\u2013603. University of\nCalifornia Press, 1972.\nRichard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT\npress Cambridge, 1998.\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods\nfor reinforcement learning with function approximation. Advances in neural information processing\nsystems, 12, 1999.\nChristina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari.\nTextmesh: Generation of realistic 3d meshes from text prompts. arXiv preprint arXiv:2304.12439,\n2023.\n12\nPreprint\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian\nchaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 12619\u201312629, 2023a.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023b.\nFrederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski, and Angjoo Kanazawa. Nerf-\nbusters: Removing ghostly artifacts from casually captured nerfs. arXiv preprint arXiv:2304.10532,\n2023.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8:229\u2013256, 1992.\nJiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a proba-\nbilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural\ninformation processing systems, 29, 2016.\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-\n360: Lifting an in-the-wild 2d photo to a 3d object with 360 {\\deg} views. arXiv preprint\narXiv:2211.16431, 2022.\nGuandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan.\nPointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the\nIEEE/CVF international conference on computer vision, pp. 4541\u20134550, 2019.\n13\nPreprint\nA\nDEFERRED DERIVATIONS\nGradient of KL divergence.\nLet \u03b8 parameterize the underlying 3D representation, such as NeRF\n(Mildenhall et al., 2020). We intend to optimize \u03b8 such that each view matches the prior of 2D\ndistribution. This can be formulated by minimizing the KL divergence below 3:\nmin\n\u03b8 Et,c\u223cp(c) DKL(qt(x|\u03b8, c)\u2225pt(x|y)),\n(12)\nwhere c is the camera pose sampled from a prior distribution, y is the user-specified text prompt, and\nqt(x|\u03b8, c) = N(x|\u03b1tg(\u03b8, c), \u03c32\nt I), where g(\u03b8, c) is a differentiable renderer that displays scene \u03b8\nfrom the camera angle c.\nTo optimize Eq. 12, we take the gradient in terms of \u03b8 and derive the following update formula:\n\u2207\u03b8 Et,c DKL(qt(x|\u03b8, c)\u2225pt(x|y)) = Et,c\u2207\u03b8DKL(qt(x|\u03b8, c)\u2225pt(x|y))\n(13)\n= Et,c \u2207\u03b8 Ex\u223cqt(x|\u03b8,c)\n\u0014\nlog qt(x|\u03b8, c)\npt(x|y)\n\u0015\n(14)\n= Et,c,\u03f5\u223cN (0,\u03c32\nt I)\n\uf8ee\n\uf8ef\uf8f0\u2207\u03b8 log qt(\u03b1tg(\u03b8, c) + \u03f5|\u03b8, c)\n|\n{z\n}\n(a)\n\u2212 \u2207\u03b8 log pt(\u03b1tg(\u03b8, c) + \u03f5|y)\n|\n{z\n}\n(b)\n\uf8f9\n\uf8fa\uf8fb\n(15)\nWe notice that qt(\u03b1tg(\u03b8, c) + \u03f5|\u03b8, c) = N(\u03f5|0, \u03c32\nt I), which is independent of \u03b8. Thus (a) = 0. For\nterm (b), we have:\n\u2207\u03b8 log pt(\u03b1tg(\u03b8, c) + \u03f5|y) = \u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(\u03b1tg(\u03b8, c) + \u03f5|y).\n(16)\nTherefore, \u03b8 should be iteratively updated by:\nEt,c,\u03f5\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(\u03b1tg(\u03b8, c) + \u03f5|y)\n\u0015\n= Et,c,x\u223cqt(x|\u03b8,c)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(x|y)\n\u0015\n(17)\nSDS equals to the gradient of KL.\nBy the following derivation, we demonstrate that SDS essen-\ntially minimizes the KL divergence: \u2206SDS = \u2207\u03b8 Et,c DKL(qt(x|\u03b8, c)\u2225pt(x|y)):\nEt,c,x\u223cqt(x|\u03b8,c)\n\u0014\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u2207 log pt(x|y) \u2212 \u03f5)\n\u0015\n(18)\n= Et,c,x\u223cqt(x|\u03b8,c)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(x|y)\n\u0015\n\u2212 Et,c,\u03f5\u223cN (0,\u03c32\nt I)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03f5\n\u0015\n|\n{z\n}\n=0\n.\n(19)\nVSD equals to the gradient of KL.\nWe show that VSD also equals to the gradient of KL \u2206V SD =\n\u2207\u03b8 Et,c DKL(qt(x|\u03b8, c)\u2225pt(x|y)) due to the simple fact that the first-order of score equals to zero:\nEt,c,x\u223cqt(x|\u03b8,c)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log qt(x|\u03b8, c)\n\u0015\n= Et,c,x\u223cqt(x|\u03b8,c) [\u2207\u03b8 log qt(x|\u03b8, c)]\n(20)\n= Et,c\n\u0014Z \u2207\u03b8qt(x|\u03b8, c)\nqt(x|\u03b8, c) qt(x|\u03b8, c)dx\n\u0015\n(21)\n= Et,c\n\u0014\n\u2207\u03b8\nZ\nqt(x|\u03b8, c)dx\n\u0015\n= 0.\n(22)\nControl Variate for SSD.\nDue to Stein\u2019s identity, the following is constantly zero:\nEx\u223cqt(x|\u03b8,c) [\u2207 log qt(x|\u03b8, c)\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c)] = 0.\n(23)\n3Without loss of generality, we intend to omit coefficients \u03c9(t) in all derivations for the sake of simplicity.\n14\nPreprint\nPlug into Eq. 17, we can obtain:\nEt,c,x\u223cqt(x|\u03b8,c)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(x|y)\n\u0015\n(24)\n= Et,c\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\nEx\u223cqt(x|\u03b8,c) [\u2207 log pt(x|y) + \u2207 log qt(x|\u03b8, c)\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c)]\n\u0015\n(25)\n= Et,c\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\nEx\u223cqt(x|\u03b8,c) [\u2207 log pt(x|y) + \u03f5\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c)]\n\u0015\n(26)\n= Et,c,\u03f5\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u2207 log pt(x|y) + \u03f5\u03d5(t, \u03b8, x, c) + \u2207x\u03d5(t, \u03b8, x, c))\n\u0015\n,\n(27)\nwhere Eq. 26 can be derived by noticing qt(x|\u03b8, c) follows from a Gaussian distribution.\nB\nDEFERRED DISCUSSION\nIn this section, we continue our discussion from Sec. 4.3.\nHow does baseline function reduce variance?\nThe baseline function \u03d5 can be regarded as a\nguidance introduced into the distillation process. We contend that control variates when equipped\nwith pre-trained models incorporating appropriate 2D/3D prior knowledge, are likely to exhibit a\nhigher correlation with the score function. Intuitively, enforcing priors and constraints on the gradient\nspace can also stabilize the training process by regularizing the optimization trajectory. Therefore,\nin our empirical design, the inclusion of geometric information expressed by a pre-trained MiDAS\nestimator is expected to result in superior variance reduction compared to SSD and VSD.\nComparison with VSD.\nIn VSD, the adopted control variate \u2207 log qt(x|c) is fine-tuned based\non a pre-trained score function using LoRA (Hu et al., 2021). However, this approach presents\ntwo primary drawbacks: 1) The trained control variate may not fully converge to the desired score\nfunction, potentially resulting in non-zero mean and biased gradient estimations. 2) Fine-tuning\nanother large diffusion model also significantly increases the computation expenses. Our SSD\neffectively circumvents these two limitations. Firstly, the control variate in SSD is provably zero-\nmean, as per Stein\u2019s identity. Additionally, the computational cost associated with differentiating the\nfrozen \u03d5 and optimizing the weights u remains manageable. We verify the computational efficiency\nof SSD in Appendix C.3.\nOther baseline functions.\nIt is noteworthy that baseline functions other than depth/normal predic-\ntors are also applicable. As we discussed above, choosing the right baseline functions can implicitly\nincorporate desirable prior information. Here we provide some tentative options for future exploration.\nA foreground-background segmenter coupled with a classification loss can be useful to mitigate\nthe artifacts of missing parts in generated 3D objects. A discriminator from a pre-trained GAN\n(Goodfellow et al., 2014) associated with the discriminative loss can be utilized to implement the\nbaseline function to improve the fidelity of each view. Similarly, CLIP loss (Jain et al., 2022) inducted\nbaseline function might help increase relevance with specified text. Multi-view prior such as Zero123\n(Liu et al., 2023) can be also introduced by sampling another view as a function of \u03b8 and comparing\nit with the current view x. Notably, our method supports freely combining all these aforementioned\nbaseline functions.\nC\nADDITIONAL EXPERIMENTS\nC.1\nIMPLEMENTATION DETAILS\nAll of them are implemented based on the threestudio framework. For fairness, we only compare\nthe results yielded in the coarse generation stage for object generation without geometry refinement\nspecified in ProlificDreamer. We employ hash-encoded NeRF (M\u00a8uller et al., 2022) as the underlying\n3D representation, and disentangle representations for foreground and background separately. All\n15\nPreprint\nMethods\n\u201cblue tulip\u201d\n\u201csushi car\u201d\n\u201clionfish\u201d\nSDS (Poole et al., 2022)\n0.777\n0.862\n0.751\nVSD (Wang et al., 2023b)\n0.751\n0.835\n0.749\nSSD w/ Depth (Ours)\n0.734\n0.754\n0.735\n\u201ccheesecake castle\u201d\n\u201cdragon toy\u201d\n\u201cdog statue\u201d\nSDS (Poole et al., 2022)\n0.902\n0.904\n0.789\nVSD (Wang et al., 2023b)\n0.843\n0.852\n0.775\nSSD w/ Normal (Ours)\n0.794\n0.806\n0.751\nTable 1: Quatitative results. We compare the CLIP distance (\u2193 the lower the better) of demonstrated\nresults among different approaches. Best results are marked in bold font. Prompts: \u201cblue tulip\u201d\nis short for \u201ca blue tulip\u201d, \u201csushi car\u201d for \u201ca car made out of sushi\u201d, \u201clionfish\u201d for \u201ca lionfish\u201d,\n\u201ccheesecake castle\u201d for \u201ca Matte painting of a castle made of cheesecake surrounded by a moat made\nof ice cream\u201d, \u201cdragon toy\u201d for \u201ca plush dragon toy\u201d, and \u201cdog statue\u201d for \u201cmichelangelo style statue\nof dog reading news on a cellphone\u201d.\nscenes are trained for 25k steps with a learning rate of 1e-2. At each iteration, we randomly sample\none view for supervision. We progressively increase rendering resolution from 64\u00d764 resolution to\n256\u00d7256 resolution after 5k steps. View-dependent prompting is enabled to alleviate Janus problem.\nOther hyperparameters are kept consistent with the default values.\nIn our implementation of SteinDreamer, the depth estimator is operated on images decoded from the\nlatent space. We further scale the baseline function by a coefficient 1e-2. The pre-trained MiDAS\nis based on a hybrid architecture of transformer and ResNet (Ranftl et al., 2021). We convert the\nestimated inverse depth to normal by directly taking spatial derivatives and normalization. Such\noperation is equivalent to the standard computation via normalizing spatial derivatives of non-inverse\ndepth. The rendered normal is analytically computed as the gradient of the density field. Additionally,\nwe reweight the pre-average loss map via the alpha mask rendered from NeRF. The weighting\ncoefficients \u00b5 are initialized with an all-one vector.\nC.2\nQUANTITATIVE RESULTS\nIn addition to the qualitative comparison in Fig. 3, we also provide a numerical evaluation of these\nresults in Tab. 1. Our observations indicate that SteinDreamer consistently outperforms all other\nmethods, which improves CLIP score by \u02dc0.5 over ProlificDreamer. This superior result suggests our\nflexible control variates are more effective than the one adopted by ProlificDreamer.\nC.3\nWALL-CLOCK TIME BENCHMARKING\nIn addition to faster convergence, we also test per-iteration wall-clock time for all methods. Results are\nlisted in Tab. 2. The reported numbers are obtained by averaging the running time of corresponding\nmethods with six prompts in Tab. 1 for 10k iterations on the same device. In summary, SteinDreamer\nexhibits comparable per-iteration speed to SDS while significantly outperforming VSD in terms of\nspeed. The trainable component \u00b5 in SSD comprises only thousands of parameters, which minimally\nincreases computational overhead and becomes much more efficient than tuning a LoRA in VSD.\nNotably, given that SSD can reach comparable visual quality in fewer steps, SteinDreamer achieves\nsignificant time savings for 3D score distillation.\nMethods\nSec. / Iter.\nSDS (Poole et al., 2022)\n1.063 \u00b1 0.002\nVSD (Wang et al., 2023b)\n1.550 \u00b1 0.004\nSSD w/ Depth (Ours)\n1.093 \u00b1 0.005\nSSD w/ Normal (Ours)\n1.087 \u00b1 0.004\nTable 2: Benchmarking wall-clock time. We test wall-clock time (seconds per iteration) for all\nconsidered methods.\n16\nPreprint\nSDS\nVSD\n25k\n35k\n25k\n35k\nFigure 8: Longer Training Results. We train high-variance score distillation approaches SDS and VSD for\nextra 10k steps. Prompts: \u201c car made out of sush\u201d for SDS and \u201ca lionfish\u201d for VSD\nC.4\nLONGER TRAINING FOR BASELINES\nA naive solution to achieve better convergence with high-variance gradient descent is to increase\ntraining steps. We test this hypothesis in this section by training SDS and VSD on two scenes with\n10k more steps. Qualitative results are presented in Fig. 8. We notice that longer training time cannot\nguarantee better convergence. We also quantitatively find that more optimization steps have negligible\ninfluence on the final CLIP scores, which float between 0.84 \u02dc0.86 for the prompt \u201c car made out of\nsush\u201d and 0.74 \u02dc0.75 for the prompt \u201ca lionfish\u201d.\nIn optimization theory, variance plays a crucial role in determining the convergence rate of SGD\nalgorithms (Garrigos & Gower, 2023) With a finite number of optimization steps and a standard\nlearning rate, maintaining low variance is pivotal to ensure convergence. Training with noisy\ngradients introduces high instability, potentially resulting in a suboptimal solution or even divergence,\nas illustrated in Fig. 4a.\nC.5\nMORE QUALITATIVE RESULTS\nWe demonstrate more results on scene generation in Fig. 9. Consistent with our observation in Sec.\n5, our method yields smooth and consistent renderings. Text prompts are sampled from Wang et al.\n(2023b) and H\u00a8ollein et al. (2023). We refer interested readers to our project page for video demos.\n(a)\n(c)\n(d)\n(b)\nFigure 9: More qualitative results by SteinDreamer. We visualize two views for each generated scene.\nPrompts: (a) \u201cA DSLR photo of a table with dim sum on it\u201d, (b) \u201cEditorial Style Photo, Eye Level, Coastal\nBathroom, Clawfoot Tub, Seashell, Wicker, Blue and White\u201d, (c) \u201cEditorial Style Photo, Wide Shot, Modern\nNursery, Table Lamp, Rocking Chair, Tree Wall Decal, Wood\u201d, (d) \u201cA library with tall bookshelves, tables,\nchairs, and reading lamps\u201d.\n17\n"
  }
]