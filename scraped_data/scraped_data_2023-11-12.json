[
  {
    "title": "LCM-LoRA: A Universal Stable-Diffusion Acceleration Module",
    "link": "https://arxiv.org/pdf/2311.05556.pdf",
    "upvote": "70",
    "text": "Technical Report\nLCM-LORA: A UNIVERSAL STABLE-DIFFUSION\nACCELERATION MODULE\nSimian Luo\u2217,1\nYiqin Tan\u2217,1\nSuraj Patil\u2020,2\nDaniel Gu\u2020\nPatrick von Platen2\nApolin\u00b4ario Passos2\nLongbo Huang1\nJian Li1\nHang Zhao1\n1 IIIS, Tsinghua University\n2 Hugging Face\n{luosm22, tyq22}@mails.tsinghua.edu.cn\n{suraj, patrick, apolinario}@huggingface.co\n{dgu8957}@gmail.com\n{longbohuang, lijian83, hangzhao}@tsinghua.edu.cn\nABSTRACT\nLatent Consistency Models (LCMs) (Luo et al., 2023) have achieved impres-\nsive performance in accelerating text-to-image generative tasks, producing high-\nquality images with minimal inference steps.\nLCMs are distilled from pre-\ntrained latent diffusion models (LDMs), requiring only \u223c32 A100 GPU train-\ning hours. This report further extends LCMs\u2019 potential in two aspects: First,\nby applying LoRA distillation to Stable-Diffusion models including SD-V1.5\n(Rombach et al., 2022), SSD-1B (Segmind., 2023), and SDXL (Podell et al.,\n2023), we have expanded LCM\u2019s scope to larger models with significantly less\nmemory consumption, achieving superior image generation quality. Second, we\nidentify the LoRA parameters obtained through LCM distillation as a universal\nStable-Diffusion acceleration module, named LCM-LoRA. LCM-LoRA can be\ndirectly plugged into various Stable-Diffusion fine-tuned models or LoRAs with-\nout training, thus representing a universally applicable accelerator for diverse\nimage generation tasks. Compared with previous numerical PF-ODE solvers such\nas DDIM (Song et al., 2020), DPM-Solver (Lu et al., 2022a;b), LCM-LoRA\ncan be viewed as a plug-in neural PF-ODE solver that possesses strong gen-\neralization abilities. Project page: https://github.com/luosiallen/\nlatent-consistency-model.\nCustomized LDM \ud835\udf3d\u2032\nLCM \ud835\udf3d!\"#\nStyle vector\n\ud835\udf49$ = \ud835\udf3d$ \u2212 \ud835\udf3d\ud835\udc1b\ud835\udc1a\ud835\udc2c(\nAcceleration vector\n\ud835\udf49\ud835\udc0b\ud835\udc02\ud835\udc0c = \ud835\udf3d\ud835\udc0b\ud835\udc02\ud835\udc0c \u2212 \ud835\udf3d\ud835\udc1b\ud835\udc1a\ud835\udc2c(\nCombination\n\ud835\udf49\ud835\udc0b\ud835\udc02\ud835\udc0c\n$\n= \ud835\udf40\ud835\udfcf\ud835\udf49$ + \ud835\udf40\ud835\udfd0\ud835\udf49\ud835\udc0b\ud835\udc02\ud835\udc0c\nCustomized LCM \ud835\udf3d!\"#\n$\nBase LDM \ud835\udf3d\ud835\udc1b\ud835\udc1a\ud835\udc2c\ud835\udc1e\n(LCM-LoRA)\n(Style-LoRA)\nStyle:\nFast:\nStyle:\nFast:\nStyle:\nFast:\nStyle:\nFast:\nFigure 1: Overview of LCM-LoRA. By introducing LoRA into the distillation process of LCM, we signif-\nicantly reduce the memory overhead of distillation, which allows us to train larger models, e.g., SDXL and\nSSD-1B, with limited resources. More importantly, LoRA parameters obtained through LCM-LoRA training\n(\u2018acceleration vector\u2019) can be directly combined with other LoRA parameters (\u2018style vetcor\u2019) obtained by fine-\ntuning on a particular style dataset. Without any training, the model obtained by a linear combination of the\nacceleration vector and style vetcor acquires the ability to generate images of a specific painting style in mini-\nmal sampling steps.\n\u2217Leading Authors\n\u2020Core Contributors\n1\narXiv:2311.05556v1  [cs.CV]  9 Nov 2023\nTechnical Report\n1\nINTRODUCTION\nLatent Diffusion Models (LDMs) (Rombach et al., 2022) have been pivotal in generating highly\ndetailed and creative imagery from various inputs such as text and sketches. Despite their success,\nthe slow reverse sampling process inherent to LDMs hampers real-time application, compromising\nthe user experience. Current open-source models and acceleration techniques have yet to bridge the\ngap to real-time generation on standard consumer GPUs. Efforts to accelerate LDMs generally fall\ninto two categories: the first involves advanced ODE-Solvers, like DDIM (Song et al., 2020), DPM-\nSolver (Lu et al., 2022a) and DPM-Solver++ (Lu et al., 2022b), to expedite the generation process.\nThe second strategy involves distillation of LDMs to streamline their functioning. The ODE-Solver\nmethods, despite reducing the number of inference steps needed, still demand a significant compu-\ntational overhead, especially when incorporating classifier-free guidance (Ho & Salimans, 2022).\nMeanwhile, distillation methods such as Guided-Distill (Meng et al., 2023), although promising,\nface practical limitations due to their intensive computational requirements. The quest for a balance\nbetween speed and quality in LDM-generated imagery continues to be a challenge in the field.\nRecently, Latent Consistency Models (LCMs) (Luo et al., 2023) have emerged, inspired by Consis-\ntency Models (CMs) (Song et al., 2023), as a solution to the slow sampling issue in image genera-\ntion. LCMs approach the reverse diffusion process by treating it as an augmented probability flow\nODE (PF-ODE) problem. They innovatively predict the solution in the latent space, bypassing the\nneed for iterative solutions through numerical ODE-Solvers. This results in a remarkably efficient\nsynthesis of high-resolution images, taking only 1 to 4 inference steps. Additionally, LCMs stand\nout in terms of distillation efficiency, requiring merely 32 A100 training hours for a minimal-step\ninference.\nBuilding on this, Latent Consistency Finetuning (LCF) (Luo et al., 2023) has been developed as\na method to fine-tune pre-trained LCMs without starting from the teacher diffusion model. For\nspecialized datasets\u2014like those for anime, photo-realistic, or fantasy images\u2014additional steps are\nnecessary, such as employing Latent Consistency Distillation (LCD) (Luo et al., 2023) to distill\na pre-trained LDM into an LCM or directly fine-tuning an LCM using LCF. However, this extra\ntraining can be a barrier to the quick deployment of LCMs across diverse datasets, posing the critical\nquestion of whether fast, training-free inference on custom datasets is attainable.\nTo answer the above question, we introduce LCM-LoRA, a universal training-free acceleration\nmodule that can be directly plugged into various Stable-Diffusion (SD) (Rombach et al., 2022) fine-\ntuned models or SD LoRAs (Hu et al., 2021) to support fast inference with minimal steps. Compared\nto earlier numerical probability flow ODE (PF-ODE) solvers such as DDIM (Song et al., 2020),\nDPM-Solver (Lu et al., 2022a), and DPM-Solver++ (Lu et al., 2022b), LCM-LoRA represents a\nnovel class of neural network-based PF-ODE solvers module. It demonstrates robust generalization\ncapabilities across various fine-tuned SD models and LoRAs.\n2\nRELATED WORK\nConsistency Models\nSong et al. (2023) have showcased the remarkable potential of consistency\nmodels (CMs), a novel class of generative models that enhance sampling efficiency without sacri-\nficing the quality of the output. These models employ a consistency mapping technique that deftly\nmaps points along the Ordinary Differential Equation (ODE) trajectory to their origins, thus en-\nabling expeditious one-step generation. Their research specifically targets image generation tasks\non ImageNet 64x64 (Deng et al., 2009) and LSUN 256x256 (Yu et al., 2015), demonstrating CMs\u2019\neffectiveness in these domains. Further advancing the field, Luo et al. (2023) has pioneered latent\nconsistency models (LCMs) within the text-to-image synthesis landscape. By viewing the guided\nreverse diffusion process as the resolution of an augmented Probability Flow ODE (PF-ODE), LCMs\nadeptly predict the solution of such ODEs in latent space. This innovative approach significantly re-\nduces the need for iterative steps, thereby enabling the rapid generation of high-fidelity images from\ntext inputs and setting a new standard for state-of-the-art performance on LAION-5B-Aesthetics\ndataset (Schuhmann et al., 2022).\nParameter-Efficient Fine-Tuning\nParameter-Efficient Fine-Tuning (PEFT) (Houlsby et al., 2019)\nenables the customization of pre-existing models for particular tasks while limiting the number of\n2\nTechnical Report\nparameters that need retraining. This reduces both computational load and storage demands. Among\nthe assorted techniques under the PEFT umbrella, Low-Rank Adaptation (LoRA) (Hu et al., 2021)\nstands out. LoRA\u2019s strategy involves training a minimal set of parameters through the integration of\nlow-rank matrices, which succinctly represent the required adjustments in the model\u2019s weights for\nfine-tuning. In practice, this means that during task-specific optimization, only these matrices are\nlearned and the bulk of pre-trained weights are left unchanged. Consequently, LoRA significantly\ntrims the volume of parameters to be modified, thereby enhancing computational efficiency and\npermitting model refinement with considerably less data.\nTask Arithmetic in Pretrained Models\nTask arithmetic (Ilharco et al., 2022; Ortiz-Jimenez et al.,\n2023; Zhang et al., 2023) has become a notable method for enhancing the abilities of pre-trained\nmodels, offering a cost-effective and scalable strategy for direct edits in weight space. By apply-\ning fine-tuned weights of different tasks to a model, researchers can improve its performance on\nthese tasks or induce forgetting by negating them. Despite its promise, the understanding of task\narithmetic\u2019s full potential and the principles that underlie it remain areas of active exploration.\n3\nLCM-LORA\n3.1\nLORA DISTILLATION FOR LCM\nThe Latent Consistency Model (LCM) (Luo et al., 2023) is trained using a one-stage guided dis-\ntillation method, leveraging a pre-trained auto-encoder\u2019s latent space to distill a guided diffusion\nmodel into an LCM. This process involves solving an augmented Probability Flow ODE (PF-ODE),\na mathematical formulation that ensures the generated samples follow a trajectory that results in\nhigh-quality images. The distillation focuses on maintaining the fidelity of these trajectories while\nsignificantly reducing the number of required sampling steps. The method includes innovations\nlike the Skipping-Steps technique to quicken convergence. The pseudo-code of LCD is provided in\nAlgorithm 1.\nAlgorithm 1 Latent Consistency Distillation (LCD) (Luo et al., 2023)\nInput: dataset D, initial model parameter \u03b8, learning rate \u03b7, ODE solver \u03a8(\u00b7, \u00b7, \u00b7, \u00b7), distance metric d(\u00b7, \u00b7),\nEMA rate \u00b5, noise schedule \u03b1(t), \u03c3(t), guidance scale [wmin, wmax], skipping interval k, and encoder E(\u00b7)\nEncoding training data into latent space: Dz = {(z, c)|z = E(x), (x, c) \u2208 D}\n\u03b8\u2212 \u2190 \u03b8\nrepeat\nSample (z, c) \u223c Dz, n \u223c U[1, N \u2212 k] and \u03c9 \u223c [\u03c9min, \u03c9max]\nSample ztn+k \u223c N(\u03b1(tn+k)z; \u03c32(tn+k)I)\n\u02c6z\u03a8,\u03c9\ntn\n\u2190 ztn+k + (1 + \u03c9)\u03a8(ztn+k, tn+k, tn, c) \u2212 \u03c9\u03a8(ztn+k, tn+k, tn, \u2205)\nL(\u03b8, \u03b8\u2212; \u03a8) \u2190 d(f\u03b8(ztn+k, \u03c9, c, tn+k), f\u03b8\u2212(\u02c6z\u03a8,\u03c9\ntn , \u03c9, c, tn))\n\u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8L(\u03b8, \u03b8\u2212)\n\u03b8\u2212 \u2190 stopgrad(\u00b5\u03b8\u2212 + (1 \u2212 \u00b5)\u03b8)\nuntil convergence\nSince the distillation process of Latent Consistency Models (LCM) is carried out on top of the\nparameters from a pre-trained diffusion model, we can consider latent consistency distillation as\na fine-tuning process for the diffusion model. This allows us to employ parameter-efficient fine-\ntuning methods, such as LoRA (Low-Rank Adaptation) (Hu et al., 2021). LoRA updates a pre-\ntrained weight matrix by applying a low-rank decomposition. Given a weight matrix W0 \u2208 Rd\u00d7k,\nthe update is expressed as W0 + \u2206W = W0 + BA, where B \u2208 Rd\u00d7r, A \u2208 Rr\u00d7k, and the rank\nr \u2264 min(d, k). During training, W0 is kept constant, and gradient updates are applied only to A and\nB. The modified forward pass for an input x is:\nh = W0x + \u2206Wx = W0x + BAx.\n(1)\nIn this equation, h represents the output vector, and the outputs of W0 and \u2206W = BA are added\ntogether after being multiplied by the input x. By decomposing the full parameter matrix into the\nproduct of two low-rank matrices, LoRA significantly reduces the number of trainable parameters,\nthereby lowering memory usage. Table 3.1 compares the total number of parameters in the full\n3\nTechnical Report\nLCM-LoRA-\nSD-V1.5\nLCM-LoRA-\nSDXL\nLCM-LoRA-\nSSD-1B\n4-Step Inference \nFigure 2: Images generated using latent consistency models distilled from different pretrained diffusion mod-\nels. We generate 512\u00d7512 resolution images with LCM-LoRA-SD-V1.5 and 1024\u00d71024 resolution images\nwith LCM-LoRA-SDXL and LCM-LoRA-SSD-1B. We use a fixed classifier-free guidance scale \u03c9 = 7.5 for\nall models during the distillation process. All images were obtained by 4-step sampling .\nmodel with the trainable parameters when using the LoRA technique. It is evident that by incorpo-\nrating the LoRA technique during the LCM distillation process, the quantity of trainable parameters\nis significantly reduced, effectively decreasing the memory requirements for training.\nModel\nSD-V1.5\nSSD-1B\nSDXL\n# Full Parameters\n0.98B\n1.3B\n3.5B\n# LoRA Trainable Parameters\n67.5M\n105M\n197M\nTable 1: Full parameter number and trainable parameter number with LoRA for SD-V1.5 (Rombach\net al., 2022), SSD-1B (Segmind., 2023) and SDXL (Podell et al., 2023).\nLuo et al. (2023) primarily distilled the base stable diffusion model, such as SD-V1.5 and SD-V2.1.\nWe extended this distillation process to more powerful models with enhanced text-to-image capa-\nbilities and larger parameter counts, including SDXL (Podell et al., 2023) and SSD-1B (Segmind.,\n2023). Our experiments demonstrate that the LCD paradigm adapts well to larger models. The\ngenerated results of different models are displayed in Figure 2.\n3.2\nLCM-LORA AS UNIVERSAL ACCELERATIION MODULE\nBased on parameter-efficient fine-tuning techniques, such as LoRA, one can fine-tune pretrained\nmodels with substantially reduced memory requirements. Within the framework of LoRA, the re-\nsultant LoRA parameters can be seamlessly integrated into the original model parameters. In Sec-\ntion 3.1, we demonstrate the feasibility of employing LoRA for the distillation process of Latent\nConsistency Models (LCMs). On the other hand, one can fine-tune on customized datasets for spe-\ncific task-oriented applications. There is now a broad array of fine-tuning parameters available for\nselection and utilization. We discover that the LCM-LoRA parameters can be directly combined\nwith other LoRA parameters fine-tuned on datasets of particular styles. Such an amalgamation\nyields a model capable of generating images in specific styles with minimal sampling steps, without\n4\nTechnical Report\nPaperCut LoRA\n+ LCM LoRA\n[Prompt-1]\nPaperCut LoRA\n[Prompt-1]\n2-Step\n4-Step\n8-Step\n16-Step\n32-Step\nPaperCut LoRA\n+ LCM LoRA\n[Prompt-2]\nPaperCut LoRA\n[Prompt-2]\nFigure 3: The generation results of the specific style LoRA parameters and the combination with LCM-\nLoRA parameters. We use SDXL as the base model. All images are 1024\u00d71024 resolution. We select LoRA\nparameters fine-tuned on specific painting style datasets and combine them with LCM-LoRA parameters. We\ncompare the quality of images generated by these models at different sampling steps. For the original LoRA\nparameters, we use DPM-Solver++ (Lu et al., 2022b) sampler and classifier-free guidance scale \u03c9 = 7.5.\nFor the parameters obtained after combining LCM-LoRA with specific style LoRA, we use LCM\u2019s multi-step\nsampler. We use \u03bb1 = 0.8 and \u03bb2 = 1.0 for the combination.\nthe need for any further training. As shown in Figure 1, denote the LCM-LoRA fine-tuned param-\neters as \u03c4LCM, which is identified as the \u201cacceleration vector\u201d, and the LoRA parameters fine-tuned\non customized dataset as \u03c4 \u2032, which is the \u201cstyle vector\u201d, we find that an LCM which generates\ncustomized images can be obtained as\n\u03b8\u2032\nLCM = \u03b8pre + \u03c4 \u2032\nLCM,\n(2)\nwhere\n\u03c4 \u2032\nLCM = \u03bb1\u03c4 \u2032 + \u03bb2\u03c4LCM\n(3)\nis the linear combination of acceleration vector \u03c4LCM and style vector \u03c4 \u2032. Here \u03bb1 and \u03bb2 are hy-\nperparameters. The generation results of the specific style LoRA parameters and their combination\nwith LCM-LoRA parameters are shown in Figure 3. Note that we do not make further training on\nthe combined parameters.\n4\nCONCLUSION\nWe present LCM-LoRA, a universal training-free acceleration module for Stable-Diffusion (SD).\nLCM-LoRA can serve as an independent and efficient neural network-based solver module to pre-\ndict the solution of PF-ODE, enabling fast inference with minimal steps on various finetuned SD\nmodels and SD LoRAs. Extensive experiments on text-to-image generation have demonstrated\nLCM-LoRA\u2019s strong generalization capabilities and superiority.\n5\nCONTRIBUTION & ACKNOWLEDGEMENT\nThis work builds upon Simian Luo and Yiqin Tan\u2019s Latent Consistency Models (LCMs) (Luo et al.,\n2023). Based on LCMs, Simian Luo wrote the original LCM-SDXL distillation code, and together\n5\nTechnical Report\nwith Yiqin Tan, primarily completed this technical report. Yiqin Tan discovered the arithmetic prop-\nerty of LCM parameters. Suraj Patil first completed the training of LCM-LoRA, discovering its\nstrong generalization abilities, and conducted most of the training. Suraj Patil and Daniel Gu con-\nducted excellent refactoring of the original LCM-SDXL codebase and improved training efficiency,\nseamlessly integrating it into the Diffusers library. Patrick von Platen revised and polished this tech-\nnical report, as well as integrating LCM into the Diffusers library. Longbo Huang, Jian Li, Hang\nZhao co-advised the original LCMs paper, and polished this technical report. We further thanks\nApolin\u00b4ario Passos and Patrick von Platen for making excellent LCMs demo and deployment. We\nalso want to thank Sayak Paul and Pedro Cuenca for helping with writing documentation as well as\nRadam\u00b4es Ajna for creating demos. We appreciate the computing resources provided by the Hugging\nFace Diffusers teams to support our experiments. Finally, we value the insightful discussions from\nLCM community members.\nREFERENCES\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning, pp. 2790\u20132799. PMLR, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,\nHannaneh Hajishirzi, and Ali Farhadi.\nEditing models with task arithmetic.\narXiv preprint\narXiv:2212.04089, 2022.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\nDpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast\nsolver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095,\n2022b.\nSimian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthe-\nsizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023.\nChenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and\nTim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 14297\u201314306, 2023.\nGuillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent\nspace: Improved editing of pre-trained models. arXiv preprint arXiv:2305.12827, 2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\n6\nTechnical Report\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLaion-5b:\nAn open large-scale dataset for training next generation image-text models.\narXiv preprint\narXiv:2210.08402, 2022.\nSegmind.\nAnnouncing\nssd-1b:\nA\nleap\nin\nefficient\nt2i\ngeneration.\nhttps://blog.segmind.com/introducing-segmind-ssd-1b/, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint\narXiv:2303.01469, 2023.\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:\nConstruction of a large-scale image dataset using deep learning with humans in the loop. arXiv\npreprint arXiv:1506.03365, 2015.\nJinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules\nwith arithmetic operations. arXiv preprint arXiv:2306.14870, 2023.\n7\n"
  },
  {
    "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "link": "https://arxiv.org/pdf/2311.05437.pdf",
    "upvote": "39",
    "text": "LLAVA-PLUS: LEARNING TO USE TOOLS FOR\nCREATING MULTIMODAL AGENTS\nShilong Liu\u2660\u2217, Hao Cheng\u2663, Haotian Liu\u2662\u2217, Hao Zhang\u2661\u2217, Feng Li\u2661\u2217,\nTianhe Ren\u0011, Xueyan Zou\u2662\u2217, Jianwei Yang\u2663, Hang Su\u2660, Jun Zhu\u2660,\nLei Zhang\u0011, Jianfeng Gao\u2663, Chunyuan Li\u2663 v\n\u2660Dept. of Comp. Sci. & Tech., Institute for AI, BNRist, Tsinghua University\n\u2663Microsoft Research, Redmond\n\u2662University of Wisconsin-Madison \u2661HKUST \u0011IDEA Research\n\u2217Work performed during an internship at Microsoft\nv Project Lead\nhttps://llava-vl.github.io/llava-plus/\nABSTRACT\nThis paper presents LLaVA-Plus (Large Language and Vision Assistants that\nPlug and Learn to Use Skills), a general-purpose multimodal assistant trained\nusing an end-to-end approach that systematically expands the capabilities of large\nmultimodal models (LMMs). LLaVA-Plus maintains a skill repository that contains\na wide range of vision and vision-language pre-trained models (tools), and is able to\nactivate relevant tools, given users\u2019 multimodal inputs, to compose their execution\nresults on the fly to fulfill many real-world tasks. To acquire the ability of using\ntools, LLaVA-Plus is trained on multimodal instruction-following data that we have\ncurated. The training data covers many tool use examples of visual understanding,\ngeneration, external knowledge retrieval and their compositions. Empirical results\nshow that LLaVA-Plus outperforms LLaVA in existing capabilities, and exhibits\nmany new capabilities. Compared with tool-augmented LLMs, LLaVA-Plus is\ndistinct in that the image query is directly grounded in and actively engaged\nthroughout the entire human-AI interaction sessions, significantly improving tool\nuse performance and enabling new scenarios.\n1\nINTRODUCTION\nA long-standing aspiration in artificial intelligence is to develop general-purpose assistants that\ncan effectively follow users\u2019 (multimodal) instructions to complete a wide range of real-world\ntasks (Askell et al., 2021; Li et al., 2023c). Recently, the community has witnessed a growing interest\nin developing foundation models with emergent abilities of multimodal understanding and generation\nin open-world tasks (Gan et al., 2022; Li et al., 2022). While the recipes of using Large Language\nModels (LLMs) such as ChatGPT (OpenAI, 2023a) to develop general-purpose assistants for natural\nlanguage tasks have been proved effective, the recipes of building general-purpose, multimodal\nassistants for computer vision and vision-language tasks remain to be explored.\nOngoing efforts of developing multimodal agents can be broadly categorized into two classes (Li\net al., 2023c): (i) End-to-end training with LLMs, where image-text data and multimodal instruction-\nfollowing data are collected to continually train LLMs to acquire the ability of processing visual\ninformation, resulting in a series of Large Multimodal Models (LMMs). Impressive visual under-\nstanding and reasoning performances have been demonstrated by both proprietary models such as\nFlamingo (Alayrac et al., 2022) and multimodal GPT-4 (OpenAI, 2023c), and open-sourced models\nsuch as LLaVA (Liu et al., 2023a) and MiniGPT-4 (Zhu et al., 2023). Although these end-to-end train-\ning methods are effective in helping LMMs to gain emergent abilities (such as in-context learning), it\nContact: liusl20@mails.tsinghua.edu.cn, {jfgao,chunyl}@microsoft.com\n1\narXiv:2311.05437v1  [cs.CV]  9 Nov 2023\nEditing\nConditional Gen.\nSemantic Seg\nObject Seg and Det\nbridge\nLake\nThe\ntext\n\"2024\n16-\nMONTH CALENDAR\" is\ndetected,\nsuggesting\nthat this calendar is a\n16-month calendar for\nthe year 2024.\nThe lake in question is \nLost Lake, which is \nlocated in Whistler, \nBritish Columbia, \nCanada.\nSearch\nOCR\nInput Image\nThe image depicts a wooden \npier extending into a large \nbody of water, with a bench \nplaced in the middle of the \npier. The scene is set against \na backdrop of trees and \nmountains, creating a tranquil \nand picturesque environment.\nLLaVA\nExternal Knowledge\nWho says cats can\u2018t \nbe outdoorsy? !\"\n#naturelover\nThis image is \npaw-some! \nSocial Media Post\nPoint to multi-level Seg\nBox to Seg\nStroke to Seg\nComposition\nVisual Understanding\nVisual Generation\nVisual Interaction\nFigure 1: Visual illustration of LLaVA-Plus\u2019 capabilities enabled by learning to use skills.\nremains challenging to develop a unified architecture that can seamlessly incorporate a wide range\nof skills, such as image segmentation and generation, which are crucial for real-world multimodal\napplications. (ii) Tool chaining with LLMs, where the prompts are meticulously crafted to enable\nLLMs (e.g., through LangChain lan (2022)) to invoke different tools (e.g., pre-trained vision models)\nto perform desired (sub-)tasks, without the need of additional model training. Some prominent works\ninclude VisProg (Gupta & Kembhavi, 2022), ViperGPT (Sur\u00b4\u0131s et al., 2023), Visual ChatGPT (Wu\net al., 2023), X-GPT (Zou et al., 2023a), and MM-REACT (Yang et al., 2023b). The strength of these\nmethods is the ability to perform a broad spectrum of visual tasks through the use of (new) tools,\nwhich can be incorporated into an AI agent with very low development cost. However, prompting\nis neither adaptable nor robust enough to allow multimodal agents to always accurately select and\nactivate appropriate tools (from a large and diverse toolset) and compose their results to generate final\nanswers on the fly for real-world multimodal tasks.\nIn this paper, we present LLaVA-Plus (Large Language and Vision Assistants that Plug and Learn\nto Use Skills), a general-purpose multimodal assistant that learns to use tools using an end-to-end\ntraining approach that systematically expands the capabilities of LMMs via visual instruction tuning.\nTo the best of our knowledge, this is the first attempt reported to combine the strengths of the\nend-to-end training and tool chaining methods mentioned above. LLaVA-Plus is equipped with a\nskill repository that contains a wide range of vision and vision-language tools. The design is an\nembodiment of the \u201cSociety of Mind\u201d scheme (Minsky, 1988), where each tool is originally designed\nfor a specific skill and by itself is only useful for specific scenarios, but the combinations of these\ntools lead to emergent abilities that show signs of higher intelligence. For example, LLaVA-Plus\nis able to construct a new workflow on the fly, given users\u2019 multimodal inputs, select and activate\nrelevant tools from the skill repository, and compose their execution results to fulfill many real-world\ntasks that are unseen during model training.\nLLaVA-Plus can be continually improved by incorporating new skills or tools via instruction tuning.\nConsider a new multimodal tool that has been developed for a specific scenario or skill. We collect\npertinent user instructions that request this tool and their execution results (or following) to form\ninstruction-following data for tuning. After instruction tuning, LLaVA-Plus expands its abilities as it\nlearns to use this new tool to deal with the tasks that it cannot handle before. LLaVA-Plus also differs\nfrom those existing works on teaching LLMs to use tools (e.g., Yang et al., 2023a; Patil et al., 2023),\nwhere visual signals are only used when the multimodal tools are activated. In contrast, LLaVA-Plus\nuses the raw visual signals through the entire human-AI interaction sessions to improve LMM\u2019s\nability of planning and reasoning.\nIn summary, our paper makes the following contributions:\n\u2022 New multimodal instruction-following tool use data. We present a new pipeline for curating\nvision-language instruction-following data, dedicated for tool use in human-AI interaction\nsessions, leveraging ChatGPT and GPT-4 as labeling tools.\n\u2022 New large multimodal assistant. We have developed LLaVA-Plus, a general-purpose multimodal\nassistant that extends LLaVA (Liu et al., 2023a) by incorporating a large and diverse set of\nexternal tools that can be selected, composed, and activated on the fly for performing tasks.\nAs shown in Figure 1, LLaVA-Plus significantly extends LMM\u2019s capabilities. Our empirical\n2\nstudy validates the effectiveness of LLaVA-Plus with consistently improved results on multiple\nbenchmarks, and in particular, new SoTA on VisiT-Bench with a diverse set of real-life tasks.\n\u2022 Open-source. We will release the following assets to the public: the generated multimodal\ninstruction data, the codebase, the LLaVA-Plus checkpoints, and a visual chat demo.\n2\nLEARNING TO USE TOOLS WITH VISUAL INSTRUCTION TUNING\nInspired by the impressive performance of multimodal GPT-4 and the open-source LMMs such as\nLLaVA/MiniGPT-4, the community has witnessed a surge in developing LMMs and the multimodal\ninstruction-following data, following the instruction tuning paradigm (e.g., Liu et al., 2023a; Peng\net al., 2023a). In this paper, we use LLaVA as a running example. But note that the proposed recipe\ncan be easily applied to other LMMs. Starting with a user input image query Iq, existing LMMs\nsuch as LLaVA typically accept a natural language instruction input Xq from the user, and output a\nnatural language response Xanswer. Therefore, we can use a unified scheme to represent multimodal\ninstruction-following data as:\nHuman : Iq <\\n> Xq<STOP> Assistant : Xanswer<STOP>,\n(1)\nwhere Human and Assistant are special role tokens, < \\n > and <STOP> are the line break\ntoken and sequence end token, respectively. It naturally covers any multimodal tasks that can be\nformulated as language-image input and language output, ranging from simple visual understanding\ntasks such as recognition, captioning, and visual question answering (VQA) to complex visual\nreasoning tasks. Due to its simplicity, the data pipeline is easy to construct and scale. By training\na single Transformer-based model with an auto-regressive objective, the resulting LMM enables a\nseamless human-assistant interaction, proficiently completing many visual tasks in the wild. However,\nit is limited in flexibility regarding skill expansion and engagement in human-AI interactions.\n2.1\nLLAVA-PLUS\nWe propose a modularized system architecture that allows an LMM, working as a planner, to learn to\nuse a wide range of skills at scale, and thus facilitating easy expansion of its capabilities and interface.\nSpecifically, we build a skill repository, where the LMM can leverage a broad range of existing\nvision and vision-language specialist models as tools for their respective skills when needed, to\ncomplete various tasks in the wild. The LMMs in most existing multimodal agents typically perform\nuser-oriented dialogues, where the LMMs are required to immediately respond to user instructions\nbased solely on the knowledge encoded in model weights, as shown in equation 1 and the left part\nof Figure 2. In addition to this, the LMM in LLaVA-Plus also performs skill-oriented dialogues,\nwhere the LMM initiates requests to call appropriate tools from the skill repository, and subsequently\naggregate the tool execution results after applying proper skills, as shown in the right part of Figure 2.\nAssistant\nHuman\nSkill\nRepository\n1\n2\n3\n4\nInstruction\nResponse\nTool Use\nTool Output\nFigure 2: The four-step LLaVA-Plus pipeline.\nA Full Dialogue of LLaVA-Plus.\nWe illustrate\nhow LLaVA-Plus works with a full dialogue session\nin Figure 2. It proceeds in four steps: 1\u20dd Humans\nprovide a task instruction Xq related to an image\nIq. 2\u20ddThe LMM-powered assistant analyzes both\nXq and Iq, and outputs Xskill use that chooses the\ntool from skill repository and writes the appropriate\nprompt as the tool argument. 3\u20dd By executing the\ntool, the result Xskill result is returned to the assistant. 4\u20dd The assistant aggregates Xskill result\nwith Xq and Iq, and outputs Xanwser to humans. The interaction can be represented as:\nHuman : Iq <\\n> Xq<STOP> Assistant : Xskill use<STOP>\nHuman : Xskill result<STOP> Assistant : Xanwser<STOP>\n(2)\nCompared with equation 1 which is used to train LLaVA, the only newly introduced component\nfor LLaVA-Plus training is the skill-oriented dialogue. Table 1 illustrates one sequence example of\ncalling detection and segmentation skills in human-AI interactions. LLaVA-Plus is trained with an\nauto-regressive objective on the sequence of equation 2, where only the green sub-sequences (or\ntokens) are used to compute the loss, and thus the model learns to predict skill use, answers, and\nwhen to stop.\n3\nHuman\n(Xq, Iq )\nCan you segment the girl and the cart in the image and describe\ntheir relationship?\n(The source image from COCO with ID 569536)\nAssistant\n(Xskill use)\n\u201cthoughts\u201d To accurately segment and identify the girl and the cart in the image, we can leverage\nthe grounding dino model for open-set detection and the segmentation model SAM. Integrating\nthese models will enable us to generate a comprehensive summary of the segmented subjects.\n\u201cactions\u201d [\u201dAPI name\u201d: \u201dgrounding dino+sam\u201d, \u201dAPI params\u201d: \u201dcaption\u201d: \u201dlittle girl and cart .\u201d]\n\u201cvalue\u201d I will use grounding dino+sam to help to answer the question. Please wait for a moment.\nSkill Repo.\nTool execution: Grounding DINO + SAM\nHuman\n(Xskill result )\ngrounding dino+sam model outputs: \u2019boxes\u2019: [[0.35, 0.37, 0.66, 0.97], [0.0, 0.57, 0.69, 1.0]],\n\u2019logits\u2019: [0.58, 0.41], \u2019phrases\u2019: [\u2019little girl\u2019, \u2019cart\u2019]\nCan you segment the girl and the cart in the image and describe their relationship?\nAssistant\n\u201cthoughts\u201d Thanks to the output of grounding dino+sam. I can answer the question better.\n(Xanwser)\n\u201cactions\u201d []\n\u201cvalue\u201d Sure! I segment and identify the little girl and the cart\nin the image.\n- Centered in the image, a little girl is discernible with bound-\ning box coordinates [0.35, 0.37, 0.66, 0.97]. She\u2019s seen actively\nmaneuvering a cart within the airport\u2019s ambiance.\n- To the bottom left of the image, a cart can be pinpointed,\nbounded by the coordinates [0.0, 0.57, 0.69, 1.0]. It\u2019s laden with\nvarious luggage items, indicating travel preparations or arrivals.\nThe interaction between the girl and the cart epitomizes a typical airport scenario: a traveler\nutilizing the cart to facilitate her movement with luggage. This tableau, whether signifying an\narrival or departure, encapsulates the vibrancy and expectation inherent to air travel.\nTable 1: An example of a LLaVA-Plus workflow that plugs and learns to use the skills of object\ndetection and segmentation, enhanced by a rich region language description. The gray text is not in\nthe training sequence.\nUnified Prediction Format from LMMs.\nFigure 2 shows that the LMM of LLaVA-Plus needs to\nperform both user-oriented and skill-oriented dialogues. To this end, we use a unified model prediction\nformat to represent dialogues with and without the need of calling the skill repository. Inspired by Yao\net al. (2022), the format consists of three fields, as illustrated in Table 1: (i) Thought is a text\nsequence representing a reasoning process, which determines whether the skill repository is needed\nto follow the user instruction, and if so, which tools to use. (ii) Action is a list of function calls for\nthe tools to execute the thought. The list is in the JSON format, with each item consisting of two\nsub-fields: API name to call the tool and API params for the corresponding function arguments\nif applicable. When action is an empty list, no skill is invoked. (iii) Value is a natural language\nresponse that LLaVA-Plus generates by aggregating tool execution results and the human-AI session\nhistory. When presented in Xskill use of user-oriented dialogues, it is the final response returned\nto human users. When presented in Xanwser of skill-oriented dialogues, it is a natural language\ndescription about tool execution. In the serving stage, we find it important to ensure a good user\nexperience that we only return the content in the value field of Xanwser to human users, but hide\nthe entire skill-oriented dialogues unless we need to debug the system.\n2.2\nSKILL REPOSITORY: MULTIMODAL TOOL USE INSTRUCT DATA GENERATION\nThe skill repository of LLaVA-Plus consists of multimodal tools of different skills. To allow the\nLMM to always activate the most appropriate tools to complete a task, the corresponding tool-use\nmultimodal instruction-following data is needed for LMM tuning. We follow the self-instruct method\nto curate the data by using GPT-4 as the labeler. Without loss of generality, in this study we want\nLLaVA-Plus to deal with the scenarios that requires novel skills that LLaVA does not have, e.g.,\nthe individual skills for visual understanding, generation, and external knowledge retrieval and the\ncompositions of these individual skills, as summarized in Table 2. In what follows, we treat visual\nunderstanding skills as core skills and the others as extended skills, and describe the way instruction\ndata is curated.\n4\nSkills\nTools\nSource\nSize\nIndividual Skills\nUnderstanding\nDetection/Grounding\nG-DINO (Liu et al., 2023b)\nCOCO\n13783\nSemantic Segmentation\nOpenSeeD (Zhang et al., 2023a)\nCOCO\n5989\nInstance Segmentation\nG-DINO+SAM\nCOCO\n5228\nCaption + Grounding\nBLIP2+G-DINO\nCOCO\n4037\nTagging + Grounding\nRAM+G-DINO\nCOCO\n4439\nCaption\nBLIP2 Li et al. (2023e)\nCOCO\n4064\nTagging\nRAM (Zhang et al., 2023d)\nCOCO\n6045\nOCR\nEasyOCR (JaidedAI, 2023)\nHiertext\n6528\nExternal Knowledge\nRetrieval\nCLIP Retrieval (Radford et al., 2021)\nInfoSeek\n4087\nGeneration\nImage Generation\nStable Diffusion (Rombach et al., 2021)\nJourneyDB\n4694\nImage Editing\nInstruct P2P (Brooks et al., 2023)\nInstruct P2P\n6981\nVisual Prompt\nInteractive Segmentation\nSAM (Kirillov et al., 2023)\nCOCO\n5601\nMulti-granularity\nSemantic SAM (Li et al., 2023d)\nCOCO\n5601\nExample Based Segmentation\nSEEM (Zou et al., 2023b)\nCOCO\n5601\nComposed Skills\nMix of Detection, Segmentation, Tagging, Caption\nG-DINO, SAM, BLIP2, RAM\nCOCO\n37,431\nInteractive Segmentation + Inpainting\nSAM + Stable Diffusion\nCOCO\n3063\nSemantic Segmentation + Generation\nOpenSeeD + ControlNet (Zhang et al., 2023b)\nCOCO\n5989\nImage Generation + Social Media Post\nStable Diffusion\nJourneyDB\n4694\nImage Editing + Social Media Post\nInstruct P2P Brooks et al. (2023)\nInstruct P2P\n5924\nTable 2: LLaVA-Plus skill repository and dataset statistics of our created visual instruction-following\ndata for each tool use case. G-DINO indicates Grounding DINO (Liu et al., 2023b). HierText (Long\net al., 2022; 2023), InfoSeek (Chen et al., 2023b), and JourneyDB (Pan et al., 2023) are datasets for\nOCR, external knowledge, and image generation, respectively.\n2.2.1\nCORE SKILLS: UNDERSTANDING\nVisual understanding skills enable machines to interpret and comprehend visual signals. Existing\nLMMs have only a limited subset of visual understanding skills, constrained by language inputs\nand outputs. We expand them to a broader skill set with visual input prompts and visual outputs,\nincluding open-set detection and grounding, semantic/instance/interactive segmentation, tagging,\ncaptioning, OCR and their compositions, and so on. These understanding skills can be grouped into\ntwo categories, depending on whether additional function arguments are required.\nSkills with Image-only. The skills without additional function arguments include captioning, tagging,\nsemantic segmentation, caption+grounding, tagging+grounding, and OCR. We have curated training\nsamples for each tool individually. To collect the training samples for a given skill, we fill in the\nfour data variables in equation 2 using different strategies. (i) For Xq, we use GPT-4 to generate a\nset of instructions that require the use of tools for proper answers. For each sample, we randomly\nselect a question and rewrite it to enhance data diversity. An rewriting example is shown in Table 9\nin Appendix. (ii) For Xskill use, its thoughts and value are generated by randomly selecting\nfrom some preset responses with rewriting. The actions is known, so it can be directly assigned.\n(iii) Xskill result is generated with a fixed rule: first presenting the tool outputs and then repeating\nthe initial question. (iv) For Xanwser, its thoughts is created in a similar way to thoughts in\nXskill use, and action is set empty. The value of Xanwser is the most important field, as it is the\nvisible response to humans in chat. We feed all previous information, including previous questions,\nthe previous tool outputs, and context of the image to language-only GPT-4, which then generates\nresponses to form instruction-following data. Inspired by LLaVA, we consider the ground-truth\ncaptions, object coordinates, and object categories as image contexts.\nSkills with Additional Function Arguments. Visual skills such as object detection and instance\nsegmentation often require humans to provide very specific instructions regarding the concepts of\ninterests. Their instruction-following data is more challenging to create. We use two methods in\nthis study. (i) The first method is similar to that in the image-only skill setting, where the initial\nXq contains a placeholder concept, one or more categories presented in the image are randomly\nchosen to replace this placeholder, and the final Xq is obtained via rewriting, as shown in Table 9.\n(ii) To allow the LMM to learn more diverse prompts beyond category information, we use GPT-4 to\ngenerate questions. Specifically, we manually create two seed samples following the full dialogue\nin equation 2, send them, together with image contexts, to GPT-4, and ask GPT-4 to generate a full\ndialogue based on a new image context. An example is shown in Table 10 in Appendix.\n5\n2.2.2\nEXTENDED SKILLS\nThe LLaVA-Plus recipe can be applied to any tools to improve the system capabilities. We demonstrate\nits versatility by onboarding multimodal tools of different categorizes. Due to the limited space, we\ndescribe the instruction-following data creation process in Section B in Appendix, and summarize\nthe extended skills we have enabled.\n\u2022 External Knowledge. To enable LMMs to use knowledge beyond that encoded in pre-trained\nmodel weights, we use the CLIP search API to retrieve external knowledge from LIAON.\n\u2022 Generation.\nTo allow LLaVA-Plus to output images, we use Stable Diffusion (SD) and\nInstruct-Pix2Pix for image generation and editing, respectively.\n\u2022 Visual Prompts. To better follow human intents, we support various visual prompts for human-\nAI interaction, such as user-drawn points, sketches and boxes. SAM, Semantic-SAM and SEEM\nare used for different interactive segmentation tasks.\n\u2022 Skill Composition. To allow LLaVA-Plus to deal with real-world compositional tasks. We\ncurate data for the following scenarios: (i) The scenarios where various visual understanding\nresults of the same image in a multi-turn human-AI interaction session are required. We generate\ninstruction data by applying different tools (including detection, segmentation, tagging, and\ncaptioning). (ii) Interactive Segmentation + Inpainting. By combining the SAM segmentation\nresults from the user pointing and SD, we enable inpainting with visual interaction. (iii)\nSemantic Segmentation + Generation. By combining the spatial layout from OpenSeed semantic\nsegmentation and ControlNet, we enable instructional visual-conditioned generation. (iv) Image\nGeneration/Editing + Social Media Post. It is time-consuming for human users to generate posts\nthat contains both images and text. Thus, we use SD to generate an image, or Instruct Pix2Pix\nto edit an image, then combine the image with its description generated by a pre-trained LMM\nto create a multimodal post.\n2.3\nMODEL TRAINING AND SERVING\nTraining.\nTo train LLaVA-Plus, we combine the curated tool use instruction data, as shownin\nTable 2, with the LLaVA-158K dataset. To convert LLaVA-158K into the unified prediction format\nas described in Section 2.1, we treat the responses in LLaVA-158K as value, and add the fields\nof thoughts and actions with templates, as illustrated in the example in Table 8 in Appendix.\nLLaVA-Plus are built in two settings. (i) LLaVA-Plus (All Tools), where tool use is cast as external\nknowledge. All visual understanding tools except segmentation in Table 2 are utilized to process the\ninput image, and the extracted recognition results are organized as symbolic sequence representations\nto enrich the image features in both the training and evaluation stages. (ii) LLaVA-Plus (Fly), where\ntools are used on the fly. To reduce the cost of calling all tools, we only provide the execution results\nof related tools for a given instruction. When reporting quantitative numbers, we train models on the\n81K understanding instruction data, because existing benchmarks focus mainly on understanding\ncapabilities. When building demo systems, we train our models on the full dataset.\nServing.\nLLaVA-Plus is served using the FastChat (Vicuna, 2023) system, which is composed of\nweb servers that interface with humans, model workers that host the LMM and multiple tools, and a\ncontroller to coordinate the web-server and model workers. The 7B LLaVA-Plus and all the tools can\nbe loaded and served in a 80G GPU.\n3\nRELATED WORKS\nWe summarize the connections and differences between LLaVA-Plus and existing general-purpose\nmultimodal systems in Table 3, where only representative methods are shown due to space constraint.\nThey can be broadly categorized into two classes as discussed below.\nAI Agents with Multimodal Tool Use.\nThere is a growing interest in exploring a paradigm of\nbuilding general-purpose AI agents that synergistically leverage multiple tools with LLMs to solve\nsophisticated, open-world problems. The idea is originated in NLP to invoke general tools whose skills\nare lacked from LLM (e.g., ToolFormer (Schick et al., 2023), ChatGPT-Plugin (OpenAI, 2023b)), and\nis recently extended to the multimodal space. There are two ways to leverage multimodal tools with\nthe LLM as a planner to determine which tools to invoke: (i) tool chaining by prompt engineering\n6\nCapabilities\nImage Understanding\nKnowledge\nImage Gen.\nVisual Interaction\nCombined\nToo Use\nInput\n(Text, Image)\n(Point, Box)\nAll\nAllocator\nTraining\nOutput\nText\nBox\nMask\nText\nImage\n(Text, Image, Mask)\nAll\nMM-REACT\n\u2713\n\u2713\n\u2713\nLLM\nGPT4Tools\n\u2713\n\u2713\n\u2713\n\u2713\nLLM\n\u2713\nLLaVA-Plus\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nLMM\n\u2713\nLLaVA/GPT-V\n\u2713\nKosmos-2\n\u2713\n\u2713\nCM3Leon\n\u2713\n\u2713\n\u2713\n\u2713\nTable 3: Comparison with existing multimodal systems. The empty cells indicate inapplicable.\n\u201cAllocator\u201d indicates which base model is used to invoke the tools, and \u201cTraining\u201d indicates whether\nmodel training is needed to enable tool use.\nand in-context-learning, such as Visual ChatGPT (Wu et al., 2023), MM-ReAct (Yang et al., 2023b),\nand (ii) instruction tuning of LLM with a focus on multimodal tool use, such as GPT4Tools (Yang\net al., 2023a) and Gorilla (Patil et al., 2023). LLaVA-Plus represents the first work of utilizing the\nLMM as the planner for tool use, where image inputs are considered throughout the entire interaction\nsessions for improved user experience.\nUnified Multimodal Models with Versatile Capabilities.\nInspired by the success of a unified\narchitecture of LLMs to complete many language tasks, the AI community has witnessed an increasing\ninterest in building unified models with versatile multimodal capabilities. Proprietary models such\nas Flamingo (Alayrac et al., 2022) and multimodal GPT-4 (OpenAI, 2023c) (or GPT-4V (OpenAI,\n2023d)) have demonstrated strong multimodal performance on zero-shot task transfer, which quickly\ninspired their open-source counterparts: LLaVA, MiniGPT-4, Open-Flamingo (Awadalla et al., 2023),\nOtter (Li et al., 2023a), to name a few. These LMMs can deal with the tasks with image-text input\nand text output. The capabilities have been extended to support the tasks with image-text output, such\nas image editing and segmentation, as demonstrated in CM3Leon (Yu & et al, 2023), Emu (Sun et al.,\n2023), and GILL (Koh et al., 2023). Bounding box outputs for grounding are recently supported, as\nshown in Kosmos-2 (Peng et al., 2023b), Shikra (Chen et al., 2023a) and DetGPT (Pi et al., 2023).\nGPT4ROI (Zhang et al., 2023c) allows users to select regions of interest with bounding boxes for\nhuman-AI visual chat. BubaGPT (Zhao et al., 2023) and LISA (Lai et al., 2023) use an extra referring\nsegmentation model to enable the mask prediction capability. Compared with them, LLaVA-Plus\nenables a much wider range of multimodal skills and their compositions, as illustrated in Table 3.\n4\nEXPERIMENTS\n4.1\nTHE EFFECTIVENESS OF LEARNING TO USE SKILLS\nTool Use Improves Existing Capabilities.\nWe consider two benchmarks. LLaVA-Bench (Liu\net al., 2023a) evaluates the visual chat of LMMs, with three types of questions: conversation, detailed\ndescription and visual reasoning. It consists of two datasets: the COCO set containing 30 COCO\nimages and 90 chat questions, and the In-the-Wild set containing 24 web images with 60 questions.\nLanguage GPT-4 (gpt4-0314) is used to score the generated answers. The relative scores between\nthe model output and gold response are reported. SEED-Bench (Li et al., 2023b) evaluates the\nimage-level and instance-level perception and reasoning of LMMs, with 19K multi-choice questions.\nThe results are shown in Table 4. Both LLaVA-Plus variants outperform LLaVA on these two\nbenchmarks, demonstrating the effectiveness of adding visual recognition results of applying new\nskills in the LMM pipeline. LLaVA-Plus (All Tools) shows superior performance to LLaVA-Plus\n(Fly) because the former leverages more tools as additional contexts. We further conducted several\nablations: (i) We tried to directly add the skill execution results in the testing stage of LLaVA, shown\nas the row of LLaVA (Tools in Test). The degraded performance compared with LLaVA demonstrates\nthe necessity of learning to use skills in training. (ii) We removed thoughts in the unified data\nformat and observed a performance drop, indicating chain-of-thoughts style data format is beneficial.\n(iii) GPT4Tools trains an LLM for multimodal tool use. Its lower performance indicates that visual\ninstruction tuning of tool use in LLaVA-Plus is important.\nLLaVA-Bench (Tools).\nTo study the novel capabilities enabled by learning to use skills, we create\nan evaluation set LLavA-Bench (Tools), which measures four capabilities (grounding, tagging,\ncaption, and OCR) with 10, 12, 12, and 10 samples in each. In Table 5, we also compare against\n7\nLLaVA-Bench (COCO)\nLLaVA-Bench (In-the-Wild)\nConv.\nDetail\nReasoning\nAll\nConv.\nDetail\nReasoning\nAll\nLLaVA\n82.0\n69.1\n92.6\n81.2\n42.6\n51.9\n68.9\n57.1\nLLaVA (Tools in Test)\n56.2\n67.9\n53.3\n59.1\n40.7\n48.1\n51.2\n47.5\nLLaVA-Plus (All Tools)\n81.6\n74.5\n95.7\n83.9\n65.5\n56.8\n79.1\n69.5\nLLaVA-Plus (Fly)\n76.2\n72.2\n92.3\n80.4\n45.2\n50.4\n72.6\n59.1\nLLaVA-Plus (Fly) (no thoughts)\n76.6\n70.4\n90.7\n79.4\n38.8\n39.8\n59.8\n48.7\nGPT4Tools\n75.3\n53.8\n86.9\n72.1\n31.1\n27.1\n54.1\n40.7\n(a) LLaVA-Bench.\nScene\nIdentity\nAttribute Location Counting Spatial\nInteract. Reason. Text\nAverage\nLLaVA\n59.50\n54.29\n56.06\n42.54\n39.35\n33.03\n43.30\n41.39\n30.59\n44.45\nLLaVA (Tools in Test)\n67.13\n56.85\n45.24\n47.24\n45.69\n40.18\n60.82\n70.09\n30.59\n51.54\nLLaVA-Plus (All Tools)\n68.94\n56.80\n58.89\n47.34\n48.14\n45.21\n60.82\n71.30\n37.65\n55.01\nLLaVA-Plus (Fly)\n68.43\n56.47\n59.69\n45.40\n41.68\n44.14\n59.79\n69.49\n34.12\n53.25\n(b) SEED-Bench.\nTable 4: LLaVA-Plus variants improves LLaVA on two LMM benchmarks.\nGrounding Tagging Caption OCR\nAll\nLLaVA\n47.1\n87.1\n77.0\n23.6\n58.7\nLLaVA (Tools in Test)\n41.7\n48.5\n72.0\n31.9\n48.5\nLLaVA-Plus (All Tools)\n89.3\n94.4\n96.7\n48.8\n82.3\nLLaVA-Plus (Fly)\n88.6\n88.9\n90.2\n38.4\n76.5\nBard (0730)\n36.5\n105.3\n103.3\n60.0\n76.3\nBing Chat (0730)\n56.0\n84.0\n96.0\n44.8\n70.2\nMM-REACT\n30.2\n94.7\n103.8\n77.3\n76.5\nAll Tools + GPT4\n77.5\n95.6\n95.2\n39.3\n76.9\nTable 5: LLaVA-Bench (Tool Use).\nthe commercial visual chat systems such as Microsoft BingChat and Google Bard. LLaVA-Plus\nsignificantly outperforms the others on this benchmark, mainly because the other systems are not\nequipped with some of these capabilities. By comparing with chaining tools with GPT-4 (row of \u201cAll\ntools + GPT4\u201d) and MM-REACT, we demonstrate the advantage of training an open-source LMM as\na planner for tool use.\n4.2\nCOMPARISONS WITH SOTA LMM SYSTEMS\nMMVet (Yu et al., 2023) contains 200 images and 218 questions, aiming to evaluate six core\nvision-language (VL) capabilities and their combinations. For evaluation, an LLM-based evaluator\n(gpt4-0613) is used to score open-ended outputs of different forms. The results are reported\nin Table 6. LLaVA-Plus consistently outperforms LLaVA on both 7B and 13B model sizes. The\ncategories with most significant improvements are OCR and spatial, indicating the positive impact of\nthe corresponding visual skills on LMM outputs.\nVisIT-Bench (Bitton et al., 2023) is a real-world use oriented LMM benchmark, comprising 592\nquestions and 1,159 public images categorized into 70 instruction families. The results are shown in\nTable 7, which summarizes the battles between LMMs with GPT-analog human judgment. Elo ratings\nare computed by treating each pairwise human judgment as a \u201cmatch\u201d. The difference between the\nElo ratings of two models provides an estimate for the win probability when pitting model A vs.\nmodel B. The \u201c#matches\u201d column indicates the number of total matches in which a particular model\nparticipates. Win-rate indicates the win rate of a model against the human-verified reference outputs.\nLLaVA-Plus significantly outperforms the leading method LLaVA by 100+ ELO score, achieving a\nnew SoTA on the leaderboard.\n4.3\nVISUAL EXAMPLES OF NEW CAPABILITIES\nIn Table 3, we illustrate new capabilities of LLaVA-Plus with visual examples. Please see Section D\nin Appendix for many other interesting scenarios that demonstrate the versatile capabilities of\nLLaVA-Plus by learning to use skills and their compositions.\nIn the left example, the questions require identifying the precise object locations. LLaVA-Plus\ncan successfully detect the frisbee\u2019s coordinates, which help determine its status of flying in the\n8\nModel\nRec\nOCR\nKnowledge\nGeneration\nSpatial\nMath\nTotal\nResults of various open-source LMM on reported in the MM-VET paper (Yu et al., 2023)\nOpenFlamingo-9B (Awadalla et al., 2023)\n24.6\n14.4\n13.0\n12.3\n18.0\n15.0\n21.8\u00b10.1\nBLIP-2-12B (Li et al., 2023e)\n27.5\n11.1\n11.8\n7.0\n16.2\n5.8\n22.4\u00b10.2\nLLaVA-7B (Liu et al., 2023a)\n28.0\n17.1\n16.3\n18.9\n21.2\n11.5\n23.8\u00b10.6\nMiniGPT-4-14B (Zhu et al., 2023)\n29.9\n16.1\n20.4\n22.1\n22.2\n3.8\n24.4\u00b10.4\nOtter-9B (Li et al., 2023a)\n28.4\n16.4\n19.4\n20.7\n19.3\n15.0\n24.6\u00b10.2\nInstructBLIP-14B (Dai et al., 2023)\n30.8\n16.0\n9.8\n9.0\n21.1\n10.5\n25.6\u00b10.3\nMM-ReAct-GPT-3.5 (Yang et al., 2023b)\n24.2\n31.5\n21.5\n20.7\n32.3\n26.2\n27.9\u00b10.1\nLLaMA-Adapter v2-7B (Gao et al., 2023)\n32.9\n20.1\n19.0\n20.1\n22.9\n3.9\n31.4\u00b10.1\nLLaVA-13B (V1.3, 336px) (Liu et al., 2023a)\n38.1\n22.3\n25.2\n25.8\n31.3\n11.2\n32.5\u00b10.1\nMM-ReAct-GPT-4 (Yang et al., 2023b)\n33.1\n65.7\n29.0\n35.0\n56.8\n69.2\n44.6\u00b10.2\nResults with our own experiment runs\nLLaVA-7B\n30.4\n13.3\n19.2\n20.1\n18.7\n8.1\n24.1\u00b10.0\nLLaVA-Plus-7B (All Tools)\n30.5\n23.6\n20.5\n22.5\n28.5\n7.7\n27.5\u00b10.3\nLLaVA-Plus-13B (All Tools, V1.3, 336px)\n37.5\n29.4\n22.3\n24.5\n37.3\n11.5\n35.0\u00b10.0\nTable 6: Performance of various open-source LMM on MM-VET. Note that MM-ReAct is not a\nsingle multimodal model, it is a system built on chaining visual tools via GPT-3.5 or GPT-4, which\nwe append as a reference. Our experiment running on LLaVA-7B yields very similar scores with the\nsame checkpoint reported in MM-VET paper, indicating that our evaluation pipelines are consistent.\nModel\nELO\nMatches\nWin(#Ratings)\nHuman Reference\n1382\n5880\n\u2014\nLLaVA-Plus (13B)\n1203\n678\n35.07% (134)\nLLaVA (13B)\n1095\n5420\n18.53% (475)\nmPLUG-Owl\n1087\n5440\n15.83% (480)\nLlamaAdapter-v2\n1066\n5469\n14.14% (488)\nLynx(8B)\n1037\n787\n11.43% (140)\nIdefics (9B)\n1020\n794\n9.72% (144)\nInstructBLIP\n1000\n5469\n14.12% (503)\nOtter\n962\n5443\n7.01% (499)\nVisual GPT\n941\n5437\n1.57% (510)\nMiniGPT-4\n926\n5448\n3.36% (506)\nOctopus V2\n925\n790\n8.90% (146)\nOpenFlamingo V1\n851\n5479\n2.95% (509)\nPandaGPT (13B)\n775\n5465\n2.70% (519)\nMultimodalGPT\n731\n5471\n0.19% (527)\nTable 7: Current ELO rankings on ViSiT-Bench leaderboard as of Sept. 27th, 2023.\nair and thus describe the outdoor scene/activity. The same example is shown to Bard, Bing Chat,\nMM-REACT and LLaVA in Figure 6 in Appendix. They all fail, revealing the lack of grounding\nability.\nIn the right example, we illustrate an interactive image editing scenario, where users aim to see the\nspatial layout of the scene first and then generate an image of a similar layout, but with a new \u201cunder\nwater\u201d scene. The LMM not only applies the correct skills, but also generates a function argument\n\u201cA bicycle parked next to a bench under the sea\u201d for conditional image generation. This reveals the\nappealing property of LMM as a planner, as it can see the raw image, and provide necessary image\nanalysis results throughout the human-AI interaction process. More such examples are in Appendix\nFigure 11.\nIn the bottom example, we show that LLaVA-Plus can be used to help create multimodal social media\nposts. For example, when capturing an image, the user wants to post the same image in an autumn\nscene and associate the image with some attractive text to post Instagram. LLaVA-Plus can use the\nediting skills to revise the image, and combine the context of visual images and their related language\ntopics to suggest several caption options. In Appendix Figure 12, we create all four seasons for the\nsame scenarios, and observe that LLaVA-Plus can follow the instruction to easily switch among them\nwhile consistently maintaining the original image cue.\n9\n\"thoughts!\" The task requires identifying the presence of a frisbee in the image\nand providing its location.\n\"actions\"\" [{\"API_name\": \"grounding_dino\", \"API_params\": {\"caption\": \"the\nfrisbee .\"}}]\n\"value#\u201d\nI will use grounding_dino to help to answer the question. Please wait for a\nmoment.\ngrounding_dino model outputs: {'boxes': [[0.76, 0.64, 0.82, 0.66]], 'logits': [0.49],\n'phrases': ['the frisbee']}\nPlease summarize the model outputs and answer my first question: Is there a\nfrisbee in the image, and if so, could you please provide its location?\n\"thoughts!\" Thanks to the output of grounding_dino. I can answer the question\nbetter.\n\"actions\"\" []\n\"value#\u201d\nYes, there is a frisbee in the\nimage. Based on the results of\nthe grounding_dino model,\nthe frisbee is located in the\nmiddle of the image, with\nbounding box coordinates of\n[0.76, 0.64, 0.82, 0.66]. This\nsuggests that the frisbee is in\nthe air, possibly being thrown\nor caught by one of the\nplayers.\nIs there a frisbee in\nthe image, and if so,\ncould\nyou\nplease\nprovide its location?\nIs it possible to perform semantic segmentation for the image?\n\"thoughts!\" I will u/lize the OpenSeeD model to assist in conduc/ng seman/c segmenta/on for the image.\n\"ac/ons\"\" [{\"API_name\": \"openseed\", \"API_params\": {}}]\n\"value#\"\nI will use openseed to help to answer the question. Please wait for a moment.\ncontrolnet model outputs: {'message': 'The controlnet has processed the image.'}\nPlease summarize the model outputs and answer my first question: Is it possible to perform\nsemantic segmentation for the image?\n\"thoughts!\" Thanks to the output of controlnet. I can answer the ques/on beIer.\n\"ac/ons\"\" []\n\"value#\u201d\nCertainly! I can assist you with performing semantic segmentation on the\nimage. To accomplish this, I will utilize the powerful OpenSeeD model.\nPlease take a look below. Let me know if you need any further\nassistance!\nCan you generate a similar image under sea?\n\"thoughts!\" I will u/lize the ControlNet model for image genera/on.\n\"ac/ons\"\" [{\"API_name\": \"controlnet\", \"API_params\": {\"prompt\": {\"cap/on\": \"A bicycle parked next to a bench under the sea.\"}}}]\n\"value#\"\nI will use controlnet to help to answer the question. Please wait for a moment.\ncontrolnet model outputs: {'message': 'The controlnet has processed the image.'}\nPlease summarize the model outputs and answer my first question: Can you generate a similar\nimage under sea?\n\"thoughts!\" Thanks to the output of controlnet. I can answer the ques/on beIer.\n\"ac/ons\"\" []\n\"value#\"\nCertainly! I've used the powerful ControlNet model to generate\nan image similar to the one you described under the sea. Please\ntake a look at the results below. I hope you like it!\nmake the image like autumn. then generate some attractive texts for Instagram posts\n\"thoughts!\" To complete the image editing task, I need the instruct pix2pix model.\n\"actions\"\" [{\"API_name\": \"instruct-pix2pix\", \"API_params\": {\"prompt\": \"make the image like autumn.\"}}]\n\"value#\"\nI will use instruct-pix2pix to help to answer the question. Please wait for a moment.\ninstruct-pix2pix model outputs: {'message': 'The instruct-pix2pix has processed the image.'}\nPlease summarize the model outputs and answer my first question: make the image like autumn. then generate some attractive texts for Instagram posts\n\"thoughts!\" Thanks to the output of instruct-pix2pix. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nGreat! I'll use instruct pix2pix to edit the image, and you can see the changes below. Now, let's cra! some Instagram captions for\nthis new image:\n1.\"Embrace the cozy vibes of autumn with this stunning lake view! !\" #AutumnFeels #LakeViews #NatureLover\"\n2.\"Who says fall's colors aren't pretty? Check out this vibrant view of the lake! !# #AutumnVibes #LakeLife #ScenicRiver\"\n3.\"Take a moment to soak in the vivid shades of autumn with this breathtaking view! !$ #AutumnHues #NaturePhotography\n#SereneLocation\"\n4.\"Why rush through the season when you have views like this? !% #AutumnDays #LakeEscape #GetawayGoals\"\n5.\"Get lost in the beauty of autumn nature with this captivating view of the lake! \"# #AutumnWonder #LakeLife #NatureLover\"\nFigure 3: New capabilities in LLaVA-Plus. Human questions Xq are in purple, LLaVA-Plus responses\nXanwser are in green. (Left) Object detection and visual chat; (Right) Semantic segmentation and\nmask-based conditional image generation; (Bottom) Multimodal social media post by editing an\nimage and writing a message.\n5\nCONCLUSION\nWe have presented LLaVA-Plus, a general-purpose, multimodal assistant which is based on an LMM\nthat plugs and learns to use skills to complete a wide range of vision-language tasks in the wild.\nThe first visual instruction dataset specifically designed for multimodal tool use has been collected\nfor model training. By incorporating the execution results of new skills, LLaVA-Plus consistently\noutperforms LLaVA across many benchmarks, creates a new SoTA and shows emergent multimodal\ninteraction capabilities. However, LLaVA-Plus is limited due to hallucinations and tool use conflicts\nin practice. There are interesting problems yet to be addressed in future research on building reliable\ngeneral-purpose multimodal AI agents.\nReproducibility\nTo ensure the reproducibility of our research, we will publicly release a com-\nprehensive set of assets including the generated multimodal instruction data, our codebase, the\nLLaVA-Plus checkpoints, and a visual chat demo. Additionally, we have ensured complete trans-\nparency by elaborating on every facet of our training data collection and model training within this\npaper, as shown in Sec. 2.\nREFERENCES\nLangchain. https://github.com/hwchase17/langchain, 2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n10\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\nAnas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. URL https:\n//doi.org/10.5281/zenodo.7733589.\nYonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner,\nRohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction\nfollowing inspired by real-world use, 2023.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image\nediting instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18392\u201318402, 2023.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023a.\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei\nChang. Can pre-trained vision and language models answer visual information-seeking questions?\nFeb 2023b.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\nZhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, and Jianfeng Gao. Vision-language\npre-training: Basics, recent advances, and future trends. Foundations and Trends\u00ae in Computer\nGraphics and Vision, 2022.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,\nConghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model.\narXiv preprint arXiv:2304.15010, 2023.\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. arXiv preprint arXiv:2211.11559, 2022.\nJaidedAI. Easyocr. https://github.com/JaidedAI/EasyOCR, 2023.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning\nsegmentation via large language model. arXiv preprint arXiv:2308.00692, 2023.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench-\nmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125,\n2023b.\nChunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping\nJin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEVATER: A benchmark\nand toolkit for evaluating language-augmented visual models. In NeurIPS Track on Datasets and\nBenchmarks, 2022.\n11\nChunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao.\nMultimodal foundation models: From specialists to general-purpose assistants. arXiv preprint\narXiv:2309.10020, 2023c.\nFeng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang,\nand Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv\npreprint arXiv:2307.04767, 2023d.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023e.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023a.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023b.\nShangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, and Michalis\nRaptis. Towards end-to-end unified scene text detection and layout analysis. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\nShangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, and Michalis\nRaptis. Icdar 2023 competition on hierarchical text detection and recognition. arXiv preprint\narXiv:2305.09750, 2023.\nMarvin Minsky. Society of mind. Simon and Schuster, 1988.\nOpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2023a.\nOpenAI. Chatgpt plugins. https://openai.com/blog/chatgpt-plugins, 2023b.\nOpenAI. Gpt-4 technical report, 2023c.\nOpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_\nCard.pdf, 2023d.\nJunting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Journeydb: A benchmark\nfor generative image understanding. Jul 2023.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model\nconnected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with\nGPT-4. arXiv preprint arXiv:2304.03277, 2023a.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023b.\nRenjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han,\nHang Xu, and Lingpeng Kong Tong Zhang. Detgpt: Detect what you need via reasoning. arXiv\npreprint arXiv:2305.14167, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models, 2021.\n12\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222, 2023.\nD\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023.\nVicuna. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https:\n//vicuna.lmsys.org/, 2023.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-\nsual chatgpt: Talking, drawing and editing with visual foundation models.\narXiv preprint\narXiv:2303.04671, 2023.\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching\nlarge language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752, 2023a.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning\nand action. arXiv preprint arXiv:2303.11381, 2023b.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nLili Yu and et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.\n2023.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,\nand Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv\npreprint arXiv:2308.02490, 2023.\nHao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, and Lei\nZhang. A simple framework for open-vocabulary segmentation and detection. arXiv preprint\narXiv:2303.08131, 2023a.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusion models, 2023b.\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and\nPing Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint\narXiv:2307.03601, 2023c.\nYoucai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin,\nTong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: A strong image tagging model.\narXiv preprint arXiv:2306.03514, 2023d.\nYang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt:\nEnabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nXueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao.\nGeneralized decoding for pixel, image, and language. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 15116\u201315127, 2023a.\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment\neverything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023b.\n13\nA\nDATA\nAugmenting LLaVA data. The original LLaVA data only consists of questions and answers. We\nneed to augment this data to make it match with our regular data format. We transformed the original\nanswers in LLaVA into a part of the values field, then added an empty list for actions, and\ngenerated a thoughts using ChatGPT. The thoughts should indicate that the model can answer\nthe question without invoking any tools. An example is shown in Table 8 in Appendix. We found the\nmodel cannot invoke tools if we did not unify the two data formats.\nLLaVA Instruction Data\nAugmented LLaVA Instruction Data\nUser\nWhat are the colors of the bus in the image?\nWhat are the colors of the bus in the image?\nLLaVA-Plus\nThe bus in the image is white and red.\n\u201cthoughts\u201d The questions can be answered by\nthe information in the context, without need\nany external tools.\n\u201cactions\u201d []\n\u201cvalue\u201d The bus in the image is white and red.\nTable 8: Augmenting LLaVA instruction data.\nDetails on data generation. The pipeline to generate questions for visual prompts is shown in\nTable 4. The pipeline to generate questions with image-related parameters is shown in Table 5. An\nexample of rewriting questions using GPT4 is shown in Table 9. The self-instruct example to generate\nmulti-turn conversation for detection is shown in Table 10.\nPrompts for ChatGPT/GPT4 to rewrite questions:\nPrompt\nSystem\nYou will be given a question. Please rephrase the question to convey the same meaning but with\ngreater diversity. The question is asking a model to generate a caption for an image.\nHuman\ngenerate a caption for the image\nAI\nPlease create a short phrase that captures the essence of this photo.\nHuman\nWhat would be an appropriate caption for this picture?\nAI\nWhat caption would you write for this image?\nHuman\nPlease examine this picture closely and provide a comprehensive caption that elucidates all of the\npertinent subjects, actions, contexts, and nuances that are crucial for understanding what is being\ndepicted. Be thorough and exact in your detailed textual description.\nGPT\nAI\nPlease study this photo carefully and generate a wordy caption that comprehensively describes\nwhat is going on, ensuring to include all pertinent subjects, actions, contexts, nuances, and other\nspecifics necessary for comprehensively representing the essence of the image.\nTable 9: Examples of rewriting questions using GPT4.\nUser actions: Click at point [0.45, 0.89]\nUser question: Perform segmentation based on the point.\nInput question: Perform segmentation\nbased on the point. input point: [0.45, 0.89]\nReformulating input questions.\nFigure 4: The pipeline to generate questions for visual prompts.\nCan you help to detect all {{classname}} in the image?\nCan you help to detect all persons, cars, dogs in the image?\nCould you assist in identifying all individuals, vehicles,\nand dogs in the photograph and provide their coordinates?\nFilling ground truth categories.\nChatGPT/GPT4 rewrite.\nFigure 5: The pipeline to generate questions with image-related parameters.\n14\nPrompts for ChatGPT/GPT4 to rewrite questions:\nPrompt\nSystem\nYou are an AI visual assistant that can analyze a single image. You receive five sentences, each describing the same image\nyou are observing. In addition, specific object locations within the image are given, along with detailed coordinates. These\ncoordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1.\nThese values correspond to the top left x, top left y, bottom right x, and bottom right y.\nDesign a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI\nassistant is seeing the image and answering the question.\nIn the first question question the user ask you to detect some objects in an image. To finish this, you will use an open-world\nobject detection model grounding dino to detect objects in the image.\nThe detection model used is grounding dino. It is a model that can detect objects with captions in an image.\nFor each question, write an answer that you think would be appropriate. It must include the <thoughts>, the <actions>, and\n<actions>. <thoughts> is you thinking about the question and the answer. <actions> is what you do to answer the question.\nThe API name should be \u201dgrounding dino\u201d or None.<values> is the value of the answer.\nYou must also include the output of the model, and how you would use it to answer the question.\nAsk diverse questions and give corresponding answers. Only include questions that have definite answers. Do not ask any\nquestions that cannot be answered confidently.\nGive the answers as you have seen the image. Do not reveal the input information of the image. DO NOT say that you are\ngiven the captions and the objects in the image, JUST answer the question as if you are seeing the image for the first time, as\nwell as the output of the model you invoked.\nHuman\nA plate filled with food sitting next to three glasses.\nA plate of meats and cheese\u2019s next to three wine glasses.\nA table displaying wine glasses and a platter of assorted meats and cheeses.\nA white plate containing sliced ham and cheese sitting on a table next to wine glasses.\nThe wine has been poured and ready to drink\ndining table: [0.0, 0.334, 0.893, 0.898]\nperson: [0.0003, 0.002, 0.111, 0.138]\nperson: [0.020, 0.0, 0.106, 0.109]\nwine glass: [0.360, 0.109, 0.212, 0.501]\nwine glass: [0.010, 0.107, 0.259, 0.612]\nwine glass: [0.646, 0.101, 0.220, 0.487]\nfork: [0.279, 0.588, 0.488, 0.225]\nAI\nQuestion:\nIs it feasible to call on a detection model to scan the image and generate a comprehensive summary of glasses?\nAnswer:\n<thoughts> It would be beneficial to utilize an open-set detection model to identify the presence of glasses. Integrating the\nmodels into the conversation would enhance the overall outcome and provide more accurate results.\n<actions> [{\u2019API name\u2019: \u2019grounding dino\u2019, \u2019API params\u2019: {\u2019image\u2019: \u2019<imagepath>\u2019, \u2019caption\u2019: \u2019glasses .\u2019}}]\n<values> Yes, it is feasible to leverage a detection model to scan the image and generate a comprehensive summary of glasses.\nIn this case, I can utilize an open-set detection model called grounding dino to detect all glasses present in the image.\nBased on the bounding box coordinates detected, the image contains three wine glasses. Here\u2019s a summary of their positions:\n1. The first wine glass is located towards the middle of the image, with bounding box coordinates of [0.360, 0.109, 0.212,\n0.501]. This suggests that the glass is somewhat centrally placed on the table.\n2. The second wine glass is located towards the left of the image, with bounding box coordinates of [0.010, 0.107, 0.259,\n0.612]. This indicates that the glass is closer to the edge of the table or possibly near a person, given the presence of person\nbounding boxes in the image.\n3. The third wine glass is located towards the right of the image, with bounding box coordinates of [0.646, 0.101, 0.220, 0.487].\nThis suggests that the glass is also closer to the edge of the table or near another person.\nThe bounding box coordinates suggest that all three glasses are roughly in a line, possibly indicating a formal dining setup.\nThe glasses are also likely filled with wine.\n======\nQuestion:\nCan a detection model be used to identify and summarize the presence of forks in the image?\nAnswer:\n<thoughts> It would be beneficial to utilize an open-set detection model to identify the presence of forks. Integrating the\nmodels into the conversation would enhance the overall outcome and provide more accurate results.\n<actions> [{\u2019API name\u2019: \u2019grounding dino\u2019, \u2019API params\u2019: {\u2019image\u2019: \u2019<imagepath>\u2019, \u2019caption\u2019: \u2019forks .\u2019}}]\n<values> Yes, it is feasible to leverage a detection model to scan the image and generate a comprehensive summary of forks.\nIn this case, I can utilize an open-set detection model called grounding dino to detect all forks present in the image.\nBased on the bounding box coordinates detected, the image contains one fork. Here\u2019s a summary of its position:\n1. The fork is located towards the middle of the image, with bounding box coordinates of [0.279, 0.588, 0.488, 0.225]. This\nsuggests that the fork is placed near the plate of food, possibly indicating that someone is about to eat or is in the process of\neating.\nHuman\nA girl with some food and drink at a table.\nA YOUNG GIRL EATING PIZZA DRESSED AS A EGYPTION\nA girl sitting around a table in a restaurant eating a pizza.\nA young girl eating pizza in a restaurant.\nA girl in a Cleopatra outfit eating pizza at a table.\nchair: [0.394, 0.491, 0.248, 0.148]\ndining table: [0.007, 0.573, 0.974, 0.416]\ndining table: [0.663, 0.219, 0.101, 0.059]\nperson: [0.497, 0.229, 0.488, 0.634]\nAI\nQuestion:\nPerform an exhaustive search for the book in the image, and count the number of girls.\nAnswer:\n<thoughts> Based on the information provided, the task requires me to search for the book in the image and count the number\nof girls.\n<actions> [{\u2019API name\u2019: \u2019grounding dino\u2019, \u2019API params\u2019: {\u2019image\u2019: \u2019<imagepath>\u2019, \u2019caption\u2019: \u2019the book and the girls in\nthe image.\u2019}}]\n<values> Certainly! To accomplish this task, I will utilize an object detection model to identify the book in the image and\ncount the number of girls.\nBased on the results of grounding dino, I can identify the book and girls in the image. According to the available data, there is\none \u2019book\u2019 object detected and there are three \u2019person\u2019 objects detected. Hence, there are three girls in the image.\n======\nQuestion:\nHi, can you tell me what\u2019s happening in this image?\nAnswer:\n<thoughts> I don\u2019t need to utilize any specific tools to provide an answer to your question. I can answer the question by\nreading the image.\n<actions> []\n<value> Hello! According to the information provided, it seems like the image depicts a young girl, dressed as Cleopatra,\nwho is eating pizza in a restaurant.\nTable 10: Self-instruct examples of generating multi-turns conversations with in-context learning.\n15\nB\nEXTENDED SKILLS\nExternal Knowledge.\nTo enable LMMs to gain knowledge beyond that encoded in pre-trained\nmodel weights, we use the CLIP search API to retrieve external knowledge from LIAON. We utilize\nthe images Iq and questions Xq from the InfoSeek dataset, and generate the other fields of the training\nsequence by following the image-only skill data creation pipeline. Input images are considered as\nqueries, and image-to-text retrieval is performed to get top-K items for each query. To encourage the\nLMM to leverage external knowledge, we only consider the subset of questions whose ground truth\nanswers can be extracted or derived from the retrieved knowledge. This subset can be selected using\nChatGPT that compares the answers and retrieved knowledge.\nGeneration.\nFor image generation, we employ Stable Diffusion (SD) as the tool, and generate\ninstruction data based on the JourneyDB dataset due to its high quality in language prompt and images.\nWe ask ChatGPT to generate human-like instructions based on the original, detailed prompt for image\ngeneration, focusing on the scenarios where human-specified instructions are ambiguous and short,\nand thus cannot easily align with the prompt distribution of SD. Similarly, we use Instruct-Pix2Pix\nfor image editing. The Instruct Pix2Pix dataset contains both instructions and prompts of source and\ntarget images. We directly use their editing instructions and follow the image-only skill data creation\npipeline to fill the other fields.\nVisual Prompts.\nThe visual prompt data is constructed similarly to that for visual understanding\nskills, except that additional visual inputs, such as user-drawn points, sketches and boxes, are required.\nTake SAM as an example. A point is required as input for interactive segmentation. We simply\ngenerate a random point and then convert it into a text sequence, and append it to a user question\nto form a concatenated text sequence Xq, which is a standard format that LMMs such as LLaVA\ncan deal with. Sometimes, a user point might correspond to segmented masks at multiple levels.\nTo support this skill, we use Semantic-SAM (Li et al., 2023d) to create training data where the\nmulti-granularity segmentation functionality is explicitly specified by instructions.\nSkill Composition.\nThe scenarios described so far are designed to create training samples for\nsingle-skill tasks. However, many real-world scenarios often require some compositions of several\nskills. To allow LLaVA-Plus to deal with such compositional tasks, we have curated instruction-\nfollowing data for compositional skills as follows. (i) Various visual understanding results of the same\nimage can be requested. To teach an LMM to learn to use multiple skills in a multi-turn human-AI\ninteraction session, we generate instruction data by applying different tools (including detection,\nsegmentation, tagging, and captioning) to the same image from COCO, combining the results with\nLLaVA instruction data, and then randomly mixing these datasets. This produces instruction data\nthat simulates users\u2019 behavior of using multiple tools to deal with real-world tasks. (ii) Interactive\nSegmentation + Inpainting. In one editing scenario, we ask a user to specify an area of an image with\nvisual pointing along with language instruction. We then combine the SAM segmentation results and\nthe SD inpainting results to create an instruction-following sample. (iii) Semantic Segmentation +\nGeneration. In another image editing scenario, we ask a user to specify the spatial layout of an image,\nusing an user-provided image and a language instruction. We then combine the OpenSeed semantic\nsegmentation results and ControlNet conditional generation results to create an instruction-following\nsample. (iv) Image Generation/Editing + Social Media Post. It is time-consuming for human users to\ngenerate posts that contains both images and text. Thus, we use existing tools to create large amounts\nof multimodal posts for model tuning as follows. We use SD to generate an image, or Instruct Pix2Pix\nto edit an image. We then combine the image with its description generated by a pre-trained LMM to\ncreate a multimodal post.\nC\nRESULTS\nComparisons on COCO Caption.\nWe aim to investigate the potential of the LMM in enhancing\nexisting tools. A comparison of three distinct models on the COCO caption benchmark is presented\nin Table 11. We employed BLIP2 as our primary captioning tool and hence, use it as the benchmark\nmodel. Additionally, the original LLaVA is also included for reference. The enhanced LLaVA-Plus\nmodel refines BLIP2\u2019s outputs, leading to richer details.\n16\nThe table reveals that LLaVA-Plus outperforms the others in terms of the CLIP score. Intrigu-\ningly, both language models exhibit subpar performance on language-language metrics. A striking\nobservation is the significantly lower CIDEr scores for these models when juxtaposed with BLIP2.\nbleu1\nbleu2\nbleu3\nbleu4\nmeteor\nrouge l\nCIDEr\nSPICE\nCLIP score\nRefCLIP score\nBLIP2\n77.0\n62.1\n48.0\n36.4\n28.2\n57.2\n123.5\n22.3\n0.788\n0.836\nLLaVA-Plus-7B\n50.8\n35.4\n23.8\n15.7\n27.7\n44.5\n31.0\n22.9\n0.815\n0.813\nLLaVA-7B\n21.1\n13.7\n8.4\n5.1\n19.3\n21.5\n0.0\n17.6\n0.785\n0.785\nTable 11: Comparisons on COCO Caption.\nFalse Positives of Grounding DINO\nGrounding DINO, despite its commendable object detection\nprowess, occasionally exhibits hallucinations, leading it to generate false positive instances. Our\nLLaVA-Plus model, capable of simultaneously analyzing model outputs and image content, holds the\npotential to reduce such false positives.\nTo harness this potential, we crafted examples using negative prompts from COCO and directed\nthe model to eliminate false positive outputs. We subsequently evaluated the model on the first 100\nimages from the COCO validation set. By using all negative categories of an image as prompts, we\ngauged the presence of false positive objects. The results are tabulated in Table 12.\nThe results show that Grounding DINO has a high possibility of resulting in false positive examples.\nWith the LLaVA-Plus model, it can help to reduce the false positive rate significantly.\n#Ins. FP\n#Img. FP\nGrounding DINO\n90\n41\nLLaVA-Plus-7B\n20\n12\nTable 12: Comparisons of false positive of Grounding DINO. \u2018#Ins. FP\u2019 is the number of false\npositive examples in a whole test set, while the \u2018#Img. FP\u2019 is the number of images that have false\npositive examples. The test set is the first 100 images of COCO val set.\nD\nEXAMPLE SCENARIOS\nWe show more scenarios of LLaVA-Plus in leveraging new skills to improve visual chat experience.\nObject Detection for Visual Chat.\nFigure 6 compares object localization capability of LLaVA-Plus\nwith Bard, Bing Chat, MM-REACT and LLaVA. It turns out the commercial visual chat do not have\nthe ability to tell the object spatial location, while LLaVA-Plus can successfully identify the object\nlocation and thus describe the outdoor scene and activity correctly.\nDetection and Segmentation in Contexts.\nFigure 7 (a) shows an example to detect and count the\nnumber of objects. Figure 7 (b) shows a real-life scenarios to pick up the appropriate tools and teach\nthe users how to use them. Compared langauge-output-only LMM such as LLaVA/GPT-V, identify\nand visualization the location of object is an more intuitive approach for users to comprehend. Figure 8\nprovides object segmentation results, but enriched with language descriotion at the instance level. It\nis the synergy of LMM and segmentation that improve the enhanced fine-grained understanding.\nExternal Knowledge\nIn Figure 9, we compare LLaVA-Plus and LLaVA in terms of generating\nresponse with detailed facts and entities. The retrieval external knowledge of LLaVA-Plus introduces\nmore relevant information that allows LLaVA-Plus to ground in generation.\nImage Generation.\nIn Table 10, we show that LLaVA-Plus can produce detailed SD-favored\nlanguage prompts for image generation, based on the high-level and brief requests. This can help\nimprove image generation quality.\n17\nInteractive Image Editing.\nFigure 11 demonstrate the multi-turn interactive image segmentation\nand editing capabilities. By leveraging OpenSEED, LLaVA-Plus can apply the skill of full-image\nsemantic segmentation to group pixels of the same object together, providing the spatial layout of the\nscene. With further requests to produce new images that follow the same layout but change other\naspects, the corresponding editing skills can be executed, through InstructPix2Pix and ControlNet.\nMultimodal Social Meida Post.\nIn Figure 12, the four seasons of the same scene are used as\ninstructions to ask LLaVA-Plus to provide the edited images and attractive texts. Another example on\nfireworks is shown in Figure 13\nVisual Prompt for Interactions.\nFigure 14 demonstrates the use of semantic SAM to support\nvisual pointing on the image from humans, after which multiple segmentation masks at different\nlevels are shown. Figure 15 demonstrates the visual referring segmentation capabilities. LLaVA-Plus\nallows humans to specify the segmentation intents on the object of interest with the selected regions\nfrom another image. This is useful because some concepts can be hard described in language, but\neasier to express with reference visual regions.\n18\nHuman: Is there a frisbee in the image, and if so, could you please provide its location?\nLLaVA: Yes, there is a frisbee in the image. It is located on the ground, near the two men who are playing with it.\nBard: I have examined the image you sent, and I can confirm that there is no frisbee in the image. The image shows two men\nstanding in a park, but there is no flying disc or any other object that could be mistaken for a frisbee.\nI hope this answers your question! Let me know if you have any other questions.\nBing Chat: Yes, there is a frisbee in the image. It is located near the bottom left corner of the image, on the grass. It is orange in\ncolor and has a white stripe around it.\nMM-REACT: To summarize, This is an image of a couple of men playing frisbee.One man is wearing a white shirt and black pants,\nand the other man is in a blue jacket. The scene is outdoors with grass, trees, and sky in the background. It seems like they are\nstanding on a lawn or a field. Is there a frisbee in the image, and if so, could you please provide its location?\nI\u2018m sorry, but the information provided does not mention a frisbee or its location in the image.\nLLaVA-Plus:\n\"thoughts!\" The task requires identifying the presence of a frisbee in the image and providing its location.\n\"actions\"\" [{\"API_name\": \"grounding_dino\", \"API_params\": {\"caption\": \"the frisbee .\"}}]\n\"value#\u201d\nI will use grounding_dino to help to answer the question. Please wait for a moment.\ngrounding_dino model outputs: {'boxes': [[0.76, 0.64, 0.82, 0.66]], 'logits': [0.49], 'phrases': ['the frisbee']}\nPlease summarize the model outputs and answer my first question: Is there a frisbee in the image, and if so, could you please provide its location?\n\"thoughts!\" Thanks to the output of grounding_dino. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nYes, there is a frisbee in the image. Based on the results of the grounding_dino model, the frisbee is located in the middle of the\nimage, with bounding box coordinates of [0.76, 0.64, 0.82, 0.66]. This suggests that the frisbee is in the air, possibly being thrown\nor caught by one of the players.\nFigure 6: The comparison of detection capabilities and its impact on visual chat. LLaVA-Plus is the\nonly system that is able to detect the frisbee and leverage the location information to tell the motion\nand status of the object as well as human activity, revealing the importance of object localization on\nthe LMM response.\n19\ndetect birds and human head in the image, then tell how many birds in it\n\"thoughts!\" To answer the question, I must utilize the pre-trained grounding_dino model specifically designed \nfor open-set detection.\n\"actions\"\" [{\"API_name\": \"grounding_dino\", \"API_params\": {\"caption\": \"bird . human . head\"}}]\n\"value#\"\nI will use grounding_dino to help to answer the question. Please wait for a moment.\ngrounding_dino model outputs: {'boxes': [[0.3, 0.12, 0.7, 0.91], [0.15, 0.77, 0.27, 0.94], [0.83, 0.14, 0.92, 0.28], \n[0.64, 0.05, 0.72, 0.17], [0.82, 0.77, 0.97, 0.97], [0.05, 0.06, 0.15, 0.2], [0.66, 0.71, 0.77, 0.87], [0.14, 0.2, 0.27, 0.39], \n[0.26, 0.04, 0.33, 0.13], [0.86, 0.35, 0.98, 0.51], [0.1, 0.45, 0.2, 0.59], [0.74, 0.39, 0.82, 0.5], [0.83, 0.58, 0.89, 0.67], \n[0.06, 0.66, 0.14, 0.77]], 'logits': [0.61, 0.45, 0.45, 0.45, 0.44, 0.44, 0.42, 0.41, 0.41, 0.41, 0.4, 0.39, 0.39, 0.39], \n'phrases': ['head', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird']}\nPlease summarize the model outputs and answer my first question: detect birds and human head in the image, \nthen tell how many birds in it\n\"thoughts!\" Thanks to the output of grounding_dino. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nBased on the output of the grounding_dino model, there are 14 boxes detected in \nthe image. Out of these boxes, 13 have been labeled as \"bird\" and 1 has been \nlabeled as \"head\". The coordinates of the boxes have also been provided. The boxes \nlabeled as \"bird\" are located at [0.3, 0.12, 0.7, 0.91], [0.15, 0.77, 0.27, 0.94], [0.83, \n0.14, 0.92, 0.28], [0.64, 0.05, 0.72, 0.17], [0.82, 0.77, 0.97, 0.97], [0.05, 0.06, 0.15, 0.2], \n[0.66, 0.71, 0.77, 0.87], [0.14, 0.2, 0.27, 0.39], [0.26, 0.04, 0.33, 0.13], [0.86, 0.35, 0.98, \n0.51], [0.1, 0.45, 0.2, 0.59], [0.74, 0.39, 0.82, 0.5], [0.83, 0.58, 0.89, 0.67], [0.06, 0.66, \n0.14, 0.77]. The box labeled as \"head\" is located at 0.3, 0.12, 0.7, 0.91.\nTherefore, the image contains 13 birds and 1 human head.\n(a) Detection and counting\nMy bike is broken. I want to use a wrench to fix it. Can you show me what is the location of wrenches \nand how to use it\u0001\n\"thoughts!\" I am unable to give a direct response to the user, so I will have to use the pre-trained open-set detection model \ngrounding_dino to reply.\n\"actions\"\" [{\"API_name\": \"grounding_dino\", \"API_params\": {\"caption\": \"wrench . bicycle\"}}]\n\"value#\"\nI will use grounding_dino to help to answer the question. Please wait for a moment.\ngrounding_dino model outputs: {'boxes': [[0.64, 0.28, 0.69, 0.5], [0.69, 0.28, 0.73, 0.46], [0.81, 0.27, 1.0, 0.51], [0.31, 0.14, 0.69, 0.24],\n[0.08, 0.5, 0.21, 0.67]], 'logits': [0.42, 0.41, 0.4, 0.34, 0.31], 'phrases': ['wrench', 'wrench', 'wrench', 'wrench', 'wrench']}\nPlease summarize the model outputs and answer my first question: My bike is broken. I want to use a wrench to fix it. Can you show \nme what is the location of wrenches and how to use it\u0001\n\"thoughts!\" Thanks to the output of grounding_dino. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nBased on the output of the grounding_dino model, there are five wrenches in the image, and their \nlocations are as follows:\n- The first wrench is located in the upper right corner, with coordinates (0.64, 0.28) for the top le\" \npoint and (0.69, 0.5) for the bottom right point.\n- The second wrench is located in the upper middle area, with coordinates (0.69, 0.28) for the top le\" \npoint and (0.73, 0.46) for the bottom right point.\n- The third wrench is located in the lower le\" corner, with coordinates (0.81, 0.27) for the top le\" \npoint and (1.0, 0.51) for the bottom right point.\n- The fourth wrench is located in the middle le\" area, with coordinates (0.31, 0.14) for the top le\" \npoint and (0.7, 0.24) for the bottom right point.\n- The fi\"h wrench is located in the upper middle area, with coordinates (0.08, 0.5) for the top le\" \npoint and (0.21, 0.67) for the bottom right point.\nAs for how to use a wrench, it is a tool used to tighten or loosen nuts and bolts. To use a wrench, you \nneed to hold it with the handle facing the direction you want to turn the nut or bolt, and then turn \nthe handle to apply pressure on the fastener. The wrench is typically marked with numerical degrees \nto indicate the amount of torque applied.\n(b) Detection for real-life actions\nFigure 7: Detection for counting and actions.\n20\nSegmentation\nPlease detect and segment the cat and computer from the image.\n\"thoughts!\" To accurately detect and segment the cat and computer in the image, I can leverage the grounding_dino model for \nopen-set detection and the segmentation model SAM.\n\"actions\"\" [{\"API_name\": \"grounding_dino+sam\", \"API_params\": {\"caption\": \"cat and computer .\"}}]\n\"value#\"\nI will use grounding_dino+sam to help to answer the question. Please wait for a moment.\ngrounding_dino+sam model outputs: {'boxes': [[0.0, 0.0, 0.59, 0.75], [0.23, 0.2, 1.0, 1.0], [0.01, 0.01, 0.99, 0.99]], 'logits': [0.55, 0.48, \n0.31], 'phrases': ['cat', 'computer', 'cat']}\nPlease summarize the model outputs and answer my first question: Please detect and segment the cat and computer from the image.\n\"thoughts!\" Thanks to the output of grounding_dino+sam. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nBy using the grounding_dino model for open-set detection and the segmentation model SAM, we \ncan effectively detect and segment the cat and computer in the image. The cat is laying down on a \npillow, resting its paw on the laptop, creating a cozy and captivating scene. The computer, in this \ncase a laptop, is positioned next to the cat, reinforcing the connection between the two subjects. \nThe segmentation model SAM enables us to isolate and analyze these elements, providing a \ndetailed and comprehensive understanding of the image composition.\nSegmentation\ndetect and segment dog and phone from the image, and describe them\n\"thoughts!\" To accurately detect and segment the dog and phone in the image, we can leverage the grounding_dino model for \nopen-set detection and the segmentation model SAM. Integrating these models will enable us to generate a comprehensive summary \nof the segmented objects.\n\"actions\"\" [{\"API_name\": \"grounding_dino+sam\", \"API_params\": {\"caption\": \"dog and phone .\"}}]\n\"value#\"\nI will use grounding_dino+sam to help to answer the question. Please wait for a moment.\ngrounding_dino+sam model outputs: {'boxes': [[0.18, 0.01, 0.99, 1.0], [0.42, 0.08, 0.76, 1.0]], 'logits': [0.72, 0.58], 'phrases': ['dog', \n'phone']}\nPlease summarize the model outputs and answer my first question: detect and segment dog and phone from the image, and describe \nthem\u0002\n\"thoughts!\" Thanks to the output of grounding_dino+sam. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nBy utilizing the grounding_dino model for open-set detection and the segmentation model SAM, we \ncan e\ufb00ectively segment and detect the dog and phone in the image. The dog is a small pug with a \nwrinkly face, and it is positioned close to the phone. The phone is held up to the dog's ear, as if the \ndog is talking on the phone.\nThe segmentation model SAM allows us to isolate and analyze these objects, providing a detailed \nand comprehensive summary of the dog and phone in the image. Integrating these models \nenhances the accuracy and precision of the segmentation process, resulting in a deeper \nunderstanding of the scene depicted in the image.\nFigure 8: Language-enriched detection and description.\n21\nDescribe the food in the image? search on the internet\n\"thoughts!\" To tackle that question, one needs the assistance of web research using clip retriever.\n\"actions\"\" [{\"API_name\": \"clip\", \"API_params\": {}}]\n\"value#\"\nI will use clip to help to answer the question. Please wait for a moment.\nclip model outputs: {'retrieval_results': [{'caption': '\r\u001c0\u001d\u000f\u0005&\u0017?\u0001'*\r\u001c0$ \u0019\u0004/', 'similarity': 0.98}, {'caption': '\u0001\n'\u001e\f\u0011(\u000b\u001a\u001f \b\u0013%\t\u001c0\u001b\u0014\r', 'similarity': 0.95}, {'caption': 'C\u00e1ch l\u00e0m b\u00e1nh n\u01b0\u1edbng nh\u00e2n \u0111\u1eadu xanh 12', 'similarity': 0.95}, \n{'caption': 'S\u1ef1 \u0111\u1eb7c bi\u1ec7t trong nh\u1eefng chi\u1ebfc b\u00e1nh trung thu ngon v\u00e0 \u0111\u1eb9p (2)', 'similarity': 0.94}, {'caption': '\u0001'*\u0002\u0007\u0003,\r\u001c0 .\n, \u0019$\u0004\u0018\u001d\u000f\u0005', 'similarity': 0.94}, {'caption': 'Kh\u00e1m ph\u00e1 h\u01b0\u01a1ng v\u1ecb b\u00e1nh trung thu c\u00e1c n\u01b0\u1edbc Ch\u00e2u \u00c1', 'similarity': 0.94}, \n{'caption': '\u0015\u0016\u001c0,\u0006\u0016\u001c0,+\u0016\u001c0\u000e\"\u0016\u001c0.\u0010#', 'similarity': 0.94}, {'caption': '\u0001'*\u001c0\u0010#\u0012\n )!-1', \n'similarity': 0.94}]}\nPlease summarize the model outputs and answer my first question: Describe the food in the image? search on the internet\n\"thoughts!\" Thanks to the output of clip. I can answer the question better.\n\"actions\"\" []\n\"value#\"\nBased on the retrieved information, it seems that the image is related to month cakes, which are a traditional \nfood served during the Mid-Autumn Festival in China.\nDescribe the food in the image?\nThe image features a variety of food items, including a plate of pastries, a bowl of soup, and a cup of tea. There \nare also some cakes and a flower arrangement on the table.\nwhat it is the food?\nThe food in the image consists of a plate of pastries, a bowl of soup, a cup of tea, and some cakes. The pastries \nare likely a type of pastry or dessert, while the soup could be a savory dish. The cakes are also dessert items, and \nthe tea is a beverage.\nLLaVA\nLLaVA-Plus\nwhat's the image? search on the internet\n\"thoughts!\" I need to search for more information on the internet, which can be done by using the clip retriever.\n\"actions\"\" [{\"API_name\": \"clip\", \"API_params\": {}}]\n\"value#\"\nI will use clip to help to answer the question. Please wait for a moment.\nclip model outputs: {'retrieval_results': [{'caption': 'Fresk Giotto - Poca\u0142unek Judasza', 'similarity': 0.91}, {'caption': '\u0421\u0435\u0432\u0435\u0440\u043d\u043e\u0435 \n\u0432\u043e\u0437\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u0436\u0438\u0432\u043e\u043f\u0438\u0441\u044c \u044d\u043f\u043e\u0445\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u044b \u0445\u0443\u0434\u043e\u0436\u043d\u0438\u043a\u0438', 'similarity': 0.91}, {'caption': 'Kiss of Judas (1304\u201306), fresco by Giotto, \nScrovegni Chapel, Padua, Italy', 'similarity': 0.91}, {'caption': 'Giotto di Bondone - 21b Prendimiento de Jes\u00fas.jpg', 'similarity': 0.91}, \n{'caption': 'Giotto - Kiss of Judas. Fresco in Scrovegni Chapel, Padua, Italy (1304\u201306)', 'similarity': 0.91}]}\nPlease summarize the model outputs and answer my first question: what's the image? search on the internet\n\"thoughts!\" Thanks to the output of clip. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nBased on the retrieved information, it is highly likely that the image belongs to the fresco painting \"Kiss of \nJudas\" by Giotto, which is located in the Scrovegni Chapel in Padua, Italy.\nLLaVA-Plus\nwhat's the image?\nThe image is a painting depicting a group of people gathered around Jesus, with some of them holding torches.\nLLaVA\nFigure 9: External knowledge retrieval help improve the entity and fact based responses.\ngenerate a view \nof the city \nskyline of \ndowntown \nSeattle in a \nsketch style\nview of the \ncity skyline of \ndowntown \nseattle, sketch \ngenerate a \nview of the city \nskyline of \nShanghai in a \ncyberpunk \nstyle\nview of the city \nskyline of shanghai, \nchina, cyberpunk \nstyle, photorealistic, \noctane render, by \nwlop, pixiv\ngenerate a view \nof the city \nskyline of \nSeattle in a \nfuture and \ntechnique style\nseattle city \nskyline future \ntechnique \nview of the city \nskyline of \nseattle, \nfuturistic, \ndigital art, \ntechnique, \nphotorealistic, \nhigh quality, \nresolution, 8 k \ngenerate a \nview of the city \nskyline of \nSeattle in a \nfuture and \ntechnique style\ngenerate a view \nof the city \nskyline of \nShenzhen in a \nfuture and \ntechnique style\nview of the city \nskyline of shenzhen\nin a future and \ntechnique style, \ndigital art, cinematic, \nhyperrealistic 3 5 \nmm lens, aerial \nphotography, \nvolumetric lighting, \nby alena aenami and \nmax hay\nGenerated\nImages\nSD Prompt\nGenerated\nby LMM\n(LLaVA-Plus)\nHuman\nInstructions\nFigure 10: LLaVA-Plus improves SD-favored language prompt from user instructions for image\ngeneration.\n22\nIs it possible to perform semantic segmentation for the image?\n\"thoughts!\" I have decided to utilize the OpenSeeD model in order to improve the performance of semantic segmentation on the image.\n\"actions\"\" [{\"API_name\": \"openseed\", \"API_params\": {}}]\n\"value#\"\nI will use openseed to help to answer the question. Please wait for a moment.\ncontrolnet model outputs: {'message': 'The controlnet has processed the image.'}\nPlease summarize the model outputs and answer my first question: Is it possible to perform semantic segmentation for\nthe image?\n\"thoughts!\" Thanks to the output of controlnet. I can answer the question better.\n\"actions\"\" []\n\"value#\"\nYes, I can definitely help you with semantic segmentation for the image! I have utilized the powerful\nOpenSeeD model to enhance the accuracy and performance of the segmentation process. Here are the\nresults I have obtained for you to analyze and explore.\nTurn the image into cartoon style.\n\"thoughts!\" To enhance the image editing process, I have collaborated with the instruct pix2pix model.\n\"actions\"\" [{\"API_name\": \"instruct-pix2pix\", \"API_params\": {\"prompt\": \"Make the image look like a cartoon.\"}}]\n\"value#\"\nI will use instruct-pix2pix to help to answer the question. Please wait for a moment.\ninstruct-pix2pix model outputs: {'message': 'The instruct-pix2pix has processed the image.'}\nPlease summarize the model outputs and answer my first question: Turn the image into cartoon style.\n\"thoughts!\" Thanks to the output of instruct-pix2pix. I can answer the question better.\n\"actions\"\" []\n\"value#\"\nSure, I can definitely help you with that! I've used the instruct pix2pix model to turn the image into a\ncartoon style. Here are the results for you to check out. Let me know if you need any further assistance!\nGenerate a similar image with cartoon style.\n\"thoughts!\" I will utilize the ControlNet model in order to generate images.\n\"actions\"\" [{\"API_name\": \"controlnet\", \"API_params\": {\"prompt\": {\"caption\": \"A table topped with plates of food in a cartoon style.\"}}}]\n\"value#\"\nI will use controlnet to help to answer the question. Please wait for a moment.\ncontrolnet model outputs: {'message': 'The controlnet has processed the image.'}\nPlease summarize the model outputs and answer my first question: Generate a similar image with cartoon style.\n\"thoughts!\" Thanks to the output of controlnet. I can answer the question better.\n\"actions\"\" []\n\"value#\"\nCertainly! I've used the ControlNet model to generate a similar image with a cartoon style. Please take a\nlook at the results below.\nIs it possible to perform semantic segmentation for the image?\n\"thoughts!\" I will utilize the OpenSeeD model to assist in conducting semantic segmentation for the image.\n\"actions\"\" [{\"API_name\": \"openseed\", \"API_params\": {}}]\n\"value#\"\nI will use openseed to help to answer the question. Please wait for a moment.\ncontrolnet model outputs: {'message': 'The controlnet has processed the image.'}\nPlease summarize the model outputs and answer my first question: Is it possible to perform semantic segmentation for the image?\n\"thoughts!\" Thanks to the output of controlnet. I can answer the question better.\n\"actions\"\" []\n\"value#\"\nOf course! I can definitely assist you with performing semantic segmentation for the image. To\naccomplish this, I will utilize the powerful OpenSeeD model. It has the capability to accurately segment\nobjects in the image based on their semantic meaning. I have already triggered the model and the\nresults are now available for you to review. Please take a look below.\nPlease generate a similar image of a street in India.\n\"thoughts!\" I will utilize the ControlNet model for image generation.\n\"actions\"\" [{\"API_name\": \"controlnet\", \"API_params\": {\"prompt\": {\"caption\": \"A street in India with people and cars.\"}}}]\n\"value#\"\nI will use controlnet to help to answer the question. Please wait for a moment.\ncontrolnet model outputs: {'message': 'The controlnet has processed the image.'}\nPlease summarize the model outputs and answer my first question: Please generate a similar image of a street in India.\n\"thoughts!\" Thanks to the output of controlnet. I can answer the question better.\n\"actions\"\" []\n\"value#\"\nCertainly! I've used the powerful ControlNet model to generate an image of a street in India. The results\nare now ready for you to see below.\nPlease generate a similar image of a street in New York.\n\"thoughts!\" I will utilize the ControlNet model for image generation.\n\"actions\"\" [{\"API_name\": \"controlnet\", \"API_params\": {\"prompt\": {\"caption\": \"A street in New York City with cars and people.\"}}}]\n\"value#\"\nI will use controlnet to help to answer the question. Please wait for a moment.\ncontrolnet model outputs: {'message': 'The controlnet has processed the image.'}\nPlease summarize the model outputs and answer my first question: Please generate a similar image of a street in New York.\n\"thoughts!\" Thanks to the output of controlnet. I can answer the question better.\n\"actions\"\" []\n\"value#\"\nSure! I've used the advanced ControlNet model to generate an image of a street in New York. Here are\nthe results for you to see.\nFigure 11: Semantic segmentation and mask-based conditional image generation of LLaVA-Plus.\nPurple is human questions, green is LLaVA-Plus response. The semantic segmentation task is\nfullfilled via OpenSEED. Based on the segmented images, new editing instructions and history,\nInstructPix2Pix and ControlNet can be called to complete the tasks. The captions of the target edited\nimages are generated by LLaVA-Plus, revealing the unique advantage of LMM for tool use.\n23\nmake the image like spring. then generate some attractive texts for Instagram posts\n\"thoughts!\" To accomplish the task, I need to utilize the specialized abilities of the instruct pix2pix model for the required image editing.\n\"actions\"\" [{\"API_name\": \"instruct-pix2pix\", \"API_params\": {\"prompt\": \"make the image like spring.\"}}]\n\"value#\"\nI will use instruct-pix2pix to help to answer the question. Please wait for a moment.\ninstruct-pix2pix model outputs: {'message': 'The instruct-pix2pix has processed the image.'}\nPlease summarize the model outputs and answer my first question: make the image like spring. then generate some attractive texts for Instagram posts\n\"thoughts!\" Thanks to the output of instruct-pix2pix. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nAbsolutely! I've got you covered. Let's spruce up that image. See the enhancements below.\nAnd now, let me share some Instagram captions to go with the new image:\n1.\"Spring has sprung, and this stunning lake view is proof! Can you feel the sunshine and fresh air? &'(\n#SpringVibes #NatureLover\"\n2.\"Feeling rejuvenated by this breathtaking image of a serene lake in springtime. Mother Nature truly is a wonder! &)\n#NaturePhotography #SpringFeels\"\n3.\"This image transported us straight to a peaceful spring morning with its vibrant colors and tranquil surroundings.\nLet's take a moment to appreciate the beauty of nature! %' #SpringMood #NatureLove\"\n4.\"Who needs a spring break when you have a view like this? * This image perfectly captures the beauty of nature in\nall its glory. &( #NatureIsAmazing #Springtime\"\n5.\"Spring has arrived with a bang, as evidenced by this picture-perfect view of pristine waters and lush greenery. Can\nyou feel the excitement in the air? &'+ #SpringFeels #NatureBeauty\"\nmake the image like summer. then generate some attractive texts for Instagram posts\n\"thoughts!\" In order to successfully conclude the task at hand, I must employ the specialized capabilities of the instruct pix2pix model to facilitate the \nnecessary image editings.\n\"actions\"\" [{\"API_name\": \"instruct-pix2pix\", \"API_params\": {\"prompt\": \"make the image like summer.\"}}]\n\"value#\"\nI will use instruct-pix2pix to help to answer the question. Please wait for a moment.\ninstruct-pix2pix model outputs: {'message': 'The instruct-pix2pix has processed the image.'}\nPlease summarize the model outputs and answer my first question: make the image like summer. then generate some attractive texts for Instagram posts\n\"thoughts!\" Thanks to the output of instruct-pix2pix. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nGreat news! I'll utilize the instruct pix2pix tool to help make the image look like a beautiful summer day. Check out the \namazing result below.\nNow, let's explore some Instagram captions that perfectly capture the essence of this new image:\n1.\"Feeling the summer vibes on this mountain pier! \u2600-# #MountainPier #SummerFeels #NatureLovers\"\n2.\"Escape to paradise on this mountain pier piercing the water! %#) #IslandLife #SummerVibes #TravelGoals\"\n3.\"Basking in the tranquility of summer on this breathtaking mountain pier \u2600.% #SummerDays #NatureLover\n#PeacefulRetreat\"\n4.\"Dive into the crystal clear waters of this stunning mountain pier this summer #/. #TropicalVibes\n#SummerCruising #BucketListGoals\"\n5.\"Immerse yourself in the beauty of nature on this serene mountain pier '#% #SummerMood #NatureIsAmazing\n#IslandParadise\"\nI hope these captions spark some creativity for your Instagram posts!\n4 seasons: summer\n4 seasons: autumn\nmake the image like autumn. then generate some attractive texts for Instagram posts\n\"thoughts!\" To complete the image editing task, I need the instruct pix2pix model.\n\"actions\"\" [{\"API_name\": \"instruct-pix2pix\", \"API_params\": {\"prompt\": \"make the image like autumn.\"}}]\n\"value#\"\nI will use instruct-pix2pix to help to answer the question. Please wait for a moment.\ninstruct-pix2pix model outputs: {'message': 'The instruct-pix2pix has processed the image.'}\nPlease summarize the model outputs and answer my first question: make the image like autumn. then generate some attractive texts for Instagram posts\n\"thoughts!\" Thanks to the output of instruct-pix2pix. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nGreat! I'll use instruct pix2pix to edit the image, and you can see the changes below. Now, let's cra! some Instagram\ncaptions for this new image:\n1.\"Embrace the cozy vibes of autumn with this stunning lake view! !\" #AutumnFeels #LakeViews #NatureLover\"\n2.\"Who says fall's colors aren't pretty? Check out this vibrant view of the lake! !# #AutumnVibes #LakeLife\n#ScenicRiver\"\n3.\"Take a moment to soak in the vivid shades of autumn with this breathtaking view! !$ #AutumnHues\n#NaturePhotography #SereneLocation\"\n4.\"Why rush through the season when you have views like this? !% #AutumnDays #LakeEscape #GetawayGoals\"\n5.\"Get lost in the beauty of autumn nature with this captivating view of the lake! \"# #AutumnWonder #LakeLife\n#NatureLover\"\nmake the image like winter. then generate some attractive texts for Instagram posts\n\"thoughts!\" The AI will rely on the instruct pix2pix model for the image modifications.\n\"actions\"\" [{\"API_name\": \"instruct-pix2pix\", \"API_params\": {\"prompt\": \"make the image like winter.\"}}]\n\"value#\"\nI will use instruct-pix2pix to help to answer the question. Please wait for a moment.\ninstruct-pix2pix model outputs: {'message': 'The instruct-pix2pix has processed the image.'}\nPlease summarize the model outputs and answer my first question: make the image like winter. then generate some attractive texts for Instagram posts\n\"thoughts!\" Thanks to the output of instruct-pix2pix. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nGreat! Let's enhance the image to make it look like winter. Please take a look at the edited version below.\nAdditionally, I have also prepared some Instagram captions that can accompany the new image. Please see the options\nbelow:\n1.\"Experience winter wonderland vibes with this stunning image captured by Andrew MacLean. #winterphotography\n#naturelover\"\n2.\"Embrace the winter season with this breathtaking image! Andrew MacLean never fails to amaze us. #winterwonderland\n#naturephotography\"\n3.\"Let this magical winter scene transport you to a world of wonder and awe. Andrew MacLean has truly captured the\nessence of winter in this image. #wintermagic #naturelovers\"\n4.\"This winter wonderland is sure to spread warmth and joy. Andrew MacLean's image captures the beauty of the season\nperfectly. #winterbliss #naturephotography\"\n5.\"Indulge in the serene beauty of winter with this captivating image by Andrew MacLean. It will surely leave you\nmesmerized and inspired. #winterchills #naturelover\"\n4 seasons: autumn\nFigure 12: Multimodal social media post by editing an image and writing a message. Four season of\nthe same image are considered to edit and further associate the text to attract attention of Instagram.\n24\ni want to post a message on Instagram. add some firework to the image, and write an attractive post for my ins.\n\"thoughts!\" My success in completing the task depends on my capability to utilize the functionalities of the instruct pix2pix model, which has been \ntailored for this type of image editing.\n\"actions\"\" [{\"API_name\": \"instruct-pix2pix\", \"API_params\": {\"prompt\": \"add some fireworks\"}}]\n\"value#\"\nI will use instruct-pix2pix to help to answer the question. Please wait for a moment.\ninstruct-pix2pix model outputs: {'message': 'The instruct-pix2pix has processed the image.'}\nPlease summarize the model outputs and answer my first question: i want to post a message on Instagram. add some firework to the image, and write \nan attractive post for my ins.\n\"thoughts!\" Thanks to the output of instruct-pix2pix. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nI have used Instruct pix2pix technique to make the necessary edits and the revised image is now available for your \nviewing pleasure.\nNow, let me share with you some exciting captions to go with our new image on Instagram:\n1.\"Paris shines extra bright with the added sparkle of fireworks $\u2728 #CityOfLight #FireworksFest\"\n2.\"Bonjour, Paris! & It's even more magical with the dazzling fireworks illuminating the night sky '\n#TravelGoals\"\n3.\"Dreaming of a Parisian adventure? (& Our stunning new image with fireworks will inspire your wanderlust \n)* #Wanderlust\"\n4.\"Who needs to wait for New Year's Eve to feel the magic of fireworks? $' Our new image of the Ei\ufb00el Tower \nsparkling in the night sky will transport you to the City of Love \u2764 #BonJourParis\"\n5.\"Experience the enchanting beauty of Paris like never before! , Our new image with added fireworks will \ntransport you to a fairytale setting - #MidnightMagic\"\nFigure 13: Multimodal social media post on fireworks.\nsegment with multi granularity. \n\u201cthoughts!\u201d I will use the Semantic SAM as my segmentation tool for these input point.\n\"actions\"\" [{\"API_name\": \"semantic-sam\", \"API_params\": {\"point\": [0.5, 0.32]}, \"value\": \"I will use semantic-sam to help to answer the question. Please wait for a moment.\"}]\n\"value#\"\nI will use semantic-sam to help to answer the question. Please wait for a moment.\nsemantic-sam model outputs: {'message': 'The semantic-sam has processed the image.'}\nPlease summarize the model outputs and answer my first question: segment with multi granularity. The input point: [0.5, 0.32]\n\"thoughts!\" Thanks to the output of semantic-sam. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nCertainly! We tackle the challenge by utilizing the Semantic-SAM, which divides subjects using the provided point. When 0.5 and \n0.32 are inputted, 6 masks are generated. The outcomes are displayed below.\nThe input point: [0.5, 0.32]\nFigure 14: Multi-granularity segmentation with an user input point, using Semantic SAM.\nIs it possible to perform segmentation based on the\nstroke on the reference image?\n\"thoughts!\" I have decided to utilize the SEEM model in order to improve the \nperformance of segmentation on the image.\n\"actions\"\" [{\"API_name\": \"seem\", \"API_params\": {}}]\n\"value#\"\nI will use semm to help to answer the question. Please wait for a \nmoment.\nseem outputs: {'message': 'The seem has processed the image.'}\nPlease summarize the model outputs and answer my first question: \nIs it possible to perform segmentation based on the stroke on the \nreference image?\n\"thoughts!\" Thanks to the output of seem. I can answer the question better.\n\"actions\"\" []\n\"value#\u201d\nYes, I can definitely help you with the segmentation\nfor the image! I have utilized the powerful seem\nmodel to finish the task. See the results below!\nInput\nReference\nOutput\nFigure 15: Visual referring image segmentation of LLaVA-Plus. Purple is human questions, green is\nLLaVA-Plus response. Users can make a stroke on the reference image (a red curve) as the visual\ntarget to segment, LLaVA-Plus calls SEEM model to predict the corresponding masks in the target\nimage.\n25\n"
  },
  {
    "title": "Prompt Cache: Modular Attention Reuse for Low-Latency Inference",
    "link": "https://arxiv.org/pdf/2311.04934.pdf",
    "upvote": "22",
    "text": "PROMPT CACHE: MODULAR ATTENTION REUSE\nFOR LOW-LATENCY INFERENCE\nIn Gim 1 Guojun Chen 1 Seung-seob Lee 1 Nikhil Sarda 2 Anurag Khandelwal 1 Lin Zhong 1\nABSTRACT\nWe present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing\nattention states across different LLM prompts. Many input prompts have overlapping text segments, such as\nsystem messages, prompt templates, and documents provided for context. Our key insight is that by precomputing\nand storing the attention states of these frequently occurring text segments on the inference server, we can\nefficiently reuse them when these segments appear in user prompts. Prompt Cache employs a schema to explicitly\ndefine such reusable text segments, called prompt modules. The schema ensures positional accuracy during\nattention state reuse and provides users with an interface to access cached states in their prompt. Using a prototype\nimplementation, we evaluate Prompt Cache across several LLMs. We show that Prompt Cache significantly reduce\nlatency in time-to-first-token, especially for longer prompts such as document-based question answering and\nrecommendations. The improvements range from 8\u00d7 for GPU-based inference to 60\u00d7 for CPU-based inference,\nall while maintaining output accuracy and without the need for model parameter modifications.\n1\nINTRODUCTION\nA substantial fraction of large language model (LLM)\nprompts are reused frequently. For example, prompts usu-\nally commence with identical \u201csystem messages\u201d that pro-\nvide initial guidelines for its functionality. Documents can\nalso overlap in multiple prompts. In a wide range of long-\ncontext LLM applications, such as legal analysis (Cui et al.,\n2023; Nay et al., 2023), healthcare applications (Steinberg\net al., 2021; Rasmy et al., 2021), and education (Shen et al.,\n2021), the prompt includes one or several documents from\na pool. Additionally, prompts are often formatted with\nreusable templates (White et al., 2023). Such examples are\ncommon in LLM for robotics (Huang et al., 2022; Driess\net al., 2023), and tool learning (Qin et al., 2023). This fur-\nther results in a high degree of text overlap between prompts\nleveraging the same template.\nWe introduce a novel technique termed Prompt Cache to\nreduce the computational overhead in generative LLM in-\nference. Prompt Cache is motivated by the observation\nthat input prompts served by LLM servers often share com-\nponents in a highly structured manner. The key idea is\nto precompute attention states of the frequently revisited\nprompt segments in memory for reuse.\n1Department\nof\nComputer\nScience,\nYale\nUniversity,\nUSA. {in.gim, guojun.chen, seung-seob.lee, anurag.khandelwal,\nlin.zhong}@yale.edu\n2Google, Labs@Google, Mountain View,\nCalifornia, USA. nikhilsarda@google.com.\nPreliminary work. Under review.\nReusing attention states is a popular strategy for accelerating\nthe service of a single prompt (Ott et al., 2019; Shoeybi\net al., 2019; Pope et al., 2022). The existing approach, often\nreferred to as Key-Value (KV) Cache, reuses the key-value\nattention states of input tokens during the autoregressive\ntoken generation. This eliminates the need to compute full\nattention for every token generation (\u00a7 2.2). By caching the\nkey-value attention computed for the previously generated\ntoken, each token generation requires the computation of\nkey-value attention states only once.\nBuilding on top of KV Cache, Prompt Cache extends atten-\ntion state reuse from a single prompt to multiple prompts\nby making attention state reuse modular. In our approach,\nfrequently reused text segments are individually precom-\nputed and stored in memory. When such \u201ccached\u201d segments\nappear in the input prompt, the system uses the precomputed\nkey-value attention states from memory instead of recom-\nputing them. As a result, attention computations are only\nrequired for uncached text segments. Figure 1 illustrates\nthe difference between full autoregressive generation, KV\nCache, and Prompt Cache. We note that the performance\nadvantage becomes more pronounced as the size of cached\nsegments grows since the computation overhead of attention\nstates scales quadratically with input sequence size (Keles\net al., 2022; Tay et al., 2023) while the storage overhead of\nPrompt Cache scales linearly.\nTwo challenges arise when reusing attention states across\nprompts. First, attention states are position-dependent due to\nthe positional encoding in Transformers. Thus, the attention\narXiv:2311.04934v1  [cs.CL]  7 Nov 2023\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nLLM\nA\nB\nC\nA\nB\n1\n2\n3\n(a) Autoregressive token generation\nLLM\n...\nA\nB\nC\nA\nB\nAttention states\n1\n2\n3\n(b) Generation with KV Cache\nLLM\nAttention states\n...\nPrompt\nCache\nA\nB\nC\nA\nB\n1\n2\n3\n(c) Generation with Prompt Cache\nFigure 1. Comparison of LLM token generation methods, each showing three steps ( 1 to 3 ). Each box indicates a token. Blue boxes\nrepresent the prompt. (a) An LLM takes in a prompt (blue tokens) and predicts the next token ( A ) ( 1 ). It then appends the generated\ntoken ( A ) to the prompt to predict the next token ( B ) ( 2 ). This process, called autoregressive, continues until a stop condition is met.\n(b) KV Cache computes time attention states for the prompt only once ( 1 ) and reuses them in the following steps; (c) Prompt Cache\nreuses the KV state across services to bypass prompt attention computation. Prompt Cache populates its cache when a schema is loaded\nand reuses the cached states for prompts that are derived from the schema ( 1 ). Figure 2 further elaborates Step 1 .\nstates of a text segment can only be reused if the segment\nappears at the same position. Second, the system must be\nable to efficiently recognize a text segment whose attention\nstates may have been cached in order to reuse.\nTo tackle these two problems, Prompt Cache combines two\nideas. The first is to make the structure of a prompt ex-\nplicit with a Prompt Markup Language (PML). PML makes\nreusable text segments explicit as modules, i.e., prompt mod-\nule. It not only solves the second problem above but opens\nthe door for solving the first, since each prompt module can\nbe assigned with unique position IDs. Our second idea is\nour empirical finding that LLMs can operate on attention\nstates with discontinuous position IDs. This means that we\ncan extract different segment of attention states and concate-\nnate them to formulate new meanings. We leverage this to\nenable users to select prompt modules based on their needs,\nor even update some prompt modules during the runtime.\nWe explain how Prompt Cache works in \u00a73. In summary, an\nLLM user writes their prompts in PML, with the intention\nthat they may reuse the attention states based on prompt\nmodules. Importantly, they must derive a prompt from\na schema, which is also written in PML. Figure 2 shows\na example prompt based on an example schema. When\nPrompt Cache receives a prompt, it first processes its schema\nand computes the attention states for its prompt modules.\nIt reuses these states for the prompt modules in the prompt\nand other prompts derived from the same schema.\nIn \u00a74, we report a prototype implementation of Prompt\nCache on top of the HuggingFace transformers library\n(Wolf et al., 2020). While Prompt Cache can work with\nany Transformer architecture compatible with KV Cache,\nwe experiment with three popular Transformer architec-\ntures powering the following open-sourced LLMs: Llama2\n(Touvron et al., 2023), Falcon (Penedo et al., 2023), and\nMPT (MosaicML, 2023). We consider two types of memory\nfor storing prompt modules: CPU and GPU memory. While\nCPU memory can scale to terabyte levels, it brings the over-\nhead of host-to-device memory copying. In contrast, GPU\nmemory does not require coping but has limited capacity.\nUsing the prototype, we conduct an extensive benchmark\nevaluation to examine the performance and quantify the ac-\ncuracy of Prompt Cache across various long-context datasets\n(\u00a75). We employ the LongBench suite (Bai et al., 2023),\nwhich includes recommendation and question-answering\n(QA) tasks based on multiple documents. In our evaluation,\nPrompt Cache reduces time-to-first-token (TTFT) latency\nfrom 1.5\u00d7 to 10\u00d7 for GPU inference with prompt modules\non GPU memory and from 20\u00d7 to 70\u00d7 for CPU inference,\nall without any significant accuracy loss. Additionally, we\nanalyze the memory overhead of the precomputed attention\nstates for each model and discuss directions for optimizing\nthe memory footprint of Prompt Cache. We subsequently\nshowcase several generative tasks, including personalization,\ncode generation, and parameterized prompts, to demonstrate\nthe expressiveness of the prompt schema and performance\nimprovement with negligible quality degradation.\nIn our present study, we mainly focus on techniques for mod-\nular attention reuse. However, we foresee Prompt Cache\nbeing utilized as a foundational component for future LLM\nserving systems. Such systems could incorporate enhanced\nprompt module management and GPU cache replacement\nstrategies, optimizing the advantages of both host DRAM\nand GPU HBM.\n2\nBACKGROUND AND RELATED WORK\nPrompt Cache builds on the ideas of the KV Cache, i.e., key-\nvalue attention state reuse during autoregressive decoding in\nLLMs. This section reviews autoregressive token generation\nin LLMs, explains how the incorporation of KV Cache\ncan speed up the token generation process, identifies its\n2\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\napproximations, and surveys recent work that leverages the\nKV Cache for acceleration. We also briefly discuss other\ntechniques for accelerating LLM inference.\n2.1\nAutoregressive Token Generation\nAn LLM generates output tokens autoregressively (Radford\net al., 2018). It starts with an initial input, often called a\nprompt, and generates the next token based on the prompt.\nThe model then appends the token to the prompt and uses it\nto generate the next token. The generation process continues\nuntil a stopping condition is met. This could be after a\npredetermined number of tokens, upon generating a special\nend-of-sequence token, or when the generated sequence\nreaches a satisfactory level of coherence or completeness.\nImportantly, in each step, the model takes the entire prompt\nand tokens generated so far as the input.\n2.2\nKey-Value Cache\nAutoregressive token generation described above incurs sub-\nstantial computation due to the self-attention mechanism\nbeing applied over the entirety of input during each step. To\nameliorate this, the Key-Value (KV) Cache mechanism (Ott\net al., 2019; Shoeybi et al., 2019; Pope et al., 2022) is fre-\nquently used. This technique computes the key and value\nembeddings for each token only once throughout the autore-\ngressive token generation.\nTo elaborate, denote a user prompt as a sequence of\nn tokens:\ns1, s2, . . . , sn, and the subsequently gen-\nerated k tokens as sn+1, sn+2, . . . , sn+k.\nIn full\nautoregressive token generation,\nthe attention states\n{(k1, v1), (k2, v2), . . . , (kn+k, vn+k)} are fully recalcu-\nlated at every step. In contrast, KV Cache initially com-\nputes attention states for the input, represented by S0 =\n{(ki, vi)|i \u2264 n}, and caches them in memory. For ev-\nery subsequent step j \u2264 k, the model reuses the cached\nvalues Sj = {(ki, vi)|i < n + j} to compute the atten-\ntion state (kn+j, vn+j) of the new token sn+j. This ap-\nproach significantly reduces the computation required for\nself-attention. Specifically, the computation in each step,\nmeasured in FLOPs for matrix operations, is reduced from\n(Q \u00d7 X) \u00d7 (K \u00d7 X)T \u00d7 (V \u00d7 X) operations\u2014where\nX \u2208 R(n+j)\u00d7d represents the input embedding matrix, and\nQ, K, V \u2208 Rd\u00d7d\u2014to xT Q + xT K + xT V + XT X oper-\nations, with x \u2208 Rd. After each step, the newly computed\nattention states are appended to the cache for subsequent\nuse, such that Sj = Sj\u22121 \u222a {(kn+j, vn+j)}.\nThe KV Cache optimizes computational efficiency at the\nexpense of precision. Instead of computing the attention\nstate for token sj over the entire sequence {si|i < n + k},\nthe computation is restricted to the sequence available at step\nj, namely {si|i < n + j}. Despite this trade-off, empirical\nresults have shown that output quality is largely maintained,\nmaking it a popular choice for LLM serving systems.\nThe KV Cache has catalyzed further exploration into LLM\nacceleration. Ensuing studies have either centered on refin-\ning memory management for KV Cache, as demonstrated\nin paged attention (Kwon et al., 2023), on pruning super-\nfluous KV Cache data (Zhang et al., 2023), or compressing\nit (Liu et al., 2023b). There are some preliminary works\nthat explore KV Cache reuse across different requests as\nwell. (Feng et al., 2023) reuse memorized attention states\nbased on an embedding similarity metric. Paged attention\nalso demonstrates simple prefix sharing, where different\nprompts with an identical prefix share KV Cache. However,\nexisting approaches are specific to certain scenarios, while\nwe investigate attention reuse for general LLM prompts.\n2.3\nOther Methods for Low-Latency LLM Inference\nPrompt Cache introduces an orthogonal optimization strat-\negy that augments existing systems dedicated to efficient\nLLM inference. This includes systems that utilize multiple\nGPUs for inference (Aminabadi et al., 2022) and those with\nhigh-performance GPU kernels for softmax attention score\ncomputation (Dao et al., 2022). Although our current fo-\ncus is on achieving low-latency inference in LLMs, Prompt\nCache can also benefit systems aiming for high through-\nput (Sheng et al., 2023) as well.\n3\nDESIGN OF PROMPT CACHE\nThe effectiveness of the KV Cache leads us to the next\nquestion: Can attention states be reused across multiple\ninference requests?\nWe observe that different prompts often have overlapping\ntext segments. For example, identical \u201csystem messages\u201d,\nor metaprompts are frequently inserted at the beginning of\na prompt to elicit desired responses from an LLM. For an-\nother example, in many legal and medical applications of\nLLMs (Cui et al., 2023; Steinberg et al., 2021; Rasmy et al.,\n2021), the same set of documents is often provided as con-\ntext to different prompts. Finally, reusable prompt formats,\ni.e., prompt templates, are commonly used by LLM applica-\ntions in robotics (Driess et al., 2023) and tool learning (Qin\net al., 2023).\nIn this section, we describe our approach called Prompt\nCache, which answers the above question affirmatively.\nPrompt Cache improves computational efficiency through\ninter-request attention state reuse by leveraging the text\nshared by prompts.\n3.1\nOverview\nThe attention states of a text segment can only be reused\nif the segment appears at the same position in the LLM\n3\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nSchema\n<schema name=\"cities\">\n  <module name=\"city-info\">...\n  </module> \n  <module name=\"trip-plan\"> ...\n    <param name=\"duration\" len=2/>\n  </module>\n  <module name=\"tokyo\"> ... \n  </module>\n  <module name=\"miami\"> ... \n  </module> ... \n</schema>\nPrompt\nLLM\nPrompt attention states\n<prompt schema=\"cities\">\n  <trip-plan duration=  \"3 days\"  /> <miami/>  Highlight the surf spots.\n</prompt>\nPrompt Cache\nempty space\n1\n2\n2\n3\n4\n5\n<city-info/>\n<trip-plan/>\n<tokyo/>\n<miami/>\n<paris/>\nFigure 2. Reuse mechanism in Prompt Cache: (i) First, PML (\u00a73.2) makes reusable prompt modules explicit in both Schema and Prompt.\nA prompt module can have parameters like trip-plan. A prompt importing the module supplies a value (3 days) to the parameter\n(duration). The prompt can include new text segments in place of excluded modules and parameters and at the end. (ii) Second,\nprompt module encoding (\u00a7 3.3) precomputes attention states ( 1 ) for all modules in the schema and caches them for future reuse. (iii)\nThird, when the prompt is served, Prompt Cache employs cached inference (\u00a73.4): it retrieves the attention states cached for imported\nprompt modules ( 2 , computes them for parameters ( 3 ) and new text segments ( 4 ), and finally concatenates them to produce the\nattention states for the entire prompt ( 5 ). This figure is an elaboration of Step 1 in Figure 1c.\ninput. This is because transformer architectures integrate\npositional embeddings into the (k, v) attention states. This\nis not a problem for KV Cache serving a single prompt: the\nsame prompt text is located at the same position, i.e., the\nbeginning of the input, in all steps.\nShared text segments, on the other hand, can appear in differ-\nent positions in different prompts. To reuse their attention\nstates across prompts, a caching system must tackle two\nproblems. First, it must allow reuse despite a text segment\nappearing in different positions in different prompts. Sec-\nond, the system must be able to efficiently recognize a text\nsegment whose attention states may have been cached in\norder to reuse.\nTo tackle these two problems, we combine two ideas. The\nfirst is to make the structure of a prompt explicit with a\nPrompt Markup Language (PML). As illustrated by Figure 2,\nthe PML makes reusable text segments explicit as modules,\ni.e., prompt module. It not only solves the second problem\nabove but opens the door for solving the first, since each\nprompt module can be assigned with unique position IDs.\nOur second idea is our empirical observation that LLMs can\noperate on attention states with discontinuous position IDs.\nFor instance, we can extract different segment of attention\nstates and concatenate them to formulate new meanings.\nEven though such concatenated attention states may miss\nsome position IDs in their embeddings, it does not affect the\noutput quality since the relative positional semantics are still\nretained. We leverage this to enable users to select prompt\nmodules based on their needs.\nPrompt Cache puts these two ideas together as follows. An\nLLM user writes their prompts in PML, with the intention\nthat they may reuse the attention states based on prompt\nmodules. Importantly, they must derive a prompt from\na schema, which is also written in PML. Figure 2 shows\na example prompt based on an example schema. When\nPrompt Cache receives a prompt, it first processes its schema\nand computes the attention states for its prompt modules.\nIt reuses these states for the prompt modules in the prompt\nand other prompts derived from the same schema.\nWe detail the design of PML in \u00a73.2 with a focus on tech-\nniques that maximize the opportunity of reusing. We explain\nhow Prompt Cache computes the attention states of prompt\nmodules in a schema in \u00a73.3. We explain how Prompt\nCache reuse attention states from a schema for the service\nof a prompt in \u00a73.4.\nThe modular KV cache construction in Prompt Cache bears\nresemblance to the approximations observed in locally\nmasked attention (Beltagy et al., 2020; Tay et al., 2023),\nwhich optimizes computations by setting a limited window\nfor attention score calculations rather than spanning its at-\ntention across every token in its input sequence. Consider a\nscenario within Prompt Cache where each prompt module\nis encoded independently. Given that attention states are\nstrictly calculated within the confines of the prompt mod-\nule, this closely mirrors the setup of an attention mask that\nscreens out sequences external to the prompt module. There-\nfore, the approximation made by Prompt Cache is to limit\nthe attention window to each prompt module. We note that\nemploying such attention masks does not necessarily reduce\noutput quality, as we will discuss in \u00a75. In some contexts,\nthese masks may even introduce beneficial inductive biases\nby effectively filtering out irrelevant information.\n3.2\nPrompt Markup Language (PML)\nWe next describe the key features of PML that is used to\ndescribe both schemas and prompts.\n4\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\n3.2.1\nSchema vs. Prompt\nA schema is a document that defines prompt modules and\ndelineates their relative positions and hierarchies. Each\nschema has a unique identifier (via the name attribute) and\ndesignates prompt modules with the <module> tag. Texts\nnot enclosed by <module> tags are treated as anonymous\nprompt modules and are always included in prompts that\nutilize the schema.\nFor an LLM user, the schema serves as an interface to create\nand reuse attention states for prompt modules. The user can\nconstruct a prompt from a schema, with the <prompt> tag.\nThis tag specifies the schema to use through the schema\nattribute, lists the prompt modules to import, and adds any\nadditional (non-cached) instructions. For example, to import\nthe module miami from the schema in Figure 2, one would\nexpress it as <miami/>. Prompt Cache will only compute\nthe attention states for the text that is not specified in the\nschema, e.g., Highlights the surf spots in Figure 2,\nand reuse those for the imported modules, e.g., trip-plan\nand miami in Figure 2.\n3.2.2\nMaximizing Reuse with Parameters\nPML allows a prompt module to be parameterized in or-\nder to maximize the reuse opportunities. A parameter is a\nnamed placeholder with a specified length that can appear\nanywhere in a prompt module in a schema. It is defined\nusing the <param> tag, with the name and len attributes\nindicating its name and the maximum number of tokens\nfor the argument, respectively. When a prompt imports\nthe prompt module, it can supply a value to the parame-\nter. Figure 2 shows an example of a paramterized prompt\nmodule (trip-plan) and how a prompt would include the\nprompt module and supply a value (3 days) to its argument\n(duration).\nThere are two important uses of parameterized prompt mod-\nules. First, it is common that a prompt module differs from\nanother only in some well-defined places. Parameters allow\nusers to provide specific arguments to customize the module\nat runtime and still benefit from reusing. Figure 2 illustrates\nthis use case with trip-plan. This is especially useful for\ntemplated prompts. Second, a parameter can be used to add\na \u201cbuffer\u201d at the beginning or end of a prompt module in\nthe schema. This buffer allows the user to add an arbitrary\ntext segment in a prompt as long as the segment is no longer\nthan the parameter it replaces.\n3.2.3\nOther Features\nUnion modules: Certain prompt modules exhibit mutually\nexclusive relationships. That is, within a set of modules,\nonly one should be selected. For instance, consider a prompt\nthat asks the LLM to suggest a book to read based on the\nreader\u2019s profile described by a prompt module. There could\nbe multiple prompt modules each describing a reader profile\nbut the prompt can include only one of them.\nTo accommodate these exclusive relationships, we introduce\nthe concept of a union for prompt modules. A union of\nmodules is denoted using the <union> tag. For example:\n<union>\n<module name=\"doc-en-US\"> ... </module>\n<module name=\"doc-zh-CN\"> ... </module>\n</union>\nPrompt modules nested within the same union share the\nsame starting position ID. A union not only streamlines the\norganization of the layout but also conserves position IDs\nused to encode prompt modules. Further, the system can\nutilize this structure for optimizations, such as prefetching.\nWhile parameterized modules and unions appear to be simi-\nlar, they are different in two aspects. First, as we will show\nin \u00a73.3, parameters and union modules are encoded in differ-\nent ways. Second, they serve different purposes: parameters\nare used for minor inline modifications to maximize the\nreuse of a module, while union modules are intended for\nbetter prompt structure and more efficient use of position\nIDs.\nNested modules: PML also supports nested modules to\nexpress hierarchical prompt modules. That is, a prompt\nmodule could include prompt modules or unions as compo-\nnents.\nCompatibility with LLM-specific template: Instruction-\ntuned LLMs often adhere to specific templates to for-\nmat conversations.\nFor example, in Llama2, a single\ninteraction between the user and the assistant follows\nthe\ntemplate:\n<s>[INST] user message [/INST]\nassistant message </s>. To reduce the effort required\nto manually format the prompt schema to match such tem-\nplates for different LLMs, we introduce three dedicated\ntags: <system> for system-level prompts, <user> for\nuser-generated prompts, and <assistant> for exemplar re-\nsponses generated by the LLM. Prompt Cache dynamically\ntranslates these specialized tags to align with the designated\nprompt template of the LLM in use.\n3.3\nEncoding Schema\nThe first time the attention states of a prompt module are\nneeded, they must be computed, which we refer to as prompt\nmodule encoding.\nFirst, Prompt Cache extracts token sequences of a prompt\nmodule from the schema. It then assigns position IDs to\neach token. The starting position ID is determined by the\nabsolute location of the prompt module within the schema.\nFor instance, if two preceding prompt modules have token\n5\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nsequence sizes of 50 and 60 respectively, the prompt module\nis assigned a starting position ID of 110. An exception exists\nfor the union modules. Since prompt modules within the\nunion start from the same positions, their token sequence\nsize is considered with the size of the largest child.\nFrom the token sequences of the prompt module and the\ncorresponding position IDs, these are then passed to the\nLLM to compute the (k, v) attention states. We note that\nthe assigned position IDs do not start from zero. This is\nsemantically acceptable since white spaces do not alter the\nmeaning of the precomputed text. However, many existing\ntransformer positional encoding implementations often re-\nquire adaptations to accommodate discontinuous position\nIDs, which we will discuss in (\u00a7 4.2).\nFor encoding parameterized prompt modules, we use the\nidea that having white space in a prompt does not affect\nits semantics. Parameters are replaced by a predetermined\nnumber of <unk> tokens, equivalent to their len value.\nThe position IDs corresponding to these <unk> tokens are\nlogged for future replacement. When this module is inte-\ngrated into a user\u2019s prompt and paired with the relevant\narguments, the token sequences of these supplied arguments\nadopt the position IDs previously linked with the <unk>\ntokens. The resulting KV attention states then replace the\nstates initially allocated for the <unk> tokens. We note that\nthe length of the newly provided tokens can be smaller than\nthe specified parameter length, as trailing white spaces do\nnot change the semantics.\n3.4\nCached Inference\nWhen a prompt is provided to Prompt Cache, Prompt Cache\nparses it to ensure alignment with the claimed schema. It\nverifies the validity of the imported modules. Then, as\nillustrated in Figure 2, Prompt Cache retrieves the (k, v)\nattention states for the imported prompt modules from the\ncache ( 2 ), computes those for new text segments ( 3 and\n4 ), and concatenates them to produce the attention states\nfor the entire prompt ( 5 ).\nTo detail the process, Prompt Cache starts by concatenating\nthe KV state tensors corresponding to each imported prompt\nmodule in the prompt. For instance, when a user prompt\nutilizes modules A, B, the concatenated KV tensor is formu-\nlated as: (kC, vC) = (concat(kA, kB), (concat(vA, vB)). It\nis worth noting that the order of concatenation does not mat-\nter due to the permutation invariance of transformers (Dufter\net al., 2022). This step solely requires memory copy.\nThen, Prompt Cache computes the attention states for the\nsegments of the prompt that are not cached, specifically,\ntoken sequences not defined in the schema and arguments\nfor parameterized prompt modules. Prompt Cache first iden-\ntifies the position IDs of uncached texts based on their posi-\ntion relative to other utilized prompt modules. For example,\nif the text is situated between module A and B, it is assigned\nthe position ID starting from the concluding positions of\nA, assuming gaps exist between the positions of A and B.\nAugments for parameterized prompt modules are assigned\nto the position IDs of <unk> tokens. Subsequently, the to-\nken sequences and position IDs are aggregated and passed\nto the LLM using (kC, vC) as a KV Cache, to compute the\nattention states for the entire prompt.\nIt is important to note that the computational complexity for\ngenerating subsequent tokens remains consistent with that of\nKV Cache, as prompt modules are not employed beyond the\ninitial token. In essence, Prompt Cache primarily diminishes\nthe latency involved in producing the first token.\n4\nIMPLEMENTATION\nWe build a Prompt Cache prototype using the Hugging-\nFace transformers library (Wolf et al., 2020) in PyTorch\nand comprises 3K lines of Python code. We aim to seam-\nlessly integrate with an existing LLM codebase and reuse\nits weights. We implement Prompt Cache to use both CPU\nand GPU memory to accommodate prompt modules and\nevaluate it on both platforms.\n4.1\nStoring Prompt Modules in Memory\nWe store encoded prompt modules in two types of memory:\nCPU memory (host DRAM) and GPU memory (HBM). To\nmanage tensors across both memory types, we employ the\nPyTorch (Paszke et al., 2019) memory allocator. Beyond\nsimply pairing CPUs with prompt modules in CPU memory\nand GPUs with GPU memory, we also enable GPUs to\naccess prompt modules stored in CPU memory. This is done\nby copying the prompt modules from the host to the device\nas needed. This process incurs a host-to-device memory\ncopy overhead. Nonetheless, it allows the GPU to leverage\nthe abundant CPU memory, which can scale up to terabyte\nlevels. As we will show in \u00a75, the computational savings\nfrom Prompt Cache more than compensate for the latencies\ncaused by memory copy operations.\nUsing GPUs exposes trade-offs between memory capacity\nand latency: GPU memory is faster but limited in capacity,\nwhile CPU memory can scale easily yet incurs additional\nmemory copy overhead. It appears feasible to contemplate\na caching mechanism that leverages both CPU and GPU\nmemory. We leave the development of a system that in-\ncorporates cache replacement and prefetching strategies to\nfuture research.\n4.2\nAdapting Transformer Architectures\nImplementing Prompt Cache requires support for discon-\ntinuous position IDs (\u00a73.2). Although the Transformers\n6\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nBaseline\nPrompt Cache (CPU memory)\nPrompt Cache (GPU memory)\nNVIDIA A40\nNVIDIA A100\nNVIDIA RTX 4090\nFigure 3. GPU latency measurements: TTFT for eight LongBench\ndatasets across three NVIDIA GPUs.\nlibrary currently does not offer these features, they can be\nintegrated with minor modifications. For instance, approxi-\nmately 20 lines of additional code are needed for each LLM.\nWe outline the required adjustments:\nEmbedding Tables\nEarly models like BERT (Vaswani\net al., 2023) and GPT-2 (Radford et al., 2018) use lookup\ntables for mapping position IDs to learned embeddings or\nfixed bias, requiring no alterations.\nRoPE\nLLMs such as Llama2 (Touvron et al., 2023) and\nFalcon (Penedo et al., 2023) adopt RoPE (Su et al., 2021),\nwhich employs rotation matrices for positional encoding in\nattention computations. We create a lookup table for each\nrotation matrix, enabling retrieval based on position IDs.\nALiBi\nUtilized in models like MPT (MosaicML, 2023)\nand Bloom (Scao et al., 2022), ALiBi (Press et al., 2022)\nintegrates a static bias during softmax score calculations.\nAnalogous to RoPE, we design a lookup table to adjust the\nbias matrix according to the provided position IDs.\n5\nEVALUATION\nOur evaluation of Prompt Cache focuses on answering the\nfollowing three research questions. (i) First, we benchmark\nthe impact of Prompt Cache on time-to-first-token (TTFT)\nlatency (\u00a75.2,\u00a75.4) and output quality (\u00a75.3) on extensive\nLLM datasets. (ii) Then we analyze the memory storage\noverhead of Prompt Cache (\u00a75.5) on a per-token basis. (iii)\nIntel i9-13900K\nAMD Ryzen 9 7950X\nBaseline\nPrompt Cache (CPU memory)\nFigure 4. CPU latency measurements: TTFT for eight LongBench\ndatasets across two CPUs.\nFinally, we demonstrate a set of LLM applications where\nPrompt Cache can have a significant effect (\u00a75.6).\nWe use the LLM inference with KV Cache (Pope et al.,\n2022) as our baseline. Prompt Cache and KV Cache share\nthe exact same inference pipeline except for attention state\ncomputation. We use TTFT latency for comparison, which\nmeasures the time to generate the first token, as Prompt\nCache and KV Cache have the same decoding latency after\nthe first token.\n5.1\nEvaluation Environment\nWe evaluate Prompt Cache on two CPU configurations: an\nIntel i9-13900K accompanied by 128 GB DDR5 RAM at\n5600 MT/s and an AMD Ryzen 9 7950X paired with 128\nGB DDR4 RAM at 3600 MT/s. For our GPU benchmarks,\nwe deploy three NVIDIA GPUs: the RTX 4090, which is\npaired with the Intel i9-13900K, and the A40 and A100,\nboth virtual nodes hosted on NCSA Delta, each provisioned\nwith a 16-core AMD EPIC 7763 and 224 GB RAM.\nWe employ several open-source LLMs, including Llama2,\nCodeLlama, MPT, and Falcon. We use LLMs that fit within\nthe memory capacity of a single GPU (40 GB).\nWe utilize the LongBench suite (Bai et al., 2023) to assess\nTTFT improvements and output quality changes. Long-\nBench encompasses a curated subsample of elongated data,\nranging from 4K to 10K context length, excerpts from\n21 datasets across 6 categories, including tasks like multi-\ndocument question answering (Yang et al., 2018; Ho et al.,\n2020; Trivedi et al., 2022; Ko\u02c7cisk`y et al., 2018; Joshi et al.,\n2017), summarization (Huang et al., 2021; Zhong et al.,\n2021; Fabbri et al., 2019), and code completion (Guo et al.,\n2023; Liu et al., 2023a). We defined the documents in the\nLongBench datasets, such as wiki pages and news articles,\nas prompt modules. We kept the task-specific directives as\n7\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nuncached user text.\n5.2\nLatency Improvements on Benchmark Datasets\nWe measured the TTFT latency on both GPU and CPU using\nLlama 7B, as shown in Figure 3 and Figure 4. In our GPU\nevaluation, we used two memory setups: storing prompt\nmodules in either CPU or GPU memory. For CPU experi-\nments, we used CPU memory. Due to space constraints, we\npresent only 8 benchmarks. The complete benchmark can\nbe found in the Appendix A.\n5.2.1\nGPU Inference Latency\nWe summarize our findings in Figure 3, evaluated on three\nNVIDIA GPUs: RTX 4090, A40, and A100. Yellow bars\nrepresent loading prompt modules from CPU memory, while\nblue bars represent the case in GPU memory. There is a con-\nsistent latency trend across the datasets since the LongBench\nsamples have comparable lengths, averaging 5K tokens.\nWe observe significant TTFT latency reductions across all\ndatasets and GPUs, ranging from 1.5\u00d7 to 3\u00d7 when using\nCPU memory, and from 5\u00d7 to 10\u00d7 when employing GPU\nmemory. These results delineate the upper and lower bounds\nof latency reductions possible with Prompt Cache. The\nactual latency reduction in practice will fall between these\nbounds, based on how much of each memory type is used.\n5.2.2\nCPU Inference Latency\nFigure 4 shows that Prompt Cache achieves up to a 70\u00d7\nand 20\u00d7 latency reduction on the Intel and AMD CPUs,\nrespectively. We surmise that this disparity is influenced\nby the difference in memory bandwidth in system setups\n(5600MT/s DDR5 RAM on the Intel CPU versus 3600MT/s\nDDR4 RAM on the AMD CPU). As expected, the latency is\nhigher for the datasets with a larger proportion of uncached\nprompts, such as TriviaQA. Interestingly, CPU inference\nbenefits more significantly from Prompt Cache than GPU\ninference does. This is attributed to the much greater la-\ntency of attention computation in the CPU, especially as the\nsequences become longer (e.g., lower FP16/FP32 FLOPs\ncompared to GPU).\n5.3\nAccuracy with Prompt Cache\nTo verify the impact of Prompt Cache on the quality of\nLLM response, we measure accuracy scores with the Long-\nBench suite. To demonstrate general applicability, we apply\nPrompt Cache to the three LLMs having different trans-\nformer architectures (\u00a74.2): Llama2, MPT, and Falcon.\nThe accuracy benchmark results shown in Table 1 demon-\nstrate Prompt Cache preserves the precision of the output.\nWe use deterministic sampling where the token with the\nCache advantage\nin CPU\nCache advantage\nin A40\nCache advantage\nin RTX 4090\nFigure 5. Cache advantage: A comparison of computational and\ncaching overheads in GPUs and CPUs. While attention computa-\ntion cost increases quadratically, the attention state memory copy\noverhead (i.e., Prompt Cache) rises linearly. Here, GPUs load\nprompt modules directly from CPU memory.\nhighest probability is chosen at every step so that the results\nwith and without Prompt Cache are comparable. Across\nall datasets, the accuracy of output with Prompt Cache is\ncomparable to the baseline.\n5.4\nUnderstanding Latency Improvements\nTheoretically, Prompt Cache should offer quadratic TTFT\nlatency reduction over regular KV Cache. This is because,\nwhile Prompt Cache\u2019s memcpy overhead grows linearly with\nsequence length, computing self-attention has quadratic\ncomputational complexity with respect to sequence length.\nTo validate this, we tested Prompt Cache on a synthetic\ndataset with varied sequence lengths, assuming all prompts\nwere cached. We compared the TTFT latency of Prompt\nCache to that of regular KV Cache using an Intel i9-13900K\nCPU and two GPUs (NVIDIA RTX 4090 and A40) with the\nLlama2 7B model. For both CPU and GPU, CPU memory\nis used for prompt module storage.\nQuadratic improvement: Our findings, presented in Fig-\nure 5, show that KV Cache\u2019s latency increases quadratically\nwith sequence length, while Prompt Cache\u2019s memory copy\ncost grows linearly. This means that the latency advantage\nof Prompt Cache (the gap between the two curves) expands\nquadratically with sequence length. This difference is more\npronounced on CPUs than GPUs since CPUs experience\nhigher attention computation latencies, whereas the dispar-\nity between Prompt Cache\u2019s overhead, i.e., host-to-device\nmemcpy in GPUs and host-to-host memcpy in CPUs is not\nsignificant. With attention states with 5K tokens, latency for\nhost-to-host, host-to-device, and device-to-device memcpy\nare respectively 3.79 ms, 5.34 ms, and 0.23 ms.\nEffect of model size: Furthermore, as the model\u2019s parameter\nsize grows, so does the computational overhead for KV\n8\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nDataset\nMetric\nLlama2 7B\nLlama2 13B\nMPT 7B\nFalcon 7B\nBaseline\nCached\nBaseline\nCached\nBaseline\nCached\nBaseline\nCached\nNarrative QA\nF1\n19.93\n19.38\n20.37\n19.94\n10.43\n11.33\n7.14\n8.87\n2 Wiki Multi-Hop QA\nF1\n16.63\n13.95\n14.59\n17.69\n10.44\n13.70\n14.42\n15.07\nMuSiQue\nF1\n7.31\n8.57\n10.03\n12.14\n7.38\n7.32\n4.81\n5.86\nGovReport\nRouge L\n24.67\n25.37\n28.13\n28.18\n26.96\n27.49\n22.39\n23.40\nQMSum\nRouge L\n19.24\n19.46\n18.80\n18.82\n15.19\n15.51\n12.84\n12.96\nMultiNews\nRouge L\n24.33\n24.22\n25.43\n26.23\n25.42\n25.66\n20.91\n21.19\nTriviaQA\nF1\n13.04\n12.33\n23.19\n22.38\n10.57\n9.17\n13.31\n11.42\nPassage Retrieval\nAcc\n7.50\n4.25\n9.08\n6.50\n3.03\n3.85\n3.00\n3.45\nTable 1. Accuracy benchmarks on LongBench datasets. We mark the outliers as bold, of which the performance is higher than 2.5\ncompared to the counter part.\nLLM\nBERT\nFalcon 1B\nLlama 7B\nLlama 13B\nMB/token\n0.03\n0.18\n0.50\n0.78\nLLM\nMPT 30B\nFalcon 40B\nLlama 70B\nFalcon 180B\nMB/token\n1.31\n1.87\n2.5\n4.53\nTable 2. Memory overhead of caching a single token\nCache. For example, moving from a 7B to 13B model at a\ntoken length of 3K added 220 ms latency, whereas Prompt\nCache added only 30 ms. This difference stems from the\nfact that LLM complexity also scales quadratically with\nhidden dimension size.\nEnd-to-end latency: Since Prompt Cache reduces only\nTTFT, its impact on the time needed to receive the complete\nLLM response diminishes as the number of generated to-\nkens increases. For instance, on the RTX 4090 with Llama\n7B for 3K context, Prompt Cache enhances TTFT from 900\nms to 90 ms, while the token generation time or the time-to-\nsubsequent-token (TTST) remains consistent between KV\nCache and Prompt Cache at an average of 32 ms per token,\nregardless of the token length. Nonetheless, a quicker re-\nsponse time contributes positively to the user experience and\nthe overall end-to-end latency (Lew et al., 2018; Liu et al.,\n2023b), For instance, Given that Prompt Cache enhances\nTTFT from 900 ms to 90 ms, this equates to the generation\nof 25 more tokens within the same timeframe.\n5.5\nMemory Overhead\nThe memory overhead associated with Prompt Cache is\nproportional to the aggregated number of tokens cached.\nThis overhead can be determined by referencing both the\nprompt schema and the target LLM. In Table 2, we eluci-\ndate the memory overhead on a per-token basis, under the\nassumption of utilizing a 16-bit precision for floating points.\nFor compact models, such as Falcon 1B, caching a docu-\nment containing 1K tokens would require approximately\n180 MB of memory. If there are hundreds of prompt mod-\nules, the combined memory consumption would range in the\ntens of gigabytes\u2014a quantity within the memory confines\nof server-grade GPUs. Conversely, for larger models like\nLlama 70B, caching a 1K length module would command a\nsubstantial 2.5 GB of memory per document, which leaves\nCPU memory as the only option for prompt module stor-\nage. Given these considerations, compression techniques\nfor attention states (Zhang et al., 2023) remain an avenue\nfor future research in this domain.\n5.6\nApplications of Prompt Cache\nWe demonstrate the expressiveness of PML with example\nuse cases that require more complicated prompt structures\nand advanced features (\u00a73.2) than the LongBench suite: (i)\nmultiple modules in a query, (ii) union, and (iii) parame-\nterization. Furthermore, these tasks underscore the notable\nlatency reduction as the number of cached tokens increases\nin such complicated use cases. Across use cases, we provide\na qualitative assessment of the output by juxtaposing cached\nand non-cached generation, showcasing that Prompt Cache\nmaintains output quality, along with the latency reductions\nachieved by Prompt Cache. We use Llama2 7B and store\nprompt modules in the local memory (i.e., GPU memory for\nGPU inference). The full schema for these tasks is available\nin Appendix B.\n5.6.1\nCode Generation\nLLMs are commonly used for code generation (Guo et al.,\n2023; Liu et al., 2023a), aiding programmers in either as-\nsisting with or directly generating code. Currently available\nmethods, such as Copilot (GitHub, 2023), typically focus on\nindividual source files. Prompt Cache, however, can extend\nthis to multiple files leveraging a modular nature of source\ncode. For instance, each class or function could be a distinct\nprompt module.\nFigure 6 illustrates multi-source code generation using\nCodeLlama 7B (Rozi`ere et al., 2023). We treat classes\nlike Unit, Map, and Player as prompt modules in our\n9\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\n<unit.py/>\n<map.py/>\n<player.py/>\n<game.py/>\n<database.py/>\n<user>\nCreate a main entry point for \nthe game, using Map, Player, \nand Game classes.\n</user>\n    map = Map(grid_size=100)\n    player1 = Player(player_id=1, name='Player 1')\n    player2 = Player(player_id=2, name='Player 2')\n    game = Game(players=[player1, player2], map=map)\n    game.start_game()\n    map = Map(grid_size=100)\n    player1 = Player(player_id=1, name='Player 1')\n    player2 = Player(player_id=2, name='Player 2')\n    game = Game(map=map, players=[player1, player2])\n    game.start_game()\nBaseline\nPrompt Caching\nUser prompt\n(GPU: 924ms, CPU: 75,976ms)\n(GPU: 93ms, CPU: 861ms)\nFigure 6. Code generation using Prompt Cache: Each source file\nbecomes a prompt module, allowing users to \u201cimport\u201d files as\ncontext with minimal overhead.\n<middle-school/>\n<beginner/>\n<studied-a-year-before/>\n<auditory/>\n<essay/>\n<high-intrinsic-motivation/>\n<user>\nConcisely describe the learn-\ner\u2019s profile.\n</user>\nThe learner is a middle school student transi-\ntioning from elementary school, \u2026 They are at the \nbeginning stage of learning new subjects \u2026 learn-\ning style, with a preference for auditory learn-\ning \u2026 They are motivated by intrinsic factors, \u2026 \nThe learner is a middle school student transi-\ntioning from elementary school, \u2026 They are at the \nbeginning stage of their learning journey \u2026 The \nlearner's preferred learning style is auditory, \u2026 \nthey have a high intrinsic motivation for \u2026 \nBaseline\nPrompt Caching\nUser prompt\n(GPU: 216ms, CPU: 22,449ms)\n(GPU: 65ms, CPU: 686ms)\nFigure 7. Personalization example: Six categories each have five\ntraits. Traits in the same category are grouped in <union>.\nschema for game programming. Users can then integrate\nthese classes similarly to code imports. There is a 4\u00d7 im-\nprovement in TTFT latency on GPUs while the output is\nalmost identical.\n5.6.2\nPersonalization\nFigure 7 shows the latency benefits and the output quality of\nPrompt Cache in a personalization use case. Personalization\nis integral to many recommender systems (Wu et al., 2023),\nfinding prominent applications in LLM contexts such as ed-\nucation, content recommendations, and targeted marketing.\nWe highlight the efficacy of feature-based personalization\nthrough Prompt Cache. Here, personalization hinges on\na defined set of features. Each feature is represented as a\ndistinct prompt module, with relationships between features\ndenoted using union tags such as grade level, proficiency,\nlearning history, learning style, and assessment type.\n5.6.3\nParameterized Prompts\nIn Figure 8, we show a trip planning use case leveraging\nparameterization (\u00a73.2). The schema used in this use case\nencompasses one adjustable parameter to specify the trip\nduration along with two union modules to select the desti-\nnation. Users can reuse the templated prompt with custom\nparameters, enjoying lower TTFT latency and the same\nquality of LLM response enabled by Prompt Cache.\n<travel-plan for=\"a week\">\n  <overseas>\n    <tokyo/>\n  </overseas>\n</travel-plan>\n<user>\nCreate a travel plan\n</user>\nGreat! Based on your preferences, I have created \na 7-day travel plan for you to explore the vi-\nbrant city of Tokyo, Japan \u2026 Day 1: Arrival and \nExploration of Tokyo * Arrive at Narita or Haneda \nAirport and transfer to your hotel \u2026 * Visit the \nfamous Shibuya Crossing  \u2026\nGreat, I'd be happy to help you plan your trip to \nTokyo, Japan! Here's a 7-day itinerary that in-\ncludes \u2026 Day 1: Arrival and Exploring Tokyo * \nArrive at Narita or Haneda Airport and take a \ntrain or bus to your hotel * Visit the famous \nShibuya Crossing, \u2026\nBaseline\nPrompt Caching\nUser prompt\n(GPU: 75ms, CPU: 4,725ms)\n(GPU: 54ms, CPU: 479ms)\nFigure 8. Parameterized prompts: The <travel-plan> is re-\nconfigured at runtime while maintaining caching efficiency, offer-\ning flexible prompt structuring.\n6\nCONCLUSIONS AND FUTURE WORK\nWe introduce Prompt Cache, an acceleration technique\nbased on the insight that attention states can be reused across\nLLM prompts. Prompt Cache utilizes a prompt schema to\ndelineate such reused text segments, formulating them into a\nmodular and positionally coherent structure termed \u201cprompt\nmodules\u201d. This allows LLM users to incorporate these\nmodules seamlessly into their prompts, thereby leveraging\nthem for context with negligible latency implications. Our\nevaluations on benchmark data sets indicate TTFT latency\nreductions of up to 8\u00d7 on GPUs and 60\u00d7 on CPUs.\nFor future work, we plan on using Prompt Cache as a build-\ning block for future LLM serving systems. Such a system\ncould be equipped with GPU cache replacement strategies\noptimized to achieve the latency lower bound made possible\nby Prompt Cache. Different strategies for reducing host-\nto-device memory overhead can also be beneficial, such as\nthe integration of compression techniques in the KV cache.\nAnother promising exploration is cache-driven retrieval aug-\nmentation. In this paradigm, the selection of prompt mod-\nules can be dynamically adapted based on user requests.\nThis offers similar advantages to the retrieval-augmented\nLLMs but with lower latency.\nACKNOWLEDGEMENTS\nThis work is supported in part by NSF Awards #2112562,\n#2047220, and Yale University. This work used the Delta\nsystem at the National Center for Supercomputing Applica-\ntions through allocation CIS230289 from the Advanced Cy-\nberinfrastructure Coordination Ecosystem: Services & Sup-\nport (ACCESS) program, which is supported by National\nScience Foundation grants #2138259, #2138286, #2138307,\n#2137603, and #2138296.\n10\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nREFERENCES\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li,\nD., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley,\nJ., and He, Y. Deepspeed- inference: Enabling efficient\ninference of transformer models at unprecedented scale.\nIn Wolf, F., Shende, S., Culhane, C., Alam, S. R., and\nJagode, H. (eds.), SC22: International Conference for\nHigh Performance Computing, Networking, Storage and\nAnalysis, Dallas, TX, USA, November 13-18, 2022, pp.\n46:1\u201346:15. IEEE, 2022. doi: 10.1109/SC41404.2022\n.00051. URL https://doi.org/10.1109/SC41\n404.2022.00051.\nBai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z.,\nDu, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J.,\nand Li, J. Longbench: A bilingual, multitask benchmark\nfor long context understanding. CoRR, abs/2308.14508,\n2023. doi: 10.48550/ARXIV.2308.14508. URL https:\n//doi.org/10.48550/arXiv.2308.14508.\nBeltagy, I., Peters, M. E., and Cohan, A.\nLongformer:\nThe long-document transformer. CoRR, abs/2004.05150,\n2020. URL https://arxiv.org/abs/2004.0\n5150.\nCui, J., Li, Z., Yan, Y., Chen, B., and Yuan, L. Chatlaw:\nOpen-source legal large language model with integrated\nexternal knowledge bases, 2023.\nDao, T., Fu, D., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. Advances in Neural Information Process-\ning Systems, 35:16344\u201316359, 2022.\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery,\nA., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu,\nT., Huang, W., Chebotar, Y., Sermanet, P., Duckworth,\nD., Levine, S., Vanhoucke, V., Hausman, K., Toussaint,\nM., Greff, K., Zeng, A., Mordatch, I., and Florence, P.\nPalm-e: An embodied multimodal language model, 2023.\nDufter, P., Schmitt, M., and Sch\u00a8utze, H. Position infor-\nmation in transformers: An overview. Computational\nLinguistics, 48(3):733\u2013763, 2022.\nFabbri, A. R., Li, I., She, T., Li, S., and Radev, D.\nMulti-news: A large-scale multi-document summariza-\ntion dataset and abstractive hierarchical model. In Pro-\nceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 1074\u20131084, 2019.\nFeng, Y., Jeon, H., Blagojevic, F., Guyot, C., Li, Q., and Li,\nD. Attmemo : Accelerating transformers with memoiza-\ntion on big memory systems, 2023.\nGitHub. Github copilot \u00b7 your ai pair programmer, 2023.\nURL https://github.com/features/copi\nlot.\nGuo, D., Xu, C., Duan, N., Yin, J., and McAuley, J. Long-\ncoder: A long-range pre-trained language model for code\ncompletion. arXiv preprint arXiv:2306.14893, 2023.\nHo, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A.\nConstructing a multi-hop qa dataset for comprehensive\nevaluation of reasoning steps. In Proceedings of the 28th\nInternational Conference on Computational Linguistics,\npp. 6609\u20136625, 2020.\nHuang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Effi-\ncient attentions for long document summarization. In Pro-\nceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 1419\u20131436, 2021.\nHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. Lan-\nguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. arXiv preprint\narXiv:2201.07207, 2022.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Trivi-\naqa: A large scale distantly supervised challenge dataset\nfor reading comprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 1601\u20131611,\n2017.\nKeles, F. D., Wijewardena, P. M., and Hegde, C. On the\ncomputational complexity of self-attention, 2022.\nKo\u02c7cisk`y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\nK. M., Melis, G., and Grefenstette, E. The narrativeqa\nreading comprehension challenge. Transactions of the\nAssociation for Computational Linguistics, 6:317\u2013328,\n2018.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. arXiv preprint arXiv:2309.06180,\n2023.\nLew, Z., Walther, J. B., Pang, A., and Shin, W. Interactivity\nin online chat: Conversational contingency and response\nlatency in computer-mediated communication. Journal\nof Computer-Mediated Communication, 23(4):201\u2013221,\n2018.\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking\nrepository-level code auto-completion systems. arXiv\npreprint arXiv:2306.03091, 2023a.\nLiu, Y., Li, H., Du, K., Yao, J., Cheng, Y., Huang, Y., Lu,\nS., Maire, M., Hoffmann, H., Holtzman, A., Anantha-\nnarayanan, G., and Jiang, J. Cachegen: Fast context\nloading for language model applications, 2023b.\n11\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nMosaicML. Introducing mpt-7b: A new standard for open-\nsource, commercially usable llms, 2023. URL www.mo\nsaicml.com/blog/mpt-7b. Accessed: 2023-05-\n05.\nNay, J. J., Karamardian, D., Lawsky, S. B., Tao, W., Bhat,\nM., Jain, R., Lee, A. T., Choi, J. H., and Kasai, J. Large\nlanguage models as tax attorneys: A case study in legal\ncapabilities emergence, 2023.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S.,\nNg, N., Grangier, D., and Auli, M.\nfairseq: A fast,\nextensible toolkit for sequence modeling. In Ammar,\nW., Louis, A., and Mostafazadeh, N. (eds.), Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Demonstra-\ntions, pp. 48\u201353. Association for Computational Lin-\nguistics, 2019. doi: 10.18653/V1/N19-4009. URL\nhttps://doi.org/10.18653/v1/n19-4009.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\nBai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances\nin Neural Information Processing Systems 32, pp. 8024\u2013\n8035. Curran Associates, Inc., 2019. URL http://pa\npers.neurips.cc/paper/9015-pytorch-a\nn-imperative-style-high-performance-d\neep-learning-library.pdf.\nPenedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cap-\npelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and\nLaunay, J. The refinedweb dataset for falcon llm: out-\nperforming curated corpora with web data, and web data\nonly. arXiv preprint arXiv:2306.01116, 2023.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and\nDean, J. Efficiently scaling transformer inference, 2022.\nPress, O., Smith, N. A., and Lewis, M. Train short, test\nlong: Attention with linear biases enables input length\nextrapolation. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022. URL https:\n//openreview.net/forum?id=R8sQPpGCv0.\nQin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng,\nZ., Huang, Y., Xiao, C., Han, C., Fung, Y. R., Su, Y.,\nWang, H., Qian, C., Tian, R., Zhu, K., Liang, S., Shen,\nX., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu,\nY., Dai, Z., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang,\nY., Yan, J., Han, X., Sun, X., Li, D., Phang, J., Yang,\nC., Wu, T., Ji, H., Liu, Z., and Sun, M. Tool learning\nwith foundation models. CoRR, abs/2304.08354, 2023.\ndoi: 10.48550/ARXIV.2304.08354. URL https:\n//doi.org/10.48550/arXiv.2304.08354.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018.\nRasmy, L., Xiang, Y., Xie, Z., Tao, C., and Zhi, D. Med-\nbert: pretrained contextualized embeddings on large-scale\nstructured electronic health records for disease prediction.\nnpj Digit. Medicine, 4, 2021. doi: 10.1038/S41746-021\n-00455-Y. URL https://doi.org/10.1038/s4\n1746-021-00455-y.\nRozi`ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan,\nX. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov,\nA., Evtimov, I., Bitton, J., Bhatt, M., Canton-Ferrer, C.,\nGrattafiori, A., Xiong, W., D\u00b4efossez, A., Copet, J., Azhar,\nF., Touvron, H., Martin, L., Usunier, N., Scialom, T., and\nSynnaeve, G. Code llama: Open foundation models for\ncode. CoRR, abs/2308.12950, 2023. doi: 10.48550/ARX\nIV.2308.12950. URL https://doi.org/10.485\n50/arXiv.2308.12950.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow,\nD., Castagn\u00b4e, R., Luccioni, A. S., Yvon, F., Gall\u00b4e, M.,\nTow, J., Rush, A. M., Biderman, S., Webson, A., Am-\nmanamanchi, P. S., Wang, T., Sagot, B., Muennighoff,\nN., del Moral, A. V., Ruwase, O., Bawden, R., Bek-\nman, S., McMillan-Major, A., Beltagy, I., Nguyen, H.,\nSaulnier, L., Tan, S., Suarez, P. O., Sanh, V., Laurenc\u00b8on,\nH., Jernite, Y., Launay, J., Mitchell, M., Raffel, C.,\nGokaslan, A., Simhi, A., Soroa, A., Aji, A. F., Alfassy,\nA., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue,\nC., Klamm, C., Leong, C., van Strien, D., Adelani, D. I.,\nand et al. BLOOM: A 176b-parameter open-access mul-\ntilingual language model. CoRR, abs/2211.05100, 2022.\ndoi: 10.48550/ARXIV.2211.05100. URL https:\n//doi.org/10.48550/arXiv.2211.05100.\nShen, J. T., Yamashita, M., Prihar, E., Heffernan, N. T., Wu,\nX., and Lee, D. Mathbert: A pre-trained language model\nfor general NLP tasks in mathematics education. CoRR,\nabs/2106.07340, 2021. URL https://arxiv.org/\nabs/2106.07340.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen,\nB., Liang, P., R\u00b4e, C., Stoica, I., and Zhang, C. Flexgen:\nHigh-throughput generative inference of large language\nmodels with a single GPU. In Krause, A., Brunskill, E.,\nCho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),\nInternational Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume\n202 of Proceedings of Machine Learning Research, pp.\n12\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\n31094\u201331116. PMLR, 2023. URL https://procee\ndings.mlr.press/v202/sheng23a.html.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B.\nMegatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism.\nCoRR, abs/1909.08053, 2019.\nURL http:\n//arxiv.org/abs/1909.08053.\nSteinberg, E., Jung, K., Fries, J. A., Corbin, C. K., Pfohl,\nS. R., and Shah, N. H. Language models are an effective\nrepresentation learning technique for electronic health\nrecord data. J. Biomed. Informatics, 113:103637, 2021.\ndoi: 10.1016/J.JBI.2020.103637. URL https://doi.\norg/10.1016/j.jbi.2020.103637.\nSu, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer:\nEnhanced transformer with rotary position embedding.\nCoRR, abs/2104.09864, 2021. URL https://arxiv.\norg/abs/2104.09864.\nTay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient\ntransformers: A survey. ACM Comput. Surv., 55(6):109:1\u2013\n109:28, 2023. doi: 10.1145/3530811. URL https:\n//doi.org/10.1145/3530811.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open foundation and fine-tuned\nchat models, 2023.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal,\nA. Musique: Multihop questions via single-hop ques-\ntion composition. Transactions of the Association for\nComputational Linguistics, 10:539\u2013554, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need, 2023.\nWhite, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert,\nH., Elnashar, A., Spencer-Smith, J., and Schmidt, D. C.\nA prompt pattern catalog to enhance prompt engineering\nwith chatgpt. CoRR, abs/2302.11382, 2023. doi: 10.485\n50/ARXIV.2302.11382. URL https://doi.org/\n10.48550/arXiv.2302.11382.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\nDavison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\nY., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M.,\nLhoest, Q., and Rush, A. M. Huggingface\u2019s transformers:\nState-of-the-art natural language processing, 2020.\nWu, L., Zheng, Z., Qiu, Z., Wang, H., Gu, H., Shen, T., Qin,\nC., Zhu, C., Zhu, H., Liu, Q., Xiong, H., and Chen, E.\nA survey on large language models for recommendation.\nCoRR, abs/2305.19860, 2023. doi: 10.48550/ARXIV.2\n305.19860. URL https://doi.org/10.48550\n/arXiv.2305.19860.\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhut-\ndinov, R., and Manning, C. D. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering. In\nProceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 2369\u20132380,\n2018.\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\nR., Song, Z., Tian, Y., R\u00b4e, C., Barrett, C., Wang, Z., and\nChen, B. H2o: Heavy-hitter oracle for efficient generative\ninference of large language models, 2023.\nZhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R.,\nHassan, A., Celikyilmaz, A., Liu, Y., Qiu, X., et al. Qm-\nsum: A new benchmark for query-based multi-domain\nmeeting summarization. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, pp. 5905\u20135921, 2021.\n13\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nA\nAPPENDIX\nA.1\nSchema Files Used for Evaluation in Section 5.3\nIn this appendix, we provide the pruned schema files that\nwere employed during our evaluations as described in Sec-\ntion 5.3.\nA.1.1\nCode Schema\n<schema name=\"code-generation-game\">\n<system>\nYou are a sophisticated ...\n</system>\n<user>\nPlease read the given source files...\n<module name=\"unit.py\">\nclass Unit:\n...\n</module>\n<module name=\"player.py\">\nclass Player:\n...\n</module>\n<module name=\"game.py\">\nclass Game:\n...\n</module>\n<module name=\"database.py\">\nclass Database:...\n</module>\n</user>\n<assistant>\nI have read and ...\n</assistant>\n</schema>\nA.1.2\nTravel Schema\n<schema name=\"travel\">\n<system>\nYou are a world-renowned travel\nplanner ...\n</system>\n<user>\n<module name=\"travel-plan\"> I\u2019m\ngearing up for a memorable escape\n...\n<parameter name=\"duration\" length=\"5\n\" />\n...\n<union>\n<module name=\u2019domestic\u2019> My eyes\nare set ...\n<parameter name=\"city\" length=\"\n10\" />\nGiven its domestic charm ...\n</module>\n<module name=\"overseas\"> I\u2019m\nyearning to tread ...\n<union>\n<module name=\"maldives\">\nThe Maldives beckons ...\n</module>\n<module name=\"amazon\">\nThe vast expanse of the\nAmazon...\n</module>\n<module name=\"sahara\">\nThe golden embrace of ...\n</module>\n<module name=\"tokyo\">\nTokyo, Japan\u2019s bustling,...\n</module>\n<module name=\"rome\">\nThe eternal city of Rome...\n</module>\n<module name=\"capetown\">\nCape Town, nestled at...\n</module>\n<module name=\"sydney\">\nSydney, the shimmering ...\n</module>\n<module name=\"buenosaires\">\nBuenos Aires, Argentina..\n</module>\n</union>\n</module>\n</union>\n</module>\n</user>\n<assistant>\nI\u2019d love to help. I\u2019ve carefully read\nthe city ...\n</assistant>\n</schema>\nA.1.3\nPersonalization Schema\n<schema name=\"personalization-education\">\n<system>Dialogues between people...\n</system>\n<user> **Tailor learning content ...\n<union>\n<module name=\"elementary\">\nThe elementary phase ..\n</module>\n<module name=\"middle-school\">\nAs students transition...\n</module>\n<module name=\"high-school\">\nHigh school acts...\n</module>\n<module name=\"college\">\nCollege is a transformative ...\n</module>\n<module name=\"graduate-school\">\nGraduate school signifies ...\n</module>\n<module name=\"adult-education\">\nIn an ever-evolving world,...\n</module>\n</union>\n2. Subject proficiency ...\n<union>\n<module name=\"beginner\">\nA beginner is often at ...\n</module>\n1\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\n<module name=\"intermediate\">\nAn intermediate learner...\n</module>\n<module name=\"advanced\">\nAn advanced learner...\n</module>\n<module name=\"expert\">\nAn expert stands at...\n</module>\n</union>\n3. Recent learning history\n<union>\n<module name=\"recently-studied\">\nIf a topic was engaged...\n</module>\n<module name=\"studied-a-month-before\n\">\nTopics encountered a ...\n</module>\n<module name=\"studied-6-months-\nbefore\">\nHalf a year is ample ...\n</module>\n<module name=\"studied-a-year-before\"\n>\nAs the year mark ...\n</module>\n<module name=\"studied-10-years-\nbefore\">\nA decade is a substantial...\n</module>\n<module name=\"never-studied\">\nVenturing into entirely ...\n</module>\n</union>\n4. Learning style...\n<union>\n<module name=\"visual\">\nVisual learners ...\n</module>\n<module name=\"auditory\">\nFor auditory learners...\n</module>\n<module name=\"kinesthetic\">\nKinesthetic learners ...\n</module>\n<module name=\"reading\">\nThose who identify ...\n</module>\n<module name=\"multimodal\">\nMultimodal learners ...\n</module>\n</union>\n5. Preferred assessment type\n<union>\n<module name=\"multiple-choice\">\nMultiple choice assessments ...\n</module>\n<module name=\"essay\">\nEssay assessments...\n</module>\n<module name=\"oral-presentation\">\nThis assessment type ...\n</module>\n<module name=\"group-projects\">\nA testament to collaborative...\n</module>\n<module name=\"self-assessment\">\nTaking a step back ...\n</module>\n</union>\n6. Motivation level Motivation...\n<union>\n<module name=\"high-intrinsic-\nmotivation\">\nLearners with a high intrinsic\nmotivation ...\n</module>\n<module name=\"high-extrinsic-\nmotivation\">\nWhile some are driven by ...\n</module>\n<module name=\"needs-encouragement\">\nSome learners, while capable,...\n</module>\n<module name=\"lacks-direction\">\nThis category encompasses...\n</module>\n</union>\nReady to tailor the content? </user>\n<assistant>\nContent tailored ...\n</assistant>\n</schema>\nA.2\nComplete Benchmarks Results\nIn this subsection, we provide complete results of the bench-\nmark that we conducted in \u00a75\u2014the following four datasets\nare added: Qasper, MFQA, HotpotQA, and PCount (to-\ntal 12 datasets). We employ LongBench suite to measure\ntime-to-first-token (TTFT) latency and accuracy. For the\ncomplete system environment setup, see \u00a75.1.\nLatency benefits on GPU\nFigure 9 to Figure 11 show\nthat the TTFT latency reduction across all dataset follows\nthe same trend reported in \u00a75. The latency reduction ranges\nfrom 1.5\u00d7 to 3.1\u00d7 when prompt modules are stored in CPU\nmemory, and from 3.7\u00d7 to 11.7\u00d7 when employing GPU\nmemory.\nLatency benefits on CPU\nThe latency reduction on CPU\nalso follow the same trend as \u00a75.2, as shown in Figure 12 and\nFigure 13. The latency improvement ranges from 9.3\u00d7 to\n63.7\u00d7 across CPU configurations and dataset. As discussed\nin \u00a75.4, the latency reduction decreases as the non-cacheable\nportion of prompt and response increases.\nQuality of responses\nWe measure accuracy in dataset-\nspecific metric as shown in Table 3. Across datasets and\nmetrics, Prompt Cache maintains negligible performance\ndegradation compared to the baseline, KV Cache.\n2\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nFigure 9. Latency benchmark results on Nvidia RTX 4090 GPU.\nFigure 10. Latency benchmark results on Nvidia A100 GPU.\nFigure 11. Latency benchmark results on Nvidia A40 GPU.\nFigure 12. Latency benchmark results on Intel i9-13900K CPU with 5600MT/s DDR5 RAM.\n3\nPrompt Cache: Modular Attention Reuse for Low-Latency Inference\nFigure 13. Latency benchmark results on AMD Ryzen 9 7950X CPU with 3600 MT/s DDR4 RAM.\nDataset\nMetric\nLlama2 7B\nLlama2 13B\nMPT 7B\nFalcon 7B\nBaseline\nCached\nBaseline\nCached\nBaseline\nCached\nBaseline\nCached\nNarrative QA\nF1\n19.93\n19.38\n20.37\n19.94\n10.43\n11.33\n7.14\n8.87\nQasper\nF1\n17.98\n19.31\n20.90\n17.79\n10.08\n13.71\n10.64\n8.90\nMulti-field QA (MFQA)\nF1\n28.61\n29.64\n32.12\n32.37\n25.15\n27.45\n17.49\n16.65\nHotpotQA\nF1\n18.32\n19.34\n22.21\n23.35\n18.97\n20.11\n12.37\n13.22\n2 Wiki Multi-Hop QA\nF1\n16.63\n13.95\n14.59\n17.69\n10.44\n13.70\n14.42\n15.07\nMuSiQue\nF1\n7.31\n8.57\n10.03\n12.14\n7.38\n7.32\n4.81\n5.86\nGovReport\nRouge L\n24.67\n25.37\n28.13\n28.18\n26.96\n27.49\n22.39\n23.40\nQMSum\nRouge L\n19.24\n19.46\n18.80\n18.82\n15.19\n15.51\n12.84\n12.96\nMultiNews\nRouge L\n24.33\n24.22\n25.43\n26.23\n25.42\n25.66\n20.91\n21.19\nTriviaQA\nF1\n13.04\n12.33\n23.19\n22.38\n10.57\n9.17\n13.31\n11.42\nPassage Count (PCount)\nAcc\n3.33\n4.00\n2.26\n2.95\n1.53\n1.81\n1.55\n1.59\nPassage Retrieval\nAcc\n7.50\n4.25\n9.08\n6.50\n3.03\n3.85\n3.00\n3.45\nTable 3. Accuracy benchmarks on LongBench datasets. We mark the outliers as bold, of which the performance is higher than 2.5\ncompared to the counter part.\n4\n"
  },
  {
    "title": "GPT4All: An Ecosystem of Open Source Compressed Language Models",
    "link": "https://arxiv.org/pdf/2311.04931.pdf",
    "upvote": "20",
    "text": "GPT4All: An Ecosystem of Open Source Compressed Language Models\nYuvanesh Anand\nNomic AI\nyuvanesh@nomic.ai\nZach Nussbaum\nNomic AI\nzach@nomic.ai\nAdam Treat\nNomic AI\nadam@nomic.ai\nAaron Miller\nNomic AI\naaron@nomic.ai\nRichard Guo\nNomic AI\nrichard@nomic.ai\nBen Schmidt\nNomic AI\nben@nomic.ai\nGPT4All Community\nPlanet Earth\nBrandon Duderstadt\u2217\nNomic AI\nbrandon@nomic.ai\nAndriy Mulyar\u2217\nNomic AI\nandriy@nomic.ai\nAbstract\nLarge language models (LLMs) have recently\nachieved human-level performance on a range\nof professional and academic benchmarks. The\naccessibility of these models has lagged behind\ntheir performance. State-of-the-art LLMs re-\nquire costly infrastructure; are only accessible\nvia rate-limited, geo-locked, and censored web\ninterfaces; and lack publicly available code and\ntechnical reports.\nIn this paper, we tell the story of GPT4All, a\npopular open source repository that aims to\ndemocratize access to LLMs. We outline the\ntechnical details of the original GPT4All model\nfamily, as well as the evolution of the GPT4All\nproject from a single model into a fully fledged\nopen source ecosystem. It is our hope that\nthis paper acts as both a technical overview of\nthe original GPT4All models as well as a case\nstudy on the subsequent growth of the GPT4All\nopen source ecosystem.\n1\nIntroduction\nOn March 14 2023, OpenAI released GPT-4, a large\nlanguage model capable of achieving human level per-\nformance on a variety of professional and academic\nbenchmarks.\nDespite the popularity of the release,\nthe GPT-4 technical report (OpenAI, 2023) contained\nvirtually no details regarding the architecture, hard-\nware, training compute, dataset construction, or training\nmethod used to create the model. Moreover, users could\nonly access the model through the internet interface at\nchat.openai.com, which was severely rate limited and\nunavailable in several locales (e.g. Italy) (BBC News,\n2023). Additionally, GPT-4 refused to answer a wide\n\u2217Shared Senior Authorship\nvariety of queries, responding only with the now infa-\nmous \"As an AI Language Model, I cannot...\" prefix\n(Vincent, 2023). These transparency and accessibility\nconcerns spurred several developers to begin creating\nopen source large language model (LLM) alternatives.\nSeveral grassroots efforts focused on fine tuning Meta\u2019s\nopen code LLaMA model (Touvron et al., 2023; McMil-\nlan, 2023), whose weights were leaked on BitTorrent\nless than a week prior to the release of GPT-4 (Verge,\n2023). GPT4All started as one of these variants.\nIn this paper, we tell the story of GPT4All. We com-\nment on the technical details of the original GPT4All\nmodel (Anand et al., 2023), as well as the evolution of\nGPT4All from a single model to an ecosystem of several\nmodels. We remark on the impact that the project has\nhad on the open source community, and discuss future\ndirections. It is our hope that this paper acts as both a\ntechnical overview of the original GPT4All models as\nwell as a case study on the subsequent growth of the\nGPT4All open source ecosystem.\n2\nThe Original GPT4All Model\n2.1\nData Collection and Curation\nTo train the original GPT4All model, we collected\nroughly one million prompt-response pairs using the\nGPT-3.5-Turbo OpenAI API between March 20, 2023\nand March 26th, 2023. In particular, we gathered GPT-\n3.5-Turbo responses to prompts of three publicly avail-\nable datasets: the unified chip2 subset of LAION OIG,\na random sub-sample of Stackoverflow Questions, and\na sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol-\nlowing the approach in Stanford Alpaca (Taori et al.,\n2023), an open source LLaMA variant that came just be-\nfore GPT4All, we focused substantial effort on dataset\ncuration.\nThe collected dataset was loaded into Atlas (AI,\n2023)\u2014a visual interface for exploring and tagging mas-\nsive unstructured datasets \u2014for data curation. Using At-\narXiv:2311.04931v1  [cs.CL]  6 Nov 2023\nlas, we identified and removed subsets of the data where\nGPT-3.5-Turbo refused to respond, had malformed out-\nput, or produced a very short response. This resulted in\nthe removal of the entire Bigscience/P3 subset of our\ndata, as many P3 prompts induced responses that were\nsimply one word. After curation, we were left with a set\nof 437,605 prompt-response pairs, which we visualize\nin Figure 1a.\n2.2\nModel Training\nThe original GPT4All model was a fine tuned variant\nof LLaMA 7B. In order to train it more efficiently, we\nfroze the base weights of LLaMA, and only trained a\nsmall set of LoRA (Hu et al., 2021) weights during the\nfine tuning process. Detailed model hyper-parameters\nand training code can be found in our associated code\nrepository1.\n2.3\nModel Access\nWe publicly released all data, training code, and model\nweights for the community to build upon. Further, we\nprovided a 4-bit quantized version of the model, which\nenabled users to run it on their own commodity hard-\nware without transferring data to a 3rd party service.\nOur research and development costs were dominated\nby \u223c$800 in GPU spend (rented from Lambda Labs and\nPaperspace) and \u223c$500 in OpenAI API spend. Our final\nGPT4All model could be trained in about eight hours\non a Lambda Labs DGX A100 8x 80GB for a total cost\nof \u223c$100.\n2.4\nModel Evaluation\nWe performed a preliminary evaluation of our model\nusing the human evaluation data from the Self Instruct\npaper (Wang et al., 2023). We reported the ground truth\nperplexity of our model against what was, to our knowl-\nedge, the best openly available alpaca-lora model at the\ntime, provided by user chainyo on HuggingFace. Both\nmodels had very large perplexities on a small number of\ntasks, so we reported perplexities clipped to a maximum\nof 100. We found that GPT4All produces stochastically\nlower ground truth perplexities than alpaca-lora (Anand\net al., 2023).\n3\nFrom a Model to an Ecosystem\n3.1\nGPT4All-J: Repository Growth and the\nimplications of the LLaMA License\nThe GPT4All repository grew rapidly after its release,\ngaining over 20000 GitHub stars in just one week, as\nshown in Figure 2. This growth was supported by an\nin-person hackathon hosted in New York City three days\nafter the model release, which attracted several hundred\nparticipants. As the Nomic discord, the home of online\ndiscussion about GPT4All, ballooned to over 10000\npeople, one thing became very clear - there was massive\ndemand for a model that could be used commercially.\n1https://github.com/nomic-ai/gpt4all\nThe LLaMA model that GPT4All was based on was\nlicensed for research only, which severely limited the\nset of domains that GPT4All could be applied in. As\na response to this, the Nomic team repeated the model\ntraining procedure of the original GPT4All model, but\nbased on the already open source and commercially li-\ncensed GPT-J model (Wang and Komatsuzaki, 2021).\nGPT4All-J also had an augmented training set, which\ncontained multi-turn QA examples and creative writing\nsuch as poetry, rap, and short stories. The creative writ-\ning prompts were generated by filling in schemas such\nas \"Write a [CREATIVE STORY TYPE] about [NOUN]\nin the style of [PERSON].\" We again employed Atlas\nto curate the prompt-response pairs in this data set.\nOur evaluation methodology also evolved as the\nproject grew.\nIn particular, we began evaluating\nGPT4All models using a suite of seven reasoning\ntasks that were used for evaluation of the Databricks\nDolly (Conover et al., 2023b) model, which was re-\nleased on April 12, 2023. Unfortunately, GPT4All-J did\nnot outperform other prominent open source models on\nthis evaluation. As a result, we endeavoured to create a\nmodel that did.\n3.2\nGPT4All-Snoozy: the Emergence of the\nGPT4All Ecosystem\nGPT4All-Snoozy was developed using roughly the same\nprocedure as the previous GPT4All models, but with a\nfew key modifications. First, GPT4All-Snoozy used the\nLLaMA-13B base model due to its superior base metrics\nwhen compared to GPT-J. Next, GPT4All-Snoozy incor-\nporated the Dolly\u2019s training data into its train mix. After\ndata curation and deduplication with Atlas, this yielded\na training set of 739,259 total prompt-response pairs.\nWe dubbed the model that resulted from training on this\nimproved dataset GPT4All-Snoozy. As shown in Figure\n1, GPT4All-Snoozy had the best average score on our\nevaluation benchmark of any model in the ecosystem at\nthe time of its release.\nConcurrently with the development of GPT4All, sev-\neral organizations such as LMSys, Stability AI, BAIR,\nand Databricks built and deployed open source language\nmodels. We heard increasingly from the community that\nthey wanted quantized versions of these models for local\nuse. As we realized that organizations with ever more\nresources were developing source language models, we\ndecided to pivot our effort away from training increas-\ningly capable models and towards providing easy access\nto the plethora of models being produced by the open\nsource community. Practically, this meant spending our\ntime compressing open source models for use on com-\nmodity hardware, providing stable and simple high level\nmodel APIs, and supporting a GUI for no code model\nexperimentation.\n3.3\nThe Current State of GPT4All\nToday, GPT4All is focused on improving the accessi-\nbility of open source language models. The repository\n(a)\n(b)\n(c)\n(d)\nFigure 1: TSNE visualizations showing the progression of the GPT4All train set. Panel (a) shows the original\nuncurated data. The red arrow denotes a region of highly homogeneous prompt-response pairs. The coloring denotes\nwhich open dataset contributed the prompt. Panel (b) shows the original GPT4All data after curation. This panel,\nas well as panels (c) and (d) are 10 colored by topic, which Atlas automatically extracts. Notice that the large\nhomogeneous prompt-response blobs no longer appearl. Panel (c) shows the GPT4All-J dataset. The \"starburst\"\nclusters introduced on the right side of the panel correspond to the newly added creative data. Panel (d) shows\nthe final GPT4All-snoozy dataset. All datasets have been released to the public, and can be interactively explored\nonline. In the web version of this article, you can click on a panel to be taken to its interactive visualization.\nModel\nBoolQ\nPIQA\nHellaSwag\nWinoG.\nARC-e\nARC-c\nOBQA\nAvg.\nGPT4All-J 6B v1.0*\n73.4\n74.8\n63.4\n64.7\n54.9\n36\n40.2\n58.2\nGPT4All-J v1.1-breezy*\n74\n75.1\n63.2\n63.6\n55.4\n34.9\n38.4\n57.8\nGPT4All-J v1.2-jazzy*\n74.8\n74.9\n63.6\n63.8\n56.6\n35.3\n41\n58.6\nGPT4All-J v1.3-groovy*\n73.6\n74.3\n63.8\n63.5\n57.7\n35\n38.8\n58.1\nGPT4All-J Lora 6B*\n68.6\n75.8\n66.2\n63.5\n56.4\n35.7\n40.2\n58.1\nGPT4All LLaMa Lora 7B*\n73.1\n77.6\n72.1\n67.8\n51.1\n40.4\n40.2\n60.3\nGPT4All 13B snoozy*\n83.3\n79.2\n75\n71.3\n60.9\n44.2\n43.4\n65.3\nGPT4All Falcon\n77.6\n79.8\n74.9\n70.1\n67.9\n43.4\n42.6\n65.2\nNous-Hermes (Nous-Research, 2023b)\n79.5\n78.9\n80\n71.9\n74.2\n50.9\n46.4\n68.8\nNous-Hermes2 (Nous-Research, 2023c)\n83.9\n80.7\n80.1\n71.3\n75.7\n52.1\n46.2\n70.0\nNous-Puffin (Nous-Research, 2023d)\n81.5\n80.7\n80.4\n72.5\n77.6\n50.7\n45.6\n69.9\nDolly 6B* (Conover et al., 2023a)\n68.8\n77.3\n67.6\n63.9\n62.9\n38.7\n41.2\n60.1\nDolly 12B* (Conover et al., 2023b)\n56.7\n75.4\n71\n62.2\n64.6\n38.5\n40.4\n58.4\nAlpaca 7B* (Taori et al., 2023)\n73.9\n77.2\n73.9\n66.1\n59.8\n43.3\n43.4\n62.5\nAlpaca Lora 7B* (Wang, 2023)\n74.3\n79.3\n74\n68.8\n56.6\n43.9\n42.6\n62.8\nGPT-J* 6.7B (Wang and Komatsuzaki, 2021)\n65.4\n76.2\n66.2\n64.1\n62.2\n36.6\n38.2\n58.4\nLLama 7B* (Touvron et al., 2023)\n73.1\n77.4\n73\n66.9\n52.5\n41.4\n42.4\n61.0\nLLama 13B* (Touvron et al., 2023)\n68.5\n79.1\n76.2\n70.1\n60\n44.6\n42.2\n63.0\nPythia 6.7B* (Biderman et al., 2023)\n63.5\n76.3\n64\n61.1\n61.3\n35.2\n37.2\n56.9\nPythia 12B* (Biderman et al., 2023)\n67.7\n76.6\n67.3\n63.8\n63.9\n34.8\n38\n58.9\nFastchat T5* (Zheng et al., 2023)\n81.5\n64.6\n46.3\n61.8\n49.3\n33.3\n39.4\n53.7\nFastchat Vicu\u00f1a* 7B (Zheng et al., 2023)\n76.6\n77.2\n70.7\n67.3\n53.5\n41.2\n40.8\n61.0\nFastchat Vicu\u00f1a 13B* (Zheng et al., 2023)\n81.5\n76.8\n73.3\n66.7\n57.4\n42.7\n43.6\n63.1\nStableVicu\u00f1a RLHF* (Stability-AI, 2023)\n82.3\n78.6\n74.1\n70.9\n61\n43.5\n44.4\n65.0\nStableLM Tuned* (Stability-AI, 2023)\n62.5\n71.2\n53.6\n54.8\n52.4\n31.1\n33.4\n51.3\nStableLM Base* (Stability-AI, 2023)\n60.1\n67.4\n41.2\n50.1\n44.9\n27\n32\n46.1\nKoala 13B* (Geng et al., 2023)\n76.5\n77.9\n72.6\n68.8\n54.3\n41\n42.8\n62.0\nOpen Assistant Pythia 12B*\n67.9\n78\n68.1\n65\n64.2\n40.4\n43.2\n61.0\nMosaic MPT7B (MosaicML-Team, 2023)\n74.8\n79.3\n76.3\n68.6\n70\n42.2\n42.6\n64.8\nMosaic mpt-instruct (MosaicML-Team, 2023)\n74.3\n80.4\n77.2\n67.8\n72.2\n44.6\n43\n65.6\nMosaic mpt-chat (MosaicML-Team, 2023)\n77.1\n78.2\n74.5\n67.5\n69.4\n43.3\n44.2\n64.9\nWizard 7B (Xu et al., 2023)\n78.4\n77.2\n69.9\n66.5\n56.8\n40.5\n42.6\n61.7\nWizard 7B Uncensored (Xu et al., 2023)\n77.7\n74.2\n68\n65.2\n53.5\n38.7\n41.6\n59.8\nWizard 13B Uncensored (Xu et al., 2023)\n78.4\n75.5\n72.1\n69.5\n57.5\n40.4\n44\n62.5\nGPT4-x-Vicuna-13b (Nous-Research, 2023a)\n81.3\n75\n75.2\n65\n58.7\n43.9\n43.6\n63.2\nFalcon 7b (Almazrouei et al., 2023)\n73.6\n80.7\n76.3\n67.3\n71\n43.3\n44.4\n65.2\nFalcon 7b instruct (Almazrouei et al., 2023)\n70.9\n78.6\n69.8\n66.7\n67.9\n42.7\n41.2\n62.5\ntext-davinci-003\n88.1\n83.8\n83.4\n75.8\n83.9\n63.9\n51.0\n75.7\nTable 1: Evaluations of all language models in the GPT4All ecosystem as of August 1, 2023. Code models are not\nincluded. OpenAI\u2019s text-davinci-003 is included as a point of comparison. The best overall performing model in the\nGPT4All ecosystem, Nous-Hermes2, achieves over 92% of the average performance of text-davinci-003. Models\nmarked with an asterisk were available in the ecosystem as of the release of GPT4All-Snoozy. Note that at release,\nGPT4All-Snoozy had the best average performance of any model in the ecosystem. Bolded numbers indicate the\nbest performing model as of August 1, 2023.\nFigure 2: Comparison of the github start growth of GPT4All, Meta\u2019s LLaMA, and Stanford\u2019s Alpaca. We conjecture\nthat GPT4All achieved and maintains faster ecosystem growth due to the focus on access, which allows more users\nto meaningfully participate.\nprovides compressed versions of open source models\nfor use on commodity hardware, stable and simple high\nlevel model APIs, and a GUI for no code model ex-\nperimentation. The project continues to increase in\npopularity, and as of August 1 2023, has garnered over\n50000 GitHub stars and over 5000 forks.\nGPT4All currently provides native support and\nbenchmark data for over 35 models (see Figure 1), and\nincludes several models co-developed with industry part-\nners such as Replit and Hugging Face. GPT4All also\nprovides high level model APIs in languages includ-\ning Python, Typescript, Go, C#, and Java, among oth-\ners. Furthermore, the GPT4All no code GUI currently\nsupports the workflows of over 50000 monthly active\nusers, with over 25% of users coming back to the tool\nevery day of the week. (Note that all GPT4All user\ndata is collected on an opt in basis.) GPT4All has be-\ncome the top language model integration in the popular\nopen source AI orchestration library LangChain (Chase,\n2022), and powers many popular open source projects\nsuch as PrivateGPT (imartinez, 2023), Quiver (StanGi-\nrard, 2023), and MindsDB (MindsDB, 2023), among\nothers. GPT4All is the 3rd fastest growing GitHub\nrepository of all time (Leo, 2023), and is the 185th most\npopular repository on the platform, by star count.\n4\nThe Future of GPT4All\nIn the future, we will continue to grow GPT4All, sup-\nporting it as the de facto solution for LLM accessibil-\nity. Concretely, this means continuing to compress and\ndistribute important open-source language models de-\nveloped by the community, as well as compressing and\ndistributing increasingly multimodal AI models. Fur-\nthermore, we will expand the set of hardware devices\nthat GPT4All models run on, so that GPT4All models\n\u201cjust work\" on any machine, whether it comes equipped\nwith Apple Metal silicon, NVIDIA, AMD, or other edge-\naccelerated hardware. Overall, we envision a world\nwhere anyone, anywhere, with any machine, can access\nand contribute to the cutting edge of AI.\nLimitations\nBy enabling access to large language models, the\nGPT4All project also inherits many of the ethical con-\ncerns associated with generative models.\nPrincipal\namong these is the concern that unfiltered language\nmodels like GPT4All enable malicious users to generate\ncontent that could be harmful and dangerous (e.g., in-\nstructions on building bioweapons). While we recognize\nthis risk, we also acknowledge the risk of concentrating\nthis technology in the hands of a limited number of in-\ncreasingly secretive research groups. We believe that\nthe risk of focusing on the benefits of language model\ntechnology significantly outweighs the risk of misuse,\nand hence we prefer to make the technology as widely\navailable as possible.\nFinally, we realize the challenge in assigning credit\nfor large-scale open source initiatives. We make a first\nattempt at fair credit assignment by explicitly includ-\ning the GPT4All open source developers as authors on\nthis work, but recognize that this is insufficient fully\ncharacterize everyone involved in the GPT4All effort.\nFurthermore, we acknowledge the difficulty in citing\nopen source works that do not necessarily have standard-\nized citations, and do our best in this paper to provide\nURLs to projects whenever possible. We encourage\nfurther research in the area of open source credit as-\nsignment, and hope to be able to support some of this\nresearch ourselves in the future.\nReferences\nNomic AI. 2023. Atlas. https://atlas.nomic.ai/.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with\nlarge scale data distillation from gpt-3.5-turbo.\nhttps://github.com/nomic-ai/gpt4all.\nBBC News. 2023. Chatgpt banned in italy over privacy\nconcerns. BBC News.\nStella Biderman, Hailey Schoelkopf, Quentin An-\nthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Puro-\nhit, USVSN Sai Prashanth, Edward Raff, Aviya\nSkowron, Lintang Sutawika, and Oskar van der Wal.\n2023. Pythia: A suite for analyzing large language\nmodels across training and scaling.\nHarrison Chase. 2022. langchain. https://github.\ncom/langchain-ai/langchain.\nMike Conover, Matt Hayes, Ankit Mathur, Xiangrui\nMeng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick\nWendell, and Matei Zaharia. 2023a. Hello dolly:\nDemocratizing the magic of chatgpt with open mod-\nels.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023b. Free dolly:\nIntroducing the world\u2019s first truly open instruction-\ntuned llm.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nimartinez. 2023. privategpt. https://github.com/\nimartinez/privateGPT.\nOscar Leo. 2023. GitHub: The Fastest Growing Repos-\nitories of All Time.\nRobert McMillan. 2023. A meta platforms leak put\npowerful ai in the hands of everyone. The Wall\nStreet Journal.\nMindsDB. 2023.\nMindsdb.\nhttps://github.com/\nmindsdb/mindsdb. GitHub repository.\nMosaicML-Team. 2023. Introducing mpt-7b: A new\nstandard for open-source, commercially usable llms.\nAccessed: 2023-08-07.\nNous-Research.\n2023a.\ngpt4-x-vicuna-13b.\nhttps://huggingface.co/NousResearch/\ngpt4-x-vicuna-13b. Model on Hugging Face.\nNous-Research.\n2023b.\nNous-hermes-13b.\nhttps://huggingface.co/NousResearch/\nNous-Hermes-13b. Model on Hugging Face.\nNous-Research. 2023c.\nNous-hermes-llama-2-7b.\nhttps://huggingface.co/NousResearch/\nNous-Hermes-llama-2-7b.\nModel on Hugging\nFace.\nNous-Research.\n2023d.\nRedmond-puffin-13b.\nhttps://huggingface.co/NousResearch/\nRedmond-Puffin-13B. Model on Hugging Face.\nOpenAI. 2023. Gpt-4 technical report.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\nGao, Tali Bers, Thomas Wolf, and Alexander M.\nRush. 2021. Multitask prompted training enables\nzero-shot task generalization.\nStability-AI. 2023. Stablelm. https://github.com/\nStability-AI/StableLM. GitHub repository.\nStanGirard. 2023.\nquivr.\nhttps://github.com/\nStanGirard/quivr. GitHub repository.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. 2023.\nLlama: Open and efficient foundation language\nmodels.\nThe Verge. 2023. Meta\u2019s powerful ai language model\nhas leaked online \u2014 what happens now? The Verge.\nJames Vincent. 2023.\nAs an ai generated language\nmodel: The phrase that shows how ai is polluting\nthe web. The Verge.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B:\nA 6 Billion Parameter Autoregressive Language\nModel.\nhttps://github.com/kingoflolz/\nmesh-transformer-jax.\nEric J. Wang. 2023. alpaca-lora. https://github.\ncom/tloen/alpaca-lora. GitHub repository.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2023. Self-instruct: Aligning lan-\nguage models with self-generated instructions.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\n"
  },
  {
    "title": "u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model",
    "link": "https://arxiv.org/pdf/2311.05348.pdf",
    "upvote": "10",
    "text": "u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model\nJinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Fanyi Wang, Yanchun Xie, Yi-Jie Huang, Yaqian Li *\nOPPO Research Institute\nAbstract\nRecent advancements in multi-modal large language\nmodels (MLLMs) have led to substantial improvements\nin visual understanding, primarily driven by sophisticated\nmodality alignment strategies. However, predominant ap-\nproaches prioritize global or regional comprehension, with\nless focus on fine-grained, pixel-level tasks. To address this\ngap, we introduce u-LLaVA, an innovative unifying multi-\ntask framework that integrates pixel, regional, and global\nfeatures to refine the perceptual faculties of MLLMs. We\ncommence by leveraging an efficient modality alignment\napproach, harnessing both image and video datasets to\nbolster the model\u2019s foundational understanding across di-\nverse visual contexts. Subsequently, a joint instruction tun-\ning method with task-specific projectors and decoders for\nend-to-end downstream training is presented. Furthermore,\nthis work contributes a novel mask-based multi-task dataset\ncomprising 277K samples, crafted to challenge and assess\nthe fine-grained perception capabilities of MLLMs.\nThe\noverall framework is simple, effective, and achieves state-\nof-the-art performance across multiple benchmarks.\nWe\nalso make our model, data, and code publicly accessible\nat https://github.com/OPPOMKLab/u-LLaVA.\n1. Introduction\nOwing to the intrinsic difficulties associated with feature\nextraction in computer vision (CV) tasks, researchers have\npredominantly focused on perception rather than cognition\nover an extended duration.\nThis emphasis bears a sub-\nstantial impact on the development and understanding of\nvarious CV methodologies [22].\nAlthough the develop-\nment of deep neural networks and pre-training techniques\nhas significantly reduced the difficulty of perception, it re-\nmains challenging to achieve homogeneity across down-\nstream tasks due to substantial differences in their respec-\ntive objectives.\nRecently, causal large language models\nsuch as GPT [5, 27, 28], Gemini [35] and LLaMA [36] have\nreached or come close to human-level performance on a va-\nriety of tasks. These advancements have also motivated re-\n*Corresponding author\nTable 1. Comparison of comprehension levels supported by exist-\ning MLLMs.\nMethods\nImage\nVideo Region\nPixel\nLLaVA [19]\n\u2713\n\u2717\n\u2717\n\u2717\nMiniGPT-4 [56]\n\u2713\n\u2717\n\u2717\n\u2717\nVideo-LLaMA [52]\n\u2713\n\u2713\n\u2717\n\u2717\nVideo-ChatGPT [23]\n\u2717\n\u2713\n\u2717\n\u2717\nShikra [6]\n\u2713\n\u2717\n\u2713\n\u2717\nCogVLM [40]\n\u2713\n\u2717\n\u2713\n\u2717\nLISA [14]\n\u2713\n\u2717\n\u2717\n\u2713\nu-LLaVA (ours)\n\u2713\n\u2713\n\u2713\n\u2713\nsearchers to incorporate LLMs as components [19, 56] or\ncore elements [35, 40] in visual tasks, leading to the devel-\nopment of visual language models (VLMs), or multi-modal\nlarge language models (MLLMs). As a result, these meth-\nods have garnered increasing attention in recent times.\nTypically, a multi-modal LLM consists of one or multi-\nple encoders to extract features, paired with suitable map-\nping components (such as MLP [19], Q-Former[56], or\ncross-attention [2]), to align the other modalities with the\ntextual domain. In comparison to the impressive perfor-\nmance of MLLMs on general-purpose understanding tasks,\nsuch as visual question answering (VQA), their capabilities\nin regional and pixel-level tasks are somewhat less remark-\nable [19, 56]. To achieve regional-level understanding, it is\nusual to convert target coordinates into tokens for causal\nlanguage modeling, such as Shikra [6] and KOSMOS-\n2[26]. To further realize pixel-level understanding, mask-\nlevel decoders or extractors are introduced, such as LISA\n[14], Osprey [50] and Next-Chat [51]. However, such re-\ngion comprehension requires extensive data for training, en-\ntailing high costs. Pixel-level understanding methods offer\nmore flexibility, but entail introducing or designing specific\nsegmentation modules.\nIn this paper, we propose u-LLaVA, a novel approach to\nenhance the general, region and even pixel-level compre-\nhension abilities of MLLMs. To this end, we first design\n1\narXiv:2311.05348v3  [cs.CV]  10 Mar 2024\nSegmentation\nVideo Alignment\nA cute kitten almost fell down.\nImage Alignment\nA bright moon is just above \nthe sea level.\nVisual Question Answering\nThe girl is happy because the man \nis proposing to her.\nPixel \nDecoder\nVision Encoder\nProjector\nLLM\nQuery\nVision Encoder\nProjector\nLLM\nQuery\n(a) Stage I\n(b) Stage II\nProjector\nInpainting\nDog to owl\nSD\nRemoval\nRemove the dog\nSD\nGrounding\nRegion\nDecoder\nProjector\nFigure 1. Overview of u-LLaVA. In stage I, image and spatio-temporal features are used to efficiently boost the general-purpose modality\nalignment. In Stage II, task-specific projectors and decoders are jointly trained for region and pixel-level understanding. Further, additional\nmodules such as stable diffusion [31] can be easily patched for downstream tasks.\na efficient visual alignment strategy with image and spatio-\ntemporal representations, and task-specific projectors and\ndecoders are integrated for joint instruction tuning.\nThe\noverall pipeline is illustrated in Figure 1.\nTo enable pixel-level understanding, we employ a pro-\njector to connect MLLMs and SAM [14], achieving two\ngoals:\na) imparting semantic understanding capacity to\nSAM by leveraging the world knowledge inherent in LLM;\nand b) enhancing the pixel-level understanding ability of\nLLM by harnessing SAM.\nTo enhance the performance of regional-level compre-\nhension, we introduced a independent location decoder to\ndecode target coordinates from the hidden state or output of\nMLLMs, which greatly reduces the amount of data required\nfor training.\nTo accommodate the training of the aforementioned\nmodels, we have carefully designed a series of task-related\nprompt pools, and introduced a mask-based, region-specific\ndataset, namely ullava-277K. Most of the data was collected\nfrom publicly available datasets, with missing annotations\ncarefully supplemented by the GPT-3.5.\nContributions can be summarized in three folds:\n\u2022 We propose a efficient visual alignment method for multi-\nmodal pre-training, which leverages image (spatio fea-\ntures) and video (spatio-temporal features) to enhance the\nperceptual faculties of MLLMs.\n\u2022 We first-time introduce joint instruction tuning approach\nin the same stage to enable multi-level understanding with\ntask-specific projectors and decoders, see Table 1 for de-\ntails.\n\u2022 We release the joint instruction tuning dataset, ullava-\n277K, the model, and the code publicly available. Ad-\nditionally, we conduct comprehensive experiments and\ndemonstrate the effectiveness of the proposed method.\n2. Related Work\n2.1. MLLMs\nSurprised by the remarkable abilities of large language\nmodels, researchers have shown great interest in transfer-\nring the capabilities of LLM to other domains [43, 48]. In\nrecent months, remarkable progress has been made in this\nfield, such as LLaVA [19], MiniGPT-4 [56], Otter [15],\nKOSMOS-1/2 [11, 26], mPLUG-owl [47] and Flamingo\n[1], etc.\nWhile having demonstrated impressive perfor-\nmance in image-level understanding, these methods show\nlimited capabilities on pixel or region level tasks.\n2.2. Region-Level Comprehension\nReferring expression comprehension (REC) is one of the\nmost typical region-Level comprehension tasks, and Ref-\nCOCO [49], RefCOCO+ [49] and RefCOCOg [24], Ref-\nCLEF [12] are popular datasets for REC. Recently, some\nmethods have employed the pix2seq approach to achieve re-\ngional understanding [6, 26]. Some strategies further incor-\nporate regional encoding-decoding [51], while others utilize\nexternal modules to complete the task [53].\n2\n2.3. Pixel-Level Understanding\nThe advent of MLLMs has reduced the difficulty of subjec-\ntive visual tasks, but progress on mask-aware tasks, such as\nreferring expression segmentation (RES) and salient object\nsegmentation, has been relatively slow due to the difficulty\nin designing pixel-level tokens. The prevalent methods cur-\nrently involve using the output of grounding as the input\nfor SAM [51], or utilizing a specific decoder for end-to-end\ntraining [14, 50].\n3. Methods\nThe overall framework of u-LLaVA is presented in Figure 1.\nAs we can see, u-LLaVA is a multi-modal multitask chatbot\nthat takes text, images, and videos as inputs. It achieves this\nby unifying the representation space of visual and textual el-\nements at stage I, and understanding region and pixel repre-\nsentations jointly at stage II. In this part, we will first intro-\nduce the model architecture and modality alignment strat-\negy in Section 3.1, followed by a discussion on the joint\ninstruction tuning process in Section 3.2. Lastly, we will\npresent dataset construction methods.\n3.1. Efficient Visual Alignment\nTo align representations among different modalities, the\nprojector-based structure is adopted in this work: the pre-\ntrained CLIP ViT-L/14 [29] and a visual projector are com-\nbined to encode image inputs, while the Vicuna [7] is em-\nployed as the cognitive module. In addition, u-LLaVA sup-\nports video modality by concatenating spatial and temporal\nrepresentations, requiring only the addition of two special\nvideo tokens and a minimal amount of trainable parameters.\nTable 2. Special tokens for modality and task expressions. Where\nT denotes the number of frames and is set 8 in this work.\nImage\nVideo\nTag\nRegion\nPixel\nBegin\n<img beg>\n<vid beg>\n<tag> <LOC> <SEG>\nPatches\n<img patch> <vid patch>\n/\n/\n/\nEnd\n</img end>\n</vid end> </tag>\n/\n/\nSpecial token\nlength\n256\n256+T\n1\n1\n1\nGenerally, maximizing the likelihood function below to\nalign the representation spaces of image/video and text is\na widely-used approach for pre-training [19]. For a given\nimage or video embeddings xe, and a conversation list of L\ntokens xt = {x1\nt, x2\nt, ..., xL\nt }, we have the following train-\ning objectives, called coarse-grained loss:\nLcgl =\nX\ni\nlog P(xi|xe, xi\u2212k, ..., xi\u22121; \u03b8),\n(1)\nwhere, in accordance with [27], k, P, and \u03b8 are the size\nof context window, the conditional probability, and network\nparameters, respectively.\n3.2. Joint Instruction Tuning\nVisual instruction tuning is a common strategy for MLLM\nfine-tuning, but most methods only include one or two out\nof general, region, and pixel-level aspects during the train-\ning phase. In this work, we first-time jointly involve gen-\neral, region, and pixel features in the same tuning stage.\nGeneral-aware tuning: In this part, there are no adjust-\nments made to the model structure. However, unlike the\nfirst stage, we emphasize the use of multi-turn dialogues and\ncomplex reasoning datasets to further enhance the model\u2019s\nunderstanding capabilities.\nMask-aware tuning: Inspired by LISA [14], we employ\na projector to map the hidden states of the mask-related spe-\ncial tokens and then incorporate them into SAM decoder as\nthe text embeddings to facilitate pixel-level understanding.\nWe use a projector to connect SAM and MLLM, and map\nthe mask-related hidden states as the text embedding for\nSAM. This endows SAM with semantic perception capa-\nbilities while achieving pixel-level perception for MLLM.\nRegion-aware tuning: Similar to pixel perception, we\nutilize a projector and a location decoder, mapping the hid-\nden states of location-related special tokens directly to the\ntarget coordinates. In addition, we utilize the label infor-\nmation outputted by LLM as an input to the off-the-shelf\nregion perception model, such as GroundingDINO [21], for\nredundant inference, thereby enhancing the accuracy of re-\ngion perception.\nIn general, we have the following training objective,\nnamely fine-grained loss:\nLfgl = Lcgl +\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nLpixel,\nif mask exists\nLregion,\nif bbox exists\n0,\notherwise\n(2)\nwhere the term Lpixel = \u03b11Lbce + \u03b21Ldice represents the\nmask prediction loss, and Lregion = \u03b12L1 +\u03b22Lgiou is the\nprediction loss for the target bounding box. The values of\n\u03b11, \u03b12, \u03b21, and \u03b22 are set to 2.0, 2.0, 0.5, and 1.0, respec-\ntively.\nTo this end, the special tokens used in this work are listed\nin Table 2.\n3.3. Dataset Construction\nTo accommodate the training of the aforementioned mod-\nels, we reorganize or rebuild various types of public\ndatasets, details are summarized in Table 3.\nAs for the referring and semantic segmentation datasets,\nall references or semantic labels are extracted and then\nformed with the given templates. However, salient object\n3\nTable 3. Construction of the training datasets. The color blue\nindicates that the dataset is utilized in Stage I, while yellow sig-\nnifies its usage in Stage II. Where annotations\nDataset\nImages/Videos Annotations\nImage\nCaptioning\nLLaVA CC3M 595K [19]\n595,375\n595,375\nConversation-58K [19]\n56,681\n256,870\nDetail-23K [19]\n23,240\n23,240\nVideo\nCaptioning\nTGIF [17]\n125,782\n125,782\nVQA\nComplex-Reasoning-77K [19]\n76,643\n76,643\nRES\nRefCOCO [49]\n16,994\n120,624\nRefCOCO+ [49]\n16,992\n120,191\nRefCOCOg [24]\n21,899\n80,512\nRefCLEF [12]\n17,978\n108,652\nSemantic\nSegmentation\nCOCO-Stuff [29]\n118,205\n742,787\nVOC2010 [8]\n4,366\n81,139\nPACO-LVIS [30]\n45,790\n612,188\nADE20K [54]\n20,196\n165,120\nSalient-15K\nMSRA-10K [9]\n10,000\n10,000\nMSRA-B [37]\n5,000\n5,000\ndetection/segmentation datasets usually lack descriptions of\nthe target objects. To address this issue, we employ mask in-\nformation to extract the primary objects from images within\nMSRA-10K [9] and MSRA-B [37]. The extracted objects\nare then fed into BLIP2 [16] to generate descriptions solely\nfor the objects. Lastly, GPT3.5 is used to phrase the object\ntags from the generated description, followed by the inte-\ngration of predefined templates to complete the reconstruc-\ntion process. We refer to the reconstructed salient instruc-\ntion dataset as Salient-15K for short. The template exam-\nples and construction process of Salient-15K are summa-\nrized in Appendix.\n4. Experiments\n4.1. Implementation Details\nAll experiments are conducted with 8 NVIDIA Tesla A100\n80G GPUs and Pytorch framework [25]. Vicuna v1.1 [7]\nand CLIP ViT-L/14 [29] are set to the foundational lan-\nguage model and image encoder. SAM ViT-H [13] is se-\nlected as the mask decoder. A three-layer MLP with chan-\nnels of [4096, 2048, 4] and GroundingDINO Swin-T OGC\n[21] are set to the region decoders for hidden states and tags,\nrespectively. The vision projector for representation align-\nment and the hidden state projector for segmentation are\ntwo MLPs with channels of [1024, 4096] and [256, 4096,\n4096]. We select AdamW optimizer with the weight decay\nof 0, and learning rates of 2e-3 and 2e-5 for the first and the\nsecond stages (if LoRA[10] used in the Stage II, the learn-\ning rate will be set to 2e-4). The batch size per device is\nset to 32 and 16 (48 if LoRA) with gradient accumulation\nstep of 1 for the two stages. The first stage of training takes\napproximately 4 hours for 1 epoch, while the second stage\nof training takes around 33 hours (20 hours if LoRA) for 5\nepochs.\n4.2. Evaluation Metrics\nWe follow the previous works [14, 18] to validate the quan-\ntitative performance of the proposed algorithm, with spe-\ncific details as follows:\nPixel Segmentation:\nCumulative-IoU (cIoU) is a\nwidely-used performance indicator in segmentation tasks,\nwhich calculates the total intersection pixels over the total\nunion pixels. In some works, it is also referred to as the\noverall-IoU (oIoU), as seen in [44, 46].\nRegion Grounding: The percentage of samples with\nIoU higher than a threshold X is a commonly used metric in\nvisual grounding tasks, denoted as Precision@X (Prec@X).\nIn this work, we set the threshold to 0.5 according to [6].\n4.3. Pixel-Level Understanding Performance\nTo demonstrate the performance of the proposed method\non pixel-level understanding, we conduct experiments on\nwidely-used RES benchmarks, RefCOCO, RefCOCO+,\nand RefCOCOg. The comparison is made between exist-\ning state-of-the-art (SOTA) specialist models and MLLMs\nwith cIoU indicator, as presented in Table 4.\nAs can be seen from the table, even with LoRA, our\nmethod still achieves the best results among the MLLMs\nmethods. More notably, u-LLaVA-7B surpasses the per-\nformance of the prevailing state-of-the-art MLLM method,\nLISA-7B\u2217(ft), achieving an average improvement of 5.84.\nIt is also noteworthy that u-LLaVA either matches or sur-\npasses the performance of the current leading expert model,\nUNINEXT(H) [44], on the three benchmarks, all while uti-\nlizing merely a tenth of the training data. These findings\nserve as a testament to the efficacy of LLM in tasks that\nnecessitate comprehension-based capabilities.\n4.4. Pixel-level Intent Understanding Performance\nWe further examine the zero-shot performance of u-LLaVA\nin widely recognized salient segmentation datasets to clarify\nthe superiority of MLLMs in comprehending human sub-\njective intentions.\nHere, DUT-OMRON [45] (5,168 test images), DUTS-\nTE [38] (5,019 test images), and ECSSD [32] (1000 test\nimages) datasets are selected for validation. To ensure fair-\nness, we draw parallels between our method and a range\nof other previously conducted unsupervised algorithms. As\nsummarized in Table 5, u-LLaVA outperforms the rest,\nachieving SOTA performance across all three benchmarks,\nfurther solidifying the effectiveness and superiority of our\nmethod.\n4\nTable 4. RES results with cIoU indicator. Specialists represent models that are specifically designed for CV tasks. Where \u22c6 in MLLMs\ndenotes using LoRA [10] for parameter efficient training. The top 2 results are outlined in bold and with underline.\nType\nMethod\nSegmentation\nMasks\nRefCOCO\nRefCOCO+\nRefCOCOg\nval\ntest A\ntest B\nval\ntest A\ntest B\nval\ntest\nSpecialists\nLAVT [46]\n0.03M\n72.73\n75.82\n68.79\n62.14\n68.38\n55.10\n61.24\n62.09\nX-Decoder(L) [57]\n0.12M\n-\n-\n-\n-\n-\n-\n64.60\n-\nReLA [18]\n-\n73.82\n76.48\n70.18\n66.04\n71.02\n57.65\n65.00\n65.97\nSEEM(B) [58]\n0.12M\n-\n-\n-\n-\n-\n-\n65.00\n-\nSEEM(L) [58]\n0.12M\n-\n-\n-\n-\n-\n-\n65.60\n-\nPolyFormer(B) [20]\n0.16M\n74.82\n76.64\n71.06\n67.64\n72.89\n59.33\n67.76\n69.05\nPolyFormer(L) [20]\n0.16M\n75.96\n78.29\n73.25\n69.33\n74.56\n61.87\n69.20\n70.19\nUNINEXT(L) [44]\n3M\n80.32\n82.61\n77.76\n70.04\n74.91\n62.57\n73.41\n73.68\nUNINEXT(H) [44]\n3M\n82.19\n83.44\n81.33\n72.47\n76.42\n66.22\n74.67\n76.37\nLISA-7B\u22c6 [14]\n\u223c 0.80M\n74.10\n76.50\n71.10\n62.40\n67.40\n56.50\n66.40\n68.50\nLISA-7B\u22c6 (ft) [14]\n-\n74.90\n79.10\n72.30\n65.10\n70.80\n58.10\n67.90\n70.60\nNExT-Chat-7B [51]\n0.15M\n74.70\n78.90\n69.50\n65.10\n71.90\n56.70\n67.00\n67.00\nu-LLaVA-7B\u22c6 (Ours)\n\u223c 0.66M\n78.70\n81.46\n75.97\n68.95\n73.19\n62.26\n71.57\n72.91\nMLLMs\nu-LLaVA-7B (Ours)\n\u223c 0.66M\n80.29\n82.81\n77.40\n72.61\n75.96\n67.14\n74.07\n75.20\nTable 5. Salient segmentation results on salient object detection\nbenchmarks among different methods, where \u2020 denotes the method\nwith Bilateral solver[3], and cIoU is adopted as the metric.\nType\nMethod\nDUT-OMRON DUTS-TE ECSSD\nSpecialists\nLOST\u2020 [34]\n48.90\n57.20\n72.30\nTokenCut\u2020 [42]\n61.80\n62.40\n77.20\nSELFMASK\u2020 [33]\n65.50\n66.60\n81.80\nMOVE \u2020 [4]\n63.60\n68.70\n80.10\nu-LLaVA-7B\u22c6 (Ours)\n72.14\n72.35\n87.11\nMLLMs\nu-LLaVA-7B (Ours)\n71.64\n74.23\n89.73\n4.5. Region-Level Understanding Performance\nIn this section, we conduct a comparative analysis to evalu-\nate u-LLaVA\u2019s performance against other 7B MLLM mod-\nels in the context of region-level understanding tasks, using\nthe REC task as the benchmark.\nIt should be highlighted that we incorporate all interme-\ndiate results of our model, including the output of the re-\ngion decoder, the mask, and the grounding results based\non tags, to generate the final regression box thus optimiz-\ning the performance. This comprehensive process is visu-\nally encapsulated in Figure 3, and the corresponding exper-\nimental results are summarized in Table 6. Observably, u-\nLLaVA exhibits results closely paralleling those of Shikra\n[6], while utilizing a mere one-tenth of the data, and outper-\nforms other models in the RefCOCO testB and RefCOCOg\nsubsets. While there exists a discernible performance gap\nwhen compared to expert models such as UNINEXT(H),\nit is essential to consider that this is influenced by multi-\nple factors, including but not limited to input resolution and\ntask interference.\n4.6. Dataset Ablation\nAs shown in Table 7, we validate the impact of employ-\ning varied types of datasets during the second stage of the\nmodel\u2019s training on its overall performance. The results in-\ndicate that embracing diversity in dataset types fosters im-\nproved generalization of the algorithm, thereby circumvent-\ning the potential risk of overfitting on specific tasks.\nIn\nessence, the algorithm\u2019s robustness is enhanced with an in-\ncreased variety of dataset types.\n4.7. Qualitative examples\nQualitative comparison with existing multi-task MLLM\nmethods, LISA [14], Shikra [6] and CogVLM [40], on\ngrounding and segmentation tasks are given in Figure 2.\nMore illustrations can be found in Appendix.\n5. Conclusions\nIn this work, we introduce u-LLaVA, a multi-modal large\nlanguage model that jointly tunes instructions at the global,\nregional, and pixel levels. Through innovative structural de-\nsign and data configuration, we have achieved optimal per-\nformance in various comprehension-based tasks.\nCurrently,\nthe pre-training and task adaptation of\nMLLMs remain an open area with many directions yet to be\nexplored. This study represents an exploratory and experi-\nmental effort building upon previous works such as LLaVA\nand LISA. We believe that the open-sourcing of our work\ncan provide valuable assistance to the development of this\nfield.\n5\nTable 6. Comparative experiments of existing 7B MLLM models on REC tasks with Prec@0.5 indicator. The top 2 results are outlined in\nbold and with underline.\nType\nMethod\nGrounding\nBoxes\nRefCOCO\nRefCOCO+\nRefCOCOg\nval\ntest A\ntest B\nval\ntest A\ntest B\nval\ntest\nSpecialists\nSeqTR [55]\n1.5M\n87.00\n90.15\n83.59\n78.69\n84.51\n71.87\n82.69\n83.37\nGroundingDINO(L) [21]\n-\n90.56\n93.19\n88.24\n82.75\n88.95\n75.92\n86.13\n87.02\nOFA [39]\n-\n92.04\n94.03\n88.44\n87.86\n91.70\n80.71\n88.07\n88.78\nUNINEXT(L) [44]\n3M\n91.43\n93.73\n88.93\n83.09\n87.90\n76.15\n86.91\n87.48\nUNINEXT(H) [44]\n3M\n92.64\n94.33\n91.46\n85.24\n89.63\n79.79\n88.73\n89.37\nShikra-7B [6]\n\u223c 4M\n87.01\n90.61\n80.24\n81.60\n87.36\n72.12\n82.27\n82.19\nVisonLLM-H-7B [41]\n0.15M\n-\n86.70\n-\n-\n-\n-\n-\n-\nNeXT-Chat-7B [51]\n\u223c 4M\n85.50\n90.00\n77.90\n77.20\n84.50\n68.00\n80.10\n79.80\nu-LLaVA-7B\u2217 (Ours)\n0.39M\n84.51\n88.25\n79.81\n72.71\n79.43\n63.02\n79.52\n79.77\nMLLMs\nu-LLaVA-7B (Ours)\n0.39M\n86.15\n90.28\n81.14\n76.16\n82.94\n68.37\n81.31\n82.61\nWhere \nis\nthe sun.\nCogVLM-17B\nShikra\u201313B\nLISA-13B\nu-LLaVA-7B\nFind the \nthird bear \nfrom the left.\nThe woman\nholding the\nelephant\ntrunk.\nFigure 2. Qualitative examples of existing methods for regional and pixel-level understanding.\nu-LLaVA: \nSure. \nInfo: [SEG]; [LOC];\n[tag] teddy bear [/tag]\nGrounding\nIoU\nUSER: Find the third bear.\nFigure 3. The inference workflow of u-LLaVA for region-level\ncomprehension tasks.\nTable 7. Ablations on the Stage II training datasets, where \u201cLoRA\u201d\nand \u201cFull\u201d indicate whether LoRA strategy is employed, and cIoU\nis used as the performance indicator.\nExp. Referring Semantic Salient Captioning RefCOCOg test DUT-OMRON\nLoRA\nFull\nLoRA\nFull\n1\n\u2713\n71.26\n72.83\n52.77\n52.04\n2\n\u2713\n\u2713\n72.55\n75.04\n53.57\n46.70\n3\n\u2713\n\u2713\n\u2713\n72.11\n74.33\n46.46\n42.04\n4\n\u2713\n\u2713\n\u2713\n72.26\n75.10\n66.47\n65.45\n5\n\u2713\n\u2713\n\u2713\n\u2713\n72.91\n75.20\n72.14\n71.64\n6. Acknowledgements\nThis work is sponsored by Shanghai Pujiang Program\n(23PJ1421800).\n6\nu-LLaVA: Unifying Multi-Modal Tasks via Large Language Model\nSupplementary Material\n7. Appendix\n7.1. Templates\nHere, we present examples of task templates used by u-\nLLaVA on different type of training data.\nTemplate examples for salient segmentation task\n<image> What makes the image stand out?\n<image> What is salient one in this image?\n<image> Look at the image, segment the main\nobject in the picture and explain.\nTemplate examples for video captioning task\n<video> Describe the video concisely.\n<video> What\u2019s happening in this video?\n<video> Write a terse but informative summary\nof the VCR.\nTemplate examples for RES task\n<image> Segment out the <class>.\n<image> Output the mask of the <class>.\n<image> Find the <class> in the picture.\n7.2. Construction of Salient-15K\nAs shown in Figure 4, given that the MSRA-10K and\nMSRA-B datasets are devoid of label information and im-\nage descriptions pertaining to the principal subjects, we ini-\ntially proceed by extracting the subjects from the images\nand subsequently inputting them into BLIP2 [16] for a rudi-\nmentary description. Following this, we employ GPT3.5 to\nparse the target labels emanating from the elementary de-\nscription, thereby enabling an expansion of the description\ninformation. This approach facilitates a more comprehen-\nsive understanding of the subjects within the datasets while\ncompensating for the initial lack of descriptive data.\n7.3. Conversations with u-LLaVA\nExamples on VQA, video understanding and inpainting are\npresented in Figure 5. Besides, we show results of complex\nreasoning in Fig. 6, and video captioning results in Fig. 7.\nWe also illustrate the object removal and inpainting results\nin Fig. 8.\n\u2022\nBLIP2: The skateboarder \ndoing a trick.\n\u2022\nGPT: skateboarder\n\u2022\nUSER: <image> What makes this\nphoto stand out?\n\u2022\nASSISTANT: It's the skateboarder\ndoing a trick.\nInfo:\n[SEG] ; [LOC];\n[tag] skateboarder [/tag]\nFigure 4. The process workflow of Salient-15K.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 2\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond. 2023.\n1\n[3] Jonathan T Barron and Ben Poole. The fast bilateral solver.\nIn European conference on computer vision, pages 617\u2013632.\nSpringer, 2016. 5\n[4] Adam Bielski and Paolo Favaro. Move: Unsupervised mov-\nable object segmentation and detection. Advances in Neural\nInformation Processing Systems, 35:33371\u201333386, 2022. 5\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 1\n[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 1, 2, 4, 5, 6\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 3, 4\n[8] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88:303\u2013338, 2010. 4\n[9] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji,\n1\nHow many birds are there\nin this image?\nDescribe the video\nconcisely.\nThis is a video of\na man starting a\nfire using wood\nand matches.\nThe dog.\nInpainting: a cute cat\nThe bench.\nInpainting: a grey sofa\nThere are three birds in\nthis image.\nWhere is the first bird?\nVQA\nVideo Captioning\nInpainting\nFigure 5. VQA, video captioning and inpainting examples.\nIt is a dog. Info: \n[SEG] ; [LOC];\n[tag] a dog [/tag].\nPoint out the salient object in the image\nSomething that can emit light is called a\nlight source. Please find the light source\nin the picture.\nSure. Info: \n[SEG] ; [LOC];\n[tag] light source [/tag].\nRES\nSalient\nSure. Info: \n[SEG] ; [LOC];\n[tag] hair [/tag].\nFind the hair of the lady.\nPart\nThe girl is happy because the man is\nproposing to her with a ring while holding a\nbouquet of red roses. This romantic gesture\nand the act of proposing marriage are likely\nto bring joy and excitement to the girl,\nmaking the moment special and memorable.\nWhy the girl is so happy?\nReasoning\nFigure 6. Conversation examples with u-LLaVA-7B.\nA woman is\npracticing\narchery.\nDescribe the\nvideo.\nA volleyball \ngame.\nWhat's \nhappening \nin the VCR?\nFigure 7. Video captioning examples with u-LLaVA-7B.\nZhuowen Tu, and Philip Torr. Deeply supervised salient ob-\nject detection with short connections. IEEE TPAMI, 41(4):\n815\u2013828, 2019. 4\n[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 4, 5\n[11] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\n2\nThe dog.\nRemoval\nThe dog.\nDiffusion\nDiffusion\nInpainting\nFigure 8. Image inpainting and object removal examples with u-LLaVA-7B.\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv preprint arXiv:2302.14045, 2023. 2\n[12] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing\n(EMNLP), pages 787\u2013798, 2014. 2, 4\n[13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 4\n[14] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692,\n2023. 1, 2, 3, 4, 5\n[15] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 2\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 4, 1\n[17] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault,\nLarry Goldberg, Alejandro Jaimes, and Jiebo Luo.\nTgif:\nA new dataset and benchmark on animated gif description.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 4641\u20134650, 2016. 4\n[18] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gener-\nalized referring expression segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 23592\u201323601, 2023. 4, 5\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 1, 2, 3, 4\n[20] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Ku-\nmar Satzoda, Vijay Mahadevan, and R. Manmatha. Poly-\nformer: Referring image segmentation as sequential poly-\ngon generation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n18653\u201318663, 2023. 5\n[21] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 3, 4, 6\n[22] David G Lowe.\nDistinctive image features from scale-\ninvariant keypoints. International journal of computer vi-\nsion, 60:91\u2013110, 2004. 1\n[23] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video\nunderstanding via large vision and language models. arXiv\npreprint arXiv:2306.05424, 2023. 1\n[24] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11\u201320, 2016. 2, 4\n[25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-\nson, Andreas Kopf, Edward Yang, Zachary DeVito, Mar-\ntin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit\nSteiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:\nAn imperative style, high-performance deep learning library.\nIn Advances in Neural Information Processing Systems 32,\npages 8024\u20138035. Curran Associates, Inc., 2019. 4\n[26] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 1, 2\n[27] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. OpenAI blog, 2018. 1, 3\n[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 1\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 4\n[30] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi\nWen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar-\nquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts\nand attributes of common objects.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7141\u20137151, 2023. 4\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n3\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 2\n[32] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical\nimage saliency detection on extended cssd. IEEE transac-\ntions on pattern analysis and machine intelligence, 38(4):\n717\u2013729, 2015. 4\n[33] Gyungin Shin, Samuel Albanie, and Weidi Xie. Unsuper-\nvised salient object detection with spectral cluster voting. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3971\u20133980, 2022. 5\n[34] Oriane Sim\u00b4eoni, Gilles Puy, Huy V Vo, Simon Roburin,\nSpyros Gidaris, Andrei Bursuc, Patrick P\u00b4erez, Renaud\nMarlet, and Jean Ponce.\nLocalizing objects with self-\nsupervised transformers and no labels.\narXiv preprint\narXiv:2109.14279, 2021. 5\n[35] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 1\n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 1\n[37] Jingdong Wang, Huaizu Jiang, Zejian Yuan, Ming-Ming\nCheng, Xiaowei Hu, and Nanning Zheng.\nSalient object\ndetection: A discriminative regional feature integration ap-\nproach. International Journal of Computer Vision, 123(2):\n251\u2013268, 2017. 4\n[38] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,\nDong Wang, Baocai Yin, and Xiang Ruan. Learning to de-\ntect salient objects with image-level supervision. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 136\u2013145, 2017. 4\n[39] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In International Conference on Machine Learn-\ning, pages 23318\u201323340. PMLR, 2022. 6\n[40] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong,\nYan Wang, Junhui Ji, Zhuiyi Yang, Lei Zhao, Xixuan Song,\nJiazheng Xu, Xu Bin, Huanzi Li, Yuxiao Dong, Ming Ding,\nand Jie Tang. Cogvlm: Visual expert for large language mod-\nels. arXiv preprint, 2023. 1, 5\n[41] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al. Visionllm: Large language model is also an open-\nended decoder for vision-centric tasks. Advances in Neural\nInformation Processing Systems, 36, 2024. 6\n[42] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L\nCrowley, and Dominique Vaufreydaz. Self-supervised trans-\nformers for unsupervised object discovery using normalized\ncut. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14543\u201314553,\n2022. 5\n[43] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng\nChua. Next-gpt: Any-to-any multimodal llm. arXiv preprint\narXiv:2309.05519, 2023. 2\n[44] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-\nhuan Yuan, and Huchuan Lu.\nUniversal instance percep-\ntion as object discovery and retrieval.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 15325\u201315336, 2023. 4, 5, 6\n[45] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and\nMing-Hsuan Yang. Saliency detection via graph-based man-\nifold ranking.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3166\u20133173,\n2013. 4\n[46] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-\nshuang Zhao, and Philip H.S. Torr. Lavt: Language-aware\nvision transformer for referring image segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 18155\u201318165, 2022.\n4, 5\n[47] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 2\n[48] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen. A survey on multimodal large\nlanguage models. arXiv preprint arXiv:2306.13549, 2023. 2\n[49] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions.\nIn Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\n2, 4\n[50] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie\nLuo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel\nunderstanding with visual instruction tuning. arXiv preprint\narXiv:2312.10032, 2023. 1, 3\n[51] Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji,\nand Tat-Seng Chua. Next-chat: An lmm for chat, detection\nand segmentation. arXiv preprint arXiv:2311.04498, 2023.\n1, 2, 3, 5, 6\n[52] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv preprint arXiv:2306.02858, 2023. 1\n[53] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi\nFeng, and Bingyi Kang. Bubogpt: Enabling visual ground-\ning in multi-modal llms. arXiv preprint arXiv:2307.08581,\n2023. 2\n[54] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017. 4\n[55] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia\nPan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun,\nand Rongrong Ji. Seqtr: A simple yet universal network for\nvisual grounding. In European Conference on Computer Vi-\nsion, pages 598\u2013615. Springer, 2022. 6\n4\n[56] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 2\n[57] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,\nChunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu\nYuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jian-\nfeng Gao. Generalized decoding for pixel, image, and lan-\nguage. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 15116\u2013\n15127, 2023. 5\n[58] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Gao, and Yong Jae Lee. Segment everything every-\nwhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n5\n5\n"
  },
  {
    "title": "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving",
    "link": "https://arxiv.org/pdf/2311.05332.pdf",
    "upvote": "7",
    "text": "On the Road with GPT-4V(ision): Early Explorations\nof Visual-Language Model on Autonomous Driving\nLicheng Wen1\u2020, Xuemeng Yang1\u2020, Daocheng Fu1\u2020, Xiaofeng Wang2\u2020, Pinlong Cai1, Xin Li1,3,\nTao Ma1,4, Yingxuan Li2, Linran Xu2, Dengke Shang2, Zheng Zhu2\u2660, Shaoyan Sun2,\nYeqi Bai1, Xinyu Cai1, Min Dou1, Shuanglu Hu5, Botian Shi1\u2660, Yu Qiao1\n\u2020 Core Contributors\n\u2660 Corresponding Authors\n1 Shanghai Artificial Intelligence Laboratory, Shanghai, China\n2 GigaAI, Beijing, China\n3 East China Normal University, Shanghai, China\n4 The Chinese University of Hong Kong, Hong Kong, China\n5 WeRide.ai, Shanghai, China\nshibotian@pjlab.org.cn\nzhengzhu@ieee.org\nAbstract\nThe pursuit of autonomous driving technology hinges on the sophisticated in-\ntegration of perception, decision-making, and control systems. Traditional ap-\nproaches, both data-driven and rule-based, have been hindered by their inability\nto grasp the nuance of complex driving environments and the intentions of other\nroad users. This has been a significant bottleneck, particularly in the develop-\nment of common sense reasoning and nuanced scene understanding necessary for\nsafe and reliable autonomous driving. The advent of Visual Language Models\n(VLM) represents a novel frontier in realizing fully autonomous vehicle driv-\ning. This report provides an exhaustive evaluation of the latest state-of-the-art\nVLM, GPT-4V(ision), and its application in autonomous driving scenarios. We\nexplore the model\u2019s abilities to understand and reason about driving scenes, make\ndecisions, and ultimately act in the capacity of a driver. Our comprehensive\ntests span from basic scene recognition to complex causal reasoning and real-\ntime decision-making under varying conditions. Our findings reveal that GPT-4V\ndemonstrates superior performance in scene understanding and causal reasoning\ncompared to existing autonomous systems. It showcases the potential to handle\nout-of-distribution scenarios, recognize intentions, and make informed decisions\nin real driving contexts. However, challenges remain, particularly in direction\ndiscernment, traffic light recognition, vision grounding, and spatial reasoning\ntasks. These limitations underscore the need for further research and development.\nProject is now available on GitHub for interested parties to access and utilize:\nhttps://github.com/PJLab-ADG/GPT4V-AD-Exploration\narXiv:2311.05332v2  [cs.CV]  28 Nov 2023\nContents\nList of Figures\n3\n1\nIntroduction\n4\n1.1\nMotivation and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nGuidance\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2\nBasic Capability of Scenario Understanding\n6\n2.1\nUnderstanding of Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nUnderstanding of Traffic Participants . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3\nAdvanced Capability of Reasoning\n18\n3.1\nCorner Cases\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.2\nMulti-view Images\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.3\nTemporal Sequences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.4\nVisual-Map Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4\nAct as A Driver\n33\n4.1\nDriving in Parking Lot\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2\nTurning at Traffic Intersection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.3\nTurning at Highway Ramp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.4\nRoad Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n4.5\nU-Turning at Traffic Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5\nConclusions\n40\n5.1\nCapabilities of GPT-4V in Autonomous Driving . . . . . . . . . . . . . . . . . . .\n40\n5.2\nLimitations of GPT-4V in Autonomous Driving . . . . . . . . . . . . . . . . . . .\n40\n2\nList of Figures\n1\nSection 1: Introduction figure . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nSection 2.1: Time understanding throughout the day . . . . . . . . . . . . . . . . .\n6\n3\nSection 2.1: Weather understanding\n. . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4\nSection 2.1: Traffic light understanding, Part I . . . . . . . . . . . . . . . . . . . .\n9\n5\nSection 2.1: Traffic light understanding, Part II\n. . . . . . . . . . . . . . . . . . .\n9\n6\nSection 2.1: Understanding of traffic signs . . . . . . . . . . . . . . . . . . . . . .\n10\n7\nSection 2.2: Front-view camera images, Part I . . . . . . . . . . . . . . . . . . . .\n11\n8\nSection 2.2: Front-view camera images, Part II\n. . . . . . . . . . . . . . . . . . .\n13\n9\nSection 2.2: Fisheye camera images . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n10\nSection 2.2: Point cloud lidar images . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n11\nSection 2.2: V2X equipment images . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n12\nSection 2.2: Simulated images . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n13\nSection 3.1: Corner cases, Part I . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n14\nSection 3.1: Corner cases, Part II . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n15\nSection 3.1: Corner cases, Part III\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n16\nSection 3.2: Multi-view images, Part I . . . . . . . . . . . . . . . . . . . . . . . .\n23\n17\nSection 3.2: Multi-view images, Part II . . . . . . . . . . . . . . . . . . . . . . . .\n24\n18\nSection 3.2: Multi-view images, Part III . . . . . . . . . . . . . . . . . . . . . . .\n25\n19\nSection 3.3: Temporal sequences, Part I\n. . . . . . . . . . . . . . . . . . . . . . .\n27\n20\nSection 3.3: Temporal sequences, Part II . . . . . . . . . . . . . . . . . . . . . . .\n28\n21\nSection 3.3: Temporal sequences, Part III\n. . . . . . . . . . . . . . . . . . . . . .\n29\n22\nSection 3.3: Temporal sequences, Part IV\n. . . . . . . . . . . . . . . . . . . . . .\n30\n23\nSection 3.4: Visual-map navigation, Part I . . . . . . . . . . . . . . . . . . . . . .\n31\n24\nSection 3.4: Visual-map navigation, Part II . . . . . . . . . . . . . . . . . . . . . .\n32\n25\nSection 4.1: Driving in parking lot . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n26\nSection 4.2: Turning at traffic intersection . . . . . . . . . . . . . . . . . . . . . .\n35\n27\nSection 4.3: Turning at highway ramp . . . . . . . . . . . . . . . . . . . . . . . .\n37\n28\nSection 4.4: Road merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n29\nSection 4.5: U-turning at traffic intersection . . . . . . . . . . . . . . . . . . . . .\n39\n3\nFigure 1: An illustration showing the transition from the traditional autonomous driving pipeline to\nthe integration of visual language models such as GPT-4V. This picture is generated by DALL\u00b7E 3.\n1\nIntroduction\n1.1\nMotivation and Overview\nThe quest for fully autonomous vehicles has long been constrained by a pipeline that relies on\nperception, decision-making, and planning control systems. Traditional approaches, whether rooted\nin data-driven algorithms or rule-based methods, fall short in several key areas. Specifically, they\nexhibit weaknesses in accurately perceiving open-vocabulary objects and struggle with interpreting\nthe behavioral intentions of surrounding traffic participants. The reason is that traditional approaches\nonly characterize abstract features of limited acquisition data or deal with problems according\nto predetermined rules, whereas they lack the \u201ccommon sense\u201d to deal with rare but significant\ncorner cases and fail to summarize driving-related knowledge from the data for nuanced scenario\nunderstanding and effective causal reasoning.\nThe emergence of Large Language Models (LLMs), exemplified by GPT-3.5 [12], GLM [7, 24],\nLlama [18, 19], et al., has shown a glimmer of hope in addressing these issues. The LLMs are\nequipped with a rudimentary form of common sense reasoning, thereby showing promise in under-\nstanding complex driving scenarios. However, their application in autonomous driving has been\nrestricted mainly to decision-making and planning phases [8, 20, 5, 11]. This limitation is due to their\ninherent inability to process and understand visual data, which is critical for accurately perceiving the\ndriving environment and driving the vehicle safely.\nThe recent development of GPT-4V [15, 16, 13, 22], a cutting-edge Vision-Language Model (VLM),\nhas opened up new vistas for research and development. Unlike its predecessors (GPT-4 [14]),\nGPT-4V possesses robust capabilities in image understanding, marking a significant step forward\nin closing the perception gap in autonomous driving technologies. This newfound strength raises\nthe question: Can GPT-4V serve as a cornerstone for improving scene understanding and causal\nreasoning in autonomous driving?\nIn this paper, we aim to answer this pivotal question by conducting an exhaustive evaluation of\nGPT-4V\u2019s abilities. Our research delves into the model\u2019s performance in the intricate aspects of\nscene understanding and causal reasoning within the domain of autonomous driving. Through\nexhaustive testing and in-depth analysis, we have elucidated both the capabilities and limitations of\nGPT-4V, which is anticipated to offer valuable support for researchers to venture into potential future\napplications within the autonomous driving industry.\n4\nWe have tested the capabilities of GPT-4V with increasing difficulty, from scenario understanding\nto reasoning, and finally testing its continuous judgment and decision-making ability as drivers in\nreal-world driving scenarios. Our exploration of GPT-4V in the field of autonomous driving mainly\nfocuses on the following aspects:\n1. Scenario Understanding: This test aims to assess GPT-4V\u2019s fundamental recognition abili-\nties. It involves recognizing weather and illumination conditions while driving, identifying\ntraffic lights and signs in various countries, and assessing the positions and actions of other\ntraffic participants in photos taken by different types of cameras. Additionally, we explored\nsimulation images and point cloud images of different perspectives for curiosity\u2019s sake.\n2. Reasoning: In this phase of the test, we delve deeper into assessing GPT-4V\u2019s causal\nreasoning abilities within autonomous driving contexts. This evaluation encompasses several\ncrucial aspects. Firstly, we scrutinize its performance in tackling complex corner cases,\nwhich often challenge data-driven perception systems. Secondly, we assess its competence\nin providing a surround view, which is a vital feature in autonomous driving applications.\nGiven GPT-4V\u2019s inability to directly process video data, we utilize concatenated time series\nimages as input to gauge its temporal correlation capabilities. Additionally, we conduct\ntests to validate its capacity to associate real-world scenes with navigation images, further\nexamining its holistic understanding of autonomous driving scenarios.\n3. Act as a driver: To harness the full potential of GPT-4V, we entrusted it with the role of\na seasoned driver, tasking it with making decisions in real driving situations based on the\nenvironment. Our approach involved sampling driving video at a consistent frame rate and\nfeeding it to GPT-4V frame by frame. To aid its decision-making, we supplied essential\nvehicle speed and other relevant information and communicated the driving objective for each\nvideo. We challenged GPT-4V to produce the necessary actions and provide explanations for\nits choices, thereby pushing the boundaries of its capabilities in real-world driving scenarios.\nIn conclusion, we offer initial insights as a foundation for inspiring future research endeavors in\nthe realm of autonomous driving with GPT-4V. Building upon the information presented above, we\nmethodically structure and showcase the qualitative results of our investigation using a unique and\nengaging compilation of image-text pairs. While this methodology may be somewhat less stringent,\nit affords the opportunity for a comprehensive analysis.\n1.2\nGuidance\nThis article focuses on testing in the field of autonomous driving, employing a curated selection of\nimages and videos representing diverse driving scenarios. The test samples are sourced from various\noutlets, including open-source datasets such as nuScenes [3], Waymo Open dataset [17], Berkeley\nDeep Drive-X (eXplanation) Dataset (BDD-X) [9], D2-city [4], Car Crash Dataset (CCD) [2],\nTSDD [1], CODA [10], ADD [21], as well as V2X datasets like DAIR-V2X [23] and CitySim [25].\nAdditionally, some samples are derived from the CARLA [6] simulation environment, and others are\nobtained from the internet. It\u2019s worth noting that the image data used in testing may include images\nwith timestamps up to April 2023, potentially overlapping with the GPT-4V model\u2019s training data,\nwhile the text queries employed in this article are entirely generated anew.\nAll experiments detailed in this paper were conducted before November 5th, 2023, utilizing the\nweb-hosted GPT-4V(ision) (version from September 25th). We acknowledge that the most recent\nversion of GPT-4V, which has received updates following the November 6th OpenAI DevDay, may\nproduce different responses when presented with the same images compared to our test results.\n5\n2\nBasic Capability of Scenario Understanding\nTo achieve safe and effective autonomous driving, a fundamental prerequisite is a thorough under-\nstanding of the current scenario. Complex traffic scenarios encompass a myriad of driving conditions,\neach hosting a diverse array of traffic participants. Accurate recognition and comprehension of these\nelements serve as basic capabilities for an autonomous vehicle to make informed and appropriate\ndriving decisions. In this section, we present a series of tests aimed at evaluating GPT-4V\u2019s ability to\ncomprehend traffic scenarios. We focus on two primary aspects: the model\u2019s understanding of the\nsurrounding environment and its capacity to discern the behavior and status of various traffic partici-\npants. Through these assessments, we aim to shed light on GPT-4V\u2019s competence in interpreting the\ndynamic traffic environment.\n2.1\nUnderstanding of Environment\nIn assessing the capabilities of GPT-4V to comprehend its surrounding environments, we conducted\na series of tests encompassing the following key aspects: its ability to discern the time of day, its\nunderstanding of prevailing weather conditions, and its proficiency in recognizing and interpreting\ntraffic lights and signs. These elements hold paramount significance in shaping the autonomous\ndriving system\u2019s decision-making process. For instance, it is evident that driving at night or in\nchallenging weather conditions requires a heightened level of caution, whereas during daylight hours\nor in favorable weather conditions a more leisurely driving strategy can be adopted. Besides, the\ncorrect interpretation of traffic lights and road signs is essential for the effectiveness of autonomous\ndriving systems. We utilize vehicles\u2019 front-view images as the primary visual input throughout this\nsection. The visual data employed here is drawn from nuScenes [3], D2-city[4], BDD-X [9] and\nTSDD [1].\nTime understanding throughout the day. We evaluate GPT-4V\u2019s ability to comprehend temporal\ndifferences by providing it with both daytime and nighttime images. We instruct the model to describe\nthe traffic scenarios depicted in these images, and the results are presented in Figure 2. The findings\nreveal that, when presented with daytime scenes, GPT-4V successfully identifies them as multi-lane\nSec. 2.1 Time understanding throughout the day\nPrompt:\nThis is the front-view image of your car. Please\ndescribe the current traffic scenario you're in.\nGPT-4V:\nYou're on a city street with multiple lanes. There\nare parked cars on both sides of the road.\nAhead, there's moderate traffic with vehicles\nmoving\nin\nboth\ndirections.\nThere's\nalso\na\npedestrian crossing zone visible. Buildings and\ntrees line the street, and there are various street\nsigns along the way.\nPrompt:\nThis is the front-view image of your car. Please\ndescribe the current traffic scenario you're in.\nGPT-4V:\nYou are on a road during twilight or early evening. \nThe road has a pedestrian crossing ahead. \nThere's a vehicle with its tail lights on, indicating \nit's either stationary or moving away from you. \nThere are traffic signs on both sides of the road, \nand the area appears to be moderately lit. Trees \nand other structures are visible in the background.\nFigure 2: Results on the ability to comprehensively understand time over the course of a day. Green\nhighlights the right answer in understanding. Check Section 2.1 for detailed discussions.\n6\nurban roads with \u201cmoderate traffic\u201d. Furthermore, the model adeptly recognizes the presence of a\ncrosswalk on the road. When confronted with similar nighttime scenes, GPT-4V\u2019s performance is\neven better. It not only discerns the time as \u201ctwilight or early evening\u201d but also detects a vehicle with\nits tail lights on in the distance, and infers that \u201cit\u2019s either stationary or moving away from you\u201d.\nWeather understanding. Weather is a crucial environmental factor that significantly influences\ndriving behavior. We selected four photographs captured at the same intersection under varying\nweather conditions from the nuScenes [3] dataset. We tasked GPT-4V with identifying the weather\nconditions depicted in these images. The results are presented in Figure 3. The results demonstrate\nthat GPT-4V exhibits remarkable accuracy in recognizing the weather conditions in each image,\nnamely, cloudy, sunny, overcast, and rainy. Moreover, it provides sound justifications for these\nconclusions, citing factors such as the presence of sunny shadows or the wetness of the streets.\nTraffic light understanding. Recognition of traffic lights plays a pivotal role in the functionality\nof an autonomous driving system. Incorrectly identifying or missing traffic lights not only leads\nto violations of traffic regulations but also poses a serious risk of traffic accidents. Unfortunately,\nthe performance of GPT-4V in this test falls short, as evident in Figure 4 and Figure 5. In Figure 4,\nGPT-4V demonstrates proficiency in distinguishing between yellow street lights and red traffic lights,\nparticularly during nighttime conditions. However, in Figure 5, when confronted with a smaller traffic\nlight with a countdown timer in the distant part of the image, GPT-4V inaccurately identifies the\ncountdown as red and overlooks the genuine 2-second red countdown. The model can provide the\ncorrect response only when the traffic light is zoomed in to occupy a significant portion of the image.\nFurthermore, GPT-4V exhibited instances of misrecognition of traffic lights during subsequent tests,\nwhich is deemed unacceptable for a mature autonomous driving system.\nTraffic signs understanding. Traffic signs contain various rules and instructions that drivers need\nto follow. Autonomous driving systems can understand and comply with these rules by identifying\ntraffic signs, thereby reducing the risk of traffic accidents and improving driving safety. Therefore,\nwe selected representative images from Singapore and China for testing. As can be seen from the\nleft sample in Figure 6, the model can recognize most road signs, including the nearby \u201cSLOW\u201d and\nthe distant height limit \u201c4.5m\u201d, but it misrecognizes the \u201cSpeed Bump\u201d sign. The three signs from\nthe right sample are all correctly recognized. This indicates that GPT-4V possesses a commendable\nability to recognize traffic signs, yet there remains room for further enhancement.\n7\nSec. 2.1 Weather understanding\nPrompt:\nPlease describe the weather in this image\nGPT-4V:\nThe image depicts a clear day with no visible\nsigns of precipitation. The sky appears partly \ncloudy, with patches of blue sky. The sunlight\ncasts shadows on the ground, indicating it's either\nmorning or afternoon.\nPrompt:\nPlease describe the weather in this image\nGPT-4V:\nThe image depicts a clear and sunny day with\nshadows cast by the buildings and vehicles,\nindicating bright sunlight. The sky appears to be\nmostly cloudless.\nPrompt:\nPlease describe the weather in this image\nGPT-4V:\nThe image shows an overcast day. The lighting is\ndiffused, with no strong shadows visible on the\nground or buildings. The sky appears to be cloudy,\nsuggesting a lack of direct sunlight.\nPrompt:\nPlease describe the weather in this image\nGPT-4V:\nThe image depicts a rainy or recently rainy \nenvironment. The roads and pavements are wet,\nand there is a visible sheen on the street. The\nsky appears overcast, and the overall lighting is\ndiffused, indicating cloudy conditions.\nFigure 3: Results that identify the season in which the vehicle is driven. Green highlights the right\nanswer in understanding. Check Section 2.1 for detailed discussions.\n8\nSec. 2.1 Traffic light understanding\nPrompt:\nDescribe the condition of all the traffic lights \nin the image, if there are countdowns, \nplease describe them together.\nGPT-4V:\nThe image has two red traffic lights, both without\nany visible countdowns.\nPrompt:\nHow about the yellow lights in the image?\nGPT-4V:\nIn addition to the red traffic lights, there are two \nyellow streetlights illuminating the road, but they \nare not traffic signals.\nFigure 4: Results on the ability to understand the traffic light. Green highlights the right answer in\nunderstanding. Check Section 2.1 for detailed discussions.\nSec. 2.1 Traffic light understanding\nGPT-4V:\nThe traffic lights in the center show a green light \nand a red arrow with a countdown displaying \"4\". \nOn the left and right, the traffic lights are\nshowing red.\nPrompt:\nDescribe the condition of all the traffic lights in\nthe image, if there are countdowns, please \ndescribe them together.\nGPT-4V:\nThe traffic light is showing a green light with \na countdown displaying \"4\", and a red arrow\nwith a countdown displaying \"2\".\nPrompt:\nDescribe the condition of all the traffic lights in\nthe image, if there are countdowns, please \ndescribe them together.\nFigure 5: Results on the ability to understand the traffic light. Green highlights the right answer in\nunderstanding, Red highlights the wrong answer in understanding. Check Section 2.1 for detailed\ndiscussions.\n9\nSec. 2.1 Traffic signs understanding\nPrompt:\nThis is a front-camera view of a driving car.\nPlease identify all the traffic signs in the image,\nand explain each of them.\nGPT-4V:\nThere are three traffic signs in the image:\nA sign under the overhead structure displaying \n\"4.5m\":\nThis\nsign\nindicates\nthe\nmaximum\nheight\nof\nvehicles\nthat\ncan\nsafely\npass\nunderneath the structure. Vehicles taller than\n4.5 meters should not proceed.\nA triangular sign with a curved arrow pointing \nleft: This is a warning sign that indicates there\nis a left bend or curve ahead. Drivers should\nbe cautious and possibly reduce their speed.\nA rectangular sign with the word \"SLOW\": This\nsign instructs drivers to reduce their speed,\ntypically because of potential hazards or a\ncondition ahead that requires slower driving.\nPrompt:\nThis is a front-camera view of a driving car.\nPlease identify all the traffic signs in the image,\nand explain each of them.\nGPT-4V:\nThere are three traffic signs displayed:\n60: This is a speed limit sign, indicating that the\nmaximum allowed speed on this road or section\nof the road is 60 kilometers per hour (km/h).\nTruck with a slash through it: This is a no trucks\nor no heavy vehicles sign, indicating that trucks\nor heavy vehicles are not permitted on this road\nor section of the road.\nCar with a horn and a slash through it: This\nsignifies a no honking or horn prohibited zone.\nDrivers are not allowed to use their vehicle's\nhorn in the area where this sign is displayed.\nFigure 6: Results on the ability to understand the traffic signs. Green highlights the right answer in\nunderstanding, Red highlights the wrong answer in understanding. Check Section 2.1 for detailed\ndiscussions.\n10\n2.2\nUnderstanding of Traffic Participants\nAccurately understanding the status and behavior of traffic participants is the foundation of driving.\nExisting autonomous driving systems often use a variety of cameras and sensors to perceive traffic\nparticipants in order to obtain more comprehensive information about them. In this section, we assess\nGPT-4V\u2019s proficiency in comprehending the behavior of traffic participants using various sensor\ninputs, including 2D images, visualizations of 3D point clouds, and images acquired from V2X\ndevices and autonomous driving simulation software. The visual data employed here is drawn from\nnuScenes [3], ADD [21], Waymo [17], DAIR-V2X [23], CitySim [25] and Carla [6] simulation.\nFront-view camera photos. In order to test the model\u2019s basic recognition capabilities, including\ntraffic participant recognition and vehicle counting, we input a series of front views of driving scenes\nand obtained the output results of GPT-4V. As can be seen from the left side of Figure 7, the model\ncan completely and accurately describe the driving scene: it recognizes pedestrians, traffic signs,\ntraffic light status and the surrounding environment. The right side of Figure 7 shows that the model\ncan identify the vehicle type and its taillights, and can guess its intention to turn on the taillights.\nHowever, the model outputs some incorrect statements in irrelevant places, such as thinking that the\ncar in front has a rearview camera. In Figure 8, the counting proficiency of GPT-4V is put to the test.\nSec. 2.2 Front view camera images\nPrompt:\nThis is what you see behind the wheel. Describe\nthe traffic scenario as detailed as possible.\nGPT-4V:\nYou're approaching a city intersection with\nmultiple \npedestrian \ncrosswalks.\nSeveral\npedestrians are crossing the street in various\ndirections. There's a sign indicating \"NO TURN\nON\nRED\"\nand overhead traffic\nlights are\nshowing red for your lane. [...] On the right,\nthere are some buildings and construction\ncranes in the distance..\nPrompt:\nThis is what you see behind the wheel. Please\ndescribe your front car as detailed as possible.\nGPT-4V:\nThe car in front of you is a white vehicle, likely a\nhatchback or compact SUV [...] The license plate\nis visible but not readable in the picture. The rear\nlights are turned on, indicating either braking or \nnighttime running lights. The car also has a\nrearview camera located above the license plate.\nPrompt:\nBased on the image, what reason do you think \ncaused the car in front braked?\nGPT-4V:\nThe car in front might have braked due to the\npresence\nof\ntraffic\ncones\nlining\nthe\nroad,\nindicating potential roadwork, lane closures, or a\nchange in traffic flow. Another reason could be\nthe presence of other vehicles ahead, though\nthey are not visible in the image provided.\nPrompt:\nPlease output the coordinate position or \nbounding box for each pedestrian.\nGPT-4V:\nI don't have the capability to provide real-time \ncoordinates or bounding boxes for objects within\nimages.\nFigure 7: Results describing traffic participants via the front-view camera photo. Green highlights\nthe right answer in understanding, Red highlights the wrong answer in understanding, Yellow\nhighlights the incompetence in performing the task. Check Section 2.2 for detailed discussions.\n11\nUtilizing both a daytime and a nighttime snapshot from the vehicle\u2019s front view, the model precisely\ntallies the vehicular presence and their statuses in the daylight capture. In contrast, during nighttime\nconditions, despite GPT-4V accurately enumerating the discernible vehicles, its elaborate description\nof each individual vehicle sometimes falls short of accuracy.\nFish-eye camera photo. The fisheye camera, a prevalent imaging device within autonomous vehicle\nsystems, was also employed to evaluate the perception abilities of GPT-4V. Results derived from\nimages captured by a fisheye lens are documented in Figure 9. GPT-4V exhibits an impressive\nrobust tolerance for the distinctive fisheye distortion and shows a commendable understanding of the\nindoor parking environment. It reliably identifies parked vehicles and the presence of pedestrians in\nproximity, although there are hallucinations describing a charging station that doesn\u2019t exist. Moreover,\nwhen queried about the potential apparatus used to take the photo, GPT-4V accurately discerns it as\nthe work of a fisheye camera.\nPoint cloud visualization images. Out of curiosity, we captured two screenshots of a 64-line LiDAR\npoint cloud, one from the bird\u2019s-eye view and the other from the front view. Although compressing\nthe point cloud on a two-dimensional image will inevitably lose the three-dimensional geometric\ninformation, several distinctive features can still be discerned and classified. The test is shown in\nFigure 10. Subsequently, we feed these two images into GPT-4V, and to our surprise, it exhibits the\ncapability to recognize certain road and building patterns within them. Since the model has rarely\nseen this type of data before, it inevitably assumed that the circular pattern in the bird\u2019s-eye view\nrepresented a roundabout or a central plaza. Furthermore, when tasked with identifying vehicles,\nthe model is largely successful in estimating the number of vehicles in the scene. We also observe\ncounting errors in the front view, which are caused by the outlines of some vehicles being incomplete\nand difficult to discern. Through this test, the powerful ability of the model to process unconventional\ndata is demonstrated.\nV2X devices photos. V2X, which stands for Vehicle-to-Everything, encompasses a range of tech-\nnologies that enable vehicles to communicate with not only each other but also with infrastructure and\nvarious other entities. V2X cameras play a pivotal role in capturing and processing visual information\nas part of this interconnected ecosystem. In Figure 11, we present the responses generated by GPT-4V\nfor a drone-view photograph and two intersection camera images. GPT-4V exhibits commendable\nperformance in all three instances. In the drone view, GPT-4V accurately identifies the freeway\nin both directions and recognizes the on-ramp situated on the right side of the photo. And in the\nintersection V2X device view, the response identifies a mixed traffic flow containing cars, cyclists,\nand pedestrians in the image, as well as accurate traffic light recognition.\nImages taken in CARLA simulator. In the realm of autonomous driving research and development,\nsimulators like CARLA serve as invaluable tools, providing a controlled and virtual environment\nwhere algorithms can be tested, trained, and refined before their deployment on real-world roads [6].\nWe captured a series of images within CARLA\u2019s map of the Town 10, using the ego car as the primary\nviewpoint. Subsequently, we posed several questions based on these images, and the outcomes are\nshowcased in Figure 12. GPT-4V not only identifies these images as originating from simulation\nsoftware but also demonstrates a high level of awareness regarding the virtual vehicles and pedestrians\nwithin them. Furthermore, in rare instances where simulated pedestrians run red lights, GPT-4V\nappropriately acknowledges this scenario in its responses. However, it\u2019s worth noting that the model\nstill struggles with recognizing traffic lights in simulation, like misidentifying red lights as yellow.\n12\nSec. 2.2 Front view camera images\nPrompt:\nThis is what you see behind the wheel. Please\ncount how many vehicles are in the scenario and\ndescribe each vehicle.\nGPT-4V:\nI see 4 vehicles in the scenario. They are:\n- A yellow car driving down the road.\n- A dark-colored car parked on the right side of\nthe road.\n- A black SUV parked further ahead on the right.\n- A larger black vehicle with an open cargo area,\nparked on the right side near the grassy area.\nPrompt:\nThis is what you see behind the wheel. Please\ncount\nhow many vehicles are in the scenario\nand describe each vehicle.\nGPT-4V:\nI see 6 vehicles in the scenario. They are:\n- A motorcycle on the leftmost lane.\n- A car parked on the left side of the road.\n- Another car parked a bit ahead of the second\ncar on the left side.\n- A car in the middle of the road with its rear\nlights on.\n- A car on the rightmost lane, moving.\n- Another vehicle (possibly a car) on the far\nright, but only its headlights are visible.\nFigure 8: Results describing traffic participants via the front-view camera photo. Green highlights\nthe right answer in understanding, Red highlights the wrong answer in understanding, Yellow\nhighlights that the result is correct but the details are wrong. Check Section 2.2 for detailed discus-\nsions.\n13\nSec. 2.2 Fisheye camera image\nPrompt:\nThis is what you see behind the wheel. Please describe your front car as detailed as possible.\nGPT-4V:\nYou are in an indoor parking garage. Directly ahead, there's a red Mercedes-Benz car parked \nperpendicular to your direction of view. To the right of the Mercedes, there's a red, compact car parked\nfacing the camera. Two men are walking in the garage; one is in a red jacket moving to the left and the\nother is in a blue jacket standing near the Mercedes. There's a designated pedestrian crossing area on\nthe floor with white markings. To the right, there's a column with a green sign indicating a right turn, and\na charging station for electric cars. The ceiling has lights and ventilation ducts. On the far right, there's\nan area marked \"A\" with arrows directing the traffic flow.\nPrompt:\nWhat kind of camera do you think that take this image?\nGPT-4V:\nThe image appears to be taken from a wide-angle or fisheye lens, common in security or dashboard\ncameras. Given the context of being behind the wheel, it's likely a dashboard camera.\nFigure 9: Results describing traffic participants via the photos taken by a fisheye camera. Green\nhighlights the right answer in understanding, Red highlights the wrong answer in understanding.\nCheck Section 2.2 for detailed discussions.\n14\nSec. 2.2 Point clouds images\nPrompt:\nThis is a lidar point cloud visualization image\nfrom a Bird-eye view. Can you describe the\nimage and identify the objects?\nGPT-4V:\nThis image appears to be a LiDAR point cloud\nvisualization showing a bird's-eye view of a\nlandscape.\nThe\nvarious\nshades\nand\ndots\nrepresent different\nheights and\ndensities of\nobjects. There are distinct lines and shapes \nsuggesting roads or pathways, with vehicles \nindicated by rectangular shapes. Open spaces\nthat could be fields or empty plots are visible.\nThe circular pattern in the center might \nrepresent a roundabout or a central plaza with\nradial\nlines\ncoming\nout\nof\nit,\nindicating\nsurrounding streets. Buildings are hinted at by \nblocky shapes and irregular patterns to the sides\nof the open areas and roads.\nPrompt:\nThis is a visualization of a point cloud image of a\nroof-mounted 64-line LiDAR point cloud that took\ntowards the front of the vehicle's travel. Can you\ndescribe this image and identify the objects in it?\nGPT-4V:\nThis image shows a point cloud representation,\nlikely from LiDAR data, capturing a scene from a\nhigh vantage point looking down a series of\nterraced levels or steps leading to a central, flat\narea. The terraces have trees or shrubs on either\nside, and there are objects resembling vehicles\nparked on either side of the terraces. The dotted\ntexture\nthroughout\nthe\nimage\nindicates\nthe\nvarious points that the LiDAR has captured,\ndepicting the surface geometry of the objects and\nlayout of the environment. The lack of continuous\nlines and the discrete nature of the points are\ncharacteristic of point cloud visualizations.\nPrompt:\nCan you count how many cars are there showed\nin the picture?\nGPT-4V:\nFrom this bird's-eye view, there seem to be \naround 15 distinct rectangular shapes that could\nbe interpreted as cars. Please note that this is a \nrough estimate, as the resolution and nature of\nLiDAR imagery do not allow for precise \nidentification and counting.\nPrompt:\nHow many cars are there showed on the Central\nPathway? Please count for me.\nGPT-4V:\nIn this point cloud visualization, there appear to\nbe around four clusters on the sides of the \nterraces that could be interpreted as vehicles. It's\nimportant to note that this is an estimate, as the\nresolution and the nature of point cloud data do\nnot allow for precise identification.\nFigure 10: Results describing traffic participants via visualization image from the point cloud taken\nby lidar. Green highlights the right answer in understanding, Red highlights the wrong answer in\nunderstanding. Check Section 2.2 for detailed discussions.\n15\nSec. 2.2 V2X images\nPrompt:\nThis is a bird-eye view of a traffic scenario. Describe what you see, especially the traffic participants\nand road topology. No need for sub-points.\nGPT-4V:\nThe image provides a bird-eye view of a multilane highway. Multiple vehicles, including cars, trucks,\nand vans, are scattered across the lanes, moving in both directions. Some vehicles are merging onto\nor exiting the highway via on and off-ramps. The highway has a central barrier separating the lanes of\nopposing traffic. On one side of the highway, there's a railway track, while on the other side, a rocky\ncoastline can be observed. Pedestrian pathways and greenery are visible alongside the road.\nJunctions and intersections are marked with road signs, traffic lights, and lane indicators, making\nnavigation easier for the drivers. There are no evident traffic congestions or incidents, and traffic\nappears to be flowing smoothly.\nGPT-4V:\nThe image presents a bird-eye view of an urban\nintersection. Several cars, including a prominent\nyellow taxi, are at various positions on the road.\nCyclists navigate the intersection, with bikes in\ncolors like yellow and white. Pedestrians cross \nthe streets using zebra crosswalks. The road\ndesign\nconsists\nof\nmultiple\nlanes,\nzebra\ncrosswalks, and cyclist pathways marked by\nbicycle\nsymbols.\nA\ncenter\nmedian\nwith\ndecorative lights splits the main traffic directions.\nTraffic lights, currently displaying red signals, are \npositioned around the intersection. Barriers and\nroad\ndividers\nindicate\npossible\nroadwork\nor\nconstruction zones, and a blue structure is\nvisible on one side.\nPrompt:\nThis is a photo from bird-eye view of a traffic\ncamera. Describe what you see, especially the\ntraffic participants, road topology and traffic lights\n(if applicable). No need for sub-points.\nPrompt:\nThis is a photo\nfrom bird-eye view of a traffic\ncamera. Describe what you see, especially the\ntraffic participants, road topology and traffic lights\n(if applicable). No need for sub-points.\nGPT-4V:\nThe image captures a bird-eye view of an urban\nintersection. Cars are aligned, waiting for the \ngreen signal, with a lone motorbike, driven by a\nrider wearing a yellow helmet and carrying a rear \nbox, crossing the intersection. A few pedestrians\nare on the sidewalks, some waiting to cross. The\nroad design includes multiple lanes with zebra\ncrosswalks, cyclist pathways marked by bicycle\nsymbols, and various directional arrows. Traffic\nlights are visible around the intersection, with the \nmajority showing a red signal. Barriers and road\ndividers\nare\nscattered\nthroughout,\nindicating\nroadwork or designated pathways. There's a\ncentral median with trees and a blue-tarped area\non one side of the road.\nFigure 11: Results describing traffic participants via the V2X equipment\u2019s photos. Green highlights\nthe right answer in understanding, Red highlights the wrong answer in understanding. Check\nSection 2.2 for detailed discussions.\n16\nSec. 2.2 Simulated images\nPrompt:\nThis image is from front-view camera of your car.\n1. Do you think this is a simulation or the real\nworld?\n2. Describe what you see, especially the traffic\nparticipants and road topology.\n3. Do you think there's anything abnormal or\nneed to pay attention in this situation?\nGPT-4V:\n1. It appears to be a simulation.\n2. The scene depicts an urban intersection\nsurrounded by tall buildings. The road is\nmulti-laned and marked with yellow boxes, \npossibly indicating a \"no block\" zone. There\nare multiple traffic signals, some showing \nyellow. There are cars in the distance, and a\nfew\npedestrians\non\nthe\nsidewalks\nand\ncrossing the road.\n3. The traffic signals displaying yellow for \ndifferent directions are unusual and might be \na cause for confusion among drivers. Also,\nthe presence of yellow box markings implies\nthe need for drivers to ensure the exit is clear\nbefore entering to avoid gridlock.\nPrompt:\nThis image is from front-view camera of your car.\n1. Do you think this is a simulation or the real\nworld?\n2. Describe what you see, especially the traffic\nparticipants and road topology.\n3. Do you think there's anything abnormal or\nneed to pay attention in this situation?\nGPT-4V:\n1. It appears to be a simulation.\n2. I see an urban intersection with multiple\nbuildings, palm trees, pedestrian crossings,\nand traffic lights. There are several traffic\nparticipants including cars, a fire truck, and \npedestrians. The road topology shows a\nmulti-lane road intersecting with another, with\ntraffic signals and crosswalks.\n3. The traffic lights for our direction show both a\ngreen and a red arrow simultaneously, which\nis abnormal and could be confusing for\ndrivers.\nAdditionally,\nthere's\na\npedestrian\ncrossing the road while the traffic signal for \nvehicles seems green, which could be a\npotential safety concern.\nFigure 12: Results describing traffic participants via the images taken in the CARLA simulator.\nGreen highlights the right answer in understanding, Red highlights the wrong answer in under-\nstanding. Check Section 2.2 for detailed discussions.\n17\n3\nAdvanced Capability of Reasoning\nReasoning is another important trait for proper driving behavior. Given the dynamic and unpredictable\nnature of traffic environments, drivers often encounter a range of unexpected events. It is imperative\nfor proficient drivers to make accurate judgments and decisions, drawing on their experience and\ncommon sense when faced with such unforeseen circumstances. In this section, we have carried out a\nseries of tests to evaluate GPT-4V\u2019s responses to unexpected events and its proficiency in navigating\ndynamic environments.\n3.1\nCorner Cases\nIn the ongoing research for autonomous driving, the common approach to enhancing the system\u2019s\nability to handle unexpected events is through the continuous collection of data. However, given\nthe dynamic, continuous, and random nature of driving environments, the data collected can only\napproximate its boundaries but never fully encapsulate it, that is, unexpected events are inevitable.\nHuman drivers, equipped with common sense, are often able to improvise and navigate safely through\nthese unforeseen circumstances. This highlights the importance of incorporating not just data-driven\nmethods, but also the principles of reasoning and common sense into autonomous driving systems.\nThe visual data employed here is drawn from CODA [10] and the internet.\nIn this section, we have carefully curated a set of perceptual corner cases to assess the model\u2019s\ncapacity for common-sense reasoning. These examples deliberately include objects that fall out of\nthe typical distribution, often posing challenges for conventional perception systems and creating\ndifficulties in decision-making planning. Now, let\u2019s see how GPT-4V fares in addressing these cases.\nOn the left side of Figure 13, GPT-4V can clearly describe the appearance of the vehicles that are not\ncommonly seen, the traffic cone on the ground, and the staff beside the vehicle. After identifying\nthese conditions, the model realizes that the ego car can move slightly to the left, maintain a safe\ndistance from the work area on the right, and drive cautiously. In the right example, GPT-4V adeptly\nidentifies a complex traffic scenario, encompassing an orange construction vehicle, sidewalks, traffic\nlights, and cyclists. When queried about its driving strategy, it articulates an intention to maintain\na safe distance from the construction vehicle and, upon its passage, execute a smooth acceleration\nwhile conscientiously observing pedestrian presence.\nOn the left side of Figure 14, GPT-4V can accurately identify that an airplane has made an emergency\nlanding on the road and authorities are handling the situation nearby. For traditional perception\nalgorithms, it is difficult to recognize without specific training. Under these conditions, the model\nknows that it should slow down and turn on its hazard lights while awaiting clearance to pass the\naircraft before resuming regular driving. In the right example, GPT-4V accurately identifies the\ncement mixer truck and the red traffic light ahead. It discerns the importance of maintaining a safe\ndistance from the truck in the lead until the red light transitions to green, at which point it proceeds\nwith its journey.\nOn the left side of Figure 15, GPT-4V describes a scene where a pedestrian accompanied by two dogs\nis crossing the crosswalk, positioned in the center-right area of the image. Remarkably, the model\naccurately counts the number of dogs present. The model concludes that the vehicle should patiently\nwait for the pedestrian and dogs to clear the way before resuming its journey, though it remains unable\nto ascertain the status of the traffic light. It\u2019s worth mentioning that this isn\u2019t the first occurrence\nof such an incident for GPT-4V. The image on the right depicts a nocturnal traffic scenario, which\nis well recognized by GPT-4V. In this instance, the model astutely discerned the illuminated brake\nlights of the vehicle ahead and noted the presence of pedestrians and cyclists, patiently waiting to\ncross the road. It intelligently deduced that it should remain stationary until the traffic signal turns\ngreen, exercising due caution when resuming its journey.\n18\nSec. 3.1 Corner cases\nPrompt:\nYou're the mature driver behind the wheel and\nthis image is what you see now. Describe what\nyou see and what you plan to drive next.\nGPT-4V:\nAs\nI\nobserve\nthe\nscene\nthrough the\nrain-\nspeckled windshield, I see a blue truck to my left\ncarrying a load under a white cloth. Directly\nahead, there's an area cordoned off with traffic\ncones and barricades indicating some roadwork \nor\nmaintenance\nactivity.\nNear\nthis\nsection,\nthere's\na large orange industrial tank-like \nequipment with workers wearing safety gear\nactively\nengaged\nin\ntheir\ntasks.\nThe\nroad\ncontinues forward with buildings on both sides,\nand a few vehicles and pedestrians can be\nspotted in the distance. Overhead, trees line the\nroad, providing a canopy.\nGiven the current conditions, I'd slow down due\nto the rain and potential slippery roads. I'd \napproach the work area with caution, ensuring I \nprovide ample space for the workers. If the path\nis clear and there's no oncoming traffic, I'd move \nslightly to the left side to maintain a safe \ndistance from the ongoing work. As I pass this\narea, I'd remain vigilant of the blue truck to my\nleft, making sure it doesn't obstruct my path.\nAfter navigating this section, I'd continue driving,\nkeeping in mind the importance of maintaining a\nsafe speed given the rain and surrounding\nactivities.\nPrompt:\nYou're the mature driver behind the wheel and\nthis image is what you see now. Describe what\nyou see and what you plan to drive next\nGPT-4V:\nFrom\nbehind\nthe\nwheel,\nI\nsee\na large \nconstruction truck in the lane ahead of me. It's a\nprominent\norange\nand\nyellow\nvehicle\nwith\nnoticeable safety markings, and it seems to be\ncarrying construction equipment. Further ahead,\nthere's a\ncrossroads with\ntraffic\nlights,\nand\nseveral vehicles are either stopped or in motion.\nOn my right, there's a crane in the distance,\nindicating ongoing construction work in the area.\nIn addition to cars, there are scooters and\ncyclists crossing the junction, suggesting that the\narea\nis\nbustling\nwith\nmixed\nvehicular\nand\npedestrian traffic.\nWith safety in mind, I plan to maintain a safe \ndistance from the large construction vehicle and\nremain alert to the various road users around me.\nAs the traffic light seems to be in favor of our\ndirection, I'll smoothly accelerate once the truck \nahead moves,\nensuring\nI\nwatch\nfor\nany\npedestrians or smaller vehicles that might dart\nout unexpectedly.\nFigure 13: Illustration of GPT-4V\u2019s ability to make correct decisions in corner cases.\nGreen\nhighlights the right answer in understanding. Check Section 3.1 for detailed discussions.\n19\nSec. 3.1 Corner cases\nPrompt:\nYou're the mature driver behind the wheel and\nthis image is what you see now. Describe what\nyou see and what you plan to drive next.\nGPT-4V:\nDriving on a clear day with a blue sky scattered\nwith\nclouds,\nI\nsuddenly\ncome\nacross\nan\nunexpected\nsight\non\nthe\nhighway:\na black \nairplane has made an emergency landing. The\nwings of the plane stretch across the lanes, and\ntraffic has come to a halt. Nearby, vehicles, \nincluding a truck and a police car, are parked on \nthe side of the road. Orange traffic cones have\nbeen placed around the area, indicating that the\nauthorities are managing the situation.\nAs a mature driver, I recognize the importance\nof safety and patience in this situation. I would\nreduce my speed, put on my hazard lights, and\ncarefully navigate around the incident, giving \nplenty\nof\nspace\nto\nthe\nairplane\nand\nany\nemergency personnel on the scene. Once past \nthe airplane, I would continue on my journey,\nstaying alert and vigilant, grateful for the training\nand experience that has taught me to handle\nunexpected situations on the road.\nPrompt:\nYou're the mature driver behind the wheel and\nthis image is what you see now. Describe what\nyou see and what you plan to drive next.\nGPT-4V:\nDriving through the city at night, I'm closely \nfollowing a large cement mixer truck, identifiable\nby its signature rotating drum and the chutes\nattached to the back. The truck's taillights and\nreflective markers offer a clear view despite the\ndim setting. To the left, a traffic light displays a\ngreen arrow, signaling it's safe to proceed in that\ndirection, while the straight path shows a red \nlight. The surrounding urban environment is\nilluminated [\u2026]\nGiven the current scene, I would cautiously\nmaintain a safe distance from the cement mixer \ntruck, staying alert for any sudden stops or turns\nit might make. Seeing the green arrow to the left,\nI'd consider making a left turn if that's the\ndirection I need to go, but if my destination \nrequires me to continue straight, I'd patiently wait\nfor the light to turn green, ensuring a safe and\nsmooth journey ahead.\nFigure 14: Illustration of GPT-4V\u2019s ability to make correct decisions in corner cases.\nGreen\nhighlights the right answer in understanding. Check Section 3.1 for detailed discussions.\n20\nSec. 3.1 Corner cases\nPrompt:\nYou're the mature driver behind the wheel and\nthis image is what you see now. Describe what\nyou see and what you plan to drive next.\nGPT-4V:\nI see a nighttime city scene with well-lit streets.\nThere are buildings on both sides of the road\nwith illuminated signage. One of the signs reads\n\"BIG\" in bright white letters. On the road, there\nare pedestrian crosswalk lines. A person is \nwalking across the crosswalk, accompanied by \ntwo dogs. Traffic lights are visible, but their \ncurrent status isn't clear from the image.\nGiven the presence of the pedestrian and the\ndogs on the crosswalk, I would come to a\ncomplete stop if I haven't already. I would\nensure the safety of the pedestrian and the dogs\nby waiting for them to completely cross the road \nbefore proceeding. Safety always comes first,\nespecially in scenarios involving pedestrians at\nnight.\nPrompt:\nYou're the mature driver behind the wheel and\nthis image is what you see now. Describe what\nyou see and what you plan to drive next.\nGPT-4V:\nI see a nighttime city street scene. The view is\nfrom behind a large vehicle with its brake lights \nilluminated,\nsuggesting\nthat\nit\nis\ncurrently\nstopped. To the left of this vehicle is a white\nsedan.\nThe\nsurroundings\nhave\nvarious\nilluminated signs, street lights, and building lights.\nOn the right side of the image, there is a\npedestrian crosswalk with a couple of people on \nbicycles waiting to cross. The traffic lights, which\nare visible on the right, are red, indicating that\nvehicles should be stopped.\nGiven the situation, as a mature driver, I would\nremain stationary with my foot on the brake\npedal, ensuring that the car is at a complete stop.\nI would continue to monitor the traffic lights and\nwait for them to turn green before proceeding. I\nwould also keep an eye on the cyclists and\npedestrians to ensure their safety. Once the light\nturns green and it's safe to move forward, I \nwould proceed with caution, being mindful of\nother vehicles, cyclists, and any pedestrians in\nthe vicinity. Safety is paramount, especially at\nintersections and in nighttime conditions.\nPrompt:\nCan you tell me which part is the pedestrian on\nthe picture? e.g. left-bottom, upper-right etc.\nGPT-4V:\nCertainly.\nThe\npedestrian\nin\nthe\npicture\nis\nlocated towards the center-right area of the\nimage, walking across the crosswalk.\nFigure 15: Illustration of GPT-4V\u2019s ability to make correct decisions in corner cases.\nGreen\nhighlights the right answer in understanding, Yellow highlights the incompetence in performing the\ntask. Check Section 3.1 for detailed discussions.\n21\n3.2\nMulti-view Images\nBy utilizing multi-view cameras, GPT-4V captures a comprehensive view of the driving environment.\nPrecisely interpreting the spatial relationships between these cameras and the overlapping areas\nwithin the images is crucial for the model to effectively harness the potential of the multi-view camera\nsystem. In this section, we evaluate GPT-4V\u2019s competence in handling multi-view images. All data\nin this section comes from the nuScenes [3] dataset.\nIn Figure 16, we select a set of surrounding images and input them to the model in the correct\nsequence. The model adeptly recognizes various elements within the scene, such as buildings,\nvehicles, barriers, and parking. It can even deduce from the overlapping information that there are\ntwo cars in the scene, with one white SUV positioned to the front and a truck with a trailer to the right.\nWhile the model\u2019s performance is generally impressive, an insignificant error is the misidentification\nof the pedestrian crossing.\nIn another experiment shown in Figure 17, we similarly employ a set of combined surrounding\nimages for test. While the model managed to provide a mostly accurate description of the scene,\nit also exhibited several recognition errors, particularly in regard to the number and shapes of the\nvehicles present. Notably, the model produces some confusing illusions, such as thinking there is a\nleft-turn sign on the picture. We hypothesize that these issues may arise from the model\u2019s limited\nspatial reasoning capabilities.\nFinally, in Figure 18, we present the correct front view and try to let GPT-4V identify and sort the\nout-of-order surrounding images. Although the model performs a lot of analysis and reasoning that\nseems to make sense, it still ends up outputting all wrong answers. It becomes evident that the model\nencounters challenges in establishing connections between adjacent images. We acknowledge the\nintricacies associated with this particular task.\n22\nSec. 3.2 Multi-view images\nPrompt:\nYou are a mature driver behind the wheel. These five images were captured and put together by the\nsurround view camera in your vehicle. The FOV of each surround view camera is approximately 120\u00b0 .\nAnd the middle image is the directly front.\nPlease describe in detail the scenario you are in, make a special distinction between what's in front of\nyou and what's on either side of you!\nGPT-4V:\nYou're situated in a semi-industrial or commercial area. In front of you is a broad street lined by multi-\nstoried buildings, probably commercial or office spaces. Directly ahead, there's a white SUV parked\nbeside a white-lined pedestrian crossing. The road has multiple lanes and is marked with clear road\nmarkings and arrows. Traffic cones are visible, indicating potential roadwork or lane closures.\nTo your left, there are large bay doors, possibly loading docks, with orange barriers in front. There are\nno vehicles or people in this immediate view, but there are parking spaces marked.\nTo your right, there's a large truck with a trailer, indicating that this area is used for deliveries or freight.\nThe truck is parked or possibly waiting beside another set of bay doors or loading docks.\nOn either side of you, the architecture and design of the buildings appear consistent, suggesting that\nyou are in a coordinated commercial complex or business park. The presence of the large truck and\nthe bay doors confirm the commercial nature of this area.\nPrompt:\nPlease count and describe how many vehicles are in the current scene.\nGPT-4V:\nThere are 2 vehicles in the current scene: A white SUV parked on the street, seen in the middle\nimage. A large truck with a trailer on the right side, which appears to be parked or waiting next to the\nbay doors.\nFigure 16: Illustration of GPT-4V\u2019s ability to infer the relationship between multi-view images.\nGreen highlights the right answer in understanding. Check Section 3.2 for detailed discussions.\n23\nSec. 3.2 Multi-view images\nPrompt:\nYou are a mature driver behind the wheel. These six images were captured and put together by the\nsurround view camera in your vehicle. The FOV of each surround view camera is approximately 120\u00b0 .\nAnd the middle two images are the directly front.\nPlease describe in detail the scenario you are in, make a special distinction between what\u2018s in front of\nyou and what\u2019s on either side of you.\nGPT-4V:\nYou're in a mostly empty urban area with roads bordered by structures and some greenery.\nDirectly in front: There's a relatively wide road with a few lane markings. Directly ahead, there's a\nlarge modern building with a unique architectural design featuring angled glass panels. To the left of\nthe road, there are barriers indicating construction or some restricted area. There's a parked car on the\nright of these barriers. On the far left of the road, there's another street branching out.\nTo the left: The beginning of a pedestrian crosswalk is visible, leading towards a traffic island with\nsome trees. On the far side, there's a street that turns to the left with another building in the\nbackground.\nTo the right: There's a continuation of the pedestrian crosswalk. Further right, a road intersects the\none you're on, with a sign indicating a left turn. There are barriers similar to the ones directly in front of\nyou. On the far right, you can see the corner of another building.\nAt the back: It seems like you've just passed a junction. There are more road barriers and markings.\nOn the left side of the road, there's a structure that appears to be a bus stop or a shelter. Further back,\nthere's another large building and a traffic light. On the right side, you can see a continuation of the\nroad you're on, flanked by more buildings.\nPrompt:\nPlease count and describe how many vehicles are in the current scene.\nGPT-4V:\nThere are two vehicles in the current scene:\nA red car parked on the right side, near the construction or restricted area barriers in front of you.\nAnother vehicle (due to the angle, specifics are hard to discern) is seen on the far right image, turning\nonto the road that intersects the one you're on.\nFigure 17: Illustration of GPT-4V\u2019s ability to infer the relationship between multi-view images.\nGreen highlights the right answer in understanding, Red highlights the wrong answer in under-\nstanding, Yellow highlights the incompetence in performing the task. Check Section 3.2 for detailed\ndiscussions.\n24\nSec. 3.2 Multi-view images\nPrompt:\nYou are a mature driver behind the wheel. These six images were captured and put together by the\nsurround view camera in your vehicle. The FOV of each surround view camera is approximately 120\u00b0 .\nNow you know this circular view that should consist of 'CAM_FRONT_LEFT', 'CAM_FRONT',\n'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT ' You only know that\n'CAM_FRONT' is image A.\nPlease analyze which image corresponds to each of the other five views, and explain why.\nGPT-4V:\nAnalyzing the images in reference to 'CAM_FRONT' which is image A:\n'CAM_FRONT_LEFT': This would be the image to the immediate left of 'CAM_FRONT'. The scene\nshould show a gradual transition from the front view to the side. This corresponds to image D, as it\nshowcases a continuation of the road and barriers on the left side.\n'CAM_FRONT_RIGHT': This would be to the immediate right of 'CAM_FRONT'. The scene should\nshow a transition from the front view to the side view on the right. Image B fits this description, as it\ndisplays a continuation of the road, the vehicle's side, and barriers on the right.\n'CAM_BACK_LEFT': Following the circular view, this would be after 'CAM_FRONT_LEFT'. It would\nshow the scene behind the vehicle on the left. Image C is the correct match, displaying the rear view\non the left side, with an intersection and a round building in view.\n'CAM_BACK': This would be opposite 'CAM_FRONT' showing the full rear view. Image E is the match.\nIt shows a scene directly behind the vehicle, with the back of other cars and barriers.\n'CAM_BACK_RIGHT': This would be after 'CAM_FRONT_RIGHT' and before 'CAM_BACK'. It shows\nthe scene behind the vehicle on the right side. Image F fits this description, with the road turning right\nand trees lining the side.\nSo, in order:\n'CAM_FRONT_LEFT': Image D; 'CAM_FRONT_RIGHT': Image B; 'CAM_BACK_LEFT': Image C;\n'CAM_BACK': Image E; 'CAM_BACK_RIGHT': Image F.\nFigure 18: Illustration of GPT-4V\u2019s ability to infer the relationship between multi-view images. Red\nhighlights the wrong answer in understanding. Check Section 3.2 for detailed discussions.\n25\n3.3\nTemporal Sequences\nIn this section, we assess the capability of GPT-4V in understanding temporal images. Our method-\nology involves the utilization of multiple sequences from first-person driving videos. From each\nvideo segment, we extract four keyframes, label them with sequential numbers, and combine them\ninto a single image for input. Subsequently, we task GPT-4V with describing what events occurred\nduring this time period, as well as the actions taken by ego vehicle and the reasons behind them. The\nexamples are sourced from nuScenes [3], D2-city and Carla [6] simulation.\nFigure 19 shows a video captured in CARLA\u2019s map of Town 10 where the GPT-4V clearly explains\nthe action of the ego car stopping at a crosswalk because of a pedestrian crossing the road, just before\nthe traffic signal turning red.\nFigure 20 showcases a video segment extracted from the NuScene dataset [3]. During the process\nof capturing keyframes, we assigned the labels \u201c1\u201d and \u201c2\u201d to the leading SUV and a pedestrian,\nrespectively. GPT-4V not only responds accurately to inquiries regarding the objects represented by\nthese labels but also offers a comprehensive explanation of the interaction observed between the front\nSUV and the pedestrian. This interaction entails the pedestrian crossing the street while the white\nSUV yields the right-of-way.\nThe video sequence depicted in Figure 22 exhibits a noticeable camera flare caused by rain. Never-\ntheless, GPT-4V adeptly identifies the intersection signals and the front taillights. It also infers the\nsemantic information about the start of the front car after the red light turns green.\nHowever, it\u2019s important to note that GPT-4V is not always able to fully analyze temporal driving\nscenarios accurately. As illustrated in Figure 21, this video segment captures the vehicle making a\nlane change to overtake the scooter ahead. Regrettably, GPT-4V incorrectly interprets the scooter\u2019s\nactions as if it were crossing lanes in front of the vehicle and misconstrues the vehicle\u2019s behavior as\ndecelerating to yield to the scooter. This underscores the limitations of GPT-4V in spatial reasoning\nwithin temporal video contexts. Also in Figure 22, GPT-4V once again misidentifies a green light as\na red light.\n26\nSec. 3.3 Temporal sequences\nPrompt:\nThis is a time series of four pictures taken from a driving recorder. The chronological order is marked\non the corresponding subfigure. As the driver of the car, please describe in detail what happened in\nthese four seconds, what actions you performed, and why you performed the actions you did.\nGPT-4V:\nI approached an intersection with a green traffic light ahead.\nAs I neared the crosswalk, I noticed\npedestrians beginning to cross the street. I maintained my position, ensuring not to obstruct the\ncrosswalk, and waited for the pedestrians to clear. The light turned yellow, then red, indicating for me\nto remain stopped. I obeyed the traffic signals and waited for the appropriate time to proceed.\nFigure 19: Illustration of GPT-4V\u2019s ability to understand temporal sequential images.\nGreen\nhighlights the right answer in understanding. Check Section 3.3 for detailed discussions.\n27\nSec. 3.3 Temporal sequences\nPrompt:\nThis is a time series of four pictures taken from your driving recorder. The chronological order is marked on the\ncorresponding subfigure. First please identify what objects are marked as 1 and 2.\nGPT-4V:\nObject 1 is a car and object 2 is a pedestrian.\nPrompt:\nIs object 1 the car you are driving?\nGPT-4V:\nNo, object 1 is not the car I am driving; it is a car in front of the camera's viewpoint.\nPrompt:\nIs object 1 the car you are driving?\nGPT-4V:\n- In the first second (Time: 0 s), a car (object 1) is directly in front of the viewpoint, indicating that you're following\nit. Simultaneously, a pedestrian (object 2) is on the left side of the road, likely preparing to cross.\n- By the second frame (Time: 1 s), the car (object 1) has moved slightly forward, and the pedestrian (object 2) is\nno longer visible from this angle. The pedestrian might have crossed or stayed on the sidewalk.\n- In the third frame (Time: 2 s), the car (object 1) continues its trajectory, and the pedestrian (object 2) has\nappeared again on the right side of the road, having crossed it.\n- By the fourth frame (Time: 3 s), the car (object 1) is farther ahead and the area seems clearer with no\npedestrian in sight.\n- Your actions as a driver:\n1. You followed the car (object 1) while maintaining a safe distance.\n2. As a pedestrian (object 2) was nearby and possibly crossing the street, you likely remained cautious\nand prepared to brake if necessary.\n3. Once the pedestrian had safely crossed, you continued to follow the car in front without any obstruction.\n- Why you performed these actions:\n1. Maintaining a safe distance ensures you have ample time to react to unexpected events.\n2. Pedestrians have the right of way, so being cautious around them is essential for their safety and yours.\n3. With the path clear, continuing at a steady pace ensures the smooth flow of traffic.\nTime: 0 s\nTime: 1 s\nTime: 2 s\nTime: 3 s\n1\n1\n1\n1\n2\n2\nFigure 20: Illustration of GPT-4V\u2019s ability to understand temporal sequential images.\nGreen\nhighlights the right answer in understanding. Check Section 3.3 for detailed discussions.\n28\nSec. 3.3 Temporal sequences\nPrompt:\nThis is a time series of four pictures taken from a driving recorder. The chronological order is marked\non the corresponding subfigure. As the driver of the car, please describe what happened in these four\nseconds, what actions you performed, and why you performed the actions you did.\nGPT-4V:\nIn the four-second time series:\nAt 0 seconds, I was driving on a straight road with a car in front of me and saw a scooter rider\napproaching the divider.\nBy 1 second, I noticed the scooter rider making a move to cross the road, so I started to slow down\nand steer slightly to the left to create space and avoid any potential hazard.\nAt 2 seconds, the scooter rider was now crossing my lane, and I continued to maintain a safe distance\nfrom the car in front and the scooter.\nBy 3 seconds, the scooter rider had crossed to the other side, and I realigned my car to the center of\nthe lane.\nI slowed down and steered to avoid a potential collision with the scooter rider, ensuring both our safety\nand the safety of other road users.\nTime: 0 s\nTime: 1 s\nTime: 2 s\nTime: 3 s\nFigure 21: Illustration of GPT-4V\u2019s ability to understand temporal sequential images.\nGreen\nhighlights the right answer in understanding, Red highlights the wrong answer in understanding.\nCheck Section 3.3 for detailed discussions.\n29\nSec. 3.3 Temporal sequences\nPrompt:\nThis is a time series of four pictures taken from a driving recorder. The chronological order is marked\non the corresponding subfigure. As the driver of the car, please describe what happened in these four\nseconds, what actions you performed, and why you performed the actions you did.\nGPT-4V:\nDuring these four seconds, I was driving in rainy conditions.\nAt Time: 0 s, I was approaching a red traffic light and cars ahead were stopping. I began to slow down\nand maintain a safe distance from the car in front.\nBy Time: 1 s, I continued braking gradually as the car in front was still stationary. The traffic light was\nstill red.\nAt Time: 2 s, the traffic light turned green, but I remained stationary, ensuring the cars in front started\nmoving first.\nBy Time: 3 s, cars began to move, and I prepared to accelerate. I performed these actions to ensure\nsafety and follow traffic rules.\nTime: 0 s\nTime: 1 s\nTime: 2 s\nTime: 3 s\nFigure 22: Illustration of GPT-4V\u2019s ability to understand temporal sequential images.\nGreen\nhighlights the right answer in understanding, Red highlights the wrong answer in understanding.\nCheck Section 3.3 for detailed discussions.\n30\n3.4\nVisual-Map Navigation\nIn practical driving scenarios, drivers often utilize auxiliary information from external devices to\nenhance their decision-making. For instance, a mapping application can provide detailed information\nabout road geometry and route guidance, enabling drivers to make more informed and rational driving\ndecisions. In this section, we equip GPT-4V with a front-view camera image and corresponding\nnavigation information from the mapping software. This setup allows GPT-4V to describe the scene\nand make informed decisions, mirroring the way a human driver would in a similar situation.\nAs depicted in Figure 23, GPT-4V accurately localizes its position using the front-view camera and\nmap APP information, subsequently executing the correct left-turn maneuver. Utilizing the front view\ncamera, it made reasonable assessments of the road conditions and, in conjunction with the speed\ninformation provided by the mapping software, offered appropriate driving suggestions.\nAs illustrated in Figure 24, GPT-4V is able to accurately localize itself even in a more complex\nscenario. However, in this particular situation, it incorrectly decided to make a left turn. Despite this,\nGPT-4V demonstrated proficiency in recognizing information about parked cars and stores on the\nroadside based on the front view camera. It also correctly obtained speed and distance information\nfrom the mapping software.\nSec. 3.4 Visual-map navigation\nPrompt:\nPlease combine the navigation on map APP and the front-view camera image of the car. Write a\nparagraph to instruct how to drive in the current situation to the driver behind the wheel.\nGPT-4V:\nIn your current position, you're approaching an intersection on Longyao Rd. You'll need to make a left\nturn in approximately 80 meters. As you can see from the front-view camera image of your car, the \nroad ahead is clear with minimal traffic. Ensure you signal in advance and check both sides for\noncoming traffic or pedestrians before making the turn. Once you've made the left turn onto Longyao\nRd, continue straight and maintain a speed close to 52 km/h, but always adjust to the current traffic\nconditions and any speed limits posted. Your destination is roughly 4.4 km away and is estimated to \ntake you another 16 minutes to reach. Safe driving!\nFigure 23: Illustration of GPT-4V\u2019s ability to get information combines the navigation on map APP\nand the front-view camera image.\nGreen highlights the right answer in understanding. Check\nSection 3.4 for detailed discussions.\n31\nSec. 3.4 Visual-map navigation\nPrompt:\nPlease combine the navigation on map APP and the front-view camera image of the car. Write a\nparagraph to instruct how to drive in the current situation to the driver behind the wheel.\nGPT-4V:\nIn your current location, you're on Avenue Kl\u00e9ber, approaching the intersection with Avenue Paul \nDoumer. In about 70 meters, make a slight left turn onto Avenue Paul Doumer. As you drive, keep an \neye out for the prominent building on your right with the \"CAFE KL\u00c9BER\" signage. Be mindful of the\nparked vehicles and pedestrians on both sides of the road. Additionally, the street view shows a few\ntaxis and a scooter parked on the right, so exercise caution while maneuvering. Maintain a speed of \naround 30 km/h as indicated, although you're currently going at 49 km/h. You are expected to reach\nyour destination in approximately 1 minute, covering a distance of 431 meters. Safe driving!\nFigure 24: Illustration of GPT-4V\u2019s ability to get information combines the navigation on map APP\nand the front-view camera image.\nGreen highlights the right answer in understanding. Check\nSection 3.4 for detailed discussions.\n32\n4\nAct as A Driver\nThe ultimate goal of autonomous driving algorithms is to replicate the decision-making abilities\nof human drivers. Achieving this goal necessitates precise identification, spatial awareness, and\nan in-depth understanding of spatiotemporal relationships among various traffic elements. In this\nsection, we assess GPT-4V\u2019s full potential in autonomous driving by testing its decision-making\nprowess across five distinct real-world driving scenarios. These scenarios encompass varying traffic\nconditions, different times of the day, and multiple driving tasks. During the assessment, ego-vehicle\nspeed and other relevant information are provided, and GPT-4V is desired to produce the observation\nand driving actions. Through these carefully designed evaluations, our goal is to push the boundaries\nof GPT-4V\u2019s capabilities in real-world driving scenarios, shedding light on its potential as a driving\nforce in the future of autonomous transportation.\n4.1\nDriving in Parking Lot\nIn this section, we test the driving decision-making ability of GPT-4V in an enclosed area. The\nselected scenario is turning right to exit a parking lot, which requires passing through a security check.\nAs shown in Figure 25, in the first frame, GPT-4V accurately identifies key elements affecting driving,\nsuch as pedestrians and vehicle lights. However, GPT-4V has ambiguity regarding the status of\npedestrians and distant vehicles. As a result, it provides conservative driving decisions by maintaining\nlow speed and being prepared to stop. In the second frame, GPT-4V detects that pedestrians have\nalready left but mistakenly mentions the information of zebra crossings. It still follows a cautious\nright-turn driving strategy. In the third frame, GPT-4V accurately recognizes elements such as\ngated checkpoints, guard booths, and fencing, inferring that the vehicle is approaching the exit and\npreparing to stop for a security check. In the fourth frame, GPT-4V correctly identifies that the\nsecurity checkpoint is now fully open so we can safely exit the parking lot. Additionally, GPT-4V\nalso locates pedestrians near the exit area and advises waiting for them to pass safely before slowly\nproceeding out.\nFrom this example, GPT-4V can accurately identify key elements within enclosed areas (such\nas parking lots), including gated checkpoints, guard booths, and fencing. Moreover, GPT-4V\nunderstands driving procedures for leaving parking lots, which requires waiting for security checks\nand paying attention to pedestrians and vehicles. However, some misjudgments may still occur, such\nas mentioning zebra crossings erroneously.\n4.2\nTurning at Traffic Intersection\nIn this section, we assess GPT-4V the turning ability at traffic intersections. As depicted in Figure 26,\nthe selected scenario is a crossroad with heavy traffic. In the first frame, GPT-4V observes that the\ntraffic light is green and infers the driving action as continuing to turn left. In the second frame, due\nto the distance and limited perception fields, GPT-4V regards that the traffic light is invisible, but it\nobserves that front vehicles were braking based on their taillights. Therefore, its driving strategy was\nto maintain the current position. In the third frame, GPT-4V mistakes the status of the traffic light,\nand deems that turning is not allowed. In the fourth frame, GPT-4V still mistakes the traffic light\nstatus. The final decision is to make a cautious left turn while ensuring safety by avoiding collisions\nwith other vehicles and pedestrians.\nThis example shows that when making turns at intersections, GPT-4V pays attention to various\ninformation such as traffic lights, and taillights of other vehicles. However, GPT-4V\u2019s ability to\nidentify states of small objects at long distances (such as distant traffic lights) is poor which may\naffect its behavioral decisions at intersections.\n4.3\nTurning at Highway Ramp\nIn this section, we test GPT-4V the capability to drive in highway areas. As illustrated in Figure 27,\nwe select a challenging scenario where the vehicle needs to perform a highway ramp turnaround at\nnight. In the first frame, GPT-4V accurately identifies arrow signs and a dividing lane line, and infers\nfrom the red taillights of the preceding vehicle that it is slowing down. Therefore, the ego-vehicle\nshould decelerate and follow the lane line. In the second frame, although GPT-4V mistakes the\nnumber of preceding vehicles, it precisely located the lane line and road sign, indicating a left turn\n33\nSec. 4.1 Right-turn at the Exit\nNavigation Prompt:\nYou will encounter a parking lot exit in this \nscenario and you will need to turn right to exit the \nparking lot. You are moving at a relatively low \nspeed. Please answer me using the prescribed \nformat.\nGPT-4V:\nObservation: The barrier gate at the security \ncheckpoint is now fully open. Two cyclists are \nvisible, one passing in front of the vehicle and \nanother one approaching. The security guard is \nstill at his post on the right.\nAction: The vehicle is either stopped or moving \nvery slowly, waiting for the cyclists to pass \nbefore proceeding through the security \ncheckpoint. The driver should continue to wait \nuntil the cyclist in front has passed and then \ndrive through the checkpoint slowly and \ncautiously, being mindful of any other \npedestrians or cyclists.\nGPT-4V:\nObservation: The image shows that the \npedestrian has cleared the zebra crossing. \nThere are no other pedestrians in immediate \nsight who are about to cross. A white car is on \nthe opposite side of the road, indicating a \npotential for crossing traffic. [...] \nAction: Maintaining a low speed while \napproaching the exit of the parking lot. Proceed \nwith caution, turn right.\nGPT-4V:\nObservation: The scene includes a parking lot \nexit with a pedestrian crossing in front of the car, \nother parked cars on the left, and a vehicle on the \nopposite side of the road that appears to be \nmoving. [...] \nAction: The vehicle is moving at a low speed, \npreparing to exit the parking lot. Continue at a low \nspeed, prepare to stop.\nGPT-4V:\nObservation: The image appears to be taken \nfrom the inside of a vehicle approaching a gated \ncheckpoint, with pedestrians and another \nvehicle in view. The presence of a guard booth \nand fencing indicates controlled access to the \narea.\nAction: Prepare to stop if required for security \nchecks or to wait for the gate to open. Comply \nwith any directions given by security personnel.\nFrame 1\nFrame 2\nFrame 3\nFrame 4\nFigure 25: Illustration of GPT-4V\u2019s ability to drive in parking lot. Green highlights the right answer\nin understanding.\nRed highlights the wrong answer in understanding.\nYellow highlights the\nincompetence in performing the task. Check Section 4.1 for detailed discussions.\n34\nSec. 4.2 Left-turn at intersection\nNavigation Prompt:\nThere is an intersection with traffic lights ahead \nand you need to turn left at this intersection and \ndrive into the proper lane. You are moving at a \nrelatively low speed. \nGPT-4V:\nObservation: The image shows an intersection \nduring twilight, with the driver's perspective \nsuggesting a position in the left turn lane. The \ntraffic light visible is green for vehicles going \nstraight but does not have a visible arrow \nindicating permission for a left turn. There are \nvehicles and a motorbike in motion across the \nintersection, indicating the green light is for \nthrough traffic, not left turns.\nAction: [...] after ensuring there are no \noncoming vehicles or pedestrians in the \ncrosswalk, the driver should proceed with the \nleft turn cautiously.\nGPT-4V:\nObservation: [...] The traffic is dense, and \nmultiple vehicles are visible ahead, with brake \nlights illuminated, indicating either a slowdown \nor a complete stop. The road has at least three \nlanes with visible lane markings. [...] the status \nof the traffic lights is not visible in this frame. \nAction: The car should maintain its current \nposition and continue to observe the vehicles \nahead.\nGPT-4V:\nObservation: The traffic lights are now visible and \nthey show green, which indicates it is permissible \nto proceed through the intersection if it's safe to \ndo so. The white car ahead seems to be moving \nforward, as indicated by the lack of brake lights. \nVehicles in the other lanes appear to be in motion \nas well. [...] \nAction: Turn left if the lane is confirmed for left \nturn and it's safe.\nGPT-4V:\nObservation: [...] Vehicles are visible in several \nlanes, with a bus to the right side of the frame. \n[...] A traffic light is visible showing a red light for \nvehicles intending to turn left and a green light \nfor those going straight or right, which includes \nthe lane that the observer's vehicle appears to \nbe in.\nAction: [...] continue to drive straight through \nthe intersection, assuming the lane corresponds \nto the green traffic light indication.\nFrame 1\nFrame 2\nFrame 3\nFrame 4\nFigure 26: Illustration of GPT-4V\u2019s ability to turn at traffic intersection. Green highlights the right\nanswer in understanding. Red highlights the wrong answer in understanding. Check Section 4.2 for\ndetailed discussions.\n35\nahead. As a result, GPT-4V suggests applying the brakes lightly and signaling left to inform other\ndrivers. In the third frame, due to limited visibility at night, GPT-4V only locates yellow lane dividers.\nThus, it advises slow driving within the lane lines using these dividers as references. In the fourth\nframe, GPT-4V accurately determines that the ego-vehicle has entered the main highway road and\nobserved potential merging vehicles on its right side. Consequently, it decides to adjust speed for\nhighway driving while occasionally activating high beams within legal limits to expand the nighttime\nvisibility range.\nFrom this example, we can see that when driving in highway areas, GPT-4V follows road signs and\nassists in decision-making based on the status of surrounding vehicles. However, it has limitations in\nobject recognition and positioning during nighttime.\n4.4\nRoad Merging\nIn this section, we evaluate the lane merging capability of GPT-4V. As shown in Figure 28, the\nselected scenario is exiting the main road at night and merging onto a ramp. In the first frame,\nGPT-4V accurately identifies the lane markings and determines that the current lane is ending or\nmerging. Therefore, it decides to decelerate and prepare to merge into the right-turn lane. During this\nprocess, it mistakenly recognizes a nearby hospital sign and cautiously considers paying attention to\npedestrians and emergency vehicles in the vicinity. In the second frame, GPT-4V correctly identifies\nthe merging point and advises smoothly steering into the lane. In the third frame, based on changes\nin lanes, GPT-4V predicts that merging is about to end while reminding us to be cautious of other\nvehicles cutting in. In the fourth frame, GPT-4V determines that it has successfully merged onto the\nroad. However, it incorrectly detects a solid white line, and mistakenly believes that a motorcycle is\non the same lane. The final decision given was to pay attention to motorcycles on the main road and\nadjust speed or change lanes if necessary.\nFrom this example, it is observed that GPT-4V can assess current merge progress by observing\nchanges in lanes and providing reasonable driving suggestions. However, there is still an increased\nprobability of misjudging road signs and lanes during nighttime. Overall, GPT-4V tends to adopt a\nconservative approach when it comes to lane merging.\n4.5\nU-Turning at Traffic Intersection\nIn this section, we test the U-turn capability of GP-4V. As depicted in Figure 29, we select a scenario\nwhere the U-turn is performed at an intersection with heavy nighttime traffic. In the first frame,\nGPT-4V accurately identifies other vehicles ahead and reminds ego car to maintain distance, but it\nomits the distant traffic light. In the second frame, GPT-4V still fails to locate the traffic light but\ninfers from surrounding vehicle behavior that there might be a signal controlling the intersection. It\nsuggests slowing down to prepare for entering the U-turn. In the third frame, GPT-4V disregards\ntemporal and spatial context, and mistakes traffic lights on a side road as control signals for its current\nlane. Therefore, it decides to remain stopped. In the fourth frame, GPT-4V still mistakes the traffic\nlights, resulting in maintaining a stationary position as its strategy.\nFrom this example, we can see that when encountering significant changes in road structure like\nU-turns, GPT-4V tends to overlook temporal and spatial contextual relationships. Nevertheless, the\noverall driving strategies provided are quite conservative.\nThrough the aforementioned five tests, it is observed that GPT-4V has initially acquired decision-\nmaking abilities similar to human drivers. It can combine the states of various traffic elements (e.g.,\npedestrians, vehicles, traffic lights, road signs, lanes) to provide the final driving strategy. Besides,\nGPT-4V can make reasonable decisions in diverse driving scenarios such as parking lots, intersections,\nhighways, and ramps. Overall, GPT-4V demonstrates strong adherence to rules and safety awareness\nwith relatively conservative driving strategies. However, there are still limitations in its driving\nperformance. For instance, it struggles to determine the status of distant objects (vehicles, traffic\nlights), and its perception range is restricted during nighttime. These limitations affect further driving\ndecisions. Additionally, GPT-4V\u2019s ability for spatiotemporal context inference remains limited (e.g.,\nGPT-4V gets confused in a U-turn scenario with multiple traffic lights).\n36\nSec. 4.3 Highway ramp turnaround\nNavigation Prompt:\nYou are in a scenario where you come across a \nturnaround intersection and you need to turn \naround according to the lane line instructions. \nYou are moving at a normal  speed. Please \nanswer me using the prescribed format.\nGPT-4V:\nObservation: The image is taken from a \nvehicle's perspective during nighttime driving on \na multi-lane road. There is a white car to the \nright that appears to be merging or taking an \nexit, and the glare of streetlights and other \nvehicle lights is noticeable. The surroundings \nare dimly lit, and the road seems to curve to the \nright ahead.\nAction: [...] adjust the vehicle's speed or \np o s i t i o n  o n  t h e  r o a d  a s  n e c e s s a r y  t o \naccommodate the traffic flow. Use the vehicle's \nheadlights and consider high beams if \nappropriate and legal  [...].\nGPT-4V:\nObservation: [...] with several vehicles ahead. \nReflective lane markers outline a path that \ncurves to the left. There's a green road sign that \nappears to indicate the direction for the \nturnaround, with a no-entry sign for the adjacent \nlane on the right [...] \nAction: Apply the brakes lightly to decelerate in \npreparation for the left turn. Signal left to inform \nother drivers of the intention to enter the \nturnaround.[...] \nGPT-4V:\nObservation: [...]what appears to be a turnaround \nintersection ahead, indicated by arrow signs and a \ndividing lane line. There are red rear lights of \nanother vehicle at the intersection, suggesting it is \neither stopped or moving slowly. The road seems \nto be wet [...] \nAction: Reduce speed, turn left following the lane \nline instructions.\nGPT-4V:\nObservation: [...] depicts a vehicle navigating a \nnighttime road environment, with the focus on a \ncurved section lit by a single streetlight. The road \nis delineated with yellow lane dividers, and the \nsurrounding area is poorly lit, limiting visibility.\nAction: Gently decelerate if necessary to \nmaintain a safe speed that matches the \ncurvature of the road and the existing visibility \nconditions. Keep the vehicle within the lane, \nusing the yellow lane dividers as a guide.  [...]\nFrame 1\nFrame 2\nFrame 3\nFrame 4\nFigure 27: Illustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right\nanswer in understanding. Red highlights the wrong answer in understanding. Yellow highlights\nthe incompetence in performing the task. Check Section 4.3 for detailed discussions.\n37\nSec. 4.4 Leave the main road\nNavigation Prompt:\nYou are on a busy road and you want to leave the \nmain road and merge onto the ramp.\nGPT-4V:\nObservation: The vehicle has successfully \nmerged into the main flow of traffic, with the lane \nmarkings now a solid white line indicating that \nlane changing at this point is generally \ndiscouraged or unlawful. A motorcycle is visible \nahead in the same lane as the vehicle, and it's a \nnight scene with artificial lighting from street \nlamps and surrounding vehicles.\nAction: The driver should continue to follow the \nmotorcycle at a safe distance and be prepared \nto adjust speed or change lanes if necessary to \nmaintain safety, especially given that \nmotorcycles can stop more quickly than cars. [...]\nGPT-4V:\nObservation: The vehicle is now at a point \nwhere the lane is merging, with traffic in the \nmain lanes continuing to be congested. The \nvehicle on the right appears to be very close to \nthe merging area, indicating a critical point to \neither complete the merge or adjust speed to \nfind a safer opportunity. \nAction: If a safe gap is confirmed, the next \naction would be to smoothly steer into the lane, \nadjust speed to match the flow of traffic, and \nturn off the signal once the merge is complete.\nGPT-4V:\nObservation: [...] The lane markings indicate that \nthe current lane is ending or merging, as \nsuggested by the dashed lines becoming solid \nand the arrow on the road pointing towards the \nmerge. There is a sign indicating a hospital \nnearby, which may not directly affect driving \ndecisions but suggests a possible increase in \npedestrian and emergency vehicle activity.\nAction: Reduce speed, prepare to merge right.\nGPT-4V:\nObservation: The vehicle is now in a position \nwhere the merging lane is about to end, \nindicated by the dashed lane markings \ntransitioning to a solid line. [...] and there is a \ncyclist ahead on the roadside, which requires \ncareful attention.\nAction: If the merge is not yet complete, the \ndriver should do so immediately before the lane \nends, taking care to avoid the bus and the \ncyclist. The driver must also ensure that the \nmerge does not cut off other vehicles or force \nthem to adjust speed abruptly.\nFrame 1\nFrame 2\nFrame 3\nFrame 4\nFigure 28: Illustration of GPT-4V\u2019s ability to merge onto other road. Green highlights the right\nanswer in understanding. Red highlights the wrong answer in understanding. Yellow highlights\nthe incompetence in performing the task. Check Section 4.4 for detailed discussions.\n38\nSec. 4.5 U-turn at intersection\nNavigation Prompt:\n This is the first frame. At this moment, you would \nlike to make a U-turn at the intersection ahead.\nGPT-4V:\nObservation: The image shows a view from a \ndriver's perspective at a nighttime intersection. \nThe traffic light facing the driver remains red. \nCross traffic has passed, and the intersection \nahead appears clear.[...] \nAction: It appears that the vehicle is either in \nmotion or about to move, which should not be \nthe case since the traffic signal is red.The driver \nshould be at a complete stop.\nGPT-4V:\nObservation: There's a vehicle directly in front \nof us, and it appears to be positioned for a left \nturn or U-turn based on its angle and the \nactivated left turn signal. [...] No traffic lights are \nvisible in this image, but the positioning of \nvehicles suggests there may be a signal \ncontrolling the intersection. [...]\nAction: The autonomous vehicle should be \ncontinuing to slow down in response to the \nslowing traffic ahead, maintaining a safe \ndistance from the vehicle in front, and preparing \nfor the intended U-turn.\nGPT-4V:\nObservation: There are cars in front of us, and \nthe brake lights indicate that they might be \nslowing down or stopping. A U-turn is intended at \nthe intersection ahead. There are no visible traffic \nlights in this image [...] \nAction: I am presumably maintaining a safe \nfollowing distance behind the vehicles in front, \nmatching their speeds.\nGPT-4V:\nObservation: [...] The traffic light for the driver\u2019s \ndirection is red, and other vehicles and \nmotorbikes are seen crossing the intersection \nfrom left to right, implying cross traffic has the \ngreen light.\nAction:The driver should remain stopped until \nthe traffic light turns green. [...] If U-turns are \npermitted, the driver should also check for \noncoming traffic, pedestrians, and any other \npotential hazards before executing the U-turn.\nFrame 1\nFrame 2\nFrame 3\nFrame 4\nFigure 29: Illustration of GPT-4V\u2019s ability to u-turn at traffic intersection. Green highlights the right\nanswer in understanding. Red highlights the wrong answer in understanding. Yellow highlights\nthe incompetence in performing the task. Check Section 4.5 for detailed discussions.\n39\n5\nConclusions\n5.1\nCapabilities of GPT-4V in Autonomous Driving\nIn this paper, we have conducted a comprehensive and multi-faceted evaluation of the GPT-4V in\nvarious autonomous driving scenarios. The results indicate that GPT-4V exhibits capabilities that\nhave the potential to surpass those of existing autonomous driving systems in aspects such as scenario\nunderstanding, intention recognition, and driving decision-making.\nIn corner cases, GPT-4V leverages its advanced understanding capabilities to handle out-of-\ndistribution scenarios and can accurately assess the intentions of surrounding traffic participants.\nGPT-4V utilizes multi-view images and temporal photos to achieve a complete perception of the\nenvironment, accurately identifying dynamic interactions between traffic participants. Moreover,\nit can infer the underlying motives behind these behaviors. As highlighted in Section 4, we also\nwitnessed the performance of GPT-4V in making continuous decisions on open roads. It can even\ninterpret the user interface of navigation apps in a human-like manner, assisting and guiding drivers\nin their decision-making processes.\nOverall, the performance of GPT-4V demonstrates the significant potential of Vision-Language\nModels (VLMs) to tackle complex challenges in the field of autonomous driving.\n5.2\nLimitations of GPT-4V in Autonomous Driving\nHowever, during our testing, we also found that GPT-4V performs poorly on the following tasks:\nDistinguishing left from right: As depicted in Figure 17, there were instances where the model\nstruggled with recognizing directions, which is a critical aspect of autonomous navigation. Similar\nissues are also observed in Figures 8 and 21. These figures highlight the model\u2019s occasional confusion\nwhen interpreting complex junctions or making lane-changing decisions.\nTraffic light recognition: Issues are observed in Figures 12, 15, 22, 26 and 29. We suspect this\nproblem is due to the extensive semantic information contained within the full image, leading to a\nloss in the embedding information of traffic lights. When the region of the traffic lights in the image\nis cropped and inputted separately, the model is capable of successful recognition shown in Figure 5.\nVision Grounding tasks: As shown in Figure 7, GPT-4V finds it difficult to specify pixel-level\ncoordinates or bounding boxes, managing only to indicate approximate areas within the image.\nSpatial Reasoning: Accurate spatial reasoning is paramount for the safe operation of autonomous\nvehicles. Whether it is the stitching of multiview images as illustrated in Figure 18 or the estimation\nof the relative positional relationship between a scooter and the self-driving car as shown in Figure\n21, GPT-4V struggles with making precise judgments. This may stem from the inherent complexity\nin understanding and interpreting three-dimensional space based on two-dimensional image inputs.\nAdditionally, issues were found with the model\u2019s interpretation of non-English traffic signs, which\nposes a challenge in regions where multiple languages are used on signage. The accuracy of counting\ntraffic participants was also found to be less reliable in congested environments where overlapping\nobjects can occur.\nIn conclusion, the above limitations indicate that even the most advanced Vision-Language Models\n(VLMs) currently exhibit deficiencies in basic directional recognition and traffic light identification,\nas well as a lack of 3D spatial reasoning capabilities. Furthermore, VLMs struggle to accurately\nlocalize key entities in various scenarios, suggesting that they are not yet suitable replacements for\nthe perception methods used in existing autonomous driving pipelines. However, it is noteworthy\nthat VLMs demonstrate a deep understanding of traffic common sense and strong generalization\ncapabilities in out-of-distribution cases. Looking ahead, a key area of development will be to integrate\nthe innate common sense knowledge of VLMs with conventional autonomous driving perception\ntechniques. In addition, ensuring the safety and reliability of VLM outputs remains an essential and\nongoing challenge.\n40\nReferences\n[1] Chinese traffic sign database. http://www.nlpr.ia.ac.cn/pal/trafficdata/\ndetection.html.\n[2] Wentao Bao, Qi Yu, and Yu Kong. Uncertainty-based traffic accident anticipation with spatio-\ntemporal relational learning. In ACM Multimedia Conference, May 2020.\n[3] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,\nAnush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal\ndataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 11621\u201311631, 2020.\n[4] Zhengping Che, Guangyu Li, Tracy Li, Bo Jiang, Xuefeng Shi, Xinsheng Zhang, Ying Lu,\nGuobin Wu, Yan Liu, and Jieping Ye. D2-city: A large-scale dashcam video dataset of diverse\ntraffic scenarios. 2019.\n[5] Long Chen, Oleg Sinavski, Jan H\u00fcnermann, Alice Karnsund, Andrew James Willmott, Danny\nBirch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality\nfor explainable autonomous driving. arXiv preprint arXiv:2310.01957, 2023.\n[6] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla:\nAn open urban driving simulator. In Conference on robot learning, pages 1\u201316. PMLR, 2017.\n[7] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\nGlm: General language model pretraining with autoregressive blank infilling. In Proceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 320\u2013335, 2022.\n[8] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive\nlike a human: Rethinking autonomous driving with large language models. arXiv preprint\narXiv:2307.07162, 2023.\n[9] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explana-\ntions for self-driving vehicles. Proceedings of the European Conference on Computer Vision\n(ECCV), 2018.\n[10] Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen,\nWei Zhang, Chunjing Xu, Dit-Yan Yeung, et al. Coda: A real-world road corner case dataset for\nobject detection in autonomous driving. In European Conference on Computer Vision, pages\n406\u2013423. Springer, 2022.\n[11] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang. Gpt-driver: Learning to drive with gpt.\narXiv preprint arXiv:2310.01415, 2023.\n[12] OpenAI. https://chat.openai.com, 2023.\n[13] OpenAI.\nChatgpt can now see, hear, and speak.\nhttps://openai.com/blog/\nchatgpt-can-now-see-hear-and-speak, 2023.\n[14] OpenAI. Gpt-4 technical report, 2023.\n[15] OpenAI. Gpt-4v(ision) system card. 2023.\n[16] OpenAI.\nGpt-4v(ision) technical work and authors.\nhttps://openai.com/\ncontributions/gpt-4v, 2023.\n[17] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul\nTsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for\nautonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2446\u20132454, 2020.\n[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n41\n[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[20] Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi,\nLiang He, and Yu Qiao. Dilu: A knowledge-driven approach to autonomous driving with large\nlanguage models. arXiv preprint arXiv:2309.16292, 2023.\n[21] Zizhang Wu, Xinyuan Chen, Hongyang Wei, Fan Song, and Tianhao Xu. Add: An automatic\ndesensitization fisheye dataset for autonomous driving. Engineering Applications of Artificial\nIntelligence, 126:106766, 2023.\n[22] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421, 9, 2023.\n[23] Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo,\nHanyu Li, Xing Hu, Jirui Yuan, et al. Dair-v2x: A large-scale dataset for vehicle-infrastructure\ncooperative 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 21361\u201321370, 2022.\n[24] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,\nYifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv\npreprint arXiv:2210.02414, 2022.\n[25] Ou Zheng, Mohamed Abdel-Aty, Lishengsa Yue, Amr Abdelraouf, Zijin Wang, and Nada\nMahmoud. Citysim: A drone-based vehicle trajectory dataset for safety oriented research and\ndigital twins. arXiv preprint arXiv:2208.11036, 2022.\n42\n"
  }
]