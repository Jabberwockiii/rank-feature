[
  {
    "title": "Retentive Network: A Successor to Transformer for Large Language Models",
    "link": "https://arxiv.org/pdf/2307.08621.pdf",
    "upvote": "165",
    "text": "Retentive Network: A Successor to Transformer\nfor Large Language Models\nYutao Sun\u2217\u2020\u2021\nLi Dong\u2217\u2020\nShaohan Huang\u2020\nShuming Ma\u2020\nYuqing Xia\u2020\nJilong Xue\u2020\nJianyong Wang\u2021\nFuru Wei\u2020\u22c4\n\u2020 Microsoft Research\n\u2021 Tsinghua University\nhttps://aka.ms/GeneralAI\nAbstract\nIn this work, we propose Retentive Network (RETNET) as a foundation archi-\ntecture for large language models, simultaneously achieving training parallelism,\nlow-cost inference, and good performance. We theoretically derive the connection\nbetween recurrence and attention. Then we propose the retention mechanism for\nsequence modeling, which supports three computation paradigms, i.e., parallel,\nrecurrent, and chunkwise recurrent. Specifically, the parallel representation allows\nfor training parallelism. The recurrent representation enables low-cost O(1) infer-\nence, which improves decoding throughput, latency, and GPU memory without\nsacrificing performance. The chunkwise recurrent representation facilitates effi-\ncient long-sequence modeling with linear complexity, where each chunk is encoded\nparallelly while recurrently summarizing the chunks. Experimental results on\nlanguage modeling show that RETNET achieves favorable scaling results, parallel\ntraining, low-cost deployment, and efficient inference. The intriguing properties\nmake RETNET a strong successor to Transformer for large language models. Code\nwill be available at https://aka.ms/retnet.\n0\n20\n40\n0\n150\n300\n0\n150\n300\nGPU Memory\u2193\n(GB)\nThroughput\u2191\n(wps)\nLatency\u2193\n(ms)\n3.4X\n15.6X\n8.4X\nInference Cost\nScaling Curve\nRetNet\nTransformer\n1\n3\n7\nLM Perplexity\nModel Size (B)\nFigure 1: Retentive network (RetNet) achieves low-cost inference (i.e., GPU memory, throughput,\nand latency), training parallelism, and favorable scaling curves compared with Transformer. Results\nof inference cost are reported with 8k as input length. Figure 6 shows more results on different\nsequence lengths.\n\u2217 Equal contribution. \u22c4 Corresponding author.\narXiv:2307.08621v4  [cs.CL]  9 Aug 2023\n\u201c\nThe only way to discover the limits of the possible is to go beyond them into the impossible.\nArthur C. Clarke\u201d\n1\nIntroduction\nLow-Cost\nInference\nTransformer\nRetNet\nFigure 2: RetNet makes the \u201cimpossible triangle\u201d\npossible, which achieves training parallelism, good\nperformance, and low inference cost simultane-\nously.\nTransformer [VSP+17] has become the de\nfacto architecture for large language mod-\nels [BMR+20], which was initially proposed\nto overcome the sequential training issue of\nrecurrent models [HS97]. However, training\nparallelism of Transformers is at the cost of in-\nefficient inference, because of the O(N) com-\nplexity per step and memory-bound key-value\ncache [Sha19], which renders Transformers un-\nfriendly to deployment. The growing sequence\nlength increases GPU memory consumption as\nwell as latency and reduces inference speed.\nNumerous efforts have continued to develop the\nnext-generation architecture, aiming at retain-\ning training parallelism and competitive perfor-\nmance as Transformers while having efficient\nO(1) inference. It is challenging to achieve the\nabove goals simultaneously, i.e., the so-called\n\u201cimpossible triangle\u201d as shown in Figure 2.\nThere have been three main strands of research.\nFirst, linearized attention [KVPF20] approximates standard attention scores exp(q \u00b7 k) with kernels\n\u03d5(q) \u00b7 \u03d5(k), so that autoregressive inference can be rewritten in a recurrent form. However, the\nmodeling capability and performance are worse than Transformers, which hinders the method\u2019s popu-\nlarity. The second strand returns to recurrent models for efficient inference while sacrificing training\nparallelism. As a remedy, element-wise operators [PAA+23] are used for acceleration, however,\nrepresentation capacity and performance are harmed. The third line of research explores replacing\nattention with other mechanisms, such as S4 [GGR21], and its variants [DFS+22, PMN+23]. None\nof the previous work can break through the impossible triangle, resulting in no clear winner compared\nwith Transformers.\nIn this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long-\nsequence modeling, Transformer-comparable performance, and parallel model training simultane-\nously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention,\nwhich has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent repre-\nsentations. First, the parallel representation empowers training parallelism to utilize GPU devices\nfully. Second, the recurrent representation enables efficient O(1) inference in terms of memory\nand computation. The deployment cost and latency can be significantly reduced. Moreover, the\nimplementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrent\nrepresentation can perform efficient long-sequence modeling. We parallelly encode each local block\nfor computation speed while recurrently encoding the global blocks to save GPU memory.\nWe conduct extensive experiments to compare RetNet with Transformer and its variants. Experi-\nmental results on language modeling show that RetNet is consistently competitive in terms of both\nscaling curves and in-context learning. Moreover, the inference cost of RetNet is length-invariant.\nFor a 7B model and 8k sequence length, RetNet decodes 8.4\u00d7 faster and saves 70% of memory\nthan Transformers with key-value caches. During training, RetNet also achieves 25-50% memory\nsaving and 7\u00d7 acceleration than standard Transformer and an advantage towards highly-optimized\nFlashAttention [DFE+22]. Besides, RetNet\u2019s inference latency is insensitive to batch size, allowing\nenormous throughput. The intriguing properties make RetNet a strong successor to Transformer for\nlarge language models.\n2\n2\nRetentive Networks\nRetentive network (RetNet) is stacked with L identical blocks, which follows a similar layout (i.e.,\nresidual connection, and pre-LayerNorm) as in Transformer [VSP+17]. Each RetNet block contains\ntwo modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module.\nWe introduce the MSR module in the following sections. Given an input sequence x = x1 \u00b7 \u00b7 \u00b7 x|x|,\nRetNet encodes the sequence in an autoregressive way. The input vectors {xi}|x|\ni=1 is first packed\ninto X0 = [x1, \u00b7 \u00b7 \u00b7 , x|x|] \u2208 R|x|\u00d7dmodel, where dmodel is hidden dimension. Then we compute\ncontextualized vector representations Xl = RetNetl(Xl\u22121), l \u2208 [1, L].\n2.1\nRetention\nIn this section, we introduce the retention mechanism that has a dual form of recurrence and\nparallelism. So we can train the models in a parallel way while recurrently conducting inference.\nGiven input X \u2208 R|x|\u00d7dmodel, we project it to one-dimensional function v(n) = Xn \u00b7 wV . Consider a\nsequence modeling problem that maps v(n) 7\u2192 o(n) through states sn. Let vn, on denote v(n), o(n)\nfor simplicity. We formulate the mapping in a recurrent manner:\nsn = Asn\u22121 + K\u22ba\nnvn,\nA \u2208 Rd\u00d7d, Kn \u2208 R1\u00d7d\non = Qnsn =\nn\nX\nm=1\nQnAn\u2212mK\u22ba\nmvm,\nQn \u2208 R1\u00d7d\n(1)\nwhere we map vn to the state vector sn, and then implement a linear transform to encode sequence\ninformation recurrently.\nNext, we make the projection Qn, Kn content-aware:\nQ = XWQ,\nK = XWK\n(2)\nwhere WQ, WK \u2208 Rd\u00d7d are learnable matrices.\nWe diagonalize the matrix A = \u039b(\u03b3ei\u03b8)\u039b\u22121, where \u03b3, \u03b8 \u2208 Rd.\nThen we obtain An\u2212m =\n\u039b(\u03b3ei\u03b8)n\u2212m\u039b\u22121. By absorbing \u039b into WQ and WK, we can rewrite Equation (1) as:\non =\nn\nX\nm=1\nQn(\u03b3ei\u03b8)n\u2212mK\u22ba\nmvm\n=\nn\nX\nm=1\n(Qn(\u03b3ei\u03b8)n)(Km(\u03b3ei\u03b8)\u2212m)\u22bavm\n(3)\nwhere Qn(\u03b3ei\u03b8)n, Km(\u03b3ei\u03b8)\u2212m is known as xPos [SDP+22], i.e., a relative position embedding\nproposed for Transformer. We further simplify \u03b3 as a scalar, Equation (3) becomes:\non =\nn\nX\nm=1\n\u03b3n\u2212m(Qnein\u03b8)(Kmeim\u03b8)\u2020vm\n(4)\nwhere \u2020 is the conjugate transpose. The formulation is easily parallelizable within training instances.\nIn summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallel\nformulation in Equation (4). We consider the original mapping v(n) 7\u2192 o(n) as vectors and obtain\nthe retention mechanism as follows.\nThe Parallel Representation of Retention\nAs shown in Figure 3a, the retention layer is defined as:\nQ = (XWQ) \u2299 \u0398,\nK = (XWK) \u2299 \u0398,\nV = XWV\n\u0398n = ein\u03b8,\nDnm =\n\u001a\u03b3n\u2212m,\nn \u2265 m\n0,\nn < m\nRetention(X) = (QK\u22ba \u2299 D)V\n(5)\nwhere \u0398 is the complex conjugate of \u0398, and D \u2208 R|x|\u00d7|x| combines causal masking and exponential\ndecay along relative distance as one matrix. Similar to self-attention, the parallel representation\nenables us to train the models with GPUs efficiently.\n3\n\ud835\udc4b\n\ud835\udc3e\n\ud835\udc44\n\ud835\udc49\n\ud835\udc42\nGN\n\ud835\udc44\ud835\udc3e\u22ba \u2299 \ud835\udc37 \ud835\udc49\n(a) Parallel representation.\n\ud835\udc4b\ud835\udc5b\n\ud835\udc46\ud835\udc5b\u22121\n\ud835\udc46\ud835\udc5b\n\ud835\udefe\n\ud835\udc3e\ud835\udc5b\n\ud835\udc49\ud835\udc5b\n\ud835\udc44\ud835\udc5b\n\ud835\udc42\ud835\udc5b\nRecurrent\nState\nInput\nOutput\nGN\n(b) Recurrent representation.\nFigure 3: Dual form of RetNet. \u201cGN\u201d is short for GroupNorm.\nThe Recurrent Representation of Retention\nAs shown in Figure 3b, the proposed mechanism\ncan also be written as recurrent neural networks (RNNs), which is favorable for inference. For the\nn-th timestep, we recurrently obtain the output as:\nSn = \u03b3Sn\u22121 + K\u22ba\nnVn\nRetention(Xn) = QnSn,\nn = 1, \u00b7 \u00b7 \u00b7 , |x|\n(6)\nwhere Q, K, V, \u03b3 are the same as in Equation (5).\nThe Chunkwise Recurrent Representation of Retention\nA hybrid form of parallel representation\nand recurrent representation is available to accelerate training, especially for long sequences. We\ndivide the input sequences into chunks. Within each chunk, we follow the parallel representation\n(Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the\nrecurrent representation (Equation (6)). Specifically, let B denote the chunk length. We compute the\nretention output of the i-th chunk via:\nQ[i] = QBi:B(i+1),\nK[i] = KBi:B(i+1),\nV[i] = VBi:B(i+1)\nRi = K\u22ba\n[i](V[i] \u2299 \u03b6) + \u03b3BRi\u22121,\n\u03b6ij = \u03b3B\u2212i\u22121\nRetention(X[i]) = (Q[i]K\u22ba\n[i] \u2299 D)V[i]\n|\n{z\n}\nInner-Chunk\n+ (Q[i]Ri\u22121) \u2299 \u03be\n|\n{z\n}\nCross-Chunk\n,\n\u03beij = \u03b3i+1\n(7)\nwhere [i] indicates the i-th chunk, i.e., x[i] = [x(i\u22121)B+1, \u00b7 \u00b7 \u00b7 , xiB].\n2.2\nGated Multi-Scale Retention\nWe use h = dmodel/d retention heads in each layer, where d is the head dimension. The heads use\ndifferent parameter matrices WQ, WK, WV \u2208 Rd\u00d7d. Moreover, multi-scale retention (MSR) assigns\ndifferent \u03b3 for each head. For simplicity, we set \u03b3 identical among different layers and keep them\nfixed. In addition, we add a swish gate [HG16, RZL17] to increase the non-linearity of retention\nlayers. Formally, given input X, we define the layer as:\n\u03b3 = 1 \u2212 2\u22125\u2212arange(0,h) \u2208 Rh\nheadi = Retention(X, \u03b3i)\nY = GroupNormh(Concat(head1, \u00b7 \u00b7 \u00b7 , headh))\nMSR(X) = (swish(XWG) \u2299 Y )WO\n(8)\nwhere WG, WO \u2208 Rdmodel\u00d7dmodel are learnable parameters, and GroupNorm [WH18] normalizes the\noutput of each head, following SubLN proposed in [SPP+19]. Notice that the heads use multiple \u03b3\nscales, which results in different variance statistics. So we normalize the head outputs separately.\nThe pseudocode of retention is summarized in Figure 4.\n4\ndef ParallelRetention(\nq, # bsz \u2217 num_head \u2217 len \u2217 qk_dim\nk, # bsz \u2217 num_head \u2217 len \u2217 qk_dim\nv, # bsz \u2217 num_head \u2217 len \u2217 v_dim\ndecay_mask # num_head \u2217 len \u2217 len\n):\nretention = q @ k.transpose(\u22121, \u22122)\nretention = retention \u2217 decay_mask\noutput = retention @ v\noutput = group_norm(output)\nreturn output\ndef RecurrentRetention(\nq, k, v, # bsz \u2217 num_head \u2217 len \u2217 qkv_dim\npast_kv, # bsz \u2217 num_head \u2217 qk_dim \u2217 v_dim\ndecay # num_head \u2217 1 \u2217 1\n):\ncurrent_kv = decay \u2217 past_kv + k.unsqueeze\n(\u22121) \u2217 v.unsqueeze(\u22122)\noutput = torch.sum(q.unsqueeze(\u22121) \u2217\ncurrent_kv, dim=\u22122)\noutput = group_norm(output)\nreturn output, current_kv\ndef ChunkwiseRetention(\nq, k, v, # bsz \u2217 num_head \u2217 chunk_size \u2217 qkv_dim\npast_kv, # bsz \u2217 num_head \u2217 qk_dim \u2217 v_dim\ndecay_mask, # num_head \u2217 chunk_size \u2217 chunk_size\nchunk_decay, # num_head \u2217 1 \u2217 1\ninner_decay, # num_head \u2217 chunk_size\n):\nretention = q @ k.transpose(\u22121, \u22122)\nretention = retention \u2217 decay_mask\ninner_retention = retention @ v\ncross_retention = (q @ past_kv) \u2217 inner_decay\nretention = inner_retention + cross_retention\noutput = group_norm(retention)\ncurrent_kv = chunk_decay \u2217 past_kv + k.transpose(\u22121, \u22122) @ v\nreturn output, current_kv\nFigure 4: Pseudocode for the three computation paradigms of retention.\nRetention Score Normalization\nWe utilize the scale-invariant nature of GroupNorm to im-\nprove the numerical precision of retention layers. Specifically, multiplying a scalar value within\nGroupNorm does not affect outputs and backward gradients, i.e., GroupNorm(\u03b1 \u2217 headi) =\nGroupNorm(headi). We implement three normalization factors in Equation (5). First, we normalize\nQK\u22ba as QK\u22ba/\n\u221a\nd. Second, we replace D with \u02dcDnm = Dnm/\u221aPn\ni=1 Dni. Third, let R denote the\nretention scores R = QK\u22ba \u2299 D, we normalize it as \u02dcRnm = Rnm/max(| Pn\ni=1 Rni|,1). Then the\nretention output becomes Retention(X) = \u02dcRV . The above tricks do not affect the final results while\nstabilizing the numerical flow of both forward and backward passes, because of the scale-invariant\nproperty.\n2.3\nOverall Architecture of Retention Networks\nFor an L-layer retention network, we stack multi-scale retention (MSR) and feed-forward network\n(FFN) to build the model. Formally, the input sequence {xi}|x|\ni=1 is transformed to vectors by a word\nembedding layer. We use the packed embeddings X0 = [x1, \u00b7 \u00b7 \u00b7 , x|x|] \u2208 R|x|\u00d7dmodel as the input and\ncompute the model output XL:\nY l = MSR(LN(Xl)) + Xl\nXl+1 = FFN(LN(Y l)) + Y l\n(9)\nwhere LN(\u00b7) is LayerNorm [BKH16]. The FFN part is computed as FFN(X) = gelu(XW1)W2,\nwhere W1, W2 are parameter matrices.\nTraining\nWe use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representations\nduring the training process. The parallelization within sequences or chunks efficiently utilizes\nGPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful for\nlong-sequence training, which is efficient in terms of both FLOPs and memory consumption.\n5\nArchitectures\nTraining\nParallelization\nInference Cost\nLong-Sequence\nMemory Complexity\nPerformance\nTransformer\n\u2714\nO(N)\nO(N 2)\n\u2714\u2714\nLinear Transformer\n\u2714\nO(1)\nO(N)\n\u2718\nRecurrent NN\n\u2718\nO(1)\nO(N)\n\u2718\nRWKV\n\u2718\nO(1)\nO(N)\n\u2714\nH3/S4\n\u2714\nO(1)\nO(N log N)\n\u2714\nHyena\n\u2714\nO(N)\nO(N log N)\n\u2714\nRetNet\n\u2714\nO(1)\nO(N)\n\u2714\u2714\nTable 1: Model comparison from various perspectives. RetNet achieves training parallelization,\nconstant inference cost, linear long-sequence memory complexity, and good performance.\nInference\nThe recurrent representation (Equation (6)) is employed during the inference, which\nnicely fits autoregressive decoding. The O(1) complexity reduces memory and inference latency\nwhile achieving equivalent results.\n2.4\nRelation to and Differences from Previous Methods\nTable 1 compares RetNet with previous methods from various perspectives. The comparison results\necho the \u201cimpossible triangle\u201d presented in Figure 2. Moreover, RetNet has linear memory complexity\nfor long sequences due to the chunkwise recurrent representation. We also summarize the comparisons\nwith specific methods as follows.\nTransformer\nThe parallel representation of retention shares similar spirits as Transform-\ners [VSP+17]. The most related Transformer variant is Lex Transformer [SDP+22] which im-\nplements xPos as position embeddings. As described in Equation (3), the derivation of retention\naligns with xPos. In comparison with attention, retention removes softmax and enables recurrent\nformulation, which significantly benefits inference.\nS4\nUnlike Equation (2), if Qn and Kn are content-unaware, the formulation can be degenerated to\nS4 [GGR21], where O = (QK\u22ba, QAK\u22ba, .., QA|x|\u22121K\u22ba) \u2217 V .\nLinear Attention\nThe variants typically use various kernels \u03d5(qi)\u03d5(kj)/P|x|\nn=1 \u03d5(qi)\u03d5(kn) to replace\nthe softmax function. However, linear attention struggles to effectively encode position information,\nrendering the models less performant. Besides, we reexamine sequence modeling from scratch, rather\nthan aiming at approximating softmax.\nAFT/RWKV\nAttention Free Transformer (AFT) simplifies dot-product attention to element-wise\noperations and moves softmax to key vectors. RWKV replaces AFT\u2019s position embeddings with\nexponential decay and runs the models recurrently for training and inference. In comparison, retention\npreserves high-dimensional states to encode sequence information, which contributes to expressive\nability and better performance.\nxPos/RoPE\nCompared with relative position embedding methods proposed for Transformers,\nEquation (3) presents a similar formulation as xPos [SDP+22] and RoPE [SLP+21].\nSub-LayerNorm\nAs shown in Equation (8), the retention layer uses Sub-LayerNorm [WMH+22]\nto normalize outputs. Because the multi-scale modeling leads to different variances for the heads, we\nreplace the original LayerNorm with GroupNorm.\n3\nExperiments\nWe conduct experiments on language modeling to evaluate RetNet. We evaluate the proposed\narchitecture with various benchmarks, i.e., language modeling performance, and zero-/few-shot\nlearning on downstream tasks. Moreover, for training and inference, we compare speed, memory\nconsumption, and latency.\n6\nSize\nHidden Dim.\n#Layers\nBatch Size\n# Tokens\nLearning Rate\n1.3B\n2048\n24\n4M\n100B\n6 \u00d7 10\u22124\n2.7B\n2560\n32\n4M\n100B\n3 \u00d7 10\u22124\n6.7B\n4096\n32\n4M\n100B\n3 \u00d7 10\u22124\nTable 2: Sizes, and learning hyper-parameters of the models in language modeling experiments.\n1.3B\n2.7B\n6.7B\nModel Size\n12.5\n13.0\n13.5\n14.0\n14.5\n15.0\nValidation PPL\nRetNet\nTransformer\nFigure 5: Perplexity decreases along with scaling up the model size. We empirically observe that\nRetNet tends to outperform Transformer when the model size is larger than 2B.\n3.1\nSetup\nParameter Allocation\nWe re-allocate the parameters in MSR and FFN for fair comparisons. Let d\ndenote dmodel for simplicity here. In Transformers, there are about 4d2 parameters in self-attention\nwhere WQ, WK, WV , WO \u2208 Rd\u00d7d, and 8d2 parameters in FFN where the intermediate dimension is\n4d. In comparison, RetNet has 8d2 parameters in retention, where WQ, WK \u2208 Rd\u00d7d, WG, WV \u2208\nRd\u00d72d, WO \u2208 R2d\u00d7d. Notice that the head dimension of V is twice Q, K. The widened dimension\nis projected back to d by WO. In order to keep the parameter number the same as Transformer, the\nFFN intermediate dimension in RetNet is 2d. Meanwhile, we set the head dimension to 256 in our\nexperiments, i.e., 256 for queries and keys, and 512 for values. For fair comparison, we keep \u03b3\nidentical among different model sizes, where \u03b3 = 1 \u2212 elinspace(log 1/32,log 1/512,h) \u2208 Rh instead of the\ndefault value in Equation (8).\nLanguage Model Training\nAs shown in Table 2, we train language models with various sizes\n(i.e., 1.3B, 2.7B, and 6.7B) from scratch. The training corpus is a curated compilation of The\nPile [GBB+20], C4 [DMI+21], and The Stack [KLBA+22]. We append the <bos> token to indicate\nthe start of a sequence2. The training batch size is 4M tokens with 2048 maximal length. We\ntrain the models with 100B tokens, i.e., 25k steps. We use the AdamW [LH19] optimizer with\n\u03b21 = 0.9, \u03b22 = 0.98, and weight decay is set to 0.05. The number of warmup steps is 375 with linear\nlearning rate decay. The parameters are initialized following DeepNet [WMD+22] to guarantee\ntraining stability. The implementation is based on TorchScale [MWH+22]. We train the models with\n512 AMD MI200 GPUs.\n3.2\nComparisons with Transformer\nLanguage Modeling\nAs shown in Figure 5, we report perplexity on the validation set for the\nlanguage models based on Transformer and RetNet. We present the scaling curves with three model\nsizes, i.e., 1.3B, 2.7B, and 6.7B. RetNet achieves comparable results with Transformers. More\nimportantly, the results indicate that RetNet is favorable regarding size scaling. Besides performance,\nthe RetNet training is quite stable in our experiments. Experimental results show that RetNet is a\nstrong competitor to Transformer for large language models. Empirically, we find that RetNet starts\nto outperform Transformer when the model size is larger than 2B. We also summarize the language\nmodeling results with different context lengths in Appendix B.\n2We find that appending the <bos> token at the beginning benefits training stability and performance.\n7\nHS\nBoolQ\nCOPA\nPIQA\nWinograd\nWinogrande\nSC\nAvg\nZero-Shot\nTransformer\n55.9\n62.0\n69.0\n74.6\n69.5\n56.5\n75.0\n66.07\nRetNet\n60.7\n62.2\n77.0\n75.4\n77.2\n58.1\n76.0\n69.51\n4-Shot\nTransformer\n55.8\n58.7\n71.0\n75.0\n71.9\n57.3\n75.4\n66.44\nRetNet\n60.5\n60.1\n78.0\n76.0\n77.9\n59.9\n75.9\n69.76\nTable 3: Zero-shot and few-shot learning with Transformer and RetNet. The model size is 6.7B.\nModel Size\nMemory (GB) \u2193\nThroughput (wps) \u2191\nTrm\nTrm+FlashAttn\nRetNet\nTrm\nTrm+FlashAttn\nRetNet\n1.3B\n74.8\n38.8\n34.5\n10832.4\n63965.2\n73344.8\n2.7B\n69.6\n42.1\n42.0\n5186.0\n34990.2\n38921.2\n6.7B\n69.0\n51.4\n48.0\n2754.4\n16230.1\n17458.6\n13B\n61.4\n46.3\n45.9\n1208.9\n7945.1\n8642.2\nTable 4: Training cost of Transformer (Trm), Transformer with FlashAttention (Trm+FlashAttn), and\nRetNet. We report memory consumption and training throughput (word per second; wps).\nZero-Shot and Few-Shot Evaluation on Downstream Tasks\nWe also compare the language\nmodels on a wide range of downstream tasks.\nWe evaluate zero-shot and 4-shot learning\nwith the 6.7B models. As shown in Table 3, the datasets include HellaSwag (HS) [ZHB+19],\nBoolQ [CLC+19], COPA [WPN+19], PIQA [BZB+20], Winograd, Winogrande [LDM12], and Sto-\nryCloze (SC) [MRL+17]. The accuracy numbers are consistent with language modeling perplexity\npresented in Figure 5. RetNet achieves comparable performance with Transformer on zero-shot and\nin-context learning settings.\n3.3\nTraining Cost\nAs shown in Table 4, we compare the training speed and memory consumption of Transformer and\nRetNet, where the training sequence length is 8192. We also compare with FlashAttention [DFE+22],\nwhich improves speed and reduces GPU memory IO by recomputation and kernel fusion. In compari-\nson, we implement RetNet using vanilla PyTorch code, and leave kernel fusion or FlashAttention-like\nacceleration for future work. We use chunkwise recurrent representation of retention as described in\nEquation (7). The chunk size is set to 512. We evaluate the results with eight Nvidia A100-80GB\nGPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B\nand 13B models.\nExperimental results show that RetNet is more memory-efficient and has higher throughput than\nTransformers during training. Even compared with FlashAttention, RetNet is still competitive in\nterms of speed and memory cost. Moreover, without relying on specific kernels, it is easy to train\nRetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200\ncluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via\nadvanced implementation, such as kernel fusion.\n3.4\nInference Cost\nAs shown in Figure 6, we compare memory cost, throughput, and latency of Transformer and RetNet\nduring inference. Transformers reuse KV caches of previously decoded tokens. RetNet uses the\nrecurrent representation as described in Equation (6). We evaluate the 6.7B model on the A100-80GB\nGPU in our experiments. Figure 6 shows that RetNet outperforms Transformer in terms of inference\ncost.\nMemory\nAs shown in Figure 6a, the memory cost of Transformer increases linearly due to KV\ncaches. In contrast, the memory consumption of RetNet remains consistent even for long sequences,\n8\n2048\n3072\n4096\n5120\n6144\n7168\n8192\nSequence Length\n15\n20\n25\n30\n35\n40\n45\nGPU Memory (GB)\nModel Weights\nRetNet\nTransformer\n(a) GPU memory cost of Transformer and RetNet.\n2048\n3072\n4096\n5120\n6144\n7168\n8192\nSequence Length\n50\n100\n150\n200\n250\n300\nThroughput (wps)\nRetNet\nTransformer\n(b) Throughput of Transformer and RetNet.\n1\n2\n3\n4\n5\n6\n7\n8\nBatch Size\n50\n100\n150\n200\n250\n300\n350\nLatency (ms)\nTransformer (1024)\nTransformer (2048)\nTransformer (4096)\nTransformer (8192)\nRetNet (8192)\n(c) Inference latency with different batch sizes.\nFigure 6: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms\nTransformers in terms of memory consumption, throughput, and latency.\nrequiring much less GPU memory to host RetNet. The additional memory consumption of RetNet is\nalmost negligible (i.e., about 3%) while the model weights occupy 97%.\nThroughput\nAs presented in Figure 6b, the throughput of Transformer drops along with the\ndecoding length increases. In comparison, RetNet has higher and length-invariant throughput during\ndecoding, by utilizing the recurrent representation of retention.\nLatency\nLatency is an important metric in deployment, which greatly affects user experience. We\nreport decoding latency in Figure 6c. Experimental results show that increasing batch size renders\nTransformer\u2019s latency larger. Moreover, the latency of Transformers grows faster with longer input. In\norder to make latency acceptable, we have to restrict the batch size, which harms the overall inference\nthroughput of Transformers. By contrast, RetNet\u2019s decoding latency outperforms Transformers and\nkeeps almost the same across different batch sizes and input lengths.\n3.5\nComparison with Transformer Variants\nApart from Transformer, we compare RetNet with various efficient Transformer variants, including\nLinear Transformer [KVPF20], RWKV [PAA+23], H3 [DFS+22], and Hyena [PMN+23]. All\nmodels have 200M parameters with 16 layers and a hidden dimension of 1024. For H3, we set the\nhead dimension as 8. For RWKV, we use the TimeMix module to substitute self-attention layers\nwhile keeping FFN layers consistent with other models for fair comparisons. We train the models\nwith 10k steps with a batch size of 0.5M tokens. Most hyperparameters and training corpora are kept\nthe same as in Section 3.1.\nTable 5 reports the perplexity numbers on the in-domain validation set and other out-of-domain\ncorpora, e.g., Project Gutenberg 2019-2022 (PG22) [SDP+22], QMSum [ZYY+21], GovRe-\n9\nMethod\nIn-Domain\nPG22\nQMSum\nGovReport\nSummScreen\nRWKV\n30.92\n51.41\n28.17\n19.80\n25.78\nH3\n29.97\n49.17\n24.29\n19.19\n25.11\nHyena\n32.08\n52.75\n28.18\n20.55\n26.51\nLinear Transformer\n40.24\n63.86\n28.45\n25.33\n32.02\nRetNet\n26.05\n45.27\n21.33\n16.52\n22.48\nTable 5: Perplexity results on language modeling. RetNet outperforms other architectures on both the\nin-domain evaluation set and various out-of-domain corpora.\nMethod\nIn-Domain\nPG22\nQMSum\nGovReport\nSummScreen\nRetNet\n26.05\n45.27\n21.33\n16.52\n22.48\n\u2212 swish gate\n27.84\n49.44\n22.52\n17.45\n23.72\n\u2212 GroupNorm\n27.54\n46.95\n22.61\n17.59\n23.73\n\u2212 \u03b3 decay\n27.86\n47.85\n21.99\n17.49\n23.70\n\u2212 multi-scale decay\n27.02\n47.18\n22.08\n17.17\n23.38\nReduce head dimension\n27.68\n47.72\n23.09\n17.46\n23.41\nTable 6: Ablation results on in-domain and out-of-domain corpora.\nport [HCP+21], SummScreen [CCWG21, SSI+22]. Overall, RetNet outperforms previous methods\nacross different datasets. RetNet not only achieves better evaluation results on the in-domain corpus\nbut also obtains lower perplexity on several out-of-domain datasets. The favorable performance\nmakes RetNet a strong successor to Transformer, besides the benefits of significant cost reduction\n(Sections 3.3 and 3.4).\nIn addition, we discuss the training and inference efficiency of the compared methods. Let d denote\nthe hidden dimension, and n the sequence length. For training, RWKV\u2019s token-mixing complexity\nis O(dn) while Hyena\u2019s is O(dn log n) with Fast Fourier Transform acceleration. The above two\nmethods reduce training FLOPS via employing element-wise operators to trade-off modeling capacity.\nIn comparison with retention, the chunk-wise recurrent representation is O(dn(b + h)), where b is\nthe chunk size, h is the head dimension, and we usually set b = 512, h = 256. For either large model\nsize (i.e., larger d) or sequence length, the additional b + h has negligible effects. So the RetNet\ntraining is quite efficient without sacrificing the modeling performance. For inference, among the\ncompared efficient architectures, Hyena has the same complexity (i.e., O(n) per step) as Transformer\nwhile the others can perform O(1) decoding.\n3.6\nAblation Studies\nWe ablate various design choices of RetNet and report the language modeling results in Table 6. The\nevaluation settings and metrics are the same as in Section 3.5.\nArchitecture\nWe ablate the swish gate and GroupNorm as described in Equation (8). Table 6\nshows that the above two components improve the final performance. Firstly, the gating module is\nessential for enhancing non-linearity and improving model capability. Notice that we use the same\nparameter allocation as Transformers after removing the gate. Secondly, group normalization in\nretention balances the variances of multi-head outputs, which improves training stability and language\nmodeling results.\nMulti-Scale Decay\nEquation (8) shows that we use different \u03b3 as the decay rates for the retention\nheads. In the ablation studies, we examine removing \u03b3 decay (i.e., \u201c\u2212 \u03b3 decay\u201d) and applying the\nsame decay rate across heads (i.e., \u201c\u2212 multi-scale decay\u201d). Specifically, ablating \u03b3 decay is equivalent\nto \u03b3 = 1. In the second setting, we set \u03b3 = 127/128 for all heads. Table 6 indicates that both the\ndecay mechanism and using multiple decay rates can improve the language modeling performance.\nHead Dimension\nFrom the recurrent perspective of Equation (1), the head dimension implies the\nmemory capacity of hidden states. In the ablation study, we reduce the default head dimension from\n10\n256 to 64, i.e., 64 for queries and keys, and 128 for values. We keep the hidden dimension dmodel the\nsame so the number of heads increases. Experimental results in Table 6 show that the larger head\ndimension achieves better performance.\n4\nConclusion\nIn this work, we propose retentive networks (RetNet) for sequence modeling, which enables various\nrepresentations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better\ninference efficiency (in terms of memory, speed, and latency), favorable training parallelization,\nand competitive performance compared with Transformers. The above advantages make RetNet an\nideal successor to Transformers for large language models, especially considering the deployment\nbenefits brought by the O(1) inference complexity. In the future, we would like to scale up RetNet\nin terms of model size [CDH+22] and training steps. Moreover, retention can efficiently work with\nstructured prompting [HSD+22b] by compressing long-term memory. We will also use RetNet as the\nbackbone architecture to train multimodal large language models [HSD+22a, HDW+23, PWD+23].\nIn addition, we are interested in deploying RetNet models on various edge devices, such as mobile\nphones.\nAcknowledgement\nWe would like to acknowledge Jiayu Ding, Songlin Yang, and colleagues from MSRA System Group\nfor the helpful discussions.\nReferences\n[BKH16] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv\npreprint arXiv:1607.06450, 2016.\n[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. In Advances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n[BZB+20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa:\nReasoning about physical commonsense in natural language. In Thirty-Fourth AAAI\nConference on Artificial Intelligence, 2020.\n[CCWG21] Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset\nfor abstractive screenplay summarization. arXiv preprint arXiv:2104.07091, 2021.\n[CDH+22] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham\nSinghal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the\nrepresentation collapse of sparse mixture of experts. In Advances in Neural Information\nProcessing Systems, 2022.\n[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,\nand Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no\nquestions. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics, pages 2924\u20132936, 2019.\n[DFE+22] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention:\nFast and memory-efficient exact attention with io-awareness. Advances in Neural\nInformation Processing Systems, 35:16344\u201316359, 2022.\n[DFS+22] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher\nR\u00e9. Hungry hungry hippos: Towards language modeling with state space models. arXiv\npreprint arXiv:2212.14052, 2022.\n11\n[DMI+21] Jesse Dodge, Ana Marasovi\u00b4c, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and\nMatt Gardner. Documenting large webtext corpora: A case study on the colossal clean\ncrawled corpus. In Conference on Empirical Methods in Natural Language Processing,\n2021.\n[GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB\ndataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n[GGR21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with\nstructured state spaces. arXiv preprint arXiv:2111.00396, 2021.\n[HCP+21] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient\nattentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021.\n[HDW+23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,\nTengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi,\nJohan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language\nis not all you need: Aligning perception with language models. ArXiv, abs/2302.14045,\n2023.\n[HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:\nLearning, 2016.\n[HS97] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computa-\ntion, 9:1735\u20131780, November 1997.\n[HSD+22a] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\ning Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,\nabs/2206.06336, 2022.\n[HSD+22b] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured\nprompting: Scaling in-context learning to 1,000 examples. ArXiv, abs/2212.06713,\n2022.\n[KLBA+22] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos\nMu\u00f1oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf,\nDzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The Stack: 3TB of\npermissively licensed source code. Preprint, 2022.\n[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Trans-\nformers are rnns: Fast autoregressive transformers with linear attention. In International\nConference on Machine Learning, pages 5156\u20135165. PMLR, 2020.\n[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema chal-\nlenge. In Thirteenth International Conference on the Principles of Knowledge Repre-\nsentation and Reasoning, 2012.\n[LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Interna-\ntional Conference on Learning Representations, 2019.\n[MRL+17] Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James\nAllen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd\nWorkshop on Linking Models of Lexical, Sentential and Discourse-level Semantics,\npages 46\u201351, 2017.\n[MWH+22] Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong,\nAlon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. TorchScale:\nTransformers at scale. CoRR, abs/2211.13184, 2022.\n[OSG+23] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,\nRazvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long\nsequences. ArXiv, abs/2303.06349, 2023.\n12\n[PAA+23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi\nCao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He,\nHaowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra,\nHayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang,\nBolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang,\nQihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the\ntransformer era, 2023.\n[PMN+23] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus,\nYoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger\nconvolutional language models. arXiv preprint arXiv:2302.10866, 2023.\n[PWD+23] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and\nFuru Wei. Kosmos-2: Grounding multimodal large language models to the world.\nArXiv, abs/2306.14824, 2023.\n[RZL17] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Swish: a self-gated activation\nfunction. arXiv: Neural and Evolutionary Computing, 2017.\n[SDP+22] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,\nVishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer.\narXiv preprint arXiv:2212.10554, 2022.\n[Sha19] Noam M. Shazeer. Fast transformer decoding: One write-head is all you need. ArXiv,\nabs/1911.02150, 2019.\n[SLP+21] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n[SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-LM: Training multi-billion parameter language models\nusing model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n[SSI+22] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta,\nWenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison\nover long language sequences. arXiv preprint arXiv:2201.03533, 2022.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural Information\nProcessing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000\u2013\n6010, 2017.\n[WH18] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European\nconference on computer vision (ECCV), pages 3\u201319, 2018.\n[WMD+22] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu\nWei. DeepNet: Scaling Transformers to 1,000 layers. ArXiv, abs/2203.00555, 2022.\n[WMH+22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\nYu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers.\narXiv preprint arXiv:2210.06423, 2022.\n[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark for\ngeneral-purpose language understanding systems. arXiv preprint arXiv:1905.00537,\n2019.\n[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag:\nCan a machine really finish your sentence? In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, 2019.\n[ZYY+21] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Has-\nsan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new\nbenchmark for query-based multi-domain meeting summarization. arXiv preprint\narXiv:2104.05938, 2021.\n13\nA\nHyperparameters\nHyperparameters\n1.3B\n2.7B\n6.7B\nLayers\n24\n32\n32\nHidden size\n2048\n2560\n4096\nFFN size\n4096\n5120\n8192\nHeads\n8\n10\n16\nLearning rate\n6 \u00d7 10\u22124\n3 \u00d7 10\u22124\n3 \u00d7 10\u22124\nLR scheduler\nPolynomial decay\nWarm-up steps\n375\nTokens per batch\n4M\nAdam \u03b2\n(0.9, 0.98)\nTraining steps\n25,000\nGradient clipping\n2.0\nDropout\n0.1\nWeight decay\n0.01\nTable 7: Hyperparamters used for the models in Section 3.\nB\nGrouped Results of Different Context Lengths\nAs shown in Table 8, we report language modeling results with different context lengths. In order\nto make the numbers comparable, we use 2048 text chunks as evaluation data and only compute\nperplexity for the last 128 tokens. Experimental results show that RetNet outperforms Transformer\nacross different context lengths. Besides, RetNet can utilize longer context for better results.\nModel\n512\n1024\n2048\nTransformer\n13.55\n12.56\n12.35\nRetNet\n13.09\n12.14\n11.98\nTable 8: Language modeling perplexity of RetNet and Transformer with different context length. The\nresults show that RetNet has a consistent advantage across sequence length.\n14\n"
  },
  {
    "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",
    "link": "https://arxiv.org/pdf/2307.08674.pdf",
    "upvote": "46",
    "text": "TableGPT: Towards Unifying Tables, Nature\nLanguage and Commands into One GPT\nLiangyu Zha1,2\nJunlin Zhou1,2\nLiyao Li1,2\nRui Wang1,2\nQingyi Huang3\nSaisai Yang3\nJing Yuan3\nChangbao Su3\nXiang Li3\nAofeng Su3\nTao Zhang3\nChen Zhou3\nKaizhe Shou\nMiao Wang\nWufang Zhu\nGuoshan Lu\nChao Ye\nYali Ye\nWentao Ye\nYiming Zhang\nXinglong Deng\nJie Xu\nHaobo Wang4\nGang Chen4\nJunbo Zhao4\u2217\n1directional lead\n2joint first author\n3equal contribution\n4project lead\nZhejiang University\nAbstract\nTables are prevalent in real-world databases, requiring significant time and effort\nfor humans to analyze and manipulate. The advancements in large language models\n(LLMs) have made it possible to interact with tables using natural language input,\nbringing this capability closer to reality. In this paper, we present TableGPT, a\nunified fine-tuned framework that enables LLMs to understand and operate on\ntables using external functional commands. It introduces the capability to seam-\nlessly interact with tables, enabling a wide range of functionalities such as question\nanswering, data manipulation (e.g., insert, delete, query, and modify operations),\ndata visualization, analysis report generation, and automated prediction. TableGPT\naims to provide convenience and accessibility to users by empowering them to\neffortlessly leverage tabular data. At the core of TableGPT lies the novel concept\nof global tabular representations, which empowers LLMs to gain a comprehensive\nunderstanding of the entire table beyond meta-information. By jointly training\nLLMs on both table and text modalities, TableGPT achieves a deep understanding\nof tabular data and the ability to perform complex operations on tables through\nchain-of-command instructions. Importantly, TableGPT offers the advantage of\nbeing a self-contained system rather than relying on external API interfaces. More-\nover, it supports efficient data process flow, query rejection (when appropriate)\nand private deployment, enabling faster domain data fine-tuning and ensuring data\nprivacy, which enhances the framework\u2019s adaptability to specific use cases.\n1\nIntroduction\nThe vast and intricate world of data is often encapsulated in tables, being a foundation for data-\ndriven decision-making in a wide spectrum of applications, including financial analysis, supply\nchain management, and healthcare analytics. It enables stakeholders to analyze trends, patterns,\nand relationships, leading to informed business decisions, process improvements, and resource\noptimization. For years, data scientists have struggled to process tables using complicated Excel\nformulas or handcrafted programming [19, 20]. Consequently, there has been an urgent need to\nunderstand and interpret tabular data in a more efficient fashion.\nIn the field of natural language processing, Generative Pre-trained Transformers (GPTs) [24, 25, 2, 22,\n21] or Large Language Models (LLMs) [4, 36, 27, 37] have revolutionized the paradigm of language\n\u2217Correspondence to j.zhao@zju.edu.cn.\nTechnical report preprint. Work in progress.\narXiv:2307.08674v3  [cs.AI]  7 Aug 2023\nTable 1: Comparisons with previous command-using LLMs for tabular data. (See details in Sec 3.2)\nProperties\nMethods\nChatExcel [28]\nSheetCopilot [17]\nData-Copilot [38]\nTableGPT (ours)\nNature Language Operations\n!\n!\n!\n!\nGeneralization to Arbitrary Tables\n!\n!\n%\n!\nVisualization\n%\n!\n!\n!\nAnalysis & Report\n%\n%\n!\n!\nPrediction\n%\n%\n!\n!\nChain-of-command\n%\n%\n!\n!\nBase Model\nUnknown\nAPI\nAPI\nFine-tuned\nVague Input Rejection\n%\n%\n%\n!\nPrivate Deployment\n%\n%\n%\n!\ndata mining. Following this line of works, researchers have also explored large models for various\nmodalities like vision [6, 13], and speech [9]. From a technical standpoint, their ability to generate\nhuman-like text has opened new vistas of possibilities for processing tabular data. Nevertheless, it is\nnon-trivial to directly employ the vanilla ChatGPT [21] model in the tabular area for two reasons:\n(i)-Global Table Understanding: the GPTs are known to suffer from the limited token length and\nthus, they can not read a whole large table, making them hard to understand the global tabular\ninformation. (ii)-Generalized to Tabular Domain: Second, their training processes are tailored for\nnatural languages and thus, they are less generalizable when handling tabular data.\nThere have been several works [8, 39, 18, 17] developed to integrate natural language for tabular\ndata analysis. NL2SQL (Nature language to SQL) [8, 39, 18] is a long-standing research topic\nthat converts natural language to SQL commands that manipulate the relational database. Recently,\nSheetCopilot [17] explored languages to VBA (Visual Basic for Applications, an embedded script\nlanguage for Microsoft Excel) command such that benefit from a rich set of spreadsheet software\nfunctionalities. However, we found that both solutions demonstrate unsatisfactory performance. We\nspeculate that these forms of programming code, which is fundamentally unstructured, adds another\nlayer of complexity, making automated post-processing almost insurmountable.\nIn this work, we develop TableGPT that pushes the boundaries of what is possible in data analysis\nempowered by LLM techniques, marking an important step forward in our pursuit of making data\nmore accessible and understandable. Our TableGPT framework unifies tables, natural language, and\ncommands into a single GPT model, making data interpretation and manipulation more intuitive and\nuser-friendly. By rethinking the interaction of tables, natural language, and commands, we integrate\nseveral core components into TableGPT:\n\u2022 Global Table Representation: We make the first attempt to develop a global representation\nlearning paradigm for tables that encodes the whole table into one vector. By jointly training\nthe LLM and a table encoder on vast amounts of text and table data, we equip the encoder\nto adequately capture the global information in the input table. This enables the LLM to\nperceive and understand the table data effectively, thereby providing a more global and\nenhanced comprehension of tables.\n\u2022 Chain-of-Command:\nWe introduce this concept to emphasize the essential idea of a\nstructured and hierarchical execution of tasks. Just like a well-coordinated organization\nwhere each directive is cascaded from a higher level to its lower counterpart, TableGPT\nfollows a similar chain of commands, breaking down complex tasks into simpler ones\nand executing them step-by-step. Moreover, it fosters the ability to refuse ambiguous or\ninappropriate commands, much like an actual data scientist, instead of blindly following\nany potential erroneous instruction, thereby improving the interaction between humans and\nLLM systems in the field of data science. Our proposed command set is not only easier\nto control but also reduces the uncertainty that often accompanies traditional methods of\nhandling table data.\n\u2022 Domain-aware Fine-Tuning: To foster the ability to adapt to specific domains of tables and\ncorresponding textual materials, domain-aware fine-tuning hinges on customizing training in\na way that the model generates text embodying similar stylistic and logical elements found\nin a given domain, thereby augmenting its understanding of specific domain table data. To\n2\nTableGPT\nHousing_price.csv\nTable \nEncoder\nLLM111\n111GPT\nUser query: How \nhouse prices have \nchanged by region \nin the last decade?\nAnswer: According to the \ntable, the average house \nprice in each region has \ngradually increased over \nthe past decade, with the \nlargest increase in City A...\n{\n\"type\": \"commands\", // \"text\" or \"commands\"\n\"value\": {\n\"commands\": [\n\"SelectCondition\",\n\"GroupBy\u201c\n],\n\"commands_args\": [\n{\n\"columns\": [\"Year\"],\n\"index\": [],\n\"range\": [2013, 2023],\n\"condition\": \"range\",\n\"slice\": \"no\",\n\"type\": \"column\",\n\"relation\": \"none\"\n},\n{\n\"by\": [\"Region\", \"Year\"],\n\"aggregate_args\": {\"Price\": [\"mean\"]}\n}\n]\n}\n}\nCommand System\nCorrector\nExecutor\nGenerated Table\n{\n\"type\": \"text\", // \"text\" or \"commands\"\n\"value\": \"According to the table, the average \nhouse price in each region has gradually increased \nover the past decade, with the largest increase in \nCity A...\"\n}\nCommands Set\n\u2022\nInsertCondition\n\u2022\nDeleteCondition\n\u2022\nSelectCondition\n\u2022\nStatisticAnalysis\n\u2022\nSortCondition\n\u2022\nGroupBy\n\u2022\nUnaryTransform\n\u2022\nBinaryTransform\n\u2022\nVisualization\n\u2022\nPrediction\n\u2022\n\u2026\u2026\nFigure 1: An architecture of TableGPT framework.\nmake this approach scalable and feasible, we have also developed a data processing pipeline\nthat yields notable improvements with only a small amount of data, hence alleviating the\nresource-demanding aspect of training LLMs and supporting private deployment.\nFrom a real-world production standpoint, the unstructured code outputted by NL2SQL poses sig-\nnificant challenges for preemptive checks and error corrections. Hence, we advocate for the use\nof structured command sequences, simplifying post-processing. Data-Copilot [38] also embraces\nthis command-based approach with self-instruct [31], but its reliance on API-called native LLMs\nto comprehend tabular data\u2019s processing and analysis logic directly presents limitations. Given the\nintrinsic data variability and task-specificity of tabular data, we believe an effective product should be\ncustom-built for tabular data while maintaining general applicability to broader downstream tasks.\nThis conviction underscores the imperative of introducing a LLM specifically pre-trained for tabular\ndata.\nTo sum up, this work presents a pioneering TableGPT framework, which is a unified, well-fledged\nholistic solution, enabling efficient tabular data processing, analysis and visualization, driven all by\nnatural languages. We summarize several important advantages of TableGPT as follows:\n\u2022 Language-driven EDA: TableGPT understands user intent from natural language, dissects\nthe desired actions, and executes external commands on the table. It subsequently returns\nthe processed results in both tabular and textual explanations to the user. This novel\napproach simplifies the way users engage with table data, bringing an intuitive instantiation\nto Exploratory Data Analysis (EDA).\n\u2022 Unified Cross-modal Framework:\nInnovatively, we devise a global table encoder for\nunderstanding the whole table. TableGPT is able to fully understand the user query, meta-\nknowledge, and whole tabular data, which leads to much more reliable execution commands\nfor table manipulation.\n\u2022 Generalization and Privacy:\nBy domain-aware fine-tuning, our TableGPT can better\nhandle data variability of tables and generalize to different domains. Further, our framework\nsupports private deployment, offering robust data privacy protections. This aspect is critical\nin the modern age where data privacy and protection are just paramount.\n3\n2\nTableGPT\n2.1\nModel Design\nThe development of TableGPT begins with the foundation provided by pre-trained LLMs. The\nadvancements in the field of natural language processing have led to the development of a number\nof exceptional open-source LLMs, such as LLaMa [27], Phoenix [4], ChatGLM [36], Ziya [10],\nand Baichuan [12]. In designing TableGPT, we opted to use Phoenix [4] with 7B parameters as our\nbase model for fine-tuning, owing to its excellent capabilities in handling both Chinese and English\nlanguages. This choice is not, however, exclusive. Our model design supports adaptation with other\nLLMs, providing versatility and flexibility in its implementation.\nWhat sets TableGPT apart from its predecessors [28, 17, 38] is the novel approach to its fine-tuning\nprocess. We performed the fine-tuning on a vast corpus, comprising 2T tokens of textual data and\n0.3M tables. This corpus offers a diverse landscape for the model to learn from, including but not\nlimited to user query-command sequence pairs and publicly available domain-specific data for table\nanalysis reports.\nThe overall architecture of TableGPT is shown in Figure 1. When a user inputs a table and a query,\nthese are received by TableGPT, which consists of a table encoder and an LLM. The table encoder\nserves to extract vector representations from the input table. These representations, coupled with\nthe text query, are then fed into the LLM for inference. The LLM discerns the user\u2019s query intent\nand generates an output that includes both a command sequence and a textual reply. The command\nsequence undergoes error correction in the command system\u2019s corrector before it is fed into the\nexecutor for execution. The final output, provided to the user, includes the manipulated table and\na textual reply. This streamlined process delivers efficient, reliable responses to table data queries,\nenhancing user experience and simplifying data analysis.\n2.2\nGlobal Representation of Table\nThe rapid development of large language models (LLMs) has seen them interfacing with a multitude\nof modalities such as vision, and audio. For instance, the integration of vision and LLMs has led to\nmodels like CLIP [23] (Contrastive Language\u2013Image Pretraining) from OpenAI that connects images\nand text through shared latent space. The combination of audio and LLMs gave rise to models like\nWave2Vec [1] and Tacotron [32] that employ the representation of audio in the form of spectrograms\nto generate or understand speech.\nDespite these advancements, the exploration of LLMs interfacing with tabular data remains limited.\nThe question of how to enable LLMs to comprehend and interpret tables is essential. Some studies\nhave attempted to convert sample rows of table data directly into a sentence-like text description [7],\nwhile others have attempted to artificially define a global representation of table data through the\ntemplate-based extraction of column names, industry background, and other metadata schema [38].\nHowever, these approaches only extract partial information from table data for LLMs, consequently\noverlooking the global information and industry background inherent in the data.\nNotably, for the tables, it is required to embed the whole table into one single vector, instead of\ngenerating sample-wise embedding. This can be non-trivial and challenging because, unlike images,\nvideos, and audio, table data is inherently a highly abstract structured data type. Furthermore, it\npossesses a dual permutation invariance structure where shuffling rows or columns does not affect the\ninformation contained within the table, a distinct contrast to images and audio, which carry inductive\nbias in adjacent positions or sequences. Moreover, tables from different domains vary in size and\nformat, such as having different numbers of discrete and continuous columns, making it challenging\nto extract features from diverse tables using a unified neural network architecture [34].\nYet, it remains an open problem to effectively extract global representations from tables for LLMs to\nachieve comprehensive table understanding. To this end, we present a Cascaded Table Encoder that\njointly extracts knowledge from metadata and whole numerical entries.\nCascaded Table Encoder.\nConsider the approach of an experienced data scientist encountering a\ntable. They typically examine the structure of the table data, such as the table headers and distribution\nof feature columns, to understand the meaning of different cells based on their position, without\nfocusing too much on the numeric information of each cell. Following this biologically plausible\n4\napproach, we propose a novel cascading table encoder. It divides the information in the table data\ninto two main parts. The first part learns the metadata representation of the table, such as schema,\nindustry background, and the meanings of column names, which can help LLMs understand the global\ninformation of the table structure. The second part learns the numerical information representation of\nthe table, such as the distribution and trends of values in different columns, helping LLMs understand\nthe global information of the table numbers like human experts.\nWe consider the rows and columns of the table as elements of a set and learn the overall representation\nof the entire set. We use a modified set transformer [16] as the backbone of the table encoder. The set\ntransformer [16], originally designed for dealing with permutation invariant problems, aligns well\nwith the inherent structure of tabular data. We enhance it with an attention mechanism [29] that can\ncapture the interdependencies between different rows or columns of the table, enabling the model to\nunderstand the relations between different parts of the table data.\nThis encoder is pre-trained on ten thousand table datasets using a masked table modeling approach,\nsimilar to the masked language modeling used in BERT [5] but adapted to tabular data. The learned\ntable representation not only can be used for table understanding but also can enhance the predictive\nperformance of downstream classifiers.\nOur proposed method presents a significant step forward in the integration of tables, natural language,\nand commands into LLMs. It provides a comprehensive approach for extracting global representations\nfrom tables and enables LLMs to understand and manipulate.\n2.3\nChain-of-Command\nIn recognition of the fact that Large Language Models (LLMs) like GPT can struggle with numerical\nreasoning, prone to computational errors and hallucinations [11], our approach does not require them\nto operate and calculate within the tables in their latent space. Instead, we provide a series of pre-\npackaged function commands for LLMs to call upon. LLMs, understanding the global representation\nof the table and user input, generate a sequence of commands for the backend system to execute,\nresulting in a modified table. Compared to the SQL statements generated by text2SQL [8, 39, 18],\nthese command sequences are more easily examined and error-located by the backend parsing system,\nwhile SQL statements can be challenging to diagnose and correct for specific errors.\nHowever, user queries are often vague and complex, and we can only encapsulate and provide some\nbasic table operation commands. Teaching the LLM to deconstruct complex and vague queries is\ncrucial. For example, a user\u2019s query for a specified object column could be a synonym or translation\nof a column in the original table, or the user may only have a vague intent and cannot express the\ndemand clearly.\nThe Chain-of-thought [14, 33] approach emphasizes breaking down complex reasoning into a series\nof intermediate steps. We introduce the concept of Chain-of-command (CoC), an approach that\nenhances the chain-of-thought by providing a mechanism for step-by-step instructions associated\nwith these intermediate steps. For instance, when a user asks, \"Show me the five movies with the\nhighest profit margin,\" the LLM first checks if a profit margin column exists in the table. If not, it\ngenerates arithmetic instructions to calculate the profit margin using box office and cost data; next,\nit executes instructions to sort by profit margin in descending order and slice to select the top five\nmovies. When user queries are too vague, like \"Give me some numbers,\" the LLM might struggle to\ndecompose and could refuse execution, instead, it would ask the user for more specific intent.\nThe aim of the Chain-of-command is to enhance LLM\u2019s reasoning capabilities and robustness when\noperating table data. This approach involves translating user inputs into a sequence of intermediate\ncommand operations, enabling LLMs to manipulate tables more accurately and efficiently sym-\nbolically. The ability to manipulate symbolic instructions is particularly valuable for real-world\napplications involving complex and accurate interactions with historical data, such as record-keeping\nand data analysis in management environments.\nTo enhance the performance and stability of our approach, we constructed a substantial dataset of\ncommand chain instructions while fine-tuning LLMs to adapt to commands, and employed contextual\nlearning to provide prompts for multiple steps in the command chain sequence. A strong and accurate\ncommand chain process allows LLMs to better reason about table data and handle more complex\nscenarios.\n5\nThe Chain-of-command approach has three main advantages. First, it enables LLMs to execute\ncomplex table instructions accurately, thereby enhancing their multi-hop reasoning capabilities for\ntable operations. Second, by breaking down complex operations into a series of intermediate table\noperations, the chain-of-command method enhances the LLM\u2019s ability to handle complex multi-table\ninteractions. Lastly, it enables LLMs to refuse overly vague instructions and ask users for more\nspecific intent. This approach allows LLMs to handle edge cases and unexpected scenarios better,\nmaking it a promising method for real-world applications.\n2.4\nDomain Data Processing Pipeline\nDespite the broad knowledge and dialogue capabilities of large language models (LLMs) due to\nextensive pre-training on a diverse corpus, their performance often falls short in addressing the\nnuanced language styles and logic of specific industries. This is primarily due to the lack of exposure\nto proprietary domain data during their training phase. To mitigate this issue, we have developed an\nefficient domain data processing pipeline [3, 35].\nMotivated by the goal to streamline the fine-tuning process of LLMs with minimal computational\noverhead and accelerated model iteration, our pipeline is designed to harness the power of active\nlearning [26]. Through this, we curate a carefully selected set of fine-tuning examples from the\ndomain data, allowing LLMs to achieve superior fine-tuning results with a reduced number of\nexamples. This strategic utilization of resources expedites the model\u2019s learning process, thereby\nspeeding up its iteration.\nAdditionally, we have fortified the document retrieval capabilities of LLMs. We utilize technologies\nlike vector databases [30] and LangChain [15] to facilitate the retrieval of pertinent information from\na plethora of proprietary documents, further enriching the context that LLMs learn from.\nIn essence, our pipeline serves as a catalyst for the rapid and cost-effective adaptation of LLMs\nto the data needs of various specific industries. This pipeline not only addresses the challenges\nof industry-specific language styles and logic but also empowers LLMs to handle commands that\ninteract with tables, integrating the realms of natural language, tables, and commands.\n3\nEvaluation\n3.1\nCommands supported by TableGPT\nTo unleash the power of TableGPT, we have designed and supported a rich set of commands.\nFirstly, TableGPT enables natural language interaction with tables, empowering users to intuitively\nquery, filter, sort, and aggregate data using everyday language. It also facilitates tasks such as\ndata visualization and report generation, enhancing the interpretability and presentation of tabular\ninformation. Lastly, TableGPT facilitates automated decision-making processes, empowering users\nto make predictions, forecast trends, and estimate outcomes using table data and natural language\ninstructions.\nNote that when the intent of the user query is too vague, TableGPT will reject to generate commands\nand instead ask the user for more detailed intent. This is one of the benefits of chain-of-command, the\nability to think about the rationality of commands like a human expert, rather than a rigid command\ntranslator.\n3.2\nComparison with previous command-using LLMs\nSeveral existing solutions attempt to combine tables and language models, such as ChatExcel [28],\nSheetCopilot [17], and Data-Copilot [38]. These approaches typically rely on using prompts to\ninvoke pre-defined external commands through inference API of LLMs, such as OpenAI API2. In\ncontrast, TableGPT takes a different approach by fine-tuning LLM specifically for table-related tasks.\nThis key distinction allows us to harness the inherent capabilities of the LLM architecture while\ntailoring it to excel in table processing tasks. A detailed comparison of TableGPT with the previous\ncommand-using LLMs is shown in Table 1.\n2https://openai.com/blog/openai-api\n6\n3.3\nCase Study\nWe show some cases in Figure 2 - 8. More examples will be released soon.\n4\nConclusion\nWe present TableGPT, a large language model designed for table analysis, unifying tables, nature\nlanguage, and commands. It enables a variety of functions like answering questions, manipulating\ndata, visualizing information, generating analysis reports, and making predictions. Technically,\nTableGPT addresses several major challenges in developing a natural language-driven framework for\ntable data processing, including comprehensive table understanding, instruction chain generation,\nand domain-specific fine-tuning. We believe TableGPT has the potential to reshape the landscape of\ntabular data processing, accelerating the efficiency of table modeling and exploratory data analysis\n(EDA), and empowering various domains like finance, transportation, scientific research, etc.\nFigure 2: Cases of TableGPT.\n7\nFigure 3: Cases of TableGPT.\nFigure 4: Cases of TableGPT.\n8\nFigure 5: Cases of TableGPT.\nFigure 6: Cases of TableGPT.\n9\nFigure 7: Cases of TableGPT.\nFigure 8: Cases of TableGPT.\n10\nReferences\n[1] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A\nframework for self-supervised learning of speech representations, 2020.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[3] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong,\nand Junbo Zhao. Maybe only 0.5% data is needed: A preliminary exploration of low training\ndata instruction tuning, 2023.\n[4] Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo\nZhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al. Phoenix: Democratizing chatgpt across\nlanguages. arXiv preprint arXiv:2304.10453, 2023.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding, 2019.\n[6] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,\nWenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for\ndialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\n[7] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and\nDavid Sontag. Tabllm: Few-shot classification of tabular data with large language models. In\nInternational Conference on Artificial Intelligence and Statistics, pages 5549\u20135581. PMLR,\n2023.\n[8] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb:\nAugmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901,\n2023.\n[9] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning\nWu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating\nspeech, music, sound, and talking head. arXiv preprint arXiv:2304.12995, 2023.\n[10] IDEA-CCNL. Fengshenbang-lm. https://github.com/IDEA-CCNL/Fengshenbang-LM,\n2023.\n[11] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using\nlarge language models, 2023.\n[12] Baichuan Intelligence. Baichuan-7b. https://github.com/baichuan-inc/baichuan-7B,\n2023.\n[13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n[14] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\n35:22199\u201322213, 2022.\n[15] LangChain. Langchain. https://blog.langchain.dev/, 2022.\n[16] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh.\nSet transformer: A framework for attention-based permutation-invariant neural networks. In\nInternational conference on machine learning, pages 3744\u20133753. PMLR, 2019.\n[17] Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. Sheetcopilot: Bring-\ning software productivity to the next level through large language models. arXiv preprint\narXiv:2305.19308, 2023.\n11\n[18] Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang,\nWenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-\naware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023.\n[19] Liyao Li, Haobo Wang, Liangyu Zha, Qingyi Huang, Sai Wu, Gang Chen, and Junbo Zhao.\nLearning a data-driven policy network for pre-training automated feature engineering. In The\nEleventh International Conference on Learning Representations, 2022.\n[20] Guoshan Lu, Haobo Wang, Saisai Yang, Jing Yuan, Guozheng Yang, Cheng Zang, Gang Chen,\nand Junbo Zhao. Catch: Collaborative feature set search for automated feature engineering. In\nProceedings of the ACM Web Conference 2023, pages 1886\u20131896, 2023.\n[21] OpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.\n[22] OpenAI. Gpt-4 technical report, 2023.\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[24] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[26] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B. Gupta, Xiaojiang\nChen, and Xin Wang. A survey of deep active learning, 2021.\n[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[28] Peking University. Chatexcel. https://chatexcel.com/, 2023.\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[30] Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu Wang,\nXiangzhou Guo, Chengming Li, Xiaohai Xu, et al. Milvus: A purpose-built vector data\nmanagement system. In Proceedings of the 2021 International Conference on Management of\nData, pages 2614\u20132627, 2021.\n[31] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\ntions. arXiv preprint arXiv:2212.10560, 2022.\n[32] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly,\nZongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis,\nRob Clark, and Rif A. Saurous. Tacotron: Towards end-to-end speech synthesis, 2017.\n[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\n[34] Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao. Ct-\nbert: Learning better tabular representations through cross-table pre-training. arXiv preprint\narXiv:2307.04308, 2023.\n[35] Wentao Ye, Mingfeng Ou, Tianyi Li, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie Fu, Gang Chen,\nJunbo Zhao, et al. Assessing hidden risks of llms: An empirical study on robustness, consistency,\nand credibility. arXiv preprint arXiv:2305.10235, 2023.\n12\n[36] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,\nYifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning Representations, 2022.\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[38] Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. Data-copilot: Bridging\nbillions of data and humans with autonomous workflow. arXiv preprint arXiv:2306.07209,\n2023.\n[39] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.\n13\n"
  },
  {
    "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
    "link": "https://arxiv.org/pdf/2307.08581.pdf",
    "upvote": "26",
    "text": "BuboGPT: Enabling Visual Grounding\nin Multi-Modal LLMs\nYang Zhao\u2217, Zhijie Lin\u2217, Daquan Zhou, Zilong Huang, Jiashi Feng, Bingyi Kang\u2020\n{zhaoyang98, linzhijie11, daquanzhou, zilonghuang, jshfeng, bingyikang}\n@bytedance.com\n\u2217 Equal Contribution, \u2020 Project Lead\nAbstract\nLLMs have demonstrated remarkable abilities at interacting with humans through\nlanguage, especially with the usage of instruction-following data. Recent advance-\nments in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their\nabilities by incorporating multi-modal inputs, including image, video, and speech.\nDespite their effectiveness at generating precise and detailed language understand-\ning of the given modality signal, these LLMs give up the ability to ground specific\nparts of inputs, thus only constructing a coarse-grained mapping. However, explicit\nand informative correspondence between text and other modalities will not only\nimprove the user experience but also help to expand the application scenario of\nmulti-modal LLMs. Therefore, we propose BuboGPT, a multi-modal LLM with\nvisual grounding that can perform cross-modal interaction between vision, audio\nand language, providing fine-grained understanding of visual objects and other\ngiven modalities. As a result, BuboGPT is able to point out the specific location of\nan object in the image, when it is generating response or description for that object.\nOur contributions are two-fold: 1) An off-the-shelf visual grounding module based\non SAM that extracts entities in a sentence and find corresponding masks in the\nimage. 2) A two-stage training scheme and instruction dataset to endow joint\ntext-image-audio understanding. Our experiments show that BuboGPT achieves\nimpressive multi-modality understanding and visual grounding abilities during the\ninteraction with human. It performs consistently well when provided by arbitrary\nmodality combinations (either aligned or unaligned). Our code, model and dataset\nare available at https://bubo-gpt.github.io.\n1\nIntroduction\nThe large language models (LLMs) have made significant progress and demonstrated promising\nabilities in few-shot and zero-shot learning by leveraging instruct tuning [1] on carefully curated\ndatasets. To harness the potential of LLMs beyond just language, some recent studies [2, 3, 4, 5, 6, 7,\n8, 9, 10] successfully connect LLMs with more input signals (e.g., image, video, speech and audio),\nand build powerful multi-modal chatbots. However, these models often perform understanding\nwithout digging into the fine-grained relation between the visual objects and other given modalities.\nFor example, when an illustrative figure is given, a visually-enhanced LLM will generate a high-\nquality description with rich details, but in a black-box manner. Instead, an instructive teacher-bot is\ngoing to show its audience which part of the figure it is referring to and what is happening there. Such\nvisual grounding abilities are intriguing to LLMs but previously under-explored in the literature.\nIn this paper, we propose BuboGPT, the first attempt to incorporate visual grounding into LLMs\nby relating visual objects with other modalities. Moreover, it is able to perform joint multi-modal\nunderstanding and chatting for text, vision and audio, which is achieved by learning a shared\nrepresentation space that aligns well with pre-trained LLMs.\narXiv:2307.08581v1  [cs.CV]  17 Jul 2023\nBased on the image of a boy riding a bicycle, it\nappears that the source of the audio is the\nsound of a dog barking \u2026 creates a vivid and\nlively scene in the image.\nVicuna\nVisual \nGrounding\nLinear\nQ-Former\nFind the sounding \nsource.\nLinear\nQ-Former\nFigure 1: The overall framework of BuboGPT.\nTo this end, we first build an off-the-shelf visual grounding pipeline based on SAM [11] to explore\nthe fine-grained relation between different visual objects and modalities. The pipeline is composed of\nthree modules, namely, a tagging module, a grounding module and a entity-matching module. The\ntagging module is a pre-trained modal [12] that can generate multiple text tags/labels that are relevant\nto the input image. The SAM-based [11] grounding module [13] further localize the semantic mask\nor box on the image for each tag/label. Then, the entity-matching module leverages the reasoning\ncapabilities of LLMs to retrieve matched entities from tags and image descriptions. In this way, we\nconnect visual objects and other modalities by using language as a bridge.\nThen, to unlock the multi-modal understanding ability for arbitrarily combined inputs, we employ\na two-stage training scheme similar to Mini-GTP4 [2]. More specifically, we use ImageBind [14]\nas the audio encoder, BLIP-2 [15] as the vision encoder and Vicuna [16] as the LLM. In the first\nstage, we learn a Q-former to align vision or audio features with language on image or audio caption\ndatasets respectively. In the second stage, we perform multi-modal instruct tuning on a high-quality\ninstruction-following dataset. We observe that the construction of this dataset is crucial for the LLM\nto recognize whether a modality is provided and whether the input modalities are well matched with\neach other. Therefore, we devise a novel high-quality dataset, which is composed of four subsets: 1)\nvision instruction dataset; 2) audio instruction dataset; 3) sound localization dataset with positively\npaired image-audio examples; 4) image-audio captioning dataset with negative pairs. Note that by\nintroducing the negative image-audio pairs for semantic reasoning, the BuboGPT can learn better\nmulti-modal alignment and demonstrate stronger capabilities of joint understanding.\nOur experiments show that BuboGPT achieves impressive visual grounding abilities during multi-\nmodal chat, even when arbitrary combinations of multi-modal inputs are provided, whether matched\nor unmatched. We summarize our key contributions as follows:\n\u2022 We build a multi-modal LLM, BuboGPT for multi-modal understanding including image,\naudio and text by learning a common semantic space and further explore the fine-grained\nrelation between different visual objects and different modalities.\n\u2022 We construct a high-quality multi-modal instruction-tuning dataset including fine-grained\naudio descriptions and cross-modal sound localization, and introduce both positive and neg-\native image-audio pairs for semantic matching to facilitate the cross-modal understanding.\n2\nRelated Work\nPre-trained LLMs in Mutli-modal Learning. Due to the scaling up of training data and model\nsize, large language models [17, 18, 19, 16] have demonstrated remarkable abilities across various\nlinguistic tasks in a few-shot and zero-shot manner and also enabled conversational communication\nwith humans. To leverage the powerful linguistic abilities of LLMs, some methods [20, 21] propose to\nconnect different accessedation models for multi-modal tasks by using LLMs as a dispatch scheduler.\nBased on high-quality multi-modal instruction-following data, recent end-to-end methods [2, 3, 4,\n5, 6, 7, 8, 9, 10] have been introduced to extend LLMs for multi-modal learning as well. Some\nworks such as Mini-GPT4 [2], X-LLM [3] and Video-ChatGPT [10] propose to align the input\nfeatures of different modalities with pre-trained LLMs by learned visual encoder. Some works such\nas LLaMA-Adapter [5] and Otter [7] insert learnable cross-attention layers into the pre-trained LLMs\nii\nto incorporate multi-modalities knowledge. These prior methods mainly focus on tackling visual\ninputs (e.g. videos and images) [2, 5, 6, 4, 9, 7] or ignoring the fine-grained relation between the visual\nobjects and other given modalities [8, 3]. We further attempt to incorporate visual grounding into\nLLMs by relating visual objects with other modalities and propose to learn multi-modal alignment\nincluding image, audio and text in a common space.\nMulti-modal Instruction Tuning Dataset. To explore instruction tuning for multi-modal learning,\n[22] first introduces a multi-modal instruction tuning benchmark that is composed of 62 diverse\nmulti-modal tasks in a unified seq-to-seq format. Mini-GPT4 [2] curates an instruction following\ndataset by combining Conceptual Caption [23, 24], SBU [25] and LAION [26] with hand-designed\nprompt, while LLaVA [6] proposes to use GPT-4 [17] to generate more detailed captions to expand\nCOCO dataset [27]. Otter [7] further builds a multi-modal in-context tuning dataset to facilitate the\nin-context learning capabilities of multi-modal LLMs. Further, we build a high-quality instruction\ntuning dataset including fine-grained audio description and introduce the negative image-audio pairs\nfor semantic reasoning to enhance the reasoning capabilities of our model.\n3\nMethods\nThe overall framework of BuboGPT is presented in Figure 1. As the Figure shown, we perform joint\nmulti-modal understanding and chatting for text, vision and audio, which is achieved by learning a\nshared representation space that aligns well with pre-trained Vicuna [16]. We also build an off-the-\nshelf visual grounding pipeline to explore the fine-grained relation between different visual objects\nand modalities.\n3.1\nVisual Grounding Pipeline\nTo explore the relation between different visual objects and input modalities, we further build the\nvisual grounding pipeline, composed of a tagging module, a grounding module and a entity-matching\nmodule, as shown in Figure 2. Concretely, for a given image, we first use Recognize Anything\nModel (RAM) [12], a strong model based on Swin-transformer [28] for image tagging to generate\nrelevant candidate tags, denoted as {t1, t2, ..., tnt}, where ti is the i-th semantic tag and nt is the\nnumber of detected tags. We then connect the tags with comma to form the prompt \u201ct1, t2, ..., tnt\u201d\nand use the Grounding DINO [13], a open-set object detection model with referring textual queries to\nidentify the visual entities and the corresponding boxes relevant to the tags. Followed by Segment\nAnything Model (SAM) [11], the boxes are taken as prompt to get fine-grained semantic masks.\nWith the tagging and grounding module, we then obtain all the visual entities and the corre-\nsponding grounding information, denoted as {(e1, g1), (e2, g2), ..., (ene, gne)}, where ei, gi are\nseparately the i-th visual entities and grounding information (i.e. boxes and masks), ne is the\nnumber of entities.\nTo model the relation between different visual entities and input modal-\nities, we employ the text output to of our multi-modal LLM as the bridge and build a entity-\nmatching module based on GPT-4 to retrieve the matching pairs. To construct the prompt template\n\u201c<List>e1, e2, ..., ene</List>,<Text>to</Text>\u201d, we utilize the powerful LLM for reasoning and\nretrieve the matching pairs, which reflects the relation between visual entities and input modalities.\n3.2\nMulti-Modal LLM Training\nBuboGPT considers the interaction between three modalities, i.e., text, vision and audio. It aligns\na vision encoder and an audio encoder with the LLM with a Q-former for each modality. More\nspecifically, we utilize the visual encoder together with the pre-trained Q-Former in BLIP-2 [15]\nand audio encoder in ImageBind [14] for visual and audio perception. For joint understanding over\nmultiple modalities, we employ Vicuna as the LLM. We use a linear projection layer to connect\nthe modality Q-Former with the LLM. To effectively train such a model, we develop the following\ntwo-stage training scheme. The modality encoders and Vicuna model with be fixed throughout the\ntraining procedure.\nStage 1: Single-modal Pre-training\nSimilar to MiniGPT-4 [2], the first stage is designed to align\nthe output of the linear projection layer to the word embedding space of the LLM. This is achieved by\ntraining the modality Q-Former and linear projection layer on a large number of modality-text paired\ndata. For visual perception, we only train the projection layer for image captioning with the Q-Former\nfrom BLIP2 fixed. For audio understanding, we jointly train the Q-Former and the projection layer\niii\n['dog', 'sheepdog', \u2026, 'field', \n'corgi', 'grassy', 'brown dog', \n'grass', 'park']\nbrown | chase | sheepdog \n| corgi | dog | field | \u2026 | \ngrassy | park | run \nTagging\nModule\nGrounding\nModule\nMulti-modal\nLLM\nEntity-matching\nModule\nWhat is the\nanimal?\nA brown\ndog\nFigure 2: The pipeline of visual grounding that is composed of a tagging module, a grounding module\nand a entity-matching module.\n###Human: <Vision><ModalityHere></Vision> What is the image? ###Assistant:\n###Human: <Audio><ModalityHere></Audio> Pay attention to the audio and describe what\nyou notice. ###Assistant:\n###Human: <Vision><ModalityHere></Vision> <Audio><ModalityHere></Audio> Please\nfind the source that emits the given sound in this image. ###Assistant:\n###Human: <Vision><ModalityHere></Vision> <Audio><ModalityHere></Audio> Are the\naudio and image related to each other? What are they? ###Assistant:\nTable 1: Instruction-following prompt examples for various input sources.\nfor audio captioning. There will not be any prompt used for both settings, the model just take the\ncorresponding image or audio as input and predict the corresponding caption.\nStage 2: Multi-Modal Instruct Tuning\nThis stage aims to equip the multi-modal LLM with\nthe ability to understand human instructions such that it can generate proper responses based on\nthe given modality signal. To this end, we curate a high-quality multi-modal instruction-following\ndataset, which contains image-text, audio-text and image-audio-text pairs. To make the model\nadapt to arbitrary combination of input modalities, we design a general prompt as: ###Human:\n<Vision><ModalityHere></Vision> <Audio><ModalityHere></Audio> <instruction> ###Assis-\ntant:. <Vision></Vision> and <Audio></Audio> are special identifiers for image and audio input.\n<ModalityHere> is going to be replaced by a sequence of image or audio tokens before feeding into\nthe LLM. <instruction> is the human instruction related to the input sigals for the LLM to assist\non. We list a few examples for different combinations of input modalities in Tab. 1. We empirically\naccessed that when only positively paired image-audio data are included in this stage, the model\nalways assumes the image and audio are related to each other even though random sampoles are used\nat test time. Therefore, we manually create some negative pairs and asking the LLM to tell what\nare they respectively. The experiments show that introducing such negative paired data is able to\novercome this problem significantly. We leave the creation of datasets in the next section.\n4\nDatasets\n4.1\nPretraining Datasets\nFollowing MiniGPT-4 [2], we use a combined dataset of CC3M [23], CC12M [24], SBU [29]\nand LAION [26] to train the visual projection layer, resulting in a total of 130 million image-text\npairs. For audio, we mainly use the WaveCaps [30] dataset, which contains 403,050 audio clips\nwith average duration of 67.59 seconds and average caption length of 7.8 words. It combines four\niv\ndatasets including FreeSound (262,300) [31], BBC Sound Effects (31,201)1, SoundBible (1,231)2\nand AudioSet strongly-labelled subset (108,317) 3, and transform their raw-descriptions into captions\nwith ChatGPT.\n4.2\nInstruction-Tuning Datasets\n4.2.1\nImage-Text Dataset\nWe employ two previously published datasets for visual instruct tuning. The first one is released\nby MiniGPT-4, which contains 3,439 high-quality text-image pairs. The second one provided by\nLLaVA [6] is curated from 158K samples based on the COCO dataset, including three types of\ninstructions, i.e., converstaions (58K), detailed description (23K) and complex reasnoning (77K).\n4.2.2\nAudio-Text Dataset\nWhen it comes to the field of audio understanding, we also need to conduct the instruction-tuning\noperation on the audio Q-former. However, unlike vision-language understanding, a severe need still\nexists for high-quality and well-organized instruction-tuning datasets in this field. To this end, we\ngenerate a series of expressive and descriptive data to facilitate this process.\nSpecifically, we first investigate different kinds of existing audio caption datasets and select Clotho\n[32] as the original dataset to make the description extension. The reason can be explained in two\nfolds. On the one hand, it has a moderate and acceptable scale to act as an instruction-tuning dataset,\nand the semantic range of audio is large enough. On the other hand, every audio has five short captions\nfrom different annotators, covering various possible scenes related to the audio and increasing the\ndiversity of descriptions.\nAfter obtaining the original data, we need to rewrite the short captions into descriptive and imaginative\nparagraphs. Considering the extraordinary ability of GPT-4 in the field of few-shot learning, text\ngeneration, and complex reasoning, we utilize it to help us automatically assemble short captions\ninto long descriptions to mitigate the reliance on human annotation. The final description is expected\nto cover all the related original captions. For example, given the series of captions [\u201cA person is\nturning a map over and over.\u201d, \u201cA person is very carefully wrapping a gift for someone else.\u201d, \u201cA\nperson is very carefully wrapping a gift for someone else.\u201d, \u201cHe sighed as he turned the pages of\nthe book, stopping to scan the information.\u201d, \u201cPapers are being turned, stopped, then turned again,\nand someone is breathing.\u201d], the description paragraph is expected to be \u201cA person is repeatedly\nflipping some papers. They might be reading a book, flipping through a map, or wrapping presents.\nJudging from the repeated flipping sounds, they are concentrating on repeating this action.\u201d. We\ndesign a task-related prompt and construct some few-shot examples like this to promote the in-context\nreasoning process. As a result, we collect a novel dataset Clotho-Detail 4 for instruction-tuning in\naudio understanding, which contains 3938 items and the average length of descriptions is 52.70\nwords.\n4.2.3\nAudio-Image-Text Dataset\nPositive Set In order to further empower our model with the comprehensive ability of multi-modal\nreasoning, we apply a group of audio-image pairs to help the model to understand the correspondence\nbetween the audio and its source. Among the existing audio-vision datasets, VGGSS [33] turns out to\nbe a better choice in this process. It covers a wide range of sounding objects, and the audio only relates\nto a specific region in the corresponding image. Therefore, we retrieve all the data cases and use a\ngroup of fixed templates to wrap the corresponding class labels into natural sentence descriptions.\nAs a result, we generate a total of 5,158 <audio, image, text> pairs to act as the triple-modality\ninstruction tuning dataset 5.\n1https://sound-effects.bbcrewind.co.uk/\n2https://soundbible.com/\n3https://research.google.com/audioset/download_strong.html\n4https://huggingface.co/datasets/magicr/BuboGPT/blob/main/Clotho-detail-annotation.\njson\n5https://huggingface.co/datasets/magicr/BuboGPT/blob/main/vggss-instruction-tuning.\njson\nv\nNegative Set As discussed in the method section (Sec. 3.2), relying solely on the above dataset causes\nthe LLM fail to recognize irrelevant audio-image pairs. Therefore, we construct negative <audio,\nimage, text> pairs such that <text> gives independent descriptions for audio and image inputs. The\naudio is randomly sampled from the audio-text dataset presented in Sec. 4.2.2, while the image is\nrandomly sampled from the MiniGPT-4 dataset discussed in Sec. 4.2.2. The text is constructed by\nconcatenating the two captions that starts with \u201cThe image\u201d and \u201cThe audio\u201d.\n5\nExperiment Results\nIn this section, we aim to answer the following two questions: 1) whether our BuboGPT is able to\nprovide accurate and instructive visual grounding when the inputs contain images? 2) whether the\nmodal is able to perceive arbitrary combinations of modalities and generate proper responses.\nWe first consider using a single image as input for fine-grained visual understanding with ground-\ning. As shown in Fig. 3-7, the model can accurately associate textural words or phrases with image\nregions in various scenarios with different complexities. Then, when a single audio clip is provided\nfor audio understanding, BuboGPT gives informative descriptions covering nearly all acoustic parts\nincluded, even when some audio fragments are too short for humans to notice, see Fig. 8-13 for\ndetails. Next, we show that the model can perform sound localization with a matched audio-image\npair provided, which gives a perfect example for aligned audio-image understanding. As illustrated\nin Fig. 14-17, the model is going to generate an overall description for both input image and audio,\nthen point out which object in the image emits the sound after reasoning. It is worth noting that our\nmodel can give correct predictions when we provide different audio and keep the image unchanged.\nThis demonstrates that our model can understand both modalities comprehensively rather than gener-\nate answers with prior bias from a single modality. Moreover, we empirically accessed that if the\nmodel is only tuned with well-aligned image-audio data, it actually fails to discriminate when an\nirrelevant image and audio pair is provided, resulting in a non-factual response that is not consistent\nwith the given image or audio (Fig. 20). After introducing the negatively paired dataset as discussed\nin Sec. 4.2.3, the model can tell whether the image and audio are relevant to each other and generate\nhigh-quality response for arbitrary audio-image understanding, as evidenced by Fig. 18-19.\n6\nConclusion\nIn this report, we propose a multi-modal LLM, BuboGPT, which is capable of joint multi-modal\nunderstanding including image, audio and text, and perform more fine-grained understanding of\nmulti-modal inputs by exploring the relation between different visual objects and modalities. We\nalso build a high-quality instruction tuning dataset and the experiments show that BuboGPT achieves\nimpressive visual grounding abilities during multi-modal chat, even when arbitrary combinations of\nmulti-modal inputs are provided, whether matched or unmatched.\n7\nLimitations\nLanguage hallucination. Following prior works, our method is based on the pre-trained Vicuna\nmodel, which inherits the limitations of LLMs including generating non-existent knowledge or\nnon-factual information. The problem might be resolved by training with more high-quality data and\ndeveloping trustworthy LLMs.\nInadequate capacities of Grounding QA. Since the connection between grounding results and\ndifferent modalities is built upon text conversations without extra training, the capacities of QA\non specific objects remain limited. The model can be improved by introducing fine-grained visual\ngrounding datasets and considering the spatial location as extra input.\nvi\nPlease describe the image.\n\u2460 cat\n\u2461 sunglasses\n\u2462 float\n\u2463 swimming pool\nThe image shows a \u2460 cat wearing \u2461 sunglasses and \nsitting on an orange \u2462 float in a \u2463 swimming pool . \nThe cat appears to be relaxed and enjoying its time in the \nwater.\nIs this image funny and why?\nThis image can be considered funny because it depicts a \ncat , a typically indoor pet, wearing sunglasses and laying \nin a swimming pool , which is a place that is typically \nassociated with water activities for humans. It is an \nunexpected and unusual scene, which can make people \nchuckle. Additionally, the cat's expression and relaxed \nposture make it seem comfortable and content, adding to \nthe humor of the image.\nFeel free to chat with Bubo-GPT !\nFigure 3: Example 1 of fine-grained visual understanding with grounding.\nvii\nwhat is the image?\n\u2460 two\n\u2461 swans\n\u2462 water\n\u2463 beak\nThe image shows  \u2460 two black  \u2461 swans swimming in the  \u2462\nwater and forming a heart shape with their  \u2463 beak. The swans \nappear to be engaging in a playful or affectionate gesture as they \nmove around each other gracefully, displaying their elegant black \nfeathers and white beaks. It captures a moment of harmony and \nbeauty in the natural world, where these creatures appear to be \nenjoying each other's company.\nwhat is the meaning of the behaviour?\nThe behaviour of the black swans in the image is an act of \naffection, usually between a mated pair or between friends . It is a \nvisual display of bonding and mutual affection. Swans are social \nbirds and engage in various social behaviours to strengthen their \nrelationships. In this particular case, the two black swans are \nstanding next to each other and creating a heart shape by joining \ntheir beaks together. This behaviour might have been observed \nduring breeding season or as a way of communication and \nrecognition among mated pairs. Regardless of the specific \ncontext, the heart shape represents the bond between the two \nswans, and their act of creation reflects the beauty and harmony \nthat nature can bring to our lives.\nFeel free to chat with Bubo-GPT !\nFigure 4: Example 2 of fine-grained visual understanding with grounding.\nviii\nDescribe the image.\n\u2460 small kitten\n\u2461 tote bag\n\u2462 wooden table\n\u2463 strap\nThe image shows a  \u2460 small kitten in a white and brown color\npattern laying in the bottom of a tan and brown leather purse or \nsmall  \u2461 tote bag on a  \u2462 wooden table . The kitten appears to \nbe curled up and comfortably nestled in the bag. The bag itself \nhas a  \u2463 strap on the side that allows the bag to be held and \ncarried, likely making it an attractive and convenient option for \nthe cat to nap and rest.\nCan you tell me why the cat laying down inside the bag?\nThe kitten is laying down inside the purse or bag because it may \nfeel comfortable or safe inside the bag. Cats often curl up and \nsleep in small spaces, like bags or boxes , as it provides a sense \nof security and comfort. The purse, with its zippered opening, \nmay allow the cat to move around and get comfortable in the bag \nwithout being too confined or frightened by its size. Additionally, \nthe soft material of the purse may mimic the texture of a cat's \nbedding , making it a desirable sleeping spot.\nFeel free to chat with Bubo-GPT !\nAny other possible reason?\nAnother possible reason why the kitten is laying down inside the \nbag is that it might have found a cozy and sheltered place to nap. \nThe bag can act as an enclosed space that provides a sense of \nprivacy for the cat, allowing it to rest without fear of being \ndisturbed or startled. The soft interior of the bag might also be \ninviting for the kitten to rest and snuggle into. It is important to \nremember that cats love to curl up and hide , and a bag can offer \nthem a comfortable space to do so. Overall, there are various \nreasons why a kitten might choose to sleep inside a purse or bag, \nand the comfort and privacy it provides can be a significant \nfactor.\nFigure 5: Example 3 of of fine-grained visual understanding with grounding.\nix\nwhat is the image?\n\u2460 mouth\n\u2461 chimpanzee\n\u2462 grass\n\u2463 woman\nThe image shows a  \u2463 woman with a black fur and white hair \nsitting on the  \u2462 grass , holding the back of a  \u2461 chimpanzee. The \nwoman is wearing a white shirt and is smiling. The chimpanzee is \nreaching its  \u2460 mouth towards the woman's mouth and the \nwoman is making a kissing sound with her mouth open.\nWhat does the image suggest?\nThe image suggests that humans and animals can have positive \nrelationships and enjoy each other's company. It shows a woman \nplaying with a chimpanzee , suggesting that they are engaging in \na form of play or bonding activity. The image may suggest that it \nis possible for humans and animals to communicate and interact \nwith each other in a friendly manner. \nFeel free to chat with Bubo-GPT !\nFigure 6: Example 4 of of fine-grained visual understanding with grounding.\nx\nwhat is the image?\n\u2460 frog\n\u2461 banjo\n\u2462 moss-covered tree stump\nThe image is a small  \u2460 frog holding a  \u2461 banjo , with its legs \nspread out on either side of the banjo. The frog is sitting on top of \na  \u2462 moss-covered tree stump.\nAnything interesting?\nYes, there are several interesting features in the image. The first \nis the frog's intricate posing on the instrument. The frog appears \nto be playing the instrument with its legs , which adds an \nelement of creativity and playfulness to the image. The second \ninteresting aspect is the forest setting, which adds a sense of \nnature and serenity to the image. Finally, the mossy ground adds \na touch of whimsy and makes the overall scene appear more \nrustic and charming.\nFeel free to chat with Bubo-GPT !\nFigure 7: Example 5 of of fine-grained visual understanding with grounding.\nxi\nPlease describe this audio.\nIn this audio, a bell tower chimes in the background, \ncreating a serene atmosphere, blending with the \nsounds of a breeze gently blowing through trees and \nbirds singing in the distance . The combination of these \nelements provides a calming ambiance with a touch of \nnatural harmony.\nFeel free to chat with Bubo-GPT !\nAudio 1: chime, bird, breeze\nFigure 8: Example 1 of audio understanding. Audio 1 can be accessed via this link.\nPlease describe this audio.\nThe audio features a clear, melodic tune that resonates \nwithin the scene. The performance sounds as if it is \nhappening in a spacious area, with the musician \nplaying their instrument and creating a soothing \natmosphere. The melody is made up of a series of \nchords, with different notes being played in unison. \nThis audio captures the essence of a person creating \nmusic in an unspecified setting.\nFeel free to chat with Bubo-GPT !\nAudio 2: music, box, melody \nFigure 9: Example 2 of audio understanding. Audio 2 can be accessed via this link.\nxii\nPlease describe this audio.\nThe audio depicts a lively atmosphere in a restaurant \nor gathering, where a group of people engage in \nconversations. Amidst the chatter, one individual \nintermittently joins the conversation. The multitude of \nvoices creates a bustling, dynamic atmosphere, \ncapturing the essence of social interaction and \ncamaraderie.\nFeel free to chat with Bubo-GPT !\nAudio 3: people talking\nFigure 10: Example 3 of audio understanding. Audio 3 can be accessed via this link.\nPlease describe this audio.\nThe audio captures the sound of someone typing \nrapidly and energetically on a keyboard, creating a \nrhythmic pattern of clicks and keyboard sounds. This \nintense activity could be associated with working on a \ncomputer or writing a long-form document.\nFeel free to chat with Bubo-GPT !\nAudio 4: keyboard typing\nCould this audio come from a \nswimming pool?\nNo, the audio does not suggest the sound of swimming \nor water. The keyboard clicking and typing sounds \ndominate the scene, as they would in an office or work \nenvironment.\nFigure 11: Example 4 of audio understanding. Audio 4 can be accessed via this link.\nxiii\nPlease describe this audio.\nIn the audio, rain is the primary focus, with heavy rain \nfalling down and hitting the ground with force. This \ncreates a vivid and immersive atmosphere of a stormy \nday, with rain pouring down and creating a dynamic \nsoundscape. Additionally, there may be some distant \nsounds of traffic or vehicles passing by, adding further \ncontext to the ambiance.\nFeel free to chat with Bubo-GPT !\nAudio 5: heavy rain\nIf I\u2019m in the scene corresponding to \nthe audio, what should I do first?\nBased on the audio, it appears to be a heavy rainstorm. \nIf you\u2019re in the scene, you might first take cover from \nthe rain, potentially by seeking shelter under a roof or \nan overhang, or by running for cover. Alternatively, you \nmight simply enjoy the sound of the rain and take in \nthe experience.\nFigure 12: Example 5 of audio understanding. Audio 5 can be accessed via this link.\nxiv\nPlease describe this audio.\nThe audio features a cacophony of loud crashes and the \nsound of glass shattering, suggesting a situation \ninvolving multiple instances of breaking glass. The \nsound is consistent with what could be produced by \nbreaking glass objects, such as breaking wine glasses, a \ncar back window, or breaking glass bottles. It could \nalso represent the impact of breaking a pane of glass \nwhen a large object strikes it, producing a loud, jarring \nnoise.\nFeel free to chat with Bubo-GPT !\nAudio 6: breaking glasses\nDoes this sound portend \nany potential danger?\nThe audio doesn\u2019t explicitly convey any immediate \ndanger, but the sound of breaking glass can be \nunsettling and potentially alarming. The shattering \ncould be the result of an accident or intentional act, and \nthe level of danger would depend on the situation. The \nsound of breaking glass can also be associated with \ncertain types of events like explosions or breaking \nwindows during a storm. In any case, it\u2019s important to \npay attention and take appropriate precautions.\nFigure 13: Example 6 of audio understanding. Audio 6 can be accessed via this link.\nxv\nTry to find the sounding source in the image according to the \ngiven audio and generate a detailed description about it.\nBased on the provided image of a \u2460 boy riding a \u2461 bicycle , it \nappears that the source of the audio is the sound of a \u2462 dog\nbarking. The boy appears to be on a \u2463 dirt road , likely \ntraveling through a wooded area, as the tree branches are visible \nin the background. The dog, which is running alongside the boy's \nbicycle , is barking in an excited manner, suggesting that it may be \nchasing after the boy or simply running alongside him. This \ndynamic duo's adventurous journey through nature creates a vivid \nand lively scene in the image.\nFeel free to chat with Bubo-GPT !\nAudio 7: dog barking\n\u2460 boy\n\u2461 bicycle\n\u2462 dog\n\u2463 dirt road\nFigure 14: Example 1 of aligned audio-image understanding. Audio 7 can be accessed via this link.\nxvi\nTry to find the sounding source in the image according to the \ngiven audio and generate a detailed description about it.\nThe sound heard in the image appears to be a \u2460 bicycle bell, \nwhich is the device located near the right rear wheel of the bike , \nand is associated with the action of riding the bicycle, particularly \nwhen the rider accelerates or slows down. The  \u2461 person on the \nbicycle is wearing a blue shirt and is either wearing \u2462 a hat or \nhas one on their head.\nFeel free to chat with Bubo-GPT !\nAudio 8: bicycle bell\n\u2460 bicycle\n\u2461 person on the bicycle\n\u2462 a hat\nFigure 15: Example 2 of aligned audio-image understanding. Audio 8 can be accessed via this link.\nxvii\nTry to find the sounding source in the image according to the \ngiven audio and generate a detailed description about it.\nThe sound in the image can be attributed to a  \u2460 hair dryer . \nThe  \u2461 woman in the picture is holding a hair dryer in her  \u2462\nhand and using it to dry her \u2463 cat 's fur . The blowing air from \nthe hair dryer can be heard, providing an auditory representation \nof the activity taking place in the image.\nFeel free to chat with Bubo-GPT !\nAudio 9: hair dryer \n\u2460 hair dryer\n\u2461 woman\n\u2462 hand\n\u2463 cat\nFigure 16: Example 3 of aligned audio-image understanding. Audio 9 can be accessed via this link.\nxviii\nTry to find the sounding source in the image according to the \ngiven audio and generate a detailed description about it.\nThe audio in the image is produced by a  \u2460 cat hissing at the  \n\u2461 person , who is either holding or near the cat. This can be \nidentified through the sound of the cat's loud , agitated hissing as \nwell as the context of the person interacting with the cat.\nFeel free to chat with Bubo-GPT !\nAudio 10: cat meowing\n\u2460 cat\n\u2461 person\nFigure 17: Example 4 of aligned audio-image understanding. Audio 10 can be accessed via this link.\nxix\nPlease tell me a story based on the given audio and image.\nIn this audio, a clock strikes hour after hour, creating a haunting \nand mournful sound. The sound echoes through the forest, \nwafting through the trees and into the distance. The image depicts \na  \u2460 mink perched on a mossy \u2461 tree trunk. The mink stares \nintently into the distance, possibly listening to the toll of the clock, \nrepresenting the passing of time. This atmosphere might suggest a \nsense of solitude, introspection, or reminiscence.\nFeel free to chat with Bubo-GPT !\nAudio 11: continual bell chime\n\u2460 mink\n\u2461 tree trunk\nFigure 18: Example 1 of arbitrary audio-image understanding. Audio 11 can be accessed via this\nlink.\nxx\nPlease tell me a story based on the given audio and image.\nIn this audio, there is a group of kids playing on the \u2460 beach\nduring \u2461 sunset , enjoying the beautiful view and the gentle \nbreeze. They can be seen running, playing, and laughing together, \npossibly creating cheerful and light-hearted memories that could \nlast a lifetime. The serene environment, characterized by soothing \nsounds of seagulls and the \u2462 gentle waves , provides a soothing \nbackdrop to the lively and innocent activities.\nFeel free to chat with Bubo-GPT !\nAudio 12: children playing\n\u2460 beach\n\u2461 sunset\n\u2462 gentle waves\nFigure 19: Example 2 of arbitrary audio-image understanding. Audio 12 can be accessed via this\nlink.\nxxi\nPlease tell me a story based on the given audio and image.\nIn the image, we see a small brown squirrel perched on \na branch of a tree in the forest . The sound of wind rustling \nthrough the leaves provides the background ambiance. \nSuddenly, a bird starts chirping and singing a melody. The \nchirping gradually becomes louder and more persistent, \ncreating a harmonious and peaceful atmosphere. As the bird \nsings, the squirrel starts moving around on the branch. Its ears \nperk up, and its tail is alert. Then, the squirrel suddenly stops \nand listens intently, its eyes focused on the bird's melody. The \nbird's song reaches its peak and then gradually fades away, \nleaving the forest silent. The squirrel remains on the branch, \nlooking out into the woods, as if contemplating the beauty of \nnature and the solitude of its surroundings. The contrast \nbetween the peaceful bird song and the serene silence of the \nforest creates a captivating scene. This combination of the \nimage and audio represents a moment of stillness and awe as \nthe squirrel and bird share the beauty of their respective \nexperiences in the forest.\nFeel free to chat with Bubo-GPT !\nAudio 11: continual bell chime\nFigure 20: Failure case of arbitrary audio-image understanding without using negative audio-image\npairs.\nxxii\nReferences\n[1] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652,\n2021. i\n[2] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. i,\nii, iii, iv\n[3] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm:\nBootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv\npreprint arXiv:2305.04160, 2023. i, ii, iii\n[4] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\nfor video understanding. arXiv preprint arXiv:2306.02858, 2023. i, ii, iii\n[5] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint\narXiv:2303.16199, 2023. i, ii, iii\n[6] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023. i, ii, iii, v\n[7] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. i, ii, iii\n[8] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-\nfollow them all. arXiv preprint arXiv:2305.16355, 2023. i, ii, iii\n[9] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu\nWei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207,\n2023. i, ii, iii\n[10] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\ndetailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424,\n2023. i, ii\n[11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023. ii, iii\n[12] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong\nLuo, Yaqian Li, Shilong Liu, et al. Recognize anything: A strong image tagging model. arXiv preprint\narXiv:2306.03514, 2023. ii, iii\n[13] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023. ii, iii\n[14] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 15180\u201315190, 2023. ii, iii\n[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. ii, iii\n[16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. ii, iii\n[17] OpenAI. Gpt-4 technical report, 2023. ii, iii\n[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023. ii\n[19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022. ii\n[20] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. ii\n[21] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. ii\nxxiii\n[22] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via\ninstruction tuning. arXiv preprint arXiv:2212.10773, 2022. iii\n[23] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565,\n2018. iii, iv\n[24] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021. iii, iv\n[25] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned\nphotographs. Advances in neural information processing systems, 24, 2011. iii\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\nmillion image-text pairs. arXiv preprint arXiv:2111.02114, 2021. iii, iv\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740\u2013755. Springer, 2014. iii\n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 10012\u201310022, 2021. iii\n[29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022. iv\n[30] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley,\nYuexian Zou, and Wenwu Wang. Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset\nfor audio-language multimodal research. arXiv preprint arXiv:2303.17395, 2023. iv\n[31] Frederic Font, Gerard Roma, and Xavier Serra. Freesound technical demo. In Proceedings of the 21st\nACM international conference on Multimedia, pages 411\u2013412, 2013. v\n[32] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. ICASSP\n2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n736\u2013740, 2019. v\n[33] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman.\nLocalizing visual sounds the hard way. 2021 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 16862\u201316871, 2021. v\nxxiv\n"
  },
  {
    "title": "AlpaGasus: Training A Better Alpaca with Fewer Data",
    "link": "https://arxiv.org/pdf/2307.08701.pdf",
    "upvote": "21",
    "text": "Published as a conference paper at ICLR 2024\nALPAGASUS: TRAINING A BETTER ALPACA WITH\nFEWER DATA\nLichang Chen\u2217\u2020, Shiyang Li \u2217\u2021, Jun Yan\u266f, Hai Wang \u2021, Kalpa Gunaratna\u2021, Vikas Yadav\u2021,\nZheng Tang\u2021, Vijay Srinivasan\u2021, Tianyi Zhou\u2020, Heng Huang\u2020, Hongxia Jin\u2021\n\u2020 University of Maryland, College Park \u2021 Samsung Research America \u266f University of Southern California\n{bobchen, tianyi, heng}@umd.edu\n{shiyang.li, h.wang2, k.gunaratna, vikas.y, zheng.tang,\nv.srinivasan, hongxia.jin}@samsung.com\nyanjun@usc.edu\nABSTRACT\nLarge language models (LLMs) strengthen instruction-following capability through\ninstruction-finetuning (IFT) on supervised instruction/response data. However,\nwidely used IFT datasets (e.g., ALPACA\u2019s 52k data) surprisingly contain many low-\nquality instances with incorrect or irrelevant responses, which are misleading and\ndetrimental to IFT. In this paper, we propose a simple and effective data selection\nstrategy that automatically identifies and filters out low-quality data using a strong\nLLM (e.g., ChatGPT). To this end, we introduce ALPAGASUS, which is finetuned\non only 9k high-quality data filtered from the 52k ALPACA data. ALPAGASUS\nsignificantly outperforms the original ALPACA as evaluated by GPT-4 on multiple\ntest sets and the controlled human evaluation. Its 13B variant matches > 90%\nperformance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data)\non test tasks. It also provides 5.7x faster training, reducing the training time\nfor a 7B variant from 80 minutes (for ALPACA) to 14 minutes 1. Moreover, the\nexperiments prove the efficacy of our method across diverse datasets, base models,\nand LLM filters. Overall, ALPAGASUS demonstrates a novel data-centric IFT\nparadigm that can be generally applied to instruction-tuning data, leading to faster\ntraining and better instruction-following models. Our project page is available at:\nhttps://lichang-chen.github.io/AlpaGasus/.\n1\nINTRODUCTION\nInstruction fine-tuning (IFT) (Longpre et al., 2023) has been recently applied as an essential con-\ntinual training stage for pre-trained large language models (LLMs) to achieve instruction-following\ncapability (Ouyang et al., 2022b; Chen et al., 2023b), which is often attributed to aligning the models\u2019\nbehavior with a diverse set of human instructions and responses (Taori et al., 2023; Askell et al.,\n2021). The recent series of open-sourced instruction-tuned models (Taori et al., 2023; Xu et al., 2023)\nreveal that the alignment of better IFT data could result in better instruction-following skills. For\nexample, GPT-4-LLM (Peng et al., 2023) (with GPT-4 (OpenAI, 2023b) as its teacher) exhibits better\nreasoning and math ability than ALPACA (Taori et al., 2023) (with Text-davinci-003 as its teacher),\nthough they share the same base model LLaMA (Touvron et al., 2023), demonstrating the importance\nof data quality.\nAlthough stronger teachers can usually bring further improvement by providing better IFT data, their\nresponses inevitably include incorrect or irrelevant answers to the corresponding instructions (see\nexamples in Fig. 2), which can be misleading or detrimental to IFT. Moreover, these data also increase\nunnecessary training costs. Alpaca-cleaned2 is the pioneer of filtering bad data in ALPACA dataset\nthough it requires humans fully involved in examining and filtering the data. Nonetheless, how to\nautomatically filter out poor-quality data from IFT datasets has not been investigated yet. A primary\n*Equal Contribution. This work was done when Lichang Chen and Jun Yan interned at Samsung Research\nAmerica.\n1We apply IFT for the same number of epochs as ALPACA(7B) but on fewer data, using 4\u00d7NVIDIA A100\n(80GB) GPUs and following the original ALPACA setting and hyperparameters.\n2https://github.com/gururise/AlpacaDataCleaned/\n1\narXiv:2307.08701v5  [cs.CL]  13 Feb 2024\nPublished as a conference paper at ICLR 2024\nbottleneck is that rating the data quality usually requires expensive human labor but still may not be\naccurate for IFT because stronger teachers are more powerful in generating eloquent but incorrect\nresponses that are more subtle to detect by humans. When considering datasets crafted by humans,\nsuch as the Dolly dataset (Dolly, 2023), assessing quality becomes even more intricate, given that\nresponses stem from seasoned writers.\nThis paper aims to bridge the gap by proposing a novel data-filtering strategy for IFT that is efficient,\nautomatic, and accurate. Specifically, we design a prompt applied to a powerful LLM (e.g., ChatGPT)\nfor evaluating the quality of each (instruction, input, response) tuple and then filter out the ones with\nscores lower than a threshold. By applying this filter to the 52k data used to train ALPACA, we find\nthat a majority of the data suffer from low-quality issues. Using the LLM filter, IFT on a much smaller\nbut carefully filtered subset of 9k data produces a much better model, i.e., ALPAGASUS, than the\noriginal ALPACA, as shown in Fig. 1, following exactly the same training configuration of ALPACA.\nThis also reduces the training time from 80 minutes to merely 14 minutes on 4\u00d7 NVIDIA A100\n(80GB) GPUs. Moreover, we validate the versatility of our method, demonstrating its effectiveness on\na range of datasets(e.g., Dolly, Alpaca, GPT4LLM), base models(e.g., LLaMA-1 and LLaMA-2), and\nLLM filters(e.g., ChatGPT and Claude-2). This discovery is inspiring, as it shows that the data quality\nin IFT can outweigh the quantity. In addition, this shift towards prioritizing data quality presents a\nnew and more efficient paradigm that can generally improve the fine-tuning of LLMs.\n0.7\n0.85\n1\n1.15\n1.3\n3k\n6k\n9k\n39k\n52k\nAlpaGasus Wining Score\nTraining Data Size\nVicuna\nKoala\nWizardLM\nSelf-Instruct\nBest Size\nFour Testsets:\nFigure 1:\nPerformance of ALPAGASUS on\nfour test sets when increasing its finetuning data,\nwhere the winning score is #Win\u2212#Lose\n#Testset\n+ 1 with\n#Testset = #Win + #Tie + #Lose to be the test set\nsize and #Win/#Tie/#Lose to be the number of sam-\nples on which ALPAGASUS wins/ties/loses com-\npared to ALPACA 52K.\nOur experiments include comprehensive eval-\nuations for our ALPAGASUS, incorporating\nfree-form instruction evaluation, various bench-\nmarks, and human studies. We select four dif-\nferent human-instruction test sets for evaluat-\ning instruction-following capability, including\nthe ones used by WizardLM (Xu et al., 2023),\nVicuna (Chiang et al., 2023), Koala (Geng\net al., 2023), and Self-Instruct (Wang et al.,\n2022). Given the notable advantages that GPT-\n4 judge could match with both the controlled\nand crowdsourced human preferences (> 80%\nagreement) (Zheng et al., 2023), we employ\nGPT-4 as our judge for the major evaluations. In\nthe 7B and 13B model comparisons, ALPAGA-\nSUS performs significantly better than ALPACA\non all four test sets. To address potential con-\ncerns regarding biases in model-based evalua-\ntions, we conduct human studies and benchmark\nevaluations, both of which corroborate the su-\nperiority of our model compared to baseline counterparts. Furthermore, we present a fine-grained\nevaluation of ALPAGASUS on individual tasks including Generic, Roleplay, Knowledge, and Com-\nmonsense from the Vicuna test set. The results indicate ALPAGASUS exhibits advantages on a\nmajority of the tasks.\nTo sum up, our data-filtering approach exhibits significant benefits in terms of scalability and\nautomation. We also demonstrate that prudent management of training data quality can lead to\nsubstantial performance improvement and computation savings of IFT. In addition, our data selection\nand evaluation strategies can generalize to other instruction finetuning datasets and LLMs, thereby\npaving the way for a promising new research trajectory aimed at pragmatic LLM deployment.\n2\nMETHODOLOGY\n2.1\nOVERVIEW\nUnlike the recent work (Zhou et al., 2023), which relies on human labor to curate 1k high-quality\ninstruction data that leads to a better finetuned model, we aim to avoid the expensive and time-\nconsuming human annotations. Hence, we exploit the potential of strong LLMs to be auto-graders of\nthe training data and then filter out the data with lower scores.\nIn particular, we prompt a strong API LLM, i.e., ChatGPT, to produce a score for each triplet\nof (instruction, input, response). The prompt is given in Fig. 3, where \u201cdimension\u201d denotes a\n2\nPublished as a conference paper at ICLR 2024\nChatGPT-based\nResponse Quality Evaluator\n...\nInstruction: Classify the item as either \nanimal or vegetable.\nInput: Banana\nResponse: Animal: No, it's a vegetable.\nInstruction: Rewrite the following \nsentence omitting the pronouns. \nInput: She told us she was busy.\nResponse: Told busy.\nInstruction: Translate the phrase \u201dBonne\nchance\u201d into English\nResponse: Good Luck.\n...\nScore: 2.0: The response partially follows \nthe instruction by omitting the pronouns, \nbut it does not provide a complete and \ncoherent sentence. The response lacks a \nsubject and a verb, making it difficult to \nunderstand the intended meaning. The \ncorrect response would be \u201cTold us busy\u201d\nScore 5.0: The AI assistant provided a \ncorrect and accurate response to the \ngiven instruction and input. \\\"Bonne \nchance\\\" is a French phrase that translates \nto \\\"Good luck\\\" in English, and the AI \nassistant provided the correct translation.\nScore 2.0: The AI assistant's response is \nincorrect. A banana is a fruit, which is a \ntype of plant-based food, making it a \nvegetable\nDiscarded X\nDiscarded X\nIncluded \u2705\n\u2713 Less Data\n\u2713 Faster Training\n\u2713 Stronger Performance\nTraining\nIncluded \u2705\nIncluded \u2705\nIncluded \u2705\nTraining\n...\n...\nFigure 2: The fine-tuning pipeline of ALPAGASUS. We prompt ChatGPT as our auto-grader to score\neach training triplet on a scale of 0 to 5. We then use the exact same instruction fine-tuning script of\nALPACA to train ALPAGASUS on the filtered data with scores higher than a threshold.\nSystem Prompt:\nWe would like to request your feedback on the performance of AI assistant in response to the instruction \nand the given input displayed following.\nInstruction: [Instruction]\nInput: [Input]\nResponse: [Response]\nUser Prompt:\nPlease rate according to the [dimension] of the response to the instruction and the input. Each assistant \nreceives a score on a scale of 0 to 5, where a higher score indicates higher level of the [dimension]. Please \nfirst output a single line containing the value indicating the scores. In the subsequent line, please provide a \ncomprehensive explanation of your evaluation, avoiding any potential bias.\nFigure 3: Prompt pG to ChatGPT for rating and filtering training data in Eq. (1).\nuser-preferred property such as helpfulness and accuracy. We then only select the triplets with scores\nhigher than a certain threshold to fine-tune a LLaMA-series model following an existing IFT pipeline.\nFig. 2 illustrates the data selection and training pipeline.\n2.2\nDATA RATING AND FILTERING\nGiven an IFT dataset V of triplets x =(instruction, input, response) with x \u2208 V and an open-sourced\nLLM \u03b8 (e.g., LLaMA), let \u03b8V denote the finetuned \u03b8 on V , our overarching goal is to select a subset\nS \u2282 V such that IFT on S results in a better model \u03b8S than \u03b8V .\nIn order to select S from V , we prompt an API LLM G(\u00b7) (e.g., ChatGPT3) as an auto-grader rating\neach sample x \u2208 V by a score G(x, pG) wherein pG is the rating prompt in Fig. 3. We then select xi\nwhose score is above a certain threshold \u03c4, i.e.,\nS \u225c {x \u2208 V : G(x, pG) \u2265 \u03c4}.\n(1)\nWe achieve \u03b8S by finetuning \u03b8 on S using an existing IFT framework.\n3We also use claude-2 as our response quality evaluator, which can be found in Appendix A.2\n3\nPublished as a conference paper at ICLR 2024\n2.3\nALPAGASUS: 9K TRAINING DATA FILTERED FROM ALPACA\nFor \u201cdimension\u201d in the rating prompt pG shown in Fig. 3, given that \u201caccuracy\u201d closely aligns with hu-\nman expectations of LLMs\u2019 responses, we designate \u201caccuracy\u201d as the dimension for rating purposes.4\nCorrespondingly, we establish \u03c4 in Eq. (1) as an accuracy threshold for the subsequent experiments.\nThe distribution of scores in relation to the 52k Alpaca dataset is presented in Fig. 4.\n172\n1550\n10811\n30240\n9218\n11\n0\n8000\n16000\n24000\n32000\n< 3\n3\n3.5\n4\n4.5\n5\nScore Distribution\nCount\nFigure 4: Histogram of Scores (Al-\npaca Dataset).\nIn particular, we choose the threshold \u03c4 = 4.5 according to\nthe score histogram. For the ALPACA dataset V with 52,002\nsamples, this filtering criterion leads to a subset S of 9,229\nsamples 5.\n3\nEXPERIMENTAL SETUP\n3.1\nFREE-FORM INSTRUCTION EVALUATION\nMost instruction-tuned models are evaluated on one test set that\nmight not cover sufficient diverse instructions and thus leads\nto a risk of biased evaluation (Chia et al., 2023). To conduct\na holistic evaluation of ALPAGASUS, we curate our test sets\nfrom Self-instruct (Wang et al., 2022), Vicuna (Chiang et al.,\n2023), WizardLM (Xu et al., 2023), and Koala (Geng et al., 2023), which together can cover more\ntypes of instructions and reduce the evaluation bias. Details of these four test sets are provided in\nTable 1.\n3.2\nBASELINE MODELS\nTest Set\n# Samples\nCategory\nKoala\n180\nVicuna\n80\n\u2713\nWizardLM\n218\n\u2713\nSelf-Instruct\n252\nTable 1: Four test sets used in this paper.\nWe compare our ALPAGASUS with the following four\nrecent LLMs.\nALPACA\n(Taori et al., 2023) is an open-sourced model\ndeveloped by Stanford University through IFT of LLaMA\non a training dataset of 52,002 (instruction, input, re-\nsponse) samples with the responses generated by Text-\nDavinci-003 (teacher).\nTEXT-DAVINCI-003\nis an OpenAI LLM trained with an\nincreased emphasis on contextual understanding and response accuracy. Its proficiency in capturing\ncomplex linguistic patterns makes it a powerful teacher LLM for generating high-quality training\ndata for finetuning LLMs such as ALPACA.\nCHATGPT\n(OpenAI, 2023a) is an AI chatbot finetuned via reinforcement learning with human\nfeedback (RLHF). It exhibits exceptional capability across a wide range of tasks and might be the\nmost popular chatbot recently. Hence, it would be interesting to study to what extent ALPAGASUS\ncan match its performance.\nCLAUDE\n(Bai et al., 2022) is an AI chatbot developed by Anthropic. It was finetuned by RLHF to\nalign with humans\u2019 preference on three dimensions, i.e., helpful, honest, and harmless. We use Claude-\nv1.1 for comparison, which is comparable to ChatGPT on the AlpacaEval (Li et al., 2023).\n3.3\nEVALUATION METRICS\nThe evaluation of the instruction-following capability of LLMs is usually challenging due to the\nexistence of multiple eligible responses to one instruction and the difficulty of reproducing human\nevaluations. In light of the recent advancements in automated evaluation (Dubois et al., 2023; Zheng\net al., 2023; Chiang et al., 2023), which offer superior scalability and explainability than human\nstudies, we also apply an API LLM J(\u00b7) (e.g., GPT-4) as the judge to evaluate \u03b8S and compare it with\n\u03b8V . In particular, we apply J(\u00b7) to compare the responses of \u03b8S and \u03b8V to each instruction z drawn\nfrom a test set D. Let F(z; \u03b8V ) and F(z; \u03b8S) denote the two models\u2019 responses to instruction z \u2208 D,\n4We defer the experiment of other dimensions, e.g., helpfulness, to the Appendix A.5.\n552k denotes 52002 samples from the original Alpaca training set and 9k represents 9229 data samples.\n(either randomly sampled or filtered in our experiments)\n4\nPublished as a conference paper at ICLR 2024\nthe judge outputs a score for each response and we aim to achieve a higher score on \u03b8S, i.e.,\nJ(F(z; \u03b8S)) \u2265 J(F(z; \u03b8V ))\n(2)\nfor most z \u2208 D. In our experiments, we include both models\u2019 responses in the input to the judge\n(e.g., GPT-4), followed by an instruction to the judge, which aims to rate the responses with a score\nbetween 1 and 10. Details of the input and prompt to the judge can be found in Appendix C6\nSince there exists position bias within LLM judges, which refers to a phenomenon where LLM judges\nhave tendencies to prefer specific positions over others (Wang et al., 2018; Ko et al., 2020; Wang\net al., 2023), to mitigate it, we try both orders (i.e., placing ALPAGASUS\u2019s response before/after the\nbaseline model\u2019s response) and define the final judge of \u201cWin-Tie-Lose\u201d to be:(1) Win: ALPAGASUS\nwins twice, or wins once and draws once. (2) Tie: ALPAGASUS draws twice, or wins once and loses\nonce. (3) Lose: ALPAGASUS loses twice, or loses once and draws once. To avoid cut-off responses,\nwe allow models to generate up to 1024 tokens. For ChatGPT, Claude, and Text-Davinci-003, we\nset the temperature to 0.0, respectively, to reduce randomness and ensure a fair comparison.\n4\nEXPERIMENTAL RESULTS\n4.1\nQUALITY MATTERS MORE THAN QUANTITY\n90\n83\n70\n41\n86\n65\n44\n15\n76\n70\n66\n24\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaGasus(9k) wins\nTie\nAlpaca(52k) wins\n13B: AlpaGasus-9k vs. Alpaca-52k\n7B: AlpaGasus-9k vs. Alpaca-52k\n88\n96\n67\n42\n81\n55\n49\n13\n83\n67\n64\n25\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nFigure 5:\nMain results: comparing ALPAGASUS and ALPACA on their 7B and 13B models.\nALPAGASUS-9k achieves much better performance than ALPACA-52k on all four test sets: Vicuna,\nKoala, Self-Instruct, and WizardLM.\nAlpaGasus-9k vs. Alpaca-52k\nWe compare ALPAGASUS and ALPACA on two sizes of models\nin Fig. 5. They only differ in the training data: ALPACA uses all the 52k data while ALPAGASUS\nonly uses 9k data selected from the 52k. Their hyperparameters and training scripts are the same.\nAs shown in the evaluation results, ALPAGASUS significantly outperforms the original ALPACA\nacross all four test sets. Moreover, when using LLaMA-2 as the base model, we observe consistent\noutcomes (See Appendix A.3). This consistency underscores the universality of our data filtering\nmethod, irrespective of the model choices. These findings also confirm that our training data selection\napproach leads to superior performance even when the selected training data are only 17.75% of the\noriginal dataset.\n7B: AlpaGasus-9k vs. Alpaca-9k-Random\n13B: AlpaGasus-9k vs. Alpaca-9k-Random\n103\n93\n80\n42\n82\n72\n53\n14\n67\n53\n47\n24\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaGasus wins\nTie\nAlpaca wins\n83\n88\n59\n38\n92\n74\n63\n13\n77\n56\n58\n29\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nFigure 6: Comparing ALPAGASUS with LLaMA finetuned on randomly selected data.\n6To address potential concerns regarding bias in the evaluation prompts, we also present results of using\nalternative evaluation prompts in Appendix A.1.\n5\nPublished as a conference paper at ICLR 2024\nQuality-Guided Filtering vs. Random Filtering\nTo investigate the efficacy of our data selection\nstrategy, we compare ALPAGASUS with LLaMA models fine-tuned on a randomly sampled subset\nof the ALPACA 52k data, denoted by ALPACA-9k-random in Fig. 6. Both models start from the same\ninitial model (i.e., LLaMA) and are then finetuned on the same number of samples (i.e., 9k). They\nonly differ in terms of the data selection criteria. In Fig. 6, we compare the two types of models under\ntwo model sizes, i.e., 7B and 13B. ALPAGASUS-9k significantly outperforms ALPACA-9k-random,\nshowing the high quality of our selected data and their importance to the performance of IFT.\n4.2\nHOW MUCH DATA SHOULD BE FILTERED?\n7B: Alpaca-39k-Filtered vs. Alpaca-52k\n74\n77\n59\n31\n101\n74\n51\n19\n77\n67\n70\n30\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaca-39k wins\nTie\nAlpaca-52k wins\nFigure 7: Comparing ALPACA-7B (39k data) with\nALPACA-7B (52k data).\nThreshold \u03c4 of data filtering.\nIn Eq. (1), we\nselect data with score\u2265 \u03c4 and we set \u03c4 = 4.5 in\nour main experiments, which results in 9k out of\nthe 52k data to finetune ALPAGASUS. To study\nthe impact of the threshold \u03c4 on IFT, we com-\npare ALPAGASUS with LLaMA finetuned on\n39k data selected by applying a lower threshold\nof \u03c4 = 4.0. We report the comparison results in\nFig. 7. When tested on the Koala and WizardLM\ntest sets, ALPACA-39k model outperforms the\noriginal ALPACA-52k model. However, when\nusing the Vicuna and Self-Instruct as test sets,\nALPACA-39k does not exhibit advantages over\nthe original ALPACA-52k model. Hence, a loose criterion (a lower threshold) includes more data in\nthe selected data and a model with comparable performance as the original ALPACA. However, it\nstill performs poorer than ALPAGASUS trained on much fewer but higher-quality data, indicating the\nnegative impact of low-quality data to IFT.\nAlpaGasus trained on 3k/6k/9k selected data.\nOn the other hand, high-quality data show a\npositive impact on IFT. To verify this, we randomly draw 3k and 6k data from the 9k data selected for\ntraining ALPAGASUS and finetune two variants of ALPAGASUS from LLaMA using the same training\nscript. Fig. 8 reports the evaluation results of these variants: ALPAGASUS trained on 9k data performs\nthe best on all four test sets, indicating that more high-quality data leads to better IFT models.\nAlpaGasus-7B(9k) vs. Alpaca-7B(6k)\n117\n80\n78\n35\n69\n76\n57\n18\n66\n62\n45\n27\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpagasus-7B(9k) wins\nTie\nAlpaca-7B(3k) wins\n96\n76\n67\n31\n82\n87\n59\n22\n74\n55\n54\n27\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaGasus-7B(9k) wins\nTie\nAlpaca-7B(6k) wins\nAlpaGasus-7B(9k) vs. Alpaca-7B(3k)\nFigure 8: Comparing models finetuned on 3k/6k/9k high-quality data (3k and 6k data are randomly\ndrawn from the 9k data selected for ALPAGASUS).\nMinimum training data for AlpaGasus to match the performance of Alpaca.\nAccording to\nFig. 1, \u223c6k high-quality data suffices to finetune LLaMA achieving similar performance as the\noriginal ALPACA.\n4.3\nHUMAN STUDY\nWe further undertake human studies by enlisting three participants tasked with labeling the ques-\ntion/answer pairs. To be specific, we select 40 prompts from each test set, resulting in a total of\n160 prompts. These are then presented to the participants alongside the corresponding responses\ngenerated by both ALPAGASUS-13B and Alpaca-13B. The final answers are determined by majority\nvoting. There are 63/160 wins for ALPAGASUS-13B, 64/160 ties and 33/160 loses, which indicates\nthe superiority of our ALPAGASUS. Comprehensive results on each test set and user guidelines could\nbe found in Appendix J.\n6\nPublished as a conference paper at ICLR 2024\n0\n20\n40\n60\n80\n100\nGeneric\nKnowledge\nRoleplay\nCommonsense\nFermi\nCounterfactual\nCoding\nMath\nWriting\nAlpaGasus-13B vs. ChatGPT\nAlpaGasus-13B\nChatGPT\n0\n20\n40\n60\n80\n100\nGeneric\nKnowledge\nRoleplay\nCommonsense\nFermi\nCounterfactual\nCoding\nMath\nWriting\nAlpaGasus-13B vs. Davinci-003\nAlpaGasus-13B\nText-Davinci003\n0\n20\n40\n60\n80\n100\nGeneric\nKnowledge\nRoleplay\nCommonsense\nFermi\nCounterfactual\nCoding\nMath\nWriting\nAlpaGasus-13B vs. Claude\nAlpaGasus-13B\nClaude\nFigure 9: ALPAGASUS-13B vs. Davinci-003, Claude, and ChatGPT. ALPAGASUS achieves average\n90.1% capacity of Davinci003, 81.2% of Claude and 78.4% of ChatGPT.\n4.4\nCOMPARISON WITH CHATGPT/CLAUDE/DAVINCI003.\nIn Fig. 9, we compare ALPAGASUS with text-Davinci-003, ChatGPT, and Claude. The results show\nthat ALPAGASUS-13B can achieve \u2265 90% capacity of its teacher model, text-Davinci-003, which is\nused to generate the ALPACA-52k instruction data.\n4.5\nBENCHMARK PERFORMANCE\nFollowing InstructEval (Chia et al., 2023), we also evaluate our models on benchmark datasets,\ni.e., MMLU (Hendrycks et al., 2020), DROP (Dua et al., 2019) Humaneval (Chen et al., 2021),\nBBH (Suzgun et al., 2022), to evaluate the models\u2019 performance. The details of the benchmark\nsetting can be found in Appendix B. Benchmark results of our ALPAGASUS are shown in Table 2,\nwhere higher values indicate better performance. ALPAGASUS-7B, 13B show superiority on the 3/4\ndatasets, which demonstrates the effectiveness of our filtering algorithm. Another interesting finding\nis that the models trained with our filtered data can be better on all the benchmarks than training with\nrandomly selected data.7\nDatasets\n7B(9k-random)\n7B(9k)\n7B(52k)\n13B(9k-random)\n13B(9k)\n13B(52k)\nBBH\n31.89\n33.76\n33.01\n38.60\n38.92\n38.67\nDrop\n25.88\n26.03\n25.87\n33.40\n34.4\n33.84\nHumaneval\n11.59\n12.20\n11.69\n15.24\n15.86\n15.74\nMMLU\n36.93\n38.78\n40.86\n44.98\n46.12\n47.89\nTable 2: The benchmark results of filtering the Alpaca dataset.\n5\nHUMAN-WRITTEN INSTRUCTION SET FILTERING\nIn addition to filtering machine-generated datasets, our approach is capable of filtering human-written\ndatasets. Specifically, we investigate the Databricks-dolly-15k dataset (Dolly, 2023), a seminal collec-\ntion of 15,000 high-quality human-generated prompt/response pairs. Notably, this unparalleled dataset\nis a product of the collective efforts of more than 5,000 Databricks contributors and the included\nprompts and responses are more than just simple text; they embody a comprehensive spectrum of\nhuman cognition, covering activities from inventive brainstorming to succinct summarization.\nWe also applied a threshold of 4.5 for data filtration, resulting in a filtered dataset of 2,996 samples.\n(Score distribution can be found in Appendix B) A comparison between the 7B/13B LLaMA trained\non our filtered 3k dataset and the one trained on the entire Dolly 15k dataset is illustrated in Fig. 10\nand Fig. 21. Our evaluation suggests that the model trained on our filtered data exhibits superior\nperformance, thus underscoring the efficacy of our filtering method on human-composed datasets.\nComprehensive details regarding training hyperparameters are provided in the Appendix D.8\n7We observe similar performance gains of the 7B model on Dolly, and our 13B (3k) model consistently\noutperforms baselines, i.e., 13B(random-3k) and 13B(15k), on all four benchmark datasets, which are deferred\nto the Appendix B.\n8The result in Appendix A.4 (GPT4LLM dataset) shows the potential of applying our ChatGPT-based\nresponse quality evaluator to filter GPT-4\u2019s responses, which is considered as the most powerful model.\n7\nPublished as a conference paper at ICLR 2024\n107\n98\n73\n40\n57\n46\n42\n11\n88\n74\n65\n29\nSelf-\nInstruct\nWizardL\nM\nKoala\nVicuna\nDolly-7B(3k) wins\nTie\nDolly-7B(15k) wins\n115\n88\n83\n41\n63\n58\n50\n13\n74\n72\n47\n26\nSelf-\nInstruct\nWizardL\nM\nKoala\nVicuna\nDolly-7B(3k) wins\nTie\nDolly-7B(3k-random) wins\nFigure 10: Comparing models finetuned on filtered 3k data and original Dolly 15k data.\n6\nCASE STUDY & ANALYSIS\nAlpaca-9k-random vs. AlpaGasus\nFind the area of a circle with radius 5 cm.\nThe area of a circle with radius 5 cm is \n50.26548245743669.\nThe area of a circle with radius 5 cm is 78.54 square \ncentimeters.\nInstruction\nAlpaca-9k-\nrandom\nAlpaGasus\nAlpaGasus vs. Alpaca-52k\nImplement a regular expression in Python to validate\nan email address.\nThe regular expression in Python to validate an email \naddress is:\\n\\n\\\\s(https?://[^\\\\s]+)\\\\s\\n\\n\n^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,3}$\nInstruction\nAlpaca-52k\nAlpaGasus\nFigure 11:\nCase study on 13B models of ALPAGASUS and ALPACA. Left: Math capability\ncomparison based on WizardLM test set. Right: Coding skill comparison based on Vicuna test set.\nFig. 11 shows two case studies of 13B models trained on 52k data (ALPACA), 9k selected data\n(ALPAGASUS), and 9k randomly selected data (ALPACA-9k-random). The left case study focuses on\nthe math capability, where ALPAGASUS can produce a correct answer while ALPACA-9k-random\ncannot. As the judge, GPT-4 rates the answer of ALPAGASUS by a score of 10.0 while ALPACA-\n9k-random receives a score of 2.0. The right case study focuses on coding skills, ALPACA-52k\ncannot follow the instructions but produces a regular expression to validate the website address while\nALPAGASUS directly generates the correct code.\nWe also conduct a fine-grained evaluation of ALPAGASUS on each skill/category in the WizardLM\nand Vicuna test sets, whose samples are split into a list of skill sets/categories and thus facilitate\ndetailed analyses of the capabilities achieved by IFT (Appendix H). We compare two 7B models\non the WizardLM test set and report the results in Fig. 25. Our ALPAGASUS achieves better or\nequally good performance than ALPACA on 22/29 skills but does not show advantages on the\nremaining 7 skills such as coding (e.g., code generation). To investigate the reasons, we notice\nthat the coding categories include \u201cpython\u201d, \u201cJava\u201d, \u201cC++\u201d, and \u201cC#\u201d, which indicate that we can\nallocate training samples regarding coding skills based on these related keywords (Appendix E). We\nfind that our data selection/filtering, without specifying the proportions of skill categories, leads\nto a much higher filtering ratio of coding-related data 718\u221285\n718\n= 88.16% than the average filtering\nratio 52002\u22129229\n52002\n= 82.25%. Hence, the resulting coding skill is weaker than other skills. This\nindicates the importance of keeping the training data diverse and balanced across different categories\nin IFT.\n7\nCOST SAVING\nWe compare the training cost of ALPAGASUS and ALPACA in terms of the estimated expenses for\nthe required computation on AWS. Notably, the training time is reduced from 80m to 14m for the 7B\nmodel and 5.5h to 1h for the 13B model. Such training time reduction not only substantially enhances\nmodel iteration speed, but also reduces the cost from $27.31 to $4.78 for the 7B model and $225.28\nto $40.969 for the 13B model. It\u2019s noteworthy that instruction-tuning 65B LLaMA models require a\ngreater number of GPUs and an extended training duration. Consequently, as the model size scales\nup, our data selection method yields progressively pronounced cost savings.\n9The hyperparameters for IFT and the projected costs calculation method are deferred in Table 5.\n8\nPublished as a conference paper at ICLR 2024\n8\nRELATED WORK\nOpen-sourced Instruction-following models.\nInstruction-tuning datasets can be gathered in two\nways. A number of studies (K\u00f6pf et al., 2023; Dolly, 2023; Zhou et al., 2023) utilize crowdsourcing\nto produce human-generated pairs of instructions and responses. This approach, while effective,\ncan be laborious and costly. Alternatively, ALPACA (Taori et al., 2023) opens the door to create\nmachine-generated IFT sets from the distillation of the \u201cteacher\u201d LLM, i.e., Text-Davinci-003. Peng\net al. (2023) keep the instructions from ALPACA intact but using GPT-4 as the \u201cteacher\u201d LLM, which\nenhances model on 3H (Helpfulness, Honesty and Harmlessness) (Askell et al., 2021) alignment\ncriteria. Vicuna (Chiang et al., 2023) is the first to adopt ShareGPT (ShareGPT, 2023) data, which\nis the realistic dialogue data chatting with ChatGPT shared by users. Xu et al. (2023) and Luo\net al. (2023) evolve the original Alpaca instruction set and obtain more complex instructions which\nhelp better elicit the instruction-following ability of LLMs. There also exists concurrent work like\nKoala (Geng et al., 2023) and UltraChat (Ding et al., 2023), using dialogue & preference data as well\nas the adversarial prompts to conduct safe alignment.\nData-centric AI.\nOver the last decade, the realm of data-centric AI (Chu et al., 2016; Motamedi\net al., 2021) has witnessed substantial progress. Central to this concept is the belief that the quality of\ndata (Hajij et al., 2021; Zha et al., 2023; Chen et al., 2023a;c;d) warrants the same level of importance\nas algorithms within the AI/ML lifecycle. As noted by Chu et al. (2016), for an effective engagement\nwith diverse types of data across various domains, data cleaning processes should exhibit a higher\ndegree of automation and adaptability. With the advent of the Transformer architecture (Vaswani et al.,\n2017b), a shift in the paradigm of language models has occurred. Models such as RoBERTa (Liu\net al., 2019), BERT (Vaswani et al., 2017a), and Bard 10 all have incorporated this effective structure,\nstacking varying quantities of transformer blocks to create more potent models. This marked a turning\npoint in NLP research, signifying a heightened emphasis on data as opposed to model structure.\nPresently, SOTA LLMs like ChatGPT also underscore this shift toward data. They employ user data\nto conduct Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022a; Gao et al.,\n2022), which further aligns with the Data-centric AI philosophy.\nEvaluation of LLMs.\nEvaluating the open-ended instruction-following ability of LLMs is often\nneglected by previous works (Chung et al., 2022; Anil et al., 2023), though they conduct a series of\nbenchmark evaluations centered around factuality (Hendrycks et al., 2020) and reasoning (Bisk et al.,\n2020) for their pre-training models. Similarly, the frameworks proposed by Liang et al. (2022) and\nGao et al. (2021) focus more on the evaluation of the base models but not on the evaluation of the\nIFT models, where open-ended instruction-following capability are supposed to be prioritized. Since\ninstruction-following is a general ability but the scope of benchmarks is limited, the recent works such\nas Koala (Geng et al., 2023), Vicuna (Chiang et al., 2023), Self-Instruct (Wang et al., 2022), and Wiz-\nardLM (Xu et al., 2023) all provide the instruction sets they collected and some of them also include\nthe categories of the instructions for the evaluation of instruction-tuned LLMs. There are also some\nleaderboards like Alpaca-Eval (Li et al., 2023) measuring the model\u2019s instruction-following ability.\nLeveraging these recent advancements, we evaluate our models on human instruction sets.\n9\nCONCLUSION\nIn conclusion, our study reveals significant insights about the influence of data quality over quantity in\nIFT. Through our proposed data-filtering method, we have demonstrated that relying on a small subset\nof high-quality IFT data can lead to LLMs that exhibit enhanced instruction-following capabilities,\nwhile also offering substantial computational advantages. Notably, our method proves versatile\nacross different rating dimensions (e.g., Accuracy and helpfulness), LLM filters (e.g., ChatGPT and\nClaude-2), base model families (e.g., LLaMA-1 and LLaMA-2), model sizes (e.g., 7B and 13B),\ndataset types(e.g., machine-generated and human-written). By emphasizing the importance of data\nquality, we advocate for a transition in the existing paradigm where data accumulation has been a\nprimary focus. This perspective transition can lead to more meaningful advancements in the field\nof LLMs, making models more aligned with human intentions and less prone to errors induced by\npoor-quality data.\n10https://bard.google.com/\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGE\nLichang Chen and Heng Huang were partially supported by U.S. NSF IIS 2347592, 2347604,\n2348159, 2348169, DBI 2405416, CCF 2348306, CNS 2347617.\nREFERENCES\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073, 2022.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\n2020.\nJiuhai Chen, Lichang Chen, and Tianyi Zhou. It takes one to tango but more make trouble? in-context\ntraining with different number of demonstrations. arXiv preprint arXiv:2303.08119, 2023a.\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient\ninstruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082,\n2023b.\nLichang Chen, Minhao Cheng, and Heng Huang. Backdoor learning on sequence to sequence models.\narXiv preprint arXiv:2305.02424, 2023c.\nLichang Chen, Heng Huang, and Minhao Cheng. Ptp: Boosting stability and performance of prompt\ntuning with perturbation-based regularizer. arXiv preprint arXiv:2305.02423, 2023d.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic\nevaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nXu Chu, Ihab F Ilyas, Sanjay Krishnan, and Jiannan Wang. Data cleaning: Overview and emerging\nchallenges. In Proceedings of the 2016 international conference on management of data, pp.\n2201\u20132206, 2016.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXiv preprint arXiv:2305.14233, 2023.\nDolly.\nFree\ndolly:\nIntroducing\nthe\nworld\u2019s\nfirst\ntruly\nopen\ninstruction-tuned\nllm.\nBlog\nPost,\n2023.\nURL\nhttps://www.databricks.com/blog/2023/04/12/\ndolly-first-open-commercially-viable-instruction-tuned-llm.\n10\nPublished as a conference paper at ICLR 2024\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\nProc. of NAACL, 2019.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that\nlearn from human feedback. arXiv preprint arXiv:2305.14387, 2023.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Gold-\ning, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang,\nAnish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model\nevaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. arXiv\npreprint arXiv:2210.10760, 2022.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and\nDawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL\nhttps://bair.berkeley.edu/blog/2023/04/03/koala/.\nMustafa Hajij, Ghada Zamzmi, Karthikeyan Natesan Ramamurthy, and Aldo Guzman Saenz. Data-\ncentric ai requires rethinking data notion. arXiv preprint arXiv:2110.02491, 2021.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nMiyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first\nsentence: Position bias in question answering. arXiv preprint arXiv:2004.14602, 2020.\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, et al. Openassistant\nconversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327,\n2023.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following\nmodels. https://github.com/tatsu-lab/alpaca_eval, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110, 2022.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688, 2023.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\nevol-instruct, 2023.\nMohammad Motamedi, Nikolay Sakharnykh, and Tim Kaldewey. A data-centric approach for training\ndeep neural networks with less data. arXiv preprint arXiv:2110.03613, 2021.\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2023a.\nOpenAI. Gpt-4 technical report. arXiv, 2023b.\n11\nPublished as a conference paper at ICLR 2024\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022a.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022b.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277, 2023.\nShareGPT. Sharegpt. 2023. URL sharegpt.com.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging\nbig-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,\n2022.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-\nvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n2017a.\nURL https://proceedings.neurips.cc/paper_files/paper/2017/\nfile/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017b.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,\n2023.\nXuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. Position\nbias estimation for unbiased learning to rank in personal search. In Proceedings of the eleventh\nACM international conference on web search and data mining, pp. 610\u2013618, 2018.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.\nDaochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and\nXia Hu. Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n12\nPublished as a conference paper at ICLR 2024\nAppendix\nTable of Contents\nA Frequently Asked Questions\n14\nA.1\nIs there any bias contained in the evaluation prompts?\n. . . . . . . . . . . . . .\n14\nA.2\nHave you tried other LLM filter?\n. . . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.3 What about the results on other base models, e.g., LLaMA-2?\n. . . . . . . . . .\n15\nA.4\nCan your LLM filter evaluate the stronger model\u2019s responses, e.g., filtering the\nresponses given by GPT-4?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.5\nResults on other rating dimensions, e.g., helpfulness? . . . . . . . . . . . . . . .\n16\nB Additional Results on Dolly Dataset\n17\nB.1\nScore Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB.2\nBenchmark results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB.3\nDolly-13B Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nC Details of GPT-4 Evaluation Prompt\n18\nD Training Hyperparameter Details\n19\nD.1 Alpaca Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nD.2\nDolly Dataset\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nE\nKeywords set for detailed analysis\n19\nF\nRated examples in Alpaca Dataset\n20\nG Rated examples in Dolly Dataset\n23\nH Analysis\n26\nH.1 Analysis on WizardLM Test Set . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nH.2 Analysis on Vicuna Test Set . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nI\nDetailed Analysis on the WizardLM testset\n27\nJ\nHuman Study\n31\nK Limitations\n31\n13\nPublished as a conference paper at ICLR 2024\nA\nFREQUENTLY ASKED QUESTIONS\nA.1\nIS THERE ANY BIAS CONTAINED IN THE EVALUATION PROMPTS?\nWe also explore alternate evaluation prompts such as the prompts provided by Zheng et al. (2023),\nwhich are shown in Table 3. We apply the same rules to calculate the \u201cWin-Tie-Lose\u201d and show the\nresults in Fig. 12. Notably, ALPAGASUS consistently outperforms across all test sets.\n96\n94\n68\n40\n31\n21\n30\n11\n91\n70\n59\n21\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaGasus wins\nTie\nAlpaca wins\nFigure 12: The experimental results when using the evaluation prompt from Zheng et al. (2023) to\njudge the two responses. ALPAGASUS could still maintain its advantage.\nSystem Prompt\nPlease act as an impartial judge and evaluate the quality of the responses\nprovided by two AI assistants to the user question displayed below. You\nshould choose the assistant that follows the user\u2019s instructions and answers\nthe user\u2019s question better. Your evaluation should consider factors such as\nthe helpfulness, relevance, accuracy, depth, creativity, and level of detail\nof their responses. Begin your evaluation by comparing the two responses\nand provide a short explanation. Avoid any positional biases and ensure\nthat the order in which the responses were presented does not influence\nyour decision. Do not allow the length of the responses to influence your\nevaluation. Do not favor certain names of the assistants. Be as objective\nas possible. After providing your explanation, output your final verdict\nby strictly following this format: \u201c[[A]]\u201d if assistant A is better, \u201c[[B]]\u201d if\nassistant B is better, and \u201c[[C]]\u201d for a tie.\nPrompt Template\n[User Question]\n{question}\n[The Start of Assistant A\u2019s Answer]\n{Answera}\n[The End of Assistant A\u2019s Answer]\n[The Start of Assistant B\u2019s Answer]\n{Answerb}\n[The End of Assistant B\u2019s Answer]\nTable 3: The GPT-4 evaluation prompt from Zheng et al. (2023).\nA.2\nHAVE YOU TRIED OTHER LLM FILTER?\nYes, we also try to use Claude-211 as our response quality evaluator (LLM filter). Fig. 13 and\nFig. 14 demonstrate the score distribution and evaluation results on the four testsets, respectively.\nRemarkably, the 7B model instruction-tuned with 8k selected data could be better than the model\ninstruction-tuned with 52k Alpaca data on 3/4 testsets and achieves significantly better over the model\ninstruction-tuned with 8k random selected data.\n11https://www.anthropic.com/index/claude-2\n14\nPublished as a conference paper at ICLR 2024\n4\n41576\n2295\n8088\n0\n8000\n16000\n24000\n32000\n40000\n< 3\n3\n4\n5\nScore Distribution(Claude-2 as LLM filter)\nCount\nFigure 13: The score distribution of using Claude2 as the LLM filter.\n102\n69\n71\n38\n57\n71\n49\n19\n83\n78\n60\n23\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaca-7B(claude-2\nselected 8k) wins\nTie\nAlpaca-7B(52k) wins\n107\n90\n82\n41\n76\n58\n39\n14\n69\n70\n59\n31\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaca-7B(claude2\nselected 8k) wins\nTie\nAlpaca-7B(8k-random)\nwins\nFigure 14: The experimental results by using the Claude2 as response quality evaluator.\nAs Fig. 13 shows, the interval between two scores is 1, which is different from the ChatGPT-based\nfilter, where the interval is 0.5. Thus, if we would like to have fine-grained scores, a larger rating\nscale should be applied to the prompt as the present 5-point scale does not suffice. We leave the\nexploration of the rating scales to future work.\nA.3\nWHAT ABOUT THE RESULTS ON OTHER BASE MODELS, E.G., LLAMA-2?\nWe also have the results of LLaMA2 in Fig. 15, which shows the superiority of our method.\nAlpagasus2-7B(9k) vs. Alpaca2-7B(52k)\n102\n90\n68\n36\n72\n63\n51\n20\n78\n65\n61\n24\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpagasus-9k wins\nTie\nAlpaca2-9k-random wins\n102\n91\n70\n33\n80\n58\n59\n17\n70\n69\n51\n30\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaGasus2-7B(9k) wins\nTie\nAlpaca2-7B(52k) wins\nAlpaGasus2-7B(9k) vs. Alpaca2-7B(9k-random)\nFigure 15: The experimental results on LLaMA2. Alpagasus2 and Alpaca2 means using 9k and 52k\ndata to IFT LLaMA2, respectively.\nA.4\nCAN YOUR LLM FILTER EVALUATE THE STRONGER MODEL\u2019S RESPONSES, E.G.,\nFILTERING THE RESPONSES GIVEN BY GPT-4?\nTo answer the question, we apply our LLM filter to GPT4LLM (Peng et al., 2023) data. According to\nthe score distribution, we use 4.5 as the threshold and select 13721 data samples from the GPT4LLM\ndataset for IFT LLaMA-7B.\n15\nPublished as a conference paper at ICLR 2024\n100\n1685\n9372\n27124\n13710\n11\n0\n5000\n10000\n15000\n20000\n25000\n30000\n< 3\n3\n3.5\n4\n4.5\n5\nScore Distribution(Alpaca-gpt4)\nCount\nFigure 16: The score distribution of Alpaca-GPT4 dataset.\nAlpaca-GPT4(13k) vs. Alpaca-GPT4(52k)\n99\n80\n72\n35\n73\n78\n52\n23\n80\n60\n56\n22\nSelf-Instruct\nWizardLM\nKoala\nVicuna\nAlpaca-gpt4(13k) wins\nTie\nAlpaca-gpt4(13k\nrandom) wins\n84\n72\n64\n35\n70\n80\n44\n30\n98\n66\n72\n15\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaca-gpt4(13k) wins\nTie\nAlpaca-gpt4(52k) wins\nAlpaca-GPT4(13k) vs. Alpaca-GPT4(13k-random)\nFigure 17: The evaluation results on Alpaca-GPT4 dataset.\nThe results presented in Fig. 17 demonstrate the superiority of our method on the Vicuna and\nWizardLM test sets. Even though the responses from GPT4LLM are generated by GPT-4, recognized\nas the most advanced LLM globally, our approach attains comparable outcomes using merely 25%\nof the original data. Notably, the performance of our method markedly surpasses that of randomly\nselected counterparts. In summary, our LLM filter exhibits promise in discerning superior responses\nfrom teacher models.\nA.5\nRESULTS ON OTHER RATING DIMENSIONS, E.G., HELPFULNESS?\nWe also use \u201chelpfulness\u201d as our rating dimension and find that we only need 2k data to train the\nbase model that can surpass the base model trained with 52k Alpaca data. The score distributions are\nshown in Fig. 18.\n16\nPublished as a conference paper at ICLR 2024\n4\n2516\n5280\n42063\n2020\n2\n0\n8000\n16000\n24000\n32000\n40000\n< 3\n3\n3.5\n4\n4.5\n5\nScore Distribution(helpfulness)\nCount\nFigure 18: The score distribution of helpfulness.\nEvaluation Results\nFrom Figure 19, it is evident that the models trained using our filtered Alpaca\ndataset outperform those trained on randomly selected datasets across all instruction test sets. Fur-\nthermore, our model outperforms the model trained on the complete Alpaca set in 3 out of 4 test\nsets. This underscores the significant potential of our filtering approach, especially considering that\na model trained with a mere 2k data points can surpass one trained with the original 52k Alpaca\ndataset.\n108\n79\n69\n35\n51\n64\n45\n14\n93\n75\n66\n31\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaca-7B(2k-\nhelpfulness) wins\nTie\nAlpaca-7B(2k\nrandom) wins\n77\n74\n71\n47\n102\n74\n41\n14\n73\n84\n58\n19\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nAlpaca-7B(2k-\nhelpfulness) wins\nTie\nAlpaca-7B(52k) wins\nFigure 19: Evaluation results regarding on the \u201chelpfulness\u201d dimension.\nB\nADDITIONAL RESULTS ON DOLLY DATASET\nB.1\nSCORE DISTRIBUTION\nWe show the score distribution of Dolly dataset(rated by ChatGPT) in Fig. 20.\nB.2\nBENCHMARK RESULTS\nWe use the code provided by Chia et al. (2023) to conduct benchmark evaluation. For MMLU, BBH,\nDrop, and humaneval, we also use 5-shot, 3-shot, 3-shot, and 0-shot settings, respectively. We show\nthe benchmark results in Table 4 of Dolly and the filtered set.\nDatasets\n7B(3k-random)\n7B(3k)\n7B(15k)\n13B(3k-random)\n13B(3k)\n13B(15k)\nBBH\n31.33\n31.76\n30.73\n36.15\n36.37\n35.8\nDrop\n20.73\n22.45\n22.33\n31.61\n34.24\n26.94\nHumaneval\n9.76\n9.78\n7.93\n10.98\n14.92\n14.63\nMMLU\n35.01\n35.83\n36.25\n44.39\n46.92\n46.13\nTable 4: The benchmark results of filtering the Dolly dataset.\nHere are the hyperparameters we select for the training of the LLaMA-7B and LLaMA-13B are the\nsame as the Alpaca except for the training epochs. To avoid the under-train issue, we train 10 epochs,\n17\nPublished as a conference paper at ICLR 2024\n2\n808\n2335\n8948\n2981\n15\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n< 3\n3\n3.5\n4\n4.5\n5\nScore Distribution(Dolly)\nCount\nFigure 20: The score distribution of the Dolly.\ninstead of 3 in Alpaca, for all the 7B models and 15 epochs, instead of 5 in Alpaca, for all the 13B\nmodels.\nB.3\nDOLLY-13B RESULTS\nWe show the dolly-13B results. As Fig. 21 shows, our filtered Dolly dataset is better than the original\nDolly dataset since it can achieve stronger instruction-following capacity of the instruction-tuned\nLLaMA-7B models via ours. (See the results on the four tests)\nDolly-13B(3k) vs. Dolly-13B(15k)\n106\n99\n71\n35\n62\n61\n42\n14\n84\n58\n67\n31\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nDolly-13B-3k wins\nTie\nDolly-13B-3k-random\nwins\n103\n94\n72\n40\n64\n55\n54\n11\n85\n69\n54\n29\nSelf-\nInstruct\nWizardLM\nKoala\nVicuna\nDolly-13B(3k) wins\nTie\nDolly-13B(15k) wins\nDolly-13B(3k) vs. Dolly-13B(3k-random)\nFigure 21: Dolly 13B results. We show the dolly-13B results here. With the model size going up, our\nmethod can still perform pretty well.\nC\nDETAILS OF GPT-4 EVALUATION PROMPT\nWe provide the detailed form of the prompt to GPT-4 used for evaluation in Fig. 22. It is the prompt\nfor evaluation used in the original Vicuna blog 12\n12https://lmsys.org/blog/2023-03-30-vicuna/\n18\nPublished as a conference paper at ICLR 2024\nSystem Prompt:\nYou are a helpful and precise assistant for checking the quality of the answer.\nUser Prompt:\n[Question]\n[The Start of Assistant 1's Answer] \n{answer_1}\n[The End of Assistant 1's Answer]\n[The Start of Assistant 2's Answer] \n{answer_2}\n[The End of Assistant 2's Answer]\nWe would like to request your feedback on the performance of two AI assistants in response to the user question \ndisplayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant \nreceives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease\nfirst output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The \ntwo scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your \nevaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not \naffect your judgment.\"\nFigure 22: The prompt for evaluation using GPT-4 as the judge.\nD\nTRAINING HYPERPARAMETER DETAILS\nD.1\nALPACA DATASET\nWe show the training hyperparameters and costs in Table 5. 13\nModel Size\nData Size\n# GPUs\nEpoch\nLR\nBatch Size\nTime\nCost\n7B\n9k\n4\n3\n2e-5\n128\n14m\n$ 4.78\u2217\n7B\n52k\n4\n3\n2e-5\n128\n80m\n$ 27.31\u2217\n13B\n9k\n8\n5\n1e-5\n128\n1h\n$ 40.96\n13B\n52k\n8\n5\n1e-5\n128\n5.5h\n$ 225.28\nTable 5: All the cost is estimated based on the price provided by AWS. We assume the training scripts\nfor all models are the same (e.g., training epochs, batch size on each GPU, accumulation steps, etc.)\nD.2\nDOLLY DATASET\nWe show the training hyperparameters in Table 6.\nModel Size\nData Size\nEpoch\nLR\nBatch Size\n7B\n3k\n3\n2e-5\n128\n7B\n15k\n3\n2e-5\n128\n13B\n3k\n5\n1e-5\n128\n13B\n15k\n5\n1e-5\n128\nTable 6: IFT hyperparameter details. (Dolly Dataset)\nE\nKEYWORDS SET FOR DETAILED ANALYSIS\nWe use the keyword set of [Java, java, C++, c++, C#, c#, Python, python] and count the number of\n(instruction, input, output) tuples which contain the keyword in this set.\n13https://aws.amazon.com/ec2/instance-types/p4/ a p4de.24xlarge(preview) node has 8\n\u00d7 80GB A100 and it costs $40.96/h.*we assume training time of using 8 GPUs is half of using 4 GPUs\n19\nPublished as a conference paper at ICLR 2024\nF\nRATED EXAMPLES IN ALPACA DATASET\nWe include more examples rated by the response quality evaluator, i.e., ChatGPT, in this section. The\nexamples of Score 5.0, Score 4.5, Score 4.0, Score 3.5, Score 3.0, Score 2.5, Score 2.0 are shown in\nTable 7, Table 8, Table 9, and Table 10, respectively.\n[Instruction] Instruction: Answer this true or false question: The capital of France is London.\n[Input] None\n[Response] False. The capital of France is Paris\n[Score & Explanation]: 5.0. The AI assistant provided a correct and accurate response to the\ninstruction and input given. It correctly identified that the statement \u00a8The capital of France\nis London\u00efs false and provided the correct answer that the capital of France is Paris. The\nresponse was clear and concise, and there were no errors or misunderstandings in the AI\nassistant\u2019s interpretation of the question. Therefore, the AI assistant deserves a score of 5 for its\nperformance in this task.\n[Instruction] Translate the phrase \"Bonne chance\" into English\n[Input] None\n[Response] Good luck.\n[Score & Explanation]: 5.0. The AI assistant provided a correct and accurate response to the\ngiven instruction and input. \u00a8Bonne chance\u00efs a French phrase that translates to \u00a8Good luck\u00efn\nEnglish, and the AI assistant provided the correct translation. Therefore, the assistant deserves a\nscore of 5 for its accuracy\n[Instruction] Complete the following sentence given the context:\n[Input] My grandmother always said, \" An apple a day\n[Response] Keeps doctor away.\"\n[Score & Explanation]: 5.0. The AI assistant provided a perfect response to the given input and\ninstruction. The response accurately completed the sentence with the correct phrase that follows\n\u00c4n apple a day\u00a8. Therefore, the AI assistant deserves a score of 5 for its accuracy.\nTable 7: The score=5.0 examples in Alpaca dataset.\n20\nPublished as a conference paper at ICLR 2024\n[Instruction] Instruction: Suggest ways to reduce plastic waste in everyday life.\n[Input] None\n[Response] One way to reduce plastic waste in everyday life is to make the switch to reusable\nitems. This includes bringing a reusable water bottle instead of buying bottled water, investing\nin a reusable coffee cup, bringing your own bags when shopping, and using reusable containers\nfor storing and transporting food. You can also reduce the amount of plastic waste you create by\navoiding or limiting the use of take-out or single-use containers, utensils, and straws. Addition-\nally, try to find sustainable alternatives to plastic items when shopping and avoid purchasing\nitems wrapped in plastic whenever possible. Finally, make sure you recycle any plastic items\nyou do have.\n[Score & Explanation]: 4.5.The response provided by the AI assistant is highly accurate and\nrelevant to the given instruction. It suggests various ways to reduce plastic waste in everyday life,\nincluding using reusable items, avoiding single-use containers and utensils, finding sustainable\nalternatives, and recycling. The only potential improvement could be providing more specific\nexamples of sustainable alternatives to plastic items. Overall, the response is informative and\nhelpful in addressing the given instruction\n[Instruction] Generate a list of non-alcoholic drinks.\n[Input] None\n[Response]\n- Fruit juices\n- Tea\n- Smoothies\n- Iced coffee\n- Milkshakes\n- Fizzy drinks\n- Hot chocolate\n- Lemonades\n- Kombucha\n- Energy drinks\n- Milkshake\n- Coconut water\n- Ginger beer\n- Ice tea\n- Sparkling water\n- Aloe vera drink\n- Vegetable juice\n[Score & Explanation]: 4.5. The AI assistant provided a comprehensive list of non-alcoholic\ndrinks that are commonly consumed. The list includes a variety of options such as fruit juices, tea,\nsmoothies, and fizzy drinks. However, there are some duplicates in the list such as milkshakes\nand iced coffee. Additionally, some of the options may not be as commonly consumed as others,\nsuch as kombucha and aloe vera drink. Overall, the response is accurate and provides a good\nrange of options for non-alcoholic drinks.\nTable 8: The score=4.5 examples in Alpaca dataset.\n21\nPublished as a conference paper at ICLR 2024\n[Instruction] Instruction: Compare and contrast commercial banks and investment banks.\n[Input] None\n[Response] Commercial banks are financial institutions that provide services to their customers,\nsuch as loans, deposits, and investment management. Investment banks, on the other hand, are a\ntype of financial institution that specializes in the trading of securities, such as stocks and bonds.\nThe main difference between the two is that commercial banks provide services to customers,\nwhile investment banks provide services to governments, corporations, and wealthy individuals.\nCommercial banks are also subject to more regulations than investment banks, as they offer\ndeposit accounts to customers. Investment banks, however, do not accept deposits from the\ngeneral public and are subject to fewer regulations. Both types of banks are important for the\nglobal economy, but their roles and services are different.\n[Score & Explanation]: 4.0. The response provided by the AI assistant is mostly accurate and\nrelevant to the given instruction and input. It clearly explains the differences between commercial\nbanks and investment banks, highlighting their respective roles and services. However, it could\nhave provided more specific examples of the services offered by each type of bank, and could\nhave elaborated further on the regulations that apply to them. Overall, the response is informative\nand helpful, but could benefit from more detail.\n[Instruction] Determine the application of statistics in the field of economics\n[Input] None\n[Response] Statistics is widely used in economics to analyze and predict markets, examine the\nperformance of companies, and evaluate economic policies. Statistics is used to identify trends\nand correlations and to measure variables such as prices and wages. Statistical models are also\nused to predict economic growth and changes in interest rates and to test the effects of regulation\nand taxation. Statistical tools are also used to estimate the impacts of policy changes, assess the\neffectiveness of government programs and identify the root causes of problems.\n[Score & Explanation]: 4.0. The AI assistant provided a fairly accurate response to the in-\nstruction. It correctly identified the various applications of statistics in the field of economics,\nincluding market analysis, company performance evaluation, policy evaluation, and prediction\nof economic growth and interest rates. However, the response could have been more detailed\nand provided specific examples of statistical methods used in economics. Overall, the response\nwas informative and provided a good overview of the topic\nTable 9: The triplets (instruction, input, response) obtain Score 4.0.\n[Instruction] Design a company logo.\n[Input] None\n[Response] <nooutput>\n[Score & Explanation]: 2.0. The AI assistant did not provide any response to the given instruction\nand input. Therefore, it cannot be evaluated for accuracy.\n[Instruction] Classify the item as either animal or vegetable.\n[Input] Banana\n[Response] Animal: No, it\u2019s a vegetable.\n[Score & Explanation]: 2.0. The AI assistant\u2019s response is incorrect. A banana is a fruit, which\nis a type of plant-based food, making it a vegetable. However, the AI assistant classified it as an\nanimal, which is completely inaccurate. Therefore, the score is low.\n[Instruction] Design a poster to advertise a product.\n[Input] None\n[Response] No output required.\n[Score & Explanation]: 2.5.The AI assistant did not provide any response to the given input,\nwhich makes it difficult to evaluate its performance accurately. However, it is possible that the\nAI assistant was designed to only respond to textual input and not visual input, which is why it\ndid not provide a response. Therefore, I have given it a score of 2.5, which is the average score\nbetween a completely inaccurate response and a completely accurate response.\nTable 10: The score=2.0 and score=2.5 examples in Alpaca dataset.\n22\nPublished as a conference paper at ICLR 2024\nG\nRATED EXAMPLES IN DOLLY DATASET\n[Instruction] What is Jenkins?\n[Context] None\n[Response] Jenkins is an open-source automation server. It helps automate the parts of software\ndevelopment related to building, testing, and deploying, facilitating continuous integration and\ncontinuous delivery.\n[Score & Explanation]: 5.0. The response accurately defines Jenkins as an open source au-\ntomation server that helps automate software development processes related to building, testing,\nand deploying. It also mentions that Jenkins facilitates continuous integration and continuous\ndelivery, which are key aspects of modern software development practices. Overall, the response\nis concise, clear, and directly addresses the given instruction. Therefore, I have given it a score\nof 5.\n[Instruction] Who was the first president of the United States of America?\n[Context] None\n[Response] George Washington\n[Score & Explanation]: 5.0. The AI assistant provided the correct and accurate response to the\ngiven context and instruction. The question asked for the first president of the United States of\nAmerica, and the AI assistant correctly identified George Washington as the answer. Therefore,\nthe AI assistant deserves a score of 5 for its accuracy.\n[Instruction] What country is to the north of the United States?\n[Context] None\n[Response] Canada is America\u2019s northern neighbor.\n[Score & Explanation]: 5.0. The response is accurate and directly answers the question asked in\nthe instruction. It provides the correct country name and also uses the appropriate terminology\nto describe the relationship between the United States and Canada as neighboring countries.\nTherefore, the AI assistant has performed well in this context.\nTable 11: The score=5.0 examples in Dolly dataset.\n23\nPublished as a conference paper at ICLR 2024\n[Instruction] Help me with spending a day in New Delhi. I like architecture, culture, and am a\nfoodie.\n[Context] None\n[Response] North India has a lot to offer for an aesthete, epicure, and a xenophile. Day 1 arrive\nin Delhi as many flights arrive early in the morning, settle into a hotel of your choice. Today\ntravel around Delhi especially old Delhi and start your trip by visiting Red Fort which is an\nhistorical Mogul Fort commissioned by Mogul Emperor Shah Jahan in 1638. The fort is steeped\nin history and has a beautiful combination of Persian architecture with Indian influence. After\nthat have lunch in Chandi Chowk where you will find incredible street food and especially\nKarim\u2019s which specializes in Mughlai food. Chandi Chowk is full of verve and energy and you\nwill experience a slice of Indian culture. After the delicious lunch and walking through the\nnarrow streets of Chandi Chowk, it is time to visit Qutb Minar a Unesco World Heritage Site\nwhich is considered the tallest minaret in the world at 240 ft constructed out of sandstone in\n1192. Close by you can also visit Jamal Kalami mosque and relax at the nearby 200 acre park.\nIn the evening you can travel to visit the Lotus Temple in Delhi which is a Bah\u02d800e1\u02d802bc\u02d800ed\nHouse of Worship and architected by Fariborz Sahba in 1976. For dinner, it is time to indulge\nyourself at Oberoi Delhi at Dhilli under the mentorship of Michelin-starred Chef Vineet Bhatia\nMBE and finish your night at the rooftop bar Cirrus 9 with a smooth cocktail.\n[Score & Explanation]: 4.5. The response provides a comprehensive itinerary for spending a\nday in New Delhi, catering to the interests of the user who likes architecture, culture, and food.\nThe assistant suggests visiting historical sites such as Red Fort and Qutb Minar, experiencing\nthe culture and food of Chandi Chowk, and visiting the Lotus Temple. The assistant also\nrecommends dining at a Michelin-starred restaurant and finishing the night at a rooftop bar. The\nresponse is accurate and provides a well-rounded itinerary for a day in New Delhi. However, it\ncould have been improved by providing more specific details such as the opening and closing\ntimes of the suggested.\n[Instruction] Why is the sky blue?\n[Context] None\n[Response] All the light in our daytime sky comes from the sun, and our sun produces light\nin many different colors. To understand this, just think of a prism. White light enters, and a\nrainbow of colors exits. Our sun is like that. Its light may seem uniform but it is actually emitting\nmany different colors. Our sky though is not a mix of blue and red and green because the earth\u2019s\natmosphere is sandwiched between the sun and us, getting in the way. Atmospheric gases and\nparticles scatter the incoming colors, bouncing them in all different directions. The amount of\nscattering increases as the wavelength of light decreases. Red light which has a long wavelength\nis scattered much less than blue light which has a short wavelength. As blue is scattered more,\nit fills our daytime sky. As the sun traverses the sky and moves low on the horizon, the light\nthat reaches our eyes must actually travel further through the atmosphere. This has the effect of\ncompletely bouncing away all the blue light. We are left with the reds and yellows and oranges\nin wonderful mixture, making sunrises and sunsets so beautiful.\n[Score & Explanation]: 4.5. The response accurately explains why the sky appears blue during\nthe day and why it appears red during sunrise and sunset. It also provides a clear explanation of\nhow atmospheric gases and particles scatter light, causing blue light to be scattered more than\nother colors. The only potential improvement could be to provide a more concise explanation,\nas the response is somewhat lengthy. Overall, the response is highly accurate and informative.\nTable 12: The score=4.5 examples in Dolly dataset.\n24\nPublished as a conference paper at ICLR 2024\n[Instruction] What is a bicameral legislature?\n[Context] None\n[Response] A legislative body comprised of two separate chambers.\n[Score & Explanation]: 4.0. The response provided by the AI assistant is accurate and directly\nanswers the question asked in the instruction. The definition provided is concise and clear,\nand demonstrates a good understanding of the concept of bicameral legislature. However, the\nresponse could have been improved by providing additional information or examples to further\nclarify the concept.\n[Instruction] When is tax day?\n[Context] None\n[Response] Tax day usually refers to the day in the United States when tax forms must be\nsubmitted and paid for individual and businesses. It is typically April 15 or later.\n[Score & Explanation]: 4.0. The response accurately defines what tax day is and provides the\ntypical date for it in the United States. However, it could have been more specific by mentioning\nthat the deadline for tax day can vary depending on certain circumstances, such as weekends or\nholidays.\n[Instruction] Write a bullet point list of at least 7 crops that are grown in California.\n[Context] None\n[Response] - Artichokes\n- Almonds\n- Plums\n- Cherries\n- Garlic\n- Apples\n- Pears\n[Score & Explanation]: 4.0. The AI assistant provided a list of 7 crops that are grown in\nCalifornia, which is accurate and relevant to the given instruction. However, there are many\nother crops that are also grown in California, and the assistant could have provided a more\ncomprehensive list. Therefore, the response is not perfect but still satisfactory.\nTable 13: The score=4.0 examples in Dolly dataset.\n25\nPublished as a conference paper at ICLR 2024\n[Instruction] How to make a cup of spiced chai?\n[Context] None\n[Response] Among all ingredients, the most important element is cardamom. Of course, you\ncannot skimp on loose black tea leaves too!\n[Score & Explanation]: 2.5. The response mentions two important ingredients for making spiced\nchai - cardamom and loose black tea leaves. However, it does not provide any instructions on\nhow to actually make the chai. The response is partially accurate as it addresses the context of\nmaking spiced chai but does not fully answer the instruction of providing the steps to make it.\n[Instruction] Is Seattle a nice place to live?\n[Context] Seattle came in at No. 39 in the 2023 World\u2019s Best Cities Report, a comprehensive\nannual study by Resonance Consultancy. Seattle is a seaport city on the West Coast of the\nUnited States. The culture of Seattle is heavily defined by its significant musical history.\nBetween 1918 and 1951, nearly 24 jazz nightclubs existed along Jackson Street, from the current\nChinatown/International District to the Central District. The jazz scene nurtured the early careers\nof Ernestine Anderson, Ray Charles, Quincy Jones, and others. The city is also the origin of\nrock acts such as Foo Fighters, Heart, and Jimi Hendrix, as well as the grunge subgenre of rock\nand its pioneering bands Alice in Chains, Pearl Jam, Nirvana, and Soundgarden. The city itself\nis hilly, though not uniformly so. Like Rome, the city is said to lie on seven hills; the lists vary\nbut typically include Capitol Hill, First Hill, West Seattle, Beacon Hill, Queen Anne, Magnolia,\nand the former Denny Hill. Due to its location in the Pacific Ring of Fire, Seattle is in a major\nearthquake zone. Temperature extremes are moderated by the adjacent Puget Sound, greater\nPacific Ocean, and Lake Washington. Thus extreme heat waves are rare in the Seattle area, as are\nvery cold temperatures (below about 15 F). The Seattle area is the cloudiest region of the United\nStates, due in part to frequent storms and lows moving in from the adjacent Pacific Ocean. With\nmany more \"rain days\" than other major American cities, Seattle has a well-earned reputation\nfor frequent rain. Seattle\u2019s economy is driven by a mix of older industrial companies and \"new\neconomy\" internet and technology companies, as well as service, design, and clean technology\ncompanies.\n[Response] Yes.\n[Score & Explanation]: 2.5. The AI assistant\u2019s response is not accurate enough. While the\ncontext provides information about Seattle\u2019s culture, geography, climate, and economy, the\ninstruction asks whether Seattle is a nice place to live. The AI assistant\u2019s response is too brief\nand does not provide any supporting evidence or explanation to justify its answer. Therefore, the\nresponse is not helpful or informative enough for the user.\n[Instruction] Best workout in the world.\n[Context] None\n[Response] Walk. Just walk as much as you can.\n[Score & Explanation]: 2.0. The response provided by the AI assistant is not very accurate.\nWhile walking is a good exercise, it is not necessarily the \"best workout in the world\" as the\ninstruction suggests. The response also lacks specificity and does not provide any additional\ninformation or guidance on how to make the most out of the workout. Therefore, I would rate\nthe accuracy of the response as 2 out of 5.\nTable 14: The score=2.0 and score=2.5 examples in Dolly dataset.\nH\nANALYSIS\nH.1\nANALYSIS ON WIZARDLM TEST SET\nWe conduct a fine-grained evaluation of ALPAGASUS on each skill/category in the WizardLM and\nVicuna test sets, whose samples are split into a list of skill sets/categories and thus facilitate detailed\nanalyses of the capabilities achieved by IFT.\nALPAGASUS-7B(9k) vs. ALPACA-7B(52k).\nWe compare these two 7B models on the WizardLM\ntest set and report the results in Fig. 25. Our ALPAGASUS achieves better or equally good performance\nthan ALPACA on 22/29 skills but does not show advantages on the remaining 7 skills such as coding\n(e.g., code generation). To investigate the reasons, we notice that the coding categories include\n26\nPublished as a conference paper at ICLR 2024\n\u201cpython\u201d, \u201cJava\u201d, \u201cC++\u201d, and \u201cC#\u201d, which indicate that we can allocate training samples regarding\ncoding skills based on these related keywords (Appendix E). We find that our data selection/filtering,\nwithout specifying the proportions of skill categories, leads to a much higher filtering ratio of coding-\nrelated data 718\u221285\n718\n= 88.16% than the average filtering ratio 52002\u22129229\n52002\n= 82.25%. Hence, the\nresulting coding skill is weaker than other skills. This indicates the importance of keeping the training\ndata diverse and balanced across different categories in IFT.\nH.2\nANALYSIS ON VICUNA TEST SET\n6\n6\n7\n3\n5\n5\n1\n1\n7\n1\n2\n1\n2\n2\n2\n3\n0\n2\n3\n2\n2\n5\n3\n3\n3\n2\n1\nGeneric\nKnowledge\nRoleplay\nCommon\nFermi\nConterfactual\nCoding\nMath\nWriting\nAlpaGasus-9k wins\nTie\nAlpaca-52k wins\n7\n8\n6\n4\n5\n7\n4\n0\n1\n1\n1\n0\n2\n2\n1\n1\n2\n3\n2\n1\n4\n4\n3\n2\n2\n1\n6\nGeneric\nKnowledge\nRoleplay\nCommon\nFermi\nConterfactual\nCoding\nMath\nWriting\nFigure 23: Fine-grained evaluation of ALPAGASUS-13B-9k vs. ALPACA-13B-52k on categories of\nthe Vicuna test set.\nFig. 23 demonstrates the detailed analysis on Vicuna testset. ALPAGASUS-7B is better than the\nALPACA-7B in the majority of the categories, including Counterfactual, Roleplay, Knowledge, and\nGeneric, etc. Another strong point is that when the base model scales up, the conclusion still holds.\n(See right part of the Fig. 23)\nI\nDETAILED ANALYSIS ON THE WIZARDLM TESTSET\nIn Fig. 26, Fig. 27, and Fig. 28, we compare ALPAGASUS with text-Davinci-003, ChatGPT, and\nClaude, respectively. The results show that ALPAGASUS-13B can achieve \u2265 91% capacity of its\n\u201cteacher\u201d model, text-Davinci-003 (all the responses in the ALPACA-52k dataset are generated by\ntext-Davinci-003 so we call it \u201cteacher\u201d LLM). The results also show that our model could achieve\npretty good performance on tasks like Writing, RolePlay, Toxicity, Art, etc., while it still needs\nimprovement on coding and math capacity when compared with stronger LLMs.\n27\nPublished as a conference paper at ICLR 2024\n1\n3\n1\n3\n4\n3\n4\n5\n5\n4\n4\n4\n7\n9\n7\n3\n1\n0\n3\n2\n2\n2\n1\n0\n1\n4\n6\n7\n7\n3\n8\n14\n0\n1\n0\n0\n0\n1\n1\n1\n2\n1\n2\n2\n1\n6\n3\n2\nChemistry\nAcademic Writing\nHistory\nSport\nEthics\nTechnology\nBiology\nRolePlay\nCounterfactual\nCommonsense\nComplex Format\nReasoning\nComputer Science\nWriting\nCode Generation\nMath\n2\n0\n1\n0\n1\n1\n0\n1\n0\n1\n0\n1\n1\n1\n0\n2\n2\n1\n1\n3\n1\n2\n1\n1\n4\n7\n2\n1\n2\n2\n3\n3\n2\n3\n3\n3\n4\n2\n2\nEconomy\nPhilosophy\nPhysics\nToxicity\nMusic\nArt\nEntertainment\nLiterature\nMedicine\nLaw\nTruthfulQA\nMultilingual\nCode Debug\nAlpaGasus-9k wins\nTie\nAlpaca-52k wins\nFigure 24: Fine-grained evaluation of ALPAGASUS-9k(13B) vs. ALPACA-52k(13B) on categories of\nthe WizardLM test set.\n28\nPublished as a conference paper at ICLR 2024\n2\n3\n2\n2\n3\n2\n4\n4\n3\n3\n8\n10\n9\n0\n0\n2\n2\n0\n2\n0\n1\n1\n2\n3\n2\n8\n0\n1\n0\n1\n2\n1\n2\n1\n2\n2\n4\n6\n2\nChemistry\nAcademic\nWriting\nHistory\nEconomy\nLaw\nSport\nEthics\nTechnology\nRoleplay\nMultilingual\nComputer\nScience\nWriting\nMath\n0\n0\n1\n2\n2\n1\n3\n0\n1\n0\n4\n5\n8\n5\n1\n4\n4\n3\n3\n4\n10\nPhilosophy\nArt\nLiterature\nCommonsense\nCode Debug\nReasoning\nCode Generation\n1\n1\n2\n1\n1\n2\n2\n6\n3\n2\n1\n3\n3\n1\n4\n0\n1\n1\n2\n1\n1\n2\n2\n6\nPhysics\nToxicity\nMusic\nEntertainment\nMedicine\nTruthfulQA\nCounterfactual\nComplex Format\nAlpaGasus-9k wins\nTie\nAlpaca-52k wins\nFigure 25: Fine-grained evaluation of ALPAGASUS-9k(7B) vs. ALPACA-52k(7B) on categories of\nthe WizardLM test set.\n29\nPublished as a conference paper at ICLR 2024\nAvg. = 78.28%\n0.00%\n20.00%\n40.00%\n60.00%\n80.00%\n100.00%\n120.00%\nAcademic Writing\nEconomy\nChemistry\nEthics\nEntertainment\nBiology\nArt\nMedicine\nHistory\nWriting\nLaw\nToxicity\nMusic\nLiterature\nTechnology\nRoleplay\nTruthfulQA\nComputer Science\nCounterfactual\nSport\nMultilingual\nPhysics\nPhilosophy\nReasoning\nCode Generation\nComplex Format\nCommon-Sense\nCode Debug\nMath\nAlpaca-13B-9k / ChatGPT\nWizardLM Test Set (Skills Details)\nAlpaca-13B-9k vs. ChatGPT\nFigure 26: Compare with ChatGPT. Achieve average 78.26% capacity of ChatGPT on all 29 skills.\nAvg. =78.41%\n0.00%\n20.00%\n40.00%\n60.00%\n80.00%\n100.00%\n120.00%\nRoleplay\nHistory\nAcademic Writing\nWriting\nEconomy\nMedicine\nEthics\nSport\nEntertainment\nLaw\nTechnology\nArt\nLiterature\nMusic\nChemistry\nComputer Science\nToxicity\nTruthfulQA\nCounterfactual\nMultilingual\nBiology\nCommon-Sense\nPhilosophy\nPhysics\nCode Debug\nMath\nComplex Format\nCode Generation\nReasoning\nAlpaca-13B-9k / Claude\nWizardLM Test Set (Skills Details)\nAlpaca-13B-9k vs. Claude\nFigure 27: Compare with Claude-v1. Achieve average 78.41% capacity of ChatGPT on all 29 skills.\nAvg. =91.11%\n0.00%\n20.00%\n40.00%\n60.00%\n80.00%\n100.00%\n120.00%\n140.00%\n160.00%\nToxicity\nAcademic Writing\nPhysics\nEntertainment\nArt\nEthics\nChemistry\nHistory\nLaw\nWriting\nEconomy\nMedicine\nComplex Format\nRoleplay\nTechnology\nTruthfulQA\nComputer Science\nSport\nCounterfactual\nBiology\nLiterature\nMusic\nMath\nCommon-Sense\nMultilingual\nPhilosophy\nReasoning\nCode Generation\nCode Debug\nAlpaca-13B-9k / Davinci-003\nWizardLM Test Set (Skills Details)\nAlpaca-13B-9k vs. Davinci-003\nFigure 28: Compare with Davinci-003. Achieve an average 91.11% capacity of ChatGPT on all 29\nskills.\n30\nPublished as a conference paper at ICLR 2024\nJ\nHUMAN STUDY\nWe conduct the human study among three different users. The evaluation interface is shown as\nTable 15:\nYou\u2019ll be presented with a series of questions. For each question, two answers\nwill be provided. Your task is to read both answers carefully and decide which\none you believe is better. When judging, consider:\nRelevance: Does the answer directly address the question?\nCompleteness: Is the answer comprehensive?\nCoherence: Is the answer logically structured and easy to understand?\nAccuracy: Is the information provided in the answer correct?\nQuestion:\n<QUESTION>\nAnswer A:\nAnswer B:\n<ANSWER A>\n<ANSWER B>\nComparing these two answers, which answer is better?\n1. Answer A is significantly better.\n2. Answer B is significantly better.\n3. Neither is significantly better.\nTable 15: Human annotation interface.\nWe show more detailed results of human evaluations in Fig. 29.\nHuman Study:Alpagasus-13B(9k) vs. Alpaca-13B(52k)\n25\n22\n15\n17\n10\n10\n18\n6\n5\n8\n7\n17\nSelf-Instruct\nWizardLM\nKoala\nVicuna\nAlpagasus-9k wins\nTie\nAlpaca-52k wins\nFigure 29: The detailed results of human study.\nK\nLIMITATIONS\nModel Size.\nIn our experiments, we evaluated our IFT strategy by training models of two different\nsizes, 7B and 13B, since they are the most common sizes for recent open-source LLMs. We plan to\nextend this study to larger model sizes such as 33B, 65B, or even 175B, and verify whether the same\nconclusion still holds, i.e., a small subset of high-quality data selected by our method can improve the\ninstruction-finetuned model. We leave analysis on the IFT of larger models as future work.\n31\n"
  },
  {
    "title": "Diffusion Models Beat GANs on Image Classification",
    "link": "https://arxiv.org/pdf/2307.08702.pdf",
    "upvote": "17",
    "text": "Diffusion Models Beat GANs on Image Classification\nSoumik Mukhopadhyay\u2217\nMatthew Gwilliam\u2217\nVatsal Agarwal\nNamitha Padmanabhan\nArchana Swaminathan\nSrinidhi Hegde\nTianyi Zhou\nAbhinav Shrivastava\nUniversity of Maryland, College Park\nAbstract\nWhile many unsupervised learning models focus on one family of tasks, either\ngenerative or discriminative, we explore the possibility of a unified representation\nlearner: a model which uses a single pre-training stage to address both families\nof tasks simultaneously. We identify diffusion models as a prime candidate. Dif-\nfusion models have risen to prominence as a state-of-the-art method for image\ngeneration, denoising, inpainting, super-resolution, manipulation, etc. Such models\ninvolve training a U-Net to iteratively predict and remove noise, and the resulting\nmodel can synthesize high fidelity, diverse, novel images. The U-Net architecture,\nas a convolution-based architecture, generates a diverse set of feature represen-\ntations in the form of intermediate feature maps. We present our findings that\nthese embeddings are useful beyond the noise prediction task, as they contain\ndiscriminative information and can also be leveraged for classification. We explore\noptimal methods for extracting and using these embeddings for classification tasks,\ndemonstrating promising results on the ImageNet classification task. We find that\nwith careful feature selection and pooling, diffusion models outperform compara-\nble generative-discriminative methods such as BigBiGAN for classification tasks.\nWe investigate diffusion models in the transfer learning regime, examining their\nperformance on several fine-grained visual classification datasets. We compare\nthese embeddings to those generated by competing architectures and pre-trainings\nfor classification tasks.\n1\nIntroduction\nUnified unsupervised image representation learning is a critical but challenging problem. Many\ncomputer vision tasks can be broadly classified into two families, discriminative and generative. With\ndiscriminative representation learning, one seeks to train a model that can apply labels to images or\nparts of images. For generative learning, one would design a model that generates or edits images,\nand performs similar tasks like inpainting, super-resolution, etc. Unified representation learners\nseek to achieve both simultaneously, and the resulting model would be able to both discriminate and\ngenerate novel visual artifacts.\nSuch unified representation learning is an arduous undertaking. BigBiGAN [2, 3] is one of the earliest\ndeep learning methods to address both families of tasks simultaneously. However, more recent\napproaches outperform BigBiGAN in terms of both classification and generation performance by more\nspecialized models. Beyond BigBiGAN\u2019s key accuracy and FID deficiencies, it is also much more\n*These authors contributed equally to this work\nPreprint. Under review.\narXiv:2307.08702v1  [cs.CV]  17 Jul 2023\n2. Pool Sizes\nUNet\nQ K\nV\nQ K\nV\nProbing\n1. Extract Features from Network\nt\n3. Heads\nModel Ef\ufb01ciency on ImageNet-50\nA\n\u00a0 \u00a0C\nD\nD\nB\nFigure 1: An overview of our method and results. We propose that diffusion models are unified\nself-supervised image representation learners, with impressive performance not only for generation,\nbut also for classification. We explore the feature extraction process in terms of U-Net block number\nand diffusion noise time step. We also explore different sizes for the feature map pooling. We\nexamine several lightweight architectures for feature classification, including linear (A), multi-layer\nperceptron (B), CNN (C), and attention-based heads (D). We show the results on such explorations\non the right, for classification heads trained on frozen features for ImageNet-50 [1], computed at\nblock number 24 and noise time step 90. See Section 4.1 for a detailed discussion.\nburdensome to train than other methods; its encoder makes it larger and slower than comparable GANs,\nand its GAN makes it more expensive than ResNet-based discriminative methods [4]. PatchVAE [5]\nattempts to adapt VAE [6] to perform better for recognition tasks by focusing on learning mid-level\npatches. Unfortunately, its classification gains still fall well short of supervised methods, and come at\ngreat cost to image generation performance. Recent works have taken valuable steps by delivering\ngood performance in both generation and classification, both with [7] and without [8] supervision.\nHowever, this field is relatively underexplored in comparison to the volume of work in self-supervised\nimage representation learning, and therefore, unified self-supervised representation learning remains\nlargely under-addressed.\nAs a result of previous shortcomings, some researchers have argued that there are inherent differences\nbetween discriminative and generative models, and the representations learned by one are not well-\nsuited for the other [9]. Generative models naturally need representations that capture low-level, pixel\nand texture details which are necessary for high fidelity reconstruction and generation. Discriminative\nmodels, on the other hand, primarily rely on high-level information which differentiates objects at a\ncoarse level based not on individual pixel values, but rather on the semantics of the content of the\nimage. Despite these preconceptions, we suggest that the early success of BigBiGAN is endorsed by\nrecent approaches such as MAE [10] and MAGE [8], where the model must tend to low-level pixel\ninformation, but learns models which are also very good for classification tasks.\nFurthermore, state-of-the-art diffusion models have already achieved great success for generative\nobjectives. However, their classification capacity is largely ignored and unexplored. Thus, rather\nthan build a unified representation learner from the ground up, we posit that state-of-the-art diffusion\nmodels, which are powerful image generation models, already possess potent emergent classification\nproperties. We demonstrate their impressive performance on these two very different tasks in Figure 1.\nOur method for utilizing diffusion models results in much better image generation performance than\nBigBiGAN, with better image classification performance as well. Thus, in terms of optimizing for\nboth classification and generation simultaneously, we show that diffusion models are already near\nstate-of-the-art unified self-supervised representation learners.\nOne of the main challenges with diffusion models is feature selection. In particular, the selection\nof noise steps and feature block is not trivial. So, we investigate and compare the suitability of\nthe various features. Additionally, these feature maps can be quite large, in terms of both spatial\nresolution and channel depth. To address this, we also suggest various classification heads to take the\nplace of the linear classification layer, which can improve classification results, without any addition\nof parameters or sacrifice in generation performance. Critically, we demonstrate that with proper\nfeature extraction, diffusion models work very well as classifiers out-of-the-box, such that diffusion\nmodels can be used for classification tasks without the need to modify the diffusion pre-training. As\n2\nsuch, our approach is flexible for any pre-trained diffusion model and can thus benefit from future\nimprovements to such models in terms of size, speed, and image quality.\nWe also investigate the performance of diffusion features for transfer learning on downstream tasks,\nand we compare the features themselves directly to those from other methods. For downstream\ntasks, we choose fine-grained visual classification (FGVC), an appealing area to use unsupervised\nfeatures due to implied scarcity of data for many FGVC datasets. This task is of particular interest\nwith a diffusion-based method since it does not rely on the sorts of color invariances that previous\nworks suggest may limit unsupervised methods in the FGVC transfer setting [11, 12]. To compare\nthe features, we rely on the popular centered kernel alignment (CKA) [13], which allows for a rich\nexploration of the importance of feature selection as well as how similar diffusion model features are\nto those from ResNets [4] and ViTs [14].\nIn summary, our contributions are as follows:\n\u2022 We demonstrate that diffusion models can be used as unified representation learners, with\n26.21 FID (-12.37 vs. BigBiGAN) for unconditional image generation and 61.95% accuracy\n(+1.15% vs. BigBiGAN) for linear probing on ImageNet.\n\u2022 We present analysis and distill principles for extracting useful feature representations from\nthe diffusion process.\n\u2022 We compare standard linear probing to specialized MLP, CNN, and attention-based heads\nfor leveraging diffusion representations in a classification paradigm.\n\u2022 We analyze the transfer learning properties of diffusion models, with fine-grained visual\ncategorization (FGVC) as a downstream task, on several popular datasets.\n\u2022 We use CKA to compare the various representations learned by diffusion models, both\nin terms of different layers and diffusion properties, as well as to other architectures and\npre-training methods.\n2\nRelated Work\nGenerative Models.\nGenerative Adversarial Networks (GANs) [15] constitute a class of deep\nneural networks which are capable of generating novel images in a data distribution given a random\nlatent vector z \u2208 Z as input, and are trained by optimizing a min-max game objective. GANs can be\nclass-conditioned, where they generate images given noise and class input, or unconditional, where\nthey generate random images from noise alone. Popular examples of GANs which have produced\nhigh quality images include PGGAN [16], BigGAN [17], and StyleGANs [18\u201322]. Recent work in\nGAN inversion finds that images can be mapped to GAN latent space [23], meaning that the GAN\nlearns a representation for the image in noise/latent space. Some of these approaches directly optimize\nlatent vectors to reconstruct the input image [24]. Others train encoders to generate the latent vector\ncorresponding to a given input image [25, 26]. Hybrid approaches are also popular, where an encoder\ngenerates a latent vector which is then optimized to generate better reconstructions [27\u201330].\nDiffusion denoising probabilistic models (DDPM) [31], a.k.a. diffusion models, are a class of\nlikelihood-based generative models which learn a denoising Markov chain using variational inference.\nDiffusion models have proven to produce high-quality images [32] beating previous SOTA generative\nmodels like BigGAN [17], VQVAE-2 [33] on FID metric on ImageNet[34]. These models enjoy\nthe benefit of having a likelihood-based objective like VAEs as well as high visual sample quality\nlike GANs even on high variability datasets. Recent advances in this area have also shown amazing\nresults in text-to-image generation including works like DALLE2 [35], Imagen [36], and Stable\nDiffusion [37]. Application of these models is not just limited to generation but spans tasks like\nobject detection [38], and image segmentation [39]. While these are all trained and evaluated for\ngenerative tasks, we observe they have discriminative capacity as well, and thus investigate their\npotential for classification tasks.\nDiscriminative Models.\nDiscriminative models learn to represent images, and extract useful infor-\nmation from images that can then be used to solve downstream tasks. Early representation learning\nmethods tried training neural network backbones with partially degraded inputs and learn image\nrepresentation by making the model predict the rest of the information in the actual image like Colori-\nsation [40], Jigsaw [41], PIRL [42], Inpainting [43]. More recently, many approaches have emerged\n3\nthat revolve around a contrastive loss objective, maximizing distance between positive-negative pairs,\nsuch as SimCLR [44, 45], MoCo [46\u201348], Barlow Twins [49], and ReLICv2 [50]. On the other\nhand, BYOL [9], SimSiam [51], and VICReg [52] introduce methods that work without negative\nsamples. DeepCluster [53] uses offline clustering whereas SwAV [54] introduces online clustering\nand multi-view augmentation methods to get a better representation. DINO [55] uses self supervised\nknowledge distillation between various views of an image in Visual Transformers [14]. PatchGame\nintroduces a referential games where two unsupervised models develop a mutual representation\nthrough goal-oriented communication [56]. SEER [57] demonstrates the success of strong self-\nsupervised pre-training methods at the scale of billions of images. With all the recent advances, the\nlatest self-supervised methods have leveraged transformers and iteratively improved upon contrastive\nand clustering objectives to surpass supervised methods on many key baselines [58\u201361].\nUnified Models.\nOther methods leverage the unsupervised nature of GANs to learn good image\nrepresentations [2, 62\u201364]. BiGAN [2] does joint Encoder-Generator training with a discriminator\nwhich jointly discriminates image-latent pair. ALI [62] uses reparameterized sampling from the\nencoder output. BigBiGAN [3] is the most popular among these methods \u2013 it is a BiGAN with a\nBigGAN [17] generator and a discriminator with additional unary loss terms for image and latent\ncodes. In spite of their promising performance for downstream classification tasks, subsequent\ncontrastive pre-training methods that train more quickly, reliably, and with fewer parameters have\nbeaten their performance.\nDistinct from GANs, autoencoders are a natural fit for the unified paradigm. ALAE attempts to learn\nan encoder-generator map to perform both generation and classification [65]. PatchVAE improves on\nthe classification performance of VAE [6] by encouraging the model to learn good mid-level patch\nrepresentations [5]. MAE [10] and iBOT [66] train an autoencoder via masked image modeling, and\nseveral other transformer-based methods have been built under that paradigm [67\u201369]. MAGE [8],\nwhich uses a variable masking ratio to optimize for both recognition and generation, is the first\nmethod to achieve both high-quality unconditional image generation and good classification results.\n3\nApproach\n3.1\nDiffusion Models Fundamentals\nDiffusion models first define a forward noising process where gradual Gaussian noise is iteratively\nadded to an image x0, which is sampled from the data distribution q(x0), to get a completely\nnoised image xT in T steps. This forward process is defined as a Markov chain with latents\nx1, x2 . . . , xt, . . . , xT \u22121, xT which represent noised images of various degrees. Formally, the forward\ndiffusion process is defined as\nq(x1, . . . xT |x0) :=\nT\nY\nt=1\nq(xt|xt\u22121)\nq(xt|xt\u22121) := N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI)\n(1)\nwhere {\u03b2t}T\nt=1 is the variance schedule and N is a normal distribution. As T \u2192 \u221e, xT nearly is\nequivalent to the isotropic Gaussian distribution. With \u03b1t := 1 \u2212 \u03b2t and \u00af\u03b1t := Qt\ni=0 \u03b1i one can\nsample a noised image xt at diffusion step t directly from a real image x0 using\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, \u03f5 \u223c N(0, I)\n(2)\nThe reverse diffusion process aims to reverse the forward process and sample from the posterior\ndistribution q(xt\u22121|xt) which depends on the entire data distribution. Doing this iteratively can\ndenoise a completely noisy image xT , such that one can sample from the data distribution q(x0).\nThis is typically approximated using a neural network \u03f5\u03b8 as\np\u03b8(xt\u22121|xt) := N\n\u0012\nxt\u22121;\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t)\n\u0013\n, \u03a3\u03b8(xt, t)\n\u0013\n(3)\nWhen p and q are interpreted as a VAE, a simplified version of the variational lower bound objective\nturns out to be just a mean squared error loss [31]. This can be used to train \u03f5\u03b8 which learns to\napproximate the Gaussian noise \u03f5 added to the real image x0 in Eq. 2 as\nLsimple = Ex0,t,\u03f5[\u2225\u03f5\u03b8(xt, t) \u2212 \u03f5\u22252\n2]\n(4)\n4\nAs for \u03a3\u03b8(xt, t), previous works keep it either fixed [31] or learn it using the original variational\nlower-bound objective [70, 32].\n3.2\nDiffusion Models Feature Extraction\nIn this work, we use the guided diffusion (GD) implementation, which uses a U-Net-style architecture\nwith residual blocks for \u03f5\u03b8. This implementation improves over the original [31] architecture by\nadding multi-head self-attention at multiple resolutions, scale-shift norm, and using BigGAN [17]\nresidual blocks for upsampling and downsampling. We consider each of these residual blocks,\nresidual+attention blocks, and downsampling/upsampling residual blocks as individual blocks and\nnumber them as b \u2208 {1, 2, ..., 37} for the pre-trained unconditional 256\u00d7256 guided diffusion model.\nOur feature extraction is parameterized with the diffusion step t and model block number b. We show\nan illustration of how input images vary at different time steps in Figure 3. For feature extraction of\nimage x0, we use Eq. 2 to get noised image xt. In the forward pass through the network \u03f5\u03b8(xt, t), we\nuse the activation after the block number b as our feature vector f\u03b8(x0, t, b).\n3.3\nLinear Probing and Alternatives\nThe two most common methods for evaluating the effectiveness of self-supervised pre-training are\nlinear probing and finetuning, and we match the popular recipes documented by VISSL [71] to the\nextent possible. While correlated, these test different properties of the pre-training. Linear probing,\nwhich learns a batch normalization and linear layer on top of frozen features, tests the utility of the\nlearned feature representations \u2013 it shows whether the pre-training learns disentangled representations,\nand whether these feature meaningful semantic correlations. Finetuning, on the other hand, learns a\nbatch normalization and linear layer but with no frozen features. In the finetuning regime, we treat the\npre-training method as an expensive weight initialization method, and retrain the entire architecture\nfor classification.\nIn this paper, we focus more on the representative capacity of the frozen features, which is of\nparticular interest in areas like fine-grained classification and few shot learning, where data may be\ninsufficient for finetuning. Additionally, this allows us to make statements with respect to the utility\nof the learned features, rather than the learned weights. We note that the diffusion models are like\nregular convolutional nets in the sense that they do not natively produce a linear feature, instead\ngenerating a series of feature maps at various points in the network. Thus, similar to other CNNs, we\nuse a combination of pooling and flattening to yield a vector feature representation for each image.\nThe channel depth and feature map size are naturally quite large, so in addition to standard pooling,\nwe also try other methods. We investigate multi-layer perceptron heads. Due to the large size, we\nalso try CNNs as a learned pooling mechanism, and give more complete details for the design in the\nappendix. We also investigate the ability of attention heads to perform appropriate aggregation of\nboth spatial and channel information, with full details in the appendix.\n4\nExperiments\nWe first provide some preliminaries for setup and replication purposes, specifically with respect to\nmodel architecture, critical hyperparameters, and hardware details. Then, we give statistics for the\ndatasets we use for our experiments. We give our primary results in Section 4.1 \u2013 we compare our\ndiffusion extraction to baselines as well as competing unified representation methods. We provide\nablations in Section 4.1.1 to discover optimal block numbers, pooling sizes, and time steps for feature\nextraction. We evaluate the fitness of diffusion for downstream classification tasks by providing\nresults for popular FGVC datasets in Section 4.2. We perform the analysis of our representations\nin Section 4.3 to compare representations both internally (between blocks of the U-Net) as well as\nexternally (between different U-Nets and with other self-supervised learning architectures).\nExperiment Details. Unless otherwise specified, we use the unconditional ADM U-Net architecture\nfrom Guided Diffusion [32] with total timesteps T = 1000. We use the 256\u00d7256 checkpoint; thus\nwe resize all inputs to this size and use center-crop and flipping for data augmentation. We use an\nadaptive average pool to reduce the spatial dimension, followed by a single linear layer. For linear\nprobing, we train only this single layer. We use cross entropy loss with an Adam optimizer [72].\nWe follow the VISSL protocol for linear probing \u2013 28 epochs, with StepLR at 0.1 gamma every 8\n5\nTable 1: Main results. We compare unified learn-\ners in terms of classification and generation at\nresolution 256.\nMethod\nAccuracy\nFID\nBigBiGAN*\n60.8%\n28.54\nMAGE\n78.9%\n9.10\nU-Net Encoder\n64.32%\nn/a\nGD (L, pool 1\u00d71)\n61.95%\n26.21\nGD (L, pool 2\u00d72)\n64.96%\n26.21\nGD (Attention)\n71.89%\n26.21\n*BigBiGAN\u2019s best FID is at generator resolution 128.\nTable 2: Finetuning results. Non-GD methods use\nViT-L. Except for MAGE, all other methods use\n224\u00d7224 images.\nMethod\nAccuracy\nSupervised\n82.5%\nMoCo v3\n84.1%\nMAE\n84.9%\nMAGE\n84.3%\nGD (Linear, pool 2\u00d72)\n73.17%\nGD (Linear, pool 4\u00d74)\n73.50%\nTable 3: Attention head ImageNet-1k classifica-\ntion results.\nb\nt\nAccuracy (L)\nAccuracy (A)\n19\n90\n55.09%\n66.03%\n19\n150\n54.77%\n64.85%\n24\n90\n61.95%\n71.89%\n24\n150\n61.86%\n70.98%\nTable 4: Stable Diffusion linear probe results.\nCondition\nb\nSize\nAccuracy\nNull Text\n18\n512\n64.67%\nNull Text\n15\n512\n55.77%\nNull Text\n18\n256\n41.37%\nLearnable\n18\n512\n65.18%\nGuided Diffusion\n24\n256\n61.86 %\nepochs. However, we do not use random cropping or batch norm. For hardware, the majority of our\nexperiments are run on 4 NVIDIA RTX A5000 GPUs.\nDatasets. The dataset we use for our main result is ImageNet-1k [34]. Additionally, we run ablations\nand similar explorations on ImageNet-50, which is a selection of 50 classes of ImageNet as also used\nin [1]. Please see Table 5 for exact datasets and details.\n4.1\nMain Results: ImageNet Classification\nFirst, we show the promising linear probing performance of diffusion in Table 1, using settings we\nselect via the ablations described in Section 4.1.1. As a baseline, we compare to the diffusion pre-\ntrained classifier, since it uses the same U-Net encoder. We also offer a comparison to other unified\nmodels: BigBiGAN [3] and MAGE [8]. We outperform BigBiGAN in terms of both generation and\nclassification, especially when BigBiGAN is forced to handle the higher resolution, 256\u00d7256 images.\nHence, diffusion models beat GANs for image classification (and generation). We acknowledge\nthat diffusion is not yet state-of-the-art compared to classification-only models, with a gap of over\n10% top-1 accuracy, or compared to the powerful unified MAGE model. However, we note that we\nare unable to completely match the resources necessary to mimic the linear probe settings of other\nmethods. MAE [10], for example, trains their linear layer for 100 epochs with 16,384 images per\nbatch. Thus, it is difficult to present \u201cfair\u201d comparisons with such methods.\nWe perform finetuning, under similar conditions. Shown in Table 2, guided diffusion lags behind\nother methods which use classification specific adjustments. Regardless, this is a better result than\nthe U-Net encoder by a fair margin (+9.38%), which suggests that guided diffusion is a useful\npre-training for classification.\nAs described previously, we also propose several approaches to deal with the large spatial and\nchannel dimensions of U-Net representations. Naively, we can use a single linear layer with different\npreliminary pooling, and we show results for various pooling dimensions. Alternatively, we can use a\nmore powerful MLP, CNN, or attention head to address varying aspects of the feature map height,\nwidth, and depth. For fairness, we train CNNs, MLPs, and attention heads with comparable parameter\ncounts to our linear layers under the various pooling settings. We show results for such heads, on\nImageNet-50, in Figure 1 (right), with full numerical results and model details in the appendix. We\nnote that the attention head performs the best by a fair margin. In Table 3, we try the best-performing\nattention head on ImageNet (all classes), and find it significantly outperforms the simple linear probe,\n6\n0\n100\n200\n300\n400\n500\nTime Step\n30\n35\n40\n45\n50\n55\nAccuracy (%)\nb=19\n0\n5\n10\n15\n20\n25\n30\n35\nBlock Number\n10\n20\n30\n40\n50\n60\nt=90\nt=150\n1\n2\n4\n8\nPooling\n50\n55\n60\n65\n70\nt=90, b=24\nt=150, b=19\nFigure 2: Ablations on ImageNet (1000 classes) with varying block numbers, time steps, and pooling\nsize, for a linear classification head on frozen features. We find the model is least sensitive to pooling,\nand most sensitive to block number, although there is also a steep drop-off in performance as inputs\nand predictions become noisier.\nt=0\nt=300\nt=150\nt=90\nt=50\nt=30\nt=10\nt=500\nFigure 3: Images at different time steps of the diffusion process, with noise added successively. We\nobserve that the best accuracies are obtained at t = 90.\nregardless of pooling. This suggests the classification head is an important mechanism for extracting\nuseful representations from diffusion models, and it could be extended to other generative models.\n4.1.1\nAblations\nAs shown in Figure 1, extracting good features from diffusion models requires careful consideration\nof noise step, block number, and pooling size. We initiate a search of that hyperparameter space\nfor ImageNet. We set a search space of roughly log-equidistant time steps for the noise. We try\nseveral blocks at even intervals around the U-Net bottleneck. We also address the feature height\nand width (pooling). From our linear search, shown in Figure 2, we find t should be set to 90 and\nb to 24. However, as we discuss in Section 4.2, we find that such settings are at least somewhat\ndata dependent. Thus, while in this work we distill some general settings and principles, automatic\nselection and combination of features could be explored in future work.\nFor further ablations, we explore to what extent our idea is valid for other diffusion models. We\nspecifically examine stable diffusion, training a classifier on frozen features for 15 epochs, with t\nfixed at 150. Thus, in Table 4, we show that stable diffusion features also lend themselves well to\nclassification. Critically, this means not only that our approach is flexible, but that lighter diffusion\nmodels with better performance that are developed in the future could be immediately leveraged as\nunified representation models by our method.\n4.2\nResults: Fine-grained Visual Classification (FGVC)\nHere, we give results for applying our method in the transfer setting to the datasets defined in Table 5.\nWe use both standard linear probing, as well as each of our classification heads (with their best\nImageNet-50 configurations). We show these results in Figure 4. Note that there is a performance\ngap between the diffusion model and SimCLR, regardless of classification head used. One notable\nexception is Aircraft, where diffusion outperforms SimCLR for 3 of the 4 heads; this is indicative of\nits promising performance.\nAdditionally, we find that feature selection is not trivial, and often the settings that work for various\nFGVC datasets do not correspond to the ideal ImageNet settings. For example, consider that attention,\nthe best head for ImageNet-50, tends to perform the worst for FGVC. This may be due to their reliance\non the amount of data to learn properly. Furthermore, as we explore the feature selection problem\non CUB on Figure 5, we find that the ideal block number for ImageNet (b = 24) underperforms\nsubstantially for CUB compared to b = 19. Hyperparameter changes that have a more subdued effect\n7\nAircraft\nCars\nCUB\nDogs\nFlowers\nNABirds\nDataset\n20\n40\n60\n80\n100\nAccuracy (%)\nGD (L)\nGD (MLP)\nGD (CNN)\nGD (Attention)\nSimCLR (L)\nSwAV (L)\nFigure 4: Fine-Grained Visual Classification (FGVC) results. We train our best classification heads\nfrom our ImageNet-50 explorations on FGVC datasets (denoted with GD), and compare against\nthe results from linear probing a SimCLR ResNet-50 on the same datasets. Linear is denoted by\n(L). While SimCLR and SwAV tend to perform better, diffusion achieves promising results, slightly\noutperforming SimCLR for Aircraft.\n100\n200\n300\n400\n500\nTime Step\n5\n10\n15\n20\n25\n30\n35\nAccuracy (%)\nBlock 15\n100\n200\n300\n400\n500\nTime Step\nBlock 19\n100\n200\n300\n400\n500\nTime Step\nBlock 24\nPool Size\n1\n2\n4\n8\nFigure 5: FGVC feature extraction analysis. We show accuracy for different block numbers, time\nsteps, and pooling sizes. Block 19 is superior for FGVC, in contrast to ImageNet where 24 was ideal.\non ImageNet, such as pooling size, can result in up to 3\u00d7 change in performance on accuracy for\nCUB. Thus, determining a more robust feature selection procedure or introducing some regularization\nduring the diffusion training might be important future work to make transfer more reliable.\n4.3\nRepresentation Analysis\nWe use linear centered kernel alignment (CKA) [13] to find the degree of similarity between the\nrepresentations of different blocks of the diffusion model. Following conventions from prior work\nthat use samples for CKA [12, 73], we use the 2,500 image test set of ImageNet-50 (see Table 5). We\nfirst examine differences in the representations between guided diffusion blocks at various time steps\nand feature dimensions (pooling size) within our diffusion method in Figure 6. We also compare\nour standard setting (t = 90 and d = 4096) against ResNet-50 and ViT representations with a\nrepresentative set of popular pre-training methods, as well as stable diffusion. For ResNet-50, we\nextract the features from each bottleneck block while for ViT we extract features from each hidden\nlayer.\nWe note that the early layers tend to have higher similarity in all cases, suggesting that diffusion\nmodels likely capture similar low-level details in the first few blocks. Also note the impact of\nthe time step: the representations are very dissimilar at later layers when the representations are\ncomputed using images from different noise time steps. However, interestingly, we find that around\nthe bottleneck, the layers of GD tend to have similar representations to ResNets and ViTs, suggesting\nthat GD\u2019s later layers naturally learn discriminative properties. This further supports our findings in\nTable 1 and Table 3, where we show the promising classification performance with GD features.\n8\nFigure 6: Feature representation comparisons via centered kernel alignment (CKA). On the top 2\nrows, we compare guided diffusion (GD) representations between its own layers, at varying time\nsteps and feature size. On the bottom 2 rows, we compare GD, with standard t = 90 and d = 4096,\nagainst both ResNets and ViTs with various pre-training methods. For the bottom right corner we\ncompare against Stable Diffusion (SD), b = 18, size = 512.\n5\nConclusion\nIn this paper, we present an approach for using the representations learned by diffusion models\nfor classification tasks. This re-positions diffusion models as potential state-of-the-art unified self-\nsupervised representation learners. We explain best practices for identifying these representations\nand provide initial guidance for extracting high-utility discriminative embeddings from the diffusion\nprocess. We demonstrate promising transfer learning properties and investigate how different datasets\nrequire different approaches to feature extraction. We compare the diffusion representations in\nterms of CKA, both to show what diffusion models learn at different layers as well as how diffusion\nrepresentations compare to those from other methods.\nBroader Impacts. With our paper, we analyze algorithms; we do not provide new real-world\napplications. Nevertheless, our work deals with image generation, which carries ethical concerns with\nrespect to potential misinformation generation. However, we do not improve over existing generation\napproaches, so the potential harms seem negligible.\nLimitations. Training diffusion models, even just for linear probing, is very computationally intensive.\nSo, we could not provide an analysis of variability in this work. Nevertheless, our work is an important\nfirst step for leveraging the capacity of diffusion models for discriminative tasks.\n9\nReferences\n[1] W. Van Gansbeke, S. Vandenhende, S. Georgoulis, M. Proesmans, and L. Van Gool, \u201cScan: Learning to\nclassify images without labels,\u201d in Proceedings of the European Conference on Computer Vision, 2020.\n[2] J. Donahue, P. Kr\u00e4henb\u00fchl, and T. Darrell, \u201cAdversarial feature learning,\u201d arXiv preprint arXiv:1605.09782,\n2016.\n[3] J. Donahue and K. Simonyan, \u201cLarge scale adversarial representation learning,\u201d Advances in neural\ninformation processing systems, vol. 32, 2019.\n[4] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d 2015.\n[5] K. Gupta, S. Singh, and A. Shrivastava, \u201cPatchvae: Learning local latent codes for recognition,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020.\n[6] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d 2022.\n[7] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, \u201cMaskgit: Masked generative image transformer,\u201d\nin The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.\n[8] T. Li, H. Chang, S. K. Mishra, H. Zhang, D. Katabi, and D. Krishnan, \u201cMage: Masked generative encoder\nto unify representation learning and image synthesis,\u201d 2022.\n[9] J. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. \u00c1. Pires, Z. D.\nGuo, M. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko, \u201cBootstrap your own latent: A new\napproach to self-supervised learning,\u201d CoRR, vol. abs/2006.07733, 2020.\n[10] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. B. Girshick, \u201cMasked autoencoders are scalable vision\nlearners,\u201d CoRR, vol. abs/2111.06377, 2021.\n[11] T. Xiao, X. Wang, A. A. Efros, and T. Darrell, \u201cWhat should not be contrastive in contrastive learning,\u201d\n2021.\n[12] M. Gwilliam and A. Shrivastava, \u201cBeyond supervised vs. unsupervised: Representative benchmarking\nand analysis of image representation learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 9642\u20139652, June 2022.\n[13] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton, \u201cSimilarity of neural network representations revisited,\u201d\nin Proceedings of the 36th International Conference on Machine Learning (K. Chaudhuri and R. Salakhut-\ndinov, eds.), vol. 97 of Proceedings of Machine Learning Research, pp. 3519\u20133529, PMLR, 09\u201315 Jun\n2019.\n[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at\nscale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n\u201cGenerative adversarial networks,\u201d Communications of the ACM, vol. 63, no. 11, pp. 139\u2013144, 2020.\n[16] T. Karras, T. Aila, S. Laine, and J. Lehtinen, \u201cProgressive growing of gans for improved quality, stability,\nand variation,\u201d arXiv preprint arXiv:1710.10196, 2017.\n[17] A. Brock, J. Donahue, and K. Simonyan, \u201cLarge scale gan training for high fidelity natural image synthesis,\u201d\narXiv preprint arXiv:1809.11096, 2018.\n[18] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2019.\n[19] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, \u201cAnalyzing and improving the\nimage quality of stylegan,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 8110\u20138119, 2020.\n[20] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila, \u201cTraining generative adversarial\nnetworks with limited data,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 12104\u201312114,\n2020.\n10\n[21] T. Karras, M. Aittala, S. Laine, E. H\u00e4rk\u00f6nen, J. Hellsten, J. Lehtinen, and T. Aila, \u201cAlias-free generative\nadversarial networks,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 852\u2013863, 2021.\n[22] A. Sauer, K. Schwarz, and A. Geiger, \u201cStylegan-xl: Scaling stylegan to large diverse datasets,\u201d\nvol. abs/2201.00273, 2022.\n[23] W. Xia, Y. Zhang, Y. Yang, J. Xue, B. Zhou, and M. Yang, \u201cGAN inversion: A survey,\u201d CoRR,\nvol. abs/2101.05278, 2021.\n[24] R. Abdal, Y. Qin, and P. Wonka, \u201cImage2stylegan: How to embed images into the stylegan latent space?,\u201d\nCoRR, vol. abs/1904.03189, 2019.\n[25] E. Richardson, Y. Alaluf, O. Patashnik, Y. Nitzan, Y. Azar, S. Shapiro, and D. Cohen-Or, \u201cEncoding in\nstyle: a stylegan encoder for image-to-image translation,\u201d CoRR, vol. abs/2008.00951, 2020.\n[26] O. Tov, Y. Alaluf, Y. Nitzan, O. Patashnik, and D. Cohen-Or, \u201cDesigning an encoder for stylegan image\nmanipulation,\u201d CoRR, vol. abs/2102.02766, 2021.\n[27] J.-Y. Zhu, P. Kr\u00e4henb\u00fchl, E. Shechtman, and A. A. Efros, \u201cGenerative visual manipulation on the natural\nimage manifold,\u201d in European conference on computer vision, pp. 597\u2013613, Springer, 2016.\n[28] J. Zhu, Y. Shen, D. Zhao, and B. Zhou, \u201cIn-domain gan inversion for real image editing,\u201d in European\nconference on computer vision, pp. 592\u2013608, Springer, 2020.\n[29] D. Roich, R. Mokady, A. H. Bermano, and D. Cohen-Or, \u201cPivotal tuning for latent-based editing of real\nimages,\u201d ACM Transactions on Graphics (TOG), vol. 42, no. 1, pp. 1\u201313, 2022.\n[30] Y. Alaluf, O. Tov, R. Mokady, R. Gal, and A. Bermano, \u201cHyperstyle: Stylegan inversion with hypernetworks\nfor real image editing,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18511\u201318521, 2022.\n[31] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in Neural Information\nProcessing Systems, vol. 33, pp. 6840\u20136851, 2020.\n[32] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image synthesis,\u201d 2021.\n[33] A. Razavi, A. Van den Oord, and O. Vinyals, \u201cGenerating diverse high-fidelity images with vq-vae-2,\u201d\nAdvances in neural information processing systems, vol. 32, 2019.\n[34] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image\ndatabase,\u201d in 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255, Ieee, 2009.\n[35] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHierarchical text-conditional image generation\nwith clip latents,\u201d arXiv preprint arXiv:2204.06125, 2022.\n[36] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\nB. Karagol Ayan, T. Salimans, et al., \u201cPhotorealistic text-to-image diffusion models with deep language\nunderstanding,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 36479\u201336494, 2022.\n[37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with\nlatent diffusion models,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 10684\u201310695, 2022.\n[38] S. Chen, P. Sun, Y. Song, and P. Luo, \u201cDiffusiondet: Diffusion model for object detection,\u201d arXiv preprint\narXiv:2211.09788, 2022.\n[39] R. Burgert, K. Ranasinghe, X. Li, and M. S. Ryoo, \u201cPeekaboo: Text to image diffusion models are zero-shot\nsegmentors,\u201d arXiv preprint arXiv:2211.13224, 2022.\n[40] R. Zhang, P. Isola, and A. A. Efros, \u201cColorful image colorization,\u201d in European conference on computer\nvision, pp. 649\u2013666, Springer, 2016.\n[41] M. Noroozi and P. Favaro, \u201cUnsupervised learning of visual representations by solving jigsaw puzzles,\u201d in\nEuropean conference on computer vision, pp. 69\u201384, Springer, 2016.\n[42] I. Misra and L. v. d. Maaten, \u201cSelf-supervised learning of pretext-invariant representations,\u201d in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6707\u20136717, 2020.\n11\n[43] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, \u201cContext encoders: Feature learning\nby inpainting,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 2536\u20132544, 2016.\n[44] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual\nrepresentations,\u201d in International conference on machine learning, pp. 1597\u20131607, PMLR, 2020.\n[45] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, \u201cBig self-supervised models are strong\nsemi-supervised learners,\u201d Advances in neural information processing systems, vol. 33, pp. 22243\u201322255,\n2020.\n[46] X. Chen, H. Fan, R. B. Girshick, and K. He, \u201cImproved baselines with momentum contrastive learning,\u201d\nCoRR, vol. abs/2003.04297, 2020.\n[47] X. Chen, H. Fan, R. Girshick, and K. He, \u201cImproved baselines with momentum contrastive learning,\u201d arXiv\npreprint arXiv:2003.04297, 2020.\n[48] X. Chen*, S. Xie*, and K. He, \u201cAn empirical study of training self-supervised vision transformers,\u201d arXiv\npreprint arXiv:2104.02057, 2021.\n[49] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, \u201cBarlow twins: Self-supervised learning via redundancy\nreduction,\u201d in Proceedings of the 38th International Conference on Machine Learning (M. Meila and\nT. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 12310\u201312320, PMLR, 18\u201324\nJul 2021.\n[50] N. Tomasev, I. Bica, B. McWilliams, L. Buesing, R. Pascanu, C. Blundell, and J. Mitrovic, \u201cPushing the\nlimits of self-supervised resnets: Can we outperform supervised learning without labels on imagenet?,\u201d\n2022.\n[51] X. Chen and K. He, \u201cExploring simple siamese representation learning,\u201d in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 15750\u201315758, 2021.\n[52] A. Bardes, J. Ponce, and Y. LeCun, \u201cVicreg: Variance-invariance-covariance regularization for self-\nsupervised learning,\u201d ArXiv, vol. abs/2105.04906, 2021.\n[53] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, \u201cDeep clustering for unsupervised learning of visual\nfeatures,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), pp. 132\u2013149, 2018.\n[54] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, \u201cUnsupervised learning of visual\nfeatures by contrasting cluster assignments,\u201d Advances in Neural Information Processing Systems, vol. 33,\npp. 9912\u20139924, 2020.\n[55] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin, \u201cEmerging properties in\nself-supervised vision transformers,\u201d in Proceedings of the International Conference on Computer Vision\n(ICCV), 2021.\n[56] K. Gupta, G. Somepalli, A. Anubhav, V. Y. J. Magalle Hewa, M. Zwicker, and A. Shrivastava, \u201cPatchgame:\nLearning to signal mid-level patches in referential games,\u201d in Advances in Neural Information Processing\nSystems (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 26015\u2013\n26027, Curran Associates, Inc., 2021.\n[57] P. Goyal, Q. Duval, I. Seessel, M. Caron, I. Misra, L. Sagun, A. Joulin, and P. Bojanowski, \u201cVision models\nare more robust and fair when pretrained on uncurated images without supervision,\u201d 2022.\n[58] B. Pang, Y. Zhang, Y. Li, J. Cai, and C. Lu, \u201cUnsupervised visual representation learning by synchronous\nmomentum grouping,\u201d 2022.\n[59] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa,\nA. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat,\nV. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski, \u201cDinov2:\nLearning robust visual features without supervision,\u201d 2023.\n[60] P. Zhou, Y. Zhou, C. Si, W. Yu, T. K. Ng, and S. Yan, \u201cMugs: A multi-granular self-supervised learning\nframework,\u201d 2022.\n[61] C. Li, J. Yang, P. Zhang, M. Gao, B. Xiao, X. Dai, L. Yuan, and J. Gao, \u201cEfficient self-supervised vision\ntransformers for representation learning,\u201d 2022.\n12\n[62] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville, \u201cAdversari-\nally learned inference,\u201d arXiv preprint arXiv:1606.00704, 2016.\n[63] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, \u201cInfogan: Interpretable\nrepresentation learning by information maximizing generative adversarial nets,\u201d Advances in neural\ninformation processing systems, vol. 29, 2016.\n[64] W. Nie, T. Karras, A. Garg, S. Debnath, A. Patney, A. B. Patel, and A. Anandkumar, \u201cSemi-supervised\nstylegan for disentanglement learning,\u201d in Proceedings of the 37th International Conference on Machine\nLearning, pp. 7360\u20137369, 2020.\n[65] S. Pidhorskyi, D. A. Adjeroh, and G. Doretto, \u201cAdversarial latent autoencoders,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14104\u201314113, 2020.\n[66] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, \u201cibot: Image bert pre-training with\nonline tokenizer,\u201d 2022.\n[67] M. Assran, M. Caron, I. Misra, P. Bojanowski, F. Bordes, P. Vincent, A. Joulin, M. Rabbat, and N. Ballas,\n\u201cMasked siamese networks for label-efficient learning,\u201d 2022.\n[68] H. Bao, L. Dong, S. Piao, and F. Wei, \u201cBeit: Bert pre-training of image transformers,\u201d 2022.\n[69] Z. Huang, X. Jin, C. Lu, Q. Hou, M.-M. Cheng, D. Fu, X. Shen, and J. Feng, \u201cContrastive masked\nautoencoders are stronger vision learners,\u201d 2022.\n[70] A. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffusion probabilistic models,\u201d in International\nConference on Machine Learning, pp. 8162\u20138171, PMLR, 2021.\n[71] P. Goyal, Q. Duval, J. Reizenstein, M. Leavitt, M. Xu, B. Lefaudeux, M. Singh, V. Reis, M. Caron,\nP. Bojanowski, A. Joulin, and I. Misra, \u201cVissl.\u201d https://github.com/facebookresearch/vissl,\n2021.\n[72] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d 2017.\n[73] M. Walmer, S. Suri, K. Gupta, and A. Shrivastava, \u201cTeaching matters: Investigating the role of supervision\nin vision transformers,\u201d 2023.\n[74] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi, \u201cFine-grained visual classification of aircraft,\u201d\ntech. rep., 2013.\n[75] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, \u201c3d object representations for fine-grained categorization,\u201d in\n4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), (Sydney, Australia),\n2013.\n[76] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, \u201cThe Caltech-UCSD Birds-200-2011 Dataset,\u201d\nTech. Rep. CNS-TR-2011-001, California Institute of Technology, 2011.\n[77] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei, \u201cNovel dataset for fine-grained image categoriza-\ntion,\u201d in First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision\nand Pattern Recognition, (Colorado Springs, CO), June 2011.\n[78] M.-E. Nilsback and A. Zisserman, \u201cAutomated flower classification over a large number of classes,\u201d in\nIndian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.\n[79] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie, \u201cBuilding a\nbird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset\ncollection,\u201d in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 595\u2013604,\n2015.\n13\nDiffusion Models Beat GANs on Image Classification\nSupplementary Material\nA\nMethod Details\nConvolutional Heads. In the context of standard linear probing, we treat our convolutional head as a\nreplacement for the feature pooling step. That is, instead of an adaptive average pool followed by a\nlinear layer, we train a convolutional head, followed by a linear layer. While we explore different\nchannel dimensions, the architecture consists of 2 blocks, each with a 2\u00d72 convolution followed by a\n2\u00d72 maximum pooling operation. We perform a 2\u00d72 adaptive average pool, flattening the result so\nit can be used by a linear layer. We indicate the convolution heads we train in Table 8, where the first\n(input) channels is always 1024 (the number of channels of the feature maps at the chosen U-Net\nblock), but the output channels of both learned convolution operations is treated as a hyperparameter.\nAttention Heads.\nIn addition to applying convolutional heads, we experiment with a more sophisticated architecture\napplying Transformer blocks. Specifically, we first use a 2\u00d72 pooling layer to reduce the spatial\nresolution of the feature maps to 8\u00d78. Each token has a channel dimension of 1024, corresponding\nto the number of channels of the feature maps extracted from the U-Net block. We then flatten\nthese features to generate 64 tokens for our Transformer. We append a CLS token to these set of\ntokens which we then use to make the final classification. We follow the standard Transformer block\narchitecture consisting of Layer Normalization and QKV-Attenton. We treat the number of attention\nblocks as a hyperparameter and our choices and the corresponding results are shown in Table 9.\nB\nExperiment Details\nWe provide additional clarifying details for our experiments. First, the ablations shown in Figure 2 are\nthe result of training on ImageNet for 15 epochs, instead of our complete linear probing recipe, due\nto resource constraints. Second, the stable diffusion model we compare to was text-guided during its\npre-training, and thus, unlike our guided diffusion checkpoint, it was not fully unsupervised. Finally,\nwe provide parameter count comparisons for our method and the other unified representation learners\nin Table 10.\nC\nAblations\nAircraft\nCars\nCUB\nDogs\nFlowers\nNABirds\nDataset\n20\n40\n60\n80\n100\nAccuracy (%)\nGD (L)\nGD (MLP)\nGD (CNN)\nGD (Attention)\nBigBiGAN (L)\nSimCLR (L)\nSwAV (L)\nFigure 7: Fine-Grained Visual Classification (FGVC) results, where the classification heads were\ntrained with random resize cropping. Similar to Figure 4 in terms of methods chosen, except now we\nprovide results for BigBiGAN also.\nAs noted, in the main paper, we use only random flipping for our linear probe, unlike standard\nsettings which also use a random resized crop. This is in part due to the fact that while standard\nunsupervised pre-training would use random cropping, guided diffusion does not. However, for the\nsake of thorough comparison, we also try FGVC linear probing with the standard, random resize\n1\ncropping setup. We provide these results in Figure 7. Note that the diffusion heads tend to perform\nworse with the extra augmentations, possibly due to a lack of learned invariance. We also include\nBigBiGAN. Note that this model was implemented in Tensorflow, and we also perform the linear\nprobing in Tensorflow for this model only.\nTable 5: Dataset details used in this work.\nDataset\n#Cls\n#Train\n#Test\nAircraft [74]\n100\n6,667\n3,333\nCars [75]\n196\n8,144\n8,041\nCUB [76]\n200\n5,994\n5,794\nDogs [77]\n120\n12,000\n8,580\nFlowers [78]\n102\n2,040\n6,149\nNABirds [79]\n555\n23,929\n24,633\nImageNet [34]\n1000\n1.3mil\n50,000\nImageNet-50 [1]\n50\n64,274\n2,500\nTable 6: Linear probing result with our best setting\nand augmentation.\nSetting\nAccuracy\nLinear Probe (t = 90, b = 24, Pool=2)\n65.07 %\nWe also try data augmentations for our best linear probe setting, with results provided in Table 6\nafter 15 epochs. We do not observe any significant improvement over the original setting without\naugmentation.\nD\nFigure 1 Details\nNow, we provide more details, in Table 7, Table 8, and Table 9 for the information shown in Figure 1.\nWe show exact accuracies and parameter counts. We also specify our hyperparameter selection for\neach head.\nTable 7: Linear and MLP results. For linear, 1k,\n4k, 16k, and 65k indicate the size of the feature\nafter pooling and flattening. For MLP, the first\nnumber is the size of the feature after pooling\nand flattening, and the succeeding numbers are\nhidden sizes of layers before the last (classifica-\ntion) layer.\nHead\nParams\nAccuracy\nLinear-1k\n1M\n87.20%\nLinear-4k\n4M\n89.41%\nLinear-16k\n16M\n89.58%\nLinear-65k\n65M\n88.28%\nMLP-1k-2k\n4M\n87.93%\nMLP-4k-2k\n10M\n88.50%\nMLP-4k-2k-2k\n14M\n88.67%\nMLP-16k-2k\n34M\n89.76%\nTable 8: CNN head results. Channel sizes are\nseparated by dashes. The first is the channel size\nof the input feature maps, the next is the output\nchannels from the first convolution, and the last\nis the output dimension of the second convolu-\ntion. These are treated as hyperparameters. For\nmore detail, see our code.\nHead\nParams\nAccuracy\nCNN-1k-256-256\n2.5M\n88.98%\nCNN-1k-512-256\n3.5M\n88.67%\nCNN-1k-1k-256\n6M\n88.93%\nCNN-1k-1k-1k\n12M\n89.50%\nCNN-1k-2k-512\n14M\n89.19%\nCNN-1k-2k-2k\n32M\n89.76%\nCNN-1k-4k-2k\n48M\n89.89%\nCNN-1k-4k-2.5k\n66M\n89.50%\nTable 9: Attention head results. The hyperparameters are\ndenoted following the dashes. The first hyperparameter is\nsimilarly the channel size of the input feature maps and the\nnext is the number of Transformer blocks used.\nHead\nParams\nAccuracy\nAttention-1K-1\n13.7M\n91.67%\nAttention-1K-2\n26.2M\n91.58%\nAttention-1K-3\n38.8M\n92.01%\nAttention-1K-4\n51.4M\n92.27%\nAttention-1K-5\n64.0M\n92.71%\nTable 10: Parameter counts of ma-\njor unified unsupervised represen-\ntation learning methods. For each,\nwe consider the whole system, not\njust the encoding network.\nMethod\n# Params\nBigBiGAN\n502M\nMAGE\n439M\nOurs\n553M\n2\n"
  },
  {
    "title": "CoTracker: It is Better to Track Together",
    "link": "https://arxiv.org/pdf/2307.07635.pdf",
    "upvote": "9",
    "text": "CoTracker: It is Better to Track Together\nNikita Karaev1,2\nIgnacio Rocco1\nBenjamin Graham1\nNatalia Neverova1\nAndrea Vedaldi1\nChristian Rupprecht2\n1 Meta AI\n2 Visual Geometry Group, University of Oxford\nnikita@robots.ox.ac.uk\nFigure 1. CoTracker is a new dense point tracker. It can track 70k points jointly on a single GPU and exploit the correlation between\ndifferent tracks to improve its performance. It has exceptional long-term tracking performance even in the presence of occlusions or when\npoints leave the camera view (examples from DAVIS [32], the bottom row shows object tracks only).\nAbstract\nWe introduce CoTracker, a transformer-based model that\ntracks dense points in a frame jointly across a video se-\nquence. This differs from most existing state-of-the-art ap-\nproaches that track points independently, ignoring their\ncorrelation. We show that joint tracking results in a sig-\nnificantly higher tracking accuracy and robustness.\nWe also provide several technical innovations, includ-\ning the concept of virtual tracks, which allows CoTracker to\ntrack 70k points jointly and simultaneously. Furthermore,\nCoTracker operates causally on short windows (hence, it\nis suitable for online tasks), but is trained by unrolling the\nwindows across longer video sequences, which enables and\nsignificantly improves long-term tracking.\nWe demonstrate qualitatively impressive tracking results,\nwhere points can be tracked for a long time even when they\nare occluded or leave the field of view. Quantitatively, Co-\nTracker outperforms all recent trackers on standard bench-\nmarks, often by a substantial margin.\nCode and model\nweights are available at: https://co-tracker.github.io/\n1. Introduction\nEstablishing point correspondences is a fundamental prob-\nlem in computer vision, necessary for many downstream\ntasks. Here, we are interested in estimating point correspon-\ndences in videos containing dynamic objects and a moving\ncamera. Specifically, given one or more 2D points which\nare the projection of certain physical points of the underly-\ning 3D scene, the goal is to find the location of the same\nphysical points in all other frames of the video.\nThere are two variants of this problem. In optical flow,\nthe objective is to estimate the velocity of all points within\na video frame. This estimation is performed jointly for all\npoints, but the motion is only predicted at an infinitesimal\ndistance. In tracking, the goal is to estimate the motion\nof points over an extended period of time. For efficiency,\ntracking methods typically focus on a sparse set of points\nand treat them as statistically independent.\nEven recent\ntechniques such as TAPIR [11] and PIPs++ [52], which em-\nploy modern architectures and can track points even in the\npresence of occlusions, model tracks independently. How-\never, this approximation is crude as points are often strongly\ncorrelated (e.g., when they belong to the same object).\narXiv:2307.07635v2  [cs.CV]  26 Dec 2023\nIn this paper, we hypothesise that accounting for the cor-\nrelation between tracked points can significantly improve\nperformance. Thus, we introduce CoTracker, a new neu-\nral tracker that, compared to prior deep networks for optical\nflow and point tracking, supports joint estimation of a very\nlarge number of tracks simultaneously. CoTracker outper-\nforms prior trackers on all common tracking benchmarks\nby substantial margins, is based on a simple and extensible\narchitecture, and is as flexible as prior solutions for single-\npoint tracking, meaning that it can track arbitrary points,\nselected at any spatial location and time in the video, and\nsupport arbitrary configurations of such points.\nWe show that tracking more points jointly significantly\nimproves tracking metrics. In fact, we also show that in-\njecting additional support points (i.e., points that are not re-\nquested by the user but help the tracker) can further improve\nperformance, and we study several support configurations.\nCoTracker achieves its high performance also due to sev-\neral other innovations in its design and learning formula-\ntion. The network is a transformer operating in a sliding\nwindow fashion on a two-dimensional representation of to-\nkens, where dimensions are time and the set of tracked\npoints. Via suitable self-attention operators, the transformer\ncan consider each track as a whole for the duration of a\nwindow and can exchange information between tracks, thus\nexploiting their correlation.\nWhen tracking a very large\nnumber of points, furthermore, CoTracker introduces in its\ntransformer design a small number of representative vir-\ntual track tokens, switching from expensive self-attention\nbetween virtual tracks to cross-attention between the latter\nand the real ones. This way, CoTracker can jointly track a\nnear-dense set of tracks on a single GPU.\nAnother important contribution is unrolled training. Co-\nTracker is an online algorithm that processes videos in a\nserial fashion via a sliding window. Within a window, the\ntracks are initialized with queried points, and the network\nis tasked with progressively refining these initial estima-\ntions through iterative applications of the transformer. Each\nsubsequent overlapping window starts with refined predic-\ntions of the previous window and updates tracks for the new\nframes. We mask out points that have not yet been initial-\nized in the current sliding window, which allows initializing\ntracks from any point in the video, including in the mid-\ndle of a window, and to join windows together. We show\nthat, by training the network by unrolling several sliding\nwindows, CoTracker achieves excellent long-term tracking\nperformance as well.\nWe\ntrain\nCoTracker\non\nthe\nsynthetic\nTAP-Vid-\nKubric [10] and evaluate it on three different bench-\nmarks, TAP-Vid-DAVIS, PointOdyssey [52] and Dynami-\ncReplica [23]. Our architecture works well for tracking sin-\ngle points and excels for groups of points, obtaining state-\nof-the-art tracking performance in several benchmarks.\n2. Related work\nOptical flow. Optical flow estimates dense instantaneous\nmotion. Originally approached by studying the colour con-\nstancy equation [3, 4, 17, 27], optical flow is nowadays tack-\nled by using deep learning, starting with FlowNet [12, 19].\nMore recently, DCFlow [48] has introduced the computa-\ntion of a 4D cost volume, used in most follow-up works [41,\n43, 45]. Notable is RAFT [43], which introduced incre-\nmental flow updates and inspired a wave of subsequent\nworks [22, 22, 47, 49]. We also adopt 4D cost volumes\nand iterated updates, but apply them to tracking.\nTransformers [44] have also been applied to the optical\nflow problem [18, 36, 50]. Flowformer [18] drew inspi-\nration from RAFT and proposed a transformer-based ap-\nproach that tokenizes the 4D cost volume. GMFlow [50]\nreplaced the update network with a softmax with self-\nattention for refinement. Perceiver IO [20] proposed a uni-\nfied transformer for several tasks including optical flow.\nOptical flow can in principle be applied to tracking by in-\ntegrating predictions over time, but this open-loop approach\naccumulates errors in the form of drift, which motivates de-\nveloping dedicated architectures like ours.\nMulti-frame optical flow. There have been several ap-\nproaches [21, 28, 33, 35] to extend optical flow to multi-\nple frames. Traditionally, Kalman filtering [9, 13] was a\nstaple mechanism to ensure temporal consistency. Modern\nmulti-frame methods produce dense flow. RAFT [43] can\nbe applied in a warm-start manner [39, 42] for multi-frame\noptical flow estimation. However, these methods are not\ndesigned for long-term tracking and do not consider points\noccluded for a long time. VideoFlow [35] extends optical\nflow to three and five consecutive frames by integrating for-\nward and backward motion features during flow refinement.\nMFT [31] conducts optical flow estimation between distant\nframes and chooses the most reliable chain of optical flows.\nOur tracker scales to produce semi-dense tracks.\nJoint Tracking. Before the emergence of deep learning,\nsome authors proposed handcrafted joint trackers [2, 34],\nbut there is less work on doing so with modern deep net-\nwork architectures like ours. Our work is weakly related to\nmultiple object tracking [8] where tracking through occlu-\nsions [51], with changes in appearance [29], and with tem-\nporal priors [37] have been extensively studied. However,\nour focus is the tracking of points, not objects.\nTracking any point. TAP-Vid [10] introduced the problem\nof tracking any physical point in a video and proposed a\nbenchmark and a simple baseline method for it, but is un-\nable to track occluded points. PIPs [15] revisits the clas-\nsic Particle Video [34] problem by introducing a model for\npoint tracking through occlusions. The method tracks se-\nlected points with a fixed sliding window and restarts from\n2\n<latexit sha1_base64=\"o+leYz1jIkmMxZqS249\n8E8pd5tg=\">ACI3icbVDLSsNAFJ3UV42vqEs3wVaom5J0oeKq4EZ3FewDmlAm05t26GQSZiZCf0XN/\n6KGxdKcePCf3HaRtDWAwOHc85l7j1BwqhUjvNpFNbWNza3itvmzu7e/oF1eNScSoINEnMYtEJsARGOTQ\nVQw6iQAcBQzawehm5rcfQUga8wc1TsCP8IDTkBKstNSzr0ABpRnBLgCMTGp9sHzBCwSgVITcteMqSV\nu546L5se8P5PtmeVnKozh71K3JyUI5Gz5p6/ZikR4nDEvZdZ1E+RkWihIGE9NLJSYjPQGXU05jkD62\nfzGiX2mlb4dxkI/ruy5+nsiw5GU4yjQyQiroVz2ZuJ/XjdV4ZWfUZ6kCjhZfBSmzFaxPSvM7lMBRLGxJ\npgIqne1yRALTHQH0tQluMsnr5JWrepeVGv3tVLdyesohN0irIRZeojm5RAzURQU/oBb2hd+PZeDWmxs\nciWjDymWP0B8bXN3TkpBQ=</latexit> image\nfeatures\n\u03c6(It)\n<latexit sha1_base64=\"69KHIzyG1sqYJwDy+kZYmumMoSA=\">ACG3icbVDLSsNAFJ34rPEVdekm2\nAquStKFuiy40V0F+4AmlMn0ph06mYSZiVBC/8ONv+LGhSKuBf+jZM0grYeGDicy93zgkSRqVynC9jZXVtfWOzsmVu7+zu7VsHhx0Zp4JAm8QsFr0AS2CUQ1tRxaCXCMBRwKAbTK5yv3sPQtKY36lpAn6ER5yGlGC\nlpYHV8AIYUZ4R4ArEzKQ8SZXnmaHAEUhNajcDVTM94MOfmYFVdepOAXuZuCWpohKtgfXhDWOSRnqdMCxl3US5WdYKEoYzEwvlZBgMsEj6GvK8t+VmSb2adaGdphLPTjyi7U3xsZjqScRoGejLAay0UvF/z+qkKL\n/2syAuczA+FKbNVbOdF2UMqgCg21QTQfVfbTLGAhPdgTR1Ce5i5GXSadTd83rjtlFtOmUdFXSMTtAZctEFaqJr1EJtRNADekIv6NV4NJ6N+N9PrpilDtH6A+Mz2+zMqEi</latexit> input\nframes\nIt\n<latexit sh\na1_base64=\"PwtfvFDbg\ndWVdu4z4sj41lAP3S4=\"\n>ACLXicbVDLSsNAFJ3\nUV42vqks3g23BVUm6UJcF\nXbisYB/QhDKZ3LRDJ5Mw\nMxFK6A+58VdEcFERt/6G\n04egrRcGzpx7D/fcE6Sc\nKe04U6uwsbm1vVPctf2\nDw6PSscnbZVkKLJjyR3\nYAo4ExASzPNoZtKIHAo\nROMbmb9ziNIxRLxoMcp+\nDEZCBYxSrSh+qXbqhfAg\nImcgtAgJ7YnEiZC8GgNI\nuJhtDzbC0JHSlc8YZE58\n1Jxa56IMIfUb9UdmrOvP\nA6cJegjJbV7JdevTChW\nzklBOleq6Taj8nUjPKwZ\njIFKRmIxlAz0BYlB+Pr9\n2gquGCXGUSPOMzTn7W5G\nTWKlxHJhJY3+oVnsz8r9\neL9PRtZ8zkWYaBF0sijK\nOdYJn0eGQSaCajw0gVDLj\nFdMhMcmYDJRtQnBXT14H\n7XrNvazV7+vlhrOMo4jO\n0Dm6QC6Qg10h5qohSh6\nQi9oit6tZ+vN+rA+F6MF\na6k5RX/K+voGUNWouA=<\n/latexit>estimated\ntracks \u02c6P\n<latexit sh\na1_base64=\"hqk+fqUM\n8qcUoYPIqe70JDGKpw=\"\n>ACK3icbVDLSgMxFM3\nUVx1fVZdugm3BVZnpQl0W\n3bisYB/QKSWTuW1DM8mQ\nZIQy9H/c+CsudOEDt/6H\n6UPQ1gOBk3Pv4d57woQz\nbTzv3cmtrW9sbuW3Z3d\nvf2DwuFRU8tUWhQyaVqh\n0QDZwIahkO7UQBiUMOr\nXB0Pa237kFpJsWdGSfQj\nclAsD6jxFipV7gqByEMm\nMgoCANq4gZCMhHZD9aGKM\nPEIAhcLuf9Gpey+qTklg\nMQ0Y+lVyh6FW8GvEr8BS\nmiBeq9wnMQSZrG1k450b\nrje4npZtNplINdIdWQED\noiA+hYKkgMupvNbp3gslU\ni3JfKPrvkTP3tyEis9Tg\nObWdMzFAv16bif7VOavq\nX3YyJDUg6HxQP+XYSDw\nNDkdMATV8bAmhitldMR0S\nRajNQLs2BH/5FXSrFb8\n80r1tlqseYs48ugEnaIz\n5KMLVEM3qI4aiKIH9IRe\n0Zvz6Lw4H87nvDXnLDzH\n6A+cr2/Xaf7</latexit\n>starting\nlocations P\n<latex\nit sha1_ba\nse64=\"lF/1\nSKOpx9gH84\n1lDpGx/cwG\n/jA=\">AC\nKXicbVDLSg\nMxFM3UVx1f\nVZdugq3gqs\nx0oS4LblxWs\nA/olJLJ3La\nhmWRIMkIZ+\njtu/BU3Coq\n69UfMtBW09\nUDg5Nx7uPe\neMOFMG8/7c\nApr6xubW8V\ntd2d3b/+gd\nHjU0jJVFJpU\ncqk6IdHAmY\nCmYZDJ1FA\n4pBDOxf5/\nX2PSjNpLgz\nkwR6MRkKNm\nCUGCv1S/Ug\nhCETGQVhQE\n3dQEgmIvB\n2hBlmBgGgcv\nlvF3jStaYV\ntwARPTj6Jf\nKXtWbAa8Sf\n0HKaIFGv/Q\nSRJKmsbVT\nrTu+l5ielk\n+jHKwG6QaE\nkLHZAhdSwW\nJQfey2aVTfG\naVCA+ks/u\nOFN/OzISaz\n2JQ9sZEzPS\ny7Vc/K/WTc\n3gqpcxkaQG\nBJ0PGqQcG4\nnz2HDEFD\nJ5YQqpjdFd\nMRUYTaDLRrQ\n/CXT14lrVr\nVv6jWbmvlu\nreIo4hO0Ck\n6Rz6RHV0g\nxqoiSh6QE/\noFb05j86z8\n+58zlsLzsJ\nzjP7A+foG/\nY6nQ=</la\ntexit> starting\nlocations P\n<latexit sha1_base64=\n\"JUBaTEzEF5DBdTa6H+4C9W+xdQ=\">ACDHicb\nVDLSsNAFJ3UV42vqks3wVaoICXJQl0W3LiSCvYBTS\niTyW07dDIJMxOhH6AG3/FjQtF3PoB7vwbp20EbT\n0wcDjnXO7cEySMSmXbX0ZhZXVtfaO4aW5t7+zulfY\nPWjJOBYEmiVksOgGWwCiHpqKQScRgKOAQTsYXU3\n9j0ISWN+p8YJ+BEecNqnBCst9UplL4AB5RkBrkB\nMzEr15sw9rZge8PBH1Cm7Zs9gLRMnJ2WUo9ErfXph\nTNJIjxOGpew6dqL8DAtFCYOJ6aUSEkxGeABdTmO\nQPrZ7JiJdaKV0OrHQj+urJn6eyLDkZTjKNDJCKuhX\nPSm4n9eN1X9Sz+jPEkVcDJf1E+ZpWJr2owVUgFEs\nbEmAiq/2qRIRaY6A6kqUtwFk9eJi235pzX3Fu3XK\n/mdRTRETpGVeSgC1RH16iBmoigB/SEXtCr8Wg8G2\n/G+zxaMPKZQ/QHxsc38AKaMw=</latexit>(N, 2)\n<latexit sh\na1_base64=\"JUBaTEzEF\n5DBdTa6H+4C9W+xdQ=\"\n>ACDHicbVDLSsNAFJ3\nUV42vqks3wVaoICXJQl0W\n3LiSCvYBTSiTyW07dDIJ\nMxOhH6AG3/FjQtF3PoB\n7vwbp20EbT0wcDjnXO7c\nEySMSmXbX0ZhZXVtfaO4\naW5t7+zulfYPWjJOBYEmi\nVksOgGWwCiHpqKQScRg\nKOAQTsYXU39j0ISWN+p\n8YJ+BEecNqnBCst9UplL\n4AB5RkBrkBMzEr15sw9rZ\nge8PBH1Cm7Zs9gLRMnJ2\nWUo9ErfXphTNJIjxOGpe\nw6dqL8DAtFCYOJ6aUSEk\nxGeABdTmOQPrZ7JiJda\nKV0OrHQj+urJn6eyLDkZT\njKNDJCKuhXPSm4n9eN1X\n9Sz+jPEkVcDJf1E+ZpWJ\nr2owVUgFEsbEmAiq/2q\nRIRaY6A6kqUtwFk9eJi23\n5pzX3Fu3XK/mdRTRETpG\nVeSgC1RH16iBmoigB/SE\nXtCr8Wg8G2/G+zxaMPKZ\nQ/QHxsc38AKaMw=</la\ntexit>(N, 2)\n<latexit sha1_base64=\n\"B8eX6/eadBGyz1JxXv4qCiLDY0I=\">ACGXicb\nVDLSsNAFJ34rPEVdelmsBVclaQLdVlw47IF+4AmlM\nn0ph06mYSZiVBCf8ONv+LGhSIudeXfOG0jaOuBgc\nM53LnjDlTGnX/bLW1jc2t7ZLO/bu3v7BoXN03FZ\nJim0aMIT2Q2JAs4EtDTHLqpBKHDrh+Gbmd+5\nBKpaIOz1JIYjJULCIUaKN1HdcP4QhEzkFoUFObS0\nJHfu+HQHRmQSFK82K7YMY/CT6TtmtunPgVeIVpIwK\nNPrOhz9IaBabcqJUj3PTXWQE6kZ5TC1/UxBapaS\nIfQMFSQGFeTzy6b43CgDHCXSPKHxXP09kZNYqUkcm\nmRM9EgtezPxP6+X6eg6yJlIMw2CLhZFGc6wbOa8\nIBJoJpPDCFUMvNXTEfElGM6ULYpwVs+eZW0a1Xvsl\nqP1rP1Zr0vomtWMXOC/sD6/AZH9qBn</latexit>pr1sp1t6ijhE7RGbpAHrpCdXSLGqiFKHpAT+gFvV\ntrack\nfeatures Q\n<latexit sha1_\nbase64=\"CkCsl0ugZseh/npGBU\npjJVLvXQ=\">ACDHicbVDLSsNA\nFJ3UV42vqks3wVaoICXpQl0W3L\niSCvYBTSiTyU07dDIJMxOhH6AG\n3/FjQtF3PoB7vwbp20EbT0wcDjn\nXO7c4yeMSmXbX0ZhZXVtfaO4aW5\nt7+zulfYP2jJOBYEWiVksuj6WwC\niHlqKQTcRgCOfQcfXU39zj0IS\nWN+p8YJeBEecBpSgpW+qWy68O\nA8owAVyAmZqV6cxacVkwXePAj6p\nRds2ewlomTkzLK0eyXPt0gJmkx\nwnDUvYcO1FehoWihMHEdFMJCSYj\nPICephxHIL1sdszEOtFKYIWx0I8\nra6b+nshwJOU48nUywmoF72p+J\n/XS1V46WUJ6kCTuaLwpRZKram\nzVgBFUAUG2uCiaD6rxYZYoGJ7kC\naugRn8eRl0q7XnPNa/bZebth5HU\nV0hI5RFTnoAjXQNWqiFiLoAT2hF\n/RqPBrPxpvxPo8WjHzmEP2B8fEN\nQS+abQ=</latexit>(N, d)\n<latexit sha1_base64=\n\"Or3XL5IXh8lzuIVDZ2SARViXoCE=\">ACD3icb\nVDLSsNAFJ3UV42vqks3wVatUEqShbosuHElFfqCJp\nTJ9LYdOpmEmYlQv/Ajb/ixoUibt2682+cPgRtPT\nBwOce7twTxIxKZdtfRmZldW19I7tpbm3v7O7l9g8\naMkoEgTqJWCRaAZbAKIe6opBKxaAw4BMxheT/z\nmPQhJI15Toxj8EPc57VGClZY6uVMvgD7lKQGuQIz\nNQrF2VrotuecF0wPe/dE7ubxdtqewlokzJ3k0R7WT\n+/S6EUlCHScMS9l27Fj5KRaKEgZj0skxJgMcR/a\nmnIcgvT6T1j60QrXasXCf24sqbq70SKQylHYaAnQ\n6wGctGbiP957UT1rvyU8jhRwMlsUS9hloqsSTlWl\nwogio0wURQ/VeLDLDARHcgTV2Cs3jyMm4Zei7N\n65+UpxXkcWHaFjVEQOukQVdIOqI4IekBP6AW9Go\n/Gs/FmvM9GM8Y8c4j+wPj4Bnw0mvg=</latexit>(T 0, N, 2)\n<latexit sha1_base64=\"c1+Q7kqgU9phfQi84+b\nd1J38UQ=\">ACFXicbVDLSsNAFJ3UV42vqEs3g0XoQkrShbosuHFZwT6gLWUyuUmHTiZhZmIpoT/hxl\n9x40IRt4I7/8bpQ9DWAwOHc+7hzj1+ypnSrvtlFdbWNza3itv2zu7e/oFzeNRUSYpNGjCE9n2iQLOBDQ\n0xzaqQS+xa/vB6rfuQSqWiDs9TqEXk0iwkFGijdR3zrs+REzkFIQGObEVZwETER4xESQjXLW7If\nt+U3Io7A14l3oKU0AL1vPZDRKaxSZOVGq47mp7uVEakY5TOxupiAldEgi6BgqSAyql8+umuAzowQ4T\nKR5QuOZ+juRk1ipceybyZjogVr2puJ/XifT4VUvZyLNAg6XxRmHOsETyvCAZNANR8bQqhk5q+YDogk1\nHSgbFOCt3zyKmlWK95FpXpbLdXKizqK6ASdojLy0CWqoRtURw1E0QN6Qi/o1Xq0nq03630+WrAWmWP0B9\nbHN/dqnzw=</latexit>sliding window 2\n<latexit sha1_base64=\"yXo3OgZBzO+ZCcD3sqO\nJqVpUKk=\">ACFXicbVDLSsNAFJ3UV42vqEs3g0XoQkrShbosuHFZwT6gLWUyuUmHTiZhZmIpoT/hxl9x4\n0IRt4I7/8bpQ9DWAwOHc+7hzj1+ypnSrvtlFdbWNza3itv2zu7e/oFzeNRUSYpNGjCE9n2iQLOBDQ0xza\nqQS+xa/vB6rfuQSqWiDs9TqEXk0iwkFGijdR3zrs+REzkFIQGObEVZwETER4xESQj7NldEMGP23dKbs\nWdAa8Sb0FKaIF63/nsBgnNYhOnCjV8dxU93IiNaMcJnY3U5ASOiQRdAwVJAbVy2dXTfCZUQIcJtI8ofFM/\nZ3ISazUOPbNZEz0QC17U/E/r5Pp8KqXM5FmGgSdLwozjnWCpxXhgEmgmo8NIVQy81dMB0QSajpQtinBWz5\nlTSrFe+iUr2tlmrlR1FdIJOURl56BLV0A2qowai6AE9oRf0aj1az9ab9T4fLViLzDH6A+vjG/XZnzs=</l\natexit>sliding window 1\n<latexit sha1_base64=\"i36qv+cQyNjWQaUdep4q\n+ECAmMo=\">ACFXicbVDLSsNAFJ34rPEVdelmsAhdSEkqMuCG5cV7AOaUiaT23boZBJmJpYS+hNu/BU3L\nhRxK7jzb5y2EbT1wMDhnHu4c0+QcKa0635ZK6tr6xubhS17e2d3b985OGyoOJU6jTmsWwFRAFnAuqaQ6t\nRAKJAg7NYHg9Zv3IBWLxZ0eJ9CJSF+wHqNEG6nrnPkB9JnIKAgNcmIrzkIm+njERBiP8Lntgwh/3K5TdM\nvuDHiZeDkpohy1rvPphzFNIxOnCjV9txEdzIiNaMcJrafKkgIHZI+tA0VJALVyWZXTfCpULci6V5QuOZ+\njuRkUipcRSYyYjogVr0puJ/XjvVvatOxkSahB0vqiXcqxjPK0Ih0wC1XxsCKGSmb9iOiCSUNOBsk0J3uLJ\ny6RKXsX5cptpVgt5XU0DE6QSXkoUtURTeohuqIogf0hF7Qq/VoPVtv1vt8dMXKM0foD6yPb/j7nz0=</l\natexit>sliding window 3\n<latexit sh\na1_base64=\"nbalF/ID6\nNaAdAZY/1W2cG1hDkU=\"\n>ACDnicbVDLSsNAFJ3\nUV42vqks3wbZQoZSkC3VZ\ncONKvQhNKFMJjft0Mk\nzEyEvoFbvwVNy4Uceva\nnX/j9CFo64GBwzn3cOce\nP2FUKtv+MnJr6xubW/lt\nc2d3b/+gcHjUkXEqCLRJz\nGJx52MJjHJoK6oY3CUCc\nOQz6Pqjq6nfvQchacxba\npyAF+EBpyElWGmpXyi7P\ngwozwhwBWJiliqt6k21fl\nYyXeDBj9wvFO2aPYO1Sp\nwFKaIFmv3CpxvEJI10nD\nAsZc+xE+VlWChKGExMN5\nWQYDLCA+hpynE0stm50\nyslYCK4yFflxZM/V3IsO\nRlOPI15MRVkO57E3F/7x\neqsJL6M8SRVwMl8Upsx\nSsTXtxgqoAKLYWBNMBNV\n/tcgQC0x0B9LUJTjLJ6+S\nTr3mnNfqt/Vio7KoI49O\n0CmqIAdoAa6Rk3URgQ9\noCf0gl6NR+PZeDPe56M5\nY5E5Rn9gfHwDEmSaxw=\n</latexit>(T, N, 2)\n<latexit sha1_base64=\n\"q8H8SZpzSqHAJRFr2ClVUAJec=\">ACD3icb\nVDLSsNAFJ3UV42vqks3wVatUErShbosuHElFfqCJp\nTJ5LYdOpmEmYlQv/Ajb/ixoUibt2682+cPgRtPT\nBwOce7tzjx4xKZdtfRmZldW19I7tpbm3v7O7l9g+\naMkoEgQaJWCTaPpbAKIeGopBOxaAQ59Byx9eT/z\nWPQhJI15Xoxi8EPc57VGClZa6uVPXhz7lKQGuQIz\nNQrF+VrotBecF0wUe/OjdXN4u21NYy8SZkzyao9bN\nfbpBRJQxwnDUnYcO1ZeioWihMHYdBMJMSZD3IeO\nphyHIL10es/YOtFKYPUioR9X1lT9nUhxKOUo9PVki\nNVALnoT8T+vk6jelZdSHicKOJkt6iXMUpE1KcKq\nACi2EgTATVf7XIAtMdAfS1CU4iycvk2al7FyUK3\neVfLU4ryOLjtAxKiIHXaIqukE1EAEPaAn9IJejU\nfj2Xgz3mejGWOeOUR/YHx8A8rqmyo=</latexit>(T 0, N, d)\n<latexit sha1_base64=\n\"nbalF/ID6NaAdAZY/1W2cG1hDkU=\">ACDnicb\nVDLSsNAFJ3UV42vqks3wbZQoZSkC3VZcONKvQhNK\nFMJjft0MkzEyEvoFbvwVNy4UcevanX/j9CFo64\nGBwzn3cOceP2FUKtv+MnJr6xubW/ltc2d3b/+gcHj\nUkXEqCLRJzGJx52MJjHJoK6oY3CUCcOQz6Pqjq6n\nfvQchacxbapyAF+EBpyElWGmpXyi7PgwozwhwBWJ\niliqt6k21flYyXeDBj9wvFO2aPYO1SpwFKaIFmv3C\npxvEJI10nDAsZc+xE+VlWChKGExMN5WQYDLCA+hp\nynE0stm50yslYCK4yFflxZM/V3IsORlOPI15MRV\nkO57E3F/7xeqsJL6M8SRVwMl8UpsxSsTXtxgqoA\nKLYWBNMBNV/tcgQC0x0B9LUJTjLJ6+STr3mnNfqt/\nVio7KoI49O0CmqIAdoAa6Rk3URgQ9oCf0gl6NR+\nPZeDPe56M5Y5E5Rn9gfHwDEmSaxw=</latexit>(T, N, 2)\n<latexit sha1_base64=\"/IaEc7G84YAxdUtN4xM\nQODI6qQI=\">ACHicbVDLSsNAFJ3UV42vqEsXBovgqiRdqMtCN65KBfuAJpTJ5KYdOpmEmYlQpdu/B\nU3LhRx6ye482+cthG09cDA4Zx7uHNPkDIqleN8GaW19Y3NrfK2ubO7t39gHR51ZJIJAm2SsET0AiyBUQ5\ntRWDXioAxwGDbjBuzPzuPQhJE36nJin4MR5yGlGClZYG1qkXwJDynABXIKZmo9k0PeDhjzCwKk7VmcNe\nJW5BKqhAa2B9emFCsljHCcNS9l0nVX6OhaKEwdT0MgkpJmM8hL6mHMcg/Xx+yNQ+10poR4nQjyt7rv5O5\nDiWchIHejLGaiSXvZn4n9fPVHTt5SnmQJOFouijNkqsWet2CEVQBSbaIKJoPqvNhlhgYnuQJq6BHf5\nFXSqVXdy2rtlapO0UdZXSCztAFctEVqMb1EJtRNADekIv6NV4NJ6N+N9MVoyiswx+gPj4xui6Jmt</\nlatexit>CNN\n<latexit sha1_base6\n4=\"ei1qXxquCDwOGZN+3AwDxEzTMAc=\">AB8\nHicbVDLSgNBEOz1GeMr6tHLYBA8hd0cNMeAF48\nRzEOSJczOziZD5rHMzAphyVd48aCIVz/Hm3/jJ\nNmDJhY0FXdHdFKWfG+v63t7G5tb2zW9or7x\n8cHh1XTk47RmWa0DZRXOlehA3lTNK2ZbTXqop\nFhGn3WhyO/e7T1QbpuSDnaY0FHgkWcItk56jL\nTCMcHGDitVv+YvgNZJUJAqFGgNK1+DWJFMUGkJ\nx8b0Az+1Y61ZYTWXmQGZpiMsEj2ndUYkFNm\nC8OnqFLp8QoUdqVtGih/p7IsTBmKiLXKbAdm1V\nvLv7n9TObNMKcyTSzVJLloiTjyCo0/x7FTFNi+\ndQRTDRztyIyxhoT6zIquxC1ZfXSadeC65r9ft\n6tdko4ijBOVzAFQRwA024gxa0gYCAZ3iFN097\nL96797Fs3fCKmTP4A+/zBwCpkI=</latexit>broadcast\n<latexit sha1_base64=\"ei1qXxquCDwOGZN+\n3AwDxEzTMAc=\">AB8HicbVDLSgNBEOz1GeMr6tHLYBA8hd0cNMeAF48RzEOSJczOziZD5rHMzA\nphyVd48aCIVz/Hm3/jJNmDJhY0FXdHdFKWfG+v63t7G5tb2zW9or7x8cHh1XTk47RmWa0DZRX\nOlehA3lTNK2ZbTXqopFhGn3WhyO/e7T1QbpuSDnaY0FHgkWcItk56jLTCMcHGDitVv+YvgNZJU\nJAqFGgNK1+DWJFMUGkJx8b0Az+1Y61ZYTWXmQGZpiMsEj2ndUYkFNmC8OnqFLp8QoUdqVtGih\n/p7IsTBmKiLXKbAdm1VvLv7n9TObNMKcyTSzVJLloiTjyCo0/x7FTFNi+dQRTDRztyIyxhoT6zIq\nuxC1ZfXSadeC65r9ft6tdko4ijBOVzAFQRwA024gxa0gYCAZ3iFN097L96797Fs3fCKmTP4A+/\nzBwCpkI=</latexit>broadcast\nFigure 2. CoTracker architecture. We compute convolutional features \u03d5(It) for every frame and process them with sliding windows. In\norder to initialize track features Q, we bilinearly sample from \u03d5(It) with starting point locations P. Locations P also serve to initialize\nestimated tracks \u02c6P. See Fig. 3 for a detailed visualization of one sliding window.\nthe last frame where the point is visible.\nHowever, the\nmodel loses the target if it stays occluded beyond the size\nof the window. While the original Particle Video does track\npoints jointly, PIPs and TAP-Vid track points in parallel, but\nindependently. TAPIR [11] proposes a feed-forward point\ntracker with a matching stage inspired by TAP-Vid [10]\nand a refinement stage inspired by PIPs [15], which fur-\nther improves tracking. PointOdyssey [52] addresses long-\nterm tracking with PIPs++, a simplified version of PIPs,\nand introduces a benchmark for long-term tracking. How-\never, both of these methods still track points independently.\nOmniMotion [46] optimizes a volumetric representation for\neach video, refining estimated correspondences in a canon-\nical space. However, this approach requires test-time op-\ntimization, which, due to its cost, is not suitable for many\npractical applications, especially online ones.\nTrackers and optical flow estimators are often trained us-\ning synthetic datasets [5, 12, 24, 30, 40, 52], as annotating\nreal data can be challenging. Synthetic datasets provide ac-\ncurate annotations, and training on them has demonstrated\nthe ability to generalize to real-world data [30, 40].\n3. CoTracker\nOur goal is to track 2D points throughout the duration of a\nvideo V = (It)T\nt=1, which is a sequence of T RGB frames\nIt \u2208 R3\u00d7H\u00d7W . The goal of the tracker is to predict N point\ntracks P i\nt = (xi\nt, yi\nt) \u2208 R2, t = ti, . . . , T, i = 1, . . . , N,\nwhere ti \u2208 {1, . . . , T} is the time when the track starts. The\ntracker also predicts the visibility flag vi\nt \u2208 {0, 1} which\ntells if a point is visible or occluded in a given frame. To\nmake the task definition unambiguous [10], we assume each\npoint is visible at the start of their track (i.e., vi\nti = 1). The\ntracker is thus given as input the video V and the starting\nlocations and times (P i\nti, ti)N\ni=1 of N tracks, and outputs\nan estimate ( \u02c6P i\nt = (\u02c6xi\nt, \u02c6yi\nt), \u02c6vi\nt) of the track locations and\nvisibility for all valid (t \u2265 ti) times.\n3.1. Transformer formulation\nWe implement the tracker as a transformer neural network\n(Figs. 2 and 3) \u03a8 : G 7\u2192 O. The goal of this transformer\nis to improve an initial estimate of the tracks. Tracks are\nencoded as a grid of input tokens Gi\nt, one for each track\ni = 1, . . . , N, and time t = 1, . . . , T. The updated tracks\nare expressed by a corresponding grid of output tokens Oi\nt.\nImage features. We extract dense d-dimensional appear-\nance features \u03d5(It) \u2208 Rd\u00d7 H\nk \u00d7 W\nk from each video frame\nIt using a convolutional neural network (trained end-to-\nend).\nWe reduce the resolution by k\n=\n4 for effi-\nciency. We also consider several scaled versions \u03d5s(It) \u2208\nRd\u00d7\nH\nk2s\u22121 \u00d7\nW\nk2s\u22121 , of the features with strides s = 1, . . . , S\nand use S = 4 scales. These downscaled features are ob-\ntained by applying average pooling to the base features.\nTrack features. The appearance of the tracks is captured\nby feature vectors Qi\nt \u2208 Rd (these are time-dependent to\naccommodate changes in the track appearance). They are\ninitialized by sampling image features at the starting loca-\ntions, and then they are updated by the neural network, as\n3\n<latexit sha1_base64=\"\n3PQz0+Lyk1FpK9/tbwcsZseM4U=\">ACFHicbVDLS\ngMxFM34rOr6tJNsBUqQpnpQl0W3LhswT6gHUsmvW1D\nM5khyQhlmI9w46+4caGIWxfu/BvTaQVtPRA4nHMvN+f\n4EWdKO86XtbK6tr6xmduyt3d29/bzB4dNFcaSQoOGPJR\ntnyjgTEBDM82hHUkgc+h5Y+vp37rHqRiobjVkwi8gA\nwFGzBKtJF6+fOuD0MmEgpCg0xtJqJY42L9LikFZ2nR7\noLo/5i9fMEpOxnwMnHnpIDmqPXyn91+SOPArFNOlOq4\nTqS9hEjNKIfU7sYKIkLHZAgdQwUJQHlJFirFp0bp40E\nozRMaZ+rvjYQESk0C30wGRI/UojcV/M6sR5ceUkWFA\nSdHRrEHOsQTxvCfSaBaj4xhFDJzF8xHRFJqOlA2aYEd\nzHyMmlWyu5FuVKvFKrOvI4cOkYnqIRcdImq6AbVUANR9\ntexit>ICe0At6tR6tZ+vNep+NrljznSP0B9bHN0GsnkM=</la\ninput Q(m)\n<latexit sha1_base64=\"QGrCiQtBxOWer1mAiVacv\ntCH2E=\">ACEHicbVC7SgNBFJ2Nr7i+opY2g4kYm7CbQi0DNtpFMA/ILsvs5CYZMju7zMwKYckn2PgrNhaK2F\nra+TdOHoImHhg4nHMPd+4JE86UdpwvK7eyura+kd+0t7Z3dvcK+wdNFaeSQoPGPJbtkCjgTEBDM82hnUgUcih\nFQ6vJn7rHqRisbjTowT8iPQF6zFKtJGCwqkXQp+JjILQIMd2yUsGrHwT6LOS7YHo/hBoehUnCnwMnHnpIjmqA\neFT68b0zQycqJUh3XSbSfEakZ5TC2vVRBQuiQ9KFjqCARKD+bHjTGJ0bp4l4szRMaT9XfiYxESo2i0ExGRA/Uo\njcR/M6qe5d+hkTSapB0NmiXsqxjvGkHdxlEqjmI0MIlcz8FdMBkYSaDpRtSnAXT14mzWrFPa9Ub6vFmjOvI4+\nO0DEqIxdoBq6RnXUQBQ9oCf0gl6tR+vZerPeZ6M5a545RH9gfXwD416cA=</latexit>\u03c6(It)\n<latexit sha1_base64=\"CM7cuN5zYNgiZlMS047MFT\nNHBmk=\">ACG3icbVDLSgMxFM3UVx1fVZdugq0gCGVmFuqy4MZlBfuAdiZzJ02NJMZkyhDP0PN/6KGxeKuB\nJc+DemD0FbLyQczoPkniDlTGnH+bIKa+sbm1vFbXtnd2/oHR41FRJik0aMIT2Q6IAs4ENDTHNqpBIHFrB\n8Gaqt0YgFUvEvR6n4MekL1jEKNG6pW8bgB9JnIKQoOc2MzcRhoBztKQaMCV+MKt2F0Q4Y+nVyo7VWc2eBW4C1\nBGi6n3Sh/dMKFZbOKUE6U6rpNqPydSM8phYnczBSmhQ9KHjoGCxKD8fLbBJ8ZJsRIs0RGs/Y34mcxEqN48A4Y\n6IHalmbkv9pnUxH137ORJpEHT+UJRxrBM8LQqHTALVfGwAoZKZv2I6IJQ04GyTQnu8sqroOlV3cuqd+eVa86\nijiI6QafoHLnoCtXQLaqjBqLoAT2hF/RqPVrP1pv1PrcWrEXmGP0Z6/MbyE+hLQ=</latexit>iterative update m + 1\n<latexit sha1_base64=\"N7kAU5Xspt8KvHRtXQAveD\nF+hEw=\">ACFHicbVDLSsNAFJ34rPEVdekm2AoVoSRdqMuCG5cV7AOaWCbT23boZBJmJoUS8hFu/BU3LhRx68\nKdf+O0jaCtBwYO59zDnXuCmFGpHOfLWFldW9/YLGyZ2zu7e/vWwWFTRokg0CARi0Q7wBIY5dBQVDFoxwJwGDBo\nBaPrqd8ag5A04ndqEoMf4gGnfUqw0lLXOvcCGFCeEuAKRGaWvCFW6Ti7T8vOWVYyPeC9H7NrFZ2KM4O9TNycF\nGOetf69HoRSUIdJwxL2XGdWPkpFoSBpnpJRJiTEZ4AB1NOQ5B+unsqMw+1UrP7kdCP67smfo7keJQykY6MkQq\nvo4CO0QkqIxdohq6QXUQAQ9oCf0gl6NR+PZeDPe56MrRp45Qn9gfHwDgjmebA=</latexit>6Fc9Kbif14nUf0rP6U8ThRwMl/UT5itInvakN2jAohiE0wEVT/1SZDLDRHUhTl+AunrxMmtWKe1Gp3laLNSe\n\u02c6v(0)\n<latexit sha1_base64=\"jCoU0+DFcCp2jQHUvX/gN\nPYwVk=\">ACDnicbVC7SgNBFJ31GdfXqXNYhKITdhNoZaBNJYRzAOSNcxObpIhM7PLzKwQlv0CG3/FxkIRW2\ns7/8bJQ9DEAwOHc+7hzj1hzKjSnvdlra1vbG5t53bs3b39g0Pn6LipokQSaJCIRbIdYgWMCmhoqhm0YwmYhwxa\n4bg29Vv3IBWNxK2exBwPBR0QAnWRuo5xW4IQypSAkKDzOxC7S4t8fOsYHdB9H/knpP3yt4M7irxFySPFqj3nM\n9uPyIJN3HCsFId34t1kGKpKWGQ2d1EQYzJGA+hY6jAHFSQzs7J3KJR+u4gkuYJ7c7U34kUc6UmPDSTHOuRWvam4\nzVEI+ukRVdI3qIEIekBP6AW9Wo/Ws/Vmvc9H16xF5gT9gfXxDW1Gm6k=</latexit>n9eJ9GDqyClIk40CDJfNEiYqyN32o3bpxKIZhNDMJHU/NUlIywxMR0o25TgL5+8SpqVsn9RrtxU8lVvUcOnaI\nC(m)\n<latexit sha1_base64=\"yxFbY9TONPF3rxIpiM1IK\nvOWhw=\">ACDnicbVC7SgNBFJ31GdfXqXNYhKITdhNoZYBG8sEzAOSGYnN8mQmdlZlYIy36Bjb9iY6GIrb\nWdf+MkWUETDwczrmHO/cEaNKe96Xtba+sbm1nduxd/f2Dw6do+OmCmNJoEFCFsp2gBUwKqChqWbQjiRgHjBo\nBZPrmd+6B6loKG71NIexyNBh5RgbaS+U+wGMKIiISA0yNQu1O+SEj9PC3YXxOBH7jt5r+zN4a4SPyN5lKHWdz\n67g5DE3MQJw0p1fC/SvQRLTQmD1O7GCiJMJngEHUMF5qB6yfyc1C0aZeAOQ2me0O5c/Z1IMFdqygMzybEeq2VvJ\ndoRLy0SWqohtUQw1E0AN6Qi/o1Xq0nq03630xumZlmRP0B9bHN4OWm7c=</latexit>v7ndWI9vOolVESxBkEWi4Yxc3XozrpxB1QC0WxqCaSmr+6ZIwlJqYDZsS/OWTV0mzUvYvypV6JV/1sjpy6BS\nQ(m)\n<latexit sha1_base64=\"QiHE+eFjSik8RaHt+ogywa\noAfxs=\">AC3icbVDLSsNAFJ34rPEVdekmtAiuStKFuiy4cVnBPqAJZTK5aYdOZsLMRCihezf+ihsXirj1B9\nz5N07bCNp6YOBwzj3cuSfKGFXa876stfWNza3tyo69u7d/cOgcHXeUyCWBNhFMyF6EFTDKoa2pZtDLJOA0YtCN\nxtczv3sPUlHB7/QkgzDFQ04TSrA20sCpBhEMKS8IcA1yaotcZ7m2A+DxjzZwal7dm8NdJX5JaqhEa+B8BrEgeW\nrihGl+r6X6bDAUlPCYGoHuYIMkzEeQt9QjlNQYTG/ZeqeGSV2EyHN49qdq78TBU6VmqSRmUyxHqlbyb+5/Vzn\nVyFBeXmPOBksSjJmauFOyvGjakEotnEwkNX91yQhLTEwHyjYl+Msnr5JOo+5f1Bu3jVrTK+uoFNURefIR5e\noiW5QC7URQ/oCb2gV+vRerberPfF6JpVZk7QH1gf3wRhm50=</latexit>output\n<latexit sha1_base64=\"Pf+xfuYzjQbjPp2/5KOg9K\nSEHTA=\">ACFHicbVDLSsNAFJ3UV42vqEs3wVaoCXpQl0W3LisYB/QxDKZ3rZDJ5MwMxFKyEe48VfcuFDErQ\nt3/o3TNoK2Hhg4nHMPd+4JYkalcpwvo7Cyura+Udw0t7Z3dves/YOWjBJBoEkiFolOgCUwyqGpqGLQiQXgMGDQ\nDsZXU79D0LSiN+qSQx+iIecDijBSks968wLYEh5SoArEJlZ9kZYpY3sLq2Ep1nZ9ID3f8yeVXKqzgz2MnFzUk\nI5Gj3r0+tHJAl1nDAsZd1YuWnWChKGSml0iIMRnjIXQ15TgE6aezozL7RCt9exAJ/biyZ+rvRIpDKSdhoCdDr\nEZy0ZuK/3ndRA0u/ZTyOFHAyXzRIG2iuxpQ3afCiCKTBRFD9V5uMsMBEdyBNXYK7ePIyadWq7nm1dlMr1Z2\n8jiI6Qseoglx0geroGjVQExH0gJ7QC3o1Ho1n4814n48WjDxziP7A+PgGpcegw=</latexit> \u02c6P (m)\n<latexit sha1_base64=\"yxFbY9TONPF3rxIpiM1IK\nvOWhw=\">ACDnicbVC7SgNBFJ31GdfXqXNYhKITdhNoZYBG8sEzAOSGYnN8mQmdlZlYIy36Bjb9iY6GIrb\nWdf+MkWUETDwczrmHO/cEaNKe96Xtba+sbm1nduxd/f2Dw6do+OmCmNJoEFCFsp2gBUwKqChqWbQjiRgHjBo\nBZPrmd+6B6loKG71NIexyNBh5RgbaS+U+wGMKIiISA0yNQu1O+SEj9PC3YXxOBH7jt5r+zN4a4SPyN5lKHWdz\n67g5DE3MQJw0p1fC/SvQRLTQmD1O7GCiJMJngEHUMF5qB6yfyc1C0aZeAOQ2me0O5c/Z1IMFdqygMzybEeq2VvJ\ndoRLy0SWqohtUQw1E0AN6Qi/o1Xq0nq03630xumZlmRP0B9bHN4OWm7c=</latexit>v7ndWI9vOolVESxBkEWi4Yxc3XozrpxB1QC0WxqCaSmr+6ZIwlJqYDZsS/OWTV0mzUvYvypV6JV/1sjpy6BS\nQ(m)\n<latexit sha1_base64=\"q8H8SZpzSqHAJRFr2ClV\nUAJec=\">ACD3icbVDLSsNAFJ3UV42vqks3wVatUErShbosuHElFfqCJpTJ5LYdOpmEmYlQv/Ajb/ixoUibt\n2682+cPgRtPTBwOce7tzjx4xKZdtfRmZldW19I7tpbm3v7O7l9g+aMkoEgQaJWCTaPpbAKIeGopBOxaAQ59B\nyx9eT/zWPQhJI15Xoxi8EPc57VGClZa6uVPXhz7lKQGuQIzNQrF+VrotBecF0wUe/OjdXN4u21NYy8SZkzyao9\nbNfbpBRJQxwnDUnYcO1ZeioWihMHYdBMJMSZD3IeOphyHIL10es/YOtFKYPUioR9X1lT9nUhxKOUo9PVkiNVAL\nnoT8T+vk6jelZdSHicKOJkt6iXMUpE1KcKqACi2EgTATVf7XIAtMdAfS1CU4iycvk2al7FyUK3eVfLU4ryO\nLjtAxKiIHXaIqukE1EAEPaAn9IJejUfj2Xgz3mejGWOeOUR/YHx8A8rqmyo=</latexit>(T 0, N, d)\n<latexit sha1_base64=\"nbalF/ID6NaAdAZY/1W2cG\n1hDkU=\">ACDnicbVDLSsNAFJ3UV42vqks3wbZQoZSkC3VZcONKvQhNKFMJjft0MkzEyEvoFbvwVNy4Uce\nvanX/j9CFo64GBwzn3cOceP2FUKtv+MnJr6xubW/ltc2d3b/+gcHjUkXEqCLRJzGJx52MJjHJoK6oY3CUCcOQz\n6Pqjq6nfvQchacxbapyAF+EBpyElWGmpXyi7PgwozwhwBWJiliqt6k21flYyXeDBj9wvFO2aPYO1SpwFKaIFmv\n3CpxvEJI10nDAsZc+xE+VlWChKGExMN5WQYDLCA+hpynE0stm50yslYCK4yFflxZM/V3IsORlOPI15MRVkO57\nE3F/7xeqsJL6M8SRVwMl8UpsxSsTXtxgqoAKLYWBNMBNV/tcgQC0x0B9LUJTjLJ6+STr3mnNfqt/Vio7KoI49\nO0CmqIAdoAa6Rk3URgQ9oCf0gl6NR+PZeDPe56M5Y5E5Rn9gfHwDEmSaxw=</latexit>(T, N, 2)\n<latexit sha1_base64=\"nbalF/ID6NaAdAZY/1W2cG\n1hDkU=\">ACDnicbVDLSsNAFJ3UV42vqks3wbZQoZSkC3VZcONKvQhNKFMJjft0MkzEyEvoFbvwVNy4Uce\nvanX/j9CFo64GBwzn3cOceP2FUKtv+MnJr6xubW/ltc2d3b/+gcHjUkXEqCLRJzGJx52MJjHJoK6oY3CUCcOQz\n6Pqjq6nfvQchacxbapyAF+EBpyElWGmpXyi7PgwozwhwBWJiliqt6k21flYyXeDBj9wvFO2aPYO1SpwFKaIFmv\n3CpxvEJI10nDAsZc+xE+VlWChKGExMN5WQYDLCA+hpynE0stm50yslYCK4yFflxZM/V3IsORlOPI15MRVkO57\nE3F/7xeqsJL6M8SRVwMl8UpsxSsTXtxgqoAKLYWBNMBNV/tcgQC0x0B9LUJTjLJ6+STr3mnNfqt/Vio7KoI49\nO0CmqIAdoAa6Rk3URgQ9oCf0gl6NR+PZeDPe56M5Y5E5Rn9gfHwDEmSaxw=</latexit>(T, N, 2)\n<latexit sha1_base64=\"vNAtx9xEOYQBEg7yshXOzp\n1z2c=\">ACEHicbVDLSsNAFJ34rPEVdekmWERXJelCXRbcuKxgH9CEMpnctENnJmFmIpTQT3Djr7hxoYhbl+\n78G6dtBG09MHA45x7m3hNljCrteV/Wyura+sZmZcve3tnd23cODtsqzSWBFklZKrsRVsCogJamkE3k4B5xKAT\nja6nfucepKpuNPjDEKOB4ImlGBtpL5zFkQwoKIgIDTIia0lFipJQdpByDiH6PvVL2aN4O7TPySVFGJZt/5DO\nKU5NzECcNK9Xwv02GBpaEwcQOcgUZJiM8gJ6hAnNQYTE7aOKeGiV2zRrmCe3O1N+JAnOlxjwykxzroVr0puJ/X\ni/XyVYUJHlGgSZf5TkzNWpO23HjakEotnYEwkNbu6ZIglJqYDZsS/MWTl0m7XvMvavXberXhlXVU0DE6Qef\nIR5eogW5QE7UQ/oCb2gV+vRerberPf56IpVZo7QH1gf3xKFndE=</latexit>transformer\n<latexit sha1_base64=\"LBKoZHQhzHjmNTvePbsgCX0hJfg=\">ACA3icbVC7SgNBFL3rM8ZX1E6b\nwSDYGHdTaMqAjWUE84BkCbOTSTJkZneZuSuEJWDjr9hYKGLrT9j5N06SLTxwMDhnHu5c04QS2HQdb+dldW19Y3N3FZ+e2d3b79wcNgwUaIZr7NIRroVUMOlCHkdBUreijWnKpC8GYxupn7zgWsjovAexzH3FR2\nEoi8YRSt1C8dMR8ZcoKZsdIlCcUIReTg3i27JnYEsEy8jRchQ6xa+Or2IJcquM0mNaXtujH5KNQom+STfSQyP7R064G1LQ6q48dNZhgk5s0qP9CNtX4hkpv7eSKkyZqwCO6koDs2iNxX/89oJ9it+KsI4sbnY/\nFA/kQjMi2E9ITmDOXYEsq0sH8lbEhtH2hry9sSvMXIy6RLnlXpfJduVitZHXk4ARO4Rw8uIYq3EIN6sDgEZ7hFd6cJ+fFeXc+5qMrTrZzBH/gfP4ATd+X6g=</latexit>cross-track/time attention\n<latexit sha1_base64=\"WH2MyXsAXun\nXMrATGkscCwoOs8w=\">ACO3icbZA9TxtBEIb3gBDHIcSQMs0IXyQjLOvORUJpi\nRSpIhIwINnGmlvPwYq9D+3OIVkn/y8a/kS6NGlSgBt+qyNhfh6pZWefWdGu/NGu\nVaWg+C3t7C49Gr5deVN9e3Ku9X3tbX1A5sVRlJXZjozRxFa0iqlLivWdJQbwiTSd\nBid7Uzrh+dkrMrSfR7nNEjwJFWxksjOGtZ+yswY0rMbxIRcGLg7xyXjWRz4gMy+\nHs+WImabBP8xn4Tvjdhrwnt/lfSjFvhPcIWhJs+DGv1oBXMBM8hnENdzLU7rP3qj\nzJZJSy1GhtLwxyHpRoWElNk2q/sJSjPMT6jlMSE7KGe7T+CTc0YQZ8adlGHmP\npwoMbF2nESuM0E+tU9rU/OlWq/geHtQqjQvmFJ591BcaOAMpkHCSBmSrMcOUBrl/g\nElr1L7693493etS5485kP4pG8f/8BcfqoFw=</latexit>ryFA1KdnFXQjh05Wfw0G7FX5utX+0653teRwV8VFsiIYIxRfREd/ErugKS7EH3\ncorrelation features C(m) at S scales, (T, N, S, 2\u2206 + 1, 2\u2206 + 1)\n<latexit sha1_base64=\"\nuMv1k5lCmFrNZv2NcYbv9kNJMk=\">ACJnicbVDLS\ngMxFM34rONr1KWbYCu4KGWmC3UjFNy4ESrYB3RKyWRu\n29BMZkgyhTL0a9z4K25cVETc+SmD0FbLyQczoPkniD\nhTGnX/bTW1jc2t7ZzO/bu3v7BoXN0XFdxKinUaMxj2Qy\nIAs4E1DTHJqJBIFHBrB4HaqN4YgFYvFox4l0I5IT7\nAuo0QbquPc+AH0mMgoCA1ybDNzG2kIOE1CokH5vo0LX\ntHnYaxV8b5g+yDCH3vHybsldzZ4FXgLkEeLqXaciR/G\nNI1MnHKiVMtzE93OiNSMchjbfqogIXRAetAyUJAIVDu\nbrTnG54YJcTeW5giNZ+zvREYipUZRYJwR0X21rE3J/7\nRWqrvX7YyJNUg6PyhbsqxjvG0MxwyCVTzkQGESmb+i\nmfSEJNB8o2JXjLK6+CernkXZbKD+V8xV3UkUOn6AxdI\nA9doQq6Q1VUQxQ9oRc0QW/Ws/VqvVsfc+uatcicoD9j\nfX0D9fOlXw=</latexit>iterative updates\n1, . . . , M\n<latex\nit sha1_bas\ne64=\"k6Gab\n9Gk4jP+PJUY\nBpZKpVCaDE\n=\">ACGXic\nbVDLSsNAFJ3\n4rPEVdelms\nAiuStKFuiy4\ncVnBPqAJZTK\n5aYdOJmFmIt\nTQ3Djr7hxo\nYhLXfk3Tts\nI2nrgwuGc7\nncE2acKe26X\n9bK6tr6xmZl\ny97e2d3bdw\n4O2yrNJYUWT\nXkquyFRwJmA\nlmaQzeTQJK\nQycXU39zh\n1IxVJxq8cZ\nBAkZCBYzSrS\nR+o7rhzBgoq\nAgNMiJzQTj\nHB2D5Hv21o\nSOlK2DyL6Sf\nSdqltzZ8DLx\nCtJFZVo9p0P\nP0pnph1yol\nSPc/NdFAQq\nRnlMLH9XEFm\nzpAB9AwVJAE\nVFLPJvjUKB\nGOU2lGaDxTf\n28UJFqnIQ\nmRA9VIveVP\nzP6+U6vgwKJ\nrJcg6DzQ3HO\nsU7xtCYcMQ\nlU87EhErTC\ncV0SEwdpgNl\nmxK8xZeXSbt\ne85r9Zt6te\nGWdVTQMTpB\nZ8hDF6iBrlE\nTtRBFD+gJva\nBX69F6t6s9\n3l0xSp3jtA\nfWJ/fxiuhWA\n=</latexit\n>initialized\ntracks\n<latex\nit sha1_bas\ne64=\"HTP3M\nygSwS5gvDSF\ncaOR/vSCzA\n=\">ACF3ic\nbVBNS8NAEN3\nUrxq/oh69L\nBbBU0l6UI8F\nLx4r2A9oQtl\nsJu3SzSbsbo\nQS+i+8+Fe8e\nFDEq978N27\nbCNr6YODx3g\nwz8KM6Vd9\n8uqrK1vbG5V\nt+2d3b39A+\nfwqKPSXFJo0\n5SnshcSBZwJ\naGumOfQyCSQ\nJOXTD8fXM79\n6DVCwVd3qS\nQZCQoWAxo0Q\nbaeDU/RCGTB\nQUhAY5tUFpl\nhANke/bWhI\n6VrYPIvrxB0\n7Nrbtz4FXil\naSGSrQGzqcf\npTRPzDjlRKm\n+52Y6KIjUj\nHKY2n6uIDNr\nyBD6hgqSgAq\nK+V9TfGaUCM\nepNCU0nqu/J\nwqSKDVJQtN\npbh6pZW8m/u\nf1cx1fBQUTW\na5B0MWiOdY\np3gWEo6YBK\nr5xBCJTO3Y\njoiJg6TgbJN\nCN7y6uk06h\n7F/XGbaPWdM\ns4qugEnaJz\n5KFL1EQ3qIX\naiKIH9IRe0K\nv1aD1b9b7o\nrVilTPH6A+\nsj28RB6Bu</\nlatexit>estimated\ntracks\nFigure 3. CoTracker architecture. Visualization of one sliding window with M iterative updates. During one iteration, we update point\ntracks \u02c6P (m) and track features Q(m). Q(0) is initialized with the initially sampled features Q for all sliding windows, \u02c6P (0) with the starting\nlocations for the first window. For other windows, \u02c6P (0) starts with predictions for frames processed in the preceding sliding window, and\nwith the last predicted positions for the unseen frames. We compute visibility \u02c6v after the last update M.\nexplained below.\nCorrelation features. In order to facilitate matching tracks\nto images, we adopt the correlation features Ci\nt \u2208 RS\nof RAFT [43].\nEach Ci\nt is obtained by comparing the\ntrack features Qi\nt to the image features \u03d5s(It) around the\ncurrent estimate \u02c6P i\nt of the tracks\u2019s location. Specifically,\nthe vector Ci\nt is obtained by stacking the inner products\n[Ci\nt]s\u03b4 = \u27e8Qi\nt, \u03d5s(It)[ \u02c6P i\nt /ks + \u03b4]\u27e9, where s = 1, . . . , S\nare the feature scales and \u03b4 \u2208 Z2, \u2225\u03b4\u2225\u221e \u2264 \u2206 are offsets.\nThe image features \u03d5s(It) are sampled at non-integer lo-\ncations by using bilinear interpolation and border padding.\nThe dimension of Ci\nt is\n\u00002\u2206 + 1)2S = 196 for our choice\nS = 4; \u2206 = 3.\nTokens. The input tokens G( \u02c6P, \u02c6v, Q) code for position,\nvisibility, appearance, and correlation of the tracks and is\ngiven by the following concatenation of features:\nGi\nt = ( \u02c6P i\nt \u2212 \u02c6P i\n1, \u02c6vi\nt, Qi\nt, Ci\nt, \u03b7( \u02c6P i\nt \u2212 \u02c6P i\n1))+\u03b7\u2032( \u02c6P i\n1)+\u03b7\u2032(t).\n(1)\nAll the components except the last one have been introduced\nabove. The last component is derived from the estimated\nposition: it is the sinusoidal positional encoding \u03b7 of the\ntrack location with respect to the initial location at time t =\n1. We also add encodings \u03b7\u2032 of the start location P i\n1 and for\nthe time t, with appropriate dimensions. In fact, we found it\nbeneficial to separately encode the position of points at the\nfirst frame and their relative displacement to this frame.\nThe output tokens O(\u2206 \u02c6P, \u2206Q) contain updates for lo-\ncation and appearance, i.e. Oi\nt = (\u2206 \u02c6P i\nt , \u2206Qi\nt).\nIterated transformer applications. We apply the trans-\nformer M times in order to progressively improve the track\nestimates. Let m = 0, 1, . . . , M index the estimate, with\nm = 0 denoting initialization. Each update computes\nO(\u2206 \u02c6P, \u2206Q) = \u03a8(G( \u02c6P (m), \u02c6v(0), Q(m))).\nand sets \u02c6P (m+1) = \u02c6P (m)+\u2206 \u02c6P and Q(m+1) = Q(m)+\u2206Q.\nThe visibility mask \u02c6v is not updated by the transformer, but\nonce after the last of the M applications of the transformer\nas \u02c6v(M) = \u03c3(WQ(M)), where \u03c3 is the sigmoid activation\nfunction and W is a learned matrix of weights. We found\niterative updates for the visibility did not further improve\nthe performance, likely due to the fact that visibility highly\ndepends on predicting an accurate location first.\nFor m = 0, the estimates \u02c6P (0), v(0) and Q(0) are ini-\ntialized by broadcasting the initial values P i\nti (the location\nof query point), 1 (meaning visible) and \u03d5(Iti)[P i\nti/k] (the\nappearance of the query point) to all times t = 1, . . . , T.\n3.2. Transformer architecture and virtual tracks\nThe transformer \u03a8 interleaves attention operators that op-\nerate across the time and track dimensions, respectively.\nFactorising the attention [1] across time and tracks makes\nthe model computationally tractable: the complexity is re-\nduced from O(N 2T 2) to O(N 2 + T 2). However, for very\n4\n(b) all target points\n(d) global \n(e) SIFT\n(a) single target point\n(c) local\nFigure 4. Tracked and support points. We visualize different configurations of points along with their tracks. Points represent the start\nof each track, which clearly illustrates grids of points in (c) and (d). (a) A single target point in a sequence from TAP-Vid-DAVIS. (b)\nAll target points \u2014 observe that they tend to cluster on objects of interest, potentially giving our tracker an \u2019unfair\u2019 advantage compared\nto single-track methods. (c) A local grid of support points. (d) A global grid of support points. (e) SIFT support points. We evaluate\nCoTracker by combining different configurations of support points, as described in Sec. 3.5\nlarge values of N, this cost is still prohibitive, especially\nwhen tracking points densely in a video. We thus propose a\nnew design, in which we introduce K virtual tracks tokens,\nwhere K \u226a N is a hyperparameter. These are just added\nas additional tokens to the input grid with a fixed, learnable\ninitialization and removed at the output. They participate in\ntime attention as regular tracks, and in cross attention with\nall other tracks, reducing the cost to O(NK + K2 + T 2).\nStarting with DETR [6], various computer vision trans-\nformers have used learnable queries. For instance, [20] uses\nthem to decode dense outputs. Our virtual tracks are differ-\nent, however: they mirror and shadow the computation of\nthe prediction targets (i.e., the tracks), actively participating\nin the calculations to reduce the computational burden.\n3.3. Windowed inference\nA key advantage of formulating tracking as iterated updates\nis that it can easily support a sliding window application to\nprocess long videos. Consider in particular a video V of\nlength T \u2032 > T longer than the maximum window length T\nsupported by the architecture. To track points throughout\nthe entire video V , we split the video in J = \u23082T \u2032/T \u2212 1\u2309\nwindows of length T, with an overlap of T/2 frames.1\nLet the superscript (m, j) denote the m-th application of\nthe transformer to the j-th window. We thus have a M \u00d7 J\ngrid of quantities ( \u02c6P (m,j), \u02c6v(m,j), Q(m,j)), spanning trans-\nformer iterations and windows.\nQuantities m = 0 and\nj = 1 are initialized as for the single window case. Then,\nthe transformer is applied M times to obtain the estimate\n(M, 1). The latter is used to initialize estimate (0, 2) by\nbroadcasting the known quantities as before. Specifically,\nthe first T/2 components of \u02c6P (0,2) are copies of the last T/2\ncomponents of \u02c6P (M,1); the last T/2 components of \u02c6P (0,2)\nare instead copies of the last time t = T/2\u22121 from \u02c6P (M,1).\nThe same update rule is used for \u02c6v(0,2), while Q(0,j) is al-\nways initialized with initial track features Q. This process\nis repeated until estimate (M, J) is obtained.\n1We assume that T is even. The last window is shorter if T/2 does not\ndivide T \u2032.\n3.4. Unrolled window training\nWe found it important to learn the windowed transformer\nin an unrolled fashion in order to learn to handle semi-\noverlapping windows. The primary loss is for track regres-\nsion, summed over iterated transformer applications and\nwindows:\nL1( \u02c6P, P) =\nJ\nX\nj=1\nM\nX\nm=1\n\u03b3M\u2212m\u2225 \u02c6P (m,j) \u2212 P (j)\u2225,\n(2)\nwhere \u03b3 = 0.8 discounts early transformer updates. Here\nP (j) contains the ground-truth trajectories restricted to win-\ndow j (trajectories which start in the middle of the window\nare padded backwards). The second loss is the cross entropy\nof the visibility flags L2(\u02c6v, v) = PJ\nj=1 CE(\u02c6v(M,j), v(j)).\nWhile only a moderate number of windows are used in the\nloss during training due to the computational cost, at test\ntime we can unroll the windowed transformer applications\narbitrarily, thus in principle handling any video length.\nImplementation details. A technical difficulty is that the\nnumber of tracks N is not fixed, but varies from window\nto window as new points can be added to the tracker at any\ntime. While conceptually this is handled by simply adding\nmore tokens when needed, in practice changing the number\nof tokens makes batching data for training difficult. Instead,\nwe allocate enough tokens to support all points that must be\ntracked, regardless of which window they are added in, and\nuse masking to ignore tokens that are not yet used.\n3.5. Support points\nCoTracker can take advantage of tracking several points\njointly. It is typical for applications to have several points to\ntrack, but in some cases, one might be interested in tracking\na few or even a single point. In fact, single-point tracking is\na case we consider in the evaluation for fairness (Sec. 1).\nIn practice, we found it beneficial to track additional\n\u2018support points\u2019 which are not explicitly requested by the\nuser, particularly when the goal is to track only a few of\nthem. Moreover, we have found that different configura-\ntions of such support points can lead to small differences\n5\nin performance.\nWe experiment with various configura-\ntion types, visualized in Fig. 4. With the \u201cglobal\u201d strategy,\nthe support points form a regular grid across the whole im-\nage. With the \u201clocal\u201d strategy, the grid of points is centred\naround the point we wish to track, thus allowing the model\nto focus on a neighbourhood of it. We also test using the\nSIFT detector [26] to select support points. Note that these\npatterns are only considered at inference time and are used\nto improve the accuracy of the tracker for the target points.\n4. Experiments\nWe thoroughly evaluate CoTracker on standard real and\nsynthetic tracking benchmarks to assess its performance\nand generalization properties in difficult conditions and\nagainst numerous state-of-the-art trackers.\nSingle target tracks. We pay particular attention to the de-\nsign of the evaluation protocol to ensure its fairness. Specif-\nically, we note that existing benchmarks contain ground\ntruth tracks which are concentrated on a few foreground ob-\njects, potentially revealing the presence of such objects to\na joint tracker like ours. In order to ensure that no ground-\ntruth information is leaked to our tracker, in the \u201csingle tar-\nget track\u201d protocol we track one benchmark point at a time,\nbut automatically add additional support points (Sec. 3.5) to\nallow the model to perform joint tracking. In this way, our\ncomparison is fair to single-point trackers. Furthermore, by\ncomparing different configurations of such support tracks,\nwe can better quantify the importance of joint tracking.\nDatasets and evaluation protocol. We use TAP-Vid [10],\na collection of four tracking benchmarks, three real ones\nfor evaluation and a synthetic one for training. The latter\nis TAP-Vid-Kubric, also used by several prior works to\ntrain their trackers. It consists of sequences of 24 frames\nshowing 3D rigid objects falling to the ground under grav-\nity and bouncing, generated using the Kubric engine [14].\nPoint tracks are selected randomly, primarily on objects,\nand some on the background. Objects occlude each other\nand the background as they move.\nFor evaluation, we consider the TAP-Vid-DAVIS set, con-\ntaining 30 real sequences of about 100 frames. Points are\nqueried on random objects at random times and evaluation\nassesses both predictions of positions and visibility flags.\nThis uses two evaluation protocols. In the \u201cqueried first\u201d\nprotocol, each point is queried only once in the video, at the\nfirst frame where it becomes visible. The tracker is expected\nto operate causally, predicting positions only for future\nframes. In the \u201cqueried strided\u201d protocol, points are queried\nevery five frames and tracking is bidirectional. Given that\nmost trackers (ours, PIPs, PIPs++) are causal, we run them\ntwice, on the video and its reverse. We assess tracking ac-\ncuracy using the TAP-Vid metrics [10]: Occlusion Accu-\nracy (OA; accuracy of occlusion prediction treated as binary\nclassification), \u03b4vis\navg (fraction of visible points tracked within\n1, 2, 4, 8 and 16 pixels, averaged over thresholds), and Aver-\nage Jaccard (AJ, measuring jointly geometric and occlusion\nprediction accuracy). Following [10], the \u03b4vis\navg and AJ track-\ning thresholds are applied after virtually resizing images to\n256 \u00d7 256 pixels.\nPointOdyssey [52] is a more recent synthetic benchmark\ndataset for long-term tracking. It contains 100 sequences\nseveral thousand frames long with objects and characters\nmoving around the scene. Scenes are much more realistic\nthan TAP-Vid-Kubric. We train and evaluate CoTracker on\nPointOdyssey and report \u03b4vis\navg and \u03b4occ\navg, \u03b4avg. The last two\nare the same as \u03b4vis\navg, but for occluded and all points, respec-\ntively; they can be computed because the dataset is synthetic\nso the ground-truth positions of invisible points are known.\nWe also report their Survival rate, which is the average frac-\ntion of video frames until the tracker fails (detected when\nthe tracking error exceeds 50 pixels).\nDynamic Replica [23] is a synthetic dataset for 3D re-\nconstruction that contains long-term tracking annotations.\nIt consists of 500 300-frame sequences of articulated mod-\nels of people and animals. We measure \u03b4vis\navg and \u03b4occ\navg on the\n\u201cvalid\u201d split of this dataset.\nImplementation details. Here we provide important im-\nplementation details and refer to the sup. mat. for others.\nFor training, we use 11,000 TAP-Vid-Kubric sequences of\nT \u2032 = 24 frames using sliding window size T = 8 and train\nfor 50,000 iterations using 32 NVIDIA A100 80GB GPUs\nwith the batch size of 1. Training tracks are sampled pref-\nerentially on objects. During training, we construct batches\nof 768 tracks out of 2048 among those that are visible ei-\nther in the first or middle frames of the sequence to train\nthe tracker to handle both cases. In a similar manner, we\nalso train a second version of CoTracker on the train split\nof PointOdyssey, using 128 tracks randomly sampled from\nsequences of length T \u2032 = 56.\nWe evaluate joint tracking using different configurations\nof target and support points (Sec. 3.5 and Fig. 4). The nat-\nural choice is to jointly track all target points specified in a\ngiven benchmark sequence, which we test for all datasets.\nWe also assess the benefits of adding various configura-\ntions of support points (Sec. 3.5). Furthermore, benchmark\npoints might be biased for objects, which is particularly ev-\nident in TAP-Vid-DAVIS (Fig. 4). In order to assess Co-\nTracker more fairly against single-point trackers, we thus\nalso experiment with tracking a single target point at a time,\nplus various configurations of support points.\nThe video resolution can affect tracking performance\nsubstantially. For TAP-Vid-DAVIS, we follow their proto-\ncol and downsample videos to 256\u00d7256, which ensures that\nno more information than expected is passed to the tracker.\nHowever, each tracker has a different native resolution at\nwhich it is trained on (384 \u00d7 512 for PIPs and CoTracker;\n6\nMethod\nTraining data\nDAVIS First\nDAVIS Strided\nDynamic Replica\nAJ \u2191\n\u03b4vis\navg \u2191\nOA \u2191\nAJ \u2191\n\u03b4vis\navg \u2191\nOA \u2191\n\u03b4avg \u2191\n\u03b4vis\navg \u2191\n\u03b4occ\navg \u2191\nTAP-Net [10]\nTAP-Vid-Kubric\n33.0\n48.6\n78.8\n38.4\n53.1\n82.3\n45.5\n53.3\n20.0\nOmniMotion [46]\n\u2014\n\u2014\n\u2014\n\u2014\n51.7\n67.5\n85.3\n\u2014\n\u2014\n\u2014\nPIPs [15]\nFlyingThings++\n42.2\n64.8\n77.7\n52.4\n70.0\n83.6\n41.0\n47.1\n21.0\nMFT [31]\nKubric + others\n47.3\n66.8\n77.8\n56.1\n70.8\n86.9\n\u2014\n\u2014\n\u2014\nPIPs++ [52]\nPointOdyssey\n\u2014\n69.1\n\u2014\n\u2014\n73.7\n\u2014\n55.5\n64.0\n28.5\nTAPIR [11]\nTAP-Vid-Kubric\n56.2\n70.0\n86.5\n61.3\n73.6\n88.8\n56.8\n66.1\n27.2\nCoTracker (Ours)\nTAP-Vid-Kubric\n62.2\n75.7\n89.3\n65.9\n79.4\n89.9\n61.6\n68.9\n37.6\nTable 1. State of the art comparison. We compare CoTracker to the best trackers available on TAP-Vid-DAVIS utilizing both their\n\u201cqueried first\u201d and \u201cqueried strided\u201d evaluation protocols as well as on Dynamic Replica. During evaluation on DAVIS, TAPIR and\nPIPs++ have access to all the video frames at once, while we process videos in an online manner, using a causal sliding window.\nPIPs++\nCoTracker (Ours)\nTAPIR\nFigure 5. Qualitative Results. For PIPs++ (top), many points are incorrectly tracked and end up being \u2018stuck\u2019 on the front of the car.\nTAPIR (middle) works well for visible points but fails to handle occluded points. As soon as a point becomes occluded, it starts moving\nchaotically around the image. Our CoTracker (bottom) produces cleaner tracks. The tracks are also more \u2018linear\u2019 than those of PIPs++ or\nTAPIR, which is accurate as the primary motion is a homography (the observer does not translate).\nMethod\nPointOdyssey\n\u03b4avg \u2191\n\u03b4vis\navg \u2191\n\u03b4occ\navg \u2191\nSurvival \u2191\nTAP-Net [10]\n28.4\n\u2014\n\u2014\n18.3\nPIPs [16]\n27.3\n\u2014\n\u2014\n42.3\nPIPs++\u2020 [52]\n29.0\n32.4\n18.8\n47.0\nCoTracker (Ours)\n30.2\n32.7\n24.2\n55.2\nTable 2. PointOdyssey. All methods are trained on PointOdyssey.\n\u2020 results for [52] obtained using released code and model.\n512 \u00d7 896 for PIPs++); We thus resize videos to the native\nresolution of each tracker before running it. For Dynamic\nReplica and PointOdyssey, we resize videos to the native\ntracker resolution, as there is no prescribed resolution.\n4.1. Results\nIs joint tracking beneficial? In Tab. 3 we demonstrate the\nimportance of tracking points jointly, which is the main mo-\ntivation of CoTracker. This is achieved by removing cross-\ntrack attention from the tracker, thus ignoring the correla-\ntion between tracks entirely. For fairness, we maintain the\nsame model size and replace the six cross-track attention\nlayers with twelve time attention layers. We test on TAP-\nVid-DAVIS tracking either a single target point at a time or\nall target points simultaneously. We also test different con-\nfigurations of support points (Sec. 3.5 and Fig. 4): none,\n7\nAttention\nPoints\nDAVIS First\ntime joint\ntarget\nsupport\nAJ \u2191\n\u03b4avg \u2191\nOA \u2191\n(a)\n\u2713\n\u2717\nsingle\n\u2014\n58.3\n71.5\n86.4\n(b)\n\u2713\n\u2713\nsingle\n\u2014\n41.1\n62.9\n75.1\n(c)\n\u2713\n\u2713\nsingle\nglob. 9\u00d79\n56.8\n71.2\n85.8\n(d)\n\u2713\n\u2713\nsingle\nSIFT 8\u00d78\n57.4\n72.1\n85.8\n(e)\n\u2713\n\u2713\nsingle loc. 10\u00d710\n60.4\n75.2\n87.5\n(f)\n\u2713\n\u2713\nsingle\nglob. 5\u00d75\n+loc. 8\u00d78\n62.2\n75.7\n89.3\n(g)\n\u2713\n\u2713\nall\n\u2014\n57.6\n73.2\n86.1\n(h)\n\u2713\n\u2713\nall\nglob. 5\u00d75\n60.5\n75.8\n87.6\n(i)\n\u2713\n\u2713\nall\nSIFT 8\u00d78\n60.7\n75.7\n88.1\nTable 3. Importance of joint tracking. We compare using time\nand cross-track attention, tracking single or multiple target points,\nand using additional support points.\nUnrolled Training\nDAVIS First\nAJ \u2191\n\u03b4avg \u2191\nOA \u2191\n\u2717\n44.6\n60.5\n75.3\n\u2713\n62.2\n75.7\n89.3\nTable 4. Unrolled training. CoTracker is built for sliding window\npredictions. Using them during training is important.\nNum. virt.\ntracks\nDAVIS First\nMax. num.\ntracks\nTime [s]\nAJ \u2191\n\u03b4avg \u2191\nOA\u2191\n0\n61.6\n75.6\n88.3\n97\u00d797\n207.3\n32\n60.2\n74.4\n88.5\n263\u00d7263\n26.8\n64\n62.2\n75.7\n89.3\n263\u00d7263\n27.9\n128\n60.9\n74.8\n88.4\n263\u00d7263\n30.1\nTable 5. The virtual tracks allow CoTracker to scale. We report\nthe maximum number of tracks that can fit on a 80 GB GPU.\nglobal, global and local, global and SIFT grids. We test\ngrids of different sizes, ranging from 1 \u00d7 1 to 10 \u00d7 10, and\nfor the sake of compactness, we report the best settings.\nWe find that: (1) for the same number of parameters,\ntracking points jointly instead of independently is signifi-\ncantly better (+3.9 AJ (a) to (f)); (2) when tracking single\ntarget points, adding support points in a global grid signifi-\ncantly boosts performance (+15 AJ (b) to (c)); (3) the latter\nis further improved by using a global/local grid (+5.4 AJ\n(c)) to (f); (4) the SIFT grid is less good than the global/local\ngrid (\u22124.8 AJ (f) to (c)), but slightly better than the global\nalone (+0.6 AJ (c) to (d)); (5) adding support points to all\ntarget points is better (+2.9 AJ (g) to (i)). This demonstrates\nthat tracking points jointly is highly beneficial in all cases.\nHow does CoTracker compare to prior work? Using\nconfiguration (f) from Tab. 3, in Tab. 1 we compare Co-\nTracker to state-of-the-art trackers on three benchmark\ndatasets, improving all metrics by substantial margins in all\ncases. Note that, for TAP-Vid-DAVIS, we assess tracking a\nsingle target point at a time, plus support points, which is\nfair to single-point trackers. Hence, the method advances\nthe state of the art.\nIs unrolled training important? We design CoTracker for\ntracking in a windowed manner, and in Tab. 4 we thus assess\nthe effect of unrolling windows during training (Sec. 3.4).\nSwitching off unrolled training decreases performance by\n18 AJ points, meaning that unrolled training helps to track\nover long periods of time, > 10\u00d7 longer than the sequences\nused in training. Hence, unrolled training is important.\nDo virtual tracks help CoTracker to scale? In Sec. 4, we\nassess the benefits of using virtual tracks. For a fixed mem-\nory budget (80 GB), using virtual tracks allows us to track\n\u00d77.4 more point points than without them; in fact, we can\ntrack a whole 263 \u00d7 263 grid, which is quasi-dense for the\ninput video resolution. The number of virtual tracks does\nnot affect scalability, but affects performance, with the best\nresults obtained using 64 virtual tracks. The overall accu-\nracy is also better when many tracks are utilized as it plays\non the strengths of the joint tracker. Hence, virtual tracks\nallow to track almost one order of magnitude more tracks.\n4.2. Limitations\nWhile better than other tracker, CoTracker still makes track-\ning mistakes that would be easily avoided by a human. An-\nother limitation is that it still processes relatively short win-\ndows of points at a time, and thus can fail for points that\nstay occluded for the length of multiple sliding windows.\nFor the same reason, it can fail to track points for thousands\nof frames. While joint tracking and unrolled training mit-\nigate this, offline applications of the tracker would benefit\nfrom considering all video frames at once.\n5. Conclusions\nWe have presented CoTracker, a transformer-based point\ntracker that tracks several points jointly, accounting for their\ncorrelation. CoTracker is state-of-the-art on the standard\ntracking benchmarks, often by a substantial margin, can\ntrack through occlusions and when points leave the field of\nview, even for hundreds of frames, can track very diverse\nconfigurations of points (target and support), and can track\na very large number of points simultaneously. The trans-\nformer architecture is flexible and future extensions could\nintegrate more functionalities, such as 3D reconstruction.\n8\n6. Acknowledgments\nWe would like to thank Luke Melas-Kyriazi for his pa-\nper comments, Jianyuan Wang, Roman Shapovalov, Luke\nMelas-Kyriazi and Adam W. Harley for the insightful dis-\ncussions. Christian Rupprecht is supported by ERC-CoG\nUNION101001212 and VisualAI EP/T028572/1.\nReferences\n[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\nIn Proc. ICML, July 2021. 4\n[2] Stanley T Birchfield and Shrinivas J Pundlik. Joint tracking\nof features and edges. In Proc. CVPR, 2008. 2\n[3] M. J. Black and P. Anandan. A framework for the robust\nestimation of optical flow. In Proc. ICCV, 1993. 2\n[4] Andr\u00b4es Bruhn, Joachim Weickert, and Christoph Schn\u00a8orr.\nLucas/kanade meets horn/schunck: Combining local and\nglobal optic flow methods. IJCV, 61, 2005. 2\n[5] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and\nMichael J Black. A naturalistic open source movie for opti-\ncal flow evaluation. In Proc. ECCV, 2012. 3\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In Proc. ECCV,\n2020. 5\n[7] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In Proc.\nCVPR, 2017. 12\n[8] Fei Chen, Xiaodong Wang, Yunxiang Zhao, Shaohe Lv, and\nXin Niu. Visual object tracking: A survey. CVIU, 222, 2022.\n2\n[9] Toshio M Chin, William Clement Karl, and Alan S Willsky.\nProbabilistic and sequential computation of optical flow us-\ning temporal coherence. IEEE Trans. on Image Processing,\n3(6), 1994. 2\n[10] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adri`a Re-\ncasens, Lucas Smaira, Yusuf Aytar, Jo\u02dcao Carreira, Andrew\nZisserman, and Yi Yang. Tap-vid: A benchmark for tracking\nany point in a video. arXiv, 2022. 2, 3, 6, 7, 11, 12\n[11] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush\nGupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman.\nTapir: Tracking any point with per-frame initialization and\ntemporal refinement, 2023. 1, 3, 7, 12\n[12] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical flow with convolutional networks. In Proc.\nICCV, 2015. 2, 3\n[13] Michael Elad and Arie Feuer. Recursive optical flow estima-\ntion\u2014adaptive filtering approach. J. of Visual Communica-\ntion and Image Representation, 9(2), 1998. 2\n[14] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, Thomas Kipf,\nAbhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-\nTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek\nNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-\nwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,\nMatan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,\nSuhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,\nFangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-\nable dataset generator. In Proc. CVPR, 2022. 6\n[15] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.\nParticle video revisited: Tracking through occlusions using\npoint trajectories. In Proc. ECCV, 2022. 2, 3, 7, 12\n[16] Adam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.\nParticle videos revisited: Tracking through occlusions using\npoint trajectories. arXiv.cs, abs/2204.04153, 2022. 7\n[17] Berthold KP Horn and Brian G Schunck. Determining opti-\ncal flow. Artificial intelligence, 17(1-3), 1981. 2\n[18] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang,\nKa Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng\nLi. Flowformer: A transformer architecture for optical flow.\nIn Proc. ECCV, 2022. 2\n[19] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,\nAlexey Dosovitskiy, and Thomas Brox. FlowNet 2.0: Evolu-\ntion of optical flow estimation with deep networks. In Proc.\nCVPR, 2017. 2\n[20] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,\nCarl Doersch, Catalin Ionescu, David Ding, Skanda Kop-\npula, Daniel Zoran, Andrew Brock, Evan Shelhamer,\nOlivier J. H\u00b4enaff, Matthew M. Botvinick, Andrew Zisser-\nman, Oriol Vinyals, and Jo\u02dcao Carreira. Perceiver IO: A gen-\neral architecture for structured inputs & outputs. In Proc.\nICLR, 2022. 2, 5\n[21] Joel Janai, Fatma Guney, Anurag Ranjan, Michael Black,\nand Andreas Geiger. Unsupervised learning of multi-frame\noptical flow with occlusions. In Proc. ECCV, 2018. 2\n[22] Shihao Jiang, Yao Lu, Hongdong Li, and Richard Hartley.\nLearning optical flow from a few matches. In Proc. CVPR,\n2021. 2\n[23] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Dy-\nnamicStereo: Consistent dynamic depth from stereo videos.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2, 6\n[24] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Dy-\nnamicstereo: Consistent dynamic depth from stereo videos.\nIn Proc. CVPR, 2023. 3\n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 13\n[26] David G. Lowe.\nDistinctive image features from scale-\ninvariant keypoints. IJCV, 60(2), 2004. 6, 11\n[27] Bruce D Lucas and Takeo Kanade. An iterative image reg-\nistration technique with an application to stereo vision. In\nProc. IJCAI, volume 2, 1981. 2\n[28] Jianqin Luo, Zhexiong Wan, Bo Li, Yuchao Dai, et al. Con-\ntinuous parametric optical flow. In Thirty-seventh Confer-\nence on Neural Information Processing Systems, 2023. 2\n[29] Lain Matthews, Takahiro Ishikawa, and Simon Baker. The\ntemplate update problem. PAMI, 26(6), 2004. 2\n[30] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical flow, and scene flow estimation.\nIn Proc. CVPR,\n9\n2016. 3\n[31] Michal Neoral, Jon\u00b4a\u02c7s \u02c7Ser\u00b4ych, and Ji\u02c7r\u00b4\u0131 Matas. Mft: Long-\nterm tracking of every pixel, 2023. 2, 7\n[32] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv, 2017.\n1\n[33] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang,\nErik B Sudderth, and Jan Kautz. A fusion approach for multi-\nframe optical flow estimation. In Proc. WACV, 2019. 2\n[34] Peter Sand and Seth Teller. Particle video: Long-range mo-\ntion estimation using point trajectories. IJCV, 80, 2008. 2\n[35] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li,\nManyuan Zhang, Ka Chun Cheung, Simon See, Hongwei\nQin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting\ntemporal cues for multi-frame optical flow estimation. arXiv,\n2023. 2\n[36] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang,\nKa Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and\nHongsheng Li. Flowformer++: Masked cost volume autoen-\ncoding for pretraining optical flow estimation. arXiv, 2023.\n2\n[37] Hedvig Sidenbladh, Michael J Black, and David J Fleet.\nStochastic tracking of 3d human figures using 2d image mo-\ntion. In Proc. ECCV, 2000. 2\n[38] Leslie N Smith and Nicholay Topin.\nSuper-convergence:\nVery fast training of neural networks using large learning\nrates.\nIn Artificial intelligence and machine learning for\nmulti-domain operations applications, volume 11006, pages\n369\u2013386. SPIE, 2019. 13\n[39] Xiuchao Sui, Shaohua Li, Xue Geng, Yan Wu, Xinxing Xu,\nYong Liu, Rick Goh, and Hongyuan Zhu.\nCraft: Cross-\nattentional flow transformer for robust optical flow. In Proc.\nCVPR, 2022. 2\n[40] Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun\nJampani, Michael Krainin, Huiwen Chang, Ramin Zabih,\nWilliam T Freeman, and Ce Liu. Autoflow: Learning a better\ntraining set for optical flow. In Proc. CVPR, 2021. 3\n[41] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPwc-net: Cnns for optical flow using pyramid, warping, and\ncost volume. In Proc. CVPR, 2018. 2\n[42] Shangkun Sun, Yuanqi Chen, Yu Zhu, Guodong Guo, and Ge\nLi. Skflow: Learning optical flow with super kernels. arXiv,\n2022. 2\n[43] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In Proc. ECCV, 2020. 2, 4\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Proc. NeurIPS,\n2017. 2\n[45] Jianyuan Wang, Yiran Zhong, Yuchao Dai, Kaihao Zhang,\nPan Ji, and Hongdong Li.\nDisplacement-invariant match-\ning cost learning for accurate optical flow estimation. Proc.\nNeurIPS, 33, 2020. 2\n[46] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,\nBharath Hariharan, Aleksander Holynski, and Noah Snavely.\nTracking everything everywhere all at once, 2023. 3, 7\n[47] Haofei Xu, Jiaolong Yang, Jianfei Cai, Juyong Zhang, and\nXin Tong. High-resolution optical flow from 1d attention\nand correlation. In Proc. CVPR, 2021. 2\n[48] Jia Xu, Rene Ranftl, and Vladlen Koltun. Accurate optical\nflow via direct cost volume processing. In Proc. CVPR, July\n2017. 2\n[49] Feihu Zhang, Oliver J Woodford, Victor Adrian Prisacariu,\nand Philip HS Torr. Separable flow: Learning motion cost\nvolumes for optical flow estimation. In Proc. CVPR, 2021. 2\n[50] Shiyu Zhao, Long Zhao, Zhixing Zhang, Enyu Zhou, and\nDimitris Metaxas. Global matching with overlapping atten-\ntion for optical flow estimation. In Proc. CVPR, 2022. 2\n[51] Tao Zhao and Ramakant Nevatia. Tracking multiple humans\nin crowded environment. In Proc. CVPR, volume 2, 2004. 2\n[52] Yang Zheng, Adam W Harley, Bokui Shen, Gordon Wet-\nzstein, and Leonidas J Guibas. Pointodyssey: A large-scale\nsynthetic dataset for long-term point tracking. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 19855\u201319865, 2023. 1, 2, 3, 6, 7, 12\n10\nCoTracker (Joint)\nCoTracker (Single)\nFigure 6. Single point vs. joint tracking. We qualitatively compare tracking individual points (top row) to tracking all points together\n(bottom). Background points are colored cyan while foreground points are magenta. We find that single-point tracking leads to background\npoints following the object motion, as the model cannot reason well about the global context of the video (cyan points in later frames).\nTracking points together results in better tracks for foreground and background points.\n16\n25\n36\n49\n64\n81\n100\nLocal\n81 100\n64\n49\n36\n25\n16\nGlobal\n0.731 0.730 0.744 0.743 0.748 0.747 0.753\n0.734 0.735 0.743 0.743 0.744 0.746 0.746\n0.735 0.737 0.745 0.748 0.749 0.748 0.750\n0.735 0.737 0.747 0.747 0.751 0.750 0.751\n0.734 0.739 0.746 0.747 0.754 0.753 0.755\n0.738 0.743 0.757 0.752 0.757 0.755 0.758\n0.735 0.739 0.755 0.746 0.757 0.755 0.755\n0.730\n0.735\n0.740\n0.745\n0.750\n0.755\nFigure 7. Number of points. Accuracy for additional global grid\nand local grid points. We compute \u03b4vis\navg on TAP-Vid-DAVIS to\nfind the optimal grid sizes. We use 25 & 64 because of speed and\nhigher occlusion accuracy (89.3 vs. 88.7).\nA. Additional ablations\nHere we provide additional ablation experiments that sup-\nplement the model choices from the main paper.\nSingle vs. joint point tracking. In Fig. 6, we demonstrate\nthe importance of joint tracking. We track a grid of points\nstarting at the first frame and compare CoTracker trained\nwith and without cross-track attention. Joint tracking pro-\nvides a global context and allows for better reasoning about\nthe positions of occluded points.\nSupport points. We test different configurations of support\npoints in Fig. 8. While adding SIFT [26] support points\nis better than sampling points uniformly on a global regu-\nlar grid, adding a local grid yields significantly better re-\nsults. The combination of global and local grids is slightly\nbetter, and in Fig. 7 we further test this combination with\n9 16 25\n36\n49\n64\n81\n100\nAdditional points\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\nvis\navg\nGlobal+Local\nLocal\nGlobal\nSIFT\nFigure 8. Accuracy on TAP-Vid-DAVIS depending on the num-\nber of additional local, global grid and SIFT points. When com-\nbining point sources, we use (close to) the same number of points.\ndifferent grid sizes ranging from 4 \u00d7 4 to 10 \u00d7 10. We\nfind that a configuration of 25 global and 64 local points\nis optimal for TAP-Vid-DAVIS in terms of speed and ac-\ncuracy. While there are differences in performance across\nthese hyper-parameters, the overall impact on performance\nis less than one percentage point for a wide variety of con-\nfigurations.\nTraining sliding window size. In, Tab. 6 we ablate the slid-\ning window size during training. While CoTracker can ben-\nefit from larger window sizes, the length of our training se-\nquences from Kubric [10] is limited to 24 frames. There-\nfore, there is a trade-off between the context length and the\nability to propagate predictions to future sliding windows.\nA sliding window of 8 is optimal for our training dataset.\nInference sliding window size. In Tab. 7, we examine the\nimpact of the sliding window size on the performance of a\nmodel trained with a sliding window of T = 8. We find that\nWindow size\nDAVIS First\nAJ\n< \u03b4x\navg\nOA\n4\n56.7\n73.1\n86.5\n8\n62.2\n75.7\n89.3\n16\n61.1\n75.5\n88.4\nTable 6. Training sliding window size. We train and evaluate\nCoTracker with varying window lengths. As the training data only\ncontains sequences of length 24, the model does not benefit from\ntraining with a bigger sliding window.\nWindow size\nDAVIS First\nAJ\n< \u03b4x\navg\nOA\n4\n54.9\n70.8\n83.2\n8\n62.2\n75.7\n89.3\n16\n54.6\n68.8\n82.9\nTable 7. Inference sliding window size. The model is trained\nwith sliding window size T = 8. The same window size gives the\nbest results.\nthe model performs best when the training and evaluation\nwindow sizes are identical.\nB. Efficiency\n1\n2154\n4641\n10000\nNumber of points\n0\n5\n10\n15\n20\n25\nTime, s\nTAPIR\nPIPs++\nCoTracker (Ours)\nFigure 9. Efficiency. We track N points sampled on the first frame\nof a 50-frame video of resolution 256\u00d7256 using a NVIDIA A40\n48GB GPU, and report the time it takes the method to process the\nvideo.\nIn Fig. 9 we evaluate efficiency of PIPs++ [52],\nTAPIR [11] and CoTracker on a 50-frame video of reso-\nlution 256 \u00d7 256 by running them on a A40 GPU. We track\npoints sampled on a regular grid at the first frame of the\nvideo and report the time it takes to process the entire video\nsequence. CoTracker is slower than TAPIR but faster than\nPIPs++ (while achieving better accuracy than both).\nStride\nKinetics First\nAJ\n\u03b4vis\navg\nOA\nPIPs++\n\u2014\n58.5\n\u2014\nTAPIR\n49.6\n64.2\n85.0\nCoTracker (Ours)\n48.8\n64.5\n85.8\nTable 8. Evaluation on TAP-Vid Kinetics.\nStride\nDAVIS First\nAJ\n\u03b4vis\navg\nOA\n8\n52.5\n68.6\n85.7\n4\n62.2\n75.7\n89.3\nTable 9. Feature stride ablation. Higher resolution features help\nto make much more accurate predictions.\nC. Evaluation on TAP-Vid-Kinetics\nIn Tab. 8 we evaluate CoTracker on TAP-Vid-Kinetics, a\ndataset of 1144 videos of approximately 250 frames each\nfrom Kinetics [7]. Some of these videos are discontinu-\nous, i.e., they are composed of continuous video chunks.\nCoTracker is designed for continuous videos, and TAPIR\ncontains a matching stage inspired by TAP-Net [10], which\ncan help with such combined video chunks. This is why\nthe performance gap between TAPIR and CoTracker for this\nbenchmark is smaller than for other benchmarks.\nD. Implementation Details\nIn this section, we complete the description of implementa-\ntion details from the main paper. We will release the code\nand models with the paper.\nFeature CNN. Given a sequence of T \u2032 frames with a res-\nolution of 384\u00d7512, we compute features for each frame\nusing a 2-dimensional CNN. This CNN downsamples the\ninput image by a factor of 4 (feature stride) and outputs fea-\ntures with 128 channels. Our CNN is the same as the one\nused in PIPs [15]. It consists of one 7 \u00d7 7 convolution with\na stride of 2, eight residual blocks with 3\u00d73 kernels and in-\nstance normalization, and two final convolutions with 3 \u00d7 3\nand 1 \u00d7 1 kernels. In Tab. 9, we train CoTracker with two\ndifferent feature downsampling factors (strides).\nSliding windows. When passing information from one\nsliding window to the next, we concatenate binary masks of\nshape (N, T) with visibility logits that indicate where the\nmodel needs to make predictions. For example, masks in\nthe first sliding window would be equal to 1 from the frame\nwhere we start tracking the point, and 0 before that. Masks\nfor all the subsequent sliding windows will be equal to 1 for\nthe first overlapping T/2 frames, and 0 for the remaining\n12\nT/2. During training, tracking starts either from the first or\nfrom a random frame where the point is visible. If a point\nis not yet visible in the current sliding window, it will be\nmasked out during the application of cross-track attention.\nIterative updates. We train the model with M = 4 itera-\ntive updates and evaluate it with M = 6. This setting pro-\nvides a reasonable trade-off between speed and accuracy, as\nevaluated in Fig. 10. Interestingly, the performance remains\nstable for 4-8 iterative updates and begins to degrade slowly\nthereafter.\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of updates\n0.69\n0.70\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\nvis\navg\nFigure 10. Inference iterative updates. We ablate the number of\niterative updates M during inference. The network is trained with\nM = 4.\nTraining. CoTracker is trained with a batch size of 32, dis-\ntributed across 32 GPUs. After applying data augmenta-\ntions, we randomly sample 768 trajectories for each batch,\nwith points visible either in the first or in the middle frame.\nWe train the model for 50,000 iterations with a learning rate\nof 5e\u22124 and a linear 1-cycle [38] learning rate schedule, us-\ning the AdamW [25] optimizer.\nAugmentations. During training, we employ Color Jitter\nand Gaussian Blur to introduce color and blur variations.\nWe augment occlusions by either coloring a randomly cho-\nsen rectangular patch with its mean color or replacing it\nwith another patch from the same image. Random scaling\nacross height and width is applied to each frame to add di-\nversity.\nE. Broader societal impact\nMotion estimation, whether in the guise of point tracking\nor optical flow, is a fundamental, low-level computer vision\ntask. Many other tasks in computer vision build on it, from\n3D reconstruction to video object segmentation. Ultimately,\nmotion estimation algorithms are an important components\nof a very large number of applications of computer vision in\nmany different areas. Point tracking has no direct societal\nimpact; however, positive or negative effects on society can\nmaterialise through its use in other algorithms, depending\non the final application.\n13\n"
  },
  {
    "title": "Planting a SEED of Vision in Large Language Model",
    "link": "https://arxiv.org/pdf/2307.08041.pdf",
    "upvote": "9",
    "text": "Planting a SEED of Vision in Large Language Model\nYuying Ge1\u22c6\nYixiao Ge1,2\u22c6\u2020\nZiyun Zeng2\nXintao Wang1,2\nYing Shan1,2\n1Tencent AI Lab\n2ARC Lab, Tencent PCG\nhttps://github.com/AILab-CVC/SEED\nAbstract\nWe present SEED, an elaborate image tokenizer that empowers Large Language\nModels (LLMs) with the emergent ability to SEE and Draw at the same time.\nResearch on image tokenizers has previously reached an impasse, as frameworks\nemploying quantized visual tokens have lost prominence due to subpar perfor-\nmance and convergence in multimodal comprehension (compared to BLIP-2, etc.)\nor generation (compared to Stable Diffusion, etc.). Despite the limitations, we\nremain confident in its natural capacity to unify visual and textual representations,\nfacilitating scalable multimodal training with LLM\u2019s original recipe. In this study,\nwe identify two crucial principles for the architecture and training of SEED that\neffectively ease subsequent alignment with LLMs. (1) Image tokens should be\nindependent of 2D physical patch positions and instead be produced with a 1D\ncausal dependency, exhibiting intrinsic interdependence that aligns with the left-\nto-right autoregressive prediction mechanism in LLMs. (2) Image tokens should\ncapture high-level semantics consistent with the degree of semantic abstraction in\nwords, and be optimized for both discriminativeness and reconstruction during the\ntokenizer training phase. As a result, the off-the-shelf LLM is able to perform both\nimage-to-text and text-to-image generation by incorporating our SEED through\nefficient LoRA tuning. Comprehensive multimodal pretraining and instruction\ntuning, which may yield improved results, are reserved for future investigation.\nThis version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M\npublicly available image-text pairs. Our preliminary study emphasizes the great\npotential of discrete visual tokens in versatile multimodal LLMs and the importance\nof proper image tokenizers in broader research.\n1\nIntroduction\nIn recent years, Large Language Models [1, 2, 3] (LLMs) pre-trained on massive text corpus with\nstraightforward training objectives such as next-word prediction have exhibited remarkable abilities\nto understand, reason, and generate texts across a variety of open-ended tasks. Recent studies\nfurther exploit the strong generality of LLMs to improve visual understanding or generation tasks,\ncollectively referred to as Multimodal LLM (MLLM). For example, previous work [4, 5, 6, 7, 8]\nperform open-ended visual QAs through aligning visual features of a pre-trained image encoder (e.g.,\nCLIP-ViT) with the input embedding space of LLMs. GILL [9] empowers LLM with the image\ngeneration ability by aligning its output embedding space with the pre-trained Stable Diffusion (SD)\nmodel [10].\nWhile these studies have contributed to technological advancements, MLLMs have yet to achieve the\nremarkable success of LLMs in terms of emergent capabilities. We have made a bold assumption that\nthe premise for the emergence of multimodal capabilities is that text and images can be represented\n*Equal Contribution.\n\u2020Correspondence to yixiaoge@tencent.com.\narXiv:2307.08041v2  [cs.CV]  12 Aug 2023\nOriginal Image\n2D Features\n(a) SEED Visual Tokenizer\nDiscrete Vision Codes\nGenerated Image\nTokenize\nDe-Tokenize\n5\n2\n3\n1\n7\nEncode\nGenerate\nNoise\nSemanCcally Consistent\nLarge Language Model\n(b) Mul6modal Autoregression with SEED tokens\nVision\nText\nVision\ns\n/s\nNext-word PredicCon\n1D Causal Dependency\nN+5\nN+2\nN+3\nN+1\nN+7\nIMG\n/IMG SOS\n1\n3\n8\n1\n4\n2\n9\n6\n5\nEOS\nA\nphoto\nof\nA\ndog\nsits\non\nthe\ngrass\nSOS\n15\n11\n12\n1\n4\n2\n9\n6\n5\nN+5\nN+2\nN+3\nN+1\nN+7\nIMG\n/IMG\nEOS\nA\ndog\nsits\non\nthe\ngrass\nGenerate an\nimage\nImage-to-Text  \nAutoregression\nText-to-Image  \nAutoregression\nSEED Tokenize\n5\n2\n3\n1\n7\n\u201cA dog sits on \nthe grass\u201d\nLLM Tokenize\n1\n4\n2\n9\n6\n5\nImage-Text Pair\nFigure 1: (a) The proposed SEED is a discrete image tokenizer, producing quantized visual codes with\n1D causal dependency and high-level semantics. (b) SEED visual tokens enable LLMs to perform\nboth visual comprehension and generation through multimodal autoregression with interleaved image-\ntext data.\nand processed interchangeably in a unified autoregressive Transformer. Fortunately, we have\njust found consensus in concurrent works [11, 12], all employing image-to-text and text-to-image\ngeneration tasks to demonstrate the emergent ability of unifying visual comprehension and generation\nin one framework. Regardless of discrete or continuous visual tokens, the training paradigm can be\nsummarised into three stages: visual tokenizer training, multimodal pretraining, and multimodal\ninstruction tuning. While concurrent studies primarily emphasize multimodal training (the latter\ntwo stages), this work focuses more on the visual tokenizer (the first stage).\nWe posit that a proper visual tokenizer can facilitate the follow-up multimodal training by (i) easing\nthe semantic alignment between visual and word tokens, and (ii) enabling LLM\u2019s original training\nrecipe (i.e., next-word prediction) for multimodal data without specific adaptation for visual tokens.\nRepresenting images as a sequence of discrete IDs is naturally compatible with the autoregressive\ntraining objective of LLMs. But unfortunately, works [13, 14] that utilize discretized visual tokens\nfor multimodal tasks have receded from prominence, as such models generally rely on super-scale\ntraining to converge, leading to substantial training costs. Moreover, we empirically found that the\ndominant tokenizer VQ-VAE [15] in existing works captures too low-level information for LLMs\nto effectively perform multimodal comprehension tasks. Existing image tokenizers fail to meet the\nrequirements of unifying visual understanding/generation tasks and facilitating multimodal training.\nTo this end, we introduce SEED, a VQ-based image tokenizer that produces discrete visual codes\nwith 1D causal dependency and necessary high-level semantics for both visual comprehension and\ngeneration tasks, as shown in Fig. 1. The off-the-shelf LLMs can be readily equipped with SEED by\ntreating discrete visual tokens as new words and updating the vocabulary with mapped visual codes.\nIn the paper, we present an MLLM by tuning the pre-trained LLM with low-rank adaptation (LoRA)\nto efficiently align with the SEED tokenizer.\nWe would like to emphasize the design principles of SEED. (1) Why causal-dependent tokens?\nExisting visual tokens (e.g., from VQ-VAE or CLIP-ViT) are generated using 2D context, which\nis incompatible with the unidirectional attention in dominant LLMs and counterintuitive for text-\nto-image tasks requiring raster order prediction. Thus, we convert 2D raster-ordered embeddings\ninto a sequence of semantic codes with 1D causal dependency. (2) Why high-level semantics? Since\nvisual and textual tokens in LLMs are expected to be interoperable\u2014sharing weights and training\nobjectives\u2014they should encompass the same degree of semantics to prevent misalignment, i.e., the\nhigh-level semantics inherently present in words.*\nSpecifically, the SEED tokenizer is composed of a ViT encoder, Causal Q-Former, VQ Codebook,\nReverse Q-Former, and a UNet decoder. The ViT encoder and UNet decoder are directly derived from\nthe pre-trained BLIP-2 and SD models, respectively. (1) Tokenize: Causal Q-Former converts 2D\nraster-ordered features produced by the ViT encoder into a sequence of causal semantic embeddings,\n*While focusing on high-level semantics during tokenization, it is still possible to achieve accurate spatial\nstructural control, such as layout and mask conditions, in image generation tasks. These spatial structural\nprompts can be tokenized similarly, as demonstrated by the success of SD [10, 16].\n2\nwhich are further discretized by the VQ Codebook. (2) De-Tokenize: The discrete visual codes are\ndecoded into generation embeddings via Reverse Q-Former. The generation embeddings are aligned\nwith the latent space of SD so that realistic images with consistent semantics can be generated using\nthe off-the-shelf SD-UNet.\nDuring SEED training, only Causal Q-Former, VQ Codebook, and Reverse Q-Former are tunable.\nCausal Q-Former is optimized by image-text contrastive loss. VQ Codebook and Reverse Q-Former\nare trained toward the objectives of dual reconstruction, i.e., the reconstruction between continuous\ncausal embeddings and discrete causal codes, the reconstruction between generation embeddings\nand the paired textual features. The training objectives ensure that SEED encapsulates the essential\nsemantics for both visual comprehension and generation. Quantitative results indicate that discrete\nSEED tokens exhibit competitive performance in text-image retrieval compared to BLIP-2, and in\nimage generation compared to Stable Diffusion. With further multimodal autoregressive training,\nSEED-OPT2.7B (efficiently tuned via LoRA using 5M image-text pairs) effectively performs image-\nto-text and text-to-image tasks, yielding promising results in zero-shot image captioning and visual\nQA, as well as generating high-quality images.\nThis effort aims to integrate multimodal comprehension and generation tasks within an LLM using\ndiscrete visual tokens. Our initial exploration of proper tokenizer designs strives to promote the\ndevelopment of emergent multimodal capabilities. Future work can further scale up training for\na better tokenizer and leverage stronger LLMs (e.g., LLaMA [1]) for comprehensive multimodal\npretraining and instruction tuning.\n2\nSEED Visual Tokenizer\n2.1\nPilot Experiments of Baseline Tokenizers\nVisual tokenizer aims to represent the image as a sequence of discrete tokens. Previous work [15,\n13, 17] trains a Vector Quantized Variational AutoEncoders (VQ-VAE) by reconstructing image\npixels, while Beit v2 [18] propose vector-quantized knowledge distillation (VQ-KD) to train a visual\ntokenizer by reconstructing high-level features from the teacher model. We conduct two experiments\nto respectively align discrete representations of VQ-VAE and Beit v2 with OPT2.7B [19] model on\nCC3M [20] dataset. We evaluate the performance with zero-shot image captioning on COCO [21].\nVQ-VAE achieves CIDEr 34.0 while Beit v2 achieves 42.0. The experiment results demonstrate that\na high-level visual tokenizer, which captures semantic representations of images instead of low-level\nimage details is more effective for multimodal comprehension.\n2.2\nArchitecture\nIn this work, we introduce a VQ-based image tokenizer SEED to produce discrete visual codes with\n1D causal dependency and high-level semantics. Specifically, as shown in Fig. 2, the SEED tokenizer\nis composed of a ViT image encoder [22], Causal Q-Former, VQ Codebook, Reverse Q-Former, and\na UNet decoder [10]. The ViT encoder and UNet decoder are directly derived from the pre-trained\nBLIP-2 and SD models, respectively. We first train a Causal Q-Former to convert 2D raster-ordered\nfeatures (16\u00d716 tokens) produced by the ViT encoder into a sequence of causal semantic embeddings\n(32 tokens). We then train a visual codebook to discretize the causal embeddings to quantized visual\ncodes (32 tokens) with causal dependency. We employ a Reverse Q-Former to decode the visual codes\ninto generation embeddings (77 tokens), which are aligned with the latent space of the pre-trained\nStable Diffusion (SD) model.\n2.2.1\nTraining Stage I: Causal Q-Former\nAs shown in Fig. 2, a set number of learnable query embeddings (32 tokens) and features of a\npre-trained ViT image encoder are fed into the Causal Q-former to encode a fixed number of\ncausal embeddings (32 tokens) of the input image. Specifically, the query embeddings can interact\nwith only previous queries through self-attention layers with causal mask, and interact with frozen\nimage features through cross-attention layers. We adopt contrastive learning to optimize Causal Q-\nformer fine-tuned from pre-trained BLIP-2 Q-Former on 5M image-text pairs including CC3M [20],\nUnsplash [23], and COCO dataset [21]. We use contrastive loss to maximize the similarity between\n3\nViT Encoder\nCausal Q-Former\nCausal Codes\n\u22ef\nCodebook\n\u22ef 1\n5\n2\n7\nReverse Q-Former\nSD Decoder\nCausal Embeddings\nGeneraAon Embeddings\nLearned Queries\n\u22ef\n\u22ef\nLearned Queries\n\u22ef\n\u201cA dog sits on the grass\u201d\nOriginal Image\nGenerated Image\nText Encoder\nText Embeddings\nReconstruct\nReconstruct\nContrasAve\nSEED Tokenize\nSEED De-Tokenize\nFigure 2: Overview of our SEED tokenizer, which produces discrete visual codes with causal\ndependency and high-level semantics.\nTable 1: Evaluation of zero-shot Image-Text Retrieval. Causal codes are quantized causal embeddings.\nModel\nFlickr30K (1K test set)\nCOCO (5K test set)\nImage \u2192 Text\nText \u2192 Image\nImage \u2192 Text\nText \u2192 Image\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@mean\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@mean\nBLIP-2 [5]\n81.9\n98.4\n99.7\n82.4\n96.5\n98.4\n92.9\n65.3\n89.9\n95.3\n59.1\n82.7\n89.4\n80.3\nSEED (causal emb)\n90.0\n99.6\n99.9\n80.0\n95.3\n97.6\n93.7\n71.9\n91.1\n95.9\n56.7\n80.7\n87.7\n80.7\nSEED (causal code)\n86.3\n98.6\n99.5\n75.9\n93.2\n96.7\n91.7\n65.7\n88.1\n93.8\n52.5\n78.0\n86.0\n77.4\nthe final causal embedding and text features of the corresponding caption, while minimizing the\nsimilarity between the final causal embedding and text features of other captions in a batch.\nEvaluation of Causal Embeddings. We evaluate the performance of Causal Q-Former on the\nzero-shot image-text retrieval task using COCO [21] and Flickr30K [24] dataset following BLIP-2.\nThe performance is measured by Recall@K (R@K) for both image-to-text retrieval and text-to-image\nretrieval. Note that we adopt the dual-stream paradigm for inference and remove the image-txt-\nmatching (ITM) rerank module in BLIP-2 for a fair comparison. As shown in Tab. 1, our Causal\nQ-former achieves better results than BLIP-2 in terms of an aggregated metric Recall@mean. It\ndemonstrates that the output query embeddings with causal dependency do not drop performance\nthan the output embeddings with bi-directional attention in BLIP-2.\n2.2.2\nTraining Stage II: Visual Quantization and De-tokenization\nAs shown in Fig. 2, we train a VQ codebook to discretize the causal embeddings (32 tokens) into\nquantized visual codes (32 tokens) on 5M image-text pairs including CC3M, Unsplash, and COCO\ndataset. Specifically, a quantizer looks up the nearest neighbor in the codebook for each causal\nembedding and obtains the corresponding code. We employ a decoder, which is a multi-layer\nTransformer [22], to reconstruct the continuous causal embeddings from discrete codes. During\ntraining, we maximize the cosine similarity between the output of the decoder and the causal\nembeddings. We further employ a Reverse Q-Former to reconstruct the textual features of a frozen\nstable diffusion model from discrete codes. A set number of learnable query embeddings (77 tokens)\nare fed into the Reverse Q-Former. The query embeddings interact with each other through self-\nattention layers, and interact with causal codes (32 tokens) through cross-attention layers for the\noutput generation embeddings (77 tokens). During training, we minimize the MSE loss between\n4\nInput\nReconstruction\nInput\nReconstruction\nFigure 3: Reconstruction images of SEED tokenizer (i.e., original image \u2192 SEED tokenize \u2192 causal\nvisual codes \u2192 SEED de-tokenize \u2192 reconstructed image), which are semantically consistent with\nthe original input images.\ngeneration embeddings and text features of SD. During inference, the generation embeddings can be\nfed into the SD-UNet to decode realistic images.\nEvaluation of Causal Codes. We evaluate the performance of SEED tokenizer on zero-shot image-\ntext retrieval, where the reconstructed causal embeddings from causal codes are used for retrieval. As\nshown in Tab. 1, discrete SEED tokens exhibit competitive performance compared to BLIP-2.\nTable 2: Evaluation of Image Genera-\ntion with CLIP similarity as the metric.\nModel\nCOCO\nFlickr30K\nGILL [9]\n67.45\n65.16\nSD [10]\n68.43\n65.40\nSEED\n68.23\n65.22\nWe further evaluate image generation on COCO and\nFlickr30K dataset. SEED first discretizes input images\ninto causal codes (32 tokens) and obtain generation em-\nbeddings (77 tokens) from Reverse Q-Former, which are\nfed into the SD-UNet for the reconstructed images. For\nthe baseline model GILL [25] and SD [10], images are\ngenerated from corresponding captions of the input images.\nWe follow GILL [25] to compute the CLIP similarity as\nthe evaluation metric for benchmarking the semantic con-\nsistency. As shown in Tab. 2, compared with the upper bound SD, our SEED only slightly drops\nperformance, and outperforms GILL in image generation.\n5\n2D Features\n(a) SEED Visual Tokenizer\nDiscrete Vision Codes\nTokenize\nDe-Tokenize\n5\n2\n3\n1\n7\nEncode\nGenerate\nNoise\nLarge Language Model\n(b) Mul6modal Autoregression with SEED tokens\nVision\nText\nVision\ns\n1D Causal Dependency\nN+5\nN+2\nN+3\nN+1\nN+7\nIMG\n/IMG SOS\n1\n3\n8\n1\n4\n2\n9\n6\n5\nEOS\nA\nphoto\nof\nA\ndog\nsits\non\nthe\ngrass\nSOS\n15\n11\n12\n1\n4\n2\n9\n6\n5\nN+5\nN+2\nN+3\nN+1\nN+7\nIMG\n/IMG\nEOS\nA\ndog\nsits\non\nthe\ngrass\nGenerate an\nimage\nImage-to-Text  \nAutoregression\nText-to-Image  \nAutoregression\nSEED Tokenize\n5\n2\n3\n1\n7\n\u201cA dog sits on \nthe grass\u201d\nLLM Tokenize\n1\n4\n2\n9\n6\n5\nImage-Text Pair\nFigure 4: Overview of the multimodal autoregressive training for SEED-OPT2.7B using efficient\nLoRA tuning. It was trained in 44 hours using only 64 V100 GPUs and 5M image-caption pairs.\nVisualization of Reconstructed Images. We visualize the reconstructed images of SEED in Fig. 3.\nThrough utilizing the Reverse Q-Former to obtain the generation embeddings from the causal visual\ncodes of the input image, realistic images can be generated using the off-the-shelf SD-UNet, which\nmaintain consistent semantics with input images.\nThe above evaluation and visualization demonstrate the versatility of SEED visual tokens for both\ncomprehension and generation tasks.\n3\nMultimodal Autoregression with SEED Visual Tokens\nBased on the pre-trained SEED tokenizer, we present SEED-OPT2.7B through fine-tuning a low-rank\nadaption (LoRA) module on a OPT2.7B [19] model with 5M image-text pairs including CC3M,\nUnsplash and COCO dataset. As shown in Fig. 4, we perform image-to-text and text-to-image\nautoregressive pre-training for unified multimodal comprehension and generation.\nImage-to-Text Autoregression. We first perform image-to-text autoregression to align the vocabulary\nof the pre-trained VQ codebook with OPT2.7B. Specifically, we use a fully-connected (FC) layer\nto linearly project the causal codes from the visual tokenizer into the same dimension as the word\nembeddings of OPT2.7B. The projected causal codes and the word embeddings of the prefix \u201cA photo\nof\u201d are concatenated as the input of the OPT2.7B. The text tokens of the corresponding caption is\nused as the generation target. We freeze OPT2.7B and fine-tune LoRA with the training objective of\npredicting the next text token.\nText-to-Image Autoregression. We then jointly perform image-to-text and text-to-image autoregres-\nsion to empower the LLM with the ability to generate vision tokens in addition to text tokens. For\ntext-to-image autoregressive pre-training, the word embeddings of the prefix \u201cGenerate an image\u201d\nand a caption are fed into OPT2.7B. The visual codes of the corresponding image from our pre-trained\ntokenizer are used as the generation target. We freeze OPT2.7B and fine-tune LoRA with the training\nobjective of predicting the next vision token.\nDuring inference, given the prompt \u201cGenerate an image\u201d and a text description, SEED-OPT2.7B\npredicts the visual tokens autoregressively. The output visual tokens are fed into the Reverse Q-Former\nfor generation embeddings, which can be decoded to generate a realistic image via SD-UNet.\nEvaluation of Multimodal Understanding. We evaluate the performance of SEED-OPT2.7B with\nzero-shot image captioning and visual question answering (vqa). For image captioning, we evaluate\non both COCO [21] test set and NoCaps [26] validation set and report BLEU@K (B@K), METEOR\n(M), ROUGEL (R), CIDEr (C), and SPICE (S) with the prompt \u201ca photo of\u201d. For visual question\nanswering, we evaluate on VQAv2 [27] validation set and GQA [28] test set and report Top-1\naccuracy with the prompt \u201cQuestion: {} Short answer.\u201d As shown in Tab. 3, compared with BLIP-2,\nwhich are trained on 129M image-text pairs, our SEED-OPT2.7B trained on 5M pairs achieves\npromising results on zero-shot image captioning and visual question answering with SEED discrete\nvisual tokens. Note that different from concurrent work CM3Leon [12] that uses image captioning\n6\nTable 3: Comparison between BLIP-2 (pre-trained with 129M image-text pairs) and SEED-OPT2.7B\n(5M pairs) on zero-shot Image Captioning and Visual Question Answering. S: SPICE, M: METEOR,\nR: ROUGEL, B: BLEU, C: CIDEr.\nModels\nNoCaps\nCOCO\nVQAv2\nGQA\nin\nnear\nout\noverall\nKarpathy test\nS\nS\nS\nS\nB@4\nM\nR\nC\nS\nTop-1\nTop-1\nBLIP-2 OPT2.7B [5]\n14.4\n13.8\n13.4\n13.8\n39.7\n28.9\n59.3\n131.0\n22.9\n51.9\n32.6\nSEED-OPT2.7B\n12.5\n12.3\n12.2\n12.3\n34.6\n28.4\n56.4\n119.0\n22.0\n42.8\n28.8\nA group of people riding \non the back of an elephant\nA woman standing on a \nbeach holding a surfboard\nA dragonfly is sitting \non top of a computer\nlittle girl eating a \nslice of watermelon\nImage Captioning\nQ: What kind of event are \nthe people involved in?\nA: wine tasting\nQ: Where is the clock located?\nA: on the side of the building\nVisual Question Answering\nQ: What is beside the elderly man?\nA: a white dog sitting on the bench\nin a park\nQ: What are they holding?\nA: a cake with a picture of\nwomen on it\nFigure 5: Qualitative examples of SEED-OPT2.7B on image captioning (with a prompt \u201ca photo of\u201d)\nand open-ended visual question answering. Our model has not been trained on any VQA dataset.\nand vqa datasets for supervised fine-tuning, our SEED-OPT2.7B pre-trained with image-to-text\nautoregression using the prefix \u201cA photo of\u201d can perform zero-shot visual question answering by\nunderstanding free-form questions and predicting open-form answers.\nWe also show qualitative examples of SEED-OPT2.7B on image captioning (with a prompt \u201ca photo\nof\u201d) and vqa. As shown in Fig. 5, our model can generate captions than describe the visual content,\nand answer a variety of questions.\nEvaluation of Multimodal Generation. We showcase qualitative examples of text-to-image genera-\ntion results with our SEED-OPT2.7B in Fig. 6. Given the textual description, SEED-OPT2.7B can\ngenerate realistic images that are semantically relevant to the description.\nSEED can facilitate alignment between visual tokens and LLMs, as evidenced by SEED-OPT2.7B,\nalready capable of performing text-to-image and image-to-text generation tasks after LoRA tuning.\n4\nRelated Work\nMultimodal Large Language Models for Comprehension. With the impressive success of Large\nlanguage models [1, 2, 3] (LLMs), recent studies work on Multimodal LLM (MLLM) to improve\nvisual comprehension through utilizing the strong generality of LLMs. Previous work [4, 5, 6, 29, 7,\n8, 30, 31] align visual features of pre-trained image encoder with LLMs on image-text datasets, and\nempower LLMs with the ability to interpret visual information with textual descriptions. However,\nthese work commonly use the prediction of the next text token as the training objective and exert no\nsupervision for vision data, thus can only output texts given multimodal vision and language inputs.\n7\nSnow mountains under a sunny sky\nA cat is lying on the couch\nA train is running on a track\nBurning fire in the open field\nA bird is sitting on top of a bare tree\nYellow flowers with\ngreen leaves in a vase\nA woman in a white dress\nis standing in front of walls\nA beach next to a rocky cliff\nA yellow car parked in front of a building\nFireworks in the city at night\nA dog is sitting on the green grass\nA room with lots of green plants\nA large house with a swimming pool\nA boy is riding a bike in the street\nA cup of coffee on\ntop of a wooden table\nA waterfall is flowing\ndown from a forest\nFigure 6: Text-to-image generation results when inferring with SEED-OPT2.7B.\nMultimodal Large Language Models for Generation. To empower LLMs with the image genera-\ntion ability, CogView [14] pre-trains a visual tokenizer by reconstructing image pixels, and fine-tunes\nGPT models [2, 32] with the objective of next token prediction, where both image and text tokens\nare equally treated. GILL [25] learns a mapping between the embeddings of a LLM and a frozen\npretrained image generation model. Both work aim to generate images with LLMs, without being\nexplicitly designed for multimodal comprehension.\nVisual Tokenizer. Visual tokenizer aims to represent the image as a sequence of discrete tokens\nsimilar to natural language. Previous work [15, 13, 17] trains a Vector Quantized Variational\nAutoEncoders (VQ-VAE) as a visual tokenizer by reconstructing the pixels of the input images,\nwhich captures only low-level details of images such as color, texture and edge. Beit v2 [18] trains a\nsemantic-rich visual tokenizer through reconstructing high-level features from the teacher model, but\nits visual codes from 2D features of a vision transformer [22] are incompatible with the unidirectional\nattention in dominant LLMs for multimodal generation.\n8\n5\nConclusion\nWe present SEED, a discrete image tokenizer, designed based on the premise that visual tokens\ncompatible with LLMs should capture high-level semantics while being generated with a 1D causal\ndependency. SEED enables LLMs to be trained with multimodal data following the original recipe\nof text (i.e., next-word prediction), which is mature and scalable. The trained multimodal LLM\nis capable of both image-to-text and text-to-image generation tasks, taking one more step toward\nemergent multimodal capabilities. We hope that our SEED would draw increased attention to visual\ntokenizers. A more rational visual tokenizer could substantially reduce the cost and complexity\nof multimodal LLM training, promoting lower-carbon, large-scale model training. Moreover, we\neagerly anticipate the \u201cgermination\u201d of vision (imagination) seeds within LLMs. The project is still\nin progress. Stay tuned for more updates!\nAcknowledgements\nWe sincerely acknowledge Sijie Zhao (Tencent AI Lab) and Chen Li (ARC Lab, Tencent PCG) for\ntheir engaging discussions.\nReferences\n[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[4] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\n[5] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[6] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[7] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint\narXiv:2304.15010, 2023.\n[8] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[9] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\n[10] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10684\u201310695, 2022.\n[11] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing\nLiu, Tiejun Huang, and Xinlong Wang.\nGenerative pretraining in multimodality.\narXiv preprint\narXiv:2307.05222, 2023.\n[12] Yu Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin, Golovneva Olga, Wang Tianlu, Babu Arun, Tang\nBinh, Karrer Brian, Sheynin Shelly, Ross Candace, Polyak Adam, Howes Russ, Sharma Vasu, Xu Jacob,\nSinger Uriel, Li (AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi Maryam, Celikyilmaz Asli,\nZettlemoyer Luke, and Aghajanyan Armen. Scaling autoregressive multi-modal models: Pretraining and\ninstruction tuning. 2023.\n[13] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,\npages 8821\u20138831. PMLR, 2021.\n9\n[14] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in\nNeural Information Processing Systems, 34:19822\u201319835, 2021.\n[15] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017.\n[16] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. Gligen: Open-set grounded text-to-image generation. 2023.\n[17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n12873\u201312883, 2021.\n[18] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with\nvector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\n[19] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[20] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565,\n2018.\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740\u2013755. Springer, 2014.\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[23] Unsplash. https://github.com/unsplash/datasets.\n[24] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67\u201378, 2014.\n[25] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\n[26] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the\nIEEE/CVF international conference on computer vision, pages 8948\u20138957, 2019.\n[27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.\n[28] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 6700\u20136709, 2019.\n[29] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint\narXiv:2303.16199, 2023.\n[30] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\n[31] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378, 2023.\n[32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n10\n"
  },
  {
    "title": "INVE: Interactive Neural Video Editing",
    "link": "https://arxiv.org/pdf/2307.07663.pdf",
    "upvote": "8",
    "text": "INVE: Interactive Neural Video Editing\nJiahui Huang1,2\nLeonid Sigal2,3,4,5\nKwang Moo Yi2\nOliver Wang1\nJoon Young Lee1\n1Adobe Research\n2University of British Columbia\n3Vector Institute for AI\n4CIFAR AI Chair\n5NSERC CRC Chair\ngabriel-huang.github.io/inve\nAbstract\nWe present Interactive Neural Video Editing (INVE), a\nreal-time video editing solution, which can assist the video\nediting process by consistently propagating sparse frame\nedits to the entire video clip. Our method is inspired by the\nrecent work on Layered Neural Atlas (LNA). LNA, however,\nsuffers from two major drawbacks: (1) the method is too\nslow for interactive editing, and (2) it offers insufficient sup-\nport for some editing use cases, including direct frame edit-\ning and rigid texture tracking. To address these challenges\nwe leverage and adopt highly efficient network architec-\ntures, powered by hash-grids encoding, to substantially im-\nprove processing speed. In addition, we learn bi-directional\nfunctions between image-atlas and introduce vectorized\nediting, which collectively enables a much greater variety\nof edits in both the atlas and the frames directly. Compared\nto LNA, our INVE reduces the learning and inference time\nby a factor of 5, and supports various video editing oper-\nations that LNA cannot. We showcase the superiority of\nINVE over LNA in interactive video editing through a com-\nprehensive quantitative and qualitative analysis, highlight-\ning its numerous advantages and improved performance. A\ndemo of our interactive editing interface can be found in the\nsupplementary materials.\n1. introduction\nImage editing methods have progressed significantly\nover the years and now allow for complex editing operations\nwith tools and interfaces that require little, to no, training\nfor the novice user. A testament to this is the availability\nand breadth of such tools in both commercial packages and\nmobile apps. Interactive video editing, on the other hand,\nremains a technical challenge that is yet to see similar suc-\ncess. At the same time, interactive video editing is an im-\nportant application that can open a breadth of creative op-\nportunities for non-professional users. Such techniques can\nenable a variety of video editing operations, including lo-\ncal object/actor recoloring, relighting, texture editing, and\nmany others.\nProgress in developing interactive video editing tech-\nniques has been slow due to the fundamental technical chal-\nlenges that must be addressed before such techniques can\nbecome practical. First, a scene being edited often consists\nof a non-static background and one-or-more foreground ob-\njects that undergo different motions. Edits must be localized\nand applied to these objects individually and then composed\nback to avoid unrealistic bleeding artifacts (e.g., a \u201cdog\u201d\nlogo added to the foreground object (car) suddenly sliding\noff and appearing in the background; see Fig 1). This re-\nquires robust temporally-consistent layered representations\nthat must be learned in an unsupervised manner, which in\nitself is a challenging task for realistic scenes. Second, ask-\ning the user to edit each frame individually is both unre-\nalistic and impractical from the user effort point of view.\nFurther, inconsistencies that may result from independent\nframe-based editing tend to have glaring visual artifacts as\nhumans are very sensitive to temporal inconsistencies. As\na result, a mechanism for sparse editing in time (and possi-\nbly in space) and an automated way to propagate such edits\nare useful features of a video editor. Third, the creative pro-\ncess of video editing often assumes some level of interactive\ncontrol over the edits. Building an approach that takes min-\nutes or hours to apply an edit would significantly stifle the\ncreativity of the user and render such techniques practically\nundesirable.\nEarlier 2D approaches advocated keyframe editing di-\nrectly in the frames and propagated these edits using frame-\nto-frame tracking (e.g., using optical flow) [3, 9]. Such ap-\nproaches tend to be challenged by drift and occlusions, pro-\nducing artifacts that highly depend on the video content, se-\nlected keyframes, and the edits applied. Recently developed\nlayered neural atlas representations [13], enables consistent\nediting of videos, containing arbitrary types of moving ob-\njects or background, by representing the video by a set of\n1\narXiv:2307.07663v1  [cs.CV]  15 Jul 2023\n\"hue -20\", \n\"brightness +10\"\nInput Texture\nLocal Adjustments\nSketches\nEdited Video\nUser edits one frame\nFigure 1. NeViE can propagate multiple types of image editing effects to the entire video in a consistent manner. In this case, the edits\nconsist of (1) adding external graphics (dog picture) to the jeep; (2) Applying local adjustments (Hue -20, Brightness +10)) to the forest in\nthe background; (3) Sketching on the road using the brush tool. All these types of edits can be propagated instantly from one frame to all\nother frames using the proposed approach.\nlayered neural 2D atlases (i.e., images), one for each ob-\nject and one for background. Such representations have a\nnumber of appealing properties, which include locality and\nconsistency of edits enabled by editing in the individual at-\nlases as opposed to the keyframes directly. However, cer-\ntain challenges remain. First, the estimated mapping from\nthe atlas to video pixels is not bijective, enabling edits only\nin the atlas. This is less ideal for certain applications, as\ntypically non-linear mapping (represented by a neural net-\nwork), makes it difficult to anticipate how a specific atlas\nedit will be perceived in the video. This results in less than\nintuitive editing and potential unexpected deformation arti-\nfacts. Second, current layered neural atlas representations\ntend to be slow to compute, making the editing effectively\nnon-interactive.\nIn this paper, our focus is on addressing these core chal-\nlenges, while, at the same time, building on the successes\nof neural atlas representations. We do this by proposing\nto learn a bi-directional mapping between the atlases and\nthe image, along with vectorized sketching that enables us\nto make consistent edits either in the atlas itself or in the\nimage (by back-projecting the edits onto the learned atlas).\nThis significantly extends the editing operations available\nto the user. Further, we adopt and develop multi-resolution\nhash coding [16] to the task of layered neural atlas represen-\ntations, which significantly improves both the learning and\ninference speed allowing more interactive user interactions\nand control.\nContributions:\nOur contributions are both technical /\nmethodological as well as user-centric \u2013 enabling richer vo-\ncabulary of consistent and interactive video edits for novice\nusers. We summarized our contributions below:\n\u2022 INVE achieves 5\u00d7 faster training and inference speed\ncompared to existing methods [13];\n\u2022 we introduce inverse mapping to enable rigid texture\ntracking effects;\n\u2022 we support editing multiple video effects independently\nvia layered editing;\n\u2022 we introduce Vectorized Sketching for artifact-free sketch\nediting at the frame level.\n2. Related Works\n2.1. Video Effects Editing\nVideo effects editing involves adding or modifying vi-\nsual effects in a video.\nMany methods have been pro-\nposed in the literature to address this problem, including\nboth traditional and deep learning-based approaches. One\ntraditional approach is to use keyframes to represent the\neffects and interpolate between them to generate a video\nwith smooth transitions [9]. Deep learning-based methods\nhave also been explored for video effects editing. For ex-\nample, Generative Adversarial Networks (GANs) [6] have\nbeen used to generate new video frames with different vi-\nsual effects, such as removing rain or snow [26], generat-\ning a photorealistic video from an input segmentation map\nvideo [28], or generating frames with controlled, plausible\nmotion [8]. In addition, other deep learning-based methods\n2\nInput\nReconstruction\nFG Atlas\nBG Atlas\nAlpha Channel\nFigure 2. Our forward mapping pipeline (solid lines) closely follows LNA\u2019s approach. Each video pixel location (x, y, t) is fed into two\nmapping networks, Mf, Mb to predict (u, v) coordinates on each atlas. Then these coordinates are fed into the atlas network A to predict\nthe RGB color on that atlas. Finally, we use the opacity value \u03b1 predicted by the alpha network Ma to compose the reconstructed color at\nlocation (x, y, t). Our backward mapping pipeline (dotted lines) maps atlas coordinates to video coordinates, it takes an (u, v) coordinate,\nas well as the target frame index t as input, and predicts the pixel location (x, y, t). With the forward and backward pipelines combined,\nwe can achieve long-range point tracking on videos.\nhave been used for video effects editing, such as video style\ntransfer [12], which involves transferring the style of one\nor few keyframes to the entire video, super-resolution [22],\nwhich involves increasing the resolution of a video. In our\nwork, we focus on propagating single-frame edits to the\nentire video in a consistent manner, where videos can be\nedited as if editing a single image, we demonstrate that our\nediting pipeline can propagate multiple types of image edit-\ning effects to the entire video consistently.\n2.2. Video Propagation\nVideo propagation is an important area of research in\ncomputer vision, which focuses on the propagation of vi-\nsual information through time in video data. Some meth-\nods [3, 9] purpose to propagate information based on con-\nstraints posed by optical flow, however, since optical flow is\nonly computed within neighboring frames, these methods\noften suffer from propagation drifting over a long period of\ntime. Deep learning-based methods [10,11,18,29,31], have\nalso been extensively explored in recent years. For exam-\nple, Video Propagation Networks [11] first splats informa-\ntion to a bilateral space, then uses a learned filter to slice the\ninformation back to image space. Some other approaches\n[13,21] learn unwarped 2D texture maps, then edits can be\nperformed on these maps, and be warped back to all frames.\nFor example, Layered Neural Atlases (LNA) decomposes\nthe input video into the foreground and background layers,\nand learns two mapping networks that map each video pixel\nto the UV coordinates on the fore-background texture maps,\nwhich they call atlases. Our method is conceptually similar\nto LNA, except that we made several improvements to the\nedit-ability and overall editing experience (including learn-\ning and inference speed).\n2.3. Implicit Neural Representation\nRecent works have shown that implicit neural represen-\ntation can be very robust for representing visual data. For\nexample, representing 3D geometry with neural radiance\nfields [2, 15, 24, 30], representing 2D image data for im-\nage compression [5], image super-resolution [4], and image\ngeneration [1, 23]. Representing 3D video volume using\nimplicit functions has also been explored, for example, Mai\net al. proposed Motion-Adjustable Neural Implicit Video\nRepresentation [14], which allows re-synthesizing videos\nwith different motion properties, and Layered Neural At-\nlases [13], which enables consistent video editing. Mean-\nwhile, highly efficient network architectures [17] have been\npurposed to reduce the computational cost of training and\ntesting of these implicit networks, and hashed encoding [16]\nwas purposed to drastically improve the convergence speed\nof training such networks. In our work, we represent an in-\nput video with six implicit neural networks: two forward\nmapping networks, two backward mapping networks, one\nopacity network, and one atlas network, all implemented\nwith high-efficiency network architectures and encoding\nfunctions. With these networks combined, our approach\nenables interactive and consistent editing, as well as basic\npoint tracking on videos.\n3\n3. Interactive Neural Video Editing (INVE)\nIn this section, we describe our method for interactive\nneural video editing, INVE. As noted in Sec. 1, our fo-\ncus is to perform edits directly on a given frame, which\nis then automatically propagated to all other frames con-\nsistently. To explain our method, we first review Layered\nNeural Atlases [13] in Sec. 3.1, which is the base frame-\nwork that we build our method on top of. We then discuss\nhow we achieve interactive performance by boosting com-\nputation speed in Sec. 3.2, then discuss how we enable rigid\ntexture tracking \u2013 a critical feature for easy video editing,\nby introducing inverse mapping in Sec. 3.3. Lastly, we dis-\ncuss how we edit videos with our method, with a focus on\nvectorized sketching that allows artifact-free sketch editing\nat the frame level in Sec. 3.5.\n3.1. Review of Layered Neural Atlases\nLayered Neural Atlases (LNA) [13] represents a video\nsequence with three sets of neural networks: (1) the map-\nping networks, which we write as M : (x, y, t) \u2192 (u, v)\nthat map 3D video pixel coordinates to 2D texture coor-\ndinates on the atlases; (2) the atlas networks, A(u, v) \u2192\n(r, g, b), which predict the color of a given texture coordi-\nnate on a given atlas; (3) the opacity network, O(x, y, t) \u2192\n\u03b1, that predicts the opacity values at each pixel w.r.t. each\natlas.\nEach of the above networks is represented by a\ncoordinate-based MLP.\nThe entire framework is trained end-to-end in a self-\nsupervised manner. The main loss is an unsupervised re-\nconstruction loss, where the network is tasked to reconstruct\nthe RGB color of a given video pixel location. LNA also has\nthree regularization losses: (1) Rigidity loss: encourages the\nmapping from video pixels to the atlas to be locally rigid;\n(2) Consistency loss: encourages corresponding pixels in\nconsecutive frames to be mapped at the same location on\nthe atlases, it uses pre-computed optical flow to estimate\nthe pixel correspondence. (3) Sparsity loss: encourages the\natlases to contain minimal content needed to reconstruct the\nvideo.\nOnce the neural representation (the atlas) for the video\nis obtained via training, video editing is performed by edit-\ning directly on the atlases. These \u2018atlas-level edits\u2019 are then\nmapped to each frame by the learned mapping function.\nThe final edited video is obtained by blending these edits\nwith the original video. Hence, this atlas is in fact an in-\ntermediate layer that eventually needs to be mapped onto\neach frame to be actually realized. Thus, while it is possi-\nble to visually inspect the atlas, edits on this atlas are not\nhow an edit would look when mapped onto an actual frame,\nmaking it suboptimal for performing video editing. More-\nover, mapping in LNA is unidirectional \u2013 from the frames\nto the atlas, which makes it difficult for any intuition for\npoint/texture tracking.\nreconstruction loss convergence speed\nflow loss convergence speed\nFigure 3. Convergence Speed Comparison. Given the same num-\nber of training iterations, both reconstruction quality (measured\nby the reconstruction loss) and mapping accuracy (measured by\nthe optical flow loss) of our model converges faster than LNA\u2019s.\nIn LNA, the authors briefly mentioned that a user can edit\nthe video by directly sketching on the frame, this is realized\nby first generating a frame edit layer (a raster image of the\nsize of the frame) containing all the sketches, then mapping\nthis frame edit layer to the atlas edit layer (this is done by\nmapping every pixel on the frame edit layer to the atlas), and\nfinally interpolate the color of the atlas edit layer. Doing so\nhas two obvious drawbacks, first, mapping the entire frame\nedit layer is computationally expensive, the total mapping\ncost is H \u00d7 W pixels, and second, the resulting atlas edit\nlayer may contain undesirable artifices stemming from the\ninterpolation (see Figure 5).\n3.2. Boosted Training & Inference Speed\nBesides the problem of mapping and editing of atlas, an-\nother important issue with LNA is that it is too slow for\ninteractive video editing. We make an observation that the\ntask of atlas-based video modeling is similar, at the core,\nto the task of gigapixel image approximation. Specifically,\nthey both use implicit neural representations to \u201cmemo-\nrize\u201d the input data. LNA uses sinusoidal positional en-\ncoding [25] to increase the frequency of the network input,\nwhich shifted all the \u201cmemorization\u201d overload to the subse-\nquent MLPs. To tackle this problem we turn our attention\nto recent neural field backbones that utilize multiresolution\nhash grids (InstantNGP) [16]. In our pipeline, instead of\nthe sinusoidal positional encoding, we opt for the multires-\nolution hash grid, which shared part of the \u201cmemorization\u201d\noverload from the MLPs to the trainable encoding itself, this\ncan lead to a significant boost in convergence speed. Fur-\nthermore, we use a GPU parallelized and fully fused MLP\nimplementation using the TinyCUDA library [17] that sig-\nnificantly improves the computation speed of our pipeline.\nWe further train significantly fewer iterations than LNA,\nwhich we detail in Sec. 3.6.\n3.3. Inverse Mapping for point tracking on videos\nAs noted earlier, LNA only supports one directional\nmapping, from frame coordinates to atlas coordinates\u2013we\n4\nrefer to this as forward mapping.:\nM(x, y, t) \u2192 (u, v) .\n(1)\nEditing using LNA\u2019s pipeline is achieved by sampling the\nedited color from the atlas layers, this is equivalent to warp-\ning from the atlas plane to the frame plane using a dense\nwarping field, defined by an untrackable inverse mapping\nfunction, which can result in undesirable warping deforma-\ntions for rigid texture tracking.\nConversely, in our work, we propose to also model the\ninverse mapping function using neural networks. Specifi-\ncally, we introduce additional mapping networks (one per\nlayer) on top of the LNA framework that map from atlases\nto frames. Formally, given a point (u, v) on the atlas, and\nthe destination frame index t, the inverse mapping function\nB will predict the landing pixel coordinate (x, y) on frame\nt:\nB(u, v, t) \u2192 (x, y, t) .\n(2)\nIn this way, given a point p on frame t, we can easily track\nits trajectory P by first mapping it to the atlas using forward\nmapping M, then use the inverse mapping to calculate its\ncorresponding locations on the rest of the frames, that is:\nP = B(u, v, T) .\n(3)\nWhere T = {t0, t1, .., tN}, indicating the frame index.\nThe training of the inverse mapping networks is super-\nvised by the forward mapping networks. After fully training\nthe forward mapping networks, we start training the inverse\nmapping by randomly sampling the video to obtain pixel\u2013\natlas coordinate pairs using forward mapping. We then use\nthese paired data to train the inverse mapping networks. As\nwe desire to be able to predict all frames that the (u, v) coor-\ndinate maps to, we extend the input domain with the frame\ntime, as seen in in Equation 2.\n3.4. Layered Editing\nImage editing is usually done with layers. For example,\nin Adobe Photoshop, users can overlay multiple editable\nlayers on top of the original image, and each layer can be\naccessed and edited individually. The final output is usually\na back-to-front composition of all layers. We adopt a sim-\nilar idea for our editing pipeline, we overlay three editable\nlayers on top of the atlases, and each one of them stores a\ndifferent type of edit, so that they can be accessed individu-\nally should one wish to do so. Specifically:\n\u2022 Sketch edits. A user can draw vectorized sketches using\nthe brush tool (see more on Sec. 3.5).\n\u2022 Texture edits. When the user \u201cdraws\u201d an imported asset\n(this is done by clicking on the frame/atlas to set the an-\nchor point and dragging to set the size), the anchor point\nsketches on frame \nsketches on atlas \nedited frames\nFigure 4. Vectoriezed Sketching. User sketches directly on the\nframe, the mouse tracks {(xi, yi)} that define these sketches will\nbe mapped to atlas coordinates {(ui, vi)}, then these tracks will\nbe used to render polylines on the atlas edit layer.\ncoordinates and the size of the texture (width and height)\nwill be stored, and the texture will be \u201cpasted\u201d onto the\ntexture edit layer in the atlas space.\n\u2022 Metadata edits. A user can perform local adjustments\n(i.e., increase the brightness) at any desired region on\nthe frame by drawing out these regions with the brush\ntool, the adjustment metadata will be carried by the brush\nstroke, and stored in the metadata edit layer in the atlas\nspace.\nA user can edit directly on those layers, or edit on the\nframes. When editing on frames, edits are first mapped to\natlas coordinates, then stored in the corresponding layer de-\npending on the edit type.\nThe final result is rendered pixel-by-pixel.\nFor each\nvideo pixel, we first map its coordinate to its atlas coor-\ndinate using the forward mapping function, we then look\nup the edits of that pixel in the atlas space, and finally, we\nrender the RGB value of that pixel by using back-to-front\ncomposition through all edits and the original pixel value.\n3.5. Vectorized Sketching\nBeing able to sketch directly on frames is a very de-\nsirable function in video editing, for example, perform-\ning free-form annotations when analysing a sports video.\nAs mention earlier in Sec. 3.1, frame sketch editing using\nLNA\u2019s pipeline is sub-optimal due to its slowness and un-\ndesirable artifacts. These artifacts arise due to the fact that\nthe atlas has to be resampled onto the target image domain\nfor rendering. If the sampling rate of the atlas is too low, we\ncan see aliasing artifacts in the rendering (see Fig. 5).\nTo address these two problems, we propose vectorized\nsketching (Fig. 4), where we represent a user sketch as a\n5\ncontinuous vectorized representation, so that we can avoid\nresampling it. We choose to represent the sketch as a polyg-\nonal chain, which is defined by a sequence of K control\npoints:\nEf = {(xi\u22121, yi\u22121) : (xi, yi)} , i \u2208 {1, 2, ...K} .\n(4)\nWe then map these control points to atlas coordinates,\n(ui, vi) = M(xi, yi), i \u2208 {1, 2, ...K} ,\n(5)\nthen define the polygonal chain in the atlas space as:\nEa = {(ui\u22121, vi\u22121) : (ui, vi)} , i \u2208 {1, 2, ...K} .\n(6)\nBy doing so, we can avoid warping artifacts and bring down\nthe mapping cost from H \u00d7 W pixels to K pixels.\nIn addition, vectorized sketches can carry additional at-\ntributes other than color alone. For example, in our edit-\ning pipeline, each sketch stroke can carry a metadata field,\nwhich includes brightness, hue and saturation values. These\ncan be used to apply local adjustments as discussed earlier\nin Sec. 3.4.\n3.6. Implementation Details\nEarly Stopping.\nIn our work, the main aim is to per-\nform video editing, not creating a neural representation for\nvideos. Hence, as long as we have accurate mappings be-\ntween the atlas and the frames, the quality of the atlas and\nthe reconstructed video frames are irrelevant. Thus, we train\nour method only until the mapping network matures, which\nwe empirically found to be much quicker than the atlas net-\nwork A of our pipeline.\nDetails.\nOur implementation of the Neural Video edit-\ning pipeline closely follows Layered Neural Atlases (LNA)\n[13]. As in LNA, we train and test our method on videos\nconsisting of 70 frames with resolution of 768 \u00d7 432. We\nrandomly sample 10,000 video pixels per batch and train\nthe model for around 12,000 iterations, which is notably\nless than the LNA implementation (300,000 iterations). In\ntotal, our model has \u223c1.7 M parameters, and requires 5 GB\nGPU memory. Training our model takes about 5 minutes,\nand rendering the final video takes 2.8s (\u223c25 fps) on an\nNVIDIA RTX 4090 GPU.\n4. Results\nIn this section, we evaluate the effectiveness of our pro-\nposed method on videos from the DAVIS dataset [20], as\nwell as our own videos. Following the approach of LNA, we\nutilize RAFT [27] for optical flow extraction. We discov-\nered that the quality of the masks significantly impacts the\nreconstruction results and convergence speed. Therefore,\nwe opted for a more precise mask extractor [19] instead of\nVectorized\nLinear Interpolation\nNearest Interpolation\nFigure 5. Our vectorized sketching allows users to perform sketch\nediting directly on frames free from resampling artifacts (left),\nwhereas frame editing using LNA\u2019s pipeline either results in in-\nconsistent color (middle) or noncontinuous sketches (right).\nMaskRCNN [7]. Our approach aims to improve two criti-\ncal aspects of LNA: training / testing speed, and edit-ability.\nWe conduct all our experiments on a single NVIDIA RTX\n4090 GPU.\n4.1. Improved Training & Inference Speed\nTo improve training and testing speed, we first adapt\nthe GPU-optimized Fully Fused MLP [17] architecture into\nour pipeline, which significantly increased the computation\nspeed per sample batch, from 23 iterations (10,000 sam-\nples/batch) per second to 48 iterations. We further improved\nthe convergence speed of our model by adapting the mul-\ntiresolution hash encoding [16], as shown in Figure 3, af-\nter training the same number of iterations, both the recon-\nstruction loss (representing reconstruction quality) and the\nflow loss (representing mapping accuracy) converges faster\non our model than LNA. On Figure 6, we show that given\nthe same training time, the quality of reconstructed frames\nfrom our model is much better than LNA\u2019s both visually and\nquantitatively (see PSNR on the bottom of each image). At\ntest time, the rendering speed of our approach is 24.81 FPS,\ncompared to LNA\u2019s 5.34 FPS. The boost in both training\nand inference speed makes our method more favorable for\ninteractive video editing.\n4.2. Inverse Mapping for Point Tracking\nThe LNA approach only supports one directional for-\nward mapping.\nEditing using forward mapping alone is\nequivalent to warping the edited texture using a dense warp-\ning field, which can be insufficient to support many editing\napplications, such as adding rigid textures that track a sin-\ngle/few points. For example, Figure 7 shows a case where\nthe user wants to add a \u201chat\u201d texture to the dancer. If the\nvideo is edited using LNA\u2019s one-directional mapping, the\nhat texture needs to be warped to the frame using the dense\nwarping field defined by the forward mapping function (see\n6\n10 s\n60 s\n120 s\nours\nLNA\n30 s\nPSNR = 22.16\nPSNR = 17.69\nPSNR = 23.93\nPSNR = 19.34\nPSNR = 24.84\nPSNR = 20.55\nPSNR = 25.43\nPSNR = 22.11\nFigure 6. Given the same training time, the quality of reconstructed frames produced by our model is much better than LNA\u2019s both visually\nand quantitatively (see PSNR onthe bottom of each image).\nWarp Field\nBackward Mapping \nEnabeld Tracking\nWarpping\u00a0 \nDirectly\nt = 0\nt = 20\nt = 40\nt = 60\nFigure 7. Inverse Mapping enabled tracking. Editing using LNA\u2019s forward mapping alone is equivalent to warping the edited texture using\na dense warping field (visualized on top row), which can lead to undesired warpping effects (bottom row). Our approach introduces inverse\nmapping, which enables video particle tracking spamming all frames, here we showcase using tracking function to insert a texture that\ntracks a selected point (middle row).\ntop row), as a result, the texture is warped completely out\nof shape (see the bottom row). With our inverse mapping\nfunction, the user can add the texture that tracks a point on\nher head, which gives more promising results (see middle\nrow).\n4.3. Layered Editing Pipeline\nOur layered editing pipeline allows users to overlay mul-\ntiple editable layers on top of the atlases, and each layer\ncan be accessed and edited individually. On Figure 8, we\ndemonstrate the results of all three types of edits supported\nby our pipeline. On the top row, we show that user sketches\ncan be consistently propagated to all frames in the video.\nIn the middle row, we show that the user can apply lo-\ncal adjustments (in this case, lower saturation and higher\nbrightness) to a specific region in the scene by using our\nvectorized sketching tool, which can carry the adjustment\nmetadata field, and on the bottom row, we show that user\ncan import external graphic textures that track and deform\nwith the moving foreground object. On Figure 9, we show-\n7\nSketch Edits\nLocal Adjustments\nTexture Edits\nFigure 8. Layered Editing. Our layered editing pipeline supports three types of edits: 1) Sketch Edits (top), where users can sketch scribbles\nusing the brush tool; 2) Local Adjustments (middle), users can apply local adjustments (brightness, saturation, hue) to a specific region in\nthe scene; 3) Texture Edits (bottom), users can import external graphics that tracks and deforms with the moving object.\nReference Frame (unedited)\nEdited\u00a0 Video\nFigure 9. Results showcase. Here we showcase some videos edited using our pipeline, on the left is a reference of an unedited frame, and\non the right are the sampled frames from the edited video.\ncase some videos edited using our pipeline; our method can\npropagate various types of edits consistently to all frames.\n4.4. Vectorized Sketching\nOur purposed vectorized sketching allows us to map the\npolygonal chains (represented by a set of control points)\nthat define the sketch strokes directly to the atlases, which\ncan help reduce computational cost, and avoid artifacts\nstemming from LNA\u2019s frame editing pipeline (map frame\nsketches as a raster image). On Figure 5, we show the re-\nsulting edited atlas produced by vectorized sketching (left),\nLNA editing using linear interpolation (middle), and LNA\nediting using nearest neighbor interpolation (right). One\ncan easily observe that mapping frame sketches using our\n8\nmethod provides a continuous sketch stroke with consis-\ntent color, whereas LNA\u2019s pipleine either produces non-\ncontinuous sketch, or inconsistent color, depending on the\ninterpolation method.\n5. Conclusion.\nWe propose INVE: Interactive Neural Video Editing, an\ninteractive video editing pipeline, which makes video edit-\ning easier and more accessible by instantly and consistently\npropagating single-frame edits to the entire video.\nOur\nmethod is inspired by the recent work Layered Neural At-\nlas (LNA), upon which we made several improvements in\nspeed and in editability. We believe that INVE can signif-\nicantly improve the video editing experience, particularly\nfor beginners who may be intimidated by the complexity of\ntraditional editing tools.\nReferences\n[1] Ivan Anokhin, Kirill V. Demochkin, Taras Khakhulin, Gleb\nSterkin, Victor S. Lempitsky, and Denis Korzhenkov. Im-\nage generators with conditionally-independent pixel synthe-\nsis. 2021 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 14273\u201314282, 2020. 3\n[2] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 5835\u20135844, 2021.\n3\n[3] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang\nHua. Coherent online video style transfer. 2017 IEEE In-\nternational Conference on Computer Vision (ICCV), pages\n1114\u20131123, 2017. 1, 3\n[4] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning con-\ntinuous image representation with local implicit image func-\ntion. 2021 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 8624\u20138634, 2020. 3\n[5] Emilien\nDupont,\nAdam\nGoli\u2019nski,\nMilad\nAlizadeh,\nYee Whye Teh, and A. Doucet. Coin: Compression with\nimplicit neural representations.\nArXiv, abs/2103.03123,\n2021. 3\n[6] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\nand Yoshua Bengio. Generative adversarial nets. In NIPS,\n2014. 2\n[7] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross B.\nGirshick. Mask r-cnn. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 42:386\u2013397, 2017. 6\n[8] Jiahui Huang, Yuhe Jin, Kwang Moo Yi, and Leonid Sigal.\nLayered controllable video generation.\nEuropean Confer-\nence on Computer Vision (ECCV), 2022. 2\n[9] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo-\nhannes Kopf. Temporally coherent completion of dynamic\nvideo.\nACM Transactions on Graphics (TOG), 35:1\u201311,\n2016. 1, 2, 3\n[10] A. Jabri, Andrew Owens, and Alexei A. Efros.\nSpace-\ntime correspondence as a contrastive random walk. ArXiv,\nabs/2006.14613, 2020. 3\n[11] V. Jampani, Raghudeep Gadde, and Peter Gehler.\nVideo\npropagation networks. 2017 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 3154\u20133164,\n2016. 3\n[12] Ondrej Jamriska, S\u00b4arka Sochorov\u00b4a, Ondrej Texler, Michal\nLuk\u00b4ac, Jakub Fiser, Jingwan Lu, Eli Shechtman, and Daniel\nS\u00b4ykora. Stylizing video by example. ACM Transactions on\nGraphics (TOG), 38:1\u201311, 2019. 3\n[13] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM Trans-\nactions on Graphics (TOG), 40:1\u201312, 2021. 1, 2, 3, 4, 6\n[14] Long Mai and Feng Liu. Motion-adjustable neural implicit\nvideo representation. 2022 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10728\u2013\n10737, 2022. 3\n9\n[15] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In European Conference on Computer Vision, 2020.\n3\n[16] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(TOG), 41:1\u201315, 2022. 2, 3, 4, 6\n[17] Thomas M\u00a8uller, Fabrice Rousselle, Jan Nov\u2019ak, and Alexan-\nder Keller. Real-time neural radiance caching for path trac-\ning. ACM Transactions on Graphics (TOG), 40:1\u201316, 2021.\n3, 4, 6\n[18] Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and\nSeon Joo Kim. Fast video object segmentation by reference-\nguided mask propagation.\n2018 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 7376\u2013\n7385, 2018. 3\n[19] Seoung Wug Oh, Joon-Young Lee, N. Xu, and Seon Joo\nKim.\nVideo object segmentation using space-time mem-\nory networks. 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 9225\u20139234, 2019. 6\n[20] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv:1704.00675, 2017. 6\n[21] Alex Rav-Acha, Pushmeet Kohli, Carsten Rother, and An-\ndrew William Fitzgibbon. Unwrap mosaics: a new repre-\nsentation for video editing. ACM SIGGRAPH 2008 papers,\n2008. 3\n[22] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00b4ar, Johannes Totz,\nAndrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan\nWang. Real-time single image and video super-resolution us-\ning an efficient sub-pixel convolutional neural network. 2016\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 1874\u20131883, 2016. 3\n[23] Ivan Skorokhodov, Savva Ignatyev, and Mohamed Elho-\nseiny. Adversarial generation of continuous images. 2021\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10748\u201310759, 2020. 3\n[24] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Bar-\nron, and Henrik Kretzschmar.\nBlock-nerf: Scalable large\nscene neural view synthesis. 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n8238\u20138248, 2022. 3\n[25] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea-\ntures let networks learn high frequency functions in low di-\nmensional domains. ArXiv, abs/2006.10739, 2020. 4\n[26] Lai Meng Tang, Li Hong Lim, and Paul Siebert. Removal of\nvisual disruption caused by rain using cycle-consistent gen-\nerative adversarial networks. In ECCV Workshops, 2018. 2\n[27] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. ArXiv, abs/2003.12039, 2020. 6\n[28] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,\nAndrew Tao, Jan Kautz, and Bryan Catanzaro.\nVideo-to-\nvideo synthesis. In Neural Information Processing Systems,\n2018. 2\n[29] X. Wang, A. Jabri, and Alexei A. Efros. Learning correspon-\ndence from the cycle-consistency of time. 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 2561\u20132571, 2019. 3\n[30] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,\nKalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-\nbased neural radiance fields. 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5428\u20135438, 2022. 3\n[31] Yi Xu, Badour Albahar, and Jia-Bin Huang. Temporally con-\nsistent semantic video editing. In European Conference on\nComputer Vision, 2022. 3\n10\n"
  },
  {
    "title": "Does Visual Pretraining Help End-to-End Reasoning?",
    "link": "https://arxiv.org/pdf/2307.08506.pdf",
    "upvote": "6",
    "text": "Does Visual Pretraining Help End-to-End Reasoning?\nChen Sun\nBrown University, Google\nCalvin Luo\nBrown University\nXingyi Zhou\nGoogle Research\nAnurag Arnab\nGoogle Research\nCordelia Schmid\nGoogle Research\nAbstract\nWe aim to investigate whether end-to-end learning of visual reasoning can be\nachieved with general-purpose neural networks, with the help of visual pretraining.\nA positive result would refute the common belief that explicit visual abstraction\n(e.g. object detection) is essential for compositional generalization on visual rea-\nsoning, and confirm the feasibility of a neural network \u201cgeneralist\u201d to solve visual\nrecognition and reasoning tasks. We propose a simple and general self-supervised\nframework which \u201ccompresses\u201d each video frame into a small set of tokens with\na transformer network, and reconstructs the remaining frames based on the com-\npressed temporal context. To minimize the reconstruction loss, the network must\nlearn a compact representation for each image, as well as capture temporal dynam-\nics and object permanence from temporal context. We perform evaluation on two\nvisual reasoning benchmarks, CATER and ACRE. We observe that pretraining is\nessential to achieve compositional generalization for end-to-end visual reasoning.\nOur proposed framework outperforms traditional supervised pretraining, including\nimage classification and explicit object detection, by large margins.\n1\nIntroduction\nThis paper investigates if an end-to-end trained neural network is able to solve challenging visual\nreasoning tasks [23, 65, 66] that involve inferring causal relationships, discovering object relations,\nand capturing temporal dynamics. A prominent approach [49] for visual reasoning is to construct a\nstructured, interpretable representation from the visual inputs, and then apply symbolic programs [42]\nor neural networks [17] to solve the reasoning task. Despite their appealing properties, such as\nbeing interpretable and enabling the injection of expert knowledge into the learning framework, it\nis practically challenging to determine what kinds of symbolic representations to use and how to\ndetect them reliably from visual data. In fact, the most suitable symbolic representation may differ\nsignificantly across different tasks: the representation for modeling a single human\u2019s kinematics (e.g.\nwith body parts and joints) is unlikely to be the same as that for modeling group social behaviors (e.g.\neach pedestrian can be viewed as its own complete, independent entity). With the success of unified\nneural frameworks for multi-task learning [7], it is desirable to utilize a unified input interface (e.g.\nraw pixels) and to have the neural network learn to dynamically extract suitable representations for\ndifferent tasks. However, how to learn a distributed representation with a deep neural network that\ngeneralizes similarly to learning methods based on symbolic representation [66] for visual reasoning\nremains an open problem.\nThe key hypothesis we make in this paper is that a general-purpose neural network, such as a\nTransformer [55], can be turned into an implicit visual concept learner with self-supervised pre-\ntraining. An implicit visual concept refers to a vector-based representation in an end-to-end neural\nnetwork, which can be \u201cfinetuned\u201d directly on the downstream tasks. Some of the learned implicit\nrepresentations may be discretized into human-interpretable symbols for the purposes of human\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.08506v2  [cs.CV]  16 Dec 2023\nNeural \nNetworks\n\u2026\n\u2026\nInterpretable \nrepresentation\nObject\nSymbolic \nPrograms\nAnswer: (x, y)\nNeural \nNetworks\nDistributed \nrepresentation\nObject\nNeural \nNetworks\nAnswer: (x, y)\nNeural \nNetworks\nDistributed \nrepresentation\nToken\nNeural \nNetworks\nAnswer: (x, y)\nStage 1\nStage 2\nStage 1\nStage 2\nEnd-to-end trained\n\u2026\n\u2026\n\u2026\n\u2026\nSingle stage\n(a) Neuro-Symbolic \nConcept Learning\n(b) Attention over Object \nEmbeddings\n(c) Implicit Visual Concept \nLearning (ours) \nFigure 1: Comparison between a neuro-symbolic approach, a hybrid approach with learned\nobject embeddings [17], and our proposed approach for visual reasoning. The illustration of\neach model family flows upwards, where visual inputs are encoded by neural networks (stage 1), and\nthen processed by symbolic programs or another neural network to generate reasoning predictions\n(stage 2). Compared to (a) and (b), our approach does not require a separate \u201cpreprocessing\u201d stage\nto extract the symbolic representation from visual inputs, and the self-supervised pretrained neural\nnetwork can be end-to-end \u201cfinetuned\u201d to the downstream visual reasoning tasks.\nunderstanding of and feedback to the model. Others may correspond to part of, or a combination of\nhuman-interpretable symbols. As opposed to explicit symbolic representation (e.g. object detection),\nimplicit visual concepts do not require pre-defining a concept vocabulary or constructing concept\nclassifiers, and also do not suffer from the early commitment or loss of information issues which may\nhappen when visual inputs are converted into explicit symbols or frozen descriptors (e.g. via object\ndetection and classification). A comparison between our approach and those that utilize explicit\nsymbols under a pipeline-styled framework is visualized in Figure 1.\nOur proposed representation learning framework, implicit visual concept learner (IV-CL) consists of\ntwo main components: first, a single image is compressed into a small set of tokens with a neural\nnetwork. This is achieved by a vision transformer (ViT) network [19] with multiple \u201cslot\u201d tokens\n(e.g. the [CLS] token in ViT) that attend to the image inputs. Second, the slot tokens are provided as\ncontext information via a temporal transformer network for other images in the same video, where\nthe goal is to perform video reconstruction via the masked autoencoding [27] objective with the\ntemporal context. Despite its simplicity, the reconstruction objective motivates the emergence of two\ndesired properties in the pretrained network: first, to provide context useful for video reconstruction,\nthe image encoder must learn a compact representation of the scene with its slot tokens. Second, to\nutilize the context cues, the temporal transformer must learn to associate objects and their implicit\nrepresentation across time, and also capture the notion of object permanence \u2013 the existence of an\nobject even when it is occluded from the visual observations.\nWe conduct extensive ablation experiments on the Compositional Actions and TEmporal Reasoning\n(CATER) [23] benchmark and the Abstract Causal REasoning (ACRE) [66] benchmark. To better\nunderstand if and how end-to-end pretraining helps visual reasoning, we also consider the supervised\npretraining paradigm, where the slot tokens in the Transformer network are pretrained to \u201cdecode\u201d\nimage-level labels or object locations and categories. Specifically, we adopt the Pix2Seq objective [13],\nwhich formulates object detection as an autoregressive \u201clanguage\u201d modeling task.\nOur experimental results reveal the following observations: first, IV-CL learns powerful implicit\nrepresentations that achieve competitive performance on CATER and ACRE, confirming that visual\npretraining does help end-to-end reasoning. Second, the pretraining objective matters: networks\n2\npretrained on large-scale image classification benchmarks [15, 52] transfer poorly to the visual\nreasoning benchmarks, while object detection learns better representation for reasoning. However,\nboth are outperformed by IV-CL by large margins. Finally, we observe that the network inductive\nbiases, such as the number of slot tokens per image, play an important role: on both datasets, we\nobserve that learning a small number of slot tokens per image (1 for CATER and 4 for ACRE) lead to\nthe best visual reasoning performance. To the best of our knowledge, our proposed framework is the\nfirst to achieve competitive performance on CATER and ACRE without the need to construct explicit\nsymbolic representation from visual inputs.\nIn summary, our paper makes the following two main contributions: First, unlike common assump-\ntions made by neuro-symbolic approaches, we demonstrate that compositional generalization for\nvisual reasoning can be achieved with end-to-end neural networks and self-supervised visual pretrain-\ning. Second, we propose IV-CL, a self-supervised representation learning framework, and validate its\neffectiveness on the challenging CATER and ACRE visual reasoning benchmarks against supervised\nvisual pretraining counterparts.\n2\nRelated Work\nNeural Network Pretraining. Huge progress has been made towards building unified learning\nframeworks for a wide range of tasks, including natural language understanding [16, 48, 8, 40],\nvisual recognition [36, 35, 63, 22], and multimodal perception [33, 50, 38, 24, 3]. Unfortunately,\nmost of the \u201cfoundation models\u201d [7] for visual data focus on perception tasks, such as object\nclassification, detection, or image captioning. Despite improved empirical performance on the visual\nquestion answering task [32, 64], visual reasoning remains challenging when measured on more\ncontrolled benchmarks that require compositional generalization and causal learning [66, 23, 14]. It\nis commonly believed that symbolic or neurosymbolic methods [42, 62, 37, 4], as opposed to general-\npurpose neural networks, are required to achieve generalizable visual reasoning [61, 66, 65]. To our\nknowledge, our proposed framework is the first to demonstrate the effectiveness of a general-purpose\nend-to-end neural network on these visual reasoning benchmarks.\nSelf-supervised Learning from Images and Videos. Self-supervised learning methods aim to\nlearn strong visual representations from unlabelled datasets using pre-text tasks. Pre-text tasks were\ninitially hand-designed to incorporate visual priors [18, 69, 10]. Subsequent works used contrastive\nformulations which encourage different augmented views of the same input to map to the same feature\nrepresentation, whilst preventing the model from collapsing to trivial solutions [45, 12, 28, 26, 2].\nOne challenge of the contrastive formulation is the construction of positive and negative views, which\nhas been shown to critically impact the learned representation [12, 59, 51]. Whereas contrastively\nlearned representations may not easily transfer across domains [46], our pretraining successfully\ngeneralizes to visually different datasets, such as from ACRE to RAVEN.\nOur work is most related to masked self-supervised approaches. Early works in this area used stacked\nautoencoders [56] or inpainting tasks [47] with convolutional networks. These approaches have seen\na resurgence recently, inspired by BERT [16] and vision transformers [19]. BEiT [6] encodes masked\npatches with discrete variational autoencoders and predicts these tokens. Masked Autoencoders\n(MAE) [27], on the other hand, simply regress to the pixel values of these tokens. MAE has been\nextended to regress features [57] and to learn video representations [53, 20]. Our training objective is\ndifferent, as it is predictive coding based on compressed video observations. We confirm empirically\nthat the proposed method outperforms MAE and its video extension by large margins.\nObject-centric Representation for Reasoning. Most of the existing neuro-symbolic [42, 62] and\nneural network [17] based visual reasoning frameworks require a \u201cpreprocessing\u201d stage of symbolic\nrepresentation construction, which often involves detecting and classifying objects and their attributes\nfrom image or video inputs. Our proposed framework aims to investigate the effectiveness of\nsingle-stage, end-to-end neural networks for visual reasoning, which is often more desirable than\nthe two-stage frameworks for scenarios that require transfer learning or multi-task learning. In order\nto obtain the object-centric, or symbolic representation in the preprocessing stage, one can rely on\na supervised object detector [42, 54], such as Mask R-CNN [29]. An alternative approach is to\nemploy self-supervised objectives and learn low-level features that are correlated with objects, such\nas textures [21, 30, 44], or objects themselves [9, 41, 11]. In practice, supervised or self-supervised\napproaches for object detection and object-centric representation learning may suffer from the lack\n3\nViT-B \nEncoder\nViT-B \nEncoder\nViT-B \nEncoder\nViT-B \nEncoder\nViT-B \nEncoder\nTemporal Transformer\nImage \nDecoder\nImage \nDecoder\nImage \nDecoder\ncontext frames\nspatially masked frames\n\u2026\n\u2026\nSlot tokens\nUn-masked pixel tokens\nMasked pixel tokens\n\u2026\n\u2026\nFigure 2: IV-CL self-supervised pretraining. We consider the video reconstruction objective via\nmasked autoencoding: A ViT-B image encoder is tasked to (1) extract visual representations (orange)\nfor the unmasked patches per image and (2) compress an image into a small set of slot tokens (blue).\nA temporal transformer then propagates the information from the slot representations and patch-level\nrepresentations from neighboring frames, which are essential for successful reconstruction.\nof supervised annotations, or from noisy object detection results. For example, it was previously\nobserved that object-centric representations are beneficial for transfer learning to temporal event\nclassification only when ground truth object detections are used for training and evaluation [68].\n3\nMethod\nWe now introduce the proposed implicit visual concept learning (IV-CL) framework. We follow the\npretraining and transfer learning paradigm: during pretraining (Figure 2), we task a shared image\nencoder to output patch-level visual embeddings along with a small set of slot tokens that compress\nthe image\u2019s information. The pretraining objective is inspired by masked autoencoding (MAE) for\nunlabeled video frames, where the aim is to reconstruct a subset of \u201cmasked\u201d image patches given\nthe \u201cunmasked\u201d image patches as context. Compared to the standard MAE for images [27], the\nimage decoder has access to two additional types of context information: (1) The encoded patch\nembedding from the unmasked image patches of the neighboring frames; (2) The encoded slot tokens\nfrom a subset of context frames. The context information is encoded and propagated by a temporal\ntransformer network. To successfully reconstruct a masked frame, the image encoder must learn a\ncompact representation of the full image via the slot tokens, and the temporal transformer has to learn\nto capture object permanence and temporal dynamics.\nAfter pretraining, the image decoder is discarded, and only the image encoder and temporal trans-\nformer are kept for downstream visual reasoning tasks. The inputs to the temporal transformer are\nthe slot tokens encoded from individual, unmasked video frames. We use the full finetuning strategy\nwhere the weights of both the newly added task decoder (e.g. a linear classifier), and the pretrained\nimage and temporal transformers are updated during transfer learning.\nImage Encoder: We adopt the Vision Transformer (ViT) backbone to encode each image inde-\npendently: an input image is broken into non-overlapping patches of 16\u00d716 pixels, which are then\nlinearly projected into patch embeddings as inputs to the transformer encoder. Spatial information is\npreserved by sinusoidal positional encodings. We use the standard ViT-Base configuration which has\n12 Transformer encoder layers. Each layer has hidden size of 768, MLP projection size of 3072, and\n12 attention heads. During pretraining, a subset of video frames are spatially masked randomly given\na masking ratio, only the unmasked image patches are fed into the ViT-B encoder. For context frames\nand during transfer learning, all image patches are provided as inputs to the image encoder.\n4\nSlot Tokens: In the seminal work by Locatello et al. [41], slot tokens are defined as soft cluster\ncentroids that group image pixels, where the goal is unsupervised object detection. Each slot token\nrepeatedly attends to the raw image inputs and is iteratively refined with a GRU network. We borrow\ntheir terminology, and use slots to denote the representational bottleneck in which we hope to encode\nimplicit visual concepts, such as object-centric information. We generalize their slot update rules\nby: (1) iteratively updating the visual representation with layers of the Transformer encoder (ViT);\n(2) replacing cross-attention with multi-headed self-attention; (3) using MLP layers with untied\nweights to update the intermediate slot representation as opposed to a shared GRU network. These\ntwo modifications allow us to implement \u201cslot attention\u201d directly with a Transformer encoder, simply\nby prepending slot tokens as additional inputs to the encoder (similar to [CLS] tokens). The initial\nslot embeddings at the input of the visual encoder are implemented as a learnable embedding lookup\ntable. To compare the effectiveness of different methods to aggregate \u201cslot\u201d information, we also\nexplore single-headed soft attention and Gumbel-max attention as used by [60].\nTemporal Transformer: To propagate temporal information across frames, we use another trans-\nformer encoder (with fewer layers than the ViT-B image encoder) which takes the tokens encoded by\nthe image encoder as its inputs. During pretraining, the slot tokens from context frames, along with\nthe unmasked patch tokens from the query frames are concatenated together and fed into the temporal\ntransformer. For each query image, the temporal transformer outputs its corresponding unmasked\npatch tokens contextualized from both the unmasked patches from neighboring query frames and the\nslot tokens from context frames. The contextualized patches are then fed into the image decoder to\ncompute the reconstruction loss. To preserve temporal position information, we use learned positional\nembeddings (implemented with an embedding lookup table). When finetuned on a reasoning task,\nthe temporal transformer takes the slot tokens encoded by the image encoder as its inputs. Putting the\nimage encoder and the temporal transformer together, the overall video encoder used for finetuning\ncan be viewed as a factorized space-time encoder proposed by [5]. It is more parameter-efficient than\nthe vanilla video transformer used by [53].\nImage Decoder for Pre-training: We use the same image decoder as in [27]. The query images\nare decoded independently given the contextualized unmasked patch tokens. The image decoder is\nimplemented with another transformer, where masked patch tokens are appended to the contextualized\nunmasked patch tokens as inputs to the image decoder. Sinusoidal positional encodings are used to\nindicate the spatial locations of individual patch tokens. We use the same number of layers, hidden\nsize, and other hyperparameters as recommended by [27]. During pre-training, we use mean squared\nerror to measure the distance between the original query image patches and the reconstructed patches.\nTransfer Learning: As the goal of pre-training is to learn the slot tokens which we hope to compress\nan input image into a compact set of implicit visual concept tokens, we only ask the image encoder to\ngenerate the slot tokens during finetuning, which are fed to the temporal transformer as its inputs.\nWe then average pool the output tokens of the temporal transformer and add a task-specific decoder\nto make predictions. Both benchmarks used in our experiments can be formulated as multi-class\nclassification: for CATER, the goal is to predict the final location of the golden snitch, where the\nlocation is quantized into one of the 6\u00d76 positions; in ACRE, the goal is to predict whether the\nplatform is activated, unactivated, or undetermined given a query scenario. We use linear classifiers\nas the task-specific decoders with standard softmax cross-entropy for transfer learning.\nSupervised Pretraining Baselines: To better understand if visual pretraining helps end-to-end\nreasoning, we consider two types of supervised pretraining baselines. The first is the \u201cclassical\u201d\nimage classification pretraining which often exhibits scaling laws [52] when transferred to other\nvisual recognition benchmarks. The second is the object detection task, which intuitively may also\nencourage the emergence of object-centric representations (per task requirement) inside the neural\nnetwork. Both pretraining objectives can be directly applied on the same Transformer architecture as\nutilized in IV-CL, with different designs on the task specific decoders (which are discarded for visual\nreasoning finetuning). For image classification, we directly treat the slot token as a [CLS] token\nand add a linear classifier on top of it. For object detection, to make minimal modification to our\nframework, we follow the design proposed by Pix2Seq [13], which parameterizes the bounding box\nannotations as discrete tokens, and formulates the training objective as an autoregressive sequence\ncompletion task. The inputs to the autoregressive decoder are the encoded slot tokens. We adopt the\nsame sequence construction and augmentation strategies as in Pix2Seq.\n5\n4\nExperiments\n4.1\nExperimental Setup\nBenchmarks: In the classic \u201cshell game\", a ball is placed under a cup and shuffled with other empty\ncups on a flat surface; then, the objective is to determine which cup contains the ball. Inspired by\nthis, CATER is a dataset composed of videos of moving and interacting CLEVR [34] objects. A\nspecial golden ball, called the \u201csnitch\", is present in each video, and the associated reasoning task\nis to determine the snitch\u2019s position at the final frame. Solving this task is complicated by the fact\nthat larger objects can visually occlude smaller ones, and certain objects can be picked up and placed\ndown to explicitly cover other objects; when an object is covered, it changes position in consistence\nwith the larger object that covers it. In order to solve the task, a model must learn to reason not only\nabout objects and movement, but also about object permanence, long-term occlusions, and recursive\ncovering relationships. Each video has 300 frames, and we use the static camera split for evaluation.\nThe ACRE dataset tests a model\u2019s ability to understand and discover causal relationships. The\nconstruction of the dataset is motivated by the Blicket experiment in developmental psychology [25],\nwhere there is a platform as well as many distinct objects, some of which contain the \u201cBlicketness\"\nproperty. When at least one object with the \u201cBlicketness\" property is placed on the platform, music\nwill be played; otherwise, the platform will remain silent. In ACRE, the platform is represented\nby a large pink block that either glows or remains dim depending on the combination of CLEVR\nobjects placed on it. Given six evidence frames of objects placed on the platform, the objective of\nthe reasoning task is to determine the effect a query frame, containing a potentially novel object\ncombination, would have on the platform. Possible answers including activating it, keeping in\ninactive, or indeterminable.\nPretraining data: We use the unlabeled videos from the training and validation splits of the CATER\ndataset for pretraining. Both the static and moving camera splits are used, which contains 9,304\nvideos in total. In our experiments, we observe that ACRE requires higher resolution inputs during\npretraining and finetuning. Our default preprocessing setup is to randomly sample 32 frames of size\n64\u00d764 for pretraining the checkpoints that are transferred to CATER, and 16 frames of size 224\u00d7224\nfor pretraining the checkpoints that are transferred to ACRE. The randomly sampled frames are sorted\nto preserve the arrow of time information. No additional data augmentations are performed.\nTransfer learning: For CATER, we evaluate on the static split which has 3,065 training, 768 valida-\ntion, and 1645 test examples. We select the hyperparameters based on the validation performance,\nthen use both training and validation data to train the model to be evaluated on the test split. By\ndefault, we use 100 randomly sampled frames of size 64\u00d764 during training, and 100 uniformly\nsampled frames of stride 3 during evaluation. For ACRE, we explore all three splits, all of which\ncontain 24,000 training, 8,000 validation, and 8,000 test examples. We use the validation set to select\nhyperparameters and use both training and validation to obtain the models evaluated on the test split.\nDefault hyperparameters: We use the Adam optimizer for pretraining with a learning rate of 10\u22123,\nand the AdamW optimizer for transfer learning with a learning rate of 5 \u00d7 10\u22125. The pretraining\ncheckpoints are trained from scratch for 1,000 epochs using a batch size of 256. For transfer learning,\nwe finetune the pretrained checkpoints for 500 epochs using a batch size of 512. All experiments are\nperformed on TPU with 32 cores. Below we study the impact of several key model hyperparameters.\n4.2\nIV-CL vs. Supervised Pretraining\nWe first compare our proposed IV-CL to traditional supervised pretraining on both detection and\nclassification tasks. For classification, we consider the same ViT-B visual encoder trained on\nImageNet-21K [15] and JFT [52]. For object detection, we consider an in-domain object detection\nbenchmark dataset called LA-CATER [49]. LA-CATER matches the visual appearance of CATER; it\nwas created to study the benefit of modeling object permanence and provides frame-level bounding\nbox annotations for all visible and occluded objects. We validated the correctness of our object\ndetector on the COCO benchmark, which achieves comparable performance to the original Pix2Seq\nimplementation. On the LA-CATER validation set, we observe 82.4% average precision (AP) at an\nIOU threshold of 50%. Whereas one might expect almost perfect performance on such a synthetic\nenvironment, this can be explained by the inherent properties of the dataset; frame-level object\ndetection on LA-CATER also evaluates the detection of occluded and invisible objects, which is\n6\nindeterminable when given only single, static images as inputs. We also consider a classification\npretraining baseline to count the number of unique objects in LA-CATER frames.\nTable 1: Self-supervised visual pretraining vs. supervised pretraining. We compare our proposed\npretraining with traditional supervised classification or detection pretraining.\nObjective\nPretrain data\nCATER\nACRE (comp)\nRandom Init.\n-\n3.34%\n38.78%\nDetection\nLA-CATER\n56.64%\n67.27%\nClassification\nLA-CATER\n41.48%\n64.78%\nClassification\nImageNet-21k\n55.58%\n60.73%\nClassification\nJFT\n54.07%\n48.32%\nIV-CL\nCATER\n70.14 (\u00b10.59)%\n93.27 (\u00b10.22)%\nWe note three remarkable trends when inspecting the results in Table 1. First, we observe that none\nof the models pretrained with supervision outperforms their self-supervised counterpart. Instead,\ntheir performance on both CATER and ACRE fall behind IV-CL by large margins. Second, when\ncomparing the detection and classification objectives, we observe that the detection pretraining\noutperforms classification pretraining significantly. This can be potentially explained by the domain\ngap between natural image datasets and CLEVR-style datasets, or that object detection objective\nencourages the learning of object-centric representations in the slot tokens. To better understand this,\nwe perform addition ablations by replacing the object detection dataset with COCO [39], which is a\nnatural image dataset. We observe similar transfer learning performance as LA-CATER pretraining.\nAdditionally, we perform a probing experiment where we ask the object detection decoder to make\npredictions with a single randomly sampled slot token. We empirically observe that each token\nappears to focus on one or a small subsets of the objects in the scene, and different tokens are\ncomplementary to each other. Both observations indicate that the stronger performance of object\ndetection pretraining is likely due to the \u201cobject-centric\u201d objective itself. Finally, we observe a\ncounterexample of the \u201cscaling law\u201d: larger scale classification pretraining (JFT) leads to significantly\nworse performance than smaller scale pretraining (ImageNet-21k).\n4.3\nVisualizations of the Learned Slots\nTo help understand what visual concepts are implicitly captured by IV-CL, we visualize the attention\nheatmaps from each learned slot token back to the image pixels. This is implemented with the attention\nrollout technique [1]. Figure 3 shows examples of the attention heatmaps after (a) self-supervised\npretraining on CATER, and after (b) finetuning for visual reasoning on ACRE.\nFigure 3: Visualization of 4 slots of an IV-CL model after pretraining on CATER (left) and\nfinetuning on ACRE (right). Each heatmap is generated by attention rollout [1] to the input pixels.\nA brighter color indicates higher attention weight.\nWe observe two general patterns by inspecting the pretrained slot heatmaps: first, a subset of the\nheatmaps exhibit object-centric behavior, as in Figure 3(a). Each slot tends to focus on an individual\nobject, and most of the objects in an image are covered by combining the heatmaps from all four slots.\n7\nTable 2: CATER Pretraining with different mask ratios, context sizes, and frame lengths.\n(a) Mask ratio\nRatio\nAcc.\n37.5%\n70.14%\n12.5%\n66.35%\n50%\n66.57%\n87.5%\n61.94%\n(b) Context size\nSize\nAcc.\n8\n70.14%\n0\n65.35%\n4\n67.47%\n16\n64.34%\n(c) Frame length\nLength\nAcc.\n32\n70.14%\n8\n62.28%\n16\n66.63%\n64\n68.25%\nTable 3: CATER Pretraining with different number of slots, and pooling strategies\n(a) Number of slots\n# slots\nAcc.\n1\n70.14%\n2\n66.52%\n8\n64.45%\n(b) Where to pool\nLayer\nAcc.\n11\n70.14%\n5\n55.80%\n9\n68.86%\n(c) How to pool\nMethod\nAcc.\nSlice\n70.14%\nSoft\n64.23%\nHard\n65.90%\nHowever, we also observe that sometimes the slots are not completely disentangled with respect to\nindividual objects, which indicates that the implicit representations obtained after IV-CL pretraining\ndo not learn perfectly disentangled visual concepts, and further finetuning is necessary to achieve\ncompositional generalization on visual reasoning. We then inspect the heatmaps after finetuning for\nvisual reasoning on ACRE in Figure 3(b). We observe some slots model relationships among objects\nand the platform, and some focus on individual objects. Intuitively, both types of information are\nneeded to solve the ACRE benchmark. Finally, we also visualized the attention of a ImageNet-21k\npretrained model after finetuning on ACRE. We observe that the heatmaps often \u201ccollapse\u201d on a\nsmall subset of the same objects, which is aligned with its lower reasoning performance.\n4.4\nAblation Study\nNext, we ablate our key design choices. We present our ablation study on CATER in Table 2.\nMasking ratio: Contrary to the large masking ratio (75%) employed in vanilla MAE [27], we found\nthat the optimal masking ratio was 37.5% on downstream CATER accuracy. This is perhaps due to\nthe fact that CATER is designed to test \u201ccompositional generalization\u201d, and so the spatial context\nprovides less information than in natural images and video.\nNumber of Total Frames and Context Frames: We also study the impact of the number of frames\nIV-CL is pretrained on, and find the best performance on 32 frames. Fixing the total number of\npretraining frames, we then ablate over the number of context frames, which are the frames from\nwhich slot representations are generated. When no context frame is used, we essentially utilize only\npatch-level representations to perform reconstruction with the temporal transformer (simulating a per-\nframe MAE followed by a temporal transformer). We find that the best performance is achieved with\n8 context frames, which balances the number of slot representations with patch-level representations.\nNumber of Slot Tokens: Another useful ablation is on the impact of the number of slots used for\nCATER and ACRE. In CATER, we find that only 1 slot token per frame is enough to solve the\nreasoning task. We believe that this may be due to how the reasoning objective of CATER is designed:\nto successfully perform snitch localization, the model need only maintain an accurate prediction\nof where the snitch actually or potentially is, and can ignore more detailed representation of other\nobjects in the scene. Under the hypothesis that the slot tokens represent symbols, perhaps the singular\nslot token is enough to contain the snitch location. On the other hand, when ablating over the number\nof tokens for the ACRE task (see Appendix), we find that a higher number of tokens is beneficial\nfor reasoning performance. This can potentially be explained by the need to model multiple objects\nacross evidence frames in order to solve the final query; under our belief that slot tokens are encoding\nsymbols, multiple may be needed in order to achieve the best final performance.\n8\nTable 4: Results on CATER (static). IV-CL performs the best among non-object-centric methods,\nand performs competitively with methods with object-supervision.\nMethod\nObject-centric\nObject superv.\nTop-1 Acc. (%)\nTop-5 Acc. (%)\nOPNet [49]\n\u2713\n\u2713\n74.8\n-\nHopper [70]\n\u2713\n\u2713\n73.2\n93.8\nALOE [17]\n\u2713\n\u2717\n70.6\n93.0\nRandom Init.\n\u2717\n\u2717\n3.3\n18.0\nMAE (Image) [27]\n\u2717\n\u2717\n27.1\n47.8\nMAE (Video)\n\u2717\n\u2717\n63.7\n82.8\nIV-CL (ours)\n\u2717\n\u2717\n70.1 \u00b1 0.6\n88.3 \u00b1 0.2\nTable 5: Results on ACRE compositionality, systematicity, and I.I.D. splits. IV-CL performs the\nbest among all methods on the compositionality split, and performs competitively on other splits.\nMethod\nObject-centric\nObject superv.\ncomp (%)\nsys (%)\niid (%)\nCNN-BERT [66]\n\u2717\n\u2717\n43.79%\n39.93%\n43.56%\nNS-RW [66]\n\u2713\n\u2713\n50.69%\n42.18%\n46.61%\nNS-OPT [66]\n\u2713\n\u2713\n69.04\n67.44\n66.29\nALOE [17]\n\u2713\n\u2717\n91.76\n93.90\n-\nRandom Init.\n\u2717\n\u2717\n38.78\n38.57\n38.67\nMAE (Image) [27]\n\u2717\n\u2717\n80.27\n76.32\n80.81\nMAE (Video)\n\u2717\n\u2717\n78.85\n71.69\n77.14\nIV-CL (ours)\n\u2717\n\u2717\n93.27 \u00b1 0.22\n92.64 \u00b1 0.30\n92.98 \u00b1 0.80\nSlot Pooling Layer and Method: We ablate over which layer to pool over to generate the slot tokens.\nThe patch tokens are discarded after the pooling layer, and only the slot tokens are further processed\nby the additional Transformer encoder layers. As expected, it is desirable to use all image encoder\nlayers to process both slot and patch tokens. Additionally, we also study the impact of slot pooling\nmethod, and observe that adding additional single-headed soft attention and Gumbel-max attention\nare outperformed by simply using the slot tokens directly.\n4.5\nComparison with State-of-the-Art\nWe compare our IV-CL framework with previously published results. As most of the prior work\nrequire explicit object detection and are not end-to-end trained, we reimplement an image-based\nMAE [27] and a video-based MAE [53] baseline and analyze the impact of inductive biases (using\nslot tokens or not) as well as pretraining objectives (predictive coding given compressed context, or\nautoencoding the original inputs) on the reasoning performance. Our reimplementation of image\nand video MAEs achieve very similar performances on their original benchmarks. However, for\nvideo-based MAE, we observe that the \u201cun-factorized\u201d backbone leads to training collapse on CATER.\nWe hence adjust the backbone to be \u201cfactorized\u201d as illustrated in Figure 2. We follow the same\npretraining and hyperparameter selection procedures as for IV-CL.\nTable 4 compares the result of IV-CL against other state-of-the-art models on CATER. We also\ncompare IV-CL on ACRE against other existing models in Table 5. We cite the comparable results\nreported by the original authors when available. IV-CL achieves the best performance among the\napproaches that do not depend on explicit object-centric representation, and overall state-of-the-art\nperformance on ACRE.\n5\nConclusion and Future Work\nIn this work we demonstrate that competitive visual reasoning can be achieved in a general-purpose\nend-to-end neural network, with the help of self-supervised visual pretraining. Our proposed implicit\nvisual concept learner (IV-CL) framework leverages a Transformer encoder to \u201ccompress\u201d visual in-\nputs into slot tokens, and is trained with a self-supervised video reconstruction objective. Quantitative\n9\nand qualitative evaluations confirm the effectiveness of IV-CL on CATER and ACRE visual reasoning\nbenchmarks, when compared to supervised visual pretraining and neuro-symbolic approaches. A\nlimitation of our work is that evaluations are performed purely on synthetic reasoning tasks. We\nbelieve extending evaluation to large-scale natural video reasoning benchmarks, building a joint\nmodel for visual recognition and reasoning, and exploring how to incorporate explicit object-centric\nknowledge when such knowledge is available are interesting future directions to pursue.\nAcknowledgements: C.S. and C.L. are in part supported by research grants from Honda Research\nInstitute, Meta AI, and Samsung Advanced Institute of Technology.\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. arXiv preprint\narXiv:2005.00928, 2020.\n[2] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and\nBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,\naudio and text. Advances in Neural Information Processing Systems, 34, 2021.\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n[4] Jacob Andreas.\nMeasuring compositionality in representation learning.\narXiv preprint\narXiv:1902.07181, 2019.\n[5] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u02c7ci\u00b4c, and Cordelia\nSchmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 6836\u20136846, 2021.\n[6] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. ICLR,\n2022.\n[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Advances in neural information processing systems, volume 33, pages\n1877\u20131901, 2020.\n[9] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt\nBotvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representa-\ntion. arXiv preprint arXiv:1901.11390, 2019.\n[10] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering\nfor unsupervised learning of visual features. In Proceedings of the European conference on\ncomputer vision (ECCV), pages 132\u2013149, 2018.\n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 9650\u20139660, 2021.\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In International conference on machine\nlearning, pages 1597\u20131607. PMLR, 2020.\n[13] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language\nmodeling framework for object detection. arXiv preprint arXiv:2109.10852, 2021.\n[14] Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua B Tenenbaum,\nand Chuang Gan. Comphy: Compositional physical reasoning of objects and events from\nvideos. arXiv preprint arXiv:2205.01089, 2022.\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n10\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[17] David Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matt Botvinick. Attention over\nlearned object embeddings enables complex visual reasoning. Advances in Neural Information\nProcessing Systems, 34, 2021.\n[18] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning\nby context prediction. In Proceedings of the IEEE international conference on computer vision,\npages 1422\u20131430, 2015.\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[20] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as\nspatiotemporal learners. arXiv preprint arXiv:2205.09113, 2022.\n[21] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann,\nand Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias\nimproves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\n[22] Golnaz Ghiasi, Barret Zoph, Ekin D Cubuk, Quoc V Le, and Tsung-Yi Lin. Multi-task self-\ntraining for learning general representations. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 8856\u20138865, 2021.\n[23] Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and\ntemporal reasoning. arXiv preprint arXiv:1910.04744, 2019.\n[24] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan\nMisra. Omnivore: A single model for many visual modalities. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 16102\u201316112, 2022.\n[25] Alison Gopnik and David M Sobel. Detecting blickets: How young children use information\nabout novel causal powers in categorization and induction. Child development, 71(5):1205\u20131222,\n2000.\n[26] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems, 33:21271\u201321284, 2020.\n[27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\n[28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 9729\u20139738, 2020.\n[29] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of\nthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.\n[30] Katherine Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture\nbias in convolutional neural networks. Advances in Neural Information Processing Systems,\n33:19000\u201319015, 2020.\n[31] Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar, Gal Chechik, Anna\nRohrbach, Trevor Darrell, and Amir Globerson. Object-region video transformers. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n3148\u20133159, 2022.\n[32] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 6700\u20136709, 2019.\n[33] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao\nCarreira. Perceiver: General perception with iterative attention. In International Conference on\nMachine Learning, pages 4651\u20134664. PMLR, 2021.\n11\n[34] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick,\nand Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary\nvisual reasoning. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2901\u20132910, 2017.\n[35] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7482\u20137491, 2018.\n[36] Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-,\nand high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 6129\u20136138, 2017.\n[37] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks. In International conference on machine\nlearning, pages 2873\u20132882. PMLR, 2018.\n[38] Valerii Likhosherstov, Anurag Arnab, Krzysztof Choromanski, Mario Lucic, Yi Tay, Adrian\nWeller, and Mostafa Dehghani. Polyvit: Co-training vision transformers on images, videos and\naudio. arXiv preprint arXiv:2111.12993, 2021.\n[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[40] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[41] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg\nHeigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with\nslot attention. Advances in Neural Information Processing Systems, 33:11525\u201311538, 2020.\n[42] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-\nsymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision.\narXiv preprint arXiv:1904.12584, 2019.\n[43] Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, and Trevor Darrell.\nSomething-else: Compositional action recognition with spatial-temporal interaction networks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 1049\u20131059, 2020.\n[44] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017.\nhttps://distill.pub/2017/feature-visualization.\n[45] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[46] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do\nself-supervised vision transformers learn? arXiv preprint arXiv:2305.00729, 2023.\n[47] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2536\u20132544, 2016.\n[48] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 2019.\n[49] Aviv Shamsian, Ofri Kleinfeld, Amir Globerson, and Gal Chechik. Learning object permanence\nfrom video. In European Conference on Computer Vision, pages 35\u201350. Springer, 2020.\n[50] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\njoint model for video and language representation learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 7464\u20137473, 2019.\n[51] Chen Sun, Arsha Nagrani, Yonglong Tian, and Cordelia Schmid. Composable augmentation\nencoding for video representation learning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 8834\u20138844, 2021.\n12\n[52] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\neffectiveness of data in deep learning era. In Proceedings of the IEEE international conference\non computer vision, pages 843\u2013852, 2017.\n[53] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are\ndata-efficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602,\n2022.\n[54] Manuel Traub, Sebastian Otte, Tobias Menge, Matthias Karlbauer, Jannik Thuemmel, and\nMartin V Butz. Learning what and where: Disentangling location and identity tracking without\nsupervision. In The Eleventh International Conference on Learning Representations, 2023.\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[56] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol,\nand L\u00e9on Bottou. Stacked denoising autoencoders: Learning useful representations in a deep\nnetwork with a local denoising criterion. Journal of machine learning research, 11(12), 2010.\n[57] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichten-\nhofer. Masked feature prediction for self-supervised visual pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14668\u201314678,\n2022.\n[58] Yuhuai Wu, Honghua Dong, Roger Grosse, and Jimmy Ba. The scattering compositional\nlearner: Discovering objects, attributes, relationships in analogical reasoning. arXiv preprint\narXiv:2007.04212, 2020.\n[59] Tete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive\nin contrastive learning. arXiv preprint arXiv:2008.05659, 2020.\n[60] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong\nWang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18134\u201318144,\n2022.\n[61] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B\nTenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint\narXiv:1910.01442, 2019.\n[62] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum.\nNeural-symbolic vqa: Disentangling reasoning from vision and language understanding. Ad-\nvances in neural information processing systems, 31, 2018.\n[63] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 3712\u20133722, 2018.\n[64] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:\nVisual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6720\u20136731, 2019.\n[65] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for\nrelational and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5317\u20135327, 2019.\n[66] Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, and Yixin Zhu. Acre: Abstract\ncausal reasoning beyond covariation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10643\u201310653, 2021.\n[67] Chi Zhang, Baoxiong Jia, Feng Gao, Yixin Zhu, Hongjing Lu, and Song-Chun Zhu. Learning\nperceptual inference by contrasting. Advances in neural information processing systems, 32,\n2019.\n[68] Chuhan Zhang, Ankush Gupta, and Andrew Zisserman. Is an object-centric video representation\nbeneficial for transfer? arXiv preprint arXiv:2207.10075, 2022.\n[69] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European\nconference on computer vision, pages 649\u2013666. Springer, 2016.\n13\n[70] Honglu Zhou, Asim Kadav, Farley Lai, Alexandru Niculescu-Mizil, Martin Renqiang Min,\nMubbasir Kapadia, and Hans Peter Graf. Hopper: Multi-hop transformer for spatiotemporal\nreasoning. arXiv preprint arXiv:2103.10574, 2021.\n14\n6\nAdditional Experimental Details\nTransfer Learning Framework. In Figure 2, we visualized our proposed self-supervised pretraining\nframework. Once the representation network has been pretrained, we discard the image decoder and\nonly use the ViT-B image encoder, along with the pretrained temporal transformer. An illustration of\nthe transfer learning process is shown in Figure A1.\nFigure A1: An illustration of the transfer learning process. Both the ViT-B image encoder and the\ntemporal transformer are transferred to downstream visual reasoning tasks to encode video inputs.\nUnlike pretraining, only the slot tokens are provided as inputs to the temporal transformer.\nFigure A2: Illustration of the CATER (top) and ACRE (bottom) benchmarks.\nIllustration of the Benchmarks. In Figure A2, we provide the illustrations of the CATER benchmark\nand the ACRE benchmark, respectively. As described in the main submission, the CATER benchmark\nfeatures a special golden ball, called the \u201csnitch\u201d, and the associated reasoning task is to determine\nthe snitch\u2019s position at the final frame despite occlusions. Object locations in the CATER dataset are\ndenoted by positions on an invisible 6-by-6 grid; therefore, in essence, the CATER task boils down to\na 36-way classification problem. The CATER dataset features a split where the camera is statically\nfixed to a particular angle and position throughout the videos, as well as a moving camera split where\nthe viewing angle is able to change over time. We use the static split for evaluation. Each video has\n300 frames.\nThe ACRE benchmark is inspired by the research on developmental psychology: Given a few context\ndemonstrations of different object combinations, as well as the resulting effect, young children\nhave been shown to successfully infer which objects contain the \u201cBlicketness\u201d property, and which\ncombinations would cause the platform to play music. ACRE explicitly evaluates four kinds of\nreasoning capabilities: direct, indirect, screened-off, and backward-blocking. Having the query\nframe be a combination that was explicitly provided in the context frames tests a model\u2019s direct\n15\nreasoning ability. Indirect reasoning can be tested by a novel query combination, the effect of which\nrequires understanding multiple context frames to deduce. In screen-off questions, the model must\nunderstand that as long as a singular Blicket object is placed on the platform, the entire combination\nwould cause it to light up. In backward-blocking questions, the model must recognize when the\neffect of a query combination cannot be determined from the provided context frames. Furthermore,\nACRE features three different dataset splits to test model generalization: Independent and Identically\nDistributed (I.I.D.), compositionality (comp), systematicity (sys). In the compositionality split, the\nshape-material-color combinations of the CLEVR objects in the test set are not seen before in the\ntrain split; therefore, the model must learn to generalize across object attributes. In the systematicity\nsplit, the evidence frames of the train split contain three lit up examples, whereas the evidence frames\nof the test split contain four.\nNumber of the Slot Tokens. In Table A1, we provide ablation experiment on the impact of the\nnumber of slot tokens for the reasoning performance on all splits. Unlike CATER, whose goal is\nto infer the position of a single object, the \u201csnitch\u201d, the ACRE benchmark requires reasoning over\ncombinations of objects, and their relationship with the platform. As a result, we generally observe\nthat more slot tokens are needed to achieve optimal performance. We observe that the performance\nstarts to saturate given four or eight slots.\nTable A1: ACRE # tokens. We show results on compositionality (comp), systematicity (sys), and\nI.I.D. (iid) splits.\n# slots\ncomp\nsys\niid\n1\n91.75%\n90.34%\n90.96%\n2\n90.82%\n88.21%\n88.73%\n4\n93.27%\n92.64%\n92.98%\n8\n95.54%\n86.18%\n88.97%\n64\n90.45%\n80.07%\n90.82%\nFigure A3: Visualizations of the Slot Tokens. The top row corresponds to the attention heatmaps\nfrom the slot tokens after pretraining on CATER, and the bottom row corresponds to the heatmaps\nafter finetuning on ACRE.\n16\nVisualizations of Slot Tokens. Figure A3 provides additional visualizations of the slot token attention\nheatmaps after pretraining on CATER, and finetuning on ACRE, respectively. We follow the same\nattention rollout technique as in Figure 3. For ACRE, we show the example when the platform is\nvisible (context information) on bottom left, and when the platform is invisible (question) on the\nbottom right. We observe a consistent trend that a subset of the heatmaps exhibit object-centric\nbehavior, especially before finetuning on ACRE. After finetuning, we observe that some slots remain\nfocusing on individual objects, while the others attempt to model the relationships among different\nobjects and the platform.\nOur MAE baselines are pretrained with the same hyper parameters (e.g. optimization and mask\nratio) as IV-CL, which we have observed to be optimal based on the validation set performance. The\nimage encoders for all methods are based on ViT-B, hence the total model sizes are comparable.\nTable A2: Transfer learning results on RAVEN. We follow the same pretrained representation and\nfinetuning hyperparameters as for ACRE.\nMethod\nAverage\nCenter\n2\u00d72 Grid\n3\u00d73 Grid\nL-R\nU-D\nO-IC\nO-IG\nLSTM\n13.1\n13.2\n14.1\n13.7\n12.8\n12.4\n12.2\n13.0\nResNet + DRT [65]\n59.6\n58.1\n46.5\n50.4\n65.8\n67.1\n69.1\n60.1\nCoPINet [67]\n91.4\n95.1\n77.5\n78.9\n99.1\n99.7\n98.5\n91.4\nSCL [58]\n91.6\n98.1\n91.0\n82.5\n96.8\n96.5\n96.0\n80.1\nIV-CL (ours)\n92.5\n98.4\n82.6\n78.4\n96.6\n97.2\n99.0\n95.4\nTransfer Learning to RAVEN. We explore generalization to a visually different reasoning bench-\nmark, RAVEN [65]. Inspired by Raven\u2019s Progressive Matrices (RPM), its goal is to evaluate a\nmachine learning model\u2019s structural, relational, and analogical reasoning capabilities. The reasoning\ntask is to determine which of eight candidate geometrical figures naturally follow the patterned\nsequence of eight context figures. We explore all seven reasoning scenarios and perform finetuning\non all training and validation examples (56,000 examples). The pretraining and finetuning hyper-\nparameters exactly match those for ACRE, but the model now takes in 16 images as input (8 for\ncontext, 8 for answers). We report generalization performance on RAVEN in Table A2. We observe\nthat the pretrained representation is generalizable, as IV-CL achieves competitive performance on the\nRAVEN [65] benchmark with the same pretrained model and finetuning hyperparameters as ACRE,\ndespite the different visual appearances across the datasets.\nTable A3: Performance Evaluation on Something-Else. We consider the base and compositional\nsplits. *: Uses groundtruth box annotations during evaluation.\nModel\nSplit\nObject Supervision\nTop-1 Acc. (%)\nTop-5 Acc. (%)\nSTIN+OIE+NL [43]\nBase\n\u2713\n78.1\n94.5\nORViT [31]\u2217\nBase\n\u2713\n87.1\n97.6\nIV-CL (Ours)\nBase\n\u2717\n79.1\n95.7\nSTIN+OIE+NL [43]\nComp\n\u2713\n56.2\n81.3\nORViT [31]\u2217\nComp\n\u2713\n69.7\n90.1\nIV-CL (Ours)\nComp\n\u2717\n59.6\n85.6\nGeneralization to Real Videos. Finally, we attempt to answer the question: Would our proposed\nself-supervised pretraining framework work on real videos? We consider the Something-Else bench-\nmark [43], which consists of short videos capturing the interactions between human hands and\ndifferent objects. This benchmark focuses on relational reasoning, especially on compositional gener-\nalization across different object categories. We consider the base split and the \u201ccompositional\u201d split.\nThe base split contains 112,397 training videos and 12,467 validation videos, across 88 categories.\nThe compositional split contains 54,919 training videos and 57,876 validation videos, across 174\ncategories. Each category corresponds to a fine-grained activity that requires spatiotemporal relation\nreasoning. The compositional split is designed to include disjoint object types for each category\nbetween the training set and the validation set.\n17\nDue to the large domain gap between CATER and Something-Else videos, we choose to perform\npretraining directly on the corresponding training splits of the Something-Else benchmark. We use the\nsame pretraining and finetuning hyper parameters as in ACRE, except that we use 16 frames sampled\nat stride size of 2 during finetuning. During both pretraining and finetuning, we apply the standard\nvideo data augmentation techniques as used by prior work (e.g. [5]). In Table A3, we observe that\nour method generalizes well to real videos, and it achieves competitive performance compared to\nmethods that use annotated boxes during training (STIN+OIE+NL) and evaluation (ORViT).\n18\n"
  },
  {
    "title": "Scale-Aware Modulation Meet Transformer",
    "link": "https://arxiv.org/pdf/2307.08579.pdf",
    "upvote": "4",
    "text": "Scale-Aware Modulation Meet Transformer\nWeifeng Lin1,2, Ziheng Wu2, Jiayu Chen2, Jun Huang 2, Lianwen Jin1*\n1 South China University of Technology, 2 Platform of AI (PAI), Alibaba Group\neelinweifeng@mail.scut.edu.cn\neelwjin@scut.edu.cn\n{ziheng.wzh, yunji.cjy, huangjun.hj}@alibaba-inc.com\nAbstract\nThis paper presents a new vision Transformer, Scale-\nAware Modulation Transformer (SMT), that can handle var-\nious downstream tasks efficiently by combining the convolu-\ntional network and vision Transformer. The proposed Scale-\nAware Modulation (SAM) in the SMT includes two primary\nnovel designs. Firstly, we introduce the Multi-Head Mixed\nConvolution (MHMC) module, which can capture multi-\nscale features and expand the receptive field. Secondly, we\npropose the Scale-Aware Aggregation (SAA) module, which\nis lightweight but effective, enabling information fusion\nacross different heads. By leveraging these two modules,\nconvolutional modulation is further enhanced.\nFurther-\nmore, in contrast to prior works that utilized modulations\nthroughout all stages to build an attention-free network, we\npropose an Evolutionary Hybrid Network (EHN), which can\neffectively simulate the shift from capturing local to global\ndependencies as the network becomes deeper, resulting in\nsuperior performance. Extensive experiments demonstrate\nthat SMT significantly outperforms existing state-of-the-art\nmodels across a wide range of visual tasks. Specifically,\nSMT with 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can\nachieve 82.2% and 84.3% top-1 accuracy on ImageNet-1K,\nrespectively. After pretrained on ImageNet-22K in 2242 res-\nolution, it attains 87.1% and 88.1% top-1 accuracy when\nfinetuned with resolution 2242 and 3842, respectively. For\nobject detection with Mask R-CNN, the SMT base trained\nwith 1\u00d7 and 3\u00d7 schedule outperforms the Swin Trans-\nformer counterpart by 4.2 and 1.3 mAP on COCO, respec-\ntively. For semantic segmentation with UPerNet, the SMT\nbase test at single- and multi-scale surpasses Swin by 2.0\nand 1.1 mIoU respectively on the ADE20K. Our code is\navailable at https://github.com/AFeng-x/SMT.\n1. Introduction\nSince the groundbreaking work on Vision Transform-\ners (ViT) [11], Transformers have gained significant atten-\n*Corresponding authors.\n0\n5\n10\n15\n20\n25\nFLOPs(G)\n70\n72\n74\n76\n78\n80\n82\n84\n86\nImageNet Top-1 Accuracy(%)\nOurs\nShunted\nResnet\nDeiT\nConvNeXt\nPvTv2\nSwin\nFocal\nFocalNet\nMaxViT\n20\n40\n60\n80\n100\n120\nParams(MB)\n70\n72\n74\n76\n78\n80\n82\n84\n86\nImageNet Top-1 Accuracy(%)\nOurs\nShunted\nResnet\nDeiT\nConvNeXt\nPvtv2\nSwin\nFocal\nFocalNet\nMaxViT\nFigure 1: Top-1 accuracy on ImageNet-1K of recent SOTA\nmodels. Our proposed SMT outperforms all the baselines.\ntion from both industry and academia, achieving remark-\nable success in various computer vision tasks, such as im-\nage classification [10], object detection [30, 12], and seman-\ntic segmentation [75, 7]. Unlike convolutional networks,\nwhich only allow for interactions within a local region us-\ning a shared kernel, ViT divides the input image into a\nsequence of patches and updates token features via self-\nattention (SA), enabling global interactions. However, self-\nattention still faces challenges in downstream tasks due to\nthe quadratic complexity in the number of visual tokens,\nparticularly for high-resolution inputs.\nTo address these challenges, several efficient spatial at-\ntention techniques have been proposed. For example, Swin\nTransformer [32] employs window attention to limit the\nnumber of tokens and establish cross-window connections\nvia shifting. PVT [56, 57] and Focal [65] reduce the cost\nof self-attention by combining token merging with spa-\ntial reduction. Shunted [42] effectively models objects at\nmultiple scales simultaneously while performing spatial re-\nduction. Other techniques such as dynamic token selec-\ntion [38, 40, 66] have also proven to be effective improve-\nments.\nRather than directly improving self-attention, several\nworks [9, 27, 37, 26] have investigated hybrid CNN-\nTransformer architectures that combine efficient convolu-\ntional blocks with powerful Transformer blocks. We ob-\nserved that most hybrid networks replace shallow Trans-\narXiv:2307.08579v2  [cs.CV]  26 Jul 2023\nformer blocks with convolution blocks to reduce the high\ncomputational cost of self-attention in the early stages.\nHowever, these simplistic stacking strategies hinder them\nfrom achieving a better balance between accuracy and la-\ntency. Therefore, one of the objectives of this paper is to\npresent a new perspective on the integration of Transformer\nand convolution blocks.\nBased on the research conducted in [11, 4], which per-\nformed a quantitative analysis of different depths of self-\nattention blocks and discovered that shallow blocks tend to\ncapture short-range dependencies while deeper ones cap-\nture long-range dependencies, we propose that substitut-\ning convolution blocks for Transformer blocks in shallow\nnetworks offers a promising strategy for two primary rea-\nsons: (1) self-attention induces significant computational\ncosts in shallow networks due to high-resolution input, and\n(2) convolution blocks, which inherently possess a capacity\nfor local modeling, are more proficient at capturing short-\nrange dependencies than SA blocks in shallow networks.\nHowever, we observed that simply applying the convolu-\ntion directly to the feature map does not lead to the desired\nperformance. Taking inspiration from recent convolutional\nmodulation networks [15, 18, 64], we discovered that con-\nvolutional modulation can aggregate surrounding contexts\nand adaptively self-modulate, giving it a stronger model-\ning capability than using convolution blocks alone. There-\nfore, we proposed a novel convolutional modulation, termed\nScale-Aware Modulation (SAM), which incorporates two\nnew modules: Multi-Head Mixed Convolution (MHMC)\nand Scale-Aware Aggregation (SAA). The MHMC mod-\nule is designed to enhance the receptive field and capture\nmulti-scale features simultaneously. The SAA module is\ndesigned to effectively aggregate features across different\nheads while maintaining a lightweight architecture.\nDe-\nspite these improvements, we find that SAM falls short of\nthe self-attention mechanism in capturing long-range de-\npendencies.\nTo address this, we propose a new hybrid\nModulation-Transformer architecture called the Evolution-\nary Hybrid Network (EHN). Specifically, we incorporate\nSAM blocks in the top two stages and Transformer blocks in\nthe last two stages, while introducing a new stacking strat-\negy in the penultimate stage. This architecture not only sim-\nulates changes in long-range dependencies from shallow to\ndeep layers but also enables each block in each stage to bet-\nter match its computational characteristics, leading to im-\nproved performance on various downstream tasks. Collec-\ntively, we refer to our proposed architecture as Scale-Aware\nModulation Transformer (SMT).\nAs shown in Fig. 1, our SMT significantly outperforms\nother SOTA vision Transformers and convolutional net-\nworks on ImageNet-1K [10]. It is worth noting that our\nSMT achieves top-1 accuracy of 82.2% and 84.3% with\nthe tiny and base model sizes, respectively.\nMoreover,\nour SMT consistently outperforms other SOTA models on\nCOCO [30] and ADE20K [75] for object detection, instance\nsegmentation, and semantic segmentation tasks.\nOverall, the contributions of this paper are as follows.\n\u2022 We introduce the Scale-Aware Modulation (SAM)\nwhich incorporates a potent Multi-Head Mixed Convo-\nlution (MHMC) and an innovative, lightweight Scale-\nAware Aggregation (SAA). The SAM facilitates the\nintegration of multi-scale contexts and enables adap-\ntive modulation of tokens to achieve more precise pre-\ndictions.\n\u2022 We propose a new evolutionary hybrid network that ef-\nfectively models the transition from capturing local to\nglobal dependencies as the network increases in depth,\nleading to improved performance and high efficiency.\n\u2022 We evaluated our proposed Scale-Aware Modulation\nTransformer (SMT) on several widely used bench-\nmarks, including classification, object detection, and\nsegmentation.\nThe experimental results indicated\nthat SMT consistently outperformed the SOTA Vision\nTransformers while requiring fewer parameters and in-\ncurring lower computational costs.\n2. Related Work\n2.1. Vision Transformers\nThe Transformer [54] was initially developed for natural\nlanguage processing tasks and has since been adapted for\ncomputer vision tasks through the introduction of the Vi-\nsion Transformer (ViT) [11]. Further improvements to ViT\nhave been achieved through knowledge distillation or more\nintricate data augmentation, as demonstrated by DeiT [52].\nHowever, Transformers do not consider the quadratic com-\nplexity of high-resolution images or the 2D structure of\nimages, which are challenges in vision tasks. To address\nthese issues and improve the performance of vision Trans-\nformers, various methods have been proposed, including\nmulti-scale architectures [3, 32, 56, 63], lightweight con-\nvolution layers [14, 28, 60], and local self-attention mecha-\nnisms [32, 6, 65, 71].\n2.2. Convolutional Neural Networks\nConvolutional neural networks (CNNs) have been the\nmain force behind the revival of deep neural networks in\ncomputer vision. Since the introduction of AlexNet [25],\nVGGNet [44], and ResNet [17], CNNs have rapidly be-\ncome the standard framework for computer vision tasks.\nThe design principles of CNNs have been advanced by sub-\nsequent models such as Inception [47, 48], ResNeXt [62],\nRes2Net [13] and MixNet [51], which promote the use of\nbuilding blocks with multiple parallel convolutional paths.\nOther works such as MobileNet [20] and ShuffleNet [73]\nH\u00d7W\u00d73\nStem\nSAM\nBlock\n!\n\"\u00d7\n#\n\" \u00d7\ud835\udc36$\n!\n$%\u00d7\n#\n$%\u00d7\ud835\udc36&\n!\n&'\u00d7\n#\n&'\u00d7\ud835\udc36\"\nStage 1\n!\n(\u00d7\n#\n( \u00d7\ud835\udc36'\nPatch Embedding\nSAM\nBlock\nStage 2\nPatch Embedding\nMIX \nBlock\nStage 3\nPatch Embedding\nMSA\nBlock\nStage 4\nSAM\nLN\nFFN\nLN\nMSA\nLN\nFFN\nLN\n(a) Architecture \n(b) Mix Block \nFigure 2: (a) The architecture of the Scale-Aware Modulation Transformer (SMT); (b) Mix Block: a series of SAM blocks\nand MSA blocks that are stacked successively (as presented in Sec. 3.3). SAM and MSA denote the scale-aware modulation\nmodule and multi-head self-attention module, respectively.\nhave focused on the efficiency of CNNs. To further improve\nthe performance of CNNs, attention-based models such as\nSE-Net [21], Non-local Networks [58], and CBAM [59]\nhave been proposed to enhance the modeling of chan-\nnel or spatial attention.\nEfficientNets [49, 50] and Mo-\nbileNetV3 [19] have employed neural architecture search\n(NAS) [77] to develop efficient network architectures. Con-\nvNeXt [33] adopts the hierarchical design of Vision Trans-\nformers to enhance CNN performance while retaining the\nsimplicity and effectiveness of CNNs.\nRecently, several\nstudies [15, 18, 64] have utilized convolutional modula-\ntion as a replacement for self-attention, resulting in im-\nproved performance. Specifically, FocalNet [64] utilizes a\nstack of depth-wise convolutional layers to encode features\nacross short to long ranges and then injects the modulator\ninto the tokens using an element-wise affine transformation.\nConv2Former [18] achieves good recognition performance\nusing a simple 11 \u00d7 11 depth-wise convolution. In con-\ntrast, our scale-aware modulation also employs depth-wise\nconvolution as a basic operation but introduces multi-head\nmixed convolution and scale-aware aggregation.\n2.3. Hybrid CNN-Transformer Networks\nA popular topic in visual recognition is the develop-\nment of hybrid CNN-Transformer architectures. Recently,\nseveral studies [14, 45, 60, 76] have demonstrated the ef-\nfectiveness of combining Transformers and convolutions to\nleverage the strengths of both architectures. CvT [60] first\nintroduced depth-wise and point-wise convolutions before\nself-attention. CMT [14] proposed a hybrid network that\nutilizes Transformers to capture long-range dependencies\nand CNNs to model local features. MobileViT [37], Ed-\ngeNeXt [36], MobileFormer [5], and EfficientFormer [27]\nreintroduced convolutions to Transformers for efficient net-\nwork design and demonstrated exceptional performance in\nimage classification and downstream applications. How-\never, the current hybrid networks lack the ability to model\nrange dependency transitions, making it challenging to im-\nprove their performance. In this paper, we propose an evo-\nlutionary hybrid network that addresses this limitation and\nshowcases its importance.\n3. Method\n3.1. Overall Architecture\nThe overall architecture of our proposed Scale-Aware\nModulation Transformer (SMT) is illustrated in Fig. 2. The\nnetwork comprises four stages, each with downsampling\nrates of {4, 8, 16, 32}. Instead of constructing an attention-\nfree network, we first adopt our proposed Scale-Aware\nModulation (SAM) in the top two stages, followed by a\npenultimate stage where we sequentially stack one SAM\nblock and one Multi-Head Self-Attention (MSA) block to\nmodel the transition from capturing local to global depen-\ndencies. For the last stage, we solely use MSA blocks to\ncapture long-range dependencies effectively. For the Feed-\nForward Network (FFN) in each block, we adopt the detail-\nspecific feedforward layers as used in Shunted [42].\n3.2. Scale-Aware Modulation\nMulti-Head Mixed Convolution\nWe propose the Multi-\nHead Mixed Convolution (MHMC), which introduces mul-\ntiple convolutions with different kernel sizes, enabling it to\ncapture various spatial features across multiple scales. Fur-\nthermore, MHMC can expand the receptive field using a\nlarge convolutional kernel, enhancing its ability to model\nlong-range dependencies. As depicted in Fig. 3(b), MHMC\npartitions input channels into N heads and applies distinct\ndepth-wise separable convolutions to each head, which re-\nduces the parameter size and computational cost. To sim-\nplify our design process, we initialize the kernel size with\n3\u00d73 and gradually increase it by 2 per head.\nThis ap-\nproach enables us to regulate the range of receptive fields\nand multi-granularity information by merely adjusting the\nMulti-Head \nMixed \nConvolution \nScale-Aware \nAggregation \n\u2026\u2026\nN heads\nKxK\nDW Conv\n7x7\nDW Conv\n5x5\nDW Conv\n3x3\nDW Conv\nGroup 1\n\u2026\nLight-weight\n1x1 conv\nGroup 2\n\u2026\nGroup 3\n\u2026\nGroup M\n\u2026\n\u2026\u2026\n\u2026\u2026\n1x1 Conv\nLinear\nLinear\n.\nLight-weight\n1x1 conv\nLight-weight\n1x1 conv\nLight-weight\n1x1 conv\n(a) SAM\n(b) MHMC\n(c) SAA\nFigure 3: (a) The schematic illustration of the proposed\nscale-aware modulation (SAM). (b) and (c) are the mod-\nule descriptions of multi-head mixed convolution (MHMC)\nand scale-aware aggregation (SAA), respectively.\nnumber of heads. Our proposed MHMC can be formulated\nas follows:\nMHMC(X) = Concat(DWk1\u00d7k1(x1), . . . , DWkn\u00d7kn(xn))\n(1)\nwhere x = [x1, x2, ..., xn] means to split up the input fea-\nture x into multiple heads in the channel dimension and\nki \u2208 {3, 5, . . . , K} denotes the kernel size increases mono-\ntonically by 2 per head.\nAs shown in Fig. 4(a), each distinct convolution feature\nmap learns to focus on different granularity features in an\nadaptive manner, as expected. Notably, when we compare\nthe single-head and multi-head by visualizing modulation\nmaps in Fig. 4(b), we find that the visualization under multi-\nhead depicts the foreground and target objects accurately in\nstage 1, while filtering out background information effec-\ntively. Moreover, it can still present the overall shape of the\ntarget object as the network becomes deeper, while the in-\nformation related to the details is lost under the single-head\nconvolution. This indicates that MHMC has the ability to\ncapture local details better than a single head at the shallow\nstage, while maintaining detailed and semantic information\nabout the target object as the network becomes deeper.\nScale-Aware Aggregation\nTo enhance information inter-\naction across multiple heads in MHMC, we introduce a\nnew lightweight aggregation module, termed Scale-Aware\nAggregation (SAA), as shown in Fig. 3(c). The SAA in-\nvolves an operation that shuffles and groups the features\nof different granularities produced by the MHMC. Specifi-\ncally, we select one channel from each head to construct a\ngroup, and then we utilize the inverse bottleneck structure\nto perform an up-down feature fusion operation within each\nHead 1\nHead 2\nHead 3\nHead 4\nSingle Head Convolution\nMulti-Head Mixed Convolution\nStage 1\nStage 2\n(a) Different Heads\n(b) Single Head vs Multi-Head\nInput\nFigure 4: (a) Visualization of the output values of different\nheads in the MHMC in the first stage. (b) Visualization of\nthe modulation values (corresponding to the left side of \u2299 in\nEq. 3) under single-head and multi-head mixed convolution\nin the last layer during the top two stages. All maps are\nupsampled for display.\ngroup, thereby enhancing the diversity of multi-scale fea-\ntures. However, a well-designed grouping strategy enables\nus to introduce only a small amount of computation while\nachieving desirable aggregation results. Notably, let the in-\nput X \u2208 RH\u00d7W \u00d7C, Groups =\nC\nHeads, which means the\nnumber of groups is inversely proportional to the number of\nheads. Subsequently, we perform cross-group information\naggregation for all features using point-wise convolution to\nachieve cross-fertilization of global information. The pro-\ncess of SAA can be formulated as follows:\nM = Winter([G1, G2, . . . , GM]),\nGi = Wintra([Hi\n1, Hi\n2, . . . , Hi\nN]),\nHi\nj = DWConvkj\u00d7kj(xi\nj) \u2208 RH\u00d7W \u00d71.\n(2)\nwhere Winter and Wintra are weight matrices of point-wise\nconvolution. j \u2208 {1, 2, . . . , N} and i \u2208 {1, 2, . . . , M},\nwhere N and M =\nC\nN denote the number of heads and\ngroups, respectively. Here, Hj \u2208 RH\u00d7W \u00d7M represents the\nj-th head with depth-wise convolution, and Hi\nj represents\nthe i-th channel in the j-th head.\nFig. 5 shows that our SAA module explicitly strengthens\nthe semantically relevant low-frequency signals and pre-\ncisely focuses on the most important parts of the target\nobject. For instance, in stage 2, the eyes, head and body\nare clearly highlighted as essential features of the target\nobject, resulting in significant improvements in classifica-\ntion performance. Compared to the convolution maps be-\nfore aggregation, our SAA module demonstrates a better\nability to capture and represent essential features for visual\nrecognition tasks. (More visualizations can be found in Ap-\npendix E).\nScale-Aware Modulation\nAs illustrated in Fig. 3(a), after\ncapturing multi-scale spatial features using MHMC and ag-\n(a) Pre SAA\n(b) After SAA\nStage 1\nStage 2\nStage 3\nStage 1\nStage 2\nStage 3\nInput\nFigure 5: (a) Visualization of the modulation values before\nSAA. (b) Visualization of the modulation values after SAA.\ngregating them with SAA, we obtain an output feature map,\nwhich we refer to as the modulator M. We then adopt this\nmodulator to modulate the value V using the scalar prod-\nuct. For the input features X \u2208 RH\u00d7W \u00d7C, we compute the\noutput Z as follows:\nZ = M \u2299 V,\nV = WvX,\nM = SAA(MHMC(WsX)).\n(3)\nwhere \u2299 is the element-wise multiplication, Wv and Ws\nare weight martices of linear layers. Since the modulator\nis calculated via Eq. 3, it changes dynamically with dif-\nferent inputs, thereby achieving adaptively self-modulation.\nMoreover, unlike self-attention, which computes an N \u00d7 N\nattention map, the modulator retains the channel dimension.\nThis feature allows for spatial- and channel-specific modu-\nlation of the value after element-wise multiplication, while\nalso being memory-efficient, particularly when processing\nhigh-resolution images.\n3.3. Scale-Aware Modulation Transformer\nEvolutionary Hybrid Network\nIn this section, we pro-\npose to reallocate the appropriate computational modules\naccording to the variation pattern in the network\u2019s capture\nrange dependencies to achieve better computational per-\nformance. We propose using MSA blocks only from the\npenultimate stage to reduce the computational burden. Fur-\nthermore, to effectively simulate the transition pattern, we\nput forth two hybrid stacking strategies for the penultimate\nstage: (i) sequentially stacking one SAM block and one\nMSA block, which can be formulated as (SAM \u00d7 1 +\nMSA\u00d71)\u00d7 N\n2 , depicted in Fig. 6(i); (ii) using SAM blocks\nfor the first half of the stage and MSA blocks for the second\nhalf, which can be formulated as (SAM \u00d7 N\n2 +MSA\u00d7 N\n2 ),\ndepicted in Fig. 6(ii).\nSAM\nBlock\nMSA\nBlock\n\u00d7 \ud835\udc3f\n\u2026\u2026\n(i) \n(ii)\nSAM\nBlock\nMSA\nBlock\nSAM\nBlock\nMSA\nBlock\nSAM\nBlock\nMSA\nBlock\n\u2026\u2026\nSAM\nBlock\nMSA\nBlock\nMSA\nBlock\nSAM\nBlock\n\u00d7 \ud835\udc3f\n2\n\u2026\u2026\n\u00d7 \ud835\udc3f\n2\nFigure 6: Two proposed hybrid stacking strategies.\nTo assess the efficacy of these hybrid stacking strategies,\nwe evaluated their top-1 accuracy on the ImageNet-1K, as\nshown in Table 9.\nMoreover, as depicted in Fig. 7, we\ncalculate the relative receptive field of the MSA blocks in\nthe penultimate stage, followed by the approach presented\nin [4]. It is noteworthy that there is a slight downward trend\nin the onset of the relative receptive field in the early lay-\ners. This decline can be attributed to the impact of the SAM\non the early MSA blocks, which emphasize neighboring to-\nkens. We refer to this phenomenon as the adaptation period.\nAs the network becomes deeper, we can see a smooth and\nsteady upward trend in the receptive field, indicating that\nour proposed evolutionary hybrid network effectively simu-\nlates the transition from local to global dependency capture.\n0\n2\n4\n6\n8\n10\n12\n14\nDepth (layer)\n0.30\n0.35\n0.40\n0.45\n0.50\nRelative Receptive Field\nFigure 7: The receptive field of SMT-B\u2019s relative attention\nacross depth, with error bars representing standard devia-\ntions across various attention heads.\n4. Experiments\nTo ensure a fair comparison under similar parameters\nand computation costs, we construct a range of SMT vari-\nants.\nWe validate our SMTs on ImageNet-1K [10] im-\nage classification, MS COCO [30] object detection, and\nADE20K [75] semantic segmentation. Besides, extensive\nablation studies provide a close look at different compo-\nnents of the SMT. (The detailed model settings are pre-\nsented in Appendix A)\n(a) Tiny Models\nmethod\nimage\nsize\n#param. FLOPs ImageNet\ntop-1 acc.\nRegNetY-1.6G [39]\n2242\n11.2M\n1.6G\n78.0\nEffNet-B3 [49]\n3002\n12M\n1.8G\n81.6\nPVTv2-b1 [57]\n2242\n13.1M\n2.1G\n78.7\nEfficientFormer-L1 [27]\n2242\n12.3M\n1.3G\n79.2\nShunted-T [42]\n2242\n11.5M\n2.1G\n79.8\nConv2Former-N [18]\n2242\n15M\n2.2G\n81.5\nSMT-T(Ours)\n2242\n11.5M\n2.4G\n82.2\n(b) Small Models\nmethod\nimage\nsize\n#param. FLOPs ImageNet\ntop-1 acc.\nRegNetY-4G [39]\n2242\n21M\n4.0G\n80.0\nEffNet-B4 [49]\n3802\n19M\n4.2G\n82.9\nDeiT-S [52]\n2242\n22M\n4.6G\n79.8\nSwin-T [32]\n2242\n29M\n4.5G\n81.3\nConvNeXt-T [33]\n2242\n29M\n4.5G\n82.1\nPVTv2-b2 [57]\n2242\n25.0M\n4.0G\n82.0\nFocal-T [65]\n2242\n29.1M\n4.9G\n82.2\nShunted-S [42]\n2242\n22.4M\n4.9G\n82.9\nCMT-S [14]\n2242\n25.1M\n4.0G\n83.5\nFocalNet-T [64]\n2242\n28.6M\n4.5G\n82.3\nConv2Former-T [18]\n2242\n27M\n4.4G\n83.2\nHorNet-T [41]\n2242\n23M\n4.0G\n83.0\nInternImage-T [55]\n2242\n30M\n5.0G\n83.5\nMaxViT-T [53]\n2242\n31M\n5.6G\n83.6\nSMT-S(Ours)\n2242\n20.5M\n4.7G\n83.7\n(c) Base Models\nmethod\nimage\nsize\n#param. FLOPs ImageNet\ntop-1 acc.\nRegNetY-8G [39]\n2242\n39M\n8.0G\n81.7\nEffNet-B5 [49]\n4562\n30M\n9.9G\n83.6\nSwin-S [32]\n2242\n49.6M\n8.7G\n83.0\nCoAtNet-1 [9]\n2242\n42M\n8.0G\n83.3\nPVTv2-b4 [57]\n2242\n63M\n10.0G\n83.6\nSwinV2-S/8 [31]\n2562\n50M\n12.0G\n83.7\nPoolFormer-m36 [67]\n2242\n56.2M\n8.8G\n82.1\nShunted-B [42]\n2242\n39.6M\n8.1G\n84.0\nInternImage-S [55]\n2242\n50.0M\n8.0G\n84.2\nConv2Former-S [18]\n2242\n50.0M\n8.7G\n84.1\nSwin-B [32]\n2242\n87.8M\n15.4G\n83.4\nConvNeXt-B [33]\n2242\n89M\n15.4G\n83.8\nFocal-B [65]\n2242\n89.8M\n16.4G\n83.8\nFocalNet-B [64]\n2242\n88.7M\n15.4G\n83.9\nHorNet-B [41]\n2242\n87M\n15.6G\n84.2\nSMT-B(Ours)\n2242\n32.0M\n7.7G\n84.3\nTable 1: Comparison of different backbones on ImageNet-\n1K classification.\n4.1. Image Classification on ImageNet-1K\nSetup\nWe conduct an evaluation of our proposed model\nand compare it with various networks on ImageNet-1K clas-\nsification [10].\nTo ensure a fair comparison, we follow\nthe same training recipes as previous works [52, 32, 42].\nImageNet-22K pre-trained models\nmethod\nimage\nsize\n#param. FLOPs ImageNet\ntop-1 acc.\nViT-B/16 [11]\n3842\n86.0M\n55.4G\n84.0\nViT-L/16 [11]\n3842\n307.0M 190.7G\n85.2\nSwin-Large [32]\n2242/2242 196.5M 34.5G\n86.3\nSwin-Large [32]\n3842/3842 196.5M 104.0G\n87.3\nFocalNet-Large [64] 2242/2242 197.1M 34.2G\n86.5\nFocalNet-Large [64] 2242/3842 197.1M 100.6G\n87.3\nInternImage-L [55]\n2242/3842\n223M\n108G\n87.7\nInternImage-XL [55] 2242/3842\n335M\n163G\n88.0\nSMT-L(Ours)\n2242/2242\n80.5M\n17.7G\n87.1\nSMT-L(Ours)\n2242/3842\n80.5M\n54.6G\n88.1\nTable 2: ImageNet-1K finetuning results with models pre-\ntrained on ImageNet-22K. Numbers before and after \u201c/\u201d are\nresolutions used for pretraining and finetuning, respectively\nSpecifically, we train the models for 300 epochs with an\nimage size of 224 \u00d7 224 and report the top-1 validation ac-\ncuracy. The batch size used is 1024, and we employ the\nAdamW optimizer [24, 34] with a weight decay of 0.05 and\na learning rate of 1 \u00d7 10\u22123. In addition, we investigate\nthe effectiveness of SMTs when pretrained on ImageNet-\n22K.(Further details regarding the training process can be\nfound in Appendix B)\nResults\nTab. 1 presents a comparison of our proposed\nSMT with various models, and the results demonstrate that\nour models outperform various architectures with fewer pa-\nrameters and lower computation costs. Specifically, con-\ncerning the tiny-sized model, SMT achieves an impressive\ntop-1 accuracy of 82.2%, surpassing PVTv2-b1 [57] and\nShunted-T [42] by significant margins of 3.5% and 2.4%,\nrespectively. Furthermore, when compared to small-sized\nand base-sized models, SMT maintains its leading posi-\ntion. Notably, SMT-B achieves a top-1 accuracy of 84.3%\nwith only 32M parameters and 7.7GFLOPs of computation,\noutperforming many larger models such as Swin-B [32],\nConvNeXt-B [33], and FocalNet-B [64], which have over\n70M parameters and 15GFLOPs of computation. Addition-\nally, to evaluate the scalability of the SMT, we have also\ncreated smaller and larger models, and the experimental re-\nsults are presented in the Appendix C.\nWe also report the ImageNet-22K pre-training results\nhere in Tab. 2. When compared to the previously best re-\nsults, our models achieve significantly better accuracy with\na reduced number of parameters and FLOPs. SMT-L attains\nan 88.1% top-1 accuracy, surpassing InternImage-XL by\n0.1% while utilizing significantly fewer parameters (80.5M\nvs. 335M) and exhibiting lower FLOPs (54.6G vs. 163G).\nThis highly encouraging outcome underscores the impres-\nsive scalability capabilities of SMT.\nBackbone\nParams FLOPs\nMask R-CNN 1\u00d7 schedule\nMask R-CNN 3\u00d7 schedule + MS\n(M)\n(G)\nAP b AP b\n50 AP b\n75 AP m AP m\n50 AP m\n75 AP b AP b\n50 AP b\n75 AP m AP m\n50 AP m\n75\nResNet50 [17]\n44.2\n260\n38.0\n58.6\n41.4\n34.4\n55.1\n36.7\n41.0\n61.7\n44.9\n37.1\n58.4\n40.1\nTwins-SVT-S [6]\n44.0\n228\n43.4\n66.0\n47.3\n40.3\n63.2\n43.4\n46.8\n69.2\n51.2\n42.6\n66.3\n45.8\nSwin-T [32]\n47.8\n264\n42.2\n64.6\n46.2\n39.1\n61.6\n42.0\n46.0\n68.2\n50.2\n41.6\n65.1\n44.8\nPVTv2-B2 [56]\n45.0\n-\n45.3\n67.1\n49.6\n41.2\n64.2\n44.4\n-\n-\n-\n-\n-\n-\nFocal-T [65]\n48.8\n291\n44.8\n67.7\n49.2\n41.0\n64.7\n44.2\n47.2\n69.4\n51.9\n42.7\n66.5\n45.9\nCMT-S [14]\n44.5\n249\n44.6\n66.8\n48.9\n40.7\n63.9\n43.4\n-\n-\n-\n-\n-\n-\nFocalNet-T [64]\n48.9\n268\n46.1\n68.2\n50.6\n41.5\n65.1\n44.5\n48.0\n69.7\n53.0\n42.9\n66.5\n46.1\nSMT-S\n40.0\n265\n47.8\n69.5\n52.1\n43.0\n66.6\n46.1\n49.0\n70.1\n53.4\n43.4\n67.3\n46.7\nResNet101 [17]\n63.2\n336\n40.4\n61.1\n44.2\n36.4\n57.7\n38.8\n42.8\n63.2\n47.1\n38.5\n60.1\n41.3\nSwin-S [32]\n69.1\n354\n44.8\n66.6\n48.9\n40.9\n63.4\n44.2\n48.5\n70.2\n53.5\n43.3\n67.3\n46.6\nSwin-B [32]\n107.1\n497\n46.9\n69.2\n51.6\n42.3\n66.0\n45.5\n48.5\n69.8\n53.2\n43.4\n66.8\n46.9\nTwins-SVT-B [6]\n76.3\n340\n45.2\n67.6\n49.3\n41.5\n64.5\n44.8\n48.0\n69.5\n52.7\n43.0\n66.8\n46.6\nPVTv2-B4 [56]\n82.2\n-\n47.5\n68.7\n52.0\n42.7\n66.1\n46.1\n-\n-\n-\n-\n-\n-\nFocal-S [65]\n71.2\n401\n47.4\n69.8\n51.9\n42.8\n66.6\n46.1\n48.8\n70.5\n53.6\n43.8\n67.7\n47.2\nFocalNet-S [64]\n72.3\n365\n48.3\n70.5\n53.1\n43.1\n67.4\n46.2\n49.3\n70.7\n54.2\n43.8\n67.9\n47.4\nSMT-B\n51.7\n328\n49.0\n70.2\n53.7\n44.0\n67.6\n47.4\n49.8\n71.0\n54.4\n44.0\n68.0\n47.3\nTable 3: Object detection and instance segmentation with Mask R-CNN on COCO. Only the 3\u00d7 schedule has the multi-scale\ntraining. All backbones are pre-trained on ImageNet-1K.\nMethod\nBackbones\n#Params FLOPs AP b AP b\n50 AP b\n75 AP m AP m\n50 AP m\n75\nCascade [2]\nResNet50 [17]\n82.0M\n739G 46.3 64.3\n50.5\n40.1\n61.7\n43.4\nSwin-T [32]\n86.0M\n745G 50.5 69.3\n54.9\n43.7\n66.6\n47.1\nConvNeXt [33]\n-\n741G 50.4 69.1\n54.8\n43.7\n66.5\n47.3\nShuffle-T [23]\n86.0M\n746G 50.8 69.6\n55.1\n44.1\n66.9\n48.0\nFocalNet-T [64] 87.1M\n751G 51.5 70.3\n56.0\n-\n-\n-\nSMT-S\n77.9M\n744G 51.9 70.5\n56.3\n44.7\n67.8\n48.6\nMethod\nBackbones\n#Params FLOPs AP b AP b\n50 AP b\n75 APS APM APL\nRetinaNet [29]\nResNet50 [17]\n37.7M\n240G 39.0 58.4\n41.8\n22.4\n42.8\n51.6\nSwin-T [32]\n38.5M\n245G 45.0 65.9\n48.4\n29.7\n48.9\n58.1\nFocal-T [65]\n39.4M\n265G 45.5 66.3\n48.8\n31.2\n49.2\n58.7\nShunted-S [42]\n32.1M\n-\n46.4 66.7\n50.4\n31.0\n51.0\n60.8\nSMT-S\n30.1M\n247G 47.3 67.8\n50.5\n32.5\n51.1\n62.3\nTable 4: COCO detection and segmentation with the Cas-\ncade Mask R-CNN and RetinaNet. The performances are\nreported on the COCO val dataset under the 3\u00d7 schedule.\n4.2. Object Detection and Instance Segmentation\nSetup\nWe make comparisons on object detection with\nCOCO 2017 [30].\nWe use SMT-S/B pretrained on\nImageNet-1K as the foundation for three well-known object\ndetectors: Mask R-CNN [16], Cascade Mask R-CNN [2],\nand RetinaNet [29]. To demonstrate a consistent compar-\nison, two training schedules (1\u00d7 schedule with 12 epochs\nand 3\u00d7 schedule with 36 epochs) are adopted in Mask R-\nCNN. In 3\u00d7 schedule, we use a multi-scale training strat-\negy by randomly resizing the shorter side of an image to be-\ntween [480, 800]. We take AdamW optimizer with a weight\ndecay of 0.05 and an initial learning rate of 2 \u00d7 10\u22124. Both\nmodels are trained with batch size 16. To further showcase\nthe versatility of SMT, we conducted a performance eval-\nuation of SMT with three other prominent object detection\nMethod\nBackbone\n#Param. FLOPs AP b AP b\n50 AP b\n75\nSparse R-CNN [46]\nR-50 [17]\n106.1M 166G 44.5 63.4\n48.2\nSwin-T [32]\n109.7M 172G 47.9 67.3\n52.3\nFocal-T [65]\n110.8M 196G 49.0 69.1\n53.2\nFocalNet-T [64] 111.2M 178G 49.9 69.6\n54.4\nSMT-S\n102.0M 171G 50.2 69.8\n54.7\nATSS [72]\nR-50 [17]\n32.1M\n205G 43.5 61.9\n47.0\nSwin-T [32]\n35.7M\n212G 47.2 66.5\n51.3\nFocal-T [65]\n36.8M\n239G 49.5 68.8\n53.9\nFocalNet-T [64] 37.2M\n220G 49.6 68.7\n54.5\nSMT-S\n28.0M\n214G 49.9 68.9\n54.7\nDINO [70]\nR-50 [17]\n47.7M\n244G 49.2 66.7\n53.8\nSwin-T [32]\n48.2M\n252G 51.3 69.0\n55.9\nSwin-S [32]\n69.5M\n332G 53.0 71.2\n57.6\nSMT-S\n39.9M\n309G 54.0 71.9\n59.0\nTable 5: A comparison of models with three different object\ndetection frameworks.\nframeworks, namely Sparse RCNN [46], ATSS [72], and\nDINO [70]. We initialize the backbone with weights pre-\ntrained on ImageNet-1K and fine-tune the model using a\n3\u00d7 schedule for Sparse RCNN and ATSS.\nResults\nTab. 3 presents the superior performance of SMT\nover other networks with Mask R-CNN [16] under various\nmodel sizes. Specifically, SMT demonstrates a significant\nimprovement in box mAP of 5.6 and 4.2 over Swin Trans-\nformer in 1\u00d7 schedule under small and base model sizes, re-\nspectively. Notably, with 3\u00d7 schedule and multi-scale train-\ning, SMT still consistently outperforms various backbones.\nFor instance segmentation, the results also demonstrate that\nour SMT achieves higher mask mAP in comparison to pre-\nvious SOTA networks. In particular, for small and base\nmodels in the 1\u00d7 schedule, we achieve 1.5 and 0.9 points\nhigher than FocalNet, respectively. Furthermore, to assess\nthe generality of SMT, we trained two additional detec-\ntion models, Cascade Mask R-CNN [2] and RetinaNet [29],\nusing SMT-S as the backbone. The results, presented in\nTab. 4, show clear improvements over various backbones\nin both box and mask mAPs. The resulting box mAPs for\nSparse R-CNN, ATSS and DINO are presented in Tab. 5,\nwhich indicate that SMT outperforms other networks con-\nsistently across all detection frameworks, highlighting its\nexceptional performance in downstream tasks.\n4.3. Semantic Segmentation on ADE20K\nSetup\nWe evaluate the SMT for semantic segmentation\nusing the ADE20K dataset. To conduct the evaluation, we\nuse UperNet as the segmentation method and closely fol-\nlowed the training settings proposed by [32]. Specifically,\nwe train UperNet [61] for 160k iterations with an input res-\nolution of 512 \u00d7 512. We employ the AdamW optimizer\nwith a weight decay of 0.01, and set the learning rate to\n6 \u00d7 10\u22125.\nResults\nThe results are presented in Tab. 6, which shows\nthat our SMT outperforms Swin, FocalNet, and Shunted\nTransformer significantly under all settings. Specifically,\nSMT-B achieves 1.5 and 0.9 mIoU gains compared to Swin-\nB and a 0.6 and 0.1 mIoU improvement over Focal-B at\nsingle- and multi-scale, respectively, while consuming sig-\nnificantly fewer FLOPs and reducing the model size by\nmore than 50%. Even for the SMT with a small model size,\nit achieves comparable accuracy with the previous SOTA\nmodels which have a larger model size.\n4.4. Ablation Study\nNumber of heads in Multi-Head Mixed Convolution\nTable 7 shows the impact of the number of convolution\nheads in the Multi-Head Mixed Convolution (MHMC) on\nour model\u2019s performance. The experimental results indi-\ncate that while increasing the number of diverse convolu-\ntional kernels is advantageous for modeling multi-scale fea-\ntures and expanding the receptive field, adding more heads\nintroduces larger convolutions that may negatively affect\nnetwork inference speed and reduce throughput. Notably,\nwe observed that the top-1 accuracy on ImageNet-1K peaks\nwhen the number of heads is 4, and increasing the number\nof heads does not improve the model\u2019s performance. This\nfindings suggest that introducing excessive distinct convo-\nlutions or using a single convolution is not suitable for our\nSMT, emphasizing the importance of choosing the appro-\nBackbone\n#Param(M) FLOPs(G) mIoUss mIoUms\nResNet-101 [17]\n86\n1029\n44.9\n-\nDeiT-S [52]\n52\n1099\n44.0\n-\nSwin-T [32]\n60\n941\n44.5\n45.8\nFocal-T [65]\n62\n998\n45.8\n47.0\nFocalNet-T [65]\n61\n949\n46.8\n47.8\nSwin-S [32]\n81\n1038\n47.6\n49.5\nConvNeXt-S [33]\n82\n1027\n49.6\n-\nShunted-S [42]\n52\n940\n48.9\n49.9\nFocalNet-S [64]\n84\n1044\n49.1\n50.1\nFocal-S [65]\n85\n1130\n48.0\n50.0\nSwin-B [32]\n121\n1188\n48.1\n49.7\nTwins-SVT-L [6]\n133\n-\n48.8\n50.2\nFocal-B [65]\n126\n1354\n49.0\n50.5\nSMT-S\n50.1\n935\n49.2\n50.2\nSMT-B\n61.8\n1004\n49.6\n50.6\nTable 6: Semantic segmentation on ADE20K [75].\nAll\nmodels are trained with UperNet [61].\nmIoUms means\nmulti-scale evaluation.\npriate number of convolution heads to model a specific de-\ngree of multi-scale spatial features.\nHeads Number Params(M) FLOPs(G) top-1 (%) throughput\n(images/s)\n1\n11.5\n2.4\n81.8\n983\n2\n11.5\n2.4\n82.0\n923\n4\n11.5\n2.4\n82.2\n833\n6\n11.6\n2.5\n81.9\n766\n8\n11.6\n2.5\n82.0\n702\nTable 7:\nModel performance with number of heads in\nMHMC. We analyzed the model\u2019s performance for the\nnumber of heads ranging from 1 to 8. Throughput is mea-\nsured using a V100 GPU, following [32].\nDifferent aggregation strategies\nAfter applying the\nMHMC, we introduce an aggregation module to achieve\ninformation fusion. Table 8 presents a comparison of dif-\nferent aggregation strategies, including a single linear layer,\ntwo linear layers, and an Invert BottleNeck (IBN) [43]. Our\nproposed scale-aware aggregation (SAA) consistently out-\nperforms the other fusion modules, demonstrating the ef-\nfectiveness of SAA in modeling multi-scale features with\nfewer parameters and lower computational costs. Notably,\nas the size of the model increases, our SAA can exhibit\nmore substantial benefits while utilizing a small number of\nparameters and low computational resources.\nDifferent hybrid stacking strategies\nIn Sec. 3.3, we pro-\npose two hybrid stacking strategies to enhance the modeling\nof the transition from local to global dependencies. The re-\nsults shown in Table 9 indicate that the first strategy which\nsequentially stacks one scale-aware modulation block and\nAggregation Strategy\nParams\n(M)\nFLOPs\n(G)\ntop-1\n(%)\nNo aggregation\n10.9\n2.2\n81.5\nSingle Linear (c \u2192 c)\n11.2\n2.3\n81.6\nTwo Linears (c \u2192 c \u2192 c)\n11.5\n2.4\n81.9\nIBN (c \u2192 2c \u2192 c)\n12.1\n2.6\n82.1\nSAA(c \u2192 2c \u2192 c)\n11.5\n2.4\n82.2\nTable 8: Model performance for different aggregation meth-\nods.\none multi-head self-attention block is better, achieving a\nperformance gain of 0.3% compared to the other strategy.\nFurthermore, the strategy stacking all MSA blocks achieves\ncomparable performance as well, which means retaining the\nMSA block in the last two stages is crucial.\nStacking Strategy\nHybrid Params\n(M)\nFLOPs\n(G)\ntop-1\n(%)\n(SAM \u00d7 N)\n%\n11.8\n2.5\n81.4\n(MSA \u00d7 N)\n%\n11.2\n2.3\n81.8\n(SAM \u00d7 1 + MSA \u00d7 1) \u00d7 N\n2\n!\n11.5\n2.4\n82.2\n(SAM \u00d7 N\n2 + MSA \u00d7 N\n2 )\n!\n11.5\n2.4\n81.9\nTable 9: Top-1 accuracy on ImageNet-1K of different stack-\ning strategies.\nComponent Analysis\nIn this section, we investigate the\nindividual contributions of each component by conducting\nan ablation study on SMT. Initially, we employ a single-\nhead convolution module and no aggregation module to\nconstruct the modulation.\nBased on this, we build an\nattention-free network, which can achieve 80% top-1 ac-\ncuracy on the ImageNet-1K dataset. The effects of all the\nproposed methods on the model\u2019s performance are given in\nTab. 10, which can be summarized as followings.\n\u2022 Multi-Head Mixed Convolution (MHMC) To enhance\nthe model\u2019s ability to capture multi-scale spatial features\nand expand its receptive field, we replaced the single-\nhead convolution with our proposed MHMC. This mod-\nule proves to be effective for modulation, resulting in a\n0.8% gain in accuracy.\n\u2022 Scale-Aware Aggregation (SAA) We replace the single\nlinear layer with our proposed scale-aware aggregation.\nThe SAA enables effective aggregation of the multi-scale\nfeatures captured by MHMC. Building on the previous\nmodification, the replacement leads to a 1.6% increase in\nperformance.\n\u2022 Evolutionary Hybrid Network (EHN) We incorporate\nthe self-attention module in the last two stages of our\nmodel, while also implementing our proposed hybrid\nMHMC SAA EHN Params(M) FLOPs(G)\ntop-1 (%)\n11.1\n2.3\n80.0 (\u21910.0)\n\u2713\n11.2\n2.3\n80.8 (\u21910.8)\n\u2713\n\u2713\n12.1\n2.5\n81.6 (\u21911.6)\n\u2713\n\u2713\n\u2713\n11.5\n2.4\n82.2 (\u21912.2)\nTable 10: Component analysis for SMT. Three variations\nare gradually added to the original attention-free network.\nstacking strategy in the penultimate stage, which im-\nproves the modeling of the transition from local to global\ndependencies as the network becomes deeper, resulting\nin a significant gain of 2.2% in performance based on the\naforementioned modifications.\n5. Conclusion\nIn this paper, we introduce a new hybrid ConvNet and vi-\nsion Transformer backbone, namely Scale-Aware Modula-\ntion Transformer (SMT), which can effectively simulate the\ntransition from local to global dependencies as the network\nbecomes deeper, resulting in superior performance. To sat-\nisfy the requirement of foundation models, we propose a\nnew Scale-Aware Modulation that includes a potent multi-\nhead mixed convolution module and a lightweight scale-\naware aggregation module. Extensive experiments demon-\nstrate the efficacy of SMT as a backbone for various down-\nstream tasks, achieving comparable or better performance\nthan well-designed ConvNets and vision Transformers, with\nfewer parameters and FLOPs. We anticipate that the excep-\ntional performance of SMT on diverse vision problems will\nencourage its adoption as a promising new generic back-\nbone for efficient visual modeling.\nAcknowledgement\nThis research is supported in part by NSFC (Grant\nNo.:\n61936003), Alibaba DAMO Innovative Research\nFoundation (20210925), Zhuhai Industry Core, Key Tech-\nnology Research Project (no.\n2220004002350) and Na-\ntional Key Research and Development Program of China\n(2022YFC3301703).\nWe thank the support from the\nAlibaba-South China University of Technology Joint Grad-\nuate Education Program.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. Advances in neural information pro-\ncessing systems, 2016.\n[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 6154\u20136162, 2018.\n[3] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.\nCrossvit: Cross-attention multi-scale vision transformer for\nimage classification. In Proceedings of the IEEE/CVF in-\nternational conference on computer vision, pages 357\u2013366,\n2021.\n[4] Wuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua\nZhai, Tsung-Yi Lin, Huizhong Chen, Jing Li, Xiaodan Song,\nZhangyang Wang, et al. A simple single-scale vision trans-\nformer for object detection and instance segmentation. In\nComputer Vision\u2013ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part X,\npages 711\u2013727. Springer, 2022.\n[5] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen\nLiu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu.\nMobile-\nformer: Bridging mobilenet and transformer. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5270\u20135279, 2022.\n[6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. Advances in Neural Information Processing\nSystems, 34:9355\u20139366, 2021.\n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nScharw\u00a8achter, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset. In CVPR Workshop on the Future of Datasets in\nVision, volume 2, 2015.\n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\npractical automated data augmenta-\ntion with a reduced search space.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 702\u2013703, 2020.\n[9] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data\nsizes. Advances in Neural Information Processing Systems,\n34:3965\u20133977, 2021.\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In International Con-\nference on Learning Representations.\n[12] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303\u2013338, 2010.\n[13] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\nZhang, Ming-Hsuan Yang, and Philip Torr. Res2net: A new\nmulti-scale backbone architecture. IEEE TPAMI, 43(2):652\u2013\n662, 2021.\n[14] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao\nChen, Yunhe Wang, and Chang Xu. Cmt: Convolutional neu-\nral networks meet vision transformers (supplementary mate-\nrial. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 12175\u201312185,\n2022.\n[15] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming\nCheng, and Shi-Min Hu. Visual attention network. arXiv\npreprint arXiv:2202.09741, 2022.\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\npages 770\u2013778, 2016.\n[18] Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, and Jiashi\nFeng. Conv2former: A simple transformer-style convnet for\nvisual recognition. arXiv preprint arXiv:2211.11943, 2022.\n[19] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3.\nIn Proceedings of the IEEE/CVF international\nconference on computer vision, pages 1314\u20131324, 2019.\n[20] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017.\n[21] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132\u20137141, 2018.\n[22] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In Com-\nputer Vision\u2013ECCV 2016: 14th European Conference, Am-\nsterdam, The Netherlands, October 11\u201314, 2016, Proceed-\nings, Part IV 14, pages 646\u2013661. Springer, 2016.\n[23] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng,\nGang Yu, and Bin Fu.\nShuffle transformer:\nRethink-\ning spatial shuffle for vision transformer.\narXiv preprint\narXiv:2106.03650, 2021.\n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. 2015.\n[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 60(6):84\u201390, 2017.\n[26] Jiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xue-\nfeng Xiao, Rui Wang, Min Zheng, and Xin Pan.\nNext-\nvit:\nNext generation vision transformer for efficient de-\nployment in realistic industrial scenarios.\narXiv preprint\narXiv:2207.05501, 2022.\n[27] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evan-\ngelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Ef-\nficientformer: Vision transformers at mobilenet speed. In\nAdvances in Neural Information Processing Systems.\n[28] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021.\n[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll\u00b4ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980\u20132988, 2017.\n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740\u2013755.\nSpringer, 2014.\n[31] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\nSwin transformer v2: Scaling up capacity and resolution. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 12009\u201312019, 2022.\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021.\n[33] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 11976\u201311986,\n2022.\n[34] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations.\n[35] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. In International Conference on\nLearning Representations.\n[36] Muhammad\nMaaz,\nAbdelrahman\nShaker,\nHisham\nCholakkal, Salman Khan, Syed Waqas Zamir, Rao Muham-\nmad Anwer, and Fahad Shahbaz Khan. Edgenext: Efficiently\namalgamated cnn-transformer architecture for mobile vision\napplications. In Computer Vision\u2013ECCV 2022 Workshops:\nTel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part\nVII, pages 3\u201320. Springer, 2023.\n[37] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-\nweight, general-purpose, and mobile-friendly vision trans-\nformer. In International Conference on Learning Represen-\ntations.\n[38] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,\nZuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim.\nAdavit:\nAdaptive vision transformers for efficient image recognition.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12309\u201312318, 2022.\n[39] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll\u00b4ar. Designing network design\nspaces. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 10428\u201310436,\n2020.\n[40] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie\nZhou, and Cho-Jui Hsieh.\nDynamicvit: Efficient vision\ntransformers with dynamic token sparsification. Advances\nin neural information processing systems, 34:13937\u201313949,\n2021.\n[41] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou,\nSer-Nam Lim, and Jiwen Lu. Hornet: Efficient high-order\nspatial interactions with recursive gated convolutions. In Ad-\nvances in Neural Information Processing Systems.\n[42] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and\nXinchao Wang. Shunted self-attention via multi-scale token\naggregation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10853\u2013\n10862, 2022.\n[43] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510\u20134520, 2018.\n[44] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. 2015.\n[45] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck\ntransformers for visual recognition.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 16519\u201316529, 2021.\n[46] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-\nfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan\nYuan, Changhu Wang, et al. Sparse r-cnn: End-to-end ob-\nject detection with learnable proposals. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14454\u201314463, 2021.\n[47] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\nAlexander A Alemi. Inception-v4, inception-resnet and the\nimpact of residual connections on learning. In AAAI, 2017.\n[48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, pages 2818\u20132826,\n2016.\n[49] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nconference on machine learning, pages 6105\u20136114. PMLR,\n2019.\n[50] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models\nand faster training. In International conference on machine\nlearning, pages 10096\u201310106. PMLR, 2021.\n[51] Mingxing Tan and Quoc V Le. Mixconv: Mixed depthwise\nconvolutional kernels. In the 30th British Machine Vision\nConference (BMVC) 2019, 2019.\n[52] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International conference on machine learning,\npages 10347\u201310357. PMLR, 2021.\n[53] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\nPeyman Milanfar, Alan Bovik, and Yinxiao Li.\nMaxvit:\nMulti-axis vision transformer. In European conference on\ncomputer vision, pages 459\u2013479. Springer, 2022.\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\n[55] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,\nZhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,\nHongsheng Li, et al. Internimage: Exploring large-scale vi-\nsion foundation models with deformable convolutions. arXiv\npreprint arXiv:2211.05778, 2022.\n[56] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions.\nIn Proceedings of the\nIEEE/CVF international conference on computer vision,\npages 568\u2013578, 2021.\n[57] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt\nv2: Improved baselines with pyramid vision transformer.\nComputational Visual Media, 8(3):415\u2013424, 2022.\n[58] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794\u20137803, 2018.\n[59] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\nKweon. Cbam: Convolutional block attention module. In\nProceedings of the European conference on computer vision\n(ECCV), pages 3\u201319, 2018.\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang.\nCvt: Introducing\nconvolutions to vision transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 22\u201331, 2021.\n[61] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In Proceedings of the European conference on computer\nvision (ECCV), pages 418\u2013434, 2018.\n[62] Saining Xie, Ross Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, pages 1492\u20131500, 2017.\n[63] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-\nscale conv-attentional image transformers. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 9981\u20139990, 2021.\n[64] Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.\nFocal modulation networks. In Advances in Neural Informa-\ntion Processing Systems.\n[65] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention\nfor local-global interactions in vision transformers.\nAd-\nvances in Neural Information Processing Systems, 2021.\n[66] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya,\nJan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for\nefficient vision transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10809\u201310818, 2022.\n[67] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,\nXinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer\nis actually what you need for vision.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10819\u201310829, 2022.\n[68] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 6023\u20136032, 2019.\n[69] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In International Conference on Learning Representa-\ntions, 2018.\n[70] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with\nimproved denoising anchor boxes for end-to-end object de-\ntection. In The Eleventh International Conference on Learn-\ning Representations, 2022.\n[71] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu\nYuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-\nformer: A new vision transformer for high-resolution image\nencoding.\nIn Proceedings of the IEEE/CVF international\nconference on computer vision, pages 2998\u20133008, 2021.\n[72] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\nStan Z Li.\nBridging the gap between anchor-based and\nanchor-free detection via adaptive training sample selection.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 9759\u20139768, 2020.\n[73] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShufflenet: An extremely efficient convolutional neural net-\nwork for mobile devices. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n6848\u20136856, 2018.\n[74] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In Proceedings\nof the AAAI conference on artificial intelligence, volume 34,\npages 13001\u201313008, 2020.\n[75] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017.\n[76] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, An-\nimashree Anandkumar, Jiashi Feng, and Jose M Alvarez.\nUnderstanding the robustness in vision transformers. In In-\nternational Conference on Machine Learning, pages 27378\u2013\n27394. PMLR, 2022.\n[77] Barret Zoph and Quoc Le. Neural architecture search with\nreinforcement learning.\nIn International Conference on\nLearning Representations.\nAppendix\nA. Detailed Architecture Specifications\nTab. 13 provides a detailed overview of the architecture\nspecifications for all models, with an assumed input image\nsize of 224\u00d7224. The stem of the model is denoted as \u201dconv\nn \u00d7 n, 64-d, BN; conv 2 \u00d7 2, 64-d, LN\u201d, representing two\nconvolution layers with a stride of 2 to obtain a more infor-\nmative token sequence with a length of H\n4 \u00d7 W\n4 . Here, \u201dBN\u201d\nand \u201dLN\u201d indicate Batch Normalization and Layer Normal-\nization [1], respectively, while \u201d64-d\u201d denotes the convolu-\ntion layer with an output dimension of 64. The multi-head\nmixed convolution module with 4 heads (conv 3 \u00d7 3, conv\n5 \u00d7 5, conv 7 \u00d7 7, conv 9 \u00d7 9) is denoted as \u201dsam. head.\n4\u201d, while \u201dmsa. head. 8\u201d represents the multi-head self-\nattention module with 8 heads. Additionally, \u201dsam. ep r. 2\u201d\nindicates a Scale-Aware Aggregation module with twice as\nmuch expanding ratio.\nB. Detailed Experimental Settings\nB.1. Image classification on ImageNet-1K\nWe trained all models on the ImageNet-1K dataset [10]\nfor 300 epochs, using an image size of 224 \u00d7 224 .\nFollowing Swin [32], we utilized a standardized set of\ndata augmentations [8], including Random Augmentation,\nMixup [69], CutMix [68], and Random Erasing [74]. To\nregularize our models, we applied Label Smoothing [48]\nand DropPath [22] techniques. The initial learning rate for\nall models was set to 2 \u00d7 10\u22123 after 5 warm-up epochs, be-\nginning with a rate of 1\u00d710\u22126. To optimize our models, we\nemployed the AdamW [34] algorithm and a cosine learning\nrate scheduler [35]. The weight decay was set to 0.05 and\nthe gradient clipping norm to 5.0. For our mini, tiny, small,\nbase, and large models, we used stochastic depth drop rates\nof 0.1, 0.1, 0.2, 0.3, and 0.5, respectively. For more details,\nplease refer to the Tab. 11 provided.\nB.2. Image classification pretrained on ImageNet-\n22K\nWe trained the SMT-L model for 90 epochs using a batch\nsize of 4096 and an input resolution of 224\u00d7224. The ini-\ntial learning rate was set to 1 \u00d7 10\u22123 after a warm-up pe-\nriod of 5 epochs. The stochastic depth drop rates were set\nto 0.1. Following pretraining, we performed fine-tuning on\nthe ImageNet-1K dataset for 30 epochs. The initial learning\nrate was set to 2 \u00d7 10\u22125, and we utilized a cosine learn-\ning rate scheduler and AdamW optimizer. The stochastic\ndepth drop rate remained at 0.1 during fine-tuning, while\nboth CutMix and Mixup augmentation techniques were dis-\nabled.\nconfig\nvalue\noptimizer\nAdamW\nLR\n2e-3\nweight decay\n0.05\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\nbatch size\n1024\nLR schedule\ncosine\nminimum learning rate\n1e-5\nwarmup epochs\n5\nwarmup learning rate\n1e-6\ntraining epochs\n300\naugmentation\nrand-m9-mstd0.5-inc1\ncolor jitter\n0.4\nmixup \u03b1\n0.2\ncutmix \u03b1\n1.0\nrandom erasing\n0.25\nlabel smoothing\n0.1\ngradient clip\n5.0\ndrop path\n[0.1, 0,1, 0,2, 0,3, 0.5] (M,T,S,B,L)\nTable 11: Image Classification Training Settings\nB.3. Object Detection and Instance Segmentation\nIn transferring SMT to object detection and instance seg-\nmentation on COCO [30], we have considered six common\nframeworks: Mask R-CNN [16], Cascade Mask RCNN [2],\nRetinaNet [29], Sparse R-CNN [46], ATSS [72], and\nDINO [70].\nFor DINO, the model is fine-tuned for 12\nepochs, utilizing 4 scale features.\nFor optimization, we\nadopt the AdamW optimizer with an initial learning rate\nof 0.0002 and a batch size of 16. When training models\nof different sizes, we adjust the training settings according\nto the settings used in image classification. The detailed\nhyper-parameters used in training models are presented in\nTab. 12.\nconfig\nvalue\noptimizer\nAdamW\nLR\n0.0002\nweight decay\n0.05\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\nbatch size\n16\nLR schedule\nsteps:[8, 11] (1\u00d7), [27, 33] (3\u00d7)\nwarmup iterations (ratio) 500 (0.001)\ntraining epochs\n12 (1\u00d7), 36 (3\u00d7)\nscales\n(800, 1333) (1\u00d7), Multi-scales [32] (3\u00d7)\ndrop path\n0.2 (Small), 0.3 (Base)\nTable 12:\nObject Detection and Instance Segmentation\nTraining Settings\nB.4. Semantic Segmentation\nFor ADE20K, we utilized the AdamW optimizer with\nan initial learning rate of 0.00006, a weight decay of 0.01,\ndownsp. rate\n(output size) Layer Name\nSAM-M\nSAM-T\nSAM-S\nSAM-B\nSAM-L\nstage 1\n4\u00d7\n(56\u00d756)\nSAM\nBlock\nconv 3\u00d73, 64-d, BN\nconv 2\u00d72, 64-d, LN\nconv 3\u00d73, 64-d, BN\nconv 2\u00d72, 64-d, LN\nconv 7\u00d77, 64-d, BN\nconv 2\u00d72, 64-d, LN\nconv 7\u00d77, 64-d, BN\nconv 2\u00d72, 64-d, LN\nconv 7\u00d77, 96-d, BN\nconv 2\u00d72, 96-d, LN\n\uf8ee\n\uf8f0\ndim 64\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 1\n\uf8ee\n\uf8f0\ndim 64\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 2\n\uf8ee\n\uf8f0\ndim 64\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 3\n\uf8ee\n\uf8f0\ndim 64\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 4\n\uf8ee\n\uf8f0\ndim 96\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 4\nstage 2\n8\u00d7\n(28\u00d728)\nSAM\nBlock\nconv 3\u00d73, 128-d, LN\nconv 3\u00d73, 128-d, LN conv 3\u00d73, 128-d, LN conv 3\u00d73, 128-d, LN\nconv 3\u00d73, 192-d, LN\n\uf8ee\n\uf8f0\ndim 128\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 1\n\uf8ee\n\uf8f0\ndim 128\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 2\n\uf8ee\n\uf8f0\ndim 128\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 4\n\uf8ee\n\uf8f0\ndim 128\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 6\n\uf8ee\n\uf8f0\ndim 192\nsam.head. 4\nsam.ep r. 2\n\uf8f9\n\uf8fb \u00d7 6\nstage 3\n16\u00d7\n(14\u00d714)\nMix\nBlock\nconv 3\u00d73, 256-d , LN conv 3\u00d73, 256-d , LN conv 3\u00d73, 256-d , LN conv 3\u00d73, 256-d , LN conv 3\u00d73, 384-d , LN\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ndim 256\nsam.head. 4\nsam.ep r. 2\nmsa.head. 8\n\uf8f9\n\uf8fa\uf8fa\uf8fb \u00d7 4\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ndim 256\nsam.head. 4\nsam.ep r. 2\nmsa.head. 8\n\uf8f9\n\uf8fa\uf8fa\uf8fb \u00d7 8\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ndim 256\nsam.head. 4\nsam.ep r. 2\nmsa.head. 8\n\uf8f9\n\uf8fa\uf8fa\uf8fb \u00d7 18\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ndim 256\nsam.head. 4\nsam.ep r. 2\nmsa.head. 8\n\uf8f9\n\uf8fa\uf8fa\uf8fb \u00d7 28\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ndim 384\nsam.head. 4\nsam.ep r. 2\nmsa.head. 8\n\uf8f9\n\uf8fa\uf8fa\uf8fb \u00d7 28\nstage 4\n32\u00d7\n(7\u00d77)\nMSA\nBlock\nconv 3\u00d73, 512-d , LN conv 3\u00d73, 512-d , LN conv 3\u00d73, 512-d , LN conv 3\u00d73, 512-d , LN conv 3\u00d73, 768-d , LN\n\u0014\ndim 512\nmsa.head 16\n\u0015\n\u00d7 1\n\u0014\ndim 512\nmsa.head 16\n\u0015\n\u00d7 1\n\u0014\ndim 512\nmsa.head 16\n\u0015\n\u00d7 1\n\u0014\ndim 512\nmsa.head 16\n\u0015\n\u00d7 2\n\u0014\ndim 768\nmsa.head 16\n\u0015\n\u00d7 3\nTable 13: Detailed architecture specifications at four stages for SMT.\nw/o. EHN (all SAM block)\nw/. EHN (SAM + MSA)\n1\n3\n5\n7\ninput\n1\n3\n5\n7\nFigure 8: Visualization of modulation values at the penultimate stage for two variants of SMT. (Left: w/o. EHN) Stacking\nof SAM blocks exclusively in the penultimate stage. (Right: w/. EHN) The utilization of an evolutionary hybrid stacking\nstrategy, wherein one SAM block and one MSA are successively stacked.\nand a batch size of 16 for all models trained for 160K itera-\ntions. In terms of testing, we reported the results using both\nsingle-scale (SS) and multi-scale (MS) testing in the main\ncomparisons. For multi-scale testing, we experimented with\nresolutions ranging from 0.5 to 1.75 times that of the train-\ning resolution. To set the path drop rates in different models,\nwe used the same hyper-parameters as those used for object\ndetection and instance segmentation.\nC. More Experiments\nC.1. More Variants of SMT\nThis section demonstrates how we scaled our SMT to\ncreate both smaller (SMT-M) and larger (SMT-L) models.\nTheir detailed architecture settings are provided in Tab. 13,\nalong with previous variants. We then evaluated their per-\nformance on the ImageNet-1K dataset.\nAs shown in Tab. 14, SMT-M achieves competitive re-\nsults with a top-1 accuracy of 78.4%, despite having only\n6.5M parameters and 1.3 GFLOPs of computation. On the\nother side, SMT-L shows an example to scale our SMT\nto larger models, which outperforms other state-of-the-art\nnetworks with similar parameters and computation costs,\nachieving a top-1 accuracy of 84.6%. These results confirm\nthe strong scalability of the SMT architecture, which can be\napplied to create models of varying sizes, demonstrating its\nimmense potential.\nD. Additional Network Analysis\nIn Fig. 8, we present the learned scale-aware modulation\n(SAM) value maps in two variants of SMT-T: evolution-\nary SMT, which employs an evolutionary hybrid stacking\nstrategy, and general SMT, which only employs SAM in the\nmethod\nimage\nsize\n#param. FLOPs ImageNet\ntop-1 acc.\nRegNetY-4G [39]\n2242\n21M\n4.0G\n80.0\nRegNetY-8G [39]\n2242\n39M\n8.0G\n81.7\nRegNetY-16G [39]\n2242\n84M\n16.0G\n82.9\nEffNet-B3 [49]\n3002\n12M\n1.8G\n81.6\nEffNet-B4 [49]\n3802\n39M\n4.2G\n82.9\nEffNet-B5 [49]\n4562\n30M\n9.9G\n83.6\nEffNet-B6 [49]\n5282\n43M\n19.0G\n84.0\nPVT-T [56]\n2242\n13M\n1.8G\n75.1\nPVT-S [56]\n2242\n25M\n3.8G\n79.8\nPVT-M [56]\n2242\n44M\n6.7G\n81.2\nPVT-L [56]\n2242\n61M\n9.8G\n81.7\nSwin-T [32]\n2242\n29M\n4.5G\n81.3\nSwin-S [32]\n2242\n49.6M\n8.7G\n83.0\nSwin-B [32]\n2242\n87.8M\n15.4G\n83.4\nTwins-S [6]\n2242\n24M\n2.9G\n81.7\nTwins-B [6]\n2242\n56M\n8.6G\n83.2\nFocal-T [65]\n2242\n29M\n4.9G\n82.2\nFocal-B [65]\n2242\n89.8M\n16.4G\n83.8\nShunted-T [42]\n2242\n11.5M\n2.1G\n79.8\nShunted-S [42]\n2242\n22.4M\n4.9G\n82.9\nShunted-B [42]\n2242\n39.6M\n8.1G\n84.0\nFocalNet-T [64]\n2242\n28.6M\n4.5G\n82.3\nFocalNet-S [64]\n2242\n50.3M\n8.7G\n83.5\nFocalNet-B [64]\n2242\n88.7M\n15.4G\n83.9\nMaxViT-T [53]\n2242\n31M\n5.6G\n83.6\nMaxViT-S [53]\n2242\n69M\n11.7G\n84.5\nMaxViT-B [53]\n2242\n120M\n23.4G\n84.9\nSMT-M\n2242\n6.5M\n1.3G\n78.4\nSMT-T\n2242\n11.5M\n2.4G\n82.2\nSMT-S\n2242\n20.5M\n4.7G\n83.7\nSMT-B\n2242\n32.0M\n7.7G\n84.3\nSMT-L\n2242\n80.5M\n17.7G\n84.6\nTable 14: Comparison of different backbones on ImageNet-\n1K classification.\npenultimate stage. In evolutionary SMT-T, comprising a to-\ntal of 8 layers in the penultimate stage, we select the lay-\ners ([1, 3, 5, 7]) containing SAM block and compare them\nwith the corresponding layers in general SMT. Through vi-\nsualization, we can observe some noteworthy patterns. In\ngeneral SMT, the model primarily concentrates on local de-\ntails in the shallow layers and on semantic information in\nthe deeper layers. However, in evolutionary SMT, the fo-\ncus region does not significantly shift as the network depth\nincreases. Furthermore, it captures local details more effec-\ntively than general SMT in the shallow layers, while pre-\nserving detailed and semantic information about the target\nobject at deeper layers. These results indicate that our evo-\nlutionary hybrid stacking strategy facilitates SAM blocks in\ncapturing multi-granularity features while allowing multi-\nhead self-attention (MSA) blocks to concentrate on captur-\ning global semantic information. Accordingly, each block\nwithin each layer is more aptly tailored to its computational\ncharacteristics, leading to enhanced performance in diverse\nvisual tasks.\nE. Additional Visual Examples\nWe present supplementary visualization of modulation\nvalue maps within our SMT. Specifically, we randomly se-\nlect validation images from the ImageNet-1K dataset and\ngenerate visual maps for modulation at different stages, as\nillustrated in Fig 9. The visualizations reveal that the scale-\naware modulation is critical in strengthening semantically\nrelevant low-frequency signals and accurately localizing the\nmost discriminative regions within images. By exploiting\nthis robust object localization capability, we can allocate\nmore effort towards modulating these regions, resulting in\nmore precise predictions. We firmly believe that both our\nmulti-head mixed convolution module and scale-aware ag-\ngregation module have the potential to further enhance the\nmodulation mechanism.\nStage 1\nStage 2\nStage 3\nInput\nInput\nStage 1\nStage 2\nStage 3\nFigure 9: Visualization of modulation value maps at the top\nthree stages.\n"
  },
  {
    "title": "Language Conditioned Traffic Generation",
    "link": "https://arxiv.org/pdf/2307.07947.pdf",
    "upvote": "4",
    "text": "Language Conditioned Traffic Generation\nShuhan Tan1\nBoris Ivanovic2\nXinshuo Weng2\nMarco Pavone2\nPhilipp Kr\u00e4henb\u00fchl1\n1UT Austin\n2NVIDIA\nAbstract: Simulation forms the backbone of modern self-driving development.\nSimulators help develop, test, and improve driving systems without putting hu-\nmans, vehicles, or their environment at risk. However, simulators face a major\nchallenge: They rely on realistic, scalable, yet interesting content. While re-\ncent advances in rendering and scene reconstruction make great strides in cre-\nating static scene assets, modeling their layout, dynamics, and behaviors remains\nchallenging. In this work, we turn to language as a source of supervision for dy-\nnamic traffic scene generation. Our model, LCTGen, combines a large language\nmodel with a transformer-based decoder architecture that selects likely map loca-\ntions from a dataset of maps, produces an initial traffic distribution, as well as the\ndynamics of each vehicle. LCTGen outperforms prior work in both unconditional\nand conditional traffic scene generation in-terms of realism and fidelity. Code and\nvideo will be available at https://ariostgx.github.io/lctgen.\nKeywords: Self-driving, Content generation, Large language model\n1\nIntroduction\nDriving simulators stand as a cornerstone in self-driving development. They aim to offer a con-\ntrolled environment to mimic real-world conditions and produce critical scenarios at scale. Towards\nthis end, they need to be highly realistic (to capture the complexity of real-world environments),\nscalable (to produce a diverse range of scenarios without excessive manual effort), and able to cre-\nate interesting traffic scenarios (to test self-driving agents under different situations).\nIn this paper, we turn to natural language as a solution. Natural language allows practitioners to\neasily articulate interesting and complex traffic scenarios through high-level descriptions. Instead\nof meticulously crafting the details of each individual scenario, language allows for a seamless\nconversion of semantic ideas into simulation scenarios at scale. To harness the capacity of natural\nlanguage, we propose LCTGen. LCTGen takes as input a natural language description of a traffic\nscenario, and outputs traffic actors\u2019 initial states and motions on a compatible map. As we will show\nin Section 5, LCTGen generates realistic traffic scenarios that closely adhere to a diverse range of\nnatural language descriptions, including detailed crash reports [1].\nThe major challenge of language-conditioned traffic generation is the absence of a shared repre-\nsentation between language and traffic scenarios. Furthermore, there are no paired language-traffic\ndatasets to support learning such a representation. To address these challenges, LCTGen (see Fig-\nure 1) uses a scenario-only dataset and a Large Language Model (LLM). LCTGen has three modules:\nInterpreter, Generator and Encoder. Given any user-specified natural language query, the LLM-\npowered Interpreter converts the query into a compact, structured representation. Interpreter\nalso retrieves an appropriate map that matches the described scenario from a real-world map library.\nThen, the Generator takes the structured representation and map to generate realistic traffic scenar-\nios that accurately follow the user\u2019s specifications. Also, we design the Generator as a query-based\nTransformer model [2], which efficiently generates the full traffic scenario in a single pass.\nThis paper presents three main contributions:\n1. We introduce LCTGen, a first-of-its-kind model for language-conditional traffic generation.\n2. We devise a method to harness LLMs to tackle the absence of language-scene paired data.\narXiv:2307.07947v1  [cs.CV]  16 Jul 2023\nGenerator\n Interpreter(LLM)\n\u201cego vehicle turns right \nat an intersection\u201d\nRetrieval\nStructured Representation\nMap\nText Description\nOutput Scenario\n...\nMap Dataset\nMap:[2,2,1,1,0,2]\nV1:[-1,0,0,2,1,1,1,1]\nV2:[ 2,0,0,3,4,4,4,4]\nV3:[ 0,0,2,2,4,4,4,4]\nFigure 1: Overview of our LCTGen model.\n3. LCTGen exhibits superior realism and controllability over prior work. We also show LCTGen\ncan be applied to instructional traffic editing and controllable self-driving policy evaluation.\n2\nRelated Work\nTraffic scenario generation traditionally rely on rules defined by human experts [3], e.g., rules that\nenforce vehicles to stay in lanes, follow the lead vehicles [4, 5, 6] or change lanes [7]. This approach\nis used in most virtual driving datasets [8, 9, 10, 11] and simulators [12, 3, 13]. However, traffic\nscenarios generated in this way often lack realism as they do not necessarily match the distribution\nof real-world traffic scenarios. Moreover, creating interesting traffic scenarios in this way requires\nnon-trivial human efforts from experts, making it difficult to scale. In contrast, LCTGen learns the\nreal-world traffic distribution for realistic traffic generation. Also, LCTGen can generate interesting\nscenarios with language descriptions, largely reducing the requirement of human experts.\nPrior work in learning-based traffic generation is more related to our work. SceneGen [14] uses au-\ntoregressive models to generate traffic scenarios conditioned on ego-vehicle states and maps. Traf-\nficGen [15] applies two separate modules for agent initialization and motion prediction. BITS [16]\nlearns to simulate agent motions with a bi-level imitation learning method. Similar to LCTGen, these\nmethods learn to generate realistic traffic scenarios from real-world data. However, they lack the\nability to control traffic generation towards users\u2019 preferences. In contrast, LCTGen achieves such\ncontrollability via natural languages and at the same time can generate highly realistic traffic sce-\nnarios. Moreover, we will show in the experiments that LCTGen also outperforms prior work in the\nsetting of unconditional traffic reconstruction, due to our query-based end-to-end architecture.\nText-conditioned generative models have recently shown strong capabilities for controllable con-\ntent creation for image [17], audio [18], motion [19], 3D object [20] and more. DALL-E [17] uses\na transformer to model text and image tokens as a single stream of data. Noise2Music [18] uses\nconditioned diffusion models to generate music clips from text prompts. MotionCLIP [19] achieves\ntext-to-human-motion generation by aligning the latent space of human motion with pre-trained\nCLIP [21] embedding. These methods typically require large-scale pairs of content-text data for\ntraining. Inspired by prior work, LCTGen is the first-of-its-kind model for text-conditioned traf-\nfic generation. Also, due to the use of LLM and our design of structured representation, LCTGen\nachieves text-conditioned generation without any text-traffic paired data.\nLarge language models have become increasingly popular in natural language processing and re-\nlated fields due to their ability to generate high-quality text and perform language-related tasks.\nGPT-2 [22] is a transformer-based language model that is pre-trained on vast amounts of text data.\nGPT-4 [23] largely improves the instruction following capacity by fine-tuning with human feedback\nto better align the models with their users [24]. In our work, we adapt the GPT-4 model [23] with\nin-context-learning [25] and chain-of-thought [26] prompting method as our Interpreter.\n3\nPreliminaries\nLet m be a map region, and st be the state of all vehicles in a scene at time t. A traffic scenario \u03c4 =\n(m, s1:T ) is the combination of a map region m and T timesteps of vehicle states s1:T = [s1, ..., sT ].\n2\nGPT4\nSummary:  V1 approaches an intersection and does not notice V2 ahead...\nExplanation:  [V1] - Because V1 is moving , we assume V1's initial speed is 10 \nm/s (index 4). V1 keeps going straight, so its actions are all 4 (keep speed). \n[V2] - As V1 is moving straight and hits V2 from behind, V2 is in front of V1....\nOutput: \n- 'V1': [-1, 0, 0, 4, 4, 4, 4, 4] - 'V2': [3, 8, 2, 0, 4, 4, 4, 4] - 'Map': [2, 2, 2, 2, 8, 1]\nAs Vehicle 1 approached the intersection, its \ndriver did not notice the vehicles stopped \nahead at the traffic light. The traffic signal \nturned green and Vehicle 2 began to slowly \nmove forward. The frontal plane of Vehicle 1 \nstruck the rear plane of Vehicle 2 ...\nFigure 2: Example Interpreter input and output. We only show partial texts for brevity.\nMap. We represent each map region m by a set of S lane segments denoted by m = {v1, ..., vS}.\nEach lane segment includes the start point and endpoint of the lane, the lane type (center lane, edge\nlane, or boundary lane), and the state of the traffic light control.\nVehicle states. The vehicle states st = {s1\nt, ..., sN\nt } at time t consist of N vehicle. For each vehicle,\nwe model the vehicle\u2019s position, heading, velocity, and size. Following prior work [14, 15], we\nchoose the vehicle at the center of the scenario in the first frame as the ego-vehicle. It represents the\nself-driving vehicle in simulation platforms.\n4\nLCTGen: Language-Conditioned Traffic Generation\nOur goal is to train a language-conditioned model \u03c4 \u223c LCTGen(L, M) that produces traffic scenar-\nios from a text description L and a dataset of maps M. Our model consists of three main compo-\nnents: A language Interpreter (Section 4.1) that encodes a text description into a structured repre-\nsentation z. Map Retrieval m \u223c Retrieval(z, M) that samples matching map regions m from a\ndataset of maps M. A Generator (Section 4.3) that produces a scenario \u03c4 \u223c Generator(z, m) from\nthe map m and structured representation z. All components are stochastic, allowing us to sample\nmultiple scenes from a single text description L and map dataset M. We train the Generator with a\nreal-world scenario-only driving dataset (Section 4.4).\n4.1\nInterpreter\nThe Interpreter takes a text description L as input and produces a structured representation z =\nInterpreter(L). After defining the representation z, we show how to produce it via GPT-4 [23].\nStructured representation z = [zm, za\n1, . . . za\nN] contains both map-specific zm and agent-specific\ncomponents za\ni . For each scenario, we use a 6-dimensional vector zm describing the local map. It\nmeasures the number of lanes in each direction (north, south, east, west), the distance of the map\ncenter to an intersection, and the lane the ego-vehicle finds itself in. This compact abstract allows a\nlanguage model to describe the important properties of a m and interact with map dataset M. For\neach agent i, za\ni is an 8-dimentional integer vector describing the agent relative to the ego vehicle. It\ncontains an agent\u2019s quadrant position index (1-4), distance range (0-20m, 20-40m,...), orientation in-\ndex (north, south, east, west), speed range (0-2.5m/s, 2.5-5m/s, ...), and action description (turn left,\naccelerate, ...). Please refer to Supp.A. for a complete definition of z. Note that the representation z\ndoes not have a fixed length, as it depends on the number of agents in a scene.\nLanguage interpretation. To obtain the structured representation, we use a large language model\n(LLM) and formulate the problem into a text-to-text transformation. Specifically, we ask GPT-4 [23]\nto translate the textual description of a traffic scene into a YAML-like description through in-context\nlearning [25]. To enhance the quality of the output, we use Chain-of-Thought [26] prompting to let\nGPT-4 summarize the scenario q in short sentences and plan agent-by-agent how to generate z. See\nFigure 2 for an example input and output. Refer to Supp. A for the full prompt and Supp. D.4 for\nmore complete examples.\n4.2\nRetrieval\nThe Retrieval module takes a map representation zm and map dataset M, and samples map re-\ngions m \u223c Retrieval(zm, M). Specifically, we preprocess the map dataset M into potentially\noverlapping map regions {m1, m2, ...}. We sample map regions, such that their center aligns with\nthe locations of an automated vehicle in an offline driving trace. This ensures that the map region\n3\nGenerative\nTransformer\nMap\nEncoder\nPE\nStructured \nRepresentation\n      Map\nMap Feature\nMotion MLP\nAttribute  MLP\nAgent Query       \nAgent Feature       \nMap Mask MLP\nPosition  MLP\n      Motion Pred       \nAttribute Pred\n   Lane Mask\nAgent Mask\nPosition Pred\nFigure 3: Architecture of our Generator model.\nis both driveable and follows a natural distribution of vehicle locations. For each map mj, we pre-\ncompute its map representation \u02c6zm\nj . This is possible, as the map representation is designed to be\nboth easy to produce programmatically and by a language model. Given zm, the Retrieval ranks\neach map region mj based its feature distance\n\r\rzm \u2212 \u02c6zm\nj\n\r\r. Finally, Retrieval randomly samples\nm from the top-K closest map regions.\n4.3\nGenerator\nGiven a structured representation z and map m, the Generator produces a traffic scenario \u03c4 =\nGenerator(z, m). We design Generator as a query-based transformer model to efficiently capture\nthe interactions between different agents and between agents and the map. It places all the agents in\na single forward pass and supports end-to-end training. The Generator has four modules (Figure 3):\n1) a map encoder that extracts per-lane map features F; 2) an agent query generator that converts\nstructured representation za\ni to agent query qi; 3) a generative transformer that models agent-agent\nand agent-map interactions; 4) a scene decoder to output the scenario \u03c4.\nMap encoder processes a map region m = {v1, . . . , vS} with S lane segments vi into a map feature\nF = {f1, . . . , fS}, and meanwhile fuse information across different lanes. Because S could be very\nlarge, we use multi-context gating (MCG) blocks [27] for efficient information fusion. MCG blocks\napproximate a transformer\u2019s cross-attention, but only attend a single global context vector in each\nlayer. Specifically, a MCG block takes a set of features v1:S as input, computes a context vector c,\nthen combines features and context in the output v\u2032\n1:S. Formally, each block is implemented via\nv\u2032\ni = MLP(vi) \u2299 MLP(c)\nwhere\nc = MaxPool(v1:S)\nwhere \u2299 is the element-wise product. The encoder combines 5 MCG blocks with 2-layer MLPs.\nAgent query generator transforms the structured representation za\ni of each agent i into an agent\nquery qi \u2208 Rd. We implement this module as an MLP of positional embeddings of the structured\nrepresentation qi = MLP(PE(za\ni )) + MLP(xi). We use a sinusoidal position encoding PE(\u00b7). We\nalso add a learnable query vector xi as inputs, as inspired by the object query in DETR [28].\nGenerative transformer.\nTo model agent-agent and agent-map interactions, we use F\n=\n{f1, . . . , fS} and Q = {q1, . . . , qN} as inputs and pass them through multiple transformer lay-\ners. Each layer follows Q\u2032 = MHCA(MHSA(Q), F), where MHCA, MHSA denote that multi-head cross-\nattention and multi-head self-attention respectively [2]. The output of Q\u2032 in each layer is used as the\nquery for the next layer to cross-attend to F. The outputs of the last-layer Q\u2217 are the agent features.\nScene decoder. For each agent feature q\u2217\ni , the scene decoder produces the agents position, attributes,\nand motion using an MLP. To decode the position, we draw inspiration from MaskFormer [29],\nplacing each actor on a lane segment in the map. This allows us to explicitly model the positional\nrelationship between each actor and the road map. Specifically, we employ an MLP to turn q\u2217\ni into\nan actor mask embedding eagent\ni\n\u2208 Rd. Likewise, we transform each lane feature fj into a per-\nlane map mask embedding elane\nj\n\u2208 Rd. The position prediction \u02c6pi \u2208 RS for the i-th agent is then\n\u02c6pi = softmax(eagent\ni\n\u00d7 [elane\n1\n, . . . , elane\nS ]T ),\n4\nFor each agent query, we predict its attributes, namely heading, velocity, size, and position shift\nfrom the lane segment center, following Feng et al. [15]. The attribute distribution of a potential\nagent is modeled with a Gaussian mixture model (GMM). The parameters of a K-way GMM for\neach attribute of agent i are predicted as [\u00b5i, \u03a3i, \u03c0i] = MLP(q\u2217\ni ), where \u00b5i, \u03a3i and \u03c0i denote the\nmean, covariance matrix, and the categorical weights of the K-way GMM model.\nWe further predict the future T \u2212 1 step motion of each agent, by outputting K\u2032 potential future\ntrajectories for each agent: {pos2:T\ni,k , probi,k}K\u2032\nk=1 = MLP(q\u2217\ni ), where pos2:T\ni,k represents the k-th\ntrajectory states for T \u22121 future steps, and probi,k is its probability. Specifically, for each timestamp\nt, post\ni,k = (x, y, \u03b8) contains the agent\u2019s position (x, y) and heading \u03b8 at t.\nDuring inference, we sample the most probable values from the predicted position, attribute, and\nmotion distributions of each agent query to generate an output agent status through T time stamps\nsi\n1:T . Compiling the output for all agents, we derive the vehicle statuses s1:T . In conjunction with\nm, the Generator outputs the final traffic scenario \u03c4 = (m, s1:T ).\n4.4\nTraining\nThe Generator is the only component of LCTGen that needs to be trained. We use real-world self-\ndriving datasets, composed of D traffic scenarios {\u03c4j}D\nj=1. For each traffic scene, we use an Encoder\nto produce the latent representation z, then train the Generator to reconstruct the scenario.\nEncoder.\nThe Encoder takes a traffic scenario \u03c4 and outputs structured agent representation:\nza = Encoder(\u03c4). As mentioned in Section 4.1, za contains compact abstract vectors of each\nagent {za\n1, ..., za\nN}. For each agent i, the Encoder extracts from its position, heading, speed, and\ntrajectory from the ground truth scene measurements si\n1:T in \u03c4, and converts it into za\ni following a\nset of predefined rules. For example, it obtains the quadrant position index with the signs of (x, y)\nposition. In this way, we can use Encoder to automatically convert any scenario \u03c4 to latent codes z.\nThis allows us to obtain a paired dataset (m, s1:N, za\n1:N) from scenario-only driving dataset.\nTraining objective.\nFor each data sample (m, s1:N, za\n1:N), we generate a prediction p\n=\nGenerator(z, m). The objective is to reconstruct the real scenario \u03c4. We compute the loss as:\nL(p, \u03c4) = Lposition(p, \u03c4) + Lattr(p, \u03c4) + Lmotion(p, \u03c4),\n(1)\nwhere Lposition, Lattr, Lmotion are losses for each of the different predictions. We pair each agent in p\nwith a ground-truth agent in \u03c4 based on the sequential ordering of the structured agent representation\nza. We then calculate loss values for each component. For Lposition, we use cross-entropy loss\nbetween the categorical output \u02c6p and the ground-truth lane segment id. For Lattr, we use a negative\nlog-likelihood loss, computed using the predicted GMM on the ground-truth attribute values. For\nLmotion, we use MSE loss for the predicted trajectory closest to the ground-truth trajectory. The\ntraining objective is the expected loss L over the dataset. We refer readers to Supp. B for more\ndetailed formulations of the loss functions.\n5\nExperiments\nDatasets. We use the large-scale real-world Waymo Open Dataset [30], partitioning it into 68k\ntraffic scenarios for training and 2.5k for testing. For each scene, we limit the maximum number\nof lanes to S = 384, and set the maximum number of vehicles to N = 32. We simulate T = 50\ntimesteps at 10 fps, making each \u03c4 represent a 5-second traffic scenario. We collect all the map\nsegments in the Waymo Open Dataset training split for our map dataset M.\nImplementation. We query GPT-4 [23] (with a temperature of 0.2) through the OpenAI API for\nInterpreter. For Generator, we set the latent dimension d = 256. We use a 5-layer MCG block for\nthe map encoder. For the generative transformer, we use a 2-layer transformer with 4 heads. We use\na dropout layer after each transformer layer with a dropout rate of 0.1. For each attribute prediction\nnetwork, we use a 2-layer MLP with a latent dimension of 512. For attribute GMMs, we use K = 5\ncomponents. For motion prediction, we use K\u2032 = 12 prediction modes. We train Generator with\nAdamW [31] for 100 epochs, with a learning rate of 3e-4 and batch size of 64.\n5\nMethod\nInitialization\nMotion\nPos\nHeading\nSpeed\nSize\nmADE\nmFDE\nSCR\nTrafficGen [15]\n0.2002\n0.1524\n0.2379\n0.0951\n10.448\n20.646\n5.690\nMotionCLIP [19]\n0.1236\n0.1446\n0.1958\n0.1234\n6.683\n13.421\n8.842\nLCTGen (w/o z)\n0.1319\n0.1418\n0.1948\n0.1092\n6.315\n12.260\n8.383\nLCTGen\n0.0616\n0.1154\n0.0719\n0.1203\n1.329\n2.838\n6.700\nTable 1: Traffic scenario generation realism evaluation (lower the better).\n5.1\nScene Reconstruction Evaluation\nWe evaluate the quality of LCTGen\u2019s generated scenarios by comparing them to real scenarios from\nthe driving dataset. For each scenario sample (\u03c4, z, m) in the test dataset, we generate a scenario\nwith \u02c6\u03c4 = Generator(z, m) and then compute different metrics with \u03c4 and \u02c6\u03c4.\nMetrics. To measure the realism of scene initialization, we follow [14, 15] and compute the maxi-\nmum mean discrepancy (MMD [32]) score for actors\u2019 positions, headings, speed and sizes. Specif-\nically, MMD measures the distance between two distributions q and p. For each pair of real and\ngenerated data (\u03c4, \u02c6\u03c4), we compute the distribution difference between them per attribute. To mea-\nsure the realism of generated motion behavior, we employ the standard mean average distance error\n(mADE) and mean final distance error (mFDE). For each pair of real and generated scenarios (\u03c4, \u02c6\u03c4),\nwe first use the Hungarian algorithm to compute a matching based on agents\u2019 initial locations with\ntheir ground-truth location. We then transform the trajectory for each agent based on its initial po-\nsition and heading to the origin of its coordinate frame, to obtain its relative trajectory. Finally, we\ncompute mADE and mFDE using these relative trajectories. We also compute the scenario collision\nrate (SCR), which is the average proportion of vehicles involved in collisions per scene.\nBaselines. We compare against a state-of-the-art traffic generation method, TrafficGen [15]. As\nTrafficGen only takes a map m as input to produce a scenario \u03c4, we train a version of LCTGen that\nalso only uses m as input for a fair comparison, referred to as LCTGen (w/o z). We also compare\nagainst MotionCLIP [19], which takes both a map m and text L as input to generate a scenario \u03c4.\nPlease refer to Supp. C for the implementation details of each baseline.\nResults.\nThe results in Table 1 indicate the superior performance of LCTGen. In terms of scene\ninitialization, LCTGen (w/o z) outperforms TrafficGen in terms of MMD values for the Position,\nHeading, and Speed attributes. Importantly, when conditioned on the language input L, LCTGen\nsignificantly improves its prediction of Position, Heading, and Speed attributes, significantly out-\nperforming both TrafficGen and MotionCLIP on MMD (> 2\u00d7). LCTGen also achieves 7-8x smaller\nmADE and mFDE than baselines when comparing generated motions. The unconditional version\nof LCTGen, without z, also outpaces TrafficGen in most metrics, demonstrating the effectiveness\nof Generator\u2019s query-based, end-to-end transformer design. We note that LCTGen (w/o) z has an\non-par Size-MMD score with TrafficGen, which is lower than LCTGen. We conjecture that this is\nbecause our model learns spurious correlations of size and other conditions in z in the real data.\n5.2\nLanguage-conditioned Simulation Evaluation\nLCTGen aims to generate a scenario \u03c4 that accurately represents the traffic description from the input\ntext L. Since no existing real-world text-scenario datasets are available, we carry out our experiment\nusing text L from a text-only traffic scenario dataset. To evaluate the degree of alignment between\neach scenario and the input text, we conduct a human study. We visualize the output scenario \u03c4\ngenerated by LCTGen or the baselines, and ask humans to assess how well it matches the input text.\nDatasets. We use a challenging real-world dataset, the Crash Report dataset [1], provided by the\nNHTSA. Each entry in this dataset comprises a comprehensive text description of a crash scenario,\nincluding the vehicle\u2019s condition, driver status, road condition, vehicle motion, interactions, and\nmore. Given the complexity and intricate nature of the traffic scenarios and their text descriptions,\nthis dataset presents a significant challenge (see Figure 2 for an example). We selected 38 cases\nfrom this dataset for the purposes of our study. For a more controllable evaluation, we also use\nan Attribute Description dataset. This dataset comprises text descriptions that highlight various\n6\nMethod\nCrash Report\nAttribute Description\nOurs Prefered (%)\nScore (1-5)\nOurs Prefered (%)\nScore (1-5)\nTrafficGen [15]\n92.35\n1.58\n90.48\n2.43\nMotionCLIP [19]\n95.29\n1.65\n95.60\n2.10\nLCTGen\n-\n3.86\n-\n4.29\nTable 2: Human study results on the language-conditioned simulation.\n\u201cV1 and V2 collide at an intersection of \ntwo urban trafficways, with V2 striking \nthe left side of V1.\u201d\n\u201cV1 is traveling east in the left turn lane \nand attempts to turn left when it \ncollides with V2 traveling west in the \nleft through lane.\u201d\n\u201cthe scene is  sparse. there are only \nvehicles behind the ego-vehicle. most \ncars are moving in fast speed. the \ncenter car turns left.\u201d\n\u201cthe ego car turns right, most cars are \nmoving in slow speed.\u201d\nFigure 4: Qualitative results on text-conditioned generation.\nattributes of a traffic scenario. These include aspects like sparsity (\"the scenario is dense\"), position\n(\"there are vehicles on the left\"), speed (\"most cars are driving fast\"), and the ego vehicle\u2019s motion\n(\"the ego vehicle turns left\"). We create more complex descriptions by combining 2, 3, and 4\nattributes. This dataset includes 40 such cases. Refer to Supp. C for more details about these\ndatasets.\nBaselines. We compare with TrafficGen and MotionCLIP. For each text input L, LCTGen outputs\na scenario \u03c4 = (m, s1:T ). To ensure fairness, we feed data m to both TrafficGen and MotionCLIP\nto generate scenarios on the same map. As TrafficGen does not take language condition as input,\nwe only feed L to MotionCLIP. In addition, TrafficGen can\u2019t automatically decide the number of\nagents, therefore it uses the same number of agents as our output \u03c4.\nHuman study protocol. For each dataset, we conduct a human A/B test. We present the evaluators\nwith a text input, along with a pair of scenarios generated by two different methods using the same\ntext input, displayed in a random order. The evaluators are then asked to decide which scenario they\nthink better matches the text input. Additionally, evaluators are requested to assign a score between\n1 and 5 to each generated scenario, indicating its alignment with the text description; a higher score\nindicates a better match. A total of 12 evaluators participated in this study, collectively contributing\n1872 scores for each model.\nQuantitative Results.\nWe show the results in Table 2. We provide preference score, reflecting\nthe frequency with which LCTGen\u2019s output is chosen as a better match than each baseline. We also\nprovide the average matching score, indicating the extent to which evaluators believe the generated\nscenario matches the text input. With LCTGen often chosen as the preferred model by human evalua-\ntors (at least 90% of the time), and consistently achieving higher scores compared to other methods,\nthese results underline its superior performance in terms of text-controllability over previous works.\nThe high matching score also signifies LCTGen\u2019s exceptional ability to generate scenarios that faith-\nfully follow the input text. We include more analysis of human study result in Supp. D.3.\nQualitative Results. We show examples of LCTGen output given texts from the Crash Report (left\ntwo) and Attribute Description (right two) datasets in Figure 4. Each example is a pair of input text\nand the generated scenario. Because texts in Crash Report are excessively long, we only show the\noutput summary of our Interpreter for each example (Full texts in Supp. C). Please refer to Supp.\nvideo for the animated version of the examples here. We show more examples in Supp. D.2.\n7\nInput\n\u201cmake the car in front turn left\u201d \u201cremove all the horizontal cars\u201d \u201cadd more cars on the left\u201d\n\u201cspeed up same-direction cars\u201d\nFigure 5: Instructional editing on a real-world scenario. Refer to Supp.A for full prompts.\nMethod\nPos\nHeading\nSpeed\nSize\nw/o Quad.\n0.092\n0.122\n0.076\n0.124\nw/o Dist.\n0.071\n0.124\n0.073\n0.121\nw/o Ori.\n0.067\n0.132\n0.082\n0.122\nLCTGen\n0.062\n0.115\n0.072\n0.120\n(a) Ablation study for scene initialization.\nMethod\nmADE\nmFDE\nSCR\nw/o Speed\n2.611\n5.188\n7.150\nw/o Action\n2.188\n5.099\n7.416\nLCTGen init. + [15] motion\n2.467\n5.682\n5.210\nLCTGen\n1.329\n2.838\n6.700\n(b) Ablation study for motion behavior generation.\nTable 3: Scene reconstruction ablation study on the Waymo Open Dataset.\n5.3\nApplication: Instructional Traffic Scenario Editing\nBesides language-conditioned scenario generation, LCTGen can also be applied to instructional traf-\nfic scenario editing. Given either a real or generated traffic scenario \u03c4, along with an editing instruc-\ntion text I, LCTGen can produce an edited scenario \u02c6\u03c4 that follows I. First, we acquire the structured\nrepresentation of the scenario using z = Encoder(\u03c4). Next, we compose a unique prompt that in-\nstructs Interpreter to alter z in accordance with I, resulting in \u02c6z = Interpreter(z, I). Finally, we\ngenerate the edited scenario \u02c6\u03c4 = Generator(\u02c6z, m), where m is the same map used in the input.\nWe show an example of consecutive instructional editing of a real-world scenario in Figure 5. We\ncan see that LCTGen supports high-level editing instructions (vehicle removal, addition and action\nchange). It produces realistic output following the instruction. This experiment highlights LCTGen\u2019s\npotential for efficient instruction-based traffic scenario editing. As another application of LCTGen,\nwe also show how LCTGen can be utilized to generate interesting scenarios for controllable self-\ndriving policy evaluation. Please refer to Supp. D.1 for this application.\n5.4\nAblation study\nScene initialization. Table 3 summarizes the results, where the last row corresponds to our full\nmethod. To validate the performance of LCTGen for scene initialization, we mask out the quadrant\nindex, distance, and orientation in the structure representation z for each agent, respectively. As\na result, we observed a significant performance drop, especially in the prediction of Position and\nHeading attributes, as shown in the left side of Table 3. This suggests that including quadrant index,\ndistance, and orientation in our structured representation is effective.\nMotion behavior generation. We summarized the results in Table 3 (right). By masking out the\nspeed range and action description in the structured representation for each agent, we observed a\nsignificant performance drop in the metrics for motion behavior. Moreover, if we initialize the scene\nwith LCTGen while generating agents\u2019 motion behavior using TrafficGen\u2019s [15], we also observed\nsignificantly worse performance than using LCTGen to generate the traffic scenario in one shot. The\nresults suggest that the end-to-end design of scene initialization and motion behavior generation by\nour LCTGen can lead to better performance. We show more ablation study results in Supp. D.5.\n6\nConclusion\nIn this work, we present LCTGen, a first-of-its-kind method for language-conditioned traffic scene\ngeneration. By harnessing the expressive power of natural language, LCTGen can generate realistic\nand interesting traffic scenarios. The realism of our generated traffic scenes notably exceeds previ-\nous state-of-the-art methods. We further show that LCTGen can be applied to applications such as\ninstructional traffic scenario editing and controllable driving policy evaluation.\n8\nLimitations. The primary constraint of LCTGen lies in the Interpreter module\u2019s inability to output\nperfect agent placements and trajectories, as it lacks direct access to detailed lane information from\nthe map. Our future work aims to overcome these issues by equipping the Interpreter with map and\nmath APIs, enabling it to fetch precise map data and output more comprehensive traffic scenarios.\nAcknowledgement. We thank Yuxiao Chen, Yulong Cao, and Danfei Xu for their insightful discus-\nsions. This material is supported by the National Science Foundation under Grant No. IIS-1845485.\nReferences\n[1] National Highway Traffic Safety Administration. Crash injury research engineering network.\nhttps://crashviewer.nhtsa.dot.gov/CIREN/SearchIndex, 2016.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.\ncc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[3] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Fl\u00f6tter\u00f6d, R. Hilbrich, L. L\u00fccken,\nJ. Rummel, P. Wagner, and E. Wie\u00dfner. Microscopic traffic simulation using sumo. In The\n21st IEEE International Conference on Intelligent Transportation Systems. IEEE, 2018. URL\nhttps://elib.dlr.de/124092/.\n[4] L. Papaleondiou and M. Dikaiakos. Trafficmodeler: A graphical tool for programming micro-\nscopic traffic simulators through high-level abstractions. In VETECS, pages 1 \u2013 5, 05 2009.\ndoi:10.1109/VETECS.2009.5073891.\n[5] J. Maroto, E. Delso, J. F\u00e9lez, and J. Cabanellas. Real-time traffic simulation with a microscopic\nmodel. Intelligent Transportation Systems, IEEE Transactions on, 7:513 \u2013 527, 01 2007. doi:\n10.1109/TITS.2006.883937.\n[6] F. E. Gunawan. Two-vehicle dynamics of the car-following models on realistic driving con-\ndition. Journal of Transportation Systems Engineering and Information Technology, 12(2):\n67 \u2013 75, 2012. ISSN 1570-6672. doi:https://doi.org/10.1016/S1570-6672(11)60194-3. URL\nhttp://www.sciencedirect.com/science/article/pii/S1570667211601943.\n[7] J. Erdmann. Lane-changing model in sumo. In Proceedings of the SUMO2014 Modeling\nMobility with Open Data, volume 24, 05 2014.\n[8] M. Wrenninge and J. Unger. Synscapes: A photorealistic synthetic dataset for street scene\nparsing. Arxiv, Oct 2018. URL http://arxiv.org/abs/1810.08705.\n[9] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A\nlarge collection of synthetic images for semantic segmentation of urban scenes. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pages 3234\u20133243, 2016.\n[10] M. Johnson-Roberson, C. Barto, R. Mehta, S. N. Sridhar, K. Rosaen, and R. Vasudevan. Driv-\ning in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?\nIn IEEE International Conference on Robotics and Automation, pages 1\u20138, 2017.\n[11] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer\ngames. In ECCV, 2016.\n[12] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. CARLA: An open urban driving\nsimulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 1\u201316, 2017.\n[13] A. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and\nS. Birchfield. Structured domain randomization: Bridging the reality gap by context-aware\nsynthetic data. In ICRA, pages 7249\u20137255, 05 2019. doi:10.1109/ICRA.2019.8794443.\n9\n[14] S. Tan, K. Wong, S. Wang, S. Manivasagam, M. Ren, and R. Urtasun. Scenegen: Learning\nto generate realistic traffic scenes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 892\u2013901, 2021.\n[15] L. Feng, Q. Li, Z. Peng, S. Tan, and B. Zhou. Trafficgen: Learning to generate diverse and\nrealistic traffic scenarios. arXiv preprint arXiv:2210.06609, 2022.\n[16] D. Xu, Y. Chen, B. Ivanovic, and M. Pavone. BITS: Bi-level imitation for traffic simulation. In\nIEEE International Conference on Robotics and Automation (ICRA), London, UK, May 2023.\nURL https://arxiv.org/abs/2208.12403.\n[17] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.\nZero-shot text-to-image generation. In International Conference on Machine Learning, pages\n8821\u20138831. PMLR, 2021.\n[18] Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank,\net al. Noise2music: Text-conditioned music generation with diffusion models. arXiv preprint\narXiv:2302.03917, 2023.\n[19] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or. Motionclip: Exposing\nhuman motion generation to clip space. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXII, pages 358\u2013374.\nSpringer, 2022.\n[20] J. Gao, T. Shen, Z. Wang, W. Chen, K. Yin, D. Li, O. Litany, Z. Gojcic, and S. Fidler. Get3d:\nA generative model of high quality 3d textured shapes learned from images. In Advances In\nNeural Information Processing Systems, 2022.\n[21] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n[22] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[23] OpenAI. Gpt-4 technical report, 2023.\n[24] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[25] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Re-\nthinking the role of demonstrations: What makes in-context learning work? In EMNLP, 2022.\n[26] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[27] B. Varadarajan, A. Hefny, A. Srivastava, K. S. Refaat, N. Nayakanti, A. Cornman, K. Chen,\nB. Douillard, C. P. Lam, D. Anguelov, and B. Sapp. Multipath++: Efficient information fusion\nand trajectory aggregation for behavior prediction, 2021.\n[28] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end\nobject detection with transformers. In Computer Vision \u2013 ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I, page 213\u2013229, Berlin, Heidel-\nberg, 2020. Springer-Verlag. ISBN 978-3-030-58451-1. doi:10.1007/978-3-030-58452-8_13.\nURL https://doi.org/10.1007/978-3-030-58452-8_13.\n[29] B. Cheng, A. G. Schwing, and A. Kirillov. Per-pixel classification is not all you need for\nsemantic segmentation. In NeurIPS, 2021.\n10\n[30] Waymo LLC. Waymo open dataset: An autonomous driving dataset, 2019.\n[31] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2017.\n[32] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel two-sample\ntest. The Journal of Machine Learning Research, 13(1):723\u2013773, 2012.\n[33] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou. Metadrive: Composing diverse driving\nscenarios for generalizable reinforcement learning. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2022.\n[34] M. Treiber, A. Hennecke, and D. Helbing. Congested traffic states in empirical observations\nand microscopic simulations. Physical Review E, 62(2):1805\u20131824, aug 2000. doi:10.1103/\nphysreve.62.1805. URL https://doi.org/10.1103%2Fphysreve.62.1805.\n11\nAppendix\nIn the appendix, we provide implementation and experiment details of our method as well as addi-\ntional results. In Section A and Section B, we show details of Interpreter and Generator respec-\ntively. In Section C we present implementation details of our experiments. Finally, in Section D, we\nshow more results on applications ablation study, as well as additional qualitative results.\nA\nInterpreter\nA.1\nStructured representation details\nThe map specific zm is a 6-dim integer vector. Its first four dimensions denote the number of lanes\nin each direction (set as north for the ego vehicle). The fifth dimension represents the discretized\ndistance in 5-meter intervals from the map center to the nearest intersection (0-5, 5-10...). The sixth\ndimension indicates the ego vehicle\u2019s lane id, starting from 1 for the rightmost lane.\nFor agent i, the agent-specific za\ni is an 8-dim integer vector describing this agent relative to the ego\nvehicle. The first dimension denotes the quadrant index (1-4), where quadrant 1 represents the front-\nright of the ego vehicle. The second dimension is the discretized distance to the ego vehicle with\na 20m interval, and the third denotes orientation (north, south, east, west). The fourth dimension\nindicates discretized speed, set in 2.5m/s intervals. The last four dimensions describe actions over\nthe next four seconds (one per second) chosen from a discretized set of seven possible actions: lane\nchanges (left/right), turns (left/right), moving forward, accelerating, decelerating, and stopping.\nA.2\nGeneration prompts\nThe scenario generation prompt used for Interpreter consists of several sections:\n1. Task description: simple description of task of scenario generation and output formats.\n2. Chain-of-thought prompting [26]: For example, \"summarize the scenario in short sen-\ntences\", \"explain for each group of vehicles why they are put into the scenario\".\n3. Description of structured representation: detailed description for each dimension of the\nstructured representation. We separately inform the model Map and Actor formats.\n4. Guidelines: several generation instructions. For example, \"Focus on realistic action gener-\nation of the motion to reconstruct the query scenario\".\n5. Few-shot examples: A few input-output examples. We provide a Crash Report example.\nWe show the full prompt below:\nPrompt 1: Full prompt for Interpreter scenario generation.\nYou are a very faithful format converter that translate natrual language traffic scenario\ndescriptions to a fix-form format to appropriately describe the scenario with motion\naction. You also need to output an appropriate map description that is able to support\nthis scenario. Your ultimate goal is to generate realistic traffic scenarios that\nfaithfully represents natural language descriptions normal scenes that follows the\ntraffic rule.\nAnswer with a list of vectors describing the attributes of each of the vehicles in the\nscenario.\nDesired format:\nSummary: summarize the scenario in short sentences, including the number of vehicles. Also\nexplain the underlying map description.\nExplaination: explain for each group of vehicles why they are put into the scenario and how\nthey fullfill the requirement in the description.\nActor Vector: A list of vectors describing the attributes of each of the vehicles in the\nscenario, only output the values without any text:\n- \u2019V1\u2019: [,,,,,,,]\n- \u2019V2\u2019: [,,,,,,,]\n12\n- \u2019V3\u2019: [,,,,,,,]\nMap Vector: A vector describing the map attributes, only output the values without any text:\n- \u2019Map\u2019: [,,,,,]\nMeaning of the Actor vector attribute:\n- dim 0: \u2019pos\u2019: [-1,3] - whether the vehicle is in the four quadrant of ego vechile in the\norder of [0 - \u2019front left\u2019, 1 - \u2019back left\u2019, 2- \u2019back right\u2019, 3 - \u2019front right\u2019]. -1 if\nthe vehicle is the ego vehicle.\n- dim 1: \u2019distance\u2019: [0,3] - the distance range index of the vehicle towards the ego vehicle\n; range is from 0 to 72 meters with 20 meters interval. 0 if the vehicle is the ego\nvehicle. For example, if distance value is 15 meters, then the distance range index is\n0.\n- dim 2: \u2019direction\u2019: [0,3] - the direction of the vehicle relative to the ego vehicle, in\nthe order of [0- \u2019parallel_same\u2019, 1-\u2019parallel_opposite\u2019, 2-\u2019perpendicular_up\u2019, 3-\u2019\nperpendicular_down\u2019]. 0 if the vehicle is the ego vehicle.\n- dim 3: \u2019speed\u2019: [0,20] - the speed range index of the vehicle; range is from 0 to 20 m/s\nwith 2.5 m/s interval. For example, 20m/s is in range 8, therefore the speed value is\n8.\n- dim 4-7: \u2019action\u2019: [0,7] - 4-dim, generate actions into the future 4 second with each two\nactions have a time interval of 1s (4 actions in total), the action ids are [0 - \u2019stop\n\u2019, 1 - \u2019turn left\u2019, 2 - \u2019left lane change\u2019, 3- \u2019decelerate\u2019, 4- \u2019keep_speed\u2019, 5-\u2019\naccelerate\u2019,\n6-\u2019right lane change\u2019, 7-\u2019turn right\u2019].\nMeaning of the Map attributes:\n- dim 0-1: \u2019parallel_lane_cnt\u2019: 2-dim. The first dim is the number of parallel same-\ndirection lanes of the ego lane, and the second dim is the number of parallel opposite-\ndirection lanes of the ego lane.\n- dim 2-3: \u2019perpendicular_lane_cnt\u2019: 2-dim. The first dim is the number of perpendicular\nupstream-direction lanes, and the second dim is the number of perpendicular downstream-\ndirection lanes.\n- dim 4: \u2019dist_to_intersection\u2019: 1-dim. the distance range index of the ego vehicle to the\nintersection center in the x direction, range is from 0 to 72 meters with 5 meters\ninterval. -1 if there is no intersection in the scenario.\n- dim 5: \u2019lane id\u2019: 1-dim. the lane id of the ego vehicle, counting from the rightmost lane\nof the same-direction lanes, starting from 1. For example, if the ego vehicle is in the\nrightmost lane, then the lane id is 1; if the ego vehicle is in the leftmost lane,\nthen the lane id is the number of the same-direction lanes.\nTransform the query sentence to the Actor Vector strictly following the rules below:\n- Focus on realistic action generation of the motion to reconstruct the query scenario.\n- Follow traffic rules to form a fundamental principle in most road traffic systems to\nensure safety and smooth operation of traffic. You should incorporate this rule into\nthe behavior of our virtual agents (vehicles).\n- Traffic rule: in an intersection, when the vehicles on one side of the intersection are\ncrossing, the vehicles on the other side of the intersection should be waiting. For\nexample, if V1 is crossing the intersection and V2 is on the perpendicular lane, then\nV2 should be waiting.\n- For speed and distance, convert the unit to m/s and meter, and then find the interval\nindex in the given range.\n- Make sure the position and direction of the generated vehicles are correct.\n- Describe the initialization status of the scenario.\n- During generation, the number of the vehicles is within the range of [1, 32].\n- Always generate the ego vehicle first (V1).\n- Always assume the ego car is in the center of the scene and is driving in the positive x\ndirection.\n- In the input descriptions, regard V1, Vehicle 1 or Unit #1 as the ego vehicle. All the\nother vehicles are the surrounding vehicles. For example, for \"Vehicle 1 was traveling\nsouthbound\", the ego car is Vehicle 1.\n- If the vehicle is stopping, its speed should be 0m/s (index 0). Also, if the first action\nis \u2019stop\u2019, then the speed should be 0m/s (index 0).\n- Focus on the interactions between the vehicles in the scenario.\n- Regard the last time stamp as the time stamp of 5 second into the future.\nGenerate the Map Vector following the rules below:\n- If there is vehicle turning left or right, there must be an intersection ahead.\n- Should at least have one lane with the same-direction as the ego lane; i.e., the first dim\nof Map should be at least 1. For example, if this is a one way two lane road, then the\nfirst dim of Map should be 2.\n- Regard the lane at the center of the scene as the ego lane.\n- Consider the ego car\u2019s direction as the positive x direction. For example, for \"V1 was\ntraveling northbound in lane five of a five lane controlled access roadway\", there\nshould be 5 lanes in the same direction as the ego lane.\n13\n- The generated map should strictly follow the map descriptions in the query text. For\nexample, for \"Vehicle 1 was traveling southbound\", the ego car should be in the\nsouthbound lane.\n- If there is an intersection, there should be at least one lane in either the upstream or\ndownstream direction.\n- If there is no intersection, the distance to the intersection should be -1.\n- There should be vehicle driving vertical to the ego vehicle in the scene only when there\nis an intersection in the scene. For example, when the road is just two-way, there\nshould not be any vehicle driving vertical to the ego vehicle.\n- If no intersection is mentioned, generate intersection scenario randomly with real-world\nstatistics.\nQuery: The crash occurred during daylight hours on a dry, bituminous, two-lane roadway under\nclear skies.\nThere was one northbound travel lane and one southbound travel lane with\nspeed limit of 40 km/h (25 mph).\nThe northbound lane had a -3.6 percent grade and\nthe southbound lane had a +3.6 percent grade.\nBoth travel lanes were divided by a\ndouble yellow line. A 2016 Mazda CX-3 (V1) was in a parking lot attempting to execute a\nleft turn to travel south.\nA 2011 Dodge Charger (V2/police car) was traveling north\nresponding to an emergency call with lights sirens activated. V1 was in a parking lot (\nfacing west) and attempted to enter the roadway intending to turn left.\nAs V1 entered\nthe roadway it was impacted on the left side by the front of V2 (Event 1).\nV1 then\nrotated counterclockwise and traveled off the west road edge and impacted an embankment\nwith its front left bumper (Event 2).\nAfter initial impact V2 continued on in a\nnorthern direction and traveling to final rest approximately 40 meters north of impact\narea facing north in the middle of the roadway.\nV1 and V2 were towed from the scene\ndue to damage.\nSummary: V1 attempts to turn left from a parking lot onto a two-lane roadway and is struck\nby V2, a police car traveling north with lights and sirens activated. There are 2\nvehicles in this scenario. This happens on a parking lot to a two-lane two-way road\nwith intersection.\nExplanation:\n- V1 (ego vehicle) is attempting to turn left from a parking lot onto the roadway. We cannot\nfind V1\u2019s speed in the query. Because V1 tries to turn left, its initial speed should\nbe set low. We set V1\u2019s speed as 5 m/s, which has the index of 2. V1 turns left, so its\nactions are all 1 (turn left).\n- V2 is a police car traveling north with lights and sirens activated. As V1 is turning left\n, 5 seconds before the crash, V1 is facing west and V2 is coming from northbound,\ncrossing the path of V1. In the coordinates of V1 (which is facing west initially), V2\ncomes from the front and is on the left side. Hence, V2\u2019s position is \"front left\" (3).\nAs V1 is facing west and V2 facing north, V2 is moving in the perpendicular down\ndirection with V1. Therefore its direction is 3 (perpendicular_down). We cannot find V2\n\u2019s speed in the query. Because V2 is a police car responding to an emergency call, we\nassume V2\u2019s init speed is 10 m/s (index 4). Given this speed, V2\u2019s distance to V1 is 10\nm/s * 5s = 50m (index 10). V2 keeps going straight, so its actions are all 4 (keep\nspeed).\n- Map: V1 tries to turn left from a partking lot onto a two-lane roadway. There are a one-\nway exit lane from parking lot (one same-direction parallel) and the ego vehicle is in\nthe left turn lane with lane id 1. On the perpendicular side there is a two-lane\nroadway. V1 is about to turn left, so the distance to the intersection is set to be 10m\n(index 2).\nActor Vector:\n- \u2019V1\u2019: [-1, 0, 0, 2, 1, 1, 1, 1]\n- \u2019V2\u2019: [0, 10, 3, 4, 4, 4, 4, 4]\nMap Vector:\n- \u2019Map\u2019: [1, 0, 1, 1, 2, 1]\nQuery: INSERT_QUERY_HERE\nOutput:\nA.3\nInstructional editing prompts\nWe also provide Interpreter another prompt for instructional scenario editing. This prompt follow\na similar structure to the generation prompt. We mainly adopt the task description, guidelines,\nand examples to scenario editing tasks. Note that for the instructional editing task, we change the\ndistance interval (second dimension) of agent-specific za\ni from 20 meters to 5 meters. This is to\nensure the unedited agents stay in the same region before and after editing.\n14\nWe show the full prompt below:\nPrompt 2: Full prompt for Interpreter instructional scenario editing.\nYou are a traffic scenario editor that edit fix-form traffic scenario descriptions according\nto the user\u2019s natural language instructions.\nThe user will input a fix-form traffic scenario description as well as the map description.\nThe user also a natural language instruction to modify the scenario. You need to output\na fix-form traffic scenario that is modified according to the instruction.\nInput format:\n- V1: [,,,,,,,]\n- V2: [,,,,,,,]\n- V3: [,,,,,,,]\n- Map: [,,,,,]\nInstruction: natural language instruction to modify the scenario.\nOutput format:\nSummary: summarize the scenario in short sentences. summarize the user instruction, and\nindicate which part of the scenario should be modified.\nExplaination: explain step-by-step how each part of the scenario is modified.\nActor Vector: A list of vectors describing the attributes of each of the vehicles. Only the\nvehicles that are modified should be included in the output.\n- V2: [,,,,,,,]\nMeaning of the Actor vector attribute:\n- dim 0: \u2019pos\u2019: [-1,3] - whether the vehicle is in the four quadrant of ego vechile in the\norder of [0 - \u2019front left\u2019, 1 - \u2019back left\u2019, 2- \u2019back right\u2019, 3 - \u2019front right\u2019]. -1 if\nthe vehicle is the ego vehicle.\n- dim 1: \u2019distance\u2019: [0,14] - the distance range index of the vehicle towards the ego\nvehicle; range is from 0 to 72 meters with 5 meters interval. 0 if the vehicle is the\nego vehicle.\n- dim 2: \u2019direction\u2019: [0,3] - the direction of the vehicle relative to the ego vehicle, in\nthe order of [0- \u2019parallel_same\u2019, 1-\u2019parallel_opposite\u2019, 2-\u2019perpendicular_up\u2019, 3-\u2019\nperpendicular_down\u2019]. 0 if the vehicle is the ego vehicle.\n- dim 3: \u2019speed\u2019: [0,8] - the speed range index of the vehicle; range is from 0 to 20 m/s\nwith 2.5 m/s interval. For example, 20m/s is in range 8, therefore the speed value is\n8.\n- dim 4-7: \u2019action\u2019: [0,7] - 4-dim, generate actions into the future 4 second with each two\nactions have a time interval of 1s (4 actions in total), the action ids are [0 - \u2019stop\n\u2019, 1 - \u2019turn left\u2019, 2 - \u2019left lane change\u2019, 3- \u2019decelerate\u2019, 4- \u2019keep_speed\u2019, 5-\u2019\naccelerate\u2019,\n6-\u2019right lane change\u2019, 7-\u2019turn right\u2019].\nMeaning of the Map attributes:\n- dim 0-1: \u2019parallel_lane_cnt\u2019: 2-dim. The first dim is the number of parallel same-\ndirection lanes of the ego lane, and the second dim is the number of parallel opposite-\ndirection lanes of the ego lane.\n- dim 2-3: \u2019perpendicular_lane_cnt\u2019: 2-dim. The first dim is the number of perpendicular\nupstream-direction lanes, and the second dim is the number of perpendicular downstream-\ndirection lanes.\n- dim 4: \u2019dist_to_intersection\u2019: 1-dim. the distance range index of the ego vehicle to the\nintersection center in the x direction, range is from 0 to 72 meters with 5 meters\ninterval. -1 if there is no intersection in the scenario.\n- dim 5: \u2019lane id\u2019: 1-dim. the lane id of the ego vehicle, counting from the rightmost lane\nof the same-direction lanes, starting from 1. For example, if the ego vehicle is in the\nrightmost lane, then the lane id is 1; if the ego vehicle is in the leftmost lane,\nthen the lane id is the number of the same-direction lanes.\nFollow the instructions below:\n- \u2019V1\u2019 is the ego vehicle, and the other vehicles are the surrounding vehicles.\n- The user will input a fix-form traffic scenario description as well as the map description\n. The user also an natural language instruction to modify the scenario. You need to\noutput a fix-form traffic scenario that is modified according to the instruction.\n- First figure out which part of the scenario should be modified according to the\ninstruction. For example, if the instruction is \"the vehicle in front of me should turn\nleft\", then the vehicle in front of the ego vehicle should be modified.\nInput:\nActor vector:\n- V1: [-1,\n0,\n0,\n0,\n4,\n4,\n4,\n4]\n- V2: [ 2,\n1,\n0,\n1,\n4,\n4,\n4,\n4]\n15\n- V3: [ 3,\n3,\n0,\n1,\n4,\n4,\n4,\n0]\n- V4: [ 3,\n4,\n0,\n8,\n4,\n4,\n2,\n0]\n- V5: [ 0,\n9,\n1,\n8, -1,\n4,\n5, -1]\n- V6: [ 3,\n5,\n0,\n0,\n0,\n0,\n0,\n0]\n- V7: [ 0,\n9,\n3,\n0,\n0,\n0,\n0,\n0]\n- V8: [ 3, 10,\n3,\n3,\n4,\n5,\n1,\n0]\n- V9: [ 0, 10,\n3,\n0,\n0,\n0,\n0, -1]\n- V10: [ 3, 10,\n2,\n0,\n0,\n0,\n0, -1]\n- V11: [ 3, 11,\n2,\n0,\n0,\n0,\n0,\n0]\n- V12: [ 3, 11,\n2,\n0,\n0,\n7,\n0,\n0]\n- Map: [4, 3, 2, 3, 6, 4]\nInstruction: move the vehicle behind the ego vehicle to the opposite lane and move faster.\nOutput:\nSummary: The instruction is to move the vehicle behind the ego vehicle to the opposite lane\nand accelerate. First find which vehicle is behind the ego vehicle. There are only 1\nvechile behind the ego vehicle, that is V2 (with position=2, indicating on the right\nback side of the ego vehicle). Therefore, the vehicle V2 should be modified.\nExplaination: The vehicle V2 is modified to move to the opposite lane and accelerate. The\nvehicle V2 is in the right back side of the ego vehicle, and the ego vehicle is in the\nrightmost lane of the same-direction lanes. Therefore, the vehicle V2 should move to\nthe leftmost lane of the opposite-direction lanes. Therefore, V2\u2019s direction should be\nopposite to the ego vehicle, changed to 1 (parallel_opposite). In this lane, V2 should\nbe moved to the left back of the ego car, its position should be changed to 1. V2\nshould move faster, its speed should be changed to 10 (25 m/s).\nActor vector:\n- V2: [ 1,\n1,\n1, 10,\n4,\n4,\n4,\n4]\nInstruction: remove all the vehicles on the front of the ego car and moving in the same\ndirection.\nOutput:\nSummary: The instruction is to remove all the vehicles on the front of the ego car and\nmoving in the same direction. First find which vehicles are on the front of the ego\nvehicle. V3-V12 are all on the front of the ego vehicle. Then, only V3, V4 and V6 has\nthe same direction as the ego vehicle (0). Therefore, V3, V4 and V6 should be removed.\nExplaination: V3, V4, V6 are on the front of the ego vehicle and moving in the same\ndirection. V3, V4 and V6 are removed from the scenario.\nActor vector:\n- V3: removed.\n- V4: removed.\n- V6: removed.\nInput: INSERT_QUERY_HERE\nOutput:\nB\nGenerator\nB.1\nTraining objectives\nIn the main paper, we show the full training objective of Generator as:\nL(p, \u03c4) = Lposition(p, \u03c4) + Lattr(p, \u03c4) + Lmotion(p, \u03c4).\n(2)\nIn this section, we provide details of each loss function. We first pair each agent \u02c6ai in p with a\nground-truth agent ai in \u03c4 based on the sequential ordering of the structured agent representation\nza. Assume there are in total N agents in the scenario.\nFor Lposition, we use cross-entropy loss between the per-lane categorical output \u02c6p and the ground-\ntruth lane segment id l. Specifically, we compute it as\nLposition(p, \u03c4) =\nN\nX\ni=1\n\u2212 log \u02c6pi(li),\n(3)\nwhere li is the index of the lane segment that the i-th ground-truth agent ai is on.\n16\nFor Lattr, we use a negative log-likelihood loss, computed using the predicted GMM on the ground-\ntruth attribute values. Recall that for each attribute of agent i, we use an MLP to predict the param-\neters of a GMM model [\u00b5i, \u03a3i, \u03c0i]. Here, we use these parameters to construct a GMM model and\ncompute the likelihood of ground-truth attribute values. Specifically, we have\nLattr(p, \u03c4) =\nN\nX\ni=1\n( \u2212 log GMMheading,i(hi) \u2212 log GMMvel,i(veli)\n\u2212 log GMMsize,i(bboxi) \u2212 log GMMpos,i(posi)),\n(4)\nwhere GMMheading,i, GMMvel,i, GMMsize,i, GMMpos,i represent the likelihood function of the pre-\ndicted GMM models of agent i\u2019s heading, velocity, size and position shift. These likelihood values\nare computed using the predicted GMM parameters. Meanwhile, hi, veli, bboxi and posi represent\nthe heading, velocity, size and position shift of the ground-truth agent ai respectively.\nFor Lmotion, we use MSE loss for the predicted trajectory closest to the ground-truth trajectory fol-\nlowing the multi-path motion prediction idea [27]. Recall that for each agent \u02c6ai, we predict K\u2032\ndifferent future trajectories and their probabilities as {pos2:T\ni,k , probi,k}K\u2032\nk=1 = MLP(q\u2217\ni ). For each\ntimestamp t, post\ni,k contains the agent\u2019s position and heading. We assume the trajectory of ground-\ntruth agent ai is pos2:T\ni,\u2217 . We can compute the index k\u2217 of the closest trajectory from the K\u2032 pre-\ndictions as k\u2217 = arg mink\nPT\nt=2(post\ni,k \u2212 post\ni,\u2217)2. Then, we compute the motion loss for agent i\nas:\nLmotion,i = \u2212 log probi,k\u2217 +\nT\nX\nt=2\n(post\ni,k\u2217 \u2212 post\ni,\u2217)2,\n(5)\nwhere we encourage the model to have a higher probability for the cloest trajectory k\u2217 and reduce\nthe distance between this trajectory with the ground truth. The full motion loss is simply:\nLmotion(p, \u03c4) =\nN\nX\ni\nLmotion,i\n(6)\nwhere we sum over all the motion losses for each predicted agent in p.\nC\nExperiment Details\nC.1\nBaseline implementation\nTrafficGen [15].\nWe use the official implementation1. For a fair comparison, we train its Initial-\nization and Trajectory Generation modules on our dataset for 100 epochs with batch size 64. We\nmodify T = 50 in the Trajectory Generation to align with our setting. We use the default values for\nall the other hyper-parameters. During inference, we enforce TrafficGen to generate N vehicles by\nusing the result of the first N autoregressive steps of the Initialization module.\nMotionCLIP [19].\nThe core idea of MotionCLIP is to learn a shared space for the interested\nmodality embedding (traffic scenario in our case) and text embedding. Formally, this model contains\na scenario encoder E, a text encoder \u02c6E, and a scenario decoder D. For each example of scene-\ntext paired data (\u03c4, L, m), we encode scenario and text separately with their encoders z = E(\u03c4),\n\u02c6z = \u02c6E(L). Then, the decoder takes z and m and output a scenario p = D(z, m). MotionCLIP\ntrains the network with Lrec to reconstruct the scenario from the latent code:\nLrec = Lposition(p, \u03c4) + Lattr(p, \u03c4) + Lmotion(p, \u03c4),\n(7)\nwhere we use the same set of loss functions as ours (Equation 2). On the other hand, MotionCLIP\naligns the embedding space of the scenario and text with:\n1https://github.com/metadriverse/trafficgen\n17\nLalign = 1 \u2212 cos(z, \u02c6z),\n(8)\nwhich encourages the alignment of scenario embedding z and text embedding \u02c6z. The final loss\nfunction is therefore\nL = Lrec + \u03bbLalign,\n(9)\nwhere we set \u03bb = 100.\nDuring inference, given an input text L and a map m, we can directly use the text encoder to obtain\nlatent code and decode a scenario from it, formally \u03c4 = D( \u02c6E(L), m).\nFor the scenario encoder E, we use the same scenario encoder as in [15], which is a 5-layer multi-\ncontext gating (MCG) block [27] to encode the scene input \u03c4 and outputs z \u2208 R1024 with the\ncontext vector output c of the final MCG block. For text encoder \u02c6E, we use the sentence embedding\nof the fixed GPT-2 model. For the scenario decoder D, we modify our Generator to take in latent\nrepresentation z with a dimension of 1024 instead of our own structured representation. Because D\ndoes not receive the number of agents as input, we modify Generator to produce the N = 32 agents\nfor every input and additionally add an MLP decoder to predict the objectiveness score of each\noutput agent. Here objectiveness score is a binary probability score indicating whether we should\nput each predicted agent onto the final scenario or not. During training, for computation of Lrec, we\nuse Hungarian algorithm to pair ground-truth agents with the predicted ones. We then supervise the\nobjectiveness score in a similar way as in DETR.\nNote that we need text-scenario paired data to train MotionCLIP. To this end, we use a rule-based\nmethod to convert a real dataset \u03c4 to a text L. This is done by describing different attributes of the\nscenario with language. Similar to our Attribute Description dataset, in each text, we enumerate\nthe scenario properties 1) sparsity; 2) position; 3) speed and 4) ego vehicle\u2019s motion. Here is one\nexample: \"the scene is very dense; there exist cars on the front left of ego car; there is no car on the\nback left of ego car; there is no car on the back right of ego car; there exist cars on the front right of\nego car; most cars are moving in fast speed; the ego car stops\".\nWe transform every scenario in our dataset into a text with the format as above. We then train\nMotionCLIP on our dataset with the same batch size and number of iterations as LCTGen.\nC.2\nMetric\nWe show how to compute MMD in this section. Specifically, MMD measures the distance between\ntwo distributions q and p.\nMMD2(p, q) =Ex,x\u2032\u223cp[k(x, x\u2032)] + Ey,y\u2032\u223cq[k(y, y\u2032)]\n\u2212 2Ex\u223cp,y\u223cq[k(x, y)],\n(10)\nwhere k is the kernel function (a Gaussian kernel in this work). We use Gaussian kernel in this work.\nFor each pair of real and generated data (\u03c4, \u02c6\u03c4), we compute the distribution difference between them\nper attribute.\nC.3\nDataset\nCrash Report.\nWe use 38 cases from the CIREN dataset [1] from the NHTSA crash report search\nengine. Each case contains a long text description of the scenario as well as a PDF diagram showing\nthe scenario. Because the texts are very long and require a long time for humans to comprehend,\nin our human study, along with each text input, we will also show the diagram of the scenario as a\nreference. We show example crash reports in Section D.4. We also refer the reader to the NHTSA\nwebsite 2 to view some examples of the crash report.\n2https://crashviewer.nhtsa.dot.gov/CIREN/Details?Study=CIREN&CaseId=11\n18\nAttribute Description.\nWe create text descriptions that highlight various attributes of a traffic\nscenario. Specifically, we use the following attributes and values:\n1. Sparsity: \"the scenario is {nearly empty/sparse/with medium density/very dense}\".\n2. Position: \"there are only vehicles on the {left/right/front/back} side(s) of the center car\" or\n\"there are vehicles on different sides of the center car\".\n3. Speed: \"most cars are moving in {slow/medium/fast} speed\" or \"most cars are stopping\".\n4. Ego-vehicle motion: \"the center car {stops/moves straight/turns left/turns right}\".\nFigure A1: Human study user interface.\nWe create sentences describing each of the single attributes with all the possible values. We also\ncompose more complex sentences by combining 2,3 or 4 attributes together with random values for\neach of them. In total, we created 40 cases for human evaluation. Please refer to Section D.4 for\nsome example input texts from this dataset.\nC.4\nHuman study\nWe conduct the human study to access how well the generated scenario matches the input text. We\nshowcase the user interface of our human study in Figure A1. We compose the output of two models\nwith the same text input in random order and ask the human evaluator to judge which one matches\nthe text description better. Then, we also ask them to give each output a 1-5 score. We allow the\nuser to select \"unsure\" for the first question.\n19\nWe invite 12 human evaluators for this study, and each of them evaluated all the 78 cases we pro-\nvided. We ensure the human evaluators do not have prior knowledge of how different model works\non these two datasets. On average, the human study takes about 80 minutes for each evaluator.\nC.5\nQualitative result full texts\nIn Figure 4 and Figure A2, we show 5 examples of the output of our model on Crash Report data\non the first row. Recall that the texts we show in the figures are the summary from our Interpreter\ndue to space limitations. We show the full input text for each example in this section.\nText 1: Full texts of examples in Figure 4 .\nFigure 4 Column 1 (CIREN ID 594):\n\"This crash occurred during daylight hours on a dry, bituminous divided trafficway (median\nstrip without positive barrier) under clear skies.\nThere were four east travel lanes\n(two through lanes, one left turn and one right turn) and four west travel lanes (two\nthrough lanes, one left and one right).\nThe east lanes have a slight right curve and\nthe west lanes curve slightly to the left.\nBoth east/west travel lanes were level\ngrade at point of impact and divided by a grass median.\nThe speed limit at this\nlocation is 80km/h (50 mph).\nThe intersecting north/south roadway consisted of one\nnorth travel lane and three south travel lanes (one through lanes, one left and one\nright).\nThese travel lanes were divided by a raised concrete median on the northern\nside of the intersection.\nThis intersection is controlled by overhead traffic signals.\nA 2017 Dodge Grand Caravan (V1) was traveling east in the left turn lane and a 2006\nNissan Sentra (V2) was traveling west in the left through lane.\nAs V1 was traveling\neast it attempted to execute a left turn to travel north when its front bumper impacted\nthe front bumper of V2 (Event 1).\nAfter initial impact, V1 rotated counterclockwise\napproximately 80 degrees before traveling to its final resting position in the middle\nof the intersection facing north.\nV2 was traveling west in the left through lane and\nattempting to travel through the intersection when its front bumper impacted the front\nbumper of V1.\nAfter initial impact V2 rotated clockwise approximately 20 degrees\nbefore traveling to its final resting position in the middle of the intersection facing\nnorthwest.\nV1 and V2 were towed from the scene due to damage sustained in the crash.\"\nFigure 4 Column 2 (CIREN ID 31):\n\"A 2016 Kia Sedona minivan (V1) was traveling southwest in the right lane of three.\nA 2015\nChevrolet Silverado cab chassis pickup (V2) was ahead of V1 in the right lane.\nV2 was\na working vehicle picking up debris on the highway in a construction zone.\nThe driver\nof V2 stopped his vehicle in the travel lane.\nThe driver of V1 recognized an\nimpending collision and applied the brakes while steering left in the last moment\nbefore impact.\nV1 slid approximately three meters before the front of V1 struck the\nback plane of V2 in a rear-end collision with full engagement across the striking\nplanes (Event 1).\nBoth vehicles came to rest approximately two meters from impact.\nV1\nwas towed due to damage while V2 continued in service.\"\nFigure A2 Row 1 Column 1 (CIREN ID 77):\n\"A 2017 Chevrolet Malibu LS sedan (V1) was traveling southeast in the right lane cresting a\nhill. A 1992 Chevrolet C1500 pickup (V2) was traveling northwest in the second lane\ncresting the same hill. Vehicle 2 crossed left across the center turn lane, an oncoming\nlane, and then into V1\\u2019s oncoming lane of travel. Vehicle 1 and Vehicle 2\ncollided in a head-on, offset-frontal configuration (Event 1). Vehicle 1 attempted to\nsteer left just before impact, focusing the damage to the middle-right of its front\nplane. Both vehicles rotated a few degrees clockwise before coming to rest in the\nroadway, where they were towed from the scene due to damage.\"\nFigure A2 Row 1 Column 2 (CIREN ID 33):\n\"This two-vehicle collision occurred during the pre-dawn hours (dark, street lights present)\nof a fall weekday at the intersection of two urban roadways. The crash only involved\nthe eastern leg of the intersection. The westbound lanes of the eastern leg consisted\nof four westbound lanes that included a right turn lane, two through lanes, and a left\nturn lane.\nThe three eastbound lanes of the eastern leg consisted of a merge lane from\nthe intersecting road and two through-lanes. The roadway was straight with a speed\nlimit of 89 kmph (55 mph), and the intersection was controlled by overhead, standard\nelectric, tri-colored traffic signals. At the time of the crash, the weather was clear\nand the roadway surfaces were dry. As Vehicle 1 approached the intersection, its driver\ndid not notice the vehicles stopped ahead at the traffic light. The traffic signal\nturned green and Vehicle 2 began to slowly move forward. The frontal plane of Vehicle 1\nstruck the rear plane of Vehicle 2 (Event 1). Both vehicles came to rest in the left\nthrough-lane of the westbound lane facing in a westerly direction. Vehicle 1 was towed\nfrom the scene due to damage sustained in the crash. Vehicle 2 was not towed nor\n20\ndisabled. The driver of Vehicle 2 was transported by land to a local trauma center and\nwas treated and released.\"\nFigure A2 Row 1 Column 3 (CIREN ID 56):\n\"A 2013 Honda CR-V utility vehicle (V1) was traveling west in the right lane approaching an\nintersection. A 2003 Chevrolet Silverado 1500 pickup (V2) was stopped facing north at a\nstop sign.\nVehicle 2 proceeded north across the intersection and was struck on the\nright plane by the front plane of V1 (Event 1). The impact caused both vehicles to\ntravel off the northwest corner of the intersection, where they came to rest. Both\nvehicles were towed due to damage.\"\nD\nAdditional Results\nD.1\nControllable self-driving policy evaluation\nWe show how LCTGen can be utilized to generate interesting scenarios for controllable self-driving\npolicy evaluation. Specifically, we leverage LCTGen to generate traffic scenario datasets possessing\ndiverse properties, which we then use to assess self-driving policies under various situations. For\nthis purpose, we input different text types into LCTGen: 1) Crash Report, the real-world crash report\ndata from CIREN; 2) Traffic density specification, a text that describes the scenario as \"sparse\",\n\"medium dense\", or \"very dense\". For each type of text, we generate 500 traffic scenarios for\ntesting. Additionally, we use 500 real-world scenarios from the Waymo Open dataset.\nWe import all these scenarios into an interactive driving simulation, MetaDrive [33]. We evaluate the\nperformance of the IDM [34] policy and a PPO policy provided in MetaDrive. In each scenario, the\nself-driving policy replaces the ego-vehicle in the scenario and aims to reach the original end-point\nof the ego vehicle, while all other agents follow the trajectory set out in the original scenario. We\nshow the success rate and collision rate of both policies in Table A1. Note that both policies experi-\nence significant challenges with the Crash Report scenarios, indicating that these scenarios present\ncomplex situations for driving policies. Furthermore, both policies exhibit decreased performance in\ndenser traffic scenarios, which involve more intricate vehicle interactions. These observations give\nbetter insight about the drawbacks of each self-driving policy. This experiment showcases LCTGen\nas a valuable tool for generating traffic scenarios with varying high-level properties, enabling a more\ncontrolled evaluation of self-driving policies.\nTest Data\nIDM [34]\nPPO (MetaDrive) [33]\nSuccess (%)\nCollision (%)\nSuccess (%)\nCollision (%)\nReal\n93.60\n3.80\n69.32\n14.67\nLCTGen + Crash Report [1]\n52.35\n39.89\n25.78\n27.98\nLCTGen + \"Sparse\"\n91.03\n8.21\n41.03\n21.06\nLCTGen + \"Medium\"\n84.47\n12.36\n43.50\n26.67\nLCTGen + \"Dense\"\n68.12\n19.26\n38.89\n32.41\nTable A1: Controllable self-driving policy evaluation.\nD.2\nText-conditioned simulation qualitative results\nWe show the more qualitative results of text-conditioned simulation in Figure A2. Here, the upper 3\nexamples are from the Crash Report dataset, the lower 3 examples are from the Attribute Description\ndataset.\nD.3\nHuman study statistics\nScore distribution.\nWe show the human evaluation scores of the two datasets in Figure A3. We\nobserve that our method is able to reach significantly better scores from human evaluators.\n21\n\u201cV1 approaches an intersection and \ndoes not notice stopped vehicles \nahead.  V1 strikes the rear of V2.\u201d\n\u201cV1 is traveling southeast and V2 is \ntraveling northwest, both cresting a \nhill. V1 crosses into V2's lane, resulting \nin a head-on collision.\u201d\n\u201cthe scene is very dense. there are only \nvehicles on the left side of the center \ncar. most cars are moving in fast \nspeed. the ego-vehicle  turns right.\u201d\n\u201cthe scene is sparse. there are vehicles \non different sides of the center car. \nmost cars are moving in medium \nspeed. the center car moves straight\u201d\n\u201cmost cars are moving in slow speed.\u201d\n\u201cV1 is traveling west in the right lane \napproaching an intersection, while V2 is \nstopped facing north at a stop sign. V2 \nproceeds north and is struck by V1.\u201d\nFigure A2: Qualitative results on text-conditioned generation.\n1\n2\n3\n4\n5\nScore\n0\n10\n20\n30\n40\n50\n60\nProportion (%)\nLCTGen\nMotionCLIP\nTrafficGen\n(a) Crash Report\n1\n2\n3\n4\n5\nScore\n0\n10\n20\n30\n40\n50\nProportion (%)\nLCTGen\nMotionCLIP\nTrafficGen\n(b) Attribute Description\nFigure A3: Human study score distribution.\n22\nOurs better\nNeutral\nOthers better\n0\n20\n40\n60\n80\nProportion (%)\nOurs vs. TrafficGen\nOurs better\nNeutral\nOthers better\n0\n20\n40\n60\n80\nProportion (%)\nOurs vs. MotionCLIP\nMC Better\nNeutral\nTG better\n0\n10\n20\n30\n40\n50\nProportion (%)\nMotionCLIP vs. TrafficGen\n(a) Crash Report\nOurs better\nNeutral\nOthers better\n0\n20\n40\n60\nProportion (%)\nOurs vs. TrafficGen\nOurs better\nNeutral\nOthers better\n0\n20\n40\n60\nProportion (%)\nOurs vs. MotionCLIP\nMC Better\nNeutral\nTG better\n0\n10\n20\n30\n40\nProportion (%)\nMotionCLIP vs. TrafficGen\n(b) Attribute Description\nFigure A4: Human study A/B test distribution.\nMethod\nCrash Report\nAttribute Description\nAvg. Score\nHuman Std.\nAvg. Score\nHuman Std.\nTrafficGen [15]\n1.58\n0.64\n2.43\n0.72\nMotionCLIP [19]\n1.65\n0.67\n2.10\n0.64\nLCTGen\n3.86\n0.87\n4.29\n0.65\nTable A2: Human study average score and variance.\nA/B test distribution.\nWe show the distribution of A/B test result for each pair of methods in Fig-\nure A4. Note that our method is chosen significantly more frequenly as the better model compared\nwith other models. We also observe that TrafficGen is slightly better than MotionCLIP in Attribute\nDescription dataset, while the two models achieve similar results in Crash Report.\nHuman score variance.\nWe show the variance of quality score across all human evaluators in\nTable A2. Specifically, for each case, we compute the standard deviation across all the human eval-\nuators for this case. Then, we average all the standard deviation values across all the cases and show\nin the table as \"Human Std.\". This value measures the variance of score due to human evaluators\u2019\nsubjective judgement differences. According to the average score and human variance shown in the\ntable, we conclude that our model outperforms the compared methods with high confidence levels.\nD.4\nInterpreter input-output examples\nHere we show the full-text input and output of Interpreter for four examples in Figure 4. Specifi-\ncally, we show two examples from Crash Report and two examples from Attribute Descriptions.\nText 2: Input-output examples of Interpreter.\nFigure 4 Column 1 (CIREN ID 594):\nInput:\n\"This crash occurred during daylight hours on a dry, bituminous divided trafficway (median\nstrip without positive barrier) under clear skies.\nThere were four east travel lanes\n(two through lanes, one left turn and one right turn) and four west travel lanes (two\nthrough lanes, one left and one right).\nThe east lanes have a slight right curve and\n23\nthe west lanes curve slightly to the left.\nBoth east/west travel lanes were level\ngrade at point of impact and divided by a grass median.\nThe speed limit at this\nlocation is 80km/h (50 mph).\nThe intersecting north/south roadway consisted of one\nnorth travel lane and three south travel lanes (one through lanes, one left and one\nright).\nThese travel lanes were divided by a raised concrete median on the northern\nside of the intersection.\nThis intersection is controlled by overhead traffic signals.\nA 2017 Dodge Grand Caravan (V1) was traveling east in the left turn lane and a 2006\nNissan Sentra (V2) was traveling west in the left through lane.\nAs V1 was traveling\neast it attempted to execute a left turn to travel north when its front bumper impacted\nthe front bumper of V2 (Event 1).\nAfter initial impact, V1 rotated counterclockwise\napproximately 80 degrees before traveling to its final resting position in the middle\nof the intersection facing north.\nV2 was traveling west in the left through lane and\nattempting to travel through the intersection when its front bumper impacted the front\nbumper of V1.\nAfter initial impact V2 rotated clockwise approximately 20 degrees\nbefore traveling to its final resting position in the middle of the intersection facing\nnorthwest.\nV1 and V2 were towed from the scene due to damage sustained in the crash.\"\nOutput:\nSummary: V1 is traveling east in the left turn lane and attempts to turn left when it\ncollides with V2 traveling west in the left through lane. There are 2 vehicles in this\nscenario. This happens at the intersection of an eight-lane divided trafficway and a\nfour-lane north/south roadway, controlled by traffic signals.\nExplanation:\n- V1 (ego vehicle) is traveling east in the left turn lane and attempts to turn left. We\ncannot find V1\u2019s speed in the query. Because V1 tries to turn left, its initial speed\nshould be set low. We set V1\u2019s speed as 5 m/s, which has the index of 2. V1 turns left,\nso its actions are all 1 (turn left).\n- V2 is traveling west in the left through lane. As V1 is turning left, 5 seconds before the\ncrash, V2 is coming from the opposite direction (westbound), crossing the path of V1.\nIn the coordinates of V1 (which is facing east initially), V2 comes from the front and\nis on the left side. Hence, V2\u2019s position is \\\"front left\\\" (0). As V1 is facing east\nand V2 facing west, V2 is moving in the parallel opposite direction with V1. Therefore\nits direction is 1 (parallel_opposite). We cannot find V2\u2019s speed in the query. Because\nV2 is traveling west and hit by V1 5 seconds later, we assume V2\u2019s init speed is 8 m/s\n(index 3). Given this speed, V2\u2019s distance to V1 is 8m/s * 5s = 40m (index 8). V2\nkeeps going straight, so its actions are all 4 (keep speed).\n- Map: This happens at the intersection of an eight-lane divided trafficway (4 lanes for\neastbound and westbound traffic) and a four-lane north/south roadway (1 lane for\nnorthbound and 3 lanes for southbound traffic). The ego vehicle is in the left turn-\nonly lane with lane id 4. There is an intersection with a four-lane north/south road\nahead. V1 is about to cross the interaction, therefore the distance to the intersection\nis with 15 meter index 3.\nActor Vector:\n- \u2019V1\u2019: [-1, 0, 0, 2, 1, 1, 1, 1]\n- \u2019V2\u2019: [0, 8, 1, 3, 4, 4, 4, 4]\nMap Vector:\n- \u2019Map\u2019: [4, 4, 1, 3, 3, 4]\nFigure A2 Row 1 Column 2 (CIREN ID 33):\nInput:\n\"This two-vehicle collision occurred during the pre-dawn hours (dark, street lights present)\nof a fall weekday at the intersection of two urban roadways. The crash only involved\nthe eastern leg of the intersection. The westbound lanes of the eastern leg consisted\nof four westbound lanes that included a right turn lane, two through lanes, and a left\nturn lane.\nThe three eastbound lanes of the eastern leg consisted of a merge lane from\nthe intersecting road and two through-lanes. The roadway was straight with a speed\nlimit of 89 kmph (55 mph), and the intersection was controlled by overhead, standard\nelectric, tri-colored traffic signals. At the time of the crash, the weather was clear\nand the roadway surfaces were dry. As Vehicle 1 approached the intersection, its driver\ndid not notice the vehicles stopped ahead at the traffic light. The traffic signal\nturned green and Vehicle 2 began to slowly move forward. The frontal plane of Vehicle 1\nstruck the rear plane of Vehicle 2 (Event 1). Both vehicles came to rest in the left\nthrough-lane of the westbound lane facing in a westerly direction. Vehicle 1 was towed\nfrom the scene due to damage sustained in the crash. Vehicle 2 was not towed nor\ndisabled. The driver of Vehicle 2 was transported by land to a local trauma center and\nwas treated and released.\"\nOutput:\nSummary: V1 approaches an intersection and does not notice stopped vehicles ahead. The\ntraffic light turns green, and V2 starts moving forward slowly. V1 strikes the rear of\nV2. There are 2 vehicles in this scenario. This happens at the intersection of two\nurban roadways with multiple lanes and traffic signals.\n24\nExplanation:\n- V1 (ego vehicle) is approaching the intersection and does not notice the stopped vehicles\nahead. We cannot find V1\u2019s speed in the query. Because V1 is moving and does not stop,\nwe assume V1\u2019s initial speed is 10 m/s (index 4). V1 keeps going straight, so its\nactions are all 4 (keep speed).\n- V2 is stopped at the traffic light and starts moving forward slowly when the light turns\ngreen. As V1 is moving straight and hits V2 from behind, 5 seconds before the crash, V2\nis in front of V1. Hence, V2\u2019s position is \\\"front right\\\" (3). As V1 and V2 are both\nmoving westbound, V2 is moving in the parallel same direction with V1. Therefore its\ndirection is 0 (parallel_same). V2\u2019s speed is slow, so we set it to 2.5 m/s (index 1).\nGiven this speed, V2\u2019s distance to V1 is 10m/s * 5s - 2.5m/s * 5s = 37.5m (index 7). V2\nkeeps going straight, so its actions are all 4 (keep speed).\n- Map: This happens at the intersection of two urban roadways with multiple lanes and\ntraffic signals. The ego vehicle is in the left through-lane of the westbound lane with\nlane id 3. There are 4 westbound lanes and 3 eastbound lanes. The distance of the ego\nvehicle to the intersection is set to be 10m/s * 5s = 50m (index 10).\nActor Vector:\n- \u2019V1\u2019: [-1, 0, 0, 4, 4, 4, 4, 4]\n- \u2019V2\u2019: [3, 7, 0, 1, 4, 4, 4, 4]\nMap Vector:\n- \u2019Map\u2019: [4, 3, 0, 0, 10, 3]\nFigure A2 Row 2 Column 1:\nInput:\n\"the scene is with sparse density. there are vehicles on different sides of the center car.\nmost cars are moving in medium speed. the center car moves straight\"\nOutput:\nSummary: The ego vehicle (V1) moves straight at a medium speed in a sparse traffic scenario.\nThere are vehicles on different sides of the ego vehicle, corresponding to the sparse\ndensity. We generate 4 vehicles in the scene on a six-lane two-way road. The ego car is\nplaced on the third lane from the right.\nExplanation:\n- V1 (ego vehicle) is in the center and moves straight. Because V1 moves with a medium speed\n, its speed is set to 4 (medium speed) and action indices are 4 (keep speed).\n- V2 is in the closest distance range (index 0). It is moving in the same direction as the\nego vehicle (index 0) and is located in the back right (index 2) of the ego vehicle. It\nis moving with a medium speed (index 4), and its actions are all to keep speed (index\n4).\n- V3 is in a further distance range (index 1). It is moving in the opposite direction of the\nego vehicle (index 1), therefore it is on the left side of the ego vehicle, locating\nin the back left (index 1) of the ego vehicle. It is moving with a medium speed (index\n4), and its actions are all to keep speed (index 4).\n- V4 is in the farthest distance range (index 2). It is moving in the same direction as the\nego vehicle (index 0) and is located in the front right (index 3) of the ego vehicle.\nIt is moving with a medium speed (index 4), and its actions are all to keep speed (\nindex 4).\n- V5 is in the farthest distance range (index 2). It is moving in the opposite direction of\nthe ego vehicle (index 1), therefore it is on the left side of the ego vehicle,\nlocating in the front left (index 0) of the ego vehicle. It is moving with a medium\nspeed (index 4), and its actions are all to keep speed (index 4).\n- Map: Given the sparse density and the presence of vehicles on different sides of the ego\nvehicle, we assume this happens on a 6-lane two-way road. The ego vehicle is in the\nthird lane from the right, with lane id 3. There is no intersection nearby, indicated\nby -1.\nActor Vector:\n- \u2019V1\u2019: [-1, 0, 0, 4, 4, 4, 4, 4]\n- \u2019V2\u2019: [2, 0, 0, 4, 4, 4, 4, 4]\n- \u2019V3\u2019: [1, 1, 1, 4, 4, 4, 4, 4]\n- \u2019V4\u2019: [3, 2, 0, 4, 4, 4, 4, 4]\n- \u2019V5\u2019: [0, 2, 1, 4, 4, 4, 4, 4]\nMap Vector:\n- \u2019Map\u2019: [3, 3, 0, 0, -1, 3]\nFigure 4 Column 3:\nInput:\n\"the scene is with sparse density. there are only vehicles on the back side of the center\ncar. most cars are moving in fast speed. the center car turns left\"\nOutput:\n25\nSummary: The ego vehicle (V1) turns left at a fast speed in a sparse traffic scenario. All\nother vehicles are located behind the ego vehicle, which corresponds to the scenario\u2019s\nsparse density. We generate 3 vehicles in the scene on a six-lane two-way road with an\nintersection in front. The ego car is placed on the left-turn lane.\nExplanation:\n- V1 (ego vehicle) is in the center and turns left. Because V1 moves with a fast speed, its\nspeed is set to 6 (fast speed) and action indices are 1 (turn left).\n- V2-V3 are in the closest distance range (index 0). They are moving in the same direction\nas the ego vehicle (index 0). Because the ego car is on the left-turn lane, they are\nboth located in the back right (index 2) of the ego vehicle. They are both moving with\na fast speed (index 6 or 7), and their actions are all to keep speed (index 4).\n- V4 is in a further distance range (index 1). It is moving in the opposite direction of the\nego vehicle (index 1), therefore it is on the left side of the ego vehicle, locating\nin the back left (index 1) of the ego vehicle. It is moving with a fast speed (index 6)\n, in the opposite direction as the ego vehicle (index 1). Its actions are all to keep\nspeed (index 4).\n- Map: Given the sparse density and the presence of vehicles behind the ego vehicle, we\nassume this happens on a 6-lane two-way road. The ego vehicle is in the left-turn lane,\nwith lane id 3. There is an intersection 10 meters ahead (index 2) as the ego vehicle\nis turning left.\nActor Vector:\n- \u2019V1\u2019: [-1, 0, 0, 6, 1, 1, 1, 1]\n- \u2019V2\u2019: [2, 0, 0, 6, 4, 4, 4, 4]\n- \u2019V3\u2019: [2, 0, 0, 7, 4, 4, 4, 4]\n- \u2019V4\u2019: [1, 1, 1, 6, 4, 4, 4, 4]\nMap Vector:\n- \u2019Map\u2019: [3, 3, 2, 2, 2, 3]\nD.5\nAttribute Description Result Split\nMethod\nDensity\nPosition\nSpeed\nEgo-car Motion\nTrafficGen [15]\n2.75\n2.03\n2.34\n2.27\nMotionCLIP [19]\n1.89\n2.24\n1.91\n1.78\nLCTGen\n4.24\n4.28\n4.38\n4.40\nTable A3: Human study result split analysis on Attribute Description scores.\nWe generate the Attribute Description dataset with different attributes. In this section, we split the\nmatching score result for the full dataset into different attributes. We show the result in Table A3.\nWe observe our method has nearly identical performance over all the attributes. TrafficGen the best\nresults with Density, while MotionCLIP performs the best with Position.\nD.6\nFull Ablation Study\nMethod\nInitialization\nMotion\nPos\nHeading\nSpeed\nSize\nmADE\nmFDE\nSCR\nw/o Quad.\n0.092\n0.122\n0.076\n0.124\n2.400\n4.927\n8.087\nw/o Dist.\n0.071\n0.124\n0.073\n0.121\n1.433\n3.041\n6.362\nw/o Ori.\n0.067\n0.132\n0.082\n0.122\n1.630\n3.446\n7.300\nw/o Speed\n0.063\n0.120\n0.104\n0.122\n2.611\n5.188\n7.150\nw/o Action\n0.067\n0.128\n0.173\n0.128\n2.188\n5.099\n7.146\nw/o xi\n0.067\n0.133\n0.076\n0.124\n1.864\n3.908\n5.929\nw/o GMM\n0.064\n0.128\n0.078\n0.178\n1.606\n3.452\n8.216\nLCTGen\n0.062\n0.115\n0.072\n0.120\n1.329\n2.838\n6.700\nTable A4: Ablation study of LCTGen\nIn our main paper, we split the ablation study into two different groups. Here we show the full results\nof all the ablated methods in Table A4. We additionally show the effect of 1) using the learnable\nquery xi and 2) using the GMM prediction for attributes.\n26\nD.7\nInstructional traffic scenario editing\nInput\n\u201cadd more cars on the left\u201d\n\u201clet the left car do lane change\u201d \u201cmake it sparser and speed up\u201d\n\u201cremove cars on the right\u201d\nFigure A5: Instructional editing on a real-world scenario\nWe show another example of instructional traffic scenario editing in Figure A5. Different from\nthe compound editing in Figure 5 in the main paper, here every example is edited from the input\nscenario.\n27\n"
  }
]