[
  {
    "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
    "link": "https://arxiv.org/pdf/2310.01798.pdf",
    "upvote": "30",
    "text": "LARGE LANGUAGE MODELS CANNOT SELF-CORRECT\nREASONING YET\nJie Huang1,2\u2217 Xinyun Chen1\u2217 Swaroop Mishra1 Huaixiu Steven Zheng1 Adams Wei Yu1\nXinying Song1 Denny Zhou1\n1Google DeepMind\n2University of Illinois at Urbana-Champaign\njeffhj@illinois.edu, {xinyunchen, dennyzhou}@google.com\nABSTRACT\nLarge Language Models (LLMs) have emerged as a groundbreaking technology\nwith their unparalleled text generation capabilities across various applications.\nNevertheless, concerns persist regarding the accuracy and appropriateness of their\ngenerated content. A contemporary methodology, self-correction, has been pro-\nposed as a remedy to these issues. Building upon this premise, this paper critically\nexamines the role and efficacy of self-correction within LLMs, shedding light on\nits true potential and limitations. Central to our investigation is the notion of in-\ntrinsic self-correction, whereby an LLM attempts to correct its initial responses\nbased solely on its inherent capabilities, without the crutch of external feedback.\nIn the context of reasoning, our research indicates that LLMs struggle to self-\ncorrect their responses without external feedback, and at times, their performance\neven degrades after self-correction. Drawing from these insights, we offer sugges-\ntions for future research and practical applications in this field.\n1\nINTRODUCTION\nThe rapid advancements in the domain of artificial intelligence have ushered in the era of Large\nLanguage Models (LLMs). These models, characterized by their expansive parameter counts and\nunparalleled capabilities in text generation, have showcased promising results across a multitude\nof applications (Chowdhery et al., 2023; Anil et al., 2023; OpenAI, 2023, inter alia). However,\nconcerns about their accuracy, reasoning capabilities, and the safety of their generated content have\ndrawn significant attention from the community (Bang et al., 2023; Alkaissi & McFarlane, 2023;\nZheng et al., 2023; Shi et al., 2023; Carlini et al., 2021; Huang et al., 2022; Shao et al., 2023; Li\net al., 2023; Wei et al., 2023; Zhou et al., 2023b; Zou et al., 2023, inter alia).\nAmidst this backdrop, the concept of \u201cself-correction\u201d has emerged as a promising solution, where\nLLMs refine their responses based on feedback to their previous outputs (Madaan et al., 2023;\nWelleck et al., 2023; Shinn et al., 2023; Kim et al., 2023; Bai et al., 2022; Ganguli et al., 2023;\nGao et al., 2023; Paul et al., 2023; Chen et al., 2023b; Pan et al., 2023, inter alia). However, the\nunderlying mechanics and efficacy of self-correction in LLMs remain underexplored. A fundamen-\ntal question arises: If an LLM possesses the ability to self-correct, why doesn\u2019t it simply offer the\ncorrect answer in its initial attempt? This paper delves deeply into this paradox, critically examining\nthe self-correction capabilities of LLMs, with a particular emphasis on reasoning (Wei et al., 2022;\nZhou et al., 2023b; Huang & Chang, 2023).\nTo study this, we first define the concept of intrinsic self-correction, a scenario wherein the model\nendeavors to rectify its initial responses based solely on its inherent capabilities, without the crutch\nof external feedback. Such a setting is crucial because high-quality external feedback is often un-\navailable in many real-world applications. Moreover, it is vital to understand the intrinsic capabilities\nof LLMs. Contrary to the optimism surrounding self-correction (Madaan et al., 2023; Kim et al.,\n2023; Shinn et al., 2023; Pan et al., 2023, inter alia), our findings indicate that LLMs struggle to\nself-correct their reasoning in this setting. In most instances, the performance after self-correction\n\u2217Equal contribution.\n1\narXiv:2310.01798v2  [cs.CL]  14 Mar 2024\nPublished as a conference paper at ICLR 2024\neven deteriorates. This observation is in contrast to prior research such as Kim et al. (2023); Shinn\net al. (2023). Upon closer examination, we observe that the improvements in these studies result\nfrom using oracle labels to guide the self-correction process, and the improvements vanish when\noracle labels are not available.\nBesides the reliance on oracle labels, we also identify other issues in the literature regarding mea-\nsuring the improvement achieved by self-correction. First, we note that self-correction, by design,\nutilizes multiple LLM responses, thus making it crucial to compare it to baselines with equivalent\ninference costs. From this perspective, we investigate multi-agent debate (Du et al., 2023; Liang\net al., 2023) as a means to improve reasoning, where multiple LLM instances (can be multiple\ncopies of the same LLM) critique each other\u2019s responses. However, our results reveal that its effi-\ncacy is no better than self-consistency (Wang et al., 2022) when considering an equivalent number\nof responses, highlighting the limitations of such an approach.\nAnother important consideration for self-correction involves prompt design. Specifically, each self-\ncorrection process involves designing prompts for both the initial response generation and the self-\ncorrection steps. Our evaluation reveals that the self-correction improvement claimed by some exist-\ning work stems from the sub-optimal prompt for generating initial responses, where self-correction\ncorrects these responses with more informative instructions about the initial task in the feedback\nprompt. In such cases, simply integrating the feedback into the initial instruction can yield better\nresults, and self-correction again decreases performance.\nIn light of our findings, we provide insights into the nuances of LLMs\u2019 self-correction capabilities\nand initiate discussions to encourage future research focused on exploring methods that can gen-\nuinely correct reasoning.\n2\nBACKGROUND AND RELATED WORK\nWith the LLM evolution, the notion of self-correction gained prominence. The discourse on self-\ncorrection pivots around whether these advanced models can recognize the correctness of their out-\nputs and provide refined answers (Bai et al., 2022; Madaan et al., 2023; Welleck et al., 2023, inter\nalia). For example, in the context of mathematical reasoning, an LLM might initially solve a com-\nplex problem but make an error in one of the calculation steps. In an ideal self-correction scenario,\nthe model is expected to recognize the potential mistake, revisit the problem, correct the error, and\nconsequently produce a more accurate solution.\nYet, the definition of \u201cself-correction\u201d varies across the literature, leading to ambiguity. A pivotal\ndistinction lies in the source of feedback (Pan et al., 2023), which can purely come from the LLM, or\ncan be drawn from external inputs. Internal feedback relies on the model\u2019s inherent knowledge and\nparameters to reassess its outputs. In contrast, external feedback incorporates inputs from humans,\nother models (Wang et al., 2023b; Paul et al., 2023, inter alia), or external tools and knowledge\nsources (Gou et al., 2023; Chen et al., 2023b; Olausson et al., 2023; Gao et al., 2023, inter alia).\nIn this work, we focus on examining the self-correction capability of LLMs for reasoning. Reasoning\nis a fundamental aspect of human cognition, enabling us to understand the world, draw inferences,\nmake decisions, and solve problems. To enhance the reasoning performance of LLMs, Kim et al.\n(2023); Shinn et al. (2023) use oracle labels about the answer correctness to guide the self-correction\nprocess. However, in practice, high-quality external feedback such as answer correctness is often\nunavailable. For effective self-correction, the ability to judge the correctness of an answer is crucial\nand should ideally be performed by the LLM itself. Consequently, our focus shifts to self-correction\nwithout any external or human feedback. We term this setting intrinsic self-correction. For brevity,\nunless explicitly stated otherwise (e.g., self-correction with oracle feedback), all references to \u201cself-\ncorrection\u201d in the remainder of this paper pertain to intrinsic self-correction.\nIn the following sections, we will evaluate a variety of existing self-correction techniques. We\ndemonstrate that existing techniques actually decrease reasoning performance when oracle labels\nare not used (Section 3), perform worse than methods without self-correction when utilizing the\nsame number of model responses (Section 4), and lead to less effective outcomes when using infor-\nmative prompts for generating initial responses (Section 5). We present an overview of issues in the\nevaluation setups of previous LLM self-correction works in Table 1, with detailed discussions in the\ncorresponding sections.\n2\nPublished as a conference paper at ICLR 2024\nTable 1: Summary of issues in previous LLM self-correction evaluation.\nMethod\nIssue\nRCI (Kim et al., 2023); Reflexion (Shinn et al., 2023)\nUse of oracle labels (Section 3)\nMulti-Agent Debate (Du et al., 2023)\nUnfair comparison to self-consistency (Section 4)\nSelf-Refine (Madaan et al., 2023)\nSub-optimal prompt design (Section 5)\n3\nLLMS CANNOT SELF-CORRECT REASONING INTRINSICALLY\nIn this section, we evaluate existing self-correction methods and compare their performance with\nand without oracle labels regarding the answer correctness.\n3.1\nEXPERIMENTAL SETUP\nBenchmarks.\nWe use datasets where existing self-correction methods with oracle labels have\ndemonstrated significant performance improvement, including\n\u2022 GSM8K (Cobbe et al., 2021): GSM8K comprises a test set of 1,319 linguistically diverse grade\nschool math word problems, curated by human problem writers. There is a notable improvement\nof approximately 7% as evidenced by Kim et al. (2023) after self-correction.\n\u2022 CommonSenseQA (Talmor et al., 2019): This dataset offers a collection of multi-choice questions\nthat test commonsense reasoning. An impressive increase of around 15% is showcased through\nthe self-correction process, as demonstrated by Kim et al. (2023). Following Kojima et al. (2022);\nKim et al. (2023), we utilize the dev set for our evaluation, which encompasses 1,221 questions.\n\u2022 HotpotQA (Yang et al., 2018): HotpotQA is an open-domain multi-hop question answering\ndataset.\nShinn et al. (2023) demonstrate significant performance improvement through self-\ncorrection. We test models\u2019 performance in a closed-book setting and evaluate them using the\nsame set as Shinn et al. (2023). This set contains 100 questions, with exact match serving as the\nevaluation metric.\nTest Models and Setup. We first follow Kim et al. (2023); Shinn et al. (2023) to evaluate the per-\nformance of self-correction with oracle labels, using GPT-3.5-Turbo (gpt-3.5-turbo-0613)\nand GPT-4 accessed on 2023/08/29.\nFor intrinsic self-correction,\nto provide a more\nthorough analysis, we also evaluate GPT-4-Turbo (gpt-4-1106-preview) and Llama-2\n(Llama-2-70b-chat) (Touvron et al., 2023). For GPT-3.5-Turbo, we employ the full evalua-\ntion set. For other models, to reduce the cost, we randomly sample 200 questions for each dataset\n(100 for HotpotQA) for testing. We prompt the models to undergo a maximum of two rounds of\nself-correction. We use a temperature of 1 for GPT-3.5-Turbo and GPT-4, and a temperature of 0 for\nGPT-4-Turbo and Llama-2, to provide evaluation across different decoding algorithms.\nPrompts. Following Kim et al. (2023); Shinn et al. (2023), we apply a three-step prompting strategy\nfor self-correction: 1) prompt the model to perform an initial generation (which also serves as the\nresults for Standard Prompting); 2) prompt the model to review its previous generation and produce\nfeedback; 3) prompt the model to answer the original question again with the feedback.\nFor our experiments, we mostly adhere to the prompts from the source papers. For GSM8K and\nCommonSenseQA, we integrate format instructions into the prompts of Kim et al. (2023) to facilitate\na more precise automatic evaluation (detailed prompts can be found in Appendix A). For HotpotQA,\nwe use the same prompt as Shinn et al. (2023). We also assess the performance of various self-\ncorrection prompts for intrinsic self-correction. For example, we use \u201cAssume that this answer\ncould be either correct or incorrect. Review the answer carefully and report any serious problems\nyou find.\u201d as the default feedback prompt for the evaluation on GPT-4-Turbo and Llama-2.\n3.2\nRESULTS\nSelf-Correction with Oracle Labels. Following previous works (Kim et al., 2023; Shinn et al.,\n2023), we use the correct label to determine when to stop the self-correction loop. This means we\n3\nPublished as a conference paper at ICLR 2024\nTable 2: Results of GPT-3.5 and GPT-4 on reasoning benchmarks with oracle labels.\nGSM8K\nCommonSenseQA\nHotpotQA\nGPT-3.5\nStandard Prompting\n75.9\n75.8\n26.0\nSelf-Correct (Oracle)\n84.3\n89.7\n29.0\nGPT-4\nStandard Prompting\n95.5\n82.0\n49.0\nSelf-Correct (Oracle)\n97.5\n85.5\n59.0\nTable 3: Results of GPT-3.5 and GPT-4 on reasoning benchmarks with intrinsic self-correction.\n# calls\nGSM8K\nCommonSenseQA\nHotpotQA\nGPT-3.5\nStandard Prompting\n1\n75.9\n75.8\n26.0\nSelf-Correct (round 1)\n3\n75.1\n38.1\n25.0\nSelf-Correct (round 2)\n5\n74.7\n41.8\n25.0\nGPT-4\nStandard Prompting\n1\n95.5\n82.0\n49.0\nSelf-Correct (round 1)\n3\n91.5\n79.5\n49.0\nSelf-Correct (round 2)\n5\n89.0\n80.0\n43.0\nutilize the ground-truth label to verify whether each step\u2019s generated answer is correct. If the answer\nis already correct, no (further) self-correction will be performed. Table 2 summarizes the results\nof self-correction under this setting, showcasing significant performance improvements, consistent\nwith the findings presented in Kim et al. (2023); Shinn et al. (2023).\nHowever, these results require careful consideration. For reasoning tasks, like solving mathematical\nproblems, the availability of oracle labels seems counter-intuitive. If we are already in possession\nof the ground truth, there seems to be little reason to deploy LLMs for problem-solving. Therefore,\nthe results can only be regarded as indicative of an oracle\u2019s performance.\nIntrinsic Self-Correction. Per the above discussion, performance improvements achieved using\noracle labels do not necessarily reflect true self-correction ability. Therefore, we turn our focus to\nthe results in the intrinsic self-correction setting as defined in Section 2. To achieve this, we elimi-\nnate the use of labels, requiring LLMs to independently determine when to stop the self-correction\nprocess, i.e., whether to retain their previous answers.\nTables 3 and 4 report the accuracies and the number of model calls. We observe that, after self-\ncorrection, the accuracies of all models drop across all benchmarks.\nTo provide a more comprehensive assessment, we also design several different self-correction\nprompts to determine if there are better prompts that could enhance reasoning performance.\nNonetheless, as shown in Tables 5 and 6, without the use of oracle labels, self-correction consis-\ntently results in a decrease in performance.\n3.3\nWHY DOES THE PERFORMANCE NOT INCREASE, BUT INSTEAD DECREASE?\nEmpirical Analysis. Figure 1 summarizes the results of changes in answers after two rounds of\nself-correction, with two examples of GPT-3.5 illustrated in Figure 2. For GSM8K, 74.7% of the\ntime, GPT-3.5 retains its initial answer. Among the remaining instances, the model is more likely to\nmodify a correct answer to an incorrect one than to revise an incorrect answer to a correct one. The\nfundamental issue is that LLMs cannot properly judge the correctness of their reasoning. For\nCommonSenseQA, there is a higher chance that GPT-3.5 alters its answer. The primary reason for\nthis is that false answer options in CommonSenseQA often appear somewhat relevant to the ques-\ntion, and using the self-correction prompt might bias the model to choose another option, leading to\na high \u201ccorrect \u21d2 incorrect\u201d ratio. Similarly, Llama-2 also frequently converts a correct answer into\nan incorrect one. Compared to GPT-3.5 and Llama-2, both GPT-4 and GPT-4-Turbo are more likely\nto retain their initial answers. This may be because GPT-4 and GPT-4-Turbo have higher confidence\n4\nPublished as a conference paper at ICLR 2024\nTable 4: Results of GPT-4-Turbo and Llama-2 with intrinsic self-correction.\n# calls\nGSM8K\nCommonSenseQA\nGPT-4-Turbo\nStandard Prompting\n1\n91.5\n84.0\nSelf-Correct (round 1)\n3\n88.0\n81.5\nSelf-Correct (round 2)\n5\n90.0\n83.0\nLlama-2\nStandard Prompting\n1\n62.0\n64.0\nSelf-Correct (round 1)\n3\n43.5\n37.5\nSelf-Correct (round 2)\n5\n36.5\n36.5\nTable 5: Results of GPT-4-Turbo with different feedback prompts.\n# calls\nGSM8K\nCommonSenseQA\nStandard Prompting\n1\n91.5\n84.0\nFeedback Prompt: Assume that this answer could be either correct or incorrect.\nReview the answer carefully and report any serious problems you find.\nSelf-Correct (round 1)\n3\n88.0\n81.5\nSelf-Correct (round 2)\n5\n90.0\n83.0\nFeedback Prompt: Review your previous answer and determine whether it\u2019s correct.\nIf wrong, find the problems with your answer.\nSelf-Correct (round 1)\n3\n90.0\n74.5\nSelf-Correct (round 2)\n5\n90.0\n81.0\nFeedback Prompt: Verify whether your answer is correct, and provide an explanation.\nSelf-Correct (round 1)\n3\n91.0\n81.5\nSelf-Correct (round 2)\n5\n91.0\n83.5\nTable 6: Results of Llama-2 with different feedback prompts.\n# calls\nGSM8K\nCommonSenseQA\nStandard Prompting\n1\n62.0\n64.0\nFeedback Prompt: Assume that this answer could be either correct or incorrect.\nReview the answer carefully and report any serious problems you find.\nSelf-Correct (round 1)\n3\n43.5\n37.5\nSelf-Correct (round 2)\n5\n36.5\n36.5\nFeedback Prompt: Review your previous answer and determine whether it\u2019s correct.\nIf wrong, find the problems with your answer.\nSelf-Correct (round 1)\n3\n46.5\n26.0\nSelf-Correct (round 2)\n5\n30.5\n37.0\nFeedback Prompt: Verify whether your answer is correct, and provide an explanation.\nSelf-Correct (round 1)\n3\n58.0\n24.0\nSelf-Correct (round 2)\n5\n41.5\n43.0\nin their initial answers, or because they are more robust and thus less prone to being biased by the\nself-correction prompt.1\n1We omit the analysis on HotpotQA because the sample size used in the source paper is quite small, which\nmay not produce meaningful statistics.\n5\nPublished as a conference paper at ICLR 2024\nGSM8K (GPT-3.5)\nNo Change\n74.7%\nIncorrect  Incorrect\n8.9%\nIncorrect  Correct\n7.6%\nCorrect  Incorrect\n8.8%\nCommonSenseQA (GPT-3.5)\n42.8%\n11.6%\n5.8%\n39.8%\nGSM8K (GPT-4)\n90.5%\n1.5%\n8.0%\nCommonSenseQA (GPT-4)\n90.5%\n1.5%\n3.0%\n5.0%\nGSM8K (GPT-4-Turbo)\n96.0%\n0.5%\n1.0%\n2.5%\nCommonSenseQA (GPT-4-Turbo)\n88.0%\n1.0%\n5.0%\n6.0%\nGSM8K (Llama-2)\n40.0%\n23.5%\n5.5%\n31.0%\nCommonSenseQA (Llama-2)\n40.0%\n16.5%\n8.0%\n35.5%\nFigure 1: Analysis of the changes in answers after two rounds of self-correction. No Change: The\nanswer remains unchanged; Correct \u21d2 Incorrect: A correct answer is changed to an incorrect one;\nIncorrect \u21d2 Correct: An incorrect answer is revised to a correct one; Incorrect \u21d2 Incorrect: An\nincorrect answer is altered but remains incorrect.\nTerry eats 2 yogurts a day. They are currently on sale at \n4 yogurts for $5.00. How much does he spend on \nyogurt over 30 days?\n... So, Terry spends $\\boxed{75.00}$ on yogurt over 30 \ndays. (Correct)\nReview your previous answer and find problems with \nyour answer.\nUpon reviewing my previous answer, I realized that I \nmade a mistake in calculating Terry's spending on \nyogurt over 30 days. I incorrectly stated that Terry \nspends $2.50 per day for 30 days, resulting in a total of \n$75.00\u2026\nBased on the problems you found, improve your \nanswer.\n... Therefore, the final answer is Terry spends \n$\\boxed{37.50}$ on yogurt over 30 days. (Incorrect)\nChristina is planning a birthday party and needs .75 gift \nbags per invited guest, because 1/4 of attendees don't \nshow up. She invited 16 friends. Gift bags are $2 each. \nHow much will she spend?\n... Therefore, Christina will spend $18 on gift bags. \nAnswer: \\boxed{18}. (Incorrect)\nReview your previous answer and find problems with \nyour answer.\nUpon reviewing my previous answer, I recognize a \nmistake. I incorrectly stated that Christina needs 0.75 \ngift bags per invited guest. The given information states \nthat 1/4 of the attendees won't show up\u2026\nBased on the problems you found, improve your \nanswer.\n... Therefore, Christina will spend $24 on gift bags. \nAnswer: \\boxed{24}. (Correct)\nFigure 2: Examples on GSM8K with GPT-3.5. Left: successful self-correction; Right: failed self-\ncorrection. Full prompts and responses can be viewed in Figures 3 and 4 of Appendix A.\nLet\u2019s take another look at the results presented in Table 2. These results use ground-truth labels to\nprevent the model from altering a correct answer to an incorrect one. However, determining how to\nprevent such mischanges is, in fact, the key to ensuring the success of self-correction.\nIntuitive Explanation. If the model is well-aligned and paired with a thoughtfully designed ini-\ntial prompt, the initial response should already be optimal relative to the prompt and the specific\ndecoding algorithm. Introducing feedback can be viewed as adding an additional prompt, poten-\ntially skewing the model towards generating a response that is tailored to this combined input. In\nan intrinsic self-correction setting, on the reasoning tasks, this supplementary prompt may not offer\nany extra advantage for answering the question. In fact, it might even bias the model away from\nproducing an optimal response to the initial prompt, resulting in a performance drop.\n6\nPublished as a conference paper at ICLR 2024\nTable 7: Results of multi-agent debate and self-consistency.\n# responses\nGSM8K\nStandard Prompting\n1\n76.7\nSelf-Consistency\n3\n82.5\nMulti-Agent Debate (round 1)\n6\n83.2\nSelf-Consistency\n6\n85.3\nMulti-Agent Debate (round 2)\n9\n83.0\nSelf-Consistency\n9\n88.2\nTable 8: Results of Constrained Generation.\n# calls\nCommonGen-Hard\nStandard Prompting*\n1\n44.0*\nSelf-Correct*\n7\n67.0*\nStandard Prompting*\n1\n53.0\nSelf-Correct*\n7\n61.1\nStandard Prompting (ours)\n1\n81.8\nSelf-Correct*\n7\n75.1\n* Prompts and results from Madaan et al. (2023).\n4\nMULTI-AGENT DEBATE DOES NOT OUTPERFORM SELF-CONSISTENCY\nAnother potential approach for LLMs to self-correct their reasoning involves allowing the models\nto critique and debate through multiple model calls (Du et al., 2023; Liang et al., 2023; Chen et al.,\n2023a). Du et al. (2023) implement a multi-agent debate method by leveraging multiple instances of\na single ChatGPT model and demonstrate significant improvements on reasoning tasks. We adopt\ntheir method to test performance on GSM8K. For an unbiased implementation, we use the exact\nsame prompt as Du et al. (2023) and replicate their experiment with the gpt-3.5-turbo-0301\nmodel, incorporating 3 agents and 2 rounds of debate. The only distinction is that, to reduce result\nvariance, we test on the complete test set of GSM8K, compared to their usage of 100 examples. For\nreference, we also report the results of self-consistency (Wang et al., 2022), which prompts models\nto generate multiple responses and performs majority voting to select the final answer.\nTable 7 presents the results. The results indicate that both multi-agent debate and self-consistency\nachieve significant improvements over standard prompting. However, when comparing multi-agent\ndebate to self-consistency, we observe that the performance of multi-agent is only slightly better than\nthat of self-consistency with the same number of agents (3 responses, the baseline also compared\nin Du et al. (2023)). Furthermore, for self-consistency with an equivalent number of responses,\nmulti-agent debate significantly underperforms simple self-consistency using majority voting.\nIn fact, rather than labeling the multi-agent debate as a form of \u201cdebate\u201d or \u201ccritique\u201d, it is more\nappropriate to perceive it as a means to achieve \u201cconsistency\u201d across multiple model generations.\nFundamentally, its concept mirrors that of self-consistency; the distinction lies in the voting mech-\nanism, whether voting is model-driven or purely based on counts. The observed improvement is\nevidently not attributed to \u201cself-correction\u201d, but rather to \u201cself-consistency\u201d. If we aim to argue that\nLLMs can self-correct reasoning through multi-agent debate, it is preferable to exclude the effects\nof selection among multiple generations.\n5\nPROMPT DESIGN ISSUES IN SELF-CORRECTION EVALUATION\nIn Section 3, we observe that although self-correction decreases reasoning performance with all\ntypes of feedback prompts we have evaluated, performance varies with different feedback prompts.\nIn this section, we further emphasize the importance of proper prompt design in generating initial\nLLM responses to fairly measure the performance improvement achieved by self-correction. For\nexample, if a task requires that the model response should meet criteria that can be easily specified\n7\nPublished as a conference paper at ICLR 2024\nin the initial instruction (e.g., the output should contain certain words, the generated code should\nbe efficient, the sentiment should be positive, etc.), instead of including such requirements only\nin the feedback prompt, an appropriate comparison would be to directly and explicitly incorporate\nthese requirements into the prompt for generating initial responses. Otherwise, when the instruction\nfor generating initial predictions is not informative enough, even if the performance improves, it\nis unclear whether the improvement merely comes from more detailed instructions in the feedback\nprompt or from the self-correction step itself.\nTo illustrate such prompt design issues in the self-correction evaluation of some prior work, we\ntake the Constrained Generation task in Madaan et al. (2023) as an example, where the task re-\nquires models to generate coherent sentences using all 20-30 input concepts. The original prompt in\nMadaan et al. (2023) (Figure 7) does not clearly specify that the LLM needs to include all concepts\nin the prompt; thus, they show that self-correction improves task performance by asking the model\nto identify missing concepts and then guiding it to incorporate these concepts through feedback.\nBased on this observation, we add the following instruction \u201cWrite a reasonable paragraph that\nincludes *ALL* of the above concepts\u201d to the prompt for initial response generation (refer to Figure 8\nfor the full prompt). Following Madaan et al. (2023), we use concept coverage as the metric. We\nreference their results and replicate their experiments using gpt-3.5-turbo-0613. Table 8\ndemonstrates that our new prompt, denoted as Standard Prompting (ours), significantly outperforms\nthe results after self-correction of Madaan et al. (2023), and applying their self-correction prompt\non top of model responses from our stronger version of the standard prompting again leads to a\ndecrease in performance.\n6\nCONCLUSION AND DISCUSSION\nOur work shows that current LLMs struggle to self-correct their reasoning without external feed-\nback. This implies that expecting these models to inherently recognize and rectify their reasoning\nmistakes is overly optimistic so far. In light of these findings, it is imperative for the community to\napproach the concept of self-correction with a discerning perspective, acknowledging its potential\nand recognizing its boundaries. By doing so, we can better equip the self-correction technique to\naddress the limitations of LLMs and develop the next generation of LLMs with enhanced capabili-\nties. In the following, we provide insights into scenarios where self-correction shows the potential\nstrengths and offer guidelines on the experimental design of future self-correction techniques to\nensure a fair comparison.\nLeveraging external feedback for correction. In this work, we demonstrate that current LLMs\ncannot improve their reasoning performance through intrinsic self-correction. Therefore, when valid\nexternal feedback is available, it is beneficial to leverage it properly to enhance model performance.\nFor example, Chen et al. (2023b) show that LLMs can significantly improve their code generation\nperformance through self-debugging by including code execution results in the feedback prompt to\nfix issues in the predicted code. In particular, when the problem description clearly specifies the in-\ntended code execution behavior, e.g., with unit tests, the code executor serves as the perfect verifier\nto judge the correctness of predicted programs, while the error messages also provide informative\nfeedback that guides the LLMs to improve their responses. Gou et al. (2023) demonstrate that LLMs\ncan more effectively verify and correct their responses when interacting with various external tools\nsuch as search engines and calculators. Cobbe et al. (2021); Lightman et al. (2023); Wang et al.\n(2023b) train a verifier or a critique model on a high-quality dataset to verify or refine LLM outputs,\nwhich can be used to provide feedback for correcting prediction errors. Besides automatically gen-\nerated external feedback, we also often provide feedback ourselves when interacting with LLMs,\nguiding them to produce the content we desire. Designing techniques that enable LLMs to interact\nwith the external environment and learn from different kinds of available feedback is a promising\ndirection for future work.\nEvaluating self-correction against baselines with comparable inference costs. By design, self-\ncorrection requires additional LLM calls, thereby increasing the costs for encoding and generating\nextra tokens. Section 4 demonstrates that the performance of asking the LLM to produce a final\nresponse based on multiple previous responses, such as with the multi-agent debate approach, is in-\nferior to that of self-consistency (Wang et al., 2022) with the same number of responses. Regarding\n8\nPublished as a conference paper at ICLR 2024\nthis, we encourage future work proposing new self-correction methods to always include an in-depth\ninference cost analysis to substantiate claims of performance improvement. Moreover, strong base-\nlines that leverage multiple model responses, like self-consistency, should be used for comparison.\nAn implication for future work is to develop models with a higher probability of decoding the opti-\nmal solution in their answer distributions, possibly through some alignment techniques. This would\nenable the model to generate better responses without necessitating multiple generations.\nPutting equal efforts into prompt design. As discussed in Section 5, to gain a better understanding\nof the improvements achieved by self-correction, it is important to include a complete task descrip-\ntion in the prompt for generating initial responses, rather than leaving part of the task description for\nthe feedback prompt. Broadly speaking, equal effort should be invested in designing the prompts\nfor initial response generation and for self-correction; otherwise, the results could be misleading.\n7\nLIMITATIONS AND BROADER IMPACT\nAlthough we have conducted a comprehensive evaluation spanning a variety of self-correction strate-\ngies, prompts, and benchmarks, our work focuses on evaluating reasoning of LLMs. Thus, it is\nplausible that there exist self-correction strategies that could enhance LLM performance in other\ndomains. For example, prior works have demonstrated the successful usage of self-correction that\naligns model responses with specific preferences, such as altering the style of responses or enhanc-\ning their safety (Bai et al., 2022; Ganguli et al., 2023; Madaan et al., 2023). A key distinction arises\nin the capability of LLMs to accurately assess their responses in relation to the given tasks. For\nexample, LLMs can properly evaluate whether a response is inappropriate (Ganguli et al., 2023),\nbut they may struggle to identify errors in their reasoning.\nFurthermore, several prior works have already shown that LLM self-correction performance be-\ncomes significantly weaker without access to external feedback (Gou et al., 2023; Zhou et al., 2023a)\nand can be easily biased by misleading feedback (Wang et al., 2023a), which is consistent with our\nfindings in this work. However, we still identified prevailing ambiguity in the wider community.\nSome existing literature may inadvertently contribute to this confusion, either by relegating crucial\ndetails about label usage to less prominent sections or by failing to clarify that their designed self-\ncorrection strategies actually incorporate external feedback. Regarding this, our paper serves as a\ncall to action, urging researchers to approach this domain with a discerning and critical perspective.\nWe also encourage future research to explore approaches that can genuinely enhance reasoning.\nREPRODUCIBILITY STATEMENT\nOur experiments utilize GPT-3.5 and GPT-4, which are accessible via the public API at https:\n//platform.openai.com/docs/models, as well as Llama-2, an open-source model. To\nfacilitate reproducibility, we detail the specific kernels used, e.g., gpt-3.5-turbo-0613, or\nprovide the access times for each experiment. We use prompts from previous works when possible.\nFor our designed prompts, we include the exact prompts in Appendix A.\nACKNOWLEDGEMENT\nWe would like to thank Chen Liang, William Cohen, Uri Alon, and other colleagues at Google\nDeepMind for valuable discussion and feedback.\nREFERENCES\nHussam Alkaissi and Samy I McFarlane. Artificial hallucinations in chatgpt: implications in scien-\ntific writing. Cureus, 15(2), 2023.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\n9\nPublished as a conference paper at ICLR 2024\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-\nlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,\nZiwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of\nchatgpt on reasoning, hallucination, and interactivity. In Proceedings of the 13th International\nJoint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 675\u2013718,\n2023.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\nfrom large language models. In USENIX Security Symposium, volume 6, 2021.\nJustin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference\nimproves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023a.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128, 2023b.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Er-\nica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\nPalm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023. URL http:\n//jmlr.org/papers/v24/22-1144.html.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.\nImprov-\ning factuality and reasoning in language models through multiagent debate.\narXiv preprint\narXiv:2305.14325, 2023.\nDeep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil\u02d9e Luko\u02c7si\u00afut\u02d9e, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for\nmoral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan,\nVincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what\nlanguage models say, using language models. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 16477\u201316508, 2023.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.\nCritic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint\narXiv:2305.11738, 2023.\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A sur-\nvey. In Findings of the Association for Computational Linguistics: ACL 2023. Association for\nComputational Linguistics, 2023.\n10\nPublished as a conference paper at ICLR 2024\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models\nleaking your personal information? In Findings of the Association for Computational Linguis-\ntics: EMNLP 2022, pp. 2038\u20132047, Abu Dhabi, United Arab Emirates, 2022. Association for\nComputational Linguistics.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\nAdvances in Neural Information Processing Systems, 2023.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\n35:22199\u201322213, 2022.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-\nstep jailbreaking privacy attacks on chatgpt. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 4138\u20134153, 2023.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\nagent debate. arXiv preprint arXiv:2305.19118, 2023.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information Processing Systems, 2023.\nTheo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-\nLezama. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896,\n2023.\nOpenAI. Gpt-4 technical report, 2023.\nLiangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang\nWang. Automatically correcting large language models: Surveying the landscape of diverse self-\ncorrection strategies. arXiv preprint arXiv:2308.03188, 2023.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,\nand Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint\narXiv:2304.01904, 2023.\nHanyin Shao, Jie Huang, Shen Zheng, and Kevin Chen-Chuan Chang. Quantifying association\ncapabilities of large language models and its implications on privacy leakage. arXiv preprint\narXiv:2305.12707, 2023.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael\nSch\u00a8arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\nIn International Conference on Machine Learning, pp. 31210\u201331227. PMLR, 2023.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Infor-\nmation Processing Systems, 2023.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers), pp. 4149\u20134158, 2019.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n11\nPublished as a conference paper at ICLR 2024\nBoshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend its belief in truth? evaluating llm\nreasoning via debate. In Findings of the Association for Computational Linguistics: EMNLP\n2023, pp. 11865\u201311881, 2023a.\nTianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O\u2019Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu,\nOlga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A\ncritic for language model generation. arXiv preprint arXiv:2308.04592, 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2022.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training\nfail? arXiv preprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. Generating sequences by learning to self-correct. In The Eleventh International Conference\non Learning Representations, 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, 2018.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. Why does chatgpt fall short in providing\ntruthful answers? ArXiv preprint, abs/2304.10513, 2023.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,\nLinqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code\ninterpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.\nDenny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-\nmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex\nreasoning in large language models. In The Eleventh International Conference on Learning Rep-\nresentations, 2023b.\nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial\nattacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.\n12\nPublished as a conference paper at ICLR 2024\nA\nPROMPTS AND EXAMPLE OUTPUTS\nCan you solve the following math problem? Christina is planning a\nbirthday party and needs .75 gift bags per invited guest, because 1/4 of\nattendees don't show up. She invited 16 friends. Gift bags are $2 each.\nHow much will she spend? Explain your reasoning. Your final answer should\nbe a single numerical number, in the form \\boxed{answer}, at the end of\nyour response.\nIf Christina invited 16 friends, then 1/4 of them won't show up, which\nmeans there will be 16 * 1/4 = 4 friends who won't attend the party. To\ndetermine how many gift bags to buy, we need to subtract the number of\nfriends who won't show up from the total number of invited guests. The\nnumber of invited guests is 16 - 4 = 12. Since Christina needs 0.75 gift\nbags per invited guest, we can multiply this quantity by the number of\ninvited guests to find the total number of gift bags needed, which is\n0.75 * 12 = 9 gift bags. If each gift bag is $2, then Christina will\nspend a total of 2 * 9 = $<<2*9=18>>18 on gift bags. Therefore, Christina\nwill spend $18 on gift bags. Answer: \\boxed{18}. (Incorrect)\nReview your previous answer and find problems with your answer.\nUpon reviewing my previous answer, I recognize a mistake. I incorrectly\nstated that Christina needs 0.75 gift bags per invited guest. The given\ninformation states that 1/4 of the attendees won't show up, so the\ncorrect calculation should take this into account. To calculate the\nnumber of gift bags needed, we will multiply the number of invited guests\nby the ratio of attendees who will actually show up (75%). The number of\ninvited guests is 16, and if 1/4 of them won't show up, then the ratio of\nattendees who will show up is 1 - 1/4 = 3/4. Multiplying the number of\ninvited guests by the ratio of attendees who will show up gives us: 16 *\n(3/4) = 12 gift bags needed. Since each gift bag costs $2, the total\namount Christina will spend on gift bags is: 12 * $2 = $<<12*2=24>>24.\nBased on the problems you found, improve your answer. Please reiterate\nyour answer, with your final answer a single numerical number, in the\nform \\boxed{answer}.\nUpon reviewing my previous answer, I recognize a mistake. I incorrectly\nstated that Christina needs 0.75 gift bags per invited guest. The given\ninformation states that 1/4 of the attendees won't show up, so the\ncorrect calculation should take this into account. To calculate the\nnumber of gift bags needed, we will multiply the number of invited guests\nby the ratio of attendees who will actually show up (75%). The number of\ninvited guests is 16, and if 1/4 of them won't show up, then the ratio of\nattendees who will show up is 1 - 1/4 = 3/4. Multiplying the number of\ninvited guests by the ratio of attendees who will show up gives us: 16 *\n(3/4) = 12 gift bags needed. Since each gift bag costs $2, the total\namount Christina will spend on gift bags is: 12 * $2 = $<<12*2=24>>24.\nTherefore, Christina will spend $24 on gift bags. Answer: \\boxed{24}.\n(Correct)\nFigure 3: Example on GSM8K where self-correction changes an incorrect answer to a correct one.\nText in bold denotes user prompts.\n13\nPublished as a conference paper at ICLR 2024\nCan you solve the following math problem? Terry eats 2 yogurts a day.\nThey are currently on sale at 4 yogurts for $5.00. How much does he spend\non yogurt over 30 days? Explain your reasoning. Your final answer should\nbe a single numerical number, in the form \\boxed{answer}, at the end of\nyour response.\nFirst, we need to calculate how many sets of 4 yogurts Terry buys in 30\ndays. Since Terry eats 2 yogurts a day, he will need 2/4 = 0.5 sets of 4\nyogurts per day. Next, we multiply the number of sets by the price of\neach set to calculate how much Terry spends per day. Each set of 4\nyogurts costs $5.00, so Terry spends 0.5 * $5.00 = $<<0.5*5=2.50>>2.50 on\nyogurt per day. Finally, we multiply the amount Terry spends per day by\nthe number of days to find the total amount he spends on yogurt over 30\ndays. Terry spends $2.50 per day for 30 days, so he spends $2.50 * 30 =\n$<<2.5*30=75.00>>75.00 on yogurt over 30 days. So, Terry spends\n$\\boxed{75.00}$ on yogurt over 30 days. (Correct)\nReview your previous answer and find problems with your answer.\nUpon reviewing my previous answer, I realized that I made a mistake in\ncalculating Terry's spending on yogurt over 30 days. I incorrectly stated\nthat Terry spends $2.50 per day for 30 days, resulting in a total of\n$75.00. However, since Terry eats 2 yogurts a day and the yogurts are\nsold in sets of 4 for $5.00, Terry would actually need to buy 1 set of 4\nyogurts to last for 2 days. Therefore, his spending on yogurt per set\nwould be $5.00 for 2 days, or $5.00/2 = $2.50 per day. To calculate the\ntotal amount spent on yogurt over 30 days, we would multiply the amount\nspent per day ($2.50) by the number of sets needed in 30 days (which is\n30 divided by 2 days per set). Therefore, the correct calculation would\nbe $2.50 * (30/2) = $2.50 * 15 = $37.50. Apologies for the mistake. The\ncorrect answer is Terry spends $\\boxed{37.50}$ on yogurt over 30 days.\nBased on the problems you found, improve your answer. Please reiterate\nyour answer, with your final answer a single numerical number, in the\nform \\boxed{answer}.\nApologies for the error in my previous response. Upon reviewing the\nproblem, here is the correct answer: Since Terry eats 2 yogurts a day and\nthe yogurts are sold in sets of 4 for $5.00, Terry needs to buy 1 set of\n4 yogurts to last for 2 days. Therefore, Terry's spending on yogurt per\nset is $5.00 for 2 days, or $5.00/2 = $2.50 per day. To calculate the\ntotal amount spent on yogurt over 30 days, we multiply the amount spent\nper day ($2.50) by the number of sets needed in 30 days. In this case,\nTerry needs 30/2 = 15 sets of yogurts over 30 days. Multiplying $2.50 by\n15, we find that Terry spends a total of $2.50 * 15 = $37.50 on yogurt\nover 30 days. Therefore, the final answer is Terry spends $\\boxed{37.50}$\non yogurt over 30 days. (Incorrect)\nFigure 4: Example on GSM8K where self-correction changes a correct answer to an incorrect one.\n14\nPublished as a conference paper at ICLR 2024\nCan you solve the following math problem? Toulouse has twice as many\nsheep as Charleston. Charleston has 4 times as many sheep as Seattle. How\nmany sheep do Toulouse, Charleston, and Seattle have together if Seattle\nhas 20 sheep? Explain your reasoning. Your final answer should be a\nsingle numerical number, in the form \\boxed{answer}, at the end of your\nresponse.\nLet's start by figuring out how many sheep Charleston has. We are given\nthat Charleston has 4 times as many sheep as Seattle, so we can calculate\nthis by multiplying Seattle's number of sheep by 4: Charleston = 4 * 20 =\n80 sheep. Next, we are told that Toulouse has twice as many sheep as\nCharleston. Therefore, we can find Toulouse's number of sheep by\nmultiplying Charleston's number of sheep by 2: Toulouse = 2 * 80 = 160\nsheep. Finally, to find the total number of sheep, we add up the number\nof sheep for each city: Total = Seattle + Charleston + Toulouse = 20 + 80\n+ 160 = 260 sheep. Therefore, Toulouse, Charleston, and Seattle have a\ntotal of 260 sheep. Answer: \\boxed{260}. (Correct)\nReview your previous answer and find problems with your answer.\nI apologize for the mistake in my previous response. Let's go through the\nproblem again and find the correct answer. We are given that Charleston\nhas 4 times as many sheep as Seattle. If Seattle has 20 sheep, then\nCharleston would have: Charleston = 4 * 20 = 80 sheep. Next, we are told\nthat Toulouse has twice as many sheep as Charleston. Therefore, Toulouse\nwould have: Toulouse = 2 * 80 = 160 sheep. To find the total number of\nsheep, we add up the number of sheep for each city: Total = Seattle +\nCharleston + Toulouse = 20 + 80 + 160 = 260 sheep. After reviewing my\nprevious answer, I have determined that there are no problems with my\nupdated response. The correct answer is 260 sheep.\nBased on the problems you found, improve your answer. Please reiterate\nyour answer, with your final answer a single numerical number, in the\nform \\boxed{answer}.\nApologies for the confusion. Let me provide an improved answer. We are\ngiven that Charleston has 4 times as many sheep as Seattle, and Seattle\nhas 20 sheep. So, Charleston would have 4 * 20 = 80 sheep. According to\nthe information, Toulouse has twice as many sheep as Charleston.\nTherefore, Toulouse would have 2 * 80 = 160 sheep. To determine the total\nnumber of sheep, we add up the number of sheep for each city: Total =\nSeattle + Charleston + Toulouse = 20 + 80 + 160 = 260 sheep. After\nreviewing the previous answer, the correct final answer is \\boxed{260}.\n(Correct)\nFigure 5: Example on GSM8K where self-correction does not change the answer.\n15\nPublished as a conference paper at ICLR 2024\nQ: A fencing thrust with a sharp sword towards a person would result in\nwhat?\nAnswer Choices: (A) injury (B) small cuts (C) fever (D) competition (E)\npuncture wound.\nExplain your reasoning. You must choose only one option from A to E. Your\nfinal answer should be a single letter from A to E, in the form (answer),\nat the end of your response.\nA: A fencing thrust with a sharp sword towards a person would result in a\npuncture wound. This is because a fencing thrust involves forcefully\nstabbing with a sword, which can penetrate the skin and create a deep,\nnarrow wound. Puncture wounds can be very dangerous as they can damage\ninternal organs and cause bleeding. Therefore, the most appropriate\nanswer is (E) puncture wound. (Correct)\nReview your previous answer and find problems with your answer.\nUpon reviewing my previous answer, I realize that there are some problems\nwith it. While a fencing thrust with a sharp sword can potentially result\nin a puncture wound, it is important to note that in a controlled fencing\nmatch, the swords used are typically blunted or have safety tips,\ndesigned to minimize the risk of injury. Therefore, the likelihood of a\npuncture wound from a fencing thrust with a sharp sword in a regulated\nfencing context is quite low. A more appropriate and accurate answer\nwould be (D) competition, as a fencing thrust with a sharp sword is\ntypically performed in the context of a fencing match or competition,\nrather than with the intention to cause injury.\nBased on the problems you found, improve your answer. You must choose\nonly one option from A to E. Please reiterate your answer, with your\nfinal answer a single letter from A to E, in the form (answer).\nBased on the problems with my previous answer, I apologize for the\nincorrect response. In a controlled fencing match with a sharp sword, a\nfencing thrust is likely to result in (D) competition, rather than injury\nor a puncture wound. Therefore, the correct final answer is (D)\ncompetition. (Incorrect)\nFigure 6: Example on CommonSenseQA where self-correction changes a correct answer to an in-\ncorrect one.\n16\nPublished as a conference paper at ICLR 2024\nConcepts: ['footage', 'motion', 'ruin', 'tilt', 'window']\nSentence: time lapse footage with tilt up motion of the sun streaking\nthrough window of ruin\n###\nConcepts: ['cause', 'hate', 'hut', 'local', 'love']\nSentence: new beach huts on the island have caused some controversy some\nlocals love them others hate them\n[8 more in-context examples]\n###\nConcepts:\n['vest', 'snow','remove', 'silhouette', 'dunk', 'step',\n'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle',\n'pull','wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit',\n'cupcake', 'priest', 'rink', 'counter', 'rest','piano', 'wine', 'serve',\n'winter']\nSentence: A toddler wearing a vest and snow boots steps onto the icy\ndeck, removes a circle-shaped piece of plaster from the wall, and pulls\nout a tool from their winter suit to wrap their head around a headphone.\nFigure 7: Example output for Constrained Generation using the prompt from Madaan et al. (2023).\nThe constraint that the output needs to contain all the concepts is not explicitly mentioned in the\nprompt for initial response generation. However, in the feedback & refine prompts, the constraint is\nimplied by \u201cwhat concepts from the concept list are missing from the sentence?\u201d (refer to Figures\n25 and 26 of Madaan et al. (2023) for the feedback & refine prompts).\nConcepts:\n['vest', 'snow','remove', 'silhouette', 'dunk', 'step',\n'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle',\n'pull','wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit',\n'cupcake', 'priest', 'rink', 'counter', 'rest','piano', 'wine', 'serve',\n'winter']\nWrite a reasonable paragraph that includes *ALL* of the above concepts.\nIn the winter wonderland, a mother dressed in a warm vest watched her\ntoddler play in the snow. As the child attempted to remove the snow from\nthe ground, their silhouette created an enchanting image against the\nwhite backdrop. Nearby, a group of friends gathered around a basketball\nhoop, taking turns to dunk the ball with skillful steps. One of them,\nholding an item resembling a tool, used it to plaster a piece of the wall\nthat had been damaged. Meanwhile, on the other side of the park, a circle\nof children pulled their hats tightly to wrap around their heads,\nprotecting their ears from the cold wind as they listened to music\nthrough their colorful headphones. On the deck of a nearby house, a man\nin a sleek suit pressed a button, causing a hidden mechanism to reveal a\ngrand piano. As the music filled the air, the host served glasses of wine\nto the guests, creating a warm and inviting atmosphere in the winter\nevening. In the distance, a priest skated gracefully in an ice rink, his\nmovements mirroring the smoothness of his words during a sermon. At the\ncounter of a bakery, a customer ordered a cupcake and watched as the\nbaker skillfully decorated it with a winter-themed design. Not far away,\na group of friends gathered around a pit, roasting marshmallows and\nenjoying the warmth of the fire. The scene painted a picture of joy,\nfriendship, and the beauty of winter.\nFigure 8: Example output for Constrained Generation using a prompt casually written by the authors.\n17\n"
  },
  {
    "title": "ImagenHub: Standardizing the evaluation of conditional image generation models",
    "link": "https://arxiv.org/pdf/2310.01596.pdf",
    "upvote": "17",
    "text": "Published as a conference paper at ICLR 2024\nIMAGENHUB: STANDARDIZING THE EVALUATION OF\nCONDITIONAL IMAGE GENERATION MODELS\nMax Ku\u2660, Tianle Li\u2660, Kai Zhang\u2020, Yujie Lu\u2663, Xingyu Fu\u2661, Wenwen Zhuang\u2662, Wenhu Chen\u2660\nUniversity of Waterloo\u2660, Ohio State University\u2020, University of California, Santa Barbara\u2663\nUniversity of Pensylvania\u2661, Central South University\u2662\n{max.ku, t29li, wenhuchen}@uwaterloo.ca\nhttps://tiger-ai-lab.github.io/ImagenHub/\nABSTRACT\nRecently, a myriad of conditional image generation and editing models have been\ndeveloped to serve different downstream tasks, including text-to-image genera-\ntion, text-guided image editing, subject-driven image generation, control-guided\nimage generation, etc. However, we observe huge inconsistencies in experimen-\ntal conditions: datasets, inference, and evaluation metrics \u2013 render fair compar-\nisons difficult. This paper proposes ImagenHub, which is a one-stop library to\nstandardize the inference and evaluation of all the conditional image generation\nmodels. Firstly, we define seven prominent tasks and curate high-quality evalua-\ntion datasets for them. Secondly, we built a unified inference pipeline to ensure\nfair comparison. Thirdly, we design two human evaluation scores, i.e. Seman-\ntic Consistency and Perceptual Quality, along with comprehensive guidelines to\nevaluate generated images. We train expert raters to evaluate the model outputs\nbased on the proposed metrics. Our human evaluation achieves a high inter-worker\nagreement of Krippendorff\u2019s alpha on 76% models with a value higher than 0.4.\nWe comprehensively evaluated a total of around 30 models and observed three\nkey takeaways: (1) the existing models\u2019 performance is generally unsatisfying\nexcept for Text-guided Image Generation and Subject-driven Image Generation,\nwith 74% models achieving an overall score lower than 0.5. (2) we examined the\nclaims from published papers and found 83% of them hold with a few exceptions.\n(3) None of the existing automatic metrics has a Spearman\u2019s correlation higher\nthan 0.2 except subject-driven image generation. Moving forward, we will con-\ntinue our efforts to evaluate newly published models and update our leaderboard\nto keep track of the progress in conditional image generation.\n1\nINTRODUCTION\nControl-guided\nText2Image\nImageEditing\nimport imagen_hub\nmodel = imagen_hub.load(name)\nimg = model.generate(condition)\nInference\nPerceptual \nQuality (PQ) \nSemantic \nConsistency\n(SC)\nRaters\nImagenHub Data\nImagenHub Inference Library\nImagenHub Evaluator\nAPI\n\u2026\u2026.\nhyperrealism fruits and \nvegetables market\nFigure 1: The overview of ImagenHub framework, which consists of the newly curated ImagenHub\ndataset, ImagenHub library, and ImagenHub evaluation platform and standard.\nWith the recent development of diffusion models, image generation has quickly become one of\nthe most popular research areas in AI. To enable controllability in the image generation process, a\nmyriad of conditional image generation models have been proposed in the past year. Diverse set\nof conditions have been attempted to steer the diffusion process. One of the most popular tasks\nis text-guided image generation (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022),\n1\narXiv:2310.01596v4  [cs.CV]  10 Mar 2024\nPublished as a conference paper at ICLR 2024\nwhich aims to ground on a text prompt to generate the corresponding image. Besides that, there are\nalso subject-conditioned image generation (Gal et al., 2022; Ruiz et al., 2023), text-guided image\nediting (Brooks et al., 2023), multi-subject-conditioned image generation (Kumari et al., 2023),\nstyle-guided image generation (Sohn et al., 2023), etc. These different tasks aim to serve different\ntypes of downstream applications by enabling subject-level, background-level, style-level controls.\nThe field is evolving at an unprecedented pace and lots of improvements have been reported in\nthe published papers. However, one glaring issue we observed is the published work are highly\ninconsistent in their experimental setups. To summarize, the inconsistencies mainly come from\nthree aspects, namely dataset, inference and evaluation:\n\u2022 Dataset: The existing work curated their own evaluation dataset, which makes the comparison\nof different models totally incomparable.\n\u2022 Inference: Some work requires heavy hyper-parameter tuning and prompt engineering to\nachieve reasonable performance, which makes the model less robust. Due the tuning effort\non different models differe significantly, their comaprison could become unfair.\n\u2022 Evaluation: The existing work used different human evaluation protocols and guidelines. This\ninconsistency renders it impossible to compare human evaluation scores across different meth-\nods. Moreover, some of the work either employs a single rater or does not report inter-worker\nagreement. Thus, the reported results might not be comparable across different papers.\n0\n0.2\n0.4\n0.6\n0.8\nText-guided\nImagen\nSubject-Driven\nImagen\nText-guided\nImgEdit\nMask-guided\nImgEdit\nSubject-Driven\nImgEdit\nMulti-concept\nImComp\nControl-guided\nImagen\nBest\nAvg\nDallE-3\nSDXL-Inpaint\nMagicBrush\nSuTI\nPhotoSwap\nCustomDiff\nControlNet\nOverall Score of different subtasks\nFigure 2: The best and the average model performance in each task\nThese three inconsistencies make it nearly impossible to track the real progress in the field of the\nconditional image generation. Such an issue could greatly hinder the development of this field. The\ndesiderata is to build a centralized effort to fairly evaluate every model. More importantly, this effort\nneeds to be continuous to keep up with the evolvement of the field. Our paper aims to serve this\npurpose to standardize the serving and evaluation of all open-source conditional image generation\nmodels. More specifically, ImagenHub consists of the modules listed in Figure 1.\nImagenHub Dataset. We surveyed the existing public evaluation sets for all the generation tasks\nand then picked diverse instances from them to build our ImagenHub dataset. ImagenHub dataset\nconsists of 7 subsets, each with 100-200 instances. This dataset aims at standardizing the evaluation\ninput to ensure fair comparison for different models.\nImagenHub Inference Library. We built a ImagenHub library1 to evaluate all the conditional\nimage generation models. We ported the highly dispersed codebase from the existing works and\nthen standardized them into a unified format. During inference, we fixed the hyper-parameters\nand the prompt format to prevent per-instance prompt or hyper-parameter tuning, which makes the\ninference of different models fair and reproducible. The library is designed to be easily extendable.\nOur Appendix A.7, A.8, and A.9 show how a third party and researchers can benefit from our work.\nImagenHub Evaluator. We explored different human evaluation metrics and iterated over different\nversions of the rating guidelines to improve the inter-rater agreement. We settled on two three-\nvalued rating metrics \u2018Semantic Consistency\u2019 and \u2018Perceptual Quality\u2019 to achieve generally high\ninter-worker agreement measured by Fleiss Kappa (Fleiss & Cohen, 1973) and Krippendorff\u2019s Alpha\n(Krippendorff, 2011). We designed a rating standard to achieve several benefits: (1) Our rating\nguide is an unambiguous checklist table such that the rater can rate the image with ease. (2) The\n1it\u2019s similar to Huggingface libraries (Wolf et al., 2019; von Platen et al., 2022)\n2\nPublished as a conference paper at ICLR 2024\ndesigned rating guideline is unified on every task type. (3) Sustainability. Since each model is rated\nindividually, previous evaluation results can be reused when new models are added.\nWe demonstrate our evaluation results in Figure 2, where we show the overall score of the best-\nperforming model and the medium-performing model. Based on our evaluation results in section 5,\nwe made some general observations:\n\u2022 The existing models\u2019 performance is generally unsatisfying except for Text-guided Image Gen-\neration and Subject-driven Image Generation.\n\u2022 We found that evaluation results from the published papers are generally consistent with our\nevaluation. 83% of the published result ranking is consistent with our ranking.\n\u2022 Automatic evaluation metrics do not correlate well with human preference well except subject-\ndriven image generation. The correlation scores are lower than 0.2.\n2\nTHE PROBLEM OF CONDITIONAL IMAGE GENERATION\nThe goal of conditional image generation is to predict an RGB image y \u2208 R3\u00d7H\u00d7W : Y, where\nH and W are the height and width of the image. The prediction of the target image is given a set\nof input conditions X = [c1, c2, \u00b7 \u00b7 \u00b7 ], where X \u2208 X, where Ci denotes the i-th condition. In the\nproblem, we aim at learning a prediction function f : X \u2192 Y with deep learning models. Here\nwe mainly consider f parameterized with diffusion models. Here we list a set of tasks we consider\nin Figure 3, where ci can be represented as text prompt, image mask, subject image, source image,\nbackground image, control signal, etc.\nTask Definition. We formally define the tasks we consider as follows:\n\u2022 Text-guided Image Generation: y = f(p), where p is a text prompt describing a scene. The goal is to\ngenerate an image consistent with the text description.\n\u2022 Mask-guided Image Editing: y = f(p, Imask, Isrc), where Imask is a binarized masked image, and Isrc is a\nsource image. The goal is to modify given Isrc in the masked region according to p.\n\u2022 Text-guided Image Editing: y = f(p, Isrc). It\u2019s similar to Mask-guided Image Editing except that there is\nno mask being provided, the model needs to figure out the region automatically.\n\u2022 Subject-driven Image Generation: y = f(S, p), where S is a set of images regarding a specific subject,\nwhich normally ranges from 3-5. The goal is to generate an image according to p regarding the subject S.\n\u2022 Subject-driven Image Editing: y = f(S, p, Isrc), where Isrc is a source image and S is the subject reference.\nThe goal is to replace the subject in Isrc with the given subject S.\n\u2022 Multi-concept Image Composition: y = f(S1, S2, p, Isrc), where S1 and S2 are two sets of concept images.\nThe goal is to compose them together to generate a new image according to the text description p.\n\u2022 Control-guided Image Generation: y = f(Icontrol, p), where Icontrol is the control signal like depth map,\ncanny edge, bounding box, etc. The goal is to generate an image following the low-level visual cues.\n3\nRELATED WORK\nMultimodal Conditional Image Synthesis. Recent works in multimodal conditional image synthe-\nsis often rely on pre-trained vision-language models such as CLIP (Radford et al., 2021) and pre-\ntrained large diffusion models like Stable Diffusion (Rombach et al., 2022). CLIP bridges the gap\nbetween textual descriptions and visual content, while Stable Diffusion is a latent diffusion model\ntrained on a massive dataset. CLIP and Stable Diffusion are now widely used as core components\nin applications such as text-to-image generation, text-guided image editing (Patashnik et al., 2021;\nMeng et al., 2021; Couairon et al., 2022; Mokady et al., 2022; Zhang et al., 2023), and mask-guided\nimage editing (Avrahami et al., 2022; Nichol et al., 2022). Another interesting research direction is\nimage subject personalization with a few or even one image of the subject. Textual Inversion (Gal\net al., 2022) optimizes a text token vector to represent the subject, while DreamBooth (Ruiz et al.,\n2023) finetunes the Stable Diffusion model to learn the new subject concept efficiently. These works\nfoster research in potential applications like subject-driven image generation and editing (Li et al.,\n2023a;b; Gu et al., 2023), and even multi-concept image compositions (Kumari et al., 2023). To\nexplore more control conditions, ControlNet (Zhang & Agrawala, 2023) proposed the usage of zero\n3\nPublished as a conference paper at ICLR 2024\nA cartoon \nstyled \nalarm clock \nText-to-Image \nGeneration\nMask-guided \nImage Editing\nMake it a slice of pizza \ninstead of the sandwich\n\ud835\udc84\ud835\udfcf\nText-guided \nImage Editing\nTask\n\u2205\n\ud835\udc9a\nSubject-Driven \nImage Generation\nSubject-Driven \nImage Editing\nMulti-Concept\nImage Composition\nControl-guided \nImage Generation\nA [V] dog in the \nVersailles hall of \nmirrors\nA small dog is curled \nup on top of the shoes\n\u2205\nChange frisbee \nto a football\n\u2205\n\u2205\nReplace glasses \nwith [V] glasses\nA cat [V] \nstanding by a \npot [M]\n\u2205\n\ud835\udc84\ud835\udfd0\n\ud835\udc84\ud835\udfd0\nFigure 3: The visualization of all the conditional image generation tasks. Here we consider tasks\nwith 1-3 conditions, where \u2205 means empty. The special token [V] and [M] are special identifiers.\nconvolution on the Stable Diffusion model to support additional guided image control. This work\nbrings up the idea of the control-guided image generation task and inspired later work (Qin et al.,\n2023) on improving the control versatility.\nAI-generated Image Assessment. Evaluating AI-generated images holistically is a complex and\nopen problem (Salimans et al., 2016). Researchers have proposed various automatic metrics. In the\nimage quality aspect, Inception score (Salimans et al., 2016), FID (Heusel et al., 2017) are often\nused. These methods rely on statistics from an InceptionNet pre-trained on the ImageNet dataset.\nDespite being widely adopted due to their sensitivity to small changes in images, these metrics are\nnot ideal. They are biased towards the ImageNet dataset, resulting in inadequate evaluations (Borji,\n2021). Later works like LPIPS (Zhang et al., 2018) and DreamSim (Fu et al., 2023) proposed\nbetter ways to measure the perceptual similarity. In the semantic consistency aspect, the CLIP score\n(Hessel et al., 2021) is often used to measure the vision-language alignment between the generated\nimage and the prompt. Researchers also worked on alternative methods such as BLIP score (Li\net al., 2022) and ImageReward (Xu et al., 2023). However, in some tasks like subject-driven image\ngeneration and editing, the automatic measurement of semantic consistency is still an open problem.\nOne long-established yet effective approach to assessing AI-generated image performance is to rely\non human annotators to assess the visual quality (Denton et al., 2015; Isola et al., 2017; Meng\net al., 2021; Chen et al., 2023). The downside is that it entails a reliance on human judgment, which\ncan introduce subjectivity and potentially limit scalability. To mitigate the downsides, the human\nevaluation design has to be unambiguous and easy to follow.\n4\nMETHOD\n4.1\nHUMAN EVALUATION METRICS\nOur proposed metric can be used in all seven tasks with the same standard. We adopt two major\nevaluation metrics, namely semantic consistency SC and perceptive quality PQ. These two metrics\nmeasure the quality of the generated images from two aspects. The semantic consistency measures\nhow well the generated image is aligned with the condition X = [c1, c2, \u00b7 \u00b7 \u00b7 ]. Specifically, we define\n4\nPublished as a conference paper at ICLR 2024\nCondition 1\nCondition 2\nCondition 3\nSC rating\nInconsistent\nAny\nAny\n0\nAny\nInconsistent\nAny\n0\nAny\nAny\nInconsistent\n0\nPartially Consistent\nAny\nAny\n0.5\nAny\nPartially Consistent\nAny\n0.5\nAny\nAny\nPartially Consistent\n0.5\nMostly Consistent\nMostly Consistent\nMostly Consistent\n1.0\nSubjects in image\nArtifacts\nUnusual sense\nPQ rating\nUnrecognizable\nAny\nAny\n0\nAny\nSerious\nAny\n0\nRecognizable\nModerate\nAny\n0.5\nRecognizable\nAny\nModerate\n0.5\nRecognizable\nLittle/None\nLittle/None\n1.0\nTable 1: Rating guideline for computing the SC and PQ score. Detail in subsection A.2.\nFigure 4: Model performance and standard deviation in each task.\nthe semantic consistency score as:\nSC(y, X) = min{g(y, c1), g(y, c2), \u00b7 \u00b7 \u00b7 , g(y, ck)}\n(1)\nwhere g is a modularized function to compute the consistency between y and a single condition\nc. We set g(y, c1) \u2208 [0, 0.5, 1], where 0 means inconsistent, 0.5 means partially consistent and 1\nmeans fully consistent. With this formulation, as long as the output is inconsistent with any of the\nconditions, the evaluation score should become zero. Otherwise, the aggregation function will pick\nthe lowest consistency score from all the conditions. On the other hand, perceptive quality measures\nthe image quality, i.e. whether the image contains artifacts, is blurry, or has an unnatural sense.\nwe set perceptive quality PQ \u2208 [0, 0.5, 1], where 0 means extremely poor quality, 0.5 means the\nimage has an acceptable quality and 1 means high quality. In these experiments, each model is rated\nindividually. We train human raters to estimate the g function and PQ function with comprehensive\nguidelines in Table 1. We derive O = \u221aSC \u00d7 PQ as the overall rating of a model. One benefit\nof using geometric mean as the design choice is that the rating is penalized when one of the aspect\nscores is too low. We further studied the configuration in section 5.\n5\nPublished as a conference paper at ICLR 2024\nTask\nData Source\nInference Dataset size\nText-guided Image Generation\nDrawBench (Saharia et al., 2022)\nDiffusionDB (Wang et al., 2022)\nABC-6K (Feng et al., 2023)\nOurs\n197\nMask-guided Image Editing\nMagicBrush (Zhang et al., 2023)\nOurs\n179\nText-guided Image Editing\nMagicBrush (Zhang et al., 2023)\nOurs\n179\nSubject-driven Image Generation\nSuTI (Chen et al., 2023)\nOurs\n150\nMulti-concept Image Composition\nCustomDiffusion (Kumari et al., 2023)\nOurs\n102\nSubject-driven Image Editing\nDreamEditBench (Li et al., 2023b)\n154\nControl-guided Image Generation\nHuggingFace community\n150\nTable 2: All the human evaluation datasets from seven core tasks.\n4.2\nDATASET AND AVAILABLE MODELS\nWe present a standardized dataset for each type of task. The information of the datasets is shown in\nTable 2. Some models have different standards of inputs in one task. For example, in the text-guided\nimage editing task, DiffEdit is global description-guided while InstructPix2Pix is instruction-guided.\nWe manually created the equivalent meaning prompts for both methods so they can be aligned.\nAll datasets contain a huge variety of test cases to mimic the diversity in real-life situations. We\nhosted all of our datasets on the HuggingFace dataset for easy access and maintenance. Here we\ndemonstrate all the evaluated models in Table 3.\n5\nEXPERIMENTAL RESULTS\nExperiment Setup. All the models either used the default setting from the official implementation\nor the setting suggested in HuggingFace documentation (von Platen et al., 2022). We disabled\nnegative prompts and any prompt engineering tricks to ensure a fair comparison. We conducted\nhuman evaluation by recruiting participants from Prolific to rate the images, and our own researchers\nalso took part in the image rating process. We assigned 3 raters for each model and computed the SC\nscore, PQ score, and Overall human score. Then we computed the Fleiss kappa (Fleiss & Cohen,\n1973) for each mentioned score. We also computed Krippendorff\u2019s Alpha (Krippendorff, 2011),\nwhich is expected to yield a higher value than Fleiss kappa. This distinction arises from the nature\nof the rating categories, with Fleiss\u2019 Kappa assuming nominal categories and Krippendorff\u2019s Alpha\naccommodating ordinal inputs. Both Fleiss\u2019 Kappa and Krippendorff\u2019s Alpha are bounded within\nthe range of [-1, 1], where the value > 0 indicates an agreement and closer proximity to 1 indicates a\nhigher degree of agreement. The design choices are explained in subsection 5.2 and Appendix A.6.\nResults. In Figure 4, we present an overview of the model performance across various tasks. Our\nfindings indicate that the performance of the current models is generally underwhelming, with the\nexception being Text-guided Image Generation and Subject-driven Image Generation, which have\nmodels reaching higher than 0.6 on both SC and PQ averages. The detailed report on each model\u2019s\nperformance is shown on Table 4. We noticed that the overall automated metrics\u2019 correlation with\nthe SC score and PQ score in each task is below 0.2 except subject-driven image editing task. Metric\nvalues are at Table 5 and Table 6.\n5.1\nDISCOVERY AND INSIGHTS\nText-Guided Image Generation. We observe that all models are able to generate high-quality im-\nages. Regarding semantic consistency, all models have a good understanding of the general prompts,\nwhile Stable Diffusion XL is better at understanding complex prompts. For example, it exhibits a\nhigh degree of accuracy and detail on the prompt \u201cA panda making latte art.\u201d while other models\n6\nPublished as a conference paper at ICLR 2024\nModel\n#Params\nRuntime\nKeywords for technical detail\nText-to-Image Generation\nDalle-3 (openai, 2023)\n-\n-\nRecaptioned training data\nMidJourney (midjourney, 2023)\n-\n-\n-\nDalle-2 (Ramesh et al., 2022)\n3.5B\n10s\nunCLIP, two-stage\nStable Diffusion (Rombach et al., 2022)\n0.8B\n3s\nLatent Diffusion\nDeepFloydIF (deep floyd.ai, 2023)\n4.3B\n37s\nCascaded Pixel Diffusion\nOpenJourney (openjourney.ai, 2023)\n0.8B\n3s\nSD, Midjourney data\nStable Diffusion XL (stability.ai, 2023)\n2.3B\n11s\nStable Diffusion, X-Large\nMask-guided Image Editing\nBlendedDiffusion (Avrahami et al., 2022)\n0.5B\n57s\nNoise Blending, DDPM+CLIP\nGLIDE (Nichol et al., 2022)\n3.5B\n19s\nCLIP, Diffusion\nSD-Inpaint (runwayml, 2023)\n1.1B\n11s\nSD, Inpainting training\nSDXL-Inpaint (stability.ai, 2023)\n2.7B\n36s\nSDXL, Inpainting training\nText-guided Image Editing\nSDEdit (Meng et al., 2021)\n1.3B\n13s\nSDE Prior\nText2Live (Bar-Tal et al., 2022)\n3.1M\n36s\nZero-shot, Edit layer\nDiffEdit (Couairon et al., 2022)\n1.3B\n29s\nMask estimation\nCycle Diffusion (Wu & la Torre, 2023)\n1.1B\n9s\nDPM-Encoder, Zero-shot\nPrompt-to-Prompt (Mokady et al., 2022)\n1.1B\n2m\nCross-Attention\nPix2PixZero (Parmar et al., 2023)\n1.1B\n21s\nCross-Attention, Zero Prompt\nInstructPix2Pix (Brooks et al., 2023)\n1.1B\n11s\nSD, synthetic P2P data\nMagicBrush (Zhang et al., 2023)\n1.1B\n7s\nSD, MagicBrush data\nSubject-driven Image Generation\nTextual Inversion (Gal et al., 2022)\n1.1B\n15m\nWord embedding tuning\nDreamBooth (Ruiz et al., 2023)\n1.1B\n10m\nFinetuning with preservation loss\nDreamBooth-Lora (Hu et al., 2021)\n1.1B\n8m\nDreamBooth + Low-Rank Adaptation\nSuTI (Chen et al., 2023)\n2.5B\n30s\nIn-context + Apprenticeship learning\nBLIP-Diffusion (Li et al., 2023a)\n1.1B\n8s\nPretrained encoder, Zero-shot\nSubject-driven Image Editing\nDreamEdit (Li et al., 2023b)\n1.1B\n8m\nDreambooth + Region proposal\nPhotoSwap (Gu et al., 2023)\n1.1B\n7m\nDreambooth + Cross-Attention\nBLIP-Diffusion (Li et al., 2023a)\n1.1B\n18s\nPretrained encoder, Zero-shot\nMulti-concept Image Composition\nCustomDiffusion (Kumari et al., 2023)\n1.1B\n19m\nCross-attention updating\nDreamBooth (Ruiz et al., 2023)\n1.1B\n11m\nFinetuning with preservation loss\nTextualInversion (Gal et al., 2022)\n1.1B\n32m\nWord embedding tuning\nControl-guided Image Generation\nControlNet (Zhang & Agrawala, 2023)\n1.4B\n8s\nZero convolution + Frozen model\nUniControl (Qin et al., 2023)\n1.4B\n23s\nMulti-task pretraining, Zero-shot\nTable 3: Overview of all the evaluated models and their parameter size and runtime. The models are\nlisted in chronological order. The number of parameters and runtime for Dalle-3 and MidJourney\nare unknown. MidJourney is not opensource and does not have a whitepaper in technical detail.\noften misunderstood the prompt as \u201cpanda latte art\u201d. DALLE-3 and Midjourney understand most of\nthe complex prompts than other models, and DALLE-3 is slightly better on conflicting prompts.\nMask-guided Image Editing. We observed the outputs commonly contain obvious artifacts in the\nmasked region boundaries for Stable Diffusion and GLIDE. While Blended Diffusion and Stable\nDiffusion XL do not suffer from the same issue, they often produce unrecognizable outputs. Stable\nDiffusion XL obtains the best results but the overall model performance is still far from satisfactory.\nAnother common issue is that the filled regions can hardly harmonize with the background.\nText-guided Image Editing. One key requirement is to edit the image precisely and keep the back-\nground untouched. This requirement is indeed challenging because the network has to understand\nof editing region from the semantic inputs. We discovered that Prompt-to-Prompt, Pix2PixZero, and\nSDEdit, despite generating high-quality images, often result in completely different backgrounds.\nWe also spotted that in many cases Text2Live will simply return the input as output, this phenomenon\n7\nPublished as a conference paper at ICLR 2024\nModel\nLPIPS \u2193\nCLIP \u2191\nSCAvg\nPQAvg\nOverall\nFleiss \u00af\nO\nKd \u00af\nO\nText-guided Image Generation\nDalle-3\nN/A\n0.2697\n0.79\u00b10.02\n0.79\u00b10.14\n0.76\u00b10.08\n0.19\n0.34\nMidjourney\nN/A\n0.2839\n0.67\u00b10.06\n0.92\u00b10.06\n0.73\u00b10.07\n0.34\n0.51\nDeepFloydIF\nN/A\n0.2814\n0.65\u00b10.02\n0.62\u00b10.06\n0.59\u00b10.02\n0.32\n0.51\nStable Diffusion XL\nN/A\n0.2886\n0.62\u00b10.03\n0.64\u00b10.05\n0.59\u00b10.03\n0.37\n0.61\nDalle-2\nN/A\n0.2712\n0.58\u00b10.04\n0.62\u00b10.06\n0.54\u00b10.04\n0.27\n0.40\nOpenJourney\nN/A\n0.2814\n0.53\u00b10.02\n0.59\u00b10.05\n0.50\u00b10.02\n0.30\n0.47\nStable Diffusion 2.1\nN/A\n0.2899\n0.56\u00b10.02\n0.53\u00b10.05\n0.50\u00b10.03\n0.38\n0.50\nMask-guided Image Editing\nSDXL-Inpainting\n0.15\n0.2729\n0.49\u00b10.05\n0.51\u00b10.02\n0.37\u00b10.05\n0.50\n0.72\nSD-Inpainting\n0.21\n0.2676\n0.28\u00b10.04\n0.27\u00b10.10\n0.17\u00b10.07\n0.31\n0.49\nGLIDE\n0.18\n0.2578\n0.20\u00b10.05\n0.48\u00b10.06\n0.16\u00b10.05\n0.33\n0.56\nBlendedDiffusion\n0.33\n0.2594\n0.12\u00b10.03\n0.11\u00b10.03\n0.05\u00b10.02\n0.36\n0.44\nText-guided Image Editing\nMagicBrush\n0.22\n0.2675\n0.51\u00b10.01\n0.65\u00b10.06\n0.47\u00b10.02\n0.44\n0.67\nInstructPix2Pix\n0.32\n0.2616\n0.29\u00b10.01\n0.70\u00b10.06\n0.27\u00b10.02\n0.55\n0.74\nPrompt-to-prompt\n0.40\n0.2674\n0.17\u00b10.05\n0.55\u00b10.09\n0.15\u00b10.06\n0.36\n0.53\nCycleDiffusion\n0.28\n0.2692\n0.17\u00b10.03\n0.56\u00b10.11\n0.14\u00b10.04\n0.41\n0.63\nSDEdit\n0.61\n0.2872\n0.04\u00b10.03\n0.56\u00b10.12\n0.04\u00b10.03\n0.13\n0.13\nText2Live\n0.17\n0.2628\n0.02\u00b10.01\n0.82\u00b10.04\n0.02\u00b10.02\n0.10\n0.17\nDiffEdit\n0.22\n0.2425\n0.02\u00b10.01\n0.23\u00b10.04\n0.01\u00b10.01\n0.24\n0.24\nPix2PixZero\n0.60\n0.2510\n0.01\u00b10.00\n0.48\u00b10.09\n0.01\u00b10.01\n0.37\n0.37\nSubject-driven Image Generation\nSuTI\n0.77\n0.2895\n0.64\u00b10.11\n0.68\u00b10.08\n0.58\u00b10.12\n0.20\n0.39\nDreamBooth\n0.77\n0.2847\n0.51\u00b10.08\n0.93\u00b10.02\n0.55\u00b10.11\n0.37\n0.60\nBLIP-Diffusion\n0.77\n0.2729\n0.29\u00b10.04\n0.93\u00b10.04\n0.35\u00b10.06\n0.22\n0.39\nTextualInversion\n0.81\n0.2680\n0.21\u00b10.04\n0.74\u00b10.08\n0.21\u00b10.05\n0.35\n0.52\nDreamBooth-Lora\n0.82\n0.2988\n0.07\u00b10.01\n0.82\u00b10.07\n0.09\u00b10.01\n0.29\n0.37\nSubject-driven Image Editing\nPhotoSwap\n0.34\n0.2846\n0.34\u00b10.02\n0.65\u00b10.04\n0.36\u00b10.02\n0.35\n0.46\nDreamEdit\n0.22\n0.2855\n0.31\u00b10.03\n0.61\u00b10.03\n0.32\u00b10.03\n0.33\n0.52\nBLIP-Diffusion\n0.25\n0.2901\n0.09\u00b10.03\n0.70\u00b10.02\n0.09\u00b10.03\n0.41\n0.47\nMulti-concept Image Composition\nCustomDiffusion\n0.79\n0.2929\n0.26\u00b10.01\n0.86\u00b10.05\n0.29\u00b10.01\n0.73\n0.88\nDreamBooth\n0.78\n0.2993\n0.11\u00b10.02\n0.78\u00b10.02\n0.13\u00b10.02\n0.61\n0.71\nTextualInversion\n0.80\n0.2548\n0.04\u00b10.01\n0.74\u00b10.05\n0.05\u00b10.01\n0.62\n0.77\nControl-guided Image Generation\nControlNet\n0.80\n0.2555\n0.42\u00b10.05\n0.19\u00b10.04\n0.23\u00b10.04\n0.37\n0.57\nUniControl\n0.82\n0.2604\n0.38\u00b10.07\n0.20\u00b10.06\n0.23\u00b10.07\n0.36\n0.58\nTable 4: All the evaluated models from seven core tasks. Overall is the average of all \u221aSC \u00d7 PQ.\nFleiss \u00af\nO and Kd \u00af\nO, denoting Fleiss\u2019 Kappa and Krippendorff\u2019s alpha for the overall average, respec-\ntively. We have more automated metric results in Appendix Table 5 and correlations in Table 6.\nalso occasionally happened in other models. For paper claims, our evaluation ranking aligns with\nthe findings from CycleDiffsuion, InstructPix2Pix, MagicBrush, Prompt-to-Prompt, and DiffEdit.\nWe found our evaluation ranking does not align with Pix2PixZero, since their paper only tested on\nword-swapping examples, which is not able to generalize to more complex edits.\nSubject-driven Image Generation. Our evaluation results largely align with DreamBooth, BLIP-\nDiffusion, and SuTI findings. Specifically, Textual inversion struggles to maintain target subject fea-\ntures. DreamBooth can imitate subjects based on images but occasionally resorts to copying learned\nimages. DreamBooth-Lora struggles to generate desired subjects but can follow context prompts.\nBLIP-Diffusion can mimic target subject features but struggles with details. SuTI maintains high\nconsistency with desired subjects and context, with tolerable artifacts in some cases.\nMulti-concept Image Composition. Our evaluations validate that CustomDiffusion is consistently\nbetter than the other two models. However, while it learns the given multiple subjects\u2019 features\n8\nPublished as a conference paper at ICLR 2024\nbetter, it could fail to follow the prompts in many cases, especially on actions and positional words.\nIn contrast, DreamBooth learns the correct subjects in some cases, and TextualInversion rarely learns\nthe correct subjects. Interestingly, in some cases where DreamBooth does not learn the correct\nsubjects, it could still follow the prompts correctly.\nSubject-driven Image Editing. It is essential to modify the subject from the source to the tar-\nget without causing excess changes to the background. Human evaluation is also conducted in\nDreamEdit for the comparison between Photoswap and DreamEdit, but our rankings differ due to\nvarying evaluation criteria. PhotoSwap can adapt to the target subject from the source naturally in\nmost cases but rarely preserves the background well. DreamEdit maintains the context in most cases\nbut sometimes leaves observable distortions at the edge of contextualization. BLIP-Diffusion fails\nthe adaptation most of the time, compromising for a more realistic generation.\nControl-guided Image Generation. Our evaluation shows there is no significant difference be-\ntween the two models in both automatic metrics and human evaluation metrics. While UniControl\nalso reported that there is no significant difference in automatic metrics, our human evaluation re-\nsults do not align. This can be due to the different evaluation standards and aspects. Nevertheless,\nit has come to our attention that neither of these models demonstrates a high level of robustness.\nScratches often appeared on the generated image.\n5.2\nABLATION STUDY\nMethod of overall human score computation. We set the overall score of the model as the ge-\nometric mean of SC and PR score (i.e. O = \u221aSC \u00d7 PQ). But we also explored the weighted\nsum setting O = \u03b1 \u00d7 SC + \u03b2 \u00d7 PQ, where both \u03b1 and \u03b2 are in [0, 1]. We experimented and\nfound that the weighted sum setting yields a different ranking in the models. Take the text-guided\nimage editing task as an example, as in Appendix Table 7, Text2Live outperforms CycleDiffusion in\nthe weighted sum setting even though we found that CycleDiffusion performs better in the human\nexamination. We investigated and found that a majority of results in Text2Live simply return the\ninput as output (in that case SC=0 and PQ=1). We tried adjusting the weightings but it still failed to\nreflect the actual performance of the model. Thus we decided to use the geometric mean setting to\npenalize the models.\nDesign choice of human evaluation metric range. When it comes to human evaluation on a\nmassive scale, it\u2019s essential to create a system that\u2019s easy to understand and quick to use. In this\ninvestigation, we undertake an exploration into how different settings in this evaluation method can\naffect the results, showing in Appendix Table 8. Initially, our approach entailed the utilization of\na range encompassing [0, 0.5, 1, 2] for both the Semantic Consistency (SC) score and the Percep-\ntion Quality (PQ) score, where 2 means the image is perfect. However, this configuration yielded\nsuboptimal results in the Fleiss Kappa. Subsequently, an alternative configuration was employed,\nnarrowing the range to [0, 1] for both the SC and PQ scores. This adjustment, while accommo-\ndating a binary classification, was observed to yield values that were overly polarized and extreme.\nTo find the right balance between keeping values in a reasonable range and making sure the eval-\nuation method is reliable, we resolved to define a range of [0, 0.5, 1] while providing explicit and\nunambiguous guidelines.\nCONCLUSION\nIn this paper, we propose ImagenHub as a continuous effort to unify all efforts in conditional image\ngeneration into a library, easing access to these models. We standardize the dataset and evaluation of\nthese models to build our ImagenHub Leaderboard. We hope this leaderboard can provide a more\nreproducible and fair environment for researchers to visualize progress in this field. A limitation of\nthis work is the reliance on human raters, which is expensive and time-consuming. In the future, we\nplan to develop more generic automatic evaluation methods that approximate human ratings, helping\npeople develop better models.\n9\nPublished as a conference paper at ICLR 2024\nETHICS STATEMENT\nOur work aims to benefit the broader research community by providing a standardized framework\nfor evaluating conditional image generation models. For all the benchmarks, we are committed to\nthe ethical use of them. The datasets used in our work are either publicly available or have been\ncollected and curated exercising the utmost respect for privacy and consent. We will maintain a\nleaderboard to track latest models and encourage open collaboration and discussion in the field. In\nhuman evaluation, we followed the minimum hourly wage of $11. We also ensure that no personal\ninformation is collected and no offensive content is presented during human evaluations.\nREFERENCES\nOmri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of\nnatural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18208\u201318218, 2022.\nOmer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-\ndriven layered image and video editing. In European Conference on Computer Vision, pp. 707\u2013\n723, 2022.\nAli Borji. Pros and cons of gan evaluation measures: New developments, 2021.\nTim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image\nediting instructions. In CVPR, 2023.\nWenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W\nCohen. Subject-driven text-to-image generation via apprenticeship learning. NeurIPS, 2023.\nGuillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-\nbased semantic image editing with mask guidance. In The Eleventh International Conference on\nLearning Representations, 2022.\ndeep floyd.ai.\nIf by deepfloyd lab at stabilityai, 2023.\nURL https://github.com/\ndeep-floyd/IF.\nEmily L Denton,\nSoumith Chintala,\narthur szlam,\nand Rob Fergus.\nDeep genera-\ntive image models using a laplacian pyramid of adversarial networks.\nIn C. Cortes,\nN. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural In-\nformation\nProcessing\nSystems,\nvolume\n28.\nCurran\nAssociates,\nInc.,\n2015.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2015/file/\naa169b49b583a2b5af89203c2b78c67c-Paper.pdf.\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana,\nSugato Basu, Xin Eric Wang, and William Yang Wang.\nTraining-free structured diffusion\nguidance for compositional text-to-image synthesis.\nIn The Eleventh International Confer-\nence on Learning Representations, 2023. URL https://openreview.net/forum?id=\nPUIqjT4rzq7.\nJoseph L Fleiss and Jacob Cohen. The equivalence of weighted kappa and the intraclass correlation\ncoefficient as measures of reliability. Educational and psychological measurement, 33(3):613\u2013\n619, 1973.\nStephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and\nPhillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic\ndata, 2023.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. arXiv preprint arXiv:2208.01618, 2022.\nJing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang,\nJianming Zhang, HyunJoon Jung, et al. Photoswap: Personalized subject swapping in images.\narXiv preprint arXiv:2305.18286, 2023.\n10\nPublished as a conference paper at ICLR 2024\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\nCLIPScore: a\nreference-free evaluation metric for image captioning. In EMNLP, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. CVPR, 2017.\nKlaus Krippendorff.\nComputing krippendorff\u2019s alpha-reliability, 2011.\nURL https://api.\nsemanticscholar.org/CorpusID:59901023.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 1931\u20131941, 2023.\nDongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for\ncontrollable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023a.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In ICML, 2022.\nTianle Li, Max Ku, Cong Wei, and Wenhu Chen. Dreamedit: Subject-driven image editing. arXiv\npreprint arXiv:2306.12624, 2023b.\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations. In International\nConference on Learning Representations, 2021.\nmidjourney. Midjourney, 2023. URL https://www.midjourney.com/home.\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for\nediting real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models. In International Conference on Machine Learning, pp.\n16784\u201316804. PMLR, 2022.\nopenai. Improving image generation with better captions, 2023. URL https://cdn.openai.\ncom/papers/dall-e-3.pdf.\nopenjourney.ai. Openjourney is an open source stable diffusion fine tuned model on midjourney\nimages, 2023. URL https://huggingface.co/prompthero/openjourney.\nGaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu.\nZero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pp.\n1\u201311, 2023.\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-\ndriven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pp. 2085\u20132094, October 2021.\nCan Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Car-\nlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: A unified diffusion model for\ncontrollable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023.\n11\nPublished as a conference paper at ICLR 2024\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In Interna-\ntional Conference on Machine Learning, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500\u2013\n22510, 2023.\nrunwayml. Stable diffusion inpainting, 2023. URL https://huggingface.co/runwayml/\nstable-diffusion-inpainting.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and\nXi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Cur-\nran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_files/\npaper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf.\nKihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred\nBarber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any\nstyle. arXiv preprint arXiv:2306.00983, 2023.\nstability.ai. Stable diffusion xl, 2023. URL https://stability.ai/stable-diffusion.\nPatrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https:\n//github.com/huggingface/diffusers, 2022.\nZijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and\nDuen Horng Chau. DiffusionDB: A large-scale prompt gallery dataset for text-to-image genera-\ntive models. arXiv:2210.14896 [cs], 2022. URL https://arxiv.org/abs/2210.14896.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\nChen Henry Wu and Fernando De la Torre. A latent space of stochastic diffusion models for zero-\nshot image editing and guidance. In ICCV, 2023.\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao\nDong. Imagereward: Learning and evaluating human preferences for text-to-image generation,\n2023.\nKai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated\ndataset for instruction-guided image editing. NeurIPS dataset and benchmark track, 2023.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543, 2023.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\n12\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nMORE METRICS RESULTS\nModel\nLPIPS \u2193\nDINO \u2191\nCLIP-I \u2191\nDreamSim \u2193\nFID \u2193\nKID \u2193\nCLIP \u2191\nText-guided Image Generation\nDalle-3\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n0.2697\nMidJourney\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n0.2839\nDeepFloydIF\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n0.2814\nStable Diffusion XL\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n0.2886\nDalle-2\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n0.2712\nOpenJourney\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n0.2814\nStable Diffusion\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n0.2899\nMask-guided Image Editing\nSDXL-Inpainting\n0.15\n0.90\n0.94\n0.15\n59.04\n-0.0033\n0.2729\nSD-Inpainting\n0.21\n0.85\n0.91\n0.20\n86.23\n-0.0008\n0.2676\nGLIDE\n0.18\n0.91\n0.94\n0.14\n61.71\n-0.0028\n0.2578\nBlendedDiffusion\n0.33\n0.80\n0.87\n0.26\n102.73\n-0.0012\n0.2594\nText-guided Image Editing\nMagicBrush\n0.22\n0.88\n0.92\n0.19\n77.81\n-0.0027\n0.2675\nInstructPix2Pix\n0.32\n0.77\n0.86\n0.30\n100.30\n-0.0014\n0.2616\nPrompt-to-prompt\n0.40\n0.68\n0.81\n0.36\n137.68\n0.0003\n0.2674\nCycleDiffusion\n0.28\n0.79\n0.88\n0.27\n108.29\n-0.0008\n0.2692\nSDEdit\n0.61\n0.57\n0.76\n0.50\n156.12\n0.0025\n0.2872\nText2Live\n0.17\n0.87\n0.92\n0.18\n68.33\n-0.0028\n0.2628\nDiffEdit\n0.22\n0.78\n0.85\n0.31\n121.79\n0.0069\n0.2425\nPix2PixZero\n0.60\n0.53\n0.76\n0.52\n161.64\n0.0072\n0.2510\nSubject-driven Image Generation\nSuTI\n0.77\n0.69\n0.81\n0.32\n164.97\n0.0111\n0.2895\nDreamBooth\n0.77\n0.61\n0.80\n0.40\n173.71\n0.0057\n0.2847\nBLIP-Diffusion\n0.77\n0.63\n0.82\n0.36\n166.53\n0.0102\n0.2729\nTextualInversion\n0.81\n0.37\n0.65\n0.61\n229.18\n0.0122\n0.2680\nDreamBooth-Lora\n0.82\n0.36\n0.70\n0.61\n234.95\n0.0125\n0.2988\nSubject-driven Image Editing\nPhotoSwap\n0.34\n0.65\n0.83\n0.37\n130.81\n0.0009\n0.2846\nDreamEdit\n0.22\n0.74\n0.87\n0.28\n118.95\n0.0011\n0.2855\nBLIP-Diffusion\n0.25\n0.80\n0.89\n0.23\n102.17\n0.0007\n0.2901\nMulti-concept Image Composition\nCustomDiffusion\n0.79\n0.50\n0.70\n0.55\n207.43\n0.0341\n0.2929\nDreamBooth\n0.78\n0.39\n0.64\n0.66\n257.60\n0.0533\n0.2993\nTextualInversion\n0.80\n0.44\n0.66\n0.65\n226.25\n0.0491\n0.2548\nControl-guided Image Generation\nControlNet\n0.80\n0.43\n0.70\n0.56\n219.29\n0.0330\n0.2555\nUniControl\n0.82\n0.42\n0.68\n0.58\n222.57\n0.0192\n0.2604\nTable 5: All automated metrics for the evaluated models based on our benchmark dataset.\n13\nPublished as a conference paper at ICLR 2024\nPQ \u2191\nSC \u2191\nTask\ncorr(LPIPS)\ncorr(DINO)\ncorr(CLIP-I)\ncorr(DreamSim)\ncorr(CLIP)\nText-guided Image Generation\nN/A\nN/A\nN/A\nN/A\n-0.0819\nMask-guided Image Editing\n-0.0502\n-0.0421\n-0.0308\n-0.0294\n-0.0257\nText-guided Image Editing\n0.0439\n0.0449\n0.0765\n0.0432\n0.0437\nSubject-driven Image Generation\n0.0905\n0.1737\n0.1204\n0.0943\n-0.0117\nSubject-driven Image Editing\n0.2417\n0.1690\n0.3770\n0.3846\n-0.0443\nMulti-concept Image Composition\n-0.1018\n-0.1657\n-0.0965\n0.0140\n-0.0107\nControl-guided Image Generation\n0.1864\n0.1971\n-0.0985\n-0.0045\n-0.2056\nTable 6: Metrics correlation. we inverted the signs for metric that is the lower the better. The\ncomputable metrics all hold different assumptions which only contribute part of the aspects in human\nevaluation. This makes a low correlation with human evaluations.\nSetting\nO = \u221aSC \u00d7 PQ\nO = 0.5SC + 0.5PQ\nO = 0.7SC + 0.3PQ\nOSum\nOAvg\nOSum\nOAvg\nOSum\nOAvg\nMagicBrush\n83.51\n0.47\n103.75\n0.58\n98.85\n0.55\nCycleDiffusion\n24.89\n0.14\n65.25\n0.36\n51.28\n0.29\nDiffEdit\n1.71\n0.01\n22.08\n0.12\n14.38\n0.08\nText2Live\n4.08\n0.02\n75.25\n0.42\n46.75\n0.26\nTable 7: Comparison on overall human score computation setting.\nSetting\ncategories = [0, 0.5, 1]\ncategories = [0, 0.5, 1, 2]\ncategories = [0, 1]\nOAvg\nFleiss \u00af\nO\nKd \u00af\nO\nOAvg\nFleiss \u00af\nO\nKd \u00af\nO\nOAvg\nFleiss \u00af\nO\nKd \u00af\nO\nBLIP-Diffusion\n0.13\n0.41\n0.47\n0.09\n0.29\n0.50\n0.05\n0.40\n0.40\nDreamEdit\n0.50\n0.33\n0.52\n0.32\n0.26\n0.55\n0.21\n0.38\n0.38\nPhotoSwap\n0.62\n0.35\n0.46\n0.36\n0.25\n0.50\n0.29\n0.37\n0.37\nTable 8: Ablation study to understand the impact of the granularity of SC and PQ.\n14\nPublished as a conference paper at ICLR 2024\nA.2\nHUMAN EVALUATION GUIDELINE\nTo standardize the conduction of a rigorous human evaluation, we stipulate the criteria for each\nmeasurement as follows:\n\u2022 Semantic Consistency (SC), score in range [0, 0.5, 1]. It measures the level that the generated\nimage is coherent in terms of the condition provided (i.e. Prompts, Subject Token, etc.).\n\u2022 Perceptual Quality (PQ), score in range [0, 0.5, 1]. It measures the level at which the generated\nimage is visually convincing and gives off a natural sense.\nMeaning of Semantic Consistency (SC) score:\n\u2022 SC=0 : Image not following one or more of the conditions at all (e.g. not following the prompt\nat all, different background in editing task, wrong subject in subject-driven task, etc.)\n\u2022 SC=0.5 : all the conditions are partly following the requirements.\n\u2022 SC=1 : The rater agrees that the overall idea is correct.\nMeaning of Perceptual Quality (PQ) score:\n\u2022 PQ=0 : The rater spotted obvious distortion or artifacts at first glance and those distorts make\nthe objects unrecognizable.\n\u2022 PQ=0.5 : The rater found out the image gives off an unnatural sense. Or the rater spotted spotted\nsome minor artifacts and the objects are still recognizable.\n\u2022 PQ=1 : The rater agrees that the resulting image looks genuine.\nRaters have to strictly adhere to Table 9 when rating.\nCondition 1\nCondition 2 (if applicable)\nCondition 3 (if applicable)\nSC rating\nno following at all\nAny\nAny\n0\nAny\nno following at all\nAny\n0\nAny\nAny\nno following at all\n0\nfollowing some part\nfollowing some or most part\nfollowing some or most part\n0.5\nfollowing some or most part\nfollowing some part\nfollowing some or most part\n0.5\nfollowing some part or more\nfollowing some or most part\nfollowing some part\n0.5\nfollowing most part\nfollowing most part\nfollowing most part\n1\nObjects in image\nArtifacts\nUnusual sense\nPQ rating\nUnrecognizable\nserious\nAny\n0\nRecognizable\nsome\nAny\n0.5\nRecognizable\nAny\nsome\n0.5\nRecognizable\nnone\nlittle or None\n1\nTable 9: Rating guide checklist table.\nArtifacts and Unusual sense, respectively, are:\n\u2022 Distortion, watermark, scratches, blurred faces, unusual body parts, subjects not harmonized\n\u2022 wrong sense of distance (subject too big or too small compared to others), wrong shadow, wrong\nlighting, etc.\n15\nPublished as a conference paper at ICLR 2024\nA.3\nRATING EXAMPLES\nThere are some examples when evaluating:\nText-to-Image Generation.\noutputB\noutputA\n\"prompt\": \"One cat and two dogs sitting on the grass.\",\n\"category\": \"Counting\".\noutputC\n\"prompt\": \"A sign that says 'Deep Learning'.\",\n\"category\": \"Text\"\nFigure 5: Rating examples on Text-to-Image Generation task.\n\u2022 OutputA1: [1, 0.5]. SC=1: Prompt perfectly align. PR=0.5: Minor distortion on the cat\u2019s\nfacial features.\n\u2022 OutputB1: [0.5, 0.5]. SC=0.5: 3 dogs appeared instead of 1 cat and 2 dogs. PR=0.5: Minor\ndistortion was found on the animal\u2019s face and the watermark.\n\u2022 OutputC1: [1, 0.5]. SC=1: The prompt match perfectly with the image. PR=0.5: Minor\ndistortion on the dog\u2019s facial features.\n\u2022 OutputA2: [0.5, 1]. SC=0.5: A sign appeared, but failed to spell the word. PR=1: The\nimage look generally real but with some lighting issues.\n\u2022 OutputB2: [0.5, 0.5]. SC=0.5: A sign appeared, but failed to spell the word. PR=0.5: The\nbackground looks so unnatural.\n\u2022 OutputC2: [0.5, 0]. SC=0.5: A sign appeared, but failed to spell the word. PR=0: Heavy\ndistortion on both text and strong artifacts in the background.\n16\nPublished as a conference paper at ICLR 2024\nMask-guided Image Editing.\n\"source_global_caption\": \"A small black dog playing with a frisbee.\",\n\"instruction\": \"turn the frisbee into a soccer ball\",\n\"target_global_caption\": \"A small black dog playing with a soccer ball.\"\noutputB\noutputA\nmask\ninput\n\"source_global_caption\": \"A Zebra standing in between a group of large rocks\",\n\"instruction\": \"Make the zebra a regular horse.\",\n\"target_global_caption\": \"A horse standing in between a group of large rocks\"\nFigure 6: Rating examples on Mask-guided Image Editing task.\n\u2022 OutputA1: [1, 0.5]. SC=1: Clearly generate the soccer. PR=0.5: The edit region does not\nblend well with the context.\n\u2022 OutputB1: [1, 1]. SC=1: Successfully add a soccer. PR=1: The soccer is naturally blended\nwith the context.\n\u2022 OutputA2: [0, 0]. SC=0: Generated content can not be regarded as horse. PR=0: The middle\nleft part is not natural.\n\u2022 OutputB2: [0.5, 0]. SC=0.5: The object is horse-like but not good. PR=0: The whole image\nis not natural.\n17\nPublished as a conference paper at ICLR 2024\nText-guided Image Editing.\noutputB\noutputA\n\"source_global_caption\": \"A bull is on a farm walking around a pen.\",\n\"instruction\": \"Have the cow wear a hat.\",\n\"target_global_caption\": \"A stylish cow wearing a hat walks around a pen on a farm.\"\ninput\n\"source_global_caption\": \"A skateboarder is doing a trick on a hand rail.\",\n\"instruction\": \"What if the man had a hat?\",\n\"target_global_caption\": \"A skateboarder with a hat is doing a trick on a hand rail.\"\nFigure 7: Rating examples on Text-guided Image Editing task.\n\u2022 OutputA1: [0, 0]. SC=0: The image does not follow the instruction at all. PR=0: Heavy\ndistortion on the cow\u2019s facial features.\n\u2022 OutputB1: [1, 1]. SC=1: The hat gives a nice specular reflection. PR=1: It looks real.\n\u2022 OutputA2: [0.5, 0.5]. SC=0.5: The hat exists but does not suit well. PR=0.5: The impor-\ntant object look distorted.\n\u2022 OutputB2: [0, 0]. SC=0: The background completely changed. PR=0: The whole image\nlook distorted.\n18\nPublished as a conference paper at ICLR 2024\nSubject-driven Image Generation.\noutputA\ninput\n\"subject_id\": 16,\n\"subject\": \"dog8\",\n\"prompt\": \"A <token> dog swimming in a river.\"\noutputB\n\"subject_id\": 1,\n\"subject\": \"backpack_dog\",\n\"prompt\": \"a man carrying <token> backpack dog.\"\nFigure 8: Rating examples on Subject-driven Image Generation task.\n\u2022 OutputA1: [1, 1]. SC=1: The output does match the subject and prompt. PR=1: It looks real.\n\u2022 OutputB1: [0, 0]. SC=0: The subject dog missing. PR=0: Serious unusual body.\n\u2022 OutputA2: [0, 0.5]. SC=0: The output does not match the subject. PR=0.5: Some unusual\nsense on the backpack.\n\u2022 OutputB2: [0, 0.5]. SC=0: The output does not match the prompt. PR=0.5: Some unusual\nsense on the desk with the grass.\n19\nPublished as a conference paper at ICLR 2024\nMulti-concept Image Composition.\ninput\n\"target_caption\": \"flower in the wooden pot on a table.\"\noutputB\noutputA\n\"target_caption\": \"chair with a screaming cat sitting on it.\"\nFigure 9: Rating examples on Multi-concept Image Composition task.\n\u2022 OutputA1: [1, 1]. SC=1: Both subjects accurately represent the intended subjects and the\nprompt actions match. PR=1: In general the image is natural and as real.\n\u2022 OutputB1: [0, 1]. SC=0: Subjects are not correct even though prompt action matches. PR=1:\nThe image looks real.\n\u2022 OutputA2: [0.5, 0.5]. SC=0.5: Both subjects accurately represent the intended subjects\nand but the prompt action doesn\u2019t match. PR=0.5: There is a human face on the chair so the\nimportant subject chair looks unrealistic, but do not strongly detract from the image\u2019s overall\nappearance.\n\u2022 OutputB2: [0, 0.5]. SC=0: Subject chair missing. PR=0.5: Minor distortion on the cat body\nand background.\n20\nPublished as a conference paper at ICLR 2024\nSubject-driven Image Editing.\ntoken\ninput\n\"subject\": \"colorful_sneaker\"\noutputB\noutputA\n\"subject\": \"clock\"\nFigure 10: Rating examples on Subject-driven Image Editing task.\n\u2022 OutputA1: [0.5, 0.5]. SC=0.5: Some details of the subject do not match the token. PR=0.5:\nSome unnatural sense on the hand features.\n\u2022 OutputB1: [0.5, 1]. SC=0.5: The subject does match the token, but removing the hand is an\nunnecessary edit. PR=1: It looks real.\n\u2022 OutputA2: [0, 0.5]. SC=0: Subject clock does not match. PR=0.5: Some distortion on the\nnumbers of the clock.\n\u2022 OutputB2: [0, 0.5]. SC=0: Subject clock does not match. PR=0.5: Some distortion on the\nnumbers of the clock and the edit region does not blend well with the context.\n21\nPublished as a conference paper at ICLR 2024\nControl-guided Image Generation.\noutputA\ninput\n\"prompt\": \"golden gate bridge at sunset, Golden Gate Bridge in San Francisco, USA\",\n\"control_type\": \"hed\".\noutputB\n\"text\": \"a man riding a skateboard up the side of a ramp\",\n\"control_type\": \"depth\".\nFigure 11: Rating examples on Control-guided Image Generation task.\n\u2022 OutputA1: [1, 0.5]. SC=1: The generated image perfectly described all the required at-\ntributes of the user prompt, and even work. PR=0.5: There are some missing details in the\nbackground. The details in the far-side are mostly blurred unnaturally.\n\u2022 OutputB1: [1, 0.5]. SC=1: The generated image perfectly described all the required at-\ntributes of the user prompt. PR=0.5: The details of golden gate bridge are mostly blurred unnat-\nurally.\n\u2022 OutputA2: [0, 0]. SC=0: The output does not correspond to the controlled depth image.\nPR=0: Heavy distortion on the background and the man.\n\u2022 OutputB2: [0, 0]. SC=0: The meaning of the text cannot be obtained from the output. PR=0:\nHeavy distortion.\n22\nPublished as a conference paper at ICLR 2024\nA.4\nDATASET INFORMATION\nDataset Details.\nDataset Distribution.\n33\n31\n30\n29\n21\n14\n8\n7\n7 6\n6\n5\nArtifacts\nAnimals\nArts\nPeople\nOutdoor Scenes\nVehicles\nAbstract\nIndoor Scenes\nWorld Knowledge\nFood\nPlants\nIllustrations\n(a) Text-to-Image Generation dataset\n36\n33\n33\n27\n24\n11\n8\n5 2\nPeople\nAnimals\nOutdoor Scenes\nIndoor Scenes\nFood\nArtifacts\nVehicles\nPlants\nWorld Knowledge\n(b) Mask-guided/Text-guided Image Editing dataset\n90\n45\n10\n5\nArtifacts\nAnimals\nIllustrations\nFood\n(c) Subject-driven Image Generation dataset\n26\n19\n18\n18\n13\n5\n2 1\nAnimals\nIllustrations\nArtifacts\nArts\nOutdoor Scenes\nIndoor Scenes\nPlants\nVehicles\n(d) Multi-concept Image Composition dataset\n119\n14\n14\n7\nArtifacts\nIllustrations\nAnimals\nFood\n(e) Subject-driven Image Editing dataset\n45\n37\n27\n11\n7\n7\n6 4 2 2 2\nPeople\nArtifacts\nOutdoor Scenes\nIndoor Scenes\nAnimals\nWorld Knowledge\nFood\nVehicles\nArts\nIllustrations\nPlants\n(f) Control-guided Image Generation dataset\nFigure 12: Objects presented in each task\u2019s prompt text.\n23\nPublished as a conference paper at ICLR 2024\nA.5\nVISUALIZATION\nDALLE\nDeepFloydIF\nOpenJourney\nSD\nSDXL\nA panda making latte art.\nA giraffe underneath a microwave.\nA smafml vessef epropoeilled on watvewr by ors, sauls, or han engie.\nA beautiful photorealistic anime illustration of urbex industrial architecture city architecture unfinished building\nabandoned post office by renzo piano, laser extraterrestial sunset lake vaporwave elysian at night reclaimed by\nnature magic realism myst wilderness, archdaily, wallpaper, highly detailed, trending on artstation.\nFour dogs on the street.\nFigure 13: More samples from our Text-guided Image Generation dataset and ImagenHub outputs.\n24\nPublished as a conference paper at ICLR 2024\nmask\nBlendedDiffusion\nGlide\nSDInpaint\nSDXLInpaint\n\"source_global_caption\": \"A man and a woman eating food at the park.\"\n\"instruction\": \"Have the man be wearing a kilt\"\n\"target_global_caption\": \"A man wearing a kilt and a woman eating food at the park.\"\ninput\n\"source_global_caption\": \"Three zebras standing in the middle of a plain with majestic mountains in the background.\"\n\"instruction\": \"put the zebras next to a river\"\n\"target_global_caption\": \"Three zebras standing next to a river with majestic mountains in the background.\"\n\"source_global_caption\": \"A white plate topped with a sandwich and rice.\"\n\"instruction\": \"Make it a slice of pizza instead of the sandwich.\"\n\"target_global_caption\": \"A white plate topped with pizza and rice.\"\n\"source_global_caption\": \"A kitchen filled with wooden floors and a stove top oven.\"\n\"instruction\": \"get rid of the vase on top of the table\"\n\"target_global_caption\": \"A kitchen filled with wooden floors and a stove top oven, minus the vase on top of the table.\"\n\"source_global_caption\": \"Car parked in parking lot in front of a building.\"\n\"instruction\": \"edit the background by removing the museum and placing a castle\"\n\"target_global_caption\": \"Car parked in front of a castle in a parking lot.\"\nFigure 14: More samples from our Mask-guided Image Editing dataset and ImagenHub outputs.\n25\nPublished as a conference paper at ICLR 2024\nCycleDiffusion\nDiffEdit\nInstructPix2Pix\nMagicBrush\nPix2PixZero\nPrompt2prompt\nSDEdit\n\"source global caption\": \"A man making a goofy face with a pizza in his mouth.\"\n\"instruction\": \"give the man glasses\"\n\"target_global_caption\": \"A bespectacled man making a goofy face with a pizza in his mouth.\"\ninput\n\"source_global_caption\": \"Full grown cat laying down and sleeping on top of a car.\"\n\"instruction\": \"change the cat to a fox.\"\n\"target_global_caption\": \"Full grown fox laying down and sleeping on top of a car.\"\n\"source_global_caption\": \"A piece of pie has bananas and whipped cream surrounding it on a white plate.\"\n\"instruction\": \"put strawberry on the plate\"\n\"target_global_caption\": \"A piece of pie with bananas, whipped cream, and strawberries surrounding it on a white plate.\"\n\"source_global_caption\": \"A white wall mounted oven sitting next to a stove.\"\n\"instruction\": \"Let water run from the faucet.\"\n\"target_global_caption\": \"A white wall mounted oven sitting next to a stove with water running from the faucet.\"\n\"source_global_caption\": \"A man riding a bike on a beach next to the ocean.\"\n\"instruction\": \"Add a cruise ship to the ocean.\"\n\"target_global_caption\": \"A man riding a bike on a beach next to the ocean with a cruise ship in the background.\"\nFigure 15: More samples from our Text-guided Image Editing dataset and ImagenHub outputs.\n26\nPublished as a conference paper at ICLR 2024\nBLIPDiffusion_Gen\nDreamBooth\nDreamBoothLora\nSuTI\nTextualInversion\ninput\n\"subject\": \"berry_bowl\",\n\"prompt\": \"a <token> berry bowl floating on a river\"\n\"subject\": \"dog7\",\n\"prompt\": \"a <token> dog reading a book with a pink glasses on\"\n\"subject\": \"poop_emoji\",\n\"prompt\": \"a <token> poop emoji wearing sunglasses\"\n\"subject\": \"rc_car\",\n\"prompt\": \"a <token> rc car driven by the super mario\"\n\"subject\": \"teapot\",\n\"prompt\": \"a clay <token> teapot sitting on a glass table\"\nFigure 16: More samples from our Subject-Driven Image Generation dataset and ImagenHub out-\nputs.\n27\nPublished as a conference paper at ICLR 2024\ntoken\nBLIPDiffusion_Edit\nDreamEdit\nPhotoSwap\ninput\n\"subject\": \"pink_sunglasses\"\n\"subject\": \"grey_sloth_plushie\"\n\"subject\": \"duck_toy\"\n\"subject\": \"fancy_boot\"\n\"subject\": \"dog\"\nFigure 17: More samples from our Subject-Driven Image Editing dataset and ImagenHub outputs.\n28\nPublished as a conference paper at ICLR 2024\nCustomDiffusion\nDreamBooth\nTextualInversion\ninput\n\"prompt\": \"the cat playing with a wooden pot in a garden\",\n\"concept1\": \"wooden pot\",\n\"concept2\": \"cat\"\n\"prompt\": \"Vintage poster of a car with a dog in the backseat\",\n\"concept1\": \"dog\",\n\"concept2\": \"car\"\n\"prompt\": \"a digital illustration of flower in wooden pot\",\n\"concept1\": \"wooden pot\",\n\"concept2\": \"flower\"\n\"prompt\": \"Oil painting of a teddybear in front of the barn\",\n\"concept1\": \"teddybear\",\n\"concept2\": \"barn\"\n\"prompt\": \"Watercolor painting of tortoise plushy next to a cat\",\n\"concept1\": \"tortoise plushy\",\n\"concept2\": \"cat\"\nFigure 18: More samples from our Multi-Concept Image Composition dataset and ImagenHub out-\nputs.\n29\nPublished as a conference paper at ICLR 2024\nControlNet\nUniControl\ninput\nGolden gate bridge san francisco, Golden Gate Bridge in San Francisco, USA\nCloseup of a brown bear sitting in a grassy area.\n\"New Year's Eve: Dubai Will Attempt \"\"Largest Fireworks Display\"\" World Record\"\nA yellow fire hydrant sitting in the grass\nA man standing on a mat with a pair of dumbbells, deep fake, youtube video screenshot,\noff the shoulder shirt, underbrush wash, two legs two arms one head, neck shackle, grainy\nfootage, surfaces blemishes, artem, uses c4, buttshape, sconces, threes, head to waist\nFigure 19: More samples from our Control-guided Image Generation dataset and ImagenHub out-\nputs.\n30\nPublished as a conference paper at ICLR 2024\nFigure 20: Mean scores of overall human rating and Krippendorff\u2019s Alpha for each number of raters\non one model.\nA.6\nHUMAN EVALUATION PROCEDURE\nProlific. In this work, we used the Prolific platform to receive human feedback on rating AI-\ngenerated images. We applied the following filter for worker requirements when creating the project:\n1) The Worker\u2019s primary speaking language has to be English. 2) Worker must complete rating one\nset of images (around 150-200 images) in one go. We do that to ensure workers can fully under-\nstand the rating instructions, and each set of image is rated by the same person so we can perform\nInter-rater Reliability Analysis across the whole set of the benchmark. Workers will rate each image\nin [SC, PQ], while SC in [0, 0.5, 1] and PQ in [0, 0.5, 1]. Instructions and images are hosted on a\nwebsite. Workers are provided with detailed instructions and rating examples to assist their rating\nprocess. Workers will submit the responses in a tsv file. To validate the annotation quality, we se-\nlected a few (around 10-20) significant samples with obvious ratings for each task. We rated them\nby ourselves and then used them as a reference to determine the performance of annotators. Poor\nperformance will be rejected.\nSample Size Consideration of Human Raters.\nWe studied the effect on the number of human raters. We recruited 10 human raters on Prolific to rate\n15 samples generated using the Midjourney model. In Figure 20, the mean scores seem to remain\nrelatively constant across the number of raters, with a slight increase at 4 raters and the highest mean\nscore at 5 raters. The values for the mean scores are hovering around 0.70, as the error bar shows\nthe standard deviation increases. On the other hand, Krippendorff\u2019s Alpha generally decreases as\nthe number of raters increases. It starts at around 0.70 with 3 raters and has a noticeable drop at 10\nraters to 0.64. We can observe that the mean score will be always consistent, while the reliability\nof that score as measured by Krippendorff\u2019s Alpha decreases slightly as more raters are involved, as\nmore variance is introduced. To make our human evaluation protocol efficient, we picked 3 as the\nnumber of human raters.\n31\nPublished as a conference paper at ICLR 2024\nA.7\nVISUALIZATION TOOLS (IMAGEN MUSEUM)\nExperiment Transparency. We are hosting the benchmarking results publicly. Thus showing the\ntrue performance of each model. Viewers can gain their insights from the result by looking at\nthe actual results. Our framework encourages future researchers to release the human evaluation\nset generation results publicly. By standardizing the human evaluation protocol, human evaluation\nresults would become far more convincing with the experiment transparency.\nFigure 21: The webpage design of Imagen Museum (Text-Guided Image Generation page).\nFigure 22: The webpage design of Imagen Museum (Text-Guided Image Editing page).\nPlease refer to https://github.com/ChromAIca/ChromAIca.github.io for the visualization results.\n32\nPublished as a conference paper at ICLR 2024\nA.8\nADDING NEW MODELS TO OUR FRAMEWORK.\nThird parties are welcome to use our provided tools and frameworks for their work. Here is a quick\nwalkthrough of how a third-party member can extend the framework to support their own model\u2019s\ninference. We use the Text-Guided Image Generation (Text-To-Image) task as an example for the\nsake of simplicity.\nThe infermodel class is designed to have the following two methods:\n\u2022\ninit (args) for class initialization.\n\u2022 infer one image(args) to produce 1 image output. Please try to set the seed as 42.\nIn\nthat\ncase,\nyou\nwill\nadd\na\nnew\nfile\nin\nimagen hub/infermodels\nfolder.\nimagen hub/infermodels/awesome model.py\nclass AwesomeModelClass():\ndef __init__(self, device=\"cuda\"):\n#your implementation goes here\nself.pipe = AwesomeModelClassWrapper(device)\ndef infer_one_image(self, prompt, seed=42):\n#parameters are same in the same task\nself.pipe.set_seed(seed)\nimage = self.pipe(prompt=prompt).images[0]\nreturn image\nThen a line can be added in imagen hub/infermodels/ init .py:\nfrom .awesome_model import AwesomeModelClass\nAnd modify the template config.yml file and add your own model.\n#...\ninfo:\ntask_id: 6\nrunning_models: [\n# ...\n\"AwesomeModelClass\"\n] # Determine which model to run\n#...\nFinally run\npython benchmarking.py --cfg path_to_your_config.yml\npython visualize.py --cfg path_to_your_config.yml\nPlease refer to https://imagenhub.readthedocs.io/en/latest/Guidelines/custommodel.html for details.\n33\nPublished as a conference paper at ICLR 2024\nA.9\nIMAGENHUB AS AN INFERENCE LIBRARY.\nCustom inference of multiple models.\nimport imagen_hub\nfrom imagen_hub.loader.infer_dummy import load_text_guided_ig_data\nfrom imagen_hub.utils import get_concat_pil_images\ndummy_data = load_text_guided_ig_data(get_one_data=True)\nprint(dummy_data)\nprompt = dummy_data['prompt']\nmodel_list = [\"SD\", \"SDXL\"]\nimage_list = []\nfor model_name in model_list:\nmodel = imagen_hub.load(model_name)\noutput = model.infer_one_image(prompt=prompt, seed=42)\nimage_list.append(output.resize((512,512)))\nshow_image = get_concat_pil_images(image_list)\nshow_image\nFigure 23: Image generated in this custom inference example.\nEvaluate images with autometrics.\nfrom imagen_hub.metrics import MetricLPIPS\nfrom imagen_hub.utils import load_image, get_concat_pil_images\ndef evaluate_one(model, real_image, generated_image):\nscore = model.evaluate(real_image, generated_image)\nprint(\"====> Score : \", score)\nimage_I = load_image(\"input.jpg\")\nimage_O = load_image(\"output.jpg\")\nshow_image = get_concat_pil_images([image_I, image_O], 'h')\nmodel = MetricLPIPS()\nevaluate_one(model, image_I, image_O) # ====> Score :\n0.1122\nshow_image\nFigure 24: Image evaluated in this autometrics example.\nPlease refer to https://imagenhub.readthedocs.io/en/latest/Guidelines/deepdive.html for details.\n34\n"
  },
  {
    "title": "Large Language Models as Analogical Reasoners",
    "link": "https://arxiv.org/pdf/2310.01714.pdf",
    "upvote": "13",
    "text": "Published as a conference paper at ICLR 2024\nLARGE LANGUAGE MODELS AS ANALOGICAL REASONERS\nMichihiro Yasunaga,1,2 Xinyun Chen,1 Yujia Li,1 Panupong Pasupat,1 Jure Leskovec,2\nPercy Liang,2 Ed H. Chi,1 Denny Zhou1\n1 Google DeepMind\n2 Stanford University\nmyasu@cs.stanford.edu, {xinyunchen,dennyzhou}@google.com\nABSTRACT\nChain-of-thought (CoT) prompting for language models demonstrates impressive\nperformance across reasoning tasks, but typically needs labeled exemplars of the\nreasoning process. In this work, we introduce a new prompting approach, analogical\nprompting, designed to automatically guide the reasoning process of large language\nmodels. Inspired by analogical reasoning, a cognitive process in which humans draw\nfrom relevant past experiences to tackle new problems, our approach prompts language\nmodels to self-generate relevant exemplars or knowledge in the context, before proceed-\ning to solve the given problem. This method presents several advantages: it obviates the\nneed for labeling or retrieving exemplars, offering generality and convenience; it can\nalso tailor the generated exemplars and knowledge to each problem, offering adaptability.\nExperimental results show that our approach outperforms 0-shot CoT and manual few-\nshot CoT in a variety of reasoning tasks, including math problem solving in GSM8K\nand MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.\n1\nINTRODUCTION\nLarge language models (LLMs) demonstrate strong performance across various tasks (Brown et al., 2020;\nChowdhery et al., 2022; Liang et al., 2022; Qin et al., 2023). Recently, chain-of-thought (CoT) prompting\nhas demonstrated LLMs\u2019 abilities to tackle complex tasks, such as solving math problems, by prompting\nthem to generate intermediate reasoning steps (Wei et al., 2022b; Kojima et al., 2022). For instance,\ncommon methods like few-shot CoT (Wei et al. 2022b; Figure 1, middle) make LLMs generate reasoning\nsteps by offering a few exemplars of question\u2013rationale\u2013answer triplets; 0-shot CoT (Kojima et al. 2022;\nFigure 1, left) aims for the same objective by offering instructions like \u201cthink step by step.\u201d These studies\nhighlight the importance of devising effective methods to guide LLMs to reason.\nHowever, the existing CoT paradigm faces two key challenges: providing relevant guidance or exemplars\nof reasoning, and minimizing the need for manual labeling. Specifically, 0-shot CoT offers generic\nreasoning guidance, which may not suffice for complex tasks like code generation (\u00a76). Few-shot CoT\nprovides more detailed guidance but demands labeled exemplars of the reasoning process, which can be\ncostly to obtain for every task. This raises a research question: can we achieve the best of both worlds\nand automate the generation of relevant exemplars to guide LLMs\u2019 reasoning process?\nIn this work, we propose analogical prompting, a new prompting approach that automatically guides\nthe reasoning process of LLMs. Our inspiration comes from analogical reasoning in psychology, a concept\nwhere humans draw from relevant past experiences to tackle new problems (Vosniadou & Ortony, 1989).\nFor instance, when faced with a new math problem (e.g., finding the area of a square given four points\nin a coordinate system; Figure 1), humans often think about \u201cdo I know a related problem?\u201d (Polya, 2004)\nand recall how they solved related problems in the past (e.g., finding the area of a square with a known\nside length) to derive insights for solving the new problem. They also recall high-level knowledge, such\nas the need to find the side length to calculate a square\u2019s area. Our idea is to prompt LLMs to mimic this\nreasoning process to effectively solve new problems.\nConcretely, given a problem to solve, we prompt LLMs to self-generate relevant exemplars in the context,\nusing instructions like \u201c# Recall relevant problems and solutions:...\u201d, and then proceed to\nsolve the original problem (Figure 1, 2). Simultaneously, we can also prompt LLMs to generate high-level\nknowledge that complements specific exemplars, using instructions like \u201c# Provide a tutorial:...\u201d\n(Figure 3). This proves particularly useful for complex tasks like code generation (see \u00a76). Notably, our\nmethod can operate in a single prompt, generating knowledge, exemplars, and a solution to the initial\n1\narXiv:2310.01714v3  [cs.LG]  9 Mar 2024\nPublished as a conference paper at ICLR 2024\nQ: Roger has 5 tennis balls. He \nbuys 2 more cans of tennis balls. \nEach can has 3 tennis balls. How \nmany tennis balls does he have?\nA: Roger started with 5 balls. 2 \ncans of 3 balls each is 6 balls. \nThe answer is 5 + 6 = 11.\n\u2026\nQ: What is the area of the square \nwith the four vertices at (-2, 2), (2, \n-2), (-2, -6), and (-6, -2)?\nModel Input\nFew-shot CoT\nQ: What is the area of the square \nwith the four vertices at (-2, 2), (2, \n-2), (-2, -6), and (-6, -2)?\nThink step by step.\nModel Input\n0-shot CoT\nQ: What is the area of the square \nwith the four vertices at (-2, 2), (2, \n-2), (-2, -6), and (-6, -2)?\nModel Input\n0-shot\nQ: What is the area of the square with the four vertices at \n(-2, 2), (2, -2), (-2, -6), and (-6, -2)?\n# Instruction:\n## Recall relevant exemplars:\n## Solve the initial problem:\nModel Input\nAnalogical Prompting (Ours)\n# Relevant exemplars:\nQ: What is the area of the square with a side length of 5?\nA: The area of a square is found by squaring the length of its \nside. So, the area of this square is 5^2 = 25.  \u2026\n# Solve the initial problem:\nTo find the area of the square, we need to find the side \nlength. The length is \u2026 \\sqrt{(2 - (-2))^2 + (-2 - 2)^2} = \n\\sqrt{32}. So, the area of the square is (\\sqrt{32})^2 = 32. \nModel Output\n\u25cf Need labeled exemplars of reasoning \n\u25cf Generic guidance of reasoning\n\u25cf Exemplars are automatically generated\nand tailored to each problem\n\u2713\nFigure 1: Overview of our approach, analogical prompting. Left: Existing methods for prompting\nLLM to reason are either generic (0-shot CoT) or demand labeled exemplars (few-shot CoT). Right: Given\na problem, our method prompts LLMs to self-generate relevant exemplars before solving the problem.\nThis eliminates the need for labeling and also tailors the exemplars to each individual problem. See Figure 3\nfor a sample prompt where the LLM self-generates both knowledge and exemplars.\nproblem end-to-end in one pass. The underlying idea here is that modern LLMs have already acquired\nknowledge of various problems during training. Explicitly prompting them to recall relevant problems\nand solutions in the context guides LLMs to perform in-context learning to solve new problems.\nOur proposed approach offers several advantages. It self-generates exemplars and obviates the need for\nmanually labeling reasoning exemplars for each task, addressing the challenges faced by 0-shot and few-shot\nCoT. Furthermore, the self-generated exemplars are tailored to individual problems, such as \u2018geometry\u2019 or\n\u2018probability\u2019, rather than generic \u2018math problems\u2019. This can simplify the complexity associated with recent\nCoT techniques that retrieve relevant exemplars from external data (Zhang et al., 2022b; Shum et al., 2023).\nWe evaluate the proposed approach in various reasoning-intensive tasks, including mathematical problem\nsolving in GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), code generation in\nCodeforces, and other reasoning tasks in BIG-Bench (Srivastava et al., 2022). We use several base LLMs:\nGPT-3.5, GPT-4 (OpenAI, 2023; Ouyang et al., 2022), and PaLM2 (Anil et al., 2023). Experimental\nresults show that our approach outperforms 0-shot CoT and few-shot CoT across a range of tasks and base\nLLMs, achieving an average accuracy gain of +4%. Notably, our approach improves performance on tasks\ninvolving diverse types of reasoning, such as MATH (including algebra, probability, geometry, etc.) and\nCodeforces (involving dynamic programming, graph algorithms, etc.). This result suggests the effectiveness\nof generating tailored exemplars for individual problems to guide the reasoning process of LLMs.\n2\nRELATED WORKS\n2.1\nLARGE LANGUAGE MODELS AND PROMPTING\nA language model estimates probabilities over text. Recent research has scaled up these models from\nmillions (Devlin et al., 2019) to billions of parameters (Brown et al., 2020) and expanded training data\nto include web texts and instruction data (Gao et al., 2020; Ouyang et al., 2022; Chung et al., 2022). These\nadvances have made large language models proficient in various NLP tasks.\nLLMs with billions of parameters demonstrate in-context learning and few-shot learning abilities (Brown\net al., 2020; Liu et al., 2022; Su et al., 2022; Mishra et al., 2022; Wei et al., 2022a; Yasunaga et al.,\n2023; Shi et al., 2023). They use input prompts (instructions or a few exemplars) to guide LLMs to\ngenerate desired responses for tasks, marking the advent of the prompting era. Our approach harnesses\nthe in-context learning abilities of LLMs to guide their reasoning process using self-generated exemplars.\nClosely related to ours are works that perform self-generation in LLM prompting (Sun et al., 2022; He et al.,\n2023; Kim et al., 2022; Li et al., 2022a). For instance, Sun et al. (2022) prompts LLMs to recite relevant\nfacts in context for open-domain question answering. Our idea of self-generating exemplars is related to\nrecitation, but focuses on recalling problem-solving and reasoning processes rather than factual knowledge.\n2\nPublished as a conference paper at ICLR 2024\n2.2\nCHAIN-OF-THOUGHT PROMPTING\nChain-of-thought (CoT; Wei et al. 2022b) is a prompting strategy that guides LLMs to produce intermediate\nreasoning steps towards a final answer, enhancing problem-solving performance. Common instances of\nCoT include 0-shot CoT (Kojima et al., 2022) and few-shot CoT Wei et al. (2022b).\n0-shot CoT prompts LLMs with a general instruction like \u201cthink step by step\u201d to produce intermediate rea-\nsoning steps. Few-shot CoT achieves stronger performance by providing multiple exemplars of reasoning\nprocess (question\u2013rationale\u2013answer), leveraging LLMs\u2019 in-context learning abilities. However, it requires\nlabeled exemplars. Our approach tackles this challenge by prompting LLMs to self-generate exemplars.\nWithin few-shot CoT, the original approach employs a fixed set of labeled exemplars for all test problems.\nRecent work explores retrieval-based CoT, which aims to obtain more relevant exemplars from external\ndata for each problem (Zhang et al., 2022b; Shum et al., 2023). While our work shares the goal of\nproviding relevant exemplars, instead of retrieval, we make LLMs self-generate exemplars. Self-generation\noffers several advantages: it is simpler, as it does not require external data retrieval, and it is more versatile,\nas it can produce not only specific exemplars but also broader insights or knowledge that complement\nthem. Empirically, our generation-based CoT outperforms retrieval-based CoT, especially with larger\nbase LLMs, while retrieval-based CoT excels with smaller base LLMs (\u00a76).\nFinally, there are other techniques for enhancing CoT, such as self-consistency (Wang et al., 2022) and\nleast-to-most (Zhou et al., 2022). Our work can complement and integrate with these efforts.\nPlease see \u00a7A for additional related works.\n3\nPRELIMINARIES\nWe focus on problem-solving tasks, where the objective is to produce a solution y for a given problem\nstatement x, such as mathematical questions or code generation specifications. The solution may include\nboth the intermediate reasoning steps or rationale r and the final answer a.\nA prompting method \u03d5 is a function that maps a problem statement x into a specific textual input \u03d5(x)\nfor an LLM, which then generates a solution \u02c6y=LLM(\u03d5(x)). For instance,\n\u2022 In 0-shot prompting, \u03d5 directly yields x.\n\u2022 In 0-shot CoT, \u03d5 supplements x with a general instruction, such as \u201c[x] think step by step\u201d.\n\u2022 In few-shot CoT, \u03d5 supplements x with several labeled exemplars, {(xi,ri,ai)}K\ni=1, such as \u201c[x1]\n[r1][a1]...[xK][rK][aK] [x]\u201d.\nOur aim is to design a prompting method \u03d5 that enhances the accuracy of solutions LLMs generate.\n4\nAPPROACH\nWe introduce analogical prompting, a new prompting approach that automatically provides exemplars\nto guide LLMs\u2019 reasoning process. Inspired by how humans recall relevant past experiences when tackling\nnew problems, our approach makes LLMs self-generate relevant exemplars or knowledge in context,\nbefore proceeding to solve the problem (Figure 1, right). We present two techniques to achieve this:\nself-generated exemplars (\u00a74.1) and self-generated knowledge + exemplars (\u00a74.2).\n4.1\nSELF-GENERATED EXEMPLARS\nOur approach is based on the idea that modern LLMs possess a broad range of problem-solving knowledge\nacquired during training. Explicitly prompting them to recall or generate relevant problems and solutions\nin context aids LLMs to perform in-context learning to solve new problems.\nSpecifically, given a target problem to solve x, our prompt augments it with instructions like:\n# Problem: [x]\n# Relevant problems: Recall three relevant and distinct problems. For each\nproblem, describe it and explain the solution.\n# Solve the initial problem:\n3\nPublished as a conference paper at ICLR 2024\nFor a concrete example, see Figure 2. The LLM first generates several (K) relevant exemplars in the form of\nquestion-rationale-answer sequences (\u201c# Relevant problems:\u201d part of the instruction). Then the model\nproceeds to solve the initial problem, leveraging the recalled exemplars in the context (\u201c# Solve the ini-\ntial problem:\u201d part of the instruction). Note that all these instructions are provided within a single prompt,\nallowing the LLM to generate relevant problems and solution to the initial problem in one continuous pass.\nUsing \u2018#\u2019 symbols in the prompt (e.g., \u2018# Relevant Problems\u2019) helps LLMs structure the response better.\nBelow are key technical decisions we made:\n\u2022 Generating relevant and diverse exemplars is important: To achieve this, we explicitly include an\ninstruction in the prompt, such as \u201cgenerate problems that are distinct from each other\u201d (e.g., Figure\n2). This step is crucial as some LLMs have a tendency to repetitively generate identical problems,\nwhich can be misleading when solving the target problem.\n\u2022 Single-pass vs. independent exemplar generation: An alternative approach is to independently\ngenerate exemplars by separately sampling them from the LLM and then re-prompt the LLM with\nall the exemplars. While this method does work, our current single-pass prompt approach achieves\ncomparable performance and offers greater convenience, eliminating the need for multiple prompts.\nConsequently, we have chosen to adopt the single-pass method.\n\u2022 The number of exemplars to generate (K): Through experimentation, we have found that generating\nK = 3 to 5 exemplars works the best (more details in \u00a76.5).\nOur approach offers two advantages. It offers detailed exemplars of reasoning without manual labeling,\naddressing the challenges in 0-shot and few-shot CoT. The generated exemplars are tailored to individual\nproblems (e.g., \u2018geometry\u2019 or \u2018probability\u2019), offering more relevant guidance than traditional few-shot\nCoT, which uses fixed exemplars (e.g., general math problems; Figure 1, middle).\n4.2\nSELF-GENERATED KNOWLEDGE + EXEMPLARS\nWhile generating exemplars is useful, in complex tasks like code generation, LLMs may overly rely on\nthe low-level exemplars and fail to generalize when solving the target problems. To address this challenge,\nwe also allow LLMs to self-generate high-level takeaways that complements the exemplars, which we refer\nto as \u201cknowledge.\u201d Specifically, we enhance the prompt with an additional instruction like the following.\nFor a concrete example, see Figure 3.\n# Tutorial: Identify core concepts in the problem and provide a tutorial.\nOne technical consideration is whether to generate knowledge before or after exemplars. We found that\ngenerating knowledge before exemplars yields superior results (Table 7). By generating knowledge first,\nLLMs identify the core concepts of the problem. This, in turn, helps LLMs generate exemplars that align\nmore closely in terms of the fundamental problem-solving approaches rather than surface-level lexical\nsimilarities. For further discussion, please refer to \u00a76.2.\n5\nEXPERIMENTAL SETUP\n5.1\nTASKS\nWe evaluate the proposed approach in diverse reasoning-intensive tasks, including mathematical problem\nsolving, code generation, and other reasoning tasks like logical and temporal reasoning.\nMathematical problem solving.\nWe use popular benchmarks, GSM8K (Cobbe et al., 2021), comprising\nelementary math word problems, and MATH (Hendrycks et al., 2021b), consisting of advanced math\nproblems from high school math competitions. For each problem, we obtain an output from LLMs using\na temperature of 0, and report the accuracy.\nCode generation.\nCode generation involves synthesizing programs to solve algorithmic problems.\nCompetitive programming is especially challenging, requiring reasoning about various algorithms like\ndynamic programming and graphs (Li et al., 2022b; Kulal et al., 2019; Yasunaga & Liang, 2020).\nAs a benchmark, we collected competitive programming problems from codeforces.com (details in \u00a7B).\nWe focus on level-A problems published in 2023 to prevent test set contamination (Magar & Schwartz,\n2022). Each problem comprises a problem statement, which serves as input to LLMs, and a set of test\ncases to assess generated code. The correctness of code is determined by whether it passes all test cases.\n4\nPublished as a conference paper at ICLR 2024\nn array. The algorithm works \nation, the current element is \ns track of the product of all \nthe input array. ......\n1, and then iterate through \ntarget.\nefix of the array is equal to \nabout the core concepts in \nthe following conditions are \non, print the smallest.\nblem, describe the problem, \nelf-generated Lessons\nf-generated Exemplars\nYour task is to tackle mathematical problems. When presented with a math problem, recall relevant problems as examples. Afterward, \nproceed to solve the initial problem.\n# Problem:\nAn airline serves a dinner to all the passengers on an airplane. They get their choice of steak or fish. Three steak meals and three \nfish meals are set aside for the six-member crew. If the meals are distributed to the crew members randomly, what is the probability \nthat both pilots get the fish?\n# Instructions:\n## Relevant Problems:\nRecall three examples of math problems that are relevant to the initial problem. Your problems should be distinct from each other and \nfrom the initial problem (e.g., involving different numbers and names). For each problem:\n- After \"Q: \", describe the problem \n- After \"A: \", explain the solution and enclose the ultimate answer in \\boxed{}.\n## Solve the Initial Problem:\nQ: Copy and paste the initial problem here.\nA: Explain the solution and enclose the ultimate answer in \\boxed{} here.\nRelevant Problems:\nQ: In a bag, there are 5 red balls and 3 blue balls. If two balls are drawn at random without replacement, what is the probability \nthat both balls are red?\nA: The total number of balls is 8. The probability of drawing a red ball on the first draw is 5/8. After one red ball is drawn, there \nare 4 red balls left out of 7 total balls. Therefore, the probability of drawing a second red ball is 4/7. The probability of both \nevents happening is the product of the individual probabilities, so the probability of drawing two red balls is (5/8) * (4/7) = \n\\boxed{\\frac{5}{14}}.\nQ: A box contains 10 red marbles and 5 blue marbles. If three marbles are drawn at random without replacement, what is the probability \nthat all three marbles are red?\nA: The total number of marbles is 15. The probability of drawing a red marble on the first draw is 10/15. After one red marble is \ndrawn, there are 9 red marbles left out of 14 total marbles. Therefore, the probability of drawing a second red marble is 9/14. After \ntwo red marbles are drawn, there are 8 red marbles left out of 13 total marbles. Therefore, the probability of drawing a third red \nmarble is 8/13. The probability of all three events happening is the product of the individual probabilities, so the probability of \ndrawing three red marbles is (10/15) * (9/14) * (8/13) = \\boxed{\\frac{24}{91}}.\nQ: A bag contains 4 green marbles, 3 red marbles, and 2 blue marbles. If two marbles are drawn at random without replacement, what is \nthe probability that both marbles are green?\nA: The total number of marbles is 9. The probability of drawing a green marble on the first draw is 4/9. After one green marble is \ndrawn, there are 3 green marbles left out of 8 total marbles. Therefore, the probability of drawing a second green marble is 3/8. The \nprobability of both events happening is the product of the individual probabilities, so the probability of drawing two green marbles \nis (4/9) * (3/8) = \\boxed{\\frac{1}{6}}.\nLet's solve the following math problem:\nAn airline serves a dinner to all the passengers on an airplane. They get their choice of steak or fish. Three steak meals and three \nfish meals are set aside for the six-member crew. If the meals are distributed to the crew members randomly, what is the probability \nthat both pilots get the fish?\nA: There are a total of 6 crew members, so the probability of the first pilot getting the fish is 3/6. After one fish meal is given to \nthe first pilot, there are 2 fish meals left out of 5 total meals. Therefore, the probability of the second pilot getting the fish is \n2/5. The probability of both events happening is the product of the individual probabilities, so the probability that both pilots get \nthe fish is (3/6) * (2/5) = \\boxed{\\frac{1}{5}}.\nOur Prompt:  Self-generate Exemplars\nOutput by GPT3.5-turbo\nSelf-generated Exemplars\nFigure 2: Actual example of our prompt (top) and LLM output (bottom) for MATH task. Top: Our prompt\nsupplements the problem statement with instructions to generate relevant exemplars and then solve the\nproblem. Bottom: Exemplars generated by GPT3.5-turbo are indeed relevant to the problem, focusing on\nprobability. It then accurately solves the problem. See \u00a7D.1 for the complete prompt and output. Using \u2018#\u2019\nsymbols in the prompt (e.g., \u2018# Relevant Problems\u2019) helps LLMs structure the response better.\nIn line with existing work on code generation (Li et al., 2022b; Chen et al., 2023), we report the Acc@1\nand Acc@10 metrics. Acc@k measures whether at least one of the k sampled model outputs is correct.\nFor each problem, we sample 10 outputs from LLMs, using a temperature of 0.7.\nOther reasoning tasks.\nWe further evaluate on various reasoning tasks in BIG-Bench (Srivastava et al.,\n2022; Suzgun et al., 2022): word sorting, logical deduction five objects, temporal sequences, reasoning\nabout colored objects, and formal fallacies. These tasks are diverse and may not have dedicated training\ndata, so they align well with our approach of self-generating custom exemplars. For each problem, we\nobtain an output from LLMs using a temperature of 0, and report the accuracy.\n5.2\nMODELS\nWe experiment with several base LLMs: GPT-3.5-turbo, GPT-4 (OpenAI, 2023; Ouyang et al., 2022)\n(accessed in June\u2013September 2023), and PaLM 2-L (Anil et al., 2023).\n5.3\nMETHODS TO COMPARE\nWe compare the following prompting methods, including ours.\n0-shot and 0-shot CoT.\nThese methods, like ours, do not use labeled exemplars. We aim to show that\nour method offers more tailored guidance for LLM reasoning and yields superior task performance.\n5\nPublished as a conference paper at ICLR 2024\n## Algorithms: The core algorithm for solving this problem is the prefix product algorithm. \n## Tutorial: The prefix product algorithm is used to calculate the product of all the elements in a prefix of an array. The algorithm works \nby initializing a variable \"prefix\" to 1, and then iterating through the array from left to right. At each iteration, the current element is \nmultiplied with the prefix, and the result is stored in the prefix variable. This way, the prefix variable keeps track of the product of all \nthe elements in the prefix of the array. The time complexity of this algorithm is O(n), where n is the size of the input array. ......\n## Example Problems:\n1. Problem: Given an array of integers, find the product of all the elements in each subarray of size k.\n   Solution: We can use the prefix product algorithm to solve this problem. We initialize a prefix variable to 1, and then iterate through \nthe array from left to right. At each iteration, we multiply the current element with the prefix, ......\ndef product_of_subarrays(arr, k):\n    prefix = 1\n    result = []\n    for i in range(len(arr)):\n        prefix = prefix * arr[i]\n        if i >= k-1:\n            result.append(prefix)\n            prefix = prefix // arr[i-k+1]\n    return result\n2. Problem: Given an array of integers, find the longest subarray with a product less than or equal to a given target.\n   Solution: ......\n3. Problem: Given an array of integers, find the smallest k such that the product of all the elements in the prefix of the array is equal to \nthe product of all the elements in the suffix of the array.\n   Solution: ......\n## Python3 code to solve the original problem:\nExplanation of the solution:\n  - Initialize prefix and suffix variables to 1, and k to -1.\n  - Calculate the product of all the elements in the sequence using the prefix product algorithm.\n  - Iterate through the sequence from right to left.\n    - Update the prefix and suffix variables.\n    - If the prefix is equal to the suffix, update k to the current index.\n  - Print the value of k.\nt = int(input())\nfor _ in range(t):\n    n = int(input())\n    arr = list(map(int, input().split()))\n    prefix, suffix = 1, 1\n    k = -1\n    for i in range(n):\n        prefix = prefix * arr[i]\n    for i in range(n-1, 0, -1):\n        prefix = prefix // arr[i]\n        suffix = suffix * arr[i]\n        if prefix == suffix:\n            k = i\n    print(k)\nYour goal is to write Python3 code to solve competitive programming problems. Given a problem, explain the core concepts in it and provide \nother relevant problems. Then solve the original problem.\n# Problem:\nYou are given a sequence a_1, a_2, \u2026, a_n. Each element of a is 1 or 2. Find out if an integer k exists so that the following conditions are \nmet. 1 \u2264 k \u2264 n-1, and a_1 \u22c5 a_2 \u2026 a_k = a_{k+1} \u2026 a_n. If there exist multiple k that satisfy the given condition, print the smallest.\n# Instruction:\n## Algorithms:\nIdentify the core concepts or algorithms used to solve the problem. \n## Tutorial:\nWrite a tutorial about these algorithms.\n## Example Problems:\nProvide three examples of relevant competitive programming problems that involve these algorithms. For each problem, describe the problem, \nexplain the solution in detail, and then write the correct Python3 code.\n## Python3 code to solve the original problem:\n- Explanation of the solution:\n- Python3 code to solve the problem:\nOur Prompt:  Self-generate Exemplars + Knowledge\nOutput by GPT3.5-turbo\nSelf-generated Knowledge\nSelf-generated Exemplars\nYour task is to tackle mathemati\nproceed to solve the initial pro\n# Problem:\nAn airline serves a dinner to al\nfish meals are set aside for the\nthat both pilots get the fish?\n# Instructions:\n## Relevant Problems:\nRecall three examples of math pr\nfrom the initial problem (e.g., \n- After \"Q: \", describe the prob\n- After \"A: \", explain the solut\n## Solve the Initial Problem:\nQ: Copy and paste the initial pr\nA: Explain the solution and encl\nRelevant Problems:\nQ: In a bag, there are 5 red bal\nthat both balls are red?\nA: The total number of balls is \nare 4 red balls left out of 7 to\nevents happening is the product \n\\boxed{\\frac{5}{14}}.\nQ: A box contains 10 red marbles\nthat all three marbles are red?\nA: The total number of marbles i\ndrawn, there are 9 red marbles l\ntwo red marbles are drawn, there\nmarble is 8/13. The probability \ndrawing three red marbles is (10\nQ: A bag contains 4 green marble\nthe probability that both marble\nA: The total number of marbles i\ndrawn, there are 3 green marbles\nprobability of both events happe\nis (4/9) * (3/8) = \\boxed{\\frac{\nLet's solve the following math p\nAn airline serves a dinner to al\nfish meals are set aside for the\nthat both pilots get the fish?\nA: There are a total of 6 crew m\nthe first pilot, there are 2 fis\n2/5. The probability of both eve\nthe fish is (3/6) * (2/5) = \\box\nOur Prompt:  Self-generate Ex\nOutput by GPT3.5-turbo\nFigure 3: Actual example of our prompt (top) and LLM output (bottom) for the Codeforces task. Top: Our\nprompt supplements the problem statement with instructions to generate knowledge (e.g., tutorials on core\nconcepts) and relevant exemplars, followed by solving the original problem. Bottom: The knowledge\nand exemplars generated by GPT3.5-turbo are indeed relevant to the problem to solve, focusing on the\nprefix product algorithm. The final code generated by the LLM effectively applies the algorithm to solve\nthe problem. See \u00a7D.3 for the complete prompt and output.\nFew-shot CoT.\nThis is the standard few-shot CoT, using a fixed set of reasoning exemplars across all\ntest problems within a dataset. For the GSM8K and MATH datasets, as their training sets include solutions\nlabeled with reasoning steps, we use K = 5 exemplars from these training sets. For the other datasets,\nwe use K = 3 manually-annotated exemplars. We aim to show that our method, which self-generates\nexemplars, can match or surpass this baseline, which uses labeled exemplars.\nFew-shot retrieved CoT.\nInstead of using a fixed set of exemplars, for each test problem, we dynamically\nretrieve relevant labeled problem-solution pairs from the train set for each test problem. Specifically, we\n6\nPublished as a conference paper at ICLR 2024\nPrompting Method\nGSM8K Accuracy\nMATH Accuracy\nGPT3.5-turbo text-davinci-003 PaLM2\nGPT3.5-turbo\nPaLM2\n0-shot\n75.0%\n14.8%\n60.8%\n33.0%\n27.1%\n0-shot CoT\n75.8%\n50.3%\n78.2%\n33.9%\n29.8%\n5-shot CoT\n76.7%\n54.0%\n80.7%\n34.9%\n34.3%\nOurs: Self-generated Exemplars\n77.8%\n61.0%\u2020\n81.7%\n37.3%\n34.8%\nTable 1: Performance on mathematical tasks, GSM8K and MATH. Our prompting method, which self-generates\nexemplars, outperforms baselines such as 0-shot CoT and few-shot CoT. \u2020For text-davinci models, we use an in-context\ndemonstration of how to generate exemplars. For the other models, we do not.\nPrompting Method\nGPT3.5-turbo-16k\nGPT4\nAcc@1\nAcc@10\nAcc@1\nAcc@10\n0-shot\n8%\n24%\n16%\n30%\n0-shot CoT\n9%\n27%\n16%\n29%\n3-shot CoT\n11%\n27%\n17%\n31%\nOurs: Self-generated Exemplars\n13%\n25%\n17%\n32%\nOurs: Self-generated Knowledge + Exemplars\n15%\n29%\n19%\n37%\nTable 2: Performance on Codeforces code generation task. Our prompting method outperforms baselines such as\n0-shot CoT and few-shot CoT. Moreover, self-generating knowledge provides additional gains over self-generating\nexemplars, demonstrating its usefulness for the challenging Codeforces task.\nPrompting Method\nWord\nsorting\nLogical deduction\nfive objects\nTemporal\nsequences\nReasoning about\ncolored objects\nFormal\nfallacies\n0-shot\n66.8%\n30.0%\n40.4%\n50.4%\n53.6%\n0-shot CoT\n67.6%\n35.2%\n44.8%\n61.6%\n55.6%\n3-shot CoT\n68.4%\n36.4%\n58.0%\n62.0%\n55.6%\nOurs: Self-generated Exemplars\n75.2%\n41.6%\n57.6%\n68.0%\n58.8%\nTable 3: Performance on BIG-Bench reasoning tasks in accuracy. GPT3.5-turbo is used as the base LLM. Across\ndiverse tasks, our method outperforms baselines (0-shot CoT) and is competitive with manual 3-shot CoT.\nuse Sentence-BERT (Reimers & Gurevych, 2019) to encode each problem statement. For each problem\nin the test set, we retrieve the top K =5 similar problems from the training set based on cosine similarity.\nOur method.\nWe let LLMs self-generate K =5 exemplars for GSM8K and K =3 exemplars for MATH\nand BIG-Bench tasks. For Codeforces, we self-generate both knowledge and K =3 exemplars.\n6\nRESULTS\n6.1\nMAIN RESULTS\nMathematical problem solving.\nTable 1 presents results for GSM8K and MATH tasks. Our prompting\nmethod, which self-generates exemplars, outperforms baselines such as 0-shot CoT and few-shot CoT.\nThe improvement over few-shot CoT is notable for the MATH task, which involves a range of reasoning\ntypes, including algebra, probability, and geometry. This aligns with our approach of crafting tailored\nexemplars for each problem.\nFigure 1 and 2 provide qualitative examples of GPT3.5-turbo outputs generated using our prompt. In\nboth examples, the LLM indeed generates relevant exemplars (geometry problems in Figure 1. probability\nproblems in Figure 2), and subsequently produces correct solutions. In contrast, in the standard few-shot\nCoT (Figure 1, middle), the exemplars are math-related (e.g., algebra) but may not always match the test\nproblem (e.g., geometry), as the dataset contains diverse test problems.\nCode generation.\nTable 2 presents results for Codeforces task. Our prompting method outperforms\nbaselines such as 0-shot CoT and few-shot CoT in both GPT3.5-turbo and GPT4. Moreover, self-generating\nknowledge provides additional performance boost over self-generating exemplars, demonstrating its\nusefulness for the challenging Codeforces task. With our prompting method, GPT3.5-turbo achieves\ncompetitive performance with GPT4, with a 15% Acc@1 compared to GPT4\u2019s 16% Acc@1.\nFigure 3 (more complete version in \u00a7D.3) provides a qualitative example of GPT3.5-turbo output generated\nusing our prompt. The knowledge and exemplars generated by GPT3.5-turbo are indeed relevant to\n7\nPublished as a conference paper at ICLR 2024\nPrompting Method\n(\u2190 scale down)\ntext-curie-001\ntext-davinci-001\ntext-davinci-002\n(scale up \u2192)\ntext-davinci-003\n0-shot\n2%\n6%\n13%\n14%\n0-shot CoT\n2%\n6%\n22%\n50%\n5-shot (fixed) CoT\n2%\n10%\n43%\n54%\n5-shot retrieved CoT\n3%\n11%\n47%\n57%\nOurs: Self-generated Exemplars\n2%\n9%\n48%\n61%\nTable 4: Performance analysis using GSM8K task. Across varied scales/strengths of base LLMs (increasing\nfrom left to right), our prompting method outperforms 0-shot CoT and standard few-shot CoT with fixed exemplars.\nSelf-generated exemplars vs. retrieved exemplars: our method, with self-generated exemplars, performs better with\nlarger-scale LLMs, while few-shot CoT with retrieved exemplars performs better with smaller-scale LLMs.\nthe problem to solve, focusing on the prefix product algorithm. The final code generated by the LLM\neffectively applies the algorithm to solve the problem. In contrast, in the 0-shot CoT baseline, the LLM\noutput does not recall relevant exemplars and fails to employ the prefix product algorithm, resulting in\nan incorrect solution (\u00a7D.3).\nBIG-Bench reasoning tasks.\nTable 3 presents results for BIG-Bench tasks. Our prompting method\noutperforms baselines like 0-shot CoT, confirming its effectiveness across a wide range of tasks. Our\nmethod is also competitive with manual few-shot CoT. \u00a7D.4 offers GPT3.5-turbo output examples for\nthe deductive reasoning task (\u201cBIG-Bench formal fallacies\u201d). Using our prompting method, the LLM\ngenerates relevant deductive reasoning exemplars. Conversely, 0-shot CoT, with no relevant exemplars,\ntends to adopt an incorrect approach to address the deductive reasoning problem.\n6.2\nKNOWLEDGE CAN COMPLEMENT EXEMPLARS\nGenerating knowledge alongside exemplars is particularly useful in Codeforces task (Table 2), where\nLLMs need to apply nontrivial algorithms for code generation. In our qualitative analysis, we observe two\nconcrete advantages of generating knowledge: (1) knowledge act as high-level takeaways that complement\nlow-level exemplars, which prevents LLMs from overly relying on specific exemplars and helps to\ngeneralize to new problems; (2) when generating knowledge, LLMs identify the core concepts of the\nproblem and produce exemplars that align more closely in fundamental problem-solving approaches\n(e.g., the prefix product algorithm in Figure 3), rather than surface-level lexical similarities (e.g., without\nknowledge, LLMs tend to produce exemplars on palindromic sequences).\nThe performance gains achieved by generating knowledge are less significant in other tasks like GSM8K\nand BIG-Bench, however, likely because these tasks are less complex.\n6.3\nGENERATING VS RETRIEVING EXEMPLARS\nA key motivation behind our idea of self-generating exemplars is its ability to offer relevant exemplars\nfor problem solving. An alternative approach is to retrieve relevant exemplars from external data, provided\nthere is a labeled dataset of exemplars (e.g., the training set of GSM8K, which includes solutions labeled\nwith reasoning steps). What trade-offs exist between these two approaches?\nThe advantage of retrieval lies in its reliability. Exemplars retrieved from a labeled dataset are inherently\nvalid and correct, unlike generated exemplars, which lack this guarantee. Nevertheless, retrieval typically\nneeds labeled exemplars and involves a complex additional retrieval step.\nIn contrast, generation is more self-contained and convenient, as it does not rely on external labeled data\nor retrieval steps. Additionally, generation may yield exemplars better tailored to specific test problems\nbecause it can draw upon the entire (pre-)training data the LLM has been exposed to. The downside\nof generation is that it may fail to produce valid exemplars if the LLMs are weak or have not learned\nproblems related to the ones to be solved.\nTable 4 shows empirical results for GSM8K task, comparing our self-generated exemplars method (\u201cOurs\u201d)\nand the few-shot CoT method using exemplars retrieved from the GSM8K train set (\u201c5-shot retrieved CoT\u201d).\nWe conducted experiments using base LLMs of various scales, from text-curie-001 to text-davinci-003,\nwhere scale broadly indicates the amount of training data and parameter count used by the LLM.\nOur method outperforms the retrieved CoT with larger-scale LLMs, such as text-davinci-003. This is\nlikely because the LLM has effectively learned related tasks during training and can generate useful\n8\nPublished as a conference paper at ICLR 2024\nexemplars. Conversely, with smaller-scale LLMs, the retrieved CoT performs better, and self-generation\nfails to produce useful or valid exemplars.\n6.4\nSCALE OF BASE LLMS: ANALOGICAL PROMPTING EXCELS WITH LARGER MODELS\nTable 4 presents the result of using varying scales and strengths of base LLMs, ranging from text-curie-001\nto text-davinci-001 to text-davinci-002 and text-davinci-003 (more parameters and training data). Our\nprompting method surpasses vanilla 0-shot and 0-shot CoT across all scales. When using smaller-scale\nLLMs (text-curie-001 and text-davinci-001), few-shot CoT leveraging labeled exemplars exhibits\nsuperior performance compared to ours. However, as the LLMs are scaled up to text-davinci-002 and\ntext-davinci-003, our method outperforms few-shot CoT. This is due to the LLMs\u2019 enhanced ability to\nself-generate more relevant and useful exemplars.\n6.5\nNUMBER OF EXEMPLARS TO GENERATE\n# Exemplars to\nself-generate\nGSM8K\nMATH\nK =1\n76.1\n34.8\nK =2\n77.0\n36.7\nK =3\n77.5\n37.3\nK =4\n77.3\n37.0\nK =5\n77.8\n37.1\nTable 5: Analyzing the effect of varying the\nnumber of self-generated exemplars (K) in\nour approach. We assess performance on\nGSM8K and MATH tasks using GPT3.5-\nturbo as the base LLM.\nIn Table 5, we analyze the effect of varying the number of\nself-generated exemplars (K) in our approach. When K =1,\nthe LLM underperforms due to excessive reliance on a single\nexemplar generated. When K \u2265 3, the LLM demonstrates\nconsistent performance, with the best results observed at K =3\nor 5. This observation aligns with the findings in the standard\nfew-shot in-context learning in LLMs (Brown et al., 2020).\n6.6\nQUALITATIVE ANALYSIS\nWe manually analyzed the performance of our prompting ap-\nproach, based on 50 correctly and 50 incorrectly solved problems from GSM8K + MATH (50%, 50%).\n50 correctly solved problems:\n\u2022 (6/50) Generated exemplars are irrelevant\n\u2022 (9/50) Generated exemplars are relevant but contain incorrect solutions\n\u2022 (35/50) Generated exemplars are relevant and correct\n50 incorrectly solved problems:\n\u2022 (10/50) Generated exemplars are irrelevant\n\u2022 (12/50) Generated exemplars are relevant but contain incorrect solutions\n\u2022 (28/50) Generated exemplars are relevant and correct, but LLM fails to solve the new problem:\n\u2013 (12/50) A generalization gap between the exemplars and the new problem.\n\u2013 (8/50) Overreliance on specific exemplars, leading to misdirection.\n\u2013 (8/50) Other issues, such as calculation errors.\nThe generated exemplars were often relevant or correct. A common failure occurred when the LLM could\nnot solve the new problem due to a generalization gap (e.g., the new problem is harder than the exemplars).\nThis observation motivates future research to generate exemplars that not only possess relevance but also\nfacilitate generalization for solving new problems.\n7\nCONCLUSION\nWe introduced analogical prompting, a new language model prompting approach that self-generates\nrelevant reasoning exemplars for solving problems. This approach provides detailed, customized exemplars\nfor individual problems without requiring labeled data, effectively addressing the challenges faced by\nexisting 0-shot CoT and few-shot CoT prompting methods. Experimental results show that our approach\noutperforms 0-shot CoT and few-shot CoT in various reasoning tasks, including math problem solving,\ncode generation, and other logical/temporal reasoning tasks.\n9\nPublished as a conference paper at ICLR 2024\n8\nLIMITATIONS AND FUTURE RESEARCH\nOne limitation of our approach is increased inference computation, as our approach generates more tokens\nthan vanilla 0-shot and 0-shot CoT prompting. Compared to few-shot CoT, we use fewer input tokens and\nmore output tokens, as exemplars are counted as input in few-shot CoT and as output in our approach.\nAnother limitation is that self-generation can fail if the LLM lacks sufficient strength or has not learned\nrelevant knowledge to the new problems to solve. Conversely, with a stronger LLM, it can draw upon\nrelevant prior knowledge to tackle slightly more complex problems. Therefore, our approach is better\nsuited for stronger or larger-scale LLMs.\nFinally, it is known that LLM performance can be influenced by specific prompt phrases used to query the\nmodel (Jiang et al., 2020), and our work is also subject to this prompt sensitivity.\nACKNOWLEDGMENTS\nWe thank Qian Huang, Chen Liang, Andrew Dai and other colleagues at Google DeepMind, Stanford\nSNAP and P-Lambda groups for valuable feedback on this work.\nREFERENCES\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint\narXiv:2305.10403, 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nJiang Bian, Bin Gao, and Tie-Yan Liu. Knowledge-powered deep learning for word embedding. In\nMachine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014,\nNancy, France, September 15-19, 2014. Proceedings, Part I 14, pp. 132\u2013148. Springer, 2014.\nL\u00b4eon Bottou. From machine learning to machine reasoning: An essay. Machine learning, 94:133\u2013149,\n2014.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool\nmakers. arXiv preprint arXiv:2305.17126, 2023.\nJaime G Carbonell. Learning by analogy: Formulating and generalizing plans from past experience. In\nMachine learning, pp. 137\u2013161. Elsevier, 1983.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disen-\ntangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588,\n2022.\n10\nPublished as a conference paper at ICLR 2024\nXinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International\nConference on Learning Representations, 2018.\nXinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V Le. Neural symbolic\nreader: Scalable integration of distributed and symbolic representations for reading comprehension. In\nInternational Conference on Learning Representations, 2019.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir\nRadev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv\npreprint arXiv:2210.02875, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language\nmodeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.\norg/N19-1423.\nAndrew Drozdov, Nathanael Sch\u00a8arli, Ekin Aky\u00a8urek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint\narXiv:2209.15003, 2022.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop:\nA reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint\narXiv:1903.00161, 2019.\nKevin Dunbar. The analogical paradox: Why analogy is so easy in naturalistic settings yet so difficult in the\npsychological laboratory. The analogical mind: Perspectives from cognitive science, pp. 313\u2013334, 2001.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-\nhop relational reasoning for knowledge-aware question answering. In Empirical Methods in Natural\nLanguage Processing (EMNLP), 2020.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb\ndataset of diverse text for language modeling. arXiv preprint arXiv: Arxiv-2101.00027, 2020. URL\nhttps://arxiv.org/abs/Arxiv-2101.00027.\n11\nPublished as a conference paper at ICLR 2024\nDedre Gentner. Structure-mapping: A theoretical framework for analogy. Cognitive science, 7(2):155\u2013170,\n1983.\nDedre Gentner and Keith J Holyoak. Reasoning and learning by analogy: Introduction. American\npsychologist, 52(1):32, 1997.\nDedre Gentner and Arthur B Markman. Structure mapping in analogy and similarity. American psycholo-\ngist, 52(1):45, 1997.\nKavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. Synthesize, execute and debug:\nLearning to repair for neural program synthesis. Advances in Neural Information Processing Systems,\n33:17685\u201317695, 2020.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\nReasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.\nZhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming\nShi, and Xing Wang. Exploring human-like translation strategy with large language models. arXiv\npreprint arXiv:2305.04118, 2023.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv\npreprint arXiv:2105.09938, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint\narXiv:2103.03874, 2021b.\nKeith J Holyoak. Analogy and relational reasoning. The Oxford handbook of thinking and reasoning, pp.\n234\u2013259, 2012.\nXiaoyang Hu, Shane Storks, Richard L Lewis, and Joyce Chai. In-context analogical reasoning with\npre-trained language models. arXiv preprint arXiv:2305.17626, 2023.\nZiqi Huang, Hongyuan Zhu, Ying Sun, Dongkyu Choi, Cheston Tan, and Joo-Hwee Lim. A diagnostic\nstudy of visual question answering with analogical reasoning. In 2021 IEEE International Conference\non Image Processing (ICIP), pp. 2463\u20132467. IEEE, 2021.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models\nknow? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin\nChoi. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint\narXiv:2205.11822, 2022.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei\nZaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. arXiv preprint arXiv:2212.14024, 2022.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish\nSabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint\narXiv:2210.02406, 2022.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv\npreprint arXiv:2303.17491, 2023.\nHyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-\ngenerated in-context learning: Leveraging auto-regressive language models as a demonstration generator.\narXiv preprint arXiv:2206.08082, 2022.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners. In Thirty-sixth Conference on Neural Information Processing Systems\n(NeurIPS 2022), 2022. URL https://arxiv.org/abs/2205.11916.\n12\nPublished as a conference paper at ICLR 2024\nSumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang.\nSpoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems, 32, 2019.\nJunlong Li, Zhuosheng Zhang, and Hai Zhao. Self-prompting large language models for open-domain qa.\narXiv preprint arXiv:2212.08635, 2022a.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00b4emi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with\nalphacode. Science, 378(6624):1092\u20131097, 2022b.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models.\narXiv preprint arXiv:2211.09110, 2022.\nHunter\nLightman,\nVineet\nKosaraju,\nYura\nBurda,\nHarri\nEdwards,\nBowen\nBaker,\nTeddy\nLee,\nJan Leike,\nJohn Schulman,\nand Ilya Sutskever.\nImproving mathematical rea-\nsoning\nwith\nprocess\nsupervision.\n2023.\nURL\nhttps://openai.com/research/\nimproving-mathematical-reasoning-with-process-supervision.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph networks\nfor commonsense reasoning. In Empirical Methods in Natural Language Processing (EMNLP), 2019.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What\nmakes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,\npp. 100\u2013114, 2022. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.\ndeelio-1.10.\nInbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint\narXiv:2203.08242, 2022.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\nvia natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, 2022. doi:\n10.18653/v1/2022.acl-long.244. URL https://aclanthology.org/2022.acl-long.244.\nTom M Mitchell, Jaime G Carbonell, Ryszard S Michalski, and Rogers Hall. Analogical reasoning in the\ncontext of acquiring problem solving expertise. Machine Learning: A Guide to Current Research, pp.\n85\u201388, 1986.\nMichael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka,\nEduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In\nMachine Learning for Health (ML4H), pp. 353\u2013367. PMLR, 2023.\nOpenAI. Gpt-4 technical report. 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442,\n2023.\nGeorge Polya. How to solve it: A new aspect of mathematical method, volume 85. Princeton university\npress, 2004.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and\nnarrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is\nchatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476,\n2023.\n13\nPublished as a conference paper at ICLR 2024\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982\u20133992,\nHong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL\nhttps://aclanthology.org/D19-1410.\nHongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure\nLeskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop question answering\non knowledge graphs. In International Conference on Machine Learning (ICML), 2021.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761, 2023.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint\narXiv:2301.12652, 2023.\nKaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with\nchain-of-thought from labeled data. arXiv preprint arXiv:2302.12822, 2023.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615,\n2022.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot\nlearners. arXiv preprint arXiv:2209.01975, 2022. URL https://arxiv.org/abs/2209.01975.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\nmodels. arXiv preprint arXiv:2210.01296, 2022.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.\nStella Vosniadou and Andrew Ortony. Similarity and analogical reasoning. Cambridge University Press,\n1989.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv\npreprint arXiv:2203.11171, 2022.\nThomas B Ward, Steven M Smith, and Jyotsna Ed Vaid. Creative thought: An investigation of conceptual\nstructures and processes. American Psychological Association, 1997.\nTaylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models.\nNature Human Behaviour, pp. 1\u201316, 2023.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on\nLearning Representations, 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain\nof thought prompting elicits reasoning in large language models. In Thirty-sixth Conference on Neural\nInformation Processing Systems (NeurIPS 2022), 2022b. URL https://arxiv.org/abs/2201.11903.\nYuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse, and Christian Szegedy. Lime:\nLearning inductive bias for primitives of mathematical reasoning. In International Conference on\nMachine Learning, pp. 11251\u201311262. PMLR, 2021.\n14\nPublished as a conference paper at ICLR 2024\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. Unifiedskg: Unifying and multi-tasking structured\nknowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.\narXiv preprint arXiv:1809.09600, 2018.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint\narXiv:2305.10601, 2023.\nMichihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic\nfeedback. In International Conference on Machine Learning (ICML), pp. 10799\u201310808. PMLR, 2020.\nMichihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In\nInternational Conference on Machine Learning, pp. 11941\u201311952. PMLR, 2021.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. QA-GNN:\nReasoning with language models and knowledge graphs for question answering. In North American\nChapter of the Association for Computational Linguistics (NAACL), 2021.\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S\nLiang, and Jure Leskovec. Deep bidirectional language-knowledge graph pretraining. Advances in\nNeural Information Processing Systems, 35:37309\u201337323, 2022a.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pretraining language models with\ndocument links. In Association for Computational Linguistics (ACL), 2022b.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. In\nInternational Conference on Machine Learning (ICML), 2023.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning\nYao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain\nsemantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.\nEric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, and Nick Haber. Parsel: A unified natural\nlanguage framework for algorithmic reasoning. arXiv preprint arXiv:2212.10561, 2022.\nHanlin Zhang, Jiani Huang, Ziyang Li, Mayur Naik, and Eric Xing. Improved logical reasoning of language\nmodels via differentiable symbolic programming. arXiv preprint arXiv:2305.03742, 2023.\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning,\nand Jure Leskovec. Greaselm: Graph reasoning enhanced language models for question answering. In\nInternational Conference on Learning Representations (ICLR), 2022a.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large\nlanguage models. arXiv preprint arXiv:2210.03493, 2022b.\nWenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, Aman Madaan, and Tao Yu. Complex\nreasoning in natural language. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 6: Tutorial Abstracts), pp. 11\u201320, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-tutorials.2. URL https://\naclanthology.org/2023.acl-tutorials.2.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi\nSong, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with\ncode-based self-verification. arXiv preprint arXiv:2308.07921, 2023.\n15\nPublished as a conference paper at ICLR 2024\nDenny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/abs/2205.\n10625.\nA\nADDITIONAL RELATED WORKS\nA.1\nLANGUAGE MODELS AND REASONING\nReasoning involves the application of knowledge to derive solutions for new problems, often through a\nseries of steps. Teaching language models to reason has been a long-standing area of research (Bottou,\n2014; Zhao et al., 2023; Wei et al., 2022b).\nTo assess the reasoning capabilities of language models, researchers have created datasets for various tasks\nthat demand reasoning skills. These tasks include multi-step question answering (Yang et al., 2018; Dua\net al., 2019; Talmor et al., 2018), mathematical problem-solving (Cobbe et al., 2021; Hendrycks et al.,\n2021b), and code generation (Yu et al., 2018; Chen et al., 2021; Hendrycks et al., 2021a; Austin et al.,\n2021). In this study, we evaluate our methods using these diverse datasets.\nTo teach language models to reason effectively, one line of approaches involve training or fine-tuning\nthem. This can include using reasoning-intensive data during training (Wu et al., 2021; Yasunaga et al.,\n2022b; Lightman et al., 2023; Moor et al., 2023), retrieving structured knowledge (Lin et al., 2019; Feng\net al., 2020; Zhang et al., 2022a; Yasunaga et al., 2021; 2022a; Xie et al., 2022), and incorporating external\nmodules for reasoning such as logic and program execution (Chen et al., 2018; 2019; Yasunaga & Liang,\n2020; Gupta et al., 2020; Ren et al., 2021; Zhang et al., 2023).\nRecently, with the rise of large language models (LLMs), prompting them to engage in reasoning has\nproven effective and gained attention. A common approach is prompting LLMs to generate intermediate\nreasoning steps, as demonstrated by the chain-of-thought method (Wei et al., 2022b; Kojima et al., 2022;\nZhou et al., 2022; Wang et al., 2022), which assists LLMs in tackling complex reasoning tasks. Several\nstudies have extended this approach with more structured algorithms and search methods (Khot et al.,\n2022; Drozdov et al., 2022; Zelikman et al., 2022; Yao et al., 2023; Press et al., 2022; Khattab et al., 2022;\nJung et al., 2022), as well as longer-horizon action and planning (Yao et al., 2022; Hao et al., 2023; Park\net al., 2023). Another line of work incorporates tools and programs into the prompting process to facilitate\nreasoning (Chen et al., 2023; 2022; Cai et al., 2023; Cheng et al., 2022; Kim et al., 2023; Zhou et al., 2023;\nSchick et al., 2023).\nOur work complements these efforts to enhance LLM reasoning and is the first to draw inspiration from\nhuman analogical reasoning to improve LLM prompting.\nA.2\nANALOGICAL REASONING\nAnalogical reasoning is a cognitive process in which humans recall relevant past experiences when facing\nnew challenges (Gentner & Holyoak, 1997; Gentner, 1983; Holyoak, 2012). This phenomenon has been\nstudied extensively in psychology, revealing its significance in various cognitive tasks such as problem-\nsolving (Gentner & Markman, 1997) and creativity (Ward et al., 1997). It is rooted in the capacity to\nidentify structural and relational similarities between past and current situations, facilitating knowledge\ntransfer (Dunbar, 2001).\nAnalogical reasoning has also influenced the development of artificial intelligence and machine learning\nalgorithms (Carbonell, 1983; Mitchell et al., 1986) and has been employed as a reasoning benchmark for\nassessing machine learning models (Bian et al., 2014; Huang et al., 2021). A recent work also evaluates the\nability of language models to identify analogies (Webb et al., 2023; Hu et al., 2023).\nOur work makes a pioneering effort of applying analogical reasoning principles to enhance language model\ninference.\n16\nPublished as a conference paper at ICLR 2024\nB\nCODEFORCES DATA COLLECTION\nWe scraped data from codeforces.com, following the procedure in prior works (Li et al., 2022b; Kulal et al.,\n2019; Yasunaga & Liang, 2021). We use Level-A problems that were published between January 2023 and\nAugust 2023. Each problem includes the full problem descriptions and test cases accessible on the website.\nThe test cases include the public test cases found on the problem page and hidden test cases made available\non the evaluation result pages once a contest is finished. Some of the hidden test cases were truncated\non the website due to excessive input/output length, and we skipped those. We retained problems whose\nproblem descriptions were within the length of 2000 tokens in GPT3.5-turbo, resulting in 50 problems.\nBecause this dataset is relatively small, we conduct the evaluation twice and then report the average results.\nC\nADDITIONAL RESULTS\nPrompting Method\nGSM8K\nMATH\nOurs: Non-diverse exemplars\n75.9%\n35.2%\nOurs: Diverse exemplars\n77.8%\n37.3%\nTable 6: Ablation study on the effect of encouraging diversity in self-generating exemplars (\u00a74.1).\nPrompting Method\nAcc@1\nAcc@10\nOurs: Self-generate knowledge after exemplars\n14%\n27%\nOurs: Self-generate knowledge before exemplars\n15%\n29%\nTable 7: Ablation study on the Codeforces task using GPT3.5-turbo. Self-generating knowledge before exemplars\nperforms slightly better than self-generating knowledge after exemplars (\u00a74.2).\nPrompting Method\nGSM8K\nMATH\nOurs\n77.8%\n37.3%\nOurs + Self-consistency\n85.3%\n46.0%\nTable 8: Self-consistency can be used to further boost the performance of our analogical prompting approach.\nD\nFULL EXAMPLES\nD.1\nMATH\nOur prompt (self-generate exemplars):\nYour task is to tackle mathematical\nproblems. When presented with a math\nproblem , recall relevant problems as examples. Afterward , proceed to\nsolve the initial problem.\n# Initial Problem:\nAn airline serves a dinner to all the passengers on an airplane. They get\ntheir choice of steak or fish. Three steak meals and three fish\nmeals are set aside for the six -member crew. If the meals are\ndistributed to the crew members randomly , what is the probability\nthat both pilots get the fish?\n# Instructions :\nMake sure to include all of the following points:\n## Relevant Problems:\nRecall three examples of math problems that are relevant to the initial\nproblem. Note that your problems should be distinct from each other\nand from the initial problem (e.g., involving\ndifferent numbers and\nnames). For each problem:\n- After \"Q: \", describe the problem\n17\nPublished as a conference paper at ICLR 2024\n- After \"A: \", explain the solution and enclose the ultimate answer in \\\nboxed {}.\n## Solve the Initial Problem:\nSay \"Let 's solve the following math problem .\" Then formulate your\nresponse in the following format:\nQ: Copy and paste the initial problem here.\nA: Explain the solution and enclose the ultimate answer in \\boxed {} here.\nOutput by GPT3.5-turbo:\nRelevant Problems:\nQ: In a bag , there are 5 red balls and 3 blue balls. If two balls are\ndrawn at random without replacement , what is the probability that\nboth balls are red?\nA: The total number of balls is 8. The probability of drawing a red ball\non the first draw is 5/8. After one red ball is drawn , there are 4\nred balls left out of 7 total balls. Therefore , the probability of\ndrawing a second red ball is 4/7. The probability of both events\nhappening is the product of the individual probabilities , so the\nprobability of drawing two red balls is (5/8) * (4/7) = \\boxed {\\ frac\n{5}{14}}.\nQ: A box contains 10 red marbles and 5 blue marbles. If three marbles are\ndrawn at random without replacement , what is the probability that\nall three marbles are red?\nA: The total number of marbles is 15. The probability of drawing a red\nmarble on the first draw is 10/15. After one red marble is drawn ,\nthere are 9 red marbles left out of 14 total marbles. Therefore , the\nprobability of drawing a second red marble is 9/14. After two red\nmarbles are drawn , there are 8 red marbles left out of 13 total\nmarbles. Therefore , the probability of drawing a third red marble is\n8/13. The probability of all three events happening is the product of\nthe individual probabilities , so the probability of drawing three\nred marbles is (10/15) * (9/14) * (8/13) = \\boxed {\\ frac {24}{91}}.\nQ: A bag contains 4 green marbles , 3 red marbles , and 2 blue marbles. If\ntwo marbles are drawn at random without replacement , what is the\nprobability that both marbles are green?\nA: The total number of marbles is 9. The probability of drawing a green\nmarble on the first draw is 4/9. After one green marble is drawn ,\nthere are 3 green marbles left out of 8 total marbles. Therefore , the\nprobability of drawing a second green marble is 3/8. The probability\nof both events happening is the product of the individual\nprobabilities , so the probability of drawing two green marbles is\n(4/9) * (3/8) = \\boxed {\\ frac {1}{6}}.\nLet 's solve the following math problem:\nAn airline serves a dinner to all the passengers on an airplane. They get\ntheir choice of steak or fish. Three steak meals and three fish\nmeals are set aside for the six -member crew. If the meals are\ndistributed to the crew members randomly , what is the probability\nthat both pilots get the fish?\nA: There are a total of 6 crew members , so the probability of the first\npilot getting the fish is 3/6. After one fish meal is given to the\nfirst pilot , there are 2 fish meals left out of 5 total meals.\nTherefore , the probability of the second pilot getting the fish is\n2/5. The probability of both events happening is the product of the\nindividual probabilities , so the probability that both pilots get the\nfish is (3/6) * (2/5) = \\boxed {\\ frac {1}{5}}.\n18\nPublished as a conference paper at ICLR 2024\nD.2\nGSM8K\nOur prompt (self-generate exemplars):\nYour task is to tackle mathematical\nproblems. When presented with a math\nproblem , recall relevant problems as examples. Afterward , proceed to\nsolve the initial problem.\n# Initial Problem:\nKyle bought last year 's best -selling book for $19 .50. This is with a 25%\ndiscount from the original price. What was the original price of the\nbook?\n# Instructions :\nMake sure that your response follows the instructions below.\n## Analogous Problems:\nOffer five diverse examples of math problems that are relevant or\nanalogous to the initial problem. For each problem , elaborate on the\nsolution and conclude with the ultimate answer (enclosed in \\boxed {})\n. For each problem:\n- After \"Q: \", describe the problem\n- After \"A: \", explain the solution and enclose the ultimate answer in \\\nboxed {}.\n## Solve the Initial Problem:\nQ: Copy and paste the initial problem here.\nA: Explain the solution and enclose the ultimate answer in \\boxed {} here.\nOutput by GPT3.5-turbo:\nAnalogous Problems:\nQ1: A shirt is on sale for 20% off the original price of $40. What is the\nsale price of the shirt?\nA: The original price of the shirt is $40. With a 20% discount , the\ndiscount amount is 20% of $40 , which is 0.20 * $40 = $8. Therefore ,\nthe sale price of the shirt is $40 - $8 = $32. The answer is $\\boxed\n{32}$.\nQ2: A store is offering a 15% discount on all items. If a customer buys a\npair of shoes for $85 , what was the original price of the shoes?\nA: Let 's assume the original price of the shoes is $x. With a 15%\ndiscount , the discount amount is 15% of $x , which is 0.15 * $x = $0\n.15x. The sale price of the shoes is $85 , so we can set up the\nequation: $x - $0 .15x = $85. Simplifying the equation , we get 0.85x =\n$85. Dividing both sides by 0.85 , we find that x = $100. Therefore ,\nthe original price of the shoes was $\\boxed {100}$.\nQ3: A restaurant offers a 10% discount on all meals. If a customer pays\n$45 for a meal , what was the original price of the meal?\nA: Let 's assume the original price of the meal is $x. With a 10% discount\n, the discount amount is 10% of $x , which is 0.10 * $x = $0 .10x. The\nsale price of the meal is $45 , so we can set up the equation: $x - $0\n.10x = $45. Simplifying the equation , we get 0.90x = $45. Dividing\nboth sides by 0.90 , we find that x = $50. Therefore , the original\nprice of the meal was $\\boxed {50}$.\nQ4: A laptop is on sale for 30% off the original price of $800. What is\nthe sale price of the laptop?\nA: The original price of the laptop is $800. With a 30% discount , the\ndiscount amount is 30% of $800 , which is 0.30 * $800 = $240.\nTherefore , the sale price of the laptop is $800 - $240 = $560. The\nanswer is $\\boxed {560}$.\n19\nPublished as a conference paper at ICLR 2024\nQ5: A toy store is offering a 12% discount on all toys. If a customer\nbuys a toy for $22 , what was the original price of the toy?\nA: Let 's assume the original price of the toy is $x. With a 12% discount ,\nthe discount amount is 12% of $x , which is 0.12 * $x = $0 .12x. The\nsale price of the toy is $22 , so we can set up the equation: $x - $0\n.12x = $22. Simplifying the equation , we get 0.88x = $22. Dividing\nboth sides by 0.88 , we find that x = $25. Therefore , the original\nprice of the toy was $\\boxed {25}$.\nSolve the Initial Problem:\nQ: Kyle bought last year 's best -selling book for $19 .50. This is with a\n25% discount from the original price. What was the original price of\nthe book?\nA: Let 's assume the original price of the book is $x. With a 25% discount\n, the discount amount is 25% of $x , which is 0.25 * $x = $0 .25x. The\nsale price of the book is $19 .50, so we can set up the equation: $x -\n$0 .25x = $19 .50. Simplifying the equation , we get 0.75x = $19 .50.\nDividing both sides by 0.75 , we find that x = $26. Therefore , the\noriginal price of the book was $\\boxed {26}$.\nD.3\nCODEFORCES\nOur prompt (self-generate exemplars and knowledge):\nYour goal is to write Python3 code to solve competitive\nprogramming\nproblems. Given a problem , explain the core concepts in it and\nprovide other relevant problems. Then solve the original problem.\n# Problem:\nYou are given a sequence a_1 , a_2 , \\ldots , a_n. Each element of a is 1 or\n2. Find out if an integer k exists so that the following\nconditions\nare met.\n1 \\leq k \\leq n-1, and\na_1 \\cdot a_2 \\cdot \\ldots \\cdot a_k = a_{k+1}\n\\cdot a_{k+2} \\cdot \\ldots \\cdot a_n , where \\cdot denotes a product\n.\nIf there exist multiple k that satisfy the given condition , print the\nsmallest.\nInput\nEach test contains multiple test cases. The first line contains the\nnumber of test cases t (1 \\le t \\le 100). Description of the test\ncases follows.\nThe first line of each test case contains one integer n (2 \\leq n \\leq\n1000).\nThe second line of each test case contains n integers a_1 , a_2 , \\ldots ,\na_n (1 \\leq a_i \\leq 2).\nOutput\nFor each test case , if there is no such k, print\n-1.\nOtherwise , print the smallest possible k.\n# Instruction :\n## Algorithms :\nIdentify the core concepts or algorithms used to solve the problem.\n## Tutorial:\nWrite a useful tutorial about these algorithms .\n## Example Problems:\nProvide three examples of relevant\ncompetitive\nprogramming\nproblems that\ninvolve these algorithms . For each problem , describe the problem ,\n20\nPublished as a conference paper at ICLR 2024\nexplain the solution in detail , and then write the correct Python3\ncode.\n## Python3 code to solve the original problem:\nInclude the following points in your response:\n- Explanation of the solution:\n- Python3 code to solve the problem:\nOutput by GPT3.5-turbo-16k:\n## Algorithms :\nThe core algorithm for solving this problem is the prefix product\nalgorithm.\n## Tutorial:\nThe prefix product algorithm is used to calculate the product of all the\nelements in a prefix of an array. It is a useful algorithm for\nsolving problems that involve calculating the product of all the\nelements in a subarray.\nThe algorithm works by initializing a variable \"prefix\" to 1, and then\niterating through the array from left to right. At each iteration ,\nthe current element is multiplied with the prefix , and the result is\nstored in the prefix variable. This way , the prefix variable keeps\ntrack of the product of all the elements in the prefix of the array.\nHere is the pseudocode for the prefix product algorithm :\n```\nprefix = 1\nfor i in range(n):\nprefix = prefix * arr[i]\n```\nThe time complexity of this algorithm is O(n), where n is the size of the\ninput array.\n## Example Problems:\n1. Problem: Given an array of integers , find the product of all the\nelements in each subarray of size k.\nSolution: We can use the prefix product algorithm to solve this\nproblem. We initialize a prefix variable to 1, and then iterate\nthrough the array from left to right. At each iteration , we\nmultiply the current element with the prefix , and store the result\nin the prefix variable. If the index i is greater than or equal\nto k-1, we divide the prefix by the element arr[i-k+1], which\nrepresents the first element of the subarray of size k starting at\nindex i-k+1. The result is the product of all the elements in the\nsubarray of size k starting at index i-k+1. We can store these\nresults in a new array and return it as the output.\n```python\ndef product_of_subarrays (arr , k):\nn = len(arr)\nprefix = 1\nresult = []\nfor i in range(n):\nprefix = prefix * arr[i]\nif i >= k -1:\nresult.append(prefix)\nprefix = prefix // arr[i-k+1]\nreturn result\n21\nPublished as a conference paper at ICLR 2024\n```\n2. Problem: Given an array of integers , find the longest subarray with a\nproduct less than or equal to a given target.\nSolution: We can use the prefix product algorithm to solve this\nproblem. We initialize a prefix variable to 1, and then iterate\nthrough the array from left to right. At each iteration , we\nmultiply the current element with the prefix , and store the result\nin the prefix variable. If the prefix becomes greater than the\ntarget , we divide the prefix by the first element of the subarray ,\nand move the start pointer of the subarray one step forward. We\nupdate the maximum length of the subarray if necessary .\n```python\ndef longest_subarray_with_product_less_than_target (arr , target):\nn = len(arr)\nprefix = 1\nstart = 0\nmax_length = 0\nfor i in range(n):\nprefix = prefix * arr[i]\nwhile prefix > target and start\n<= i:\nprefix = prefix // arr[start]\nstart += 1\nmax_length = max(max_length , i-start +1)\nreturn max_length\n```\n3. Problem: Given an array of integers , find the smallest k such that the\nproduct of all the elements in the prefix of the array is equal to\nthe product of all the elements in the suffix of the array.\nSolution: We can use the prefix product algorithm to solve this\nproblem. We initialize a prefix variable to 1, and then iterate\nthrough the array from left to right. At each iteration , we\nmultiply the current element with the prefix , and store the result\nin the prefix variable. We also initialize a suffix variable to\n1, and then iterate through the array from right to left. At each\niteration , we multiply the current element with the suffix , and\nstore the result in the suffix variable. If the prefix is equal to\nthe suffix , we return the current index as the smallest k. If no\nsuch index is found , we return\n-1.\n```python\ndef smallest_k_with_equal_product (arr):\nn = len(arr)\nprefix = 1\nsuffix = 1\nfor i in range(n):\nprefix = prefix * arr[i]\nsuffix = suffix * arr[n-i -1]\nif prefix == suffix:\nreturn i+1\nreturn\n-1\n```\n## Python3 code to solve the original problem:\nExplanation of the solution:\n- Read the number of test cases.\n22\nPublished as a conference paper at ICLR 2024\n- Iterate through each test case.\n- Read the number of elements in the sequence and the sequence itself.\n- Initialize prefix and suffix variables to 1, and k to\n-1.\n- Calculate the product of all the elements in the sequence using the\nprefix product algorithm .\n- Iterate through the sequence from right to left.\n- Update the prefix and suffix variables .\n- If the prefix is equal to the suffix , update k to the current index\n.\n- Print the value of k.\n```python\n# Read the number of test cases\nt = int(input ())\n# Process each test case\nfor _ in range(t):\n# Read the test case input\nn = int(input ())\narr = list(map(int , input ().split ()))\n# Initialize prefix and suffix variables\nprefix = 1\nsuffix = 1\nk = -1\n# Calculate prefix product\nfor i in range(n):\nprefix = prefix * arr[i]\n# Calculate suffix product and find smallest k\nfor i in range(n-1, 0, -1):\nprefix = prefix // arr[i]\nsuffix = suffix * arr[i]\nif prefix == suffix:\nk = i\n# Print the result\nprint(k)\n```\nBelow is a baseline output generated by GPT3.5-turbo-16k using vanilla 0-shot prompt. This output does\nnot recall any relevant exemplars and fails to employ the prefix product algorithm, resulting in an incorrect\nsolution.\ndef smallest_k (t, test_cases ):\nresults = []\nfor i in range(t):\nn = test_cases [i][0]\na = test_cases [i][1:]\nk = -1\nfor j in range (1, n):\nif a[:j] == a[j:]:\nk = j\nbreak\nresults.append(k)\nreturn results\nt = int(input ())\ntest_cases = []\nfor _ in range(t):\nn = int(input ())\na = list(map(int , input ().split ()))\n23\nPublished as a conference paper at ICLR 2024\ntest_cases .append ([n] + a)\noutput = smallest_k (t, test_cases )\nfor o in output:\nprint(o)\nD.4\nBIG-BENCH: FORMAL FALLACIES\nOur prompt (self-generate exemplars):\nYour task is to tackle reasoning problems. When presented with a problem ,\nrecall relevant problems as examples. Afterward , proceed to solve\nthe initial problem.\n# Initial Problem:\n\"Is Siri a stepsister of Mary? Is Susan related to Kate? In large\nfamilies , it is sometimes\ndifficult to keep track of all one 's\nrelatives. The following argument seeks to clarify some such\nrelations: To start with , everyone who is a schoolmate of Mildred is\na daughter of Jeanette , too. Now , being a daughter of Jeanette is\nsufficient for not being a cousin of Cristi. Therefore , no schoolmate\nof Mildred is a cousin of Cristi .\" Is the argument , given the\nexplicitly stated premises , deductively valid or invalid? Options: -\nvalid\n- invalid\n# Instructions :\nMake sure to include all of the following points:\n## Relevant Problems:\nRecall three examples of problems that are relevant to the initial\nproblem. Note that your problems must be distinct from each other and\nfrom the initial problem. For each problem:\n- After \"Q: \", describe the problem\n- After \"A: \", explain the solution and enclose the ultimate answer in \\\nboxed {}.\n## Solve the Initial Problem:\nSay \"Let 's solve the following\nreasoning problem .\" Then formulate your\nresponse in the following format:\nQ: Copy and paste the initial problem here.\nA: Explain the solution and enclose the ultimate answer in \\boxed {} here.\nOutput by GPT3.5-turbo:\nRelevant Problems:\nQ: In a certain town , all doctors are required to have a medical degree.\nJohn is a doctor. Therefore , John must have a medical degree. Is this\nargument valid or invalid?\nA: This argument is valid because it follows the logical form of a\ncategorical syllogism , where the first premise establishes a\nuniversal\nstatement and the second premise applies that statement to\na specific case. Therefore , the conclusion can be logically deduced.\n\\boxed{valid}\nQ: All cats have fur. Fluffy is a cat. Therefore , Fluffy has fur. Is this\nargument valid or invalid?\nA: This argument is valid because it follows the logical form of a\ncategorical syllogism , where the first premise establishes a\nuniversal\nstatement and the second premise applies that statement to\na specific case. Therefore , the conclusion can be logically deduced.\n\\boxed{valid}\nQ: If it is raining , then the ground is wet. The ground is wet. Therefore\n, it must be raining. Is this argument valid or invalid?\n24\nPublished as a conference paper at ICLR 2024\nA: This argument is invalid because it commits the fallacy of affirming\nthe consequent . Just because the ground is wet does not necessarily\nmean that it is raining , as there could be other factors that caused\nthe ground to become wet. \\boxed{invalid}\nSolve the Initial Problem:\nLet 's solve the following\nreasoning problem:\nQ: \"Is Siri a stepsister of Mary? Is Susan related to Kate? In large\nfamilies , it is sometimes\ndifficult to keep track of all one 's\nrelatives. The following argument seeks to clarify some such\nrelations: To start with , everyone who is a schoolmate of Mildred is\na daughter of Jeanette , too. Now , being a daughter of Jeanette is\nsufficient for not being a cousin of Cristi. Therefore , no schoolmate\nof Mildred is a cousin of Cristi .\" Is the argument , given the\nexplicitly stated premises , deductively valid or invalid?\nA: The argument is deductively valid. The premises state that everyone\nwho is a schoolmate of Mildred is a daughter of Jeanette , and being a\ndaughter of Jeanette is sufficient for not being a cousin of Cristi.\nTherefore , it logically follows that no schoolmate of Mildred is a\ncousin of Cristi. \\boxed{valid}\nBelow is a baseline output generated by GPT3.5-turbo using 0-shot CoT prompt. This output does not\nrecall any relevant exemplars and adopts an incorrect approach to address the deductive reasoning problem.\nThe argument is deductively invalid.\nTo determine the validity of the argument , we need to analyze the logical\nstructure and see if the conclusion\nnecessarily follows from the\npremises.\nLet 's break down the argument:\n1. Everyone who is a schoolmate of Mildred is a daughter of Jeanette.\n2. Being a daughter of Jeanette is sufficient for not being a cousin of\nCristi.\nConclusion : No schoolmate of Mildred is a cousin of Cristi.\nThe first premise establishes a relationship\nbetween being a schoolmate\nof Mildred and being a daughter of Jeanette. However , it does not\nprovide any information about the relationship\nbetween being a\nschoolmate of Mildred and being a cousin of Cristi.\nThe second premise establishes a relationship\nbetween being a daughter of\nJeanette and not being a cousin of Cristi. However , it does not\nprovide any information about the relationship\nbetween being a\nschoolmate of Mildred and being a cousin of Cristi.\nTherefore , the conclusion that no schoolmate of Mildred is a cousin of\nCristi cannot be logically derived from the given premises. The\nargument is deductively invalid.\n\\boxed{invalid}\n25\n"
  },
  {
    "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
    "link": "https://arxiv.org/pdf/2310.01557.pdf",
    "upvote": "12",
    "text": "Published as a conference paper at ICLR 2024\nSMARTPLAY : A BENCHMARK FOR LLMS AS INTELLI-\nGENT AGENTS\nYue Wu12\u2217, Xuan Tang1, Tom Mitchell1, Yuanzhi Li12\n1Carnegie Mellon University, 2Microsoft Research\nABSTRACT\nRecent large language models (LLMs) have demonstrated great potential toward\nintelligent agents and next-gen automation, but there currently lacks a systematic\nbenchmark for evaluating LLMs\u2019 abilities as agents. We introduce SmartPlay:\nboth a challenging benchmark and a methodology for evaluating LLMs as agents.\nSmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower\nof Hanoi, Minecraft. Each game features a unique setting, providing up to 20\nevaluation settings and infinite environment variations. Each game in SmartPlay\nuniquely challenges a subset of 9 important capabilities of an intelligent LLM agent,\nincluding reasoning with object dependencies, planning ahead, spatial reasoning,\nlearning from history, and understanding randomness. The distinction between the\nset of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall\nperformance of LLM agents but also as a road-map for identifying gaps in current\nmethodologies. We release our benchmark at github.com/microsoft/SmartPlay.\n1\nINTRODUCTION\nFigure 1: SmartPlay provides a unified and expandable API with text observations and guidance to\nperform turn by turn LLM inference on Two-armed Bandits, Rock Paper Scissors, Messenger (Hanjie\net al., 2021), Crafter (Hafner, 2021), and Minecraft (Fan et al., 2022) creative navigation tasks.\nCreating intelligent agents (Wooldridge & Jennings, 1995), that perceives its environment and per-\nform autonomous actions, has been one of the core objectives of A.I. (Laird et al., 1987; Russell,\n2010) Recently, large language models (LLMs) (Smith et al., 2022; Chowdhery et al., 2022; Ope-\nnAI, 2023; Manyika; Driess et al., 2023; Touvron et al., 2023) have made remarkable progress in\nvarious tasks (Bubeck et al., 2023). Some language models demonstrate exceptional planning (Ahn\net al., 2022; Wu et al., 2023b), reasoning (Wu et al., 2023a; Shinn et al., 2023), and problem-\nsolving (Madaan et al., 2023; Kim et al., 2023) abilities, enabling the potential as generalist agents\nfor virtual-reality (Park et al., 2023) or real-world problem-solving.\nSuch potential has attracted strong interest on applications where LLM systems actively invoke tools\nand APIs to complete a wide range of tasks goals (Significant-Gravitas; Yoheinakajima; Reworkd;\n\u2217Work done during internship at Microsoft Research. Correspondence to ywu5@andrew.cmu.edu\n1\narXiv:2310.01557v5  [cs.LG]  17 Mar 2024\nPublished as a conference paper at ICLR 2024\nWang et al., 2023a; Qin et al., 2023), and actively interact and make changes in an environment to\nachieve specific results (Wang et al., 2023b;a; Wu et al., 2023b;c). LLMs as agents could be seen as\nan important step toward next-gen automation.\nDespite great public attention, the capabilities of LLMs as agents have not been systematically\nstudied, partly due to the lack of standardized LLM benchmark for agent-environment interaction.\nCurrent LLM benchmarks have been designed for static knowledge and reasoning (Hendrycks et al.,\n2020; Liang et al., 2022; Srivastava et al., 2022a; Zhong et al., 2023), or helpful and harmless\nconversations (Bai et al., 2022; Zheng et al., 2023a; Dubois et al., 2023), overlooking applications to\nintelligent agents.\nWe note 4 key challenges for intelligent LLM agents not captured in previous benchmarks. First,\nlots of real-world tasks require an agent to do long-horizon planning. Second, many events are\nprobabilistic and an intelligent agent is expected to understand the odds. Third, an intelligent\nagent needs spatial reasoning to understand our 3D world. Fourth, when encountered with unseen\nsituations, an intelligent agent should be able to learn from interactions or mistakes.\nOn the other hand, games have long been identified as go-to benchmarks for intelligent generalist\nagents (Pell, 2011; Genesereth et al., 2005; Whiteson et al., 2010; Schaul et al., 2011; Bellemare\net al., 2013; C\u02c6ot\u00b4e et al., 2019; Hafner, 2021; Guss et al., 2021; Fan et al., 2022). At the core of game\ndesign (Koster, 2013), successful games often involve \u201cproblem-solving\u201d, \u201ccalculation of odds\u201d,\n\u201cspatial reasoning\u201d, \u201cchanging difficulties\u201d, and \u201cwell-defined and quantifiable outcome\u201d, therefore\noffering perfect complement to existing LLM benchmarks. Finally, some game environments are\nprocedurally generated and game states grow exponentially, making games more robust against\nevaluation dataset contamination as observed in recent works (Touvron et al., 2023). Experimentally,\nwe observe LLMs struggle to memoize intermediate states of a simple 3-disk Tower of Hanoi game.\nTaking a unique agent perspective in benchmarking LLMs, we introduce SmartPlay, a benchmark\nfrom 6 distinct games augmented with language descriptors for visual observation (Figure 1), offering\nup to 20 different settings and infinite environment variations. Each game presents unique challenges\nthat span multiple dimensions of intelligent agents, as detailed in Table 3. The games range in\ncomplexity, from requiring simple one-step reasoning and rule-following in Bandits, to intricate\nlong-term planning, multi-hop dependencies, and learning from interactions in Crafter (Hafner, 2021)\nand Hanoi. SmartPlay engages LLM agents in both deterministic and stochastic settings, demanding\nskills from basic text understanding to 3D spatial reasoning.\nGames in SmartPlay have been built with well-defined objectives and evaluation metrics: completion\nrate, reward, score. Therefore, SmartPlay provides a fully automated pipeline to conduct standardized\nevaluation for LLMs.We use SmartPlay to compare the agent performance of recent LLMs, and\nidentify several research gaps for applying LLMs as agents. We believe that SmartPlay sets a goal\nthat is reachable in a short time-frame yet formidable to require new breakthroughs.\n2\nCAPABILITIES NECESSARY FOR INTELLIGENT AGENTS\nBorrowing concepts from game design (Koster, 2013), we identify 9 key abilities important for\nintelligent LLM agents, and identify multiple degrees for each capability:\na) Long text understanding: general LLM capability.\n\u2022 We define 4 degrees based on document length and syntactic variations: 1) few fixed lines,\n2) few fixed paragraphs, 3) with syntactic variations, 4) and longer than 1 page (500 words).\nb) Reasoning: multi-hop logical reasoning and deduction, often required for analyzing the interac-\ntions of in-game objects or action conditions/dependencies.\n\u2022 We define 3 degrees based on reasoning hops: 1) (0 \u223c 1), 2) (2 \u223c 3), 3) (> 3).\nc) Instruction/Rule following: follow rules and instructions set by environment or users.\n\u2022 We define 3 degrees based on number of game rules: 1) single rule, 2) (< 5), 3) (5+)\nd) Planning: long-horizon in-context planning to achieve a complex goal.\n\u2022 We define 3 degrees based on planning steps, and concurrent objectives which requires goal\nprioritization: 1) < 5 planning steps, 2) 5+ planning steps, 3) concurrent objectives\ne) Generalization: Excels at a wide range of tasks.\n2\nPublished as a conference paper at ICLR 2024\n\u2022 We define 3 degrees based on the variability the game provides: 1) fixed environment, 2)\nfixed game word with random objectives, 3) procedurally generated game world\nf) Understanding the odds: analyze and estimate the probability of random events.\n\u2022 We define 3 degrees based on the importance randomness in the environment: 1) no\nrandomness, 2) randomness present in game, 3) randomness as core game mechanism\ng) Learning from interactions: acquire environment knowledge from live interactions.\n\u2022 We define 4 degrees based on the number of unique interactions to learn from: 1) no learning\nrequired, 2) single interaction, 3) < 5 interactions, 4) 5+ interactions\nh) Error/Mistake handling: recover from mistakes (e.g., correcting from erroneous trajectory).\n\u2022 We define 3 degrees based on if mistake handling may be necessary and if additional\nreasoning and re-planning is necessary: 1) not required, 2) simple rollback corrects error, 3)\nreasoning and re-planning required to correct error.\ni) Spatial reasoning: understand our world in 2D/3D. Spatial reasoning is typically required to\nunderstand directions and navigate through the game world (e.g., navigating the 2D/3D world).\n(a) We define 3 degrees based on dimensionality: 1) 0 \u223c 1D, 2) 2D, 3) 3D\n3\nGAMES IN SMARTPLAY\n3.1\nRESEARCH CHALLENGES\nThe SmartPlay benchmark encapsulates a diverse set of challenges that evaluate various AI capabil-\nities, as itemized in Figure 2. For instance, Bandits primarily focuses on understanding the odds,\nrequiring minimum text understanding and rule-following. On the other hand, Rock Paper Scissors\nuniquely puts an emphasis on understanding the odds and multiple game rules. Hanoi presents\nan advanced setting for object dependency reasoning, strategic planning, and handling mistakes.\nMessenger puts challenge on 2D spatial reasoning, reading syntactic variations and conducting\nmulti-hop reasoning. Meanwhile, Minecraft offers a unique challenge in 3D spatial reasoning and\ngeneralization within a procedurally generated world. We hope the SmartPlay benchmark would\nserve as a tool for identifying these nuanced gaps and directing future research.\nWhile each game poses its unique challenges, the SmartPlay benchmark also evaluates an agent\u2019s\ncapability to integrate these skills. For example, Crafter stands as the most comprehensive testbed,\ncombining long texts, multiple interactions, concurrent objectives, and error handling into a single\nenvironment. Crafter highlight the need for future research to focus not just on isolated skills, but\nalso on combining these skills into a unified, adaptive agent.\n3.2\nTWO ARMED BANDITS\nThe two armed bandit benchmark is inspired by popular implementations1 of bandit problems.\nThe LLM agent is provided two slot machines with hidden pre-defined reward probabilities p1, p2.\nFor slot machine i, the reward for the two possible out-comes are: ri for pay-off event and \u2212ri for\nno-pay-off event. The goal of the game is to find the arm with better return and maximize the reward\nover the course of 50 rounds. The human written manual informs the LLM of the number of slot\nmachines (two) and the objective.\nAn agent must keep track of win/losses from its past roll-out and balance exploration across the two\nslot machines vs. exploitation of the more rewarding one. Overall, the challenges include: 1) long\ncontext understanding, 2) understanding randomness, 3) learning from interactions.\nTo prevent game exploitation caused by biased actions, we randomize the score and probabilities for\neach action by shuffling the order of the paired list: [(p1, r1), (p2, r2)].\n3.3\nROCK PAPER SCISSORS\nSame rules as the famous zero-sum game Rock Paper Scissors2.\n1github.com/JKCooper2/gym-bandits\n2wikipedia.org/wiki/Rock paper scissors\n3\nPublished as a conference paper at ICLR 2024\nLong Text Understanding\n1.\nfew pre-defined lines\n2.\nfew paragraphs\n3.\nsyntactic variations\n4.\nlonger than 1 page\nMulti-hop Reasoning\n1.\n0 \u223c 1-hop\n2.\n2 \u223c 3-hop\n3.\nmulti-hop\nInstruction/Rule Following\n1.\nsingle game rule\n2.\n< 5 game rules\n3.\n5+ game rules\nPlanning\n1.\n< 5 planning steps\n2.\n5+ planning steps\n3.\nconcurrent objectives\nGeneralization\n1.\nfixed environment\n2.\nfixed world, random objective\n3.\nprocedurally generated world\nUnderstanding the Odds\n1.\nno randomness\n2.\nrandomness present in game\n3.\nrandomness as core mecha-\nnism\nLearning from Interactions\n1.\nno learning required\n2.\nsingle interaction\n3.\n< 5 interactions\n4.\n5+ interactions\nError/Mistake Handling\n1.\nnot required\n2.\nrollback only\n3.\nreason and replan\nSpatial Reasoning\n1.\nno \u223c 1D reasoning\n2.\n2D reasoning required\n3.\n3D reasoning required\nFigure 2: We identify a set of 9 important capabilities (section 2) for an intelligent agent. We identify\ndifferent degrees of challenge for each capability as shown on the left. Each game in SmartPlay\nchallenges unique set of capabilities at different degrees, as shown in the spider charts. We include\nnumerical values of the spider plots in Table 3.\nThe LLM agent plays against a hand-coded opponent that follows a hidden pre-defined strategy with\nprobabilities p1, p2, p3 for rock, paper, and scissors respectively. The scores for winning under each\naction is pre-defined and revealed to the LLM as s1, s2, s3. The human written manual provides\ninstruction on the possible actions and how the win/draw/lose of each round is calculated.\nAn agent must keep track of win/losses from its past roll-outs to analyze the opponent behavior,\nand then exploit the opponent to maximize payoff. Overall, the challenges include: 1) long text\nunderstanding, 2) understanding the odds, 3) learning from interactions, 4) instruction following.\nTo prevent game exploitation caused by biased actions, we randomize the score and probabilities for\neach action by shuffling the order of the paired list: [(p1, s1), (p2, s2), (p3, s3)].\n3.4\nTOWER OF HANOI\nThe Tower of Hanoi3 is a classic puzzle game that challenges the player to move a stack of disks\nfrom one rod to another, using a third rod as an auxiliary. The game has two rules: only one disk can\nbe moved at a time, and a larger disk cannot be placed on top of a smaller one.\nThe goal of the game is to move all the disks from the first rod to the last one in the minimum number\nof moves, and the game can be solved using a recursive algorithm that follows these steps:\n3github.com/RobertTLange/gym-hanoi/tree/master\n4\nPublished as a conference paper at ICLR 2024\n1.\nMove n - 1 disks from the source rod to the auxiliary rod, using the destination rod as an intermediate.\n2.\nMove the largest disk from the source rod to the destination rod.\n3.\nMove n - 1 disks from the auxiliary rod to the destination rod, using the source rod as an intermediate.\nThe human written manual contains a description of the game set-up and allowed actions. In addition,\nwe also include an example illustration of the starting and goal configuration, alongside an example\nof allowed/disallowed moves.\nThe Tower of Hanoi requires the agent to think strategically and plan ahead, and put strict requirements\non the LLM agents\u2019 ability to understand and follow the rules of the game. The game can become\nmore challenging if the agent makes a mistake. Sometimes, an agent may have to undo several moves\nto correct an error. Overall, the challenges include: 1) planing, 2) reasoning, 3) error handling.\n3.5\nMESSENGER\nMESSENGER (Hanjie et al., 2021) which features multiple game variants with procedurally generated\ngame dynamics and accompanying text manuals. The overall game mechanics of MESSENGER\ninvolve obtaining a message and delivering it to a goal. The benchmark is shipped with 3 levels of\ndifficulties (referred as stages in Hanjie et al. (2021)).\nTo succeed in MESSENGER, an agent must first relate entities and dynamics of the environment to\ntheir reference synonyms in the manual, identify message and goal objects, and navigate to bring the\nmessage to the goal while avoiding the enemy. The manual, by design, is challenging to understand\neven for human readers. Level 1 primarily challenges the agent\u2019s 1) long text understanding and 2)\ngeneralization . Level 2 includes additional challenge on the agent\u2019s 3) reasoning, and 4) 2D spatial\nreasoning. Level 3 increases difficulty by adding distraction objects.\nThe original manuals provided by Hanjie et al. (2021) contain descriptions of the entities and\nworld dynamics obtained through crowd-sourced human writers. We augment the manual with a\nspecification on the game objective, and an \u201cadvice\u201d for LLM agent to first identify goal objects and\nthen approach its objective. The \u201cadvice\u201d reduces the difficulty of the hard-to-parse manual.\n3.6\nCRAFTER\nThe Crafter environment (Hafner, 2021) is a procedurally generated, open-world survival game\ndesigned to test RL algorithms. Inspired by Minecraft, it features a grid-world with top-down\nobservation and a discrete action space of 17. The game includes 22 achievements in a tech-tree\nof depth 7 and provides information on the player\u2019s health, food, water, rest levels, and inventory.\nCrafter captures many of Minecraft\u2019s key research challenges, offering a more streamlined and faster\nenvironment for conducting experiments and gathering results.\nWe provide the \u201ccontext\u201d string from Wu et al. (2023c) as the manual, generated by parsing the LATEX\nsource-code of (Hafner, 2021). The \u201ccontext\u201d string has been shown to greatly improve performance\nof GPT-4 and text-davinci-003 on Crafter (Wu et al., 2023c).\nTo succeed in Crafter an LLM agent has to first understand and master a variety of reusable skills\ncomposed of 17 actions. The agent needs to learn to navigate through up to 5 2D terrains (biomes),\navoiding obstacles and dangerous creatures. The agent also needs to collect different resources and\ncraft more advanced weapons/tools to unlock more skills and achievements, while at the same time\nbalance crafting goals with survival goals like maintaining health, thirst, food, and rest (Hafner,\n2021). Overall, the challenges include: 1) 2D spatial reasoning, 2) mistake handling, 3) long text\nunderstanding, 4) planning, 5) generalization, 6) correcting from mistakes. Interestingly, the \u201ccontext\u201d\nstring does not capture all information necessary to succeed in the game, i.e., it requires 2 woods to\ncraft the crafting table, and 8 stones to craft the furnace. The agent has to 7) learn from interaction.\n3.7\nMINECRAFT\nMinecraft is one of the most popular games in history4. The game world is virtually infinite and\nprocedurally generated. The game observation is composed of rough 3D objects representing various\nmaterials, such as dirt, stone, ores, tree trunks, water, and lava. Minecraft has been widely studied as\n4wikipedia.org/wiki/Minecraft\n5\nPublished as a conference paper at ICLR 2024\na benchmark for intelligent multi-tasking agents (Guss et al., 2021; Fan et al., 2022; Hafner et al.,\n2023; Yuan et al., 2023; Wang et al., 2023b;a). However, due to the fact that most current LLMs\ndo not have vision capabilities, we simplify the Minecraft benchmark (Fan et al., 2022) and only\nconsider a small set of creative tasks where the primary objective is to find specific biomes, so an\nLLM could control a hand-coded agent to perform navigation in the 3D world.\nFor the human written instruction manual, we inform the agent that its goal is to find a specific biome\ng in Minecraft, and offer an advice on how to interpret the visual descriptor output for Minecraft.\nTo succeed in the creative \u201cfind\u201d tasks, a LLM agent has to have enough domain knowledge about\ndifferent biomes in Minecraft, and be able to correlate visual observation (text description of visual\nworld) with domain knowledge, and navigate in a 3D environment. Overall, the challenges include:\n1) planning, 2) domain knowledge, 3) 3D spatial reasoning, 4) generalization.\n4\nUSING SMARTPLAY\n4.1\nENVIRONMENT INTERFACE AND EVALUATION PROTOCOL\nEnv\nInput\nManual\nHistory\nRollout\nAction Space\nTrials\nBandits\nText\nBackground\n50\n50\n2\n20\nRockPaperScissors\nText\nBackground,Rules\n50\n50\n3\n20\nHanoi\nText\nBackground,Rules,Examples\n30\n30\n6\n10\nMessenger\nVisual description\nBackground,Rules,Advice\n2\n4\u223c128\n5\n100\nCrafter\nVisual description\nBackground,Rules,Advice\n5\n10k\n17\n10\nMinecraft\nVisual description\nObjective\n2\n200\n4\n20\nTable 1: Specifications for each game in SmartPlay. In addition to the table, the manual input\ncontains a list available actions for all games. Input, manual, action space, and rollout length should\nnot be modified. History length and trial numbers could be increased to suite future needs.\nFor ease of use and wide compatibility, SmartPlay follows a unified OpenAI Gym interface (Brockman\net al., 2016) for all games, with text-based observations, text-based manuals with content as described\nin Table 1, text describing historical actions and observations covering past steps of length \u201chistory\nlength\u201d, and flat categorical actions. Due to the randomness in some games, we recommend running\neach game multiple times and reporting the average metrics.\nInput, manual, action space, rollout length (the maximum environment steps allowed by each game),\nand trial numbers for each game are specified in Table 1. These settings are fixed and should not be\nmodified. However, future research may require longer history length or more trials for some games.\nThese parameters can be adjusted to suit specific needs, but the changes should be explicitly stated.\nWe provide recommended values (also used in our experiments) for the parameters in Table 1.\nFor completeness, we provide example inputs for each game in Appendix C. Note that all directions in\nSmartPlay are described in \u201ceast, south, west, north, above, below\u201d In the actual gameplay, SmartPlay\nAPI also includes a list of actions for the LLM agent to pick from.\n4.2\nEVALUATION METRICS\nWe define three metrics: reward, completion rate, score. To ensure compatibility with prior works,\nreward aligns with the score/reward definition in games originally designed for RL (i.e., Bandits,\nRock Paper Scissors, Messenger, Crafter (Hanjie et al., 2021; Hafner, 2021)). Completion rate\nmeasures the rate of successful completion for games with quantifiable objectives (i.e., Hanoi,\nMessenger, Minecraft). Finally, we introduce score for every game in the benchmark to provide a\nsummary of performance. For Bandits and Rock Paper Scissors, the score is defined the number of\ntimes the LLM action matches the environment optimal action; for Hanoi, the score is defined as\nthe number of disks successfully moved to the goal peg; for Messenger, the score is the same as the\nreward (Hanjie et al., 2021) of each round of game; for Crafter, the score is defined as the number of\nunlocked achievements at every step, summed across the whole game; for Minecraft, the score is\ndefined as the indicator of whether the \u201cfind\u201d objective for the game has been completed.\n6\nPublished as a conference paper at ICLR 2024\n5\nEXPERIMENTAL RESULTS\nUsing the SmartPlay API, we follow Wu et al. (2023c) and directly prompt an LLM: \u201cWhat is the\nnext action to take, let\u2019s think step by step.\u201d, with manual, history, and current observation as context.\nWe then query the LLM: \u201cChoose the best executable action from the list of all actions. Write the\nexact chosen action.\u201d for an answer directly mapped to one of the environment actions.\n5.1\nQUANTITATIVE ANALYSIS\nLLM\nBandit\nRPS\nHanoi\nMessengerL1\nMessengerL2\nCrafter\nMinecraft\nHuman Baseline\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nGPT-4-0613\n1.00\n0.91\n0.83\n0.90\n0.93\n0.26\n0.61\nGPT-4-0314\n0.97\n0.98\n0.90\n0.87\n0.97\n0.32\n0.59\ntext-davinci-003\n1.04\n0.40\n0.50\n0.62\n0.46\n0.07\n0.45\nClaude\n0.72\n0.47\n0.67\n0.44\n0.60\n0.05\n0.50\nBard\n0.86\n0.30\n0.67\n0.61\n0.40\n0.04\n0.54\nllama-2-13b\n0.50\n0.35\n0.37\n0.12\n0.13\n0.04\n0.61\nllama-13b\n0.68\n0.50\n0.33\n0.16\n0.06\n0.04\n0.50\nvicuna-13b\n0.64\n0.17\n0.07\n0.00\n0.12\n0.02\n0.43\nTable 2:\nComparison of performance of different LLMs in terms of average score on\nBanditTwoArmedHighLowFixed-v0, RockPaperScissorBasic-v0, Hanoi3Disk-v0, MessengerL1-\nv0, MessengerL2-v0, Crafter-v0, MinedojoCreative0-v0. All scores are normalized relative to human\nperformance (unnormalized version in Table 4). GPT-4 variants out-perform other LLMs by signifi-\ncant margins, but still greatly under-perform human baselines. We observe significant performance\ngaps between SOTA LLMs and human baseline on Hanoi, Crafter, and Minecraft. Hanoi, Crafter\nchallenges planning and reasoning, and Minecraft challenges 3D spatial reasoning.\nTo reduce the cost of queries, we pick 7 settings that requires a minimal experimentation but provides\ncomprehensive coverage over important agent capabilities. We experiment with 9 recent popular\nopen-source and proprietary LLMs and report the average score in Table 2.\nOverall, GPT-4 variants significantly out performs other proprietary models, which outperform\nopen-source models by significant margins.\nThere is still significant room for improvement for LLM as agents: Despite the impressive\nperformance of GPT-4 variants, there is still a significant gap between GPT-4 and human baseline\nperformance on more challenging benchmarks, with a 10% gap on 3DiskHanoi, 40% on Minecraft\ncreative tasks, and 70% on Crafter.\nOther proprietary LLMs struggle to keep up with GPT-4: We observe a more than 20% gap\nbetween GPT-4 and other proprietary models like Claude, Bard, and text-davinci-003 across all games\nexcept Minecraft. Furthermore, on comprehensive benchmarks like Crafter, GPT-4 variants achieves\n3 times higher scores than other proprietary models.\nOpen-source LLMs have a long way to go: Open-source LLMs achieves less than half the perfor-\nmance of GPT-4 variants on simple Bandit and Rock-Paper-Scissors tasks, and 1/8 the performance\non more challenging tasks. The fine-tuned Vicuna-13b model performs much worse than the base\nLLAMA-13b.\n3D Spatial reasoning remains a challenge for LLMs: The Minecraft benchmark appears equally\nchallenging to all LLMs due to its unique requirement for 3D spatial reasoning. All LLMs behave\nsimilarly in Minecraft creative tasks, with the best model at 60% of human baseline performance.\nTo offer additional insights into the individual agent capabilities of LLMs as identified in Figure 2,\nwe compute, for each capability c, the capability score pc\nLLM of an LLM as the average of human\nnormalized score sg over each game g, weighted by the degree dg\nc at game g presents challenge c:\npc\nLLM =\nP\ng dg\ncsg\nP\ng dg\nc . We plot the capability scores into 3 groups in Figure 3: GPT-4 variants, other\nproprietary models, and open-source models.\n7\nPublished as a conference paper at ICLR 2024\nFigure 3: Left: comparing the two GPT-4 variants with Human Baseline performance as reference.\nMiddle: comparing text-davinci-003, Claude, and Bard. Right: comparing open-source llama-2-13b,\nllama-13b, vicuna-13b models.\nThe two GPT-4 variants perform similarly overall, with GPT-0614 doing slightly worse on planning\nand reasoning. We also identify that GPT-4 variants score lower on learning from interactions,\nerror/mistake handling, and spatial reasoning.\nClaude demonstrates overall better performance than Bard, especially in planning, reasoning, in-\nstruction following. Compared to the other two proprietary models, text-davinci-003 appears biased\ntoward learning from interaction and randomness, and is particularly weaker at instruction following,\nplanning and reasoning.\nLLAMA-2-13b and LLAMA-1-13b performs similar on the high level, with LLAMA-2-13b perform-\ning better at planning, reasoning, and error handling, but worse in learning from randomness and\ninteractions. Vicuna-13b loses a lot of reasoning, planning, long text understanding, and error/mistake\nhandling capabilities after fine-tuning.\n5.2\nQUALITATIVE ANALYSIS\nLearning from interactions: In Bandits and Rock Paper Scissors, proprietary LLMs demonstrate\npromising potential for learning from history and interactions. We observe the agents first following\na exploratory strategy and then exploiting the biased opponent based on the past observations. In\nCrafter, GPT-4 variants consistently attempts to build crafting table with 1 wood and recovers from\nthe failure to build crafting table with 2 woods.\nData/environment contamination: For the Tower of Hanoi, it is expected that the LLMs have\nbeen trained on the exact same problem. Surprisingly, although all LLMs are able to provide the\nsolution at the starting configuration where all disks are on the first rod (some may even write out\nthe recurrence for the solution), most LLMs could not solve the problem and gets confused quickly\nafter a few moves, where the disks are distributed over all three rods. We suspect that this is due to\nthe intermediate states do not appear often in the LLM\u2019s training sets. Such observation verifies our\nbelief that games could be more robust to dataset contamination.\nSpatial Reasoning: We observe that LLMs often have a bad sense of spatial locations and struggle\nwith navigating to new locations. For example, in Minecraft, we often observe LLMs often take\nmoves that are contradictory over time, i.e., a bunch of \u201cmove north\u201d followed by a bunch of \u201cmove\nsouth\u201d, undoing a lot of its own efforts at exploration.\n6\nRELATED WORKS\n6.1\nLLM EVALUATION\nThe task of evaluating LLM performance has become increasingly challenging given the rapid\nprogression of LLMs. Generalist benchmarks usually employ a wide range of tasks and languages to\ntest general knowledge and reasoning (Hendrycks et al., 2020; Liang et al., 2022; Srivastava et al.,\n2022a; Zhong et al., 2023), where small language models are getting close performance compared to\nthe state-of-the-art large language models Li et al. (2023); Gunasekar et al. (2023); Eldan & Li (2023).\nHowever, those benchmarks struggle to cover interaction styles like instruction following Ziegler et al.\n(2019) or conversations Bai et al. (2022). The go-to approach for evaluating LLM for conversation\n8\nPublished as a conference paper at ICLR 2024\nis pairwise model comparison, which performs pair-wise comparison of output of the LLM and a\nreference LLMs to produce a ranking (Zheng et al., 2023b). The ranking was originally performed\nby human, but could be automated with a significantly more powerful LLM (Chiang & Lee, 2023;\nZheng et al., 2023a; Dubois et al., 2023). However, such evaluation techniques depend on an expert\nmodel or human who can reliably compare the performance of different LLMs, which limits the\napplication to SOTA LLMs like Claude-2 or GPT-4. Moreover, existing benchmarks fail to capture\nkey characteristics of intelligent agents like understanding of randomness, spatial reasoning, and\nerror handling.\n6.2\nUSING GAMES TO EVALUATE GENERALIST AGENTS\nThe idea of using games to evaluate the performance of agents has a long history in A.I. Pell\n(2011); Schaul et al. (2011); Whiteson et al. (2011) presented early ideas and motivation for using\ngames to measure the general capabilities of an agent, and discussed challenges in measuring A.I.\nagent performance. A series of popular benchmarks (Brockman et al., 2016; Vinyals et al., 2017;\nTunyasuvunakool et al., 2020) were created including Atari (Bellemare et al., 2013) and DeepMind\nlab (Beattie et al., 2016). As the capabilities of A.I. agents improve, researchers developed open-\nended generalist games (Savva et al., 2019; Abramson et al., 2020; Hafner, 2021; Srivastava et al.,\n2022b) like NetHack (K\u00a8uttler et al., 2020) or Minecraft (Guss et al., 2021; Fan et al., 2022).\nSmartPlay takes a suite of benchmarks (Brockman et al., 2016; Hafner, 2021; Fan et al., 2022)\ndeveloped over different times to best represent a broad range of difficulties and skills.\n6.3\nCREATING/CONVERTING TO TEXT GAMES\nText games (C\u02c6ot\u00b4e et al., 2018; K\u00a8uttler et al., 2020; Zhong et al., 2019; Hanjie et al., 2021) are\ninteractive simulations where the game state and action space are in natural language, often used to\nbenchmark skills like planning, exploration, and memory. SmartPlay features a text game (Messenger)\nwith procedural game rule generation (Hanjie et al., 2021) to test the generalization of the LLM\nagents at language understanding and planning.\nTo capture real-world challenges like spatial-reasoning, we study converting 2D/3D games into\ntext-games. Shridhar et al. (2020b) demonstrated the possibility of converting a 3D embodied indoor\nenvironment (Shridhar et al., 2020a) into a TextWorld (C\u02c6ot\u00b4e et al., 2018) game by \u201clisting\u201d all the\nobjects in text. However, such conversion relies on low-level controllers and teleportation, trivializing\nthe environments for current LLMs (Micheli & Fleuret, 2021; Wu et al., 2023b). Therefore, we\nfollow Wu et al. (2023c) to offer a list of objects/observations with directional relationship to the\nagent: \u201cto your south-east.\u201d Such description allows LLMs to make meaningful progress without\nlow-level controllers (Wu et al., 2023c).\n7\nCONCLUSION\nIn this work, we introduce SmartPlay, both a challenging benchmark and a methodology for evaluating\nLLMs\u2019 performance as agents. Our initial release of SmartPlay consists of Two-armed Bandits,\nRock Paper Scissors, Messenger (Hanjie et al., 2021), Crafter (Hafner, 2021), and Minecraft (Fan\net al., 2022) creative navigation tasks. SmartPlay benchmarks not only basic abilities like instruction\nfollowing and in-context reasoning, but also evaluates capabilities like planning, understanding\nof randomness, 2D/3D spatial reasoning, and error handling, which are often underrepresented in\nexisting LLM benchmarks. To achieve next-gen automation, we believe that language models should\ngo beyond speaking fluent language (Eldan & Li, 2023), and become more intelligent agents that\ncould interact with the world and human users. We hope that SmartPlay would catalyze research on\nbuilding more capable and reliable LLM agents.\nFinally, SmartPlay offers guidelines for easily adding games to the benchmarking suite. SmartPlay\nwill be continuously improved to provide up-to-date challenges for next-gen LLMs.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, et al. Imitating interactive intelligence.\narXiv preprint arXiv:2012.05672, 2020.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\nCharles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00a8uttler,\nAndrew Lefrancq, Simon Green, V\u00b4\u0131ctor Vald\u00b4es, Amir Sadik, et al. Deepmind lab. arXiv preprint\narXiv:1612.03801, 2016.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:\n253\u2013279, 2013.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nCheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human\nevaluations? arXiv preprint arXiv:2305.01937, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nMarc-Alexandre C\u02c6ot\u00b4e, Akos K\u00b4ad\u00b4ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning\nenvironment for text-based games. In Workshop on Computer Games, pp. 41\u201375. Springer, 2018.\nMarc-Alexandre C\u02c6ot\u00b4e, Akos K\u00b4ad\u00b4ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning\nenvironment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Con-\njunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm,\nSweden, July 13, 2018, Revised Selected Papers 7, pp. 41\u201375. Springer, 2019.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378, 2023.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\nlearn from human feedback, 2023.\nRonen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent\nenglish? arXiv preprint arXiv:2305.07759, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.\nMichael Genesereth, Nathaniel Love, and Barney Pell. General game playing: Overview of the aaai\ncompetition. AI magazine, 26(2):62\u201362, 2005.\n10\nPublished as a conference paper at ICLR 2024\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00b4esar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644, 2023.\nWilliam H Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno,\nCrissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov,\net al. The minerl 2020 competition on sample efficient reinforcement learning using human priors.\narXiv preprint arXiv:2101.11071, 2021.\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780,\n2021.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nAustin W Hanjie, Victor Y Zhong, and Karthik Narasimhan. Grounding language to entities and\ndynamics for generalization in reinforcement learning. In International Conference on Machine\nLearning, pp. 4051\u20134062. PMLR, 2021.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491, 2023.\nRaph Koster. Theory of fun for game design. \u201d O\u2019Reilly Media, Inc.\u201d, 2013.\nHeinrich K\u00a8uttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward\nGrefenstette, and Tim Rockt\u00a8aschel. The nethack learning environment. Advances in Neural\nInformation Processing Systems, 33:7671\u20137684, 2020.\nJohn E Laird, Allen Newell, and Paul S Rosenbloom. Soar: An architecture for general intelligence.\nArtificial intelligence, 33(1):1\u201364, 1987.\nYuanzhi Li, S\u00b4ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\nTextbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nJames Manyika. An overview of bard: an early experiment with generative ai. https://ai.\ngoogle/static/documents/google-about-bard.pdf. Accessed: May 27, 2023.\nVincent Micheli and Franc\u00b8ois Fleuret. Language models are few-shot butlers. arXiv preprint\narXiv:2104.07972, 2021.\nOpenAI. Gpt-4 technical report, 2023.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023.\nBarney Pell. Strategy generation and evaluation for meta-game playing. KI-K\u00a8unstliche Intelligenz,\n25(1):71\u201372, 2011.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,\nZhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+\nreal-world apis, 2023.\n11\nPublished as a conference paper at ICLR 2024\nReworkd. reworkd/agentgpt: Assemble, configure, and deploy autonomous ai agents in your browser.\nURL https://github.com/reworkd/AgentGPT.\nStuart J Russell. Artificial intelligence a modern approach. Pearson Education, Inc., 2010.\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,\nJulian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied\nai research. In Proceedings of the IEEE/CVF international conference on computer vision, pp.\n9339\u20139347, 2019.\nTom Schaul, Julian Togelius, and J\u00a8urgen Schmidhuber. Measuring intelligence through games. arXiv\npreprint arXiv:1109.1314, 2011.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\nfor everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 10740\u201310749, 2020a.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u02c6ot\u00b4e, Yonatan Bisk, Adam Trischler, and Matthew\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\npreprint arXiv:2010.03768, 2020b.\nSignificant-Gravitas.\nSignificant-gravitas/auto-gpt:\nAn experimental open-source attempt to\nmake gpt-4 fully autonomous. URL https://github.com/Significant-Gravitas/\nAuto-GPT.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He,\nMichael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train\nmegatron-turing NLG 530b, A large-scale generative language model. CoRR, abs/2201.11990,\n2022. URL https://arxiv.org/abs/2201.11990.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint\narXiv:2206.04615, 2022a.\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Fei Xia, Kent Elliott\nVainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for\neveryday household activities in virtual, interactive, and ecological environments. In Conference\non Robot Learning, pp. 477\u2013490. PMLR, 2022b.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nSaran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom\nErez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for\ncontinuous control. Software Impacts, 6:100022, 2020. ISSN 2665-9638. doi: https://doi.\norg/10.1016/j.simpa.2020.100022. URL https://www.sciencedirect.com/science/\narticle/pii/S2665963820300099.\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle\nYeo, Alireza Makhzani, Heinrich K\u00a8uttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A\nnew challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291, 2023a.\n12\nPublished as a conference paper at ICLR 2024\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task agents. arXiv\npreprint arXiv:2302.01560, 2023b.\nShimon Whiteson, Brian Tanner, and Adam White. Report on the 2008 reinforcement learning\ncompetition. AI Magazine, 31(2):81\u201381, 2010.\nShimon Whiteson, Brian Tanner, Matthew E Taylor, and Peter Stone. Protecting against evaluation\noverfitting in empirical reinforcement learning. In 2011 IEEE symposium on adaptive dynamic\nprogramming and reinforcement learning (ADPRL), pp. 120\u2013127. IEEE, 2011.\nMichael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge\nengineering review, 10(2):115\u2013152, 1995.\nYue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, and Tom M Mitchell. Read and\nreap the rewards: Learning to play atari with the help of instruction manuals. arXiv preprint\narXiv:2302.04449, 2023a.\nYue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell,\nand Shrimai Prabhumoye. Plan, eliminate, and track\u2013language models are good teachers for\nembodied agents. arXiv preprint arXiv:2305.02412, 2023b.\nYue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria,\nTom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by studying papers and\nreasoning. arXiv preprint arXiv:2305.15486, 2023c.\nYoheinakajima. yoheinakajima/babyagi. URL https://github.com/yoheinakajima/\nbabyagi.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023a.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023b.\nVictor Zhong, Tim Rockt\u00a8aschel, and Edward Grefenstette. Rtfm: Generalising to novel environment\ndynamics via reading. arXiv preprint arXiv:1910.08210, 2019.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu\nChen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models.\narXiv preprint arXiv:2304.06364, 2023.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019.\n13\nPublished as a conference paper at ICLR 2024\nA\nRESEARCH CHALLENGES\nGames\nBandits\nRock Paper Scissors\nHanoi\nMessengerL2+\nCrafter\nMinecraft\nLong text Understanding\n1. few pre-defined lines\n2. few paragraphs\n3. syntactic variations\n4. longer than 1 page\n1\n2\n2\n3\n4\n1\nReasoning\n1. 0 \u223c 1-hop\n2. 2 \u223c 3-hop\n3. multi-hop\n1\n1\n3\n2\n3\n1\nInstruction/Rule Following\n1. single game rule\n2. < 5 game rules\n3. 5+ game rules\n1\n3\n2\n2\n3\n2\nPlanning\n1. < 5 planning steps\n2. 5+ planning steps\n3. concurrent objectives\n1\n1\n3\n2\n3\n1\nGeneralization\n1. fixed environment\n2. fixed world, random objective\n3. procedurally generated world\n2\n2\n1\n2\n3\n3\nUnderstanding the Odds\n1. no randomness\n2. randomness present in game\n3. randomness as core mechanism\n3\n3\n1\n2\n2\n2\nLearning from Interactions\n1. no learning\n2. single interaction\n3. < 5 interactions\n4. 5+ interactions\n2\n3\n1\n1\n4\n1\nError/Mistake Handling\n1. not required\n2. rollback only\n3. reason and re-plan\n1\n1\n2\n1\n3\n2\nSpatial Reasoning\n1. 1D \u2013 no spatial reasoning\n2. 2D reasoning required\n3. 3D reasoning required\n1\n1\n1\n2\n2\n3\nTable 3: Research challenges associated with each of the 6 games. Since MessengerL1 does not\ncover multi-hop reasoning, only L2+ is included.\nB\nMINECRAFT VISUAL DESCRIPTOR\nThe raw ground truth MineDojo (Fan et al., 2022) is a block level matrix (2D matrix for lidar rays\nand 3D matrix for surrounding blocks), which is very hard for human or LLMs to comprehend.\nInspired by Wu et al. (2023c), we adopt a directional visual description scheme to encode the scene\nobservation in text. Specifically, we first run connected component algorithm to group the same\nblocks that are connect into groups, and then describe the group\u2019s relative position to the agent based\non the closest block from the group. For example, an observation in Figure 4 will be described as:\n14\nPublished as a conference paper at ICLR 2024\nFigure 4: Left: Raw Minecraft environment observation Right: Segmentation map of the environment\nobservation. as detected by lidar rays in MineDojo (Fan et al., 2022).\nYou see:\n- grass, 3.87 blocks away to your bottom, taking 38% of screen\n- plant, 22.10 blocks away to your bottom-right, taking 2% of screen\n- water, 33.88 blocks away to your bottom-left, taking 1% of screen\n- wood, 18.76 blocks away to your bottom-left, taking 0% of screen\n- flower, 32.98 blocks away to your right, taking 0% of screen\n- wood, 24.50 blocks away to your left, taking 0% of screen\n- leaves, 11.63 blocks away to your left, taking 6% of screen\n- leaves, 56.55 blocks away to your right, taking 1% of screen\nC\nEXAMPLE INPUTS\nC.1\nBANDITS\nExample input\nInstruction Manual:\nYou are in the casino with 2 slot machines in front of you.\nYour goal is to try to earn the most from those slot machines.\nObservation Example:\nYou pulled slot machine $i$, you received reward $r_i$.\nC.2\nROCK PAPER SCISSORS\nExample input\nInstruction Manual:\nFor the game Rock Paper Scissors, you and the opponent choose one of three options: rock, paper, or scissors.\nAfter both players have chosen, the winner is determined as follows:\nRock crushes scissors (Rock wins, score $s_1$)\nScissors cut paper (Scissors win, score $s_2$)\nPaper covers rock (Paper wins, score $s_3$)\nIf you lose, your score is the negative of the winner\u2019s score.\nIf both players choose the same option, it\u2019s a draw (score 0).\nYour goal is to maximize your score.\nObservation Example:\nYou chose $Rock$, and the opponent chose $Scissor$. You $won$ and received score $s_1$.\nNew round begins.\nC.3\nHANOI\nExample input\nInstruction Manual:\nThe game consists of three rods (A,B,C) and a number of disks of various sizes, which can go onto any rod.\nThe game begins with the disks stacked on rod A in order of decreasing size, the smallest at the top (righthand side).\nThe objective is to move the entire stack to rod C, obeying the following rules:\n- Only one disk may be moved at a time.\n- Each move consists of taking the top disk from one of the stacks and placing it on top of another stack\nor on an empty rod.\n- You cannot place a bigger disk on top of a smaller disk.\n15\nPublished as a conference paper at ICLR 2024\nFor example, considering movements from B under the following setting:\n- A: |bottom, [0], top|\n- B: |bottom, [1], top|\n- C: |bottom, [2], top|\nYou are only allowed to move from B to C but not A, since the top of B (1) is smaller than the top of C (2)\nbut bigger than the top of A (0).\nFinally, the starting configuration is:\n- A: |bottom, [2,1,0], top|\n- B: |bottom, [], top|\n- C: |bottom, [], top|\nand the goal configuration is:\n- A: |bottom, [], top|\n- B: |bottom, [], top|\n- C: |bottom, [2,1,0], top|\nwith top on the right and bottom on the left\nObservation Example:\nYou tried to move top disk of rod b to top of rod a. Current configuration:\n- A: |bottom, [2, 1, 0], top|\n- B: |bottom, [], top|\n- C: |bottom, [], top|\nC.4\nMESSENGER\nExample input\nInstruction Manual:\nIn the game, MESSENGER, each entity can take on one of three roles: an enemy, message, or goal.\nThe agent\u2019s objective is to bring the message to the goal while avoiding the enemies.\nIf the agent encounters an enemy at any point in the game, or the goal without first obtaining the message,\nit loses the game and obtains a reward of -1.\nthe dangerous enemy can be found next to the plane, which can not be moved.\nyou are being approached by a restricted document that is a robot.\nthe whale is the main objective.\nTo solve a game, you may find it helpful to list the objects that you see. Then for each object, match it with\nan entity description, and identify whether it is good or bad to interact with the object.\nThe name specifications of in-game objects may not be exact matches. Please try identifying with synonyms.\nObservation Example:\nYou took action Move South.\nYou (agent) don\u2019t have the message.\nYou see:\n- airplane 7 steps to your south\n- fish 13 steps to your south-east\n- robot 5 steps to your south-east\nC.5\nCRAFTER\nExample input\nInstruction Manual:\nWrite all information helpful for the game in a numbered list.\n1. Collect resources such as wood, stone, and iron to craft tools and weapons.\n2. Build shelters to protect yourself from monsters at night.\n3. Use tools and weapons to defend yourself against monsters.\n4. Build bridges to cross lakes and rivers.\n5. Dig tunnels to surprise monsters and outsmart them.\n6. Plant saplings and defend them against monsters to ensure a steady food supply.\n7. Eat Cow to restore health.\n8. Collect Drink to restore thirst.\n9. Place a Plant to eat for health.\n10. Make a Wood Pickaxe to collect Stone.\n11. Make a Wood Sword to defeat Zombies.\n12. Make a Stone Pickaxe to collect Iron.\n13. Make a Stone Sword to defeat Skeletons.\n14. Place a Furnace to smelt Iron.\n15. Collect Coal to smelt Iron.\n16. Collect Iron to make an Iron Pickaxe and Sword.\n17. Make an Iron Pickaxe to collect Diamond.\n18. Make an Iron Sword to defeat Zombies and Skeletons.\n19. Collect Diamond to progress further.\n20. Unlock achievements to receive rewards.\n16\nPublished as a conference paper at ICLR 2024\n21. Wake Up to start the episode.\nIn plain text. List all objects I need to interact/avoid to survive in the game.\nUse \"I would like to X object Y\" in each step. Replace Y by the actual object, X by the actual interaction.\nI would like to avoid zombies, skeletons, and spiders.\nI would like to collect saplings.\nI would like to craft a wood pickaxe.\nI would like to collect wood.\nI would like to craft a stone pickaxe.\nI would like to collect stone.\nI would like to craft a furnace.\nI would like to collect coal.\nI would like to collect iron.\nI would like to craft an iron pickaxe.\nI would like to collect diamonds.\nI would like to craft an iron sword.\nI would like to chase cows.\nI would like to grow fruits.\nI would like to drink from a lake.\nI would like to sleep in a safe place.\nI would like to craft a table.\nI would like to eat food.\nI would like to drink water.\nI would like to rest.\nI would like to build stone tools to defend myself against monsters.\nI would like to build bridges to cross lakes.\nI would like to dig tunnels to hide from monsters.\nI would like to block arrows with stones.\nI would like to dig through walls to surprise skeletons.\nI would like to seek shelter in caves.\nI would like to build plantations of saplings and defend them against monsters.\nI would like to eat the growing fruits to ensure a steady food supply.\nI would like to place a table.\nI would like to eat a cow.\nI would like to place a plant.\nI would like to defeat a zombie.\nI would like to place stone.\nI would like to eat a plant.\nI would like to defeat a skeleton.\nI would like to wake up.\nI would like to place a furnace.\nWrite all game objectives numbered list. For each objective, list its requirements.\n1. Collect Wood: No requirements\n2. Place Table: Requires Collect Wood\n3. Eat Cow: No requirements\n4. Collect Sampling: No requirements\n5. Collect Drink: No requirements\n6. Make Wood Pickaxe: Requires Place Table\n7. Make Wood Sword: Requires Place Table\n8. Place Plant: Requires Collect Sampling\n9. Defeat Zombie: No requirements\n10. Collect Stone: Requires Make Wood Pickaxe\n11. Place Stone: Requires Collect Stone\n12. Eat Plant: Requires Place Plant\n13. Defeat Skeleton: No requirements\n14. Make Stone Pickaxe: Requires Collect Stone\n15. Make Stone Sword: Requires Collect Stone\n16. Wake Up: No requirements\n17. Place Furnace: Requires Collect Stone\n18. Collect Coal: Requires Make Wood Pickaxe\n19. Collect Iron: Requires Make Stone Pickaxe\n20. Make Iron Pickaxe: Requires Place Furnace, Collect Coal, and Collect Iron\n21. Make Iron Sword: Requires Place Furnace, Collect Coal, and Collect Iron\n22. Collect Diamond: Requires Make Iron Pickaxe\nWrite all actions as a numbered list. For each action, list its requirements.\n1. Move West: Flat ground west of the agent.\n2. Move East: Flat ground east of the agent.\n3. Move North: Flat ground north of the agent.\n4. Move South: Flat ground south of the agent.\n5. Do: Facing creature or material; have necessary tool.\n6. Sleep: Energy level is below maximum.\n7. Place Stone: Stone in inventory.\n8. Place Table: Wood in inventory.\n9. Place Furnace: Stone in inventory.\n10. Place Plant: Sapling in inventory.\n11. Make Wood Pickaxe: Nearby table; wood in inventory.\n12. Make Stone Pickaxe: Nearby table; wood, stone in inventory.\n13. Make Iron Pickaxe: Nearby table, furnace; wood, coal, iron an inventory.\n14. Make Wood Sword: Nearby table; wood in inventory.\n17\nPublished as a conference paper at ICLR 2024\n15. Make Stone Sword: Nearby table; wood, stone in inventory.\n16. Make Iron Sword: Nearby table, furnace; wood, coal, iron in inventory.\n17. Noop: Always applicable.\nObservation Example:\nYou took action move_west.\nYou see:\n- water 5 steps to your south-west\n- grass 1 steps to your west\n- sand 4 steps to your south-west\nYou face grass at your front.\nYour status:\n- health: 9/9\n- food: 9/9\n- drink: 9/9\n- energy: 9/9\nYou have nothing in your inventory.\nC.6\nMINECRAFT\nExample input\nInstruction Manual:\nYou are in Minecraft and your goal is to find a forest biome. You are not allowed to craft anything.\nIn your observation, you are provided the amount of space an object takes in your field of view.\nNote that objects of the same size takes more space when they are closer to you.\nObservation Example:\nYou took action Move East.\nCoordinate (-253.12,71.75,248.64). Facing east.\nYou\u2019re not aiming at any block.\nAround you:\n- leaves, 3.96 blocks away, above you to north-west\n- wood, 4.24 blocks away, to north-west\n- grass block, 1.34 blocks away, below you to north-east\n- dirt, 3.33 blocks away, below you to north-east\n- stone, 4.67 blocks away, below you to north-east\nYou see:\n- grass block, 1.36 blocks away, below you to north-west, taking 51% of screen\n- sand, 8.29 blocks away, below you to south-west, taking 4% of screen\n- leaves, 4.47 blocks away, above you to north-west, taking 17% of screen\n- grass, 5.49 blocks away, above you to north-west, taking 1% of screen\n- wood, 11.07 blocks away, above you to north-west, taking 0% of screen\nD\nADDITIONAL EXPERIMENTAL RESULTS\nD.1\nHUMAN BASELINE\n3 players (including the authors) who are very familiar with the environments and API played the\ngames through the SmartPlay interface. Each human player performed 3 rounds of Bandit, RPS; 1\nround of Hanoi, Crafter, Minecraft; 5 rounds of MessengerL1, MessengerL2. We report the final\naverage score over all trials and all players.\nD.2\nNORMALIZED HUMAN SCORE\nGiven the game score of an LLM on game g, s(raw)\ng\n, we compute normalized human score sg from the\nhuman baseline on g, s(human)\ng\n, and the minimum possible game score s(min)\ng\n:\nsg = s(human)\ng\n\u2212 s(raw)\ng\ns(human)\ng\n\u2212 s(min)\ng\nD.3\nRAW SCORES\n18\nPublished as a conference paper at ICLR 2024\nLLM\nBandit\nRPS\nHanoi\nMessengerL1\nMessengerL2\nCrafter\nMinecraft\nHuman Baseline\n45\n43\n3\n1\n1\n2680\n1\nGPT-4-0613\n45.09\n39.25\n2.5\n0.8\n0.85\n700\n0.61\nGPT-4-0314\n43.86\n42.05\n2.7\n0.74\n0.93\n845.6\n0.592\ntext-davinci-003\n46.92\n17.0\n1.5\n0.24\n-0.07\n186.25\n0.449\nClaude\n32.43\n20.3\n2\n-0.12\n0.2\n143.3\n0.5\nBard\n38.85\n12.9\n2\n0.22\n-0.21\n112.3\n0.54\nllama-2-13b\n22.33\n15.05\n1.1\n-0.76\n-0.745\n115.3\n0.606\nllama-13b\n30.5\n21.4\n1\n-0.68\n-0.885\n100.2\n0.5\nvicuna-13b\n28.81\n7.1\n0.2\n-1\n-0.76\n56.7\n0.43\nTable 4:\nComparison of performance of different LLMs in terms of average score on\nBanditTwoArmedHighLowFixed-v0, RockPaperScissorBasic-v0, Hanoi3Disk-v0, MessengerL1-\nv0, MessengerL2-v0, Crafter-v0, MinedojoCreative0-v0.\n19\n"
  }
]