[
  {
    "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback",
    "link": "https://arxiv.org/pdf/2307.14936.pdf",
    "upvote": "41",
    "text": "PANGU-CODER2: BOOSTING LARGE LANGUAGE MODELS FOR\nCODE WITH RANKING FEEDBACK\nA PREPRINT\nBo Shen* Jiaxin Zhang* Taihong Chen* Daoguang Zan\u00a7 Bing Geng* An Fu* Muhan Zeng*\nAilun Yu\u2020 Jichuan Ji* Jingyang Zhao* Yuenan Guo* Qianxiang Wang*\n*Huawei Cloud Co., Ltd.\n\u00a7Chinese Academy of Science\n\u2020Peking University\nABSTRACT\nLarge Language Models for Code (Code LLM) are flourishing. New and powerful models are released\non a weekly basis, demonstrating remarkable performance on the code generation task. Various\napproaches have been proposed to boost the code generation performance of pre-trained Code LLMs,\nsuch as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we\npropose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can\neffectively and efficiently boost pre-trained large language models for code generation. Under this\nframework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval\nbenchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks,\nwe show that PanGu-Coder2 consistently outperforms all previous Code LLMs.\nKeywords Large Language Model \u00b7 Code Generation \u00b7 Reinforcement Learning \u00b7 Instruction Tuning\n1\nIntroduction\nAs one of the most promising applications of large language model (LLM), code large language models have captivated\nconsiderable attention across academia and industry due to their remarkable capability in code-related tasks Zan et al.\n[2023]. Since OpenAI released Codex Chen et al. [2021], AlphaCode Li et al. [2022], PaLM-Coder Chowdhery\net al. [2022], and PanGu-Coder Christopoulou et al. [2022] are subsequently published but in a closed-source way.\nResearchers open-source CodeParrot Huggingface [2021], PolyCoder Xu et al. [2022], PyCodeGPT Zan et al. [2022a],\nand SantaCoder Allal et al. [2023], but they fall far behind commercial models in terms of model size, capability, and\nperformance. The situation is changed by Hugging Face1, as the BigCode community releases StarCoder Li et al.\n[2023]: a 15B parameter model with 8K window size and FIM (Fill In the Middle, or infilling) capability. StarCoder\noutperforms many previous open-source large language models that support generating code from natural language\ndescriptions, and even matches the OpenAI code-cushman-001 model on the HumanEval Chen et al. [2021] and MBPP\nbenchmarks Austin et al. [2021].\nHowever, most large language models for code still fall behind the latest commercial models like GPT-3.5 and GPT-4\nfrom OpenAI OpenAI [2023], Bubeck et al. [2023]. We use Code LLM to denote the large language model majorly\npre-trained on code corpus, like PanGu-Coder Christopoulou et al. [2022], Replit 2, and StarCoder Li et al. [2023].\nCompared with open-source Code LLMs, the OpenAI GPT-family models are usually bigger in size and majorly\npre-train on natural language corpus (with a small proposition of code-related data), which can contribute to their\nsuperior natural language comprehension and instruction following capabilities. Some efforts have been made to boost\nCode LLMs, like data engineering (phi-1 Gunasekar et al. [2023]), instruction tuning (WizardCoder Luo et al. [2023]),\nretrieval-augmented generation (ReAcc Lu et al. [2022], RepoCoder Zhang et al. [2023], etc.), and reinforcement\nlearning (RLTF Liu et al. [2023], CodeRL Le et al. [2022], PPOCoder Shojaee et al. [2023], etc.).\n1https://huggingface.co\n2https://github.com/replit/ReplitLM\narXiv:2307.14936v1  [cs.CL]  27 Jul 2023\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nAlthough reinforcement learning (RL) seems to be a promising direction since programming is essentially a trial-\nand-error procedure, existing RL-based approaches face several major limitations. The motivation is intuitive and\nstraightforward: as we expect the model to generate code according to human intent and requirements, reinforcement\nlearning on Code LLMs can help the model enhance the ability to interpret and respond to code generation instructions,\nthus increasing the likelihood of generating a code to successfully solve a given problem. Typically, existing RL-\nbased approaches design value/reward functions according to feedback signals from code processors, like compilers,\ndebuggers, executors, and test cases. However, this leads to three limitations: First, regarding the test results as a\nreward directly provides limited improvements to the base model. Second, the adopted RL algorithm (like PPO) is\ncomplicated to implement and hard to train on large language models Liu et al. [2023]. Besides, running tests while\ntraining the model is time-consuming. As a result, previous works Le et al. [2022], Liu et al. [2023] only experiment on\nmodestly-sized models, and the improvement is rather limited.\nTo address the problem of existing RL-based approaches and further exploit the potential of Code LLM, we propose the\nRRTF (Rank Responses to align Test&Teacher Feedback) framework, which is a novel work to successfully apply\nnatural language LLM alignment techniques on Code LLMs. Different from previous works like CodeRL Le et al.\n[2022] and RLTF Liu et al. [2023], we follow the idea of RLHF (Reinforcement Learning from Human Feedback) that\nempowers InstructGPT/ChatGPT Ouyang et al. [2022a], but implement a much simpler and efficient training approach\nusing ranking responses as feedback instead of the absolute value of a reward model.\nAs a proof of concept, we apply RRTF on StarCoder 15B, and present a model that achieves the best performance\namong all published Code LLMs, namely the PanGu-Coder2. Through extensive evaluation on three benchmarks,\nincluding HumanEval, CoderEval, and LeetCode, we conjecture that Code LLMs do have the potential to surpass\nnatural language models of the same or larger sizes on the code generation task. Furthermore, by analyzing the training\nprocess and manually inspecting the generation code samples, we highlight the importance of high-quality data in\nimproving the models\u2019 instruction following and code writing capabilities.\nIn a nutshell, we make the following contributions:\n\u2022 We introduce a new optimization paradigm named RRTF, which is a data-efficient, easy-to-implement, and\nmodel-agnostic framework to effectively boost the code generation performance of pre-trained Code LLMs.\n\u2022 We present PanGu-Coder2, a model that improves nearly 30% over its base model and achieves new state-\nof-the-art performance on the HumanEval, CoderEval, and LeetCode benchmarks, surpassing all previously\npublished Code LLMs.\n\u2022 We share our experience and findings in constructing effective training data, training the model with RRTF,\nand optimizing such a model for fast inference.\n2\nRelated Work\n2.1\nLarge Language Model for Code (Code LLMs)\nAs a momentous milestone, Codex Chen et al. [2021] boasting a 12-billion-parameters model demonstrates the extraor-\ndinary capability to tackle up to 72% of Python programming problems. Subsequently, a new wave of code generation\nmodels, such as AlphaCode Li et al. [2022], PaLM-Coder Chowdhery et al. [2022], and PanGu-Coder Christopoulou\net al. [2022], also were proposed. Despite the remarkable prowess exhibited by the aforementioned models, it is\ndisheartening to note their unavailability as open-source projects. Therefore, several open-source code generation\nmodels, including CodeParrot Huggingface [2021], PolyCoder Xu et al. [2022], PyCodeGPT Zan et al. [2022a],\nSantaCoder Allal et al. [2023], and StarCoder Li et al. [2023], were released, injecting fresh vigor into the realm of code\ngeneration Chen et al. [2022]. Meanwhile, code generation models have also been applied to a broader range of practical\ncoding scenarios. For example, CodeGeeX Zheng et al. [2023], BLOOM Scao et al. [2022] and ERNIE-Code Chai et al.\n[2022] have been proposed to facilitate multilingual modeling; JuPyT5 Chandel et al. [2022] is trained on a large corpus\nof Jupyter notebooks, aiming to elevate the experience of interactive programming; DocCoder Zhou et al. [2023a] and\nAPICoder Zan et al. [2022b] have been proposed to empower language models with the ability to invoke APIs; Some\nmodels such as InCoder Fried et al. [2023], FIM Bavarian et al. [2022], MIM Nguyen et al. [2023], SantaCoder Allal\net al. [2023], and StarCoder Li et al. [2023] support the code generation at arbitrary positions.\nOf late, some efforts Zhou et al. [2023b], Peng et al. [2023] using the instruction tuning technique unlock the potential\nvaluable knowledge stored within large language models, by fine-tuning on meticulously curated high-quality instruction\ndatasets. In the field of code generation, WizardCoder 15B Luo et al. [2023] and phi-1 1.3B Gunasekar et al. [2023]\nachieve exceptional code generation performance by fine-tuning on the data generated by OpenAI\u2019s GPT-3.5 or GPT-4.\n2\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\n2.2\nReinforcement Learning on LLM\nReinforcement Learning from Human Feedback\nLarge language models can generate untruthful, unexpected, and\nunhelpful outputs, which are not aligned with the intention of the end users. To align the behavior of large language\nmodels with human intentions, Ouyang et al. [2022b] proposed Reinforcement Learning from Human Feedback(RLHF)\nrecently. The underlying idea is to leverage human preferences on given tasks to improve the behavior of a language\nmodel. A typical RLHF procedure consists of three steps, including supervised fine-tuning (SFT) which collects human\ndemonstrations of desired model behavior and fine-tunes a language model, reward model (RM) training which employs\nhumans to label the preferred output among various model outputs and trains a reward model based on the labeled data,\nand reinforcement learning via proximal policy optimization (PPO) which optimizes the language model against the\nreward model. OpenAI\u2019s GPT-3.5 and GPT-4 are trained with RLHF and their success demonstrates the effectiveness\nof RLHF to align the behavior of language models with human preferences. However, implementing RLHF requires\nheavy training resources and complex parameter tuning, which alleviates the technique from being easily applied in\npractice. In addition, the inefficiency and instability of RL algorithms can pose challenges to the alignment of language\nmodels. Given the limitations of heavy training resources and complex parameter tuning, Yuan et al. [2023] proposed\nthe RRHF paradigm which leverages outputs with human preferences collected from various resources to train a model\nthat aligns with human preferences. Its principle to align the model behavior to humans is to train a model to learn the\noutputs with better rewards based on human preferences among a set of outputs. Compared with RLHF, RRHF can be\neasily scaled to LLMs with larger sizes under a resource-constrained scenario. In view of the inefficiency and instability\nproblem, Dong et al. [2023] proposed the reward-ranked fine-tuning (RAFT) technique for language models. Their\nunderlying idea is to first select high-quality outputs of the model based on the output ranking estimated by a reward\nmodel and then leverage the selected outputs to train a model that aligns with human preferences. Compared with\nRLHF, the SFT-style RAFT typically converges faster than the PPO used in RLHF, while utilizing simpler parameter\nconfiguration and fewer computational resources.\nReinforcement Learning on Code\nThe successful practice of RLHF has inspired researchers to improve the capability\nof Code LLMs with reinforcement learning. For example, CodeRL Le et al. [2022] integrates actor-critic RL framework\nwith unit test signals to fine-tune models. Following CodeRL, PPOCoder Shojaee et al. [2023] uses the Proximal Policy\nOptimization (PPO) algorithm, but results in little improvements on the MBPP benchmark. Very recently, RLTF Liu\net al. [2023] moves a step forward by adopting an online RL framework with multi-granularity unit test feedback, to\novercome the limitation of offline RL adopted by CodeRL and PPOCoder.\n2.3\nFine-tuning Code LLM\nFine-tuning on pre-trained language models is a mainstream modeling paradigm that maximizes the performance at\ndownstream tasks. In the field of code, several works also adopt the paradigm to address code-related scenarios. For\ninstance, CodeGen Nijkamp et al. [2022] and StarCoder Li et al. [2023] start by pre-training on a multilingual code\ncorpus, followed by fine-tuning on monolingual data, thereby achieving superior performance on monolingual tasks.\nCodex-S Chen et al. [2021] and PanGu-Coder-FT Christopoulou et al. [2022] elevate their code generation capabilities\nby fine-tuning on competitive programming problems. Recently, instruction tuning Ouyang et al. [2022a], OpenAI\n[2023], as a form of supervised fine-tuning (SFT), is proposed to align the model with human behavior by learning\nabundant high-quality instruction corpus. In this regard, WizardCoder Luo et al. [2023] was fine-tuned on a series of\ninstruction corpora derived from a teacher model, effectively maximizing its code knowledge with relatively limited\nparameters. In this technical report, PanGu-Coder2 employs ranking feedback strategy Yuan et al. [2023] during the\nfine-tuning process, and achieves surprising code generation performance.\n3\nApproach\n3.1\nOverview\nIn this technical report, we present a simpler but powerful framework RRTF, which seamlessly combines several\ncutting-edge techniques, including instruct tuning Peng et al. [2023], Evol-Instruct method Xu et al. [2023], Luo et al.\n[2023], and reinforcement learning Yuan et al. [2023]. The core idea of our approach is to guide a model towards\nproducing higher-quality code, by utilizing the test signals and human preferences jointly as feedback to rank responses.\nInspired by recent progress in reinforcement learning and instruction fine-tuning on top of large natural language\nmodels, especially the RLHF Ouyang et al. [2022a], RRHF Yuan et al. [2023], and RLTF Liu et al. [2023], we propose\na new training paradigm, namely the RRTF framework. Figure 1 shows the overview of the RRTF framework, which\nconsists of three steps: sampling, ranking, and training. In the sampling stage, responses are sampled with prompts\ngenerated via Evol-Instruct. In the ranking stage, responses from different sources are ranked according to unit tests\n3\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nSampling\nRanking\nCode LLM\nUnit Tests\nPreference\nTraining\n<Prompt, Response>\nRejected\nChosen\nRRTF\nScore\nPrompt\nPrompt\nModels\nPrompt\nEvol-Instruct\nScore\nFigure 1: Overview of the proposed RRTF framework.\nand heuristic preferences. In the training stage, triples of prompt and chosen/rejected responses with corresponding\nscores are used to train the Code LLM.\n3.2\nModel Architecture\nIn this work, we train a 15B parameter model based on the decoder-only Transformer with Multi-Query-AttentionShazeer\n[2019] and learned absolute positional embeddings. At the same time, FlashAttention is used to reduce the amount of\ncalculation and memory usage. Hence, the max length of the model can be scaled to 8192. Tabel 1 shows the detailed\nhyper-parameters of our model.\nTable 1: The hyper-parameters of our model\nHyper-Parameters\nValue\nHidden size\n6144\nMax Length\n8192\nNum of attention heads\n48\nNum of transformer hidden layers\n40\n3.3\nTraining Corpus\nWe follow the Evol-Instruct technique Xu et al. [2023], Luo et al. [2023] to construct our training corpus, since\nmanually collecting a high-quality corpus is labor-intensive and time-consuming. Specifically, we started from Alpaca\n20K dataset3 and iteratively evolve the programming problems in this dataset via in-depth evolving to obtain new\nprogramming problems (the prompt is shown in Figure 2). With these problems, we sampled answers from different\nmodels. In total, we collected an initial corpus containing 100K programming problems with answers, which we refer\nto as instruction and solution pairs. In addition, we conducted data preprocessing on our initial corpus using several\nmanually-defined rules and reduced the size of the corpus to 68K. More importantly, to prevent data leakage, we\ndevoted considerable efforts to surveying the potential overlap between the collected 68K dataset and the HumanEval\nbenchmark. After conducting a meticulous survey, we confirm that there is no data leakage in our experiments, further\nvalidating the effectiveness of PanGu-Coder2.\n3.4\nRRTF framework\nInspired by RRHF Yuan et al. [2023], we propose the RRTF (Rank Responses to align Test&Teacher Feedback)\nframework for Code LLMs. RRHF 4 is proposed as a simplified training paradigm for RLHF, which ranks responses\nfrom different sources according to human preferences, and aligns the model through a ranking loss function. Compared\nwith RLHF, RRHF can efficiently align the output probabilities of a language model with human preferences, with only\n1-2 models required during the tuning period, and it is simpler than PPO in terms of implementation, hyperparameter\ntuning, and training.\n3https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\n4https://github.com/GanjinZero/RRHF\n4\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nI want you to act as a Programming Contest Designer. Your objective is to rewrite a programming task based on the\ngiven task by increasing the difficulty a bit.\nYou can increase the difficulty using, but not limited to, the following methods:\n{methods}\nYour response is the rewritten programming task (#Rewritten Task#).\nThe #Rewritten Task# must be reasonable and must be understood and responded by humans, and also solvable with\ncode. It should not be dependent on the #Given Task#. Your rewriting cannot omit the non-text parts such as the\ntable and code in #Given Task#:. Also, please do not omit the input in #Given Task#.\n*The rewritten task and the given task should have the similar length. **\n*The rewritten task should ask for a function-level code solution.**\n\"#Given Task#\", \"#Rewritten Task#\", \"given task\", and \"rewritten task\" are NOT allowed to appear in #Rewritten\nTask#.\n#Given Task#\n{instruction}\n#Rewritten Task#\nFigure 2: Prompt to evolve over the CodeAlpaca dataset.\nInstead of aligning the model with human intents, the purpose of code generation is to improve generating correctness,\nso we replace the H (human) with T, which can be a combination of tests and teachers (more powerful models or human\nexperts), they can jointly form a feedback signal to guide the generation of Code LLM and most of the feedback can be\nfully- or semi-automatically obtained in the faster way. The training procedures of RRTF can be divided into 3 steps:\n1. Step1: Sampling In the sampling stage, responses are sampled with prompts. Based on the prompts generated\nby the Evol-Instruct (see Section 3.3), we sample the responses both from the student model (model to train)\nand teacher models by various temperatures. The process is offline and in parallel, so we can efficiently get\nenough samples for training.\n2. Step2: Ranking In the ranking stage, responses from different sources are ranked according to unit tests and\nheuristic preferences. After obtaining all responses, we extract the programs from the responses and execute\nthem in a running environment that supports large-scale parallel execution. According to the test results, there\nare 4 situations, which are compiled error, runtime error, pass partial tests, all pass. For each data, we assign\ndifferent scores from low to high based on the above situations. Meanwhile, we filter out data whose teachers\u2019\nscore is lower than the student model. For two samples that fall into the same situation, we always assign a\nhigher rank to the sample from the teachers, since we prefer the student to learn from the teacher.\n3. Step3: Training In the training stage, triples of prompt and chosen/rejected responses with corresponding\nscores are used to train the Code LLM. During training, for each prompt x, we have a pair of response\n{ytea, ystu}, where ytea is the response generated by the teachers, and ystu is the response generated by the\nstudent model. So we can indicate the conditional log probability(length-normalized) pi by:\npi =\nP\nt log P\u03c0 (yi,t | x, yi,<t)\n\u2225yi\u2225\nwhere \u03c0 is the model, i \u2208 {tea, stu}, t is the time step. And the rank loss can be expressed as:\nLrank = \u2212\nX\nrtea>rstu\n(rtea \u2212 rstu) min (0, ptea \u2212 pstu)\nwhere rtea and rstu are the scores given in ranking stage. There is also a cross-entropy loss similar to\nsupervised fine-tuning, which lets the model learn the response generated by the teacher:\nLft = \u2212\nX\nt\nlog P\u03c0 (ytea,t | x, ytea,<t)\nFinally, the total loss is the sum of the above two losses:\nL = Lrank + Lft\n5\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\n3.5\nImplementation Details\nWe choose the StarCoder 15B Li et al. [2023] as the base model, and train it with a global batch size of 512 for 6 epochs.\nFigure 3 shows the format of one single training sample. In addition to adding a pair of triple quotation marks on the\nprompt, we only use the code snippets extracted from responses for training.\nFigure 3: Example data format of the training sample.\n4\nEvaluation\nWe have conducted an extensive evaluation to study the performance of our approach. This section describes the settings\nof our evaluation and reports the experimental results as well as our findings.\n4.1\nEvaluation Setup\n4.1.1\nMain Evaluated Models\n\u2022 CodeGen-mono 16B Nijkamp et al. [2022] is a variant of CodeGen-Multi 16B, specifically fine-tuned using\nadditional Python code from GitHub.\n\u2022 CodeGeeX 13B Zheng et al. [2023] is a multilingual language model for code with a parameter count of 13B,\nwhich is trained on approximately 850B tokens from 23 programming languages.\n\u2022 StarCoder 15B Li et al. [2023] is a Code LLM with 15B parameters and a context size of 8K, which supports\ninfilling capabilities and fast inference.\n\u2022 CodeT5+ 16B Wang et al. [2023], an encoder-decoder Code LLM, boasts modular flexibility, accommodating\ndiverse code-related downstream tasks.\n\u2022 WizardCoder 15B Luo et al. [2023] is the state-of-the-art Code LLM prior to PanGu-Coder2, and is trained\nusing the Evol-Instruct technique.\n4.1.2\nBenchmarks\n\u2022 HumanEval:5 Released alongside Codex by OpenAI Chen et al. [2021], the most widely-adopted bench-\nmark for comparing LLMs\u2019 performance on code generation. HumanEval consists of 164 manually-written\nprogramming tasks.\n\u2022 CoderEval Yu et al. [2023]: A pragmatic code generation benchmark to evaluate models under realistic\nsoftware development scenarios, including 230 functions with tests from 43 open-source Python projects.\n\u2022 LeetCode (after 2022.7): We collected problems from leetcode that meet the following criteria:\n\u2013 Problems that are publicly available and can be accessed for free.\n\u2013 Problems that were created after July 1st, 2022, which ensures that any data in this benchmark does not\noverlap with the training data of StarCoder, which only consists of code before June 2022.\nBesides the problem description, we also collected Python editor hints including method name and signature.\nWe took editor hints as prompt input and tested models\u2019 output using public tests. As a result, this benchmark\n5https://github.com/openai/human-eval\n6\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nincludes a total of 300 problems(with problem id \u2265 2325), including 79 easy problems, 150 medium problems,\nand 71 hard problems.\n4.1.3\nMetric\nPass@k\nSame as related works, we also adopt the pass@k metric implemented by OpenAI Chen et al. [2021] to\nassess the functional correctness of generated code, where n(n \u2265 k) code samples are generated for each problem, and\nthe number of correct samples c is counted. The functional correctness of a code sample is determined by executing the\ncorresponding unit tests and checking if it passes all test cases. Given the total number of generation n, the number of\ncorrect samples c, and the sampling budget k, pass@k is calculated via the unbiased estimator:\npass@k := E[1 \u2212\n\u0000n\u2212c\nk\n\u0001\n\u0000n\nk\n\u0001 ], n = 200, k \u2208 {1, 10, 100}\n4.1.4\nDecoding Strategy\nFor experiments that evaluate the performance of models on code generation by estimating pass@k, we used a\ntemperature of 0.2 to generate responses for pass@1, and a temperature of 1.2 for pass@10 and pass@100 for more\ndiversity. For closed-source models, we retrieved the data from previous papers. For available models, we generated\n200 samples to guarantee a statistically reliable result as much as possible. Additionally, we used a top_p of 0.95 for\nnucleus sampling. For comparison of PanGu-Coder2 with other latest open-source models on three benchmarks, we\nused the greedy decoding strategy.\n4.1.5\nPrompts\nWe noticed that the performance of a Code LLM could be largely affected by the prompt used for generating solutions to\na programming problem. To maintain consistency with existing studies, for a given Code LLM, we leveraged the prompt\nreported in its corresponding paper to conduct our evaluation. The detailed code generation prompt for PanGu-Coder2\nand other models are as follows:\nPrompt for PanGu-Coder2\n\"\"\"\n{docstring}\n\"\"\"\n{function signature}\nPrompt for StarCoder\n{function signature}\n\"\"\"\n{docstring}\n\"\"\"\nPrompt for WizardCoder\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\nCreate a Python Script for this problem:\n{function signature}\n\"\"\"\n{docstring}\n\"\"\"\n### Response:\n7\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nTable 2: Results of pass@1/10/100 of well-known models on HumanEval. Most scores are retrieved from previous\npapers as they are reported. For PanGu-Coder2, we follow the Codex Chen et al. [2021] and AlphaCode Li et al. [2022]\npaper to generate n=200 samples and report the optimal pass@1/10/100 when temperature=0.2/1.2/1.2 and top_p=0.95.\nThe same settings are used for StarCoder and WizardCoder (marked with *).\nModel\nParams\nPass@k (%)\nk=1\nk=10\nk=100\nClosed-source Models\nAlphaCode Li et al. [2022]\n1.1B\n17.1\n28.2\n45.3\nPhi-1 Gunasekar et al. [2023]\n1.3B\n50.6\n-\n-\nCodex Chen et al. [2021]\n12B\n28.81\n46.81\n72.31\nLaMDA Thoppilan et al. [2022]\n137B\n14.0\n-\n47.3\nPaLM-Coder Chowdhery et al. [2022]\n540B\n36.0\n-\n88.4\nGPT-3.5 OpenAI [2023]\n-\n48.1\n-\n-\nGPT-3.5 Luo et al. [2023]\n-\n68.9\n-\n-\nGPT-4 OpenAI [2023]\n-\n67.0\n-\n-\nGPT-4 Bubeck et al. [2023]\n-\n82.0\n-\n-\nOpen-source Models\nCodeGen-mono Nijkamp et al. [2022]\n16B\n29.28\n49.86\n75.00\nCodeGeeX Zheng et al. [2023]\n13B\n22.89\n39.57\n60.92\nStarCoder Li et al. [2023]*\n15B\n33.60\n45.78\n79.82\nCodeT5+ Wang et al. [2023]\n16B\n30.9\n51.6\n76.7\nWizardCoder Luo et al. [2023]*\n15B\n57.30\n73.32\n90.46\nPanGu-Coder2*\n15B\n61.64\n79.55\n91.76\n4.2\nEvaluation Results\n4.2.1\nPerformance\nWe compared PanGu-Coder2 with existing Code LLMs in terms of Python code generation performance. Table 2\nshows the comparison result of pass@k on the HumanEval benchmark. Across all open-source models, PanGu-\nCoder2 achieves the best results for all k values (pass@1=61.64, pass@10=79.55, pass@100=91.76). Compared\nwith WizardCoder which was the state-of-the-art Code LLM on the HumanEval benchmark, we can observe that\nPanGu-Coder2 outperforms WizardCoder by a percentage of 4.34%. With regard to StarCoder, we can observe 28%\nabsolute improvement in terms of pass@1 score (from 33.6% to 61.6%). In addition, for pass@10 and pass@100, the\nperformance of PanGu-Coder2 is consistently better than that of StarCoder.\nAcross all closed-source models, PanGu-Coder2 attains the second position. Compared with larger models including\nPaLM-Coder and LaMDA, PanGu-Coder2 performs better despite being smaller in scale. Another promising observation\nis that PanGu-Coder2 outperforms OpenAI\u2019s GPT-3.5. However, there is still a gap between our model and OpenAI\u2019s\nGPT-4 (the version reported in OpenAI\u2019s report OpenAI [2023]).\nTable 3 shows the comparison result of greedy decoding pass@1. Across all benchmarks, we can observe that PanGu-\nCoder2 achieves the best results among all models, with a pass@1 value of 62.20% on HumanEval, 38.26% on\nCoderEval, and 32/30/10 on LeetCode. A promising observation is that PanGu-Coder2 not only surpasses WizardCoder\nand StarCoder on HumanEval, but also outperforms these two models on CoderEval and LeetCode. This indicates that\nPanGu-Coder2 not only excels at simple programming tasks, but also performs outstandingly well on context-aware\ndevelopment tasks and programming contest problems.\nFrom the experimental results shown in Tables 2 and 3, we can conclude that:\n\u2022 PanGu-Coder2 achieves a state-of-the-art 61.64% pass@1 on HumanEval among open-source models.\n\u2022 PanGu-Coder2 outperforms models of larger scale including PaLM-Coder and LaMDA despite being smaller\nin scale.\n\u2022 PanGu-Coder2 is the only model we tested that achieves the best performance on HumanEval, CoderEval, and\nLeetCode at the same time.\n8\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nTable 3: Performance comparison of PanGu-Coder2 with previous models on three benchmarks by greedy decoding.\nModel\nParams\nHumanEval\n(text2code)\nCoderEval\n(context2code)\nLeetCode\n(easy/medium/hard)\nPanGu-Coder\n2.6B\n23.78\n15.21\n6/3/0\nReplit-code-instruct-glaive6\n2.7B\n56.10\n27.39\n3/5/2\nStarCoder\n15B\n32.93\n37.82\n18/13/2\nWizardCoder\n15B\n59.80\n33.48\n29/22/7\nPanGu-Coder2\n15B\n62.20\n38.26\n32/30/10\n51.22\n53.66\n62.2\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n11000\n38\n42\n46\n50\n54\npass@1\n(a)  18k\n72\n80\n144\n152\n160\n168\n224\n232\n248\n296\n304\n40\n44\n48\n52\n56\npass@1\n(b)  38k\n0\n100\n200\n300\n400\n500\n600\n700\n800\n20\n45\n50\n55\n60\n65\npass@1\n(c)  68k\nFigure 4: Performance change when in the training process (pass@1 on HumanEval with greedy decoding). The\nnumber of steps in an epoch for (a),(b), and (c) is roughly 2250, 74, and 132 respectively.\n4.2.2\nFindings\nTo analyze the training process of PanGu-Coder2, we focus on two of the key factors that affect the performance of\nlarge language models: the dataset size and the training compute.\nDataset size\nThe overall accuracy (estimated via greedy decoding pass@1) increases along with the growth of dataset\nsize, as shown in Figure 4. Also, as the size of the dataset grows, the training curve becomes more stable, at roughly\n2-3 epochs on 38k/68k dataset. As for the 18k dataset, performance still oscillates drastically after 3 epochs. This\nsuggests that more and variant corpus can result in better performance, while the training cost is still acceptable as\nepochs needed for reaching the best performance do not increase along with the scale of the corpus.\nTraining compute\nRegardless of dataset size, the accuracy may drop drastically or stay flat at the start of the training.\nAfter roughly 2 epochs, the training curve becomes more stable and the accuracy consistently increases as the loss\ndecreases. The best performances are reached after 3 epochs while the accuracy becomes even more stable after 4\nepochs, showing a sign of convergence. This suggests that the model needs roughly 3-4 epochs to fully capture the\nknowledge in the dataset, and training steps after that may have very little help towards increasing the model\u2019s capability.\n4.2.3\nCase Study\nTo empirically study the model and shed light on future work, we compare and analyze the successful and failed cases\nof three models: the base model StarCoder, the instruction-tuned model WizardCoder, and the PanGu-Coder2 model.\nFigure 5 shows the difference and intersection of solved problems by three models, in terms of greedy decoding and\nnucleus sampling. From the figure, we find that PanGu-Coder2 and WizardCoder can be complementary: though\nPanGu-Coder2 solves the most problems and some of them cannot be solved by WizardCoder, there are problems that\n9\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\n49\n32\n3\n1\n16\n1\nPanGu-Coder2\nWizardCoder\nStarCoder \n(a) Greedy decoding pass@1\n18\n44 (All Wrong)\n(b) Nucleus sampling pass@200\n99\n51\n0\n0\n3\n0\nPanGu-Coder2\nWizardCoder\nStarCoder \n5\n6 (All Wrong)\nFigure 5: Numbers of correctly-solved problems by three models on HumanEval.\nwere only solved by WizardCoder, which boosts the StarCoder performance in the instruct tuning way. Besides, there\nare some problems that cannot be solved by any of these models, even sampling for 200 times.\nWe choose several representative example codes generated by StarCoder, WizardCoder, and PanGu-Coder2 as the case\nstudy, to conduct a critical evaluation of the strengths and weaknesses of PanGu-Coder2. As depicted in Figure 6,\nPanGu-Coder2 adeptly comprehends the logical aspects of programming problems, while WizardCoder and StarCoder\nfall short in this regard. This observation signifies that PanGu-Coder2 has effectively established an meticulous mapping\nbetween programming problem statements and code solutions via our proposed ranking feedback. According to the\ndepiction in Figure 7, in certain instances, PanGu-Coder2 and StarCoder are outperformed by WizardCoder, which may\nbenefit from training with extensive rich-comment code instructions. To a certain extent, this observation implies that\nincorporating step-by-step comment data may yield positive effects during the training process. In addition, Figure 8\nshows a case where StarCoder, WizardCoder, and PanGu-Coder2 all give incorrect code solutions due to the intricacies\nand challenges posed by the programming problem. This observation indicates that currently code LLMs still fall short\nof human-like proficiency in tackling complex programming requirements, leaving some room for improvement.\nPrompt\nPanGu-Coder2\nWizardCoder\nStarCoder\n\"\"\"\nGiven a positive integer n, you have to make a pile of n levels of stones.\nThe first level has n stones.\nThe number of stones in the next level is:\n- the next odd number if n is odd.\n- the next even number if n is even.\nReturn the number of stones in each level in a list, where element at index i represents the number of stones in the level (i+1).\nExamples:\n>>> make_a_pile(3)\n[3, 5, 7]\n\"\"\"\ndef make_a_pile(n):\npile = []\nnext_num = n\nfor i in range(n):\npile.append(next_num)\nnext_num += 2\nreturn pile\ndef make_a_pile(n):\npile = [n]\nfor i in range(1, n):\nif pile[-1] % 2 == 1:\npile.append(pile[-1]+2)\nelse:\npile.append(pile[-1]+1)\nreturn pile\ndef make_a_pile(n):\npile = [n]\nfor i in range(n-1\nif pile[i] % 2 != 0:\npile.append(pile[i] + 1)\nelse:\npile.append(pile[i] + 2)\nreturn pile\nFigure 6: A HumanEval case of PanGu-Coder2 surpassing WizardCoder and StarCoder.\n10\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nPrompt\nPanGu-Coder2\nWizardCoder\nStarCoder\n\"\"\"\nGiven an array arr of integers and a positive integer k, return a sorted list of length k with the maximum k numbers in arr.\nExample 1:\nInput: arr = [-3, -4, 5], k = 3\nOutput: [-4, -3, 5]\nExample 2:\nInput: arr = [4, -4, 4], k = 2\nOutput: [4, 4]\nExample 3:\nInput: arr = [-3, 2, 1, 2, -1, -2, 1], k = 1\nOutput: [2]\nNote:\n1. The length of the array will be in the range of [1, 1000].\n2. The elements in the array will be in the range of [-1000, 1000].\n3. 0 <= k <= len(arr)\n\"\"\"\ndef maximum(arr, k):\narr.sort()\nreturn arr[-k:]\ndef maximum(arr, k):\n# Time Complexity: O(nlogn)\n# Space Complexity: O(n)\narr.sort()\nreturn arr[-k:]\ndef maximum(arr, k):\n# Sort the array in descending order \nto find the largest k numbers\narr = sorted(arr, reverse=True)\n# Return a sorted list with the \nfirst k elements in arr\nreturn sorted(arr[:k])\nFigure 7: A HumanEval case of WizardCoder surpassing PanGu-Coder2 and StarCoder.\nPrompt\nPanGu-Coder2\nWizardCoder\nStarCoder\n\"\"\"\nYou are given a list of two strings, both strings consist of open parentheses '(' or close parentheses ')' only.\nYour job is to check if it is possible to concatenate the two strings in some order, that the resulting string will be good.\nA string S is considered to be good if and only if all parentheses in S are balanced. For example: the string '(())()' is good, \nwhile the string '())' is not.\nReturn 'Yes' if there's a way to make a good string, and return 'No' otherwise.\nExamples:\nmatch_parens(['()(', ')']) == 'Yes'\nmatch_parens([')', ')']) == 'No'\n\"\"\"\ndef match_parens(lst):\nstack = []\nfor char in ''.join(lst):\nif char == '(':\nstack.append(char)\nelif char == ')':\nif not stack or stack.pop() != '(':\nreturn 'No'\nreturn 'Yes' if not stack else 'No'\ndef match_parens(lst):\nstr1, str2 = lst\nstack = []\nfor c in str1 + str2:\nif c == '(':\nstack.append(c)\nelif c == ')':\nif not stack:\nreturn \"No\"\nstack.pop()\nreturn \"Yes\" if not stack else \"No\"\ndef match_parens(lst):\nstack = []\nfor i in lst:\nif i == '(':\nstack.append(i)\nelif i == ')':\nif len(stack) == 0:\nreturn 'No'\nelse:\nstack.pop()\nif len(stack) == 0:\nreturn 'Yes'\nelse:\nreturn 'No'\nFigure 8: A HumanEval case where PanGu-Coder2, WizardCoder, and StarCoder all generate incorrect outputs.\n11\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\n4.3\nInference Optimization\nSince GPU memory consumption and inference speed are crucial factors for the deployment and use of model in\npractice, we conducted experiments involving the following quantization techniques to study the optimization strategies\nof model inference:\n\u2022 CTranslate2:7 CTranslate2 is a library for accelerating Transformer models when inference, developed by\nOpenNMT.\n\u2022 GPTQ:8 A LLMs quantization package based on GPTQ algorithm.\nTable 4 shows the GPU memory consumption, inference speed, and HumanEval performance of models optimized\nusing different quantization techniques. We used 8-bit (4-bit) quantization and the following decoding parameters\nin the inference stage: top_p=0.95, tempreture=0.2, max_new_tokens=512. Across all quantization techniques, we\ncan observe a significant decrease in terms of memory usage and a significant increase in terms of inference speed. It\nis incredible that after being quantized with CTranslate2, the performance of our model on HumanEval has a slight\nimprovement. A plausible reason for this phenomenon is the robustness of PanGu-Coder2 itself. We plan to conduct an\nin-depth study on this interesting result in our further work.\nTable 4: A comparison of different quantization techniques (on the same device)\nModel\nPrecision\nGPU Memory Consumption\n(GB)\nInference Speed\n(ms/token)\nHumanEval\n(greedy decoding)\nPanGu-Coder2\nfloat16\n32.36\n75\n62.20\nPanGu-Coder2-CTranslate2\nint8\n16.29\n33\n64.63\nPanGu-Coder2-GPTQ\nint8\n16.92\n51\n51.22\nPanGu-Coder2-GPTQ\nint4\n9.82\n42\n51.83\n5\nConclusion\nIn this paper, we introduce a novel frameork, namely RRTF, and present a new Code LLM, namely PanGu-Coder2.\nFirstly, we adopt the Evol-Instruct technique to obtain a substantial amount of high-quality natural language instruction\nand code solution data pairs. Then, we train the base model by ranking candidate code solutions using feedback from\ntest cases and heurstic preferences. Through comprehensive evaluations on HumanEval, CodeEval, and LeetCode\nbenchmarks, PanGu-Coder2 achieves new state-of-the-art performance among billion-parameter-level Code LLMs,\nsurpassing all of the existing ones by a large margin. In our future work, we will delve into the combination of RRTF\nand instruct tuning to boost the performance of Code LLMs.\nReferences\nDaoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou.\nLarge language models meet NL2Code: A survey. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 7443\u20137464, Toronto, Canada, July 2023. Association for\nComputational Linguistics. URL https://aclanthology.org/2023.acl-long.411.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom, Eccles, James\nKeeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d\u2019Autume, Igor\nBabuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy,\nDaniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. Competition-level code generation with alphacode. Science, 378:1092 \u2013 1097, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua\n7https://github.com/OpenNMT/CTranslate2\n8https://github.com/PanQiWei/AutoGPTQ\n12\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nMaynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du,\nBenton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra,\nKevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander\nSpiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with\npathways. ArXiv, abs/2204.02311, 2022.\nFenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhong-Yi Li, Qi Zhang,\nMeng Xiao, Bo Shen, Lin Li, Hao Yu, Li yu Yan, Pingyi Zhou, Xin Wang, Yu Ma, Ignacio Iacobacci, Yasheng\nWang, Guangtai Liang, Jia Wei, Xin Jiang, Qianxiang Wang, and Qun Liu. PanGu-Coder: Program synthesis with\nfunction-level language modeling. ArXiv, abs/2207.11280, 2022.\nHuggingface. Training CodeParrot from Scratch, 2021. https://huggingface.co/blog/codeparrot.\nFrank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. A systematic evaluation of large language models\nof code. Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, 2022.\nDaoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-\nGuang Lou. CERT: Continual pre-training on sketches for library-oriented code generation. In International Joint\nConference on Artificial Intelligence, 2022a.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Mu\u00f1oz Ferrandis, Niklas\nMuennighoff, Mayank Mishra, Alexander Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson,\nYangtian Zi, J. Poirier, Hailey Schoelkopf, Sergey Mikhailovich Troshin, Dmitry Abulkhanov, Manuel Romero,\nMichael Franz Lappert, Francesco De Toni, Bernardo Garc\u2019ia del R\u2019io, Qian Liu, Shamik Bose, Urvashi Bhat-\ntacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen,\nDanish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Christopher Hughes, Daniel Fried,\nArjun Guha, Harm de Vries, and Leandro von Werra. SantaCoder: don\u2019t reach for the stars! ArXiv, abs/2301.03988,\n2023.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\nChristopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161,\n2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie\nCai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\n2021.\nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi:10.48550/arXiv.2303.08774. URL https:\n//doi.org/10.48550/arXiv.2303.08774.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat\nLee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712, 2023.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint\narXiv:2306.11644, 2023.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei\nLin, and Daxin Jiang. WizardCoder: Empowering code large language models with evol-instruct. arXiv preprint\narXiv:2306.08568, 2023.\nShuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. Reacc: A retrieval-\naugmented code completion framework. arXiv preprint arXiv:2203.07722, 2022.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder:\nRepository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023.\nJiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. Rltf: Reinforcement learning from\nunit test feedback, 2023.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven CH Hoi. CodeRL: Mastering code genera-\ntion through pretrained models and deep reinforcement learning. arXiv preprint arXiv:2207.01780, abs/2207.01780,\n2022.\n13\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nParshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. Execution-based code generation using deep\nreinforcement learning. arXiv preprint arXiv:2301.13816, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022a.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code\ngeneration with generated tests. arXiv preprint arXiv:2207.10397, 2022.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen, Andi Wang,\nYang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation with multilingual\nevaluations on humaneval-x. ArXiv, abs/2303.17568, 2023.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexan-\ndra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. BLOOM: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\nYekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. ERNIE-Code: Beyond english-centric\ncross-lingual pretraining for programming languages. arXiv preprint arXiv:2212.06742, 2022.\nShubham Chandel, Colin B. Clement, Guillermo Serrato, and Neel Sundaresan. Training and evaluating a jupyter\nnotebook data science assistant. ArXiv, abs/2201.12901, 2022.\nShuyan Zhou, Uri Alon, Frank F Xu, Zhengbao JIang, and Graham Neubig. DocCoder: Generating code by retrieving\nand reading docs. In The Eleventh International Conference on Learning Representations, 2023a.\nDaoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. When language model meets private\nlibrary. In Conference on Empirical Methods in Natural Language Processing, 2022b.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke\nZettlemoyer, and Mike Lewis. InCoder: A generative model for code infilling and synthesis. In The Eleventh\nInternational Conference on Learning Representations, 2023.\nMohammad Bavarian, Heewoo Jun, Nikolas A. Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark\nChen. Efficient training of language models to fill in the middle. ArXiv, abs/2207.14255, 2022.\nAnh Nguyen, Nikos Karampatziakis, and Weizhu Chen. Meet in the Middle: A new pre-training paradigm. arXiv\npreprint arXiv:2303.07295, 2023.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,\net al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023b.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv\npreprint arXiv:2304.03277, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\nAmanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow\ninstructions with human feedback. In NeurIPS, 2022b. URL http://papers.nips.cc/paper_files/paper/\n2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align\nlanguage models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang.\nRAFT: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767,\n2023.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. CodeGen: An open large language model for code with multi-turn program synthesis. arXiv preprint\narXiv:2203.13474, 2022.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm:\nEmpowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\nNoam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL\nhttp://arxiv.org/abs/1911.02150.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code\nlarge language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.\n14\nPanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback\nA PREPRINT\nHao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, and Qianxiang\nWang. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. arXiv preprint\narXiv:2302.00288, 2023.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\nJin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022.\n15\n"
  },
  {
    "title": "Scaling TransNormer to 175 Billion Parameters",
    "link": "https://arxiv.org/pdf/2307.14995.pdf",
    "upvote": "21",
    "text": "Technical Report\nTRANSNORMERLLM: A FASTER AND BETTER LARGE\nLANGUAGE MODEL WITH IMPROVED TRANSNORMER\nZhen Qin\u266f, Dong Li\u266f, Weigao Sun\u266f, Weixuan Sun\u266f, Xuyang Shen\u266f,\nXiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, Yiran Zhong\u2217\nOpenNLPLab, Shanghai AI Laboratory\nhttps://github.com/OpenNLPLab/TransnormerLLM\nABSTRACT\nWe present TransNormerLLM, the first linear attention-based Large Language\nModel (LLM) that outperforms conventional softmax attention-based models in\nterms of both accuracy and efficiency. TransNormerLLM evolves from the pre-\nvious linear attention architecture TransNormer (Qin et al., 2022a) by making\nadvanced modifications that include positional embedding, linear attention ac-\nceleration, gating mechanism, tensor normalization, and inference acceleration\nand stabilization. Specifically, we use LRPE (Qin et al., 2023b) together with an\nexponential decay to avoid attention dilution issues while allowing the model to\nretain global interactions between tokens. Additionally, we propose Lightning\nAttention, a cutting-edge technique that accelerates linear attention by more than\ntwice in runtime and reduces memory usage by a remarkable four times. To further\nenhance the performance of TransNormer, we leverage a gating mechanism to\nsmooth training and a new tensor normalization scheme to accelerate the model,\nresulting in an impressive acceleration of over 20%. Furthermore, we develop a\nrobust inference algorithm that ensures numerical stability and consistent inference\nspeed, regardless of the sequence length, showcasing superior efficiency during\nboth training and inference stages. We also implement an efficient model paral-\nlel schema for TransNormerLLM, enabling seamless deployment on large-scale\nclusters and facilitating expansion to even more extensive models, i.e., LLMs with\n175B parameters. We validate our model design through a series of ablations\nand train models with sizes of 385M, 1B, and 7B on our self-collected corpus.\nBenchmark results demonstrate that our models not only match the performance of\nstate-of-the-art LLMs with Transformer but are also significantly faster.\n1\nINTRODUCTION\nThe field of Natural Language Processing (NLP) has been revolutionized by the advent of large-scale\nlanguage models (LLMs) (Touvron et al., 2023a; Biderman et al., 2023; Brown et al., 2020). These\nmodels have demonstrated exceptional performance across a multitude of tasks, elevating abilities to\ncomprehend, generate, and interact with human languages in computational frameworks. Previous\nlanguage modeling development has predominantly centered around Transformer architectures, with\nseminal models such as vanilla Transformer (Vaswani et al., 2017), GPT series (Radford et al., 2018;\n2019; Brown et al., 2020), BERT (Devlin et al., 2019), and BART (Lewis et al., 2019) standing as\nstandard backbones in related fields. The success of Transformer architectures is premised on the\nsoftmax attention mechanism, which discerns dependencies among input tokens in a data-driven\nscheme and has global position awareness, offering the model an effective way to handle the long-\nrange dynamism of natural language.\nNevertheless, conventional Transformers are not without their constraints. Primarily, their quadratic\ntime complexity with respect to the sequence length limits their scalability and hampers efficiency\nin terms of computational resources and time during the training and inference stages. Numerous\nefficient sequence modeling methods have been proposed in an attempt to reduce the quadratic time\ncomplexity to linear (Katharopoulos et al., 2020; Choromanski et al., 2021; Qin et al., 2022b; Zheng\n\u2217Yiran Zhong is the corresponding author. Email: zhongyiran@gmail.com. \u266f equal contribution.\n1\narXiv:2307.14995v2  [cs.CL]  19 Jan 2024\nTechnical Report\net al., 2023; 2022). However, there are two reasons that prohibit them to be applied to LLMs: 1)\ntheir performance in language modeling is often unsatisfactory; 2) they do not demonstrate speed\nadvantages in real-world scenarios.\nIn this paper, we introduce TransNormerLLM, the first linear attention-based LLM that surpasses con-\nventional softmax attention in both accuracy and efficiency. The development of TransNormerLLM\nbuilds upon the foundations of the previous linear attention architecture, TransNormer (Qin et al.,\n2022a), while incorporating a series of advanced modifications to achieve superior performance. The\nkey enhancements in TransNormerLLM include positional embedding, linear attention acceleration,\ngating mechanism, tensor normalization, and inference acceleration.\nOne notable improvement is the replacement of the TransNormer\u2019s DiagAttention with Linear\nAttention to enhance global interactions. To address the issue of dilution, we introduced LRPE (Qin\net al., 2023b) with exponential decay (Press et al., 2022; Qin et al., 2023a; Peng et al., 2023a).\nLightning Attention, a novel technique that significantly accelerates linear attention during training is\nintroduced, resulting in a more than two-fold improvement, while also reducing memory usage by\nfour times with IO awareness. Furthermore, we simplified GLU and Normalization, with the latter\nleading to a 20% speedup. A robust inference algorithm ensures the stability of numerical values and\nconstant inference speed, regardless of the sequence length, thereby enhancing the efficiency of our\nmodel during both training and inference stages.\nWe validate the efficacy of TransNormerLLM on our self-collected pre-train corpus, which is more\nthan 6TB in size and contains over 2 trillion tokens. We expand the original TransNormer model,\nranging from 385M to 175B parameters, and benchmark models with sizes of 385M, 1B, and 7B.\nThe benchmark results demonstrate that our models achieve competitive performance with existing\nstate-of-the-art transformer-based LLMs with similar sizes while also having faster inference speeds.\nWe will open-source our pre-trained models, enabling researchers and practitioners to build upon our\nwork and explore efficient transformer structures in LLMs.\n2\nRELATED WORK\n2.1\nTRANSFORMER-BASED LLMS\nIn recent years, the field of Large Language Models (LLMs) has experienced significant advance-\nments. Adhering to the scaling laws (Kaplan et al., 2020), various LLMs with over 100 billion\nparameters have been introduced, such as GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2022),\nPaLM (Chowdhery et al., 2022), GLM (Du et al., 2022) and etc.. More specialized models like\nGalactica (Taylor et al., 2022) have also emerged for specific domains like science. A notable\ndevelopment is Chinchilla (Hoffmann et al., 2022), an LLM model with 70 billion parameters that\nredefines these scaling laws, focusing on the number of tokens rather than model weights. Further-\nmore, LLaMA (Touvron et al., 2023a) has also sparked interest due to its promising performance\nand open-source availability. The discourse around LLMs also encompasses the dynamics between\nopen-source and closed-source models. Open-source models such as BLOOM (Workshop et al.,\n2023), OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023a), Pythia (Biderman et al., 2023) and\nFalcon (Penedo et al., 2023) are rising to compete against their closed-source counterparts, including\nGPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022). To speed up training, Sparse\nAttention (Child et al., 2019; Beltagy et al., 2020) was introduced, but among large models, only\nGPT-3 adopted it (Brown et al., 2020; Scao et al., 2022).\n2.2\nNON-TRANSFORMER-BASED LLMS CANDIDATES\nDespite the proliferation of Transformer-based large models in the research community, a portion\nof recent work has prioritized addressing its square time complexity. This focus has led to the\nexploration and development of a series of model architectures that diverge from the traditional\nTransformer structure. Among them, four significant contenders\u2014linear transformers, state space\nmodel, long convolution, and linear recurrence\u2014have shown promising results as substitutes for\nself-attention (SA) modules when modeling long sequences. These alternatives are favored for their\nsuperior asymptotic time complexity and competitive performances.\nLinear Transformer\nLinear Transformer decomposes Softmax Attention into the form of the inner\nproduct of hidden representations, which allows it to use the \"Right Product Trick,\" where the product\n2\nTechnical Report\nof keys and values is computed to avoid the quadratic n \u00d7 n matrix. Different methods utilize various\nhidden representations. For example, Katharopoulos et al. (2020) use 1+elu as an activation function,\nQin et al. (2022b) use the cosine function to approximate the properties of softmax, and Ke et al.\n(2021); Zheng et al. (2022; 2023) approximate softmax through theoretical approaches. Although its\ntheoretical complexity is O(nd2), the actual computational efficiency of Linear Attention becomes\nquite low when used in causal attention due to the need for cumsum operations (Hua et al., 2022).\nOn the other hand, most Linear Transformers still exhibit a certain performance gap compared to\ntraditional Transformers (Katharopoulos et al., 2020; Liu et al., 2022).\nState Space Model\nState Space Model is based on the State Space Equation for sequence mod-\neling (Gu et al., 2022b), using special initialization (Gu et al., 2020; 2022a), diagonalization as-\nsumptions (Gupta et al., 2022), and some techniques (Dao et al., 2022b) to achieve performance\ncomparable to Transformers. On the other hand, due to the characteristics of the State Space Equation,\nit enables inference to be conducted within constant complexity (Gu et al., 2022b).\nLong Convolution\nLong convolution models (Qin et al., 2023a; Fu et al., 2023) utilize a kernel size\nequal to the input sequence length, facilitating a wider context compared to traditional convolutions.\nTraining these models involves the efficient O(n log n) Fast Fourier Transforms (FFT) algorithm.\nHowever, long convolutions pose certain challenges, such as the need for causal convolution inference,\nwhich necessitates caching all historical computations similar to SA\u2019s key-value (KV) cache. The\nmemory requirements for handling long sequences, coupled with the higher inference complexity\ncompared to RNNs, make them less ideal for processing long sequences.\nLinear RNN\nLinear RNNs (Orvieto et al., 2023; Peng et al., 2023b), in contrast, stand out as more\nsuitable replacements for SA in long-sequence modeling. A notable example is the RWKV (Peng\net al., 2023b) model, a linear RNN-based LLM that has shown competitive performance against\nsimilarly scaled GPT models.\n3\nTRANSNORMERLLM\n3.1\nARCHITECTURE IMPROVEMENT\nIn this section, we thoroughly investigate each module of the network and propose several improve-\nments to achieve an optimal balance between efficiency and performance. Below, we outline the key\ndesigns of each block along with the inspiration behind each change. For the details of configurations\nfor TransNormerLLM variants from 385M to 175B parameters, see Appendix A.\n3.1.1\nIMPROVEMENT 1: POSITION ENCODING\nIn TransNormer, DiagAttention is used at the lower layers to avoid dilution issues. However, this\nleads to a lack of global interaction between tokens. In TransNormerLLM, we leverage LRPE (Qin\net al., 2023b) with exponential decay (Press et al., 2022; Qin et al., 2023a; Peng et al., 2023b) to\naddress this issue, retaining full attention at the lower layers. The expression of our position encoding\nis as follows:\nast = q\u22a4\ns kt\u03bbs\u2212t expi\u03b8(s\u2212t) .\n(1)\nwhich we call LRPE-d - Linearized Relative Positional Encoding with exponential decay. Similar to\nthe original LRPE, we set \u03b8 to be learnable. We empirically find that rather than applying LRPE-d to\nevery layer, applying it to the first layer and keeping other layers with exponential decay can speed\nup training by approximately 15-20% but only with a subtle effect on the performance.\nNote that this position encoding is fully compatible with Linear Attention, as it can be decomposed\nwith respect to s and t separately. The value of \u03bb for the h-th head in the l-th layer (assuming there\nare a total of H heads and L layers) is given by:\n\u03bb = exp\n\u0000\u2212 8h\nH \u00d7\n\u00001 \u2212 l\nL\n\u0001\u0001\n.\n(2)\nHere, 8h\nH corresponds to the decay rate of the h-th head, while\n\u00001 \u2212 l\nL\n\u0001\ncorresponds to the decay rate\nof the l-th layer. The term\n\u00001 \u2212 l\nL\n\u0001\nensures that the Theoretical Receptive Fields (TRF) (Qin et al.,\n2023c) at the lower layers is smaller compared to the higher layers, which aligns with TransNormer\u2019s\nmotivation. It should be noted that the decay rate in the last layer is set to 1, allowing each token\nto attend to global information. We choose \u03bb to be non-learnable since we empirically found that\ngradients become unstable when \u03bb is learnable, leading to NaN values.\n3\nTechnical Report\n3.1.2\nIMPROVEMENT 2: GATING MECHANISM\nGate can enhance the performance of the model and smooth the training process. In TransNormer-\nLLM, we adopted the approach from Flash (Hua et al., 2022) and used the structure of Gated Linear\nAttention (GLA) in token mixing:\nTokenMixer : O = Norm(QK\u22a4V) \u2299 U,\n(3)\nwhere:\nQ = \u03d5(XWq), K = \u03d5(XWk), V = XWv, U = XWu.\n(4)\nWe choose \u03d5 to be swish (Ramachandran et al., 2017) activation function as we empirically find that\nit outperforms other activation functions, as shown in Table 6.\nTo further accelerate the model, we propose Simple GLU (SGLU), which removes the activation\nfunction from the original GLU structure as the gate itself can introduce non-linearity. Therefore, our\nchannel mixing becomes:\nChannelMixer : O = [V \u2299 U]Wo, V = XWv, U = XWu,\n(5)\nWe empirically find that not using an activation function in GLU will not lead to any performance\nloss, as demonstrated in Table 7.\n3.1.3\nIMPROVEMENT 3: TENSOR NORMALIZATION\nWe employ the NormAttention introduced in TransNormer (Qin et al., 2022a) as follows:\nO = Norm((QK\u22a4)V)\n(6)\nThis attention mechanism eliminates the softmax and scaling operation. Moreover, it can be trans-\nformed into linear attention through right multiplication:\nO = Norm(Q(K\u22a4V))\n(7)\nThis linear form allows for recurrent prediction with a complexity of O(nd2), making it efficient\nduring inference. Specifically, we only update K\u22a4V in a recurrent manner without computing the full\nattention matrix. In TransNormerLLM, we replace the RMSNorm with a new simple normalization\nfunction called SimpleRMSNorm, abbreviated as SRMSNorm:\nSRMSNorm(x) =\nx\n\u2225x\u22252/\n\u221a\nd.\n(8)\nWe empirically find that using SRMSNorm does not lead to any performance loss, as demonstrated in\nthe ablation study in Table. 8.\n3.1.4\nTHE OVERALL STRUCTURE\n\ud835\udc4b\nSGLU\nAdd\nSGLU\n\ud835\udc48\n\ud835\udc44\n\ud835\udc3e\n\ud835\udc49\nSRMSNorm\nGLA\n*\n*\n\ud835\udc48\n\ud835\udc49\nGLA\nSRMSNorm\nAdd\nSRMSNorm\nFigure 1: Architecture overview of the proposed\nmodel. Each transformer block is composed of\na Gated Linear Attention(GLA) for token mixing\nand a Simple Gated Linear Unit (SGLU) for chan-\nnel mixing. We apply pre-norm for both modules.\nThe overall structure is illustrated in Figure 1.\nIn this structure, the input X is updated through\ntwo consecutive steps: First, it undergoes Gated\nLinear Attention (GLA) with the application of\nSimpleRMSNorm (SRMSNorm) normalization.\nThen, it goes through the Simple Gated Linear\nUnit (SGLU) with SRMSNorm normalization\nagain. This overall architecture helps improve\nthe model\u2019s performance based on the PreNorm\napproach. The pseudo-code of the overall pro-\ncess is as follows:\nX = X + GLA(SRMSNorm(X)),\nX = X + SGLU(SRMSNorm(X)).\n(9)\n3.2\nTRAINING OPTIMIZATION\n3.2.1\nLIGHTNING ATTENTION\nThe structure of linear attention allows for ef-\nficient attention calculation with a complexity\n4\nTechnical Report\nof O(nd2) through right-multiplication. However, for causal prediction, right-multiplication is not\nefficient as it necessitates cumsum computation (Hua et al., 2022), which hinders parallelism training.\nAs a result, during training, we continue to use the conventional left-multiplication version. To\naccelerate attention calculations, we introduce the Lightning Attention algorithm inspired by (Dao,\n2023; Dao et al., 2022a), which makes our linear attention IO-friendly. It computes the following:\nO = (QK\u22a4 \u2299 M)V.\n(10)\nHere, M is the attention mask which enables lower triangular causal masking and positional encoding.\nIn the Lightning Attention, we split the inputs Q, K, V into blocks, load them from slow HBM to\nfast SRAM, then compute the attention output with respect to those blocks. Then we accumulate\nthe final results. The computation speed is accelerated by avoiding the operations on slow HBM.\nThe implementation details of Lightning Attention are shown in Appendix B, where Algorithm 3 for\nforward pass and Algorithm 4 for backward pass.\n3.2.2\nMODEL PARALLELISM ON TRANSNORMERLLM\nTo effectively execute large-scale pre-training for TransNormerLLM, we have put efforts on sys-\ntem optimization encompassing various dimensions. Specifically, we employ fully sharded data\nparallelism (FSDP) (Zhao et al., 2023), a technique that shards all model parameters, gradients, and\noptimizer state tensors across the entire cluster. This strategic partition significantly reduces the\nmemory footprint on each individual GPU, thereby enhancing memory utilization. In our pursuit of\ngreater efficiency, we leverage activation checkpointing (Shoeybi et al., 2019), which minimizes the\ncached activations in memory during the forward pass. Instead of retaining these activations, they\nare recomputed when calculating gradients in the backward pass. This approach saves huge GPU\nmemory thus enable to apply bigger batch size. Furthermore, we harness automatic mixed precision\n(AMP) (Micikevicius et al., 2017) to simultaneously save GPU memory and expedite computational\nspeed. It\u2019s noteworthy that in our experimental setup, we employ BFloat16 (Kalamkar et al., 2019)\ndue to its observed advantage in enhancing the training stability of TransNormerLLM models.\nIn addition to the previously mentioned optimization endeavors, we delve deeper into the realm of\nsystem engineering by implementing model parallelism specifically tailored to linear transformers,\ndrawing inspiration from Megatron-LM model parallelism (Shoeybi et al., 2019). In a standard\ntransformer model, each transformer layer comprises a self-attention block followed by a two-layer\nmulti-layer perceptron (MLP) block. Megatron-LM model parallelism independently addresses these\ntwo constituent blocks. Similarly, within the architecture of TransNormerLLM, characterized by its\ntwo primary components, SGLU and GLA, we apply model parallelism to each of these components\nseparately. The intricate details of our model parallelism strategies are elaborated below.\nModel Parallelism on SGLU\nRecall the SGLU structure in (5):\nO = [(XWv) \u2299 (XWu)]Wo,\n(11)\nThe model parallelism adaptation of SGLU is as follows:\n[O\u2032\n1, O\u2032\n2] = X[W1\nv, W2\nv] \u2299 X[W1\nu, W2\nu], = [XW1\nv, XW2\nv] \u2299 [XW1\nu, XW2\nu],\n(12)\nwhich splits the weight matrices Wv and Wu along their columns and obtains an output matrix\nsplitting along its columns too. Then the split output [O1, O2] is multiplied by another matrix which\nis split along its rows as:\nO = [O\u2032\n1, O\u2032\n2][W1\no, W2\no]\u22a4 = O\u2032\n1W1\no + O\u2032\n2W2\no\n(13)\nSimilar with model parallelism in Megatron-LM, this whole procedure splits three general matrix\nmultiplies (GEMMs) inside the SGLU block across multiple GPUs and only introduces a single\nall-reduce collective communication operation in both the forward and backward passes, respectively.\nModel Parallelism on GLA\nRecall the GLA block in (3) and (4), its model parallelism version is:\n[O1, O2] = SRMSNorm(QK\u22a4V) \u2299 U,\n(14)\nwhere:\nQ = [\u03d5(XW1\nq), \u03d5(XW2\nq)], K = [\u03d5(XW1\nq), \u03d5(XW2\nq)], V = X[W1\nv, W2\nv], U = X[W1\nu, W2\nu],\n(15)\nNote that in our implementation, we use the combined QKVU projection to improve computation\nefficiency for linear attention. The obtained split output matrix [O1, O2] again is multiplied by a\nweight matrix split along its columns which is similar to (13).\n5\nTechnical Report\nAlgorithm 1 Origin Inference Algorithm\nInput: qt, kt, vt, t = 1, . . . , n;\nOutput: ot, t = 1, . . . , n;\nInitialize: [kv]0 = 0;\nfor t = 1, . . . , n do\n[kv]t = [kv]t\u22121 + kt\u03bb\u2212tv\u22a4\nt ,\not = qt\u03bbt[kv]t.\nend for\nAlgorithm 2 Robust Inference Algorithm\nInput: qt, kt, vt, t = 1, . . . , n;\nOutput: ot, t = 1, . . . , n;\nInitialize: [kv]0 = 0;\nfor t = 1, . . . , n do\n[kv]t = \u03bb[kv]t\u22121 + ktv\u22a4\nt ,\not = qt[kv]t.\nend for\n3.3\nROBUST INFERENCE\nIn this section, we discuss the inference problem in TransNormerLLM. It is important to note that the\nformula 1 can be decomposed into the following form:\nast = (qs\u03bbs expi\u03b8s)\u22a4(kt\u03bb\u2212t expi\u03b8t).\n(16)\nThis allows TransNormerLLM to perform inference in the form of an RNN. Details of the procedure\nare shown in Algorithm 1. However, it is worth noting that \u03bb < 1, which results in:\n\u2225qs\u03bbs expi\u03b8s \u22252 = \u2225qs\u22252\u03bbs \u2192 0, \u2225kt\u03bb\u2212t expi\u03b8t \u22252 = \u2225kt\u22252\u03bb\u2212t \u2192 \u221e,\n(17)\nleading to numerical precision issues.\nTo avoid these issues, we propose a Robust Inference Algorithm in 2. Since \u2225qs expi\u03b8s \u2225 = \u2225qs\u2225,\n\u2225kt expi\u03b8t \u2225 = \u2225kt\u2225, for simplicity, we will omit LRPE (Qin et al., 2023b) in the subsequent\ndiscussions, considering only ast = q\u22a4\ns kt\u03bbs\u2212t. We provide a mathematical proof of [kv]t =\n\u03bb\u2212t[kv]t in Appendix C\n4\nEXPERIMENTS\nWe use PyTorch (Paszke et al., 2019) and Triton (Tillet et al., 2019) to implement TransNormerLLM\nin Metaseq framework (Zhang et al., 2022). Our model is trained using Adam optimizer (Kingma &\nBa, 2017), and we employ FSDP to efficiently scale our model to NVIDIA A100 80G clusters. We\nadditionally leverage the model parallel as appropriate to optimize performance. In ablation studies,\nall models are trained on a sampled corpus from our corpus with 300B tokens. In order to reduce\nthe fluctuation of Losses and PPLs in the tables below, we compute the average Losses and PPLs of\nthe last 1k iterations as the final metrics. For our benchmark models, we train our 385M, 1B, and\n7B models on our corpus for 1 trillion, 1.2 trillion, and 1.4 trillion tokens respectively. We use an\ninput sequence length of 8192 tokens in our pretraining process. For a comprehensive understanding\nof our corpus, encompassing intricate details such as data preprocessing methods and tokenization\nprocedures, we direct interested readers to Appendix D.\n4.1\nARCHITECTURE ABLATIONS\nTransformer vs TransNormerLLM\nWe carried out a meticulous series of comparative tests\nbetween our TransNormerLLM and Transformer, spanning over an array of disparate sizes. The com-\nparative performance of these models is clearly illustrated in Table 1. Under identical configurations,\nit becomes evident that our TransNormerLLM exhibits a superior performance profile compared to\nTransformer. We observed that TransNormerLLM outperformed Transformer by a remarkable 5%\nat the size of 385M. More importantly, as the size reached 1B, this superiority became even more\npronounced, with an advantage of 9% for TransNormerLLM over Transformer.\nTable 1: Transformer vs TransNormerLLM. TransNormerLLM performs better than Transformer in size of\n385M and 1B under identical configurations by 5% and 9%, respectively.\nModel Size\n385M\n1B\nMethod\nUpdates\nLoss\nPPL\nUpdates\nLoss\nPPL\nTransformer\n100K\n2.362\n5.160\n100K\n2.061\n4.765\nTransNormerLLM\n100K\n2.248\n4.770\n100K\n1.896\n3.729\nTable 2: TransNormer vs TransNormerLLM.\nMethod\nParams Updates Loss\nPPL\nTransNormerLLM 385M\n100K\n2.248 4.770\nTransNormer-T1\n379M\n100K\n2.290 4.910\nTransNormer-T2\n379M\n100K\n2.274 4.858\nTransNormer vs TransNormerLLM\nWe\ncompare the original TransNormer and the im-\nproved TransNormerLLM and the results are\nshown in Table 2. TransNormerLLM exhibited\nan enhancement of 2% and 1% respectively.\n6\nTechnical Report\nTable 3: Positional encoding. LRPE-d leads to\nthe most optimal outcome.\nPE Methods\nParams\nUpdates\nLoss\nPPL\nMix\n385M\n100K\n2.248\n4.770\nAPE\n386M\n100K\n2.387\n5.253\nExp-Decay\n385M\n100K\n2.267\n4.834\nLRPE\n385M\n100K\n2.287\n4.899\nLRPE-d\n385M\n100K\n2.236\n4.728\nPositional Encoding\nIn the positional encod-\ning experiment, we conducted a series of tests,\ncomparing Mix (LRPE-d for the first layer, Exp-\nDecay for the rest), APE (Absolute Positional\nEncoding), LRPE, Exp-Decay (Exponential De-\ncay), and LRPE-d. As evident from Table 3,\nOurs and LRPE-d achieve better performance\nthan other options. We select the Mix positional\nencoding as it boosts the training speed up to\n20% while only slightly worse than LRPE-d.\nTable 4: Ablations on decay temperature. The\nresults of decay temperature proved to be superior.\nTemperature\nParams\nUpdates\nLoss\nPPL\nw/ temperature\n385M\n100K\n2.248\n4.770\nw/o temperature\n385M\n100K\n2.258\n4.804\nWe also perform ablations on the decay tem-\nperature\n\u00001 \u2212 l\nL\n\u0001\nin Eq. 2. The perplexity of\nthe TransNormerLLM is reduced by adding the\ndecay temperature, as shown in Table 4.\nTable 5: Ablations on gating mechanism. The\nperformance with the gate proved to be superior.\nGate\nParams\nUpdates\nLoss\nPPL\nw/ gate\n385M\n100K\n2.248\n4.770\nw/o gate\n379M\n100K\n2.263\n4.820\nGating Mechanism\nWe conduct ablation stud-\nies to examine the effect of including the gating\nmechanism. As observed in Table 5, gate en-\nabled the reduction of the loss value from 2.263\nto 2.248.\nTable 6: Ablations on GLA activation functions.\nThe results obtained from different activation func-\ntions were virtually identical.\nGLA Act\nParams\nUpdates\nLoss\nPPL\nSwish\n385M\n100K\n2.248\n4.770\nNo Act\n385M\n100K\n2.283\n4.882\n1+elu\n385M\n100K\n2.252\n4.767\nGLA Activation Functions\nWe conducted ex-\nperiments on the GLA (Gated Linear Attention)\nstructure with respect to the activation function.\nAs shown in Table 6, using Swish and 1+elu\nleads to similar performance. However, in our\nexperiments, using 1+elu in our 7B model may\nencounter a NaN problem, so we use Swish in\nour model.\nTable 7: Ablations on GLU activation functions.\nThe exclusion of the activation function had no\nnegative impact on the results.\nGLU Act\nParams\nUpdates\nLoss\nPPL\nNo Act\n385M\n100K\n2.248\n4.770\nSwish\n385M\n100K\n2.254\n4.788\nGLU Activation Functions\nWe conduct an\nexperiment by removing the activation function\nwithin the Gated Linear Units (GLU) structure.\nAs shown in Table 7, the results reveal that this\nalteration had a negligible impact on the final\noutcome. As a result, we decide to adopt the\nSimple Gated Linear Units (SGLU) structure in\nour final model configuration.\nTable 8: Normalization Functions. The deviation\nin results among the bellowing normalization func-\ntions is minimal.\nNorm Type\nParams\nUpdates\nLoss\nPPL\nSRMSNorm\n385M\n100K\n2.248\n4.770\nRMSNorm\n385M\n100K\n2.247\n4.766\nLayerNorm\n385M\n100K\n2.247\n4.765\nNormalization functions\nIn our study, we\nconducted a series of ablation tests employing\nvarious normalization methods including SRM-\nSNorm, RMSNorm and LayerNorm. The results\nindicate that there is almost no difference among\nthese methods when applied to TransNormer-\nLLM. Nevertheless, during the course of our\ntesting, we revisited and re-engineered the SRM-\nSNorm using Triton. As it is shown in Figure 2, empirical evidence supports that our modification\noffers a significant boost in computational speed when operating with larger dimensions, compared\nto the PyTorch implementation methods.\nLightning Attention\nWe conducted a speed and memory comparison between our Lightning\nAttention and the baseline, which is the PyTorch implementation of the NormAttention (Qin et al.,\n2022a). Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass. Baseline\nruntime grows quadratically with sequence length, while Lightning Attention operates significantly\nfaster, at least 2\u00d7 faster than the PyTorch implementation. Figure 3 (right) reports the memory\nfootprint of Lightning Attention compared to the baseline. The memory footprint of Lightning\nAttention grows linearly with sequence length, which is up to 4\u00d7 more efficient than the baseline\nwhen the sequence length is 8192. Our proposed Lightning Attention achieves superior efficiency.\n7\nTechnical Report\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\nRuntime (s)\nSequence length\nForward Pass\nTriton SRMSNorm\nPyTorch SRMSNorm\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\n3.6\n4.2\n4.8\n5.4\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\nRuntime (s)\nSequence length\nBackward Pass\nTriton SRMSNorm\nPyTorch SRMSNorm\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n512\n1024\n2048\n4096\n8192\n16384\nRuntime (s)\nFeature dimension\nForward Pass\nTriton SRMSNorm\nPyTorch SRMSNorm\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n512\n1024\n2048\n4096\n8192\n16384\nRuntime (s)\nFeature dimension\nBackward Pass\nTriton SRMSNorm\nPyTorch SRMSNorm\nFigure 2: Performance Evaluation of SRMSNorm Implementation. The upper figures exhibit the\nruntime comparison of the forward pass (left) and backward pass (right) for different sequence lengths,\nwith a fixed feature dimension of 3072. The lower two figures illustrate the runtime comparison for\nvarious feature dimensions, with a fixed sequence length of 4096.\n0\n10\n20\n30\n40\n512\n1024\n2048\n4096\n8192\nMemory footprint (GB)\nSequence Length\nMemory Footprint\n4x\n0\n50\n100\n150\n200\n250\n512\n1024\n2048\n4096\n8192\nRuntime (ms)\nSequence Length\nRuntime (Fwd Pass + Bwd Pass)\n2x\nLightningAttn\nOrignal Linear Attn\nLightningAttn\nOrignal Linear Attn\nFigure 3: Memory and speed comparison between linear attention and lightning attention. Left:\nruntime of forward + backward pass milliseconds for different sequence lengths, with a fixed feature\ndimension of 2048. Right: memory footprints of forward + backward pass for different sequence\nlengths, with a fixed feature dimension of 2048.\n0.0\n3.0\n6.0\n9.0\n12.0\n15.0\n256\n512\n1024\n2048\n4096\n8192\n16384 32000\nRuntime (ms)\nSequence length\nInference Time\nTransNormerLLM\nTransformer\n0.0\n3.0\n6.0\n9.0\n12.0\n15.0\n256\n512\n1024\n2048\n4096\n8192\n16384 32000\nMemory (GB)\nSequence length\nInference Memory Footprint\nTransNormerLLM\nTransformer\nFigure 4: Inference Time and Memory Footprint. Left: inference runtime measured in milliseconds\nacross different sequence lengths. Right: memory consumption during inference for varying sequence\nlengths. It is noteworthy that as the sequence length increases, TransNormerLLM demonstrates a\nconsistent inference time and memory footprint.\n4.2\nBENCHMARKS\nIn order to validate the effectiveness of TransNormerLLM, we tested our 385M, 1B, and 7B models\non Commonsense Reasoning Task, MMLU(Hendrycks et al., 2021), CMMLU(Li et al., 2023), and\nC-Eval(Huang et al., 2023). For comparison, we selected several open-source models as competitors,\nincluding Transformer-based models such as OPT (Zhang et al., 2022), Pythia (Biderman et al., 2023),\nBLOOM (Workshop et al., 2023), GPT-Neo (Black et al., 2022), GPT-J (Wang & Komatsuzaki, 2021),\nMPT (Team et al., 2023), Falcon (Almazrouei et al., 2023), LLaMA1/2 (Touvron et al., 2023a;b),\nOpenLLAMA v1/v2 (Geng & Liu, 2023), Baichuan 1/2 (Baichuan, 2023), ChatGLM 1/2 (Zeng et al.,\n2022; Du et al., 2022), and non-Transformer model RWKV (Peng et al., 2023a). It can be observed\nthat, compared to these models, TransNormerLLM remains highly competitive.\n8\nTechnical Report\nTable 9: Performance Comparison on Commonsense Reasoning and Aggregated Benchmarks.\nFor a fair comparison, we report competing methods\u2019 results reproduced by us using their released\nmodels. Official results are denoted in italics. PS: parameter size (billion). T: tokens (trillion). HS:\nHellaSwag. WG: WinoGrande.\nModel\nPS\nT\nBoolQ\nPIQA\nHS\nWG\nARC-e\nARC-c\nOBQA\nMMLU\nCMMLU\nC-Eval\nOPT\n0.35\n0.30\n57.74\n64.58\n36.69\n52.49\n44.02\n23.89\n28.20\n26.02\n25.34\n25.71\nPythia\n0.40\n0.30\n60.40\n67.08\n40.52\n53.59\n51.81\n24.15\n29.40\n25.99\n25.16\n24.81\nBLOOM\n0.56\n0.35\n55.14\n64.09\n36.97\n52.80\n47.35\n23.98\n28.20\n24.80\n25.35\n27.14\nRWKV\n0.43\n-\n-\n67.52\n40.90\n51.14\n52.86\n25.17\n32.40\n24.85\n-\n-\nOurs\n0.39\n1.0\n62.14\n66.70\n46.27\n54.46\n55.43\n27.99\n32.40\n25.90\n25.05\n25.24\nGPT-Neo\n1.3\n0.3\n61.99\n71.11\n48.93\n54.93\n56.19\n25.85\n33.60\n24.82\n26.03\n23.94\nOPT\n1.3\n0.3\n57.77\n71.71\n53.70\n59.35\n57.24\n29.69\n33.20\n24.96\n24.97\n25.32\nPythia\n1.4\n0.3\n60.73\n70.67\n47.18\n53.51\n56.99\n26.88\n31.40\n26.55\n25.13\n24.25\nBLOOM\n1.1\n0.35\n59.08\n67.14\n42.98\n54.93\n51.47\n25.68\n29.40\n27.30\n25.09\n26.50\nRWKV\n1.5\n-\n-\n72.36\n52.48\n54.62\n60.48\n29.44\n34.00\n25.77\n-\n-\nFalcon\n1.0\n0.35\n61.38\n75.14\n61.50\n60.30\n63.38\n32.17\n35.60\n25.28\n24.88\n25.66\nOurs\n1.0\n1.2\n63.27\n72.09\n56.49\n60.38\n63.68\n35.24\n36.60\n27.10\n25.88\n26.01\nGPT-J\n6.9\n0.3\n65.44\n75.41\n66.25\n64.09\n66.92\n36.60\n38.20\n25.40\n26.47\n23.39\nOPT\n6.7\n0.3\n66.18\n76.22\n67.21\n65.19\n65.66\n34.64\n37.20\n24.57\n25.36\n25.32\nPythia\n6.9\n0.3\n63.46\n75.14\n63.92\n60.77\n67.34\n35.41\n37.00\n24.64\n25.56\n26.40\nBLOOM\n7.1\n0.35\n62.91\n72.69\n62.33\n64.01\n65.11\n33.45\n35.80\n26.25\n24.97\n24.25\nRWKV\n7.4\n-\n-\n76.06\n65.51\n61.01\n67.80\n37.46\n40.20\n24.96\n-\n-\nMPT\n6.9\n1.0\n73.88\n79.43\n76.25\n68.27\n74.79\n41.72\n42.20\n30.80\n25.99\n24.06\nFalcon\n7.2\n1.5\n73.73\n79.38\n76.3\n67.17\n74.62\n43.60\n43.80\n27.79\n25.73\n22.92\nBaichuan1\n7.0\n1.2\n70.09\n76.01\n70.06\n64.09\n71.72\n40.53\n38.20\n42.30\n44.43\n42.80\nBaichuan2\n7.0\n2.6\n72.72\n76.50\n72.17\n68.35\n75.17\n42.32\n39.60\n54.16\n57.07\n54.00\nChatGLM1\n6.7\n1.0\n74.74\n68.88\n45.57\n52.25\n48.78\n31.66\n36.80\n40.63\n37.48\n40.23\nChatGLM2\n7.1\n1.4\n77.65\n69.37\n50.51\n57.62\n59.13\n34.30\n37.00\n45.46\n48.80\n52.55\nOpenLLaMAv1\n6.7\n1.0\n70.43\n75.68\n69.23\n66.69\n71.17\n38.57\n39.00\n30.49\n25.40\n26.09\nOpenLLaMAv2\n6.7\n1.0\n72.20\n78.84\n74.51\n65.67\n72.39\n41.30\n41.00\n41.29\n29.58\n30.01\nLLaMA1\n6.7\n1.0\n76.50\n79.80\n76.10\n70.10\n72.80\n47.60\n57.20\n35.10\n25.62\n25.72\nLLaMA2\n6.7\n2.0\n77.68\n78.07\n76.02\n68.98\n76.30\n46.33\n44.20\n45.30\n32.96\n33.20\nOurs\n6.8\n1.4\n75.87\n80.09\n75.21\n66.06\n75.42\n44.40\n63.40\n43.10\n47.99\n43.18\nCommonsense Reasoning\nWe report BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2019), SIQA\n(Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC\neasy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and their average.\nWe report 0-shot results for all benchmarks using LM-Eval-Harness (Gao et al., 2021). All of our\nmodels achieve competitive performance compared to existing state-of-the-art LLMs, showcasing a\nremarkable ability to comprehend and apply commonsense reasoning.\nAggregated Benchmarks\nWe report the overall results for MMLU (Hendrycks et al., 2021),\nCMMLU (Li et al., 2023), C-Eval (Huang et al., 2023). Official scripts were used for evaluating\nMMLU, CMMLU, and C-Eval, with all evaluation results being conducted with a 5-shot setup. In\ncomparison to top-tier open-source models available in the industry, our models have demonstrated\nmatched performance in both English and Chinese benchmarks.\n4.3\nSCALING TO 175B\nFurthermore, we have carried out a series of experiments to assess the efficacy of model parallelism\nas applied to the TransNormerLLM architecture. The comprehensive outcomes of these experiments\nhave been thoughtfully presented in Appendix E.1. Moreover, our research extends to the meticulous\nevaluation of various cutting-edge system optimization techniques. This evaluation encompasses\ntheir impact on both training speed and context length across models ranging from 7B to 175B in\nscale. We have thoughtfully documented the detailed results of these experiments in Appendix E.2.\n5\nCONCLUSION\nWe introduced TransNormerLLM in this paper, an improved TransNormer that is tailored for LLMs.\nOur TransNormerLLM consistently outperformed Transformers in both accuracy and efficiency.\nExtensive ablations demonstrate the effectiveness of our modifications and innovations in position\nencoding, gating mechanism, activation functions, normalization functions, and lightning attentions.\nThese modifications collectively contribute to TransNormerLLM\u2019s outstanding performance, posi-\ntioning it as a promising choice for state-of-the-art language models. The benchmark results for\nmodels with sizes of 385 million, 1 billion, and 7 billion parameters unequivocally demonstrate that\nTransNormerLLM not only matches the performance of current leading Transformer-based Large\nLanguage Models (LLMs) but also enjoys faster inference speeds. We will release our pre-trained\nTransNormerLLM models to foster community advancements in efficient LLM.\n9\nTechnical Report\nREFERENCES\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojo-\ncaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al.\nFalcon-40b: an open large language model with state-of-the-art performance. Technical report,\nTechnical report, Technology Innovation Institute, 2023.\nBaichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.\nURL https://arxiv.org/abs/2309.10305.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer,\n2020.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,\nLintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models\nacross training and scaling, 2023.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language, 2019.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers, 2019.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea\nGane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,\nDavid Benjamin Belanger, Lucy J Colwell, and Adrian Weller.\nRethinking attention with\nperformers. In International Conference on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=Ua6zuk0WRH.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.\nDai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge,\n2018.\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\npreprint arXiv:2307.08691, 2023.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing\nSystems, 2022a.\n10\nTechnical Report\nTri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hun-\ngry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052,\n2022b. doi: 10.48550/arXiv.2212.14052. URL https://doi.org/10.48550/arXiv.\n2212.14052.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\nGeneral language model pretraining with autoregressive blank infilling, 2022.\nDaniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri\nRudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling.\nCoRR, abs/2302.06646, 2023. doi: 10.48550/arXiv.2302.06646. URL https://doi.org/10.\n48550/arXiv.2302.06646.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot\nlanguage model evaluation. Version v0. 0.1. Sept, 2021.\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama. URL: https://github.\ncom/openlm-research/open_llama, 2023.\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with\noptimal polynomial projections, 2020.\nAlbert\nGu,\nKaran\nGoel,\nAnkit\nGupta,\nand\nChristopher\nR\u00e9.\nOn\nthe\nparam-\neterization\nand\ninitialization\nof\ndiagonal\nstate\nspace\nmodels.\nIn\nNeurIPS,\n2022a.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\ne9a32fade47b906de908431991440f7c-Abstract-Conference.html.\nAlbert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured\nstate spaces. In The Tenth International Conference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net, 2022b. URL https://openreview.net/\nforum?id=uYLFoz1vlAC.\nAnkit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured\nstate spaces. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/\n2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.\nhtml.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\nTraining compute-optimal large language models, 2022.\nWeizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv\npreprint arXiv:2202.10447, 2022.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A\nmulti-level multi-discipline chinese evaluation suite for foundation models, 2023.\nDhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,\nSasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,\net al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.\n11\nTechnical Report\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,\n2020.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns:\nFast autoregressive transformers with linear attention. In International Conference on Machine\nLearning, pp. 5156\u20135165. PMLR, 2020.\nGuolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In\nInternational Conference on Learning Representations, 2021. URL https://openreview.\nnet/forum?id=09-528y2Fgf.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension, 2019.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.\nZexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neural\narchitecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955, 2022.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740, 2017.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering, 2018.\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu,\nand Soham De. Resurrecting recurrent neural networks for long sequences, 2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in Neural Information Processing Systems 32, pp.\n8024\u20138035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin\nCheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw\nKazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri,\nFerdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak,\nRuichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv:\nReinventing rnns for the transformer era, 2023a.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin\nCheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw\nKazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri,\nFerdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak,\nRuichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv:\nReinventing rnns for the transformer era, 2023b.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=R8sQPpGCv0.\n12\nTechnical Report\nZhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong.\nThe devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pp. 7025\u20137041, Abu Dhabi, United Arab Emirates, December\n2022a. Association for Computational Linguistics. URL https://aclanthology.org/\n2022.emnlp-main.473.\nZhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng\nKong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Confer-\nence on Learning Representations, 2022b. URL https://openreview.net/forum?id=\nBl8CQrx2Up4.\nZhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng\nKong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh\nInternational Conference on Learning Representations, 2023a. URL https://openreview.\nnet/forum?id=IxmWsm4xrua.\nZhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng\nKong, and Yiran Zhong. Linearized relative positional encoding. Transactions on Machine\nLearning Research, 2023b.\nZhen Qin, Yiran Zhong, and Hui Deng. Exploring transformer extrapolation, 2023c.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\nImproving language\nunderstanding by generative pre-training. https://s3-us-west-2.amazonaws.com/\nopenai-assets/research-covers/language-unsupervised/language_\nunderstanding_paper.pdf, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,\nAida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,\nJean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume,\nYujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher,\n2022.\nPrajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale, 2019.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\nreasoning about social interactions, 2019.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella\nBiderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh,\nSheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy.\nWhat language model to train if you have one million gpu hours?, 2022.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\n13\nTechnical Report\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science,\n2022.\nMosaicML NLP Team et al. Introducing mpt-7b: A new standard for open-source, commercially\nusable llms, 2023. URL www. mosaicml. com/blog/mpt-7b. Accessed, pp. 05\u201305, 2023.\nPhilippe Tillet, Hsiang-Tsung Kung, and David D. Cox. Triton: an intermediate language and compiler\nfor tiled neural network computations. Proceedings of the 3rd ACM SIGPLAN International\nWorkshop on Machine Learning and Programming Languages, 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nBen Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model,\n2021.\nBigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana\nIli\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\nGall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka\nAmmanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral,\nOlatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu\nNguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine\nJernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa,\nAlham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani,\nDragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan,\nFrancesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza\nBenyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la\nRosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep\nBhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber,\nLong Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim\nMasoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike\nTian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora\nKassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter\nHenderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani,\nRoberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik\nBose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor,\nStanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush,\nValentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u00b8sar,\n14\nTechnical Report\nElizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine\nChaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit\nPandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful\nBari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala\nNeeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked\nBrody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang,\nOfir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max\nRyabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas\nPatry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois\nLavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena,\nSuraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure\nLigozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla,\nEhud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey\nSchoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo\nKasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton\nCheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian\nGehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz\nLimisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zden\u02c7ek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour,\nAmmar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol,\nArezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade,\nBharat Saxena, Carlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis\nDavid, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima\nMirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman,\nIrina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra,\nMairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael\nMcKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour\nElkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh,\nSarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu\nLe, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan,\nAnima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito,\nChenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano,\nDian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak,\nGully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde,\nJose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,\nMadeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna\nNezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan\nDahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya\nChandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel\nCahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott,\nSinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant,\nTomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan\nXu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence?, 2019.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414, 2022.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOpt: Open pre-trained transformer language models, 2022.\n15\nTechnical Report\nYanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid\nShojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data\nparallel. arXiv preprint arXiv:2304.11277, 2023.\nLin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mecha-\nnism. In International Conference on Machine Learning, pp. 27011\u201327041. PMLR, 2022.\nLin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong. Efficient attention via control variates.\nIn International Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=G-uNfHKrj46.\n16\nTechnical Report\nAppendix\nA\nMODEL\nWe present distinct model variants of the TransNormerLLM architecture, delineating their respective\nconfigurations with regard to parameters, layers, attention heads, and hidden dimensions. The detailed\nspecifications are meticulously tabulated in Table 10.\nTable 10: TransNormerLLM Model Variants.\nModel Size\nNon-Embedding Params\nLayers\nHidden Dim\nHeads\nEquivalent Models\n385M\n384,974,848\n24\n1024\n8\nPythia-410M\n1B\n992,165,888\n16\n2048\n16\nPythia-1B\n3B\n2,876,006,400\n32\n2560\n20\nPythia-2.8B\n7B\n6,780,547,072\n30\n4096\n32\nLLAMA-6.7B\n13B\n12,620,195,840\n36\n5120\n40\nLLAMA-13B\n65B\n63,528,009,728\n72\n8192\n64\nLLAMA-65B\n175B\n173,356,498,944\n88\n12288\n96\nGPT-3\nB\nLIGHTNING ATTENTION\nWe present the algorithm details of Lightning Attention includes forward pass and backward pass in\nAlgorithm 3 and 4, respectively.\nAlgorithm 3 Lightning Attention Forward Pass\nInput: Q, K, V \u2208 Rn\u00d7d, attention mask M \u2208 Rn\u00d7n, block sizes Bc, Br;\nInitialize: O = 0 \u2208 Rn\u00d7d;\nDivide Q into Tr =\nn\nBr blocks Q1, Q2, ...QTr of size Br \u00d7 d each.\nDivide K, V into Tc =\nn\nBc blocks K1, K2, ...KTc, V1, V2, ...VTc of size Bc \u00d7 d each.\nDivide O into Tr =\nn\nBr blocks O1, O2, ...OTr of size Br \u00d7 d each.\nDivide M into Tr \u00d7 Tc blocks M11, M12, ...MTr,Tc of size Br \u00d7 Bc each.\nfor 1 \u2264 i \u2264 Tr do\nLoad Qi \u2208 RBr\u00d7d from HBM to on-chip SRAM.\nInitialize Oi = 0 \u2208 RBr\u00d7d on SRAM.\nfor 1 \u2264 j \u2264 Tc do\nLoad Kj, Vj \u2208 RBc\u00d7d from HBM to on-chip SRAM.\nLoad Mij \u2208 RBc\u00d7Bc from HBM to on-chip SRAM.\nOn chip, compute Aij = [QiK\u22a4\nj ] \u2299 Mij \u2208 RBr\u00d7Bc.\nOn chip, compute Oi = Oi + AijVj \u2208 RBr\u00d7d.\nend for\nWrite Oi to HBM as the i-th block of O.\nend for\nreturn O\nC\nPROVING ROBUST INFERENCE ALGORITHM\nWe will use induction to prove: [kv]t = \u03bb\u2212t[kv]t.\nBase Case (n = 1):\n[kv]1 = ([kv]0 + k1\u03bb\u22121v\u22a4\n1 )\n= \u03bb\u22121(k1v\u22a4\n1 )\n= \u03bb\u22121[kv]1.\n(18)\n17\nTechnical Report\nAlgorithm 4 Lightning Attention Backward Pass\nInput: Q, K, V, dO \u2208 Rn\u00d7d, attention mask M \u2208 Rn\u00d7n, on-chip SRAM of size M, block sizes Bc, Br;\nInitialize: dQ = dK = dV = 0 \u2208 Rn\u00d7d;\nDivide Q into Tr =\nn\nBr blocks Q1, Q2, ...QTr of size Br \u00d7 d each.\nDivide K, V into Tc =\nn\nBc blocks K1, K2, ...KTc, V1, V2, ...VTc of size Bc \u00d7 d each.\nDivide O, dO into Tr =\nn\nBr blocks O1, O2, ...OTr, dO1, dO2, ...dOTr of size Br \u00d7 d each\nDivide M into Tr \u00d7 Tc blocks M11, M12, ...MTr,Tc of size Br \u00d7 Bc each.\nfor 1 \u2264 j \u2264 Tc do\nLoad Kj, Vj \u2208 RBc\u00d7d from HBM to on-chip SRAM.\nInitialize dKj = dVj = 0 \u2208 RBc\u00d7d on SRAM.\nfor 1 \u2264 i \u2264 Tr do\nLoad Qi, Oi, dOi \u2208 RBr\u00d7d from HBM to on-chip SRAM.\nLoad Mij \u2208 RBc\u00d7Bc from HBM to on-chip SRAM.\nInitialize dKj = dVj = 0 \u2208 RBc\u00d7d on SRAM.\nOn chip, compute Aij = [QiK\u22a4\nj ] \u2299 Mij \u2208 RBr\u00d7Bc.\nOn chip, compute dVj = dVj + A\u22a4\nijdOi \u2208 RBc\u00d7d.\nOn chip, compute dAij = [dOiV\u22a4\nj ] \u2299 Mij \u2208 RBr\u00d7Bc.\nOn chip, compute dKj = dkj + dA\u22a4\nijVj \u2208 RBc\u00d7d.\nLoad dQi from HBM to SRAM, then on chip, compute dQi = dKi + dAijKj \u2208 RBr\u00d7d,\nwrite back to HBM.\nend for\nWrite dKj, dVj to HBM as the j-th block of dK, dV.\nend for\nretun dQ, dK, dV\nAssume the statement holds for n = m \u2212 1, i.e., [kv]m\u22121 = \u03bb\u2212(m\u22121)[kv]m\u22121. Then, when n = m:\n[kv]m = [kv]m\u22121 + km\u03bb\u2212mv\u22a4\nm\n= \u03bb\u2212(m\u22121)[kv]m\u22121 + km\u03bb\u2212mv\u22a4\nm\n= \u03bb\u2212m(\u03bb[kv]m\u22121 + kmv\u22a4\nm)\n= \u03bb\u2212m[kv]m,\n(19)\nthe statement holds. Therefore, by induction, the statement holds for all n \u2265 1.\nThus, both the Origin Inference Algorithm and the Robust Inference Algorithm yield the same results.\nD\nCORPUS\nWe gather an extensive corpus of publicly accessible text from the internet, totaling over 700TB in\nsize. The collected data are processed by our data preprocessing procedure as shown in Figure 5,\nleaving a 6TB cleaned corpus with roughly 2 trillion tokens. We categorize our data sources to\nprovide better transparency and understanding. The specifics of these categories are outlined in\nTable 11.\nD.1\nDATA PREPROCESSING\nOur data preprocessing procedure consists of three steps: 1). rule-based filtering, 2). deduplication,\nand 3). a self-cleaning scheme. Before being added to the training corpus, the cleaned corpus needs\nto be evaluated by humans.\nRule-based filtering\nThe rules we used to filter our collected data are listed as follows:\n\u2022 Removal of HTML Tags and URLs: The initial step in our process is the elimination of\nHTML tags and web URLs from the text. This is achieved through regular expression\ntechniques that identify these patterns and remove them, ensuring the language model\nfocuses on meaningful textual content.\n\u2022 Elimination of Useless or Abnormal Strings: Subsequently, the cleaned dataset undergoes a\nsecond layer of refinement where strings that do not provide value, such as aberrant strings\n18\nTechnical Report\nAcademic\nwritings\nBooks\nCode\nWeb\n\u2026\nSelf-Clean\nScheme\nModel-based \nFiltering\nEvaluation Model\nHuman\nEvaluation\nDeduplication\nRule-based\nFiltering\nTraining data\nx N\nFigure 5: Data Preprocess Procedure. The collected data undergoes a process of rule-based filtering\nand deduplication, followed by our self-clean data processing strategy: model-based filtering, human\nevaluation, and evaluation model. After several iterations of the above cycle, we obtain high-quality\ntraining data at around 2T tokens.\nor garbled text, are identified and excised. This process relies on predefined rules that\ncategorize certain string patterns as non-contributing elements.\n\u2022 Deduplication of Punctuation Marks: We address the problem of redundant punctuation\nmarks in the data. Multiple consecutive punctuation marks can distort the natural flow and\nstructure of sentences when training the model. We employ a rule-based system that trims\nthese duplications down to a single instance of each punctuation mark.\n\u2022 Handling Special Characters: Unusual or special characters that are not commonly part of\nthe language\u2019s text corpus are identified and either removed or replaced with a standardized\nrepresentation.\n\u2022 Number Standardization: Numerical figures may be presented in various formats across dif-\nferent texts. These numbers are standardized into a common format to maintain consistency.\n\u2022 Preservation of Markdown/LaTeX Formats: While removing non-textual elements, excep-\ntions are made for texts in Markdown and LaTeX formats. Given their structured nature and\nubiquitous use in academia and documentation, preserving these formats can enhance the\nmodel\u2019s ability to understand and generate similarly formatted text.\nDeduplication\nTo ensure the uniqueness of our data and avert the risk of overfitting, we employ an\nefficient de-duplication strategy at the document or line level using MinHash and Locality-Sensitive\nHashing (LSH) algorithms. This combination of MinHash and LSH ensures a balance between\ncomputational efficiency and accuracy in the deduplication process, providing a robust mechanism\nfor data deduplication and text watermark removal.\nSelf-cleaning scheme\nOur data self-cleaning process involves an iterative loop of the following\nthree steps to continuously refine and enhance the quality of our dataset. An issue of using model-\nbased data filters is that the filtered data will have a similar distribution as the evaluation model,\nwhich may have a significant impact on the diversity of the training data. Assuming that the majority\nof the pre-processed data is of high quality, we can train an evaluation model on the entire set of\npre-processed data, and the model will automatically smooth the data manifold distribution and outlet\nlow-quality data while retaining the majority of the diversities.\nThe self-cleaning scheme unfolds as follows:\n\u2022 Evaluation Model: We train a 385M model on the pre-processed corpus to act as a data\nquality filter.\n19\nTechnical Report\nTable 11: Statistics of our corpus. For each category, we list the number of epochs performed on the\nsubset when training on the 2 trillion tokens, as well as the number of tokens and disk sizes. We also\nlist the table on the right according to the language distribution.\nDataset\nEpochs\nTokens\nDisk size\nAcademic Writings\n1.53\n200 B\n672 GB\nBooks\n2.49\n198 B\n723 GB\nCode\n0.44\n689 B\n1.4 TB\nEncyclopedia\n1.51\n5 B\n18 GB\nFiltered Webpages\n1.00\n882 B\n3.1 TB\nOthers\n0.63\n52 B\n154 GB\nTotal\n-\n2026 B\n6 TB\nLanguage\nTokens\nDisk size\nEnglish\n743 B\n2.9 TB\nChiese\n555 B\n1.7 TB\nCode\n689 B\n1.4 TB\nOthers\n39 B\n89 GB\nTotal\n2026 B\n6 TB\n\u2022 Model-Based Data Filtering: We use the evaluation model to assess each piece of data with\nperplexity. Only data achieving a score above a certain threshold is preserved for the next\nstep. Low-quality data are weeded out at this stage.\n\u2022 Human Evaluation: We sample a small portion of the filtered data and manually evaluate\nthe quality.\nThese steps are repeated in cycles, with each iteration improving the overall quality of the data and\nensuring the resulting model is trained on relevant, high-quality text. This self-cleaning process\nprovides a robust mechanism for maintaining data integrity, thereby enhancing the performance of\nthe resulting language model.\nD.2\nTOKENIZATION\nWe tokenize the data with the Byte-Pair Encoding (BPE) algorithm. Notably, to enhance compatibility\nwith Chinese language content, a significant number of common and uncommon Chinese characters\nhave been incorporated into our vocabulary. In cases where vocabulary items are not present in the\ndictionary, the words are broken down into their constituent UTF-8 characters. This strategy ensures\ncomprehensive coverage and flexibility for diverse linguistic input during model training.\nE\nADDITIONAL EXPERIMENTAL RESULTS\nE.1\nMODEL PARALLELISM ON TRANSNORMERLLM\nWe conduct a series of experiments with a 7B TransNormerLLM model to investigate the performance\nof model parallelism on TransNormerLLM in terms of speed and memory. These tests are carried\nout on a single Nvidia DGX node that houses eight A100 80G GPUs linked by NVLink. In this\nexperiment, FSDP is enabled and Flash Attention (Dao et al., 2022a) is used on the Transformer.\nTable 12 shows the results for training speed and memory consumption.\nIt can be seen that model parallelism has a significant effect on memory conservation, as increasing\nthe number of partitions for the model results in lower memory consumption per GPU. Due to\nNVLink constraints, we kept the dimension of model parallelism within 8 in all of our experiments.\nThe TransNormerLLM-7B model requires only 24.1GB of memory on a single GPU when the model\nparallel size is set to 8, representing a significant memory reduction of 62.3% when compared to the\nmodel parallel size of 1. In comparison, the Transformer-7B model consumes 28.7GB of memory\nunder the same configuration. While model parallelism conserves memory, it is worth noting that\ntraining speed is only marginally reduced. TransNormerLLM consistently outperforms Transformer\nby a wide margin.\nE.2\nSTRESS TESTS ON MODEL SIZE AND CONTEXT LENGTH\nA series of stress tests are performed to assess the efficacy of the designed system optimization strategy.\nThe model is scaled up to 175B, which is the largest released version of the TransNormerLLM\nmodel. However, this augmentation poses significant training challenges. We use a wide range of\ndistributed training techniques to effectively train such a large model, with the goal of reducing GPU\nmemory consumption while increasing computational and communication efficiencies. To ensure the\n20\nTechnical Report\nTable 12: Model Parallelism Performance. We compare the model parallelism performance of\nTransformer-7B with Flash Attention and TransNormerLLM-7B with Lightning Attention on a single\nA100 node with NVLink. All experiments use a batch size of 2 and a context length of 2048.\nModel\nModel Parallel Size\nTokens/s\nAllocated Memory/GPU\nMemory Saved\nTransformer-7B\n1\n26896.1\n66.3 GB\n-\n2\n24973.7\n44.6 GB\n32.7%\n4\n22375.8\n40.2 GB\n39.4%\n8\n19973.6\n28.7 GB\n56.7%\nTransNormerLLM-7B\n1\n32048.6\n64.0 GB\n-\n2\n29750.4\n41.0 GB\n35.9%\n4\n27885.2\n36.3 GB\n43.3%\n8\n24280.0\n24.1 GB\n62.3%\nfeasibility of training these massive TransNormerLLM models, Lightning Attention, FSDP, Model\nParallelism, AMP, and Activation Checkpointing are used. For the Transformer models, we use Flash\nAttention (Dao et al., 2022a) in all experiments.\nModel Size\nWe perform training experiments on variously sized Transformer and TransNormer-\nLLM models using a large-scale A100 80G GPU cluster, as shown in Table 13. To achieve the\nmaximum speed for various model sizes, we keep the context length constant at 2048 and increased\nthe batch size until we reached the GPU memory limit. TransNormerLLMs consistently outper-\nform their Transformer counterparts in terms of computation speed. This observation validates the\nTransNormerLLM model\u2019s advantageous linear computational complexity, reinforcing its efficacy.\nTable 13: Efficiency of training models with different sizes. For comparative purposes, we keep the\ncontext length fixed at 2048 and increased the batch size for both transformer and TransNormerLLM\nto achieve their maximum speeds without encountering out-of-memory issues.\nModel\nModel Size\nTokens/sec/GPU\nAllocated Memory/GPU\nTransformer\n7B\n3362.7\n72.5 GB\n13B\n1735.6\n70.6 GB\n65B\n318.2\n73.2 GB\n175B\n106.2\n69.5 GB\nTransNormerLLM\n7B\n4081.0\n71.9 GB\n13B\n2104.3\n73.8 GB\n65B\n406.9\n69.4 GB\n175B\n136.6\n70.3 GB\nContext Length\nOne of the strengths of TransNormerLLM lies in its utilization of linear attention\ncomputation, which exhibits computational and storage complexities linearly correlated with the\nsequence length. To validate this outstanding characteristic of TransNormerLLM, we conduct training\nexperiments on Transformer and TransNormerLLM models with varying parameter sizes. While\nmaintaining a batch size of 1, we aim to maximize the context length. All experiments run on a small\ncluster with 64 A100 GPUs. The results, as presented in Table 14, demonstrate the remarkable long\ncontext length training capability of TransNormerLLM. Under comparable computational resources,\nthe TransNormerLLM model exhibits the ability to train with longer context lengths compared to\nconventional Transformer models and achieve higher computational speeds in the process.\n21\nTechnical Report\nTable 14: Maximum context length for training Transformer and TransNormerLLM. We com-\npare the maximum context lengths with different model sizes between Transformer and TransNormer-\nLLM on 64 A100 80G GPUs. All experiments use a batch size of 1.\nModel\nModel Size\nContext Length\nRelative Speed\nAllocated Memory/GPU\nTransformer\n7B\n37K\n1\n71.1 GB\n13B\n24K\n1\n68.0 GB\n65B\n19K\n1\n73.3 GB\n175B\n10K\n1\n66.9 GB\nTransNormerLLM\n7B\n48K\n1.21\n65.8 GB\n13B\n35K\n1.23\n61.0 GB\n65B\n23K\n1.29\n68.2 GB\n175B\n12K\n1.35\n63.5 GB\n22\n"
  },
  {
    "title": "To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation",
    "link": "https://arxiv.org/pdf/2307.15063.pdf",
    "upvote": "15",
    "text": "This paper has been accepted for publication at the\nIEEE/CVF International Conference on Computer Vision (ICCV), Paris, 2023. \u00a9IEEE\nTo Adapt or Not to Adapt?\nReal-Time Adaptation for Semantic Segmentation\nMarc Botet Colomer\u2217 1,2\nPier Luigi Dovesi\u2217 3 \u2020\nTheodoros Panagiotakopoulos4\nJoao Frederico Carvalho1\nLinus H\u00a8arenstam-Nielsen5,6\nHossein Azizpour2\nHedvig Kjellstr\u00a8om2,3\nDaniel Cremers5,6,7\nMatteo Poggi8\n1Univrses\n2KTH\n3Silo AI\n4King\n5Technical University of Munich\n6Munich Center for Machine Learning\n7University of Oxford\n8University of Bologna\nhttps://marcbotet.github.io/hamlet-web/\n0.6 FPS\nRGB heavy rain (200mm)\nCoTTA\nOnDA\n1.3 FPS\nCoTTA real-time\n27 FPS\nHAMLET\n29 FPS\n48 FPS\nNo Adaptation\nFigure 1. Real-time adaptation with HAMLET. Online adaptation to continuous and unforeseeable domain shifts is hard and compu-\ntationally expensive. HAMLET can deal with it at almost 30FPS outperforming much slower online methods \u2013 e.g. OnDA and CoTTA.\nAbstract\nThe goal of Online Domain Adaptation for semantic seg-\nmentation is to handle unforeseeable domain changes that\noccur during deployment, like sudden weather events. How-\never, the high computational costs associated with brute-\nforce adaptation make this paradigm unfeasible for real-\nworld applications. In this paper we propose HAMLET, a\nHardware-Aware Modular Least Expensive Training frame-\nwork for real-time domain adaptation. Our approach in-\ncludes a hardware-aware back-propagation orchestration\nagent (HAMT) and a dedicated domain-shift detector that\nenables active control over when and how the model is\nadapted (LT). Thanks to these advancements, our approach\nis capable of performing semantic segmentation while si-\nmultaneously adapting at more than 29FPS on a single\nconsumer-grade GPU. Our framework\u2019s encouraging ac-\ncuracy and speed trade-off is demonstrated on OnDA and\nSHIFT benchmarks through experimental results.\n1. Introduction\nSemantic segmentation aims at classifying an image\nat a pixel level, based on the local and global context,\nto enable a higher level of understanding of the depicted\nclear (F)\n25 (F)\n50 (F)\n75 (F)\n100 (F)\n200 (F)\n100 (B)\n75 (B)\n50 (B)\n25 (B)\nclear (B)\n30\n40\n50\n60\n70\n80\nCoTTA\nTent\nMiT-B1 (no adapt)\nOurs\nLoading [MathJax]/extensions/MathMenu.js\nFigure 2: Online adaptation methods on the Increasing\nStorm. We plot mIoUs achieved on single domains. Colors\nfrom colder to warmer encode slower to faster methods.\nscene. In recent years, deep learning has become the dom-\ninant paradigm to tackle this task effectively employing\nCNNs [5, 69, 4] or, more recently, transformers [65], at the\nexpense of requiring large quantities of annotated images\nfor training. Specifically, annotating for this task needs per-\npixel labeling, which is an expensive and time-consuming\ntask, severely limiting the availability of training data.\nThe use of simulations and graphics engines [42] to\ngenerate annotated frames enabled a marked decrease in\n\u2217 Joint first authorship\n\u2020 Part of the work done while at Univrses\n1\narXiv:2307.15063v2  [cs.CV]  7 Aug 2023\nthe time and cost necessary to gather labeled data thanks\nto the availability of the ground truth. However, despite\nthe increasing quality in data realism [47], there is a sub-\nstantial difference between simulated data generated by\ngraphics engines and real-world images, such that leverag-\ning these data for real-world applications requires adapt-\ning over a significant domain shift. The promise of un-\nlocking this cheap and plentiful source of training data\nhas provided a major impulse behind the development of\na large body of work on Unsupervised Domain Adaptation\n(UDA) techniques [74, 61, 18, 15, 55], consisting of train-\ning semantic segmentation networks on labelled synthetic\nframes \u2013 the source domain \u2013 and then adapting the net-\nwork to operate on real images, representing the target do-\nmain, without requiring human annotation. However, the\nsynthetic-to-real shift represents only one of many possi-\nble domain transitions; specifically, when dealing with real-\nworld deployment, domain shifts can occur from various\ncauses, from different camera placements to different light-\ning, weather conditions, urban scenario, or any possible\ncombination of the above. Because of the combinatorial\nnature of the problem, it is simply impossible to evenly rep-\nresent all possible deployment domains in a dataset. This\ncurse of dimensionality prevents having generalized robust\nperfomances [41, 45]. However, the recent advent of on-\nline domain adaptation [41] potentially allows us to face\ncontinuous and unpredictable domain shifts at deployment\ntime, without requiring data associated with such domain\nshifts beforehand. Nonetheless, despite its potential, sev-\neral severe limitations still hamper the online adaptation\nparadigm.\nIn particular, continuously performing back-\npropagation on a frame-by-frame schedule [41] incurs a\nhigh computational cost, which negatively affects the per-\nformance of the network, dropping its overall framerate to\naccommodate the need for continuous adaptation.\nVari-\nous factors are involved in this matter: first, the severity\nof this overhead is proportional to the complexity of the\nnetwork itself \u2013 the larger the number of parameters, the\nheavier the adaptation process becomes; second, we argue\nthat frame-by-frame optimization is an excessive process\nfor the adaptation itself \u2013 not only the network might need\nmuch fewer optimization steps to effectively counter do-\nmain shifts, but also such an intense adaptation definitely\nincreases the likelihood of catastrophic forgetting over pre-\nvious domains [26, 45]. In summary, a practical solution\nfor online domain adaptation in semantic segmentation that\ncan effectively operate in real-world environments and ap-\nplications still seems to be a distant goal.\nIn this paper, we propose a novel framework aimed at\novercoming these issues and thus allowing for real-time, on-\nline domain adaptation:\n\u2022 We address the problem of online training by de-\nsigning an automatic lightweight mechanism capable\nof significantly reducing back-propagation complex-\nity.\nWe exploit the model modularity to automati-\ncally choose to train the network subset which yields\nthe highest improvement for the allocated optimisation\ntime. This approach reduces back-propagation FLOPS\nby 34% while minimizing the impact on accuracy.\n\u2022 In an orthogonal fashion to the previous contribution,\nwe introduce a lightweight domain detector. This al-\nlows us to design principled strategies to activate train-\ning only when it really matters as well as setting hy-\nperparameters to maximize adaptation speed. Overall,\nthese strategies increase our speed by over 5\u00d7 while\nsacrificing less than 2.6% in mIoU.\n\u2022 We evaluate our method on multiple online domain\nadaptation benchmarks both fully synthetic [45] and\nsemi-synthetic CityScapes domain sequences [41],\nshowing superior accuracy and speed compared to\nother test-time adaptation strategies.\nFig.\n1 demonstrates the superior real-time adaptation\nperformance of HAMLET compared to slower methods\nsuch as CoTTA [57], which experience significant drops in\nperformance when forced to maintain a similar framerate by\nadapting only once every 50 frames. In contrast, HAMLET\nachieves an impressive 29 FPS while maintaining high ac-\ncuracy. Additionally, Fig. 2 offers a glimpse of HAMLET\u2019s\nperformance on the Increasing Storm benchmark [41], fur-\nther highlighting its favorable accuracy-speed trade-off.\n2. Related Work\nWe review the literature relevant to our work, about se-\nmantic segmentation and UDA, with particular attention to\ncontinuous and online methodologies.\nSemantic Segmentation. Very much like classification,\ndeep learning plays a fundamental role in semantic segmen-\ntation. Fully Convolutional Network (FCN) [36] represents\nthe pivotal step in this field, adapting common networks\nby means of learned upsample operators (deconvolutions).\nSeveral works aimed at improving FCN both in terms of\nspeed [68, 38] and accuracy [5, 6, 7], with a large body of\nliterature focusing on the latter. Major improvements have\nbeen achieved by enlarging the receptive field [72, 66, 5, 6,\n7], introducing refinement modules [14, 73, 17], exploiting\nboundary cues [3, 10, 46] or using attention mechanisms\nin different flavors [13, 31, 58, 64]. The recent spread of\nTransformers in computer vision [11] reached semantic seg-\nmentation as well [64, 69, 65], with SegFormer [65] repre-\nsenting the state-of-the-art in the field and being the object\nof studies in the domain adaptation literature as well [20].\nUnsupervised Domain Adaptation (UDA). This body\nof research aims at adapting a network trained on a source,\nlabeled domain to a target, unlabeled one. Early approaches\n2\nrely on the notion of \u201cstyle\u201d and learn how to transfer it\nacross domains [74, 61, 18, 32, 12, 67]. Common strategies\nconsist of learning domain-invariant features [15, 25], often\nusing adversarial learning in the process [15, 55, 8, 19, 51].\nA popular trend in UDA is Self-Training. These methods\nrely on self-supervision to learn from unlabelled data. In\nUDA, a successful strategy consists of leveraging target-\ncurated pseudo-labels. Popular approaches for this purpose\nmake use of confidence [77, 37, 76], try to balance the class\npredictions [75, 20], or use prototypes [2, 71, 70] to im-\nprove the quality of the pseudo-labels. Among many do-\nmain shifts, the synthetic-to-real one is the most studied,\nsince the earliest works [74, 61, 18] to the latest [60, 30, 21,\n28, 16, 40, 24]. However, this shift is one of a kind since it\noccurs only once after training, and without the requirement\nof avoiding forgetting the source domain.\nContinuous/Test-Time\nUDA.\nThis\nfamily\nof\nap-\nproaches marries UDA with continuous learning, thus deal-\ning with the catastrophic forgetting issue ignored in the\nsynthetic-to-real case. Most continuous UDA approaches\ndeal with it by introducing a Replay Buffer [1, 29, 27], while\nadditional strategies make use of style transfer [62], con-\ntrastive [44, 53] or adversarial learning [63]. Despite the\ndefinition, continuous UDA often deals with offline adapta-\ntion, with well-defined target domains over which to adapt.\nConceptually similar to it, is the branch of test-time adap-\ntation, or source-free UDA, although tackling the problem\nin deployment rather than offline \u2013 i.e. with no access to the\ndata from the source domain [43]. Popular strategies to deal\nwith it consist of generating pseudo-source data to avoid\nforgetting [35], freezing the final layers in the model [33],\naligning features [34], batch norm retraining through en-\ntropy minimization [54] or prototypes adaptation [22].\nOnline UDA. Although similar in principle to test-time\nadaptation, online UDA [45, 41, 52] aims to tackle multi-\nple domain shifts, occurring unpredictably during deploy-\nment in real applications and without clear boundaries be-\ntween them. On this track, the SHIFT dataset [45] pro-\nvides a synthetic benchmark specifically thought for this\nscenario, while OASIS [52] proposes a novel protocol to\nevaluate UDA approaches, considering an online setting\nand constraining the evaluated methods to deal with frame-\nby-frame sequences. As for methods, OnDA [41] imple-\nments self-training as the orchestration of a static and a dy-\nnamic teacher to achieve effective online adaptation while\navoiding forgetting, yet introducing massive overhead.\nReal-time performance is an essential aspect of online\nadaptation, particularly in applications such as autonomous\ndriving where slow models are impractical. A slow adap-\ntation process not only limits the practicality of real-world\napplications but also fails to provide high accuracy until the\nadaptation is complete, thereby defeating the original pur-\npose. Therefore, accelerating the adaptation process is cru-\ncial for achieving high accuracy in real-time scenarios.\n3. Methods\nThis section introduces HAMLET, a framework for\nHardware-Aware Modular Least Expensive Training. The\nframework aims to solve the problem of online domain\nadaptation with real-time performance through several syn-\nergistic strategies. First, we introduce a Hardware-Aware\nModular Training (HAMT) agent able to optimize online\na trade-off between model accuracy and adaptation time.\nHAMT allows us to significantly reduce online training\ntime and GFLOPS. Nevertheless, the cheapest training con-\nsists of no training at all. Therefore, as the second strategy,\nwe introduce a formal geometric model for online domain\nshifts that enable reliable domain shift detection and domain\nestimator signals (Adaptive Domain Detection, Sec. 3.3.1).\nThese can be easily integrated to activate the adaptation pro-\ncess only at specific times, as least as possible. Moreover,\nwe can further leverage these signals by designing adaptive\ntraining policies that dynamically adapt domain-sensitive\nhyperparameters. We refer to these as Active Training Mod-\nulations. We present an overview of HAMLET in Fig. 3.\n3.1. Model Setup\nOur approach builds on the recent progress in unsuper-\nvised domain adaptation and segmentation networks. We\nstart with DAFormer [20], a state-of-the-art UDA method,\nand adopt SegFormer [65] as our segmentation backbone\ndue to its strong generalization capacity.\nWe use three\ninstances of the backbone, all pre-trained on the source\ndomain: a student, a teacher, and a static (i.e. frozen)\nteacher. During training, the student receives a mix of target\nand source images [49] and is supervised with a \u201cmixed-\nsample\u201d cross-entropy loss, LT (represented by green, blue\nand red dashed lines, in Fig. 3). This loss is computed by\nmixing the teacher\u2019s pseudo-labels and source annotations.\nTo improve training stability, the teacher is updated as the\nexponential moving average (EMA) of the student. To fur-\nther regularize the student, we use source samples stored in\na replay buffer and apply two additional losses (blue lines in\nFig. 3). First, we minimize the feature distance (Euclidean)\nbetween the student and the static teacher\u2019s encoder, LF D.\nThen, we employ a supervised cross-entropy task loss LS.\nOur complete objective is L = LS + LT + \u03bbF DLF D, with\n\u03bbF D being a weight factor. During inference on the target\ndomain, only the student is used (red lines in Fig. 3).\n3.2. Hardware-Aware Modular Training (HAMT)\nOnline adaptation requires updating the parameters dur-\ning deployment time. However, back-propagation is com-\nputationally expensive and hence too slow to be continu-\nously applied on a deployed agent.\nOpting for a partial\nweight update, for example by finetuning the last module\n3\n    |\u0394Bi| Bi\nBi+1\nno\n|\u0394Bi|\nHAMLET\nHAMT applies an expected-improvement decision policy to optimize a trade-off between\nminimizing training FLOPS and improving adaptation performance.\nActive Training Modulation leverages a specialized domain-shift detector to orchestrate\ntraining phases and identify the best hyperparameter configurations\npseudo-loss\ndomain det.\ndecoder\nstatic teacher\nteacher\nstudent\nT1\nT2\nT3\nT4\nspeed\naccuracy\nfeat. dist.\nloss\ntask loss\nH\nmixed image\ntarget image\nsource image\nsource label\nprediction\nHardware-Aware Modular Training\nEMA\nActive Training Modulation\nALR\n>z\nno adaptation\nyes\nBi,Bi+1 \n\u03b7-init\n\u03b7-decay\ntraining iter.\nclassmix %\nDCM\ndomain shift detection\ntraining phase on mixed image\ntraining phase on source image\ntraining phase on target image\ninference phase on target image\nno update\nFigure 3: HAMLET framework. We employ a student-teacher model with an EMA and a static teacher. HAMT orchestrates\nthe back-propagation over the student restricting it to a network subsection. The Active Training Modulation instead controls\nthe adaptation process by selectively enabling it only when necessary as well as tweaking sensitive training parameters.\nof the network, would enable much more efficient train-\ning time. However, domain shifts can manifest as changes\nin both the data input distribution (such as attributes of\nthe images, e.g. day/night) and the output distribution (e.g.\nclass priors). This information could be encoded in differ-\nent parts of the network, therefore just updating the very\nlast segment might not suffice.\nThis motivates the need\nfor orchestrating the training process, to ensure sufficient\ntraining while minimizing the computational overhead. In-\nspired by reward-punishment [48] and reinforcement learn-\ning [56] policies, we introduce an orchestration agent in\ncharge of deciding how deeply the network shall be fine-\ntuned through a trade-off between the pseudo-loss mini-\nmization rate and the computational time. In contrast to pre-\nvious efficient back-propagation approaches [59, 23, 9], our\nmodel is pre-trained on the task and thus requires smaller\nupdates to adapt. Let us start by modeling the problem.\nOur model backbone, f, is composed of four different mod-\nules: f = m4 \u25e6 m3 \u25e6 m2 \u25e6 m1.\nThis defines our ac-\ntion space A = {T1, T2, T3, T4} where T4 corresponds\nto training just the last module of the network, m4, while\nT3 the last two modules, i.e. m4 \u25e6 m3, T2 the last three,\ni.e. m4 \u25e6 m3 \u25e6 m2, and T1 the whole network f. We also\ndefine a continuous state space S = {R, V} where R is\nthe second derivative of the EMA teacher pseudo-loss, lt,\nover time, hence Rt = \u2212 \u22062l\n(\u2206t)2 , computed in discrete form\nas Rt = \u2212(lt \u2212 2lt\u22121 + lt\u22122). V represents a cumula-\ntive vector with the same dimension as the action space A,\ninitialized at zero. Now we have everything in place to em-\nploy an expected-improvement based decision model. At\neach time-step t, action Tj is selected for j = argmax Vt.\nDuring training step t, V[j] is updated as:\nV[j]t+1 = \u03b1Rt + (1 \u2212 \u03b1)V[j]t\n(1)\nwhere \u03b1 is a smoothing factor, e.g. 0.1. i.e. Vt hold a dis-\ncrete exponential moving average of Rt. Therefore, our pol-\nicy can be seen as a greedy module selection based on the\nhighest expected loss improvement over its linear approxi-\nmation. A notable drawback of this policy is that we will\ninevitably converge towards picking more rewarding, yet\nexpensive, actions i.e. T1, T2 compared to more efficient\nbut potentially less effective actions i.e. T3, T4. However,\nour goal is not to maximize \u2212 \u22062l\n(\u2206t)2 where \u2206t is the number\nof updates, our goal is instead to maximize \u2212 \u22062l\n(\u2206\u03c4)2 where\n\u2206\u03c4 is a real-time interval. Therefore, we have to introduce\nin the optimization policy some notion of the actual training\ncost of each action in A on the target device. To start with,\nwe measure the training time associated with each action,\nobtaining \u03c9T = {\u03c9T1, \u03c9T2, \u03c9T3, \u03c9T4}. With this we can\ncompute the time-conditioning vector \u03b3 as\n\u03b3j =\ne\n1\n\u03b2\u03c9Tj\nPK\nk=1 e\n1\n\u03b2\u03c9Tk\nfor j = 1, . . . , K\n(2)\nwhere \u03b2 is the softmax temperature, and K the number of\nactions, i.e. 4 in our model. We modify our update policy\nto favor less computationally expensive modules by scaling\nthe updates with \u03b3, replacing Eq. 1 with:\nV[j]t+1 =\n(\n\u03b3j\u03b1Rt + (1 \u2212 \u03b1)V[j]t\nif Rt \u2265 0\n(1 \u2212 \u03b3j)\u03b1Rt + (1 \u2212 \u03b1)V[j]t\nif Rt < 0\n(3)\nThis policy makes it so that more expensive actions re-\nceive smaller rewards and larger punishments. Despite its\n4\nsimplicity, this leads to a significant reduction in FLOPS for\nan average back-propagation \u03b2, i.e. \u221230% with \u03b2 = 2.75\nor \u221243% with \u03b2 = 1. We finally choose \u03b2 = 1.75 to ob-\ntain a FLOPS reduction of \u221234%. Exhaustive ablations on\nHAMT are presented in the supplementary material.\n3.3. Active Training Modulation\nContinuous and test-time adaptation methods tackle on-\nline learning as a continuous and constant process carried\nout on the data stream. Nevertheless, this approach presents\nseveral shortcomings when it comes to real-world deploy-\nments.\nPerforming adaptation when the deployment do-\nmain is unchanged does not lead to further performance im-\nprovements on the current domain; instead, it might cause\nsignificant forgetting on previous domains, hence hinder-\ning model generalization (we present evidence of this in\nthe supplementary material). Even if mitigated by HAMT,\nonline training remains a computationally expensive proce-\ndure, also due to several teachers\u2019 necessary forward passes.\nHowever, knowing when and what kind of adaptation is\nneeded is not a trivial task. We tackle this by introducing\nan Adaptive Domain Detection mechanism, in Sec. 3.3.1,\nand then a set of strategies to reduce the training time while\noptimizing the learning rate accordingly, in Sec. 3.3.2.\n3.3.1\nAdaptive Domain Detection\nA key element of an online adaptation system consists of\nacquiring awareness of the trajectory in the data distribu-\ntion space, i.e. domains, traveled by the student model dur-\ning deployment. We can model the problem by setting the\ntrajectory origin in the source domain. With high dimen-\nsional data, the data distribution is not tractable, therefore\nthe trajectory cannot be described in closed form. Recent\nwork [41] introduced the notion of distance between the\ncurrent deployed domain and source by approximating it\nwith the confidence drop of a source pre-trained model.\nThis approach heavily relies on the assumption that the pre-\ntrained model is well-calibrated. While this might hold for\ndomains close to source, the calibration quickly degrades\nin farther domains [45, 41]. This myopic behavior dampen\nthe simple use of confidence for domain detection. Further-\nmore, the additional forward pass increases the computa-\ntional cost during deployment. We tackle these limitations\nwith an equivalently simple, yet more robust, approach.\nWe modify the backbone of the static teacher f st used for\nthe feature distance loss LF D by connecting a lightweight\nsegmentation head, dst\n1, after the first encoder module mst\n1:\nhst\n1 = dst\n1 \u25e6 mst\n1. This additional decoder, hst\n1, is trained of-\nfline, on source data, without propagating gradients in the\nbackbone (mst\n1 is frozen). Given a target sample xT , we\npropose to compute the cross-entropy between the one-hot\nencoded student prediction p(xT ) = 1argmax(f(xT )) and the\nlightweight decoder prediction g(xT ) = hst\n1(xT ) as\nH(i)\nT\n= \u2212\nH\u00d7W\nX\np=1\nC\nX\nc=1\np\n\u0010\nx(i)\nT\n\u0011\nlog g\n\u0010\nx(i)\nT\n\u0011\f\f\f\np,c\n(4)\nThanks to the student model\u2019s higher generalization ca-\npability (both due to a larger number of parameters and the\nunsupervised adaptation process), it will always outperform\nthe lightweight decoder head. Nevertheless, since now the\ndistance is measured in the prediction space, we are not sub-\njected to model miscalibration. Furthermore, since the stu-\ndent model is in constant adaptation, the domain distance\naccuracy actually improves over time, leading to better re-\nsults. We present evidence of these claims in the supple-\nmentary material. We now define a denoised signal by us-\ning bin-averaging A(i)\nT\n= Pm(i+1)\u22121\nj=mi\nH(j)\nT\nm\nwhere m is the\nbin size. Domains are modeled as discrete steps of A(i)\nT\nB0 = A0\nBi =\n(\nAi\nif |Bi\u22121 \u2212 Ai| > z\nBi\u22121\notherwise\n(5)\nwhere B is the discretized signal and z is the minimum\ndistance used to identify new domains. Finally, we refer to\nthe signed amplitude of domain shifts as \u2206Bi = Bi\u2212Bi\u22121,\nand a domain change is detected whenever |\u2206Bi| > z.\n3.3.2\nLeast Training and Adaptive Learning Rate\nThe definitions of B allow us to customize the training pro-\ncess. To this end, we adopt a Least Training (LT) strat-\negy and trigger adaptation only when facing a new domain,\nwhich occurs when |\u2206Bi| > z. Effective online learning\nperformance depends heavily on the choice of hyperparam-\neters such as the learning rate \u03b7 and learning rate decay rate.\nTherefore, we can adjust these parameters to facilitate adap-\ntation according to the nature and intensity of domain shifts\nwe encounter, we refer to this orchestration as Adaptive\nLearning Rate (ALR). For example, the larger the domain\nshift (i.e. |\u2206Bi|), the more we need to adapt to counteract\nits effect. This can be achieved by either running more op-\ntimization steps or using a higher learning rate. Whenever\na domain shift is detected, we compute the number of adap-\ntation iterations L = Kl\n|\u2206Bi|\nz\n, hence proportionally to the\namplitude of the shift |\u2206Bi| relative to the threshold z. Kl\nis a multiplicative factor representing the minimum adapta-\ntion iterations. If a new domain shift takes place before the\nadaptation process completes, we accumulate the required\noptimization steps. Then, we can play on two further pa-\nrameters: Kl and the learning rate schedule. We argue that\nproper scheduling is crucial for attaining a smoother adap-\ntation. The learning rate, \u03b7, is linearly decayed until the\nadaptation is concluded \u2013 the smaller the domain shift, the\n5\n200mm\nAll-domains\nAverage GFLOPS\nAdaptation GFLOPS\nHAMT\nLT\nALR\nDCM\nRCS\n(mIoU)\n(mIoU)\nFPS\nTotal\nFwd.\nBwd.\nFwd.\nBwd.\n(A)\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n62.2 \u00b1 0.9\n69.5 \u00b1 0.3\n5.9 \u00b1 0.0\n125.2 \u00b1 0.0\n94.4 \u00b1 0.0 30.8 \u00b1 0.0\n56.6 \u00b1 0.0\n30.8 \u00b1 0.0\n(B)\n\u2713\n\u2013\n\u2013\n\u2013\n\u2013\n60.2 \u00b1 0.5\n68.7 \u00b1 0.3\n7.0 \u00b1 0.1\n114.7 \u00b1 0.0\n94.4 \u00b1 0.0 20.3 \u00b1 0.0\n56.6 \u00b1 0.0\n20.3 \u00b1 0.0\n(C)\n\u2713\n\u2713\n\u2013\n\u2013\n\u2013\n51.8 \u00b1 0.5\n65.7 \u00b1 0.2\n29.5 \u00b1 0.6\n44.4 \u00b1 0.5\n42.6 \u00b1 0.4\n1.8 \u00b1 0.2\n56.6 \u00b1 0.0\n20.2 \u00b1 0.2\n(D)\n\u2713\n\u2713\n\u2713\n\u2013\n\u2013\n54.1 \u00b1 1.2\n65.9 \u00b1 0.2\n29.5 \u00b1 0.5\n44.4 \u00b1 0.3\n42.7 \u00b1 0.2\n1.8 \u00b1 0.1\n56.6 \u00b1 0.0\n20.3 \u00b1 0.1\n(E)\n\u2713\n\u2713\n\u2713\n\u2713\n\u2013\n56.6 \u00b1 0.8\n66.3 \u00b1 0.1\n28.9 \u00b1 0.3\n44.7 \u00b1 0.2\n42.9 \u00b1 0.2\n1.8 \u00b1 0.1\n56.6 \u00b1 0.0\n20.2 \u00b1 0.0\n(F)\n\u2713\n\u2713\n\u2713\n\u2013\n\u2713\n55.8 \u00b1 1.0\n66.3 \u00b1 0.2\n29.1 \u00b1 1.1\n45.2 \u00b1 0.1\n43.2 \u00b1 0.1\n2.0 \u00b1 0.0\n56.6 \u00b1 0.0\n20.3 \u00b1 0.0\n(G)\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n58.2 \u00b1 0.8\n66.9 \u00b1 0.3\n29.7 \u00b1 0.6\n45.7 \u00b1 0.3\n43.6 \u00b1 0.2\n2.1 \u00b1 0.1\n56.6 \u00b1 0.0\n20.2 \u00b1 0.1\n(a)\nclear 1\n200mm\nclear 2\n100mm\nclear 3\n75mm\nclear 4\nclear h-mean\ntarget h-mean\ntotal h-mean\nFPS\nGFLOPS\n(A)\n72.9\n52.2\n73.6\n64.2\n73.0\n67.6\n73.4\n73.2\n60.6\n67.2\n5.6\n125.2\n(B)\n73.0\n50.4\n73.4\n62.1\n73.0\n67.3\n73.2\n73.1\n59.1\n66.4\n6.8\n114.7\n(C)\n73.4\n46.0\n73.5\n61.5\n73.6\n66.1\n73.8\n73.6\n56.5\n65.1\n7.2\n100.0\n(G)\n73.4\n53.6\n73.1\n65.2\n73.5\n68.2\n73.2\n73.3\n61.6\n67.8\n9.1\n82.2\n(b)\nTable 1: Ablation studies \u2013 HAMLET components. Top: Increasing Storm (8925 frames per domain) [41], bottom: Fast\nStorm C [41] (2975 frames per domain). For each configuration, we report mIoU, framerate, and GFLOPS.\nfaster the decay. While the initial learning rate, K\u03b7, should\nbe higher when the domain shift is triggered in domains far-\nther from the source\nK\u03b7 = K\u03b7,min + (Bi \u2212 Bsource)(K\u03b7,max \u2212 K\u03b7,min)\nBhard \u2212 Bsource\n(6)\nwhere Bsource (resp. Bhard) is an estimate of B when the\nnetwork is close to (resp. far from) the source domain; and\nK\u03b7,min (resp. K\u03b7,max) is the value of K\u03b7 assigned when the\nnetwork is close to (resp. far away from) the source. Con-\ncerning Kl, we posit that moving towards the source re-\nquires less adaptation than going towards harder domains:\nthe model shows good recalling of previously explored do-\nmains and thanks to the employed regularization strategies\nKl =\n(\nKl,max\nif \u2206Bi \u2265 0\nKl,min +\n(Bi\u2212Bsource)(Kl,max\u2212Kl,min)\nBhard\u2212Bsource\notherwise\n(7)\nwhere Kl,min (resp. Kl,max) is the value of Kl assigned\nwhen the model is close to (resp. far away from) the source\ndomain. Extensive ablations in the supplementary material\nwill highlight how the orchestration of the adaptation hyper-\nparameters improves the accuracy-speed trade-off.\n3.3.3\nDynamic ClassMix (DCM)\nClassMix [39] provides a simple mechanism for data aug-\nmentation by mixing classes from the source dataset into\ntarget images. Usually 50% of the classes in the source\ndataset are selected, however we notice that this percent-\nage is a highly sensitive hyperparameter in online domain\nadaptation. Injecting a significant portion of source classes\nhas a beneficial impact when adapting to domains closer to\nthe source domain, whereas when adapting to domains fur-\nther from the source the opposite effect can be observed, as\nit effectively slows down the adaptation process. We there-\nfore exploit once more the deployment domain awareness\nto control the mixing augmentation:\nKCM = KCM,min + (Bi \u2212 Bsource)(KCM,max \u2212 KCM,min)\nBhard \u2212 Bsource\n.\n(8)\nwhere KCM is the percentage of source classes used during\nadaptation; and KCM, min (resp. KCM, max) is the value of\nKCM assigned when the network is close to (resp. far away\nfrom) the source domain.\n3.3.4\nBuffer Sampling\nFollowing [41], to simulate real deployment, we limit our\naccess to the source domain by using a replay buffer. Ad-\nditionally, instead of initializing at random (with a uniform\nprior), we apply Rare Class Sampling (RCS) (skewed pri-\nors) as in [20]. This incentives a more balanced class distri-\nbution over the buffer, ultimately leading to better accuracy.\n4. Experimental Results\nThe experiments are carried out on (a) the OnDA bench-\nmarks [41] and (b) the SHIFT dataset [45].\n(a) is a\nsemi-syntehtic benchmark, as it applies synthetic rain and\nfog [50] over 4 different intensities profiles.\nThe main\nbenchmark, Increasing Storm, presents a storm with a pyra-\nmidal intensity profile; see Fig. 4. In contrast, (b) is a purely\nsynthetic dataset, where both the underlying image and the\nweather are synthetically generated and thus domain change\nis fully controllable. All models are evaluated using mIoU:\nfollowing [41], we report the harmonic mean over domains\nto present the overall adaptation performance. All experi-\nments were carried out using an Nvidia\u2122 RTX 3090 GPU.\nWe refer to supplementary material for further details.\n4.1. Ablation Studies\nIn Tab. 1 we study the impact of each contribution to\nadaptation performance, both in terms of accuracy and effi-\nciency. For each configuration, we report mIoU over differ-\nent portions of the sequence, the framerate and the amount\nof GFLOPS \u2013 respectively averages of: total, forward and\nbackward passes, and dedicated adaptation only, also di-\nvided in forward (Fwd) and backward (Bwd). Tab. 1 (a)\nshows results on the Increasing Storm scenario [41]. Here,\nwe show mIoU over the 200mm domain, i.e. the hardest in\nthe sequence, as well as the mIoU averaged over forward\nand backward adaptation, i.e., from clear to 200mm rain\n6\nclear\n25mm\n50mm\n75mm\n100mm\n200mm\nh-mean\nFPS\nGFLOPS\nF\nB\nF\nB\nF\nB\nF\nB\nF\nB\nF\nF\nB\nT\n(A)\nDeepLabV2 (no adaptation)\n64.5\n\u2013\n57.1\n\u2013\n48.7\n\u2013\n41.5\n\u2013\n34.4\n\u2013\n18.5\n37.3\n\u2013\n\u2013\n39.4\n\u2013\n(B)\nDeepLabV2 fully supervised (oracle)\n64.5\n\u2013\n64.1\n\u2013\n63.7\n\u2013\n63.0\n\u2013\n62.4\n\u2013\n58.2\n62.6\n\u2013\n\u2013\n39.4\n\u2013\n(C)\nOnDA\n64.5\n64.8\n60.4\n57.1\n57.3\n54.5\n54.8\n52.2\n52.0\n49.1\n42.2\n54.2\n55.1\n\u2013\n1.3\n\u2013\n(D)\nSegFormer MiT-B1 (no adaptation)\n73.4\n\u2013\n68.8\n\u2013\n64.2\n\u2013\n58.0\n\u2013\n51.8\n\u2013\n31.2\n57.8\n\u2013\n\u2013\n48.4\n34.9\n(E)\nSegFormer MiT-B5 (no adaptation)\n77.6\n\u2013\n73.9\n\u2013\n71.0\n\u2013\n67.2\n\u2013\n62.6\n\u2013\n46.7\n64.7\n\u2013\n\u2013\n11.5\n240.4\n(F)\nSegFormer MiT-B1 fully supervised (oracle)\n72.9\n\u2013\n72.4\n\u2013\n72.1\n\u2013\n71.5\n\u2013\n70.7\n\u2013\n68.6\n71.3\n\u2013\n\u2013\n48.4\n34.9\n(G)\nTENT\n73.0\n72.8\n68.5\n68.6\n64.5\n64.8\n59.7\n60.2\n54.5\n54.8\n35.9\n56.2\n63.6\n59.9\n10.0\n\u2013\n(H)\nTENT + Replay Buffer\n73.0\n72.8\n68.5\n68.6\n64.5\n64.8\n59.7\n60.2\n54.4\n54.7\n35.8\n56.1\n63.6\n59.9\n7.8\n\u2013\n(I)\nCoTTA\n72.5\n74.4\n69.5\n70.9\n65.9\n68.2\n66.1\n64.7\n64.6\n63.5\n57.2\n65.6\n68.1\n66.8\n0.6\n593.8\n(J)\nCoTTA real-time\n73.3\n75.4\n70.3\n70.6\n66.9\n66.4\n62.5\n61.4\n57.6\n56.9\n39.7\n59.2\n65.5\n62.3\n27.0\n41.7\n(K)\nHAMLET (ours)\n73.4\n71.0\n70.1\n68.8\n67.7\n67.5\n66.6\n66.4\n65.5\n64.6\n59.2\n66.8\n67.6\n67.2\n29.1\n45.7\nTable 2: Comparison against other models \u2013 Increasing storm scenario. (A-C) methods built over DeepLabv2, (D-E)\nSegFormer variants trained on source, (F) oracle, (G-K) models adapted online. We report mIoU, framerate, and GFLOPS.\nand backward. Results are averaged over 3 runs with dif-\nferent seeds, with standard deviation being reported. (A)\nreports the results achieved by na\u00a8\u0131vely performing full adap-\ntation of the model. HAMT can increase the framerate by\nroughly 15% by reducing the Bwd GFLOPS of 34%, at the\nexpense of as few as 0.7 mIoU on average, i.e., about 2\npoints on the 200mm domain. The main boost in terms\nof speed is obviously given by LT (C), which inhibits the\ntraining in absence of detected domain shifts. LT increases\nthe framerate by approximately 4\u00d7 by decimating the total\nGFLOPS, yet not affecting the adaptation Bwd GFLOPS.\nThis comes with a price in terms of mIoU, dropping by\nabout 4 points on average and more than 10 points on\n200mm \u2013 not a moderate drop anymore. LT impact highly\ndepends on the domain sequence experienced during de-\nployment: frequent domain changes could prevent training\ninhibition, thus neglecting LT gains in terms of efficiency,\nas we will appreciate later. The loss in accuracy is progres-\nsively regained by adding ALR (D), with further improve-\nments yielded by one between DCM (E) and RCS (F), or\nboth together (G) leading to the full HAMLET configura-\ntion. The three together allow for reducing the gap to 2.5\npoints mIoU \u2013 4 over the 200mm domain \u2013 without sacri-\nficing any efficiency. Tab. 1 (b) shows further results, on a\nfaster version of Storm C [41]. This represents a much more\nchallenging scenario, with harsher and 3\u00d7 more frequent\ndomain shifts. Here we show the single domains mIoU, as\nwell as harmonic mean on source and target domains, and\nall frames. As expected, in this benchmark, LT alone (C)\nresults much less effective than before, with a much lower\ngain in FPS and GFLOPS. Here, the synergy between the\nHAMT, LT, and the other components (G) allows for the\nbest accuracy and speedup \u2013 even outperforming the full\ntraining variant (A) \u2013 highlighting their complementarity.\nFurther ablations are in the supplementary material.\n4.2. Results on Increasing Storm\nTab. 2 shows a direct comparison between HAMLET\nand relevant approaches. The presented test-time adaptation\n0\n50\n100\n200\nintensity\n40\n60\nmIoU\nclear\n25mm\n50mm\n75mm\n100mm\n200mm\n0\n2\n4\n6\nlearning rate\n1e\n5\n0\n8926\n17851\n26776\n35701\n44626\n53551\n62476\n71401\n80326\n89251\n98176\nStep\n0\n20\n40\nFPS\nFigure 4: HAMLET on the Increasing Storm. We show\nrain intensity (in millimetres), mIoU over active (bold) and\ninactive (dashed) domains, learning rate and FPS.\nstrategies namely \u2013 TENT and CoTTA \u2013 were revised to\nhandle the online setting and be fairly compared with HAM-\nLET. All methods start with the same exact initial weights\n\u2013 with HAMLET requiring the additional lightweight de-\ncoder, not needed by TENT and CoTTA \u2013 using SegFormer\nMiT-B1 as the backbone, since it is 4\u00d7 faster than Seg-\nFormer MiT-B5 and thus better suited to keep real-time\nperformance even during adaptation.\nWe report results\nachieved by DeepLabv2 trained on source data only (A),\nan oracle model trained with full supervision (B), as well\nas OnDA [41] (C) as a reference. Then, we report Seg-\nFormer models trained on the source domain only (D) and\n(E). In (F) we show the performance achieved by an oracle\nSegFormer, trained on all domains fully supervised. Fol-\nlowing [41], columns \u201cF\u201d concern forward adaptation from\nclear to 200mm, while columns \u201cB\u201d show backward adap-\n7\nclear\n750m\n375m\n150m\n75m\nh-mean\nFPS\nGFLOPS\nF\nB\nF\nB\nF\nB\nF\nB\nF\nF\nB\nT\nOnDA\n64.9\n65.8\n63.3\n62.3\n60.7\n58.8\n51.6\n49.1\n42.1\n55.1\n54.1\n\u2013\n1.3\n\u2013\nSegFormer MiT-B1 (no adaptation)\n71.1\n\u2013\n70.0\n\u2013\n67.5\n\u2013\n58.8\n\u2013\n46.9\n61.3\n\u2013\n\u2013\n48.4\n34.9\nFull training\n71.5\n72.1\n72.9\n74.7\n71.9\n73.1\n67.6\n68.1\n61.3\n68.7\n71.9\n70.3\n5.6\n125.2\nHAMLET (ours)\n71.1\n71.6\n70.3\n70.8\n68.8\n69.2\n64.3\n64.3\n57.0\n65.9\n68.9\n67.4\n24.8\n50.7\nTable 3: Results on foggy domains. Comparison between OnDA, Source SegFormer, full training adaptation, and HAMLET.\nClear\nCloudy\nOvercast\nSmall rain\nMid rain\nHeavy rain\nh-mean\nFPS\nGFLOPS\nF\nB\nF\nB\nF\nB\nF\nB\nF\nB\nF\nF\nB\nT\nSegFormer MiT-B1 fully supervised (oracle)\n80.1\n\u2013\n79.9\n\u2013\n79.8\n\u2013\n78.9\n\u2013\n78.7\n\u2013\n77.1\n79.1\n\u2013\n\u2013\n48.4\n34.93\nSegFormer MiT-B1 (no adaptation)\n79.6\n\u2013\n77.1\n\u2013\n75.4\n\u2013\n73.4\n\u2013\n71.4\n\u2013\n66.7\n73.7\n\u2013\n\u2013\n48.4\n34.93\nFull training\n78.9\n79.3\n76.7\n76.8\n76.8\n77.9\n74.8\n74.8\n76.3\n76.5\n74.0\n76.2\n77.0\n76.6\n5.0\n125.1\nHAMLET (ours)\n79.6\n78.9\n76.9\n76.6\n76.1\n77.4\n73.3\n74.3\n74.2\n76.0\n74.2\n75.7\n76.6\n76.1\n26.8\n43.9\nTable 4: Results on SHIFT dataset [45]. Comparison between Source SegFormer, full training adaptation, and HAMLET.\ntation from 200mm to clear, while the h-mean T refers to\nthe overall harmonic mean. We can notice how SegFomer\nresults are much more robust to domain changes with re-\nspect to DeepLabv2. Indeed, SegFormer MiT-B5 (E), with-\nout any adaptation, results more accurate than DeepLabv2\noracle (B), as well as better and faster than OnDA (C). The\nfaster variant (D) outperforms OnDA both in speed and\naccuracy, reaching 48 FPS. Nevertheless, domain changes\nstill dampen the full potential of SegFormer. Indeed, the\noracle (F) outperforms (D) by about +14 mIoU. However,\nthis is not meaningful for real deployment experiencing un-\npredictable domain shifts, as it assumes to have data avail-\nable in advance. Concerning test-time models, TENT starts\nadapting properly only beyond 50mm, both with (G) and\nwithout (H) frame buffer, while it loses some accuracy on\n25mm. This makes its overall forward adaptation perfor-\nmance slightly worse compared to the pre-trained model\n(D), while being better at backward adaptation. Despite out-\nperforming SegFormer MiT-B1, TENT is both slower and\nless accurate than SegFormer MiT-B5 running without any\nadaptation, further suggesting the robustness of the latter\nand making TENT not suitable for real-world deployment.\nOn the contrary, CoTTA (I) outperforms both SegFormer\nmodels trained on source only, at the expense of dropping\nthe framerate below 1FPS. It is worth mentioning that these\nmetrics were collected after each domain was completed by\neach model individually. In an evaluation setup imposing\na shared time frame, slower models would present much\nlower metrics, since their adaptation process would result\nconstantly lagged. In fact, forcing CoTTA to run in real-\ntime, at nearly 30FPS \u2013 i.e. by training once every 50 frames\n\u2013 dramatically reduces the effectiveness of the adaptation\nprocess (J), with drastic drops in the hardest domains. Fi-\nnally, HAMLET (K) succeeds on any fronts, improving the\nbaseline (D) by about 10 points with only a cost of 25%\nin terms of speed, while outperforming SegFormer MiT-B5\n(E) both on accuracy (+2.5 mIoU) and speed (3\u00d7 faster)\n\u2013 being the only method achieving this, and thus the only\nsuitable choice for real-time applications. Fig. 4 shows the\noverall behavior of HAMLET while adapting over the In-\ncreasing Storm. In addition to the rain intensity and the\n0\n6\nintensity\n60\n65\n70\n75\n80\n85\nmIoU\nclear\ncloudy\novercast\nsmall_rain\nmid_rain\nheavy_rain\n0.0\n2.5\n5.0\nlearning rate\n1e\n5\n0\n21217\n35344\n43351\n50797 57172\n64363 70738\n78184\n86191\n100318\n121534\nStep\n0\n20\n40\nFPS\nFigure 5: HAMLET on the SHIFT benchmark. We show\nmIoU over active (bold) and inactive (dashed) domains,\nlearning rate and FPS.\nmIoU achieved on each domain \u2013 active (bold) or inac-\ntive (dashed), i.e. respectively the mIoU on the domain be-\ning currently faced during deployment, and how the current\nadaptation affects the performance on the other domains to\nhighlight the robustness to forgetting \u2013 we also report how\nthe learning rate is modulated in correspondence of detected\ndomain shifts, with a consequent drop in FPS due to the\nshort training process taking place. For further experiments\non harsher and sudden adaptation cycles, we include results\nof Storms A, B, C [41] in the supplementary material.\n4.3. Additional Results: Fog and SHIFT\nFog. In Tab. 3, we investigate adaptation on the Increas-\ning Fog scenario in the OnDA benchmark [41]. Crucially,\nfor this experiment, we keep the same hyperparameters used\nfor the Increasing Storm, since in both cases the starting\nSegFormer model is trained on the same source domain.\nThis allows for validating how the proposed setting general-\n8\nclean\n50mm\n100mm\n200mm\nFigure 6: Qualitative results \u2013 HAMLET in action. From left to right, we show frames from clean, 50mm, 100mm, and\n200m domains. From top to bottom: input image, prediction by SegFormer trained on source domain and HAMLET.\nizes at dealing with different kind of domain shifts, beyond\nthose considered in the main experiments. We effectively\nuse Increasing Fog as test set, and compare against Seg-\nFormer trained on source (no adaptation) and a model that\nhas been adapted by means of full online training optimiza-\ntion (configuration (A) of Table 1). HAMLET is able to\nadapt almost as well as the full online training model, with\nless than a 3 mIoU gap, while enjoying real-time adaptation\nat nearly 5\u00d7 the speed using just 40% of the FLOPS.\nSHIFT. We further test HAMLET on the SHIFT\ndataset [45]. Tab. 4 collects the results achieved by Seg-\nFormer trained on source, full online training and HAM-\nLET respectively, both at forward and backward adaptation\nacross Clear, Cloudy, Overcast, Small rain, Mid rain and\nHeavy rain domains. Here HAMLET results highly com-\npetitive with the full training regime, with only 0.5 drop in\naverage mIoU, while being more than 5\u00d7 faster. Fig. 5\ndepicts, from top to bottom, the rain intensity characteriz-\ning any domain encountered on SHIFT, the mIoU achieved\nboth on current (bold) and inactive (dashed) domains, the\nlearning rate changes based on the domain shift detection,\nand the framerate achieved at any step. We refer to the sup-\nplementary material for a deeper analysis.\nQualitative results. To conclude, Fig. 6 shows some\nqualitative examples from CityScapes. We can notice how\nSegFormer accuracy (second tow) drops with severe rain,\nwhereas HAMLET (third row) is capable of keeping the\nsame segmentation quality across the storm.\n5. Discussion\nOrthogonality.\nHAMT and LT act independently.\nIndeed, by strongly constraining the adaptation periods\nthrough LT, HAMT has a limited margin of action. The im-\npact of HAMT also depends on the backbone and by care-\nfully crafting modular architectures, one can achieve further\noptimization. Nevertheless, in a deployment environment\nwhere domain shifts occur at high frequencies (e.g., Storm\nC), LT is ineffective, while HAMT thrives.\nMeasuring forgetting. An interesting topic we have not\ninvestigated consists of introducing an explicit awareness of\nwhich domains have been explored and how well we can re-\ncall them, expanding the distance B to multiple dimensions.\nSafety. We believe dynamic adaptation has the poten-\ntial to enhance safety, but we acknowledge the necessity for\nrigorous testing and verification to safeguard against drift\nor catastrophic forgetting. This mandates a comprehensive\neffort from academia, industry, and certification authorities\nfor ensuring the integrity of dynamically adapting models.\n6. Summary & Conclusion\nWe have presented HAMLET, a framework for real-time\nadaptation for semantic segmentation that achieves state-\nof-the-art performance on established benchmarks with\ncontinuous domain changes.\nOur approach combines a\nhardware-aware backpropagation orchestrator and a spe-\ncialized domain-shift detector to enable active control over\nthe model\u2019s adaptation, resulting in high framerates on a\nconsumer-grade GPU. These advancements enable HAM-\nLET to be a promising solution for in-the-wild deployment,\nmaking it a valuable tool for applications that require robust\nperformance in the face of unforeseen domain changes.\nAcknowledgement. The authors thank Gianluca Villani\nfor the insightful discussion on reward-punishment policies,\nLeonardo Ravaglia for his expertise on hardware-aware\ntraining, and Lorenzo Andraghetti for exceptional technical\nsupport throughout the project. Their assistance was invalu-\nable in the completion of this work.\n9\nReferences\n[1] Andreea Bobu, Judy Hoffman, Eric Tzeng, and Trevor Dar-\nrell. Adapting to continuously shifting domains. In ICLR\n2018 Workshop Program Chairs, 2018. 00000.\n[2] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong,\nXinghao Ding, Yue Huang, Tingyang Xu, and Junzhou\nHuang. Progressive feature alignment for unsupervised do-\nmain adaptation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n627\u2013636, 2019.\n[3] Liang-Chieh Chen, Jonathan T Barron, George Papandreou,\nKevin Murphy, and Alan L Yuille. Semantic image segmen-\ntation with task-specific edge detection using cnns and a dis-\ncriminatively trained domain transform. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 4545\u20134554, 2016.\n[4] Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng,\nMaxwell D Collins, Ekin D Cubuk, Barret Zoph, Hartwig\nAdam, and Jonathon Shlens.\nNaive-student: Leveraging\nsemi-supervised learning in video sequences for urban scene\nsegmentation. In European Conference on Computer Vision,\npages 695\u2013714. Springer, 2020.\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834\u2013848, 2017.\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected crfs. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 40(4):834\u2013848,\nApr 2018.\n[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProceedings of the European conference on computer vision\n(ECCV), pages 801\u2013818, 2018.\n[8] Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai,\nYu-Chiang Frank Wang, and Min Sun. No more discrimina-\ntion: Cross city adaptation of road scene segmenters. In 2017\nIEEE International Conference on Computer Vision (ICCV),\npages 2011\u20132020. IEEE, 2017. 00000.\n[9] Feng Cheng, Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu\nLi, Wei Li, and Wei Xia. Stochastic backpropagation: A\nmemory efficient strategy for training video models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8301\u20138310, 2022.\n[10] Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magne-\nnat Thalmann, and Gang Wang.\nBoundary-aware feature\npropagation for scene segmentation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6819\u20136829, 2019.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[12] A. Dundar, M. Y. Liu, Z. Yu, T. C. Wang, J. Zedlewski, and\nJ. Kautz. Domain stylization: A fast covariance matching\nframework towards domain adaptation. IEEE Transactions\non Pattern Analysis and Machine Intelligence, pages 1\u20131,\n2020.\n[13] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual attention network for scene seg-\nmentation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 3146\u20133154,\n2019.\n[14] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin-\nhui Tang, and Hanqing Lu. Adaptive context network for\nscene parsing.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 6748\u20136757,\n2019.\n[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-\ncal Germain, Hugo Larochelle, Franc\u00b8ois Laviolette, Mario\nMarchand, and Victor Lempitsky. Domain-adversarial train-\ning of neural networks.\nThe journal of machine learning\nresearch, 17(1):2096\u20132030, Jan. 2016.\n[16] Rui Gong, Martin Danelljan, Dengxin Dai, Danda Pani\nPaudel, Ajad Chhatkuli, Fisher Yu, and Luc Van Gool. Tacs:\nTaxonomy adaptive cross-domain semantic segmentation. In\nEuropean Conference on Computer Vision, pages 19\u201335.\nSpringer, 2022.\n[17] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu\nQiao. Adaptive pyramid context network for semantic seg-\nmentation.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 7519\u2013\n7528, 2019.\n[18] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,\nPhillip Isola, Kate Saenko, Alexei Efros, and Trevor Dar-\nrell. CyCADA: Cycle-consistent adversarial domain adap-\ntation.\nIn Jennifer Dy and Andreas Krause, editors, Pro-\nceedings of the 35th International Conference on Machine\nLearning, volume 80 of Proceedings of Machine Learning\nResearch, pages 1989\u20131998, Stockholmsm\u00a8assan, Stockholm\nSweden, 10\u201315 Jul 2018. PMLR.\n[19] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Dar-\nrell. FCNs in the wild: Pixel-level adversarial and constraint-\nbased adaptation. CoRR, 2016. 00000.\n[20] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Daformer:\nImproving network architectures and training strategies for\ndomain-adaptive semantic segmentation.\narXiv preprint\narXiv:2111.14887, 2021.\n[21] Lukas Hoyer, Dengxin Dai, and Luc Van Gool.\nHrda:\nContext-aware high-resolution domain-adaptive semantic\nsegmentation. In European Conference on Computer Vision\n(ECCV), 2022.\n[22] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad-\njustment module for model-agnostic domain generalization.\nIn A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Process-\ning Systems, 2021.\n[23] Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G\nAndersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi,\n10\nMichael Kaminksy, Michael Kozuch, Zachary C Lipton,\net al. Accelerating deep learning by focusing on the biggest\nlosers. arXiv preprint arXiv:1910.00762, 2019.\n[24] Zhengkai Jiang, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao\nWang, Ying Tai, and Chengjie Wang.\nPrototypical con-\ntrast adaptation for domain adaptive semantic segmentation.\nIn European Conference on Computer Vision, pages 36\u201354.\nSpringer, 2022.\n[25] Myeongjin Kim and Hyeran Byun. Learning texture invari-\nant representation for domain adaptation of semantic seg-\nmentation. 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun 2020.\n[26] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\nVeness, Guillaume Desjardins, Andrei Rusu, Kieran Milan,\nJohn Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and\nRaia Hadsell. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of Sciences,\n114, 12 2016.\n[27] Yevhen Kuznietsov, Marc Proesmans, and Luc Van Gool.\nTowards unsupervised online domain adaptation for seman-\ntic segmentation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 261\u2013\n271, 2022.\n[28] Xin Lai, Zhuotao Tian, Xiaogang Xu, Yingcong Chen, Shu\nLiu, Hengshuang Zhao, Liwei Wang, and Jiaya Jia. Decou-\nplenet: Decoupled network for domain adaptive semantic\nsegmentation. In European Conference on Computer Vision,\npages 369\u2013387. Springer, 2022.\n[29] Qicheng Lao,\nXiang Jiang,\nMohammad Havaei,\nand\nYoshua Bengio. Continuous domain adaptation with vari-\national domain-agnostic feature replay.\narXiv preprint\narXiv:2003.04382, 2020.\n[30] Geon Lee, Chanho Eom, Wonkyung Lee, Hyekang Park, and\nBumsub Ham. Bi-directional contrastive learning for domain\nadaptive semantic segmentation. In European Conference on\nComputer Vision, pages 38\u201355. Springer, 2022.\n[31] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen\nLin, and Hong Liu. Expectation-maximization attention net-\nworks for semantic segmentation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9167\u20139176, 2019.\n[32] Yunsheng Li, Lu Yuan, and Nuno Vasconcelos.\nBidirec-\ntional learning for domain adaptation of semantic segmen-\ntation. 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), Jun 2019.\n[33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need\nto access the source data? source hypothesis transfer for un-\nsupervised domain adaptation. CoRR, 2020.\n[34] Yuejiang Liu, Parth Kothari, Bastien Germain van Delft,\nBaptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi.\nTTT++: When does self-supervised test-time training fail\nor thrive?\nIn A. Beygelzimer, Y. Dauphin, P. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Informa-\ntion Processing Systems, 2021.\n[35] Yuang Liu, Wei Zhang, and Jun Wang. Source-free domain\nadaptation for semantic segmentation. 2021.\n[36] Jonathan Long, Evan Shelhamer, and Trevor Darrel. Fully\nconvolutional networks for semantic segmentation. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2015.\n[37] Ke Mei, Chuang Zhu, Jiaqi Zou, and Shanghang Zhang. In-\nstance adaptive self-training for unsupervised domain adap-\ntation. Lecture Notes in Computer Science, page 415\u2013430,\n2020.\n[38] Vladimir Nekrasov, Chunhua Shen, and Ian Reid.\nLight-\nweight refinenet for real-time semantic segmentation.\nIn\nBritish Conference on Computer Vision (BMVC), 2018.\n[39] Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, and\nLennart Svensson. Classmix: Segmentation-based data aug-\nmentation for semi-supervised learning. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Com-\nputer Vision, pages 1369\u20131378, 2021.\n[40] Fei Pan, Sungsu Hur, Seokju Lee, Junsik Kim, and In So\nKweon. Ml-bpm: Multi-teacher learning with bidirectional\nphotometric mixing for open compound domain adaptation\nin semantic segmentation. In European Conference on Com-\nputer Vision, pages 236\u2013251. Springer, 2022.\n[41] Theodoros Panagiotakopoulos, Pier Luigi Dovesi, Linus\nH\u00a8arenstam-Nielsen, and Matteo Poggi. Online domain adap-\ntation for semantic segmentation in ever-changing condi-\ntions. In European Conference on Computer Vision (ECCV),\n2022.\n[42] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen\nKoltun.\nPlaying for data: Ground truth from computer\ngames. In European conference on computer vision, pages\n102\u2013118. Springer, 2016.\n[43] Serban Stan and Mohammad Rostami. Unsupervised model\nadaptation for continual semantic segmentation. In AAAI,\n2021.\n[44] Peng Su, Shixiang Tang, Peng Gao, Di Qiu, Ni Zhao, and\nXiaogang Wang. Gradient regularized contrastive learning\nfor continual domain adaptation. 2020. 00000.\n[45] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc\nVan Gool, Bernt Schiele, Federico Tombari, and Fisher Yu.\nSHIFT: a synthetic driving dataset for continuous multi-task\ndomain adaptation. In Computer Vision and Pattern Recog-\nnition, 2022.\n[46] Towaki Takikawa, David Acuna, Varun Jampani, and Sanja\nFidler. Gated-scnn: Gated shape cnns for semantic segmen-\ntation. In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 5229\u20135238, 2019.\n[47] Phillip Thomas, Lars Pandikow, Alex Kim, Michael Stan-\nley, and James Grieve. Open synthetic dataset for improving\ncyclist detection, Nov 2021.\n[48] Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mat-\ntoccia, and Luigi Di Stefano. Real-time self-adaptive deep\nstereo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 195\u2013204, 2019.\n[49] Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, and\nLennart Svensson.\nDacs: Domain adaptation via cross-\ndomain mixed sampling. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision\n(WACV), pages 1379\u20131389, January 2021.\n11\n[50] Maxime Tremblay, Shirsendu S. Halder, Raoul de Charette,\nand Jean-Franc\u00b8ois Lalonde. Rain rendering for evaluating\nand improving robustness to bad weather.\nInternational\nJournal of Computer Vision, 2020.\n[51] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki-\nhyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker.\nLearning to adapt structured output space for semantic seg-\nmentation. 2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, Jun 2018.\n[52] Riccardo Volpi, Pau de Jorge, Diane Larlus, and Gabriela\nCsurka. On the road to online adaptation for semantic image\nsegmentation. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2022.\n[53] Vibashan VS, Poojan Oza, and Vishal M. Patel. Towards on-\nline domain adaptive object detection. 2023 IEEE/CVF Win-\nter Conference on Applications of Computer Vision (WACV),\nJan 2023.\n[54] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol-\nshausen, and Trevor Darrell. Tent: Fully test-time adaptation\nby entropy minimization.\nIn International Conference on\nLearning Representations, 2021.\n[55] Haoran Wang, Tong Shen, Wei Zhang, Lingyu Duan, and\nTao Mei.\nClasses matter: A fine-grained adversarial ap-\nproach to cross-domain semantic segmentation. In The Euro-\npean Conference on Computer Vision (ECCV), August 2020.\n[56] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han.\nHaq: Hardware-aware automated quantization with mixed\nprecision, 2018.\n[57] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con-\ntinual test-time domain adaptation, 2022.\n[58] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794\u20137803, 2018.\n[59] Bingzhen Wei, Xu Sun, Xuancheng Ren, and Jingjing Xu.\nMinimal effort back propagation for convolutional neural\nnetworks. arXiv preprint arXiv:1709.05804, 2017.\n[60] Tsung-Han Wu, Yi-Syuan Liou, Shao-Ji Yuan, Hsin-Ying\nLee, Tung-I Chen, Kuan-Chih Huang, and Winston H Hsu.\nD2ada: Dynamic density-aware active domain adaptation for\nsemantic segmentation. In European Conference on Com-\nputer Vision (ECCV), 2022.\n[61] Zuxuan Wu, Xintong Han, Yen-Liang Lin, Mustafa G\u00a8okhan\nUzunbas, Tom Goldstein, Ser Nam Lim, and Larry S. Davis.\nDcan: Dual channel-wise alignment networks for unsuper-\nvised scene adaptation. Lecture Notes in Computer Science,\npage 535\u2013552, 2018.\n[62] Zuxuan Wu, Xin Wang, Joseph Gonzalez, Tom Goldstein,\nand Larry Davis. ACE: Adapting to changing environments\nfor semantic segmentation. In 2019 IEEE/CVF International\nConference on Computer Vision (ICCV), pages 2121\u20132130.\nIEEE, 2019.\n[63] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre-\nmental adversarial domain adaptation for continually chang-\ning environments. 2018. 00000.\n[64] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu,\nDing Liang, and Ping Luo. Segmenting transparent objects\nin the wild with transformer. In IJCAI, 2021.\n[65] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo.\nSegformer: Simple and\nefficient design for semantic segmentation with transform-\ners.\nAdvances in Neural Information Processing Systems,\n34:12077\u201312090, 2021.\n[66] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan\nYang. Denseaspp for semantic segmentation in street scenes.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 3684\u20133692, 2018.\n[67] Yanchao Yang and Stefano Soatto.\nFDA: Fourier domain\nadaptation for semantic segmentation. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 4084\u20134094. IEEE, 2020.\n[68] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. Bisenet: Bilateral segmentation\nnetwork for real-time semantic segmentation. In Proceed-\nings of the European conference on computer vision (ECCV),\npages 325\u2013341, 2018.\n[69] Yuhui Yuan, Xiaokang Chen, Xilin Chen, and Jingdong\nWang. Segmentation transformer: Object-contextual repre-\nsentations for semantic segmentation. In European Confer-\nence on Computer Vision (ECCV), 2020.\n[70] Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang,\nand Fang Wen. Prototypical pseudo label denoising and tar-\nget structure learning for domain adaptive semantic segmen-\ntation. 2021.\n[71] Qiming Zhang, Jing Zhang, Wei Liu, and Dacheng Tao. Cat-\negory anchor-guided unsupervised domain adaptation for se-\nmantic segmentation. Advances in Neural Information Pro-\ncessing Systems, 2019.\n[72] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017.\n[73] Yizhou Zhou, Xiaoyan Sun, Zheng-Jun Zha, and Wenjun\nZeng. Context-reinforced semantic segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4046\u20134055, 2019.\n[74] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.\nEfros.\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks.\n2017 IEEE International\nConference on Computer Vision (ICCV), Oct 2017.\n[75] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang.\nUnsupervised domain adaptation for semantic segmentation\nvia class-balanced self-training. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 289\u2013\n305, 2018.\n[76] Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and\nJinsong Wang.\nConfidence regularized self-training.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 5982\u20135991, 2019.\n[77] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V. K. Vijaya Kumar,\nand Jinsong Wang.\nConfidence regularized self-training.\n2019 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), Oct 2019.\n12\nTo Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation \u2013\nSupplementary Material\nMarc Botet Colomer\u2217 1,2\nPier Luigi Dovesi\u2217 3 \u2020\nTheodoros Panagiotakopoulos4\nJoao Frederico Carvalho1\nLinus H\u00a8arenstam-Nielsen5,6\nHossein Azizpour2\nHedvig Kjellstr\u00a8om2,3\nDaniel Cremers5,6,7\nMatteo Poggi8\n1Univrses\n2KTH\n3Silo AI\n4King\n5Technical University of Munich\n6Munich Center for Machine Learning\n7University of Oxford\n8University of Bologna\nhttps://marcbotet.github.io/hamlet-web/\nOurs\nTent\nSegFormer MiT-B1\nSegFormer MiT-B5\nCoTTA\nCoTTA real-time\n0\n10\n20\n30\n40\n50\n58\n60\n62\n64\n66\n68\nFPS\nmIoU\nOurs\nTent\nSegFormer MiT-B1\nSegFormer MiT-B5\nCoTTA\nCoTTA real-time\n0\n10\n20\n30\n40\n50\n30\n35\n40\n45\n50\n55\n60\nFPS\nmIoU\nall-domains\n200mm\nFigure 1: Model averaged performances over domain. We can observe how HAMLET reaches state-of-the-art accuracy,\nwhile running more than 6\u00d7 faster. Adaptive models are displayed in blue, while models trained on source, without adapta-\ntion, are displayed in black. On the left we show averaged performances over all domains, on the right we show how metrics\ndrop in the hardest domain (200mm). The metric drop is limited for strong adaptive networks CoTTA [9], and HAMLET,\nwhile being a drastic drop for TENT [8] and SegFormer [10] (both MiT-B1 and MiT-B5). Finally, CoTTA real-time shows\nthe performance of CoTTA in deployment conditions, hence running once every 50 frames.\nThis report introduces further details on the ICCV paper - \u201cTo Adapt or Not to Adapt? Real-Time Adaptation for Semantic\nSegmentation\u201d. On the cover of this document, Figure 1, we propose a comparison of HAMLET (Hardware-Aware Modular\nLeast Expensive Training) against state-of-the-art adaptation strategies, hence showing its highly favorable trade-off between\nspeed and accuracy.\nThen, starting with Section 1 we present an ablation study on the Hardware-Aware Modular Training (HAMT) method,\nwhere we show several speed-accuracy trade-offs. In Section 2 we dive deep in the Least Training (LT) and Adaptive Learning\nRate (ALR) methodologies. We illustrate the behavior of different policies by comparing them on the same domain shift,\nand we also show how the policy copes with noisy domain shift detection. In addition, we present a quantitative analysis\nfocusing on both speed and accuracy. A deeper analysis of the effect of the adaption on every single class is presented in\n\u2217 Joint first authorship\n\u2020 Part of the work carried out while at Univrses.\nSection 3. Then, in Sections 4 and 5 we detail the model implementation and the chosen hyperparameters. In Section 6\nwe provide extensive studies on additional storms as presented in the OnDA benchmark [5]. We particularly focus on the\nbehavior of repeated adaptation cycles and how they affect domain shift detection (hence ALR policies). Then, in Section\n7 we illustrate additional experiments on the SHIFT dataset [6]. We first present a plot summarizing the model behavior\non SHIFT and then we analyze how LT could prevent forgetting in long sequences without relevant domain shifts. We\nconclude by reporting some qualitative results in Section 8, and by referencing the qualitative videos uploaded on Youtube in\nSection 9. The first qualitative video (https://www.youtube.com/watch?v=zjxPbCphPDE&t=139s) showcases\na comparison between HAMLET, CoTTA, and SegFormer MiT-B1 (no adaptation) on a Cityscapes [2] sequence with the\nIncremental Storm. Finally, we argue that synthetic data could only partially provide evidence of our methods: purposely,\nwe run HAMLET on a real driving video taken in Korea across different rainy domains \u2013 https://www.youtube.\ncom/watch?v=Dwswey-GqQc, whose author gave us consent to use it \u2013 to further support its effectiveness. The second\nqualitative video (https://www.youtube.com/watch?v=zjxPbCphPDE) shows the outcome of this experiment.\n1. Ablation study: Hardware-Aware Modular Training\nIn this section, we present an additional ablation study on the HAMT module to further investigate its performance.\nTable 1 reports the adaptation results achieved by different configurations that exploit HAMT alone, without Active Training\nModulation being enabled. We focus on the average adaptation performance and results on the hardest domain (200mm)\nand source domain (clear) to respectively measure the effectiveness of the adaptation scheme on the hardest domain and the\nrobustness when adapting back on the source domain. Additionally, we report the framerate achieved by each configuration\nand the GFLOPS performed to backpropagate gradients through SegFormer.\nWe compare the Full training model (A) to several configurations that use a random sampling policy to pick which module\nto optimize (B, C, D, E) and HAMT sampling strategy (B\u2019, C\u2019, D\u2019, E\u2019), characterized by uniform sampling \u2013 not hardware-\naware \u2013 (B, B\u2019) or by setting \u03b2 equal to 2.75, 1.75, or 1 for time conditioning, respectively, for entries (C, C\u2019), (D, D\u2019), and\n(E, E\u2019). We also include a time-conditioned random sampling policy that uses softmax of the measured FPS for each action,\nwith the temperature controlled by the same parameter \u03b2 introduced in HAMT. The time-conditioned random sampling is\nachieved by skewing the uniform distribution with a softmax of the measured FPS for each action, effectively making more\nlikely to pick an action, the faster it is. The softmax temperature is controlled by the same parameter \u03b2 introduced in HAMT.\nThis allows us to compare how HAMT performs compared to a simpler baseline bound to achieve similar FPS, nevertheless,\nthe action choice will not be controlled by HAMT reward-punishment algorithm, but it will be randomly sampled.\nAs expected, we find that the most aggressive GFLOPS reduction corresponds to lower \u03b2 values, but this comes with\na price in metrics. Our observations show that even the na\u00a8\u0131ve hardware-aware random policy can significantly reduce the\nGFLOPS dedicated to backpropagation without drastic metric drops. However, given the same \u03b2, HAMT policy always\nresults in better performance than the na\u00a8\u0131ve time-conditioned policies, both on hard domains (200mm of rain), the source\ndomain (clear, 0mm of rain), and on average (F and T to signal forward and backward adaptation). As reported in the main\npaper, we set \u03b2 = 1.75 as the default choice in any other experiments, allowing for a trade-off between GFLOPS reduction\nand adaptation effectiveness.\nWe can notice how the gain lead by HAMT over the na\u00a8\u0131ve the metrics is more prominent for high values of \u03b2 (i.e. less\nintense time conditioning) since it leaves to the reward-punishment algorithm more freedom of action to pick the best modules\nto train.\nOverall, our ablation study shows that HAMT can effectively reduce the computational cost of adaptation without com-\npromising accuracy. HAMT is especially useful when facing harsh and frequent domain shifts, where adaptation cannot be\neasily interrupted. Moreover, our study provides insights into the impact of time conditioning on the performance of HAMT\nand the importance of setting appropriate values of \u03b2.\nFocus: Why are we using the 2nd derivative and not the 1st? Since every action corresponds to an optimization step,\nwe expect that every action will minimize the loss function. Therefore, on average, all actions would receive positive rewards.\nThis might lead to the model repeatedly taking the same action, moreover, we want to reward only those actions which are\nleading to a sharper loss reduction compared to the other optimization alternatives. Indeed the 2nd derivative will be positive\nonly if the loss minimization has been greater than the expected linear extrapolation.\n2. Ablation study: Active Training Modulation\nWe now focus on studying variations and single components of the policy we defined in Section 3.3 of the main paper.\nSpecifically, with reference to the notation in Section 3.3, we define 5 different policy variants in an incremental manner, by\nclear\n. . .\n200mm\nh-mean\nBackward\n% Backward\nF\nB\nF\nF\nB\nT\nGFLOPS\nGFLOPS\n(A)\nFull training\n73.5\n73.2\n62.6\n68.7\n71.0\n69.8\n30.8\n100.0\n(B)\nRandom policy (uniform)\n73.5\n72.9\n. . .\n61.6\n68.2\n70.4\n69.3\n22.6\n73.5\n(B\u2019)\nHAMT (no time conditioning)\n73.5\n73.1\n61.9\n68.4\n70.4\n69.4\n22.4\n72.9\n(C)\nRandom policy (time-conditioned \u03b2 = 2.75)\n73.5\n73.0\n. . .\n59.0\n67.5\n70.1\n68.8\n21.4\n69.5\n(C\u2019)\nHAMT \u03b2 = 2.75\n73.4\n73.2\n61.1\n68.0\n70.3\n69.1\n21.3\n69.3\n(D)\nRandom policy (time-conditioned \u03b2 = 1.75)\n73.3\n72.8\n. . .\n59.9\n67.6\n70.4\n69.0\n20.7\n67.3\n(D\u2019)\nHAMT \u03b2 = 1.75\n73.6\n72.9\n60.9\n67.8\n70.4\n69.1\n20.3\n65.8\n(E)\nRandom policy (time-conditioned \u03b2 = 1)\n73.1\n72.6\n. . .\n60.2\n67.6\n70.0\n68.8\n19.4\n63.1\n(E\u2019)\nHAMT \u03b2 = 1\n73.2\n72.7\n60.0\n67.6\n70.1\n68.9\n17.7\n57.6\nTable 1: Ablation studies \u2013 HAMT module (3.2). We report adaptation results on the Increasing Storm, achieved by\nexploiting different HAMT configurations. We also report the framerate, as well as the GFLOPS required to perform the\nbackward pass during optimization.\nassuming:\nI) Constant learning rate and a number of iterations proportional to |\u2206Bi|. In this policy, it is assumed that the\nlength of the adaptation window should grow with the intensity of the observed domain shift with respect to z, the minimum\n|\u2206Bi| that trigger adaptation. We then compute the number of adaptation iterations as L = Kl\n|\u2206Bi|\nz\nwith the factor Kl and\nthe learning rate \u03b7 kept constant. If new domain shifts are detected before the end of the adaptation windows, the remaining\niterations are accumulated.\nII) Constant initial learning rate with decay inversely proportional to |\u2206Bi|. In addition to the previous policy, now\nthe learning rate \u03b7 gradually decays until the adaptation is stopped, the smaller the domain shift, the faster the decay. The\ninitial learning rate K\u03b7 is kept constant.\nIII) Initial learning rate proportional to |\u2206Bi| with constant number of adaptation interations. This policy assumes\nto always adapt for a fixed amount of steps, hence L is fixed. However, the initial learning rate is proportional to the intensity\nof the measured domain shift |\u2206Bi| with respect to the minimum detectable shift z. Therefore, the initial learning K\u03b7, is\ncomputed as K\u03b7 = P\u03b7\n|\u2206Bi|\nz\n, where P\u03b7 is a constant that defines the minimum value of K\u03b7.\nIV) Number of iterations proportional to |\u2206Bi|, and initial learning rate proportional to the discretized distance\nB. This policy follows II), yet assuming an initial learning rate K\u03b7 that is higher for domains farther from the source.\nK\u03b7 = K\u03b7,min + (Bi \u2212 Bsource)(K\u03b7,max \u2212 K\u03b7,min)\nBhard \u2212 Bsource\n(1)\nwhere Bsource (resp. Bhard) is an estimate of B when the network is close to (resp. far from) the source domain; and K\u03b7,min\n(resp. K\u03b7,max) is the value of K\u03b7 assigned when the network is close to (resp. far away from) the source.\nV) Number of iterations proportional to |Si|, direction sensitive, and initial learning rate proportional to the dis-\ncretized distance B. This is the policy applied in the main paper, building upon policy IV). Here we use a variable mul-\ntiplicative factor for the number of iterations Kl which depends both on both the distance from the source domain and the\ndirection of the domain shift. The rationale is that domain shifts moving away from the source domain are likely to require a\ndeeper and longer adaptation window. On the contrary, domain shifts moving closer to the source domain require fewer and\nfewer adaptation steps as we get closer. This is because the model shows good recalling of previously experienced domains\nas well as presenting strong performances close to the source thanks to regularization strategies we put in place.\n\u02dcKl =\n(\nKl,max\nif Si \u2265 0\nKl,min +\n(Bi\u2212Bsource)(Kl,max\u2212Kl,min)\nBhard\u2212Bsource\notherwise\n(2)\nWhere Kl,min (resp. Kl,max) is the value of \u02dcKl assigned when the network is close to (resp. far away from) the source\ndomain. We will appreciate how this last policy results in the best trade-off between accuracy and speed.\nIn Tab. 2 we showcase the results achieved on the Increasing Storm by different instances of SegFormer, according to the\npolicy variants outlined so far. In (A) full training is performed, while in (B) and (C) we propose two baselines where we\nna\u00a8\u0131vely optimize the model every 15 and 20 frames respectively, or by implementing our policies (I-V). As for HAMT,\nwe report performance on clear and 200mm domains, as well as the average forward, backwards and total mIoU together\nwith the framerate. As expected, reducing the adaptation steps to one every 15 or 20 frames definitely increases the FPS,\nclear\n. . .\n200mm\nh-mean\nFPS\nF\nB\nF\nF\nB\nT\n(B)\nTrain every 15 iterations\n73.2\n73.3\n. . .\n53.3\n64.1\n68.3\n66.2\n26.5\n(C)\nTrain every 20 iterations\n73.2\n73.0\n50.2\n63.2\n67.9\n65.5\n34.0\n(I)\nAdapt. iter. (constant \u03b7)\n73.4\n72.8\n55.6\n65.5\n69.3\n67.4\n25.3\n(II)\nAdapt. iter.\n73.4\n73.1\n. . .\n58.5\n66.5\n69.7\n68.1\n25.2\n(III)\nAdapt. \u03b7\n73.4\n71.4\n55.4\n65.3\n67.9\n66.6\n31.0\n(IV)\nAdapt. iter. and \u03b7\n73.4\n73.2\n57.9\n66.0\n70.0\n68.0\n23.4\n(V)\nAdapt. iter. and \u03b7, dir. sensitive\n73.4\n73.2\n57.8\n66.0\n69.0\n67.5\n29.1\nTable 2: Ablation studies \u2013 Active Training Modulation (3.3). We report adaptation results on the Increasing Storm,\nachieved by exploiting different Active Training Modulation policies (I-V), together with framerates.\nDomain Model\nRider M.bike\nSky\nRoad Truck S.walk Wall Veget. Fence Tr.Light Terrain\nBus\nCar\nTrain Sign Build. Person Pole Bike mIoU\nclear\n(both)\n53.5\n61.6\n94.4\n98.0\n77.5\n83.2\n56.4\n92.0\n53.3\n62.8\n63.7\n78.3 93.6\n56.0\n72.9\n91.5\n76.2\n57.3 72.1\n73.4\n50mm\nNo adapt.\n43.8\n44.5\n83.5\n96.3\n67.1\n73.2\n32.5\n88.0\n43.4\n54.0\n50.6\n70.5 90.6\n51.1\n67.5\n86.3\n70.1\n40.4 65.7\n64.2\n50mm\nHAMLET\n49.0\n45.4\n92.1\n97.2\n68.5\n78.7\n45.2\n90.1\n48.1\n56.3\n55.8\n73.0 90.5\n55.5\n68.7\n89.1\n71.2\n45.2 66.7\n67.7\n100mm\nNo adapt.\n30.0\n24.1\n37.2\n92.6\n54.4\n57.6\n19.3\n80.6\n30.1\n41.9\n40.0\n58.2 85.1\n45.7\n60.5\n75.6\n63.9\n28.6 58.4\n51.8\n100mm\nHAMLET\n41.6\n48.3\n90.6\n96.8\n70.0\n76.1\n44.5\n88.7\n46.7\n48.3\n57.9\n70.7 89.5\n53.1\n64.3\n87.5\n67.7\n38.3 63.5\n65.5\n200mm\nNo adapt.\n11.7\n3.7\n1.9\n81.9\n25.1\n25.3\n7.8\n59.5\n9.4\n21.8\n19.5\n33.4 59.2\n21.3\n45.5\n59.2\n50.9\n14.8 40.1\n31.2\n200mm\nHAMLET\n36.2\n32.9\n85.4\n96.1\n65.8\n71.6\n33.1\n86.1\n40.8\n38.5\n53.6\n69.2 86.7\n35.9\n57.5\n84.4\n62.9\n29.2 59.1\n59.2\nTable 3: Single classes mIoUs. Results on single classes by the source model on clear and 200mm, and by HAMLET on\n50mm, 100mm, 200mm (Incremental storm, forward pass). The improvements achieved with online adaptation are consistent\nall across the board.\nnevertheless it also notably reduces the overall adaptation effectiveness \u2013 in particular on the hardest domain of 200mm, with\na drop of around 10% compared to full training. To attain a better accuracy-speed trade-off, we employ our policies: we\ncan appreciate how (II) allows for the best overall adaptation performance as well as over 200m of rain while achieving the\nlowest FPS among the policies. Using (III) we obtain the highest FPS while losing accuracy both on average mIoU and when\nreturning back to the source \u2013 specifically, resulting in the worse policy in backward adaptation. Policy (IV) provides for\nthe best backward adaptation results, at the expense of forward adaption and average performance. Finally, (V) balances all\nof the aspects considered before, while being the second fastest configuration among those considered. We also highlight\nhow these policies merely represent examples of potential uses of the domain detection signals and how even a simple active\ntraining configuration policy could enable very fast and effective adaptation processes.\nIn Figure 2 we provide insight into the Active Training Modulation mechanism. In the top row, we exemplify two simple\ndomain shift sequences: from clear weather to 50mm to clear (on the left) and a much more sudden change, from clear\nweather to 200mm to 75mm (on the right). In the second row, we display H, denoised in A (third row) and discretized in B\n(fourth row). We then display S = \u2206B (fifth row) acting as the first derivative of B over time frames. On the left, the domain\nshift is correctly detected as a single domain shift. This is visible by having a single spike in S. On the other hand, in the\nharder scenario (on the right), the domain shift is detected in two consecutive steps. We could define this as a false positive\ndetection. The rows below show how the ALR policies manage the training phases and the learning rate modulation. We\nremind the reader that when the learning rate is zero, the training phase is inhibited. We notice how the policy formulation\ncan withstand the double-detection of the domain shift by simply recomputing the learning and accumulating the residual\niterations, overall presenting a robust behaviour.\n3. Single Classes Analysis\nIn Table 3 we present the per-class mIoU of HAMLET and a model just trained on the source domain (No adapt.). We\npresent results on the source domain, where the two models are equivalent, and in the 50mm, 100mm, and 200mm domains\nof the forward pass of the Incremental Storm [5]. As expected HAMLET vastly improves the non-adapted baseline on each\ndomain, in every single class. Interestingly, we see that HAMLET improvement is not just on a few classes, but instead, all\nclasses are improved by a consistent amount. As expected some classes are more impacted by the domain change, such as the\nSky and rare classes (e.g. M.bike, Rider, Fence), while some others present greater robustness (e.g. Road, Vegetation, Car,\nPerson).\n0-50-0mm domain shift\n0-200-75mm domain shift\n0\n50\nintensity\n0.0\n2.5\nH\n0\n1\nA\n0\n1\nB\n0.5\n0.0\n0.5\nS\n0\n1\n(I)\n1e\n5\n0\n1\n(II)\n1e\n5\n0.0\n0.5\n1.0\n(III)\n1e\n4\n0\n2\n(IV)\n1e\n5\n0\n2976\n5951\n8926\nStep\n0\n2\n(V)\n1e\n5\n0\n75\n200\nintensity\n0\n5\nH\n0\n3\nA\n0\n2\nB\n0.5\n0.0\n0.5\nS\n0\n1\n(I)\n1e\n5\n0\n1\n(II)\n1e\n5\n0\n5\n(III)\n1e\n5\n0.0\n2.5\n5.0\n(IV)\n1e\n5\n0\n2976\n5951\n8926\nStep\n0.0\n2.5\n5.0\n(V)\n1e\n5\nFigure 2: Focus on the mechanism of domain detection signals and relative adaptation process for each applied policy.\n4. Model Setup: Additional Information\nIn this section, we present additional information on the UDA model and backbone used and provide details about the\nlightweight decoder.\nLet x \u2208 RCin\u00d7H\u00d7W denote an input image and y \u2208 [0, 1]C\u00d7H\u00d7W denote a segmentation label with C number of classes.\nLet DS = {(x(i)\nS , y(i)\nS }ns\ni=1 be the labeled source dataset and DT = {x(i)\nT }nt\ni=1 the unlabeled target dataset encounter during\ndeployment, which may contain multiple sequential domains. Our goal is to train a model f\u03b8 that predicts the probability of\neach class in each pixel of the input image, such that f\u03b8(x) \u2208 RC\u00d7H\u00d7W .\nTo this end, we use a student-teacher scheme with parameters \u03b8 for the student model and parameters \u03b8\u2032 for the teacher\nmodel. During each training iteration i, we optimize the student by minimizing the loss function in Eq. 3. The teacher is\nupdated as an EMA of the student weights f\u03b8 (Eq. 4) where \u03b1 is the decay rate of the EMA.\nL(i) = L(i)\nS + L(i)\nT + \u03bbF DL(i)\nF D\n(3)\n\u03b8\u2032(i+1) \u2190 \u03b1\u03b8\u2032(i) + (1 \u2212 \u03b1)\u03b8(i)\n(4)\nThe training loss in Equation 3 is a combination of three terms. The first term, LS, is a supervised term used to learn the\nsemantic segmentation task using the replay buffer from the labeled dataset and a Cross-Entropy loss. The second term is\nFigure 3: Adaptive Domain Detector. We attach a SegFormer all-MLP decoder after the first module mfd\n1 . This allows us\nto obtain segmentation maps of any image at a low cost and with very limited speed impact.\na self-training loss that is learned from the target dataset, and the third term is a feature distance loss used as a regularizer.\nWe perform self-training by training the student model f\u03b8 on a strongly augmented version of the target dataset, along with\none-hot encoded pseudo-labels generated by the teacher. The augmented images are generated by mixing randomly selected\nclasses from the source image with target images following ClassMix [4]. LT is the cross-entropy between the mixed image\nand the mixed label weighted by factor qT , as the ratio of the pixels that have a confidence level higher than a certain threshold.\nTo prevent the student network\u2019s weights from deviating significantly from a pre-trained model on the source dataset (static\nteacher), we incorporate a feature distance loss, denoted as LFD, in the training process. Specifically, the feature distance\nloss is computed by taking the features produced by the student network\u2019s encoder f\u03b8 and those produced by a static teacher\nnetwork with frozen weights on a given input sample, and measuring the Euclidean distance between the feature embeddings\ngenerated by these two networks.\nFor our domain adaptive detector, we utilize SegFormer [10] as a semantic segmentation backbone, incorporating both its\nencoder and decoder design. We modify the static teacher model f fd by connecting an extremely lightweight segmentation\nhead, denoted as dfd1, after the first encoder module m1fd, resulting in hfd1 = dfd1 \u25e6 mfd1. This lightweight segmentation\nhead follows the SegFormer decoder architecture, using an all-MLP decoder that takes feature encodings from mfd1 with C1\nchannels and produces segmentation maps using only MLP layers (as illustrated in Fig. 3).\n5. Implementation details\nWe report any hyper-parameters used to train the described methods. The supervised models were trained using SegFormer\npre-trained weights for 100\u2019000 iterations (selecting the checkpoint with the best validation accuracy) using a learning rate of\n6 \u00d7 10\u22125, warm-up and linear decay scheduling. The online models were trained using AdamW with \u03b21 = 0.9, \u03b22 = 0.999\nand weight decay 0.01. The hyperparameters values described in the method section are: \u03b1 = 0.1, Kl = 750, K\u03b7,min =\n1.5\u00d710\u22124 K\u03b7,max = 6\u00d710\u22125, Kl,min = 187, Kl,max = 562, KCM,min = 0.5, KCM,max = 0.75 and m = 75. For the storm\nand fog scenarios we use: Bsource = 0.8, Bhard = 2.55. For the SHIFT dataset [6], we use Bsource = 0.46, Bhard = 1.85 and\nm = 200. For the video sequences, we use the fog and storm parameters with m = 350. We also used 1000 source images as\na buffer. All models performed training with a batch size of 1 and images scaled to 512\u00d71024 resolution and random crops\nof 512 \u00d7 512 where using for training. Both in HAMLET and in the full training baseline we employ SegFormer decoder,\nwithout using DAFormer [3] custom head. It\u2019s also worth noting that, to marginalize the impact of different backbones,\nall tested models in this work are using SegFormer MiT-b1 backbone (i.e. HAMLET, TENT, CoTTA) as model backbone,\nexcept if specified otherwise (i.e. OnDA and Advent [7] are using DeepLabV2 [1]).\nDuring training (evaluation is included), HAMLET consists of the following forward passes:\n\u2022 Student model using source buffer image\n\u2022 Static teacher encoder using source buffer image (no decoder)\n\u2022 EMA teacher using a target image\n\u2022 Student model using mixed image\n\u2022 Student model using a target image to provide a prediction\n\u2022 First module of static teacher in the target image and relative small decoder\n0\n50\n100\n200\nintensity\n0\n50\n100\n200\n0\n50\n100\n200\n30\n40\n50\n60\n70\nmIoU\nNon Pre-Adapted\n30\n40\n50\n60\n70\n30\n40\n50\n60\n70\n30\n40\n50\n60\n70\nmIoU\nPre-Adapted\nclear\n50mm\n100mm\n200mm\n30\n40\n50\n60\n70\nclear\n25mm\n100mm\n200mm\n50mm\n30\n40\n50\n60\n70\nclear\n200mm\n100mm\n75mm\n5\n0\n5\n10\n20\nmIoU\nDifference\n5\n0\n5\n10\n20\n5\n0\n5\n10\n20\n0 2976\n11901\n20826\n29751\n38676\n47601\n56526\n65451\nStep\ntraining\n0 2976\n11901\n20826\n29751\n38676\n47601\n56526\nStep\n0 2976\n11901\n20826\n29751\n38676\n47601\n56526\nStep\nStorm A\nStorm B\nStorm C\nFigure 4: Experimental results \u2013 Storms A, B, C. Adaptation performance by two HAMLET instances, one trained on\nsource domain (clear) and adapted for the first time, and one that has been pre-adapted on the Increasing Storm. In the last\ntwo rows, we show the boost in accuracy achieved by the latter model compared to the former, as well as the iterations during\nwhich the two are optimized (orange: the former only, blue: the latter only, gray: both).\nclear\n50mm\n10mm\n200mm\nh-mean\nFPS\nGFLOPS\nF\nB\nF\nB\nF\nB\nF\nB\nF\nB\nT\nFull training\n73.6\n73.0\n69.3\n70.1\n66.6\n66.4\n61.4\n62.8\n67.4\n67.9\n67.6\n4.6\n125.2\nHAMLET (non pre-adapted)\n73.4\n71.8\n68.3\n68.8\n63.5\n64.6\n57.1\n58.2\n65.0\n65.4\n65.2\n22.8\n58.2\nHAMLET (pre-adapted)\n71.9\n72.1\n69.4\n69.4\n65.8\n64.6\n58.1\n59.8\n65.9\n66.1\n66.0\n20.2\n59.8\nTable 4: Experimental results \u2013 Storm A\nBackpropagation is applied on the student model only. Afterwards, the dynamic teacher is updated as EMA of the student.\nDuring simple evaluation, HAMLET consists of the following forward passes:\n\u2022 Student model using a target image to provide a prediction\n\u2022 First module of static teacher in the target image and relative small decoder\nThe full source code used for our experiments is attached to this document (hamlet code.zip).\n6. More storms and longer adaptation analysis\nWe run HAMLET on three additional rainy scenarios, generated as detailed in [5]. We both evaluate the performance\nof the brand-new adaptation cycle, starting from SegFormer trained on source domain and adapting to the new storms A, B\nand C. Additionally, we test a model previously online adapted on the Increasing Storm scenario and compare the two (Non\nPre-Adapted and Pre-Adapted) in terms of performance and training phases.\nFigure 4 collects, from left to right, the results achieved on Storms A, B, and C as defined in [5]. On top, we plot the rain\nintensity over time faced during the adaptation process, followed by mIoU plots highlighting how the two models introduced\nbefore adapt and the difference in terms of mIoU achieved by the pre-adapted model compared to the brand-new one. On the\nlast row, we show the iterations during which the models are optimized, specifically in gray when both run back-propagation,\nwhile in orange and blue when only the brand-new or the pre-adapted model are optimized, respectively.\nWe notice, similarly to OnDA [5], how HAMLET also benefits from previous adaptation on the Increasing Storm. The\nhighest gain is achieved on Storm C, in which the domain rapidly switches from source to the hardest one, i.e. 200mm.\nMoreover, we can appreciate in general how the pre-adapted model witnessed almost no drop in accuracy on the inactive\ndomains. This is caused by the Active Training Modulation strategy, which limits the amount of adaptation steps performed\nclear\n25mm\n100mm\n200mm\n50mm\n25mm 2\nclear 2\ntotal h-mean\nFPS\nGFLOPS\nFull training\n73.3\n70.6\n68.2\n64.1\n66.1\n70.9\n72.1\n69.2\n4.6\n125.2\nHAMLET (non pre-adapted)\n73.4\n70.0\n67.4\n61.6\n61.5\n68.8\n70.6\n67.3\n20.0\n50.3\nHAMLET (pre-adapted)\n71.9\n70.0\n67.8\n62.5\n63.0\n67.7\n69.9\n67.4\n25.2\n44.9\nTable 5: Experimental results \u2013 Storm B\nclear 1\n200mm\nclear 2\n100mm\nclear 3\n75mm\nclear 4\nclear h-mean\ntarget h-mean\ntotal h-mean\nFPS\nGFLOPS\nFull training\n73.6\n60.1\n73.4\n65.6\n73.0\n68.8\n72.9\n73.2\n64.6\n69.3\n4.5\n125.2\nHAMLET (non pre-adapted)\n73.4\n54.4\n72.7\n64.7\n71.0\n65.7\n71.4\n72.1\n61.1\n67.0\n16.4\n51.9\nHAMLET (pre-adapted)\n71.9\n59.5\n72.9\n65.7\n72.2\n68.6\n72.0\n72.3\n64.4\n68.7\n22.2\n48.3\nTable 6: Experimental results \u2013 Storm C\n0\n6\nintensity\n60\n65\n70\n75\n80\n85\nmIoU\nclear\ncloudy\novercast\nsmall_rain\nmid_rain\nheavy_rain\n0.0\n2.5\n5.0\nlearning rate\n1e\n5\n0\n21217\n35344\n43351\n50797 57172\n64363 70738\n78184\n86191\n100318\n121534\nStep\n0\n20\n40\nFPS\nFigure 5: Experimental results \u2013 SHIFT benchmark. We show mIoU over active (bold) and inactive (dashed) domains,\nlearning rate and FPS.\nby HAMLET to be as few as needed to adapt to the new domains while neglecting the occurrence of catastrophic forgetting\nover inactive domains. Indeed, once adaptation has been performed over the Increasing Storm, it results sufficient to maintain\nhigh accuracy when moving to the new storms, as pointed out by the almost horizontal dashed lines in the pre-adapted plots.\nBy focusing on the last row, we point out how, in most times, the non pre-adapted model runs significantly more optimization\nsteps (orange) compared to the pre-adapted one (blue), which can already deal with the domain switches occurring in these\nstorms. This is due to its prior experience on the Increasing Storm, indeed the domain detection relies both on the static\nlightweight decoder and the student itself. When the student becomes more robust to new domains, also the domain detection\nbecomes more accurate. We also notice how, despite training only a fraction of the iterations, the Non Pre-adapted model can\nstill catch-up with the pre-adapted one, with a delay, even in the hardest transition, i.e. storm C. For a quantitative overview\nof HAMLET performance on the three storms, we collect the results in Tables 4, 5 and 6, respectively for Storms A, B, and\nC. In particular, we point out how the pre-adapted model is more accurate, as well as faster than the non pre-adapted one on\nB and C, since it activates adaptation fewer times as previously discussed with reference to Figure 4.\nTo conclude, HAMLET can benefit from previous adaptation both in terms of accuracy, as well as speed (the fewer the\noptimization steps, the higher the framerate).\n0\n6\nintensity\n0\n21217\n35344\n43351\n50797 57172\n64363 70738\n78184\n86191\n100318\n121534\nStep\n55\n60\n65\n70\n75\nmIoU\nFull training\nHAMLET\nFigure 6: Experimental results \u2013 SHIFT benchmark. We show the mIoU of the heavy rain domain during the training\ncycle for Full training and HAMLET. Bold lines represent when the domain is active and dashed lines when is inactive.\nclean\n50mm\n100mm\n200mm\nFigure 7: Qualitative results \u2013 HAMLET in action. From left to right, we show the same source frame, augmented with\nincreasing rain intensity, respectively clean, 50mm, 100mm and 200m. From top to bottom: input image, prediction by\nSegFormer trained on source and HAMLET.\n7. SHIFT analysis\nWe now dive deeper in HAMLET performance on the SHIFT benchmark. Figure 5 depicts, on top, the rain intensity\ncharacterizing any domain encountered while running HAMLET on SHIFT. Then, we plot the mIoU achieved both on\ncurrent (bold) and inactive (dashed) domains, as done for the Increasing Storm in the main paper and Storms A, B, C in\nthe previous section. Then, we show how the learning rate changes based on the domain shift detection, followed by the\nframerate achieved by HAMLET at any step.\nFrom the mIoU plot, we can notice how the drop in accuracy, even on the hardest domain, is moderate compared to what\nwas observed in OnDA benchmarks [5]. We speculate that this might be caused by the full-synthetic nature of this domain,\nwhich makes the task easier. Interestingly, the performance on small rain and mid rain domains continue to improve even\nafter HAMLET moves to further domains. In general, as previously observed on Storms A, B, C, HAMLET do not experience\nany catastrophic forgetting on the inactive domains that have been faced previously. For what concerns domain shift detection,\nwe can notice how this sometimes occurs with a slight delay \u2013 i.e. clear to cloudy and vice-versa \u2013 or does not occur at all\n\u2013 i.e. overcast to small rain and vice-versa. Nevertheless, once again, this confirms that just a few adaptation steps aligned\nwith the domain shifts are enough to achieve an accuracy comparable to the one obtained with full training, as shown in Tab.\n4 in the main paper.\nThis dataset evaluation offers further insights when it comes to observing another problematic behavior of na\u00a8\u0131ve full\n0\n75\n200\nintensity\n0\n2\n4\nH\n0\n3\nA\n0\n2\nB\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nStep\n0\n1\ntraining\nFigure 8: HAMLET adaptation schedule over the qualitative video sequence. From first to last row we present: rain\nintensity, domain detection signals: H, A, B, training phases (1: training + inference, 0: inference)\ntraining. Indeed, besides being vastly more computationally expensive, training when it is not required, contributes to the\nfutile specialization of specific domains, hence leading to worse generalization on other domains. This is clearly visible in\nFigure 6 when we focus on the performances of our Full Training baseline on the heavy rain domain. During the adaptation\nto clear weather, we notice how evaluating on heavy rain leads to progressively worse performances without achieving any\nsignificant improvement in clear weather either. Despite its ability to eventually adapt, this behavior might raise concerns\nwhen it comes to sudden domain shifts and it hints to potential domain forgetting. To support this, we show in Figure 6 the\naccuracy achieved on the heavy rain domain at any time during adaptation, when being the active (bold) or inactive (dashed)\ndomain, for both SegFormer adapted with full training and HAMLET. We can notice how the full training regime leads to\ndramatic drops in accuracy on this domain when it is inactive, until it is actually encountered. HAMLET, on the contrary,\ncan preserve its original accuracy on the heavy rain, proving that selective adaptation also avoids catastrophic forgetting, to\nwhich full training is prone to.\n8. Qualitatives\nIn Figure 7 we present extensive qualitative examples from the Increasing Storm evaluation set. Figure 7 shows the results\nachieved by the source model and HAMLET on increasing rain intensity. We can notice how the source model, at first, is\nrobust to moderate rain. When moving towards higher intensity, the model gradually starts failing, whereas HAMLET keeps\nhigh accuracy.\n9. Videos\nTo conclude, we attach two qualitative videos to this document. For the first (https://www.youtube.com/watch?\nv=zjxPbCphPDE&t=139s) we emulate a realistic deployment by synthesizing rain over Cityscapes. The domain shift\nsequence follows the same pattern as the Increasing Storm. In this case, we cap the video framerate at 5.88FPS using the\nsame setup of [5]. On this video, we run SegFormer in three main flavors: 1) trained on source domain, 2) adapting using\nCoTTA and 3) adapting with HAMLET. We mainly compare against CoTTA: while HAMLET keeps the pace with the\nconsidered framerate, CoTTA \u2013 which runs at 0.6FPS \u2013 is trained over 1 frame every 10, allowing it to keep the pace with the\n0.0\n2.5\n5.0\nlearning rate\n1e\n5\n0\n2\n4\nB\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\n45000\n50000\n55000\n60000\n65000\n70000\n75000\n80000\n85000\nStep\n0.0\n0.5\n1.0\ntraining\nFigure 9: HAMLET adaptation schedule over the qualitative video sequence. From first to last row we present: learning\nrate schedule, domain detection signal B, training phases (1: training + inference, 0: inference)\nincoming frames. This emulates a realistic behavior during deployment in which all the frames are processed sequentially,\nyet favoring CoTTA \u2013 since a higher framerate, e.g. 30FPS, would require CoTTA to adapt on even fewer frames to keep\nthe pace. The video sequence is unlabelled, so we cannot compute mIoU and thus we can appreciate our results only\nqualitatively, nevertheless, we can provide an overview of the adaptation process operated by HAMLET. Figure 8 sketches\nthe domain sequence, domain shift detections, and relative training intervals, i.e. 1 when training is active and 0 otherwise.\nThe average speed theoretically obtained by HAMLET in this sequence is 20.4FPS, even though the input rate was capped\nat 5.88FPS. It is also interesting to notice how sequential frames and underlying natural domain shifts taking place over the\nvideo are making the adaptation task significantly more challenging than the Increasing Storm benchmark proposed in [5],\nnevertheless, HAMLET manages to identify and activate short training burst in correspondence to the domain shifts, enough\nto vouch for effective adaptation to the new domains encountered.\nIn the second video (https://www.youtube.com/watch?v=zjxPbCphPDE) we showcase HAMLET in action\nin a real environment \u2013 i.e., on the road from Seoul to Daegu, Korea. During the trip, we face several different domain\ntransitions, meeting heavy rain, highway environment, dusk, and even nighttime. This latter qualitative result proves that,\ndespite most experiments in the main paper having been conducted in semi-synthetic datasets, HAMLET is effective on\nreal data as well and can be effectively deployed for real applications. The video shows, on top, the input RGB images\nfrom the sequence being processed, and at the bottom, the results by SegFormer trained on the source domain (left) and\nHAMLET being adapted on the sequence itself (right). First and foremost, we point out how the video itself exposes several\ndomain shifts due to the environment itself \u2013 i.e., SegFormer has been trained on Cityscapes, featuring cities from Germany\nin a mostly urban environment, while the whole video features Korea and transits from urban roads to highways. We can\nappreciate how these domain shifts do not represent a challenge for HAMLET. Then, we observe that rain represents one of\nthe earliest, weather challenges faced in the video, both in the form of small droplets on the glass shield of the car, as well as\nin actual storms met during driving. While the accuracy of the source SegFormer model dramatically drops in these domains,\nHAMLET rapidly copes with them and maintains a much higher quality of the results. In the last part of the video, we\nencounter nighttime domains: despite the much lower brightness in the images and the lower contrast between the different\nregions (e.g., road vs vegetation or cars), HAMLET can still keep the drop in accuracy moderate, while the source SegFormer\nmodel results completely ineffective on such an unseen domain, rarely distinguishing the road from any generic vehicle. Fig.\n9 sketches the domain shift detections and relative learning rate schedules, training intervals, i.e. 1 when training is active\nand 0 otherwise. We can observe how HAMLET can identify several domain shifts and tune the adaptation rate accordingly.\nReferences\n[1] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine\nintelligence, 40(4):834\u2013848, 2017.\n[2] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,\nand Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), June 2016.\n[3] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Daformer: Improving network architectures and training strategies for domain-\nadaptive semantic segmentation. arXiv preprint arXiv:2111.14887, 2021.\n[4] Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, and Lennart Svensson. Classmix: Segmentation-based data augmentation for\nsemi-supervised learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1369\u2013\n1378, 2021.\n[5] Theodoros Panagiotakopoulos, Pier Luigi Dovesi, Linus H\u00a8arenstam-Nielsen, and Matteo Poggi. Online domain adaptation for se-\nmantic segmentation in ever-changing conditions. In European Conference on Computer Vision (ECCV), 2022.\n[6] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. SHIFT: a\nsynthetic driving dataset for continuous multi-task domain adaptation. In Computer Vision and Pattern Recognition, 2022.\n[7] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00b4erez. ADVENT: Adversarial entropy minimization for\ndomain adaptation in semantic segmentation. 2019. 00000.\n[8] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy\nminimization. In International Conference on Learning Representations, 2021.\n[9] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation, 2022.\n[10] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design\nfor semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077\u201312090, 2021.\n"
  },
  {
    "title": "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition",
    "link": "https://arxiv.org/pdf/2307.14535.pdf",
    "upvote": "12",
    "text": "Scaling Up and Distilling Down:\nLanguage-Guided Robot Skill Acquisition\nHuy Ha1, Pete Florence2, and Shuran Song1\n1Columbia University\n2Google DeepMind\nAbstract: We present a framework for robot skill acquisition, which 1) efficiently scale\nup data generation of language-labelled robot data and 2) effectively distills this data down\ninto a robust multi-task language-conditioned visuo-motor policy. For (1), we use a large\nlanguage model (LLM) to guide high-level planning, and sampling-based robot planners\n(e.g. motion or grasp samplers) for generating diverse and rich manipulation trajectories.\nTo robustify this data-collection process, the LLM also infers a code-snippet for the\nsuccess condition of each task, simultaneously enabling the data-collection process to\ndetect failure and retry as well as the automatic labeling of trajectories with success/failure.\nFor (2), we extend the diffusion policy single-task behavior-cloning approach to multi-task\nsettings with language conditioning. Finally, we propose a new multi-task benchmark\nwith 18 tasks across five domains to test long-horizon behavior, common-sense reasoning,\ntool-use, and intuitive physics. We find that our distilled policy successfully learned the\nrobust retrying behavior in its data collection procedure, while improving absolute success\nrates by 33.2% on average across five domains. All code, data, and qualitative policy\nresults are available at our project website.\nFigure 1: Language-guided Skill Acquisition enables scalable robot learning. In the data generation stage, a LLM takes\nas input task descriptions (a) and uses sampling-based robotic planners and privileged simulation information (b) to perform\ntask-directed exploration. This enables the scaling up of language and task-success labeled dataset generation (c). In the\nsecond stage, the dataset is filtered for success and distilled down into a closed-loop language-conditioned visuomotor\npolicy for real world deployment (d).\n1\nIntroduction\nHow can we scalably acquire robust, reusable, real-world manipulation skills? This question has been the driv-\ning force behind extensive research in robot learning. Attempts in the field have focused on two primary aspects:\nFirst, how to scale up the data collection for a diverse range of manipulation skills, which involves efforts\nsuch as improving the hardware [1, 2] and software [3, 4] which support demonstration collection, utilization\nof non-robotics datasets [5, 6], or trial-and-error explorations [7]. The second aspect of this question concerns\neffective learning from the collected data, which delves into exploring effective action representations [8\u201310]\nand policy formulations [11, 12] that can robustly model the training data and generalize to novel scenarios.\nThis paper proposes a new framework that provides a comprehensive solution for both aspects by\nleveraging language guidance, while using no expert demonstrations or reward specification/engineering.\nWe contribute two key components with our framework:\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2307.14535v2  [cs.RO]  1 Oct 2023\n\u2022 Scaling Up Language-Guided Data Generation: Our data-collection policy is a large language model\n(LLM) which has access to a suite of 6DoF exploration primitives (i.e., sampling-based robot planners and\nutilities). Given an input task description, this policy first simplifies the task by recursively decomposing\nit into subtasks, resulting in a hierarchical plan (i.e., task tree). Next, this plan is grounded into a sequence\nof 6DoF exploration primitives, which generates diverse robot trajectories for the task. Finally, the data\ncollection policy verifies the trajectories\u2019 success with an inferred success function and retries the task\nuntil it succeeds. This verify & retry step not only improves the data-collection policy\u2019s success, but also\nadds robot experience on how to recover from failure, an important trait for downstream policy distillation.\nThis data generation approach is scalable, enabling significantly more efficient autonomous task-directed\nexploration than unguided alternatives (i.e., reinforcement learning) while not being limited by the lack\nof low-level understanding of the LLM-only solution.\n\u2022 Distilling Down to Language-Conditioned Visuomotor Policy: We distill these robot experiences into\na visuo-linguo-motor policy that infers control sequences from visual observations and a natural language\ntask description. To enable effective learning of high entropy, diverse robot trajectories, we extend the\ndiffusion policy [12] to handle language-based conditioning for multi-task learning. This allows the learned\npolicy to be reused and recomposed through language-based planners. We found that our distilled policy\nsuccessfully learned the robust retrying behavior from its data collection policy, while improving upon\nits absolute success rate across five domains by 33.2%. Further, we demonstrate that our policy directly\ntransfers to the real-world without fine-tuning using domain randomization.\nOur framework combines these two components to get the best of both worlds \u2013 leverage LLM\u2019s\ncommon-sense reasoning abilities for efficient exploration while learning robust and re-usable 6DoF\nskills for real-world deployment. In summary, the key contribution of this paper is a new framework for\nvisuo-linguo-motor policy learning that is enabled by three novel components:\n\u2022 A new language-guided data collection framework that combines language-based task planner with 6DoF\nrobot utilities (e.g. motion planning, grasp sampling).\n\u2022 New formulation of diffusion-based policy that effectively learns multi-task language-conditioned\nclosed-loop control policies.\n\u2022 In addition to our algorithmic contributions, we also contribute a new multi-task benchmark that includes\n18 tasks across five domains, requiring long-horizon (\u2248 800 control cycles), common sense, tool-use,\nand intuitive physics understanding \u2013 capabilities lacking in existing manipulation benchmarks.\n2\nRelated Works\nScaling visuo-linguo-motor data.\nIn learning vision-and-language-conditioned motor policies for\nreal-world deployment [9, 10, 13\u201318], one of the most important questions is how to scale up \u201crobot-complete\ndata\u201d \u2013 data that has robot sensory inputs (e.g. vision), action labels (e.g. target end-effector & gripper\ncommands), and task labels (e.g. language description, success). The most prevalent paradigm is to use\nhumans to annotate both actions (e.g. teleoperation) and language [9, 10, 13\u201318]. When providing action\nlabels, humans can either provide task-specific [9, 10, 15, 18], or task-agnostic (\u201cplay\u201d) data [13, 14, 16, 19].\nA primary limitation, however, is that data scalability is human-limited.\nOther prior works have proposed strategies to enable more-autonomously-scalable data. To scale language\nannotation, prior works study using visual-language models [20, 21], or procedurally post-hoc provided\nin simulation [19]. To scale action labels, methods study how to use autonomous sub-optimal policies from\nrandom [7] to learned [22] policies. Human egocentric videos [6, 23, 24] has also been shown to be relevant to\nrobot learning [5, 25], but is not robot-complete (lacks action labels), and requires cross-embodiment transfer.\nTowards unsupervised exploration, prior works have also investigated evolving environments [26, 27] and\nembodiments [28], automatic task generation [29], leveraging language guidance [30, 31] and world-model\nerror [32], but have not been demonstrated to scale to 6 DoF robotic skill learning. While these approaches\nreduce human efforts, they are still limited in optimality, generality, and/or completeness of robot data labels.\nAnother option for the autonomous data collection policy is to use a model-based policy, e.g. task and\nmotion planning (TAMP) [33]. Our approach extends such methods in terms of flexibility and task generality\nby leveraging LLM\u2019s common-sense knowledge. However, in contrast to recent works which use LLMs\nas the final policy [34\u201340], we use the LLM-based planner as a suboptimal data-collection policy. We then\n2\nFigure 2: Benchmark. We validate our approach on a new multi-task benchmark addressing challenging long-horizon\ntasks (i.e., 800 control cycles) requiring language understanding (e.g., put [object] to [top] drawer), common sense\nknowledge (e.g., send a package for return requires raising the mailbox flag), tool-use (e.g., catapult), and intuitive physics\n(e.g., balance the bus). The tasks are best viewed on our our project website.\ndistill only successful trajectories into an observable-information [41\u201343] policy, allowing the distilled policy\nto improve upon its LLM data collection policy\u2019s performance.\nPolicy Representations and Multi-task Policy Distillation.\nOne primary question in visuo-motor\nlearning [44] has been how to represent the policy for effective learning, i.e. to enable high precision,\nmulti-modal robot behavior [2, 11, 12, 45, 46]. Another related question has been how to best train multi-task\npolicies [47, 48], including those conditioned on language [9, 10, 13, 15, 16, 18]. Our work presents the\nnovel formulation of bringing diffusion-based [49, 50] policies [12] into the language-conditioned [51, 52]\nvisuomotor domain. Additionally, prior works in multi-task language-conditioning typically focus on\ncloning policies from experts, meanwhile we study distilling data from a success-filtered suboptimal policy.\nSuccess-filtering [11, 53] can be viewed as the simplest form of offline RL [54].\n3\nApproach\nWe propose a new framework for robot learning that performs automatic data collection and policy learning\nfrom only a task description. Our design is grounded on four key observations:\n\u2022 We recognize the importance of random exploration in reinforcement learning, but aim to not be constrained\nby its inefficiency for long-horizon, sparse reward tasks.\n\u2022 We acknowledge the usefulness of LLM\u2019s common-sense and zero-shot capabilities, but believe language\nis not by itself the ideal representation for robust, rich, and precise robotic manipulation.\n\u2022 We are inspired by the effectiveness of robotic planning methods, e.g. TAMP, but wish to be flexible\nto novel tasks and domains and non-reliant on ground truth state during policy inference.\n\u2022 We aim to achieve the simplicity and effectiveness of behavior cloning in distilling collected robot\nexperience into a policy for real-world deployment, while side-stepping the requirement for costly human\ndemonstrations or play data collection.\nUsing no human demonstration or manually specified reward, our framework combines the strengths\nof these four areas into a unified framework for both efficient task-directed exploration and multi-task\nvisuo-linguo-motor policy learning.\nMethod Overview. In the data generation phase, we use an LLM to recursively decompose (\u00a73.1) tasks\ninto a hierachical plan (i.e., task tree) for exploration and ground the plan into sampling-based robot utilities\nand motion primitives (\u00a73.2). Next, the LLM infers success-detection functions for each task in the plan\n(\u00a73.3), providing success-labeling. This autonomous data generation process outputs a replay buffer of\ntask-directed exploration experience, labeled with language descriptions and success labels. In the training\nphase (\u00a73.4), we filter this data for success according to the LLM inferred success condition and distill it\ninto a multi-task vision-and-language-conditioned diffusion policy [12].\n3\nFigure 3: Language-Driven Robot Data Generation takes as input the task description and simulation state, and outputs\na replay buffer, labelled with language descriptions and success. It starts by using an LLM to simplify tasks recursively (a)\nuntil the task involves only one object, resulting in a hierarchical exploration plan. Next, the plan is grounded (b) into a\nsequence of 6 DOF exploration primitives (e.g. grasp samplers, motion planners, etc.) and rolled out in simulation to give\nan unlabelled robot trajectory. Finally, an LLM infers a success function code-snippet, and uses it to verify (c) and label it\nwith succeeded or failed. If the trajectory failed, the LLM retries the exploration plan with a different random seed (e.g. a\ndifferent grasp pose from the grasp sampler). If the robot succeeds or run out of time, the labeled trajectory is returned.\n3.1\nSimplify: Task Planning and Decomposition\nGiven a task description, the first step is to generate a high-level task plan. To improve the flexibility to\nwork with any tasks and 3D assets, we opted for an LLM-based planner to leverage their common-sense and\nzero-shot reasoning skills. Unlike classical TAMP planners, our framework does not require domain-specific\nengineering and transition function design to work with new tasks.\nConcretely, our recursive LLM planner takes as input the task description, the simulation state, and outputs\na plan in the form of a task tree (Fig. 3a). To do so, the LLM first checks whether the task description\ninvolves the robot interacting with multiple or only one object. For instance, \u201cmove the package into the\nmailbox\u201d involves opening the mailbox before picking up the package and putting the mailbox in, and should\nbe considered a multi-object task. Meanwhile, \u201cwith the mailbox opened, move the package into the mailbox\u201d\nshould be a single-object task. For the base case of single-object tasks, we prompt the LLM to which object\npart name to to interact. For the case of multi-object tasks, we prompt the LLM to decompose the task into\nsubtasks, and recurse down each subtask.\n3.2\nGround: Compiling a Plan into Robot Utilities\nWith the generated task tree \u00a73.1, the next step is to ground the high-level plan into physical actions. Here,\nthe choice of the low-level robot API critically defines the system\u2019s capability and, therefore, becomes a\nkey differentiating factor between different systems. In principle, there are three desired properties we want\nto see in the action space design:\n\u2022 Flexibility. Planar actions [10, 37] aren\u2019t flexible enough to manipulate prismatic and revolute joints.\n\u2022 Scalable. Namely, actions should not require human demonstrations to acquire [9, 10, 13\u201316, 35].\n\u2022 Language-friendly. While joint sequences can encode any action, it is not language-friendly.\nWe propose to ground the LLM\u2019s plan with API calls into a set of robot utility functions, which include a\nsampling-based motion planner, a geometry-based grasp and placement sampler, and motion primitives for ar-\nticulated manipulation. We refer to these utilities as 6 DOF Exploration Primitives (Fig 3b) because, by virtue of\nbeing pseudo-random, the sampling-based utilities generate diverse robot trajectories, enabling effective explo-\nration for rich 6 DoF manipulation settings. For instance, our grasp and placement samplers samples uniformly\namongst all points in the object part\u2019s point cloud to find good grasps and placements poses, respectively, which\nare used as input into a rapidly-exploring random trees [55] motion planner that samples uniformly in joint\nspace. This results in diverse grasps, placements, and motion trajectories connecting grasps and placements.\n4\nFor each leaf node in the inferred task tree (\u00a7 3.1), the grounding process takes as input the node\u2019s task de-\nscription (e.g. \u201copen the mailbox\u201d), its associated object part name (e.g. \u201cmailbox lid\u201d), and the simulation state,\nand outputs a sequence of 6 DoF Exploration Primitive API calls. Using the object part name, we can parse\nthe object\u2019s kinematic structure from the simulation state and handle articulated and non-articulated (i.e., rigid,\ndeformable) objects separately. For non-articulated objects, the LLM is prompted to choose the pick & place\nobject names, used to sample grasp and placement pose candidates. For articulated objects (with either revolute\nor prismatic joints), the leaf node\u2019s associated object part name is used to sample a grasp candidate followed\nby a rotation or translation primitive conditioned on its joint parameters (i.e., joint type, axis, and origin).\nExploration Plan Rollout. Each node in the exploration plan is grounded only when it is being executed,\nwhere the order of execution follows a pre-order tree traversal. By keeping track of the subtask\u2019s state,\nsub-segments of robot trajectory can be labelled with the subtask\u2019s description, thereby providing dense and\nautomatic text labels for the trajectory. For instance, all actions taken during the inferred subtask \u201copen the\nmailbox\u201d can be labeled with both the subtask\u2019s description \u201copen the mailbox\u201d and the root task description\n\u201cmove the package into the mailbox\u201d.\nSince grounding happens only when a task node is visited, each node\u2019s grounding process is independent\nof the other leaf nodes, depending only on the simulation state when it is evaluated. While this simplifies\nplanning significantly, it also means that failed execution can occur. For instance, a grasp candidate may\nrender all placement candidates infeasible.\n3.3\nVerify & Retry: Robustifying the Data Collection Policy\nRecall, the planning and grounding step can fail, especially when we consider long-horizon tasks. To address\nthis, we propose a verify & retry (Fig. 3c) scheme, which uses environment feedback to detect failed execution.\nVerify. For each task, the LLM infers a success function code snippet given the task description,\nsimulation state, and API functions to for query simulation state (e.g., checking contact or joint values, etc).\nThis amounts to prompting the LLM to complete a task success function definition that outputs a boolean\nvalue, indicating task success. For instance, given the task \u201craise the mailbox flag\u201d, the LLM\u2019s inferred\ncode snippet should check whether the mailbox\u2019s flag hinge is raised (Fig. 3c, highlighted green).\nRetry. When a trajectory is labeled failed, the robot retries the same sequence of robot utilities with a\ndifferent random seed (i.e., for the sampling-based robotic utilities) without resetting the simulation state\nuntil the task succeeds. For instance, in the bus balance task (Fig. 2, top left), the robot would repeatedly\ntry different grasp and place candidates until the bus is balanced. In the tree traversal process \u00a7 3.2, nodes\nonly yield execution to its parent task when the node\u2019s inferred success condition returns true. This design\nnot only leads to higher success rates in data generation but also provides useful demonstrations on how\nto recover from failure. In the output replay buffer, the only failed trajectories are ones which timed-out\nor led to invalid states (e.g. object dropped on the floor).\n3.4\nLanguage-conditioned Policy Distillation\nFigure 4: Language-Conditioned Policy Distillation. The\npolicy takes as input a task description, two RGB camera\nviews, and gripper proprioception data, and outputs a se-\nquence of gripper poses and closing command.\nWe extend diffusion policy [12], a state-of-the-art ap-\nproach for single-task behavior cloning, to the multi-\ntask domain by adding language-conditioning. This\npolicy takes as input a task description CLIP [56]\nfeature, proprioception history, and visual observa-\ntions, and outputs a sequence of end effector control\ncommands. Following Robomimic [4]\u2019s findings,\nwe use a wrist-mounted view in addition to a global\n(workspace) view to help with tasks requiring precise\nmanipulation. We use their ResNet18-based [57]\nvision encoders, one for each view. We found that using only the latest visual observation along with the full\nobservation horizon of proprioception maintains the policy\u2019s high performance while reducing training time.\nWhen used in conjunction with the DDIM [58] noise scheduler, we found that we could use a 10\u00d7 shorter\ndiffusion process at inference (5 timesteps at inference, 50 timesteps at training) while retaining a comparable\n5\nperformance. Quantitatively, when using a 10 dimensional action space*, our policy can be run at \u224835Hz on\nan NVIDIA RTX3080.\n4\nEvaluation\nDomain Complex\ngeometry\nArtic-\nulation\nCommon\nsense\nTool\nuse\nMulti-\ntask\nLong\nhorizon\nBalance\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nCatapult\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\nTransport\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nMailbox\n\u2717\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\nDrawer\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\nTable 1: Benchmark Suite.\nOur experiments try to validate two questions: 1) Can our data\ngeneration approach efficiently perform task-directed explo-\nration? 2) Can our policy learning approach effectively distill a\nmulti-modal, multi-task dataset into a generalizable and robust visuo-linguo-motor policy?\nOur Benchmark contains 18 tasks across 5 domains (Fig. 2 Tab. 1), with the following properties:\n\u2022 6DoF & articulated manipulation, for deadling with complex object geometry and articulation.\n\u2022 Geometry Generalization. In our bin transport domain, the robot must generalize its bin transport skill to\nunseen object instances, with novel shapes, sizes, and colors.\n\u2022 Intuitive physics. Robots should understand the physical properties of the world and use this knowledge to\nperform tasks. In the bus balance domain, the robot needs to learn the precise grasping and placement to\nbalance a large bus toy on a small block. In the catapult domain, where the block is placed along a catapult\narm determines how far the block will be launched, and, thus, which bin (if any) the block will land in.\n\u2022 Common-sense reasoning & Tool-use. Natural language task description is user-friendly but often\nunder-specifies the task. Common-sense can help to fill in the gaps. In the mailbox domain, given the task\n\u201csend the package for return\u201d, the robot should understand that it not only needs put the package inside, but\nalso raise the mailbox flag to indicate that the package is ready for pickup. In the catapult domain, the robot\nneeds to understand that pressing the catapult\u2019s button will activate the catapult, and that the block needs to\nbe placed on the catapult arm to be launched.\n\u2022 Multi-task conditioning. Given the same visual observations but different task description, the robot\nshould perform different and task-relevant actions. The catapult domain has 3 tasks for three target bins,\nand the drawer domain has 12 tasks.\n\u2022 Long horizon behaviour. Our longest horizon domain, mailbox, takes at least 4 subtasks to complete (open\nthe mailbox, put the package in the mailbox while its opened, close the mailbox, then raise the mailbox\nflag) which can require up to 800 control cycles. In the drawer domain, the robot needs to open the drawer,\nmove the object into the drawer, then close it, which takes about 300 control cycles.\nThe benchmark is built on top of the MuJoCo [3] simulator, using assets from the Google Scanned\ndataset [59, 60]. We use a table-top manipulation set-up with a 6DoF robot arm. The task success in evaluation\nis a manually designed function, instead of LLM generated function used for data collection.\nMetrics. We report the success rates (%) averaged over 200 episodes in Table 2, a task completion efficiency\nplot in Fig. 6, and qualitative results in Fig. 5. If a domain has multiple tasks then we report the average\nperformance of all tasks. We also compare different LLMs in Table 4 (10 samples per task) and investigate\nthe sources of error in our system for the mailbox domain in Table 3 (200 trials per execution).\nData Generation Baselines. Code-as-Policy [37] is a state-of-the-art approach for using an LLM directly\nas a robot policy by making state (e.g. query present objects) and action primitive API calls to a robot. Given\nan LLM-inferred code string, they execute the snippet in an open-loop fashion. Crucially, in their table\ntop manipulation setting, they assume access to planar action primitives. Thus, we introduce the following\nbaselines, which build on top of Code-as-Policy and each other as follows:\n\u2022 LLM-as-Policy (2D): Similar to code-as-policy using planar pick-and-place, but we use ground truth object\nsegmentation instead of their off-the-shelf object detectors [61, 62].\n\u2022 (+) 6 DOF robot utils: Builds on top of the previous baseline by adding access to 6 DOF robot utilities for\ngrasping, placement, motion planning, and articulated manipulation.\n\u2022 (+) Verify & Retry: Adding to the previous baselines, this baseline uses the LLM\u2019s predicted success\ncondition to label trajectories and retry failed ones. Since the robot utilities involve pseudo-random samplers\n(e.g. RRT, grasp sampling), retrying the task means running these samplers again using the pseudo-random\n*3 for position, 6 for rotation using the upper rows of the rotation matrix, and a gripper close command\n6\nFigure 5: High Entropy yet Precise Language-Guided Action Sequences. Running the pseudorandom language-\nconditioned diffusion process with different seeds on the same observations yields language-consistent (a-c, different\ncolors for different task descriptions), high entropy actions when possible (a-f, object grasping, transports, & placements)\nand precise actions when necessary (d, narrow mailbox with large package). Further, domain randomization enables a\nsimulation trained policy (e) to generalize to the real world (f).\nstate and environment state from where failed trajectory left it. Since we use this approach as our data\ngeneration policy, it also serves as an ablation of our approach.\nPolicy Distillation Ablations. We compare against BC-Z [15]\u2019s single-task policies which does not use\nFiLM conditioning (used in their bin emptying and door opening tasks). To understand the effects of our\npolicy learning design decisions in the single-task regime, we fix training time and dataset size (2 days using\nat least 500 successful trajectories), and provide the following ablations:\n\u2022 Action Generation: Instead of using diffusion processes conditioned on the policy input embedding to\ndecode actions, it is typical use multi-layer perceptrons. Following Jang et al. [15], we use one MLP with\ntwo hidden layers and ReLU activations for end effector position, one for the orientation, and another for\ngripper command. This standard policy architecture is deterministic, and is trained with mean-squared error\nloss for pose and binary cross entropy loss for gripper command.\n\u2022 Action Space: Besides our absolute end effector pose action space, Delta-Action and velocity control\nspaces is another popular action space choice [4, 15, 63\u201365]. We also ablate BC-Z\u2019s execution action\nhorizon (Exec) while keeping their original prediction horizon (Pred).\n\u2022 Observation Encoder: All approaches encode images using a ResNet18 [57] architecture. Although the\noriginal architecture was designed with an average pooling layer, its typical for robotic policies to use a\nspatial softmax pooling [44] layer instead.\n\u2022 Data usage: No-Retry trains on successful trajectories generated from the data generation approach without\nVerify & Retry, so it does not observe any recovery behavior.\n4.1\nData Collection Policy Evaluation\nApproach\nPlanar\n6DoF\nAverage\nBalance Catapult Transport Mailbox Drawer\nLLM-as-Policy (2D)\n28.0\n33.3\n21.5\n0.0\n0.0\n27.6\n(+) 6DoF Robot Utils\n5.5\n2.5\n35.0\n0.0\n1.3\n8.8\n(+) Verify & Retry\n45.0\n7.3\n82.0\n3.0\n31.8\n33.8\nDistill No Retry\n67.5\n38.5\n32.5\n0.0\n22.7\n32.2\nDistill Ours\n79.0\n58.3\n80.0\n62.0\n55.8\n67.0\nTable 2: Success Rates (%) for data generation (top) and\ndistillation approaches (bottom) over 200 trials.\n6DoF exploration is critical. First, we verify dif-\nferent approach\u2019s ability to perform and explore\nin 6DoF, which is crucial for general manipula-\ntion. When 6DoF exploration is introduced, we\nfirst observe a drop in the average success rate\nfor simple tasks that could be accomplished with\nplanar actions (Balance, Transport, Tab. 2). How-\never, this ability is critical for exploring complex\ntasks, providing data to improve upon in the later distilling stage. In particular, we observed that 6DoF actions\nare important for grasping diverse objects with complex geometry (Transport, Tab. 2), and manipulating\narticulated objects (Drawer, Mailbox, Tab. 2).\n7\nSubtask\nPlanning Verify Execution\nOpen mailbox\n100\n100\n43.5\nPut package in mailbox 100\n100\n28.5\nRaise mailbox flag\n100\n100\n62.0\nClose mailbox\n100\n100\n94.2\nTable 3: Sources & Propagation of\nError. Accuracy (%) of planning, veri-\nfication, and execution success rate (%)\nfor each mailbox subtask.\nMoreover, 6DoF exploration also helps in diversifying the data\ncollection strategy, which provides the possibility to improve upon\nin the later distilling stage. For example in the catapult domain,\nLLM-as-Policy (2D) is only able to solve one of three possible\ngoals (the closest bin) using a deterministic strategy. However, it\nprovides no useful data for learning the other two goals, making it a\npoor data-collection policy. In contrast, incorporating 6 DOF robot\nutilities achieves lower but non-zero average success rates in all bins\n(16.3%, 3.3%, and 2.2%, full table in appendix), which provide much better exploration data for distillation.\nVerify & Retry always helps. In the verify & retry step, the LLM retries all tasks until they are successful.\nThis simple addition improves performance in all domains, with 2\u00d7, 3\u00d7, 8\u00d7, and 13\u00d7 in transport, catapult,\nbalance, and drawer domains. Without this crucial step, we observe 0.0% success rate in the mailbox domain,\nunderscoring the difficulty of flawlessly executing long sequences of 6 DOF actions, and the importance of\nrecovery after failure.\nModel\nSize\nPlanning\nSuccess\nLLAMA2\n7B\n42.0\n10.0\n13B\n62.0\n48.3\nGPT3\n175B\n82.0\n91.1\nTable 4: LLM Evaluation.\nLanguage Model Scaling. In addition to the final task success, we\nprovide more detailed analysis of planning and success condition inference\naccuracy in Tab. 4. We evaluate on the proprietary GPT3 [66] (175B\ntext-davinci-003) and the open LLAMA2 [67] (7B and 13B). We found\nthat Llama models struggles in complex planning domains because they\ndo not follow instructions provided in the prompts. For instance, in the drawer domain, both models fail to\naccount for drawer opening and closing. However, we observe an upwards trend with respect to Llama model\nsize, with the 13B model outperforming the 7B model by +20.0% and +38.3% in planning and success\nverification accuracy respectively.\n4.2\nDistilled Policy Evaluation\nRobustness In, Robustness Out. By filtering trajectories with LLM\u2019s inferred success condition, distilled\npolicies inherit the robustness of their data collection policies while improving upon success rates (+23.4%\nand +33.2% for no-retry and ours, Tab. 2). Since our distilled policy learned from a robust data collection\npolicy, it also recovers from failures (e.g. failed grasps or placements) and continuously retries a task until it\nsucceeds. Meanwhile, since the no-retry distilled policy learned from a data collection policy which did not\nretry upon failure, it is sensitive and brittle, leading to \u221234.8% lower average success rate across all domains\ncompared to ours (Tab. 2).\nHigh Performance From Diverse Retry Attempts. Plotting how long policies take to solve the bal-\nance task (Fig. 6), we observed that our policy and its data collection policy continuously tries a diverse\nset of grasps and placements after each failed attempt until it succeeds. This results in higher success\nrates as the policy is given more time, and is reflected in their monotonically increasing success rates.\nsuccess by t (%)\ntime t (seconds)\nOurs 79.0%\nLLM-Policy 28.0%\n(2D) \nDistill 67.5%\n(No Retry) \nLLM-Policy (6DoF) 5.5%\nLLM-Policy 45.0%\n(6Dof+Retry) \n80\n60\n40\n20\n0\n20\n40\n60\n80\n100\nFigure 6: Distilled Robustness. Our policy inherits\nrobust recovery from failure behavior from its data\ncollection policy, while improving upon success rate.\nIn contrast, baselines plateau after their first grasp/plate-\nment attempts. This highlights the synergy of two design\ndecisions. First, the verify & retry step (\u00a7 3.3) is crucial\nfor demonstrating retrying behavior, but is by itself insuffi-\ncient if each retrying action is the identical as the previous\none. Instead, opting for a diffusion policy (\u00a7 3.4) for\nlearning from and generating high-entropy, diverse retry\nattempts (Fig 5) is also essential for high performance.\nPolicy Learning Baselines. We investigate policy learn-\ning design decisions on the single-task balance domain,\nand remove language conditioning. While BC-Z found\nspatial softmax hurt their performance and opted for a\nmean pool, we observed using spatial softmax improved\nperformance by +5.0%. Further, we found that switching\nfrom delta to absolute action spaces improved success\nrates +6.5% and +9.5% when using the MLP action decoder and our diffusion action decoder, respectively,\n8\nconfirming Chi et al. [12]\u2019s findings. Lastly, we find that using our pseudo-random diffusion-based action\nencoder consistently outperforms a deterministic MLP action mappings, regardless of other design decisions.\nMethod\nOutput\nInput\nSuccess\nGeneration\nRep. ExecPredPool\nProprio\n(%)\nBC-Z\nFeedForwardDelta\n1\n10 Avg\n\u2717\n0.0\nFeedForwardDelta\n4\n10 Avg\n\u2717\n15.0\nFeedForwardDelta\n8\n10 Avg\n\u2717\n18.5\nOurs\nFeedForwardDelta\n8\n16 Spatial\n\u2713\n29.0\nFeedForwardAbs\n8\n16 Spatial\n\u2713\n35.5\nDiffusion\nDelta\n8\n16 Spatial\n\u2713\n69.5\nDiffusion\nAbs\n8\n16 Avg\n\u2713\n76.5\nDiffusion\nAbs\n8\n16 Spatial\n\u2713\n79.0\nTable 5: Policy Learning Ablations. Ac-\ntion generation using diffusion models [50]\nrobustly outperforms feed-forward models\nacross other policy design decisions.\nSim2Real Transfer. We evaluated a policy trained on do-\nmain randomized synthetic data in a real world transport task\nwith five novel objects (Fig. 5e). Averaging across ten episodes\nper object, our policy achieved 76% success rate, demonstrat-\ning the effectiveness of our approach in Sim2Real transfer.\n4.3\nLimitations\nBy using priviledged simulation state information, the LLM\ncan infer success conditions which uses ground truth contact,\njoint information, and object poses. This means our imple-\nmentation of the data generation phase is limited to simulation\nenvironments, and our policy requires sim2real transfer. Fur-\nther, Our data generation method relies on existing 3D assets and environments, which presents a further\nopportunity for scaling up with assets from 3D generative models or procedural generation. Finally, while\nour approach\u2019s dataset contains text labels and success labels for all subtasks, we have only evaluated its\neffectiveness in learning the root task. Learning from all subtasks and growing a robot\u2019s set of learned,\nreusable sub-skills over time to enable compositional generalization is left for future work.\n5\nConclusion\nWe proposed \u201cScaling Up and Distilling Down\u201d, a framework that combines the strengths of LLMs, sampling-\nbased planners, and policy learning into a single system that automatically generates, labels, and distills\ndiverse robot-complete exploration experience into a multi-task visuo-linguo-motor policy. The distilled policy\ninherits long-horizon behaviour, rich low-level manipulation skills, and robustness from its data collection\npolicy while improving upon performance beyond its training distribution. We believe that this integrated\napproach is a step towards putting robotics on the same scaling trend as that of LLM development while not\ncompromising on the rich low-level control.\nAcknowledgments\nWe would like to thank Cheng Chi, Zeyi Liu, Samir Yitzhak Gadre, Mengda Xu, Zhenjia Xu, Mandi Zhao\nand Dominik Bauer for their helpful feedback and fruitful discussions. This work was supported in part by\nGoogle Research Award, NSF Award #2143601, and #2132519. We would like to thank Google for the\nUR5 robot hardware. The views and conclusions contained herein are those of the authors and should not be\ninterpreted as necessarily representing the official policies, either expressed or implied, of the sponsors.\nReferences\n[1] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closed-loop grasping\nfrom low-cost demonstrations. IEEE Robotics and Automation Letters, 5(3):4978\u20134985, 2020.\n[2] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with\nlow-cost hardware. arXiv preprint arXiv:2304.13705, 2023.\n[3] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012\nIEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012.\ndoi:10.1109/IROS.2012.6386109.\n[4] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and\nR. Mart\u00b4\u0131n-Mart\u00b4\u0131n. What matters in learning from offline human demonstrations for robot manipulation.\nIn arXiv preprint arXiv:2108.03298, 2021.\n[5] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation for\nrobot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n9\n[6] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu,\nX. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\n[7] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven reinforcement\nlearning. arXiv preprint arXiv:2004.07219, 2020.\n[8] J. Wu, X. Sun, A. Zeng, S. Song, J. Lee, S. Rusinkiewicz, and T. Funkhouser. Spatial action maps for\nmobile manipulation. arXiv preprint arXiv:2004.09141, 2020.\n[9] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic manipulation.\nIn Proceedings of the 6th Conference on Robot Learning (CoRL), 2022.\n[10] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In\nProceedings of the 5th Conference on Robot Learning (CoRL), 2021.\n[11] P. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and\nJ. Tompson. Implicit behavioral cloning. Conference on Robot Learning (CoRL), November 2021.\n[12] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy: Visuomotor\npolicy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023.\n[13] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. arXiv\npreprint arXiv:2005.07648, 2020.\n[14] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. Ben Amor. Language-conditioned\nimitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems,\n33:13139\u201313150, 2020.\n[15] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot\ntask generalization with robotic imitation learning. In A. Faust, D. Hsu, and G. Neumann, editors,\nProceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning\nResearch, pages 991\u20131002. PMLR, 08\u201311 Nov 2022. URL https://proceedings.mlr.press/v164/jang22a.\nhtml.\n[16] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence.\nInteractive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.\n[17] O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation learning\nover unstructured data. IEEE Robotics and Automation Letters, 7(4):11205\u201311212, 2022.\n[18] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint\narXiv:2212.06817, 2022.\n[19] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard. Calvin: A benchmark for language-conditioned\npolicy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):\n7327\u20137334, 2022.\n[20] T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson.\nRobotic skill acquisition via instruction augmentation with vision-language models. arXiv preprint\narXiv:2211.11736, 2022.\n[21] J. Zhang, K. Pertsch, J. Zhang, and J. J. Lim. Sprint: Scalable policy pre-training via language instruction\nrelabeling. arXiv preprint arXiv:2306.11886, 2023.\n[22] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn, et al. Learning language-conditioned robot behavior\nfrom offline data and crowd-sourced annotation. In Conference on Robot Learning, pages 1303\u20131315.\nPMLR, 2022.\n10\n[23] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend,\nP. Yianilos, M. Mueller-Freitag, et al. The\u201d something something\u201d video database for learning and\nevaluating visual common sense. In Proceedings of the IEEE international conference on computer\nvision, pages 5842\u20135850, 2017.\n[24] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro,\nT. Perrett, W. Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pages 720\u2013736, 2018.\n[25] A. S. Chen, S. Nair, and C. Finn. Learning generalizable robotic reward functions from\u201d in-the-wild\u201d\nhuman videos. arXiv preprint arXiv:2103.16817, 2021.\n[26] R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (poet): Endlessly\ngenerating increasingly complex and diverse learning environments and their solutions. arXiv preprint\narXiv:1901.01753, 2019.\n[27] M. Jiang, M. Dennis, J. Parker-Holder, J. Foerster, E. Grefenstette, and T. Rockt\u00a8aschel. Replay-guided\nadversarial environment design. Advances in Neural Information Processing Systems, 34:1884\u20131897,\n2021.\n[28] J.-B. Mouret and J. Clune.\nIlluminating search spaces by mapping elites.\narXiv preprint\narXiv:1504.04909, 2015.\n[29] K. Fang, T. Migimatsu, A. Mandlekar, L. Fei-Fei, and J. Bohg. Active task randomization: Learning\nvisuomotor skills for sequential manipulation by proposing feasible and novel tasks. arXiv preprint\narXiv:2211.06134, 2022.\n[30] Y. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta, and J. Andreas. Guiding\npretraining in reinforcement learning with large language models. arXiv preprint arXiv:2302.06692,\n2023.\n[31] S. Mirchandani, S. Karamcheti, and D. Sadigh. Ella: Exploration through learned language abstraction.\nAdvances in Neural Information Processing Systems, 34:29529\u201329540, 2021.\n[32] R. Mendonca, S. Bahl, and D. Pathak. Alan: Autonomously exploring robotic agents in the real world.\narXiv preprint arXiv:2302.06604, 2023.\n[33] C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-P\u00b4erez. Integrated\ntask and motion planning. Annual review of control, robotics, and autonomous systems, 4:265\u2013293,\n2021.\n[34] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. In International Conference on Machine Learning, pages\n9118\u20139147. PMLR, 2022.\n[35] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances.\narXiv preprint arXiv:2204.01691, 2022.\n[36] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\net al. Inner monologue: Embodied reasoning through planning with language models. In 6th Annual\nConference on Robot Learning.\n[37] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies:\nLanguage model programs for embodied control. In arXiv preprint arXiv:2209.07753, 2022.\n[38] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,\nT. Yu, et al. Palm-e: An embodied multimodal language model. ICML, 2023.\n11\n[39] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language instructions\nto feasible plans. arXiv preprint arXiv:2303.12153, 2023.\n[40] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\nProgprompt: Generating situated robot task plans using large language models. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 11523\u201311530. IEEE, 2023.\n[41] A. Agarwal, A. Kumar, J. Malik, and D. Pathak. Legged locomotion in challenging terrains using\negocentric vision, 2022.\n[42] D. Seita, A. Ganapathi, R. Hoque, M. Hwang, E. Cen, A. K. Tanwani, A. Balakrishna, B. Thananjeyan,\nJ. Ichnowski, N. Jamali, K. Yamane, S. Iba, J. F. Canny, and K. Goldberg. Deep imitation learning of\nsequential fabric smoothing policies. CoRR, abs/1910.04854, 2019. URL http://arxiv.org/abs/1910.\n04854.\n[43] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning robust perceptive\nlocomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822, 2022.\n[44] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The\nJournal of Machine Learning Research, 17(1):1334\u20131373, 2016.\n[45] K. Hausman, Y. Chebotar, S. Schaal, G. Sukhatme, and J. J. Lim. Multi-modal imitation learning\nfrom unstructured demonstrations using generative adversarial nets. Advances in neural information\nprocessing systems, 30, 2017.\n[46] N. M. M. Shafiullah, Z. J. Cui, A. Altanzaya, and L. Pinto. Behavior transformers: Cloning k modes\nwith one stone, 2022.\n[47] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark\nand evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages\n1094\u20131100. PMLR, 2020.\n[48] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and\nK. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint\narXiv:2104.08212, 2021.\n[49] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265.\nPMLR, 2015.\n[50] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020.\n[51] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\nB. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\n[52] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with\nlatent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\n[53] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch.\nDecision transformer: Reinforcement learning via sequence modeling. Advances in neural information\nprocessing systems, 34:15084\u201315097, 2021.\n[54] S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review, and\nperspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[55] S. M. LaValle et al. Rapidly-exploring random trees: A new tool for path planning.\n12\n[56] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al. Learning transferable visual models from natural language supervision. In International\nConference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n[57] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n[58] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International Conference on\nLearning Representations.\n[59] L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and\nV. Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items, 2022.\nURL https://arxiv.org/abs/2204.11918.\n[60] K. Zakka. Scanned Objects MuJoCo Models, 7 2022. URL https://github.com/kevinzakka/mujoco\nscanned objects.\n[61] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion. Mdetr-modulated detection for\nend-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 1780\u20131790, 2021.\n[62] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language\nknowledge distillation. arXiv preprint arXiv:2104.13921, 2021.\n[63] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and P. Abbeel. Deep imitation learning\nfor complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference\non Robotics and Automation (ICRA), pages 5628\u20135635. IEEE, 2018.\n[64] P. Florence, L. Manuelli, and R. Tedrake. Self-supervised correspondence in visuomotor policy learning.\nIEEE Robotics and Automation Letters, 5(2):492\u2013499, 2019.\n[65] A. Mandlekar, F. Ramos, B. Boots, S. Savarese, L. Fei-Fei, A. Garg, and D. Fox. Iris: Implicit\nreinforcement without interaction at scale for learning control from offline robot manipulation data. In\n2020 IEEE International Conference on Robotics and Automation (ICRA), pages 4414\u20134420. IEEE,\n2020.\n[66] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020.\n[67] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhar-\ngava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023.\n13\nA\nPolicy Rollout Visualizations\nOur policy\u2019s 6DoF manipulation behavior is best visualized through videos. Please visit our project website to\nview the videos.\nB\nLLM Prompts\nBelow, we include all prompts used in our approach. We use the same LLM pipeline and prompts in all\ndomains and tasks. We first outline the rationale behind our design of the LLM pipeline (\u00a7 B.1). Next, we\ndescribe in detail the LLM modules and how they are used in the data generation stage (\u00a7 B.2), summarize the\ngeneral prompt structure (\u00a7 B.3), and outline the API supplied to the LLM for success condition inference\n(\u00a7 B.4). Finally, we show some examples of LLM completions (\u00a7 B.5).\nIn all of our experiments, we use GPT3 (text-davinci-003) with temperature 0.0.\nB.1\nLLM Pipeline Design\nOur LLM pipeline is factorized into multiple LLM modules, allowing each module\u2019s prompt to speciallize in a\nsmall reasoning skill (e.g. one set of prompts for deciding whether a task involves a single or multiple objects).\nWe found that this not only improves the LLM\u2019s performance, but also makes designing and maintaining\nprompts easy. For instance, during development, if the LLM outputs an unexpected task tree, the error could\nbe traced back to a single module, and only that module\u2019s prompt needs to be updated. Another convenient\nfeature of this approach is that it also saves on token usage. Since each module\u2019s task is small (e.g. answer\nonly \u201cone\u201d or \u201cmultiple\u201d), the amount of completion tokens is significantly smaller than a monolithic prompt.\nFurther, when a module\u2019s prompt is updated, only that module\u2019s outputs needs to be updated, allowing\ncost-effective approaches to cache-ing LLM\u2019s completions.\nB.2\nLLM Pipeline\nThe recursive LLM-based planner starts with an ambiguous task description handler (Listing 1), which\ntransforms ambiguous task descriptions such as \u201cmove the block onto the catapult then shoot the block into\nthe furthest bin\u201d into more specific task descriptions like \u201cmove the block onto the catapult then shoot the\nblock into the furthest bin by pressing the catapult\u2019s button\u201d. While this handler\u2019s task can occasionally\noverlap with the LLM planner\u2019s task, we found that it was more effective to keep them separate.\nNext, given a un-ambiguous task description, the LLM planner first decides whether the planning step\nis necessary by checking whether the task involves touching only a single object or requiring further\ndecomposition (Listing 2). If the task involves multiple objects, it proceeds with planning (explained in the\nnext paragraph). If the task involves only one object part, an LLM identifies which object part name it should\ninteract with (Listing 3). If the object part name is a single-link rigid object, the LLM is asked for which\nobject it should move (the pick object part) and where (the place object part) using the prompt in Listing 4.\nIn the planning step, the LLM planner outputs a list of subtasks (Listing 5). Given the recursive nature of\nthis planning module, parent tasks also need to keep track of and propagate the current state of the environment\nto child tasks. For instance, the \u201copen the fridge\u201d subtask should be followed with \u201cwith the fridge door\nopened, move the eggs from the fridge ...\u201d, such that the recursive call for moving the eggs knows it does not\nneed to open the fridge door again.\nAfter it has inferred the full task tree, the LLM also infers a success condition for every task in the task tree\n(Listing 6) in the form of a code-snippet. Similar to [37], we inform the LLM which state API utilities are\navailable for its usage by including import statements at the top of the file and demonstrating how they are\nused in the examples.\nB.3\nPrompt Structure\nAll prompts start with instructions to explain to the LLM what the task is (e.g. \u201cgiven an input task description,\nthe goal is to output a list of subtasks ..\u201d), followed by a few \u201cshots\u201d of examples, separated by a \u201c#\u201d symbol\n14\n(in text-based prompts) or a multi-line comment (in code-based prompts). Each shot starts with a structured\ntext encoding of the scene\u2019s object\u2019s and their parts\u2019 names in the form of a bullet list. In the planning, success\ncondition inference, single-or-multiple , pick-and-place, and ambiguous task description LLM tasks, we found\nthat it was helpful to encourage the LLM to output its reasoning (either with an explicit \u201creasoning:\u201d field or\nthrough in-line code comments). In contrast, we found the object part identifier task to be more effective\nwithout this explicit reasoning field.\nB.4\nAPIs for Success Condition Code Generation\nAll functions take as the first argument the simulation state, which contains information on object and part\nnames, kinematic structure, contact, all degrees of freedom, and collision meshes.\nContact.\nThis function takes as input two object (part) names, and returns whether they (or any of their\nparts) are in contact.\nActivation.\nA pair of functions, check activated and check deactivated, take as input an\nobject part name and checks whether the revolute/prismatic joint connecting the object part to its parent\nlink are near their maximum or minimum values, respectively. This is useful for checking whether a lid is\nopened/closed or a button is pressed/released.\nSpatial Relations.\nWe provide two spatial relations, check on top of and check inside, which\ntakes two object (part) names and returns whether the first object (part) is on top of the second object (part)\nor inside the second object (part), respectively. An object is on top of another if they are in contact and the\ncontact normal\u2019s dot product with the up direction is greater than 0.99. An object is inside a container if\nthat the intersection of that object\u2019s axis-aligned bounding boxes with the container\u2019s axis-aligned bounding\nboxes is at least 75% of the object\u2019s axis-aligned bounding box\u2019s volume. This axis-aligned bounding box\ninformation can be parsed from the collision checker of most physics simulators.\nListing 1: Ambiguous task description handler\u2019s prompts\n1\ninstructions:\n2\ngiven an input task description, the goal is rephrase the task such that it is not ambiguous.\n3\nif the task is already specific enough, just return the original task description.\n4\nbelow are some examples:\n5\n#\n6\ntask: stack the blocks on top of each other\n7\nscene:\n8\n- navy block\n9\n- maroon block\n10\n- violet block\n11\nreasoning: the block stacking order is ambiguous. we can specify which block should be placed\non which, in which order.\n12\nanswer: move the maroon block onto the navy block, and the violet block on the maroon block.\n13\n#\n14\ntask: move the lilac block onto the brown block\n15\nscene:\n16\n- brown block\n17\n- lilac block\n18\n- yellow block\n19\nreasoning: the blocks to interact with are fully specified, so just return the original task\ndescription.\n20\nanswer: move the lilac block onto the brown block.\n21\n#\n22\ntask: sort the blocks based on their color\u2019s temperature onto corresponding plates\n23\nscene:\n24\n- red block\n25\n- orange block\n26\n- blue block\n27\n- purple block\n28\n- red plate\n29\n- blue plate\n30\nreasoning: which blocks and plates belong to the same color temperature group are ambiguous.\nwe can specify exactly which blocks should be placed on which plate.\n31\nanswer: move the red and orange blocks onto the red plate, and the purple and blue blocks onto\nthe blue plate.\n32\n#\n33\ntask: open the jar\n34\nscene:\n35\n- jar\n36\n+ jar lid\n15\n37\nreasoning: opening a jar is a primitive action and is fully specified, so just return the\noriginal task description.\n38\nanswer: open the jar.\n39\n#\n40\ntask: close the second drawer\n41\nscene:\n42\n- drawer\n43\n+ first drawer\n44\n+ first drawer handle\n45\n+ second drawer\n46\n+ second drawer handle\n47\n+ third drawer\n48\n+ third drawer handle\n49\nreasoning: closing the second drawer is a primitive action towards a specific drawer, so just\nreturn the original task description.\n50\nanswer: close the second drawer.\n51\n#\n52\ntask: move the ingredients for the omelette onto the kitchen counter\n53\nscene:\n54\n- kitchen counter\n55\n+ cupboard\n56\n+ cupboard door\n57\n+ cupboard door handle\n58\n+ salt\n59\n+ pepper\n60\n- fridge\n61\n+ fridge door\n62\n+ fridge door handle\n63\n+ fridge top shelf\n64\n+ eggs\n65\n+ butter\n66\n+ cheese\n67\n+ milk\n68\n+ fridge bottom shelf\n69\n+ mushrooms\n70\n+ broccoli\n71\n+ freezer\n72\n+ lamb shank\n73\n+ trader joe\u2019s dumplings\n74\n+ tilapia fillet\n75\nreasoning: which ingredients belong to the omelette is ambiguous. we can specify exactly which\nitems to take out of the fridge.\n76\nanswer: move the eggs, butter, cheese, and mushrooms onto the kitchen counter and the salt and\npepper onto the kitchen counter.\n77\n#\n78\ntask: open the fridge, move the cheese onto the kitchen counter, and then close the fridge.\n79\nscene:\n80\n- kitchen counter\n81\n+ cupboard\n82\n+ cupboard door\n83\n+ cupboard door handle\n84\n+ salt\n85\n+ pepper\n86\n- fridge\n87\n+ fridge door\n88\n+ fridge door handle\n89\n+ fridge top shelf\n90\n+ eggs\n91\n+ butter\n92\n+ cheese\n93\n+ milk\n94\n+ fridge bottom shelf\n95\n+ mushrooms\n96\n+ broccoli\n97\n+ freezer\n98\n+ lamb shank\n99\n+ trader joe\u2019s dumplings\n100\n+ tilapia fillet\n101\nreasoning: which actions to perform and in which order is fully specified, so just return the\noriginal task description.\n102\nanswer: open the fridge, move the cheese onto the kitchen counter, and then close the fridge.\nListing 2: One-or-Multiple module\u2019s prompts\n1\ninstructions:\n2\ngiven an input task description, the goal is to classify whether performing the task will\ninvolve touching only \"one\" object or \"multiple\" objects.\n3\nall objects start in a de-activated state (e.g., doors, drawers, cabinets, cupboards, and\nother objects with doors are closed, lights are off, etc.) unless specified otherwise (e.\ng., with the door opened).\n4\nafter performing the task, objects should be reset to their de-activated state if relevant.\n5\nbelow are some examples:\n6\n#\n7\ntask: move the blue block onto the plate\n16\n8\nscene:\n9\n- green block\n10\n- blue block\n11\n- red block\n12\n- plate\n13\nreasoning: \"moving the blue block onto the plate\" involves two objects, the blue block and the\nplate. moving the blue block requires touching it. the plate does not have any\nactivation state, so does not need to be touched.\n14\nanswer: one.\n15\n#\n16\ntask: stack the blocks on the plate\n17\nscene:\n18\n- green block\n19\n- plate\n20\n- red block\n21\n- blue block\n22\nreasoning: \"stack the blocks\" can be decomposed into moving the red block onto the plate,\nmoving the green block onto the red block, and moving the blue block onto the green block\n. performing these steps involve touching multiple blocks.\n23\nanswer: multiple.\n24\n#\n25\ntask: with the red block on the plate and the orange block on the red block, move the green\nblock onto the pink block\n26\nscene:\n27\n- orange block\n28\n- pink block\n29\n- plate\n30\n- green block\n31\n- red block\n32\nreasoning: \"moving the green block onto the pink block\" involves two objects, the green block\nand the pink block. moving the green block requires touching it. the pink block does not\nhave any activation state, so does not need to be touched.\n33\nanswer: one.\n34\n#\n35\ntask: move the lasagna into the microwave\n36\nscene:\n37\n- microwave\n38\n+ microwave door\n39\n+ microwave door handle\n40\n- kitchen counter\n41\n- fridge\n42\n+ fridge door\n43\n+ fridge door handle\n44\n- lasagna\n45\nreasoning: \"moving the pasta into the microwave\" involves only two objects, the lasagna and\nthe microwave. however, it is not a primitive task because the microwave has a door (\nactivation state), but it starts off being closed (de-activated). opening the microwave\ninvolves touching the microwave.\n46\nanswer: multiple.\n47\n#\n48\ntask: with the microwave opened, move the pasta into the microwave\n49\nscene:\n50\n- microwave\n51\n+ microwave door\n52\n+ microwave door handle\n53\n- kitchen counter\n54\n- fridge\n55\n+ fridge door\n56\n+ fridge door handle\n57\n- pasta\n58\nreasoning: \"moving the pasta into the microwave\" involves two objects, the pasta and the\nmicrowave. the microwave\u2019s door needs to be opened (activation state), but it is already\nopened. since the task asserts that the microwave is opened, it also does not need to be\nclosed afterwards. this means performing the task does not involve touching the microwave\n.\n59\nanswer: multiple.\n60\n#\n61\ntask: open the microwave\n62\nscene:\n63\n- fridge\n64\n+ fridge door\n65\n+ fridge door handle\n66\n- dumplings\n67\n- microwave\n68\n+ microwave door\n69\n+ microwave door handle\n70\n- kitchen counter\n71\nreasoning: \"opening the microwave\" is a primitive task. it involves only one object, the\nmicrowave.\n72\nanswer: one.\n73\n#\n74\ntask: with the microwave opened and the sandwich in the microwave, close the microwave\n75\nscene:\n76\n- fridge\n77\n+ fridge door\n17\n78\n+ fridge door handle\n79\n- sandwich\n80\n- microwave\n81\n+ microwave door\n82\n+ microwave door handle\n83\n- kitchen counter\n84\nreasoning: \"closing the microwave\" is a primitive task. it involves only one object, the\nmicrowave.\n85\nanswer: one.\nListing 3: Object part identifier\u2019s prompts\n1\ninstructions: given an input task description, the goal is to identify which object part from\nthe scene to interact with.\n2\n3\nbelow are some examples:\n4\n#\n5\ntask: stack the blue block on the plate\n6\nscene:\n7\n- red block\n8\n- blue block\n9\n- green block\n10\n- plate\n11\nanswer: blue block.\n12\n#\n13\ntask: with the red block on the plate, stack the green block on the red block\n14\nscene:\n15\n- red block\n16\n- blue block\n17\n- green block\n18\n- plate\n19\nanswer: green block.\n20\n#\n21\ntask: turn on the lights\n22\nscene:\n23\n- light switch\n24\n- ceiling light\n25\n- wall\n26\nanswer: light switch.\n27\n#\n28\ntask: open the microwave\n29\nscene:\n30\n- microwave\n31\n+ microwave door\n32\n+ microwave door handle\n33\n+ microwave start button\n34\n+ microwave plate\n35\n- kitchen counter\n36\n+ cupboard\n37\n+ cupboard door\n38\n+ cupboard door handle\n39\nanswer: microwave door handle.\n40\n#\n41\ntask: with microwave opened and the lasagna on the kitchen counter, move the lasagna into the\nmicrowave\n42\nscene:\n43\n- kitchen counter\n44\n+ cupboard\n45\n+ cupboard door\n46\n+ cupboard door handle\n47\n- fridge\n48\n+ fridge door\n49\n+ fridge door handle\n50\n+ fridge top shelf\n51\n+ fridge bottom shelf\n52\n+ freezer\n53\n- lasagna\n54\n- microwave\n55\n+ microwave door\n56\n+ microwave door handle\n57\n+ microwave start button\n58\n+ microwave plate\n59\nanswer: lasagna.\n60\n#\n61\ntask: with the fridge door opened, open the cupboard\n62\nscene:\n63\n- microwave\n64\n+ microwave door\n65\n+ microwave door handle\n66\n+ microwave start button\n67\n+ microwave plate\n68\n- kitchen counter\n69\n+ cupboard\n70\n+ cupboard door\n18\n71\n+ cupboard door handle\n72\n- fridge\n73\n+ fridge door\n74\n+ fridge door handle\n75\n+ fridge top shelf\n76\n+ fridge bottom shelf\n77\n+ freezer\n78\n- lasagna\n79\nanswer: cupboard door handle.\nListing 4: Pick & place handler\u2019s prompts\n1\ninstructions: given an input pick and place description, the goal is to identify which object\nto pick and where to place among the objects listed in the scene.\n2\n3\nbelow are some examples:\n4\n#\n5\ntask: move the blue block on the plate\n6\nscene:\n7\n- red block\n8\n- blue block\n9\n- green block\n10\n- plate\n11\npick: blue block.\n12\nplace: plate.\n13\n#\n14\ntask: with the red block on the plate, move the green block to the top of the red block\n15\nscene:\n16\n- red block\n17\n- blue block\n18\n- green block\n19\n- plate\n20\npick: green block.\n21\nplace: red block.\n22\n#\n23\ntask: with microwave opened and the lasagna on the kitchen counter, move the lasagna into the\nmicrowave\n24\nscene:\n25\n- kitchen counter\n26\n+ cupboard\n27\n+ cupboard door\n28\n+ cupboard door handle\n29\n- fridge\n30\n+ fridge door\n31\n+ fridge door handle\n32\n+ fridge top shelf\n33\n+ fridge bottom shelf\n34\n+ freezer\n35\n- lasagna\n36\n- microwave\n37\n+ microwave door\n38\n+ microwave door handle\n39\n+ microwave start button\n40\n+ microwave plate\n41\npick: lasagna.\n42\nplace: microwave plate.\nListing 5: Planning module\u2019s prompts\n1\ninstructions: given a input task description, the goal is to output a list of subtasks, which,\nwhen performed in sequence would solve the input task. all objects start in a de-\nactivated state (e.g., doors, drawers, cabinets, cupboards, and other objects with doors\nare closed, lights are off, etc.) unless specified otherwise (e.g., with the door opened)\n. after performing the task, objects should be reset to their de-activated state if\npossible. below are some examples:\n2\n#\n3\ntask: move the red block onto the plate, the blue block onto the red block, and the green\nblock on the blue block\n4\nscene:\n5\n- red block\n6\n- blue block\n7\n- green block\n8\n- plate\n9\nreasoning: no objects have activation states. the blocks can be directly placed onto the\nplates.\n10\nanswer:\n11\n- 1. move the red block onto the plate\n12\n- 2. with the red block on the plate, move the blue block onto the red block\n13\n- 3. with the red block on the plate and the blue block on the red block, move the green\nblock onto the blue block\n14\n#\n15\ntask: move the eggs, salt, and pepper onto the kitchen counter\n16\nscene:\n19\n17\n- kitchen counter\n18\n+ cupboard\n19\n+ cupboard door\n20\n+ cupboard door handle\n21\n+ salt\n22\n+ pepper\n23\n- fridge\n24\n+ fridge door\n25\n+ fridge door handle\n26\n+ fridge top shelf\n27\n+ eggs\n28\n+ butter\n29\n+ cheese\n30\n+ milk\n31\n+ fridge bottom shelf\n32\n+ freezer door\n33\n+ freezer door handle\n34\nreasoning: the fridge and cupboard has doors (activation states) which start off closed (de-\nactivated). they need to be opened before objects can be taken out of them. after the\ntask is done, they need to be closed (reset).\n35\nanswer:\n36\n- 1. open the fridge\n37\n- 2. with the fridge door opened, move the eggs from the fridge onto the kitchen counter\n38\n- 3. with the eggs on the kitchen counter, close the fridge\n39\n- 4. with the eggs on the kitchen counter, open the cupboard\n40\n- 5. with the eggs on the kitchen counter and the cupboard door opened, move the salt onto\nthe kitchen counter\n41\n- 6. with the eggs and salt on the kitchen counter and the cupboard door opened, move the\npepper onto the kitchen counter\n42\n- 7. with the eggs, salt, and pepper on the kitchen counter, close the cupboard door\n43\n#\n44\ntask: with the fridge door opened, move the eggs, salt, and pepper onto the kitchen counter\n45\nscene:\n46\n- kitchen counter\n47\n+ cupboard\n48\n+ cupboard door\n49\n+ cupboard door handle\n50\n+ salt\n51\n+ pepper\n52\n- fridge\n53\n+ fridge door\n54\n+ fridge door handle\n55\n+ fridge top shelf\n56\n+ eggs\n57\n+ butter\n58\n+ cheese\n59\n+ milk\n60\n+ fridge bottom shelf\n61\n+ freezer door\n62\n+ freezer door handle\n63\nreasoning: the fridge and cupboard has doors (activation states). the fridge\u2019s door is already\nopened (activated) and so don\u2019t need to be reset. the cupboard\u2019s door starts off closed\n(de-activated) but needs to be opened before objects can be taken out of it. after the\ntask is done, the cupboard need to be closed (reset).\n64\nanswer:\n65\n- 1. with the fridge door opened, move the eggs from the fridge onto the kitchen counter\n66\n- 2. with the fridge door opened and the eggs on the kitchen counter, open the cupboard\n67\n- 3. with the fridge door opened, the eggs on the kitchen counter, and the cupboard door\nopened, move the salt onto the kitchen counter\n68\n- 4. with the fridge door opened, the eggs and salt on the kitchen counter, and the cupboard\ndoor opened, move the pepper onto the kitchen counter\n69\n- 5. with the fridge door opened, the eggs, salt, and pepper on the kitchen counter, close\nthe cupboard door\nListing 6: Success Condition Inference module\u2019s prompts\n1\nfrom utils import (\n2\ncheck_contact,\n3\ncheck_activated,\n4\ncheck_deactivated,\n5\ncheck_inside,\n6\ncheck_on_top_of,\n7\nEnvState,\n8\n)\n9\n10\n11\n\"\"\"\n12\ninstructions:\n13\ngiven a input task description, the goal is to output the success condition for\n14\nthat task. unless otherwise specified, all objects start in a de-activated state\n15\n(e.g., doors, drawers, cabinets, cupboards, and other containers are closed,\n16\nlights are off, etc.) unless specified otherwise (e.g., with the door opened).\n17\nafter performing the task, objects should be reset to original state if possible.\n18\n\"\"\"\n20\n19\n20\n21\n# robot task: touch the apple\n22\n# scene:\n23\n# - apple\n24\n#\n+ apple body\n25\n#\n+ apple stem\n26\ndef touching_apple(init_state: EnvState, final_state: EnvState):\n27\nreturn check_contact(\n28\nfinal_state, \"robotiq left finger\", \"apple body\"\n29\n) and check_contact(final_state, \"robotiq right finger\", \"apple body\")\n30\n31\n32\n# robot task: release the cup\n33\n# scene:\n34\n# - cup\n35\n#\n+ cup body\n36\n#\n+ cup handle\n37\ndef released_cup(init_state: EnvState, final_state: EnvState):\n38\nfinally_touching_cup = check_contact(\n39\nfinal_state, \"robotiq left finger\", \"cup handle\"\n40\n) and check_contact(final_state, \"robotiq right finger\", \"cup handle\")\n41\nfinally_released_cup = (not finally_touching_cup) and (\n42\nnot final_state.gripper_command\n43\n)\n44\nreturn finally_released_cup\n45\n46\n47\n# robot task: move the milk carton into the shelf\n48\n# scene:\n49\n# - milk carton\n50\n# - coke can\n51\n# - shelf\n52\ndef milk_carton_is_on_shelf(init_state: EnvState, final_state: EnvState):\n53\nreturn check_on_top_of(final_state, \"milk carton\", \"shelf\")\n54\n55\n56\n# robot task: move the milk carton from the shelf\n57\n# scene:\n58\n# - milk carton\n59\n# - coke can\n60\n# - shelf\n61\ndef milk_carton_is_not_on_shelf(init_state: EnvState, final_state: EnvState):\n62\nreturn not check_on_top_of(final_state, \"milk carton\", \"shelf\")\n63\n64\n65\n# robot task: open the washing machine\n66\n# scene:\n67\n# - washing machine\n68\n#\n+ washing machine door\n69\n#\n+ washing machine door handle\n70\n#\n+ control panel\n71\n#\n+ on off button\n72\ndef washing_machine_opened(init_state: EnvState, final_state: EnvState):\n73\nreturn check_activated(final_state, \"washing machine door\")\n74\n75\n76\n# robot task: move the sock into the washing machine\n77\n# scene:\n78\n# - washing machine\n79\n#\n+ washing machine door\n80\n#\n+ washing machine door handle\n81\n#\n+ control panel\n82\n#\n+ on off button\n83\n# - sock\n84\ndef sock_inside_washing_machine(init_state: EnvState, final_state: EnvState):\n85\n# the washing machine can be opened (activated state) or closed (de-activated\n86\n# state). since its activation state was not specified, the washing machine starts\n87\n# off closed. therefore, it needs to be closed after the sock is moved inside.\n88\nsock_inside_washing_machine = check_inside(final_state, \"sock\", \"washing machine\")\n89\nwashing_machine_door_closed = check_deactivated(final_state, \"washing machine door\")\n90\nreturn sock_inside_washing_machine and washing_machine_door_closed\n91\n92\n93\n# robot task: with the washing machine opened, move the sock into the washing machine\n94\n# scene:\n95\n# - washing machine\n96\n#\n+ washing machine door\n97\n#\n+ washing machine door handle\n98\n#\n+ control panel\n99\n#\n+ on off button\n100\n# - sock\n101\ndef sock_inside_washing_machine_with_washing_machine_opened(\n102\ninit_state: EnvState, final_state: EnvState\n103\n):\n21\n104\n# the washing machine can be opened (activated state) or closed (de-activated\n105\n# state). the washing machine starts off opened, so it does not need to be closed\n106\n# after the sock is moved inside.\n107\nsock_inside_washing_machine = check_inside(final_state, \"sock\", \"washing machine\")\n108\nreturn sock_inside_washing_machine\nB.5\nExample Completions\nMost modules are designed such that their answers are multi-choice (e.g. single or multiple). We show\nexamples of more interesting open-ended completions for the ambiguous task description handler, planning,\nand success condition inference modules. The LLM\u2019s outputs are highlighted in blue.\nB.5.1\nAmbiguous Task Description Handler\ntask:\nmove the block onto the catapult arm, then press the button to shoot the block into the\nclosest box\nscene:\n- catapult\n+ button\n+ catapult arm\n- yellow block\n- closest box\n- furthest box\n- middle box\n- table\nreasoning:\nwhich block to move onto the catapult arm is ambiguous.\nwe can specify exactly\nwhich block to move onto the catapult arm.\nanswer:\nmove the yellow block onto the catapult arm, then press the button to shoot the block\ninto the closest box.\ntask:\nsend the amazon package for return\nscene:\n- mailbox\n+ mailbox lid\n+ mailbox lid handle\n+ mailbox flag\n- amazon package\n- table\nreasoning:\nwhich actions to perform and in which order is ambiguous.\nwe can specify exactly\nwhich actions to take.\nanswer:\nopen the mailbox lid, place the amazon package inside the mailbox, raise the mailbox\nflag, and then close the mailbox lid.\nB.5.2\nPlanning\ntask:\nmove the yellow block onto the catapult arm, then press the button to shoot the block\ninto the closest box\nscene:\n- catapult\n+ button\n+ catapult arm\n- yellow block\n- closest box\n- furthest box\n- middle box\n- table\nreasoning:\nthe catapult has a button (activation state) which starts off de-activated.\nit\nneeds to be pressed to shoot the block.\nafter the task is done, the button should be reset to\nits de-activated state.\nanswer:\n- 1.\nmove the yellow block onto the catapult arm\n- 2.\nwith the yellow block on the catapult arm, press the button to shoot the block into the\nclosest box\n- 3.\nwith the yellow block in the closest box, reset the button to its de-activated state\n22\ntask:\nopen the mailbox lid, place the amazon package inside the mailbox, raise the mailbox\nflag, and then close the mailbox lid\nscene:\n- mailbox\n+ mailbox lid\n+ mailbox lid handle\n+ mailbox flag\n- amazon package\n- table\nreasoning:\nthe mailbox lid has an activation state (closed/de-activated).\nit needs to be\nopened before the package can be placed inside.\nafter the task is done, the lid needs to be\nclosed (reset).\nanswer:\n- 1.\nopen the mailbox lid\n- 2.\nwith the mailbox lid opened, move the amazon package from the table into the mailbox\n- 3.\nwith the amazon package in the mailbox, raise the mailbox flag\n- 4.\nwith the amazon package in the mailbox and the mailbox flag raised, close the mailbox lid\nB.5.3\nSuccess Condition Inference\n# robot task:\nmove the yellow block onto the catapult arm\n# scene:\n# - catapult\n#\n+ button\n#\n+ catapult arm\n# - yellow block\n# - closest box\n# - furthest box\n# - middle box\n# - table\ndef yellow_block_on_catapult_arm(init_state:\nEnvState, final_state:\nEnvState):\nreturn check on top of(final_state, \"yellow block\", \"catapult arm\")\n# robot task:\nopen the mailbox lid\n# scene:\n# - mailbox\n#\n+ mailbox lid\n#\n+ mailbox lid handle\n#\n+ mailbox flag\n# - amazon package\n# - table\ndef mailbox_lid_opened(init_state:\nEnvState, final_state:\nEnvState):\nreturn check_activated(final_state, \"mailbox lid\")\nC\nTraining & Data Details.\nC.1\nData Generation\nOur data-collection policy uses the 6DoF Exploration Primitives with the Verify & Retry step. For each\ndomain, we run data generation until we get at least 500 successful trajectories per task. Although this can be\ncostly when tasks are long horizon with low success rates (the mailbox domain took 2 days on 256 CPU cores\nIntel Xeon Gold 6230R CPU @ 2.10GHz), data generation happens only once.\nC.2\nNetwork Architecture & Hyperparameters\nWe use the same network architecture and hyperparameters for all domains. Our task descriptions are encoded\nusing CLIP B/32\u2019s text encoder [56], and projected into a 512-dimensional vector. For each of the two camera\nview, we learn a separate Resnet18-based [4] vision encoder, whose features are flattened, concatenated, and\nprojected into a 512-dimensional vector. The Resnet18 architecture is pre-processed by replacing BatchNorm\nwith GroupNorm and replacing the final average pool layer with a spatial softmax pooling [4, 12]. We use an\nimage resolution of 160\u00d7240 for each view, processed with a 90% random crop to 144\u00d7216. Finally, the\nproprioception is concatenated with the vision and text encoder as the condition into the diffusion policy.\nWe use the convolution network-based diffusion policy architecture [12]. The final network has 108 million\nparameters. All networks are optimized end-to-end with the AdamW optimizer, with 5e-5 learning rate and\n1e-6 weight decay, and a cosine learning rate scheduler. For evaluation, we use an exponential moving average\nof all networks with a decay rate of 0.75.\n23\nFigure 7: Generalization to Novel Objects. The Transport domain requires generalization to diverse and novel object\nshapes and colors. Trained to transport 22 toys, our distilled policy generalizes to 8 novel toys (in blue section). All objects\nrendered from a fixed camera to show diversity of object size.\nC.3\nTraining\nWe train a separate multi-task policy for each domain using the same hyperparameters and network architecture.\nFor domains with only a single task, this amounts to a single-task policy. All networks are trained for 2 days\non a single NVIDIA A6000, and the best checkpoint\u2019s performance is reported. We found that performance\ntypically saturates around 1 day into training.\nD\nUtilities Implementation\nFor motion planning, we implemented rapidly-exploring random trees (RRT [55]) with grasped-object-aware\ncollision checking, allowing the robot to motion plan with dynamic grasping constraints. The geometry-based\ngrasp and placement sampler is implemented using point clouds created from depth maps, camera matrices,\nand segmentation maps from the simulator. While our grasp sampler uses only geometry, kinematics, and\ncontact information, including other grasp quality metrics (e.g. stability analysis) can improve its performance.\nIn the placement sampler, we sample candidate place positions at points whose estimated contact normal is\naligned against the gravity direction. The revolute and prismatic joint motion primitives are implemented by\nchecking the grasp pose relative to the joint (e.g. mailbox lid handle grasp relative to the mailbox lid hinge),\nthen performing a circular motion around the joint axis or a linear motion along the joint axis, respectively.\nE\nBenchmark\nOur benchmark is built on top of the Mujoco [3] simulator, using assets from the Google Scanned Objects\ndataset [59, 60]. We use a table-top manipulation set-up, with a WSG50 gripper and Toyota Research Institute\nFinray fingers mounted on a UR5e, with a policy control rate of 4Hz. The workspace has two cameras, one\nfront view, which observes the entire workspace and robot, and a wrist-mounted camera, which is used to help\nwith fine-grained manipulation [4]. We end episodes when any object is dropped to the floor. Below, we\nclarify how we design the tasks for each domain.\nE.1\nMailbox\nTo be considered successful, the mailbox needs to be closed with the package inside the mailbox, with the\nmailbox flag raised within 200 seconds (800 control cycles). During data generation and testing, the package\u2019s\nplanar position is uniformly random in a planar bound of dimensions [10cm, 10cm]. At evaluation, the policy\nhas to generalize to unseen package positions. The amazon has is a rigid object with 6DoF. The mailbox is a\nfixed rigid object, with one degree of freedom for each of its revolute joints, one for the mailbox lid, and one\nfor the mailbox flag.\n24\nApproach\nCrayon\nHorse\nPencilcase\nVitamin\nAvg.\nB\nM\nT\nB\nM\nT\nB\nM\nT\nB\nM\nT\nLLM-as-Policy (2D) 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0\n(+) 6DoF Robot Utils\n5.5 0.5 0.0 2.0 0.0 0.0 5.0 0.0 0.0 2.0 0.0 0.0\n1.3\n(+) Verify & Retry\n48.5 39.5 33.0 45.5 32.0 24.5 46.0 27.0 20.0 27.0 18.5 20.5 31.8\nDistill No Retry\n19.0 19.0 17.5 13.0 34.0 22.5 27.5 41.0 39.5 13.5 12.5 13.5 22.7\nDistill (Ours)\n57.5 63.0 50.0 62.5 59.0 51.5 59.5 72.5 61.5 46.0 39.5 46.5 55.8\nTable 6: Drawer Quantitative Results (Success Rate %) where B, M, T means bottom, middle, and top\ndrawers. Averaged over 200 episodes.\nE.2\nTransport\nTo be considered successful, the toy needs to be inside the left bin within 100 seconds. At the beginning of\neach episode, a random toy 3D asset is sampled. During data generation and testing, the toy\u2019s position is\nuniformly random inside the right bin, and orientation uniformly random along all three euler axes. On top of\nnovel randomized poses, the policy also has to generalize to unseen object instances with novel geometry. We\nuse 22 toys for data generation, and 8 for testing (Fig. 7). The toy is a rigid object with 6DoF, while the bins\nare fixed rigid objects with no DoF. The bin asset names corresponds with their spatial location (e.g. the left\nbin is called \u201cleft bin\u201d when the scene is presented to the LLM).\nE.3\nDrawer\nThis is a multi-task domain with 12 tasks, where each task involves moving one of the four objects (vitamin\nbottle, pencil case, crayon box, horse) into one of the three drawers (top, middle, bottom). The task description\nfollows the template \u201cmove the \u27e8object\u27e9 into the \u27e8 drawer\u27e9\u201d. To be considered successful, the specified object\nneeds to be inside the specified drawer within 120 seconds. During data generation and testing, each of\nthe four object\u2019s position is uniformly random within a planar bound of dimensions [10cm,10cm], centered\naround 4 evenly spaced locations along the table. At test time, the policy has to generalize to unseen object\npositions in the same distribution as its data generation.\nAll four objects are rigid objects with 6DoF. The drawer is a fixed articulated object with 3 DoF, one for\neach of the drawers.\nE.4\nCatapult\nThis is a multi-task domain with 3 tasks, one for each of the three bins. The task description follows the\ntemplate \u201cmove the block onto the catapult arm, then press the button to shoot the block into the \u27e8bin\u27e9\u201d where\n\u27e8bin\u27e9 is either closest, middle, or furthest bin. The bin asset names corresponds with their spatial location (e.g.\nthe furthest bin is called \u201cfurthest bin\u201d when the scene is presented to the LLM).\nIn order to be considered successful, the block needs to be inside the specified bin within 60 seconds. This\nis a short amount of time, which prevents policies from retrying after failure. The block is a rigid object with\n6DoF. The bins are fixed rigid objects with no degrees of freedom. The catapult has two degrees of freedom,\none revolute joint for the catapult arm, and one prismatic joint for the button. This task is designed to study\ntool-use, and does not have any pose randomization. Thus, different seeds affect only the policy\u2019s pseudo\nrandom samplers or the diffusion process.\nWe implement the catapult with a special callback function which checks whether the button sliding joint is\nnear its max value. If it is, then the constraint that holds the catapult arm down is disabled, releasing the spring\nloaded catapult arm hinge joint.\nE.5\nBus Balance\nIn order to be considered successful, the bus needs to be fully balanced on top of the block within 100 seconds.\nOn top of testing for intuitive physics, this high precision requirement of this task was also used to test the\npolicy\u2019s precision and ability to recover from failure, which is why we allow a generous time budget. The task\ndescription is \u201cbalance the bus on the block\u201d.\n25\nApproach\nBalance\nCatapult\nTransport Mailbox\nNear Mid Far Train Test\nLLM-as-Policy (2D)\n28.0\n100.0 0.0 0.0\n\u2013\n21.5\n0.0\n(+) 6DoF Robot Utils\n5.5\n7.0\n1.0 0.0\n\u2013\n35.0\n0.0\n(+) Verify & Retry\n45.0\n16.3 3.3 2.2\n\u2013\n82.0\n3.0\nDistill No Retry\n67.5\n2.5 56.5 56.5 31.0 32.5\n0.0\nDistill (Ours)\n79.0\n78.0 52.0 45.0 74.0 80.0\n62.0\nTable 7: Full Quantitative Results (Success Rate %). Averaged over 200 episodes.\nThe bus is a rigid object with 6DoF, dropped from a fixed location above the table with uniformly random\norientation. This means when the bus drops, it lands in different positions and orientations. The block is fixed\nwith no degrees of freedom.\nF\nFull Results\nWe include the full results for all tasks in the drawer domain in Table 6, and all other domains in Table 7. We\nomit data generation baseline numbers on the train set in the transport domain, since they are non-learning\napproaches. All approaches are evaluated on 200 different seeds, which controls pose randomization, which\nasset is sampled, the pseudo-random robotic utility samplers, and the pseudo-random diffusion process. We\nmake one exception in the catapult domain, where due to the low success rates of getting the block into\nthe middle and far bin, we run evaluation until there are 500 successful trajectories per task, then report the\naverage success rate. Since the time limit for the catapult is short, the data-collection policy will not have\nenough time to retry, leading to identical numbers with the baseline data-collection policy without verify &\nretry.\nIn the drawer domain, we observe that the task is more difficult for:\n1. Larger objects: The most challenging objects are the vitamin bottle and the horse toy, both of which\nare too large to fit the drawer if they are in an upright orientation. This means to be effective at this task,\nthe robot should perform sideway grasps on these objects, such that downstream placement is easier. In\ncontrast, the small crayon box is has the highest success rates amongst the data-collection policies.\n2. Top drawer: We observe interacting with this drawer often brings the robot close to its kinematic reach\nrange. This means slight imprecision in the policy\u2019s predicted actions or small shifts in the grasped\nobject (which is unaccounted for during motion planning) in execution could lead to failure. For instance,\nwhile moving the objects inside the top drawer, the grasped object could collide with the drawer, causing\nthe grasped object to drop or the drawer to close.\n3. Planar Action Primitives: A top-down grasp on the drawer handle will typically be in collision with\nthe drawer\u2019s body. Thus, in LLM-as-Policy (2D)\u2019s first action to open the drawer, its call to the motion\nplanner will fail due to an invalid goal configuration.\nG\nReal World Evaluation\nFigure 9: Real World Objects.\nWe train a separate policy for real-world transfer on\ndomain randomized scenes (Fig. 8). We evaluate our\npolicy on a real UR5e robot with a WSG50 gripper and\nToyota Research Institute Finray fingers, matching our\nsimulation set-up. We use five unseen objects (Fig. 9),\nranging in shape, size, and visual appearance. Each\nobject is evaluated on 10 episodes, with the object\nplaced at a random pose on the right bin. We observe 70%, 80%, 60%, 80%, and 90% for the pear, monster,\nrubiks cube, fetch controller, and mustard bottle respectively, giving a mean success rate of 76%.\n26\nFigure 8: Domain Randomization. To facilitate Sim2Real transfer, we train our policy on lighting, texture, and camera\npose randomized scenes.\n27\n"
  },
  {
    "title": "NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection",
    "link": "https://arxiv.org/pdf/2307.14620.pdf",
    "upvote": "10",
    "text": "NeRF-Det: Learning Geometry-Aware Volumetric Representation\nfor Multi-View 3D Object Detection\nChenfeng Xu1 Bichen Wu2 Ji Hou2 Sam Tsai2 Ruilong Li1 Jialiang Wang2 Wei Zhan1\nZijian He2 Peter Vajda2 Kurt Keutzer1 Masayoshi Tomizuka1\n1University of California, Berkeley\n2Meta\n3D \nDetection\nNeRF\nGeometry\n3D Object Detection\nNovel View RGB and Depth \nRendering\nRays from the \nNovel View\nMulti-View Images\nNerf-Det\nFigure 1: NeRF-Det aims to detect 3D objects with only RGB images as input. To enhance detection, we propose to embed\na NeRF branch with our designed synergy. The two joint branches share the same geometry representation and are trained\nend-to-end, which helps achieve state-of-the-art accuracy on mutli-view indoor RGB-only 3D detection, and additionally\nenables a generalizable novel view synthesis on new scenes without per-scene optimization.\nAbstract\nWe present NeRF-Det, a novel method for indoor 3D de-\ntection with posed RGB images as input. Unlike existing\nindoor 3D detection methods that struggle to model scene\ngeometry, our method makes novel use of NeRF in an end-\nto-end manner to explicitly estimate 3D geometry, thereby\nimproving 3D detection performance. Specifically, to avoid\nthe significant extra latency associated with per-scene opti-\nmization of NeRF, we introduce sufficient geometry priors\nto enhance the generalizability of NeRF-MLP. Further-\nmore, we subtly connect the detection and NeRF branches\nthrough a shared MLP, enabling an efficient adaptation of\nNeRF to detection and yielding geometry-aware volumetric\nrepresentations for 3D detection. Our method outperforms\nstate-of-the-arts by 3.9 mAP and 3.1 mAP on the ScanNet\nand ARKITScenes benchmarks, respectively.\nWe provide\nextensive analysis to shed light on how NeRF-Det works.\nAs a result of our joint-training design, NeRF-Det is able to\nThis work was done when Chenfeng was an intern at Meta.\ngeneralize well to unseen scenes for object detection, view\nsynthesis, and depth estimation tasks without requiring\nper-scene optimization.\nCode is available at https:\n//github.com/facebookresearch/NeRF-Det.\n1. Introduction\nIn this paper, we focus on the task of indoor 3D object\ndetection using posed RGB images. 3D object detection is\na fundamental task for many computer vision applications\nsuch as robotics and AR/VR. The algorithm design depends\non input sensors. In the past few years, most 3D detection\nworks focus on both RGB images and depth measurements\n(depth images, point-clouds, etc.). While depth sensors are\nwidely adopted in applications such as autonomous driv-\ning, they are not readily available in most AR/VR headsets\nand mobile phones due to cost, power dissipation, and form\nfactor constraints. Excluding depth input, however, makes\n3D object detection significantly more challenging, since\nwe need to understand not only the semantics, but also the\nunderlying scene geometry from RGB-only images.\narXiv:2307.14620v1  [cs.CV]  27 Jul 2023\nTo mitigate the absence of geometry, one straightfor-\nward solution is to estimate depth. However, depth esti-\nmation itself is a challenging and open problem. For ex-\nample, most monocular depth-estimation algorithms can-\nnot provide accurate metric depth or multi-view consis-\ntency [12, 35, 18, 32]. Multi-view depth-estimation algo-\nrithms can only estimate reliable depth in textured and non-\noccluded regions [10, 37].\nAlternatively, ImVoxelNet [34] models the scene geom-\netry implicitly by extracting features from 2D images and\nprojecting them to build a 3D volume representation. How-\never, such a geometric representation is intrinsically am-\nbiguous and leads to inaccurate detection.\nOn the other hand, Neural Radiance Field (NeRF) [24, 4,\n4] has been proven to be a powerful representation for ge-\nometry modeling. However, incorporating NeRF into the\n3D detection pipeline is a complex undertaking for sev-\neral reasons: (i) Rendering a NeRF requires high-frequency\nsampling of the space to avoid aliasing issues [24], which is\nchallenging in the 3D detection pipeline due to limited res-\nolution volume. (ii) Traditional NeRFs are optimized on a\nper-scene basis, which is incompatible with our objective of\nimage-based 3D detection due to the considerable latency\ninvolved. (iii) NeRF makes full use of multi-view consis-\ntency to better learn geometry during training. However,\na simple stitch of first-NeRF-then-perception [40, 16, 17]\n(i.e., reconstruction-then-detection) does not bring the ad-\nvantage of multi-view consistency to the detection pipeline.\nTo mitigate the issue of ambiguous scene geometry, we\npropose NeRF-Det to explicitly model scene geometry as an\nopacity field by jointly training a NeRF branch with the 3D\ndetection pipeline. Specifically, we draw inspirations from\n[44, 53] to project ray samples onto the image plane and ex-\ntract features from the high-resolution image feature map,\nrather than from the low-resolution volumes, thereby over-\ncoming the need for high-resolution volumes. To further en-\nhance the generalizability of NeRF model to unseen scenes,\nwe augment the image features with more priors as the in-\nput to the NeRF MLP, which leads to more distinguishable\nfeatures for NeRF modeling. Unlike previous works that\nbuild a simple stitch of NeRF-then-perception, we connect\nthe NeRF branch with the detection branch through a shared\nMLP that predicts a density field, subtly allowing the gradi-\nent of NeRF branches to back-propagate to the image fea-\ntures and benefit the detection branch during training. We\nthen take advantage of the uniform distribution of the vol-\nume and transform the density field into an opacity field\nand multiply it with the volume features. This reduces the\nweights of empty space in the volume feature. Then, the\ngeometry-aware volume features are fed to the detection\nhead for 3D bounding box regression. It is worth noting\nthat during inference, the NeRF branch is removed, which\nminimizes the additional overhead to the original detector.\nOur experiments show that by explicitly modeling the\ngeometry as an opacity field, we can build a much better\nvolume representation and thereby significantly improve 3D\ndetection performance. Without using depth measurements\nfor training, we improve the state-of-the-art by 3.9 and\n3.1 mAP on the ScanNet and the ARKITScenes datasets,\nrespectively. Optionally, if depth measurements are also\navailable for training, we can further leverage depth to im-\nprove the performance, while our inference model still does\nnot require depth sensors. Finally, although novel-view syn-\nthesis and depth estimation are not our focus, our analysis\nreveals that our method can synthesize reasonable novel-\nview images and perform depth prediction without per-\nscene optimization, which validates that our 3D volume fea-\ntures can better represent scene geometry.\n2. Related Work\n3D Detection in Indoor Scene. 3D detection utilizes var-\nious methods depending on inputs, and has achieved great\nsuccess on point cloud [25, 28, 29, 31, 54] and voxel repre-\nsentations [51, 14, 13, 8, 15]. 3D-SIS [13] uses anchors to\npredict 3D bounding boxes from voxels fused by color and\ngeometric features. The widely-used VoteNet [30] proposes\nhough voting to regresses bounding box parameters from\npoint features. However, depth sensors are not always read-\nily available on many devices due to its huge power con-\nsumption, such as on VR/AR headsets. To get rid of sensor\ndepth, Panoptic3D [6] operates on point clouds extracted\nfrom predicted depth. Cube R-CNN [3] directly regresses\n3D bounding boxes from a single 2D image.\nComparably, the multi-view approach is also not limited\nby depth sensors and is more accurate. However, the cur-\nrent state-of-the-art multi-view method [34] fuses the image\nnaively by duplicating pixel features along the ray, which\ndoes not incorporate a sufficient amount of geometric clues.\nTo address this, we leverage NeRF to embed geometry into\nthe volume for better 3D detection.\n3D Reconstruction with Neural Radiance Field. Neural\nRadiance Field (NeRF) [24] is a groundbreaking 3D scene\nrepresentation that emerged three years ago, and has proven\nto be powerful for reconstructing 3D geometry from multi-\nview images [24, 1, 50, 55, 42]. Early works [24, 1, 20,\n27, 52] directly optimize for per-scene density fields using\ndifferentiable volumetric rendering [23]. Later, NeuS [43]\nand VolSDF [50] improve the reconstruction quality by us-\ning SDF as a replacement of density as the geometry repre-\nsentation. Recently, Ref-NeRF [39] proposes to use reflec-\ntive direction for better geometry of glossy objects. Aside\nfrom aforementioned per-scene optimization methods, there\nare also works that aim to learn a generalizable NeRF from\nmultiple scenes, such as IBRNet [44] and MVS-NeRF [4],\nwhich predict the density and color at each 3D location con-\nditioned on the pixel-aligned image features. Despite this\n\u2026\nImage Backbone\n(\ud835\udf0e, $\u210e)\n\ud835\udefc\nDet \nHead\nOpacity Grid\n3D Detection Branch\nG-MLP\n\ud835\udc5dos enc\n\u2026\nFeature Grid\nNeRF Branch\nG-MLP\n\ud835\udf0e\n\ud835\udc50\nVolumetric \nRendering\nRays from \nNovel View\n\ud835\udc5dos enc\nSampled Features\nC-MLP\n(\ud835\udf0e, $\u210e)\n\ud835\udc50\nMulti-view Images from \nScanning Trajectory\nNovel Camera View\n\ud835\udc51\nShare Parameters\nFigure 2: The framework of NeRF-Det. Our method leverages NeRF to learn scene geometry by estimating opacity grids.\nWith the shared geometry-MLP (G-MLP), the detection branch can benefit from NeRF in estimating opacity fields and is\nthus able to mask out free space and mitigate the ambiguity of the feature volume.\namazing progress, all of these methods only focus on a sin-\ngle task \u2013 either novel-view synthesis or surface reconstruc-\ntion. In contrast, we propose a novel method that incorpo-\nrates NeRF seamlessly to improve 3D detection.\nNeRF for Perception. Being able to capture accurate ge-\nometry, NeRF has gradually been incorporated into percep-\ntion tasks such as classification [17], segmentation [40, 56],\ndetection [16], instance segmentation [19], and panoptic\nsegmentation [9]. However, most of them [17, 16, 40] fol-\nlow the pipeline of first-NeRF-then-perception, which not\nonly creates extra cost for perception tasks but also does not\nsufficiently use volumetric renderings to benefit perception\nduring training. Besides, [56, 9] demonstrate that NeRF\ncan significantly improve label efficiency for semantic seg-\nmentation by ensuring multi-view consistency and geome-\ntry constraints. Our proposed NeRF-Det method incorpo-\nrates NeRF to ensure multi-view consistency for 3D detec-\ntion. Through joint end-to-end training for NeRF and de-\ntection, no extra overheads are introduced during inference.\n3. Method\nOur method, termed as NeRF-Det, uses posed RGB im-\nages for indoor 3D object detection by extracting image fea-\ntures and projecting them into a 3D volume. We leverage\nNeRF to help infer scene geometry from 2D observations.\nTo achieve this, we entangle the 3D object detection and\nNeRF with a shared MLP, with which the multi-view con-\nstraint in NeRF can enhance the geometry estimation for the\ndetection, as shown in Fig. 2.\n3.1. 3D Detection Branch\nIn the 3D detection branch, we input posed RGB frames\nto the 2D image backbone, denoting the images as Ii \u2208\nRHi\u00d7Wi\u00d73 and the corresponding intrinsic matrix and ex-\ntrinsic matrix as K \u2208 R3\u00d73 and Ri \u2208 R3\u00d74, where\ni = 1, 2, 3, ..., T and T is the total number of views.\nWe follow [34], which uses an FPN [22] to fuse multi-\nstage features and use high resolution features, denoted as\nFi \u2208 RC\u00d7H/4\u00d7W/4, to generate a 3D feature volume.\nWe create the 3D feature volume by attaching 2D fea-\ntures from each image to their corresponding positions in\n3D. We establish a 3D coordinate system, with the z-axis\ndenoting height, and the x- and y-axis denoting two hor-\nizontal dimensions.\nThen, we build a 3D grid of with\nNx \u00d7 Ny \u00d7 Nz voxels. For each voxel center with coor-\ndinate p = (x, y, z)T , we project it to view-i to obtain the\n2D coordinates as\n\u0000u\u2032\ni, v\u2032\ni, di\n\u0001T = K\u2032 \u00d7 Ri \u00d7\n\u0000p, 1\u0001T ,\n(1)\nwhere (ui, vi) = (u\u2032\ni/di, v\u2032\ni/di) is to the pixel coordinate of\np in view-i. K\u2032 is the scaled intrinsic matrix, considering\nthe feature map downsampling. After building this corre-\nspondence, we assign 3D features as\nVi(p) = interpolate((ui, vi), Fi),\n(2)\nwhere interpolate() looks up the feature in Fi at lo-\ncation (ui, vi). Here we use nearest neighbor interpolation.\nFor p that are projected outside the boundary of the image,\nor behind the image plane, we discard the feature and set\nVi(p) = 0. Intuitively, this is equivalent to shooting a ray\nAugmented feature \nof a 3D Point\nOut of \nboundary\nMulti-view Image-based \nSampling\nFeature Grid-based \nSampling\nMean of image features\nVar of image features\nMean of image colors\nVar of image colors\nFigure 3: Different feature sampling strategies. Give a ray\nfrom a novel view, we can project the 3D points on the ray to\nthe multi-view image features, as presented in the left part,\nand attach their mean/variance as well as the corresponding\nRGB to the 3D point. On the other hand, we can also sample\nfeatures from feature grids, as shown in the right part.\nfrom the origin of camera-i through pixel (ui, vi), and for\nall voxels that are on the ray, we scatter image features to\nthe voxel in Vi. Next, we aggregate multi-view features by\nsimply averaging all effective features as done in ImVoxel-\nNet [34]. Letting Mp denote the number of effective 2D\nprojections, we compute V avg(p) = PMp\ni=1 Vi(p)/Mp.\nHowever, volume features generated this way \u201cover pop-\nulate\u201d projection rays without considering empty spaces\nand other geometric constraints. This makes the 3D rep-\nresentation ambiguous and causes detection errors. To miti-\ngate this issue, we propose to incorporate a NeRF branch to\nimprove learning geometry for the detection branch.\n3.2. NeRF Branch\nFeature Sampling.\nNeRF [24] is a neural rendering\nmethod for novel view synthesis. However, previous NeRFs\nsample features from the 3D volume with high resolution,\nsuch as 1283 [24]. In 3D detection, we use a lower grid\nresolution of 40 \u00d7 40 \u00d7 16, which suffers from the aliasing\nissue and results in degradation of geometry learning. To\nmitigate this, we draw inspiration from [44, 53] and sam-\nple features from higher resolution 2D image feature maps,\ntypically of size 160 \u00d7 120, as shown in Fig. 3. Specifi-\ncally, we first sample points along the ray originated from\nthe camera, i.e., r(t) = o + t \u00d7 d where o is the ray origin\nand d is the view direction. For a coordinate p sampled on\nthe ray and a viewing direction d, we can compute the color\nc(p, d) and density \u03c3(p) as:\n\u03c3(p), \u02c6h(p) = G-MLP( \u00afV (p), \u03b3(p)),\n(3)\nc(p, d) = C-MLP(\u02c6h(p), d).\n(4)\n\u00afV (p) is ray features aggregated and augmented from multi-\nview features, and \u03b3(p) is the position encoding same as\n[24] while \u02c6h is latent features. The first MLP is termed\nG-MLP for estimating geometry and the second MLP is\ntermed C-MLP for estimating color. For activations, we fol-\nlow [24] and use ReLU for the density \u03c3(p) and sigmoid for\nthe color c(p, d).\nAugmenting Features.\nAlthough it is similar to [44, 53]\nthat use image features, it is still difficult to make G-MLP\nestimate accurate geometry across different scenes by sim-\nply averaging features from multi-view features as detection\nbranch does. Therefore, we propose to incorporate more\npriors to help optimize G-MLP. Beyond averaging the pro-\njected features, we augment the sampled features with the\nvariance from multiple views V var(p) = PMp\ni=1(Vi(p) \u2212\nV avg(p))2/Mp. The variance of the color features is able to\nroughly describe the occupancy of the 3D field, which has\nbeen widely used as cost volume in multi-view stereo depth\nestimation [49]. If the 3D location p is occupied, the vari-\nance of the observed features should be low under the as-\nsumption that the scene only contains Lambertian surfaces.\nOn the other hand, if the location is in free space, differ-\nent appearances would be observed from different views,\nand therefore the variance of color features would be high.\nCompared to naive average of features, variance provides a\nbetter prior for estimating scene geometry.\nIn addition to extracted deep features, which are trained\nto be invariant to subtle appearance changes, we also aug-\nment pixel RGB values into sampled features on the ray.\nThis is inspired by IBRNet [44] where they attach the pixel\nRGB to the input to the MLP for better appearance mod-\neling. We compute the mean and variance for pixel RGB\nvalues in the same way as for deep features. In all, the\naugmented feature \u00afV is represented as a concatenation of\n{V avg, V var, RGBavg, RGBvar}, as shown in Fig. 3. The\nsampled augmented features are passed into NeRF MLPs\n(Eq. 3) to generate the density and color. We use volumet-\nric rendering to produce the final pixel color and depth,\n\u02c6C(r) =\nNp\nX\ni=1\nTi\u03b1ici, D(r) =\nNp\nX\ni=1\nTi\u03b1iti,\n(5)\nwhere Ti = exp (\u2212 Pi\u22121\nj=1 \u03c3j\u03b4t), \u03b1i = 1 \u2212 exp (\u2212\u03c3i\u03b4t),\nti is the distance between sampled ith point between the\ncamera, \u03b4t is the distance between sampled points of rays.\n3.3. Estimating Scene Geometry\nWe use an opacity field to model the scene geometry.\nThe opacity field is a volumetric representation that reflects\nthe probability of an object\u2019s presence in a particular area,\ni.e., if there is an object that cannot be seen through, the\nopacity field would be 1.0 in that area. To generate the opac-\nity field, we follow the same process of augmenting features\nin the detection branch as we do in the NeRF branch. A key\ningredient to the approach is sharing the G-MLP learned\nin the NeRF branch with the detection branch. This en-\nables two important capabilities. Firstly, the shared G-MLP\nsubtly connects the two branches, allowing gradients from\nthe NeRF branch to back-propagate and benefit the detec-\ntion branch during training. Secondly, during inference, we\ncan directly input the augmented volume features of 3D de-\ntection into the shared G-MLP since both inputs from two\nbranches are augmented features. The output of G-MLP is\nthe density represented as \u03c3(p) = G-MLP( \u00afV (p), \u03b3(p)),\nwhere \u03c3(p) \u2208 [0, \u221e]. Note that p is the center position of\neach voxel in the volume of the detection branch.\nNext, we aim to transform the density field into the opac-\nity field by \u03b1(p) = 1 \u2212 exp (\u2212\u03c3(p) \u00d7 \u03b4t). However, it is\ninfeasible to calculate \u03b4t as we can not decide the ray direc-\ntion and calculate the point distance within the undirected\nvolume in the detection branch. Here we subtly take advan-\ntage of the uniform distribution of the voxels in the space.\nThus, the \u03b4t in the volumetric rendering equation can be\ncanceled out as it is a constant. So obtaining opacity can be\nreduced to \u03b1(p) = 1 \u2212 exp (\u2212\u03c3(p)). After generating the\nopacity field, we multiply it with the feature grid V avg for\n3d detection, denoted as \u03b1(p) \u00d7 V avg(p).\n3.4. 3D Detection Head and Training Objectives\nOur geometry-aware volumetric representation can fit to\nmost detection heads. For fair comparison and the simplic-\nity, we use the same indoor detection head as ImVoxelNet\n[34], in which we select 27 location candidates for each ob-\njects and we use three convolution layers to predict the cat-\negories, the locations and the centerness.\nWe jointly train detection and NeRF branches end-to-\nend. No per-scene optimization for the NeRF branch is per-\nformed in test time. For the detection branch, we supervise\ntraining with ground truth 3D bounding boxes following\nImVoxelNet [34], which computes three losses: focal loss\nfor classification Lcls, centerness loss Lcntr, and localiza-\ntion loss Lloc. For the NeRF branch, we use a photo-metric\nloss Lc = || \u02c6C(r) \u2212 \u02c6Cgt(r)||2. When depth ground truth\nis used, we can further supervise the expected depth of the\nscene geometry as Ld = ||D(r) \u2212 Dgt(r)|| where D(r) is\ncomputed with Equ. 5. The final loss L is given by\nL = Lcls + Lcntr + Lloc + Lc + Ld.\n(6)\nEven though we optionally use depth during training, it\nis not required in inference. Also our trained network is\ngeneralizable to new scenes which are never seen during\ntraining.\n4. Experimental Results\nImplementation details. Our detection branch mainly fol-\nlows ImVoxelNet, including backbones, detection head, res-\nolutions and training recipe etc. Please refer to supplemen-\ntal material for more details.\nOur implementation is based on MMDetection3D [5].\nTo the best of our knowledge, we are the first to implement\nNeRF in MMDetection3D. We are also the first to conduct\nNeRF-style novel view synthesis and depth estimation on\nthe whole ScanNet dataset, while prior works only test on a\nsmall number of scenes [48, 47]. The code will be released\nfor future research.\n4.1. Main results\nQuantitative results.\nWe compare NeRF-Det with point-\ncloud based methods [46, 13, 30], RGB-D based meth-\nods [13, 11] and the state-of-the-art RGB-only method\nImVoxelNet [34] on ScanNet, as shown in Tab. 1.\nWith\nResNet50\nas\nthe\nimage\nbackbone,\nwe\nobserve\nthat\nNeRF-Det-R50-1x\noutperforms\nImVoxelNet-R50-1x\nby\n2.0\nmAP.\nOn\ntop\nof\nthat,\nNeRF-Det with depth supervision,\ndenoted as\nNeRF-Det-R50-1x*,\nfurther\nimproves\nthe\ndetec-\ntion performance by 0.6 mAP compared to only RGB\nsupervision NeRF-Det-R50-1x.\nWe denote the total training iterations of ImVoxelNet\nfrom the official code as 1x in the Tab. 1. Yet the 1x setting\nonly iterates through each scene roughly 36 times, which is\nfar from sufficient for optimizing the NeRF branch, which\nrequires thousands of iterations to optimize one scene, as\nindicated in [24, 44, 1, 20]. Thus, we further conduct ex-\nperiments with 2x training iterations to fully utilize the po-\ntential of NeRF, and we observe that NeRF-Det-R50-2x\nreaches 52.0 mAP, surpassing ImVoxelNet by 3.6 mAP un-\nder the same setting (ImVoxelNet-R50-2x). It is worth\nmentioning that we do not introduce any extra data/labels to\nget such an improvement. If we further use depth supervi-\nsion to train the NeRF branch (NeRF-Det-R50-2x*), the\ndetection branch is further improved by 1.3 in mAP@.50 as\nshown in Tab. 3. This validates better geometry modeling\n(via depth supervision) could be helpful to the 3D detec-\ntion task. While NeRF-Det provides an efficient method to\nincorporate depth supervision during the training process,\nintroducing depth supervision in ImVoxelNet is difficult.\nMoreover, when substituting ResNet50 with ResNet101,\nwe achieve 52.9 mAP@.25 on ScanNet, which outper-\nforms ImVoxelNet on the same setting over 3.9 points.\nWith depth supervision, NeRF-Det-R101-2x* reduces\nthe gap between RGB-based method ImVoxelNet [34] and\npoint-cloud based method VoteNet [30] by half (from 10\nmAP \u2192 5 mAP). Besides, we conduct experiments on the\nARKitScenes (see Tab. 2). The 3.1 mAP improvement fur-\nther demonstrates the effectiveness of our proposed method.\nQualitative results.\nWe visualize the predicted 3D\nbounding boxes from NeRF-Det-R50-2x on scene\nmeshes, as shown in Fig. 4. We observe that the proposed\nmethod gets accurate detection prediction even on the ex-\nTable 1: 3D Detection with multi-view RGB inputs on ScanNet. The first block of the table includes point-cloud based and\nRGBD-based methods, and the rest are multi-view RGB-only detection methods. \u2020 means our reproduction of ImVoxelNet\n[34] using the official code. * indicates the NeRF-Det with depth supervision. 1x and 2x refer that we train the model with\nthe same and two times of the training iteration wrt. the original iterations of ImVoxelNet, respectively.\nMethods\ncab\nbed\nchair\nsofa\ntabl\ndoor\nwind\nbkshf\npic\ncntr\ndesk\ncurt\nfridg\nshowr\ntoil\nsink\nbath\nofurn\nmAP@.25\nSeg-Cluster [46]\n11.8\n13.5\n18.9\n14.6\n13.8\n11.1\n11.5\n11.7\n0.0\n13.7\n12.2\n12.4\n11.2\n18.0\n19.5\n18.9\n16.4\n12.2\n13.4\nMask R-CNN [11]\n15.7\n15.4\n16.4\n16.2\n14.9\n12.5\n11.6\n11.8\n19.5\n13.7\n14.4\n14.7\n21.6\n18.5\n25.0\n24.5\n24.5\n16.9\n17.1\nSGPN [46]\n20.7\n31.5\n31.6\n40.6\n31.9\n16.6\n15.3\n13.6\n0.0\n17.4\n14.1\n22.2\n0.0\n0.0\n72.9\n52.4\n0.0\n18.6\n22.2\n3D-SIS [13]\n12.8\n63.1\n66.0\n46.3\n26.9\n8.0\n2.8\n2.3\n0.0\n6.9\n33.3\n2.5\n10.4\n12.2\n74.5\n22.9\n58.7\n7.1\n25.4\n3D-SIS (w/ RGB) [13]\n19.8\n69.7\n66.2\n71.8\n36.1\n30.6\n10.9\n27.3\n0.0\n10.0\n46.9\n14.1\n53.8\n36.0\n87.6\n43.0\n84.3\n16.2\n40.2\nVoteNet [30]\n36.3\n87.9\n88.7\n89.6\n58.8\n47.3\n38.1\n44.6\n7.8\n56.1\n71.7\n47.2\n45.4\n57.1\n94.9\n54.7\n92.1\n37.2\n58.7\nFCAF3D [33]\n57.2\n87.0\n95.0\n92.3\n70.3\n61.1\n60.2\n64.5\n29.9\n64.3\n71.5\n60.1\n52.4\n83.9\n99.9\n84.7\n86.6\n65.4\n71.5\nCAGroup3D [41]\n60.4\n93.0\n95.3\n92.3\n69.9\n67.9\n63.6\n67.3\n40.7\n77.0\n83.9\n69.4\n65.7\n73.0\n100.0\n79.7\n87.0\n66.1\n75.12\nImVoxelNet-R50-1x\n28.5\n84.4\n73.1\n70.1\n51.9\n32.2\n15.0\n34.2\n1.6\n29.7\n66.1\n23.5\n57.8\n43.2\n92.4\n54.1\n74.0\n34.9\n48.1\nImVoxelNet\u2020-R50-1x\n31.6\n81.8\n74.4\n69.3\n53.6\n29.7\n12.9\n50.0\n1.3\n32.6\n69.2\n12.7\n54.6\n31.8\n93.1\n55.8\n68.2\n31.8\n47.5\nNeRF-Det-R50-1x\n32.7\n82.6\n74.3\n67.6\n52.3\n34.4\n17.3\n40.1\n2.0\n49.2\n67.4\n20.0\n57.2\n41.0\n90.9\n52.3\n74.0\n33.7\n49.5 (+2.0)\nNeRF-Det-R50-1x*\n32.7\n84.7\n74.6\n62.7\n52.7\n35.1\n17.7\n48.4\n0.0\n49.8\n64.6\n18.5\n60.3\n48.3\n90.7\n51.0\n76.8\n30.4\n50.1 (+2.6)\nImVoxelNet\u2020-R50-2x\n34.5\n83.6\n72.6\n71.6\n54.2\n30.3\n14.8\n42.6\n0.8\n40.8\n65.3\n18.3\n52.2\n40.9\n90.4\n53.3\n74.9\n33.1\n48.4\nNeRF-Det-R50-2x\n37.2\n84.8\n75.0\n75.6\n51.4\n31.8\n20.0\n40.3\n0.1\n51.4\n69.1\n29.2\n58.1\n61.4\n91.5\n47.8\n75.1\n33.6\n52.0 (+3.6)\nNeRF-Det-R50-2x*\n37.7\n84.1\n74.5\n71.8\n54.2\n34.2\n17.4\n51.6\n0.1\n54.2\n71.3\n16.7\n54.5\n55.0\n92.1\n50.7\n73.8\n34.1\n51.8 (+3.4)\nImVoxelNet\u2020-R101-2x\n30.9\n84.0\n77.5\n73.3\n56.7\n35.1\n18.6\n47.5\n0.0\n44.4\n65.5\n19.6\n58.2\n32.8\n92.3\n40.1\n77.6\n28.0\n49.0\nNeRF-Det-R101-2x\n36.8\n85.0\n77.0\n73.5\n56.9\n36.7\n14.3\n48.1\n0.8\n49.7\n68.3\n23.5\n54.0\n60.0\n96.5\n49.3\n78.4\n38.4\n52.9 (+3.9)\nNeRF-Det-R101-2x*\n37.6\n84.9\n76.2\n76.7\n57.5\n36.4\n17.8\n47.0\n2.5\n49.2\n52.0\n29.2\n68.2\n49.3\n97.1\n57.6\n83.6\n35.9\n53.3 (+4.3)\nTable 2: Comparison experiments \u201dwhole-scene\u201d of ARKITScenes validation set.\nMethods\ncab\nfridg\nshlf\nstove\nbed\nsink\nwshr\ntolt\nbthtb\noven\ndshwshr\nfrplce\nstool\nchr\ntble\nTV\nsofa\nmAP@.25\nImVoxelNet-R50\n32.2\n34.3\n4.2\n0.0\n64.7\n20.5\n15.8\n68.9\n80.4\n9.9\n4.1\n10.2\n0.4\n5.2\n11.6\n3.1\n35.6\n23.6\nNeRF-Det-R50\n36.1\n40.7\n4.9\n0.0\n69.3\n24.4\n17.3\n75.1\n84.6\n14.0\n7.4\n10.9\n0.2\n4.0\n14.2\n5.3\n44.0\n26.7 (+3.1)\nFigure 4: Qualitative results of the predicted 3D bounding box on top of NeRF-Det-R50-2x. Note that our approach takes\nonly posed RGB images as input, the reconstructed mesh is only for visualization purpose.\ntremely dense scenes, e.g., the first row and the left of the\nthird row. The chairs are crowded in the room, and some of\nwhich are inserted into tables and are heavily occluded. Yet\nour method is able to detect them accurately. On the other\nhand, NeRF-Det can also tackle multiple scales, as shown in\nthe second row and the right side of the third row, in which\nthere are variant objects with difference sizes, like garbage\nbins, chairs, tables, doors, windows, and sofas etc.\nTable 3: Ablation on scene geometry modelings. GT-Depth\nindicates ground truth depth for placing 2D features in 3D\nvolume. NeuralRecon-Depth indicates NeuralRecon [38]\npre-trained on ScanNetV2 is used to predict the depth.\nDepth predictions are used in both training and inference.\nMethods\nmAP@.25\nmAP@.50\nImVoxelNet-R50-2x (baseline)\n48.4\n23.7\nGT-Depth-R50-2x (upper-bound)\n54.5 (+6.1)\n28.2 (+4.5)\nNeuralRecon-Depth-R50-2x\n48.8 (+0.4)\n21.4 (-2.3)\nCost-Volume-R50-2x (sigmoid)\n49.3 (+0.9)\n24.4 (+0.7)\nNeRF-Det-R50-2x (w/o NeRF)\n49.2 (+0.8)\n24.6 (+0.9)\nNeRF-Det-R50-2x\n52.0 (+3.6)\n26.1 (+2.5)\nNeRF-Det-R50-2x*\n51.8 (+3.4)\n27.4 (+3.7)\nAnalysis of scene geometry modeling.\nAs stated in the\nmethod section, we mitigate the ambiguity of the volume\nrepresentation by learning an opacity field. Furthermore,\nwe explore different scene geometry modeling methods,\nfrom using depth map to cost volume [38, 49], in Tab. 3.\nUsing the Depth Map. In this experiment, we assume we\nhave access to depth maps during both training and infer-\nence. When building the voxel feature grid, instead of scat-\ntering the features on all points along the ray, we only place\nfeatures to a single voxel cell according to the depth maps.\nIntuitively, this leads to less ambiguity in the volume rep-\nresentation, so we should observe better performance for\ndetection. As a proof of concept, we first use ground-truth\ndepth that comes with the dataset. This serves as an upper-\nbound for NeRF-Det as it provides a perfect geometry mod-\neling. It indeed achieves a high detection accuracy of 54.5\nmAP@.25 and 28.2 mAP@.50 (see second row), improv-\ning the baseline by 6.1 mAP@.25 and 4.5 mAP@.40. But\nin practice we cannot get access to the ground-truth depth\nmaps. Thus, we try instead to render depth maps using out-\nof-box geometry reconstruction from NeuralRecon [38].\nThe results are shown in the third row of Tab. 3. We\nobserve that the depth estimation from NeuralRecon sig-\nnificantly degenerates the detection performance by 2.3\nmAP@.50 as compared to plain ImVoxelNet, demonstrat-\ning that the estimation error of depth maps propagates that\ninaccuracy through the detection pipeline.\nCost Volume.\nNext, we compare our method with cost-\nvolume based methods [49, 26]. A common way to com-\npute cost volume is using covariance [49] between source\nview and reference view.\nThis is similar to our method\nwhich calculates the variance of multiple input views. Fol-\nlowing [49], we first use several 3D convolution layers to\nencode the cost volume, then get the probability volume\nvia sigmoid, and multiply it with the feature volume V avg.\nThe results are in the fourth row of Tab. 3. We can see that\nthe cost-volume based method improves ImVoxelNet by 0.9\nmAP@.25 and 0.7 mAP@.50 respectively. It is noteworthy\nto mention that if we remove the NeRF branch, our method\nis very similar to a cost-volume-based method with the dif-\nTable 4: The first group represents using NeRF-RPN train-\ning set, and the second group represents using ScanNet\ntraining set. The latency is measured on one V100.\nMethod\nAP25\nAP50\nLatency\nNeRF-RPN-R50[16] (NeRF-then-det)\n33.13\n5.12\n\u223c846.8s\nNeRF-Det-R50 (joint NeRF-and-Det)\n35.13\n7.80\n0.554s\nNeRF-Det-R50\u2020 (joint NeRF-and-Det)\n61.48\n25.45\n0.554s\nferences being: 1) we augment the variance in the cost vol-\nume by mean volume and color volume, 2) we use the MLP\nand opacity function instead of sigmoid to model the scene\ngeometry. The result is shown in the fifth row of Tab. 3. We\nobserve that the result is very close to a cost-volume-based\nmethod and that both ours and the cost-volume method lead\nto improvement over ImVoxelNet.\nIn contrast to explicitly estimating the depth and cost\nvolume, we leverage NeRF to estimate the opacity field\nfor the scene geometry.\nAs shown in the gray part of\nTab. 3, with the opacity field modeled using NeRF, the\nperformance is significantly improved by +3.6 mAP@.25\nand +2.5 mAP@.50 compared to the baseline. After using\ndepth supervision, NeRF-Det is able to achieve larger im-\nprovement with +3.7 mAP@.50 (the last row). As shown in\nTab. 3, our method of using NeRF to model scene geometry\nis more effective than using predicted depth or cost volume.\nComparison to NeRF-then-Det method.\nWe compare\nour proposed NeRF-Det, a joint NeRF-and-Det method,\nto NeRF-RPN [16] which is a NeRF-then-Det method, as\nshown in Tab. 4.\nWe choose 4 scenes as validation set\nthat are not included in both NeRF-RPN and ScanNet train\nset.\nWe use the official code and provided pre-trained\nmodels of NeRF-RPN to evaluate AP. Experiments in the\nfirst group demonstrate that our proposed joint-NeRF-and-\nDet paradigm is more effective than first-NeRF-then-Det\nmethod NeRF-RPN, with much faster speed.\nThe second group of Tab. 4 shows that directly using\nour model (NeRF-Det-R50-2x in Tab. 1) has drastic im-\nprovements relative to NeRF-RPN. Although our model is\ntrained on large-scale dataset, we emphasize that this is our\nadvantages since it is hard to apply NeRF-RPN on the whole\nScanNet (\u223c1500 scenes) given the heavy overhead.\nIs NeRF branch able to learn scene geometry?\nWe hy-\npothesize that the better detection performance comes from\nbetter geometry. To verify this, we perform novel view syn-\nthesis and depth estimation from the prediction of the NeRF\nbranch. The underlying assumption is that if our model has\ncorrectly inferred the scene geometry, we should be able to\nsynthesize RGB and depth views from novel camera posi-\ntions. We first visualize some of the synthesized images and\ndepth estimation in Fig. 5. The image and depth map quality\nFigure 5: Novel-view synthesis results on top of NeRF-Det-R50-2x*. For each triplet group, the left figure is the synthesized\nresults, the middle one is the ground truth RGB image, and the right part is the estimated depth map. Note that the visualiza-\ntion is from test set, which is never seen during training.\nlook reasonable and reflects the scene geometry well.\nQuantitatively, we evaluate novel-view synthesis and\ndepth estimation by following the protocol of IBRNet [44].\nWe select 10 novel views and randomly sample 50 nearby\nsource views. Then, we average the evaluation results of\nthe 10 novel views for each scene, and finally report aver-\nage results on all 312 scenes in the validation set, as shown\nin Tab. 8. Although novel-view synthesis and depth esti-\nmation are not the main focus of this paper, we achieve an\naverage of 20+ PSNR for novel view synthesis, without per-\nscene training. For depth estimation, we achieves an RMSE\nof 0.756 on all scenes. While this performance falls be-\nhind state-of-the-art depth estimation methods, we note that\n[47] reported average RMSE of 1.0125 and 1.0901 using\nColmap [36] and vanilla NeRF [24] for depth estimation on\nselected scenes in ScanNet. This comparison verifies that\nour method can render reasonable depth.\n4.2. Ablation Study\nWe conduct multiple ablation studies on how different\ncomponents affect the performance of NeRF-Det, includ-\ning different feature sampling strategies, whether to share\nG-MLP, different losses and different features feed into\nthe G-MLP. Besides, although novel-view synthesis is not\nour focus in this paper, we also provide some analysis for\nthe novel-view synthesis results coming out of the NeRF\nbranch, and show how the NeRF branch is influenced by\nthe detection branch during joint training. All experiments\nin the ablation studies are based on NeRF-Det-R50-1x*.\nAblation on G-MLP and feature sampling strategy.\nAs\nindicated in Sec. 3.2, the key ingredient in our pipeline is a\nshared G-MLP which enables the constraint of multi-view\nconsistency to be propagated from NeRF branch to detec-\ntion branch. We conduct an ablation study as shown in Tab.\n5. Without shared G-MLP, the performance drops drasti-\ncally from 50.1 to 48.1 at mAP@.25, shown in the fifth row\nTable 5: Ablation study on G-MLP and different ways to\nsample features onto the ray in the NeRF branch.\nShare G-MLP\nSample source\nmAP@.25\nmAP@.50\n\u2713\n3D volume\n49.4\n24.0\nMulti-view 2D feature\n50.1\n24.4\n3D volume\n48.2\n23.8\nMulti-view 2D feature\n48.1\n23.8\nTable 6: Ablation study for loss.\nPhoto-metric loss\nDepth loss\nmAP@.25\nmAP@.50\n\u2713\n\u2713\n50.1\n24.4\n\u2713\n-\n49.4\n24.1\n-\n\u2713\n50.0\n24.2\n-\n-\n48.5\n23.6\nof Tab. 3. In this case, the influence of multi-view consis-\ntency is only propagated into image backbone, significantly\nlimiting the improvement created by NeRF.\nMoreover, as mentioned in Sec. 3.2, we sample point fea-\ntures along the ray from the multi-view image features in-\nstead of the low-resolution volume features. This ablation is\nshown in Tab. 3. We observe that with shared G-MLP, both\napproaches outperform the baseline ImVoxelNet, and sam-\npling from image features yields better performance (+0.7\nin mAP@0.25) than sampling from volume features. For\nthe novel-view synthesis task using NeRF branch, sampling\nfrom image features achieves 20.51 in PSNR comparing to\n18.93 with volume sampling.\nThe fact that the performance of the NeRF branch is pro-\nportional to the performance of the detection branch also\nindicates that better NeRF optimization could lead to better\ndetection results.\nAblation study on different loss.\nWe study how differ-\nent losses work for the NeRF branch, as shown in Tab. 6.\nIt shows that with only photo-metric loss, the performance\nis closed to purely using depth supervision (third row) in\nTable 7: Ablation study on augmented features.\nAvg\nVar\nColor\nmAP@.25\nmAP@.50\n\u2713\n\u2713\n\u2713\n50.1\n24.4\n\u2713\n\u2713\n-\n49.7\n24.3\n\u2713\n-\n-\n49.0\n23.7\nTable 8: Ablation on how detection branch influences novel\nview synthesis (NVS) and depth estimation (DE) on test set.\nMethod\nPSNR (NVS) \u2191\nSSIM (NVS) \u2191\nRMSE (DE) \u2193\nw/ Det branch\n20.51\n0.83\n0.756\nw/o Det branch\n20.94\n0.84\n0.747\nterm of mAP@.50, indicating that the multi-view RGB con-\nsistency already provides sufficient geometry cues to en-\nable the NeRF branch to learn geometry. When using both\nphoto-metric loss and depth loss, the performance can be\nfurther improved. When neither photo-metric loss nor depth\nloss is used (last row), the performance falls back to that of\na cost-volume based method. The performance is dropped\nby 1.2 mAP@.25 and 0.5 mAP@.50, which demonstrates\nthat the NeRF branch is more effective.\nAblation study on different features.\nWe then study\nhow different features affect performance, as shown in Tab.\n7. The experiment shows that introducing variance features\nimproves performance significantly \u2013 over 0.7 mAP@.25\nand 0.6 mAP@.50 compared to using only average features,\nwhich demonstrates that variance features indeed provide a\ngood geometry prior. Moreover, incorporating image fea-\ntures also improves performance, indicating that low-level\ncolor information also provides good geometry cues.\nAblation Study on detection branch affecting novel view\nsynthesis.\nWe keep the target views and source views the\nsame with and without the detection branch. Results are\nshown in Tab. 8. While the NeRF branch significantly im-\nproves 3D detection by improving scene geometry model-\ning, the detection branch does not benefit the NeRF branch.\nIn fact, disabling the detection branch brings a 0.43db im-\nprovement.\nWe hypothesize that the detection branch is\nprone to erase low-level details which are needed for the\nNeRF, which we aim to address in our future work.\n5. Conclusion\nIn this paper, we present NeRF-Det, a novel method that\nuses NeRF to learn geometry-aware volumetric representa-\ntions for 3D detection from posed RGB images. We deeply\nintegrate multi-view geometry constraints from the NeRF\nbranch into 3D detection through a subtle shared geometry\nMLP. To avoid the large overhead of per-scene optimiza-\ntion, we propose leveraging augmented image features as\npriors to enhance the generalizablity of NeRF-MLP. In ad-\ndition, we sample features from high resolution images in-\nstead of volumes to address the need for high-resolution im-\nages in NeRF. Our extensive experiments on the ScanNet\nand ARKITScene datasets demonstrate the effectiveness of\nour approach, achieving state-of-the-art performance for in-\ndoor 3D detection using RGB inputs.\nNotably, we ob-\nserve that the NeRF branch also generalizes well to unseen\nscenes. Furthermore, our results highlight the importance\nof NeRF for 3D detection, and provide insights into the key\nfactors for achieving optimal performance in this direction.\n6. Acknowledgment\nWe sincerely thank Chaojian Li for the great help on\nNeuralRecon experiments, Benran Hu for the valuable\nadvice on the NeRF-RPN experiments, Feng (Jeff) Liang\nand Hang Gao for the insightful discussions, as well as\nMatthew Yu and Jinhyung Park for the paper proofreading.\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 2, 5\n[2] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry,\nYuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe,\nDaniel Kurz, Arik Schwartz, and Elad Shulman. Arkitscenes\n- a diverse real-world dataset for 3d indoor scene understand-\ning using mobile rgb-d data. In NeurIPS, 2021. 11\n[3] Garrick Brazil, Julian Straub, Nikhila Ravi, Justin Johnson,\nand Georgia Gkioxari.\nOmni3d: A large benchmark and\nmodel for 3d object detection in the wild. arXiv preprint\narXiv:2207.10660, 2022. 2\n[4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance field reconstruction from multi-view stereo.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14124\u201314133, 2021. 2\n[5] MMDetection3D Contributors.\nMMDetection3D: Open-\nMMLab next-generation platform for general 3D ob-\nject detection. https://github.com/open-mmlab/\nmmdetection3d, 2020. 5, 12\n[6] Manuel Dahnert, Ji Hou, Matthias Nie\u00dfner, and Angela Dai.\nPanoptic 3d scene reconstruction from a single rgb image.\nAdvances in Neural Information Processing Systems, 34,\n2021. 2\n[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3D reconstructions of indoor scenes.\nIn\nCVPR, 2017. 11\n[8] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian\nLeibe, and Matthias Nie\u00dfner. 3D-MPA: Multi-Proposal Ag-\ngregation for 3D Semantic Instance Segmentation. In CVPR,\n2020. 2\n[9] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu,\nLanyun Zhu, Xiaowei Zhou, Andreas Geiger, and Yiyi Liao.\nPanoptic nerf: 3d-to-2d label transfer for panoptic urban\nscene segmentation. arXiv preprint arXiv:2203.15224, 2022.\n3\n[10] Yasutaka Furukawa and Carlos Hern\u00b4andez.\nMulti-view\nstereo: A tutorial. Foundations and Trends\u00ae in Computer\nGraphics and Vision, 9(1-2):1\u2013148, 2015. 2\n[11] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017. 5, 6\n[12] Derek Hoiem, Alexei A Efros, and Martial Hebert.\nAu-\ntomatic photo pop-up. In ACM SIGGRAPH 2005 Papers,\npages 577\u2013584. 2005. 2\n[13] Ji Hou, Angela Dai, and Matthias Nie\u00dfner. 3D-SIS: 3D Se-\nmantic Instance Segmentation of RGB-D Scans. In CVPR,\n2019. 2, 5, 6, 11\n[14] Ji Hou, Angela Dai, and Matthias Nie\u00dfner. RevealNet: See-\ning Behind Objects in RGB-D Scans. In CVPR, 2020. 2\n[15] Ji Hou, Benjamin Graham, Matthias Nie\u00dfner, and Saining\nXie. Exploring data-efficient 3d scene understanding with\ncontrastive scene contexts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 15587\u201315597, 2021. 2\n[16] Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, and\nChi-Keung Tang. Nerf-rpn: A general framework for object\ndetection in nerfs. arXiv preprint arXiv:2211.11646, 2022.\n2, 3, 7, 13\n[17] Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Chris Choy, An-\nima Anandkumar, Minsu Cho, and Jaesik Park.\nPerfcep-\ntion: Perception using radiance fields. In Thirty-sixth Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track, 2022. 2, 3\n[18] Kevin Karsch, Ce Liu, and Sing Bing Kang. Depth transfer:\nDepth extraction from video using non-parametric sampling.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 36(11):2144\u20132158, 2014. 2\n[19] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi,\nCaroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi,\nFrank Dellaert, and Thomas Funkhouser. Panoptic Neural\nFields: A Semantic Object-Aware Neural Scene Representa-\ntion. In CVPR, 2022. 3\n[20] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jur-\ngen Gall, Angjoo Kanazawa, and Christoph Lassner. Tava:\nTemplate-free animatable volumetric actors. arXiv preprint\narXiv:2206.08929, 2022. 2, 5\n[21] Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun,\nand Zeming Li.\nBevstereo: Enhancing depth estimation\nin multi-view 3d object detection with dynamic temporal\nstereo. arXiv preprint arXiv:2209.10248, 2022. 12\n[22] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyramid\nnetworks for object detection. In CVPR, 2017. 3\n[23] Nelson Max. Optical models for direct volume rendering.\nIEEE Transactions on Visualization and Computer Graphics,\n1(2):99\u2013108, 1995. 2\n[24] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n4, 5, 8, 12\n[25] Yinyu Nie, Ji Hou, Xiaoguang Han, and Matthias Nie\u00dfner.\nRfd-net: Point scene understanding by semantic instance re-\nconstruction. In CVPR, 2021. 2\n[26] Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer,\nKris Kitani, Masayoshi Tomizuka, and Wei Zhan. Time will\ntell: New outlooks and a baseline for temporal multi-view 3d\nobject detection. arXiv preprint arXiv:2210.02443, 2022. 7,\n12\n[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan\nZhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-\nmatable neural radiance fields for modeling dynamic human\nbodies. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 14314\u201314323, 2021. 2\n[28] Charles R Qi, Xinlei Chen, Or Litany, and Leonidas J\nGuibas. Imvotenet: Boosting 3D object detection in point\nclouds with image votes. In CVPR, 2020. 2\n[29] Charles R. Qi, Or Litany, Kaiming He, and Leonidas J.\nGuibas. Deep hough voting for 3D object detection in point\nclouds. ICCV, 2019. 2\n[30] Charles R Qi, Or Litany, Kaiming He, and Leonidas J\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 9277\u20139286, 2019. 2, 5,\n6, 11\n[31] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J\nGuibas. Frustum pointnets for 3d object detection from rgb-d\ndata. In CVPR, 2018. 2\n[32] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 44(3), 2022. 2\n[33] Danila Rukhovich, Anna Vorontsova, and Anton Konushin.\nFcaf3d: Fully convolutional anchor-free 3d object detection.\nIn European Conference on Computer Vision, pages 477\u2013\n493. Springer, 2022. 6\n[34] Danila Rukhovich, Anna Vorontsova, and Anton Konushin.\nImvoxelnet: Image to voxels projection for monocular and\nmulti-view general-purpose 3d object detection. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 2397\u20132406, 2022. 2, 3, 4, 5, 6, 11\n[35] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:\nLearning 3d scene structure from a single still image. IEEE\ntransactions on pattern analysis and machine intelligence,\n31(5):824\u2013840, 2008. 2\n[36] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 8\n[37] Steven M Seitz, Brian Curless, James Diebel, Daniel\nScharstein, and Richard Szeliski. A comparison and evalua-\ntion of multi-view stereo reconstruction algorithms. In 2006\nIEEE computer society conference on computer vision and\npattern recognition (CVPR\u201906), volume 1, pages 519\u2013528.\nIEEE, 2006. 2\n[38] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and\nHujun Bao.\nNeuralRecon: Real-time coherent 3D recon-\nstruction from monocular video. CVPR, 2021. 7\n[39] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-\ntured view-dependent appearance for neural radiance fields.\nIn 2022 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 5481\u20135490. IEEE, 2022. 2\n[40] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer,\nKyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea\nTagliasacchi, and Daniel Duckworth. Nesf: Neural semantic\nfields for generalizable semantic segmentation of 3d scenes.\narXiv preprint arXiv:2111.13260, 2021. 2, 3, 13\n[41] Haiyang Wang, Lihe Ding, Shaocong Dong, Shaoshuai Shi,\nAoxue Li, Jianan Li, Zhenguo Li, and Liwei Wang. CA-\nGroup3d: Class-aware grouping for 3d object detection on\npoint clouds. In Alice H. Oh, Alekh Agarwal, Danielle Bel-\ngrave, and Kyunghyun Cho, editors, Advances in Neural In-\nformation Processing Systems, 2022. 6\n[42] Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian\nTheobalt, Taku Komura, Lingjie Liu, and Wenping Wang.\nNeuris: Neural reconstruction of indoor scenes using normal\npriors. arXiv preprint arXiv:2206.13597, 2022. 2\n[43] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 2\n[44] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\nnet: Learning multi-view image-based rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4690\u20134699, 2021. 2, 4, 5, 8,\n12\n[45] Tai Wang, Jiangmiao Pang, and Dahua Lin.\nMonocular\n3d object detection with depth from motion. In Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part IX, pages\n386\u2013403. Springer, 2022. 12\n[46] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neu-\nmann. Sgpn: Similarity group proposal network for 3d point\ncloud instance segmentation. In CVPR, 2018. 5, 6\n[47] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu,\nand Jie Zhou. Nerfingmvs: Guided optimization of neural\nradiance fields for indoor multi-view stereo. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 5610\u20135619, 2021. 5, 8\n[48] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 5\n[49] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu.\nCost volume pyramid based depth inference for multi-view\nstereo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 4877\u20134886,\n2020. 4, 7\n[50] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. Advances in Neu-\nral Information Processing Systems, 34:4805\u20134815, 2021. 2\n[51] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas\nGuibas. GSPN: Generative shape proposal network for 3D\ninstance segmentation in point cloud. In CVPR, 2019. 2\n[52] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[53] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural radiance fields from one or few images.\nIn CVPR, 2021. 2, 4\n[54] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang.\nH3dnet: 3d object detection using hybrid geometric primi-\ntives. In European Conference on Computer Vision, pages\n311\u2013329. Springer, 2020. 2\n[55] Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang,\nLiao Wang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang,\nMinye Wu, Lan Xu, et al. Human performance modeling\nand rendering via neural animated mesh.\narXiv preprint\narXiv:2209.08468, 2022. 2\n[56] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-\ndrew J Davison. In-place scene labelling and understanding\nwith implicit scene representation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 15838\u201315847, 2021. 3\nA. Dataset and Implementation Details\nDataset.\nOur experiments are conducted on ScanNetV2\n[7] and ARKITScenes dataset [2]. ScanNetV2 dataset is a\nchallenging dataset containing 1513 complex scenes with\naround 2.5 million RGB-D frames and annotated with se-\nmantic and instance segmentation for 18 object categories.\nSince ScanNetV2 does not provide amodal or oriented\nbounding box annotation, we predict axis-aligned bound-\ning boxes instead, as in [13, 30, 34]. We mainly evaluate\nthe methods by mAP with 0.25 IoU and 0.5 IoU threshold,\ndenoted by mAP@.25 and mAP@.50.\nARKITScenes dataset contains around 1.6K rooms with\nmore than 5000 scans. Each scan includes a series of RGB-\nD posed images. In our experiments, we utilize the subset of\nthe dataset with low-resolution images. The subset contains\n2,257 scans of 841 unique scenes, and each image in the\nscan is of size 256 \u00d7 192. We follow the dataset setting\nprovided by the official repository 1. We mainly evaluate\nthe methods by mAP with 0.25 IoU as follow [2].\nDetection Branch.\nWe follow ImVoxelNet, mainly use\nResNet50 with FPN as our backbone and the detection head\nconsists of three 3D convolution layers for classification, lo-\ncation, and centerness, respectively. For the experiment on\nthe ARKITScenes, we additionally predict the rotation. We\nuse the same size 40\u00d740\u00d716 of the voxels, with each voxel\nrepresents a cube of 0.16m, 0.16m, 0.2m. Besides, we also\n1https://github.com/apple/ARKitScenes/tree/main/threedod\nkeep the training recipe as same as ImVoxelNet. During\ntraining, we use 20 images on the ScanNet datatset and 50\nimages on the ARKITScenes dataset by default. During test\nwe use 50 images and 100 images on the ScanNet dataset\nand ARKITScenes dataset, respectively. The network is op-\ntimized by Adam optimizer with an initial learning rate set\nto 0.0002 and weight decay of 0.0001, and it is trained for\n12 epochs, and the learning rate is reduced by ten times after\nthe 8th and 11th epoch.\nNeRF Branch.\nIn our NeRF branch, 2048 rays are ran-\ndomly sampled at each iteration from 10 novel views for\nsupervision. Note that the 10 novel views are ensured to\nbe different with the views input to detection branch for\nboth training and inference. We set the near-far range as\n(0.2 meter - 8 meter), and uniformly sample 64 points along\neach ray. During volumetric rendering, if more than eight\npoints on the ray are projected to empty space, then we\nwould throw it and do not calculate the loss of the ray. The\ngeometry-MLP (G-MLP) is a 4-layer MLP with 256 hid-\nden units and skip connections. The color-MLP (C-MLP)\nis a one-layer MLP with 256 hidden units. Our experiments\nare conducted on eight V100 GPUs with 16G memory per\nGPU. We batched the data in a way such that each GPU\ncarries a single scene during training. During training, the\ntwo branches are end-to-end jointly trained. During infer-\nence, we can keep either one of the two branches for desired\ntasks. The whole Our implementation is based on MMDe-\ntection3D [5].\nB. Evaluation Protocol of Novel View Synthesis\nand Depth Estimation.\nTo evaluate the novel view synthesis and depth estima-\ntion performance, we random select 10 views of each scene\nas the novel view (indicated as target view in IBRNet [44]),\nand choose the nearby 50 views as the support views. To\nrender the RGB and depth for the 10 novel views, each\npoints shooting from the pixels of novel views would be\nprojected to the all support views to sample features, and\nthen pass into the NeRF MLP as illustrated in Method sec-\ntion. We keep the same novel view and support view for\nboth setting in Table. 6 of the main text. Note that the eval-\nuation is conducted on the test set of ScanNet dataset, which\nare never seen during training. The non-trivial results also\ndemonstrate the generazability of the proposed geometry-\naware volumetric representation.\nC. Additional Results\nAblation studies on number of views.\nWe conducted\nan analysis of how the number of views affects the perfor-\nmance of 3D detection, as shown in Table 9. Specifically,\nwe used the same number of training images (20 images)\nTable 9: Ablation on number of views. Due to the GPU\nmemory limitation, we downsample the image resolution\n2x when conduct experiments on 100 views (denoted as\nImVoxelNet-R50-2x\u2019 and NeRF-Det-R50-2x\u2019.).\nExperi-\nments on each setting run three times. We report the mean\nand standard deviations of our experiments.\nMethods\nmAP@.25\nmAP@.50\nImVoxelNet-R50-2x (10 views)\n37.8\u00b11.2\n17.5\u00b11.0\nImVoxelNet-R50-2x (20 views)\n46.5\u00b10.5\n21.1\u00b10.5\nImVoxelNet-R50-2x (50 views)\n48.4\u00b10.3\n23.7\u00b10.2\nImVoxelNet-R50-2x\u2019(100 views)\n48.1\u00b10.1\n24.7\u00b10.1\nNeRF-Det-R50-2x (10 views)\n41.4 \u00b11.0 (+3.6)\n19.2\u00b10.9 (+1.7)\nNeRF-Det-R50-2x (20 views)\n50.2 \u00b10.5 (+3.7)\n23.6\u00b10.4 (+2.5)\nNeRF-Det-R50-2x (50 views)\n51.8 \u00b10.2 (+3.4)\n26.0\u00b10.1 (+2.3)\nNeRF-Det-R50-2x\u2019(100 views)\n52.2\u00b10.1 (+4.1)\n27.4\u00b10.1 (+2.7)\nand tested with different numbers of images. Our proposed\nNeRF-Det-R50-2x showed a significant improvement in\nperformance as the number of views increased.\nIn con-\ntrast, the performance of ImVoxelNet-R50-2x had limited\nimprovement, and even worse, the performance decreased\nwhen the number of views increased to 100. We attribute\nthe performance improvements of NeRF-Det to its effec-\ntive scene modeling. NeRF performs better as the number\nof views increases, typically requiring over 100 views for\nan object [24]. Our proposed NeRF-Det inherits this advan-\ntage, leading to a drastic performance gain of 4.1 mAP@.25\nand 2.7 mAP@.50 on 100 views.\nOverall, our analysis demonstrates the effectiveness of\nour proposed NeRF-Det in leveraging multi-view observa-\ntions for 3D detection and the importance of utilizing a\nmethod that can effectively model the scene geometry.\nMore Qualitative Results We provide more visualization\nresults of novel-view synthesis and depth estimation, as\nshown in Fig. 6. The results come from the test set of Scan-\nNet. We can observe that the proposed method generalizes\nwell on the test scenes. Remarkably, it achieves non-trivial\nresults on the relatively hard cases. For example, the left\nof the second row presents a bookshelf with full of colorful\nbooks, our method can give reasonable novel-view synthe-\nsis results. On the other hand, for the left of fifth row, the\nextremely dense chairs are arranged in the scenes and we\ncan observe the method can predict accurate geometry.\nD. Discussion about outdoor 3D detection\nWe emphasize the differences of NeRF-Det and the other\n3D detection works in outdoor scenes. Our proposed NeRF-\nDet shares the similar intuition with many of outdoor 3D\ndetection works, such as [26, 45, 21], which try to learn\ngeometric-aware representations. However, the proposed\nNeRF-Det and the other works differ intrinsically. The out-\ndoor 3D detection works [26, 45, 21] propose to use cost\nvolume or explicitly predicted depth to model the scene\ngeometry. Instead, NeRF-Det leverage the discrepancy of\nFigure 6: Novel-view synthesis results on top of NeRF-Det-R50-2x*. For each triplet group, the left figure is the synthesized\nresults, the middle one is the ground truth RGB image, and the right part is the estimated depth map. Note that the visualiza-\ntion is from test set, which is never seen during training.\nmulti-view observations, i.e., the augmented variance fea-\ntures in our method section, as the priors of NeRF-MLP\ninput. Beyond the cost volume, we step forward to lever-\nage the photo-realistic principle to predict the density fields,\nand then transform it into the opacity field. Such a geom-\netry representation is novel to the 3D detection task. The\nanalysis in our experiment part also demonstrates the ad-\nvantages of the proposed opacity field. In addition to the\ndifferent method of modeling scene geometry, our design of\ncombining NeRF and 3D detection in an end-to-end manner\nallows the gradient of NeRF to back-propagate and benefit\nthe 3D detection branch. This is also different from previ-\nous NeRF-then-perception works [16, 40].\nOur NeRF-Det is specifically designed for 3D detection\nin indoor scenes, where objects are mostly static. Outdoor\nscenes present unique challenges, including difficulties in\nensuring multi-view consistency due to moving objects, un-\nbounded scene volume, and rapidly changing light condi-\ntions that may affect the accuracy of the RGB value used to\nguide NeRF learning. We plan to address these issues and\napply NeRF-Det to outdoor 3D detection in future work.\n"
  },
  {
    "title": "How to Scale Your EMA",
    "link": "https://arxiv.org/pdf/2307.13813.pdf",
    "upvote": "8",
    "text": "How to Scale Your EMA\nDan Busbridge\u2217\nJason Ramapuram\u2217\nPierre Ablin\u2217\nTatiana Likhomanenko\u2217\nEeshan Gunesh Dhekane\nXavier Suau\nRuss Webb\nApple\n{dbusbridge, jramapuram, p_ablin, antares,\neeshan, xsuaucuadros, rwebb}@apple.com\nAbstract\nPreserving training dynamics across batch sizes is an important tool for practical\nmachine learning as it enables the trade-off between batch size and wall-clock\ntime. This trade-off is typically enabled by a scaling rule, for example, in stochas-\ntic gradient descent, one should scale the learning rate linearly with the batch size.\nAnother important machine learning tool is the model EMA, a functional copy of a\ntarget model, whose parameters move towards those of its target model according\nto an Exponential Moving Average (EMA) at a rate parameterized by a momentum\nhyperparameter. This model EMA can improve the robustness and generalization\nof supervised learning, stabilize pseudo-labeling, and provide a learning signal for\nSelf-Supervised Learning (SSL). Prior works have not considered the optimization\nof the model EMA when performing scaling, leading to different training dynam-\nics across batch sizes and lower model performance. In this work, we provide a\nscaling rule for optimization in the presence of a model EMA and demonstrate\nthe rule\u2019s validity across a range of architectures, optimizers, and data modali-\nties. We also show the rule\u2019s validity where the model EMA contributes to the\noptimization of the target model, enabling us to train EMA-based pseudo-labeling\nand SSL methods at small and large batch sizes. For SSL, we enable training of\nBYOL up to batch size 24,576 without sacrificing performance, a 6\u00d7 wall-clock\ntime reduction under idealized hardware settings.\n1\nIntroduction\nWith data and models becoming progressively larger (Chen et al., 2020; Kaplan et al., 2020; Bom-\nmasani et al., 2021; Srivastava et al., 2022), the ability to reduce training wall\u2013clock time is a re-\nquirement for practical Machine Learning (ML) at scale. Optimizer scaling rules allow us to find\nfaster learning procedures that produce similar results. For example, the linear scaling rule for\nStochastic Gradient Descent (SGD) (Krizhevsky, 2014; Goyal et al., 2017), states that the learning\nrate should be scaled linearly with the batch size. This optimizer scaling works both ways. Access to\nlarger computational resources means one can train equivalent models in reduced wall-clock time.\nAlternatively, with access to limited computational resources, larger distributed computations can\nbe replicated at increased wall-clock time.\n\u2217Primary contributor. For a detailed breakdown of author contributions see Appendix J.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.13813v3  [stat.ML]  7 Nov 2023\nMany ML algorithms rely on a model EMA, a functional copy of a target model2, whose parame-\nters move towards those of its target model according to an Exponential Moving Average (EMA)\n(Definition 1.1) at a rate parameterized by a momentum hyperparameter \ud835\udf0c.\nDefinition 1.1 (EMA Update). The EMA update for the model EMA parameters \ud835\udec7\ud835\udc61 following target\nmodel parameters \ud835\udec9\ud835\udc61 at iteration \ud835\udc61 with momentum \ud835\udf0c \u2261 1 \u2212 \ud835\udefd\ud835\udf0c is\n\ud835\udec7\ud835\udc61+1 = \ud835\udf0c \ud835\udec7\ud835\udc61 + (1 \u2212 \ud835\udf0c) \ud835\udec9\ud835\udc61 \u2261 (1 \u2212 \ud835\udefd\ud835\udf0c) \ud835\udec7\ud835\udc61 + \ud835\udefd\ud835\udf0c \ud835\udec9\ud835\udc61.\n(1)\nThe model EMA has a number of desirable properties: i) the model EMA inhabits wider minima\nthan the target model, reducing overfitting and improving generalization (Ruppert, 1988; Polyak &\nJuditsky, 1992; Huang et al., 2017; Izmailov et al., 2018; He et al., 2022); ii) compared to the tar-\nget model, the model EMA moves slowly, making it useful as a stabilizer for networks governing\nBellman updates in reinforcement learning, (Lillicrap et al., 2016); and iii) the model EMA is rela-\ntively cheap to compute, whilst providing a valid model but different to the target model. This third\nproperty has made the model EMA a common choice for the teacher in many distillation setups,\nfrom semi-supervised learning (Tarvainen & Valpola, 2017; Sohn et al., 2020; Manohar et al., 2021;\nHiguchi et al., 2022), to Self-Supervised Learning (SSL) methods like Bootstrap Your Own Latent\n(BYOL) (Grill et al., 2020), DINO (Caron et al., 2021), and data2vec (Baevski et al., 2022b,a).\nDespite its significant role in optimization, a recipe for adapting the EMA Update (Definition 1.1)\nwhen changing batch size has, to the best of our knowledge, been absent. To address this, we derive\nan EMA Scaling Rule (Definition 1.2) which states how the EMA momentum \ud835\udf0c hyperparameter\nshould be modified3.\nDefinition 1.2 (EMA Scaling Rule). When computing the EMA update (Definition 1.1) of a model\nundergoing stochastic optimization with batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35, use a momentum \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05 and scale other\noptimizers according to their own scaling rules.\nIn Definition 1.2, the momentum \ud835\udf0c, which is defined at batch size \ud835\udc35, typically corresponds to a\n\u201cgood hyperparameter choice\u201d, although this does not need to be the case in general. In this paper,\nwe make the following contributions.\n1. With the assumptions of Goyal et al. (2017), we derive an EMA Scaling Rule: the EMA update\nmomentum should be scaled exponentially with the batch size (Definition 1.2).\n2. To validate this EMA Scaling Rule theoretically, we propose Stochastic Differential Equation\n(SDE) approximations of optimization in the presence of a model EMA (Section 2.2). This\nmodel EMA contributes to the loss, covering semi-supervised learning and SSL. We prove that\nthese approximations are first order weak approximations, and that our EMA Scaling Rule is\ncorrect in the SDE limit under realistic gradient assumptions (Corollary 2.1.1).\n3. We empirically validate the EMA Scaling Rule in synthetic settings (Section 3.1) and real-\nworld settings where the model EMA plays an increasingly significant role in optimization: i)\nwhere the model EMA is used during inference instead of the target model (Section 3.2); ii)\npseudo-labeling, where the model EMA (teacher) follows the target model (student), and the\nstudent is optimized on a mixture of a) labeled data and b) data without labels, whose pseudo-\nlabels are produced by the teacher (Section 3.3); and iii) self-supervised learning, which is the\nsame as the semi-supervised case, except there is no labeled data (Section 3.4).\n4. We observe that pseudo-labeling and SSL training dynamics during optimizer warm-up are not\nalways able to be replicated at large batch sizes using only the EMA Scaling Rule. We propose\nand verify practical methods to overcome this limitation, enabling us to scale to a batch size\nof 24,576 with BYOL Vision Transformers (ViTs), reducing wall-clock training by 6\u00d7 under\nidealized hardware scenarios while maintaining performance of the batch size 4096 baseline.\nFinally, to aid practitioners looking to scale, in Appendix C we provide a Scaling Toolbox, which\ngives practical advice on how to scale systematically, collecting known scaling rules, and explaining\nhow to think about the SDE perspective of optimization.\n2The target model usually undergoes gradient-based optimization, but this does not have to be the case.\n3We stress that the study of momentum in gradient-based optimizers is not the focus of this work. We refer\nto Smith & Le (2018); Li et al. (2019) for a discussion on scaling rules for these methods.\n2\n2\nThe EMA Scaling Rule\nWe begin with an informal discussion of scaling rules and motivate the existence of an exponential\nscaling rule for the momentum parameter controlling the update of the model EMA.\n2.1\nBackground and an informal discussion of scaling rules\nConsider a model with parameters \ud835\udec9\ud835\udc61 at iteration \ud835\udc61 updated with SGD (Definition 2.1).\nDefinition 2.1 (SGD Update). The SGD update for a model with parameters \ud835\udec9\ud835\udc61 at iteration \ud835\udc61 given\na minibatch B = {\ud835\udc65 (\ud835\udc4f) \u223c \ud835\udc43x : \ud835\udc4f = 1, 2, . . . , \ud835\udc35} of \ud835\udc35 = |B| samples with learning rate \ud835\udf02 is\n\ud835\udec9\ud835\udc61+1 = \ud835\udec9\ud835\udc61 \u2212 \ud835\udf02 \u00d7 1\n\ud835\udc35\n\u2211\ufe01\n\ud835\udc65 \u2208B\n\u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9\ud835\udc61),\n(2)\nwhere L is the loss function, \u2207\ud835\udf03L(\ud835\udc65;\ud835\udf03\ud835\udc61) is the parameter gradient for the sample \ud835\udc65 at iteration \ud835\udc61,\nand the \ud835\udc65 \u2208 B are Independent and Identically Distributed (i.i.d.) from \ud835\udc43x.\nIterating over a sequence of independent minibatches B0, B1, . . . , B\ud835\udf05\u22121 produces model parameters\n\ud835\udec9\ud835\udc61+\ud835\udf05 = \ud835\udec9\ud835\udc61 \u2212 \ud835\udf02 \u00d7 1\n\ud835\udc35\n\ud835\udf05\u22121\n\u2211\ufe01\n\ud835\udc57=0\n\u2211\ufe01\n\ud835\udc65 \u2208B\ud835\udc57\n\u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9\ud835\udc61+\ud835\udc57).\n(3)\nIf gradients vary slowly \u2207\ud835\udec9L(\ud835\udc65;\ud835\udf03\ud835\udc61+\ud835\udc57) \u2248 \u2207\ud835\udec9L(\ud835\udc65;\ud835\udf03\ud835\udc61), \ud835\udc57 = 0, . . . ,\ud835\udf05 \u22121, one SGD step with \u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02 on a\nbatch bB = \u222a\ud835\udc56B\ud835\udc56 of size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35 results in \u02c6\ud835\udec9\ud835\udc61+1 \u2248 \ud835\udec9\ud835\udc61+\ud835\udc58, yielding the SGD Scaling Rule (Definition 2.2).\nDefinition 2.2 (SGD Scaling Rule). When running SGD (Definition 2.1) with batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35,\nuse a learning rate \u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02 (Krizhevsky, 2014; Goyal et al., 2017).\nFor clarity in this work, we adopt the naming convention [Algorithm Name] Scaling Rule, which\nmeans all parameters of those algorithms are appropriately scaled from batch size \ud835\udc35 to \ud835\udf05\ud835\udc35.\nAs discussed in Goyal et al. (2017), although the assumption of slowly changing gradients is strong,\nif it is true, then \ud835\udec9\ud835\udc61+\ud835\udc58 \u2248 \u02c6\ud835\udec9\ud835\udc61+1 only if \u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02. The validity of the SGD Scaling Rule has been formally\nstudied. In particular, there was ambiguity regarding whether the scaling should be a square-root\nor linear (Krizhevsky, 2014). SDE approaches have resolved this ambiguity, and have been used to\nestimate the scaling \ud835\udf05 when the SGD Scaling Rule is no longer guaranteed to hold (Li et al., 2021).\nTo address model parameter EMAs, we first restate the EMA Update (Definition 1.1).\nDefinition 1.1 (EMA Update). The EMA update for the model EMA parameters \ud835\udec7\ud835\udc61 following target\nmodel parameters \ud835\udec9\ud835\udc61 at iteration \ud835\udc61 with momentum \ud835\udf0c \u2261 1 \u2212 \ud835\udefd\ud835\udf0c is\n\ud835\udec7\ud835\udc61+1 = \ud835\udf0c \ud835\udec7\ud835\udc61 + (1 \u2212 \ud835\udf0c) \ud835\udec9\ud835\udc61 \u2261 (1 \u2212 \ud835\udefd\ud835\udf0c) \ud835\udec7\ud835\udc61 + \ud835\udefd\ud835\udf0c \ud835\udec9\ud835\udc61.\n(1)\nThe model EMA parameters \ud835\udec7 do not typically receive gradient information, we take the convention\nthat \ud835\udf0c is close to one, and the \ud835\udefd\ud835\udf0c subscript will be omitted where it is clear from the context.\nAssuming again that gradients change slowly \u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9\ud835\udc61+\ud835\udc57, \ud835\udec7\ud835\udc61+\ud835\udc57) \u2248 \u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9\ud835\udc61, \ud835\udec7\ud835\udc61) \u2248 g, for gradient\ng, iterating over \ud835\udf05 independent minibatches produces model states (see Appendix E.1 for derivation)\n\"\ud835\udec9\ud835\udc61+\ud835\udf05\n\ud835\udec7\ud835\udc61+\ud835\udf05\ng\n#\n=\n\"\n1\n0\n\u2212\ud835\udf02\n(1 \u2212 \ud835\udf0c)\n\ud835\udf0c\n0\n0\n0\n1\n#\ud835\udf05\n\u00b7\n\"\ud835\udec9\ud835\udc61\n\ud835\udec7\ud835\udc61\ng\n#\n=\n\"\n\ud835\udec9\ud835\udc61 \u2212 \ud835\udf02 \ud835\udf05 g\n\ud835\udf0c\ud835\udf05 \ud835\udec7\ud835\udc61 + (1 \u2212 \ud835\udf0c\ud835\udf05) \ud835\udec9\ud835\udc61 + O \u0000\ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c\n\u0001\ng\n#\n.\n(4)\nThe first row is the SGD Scaling Rule (Definition 2.2). The third row implements the slowly chang-\ning gradients assumption for the first row. The second row is equivalent to a single EMA up-\ndate (Definition 1.1) with momentum \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05; we can take a single SGD update with batch size\n\u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35 and learning rate \u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02, and a single EMA update with momentum \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05, and we get\n(\u02c6\ud835\udec9\ud835\udc61+1, \u02c6\ud835\udec7\ud835\udc61+1) \u2248 (\ud835\udec9\ud835\udc61+\ud835\udf05, \ud835\udec7\ud835\udc61+\ud835\udf05) up to terms O(\ud835\udf02 \u00d7\ud835\udefd\ud835\udf0c). This yields the EMA Scaling Rule (Definition 1.2).\n3\nDefinition 1.2 (EMA Scaling Rule). When computing the EMA update (Definition 1.1) of a model\nundergoing stochastic optimization with batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35, use a momentum \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05 and scale other\noptimizers according to their own scaling rules.\nThe EMA Scaling Rule was derived for SGD, and is extended to other optimizers in the following\nway. An optimizer scaling rule ensures \u02c6\ud835\udec9\ud835\udc61+1 = \ud835\udec9\ud835\udc61+\ud835\udf05, satisfying identification for the first row. Next,\nthe zeroth order term in \ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c in the second row in Equation 4 is optimizer-independent, and\ntherefore unchanged. Finally, the first order terms in \ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c in the second row, corresponding to the\nscaling rule error, are an EMA accumulation of target model \ud835\udec9 updates under optimization, which is\nstill O(\ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c), although its functional form may be different for different optimizers.\nThe above discussion is intended to give an intuition for why the EMA momentum should be scaled\nexponentially. As we have used the same slow-moving gradient assumption as the original SGD\nScaling Rule, this may cast doubt on whether our rule is correct. To remove this ambiguity, we will\nfollow Smith & Le (2018); Li et al. (2021); Malladi et al. (2022), and show that the EMA Scaling\nRule (Definition 1.2) is correct in the SDE limit under more realistic gradient assumptions.\n2.2\nThe EMA Scaling Rule through the lens of stochastic differential equations\nSDEs are a tool typically used to obtain scaling rules from first principles (Li et al., 2021; Malladi\net al., 2022). In the following, we use SDEs to obtain strong theoretical guarantees for the EMA\nScaling Rule found in Section 2.1. We consider the following discrete dynamics for EMA:\n\ud835\udec9\ud835\udc58+1 = \ud835\udec9\ud835\udc58 \u2212 \ud835\udf02 g\ud835\udc58, with g\ud835\udc58 = \u2207\ud835\udc53 (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) + \ud835\udf0e \ud835\udedc\ud835\udc58, and \ud835\udedc\ud835\udc58 \u223c E\ud835\udf0e (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58),\n\ud835\udec7\ud835\udc58+1 = \ud835\udf0c \ud835\udec7\ud835\udc58 + (1 \u2212 \ud835\udf0c) \ud835\udec9\ud835\udc58,\n(5)\nwhere \ud835\udf0e > 0 is the noise scale, E\ud835\udf0e (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) is the gradient noise distribution, assumed to be zero-mean\nand variance \u03a3(\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) independent of \ud835\udf0e, and \u2207\ud835\udc53 (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) \u2261 \u2207\ud835\udec9\ud835\udc53 (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58). We posit a dependency of\nthe loss \ud835\udc53 on the EMA \ud835\udec7 in order to cover semi-supervised (Section 3.3) and SSL (Section 3.4). The\ncase of Polyak-Ruppert averaging (Section 3.2), is covered by letting \ud835\udc53 be independent of \ud835\udec7.\nWe aim to obtain an SDE approximation of Equation 5 as \ud835\udf02 goes to zero. The scaling rule for\niterations of \ud835\udec9 is well known (Li et al., 2021): we let \ud835\udf0e0 = \ud835\udf0e\u221a\ud835\udf02. The analysis of Section 2.1 gives\nthe scaling rule \u02c6\ud835\udf02 = \ud835\udf02\ud835\udf05 and \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05. Linearizing this rule near \ud835\udf02 = 0 gives \u02c6\ud835\udf0c = 1 \u2212\ud835\udf05 \u00d7 (1 \u2212 \ud835\udf0c), which\nis a linear relationship between 1 \u2212 \ud835\udf0c and \ud835\udf02. We therefore let \ud835\udefd0 = (1 \u2212 \ud835\udf0c)/\ud835\udf02 and consider the SDE\n\ud835\udc51\u0398\ud835\udc61 = \u2212\u2207\ud835\udc53 (\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61) \ud835\udc51\ud835\udc61 + \ud835\udf0e0 \u03a3(\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)\n1\n2 \ud835\udc51\ud835\udc4a\ud835\udc61, with \ud835\udc4a\ud835\udc61 a Wiener process,\n\ud835\udc51\ud835\udc4d\ud835\udc61 = \ud835\udefd0(\u0398\ud835\udc61 \u2212 \ud835\udc4d\ud835\udc61)\ud835\udc51\ud835\udc61,\n(6)\nwhere \u0398\ud835\udc61 and \ud835\udc4d\ud835\udc61 are SDE variables relating to model and EMA parameters respectively. The SDE in\nEquation 6 approximates the discrete iterations of Equation 5 when the learning rate \ud835\udf02 goes to zero.\nOne way to see this is that an Euler-Maruyama discretization of the SDE with learning rate \ud835\udf02 exactly\nrecovers the discrete iterations. More formally, we have Theorem 2.1, which is in the same spirit as\nthose found in Li et al. (2021); Malladi et al. (2022). In the theorem, \ud835\udc3a\ud835\udefc is the set of functions with\nderivatives up to order \ud835\udefc that have at most polynomial growth (see Definition D.1).\nTheorem 2.1 (SDE for SGD + EMA; informal see Theorem D.1). Assume that \ud835\udc53 is continuously\ndifferentiable, with \ud835\udc53 \u2208 \ud835\udc3a3. Let \u0398\ud835\udc61,\ud835\udc4d\ud835\udc61 be solutions of Equation 6, and \ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58 iterations of Equation 5\nwith \u03a3\n1\n2 \u2208 \ud835\udc3a2. Then, for any time horizon \ud835\udc47 > 0 and function \ud835\udc54 \u2208 \ud835\udc3a2, there exists a constant \ud835\udc50 > 0\nindependent of \ud835\udf02 such that\nmax\n\ud835\udc58=0, ... , \u230a\ud835\udc47/\ud835\udf02\u230b |E[\ud835\udc54(\u0398\ud835\udf02\ud835\udc58,\ud835\udc4d\ud835\udf02\ud835\udc58)] \u2212 E[\ud835\udc54(\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58)]| \u2264 \ud835\udc50 \u00d7 \ud835\udf02.\n(7)\nTheorem 2.1 formalizes the intuition that the SDE is an accurate approximation of the discrete\niterations. In turn, it allows validating the scaling rule in the same spirit as in Malladi et al. (2022).\nCorollary 2.1.1 (Validity of the EMA Scaling Rule). Assume that \ud835\udc53 is continuously differentiable,\nwith \ud835\udc53 \u2208 \ud835\udc3a3 and \u03a3\n1\n2 \u2208 \ud835\udc3a2. Let \ud835\udec9(\ud835\udc35)\n\ud835\udc58\n, \ud835\udec7(\ud835\udc35)\n\ud835\udc58\nbe iterations of Equation 5 with batch size \ud835\udc35 and hyperpa-\nrameters \ud835\udf02, \ud835\udf0c. Let \ud835\udec9(\ud835\udf05\ud835\udc35)\n\ud835\udc58\n, \ud835\udec7(\ud835\udf05\ud835\udc35)\n\ud835\udc58\nbe iterates with batch size \ud835\udf05\ud835\udc35, and \u02c6\ud835\udf02 determined by the SGD Scaling\nRule (Definition 2.2) and \u02c6\ud835\udf0c determined by the EMA Scaling Rule (Definition 1.2). Then, for any time\nhorizon \ud835\udc47 > 0 and function \ud835\udc54 \u2208 \ud835\udc3a2, there exists a constant \ud835\udc50 > 0 independent of \ud835\udf02 such that\nmax\n\ud835\udc58=0, ... , \u230a\ud835\udc47/\ud835\udf02\u230b |E[\ud835\udc54(\ud835\udec9(\ud835\udf05\ud835\udc35)\n\u230a\ud835\udc58/\ud835\udf05\u230b, \ud835\udec7(\ud835\udf05\ud835\udc35)\n\u230a\ud835\udc58/\ud835\udf05\u230b)] \u2212 E[\ud835\udc54(\ud835\udec9(\ud835\udc35)\n\ud835\udc58\n, \ud835\udec7(\ud835\udc35)\n\ud835\udc58\n)]| \u2264 \ud835\udc50 \u00d7 \ud835\udf02.\n(8)\n4\nTable 1: The role of the model EMA \ud835\udec7 in the optimization of (\ud835\udec9, \ud835\udec7) given a target model \ud835\udec9 for different tech-\nniques, ordered by increasing influence of the EMA model. All statements assume a momentum 0 \u2264 \ud835\udf0c < 1 and\nthat the target model \ud835\udec9 is subject to stochastic optimization at a batch size \ud835\udc35.\nTECHNIQUE\nROLE OF MODEL EMA\nPOLYAK-RUPPERT AV-\nERAGING, SEC. 3.2\n\ud835\udec9 undergoes optimization and is tracked by \ud835\udec7, which does not affect \ud835\udec9. \ud835\udec7 is an\nestimate of \ud835\udec9 with a time horizon and variance determined by \ud835\udc35 and \ud835\udf0c.\nCONTINUOUS\nPSEUDO-LABELING,\nSEC. 3.3\nPre-Training is as above in Polyak-Ruppert Averaging. After Pre-Training, \ud835\udec7\n(teacher) produces targets for \ud835\udec9 (student) from unlabeled data, which is com-\nbined with labeled data. The optimization endpoint is dependent on \ud835\udc35 and \ud835\udf0c.\nSELF-SUPERVISED\nLEARNING, SEC. 3.4\nAs above in After Pre-Training, except there is no labeled data. The optimiza-\ntion endpoint is dependent on \ud835\udc35 and \ud835\udf0c.\nCorollary 2.1.1 shows that two trajectories with different batch sizes are close in the limit of small\nlearning rate, demonstrating the validity of Definition 1.2. A natural follow-up question is what\nhappens when an adaptive optimizer is used instead of SGD? Malladi et al. (2022) study this without\nan EMA and characterize how hyperparameters change with the noise scale. In particular, they show\nthat under a high gradient noise hypothesis, there exists a limiting SDE. In Appendix D, we derive\nthe limiting SDEs for RMSProp and Adam with an EMA. Although a formal proof of closeness\nbetween the iterations and these SDEs is beyond the scope of this work, these SDEs indicate that\nthe EMA Scaling Rule holds for adaptive algorithms. We demonstrate this empirically in Section 3.\n3\nExperiments\nNow that we have derived and shown the validity of the EMA Scaling Rule, we verify it empirically.\nThe experiments validate the EMA Scaling Rule for a variety of uses of EMA and are ordered by\nincreasing influence of the role of EMA on the optimization procedure (see Table 1). The baseline\nin all of our experiments is without the EMA Scaling Rule, which applies all known relevant scaling\nrules except the EMA Scaling Rule, and represents previous best practice.\n3.1\nPolyak-Ruppert averaging in a simple setting\nAt inference, it is typical to use a model EMA, known as Polyak-Ruppert Averaging (Definition 3.1).\nDefinition 3.1 (Polyak-Ruppert Average). When optimizing model parameters \ud835\udec9, compute their\nEMA \ud835\udec7 (Definition 1.1). Use \ud835\udec7 instead of \ud835\udec9 at inference (Polyak & Juditsky, 1992; Ruppert, 1988).\nWe begin by showing the EMA Scaling Rule is required to match parameter trajectories in a simple\nsetting. Consider the optimization of \u03b8 in a noisy parabola whose loss L(\u03b8) is parameterized by\ncoefficients for curvature \ud835\udc4e > 0, scaled additive noise \ud835\udc4f \u2265 0, and additive noise \ud835\udc50 \u2265 0:\nL(\u03b8) = \ud835\udc4e\n2 \u03b82,\n\u03b8\ud835\udc58+1 = \u03b8\ud835\udc58 \u2212 \ud835\udf02 g\ud835\udc58,\ng\ud835\udc58 = \ud835\udc4e \u03b8\ud835\udc58 + \u03f5\ud835\udc58,\n\u03f5\ud835\udc58 \u223c N\n\u0010\n0,\n\ud835\udc4f g2\n\ud835\udc58+\ud835\udc50\n\ud835\udf05\n\u0011\n.\n(9)\nThe scaling factor \ud835\udf05 in the covariance denominator implements gradient noise reduction as scaling\n(i.e. batch size) increases (Jastrzebski et al., 2017). Let \u03b8 \u2208 R be optimized with SGD (Defini-\ntion 2.1) and \u03b6 \u2208 R be a Polyak-Ruppert average (Definition 3.1) for \u03b8 with momentum \ud835\udf0c = 1 \u2212 \ud835\udefd .\nAt scaling \ud835\udf05 = 1, we use \ud835\udefd\ud835\udc35 = \ud835\udf02\ud835\udc35 = 10\u22124 and \ud835\udc3c\ud835\udc35 = 104 iterations, to yield a total time\ud835\udc47 = \ud835\udc3c\ud835\udc35 \u00d7\ud835\udf02\ud835\udc35 = 1.\nTo keep gradients O(1) and gradient noise non-negligible, we take \ud835\udc4e = 1, \ud835\udc4f = 0.5, and \ud835\udc50 = 0.\nFirst, we observe the effect of scaling on a single run (Figure 1a) by tracking the position of the\nmodel EMA. We see that at scaling \ud835\udf05 = 8 or \ud835\udf05 = 256, the runs using the EMA Scaling Rule\nmatch the baseline trajectory, whereas the runs using the baseline momentum do not, with a greater\ndeviation induced by greater scaling \ud835\udf05. Even at \ud835\udf05 = 8, there is a significant difference between scaled\nand unscaled trajectories, despite the seemingly small numerical difference of their momenta4.\nSecond, we consider whether the EMA Scaling Rule is optimal. To do this, inspired by the SDE\nanalysis (Section 2.2), we define the approximation error, Err(\ud835\udf0c,\ud835\udf05,\ud835\udc54), of a test function \ud835\udc54 for a given\n4Momentum enters optimization exponentially; small changes can lead to very different updates.\n5\n0.0\n0.5\n1.0\nContinuous Time\n0.6\n0.8\n1.0\nEMA Position \u03b6t\nScaling \u03ba = 8\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 8, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 8, \u03c1 = \u03c1B\n0.0\n0.5\n1.0\nContinuous Time\nScaling \u03ba = 256\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 256, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 256, \u03c1 = \u03c1B\n(a) Trajectory of the model EMA \ud835\udec7 under different\nscalings \ud835\udf05, with 1 \u2212 \ud835\udf0c\ud835\udc35 = \ud835\udf02\ud835\udc35 = 10\u22124.\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n0.85\n0.90\n0.95\n1.00\nMomentum \u03c1\nEMA, \u03c1 = \u03c1B\nEMA, \u03c1 = \u03c1\u2217\nEMA, \u03c1 = \u03c1\u03ba\nB\nModel\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n10\u22123\n10\u22122\n10\u22121\n100\nApproximation Error\n(b) Choices for momentum (left) with corresponding ap-\nproximation errors (Equation 10) (right).\nFigure 1: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\ud835\udf05 = 1, black\ndashed) to \ud835\udf05 = 8 (left) and \ud835\udf05 = 256 (right), with (\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, blue) and without (\ud835\udf0c = \ud835\udf0c\ud835\udc35, red) the EMA Scaling Rule.\n(b, left) The momentum according for different scaling rules and the empirically optimal \ud835\udf0c\u2217 (Equation 10). (b,\nright) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange).\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\n0\n25\n50\n75\nEMA Test Top-1 (%)\nScaling \u03ba = 1/2\nB = 512, \u03c1 = \u03c1\u03ba\nB\nB = 512, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\nScaling \u03ba = 8\nB = 8192, \u03c1 = \u03c1\u03ba\nB\nB = 8192, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nFigure 2: ResNetv2-50 Polyak-Ruppert averaging on ImageNet1k for different scalings \ud835\udf05. The baseline model\n(\ud835\udf05 = 1, black dashed) uses batch size 1024 and momentum \ud835\udf0c\ud835\udc35 = 0.9999, is scaled down to a batch size of 512\n(left), and up to a batch size of 4096 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling\nRule (Definition 1.2). Bands indicate the mean and standard deviation across three runs.\nscaling \ud835\udf05 using momentum \ud835\udf0c, and the value of the momentum \ud835\udf0c\u2217(\ud835\udf05,\ud835\udc54) that minimizes this error:\n\ud835\udf0c\u2217(\ud835\udf05,\ud835\udc54) = arg min\n\ud835\udf0c\nErr(\ud835\udf0c,\ud835\udf05,\ud835\udc54),\nErr(\ud835\udf0c,\ud835\udf05,\ud835\udc54) \u2261\nmax\n\ud835\udc58=0,...,\ud835\udc47/\ud835\udf02\n\f\f\fE\ud835\udc54(\ud835\udec7\ud835\udc58) \u2212 E\ud835\udc54(\ud835\udec7(\ud835\udf05,\ud835\udf0c)\n\ud835\udc58/\ud835\udf05 )\n\f\f\f .\n(10)\nFor scalings \ud835\udf05 \u2208 {1, 2, 4, . . . , 1024}, we determine the optimal momentum \ud835\udf0c\u2217 and compare it to the\nEMA Scaling Rule (Figure 1b, left). The scaling rule tracks the \ud835\udf0c\u2217 until \ud835\udf05 = 256, when the \ud835\udf0c\u2217\nbecome systematically higher. We see target model error increase at \ud835\udf05 = 256 (Figure 1b, right). As\nthe target model error is EMA-independent, this indicates that the SGD Scaling Rule is breaking.\nAt the lower scaling \ud835\udf05 = 64, there is an inflection point in the EMA Scaling Rule approximation\nerror, before the model error grows. This difference indicates the O(\ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c) terms of Equation 4 are\nbeginning to influence the EMA update. Finally, these observations are true in \ud835\udc37 = 100 dimensions,\n(Appendix F.1), and we stress that not changing the momentum at every scaling \ud835\udf05 induces large\napproximation error, indicating there is merit to using the EMA Scaling Rule.\n3.2\nSupervised learning on real data with Polyak-Ruppert averaging\nWe now turn to real-world classification where the target model \ud835\udec9 optimizes a parametric log-\nlikelihood max\ud835\udec9 log\ud835\udc5d(y|x; \ud835\udec9) with inputs and labels (x, y) drawn from a joint distribution \ud835\udc5d(y, x).\nImage Classification\nWe consider a variant of the original SGD Scaling Rule result (Goyal et al.,\n2017) and train a ResNetv2 (He et al., 2016b) on ImageNet1k (Russakovsky et al., 2014) (Figure 2)\nusing a three step learning rate schedule. The base momentum \ud835\udf0c\ud835\udc35 = 0.9999 at batch size 1024\nwas found by hyperparameter optimizing for EMA test performance, and we seek to achieve this\noptimized performance at different batch sizes. We do not apply the EMA Scaling Rule on the Batch\nNormalization (Ioffe & Szegedy, 2015) statistics5. We observe that without the EMA Scaling Rule,\nthere is a significant drop in model EMA test performance, whereas with the EMA Scaling Rule, we\n5Since Batch Normalization statistics use an EMA update, it is reasonable to ask whether the EMA Scaling\nRule should be applied. We investigate this in Appendix F.3. We find one should apply the scaling rule,\nhowever, the effect is less significant than the application of the EMA Scaling Rule to model parameters.\n6\ncan approximate the baseline model EMA test top-1 performance across all batch sizes. We match\nbaseline EMA statistics across the full trajectory batch size 2048, where the test EMA performance\ndiverges. This is due to non-EMA test performance dropping for high \ud835\udf05 (see Appendix F.2). We\nobserve that model EMA top-1 is approximately 0.2% to 0.3% higher than the target model.\nAutomatic Speech Recognition (ASR)\nWe train a transformer (Vaswani et al., 2017) using the\nConnectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Adam optimizer on the\ntrain-clean-100 subset (100h) of LibriSpeech (Panayotov et al., 2015) (for details see Appendix G).\nWe apply the Adam Scaling Rule (Malladi et al. (2022), Definition C.3) and use dynamic batching\n(minibatch size \u00d7 sequence length = const = 290\ud835\udc60, and \ud835\udc60 indicates audio duration in seconds).\nWithout the EMA Scaling Rule, there is a significant difference in model EMA test Word Error\nRate (WER) trajectories compared to the baseline, whereas with the EMA Scaling Rule, trajecto-\nries match, as is shown in Figure 3. We note that compared to image classification, in ASR, the\nmodel EMA converges to similar final performance irrespective of use of the scaling rule. This con-\nvergence is due to the longer training time compared to the EMA horizon as discussed in Table 1\n(see Appendix E.2 for a proof sketch). Although in this specific case one can achieve similar final\nperformance without the EMA Scaling Rule, it is necessary to use the EMA Scaling Rule in order\nto replicate the full training trajectory, which gives guarantees on properties like final performance\n(see Corollary 2.1.1). We also observe a growing gap between the baseline and EMA-scaled trajec-\ntories as we increase \ud835\udf05. Inspecting the train loss and non-EMA test WER, which do not depend on\nthe EMA update (see Figure 14, Appendix G.1), indicates this is due to a breakdown of the Adam\nScaling Rule. In summary, evaluation on ASR shows that the EMA Scaling Rule holds in practice\nfor sequential data with dynamic batch sizes, as well as when using adaptive optimization.\n0\n2\n4\nTrain Steps\n\u00d7105\n50\n100\nEMA test-other WER\nScaling \u03ba = 1/4\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n2\n4\nTrain Steps\n\u00d7105\nScaling \u03ba = 1/2\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n2\n4\nTrain Steps\n\u00d7105\nScaling \u03ba = 2\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n2\n4\nTrain Steps\n\u00d7105\nScaling \u03ba = 4\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nFigure 3: Transformer Polyak-Ruppert averaging on LibriSpeech (trained on train-clean-100) with different\nscalings \ud835\udf05. The baseline (\ud835\udf05 = 1, black dashed) is trained with Adam and momentum \ud835\udf0c\ud835\udc35 = 0.99995 at a dynamic\nbatch size \ud835\udc35 = 8 \u00d7 290\ud835\udc60, which corresponds to a single train step on the \ud835\udc65-axis. We investigate dynamic batch\nsizes down to \ud835\udc35 = 2 \u00d7 290\ud835\udc60 (left) and up to \ud835\udc35 = 32 \u00d7 290\ud835\udc60 (right), with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35), and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35)\nthe EMA Scaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout.\n3.3\nSemi-supervised speech recognition via pseudo-labeling\nWe continue using the same ASR model and training pipeline of Section 3.2. However, we consider\nsemi-supervised learning via continuous pseudo-labeling where labeled (train-clean-100, 100h) and\nunlabeled (the rest of LibriSpeech, 860h) data are given during training, and the model EMA is in-\nvolved in the overall optimization (Likhomanenko et al., 2021a, 2022; Manohar et al., 2021; Higuchi\net al., 2022). We first pre-train a target model (student) on a limited labeled set for a short period (e.g.\n20k steps of \ud835\udc35 = 8 \u00d7 290\ud835\udc606). Concurrently, the student updates a model EMA (teacher). After pre-\ntraining, we continue training the student with both labeled and unlabeled data, with the teacher first\ntranscribing unlabeled data from the batch producing Pseudo-Labels (PLs). These PLs are treated\nby the student as ground-truth transcriptions, and standard supervised optimization is performed.\nCompared to Polyak-Ruppert Averaging (Section 3.2), where the model EMA plays no role in the\njoint optimization, we observe that in PL it is essential to employ the EMA Scaling Rule in order to\nmatch the model trajectories at scaled batch sizes. When the EMA Scaling Rule is not used, Figure 4\nreveals a significant difference in PL quality trajectory, leading to a higher test WER.\nFor \ud835\udf05 > 2, we found the Adam Scaling Rule does not perfectly match the reference trajectory in\nthe pre-training phase. This results in a significantly different PL quality at the start of pseudo-\nlabeling (20k steps of \ud835\udc35 = 8 \u00d7 290\ud835\udc60), which affects the training dynamics (Berrebbi et al., 2023). To\n6Note that number of steps is batch size dependent and should be scaled by 1/\ud835\udf05 (see Appendix C).\n7\n0\n50\nPL WER\nScaling \u03ba = 1/2\nScaling \u03ba = 2\nScaling \u03ba = 4\nScaling \u03ba = 8\n0\n2\n4\nTrain Steps\n\u00d7105\n0\n50\n100\ntest-other WER\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n2\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n2\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n2\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 64\u00d7290s,\u03c1 = \u03c1B\nB = 64\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nFigure 4: Transformer pseudo-labeling on LibriSpeech with different scalings \ud835\udf05. The baseline (\ud835\udf05 = 1, black\ndashed) is trained with Adam at a dynamic batch size of 8\u00d7290 seconds, which corresponds to a single train step\non the \ud835\udc65-axis. The model EMA (teacher) is updated with momentum \ud835\udf0c\ud835\udc35 = 0.9999. We investigate dynamic\nbatch sizes down to \ud835\udc35 = 4 \u00d7 290\ud835\udc60 (left) and up to \ud835\udc35 = 64 \u00d7 290\ud835\udc60 (right), with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without\n(red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used\nthroughout. For \ud835\udf05 \u2264 2, we start pseudo-labeling after 20k/\ud835\udf05 training steps; while for \ud835\udf05 > 2, we start when\npre-training WER matches the baseline WER.\nalleviate the Adam Scaling Rule mismatch effect for \ud835\udf05 > 2, we postpone the pseudo-labeling until\npre-training on labeled data gives similar validation WER, see Appendix G. With this heuristic, we\ncan match the baseline trajectory with the EMA Scaling Rule up to \ud835\udf05 = 8 (Figure 4).\nIn summary, (a) model EMA affects the optimization process of pseudo-labeling in ASR resulting in\nthe necessity of EMA Scaling Rule to be applied while scaling the batch size; (b) an optimizer scaling\nrule breakdown results in the EMA Scaling Rule breakdown but this effect can be alleviated by longer\npre-training on labeled data having similar PLs quality at the start across different scalings.\n3.4\nSelf-supervised image representation learning\nFinally, we turn our attention to distillation based Self-Supervised Learning (SSL). where the model\nEMA is the teacher (Grill et al., 2020; Niizumi et al., 2023; Caron et al., 2021; Oquab et al., 2023).\nWe will use BYOL (Grill et al. (2020), Definition 1.1)7 for our investigation into scaling as it is well-\nstudied (Tian et al., 2021; Richemond et al., 2023), relatively simple to implement due to minimal\nhyper-parameters, and obtains competitive results (Grill et al., 2020; Koppula et al., 2022). Since\nBYOL learns through self-referential distillation, momentum plays a significant role in optimization.\nWe analyze: i) a ResNet-18 (He et al., 2016a) on CIFAR10 (Krizhevsky et al., 2014) (Figure 5) using\nSGD (Definition 2.1); and ii) a ViT-B/16 (Dosovitskiy et al., 2021) on ImageNet1k using AdamW\n(Loshchilov & Hutter, 2019). A recipe for BYOL using ViTs is provided in Appendix H.3.\nResNet-18 on CIFAR-10 We begin with a ResNet-18 model and short training duration to enable\nquick iteration, and an SGD optimizer as it has as known scaling rule. This allows us to probe the\nEMA Scaling Rule without potential confounders like poor gradient-based optimizer scaling8.\nWe observe that without the EMA Scaling Rule, there is a drop in test top-1 linear probe (Defini-\ntion H.3) performance compared to the baseline, whereas with the EMA Scaling Rule, we closely\nmatch the baseline model until batch size 4096. We show that this result is consistent for a range of\nbase learning rates \ud835\udf02\ud835\udc35 and momenta \ud835\udf0c\ud835\udc35 in Appendix H.8. At batch size 8192, we see a performance\ngap between the scaled model using the EMA Scaling Rule and the baseline. We speculate that this\nis due to dynamics early in the BYOL training process that are challenging to replicate at larger batch\nsizes. To test, and potentially circumvent this, we introduce Progressive Scaling (Definition 3.2).\nDefinition 3.2 (Progressive Scaling, informal; see Appendix C.4). Given batch size \ud835\udc35 and hyperpa-\nrameters at \ud835\udc35, slowly increase the batch size to the desired largest batch size during training. At any\nintermediate batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35, all hyperparameters are scaled according to their scaling rules.\n7The BYOL EMA update (Equation 74) uses \ud835\udec9\ud835\udc61+1 instead of our analyzed \ud835\udec9\ud835\udc61 (Equation 4). The effect upon\nthe overall EMA update is O(\ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c) and so is captured by the EMA Scaling Rule (Definition 1.2).\n8For competitive performance with the reference BYOL (Grill et al., 2020) using a ResNet-50, adaptive\noptimization, and longer training duration, see Appendix H.10 and Figure 26.\n8\n20\n40\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 8\nB = 8192, \u03c1 = \u03c1\u03ba\nB\nB = 8192, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nProgressive Scaling \u03ba = 8\nB = 8192@10, \u03c1 = \u03c1\u03ba\nB\nB = 8192@30, \u03c1 = \u03c1\u03ba\nB\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.3\n0.4\n0.5\n0.6\n0.7\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n0\n25\n50\n75\n100\nTrain Epochs\n0\n25\n50\n75\n100\nTrain Epochs\nFigure 5: ResNet-18 BYOL on CIFAR10 for different \ud835\udf05. The baseline (\ud835\udf05 = 1, black dashed) uses batch size 1024\nand momentum \ud835\udf0c\ud835\udc35 = 0.992, and is scaled from batch size 2048 (left) to 8192 (third) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and\nwithout (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling Rule. At \ud835\udf05 = 8 we also run progressive scaling (right), with transitions\nat 10 (green) and 30 (orange) epochs. Bands indicate mean and standard deviation across three runs.\n0\n25\n50\n75\nTest Top-1 (%)\nProgressive Scaling \u03ba = 2\nProgressive Scaling \u03ba = 4\nProgressive Scaling \u03ba = 6\nProgressive Scaling \u03ba = 8\n0\n100\n200\n300\nTrain Epochs\n10\u22121\n100\nTrain Loss\nB = 8192, \u03c1 = \u03c1\u03ba\nB\nB = 8192, \u03c1 = \u03c1B\nB = 4096, \u03c1 = \u03c1B\n0\n100\n200\n300\nTrain Epochs\nB = 16384, \u03c1 = \u03c1\u03ba\nB\nB = 16384, \u03c1 = \u03c1B\nB = 4096, \u03c1 = \u03c1B\n0\n100\n200\n300\nTrain Epochs\nB = 24576, \u03c1 = \u03c1\u03ba\nB\nB = 24576, \u03c1 = \u03c1B\nB = 24576, \u03c1 = \u03c1\u03ba\nB\nB = 24576, \u03c1 = \u03c1B\nB = 4096, \u03c1 = \u03c1B\n0\n100\n200\n300\nTrain Epochs\nB = 32768, \u03c1 = \u03c1\u03ba\nB\nB = 32768, \u03c1 = \u03c1B\nB = 4096, \u03c1 = \u03c1B\nFigure 6: BYOL ViT-B/16 on ImageNet1k for different scalings \ud835\udf05. The baseline model (\ud835\udf05 = 1, black dashed) uses\nbatch size 4096 and teacher momentum \ud835\udf0c\ud835\udc35 = 0.99, and is scaled from batch size 8192 (left) to 32768 (right)\nwith progressive scaling and the EMA Scaling Rule (Definition 3.2) (orange, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35), with the EMA Scaling\nRule but without progressive scaling (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35), without the EMA Scaling Rule but with progressive scaling\n(purple, \ud835\udf0c = \ud835\udf0c\ud835\udc35), and without either (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35). Progressive scaling transitions from the reference model at\nepoch 60. See Appendix H.6 for a discussion on BYOL progressive scaling.\nWe see that transitioning to the higher batch size during the warmup period results in a model\noptimization trajectory that diverges from the baseline, whereas transitioning after warmup results in\nmatching final trajectories of the scaled and baseline models. In summary, progressive scaling allows\nus to match BYOL dynamics at large batch sizes, provided we transition after the warmup period.\nThis observation is consistent with our hypothesis regarding BYOL dynamics during warmup.\nVision Transformers on ImageNet1k\nProgressive Scaling coupled with the EMA Scaling Rule\nis required when scaling BYOL ViTs (Figure 6), enabling baseline loss tracking to a batch size of\n24,576. Perfect scaling fails at batch size 32,768, consistent with observations in supervised learning\n(Goyal et al., 2017; Huo et al., 2021). Despite the breakdown, there is only a small drop in 1.6%\nprobe performance when using the EMA Scaling Rule, compared to as 44.56% drop without it. We\nalso observe that it is sometimes possible to match test model performance using only Progressive\nScaling and not the EMA Scaling Rule, although this still induces a training loss mismatch. We\nstress that such an approach is not guaranteed to work and discuss when this approach succeeds and\nfails in Appendix H.6 and Figure 22.\n9\nAt the transition point between batch sizes, an impulse perturbation9 is measured at the student,\nvisible from the training loss. This is recovered from by the learning process, and the new model\nmatches the reference batch size. This perturbation happens in both the AdamW and SGD settings,\nleading us to suspect this is due to the BYOL learning process, rather than an artifact of optimizer or\nmomentum scaling. However, since this is not directly related to the EMA Scaling Rule proposed\nin this work, we defer this analysis to future investigation.\n4\nRelated work\nOptimizer scaling rules from SDEs The SDE perspective has uncovered optimizer scaling rules\nand allowed an understanding of their limitations. Smith & Le (2018) used SDEs to uncover the\nSGD Scaling Rule, while (Li et al., 2021) used SDEs to explain that rule\u2019s breakdown in terms of\ndiscretization error. The SDE analysis was extended to adaptive optimization by (Malladi et al.,\n2022), producing an Adam Scaling Rule (Definition C.3), indicating that along with the learning\nrate, the \ud835\udefd1,2 and \ud835\udf16 parameters transform. The \ud835\udefd1,2 transformation is consistent with the EMA Scaling\nRule in the SDE limit. Our work differs as it considers a model EMA that alters the objective.\nVarying the batch size during training Smith et al. (2018) investigated the benefits of scheduling\nthe batch size at a fixed learning rate as an alternative to scheduling the learning rate at a fixed batch\nsize. These two are equivalent through the SGD Scaling Rule. The authors do not scale the optimizer\nhyperparameters during this procedure, as they are intentionally replicating the training dynamics of\na learning rate schedule. This is in contrast with Progressive Scaling (Definition 3.2) which scales\nthe hyperparameters to maintain the optimization process at different levels of discretization.\nLarge batch training of SSL distillation methods SSL methods learn representations without\nlabels, meaning they can take advantage of web-scale data. Large batch optimization is required to\nmake use of this data in a reasonable amount of time. Grill et al. (2020) demonstrated algorithmic\nrobustness when reducing the batch size through gradient accumulation and EMA update skipping,\nwhich implements an approximation of our EMA Scaling Rule for \ud835\udf05 < 1. Our work provides a\nrecipe to scale down and up in \ud835\udf05. MoCo-v3 (Chen et al., 2021) enables contrastively distilled ViTs\nup to a batch size of 6144, where the model drops in performance. More recently, methods like\nDINO (Caron et al., 2020) present a worse scenario, and are unable to scale beyond batch size 1024\n(Koppula et al., 2022). In contrast, our work presents practical tools to scale to large batch sizes in\nthe presence of an EMA, enabling practical training of these SSL methods on large scale data.\n5\nConclusion\nWe provide an EMA Scaling Rule: when changing the batch size by a factor of \ud835\udf05, exponentiate the\nmomentum of the EMA update to the power of \ud835\udf05. This scaling rule should be applied in addition to\noptimizer scaling rules (for example, linearly scaling the SGD learning rate), and enables the scaling\nof methods which rely on EMA and are sensitive to the choice of EMA momentum.\nWe prove the validity of the EMA Scaling Rule by deriving first-order SDE approximations of dis-\ncrete model optimization when a model EMA is present and can contribute to the model objective.\nWe demonstrate empirical support for a variety of uses of EMA, ordered by increasing influence of\nthe role of EMA on the optimization procedure: supervised model tracking (i.e. Polyak-Ruppert\naveraging) in speech and vision domains, pseudo-labeling in speech, and self-supervised image rep-\nresentation learning. In almost all scenarios, using the EMA Scaling Rule enables matching of\ntraining dynamics under batch size modification, whereas not using it results in significant differ-\nences in optimization trajectories. For example, we can scale the BYOL self-supervised method to\na batch size of 24,576 without any performance loss only when using the EMA Scaling Rule.\nWhile learning rate scaling rules are relatively commonplace in ML, the role of EMA has been\noverlooked. With this work, we highlight the importance of scaling the EMA momentum, and hope\nthat future works will use the EMA Scaling Rule to scale the EMA momentum correctly, in the same\nway that learning rates and other optimizer hyperparameters are scaled.\n9Instead of a single large batch transition as in Figure 6 we perform a sequential transition in Appendix H.5.\nWe find that a slow increase in batch size minimizes the magnitude of the perturbation and leads to a final\nmodel with higher effective linear probe top-1 than the reference by approximately 1.17%.\n10\n6\nAcknowledgements\nWe thank Miguel Sarabia del Castillo, Adam Golinski, Pau Rodriguez Lopez, Skyler Seto, Ami-\ntis Shidani, Barry Theobald, Vimal Thilak, Floris Weers, Luca Zappella, and Shaungfei Zhai for\ntheir helpful feedback and critical discussions throughout the process of writing this paper; Okan\nAkalin, Hassan Babaie, Denise Hui, Mubarak Seyed Ibrahim, Li Li, Cindy Liu, Rajat Phull, Evan\nSamanas, Guillaume Seguin, and the wider Apple infrastructure team for assistance with developing\nand running scalable, fault tolerant code; and Kaifeng Lyu and Abhishek Panigrahi for discussion\nand details regarding scaling rules for adaptive optimizers. Names are in alphabetical order by last\nname within group.\nReferences\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization.\nCoRR,\nabs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.\nAlexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli.\nEfficient self-supervised learn-\ning with contextualized target representations for vision, speech and language.\nCoRR,\nabs/2212.07525, 2022a. doi: 10.48550/arXiv.2212.07525. URL https://doi.org/10.\n48550/arXiv.2212.07525.\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec:\nA general framework for self-supervised learning in speech, vision and language. In International\nConference on Machine Learning, pp. 1298\u20131312. PMLR, 2022b.\nDan Berrebbi, Ronan Collobert, Samy Bengio, Navdeep Jaitly, and Tatiana Likhomanenko. Contin-\nuous pseudo-labeling from the start. In The Eleventh International Conference on Learning Rep-\nresentations, 2023. URL https://openreview.net/forum?id=m3twGT2bAug.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen\nCreel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Dur-\nmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori\nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,\nThomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keel-\ning, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Ku-\nditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258,\n2021. URL https://arxiv.org/abs/2108.07258.\nAndy Brock, Soham De, Samuel L. Smith, and Karen Simonyan. High-performance large-scale\nimage recognition without normalization. In Marina Meila and Tong Zhang (eds.), Proceedings\nof the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learning Research, pp. 1059\u20131071. PMLR, 2021.\nURL http://proceedings.mlr.press/v139/brock21a.html.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand\nJoulin.\nUnsupervised learning of visual features by contrasting cluster assignments.\nIn\nHugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-\nTien Lin (eds.), Advances in Neural Information Processing Systems 33:\nAnnual Confer-\nence on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n70feb62b69f16e0238f741fab228fec2-Abstract.html.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Ar-\nmand Joulin. Emerging properties in self-supervised vision transformers. CoRR, abs/2104.14294,\n2021. URL https://arxiv.org/abs/2104.14294.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hin-\nton.\nBig self-supervised models are strong semi-supervised learners.\nIn Hugo Larochelle,\n11\nMarc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),\nAdvances in Neural Information Processing Systems 33:\nAnnual Conference on Neu-\nral Information Processing Systems 2020,\nNeurIPS 2020,\nDecember 6-12,\n2020,\nvir-\ntual, 2020.\nURL https://proceedings.neurips.cc/paper/2020/hash/\nfcbc95ccdd551da181207c0c1400c655-Abstract.html.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vi-\nsion transformers.\nIn 2021 IEEE/CVF International Conference on Computer Vision, ICCV\n2021, Montreal, QC, Canada, October 10-17, 2021, pp. 9620\u20139629. IEEE, 2021.\ndoi:\n10.1109/ICCV48922.2021.00950.\nURL https://doi.org/10.1109/ICCV48922.\n2021.00950.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition\nat scale.\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/\nforum?id=YicbFdNTTy.\nJohn C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learn-\ning and stochastic optimization.\nIn Adam Tauman Kalai and Mehryar Mohri (eds.), COLT\n2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pp. 257\u2013\n269. Omnipress, 2010.\nURL http://colt2010.haifa.il.ibm.com/papers/\nCOLT2010proceedings.pdf#page=265.\nAbe\nFetterman\nand\nJosh\nAlbrecht.\nUnderstanding\nself-supervised\nand\ncontrastive\nlearning\nwith\n\"bootstrap\nyour\nown\nlatent\"\n(byol),\nAug\n2020.\nURL\nhttps://generallyintelligent.ai/\nunderstanding-self-supervised-contrastive-learning.html.\nPriya Goyal, Piotr Doll\u00e1r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training im-\nagenet in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.\n02677.\nAlex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. Connectionist tem-\nporal classification: labelling unsegmented sequence data with recurrent neural networks. In\nProceedings of the 23rd international conference on Machine learning, pp. 369\u2013376, 2006.\nJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond,\nElena Buchatskaya, Carl Doersch, Bernardo \u00c1vila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko.\nBoot-\nstrap your own latent - A new approach to self-supervised learning.\nIn Hugo Larochelle,\nMarc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),\nAdvances in Neural Information Processing Systems 33:\nAnnual Conference on Neu-\nral Information Processing Systems 2020,\nNeurIPS 2020,\nDecember 6-12,\n2020,\nvir-\ntual, 2020.\nURL https://proceedings.neurips.cc/paper/2020/hash/\nf3ada80d5c4ee70142b17b8192b2958e-Abstract.html.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio Ranzato. Revisiting self-training for neural\nsequence generation.\nIn International Conference on Learning Representations, 2020.\nURL\nhttps://openreview.net/forum?id=SJgdnAVKDH.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification. In 2015 IEEE International Conference on\nComputer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026\u20131034. IEEE Com-\nputer Society, 2015. doi: 10.1109/ICCV.2015.123. URL https://doi.org/10.1109/\nICCV.2015.123.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016a.\ndoi:\n10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\n12\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vi-\nsion - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pp. 630\u2013645.\nSpringer, 2016b.\ndoi: 10.1007/978-3-319-46493-0\\_38.\nURL https://doi.org/10.\n1007/978-3-319-46493-0_38.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross B. Girshick. Masked\nautoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 15979\u201315988.\nIEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10.1109/\nCVPR52688.2022.01553.\nYosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Momentum pseudo-labeling:\nSemi-supervised asr with continuously improving pseudo-labels. IEEE Journal of Selected Topics\nin Signal Processing, 16(6):1424\u20131438, 2022.\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger.\nSnapshot ensembles: Train 1, get M for free. In 5th International Conference on Learning Repre-\nsentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Open-\nReview.net, 2017. URL https://openreview.net/forum?id=BJYwwY9ll.\nZhouyuan Huo, Bin Gu, and Heng Huang.\nLarge batch optimization for deep learning using\nnew complete layer-wise adaptive rate scaling. In Thirty-Fifth AAAI Conference on Artificial\nIntelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial In-\ntelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelli-\ngence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 7883\u20137890. AAAI Press, 2021. URL\nhttps://ojs.aaai.org/index.php/AAAI/article/view/16962.\nIEEE. Ieee standard for floating-point arithmetic. IEEE Std 754-2019 (Revision of IEEE 754-2008),\npp. 1\u201384, 2019. doi: 10.1109/IEEESTD.2019.8766229.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In Francis R. Bach and David M. Blei (eds.), Proceedings of the\n32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference Proceedings, pp. 448\u2013456. JMLR.org, 2015. URL\nhttp://proceedings.mlr.press/v37/ioffe15.html.\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wil-\nson. Averaging weights leads to wider optima and better generalization. In Amir Globerson and\nRicardo Silva (eds.), Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial In-\ntelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pp. 876\u2013885. AUAI Press,\n2018. URL http://auai.org/uai2018/proceedings/papers/313.pdf.\nStanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Ben-\ngio, and Amos J. Storkey. Three factors influencing minima in SGD. CoRR, abs/1711.04623,\n2017. URL http://arxiv.org/abs/1711.04623.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\nDiederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\nIn Yoshua\nBengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\n//arxiv.org/abs/1412.6980.\nLeslie Kish.\nSurvey Sampling, volume 59.\nCambridge University Press, 1965.\ndoi: 10.1017/\nS0003055400132113.\nSkanda Koppula, Yazhe Li, Evan Shelhamer, Andrew Jaegle, Nikhil Parthasarathy, Relja Arand-\njelovic, Jo\u00e3o Carreira, and Olivier J. H\u00e9naff. Where should I spend my flops? efficiency evalu-\nations of visual pre-training methods. CoRR, abs/2209.15589, 2022. doi: 10.48550/arXiv.2209.\n15589. URL https://doi.org/10.48550/arXiv.2209.15589.\n13\nAlex Krizhevsky.\nOne weird trick for parallelizing convolutional neural networks.\nCoRR,\nabs/1404.5997, 2014. URL http://arxiv.org/abs/1404.5997.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-\nsearch). 2014. URL http://www.cs.toronto.edu/~kriz/cifar.html.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.\nVisualizing the\nloss landscape of neural nets.\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle,\nKristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural\nInformation Processing Systems 31:\nAnnual Conference on Neural Information Process-\ning Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pp. 6391\u2013\n6401, 2018.\nURL https://proceedings.neurips.cc/paper/2018/hash/\na41b3bb3e6b050b6c9067c67f663b915-Abstract.html.\nQianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic\ngradient algorithms i: Mathematical foundations. The Journal of Machine Learning Research, 20\n(1):1474\u20131520, 2019.\nZhiyuan Li, Sadhika Malladi, and Sanjeev Arora.\nOn the validity of modeling SGD\nwith stochastic differential equations (sdes).\nIn Marc\u2019Aurelio Ranzato, Alina Beygelz-\nimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances\nin Neural Information Processing Systems 34:\nAnnual Conference on Neural Informa-\ntion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 12712\u2013\n12725, 2021.\nURL https://proceedings.neurips.cc/paper/2021/hash/\n69f62956429865909921fa916d61c1f8-Abstract.html.\nTatiana Likhomanenko, Qiantong Xu, Jacob Kahn, Gabriel Synnaeve, and Ronan Collobert. slimipl:\nLanguage-model-free iterative pseudo-labeling. Proc. Interspeech, 2021a.\nTatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov.\nCape: Encoding relative positions with continuous augmented positional embeddings. Advances\nin Neural Information Processing Systems, 34, 2021b.\nTatiana Likhomanenko, Ronan Collobert, Navdeep Jaitly, and Samy Bengio.\nContinuous soft\npseudo-labeling in ASR. In I Can\u2019t Believe It\u2019s Not Better Workshop: Understanding Deep Learn-\ning Through Empirical Falsification, 2022. URL https://openreview.net/forum?\nid=aoiqVW4ui51.\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua\nBengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR\n2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http:\n//arxiv.org/abs/1509.02971.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Con-\nference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Open-\nReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\nSadhika\nMalladi,\nKaifeng\nLyu,\nAbhishek\nPanigrahi,\nand\nSanjeev\nArora.\nOn\nthe\nsdes\nand\nscaling\nrules\nfor\nadaptive\ngradient\nalgorithms.\nIn\nNeurIPS,\n2022.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n32ac710102f0620d0f28d5d05a44fe08-Abstract-Conference.html.\nVimal Manohar, Tatiana Likhomanenko, Qiantong Xu, Wei-Ning Hsu, Ronan Collobert, Yatharth\nSaraf, Geoffrey Zweig, and Abdelrahman Mohamed. Kaizen: Continuously improving teacher\nusing exponential moving average for semi-supervised speech recognition. In 2021 IEEE Auto-\nmatic Speech Recognition and Understanding Workshop (ASRU), pp. 518\u2013525. IEEE, 2021.\nDaisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. BYOL\nfor audio: Exploring pre-trained general-purpose audio representations. IEEE ACM Trans. Audio\nSpeech Lang. Process., 31:137\u2013151, 2023. doi: 10.1109/TASLP.2022.3221007. URL https:\n//doi.org/10.1109/TASLP.2022.3221007.\n14\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khali-\ndov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud As-\nsran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan\nMisra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 J\u00e9gou, Julien Mairal,\nPatrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDinov2: Learning robust visual fea-\ntures without supervision. CoRR, abs/2304.07193, 2023. doi: 10.48550/arXiv.2304.07193. URL\nhttps://doi.org/10.48550/arXiv.2304.07193.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations, 2019.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus\nbased on public domain audio books.\nIn 2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 5206\u20135210. IEEE, 2015.\nDaniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and\nQuoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.\nProc. Interspeech 2019, pp. 2613\u20132617, 2019.\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838\u2013855, 1992.\ndoi: 10.1137/0330046.\nURL\nhttps://doi.org/10.1137/0330046.\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan L. Yuille. Weight standardization.\nCoRR, abs/1903.10520, 2019. URL http://arxiv.org/abs/1903.10520.\nPierre H. Richemond, Jean-Bastien Grill, Florent Altch\u00e9, Corentin Tallec, Florian Strub, Andrew\nBrock, Samuel L. Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. BYOL\nworks even without batch statistics. CoRR, abs/2010.10241, 2020. URL https://arxiv.\norg/abs/2010.10241.\nPierre H. Richemond, Allison C. Tam, Yunhao Tang, Florian Strub, Bilal Piot, and Felix Hill.\nThe edge of orthogonality: A simple view of what makes BYOL tick. CoRR, abs/2302.04817,\n2023. doi: 10.48550/arXiv.2302.04817. URL https://doi.org/10.48550/arXiv.\n2302.04817.\nDavid Ruppert. Efficient estimations from a slowly convergent robbins-monro process. 1988.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, and Li Fei-Fei. Imagenet large\nscale visual recognition challenge. CoRR, abs/1409.0575, 2014. URL http://arxiv.org/\nabs/1409.0575.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-\ntions. In Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.\n464\u2013468, 2018.\nSamuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient\ndescent. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\nURL https://openreview.net/forum?id=BJij4yg0Z.\nSamuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don\u2019t decay the learning rate,\nincrease the batch size. In 6th International Conference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,\n2018. URL https://openreview.net/forum?id=B1Yy1BxCZ.\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel,\nEkin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised\nlearning with consistency and confidence. Advances in neural information processing systems,\n33:596\u2013608, 2020.\n15\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W.\nKocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda\nAskell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea\nSantilli, Andreas Stuhlm\u00fcller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou,\nAngela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher\nMullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and\net al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language\nmodels. CoRR, abs/2206.04615, 2022. doi: 10.48550/arXiv.2206.04615. URL https://\ndoi.org/10.48550/arXiv.2206.04615.\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con-\nsistency targets improve semi-supervised deep learning results. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\nGarnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,\npp. 1195\u20131204, 2017. URL https://proceedings.neurips.cc/paper/2017/\nhash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html.\nYuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynam-\nics without contrastive pairs. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol-\nume 139 of Proceedings of Machine Learning Research, pp. 10268\u201310278. PMLR, 2021. URL\nhttp://proceedings.mlr.press/v139/tian21a.html.\nTijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331,\n2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn Isabelle Guyon, Ulrike\nvon Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Ro-\nman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,\npp. 5998\u20136008, 2017. URL https://proceedings.neurips.cc/paper/2017/\nhash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nYuxin Wu and Kaiming He. Group normalization. In Vittorio Ferrari, Martial Hebert, Cristian\nSminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference,\nMunich, Germany, September 8-14, 2018, Proceedings, Part XIII, volume 11217 of Lecture Notes\nin Computer Science, pp. 3\u201319. Springer, 2018.\ndoi: 10.1007/978-3-030-01261-8\\_1.\nURL\nhttps://doi.org/10.1007/978-3-030-01261-8_1.\nQiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni Hannun, Gabriel Synnaeve, and Ronan\nCollobert. Iterative pseudo-labeling for speech recognition. Proc. Interspeech 2020, pp. 1006\u2013\n1010, 2020.\nYang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training.\nCoRR, abs/1708.03888, 2017. URL http://arxiv.org/abs/1708.03888.\nYang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiao-\ndan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep\nlearning: Training BERT in 76 minutes. In 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL\nhttps://openreview.net/forum?id=Syx4wnEtvH.\n16\nAppendices\nA Broader impact\n18\nB\nLimitations\n18\nC The scaling toolbox: practical methods for enabling systematic scaling\n19\nC.1\nThe continuous time/SDE perspective . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.2\nScaling rules for optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.3\nCommonly used values of hyperparameters at different batch sizes . . . . . . . . .\n21\nC.4\nProgressive scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nD EMA approximation theorems with SDEs\n22\nD.1\nSGD with model EMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nD.2\nAdaptive gradient methods with model EMA\n. . . . . . . . . . . . . . . . . . . .\n24\nE\nAdditional proofs\n25\nE.1\nIterations of SGD + EMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nE.2\nLimiting behavior of Polyak-Ruppert averaging . . . . . . . . . . . . . . . . . . .\n26\nF\nAdditional details and results for Polyak-Ruppert averaging\n27\nF.1\nNoisy parabola\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nF.2\nImage Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nF.3\nApplying the EMA Scaling Rule to Batch Normalization . . . . . . . . . . . . . .\n31\nG Additional details and results for Automatic Speech Recognition (ASR)\n32\nG.1\nAdditional experimental settings and detailed metrics . . . . . . . . . . . . . . . .\n34\nG.2\nScaling to \ud835\udf05 = 16 with Progressive Scaling . . . . . . . . . . . . . . . . . . . . . .\n37\nH Additional details and results for self-supervised image representation learning\n38\nH.1\nComponents of self-supervised learning . . . . . . . . . . . . . . . . . . . . . . .\n38\nH.2\nA ResNet-18 recipe for BYOL . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nH.3\nA Vision Transformer recipe for BYOL\n. . . . . . . . . . . . . . . . . . . . . . .\n39\nH.4\nThe role of Batch Normalization and Layer Normalization in BYOL with ViTs\n. .\n40\nH.5\nLonger training duration with incremental Progressive Scaling . . . . . . . . . . .\n41\nH.6\nBuilding intuition around Progressive Scaling and momentum sensitivity . . . . . .\n41\nH.7\nCompute usage for ViT BYOL investigation . . . . . . . . . . . . . . . . . . . . .\n42\nH.8\nResNet-18 hyperparameter sensitivity analysis . . . . . . . . . . . . . . . . . . . .\n43\nH.9\nResNet-18 additional scaling analysis\n. . . . . . . . . . . . . . . . . . . . . . . .\n45\nH.10 Scaling a ResNet-50 BYOL using LARS and Progressive Scaling\n. . . . . . . . .\n46\nH.11 Preventing collapse phenomena in DINO at scale . . . . . . . . . . . . . . . . . .\n47\nI\nAdditional details on numerical stability\n51\nJ\nContributions\n52\n17\nA\nBroader impact\nThis work shows how to adapt Machine Learning (ML) optimization in the presence of a model\nExponential Moving Average (EMA). There are a number of benefits to this:\n1. Scaling rules democratize the training of ML models: they give ML researchers the ability to\nreplicate the optimization of large scale systems, even if those researchers do not have access\nto i) significant parallel computational resources or ii) the technical tooling to do so.\n2. Our EMA Scaling Rule lowers compute usage as it removes the necessity for a hyperparameter\nsearch over momenta; in the case where our scaling assumptions hold, if we know the value\nof the optimal momentum \ud835\udf0c\ud835\udc35 at some batch size \ud835\udc35 (for example, the momentum that gives the\nbest transfer performance), then the optimal value at another batch size \u02c6\ud835\udc35 is exactly the one\ngiven by the EMA Scaling Rule \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, for scaling \ud835\udf05 = \u02c6\ud835\udc35/\ud835\udc35.\n3. Our EMA Scaling Rule enables researchers to more quickly iterate through experimental ideas,\nand opens up access to large-scale training (for example, larger models and larger datasets) for\nPseudo-Labeling and Self-Supervised Learning (SSL) techniques.\nThese points have potential negative consequences:\n1. As our EMA Scaling Rule enables researchers to iterate the same experiments more quickly,\nand perform large-scale training with EMA-based methods, this may encourage a greater num-\nber of experiments, or the training of larger models. Either of these possibilities leads to greater\nenergy consumption.\n2. As the need to determine momentum hyperparameters has now been removed, researchers who\nwere previously discouraged from attempting to scale these methods due to an extra hyperpa-\nrameter to tune may begin to perform such experiments, leading, once more, to greater energy\nconsumption.\nThe environmental impact of each of these two points may be significant.\nB\nLimitations\nThe EMA Scaling Rule provides a recipe for producing training dynamics independent of the batch\nsize used in stochastic optimization. The technology underpinning it will not always give the desired\nbehavior, however.\nThe first issue occurs with the wording present in the EMA Scaling Rule: [...] and scale other\noptimizers according to their own scaling rules (Definition 1.2):\n1. This statement requires that the given Stochastic Differential Equation (SDE) approximation\nwe are using for the model optimizer is itself providing well-behaved scaling, that is, that in the\nabsence of a model EMA, the model optimization trajectories at the batch sizes \ud835\udc35 and \ud835\udf05\ud835\udc35, with\noptimizer hyperparameters appropriately scaled, are close. In general we know this is not true.\nFirst, we know that the SDE approximation for Stochastic Gradient Descent (SGD) breaks at a\ngiven \ud835\udf05 due to discretization error (Li et al., 2021). Second, we know that if the gradient noise\nis not sufficiently large, the SDE approximation for Adam does not exist (Malladi et al., 2022),\ni.e. an SDE motivated scaling rule has no meaning.\n2. This statement requires knowledge of how to scale the corresponding model optimizer. We\nhave principled ways to achieve this for SGD (Li et al., 2021), and for the adaptive optimiza-\ntion methods RMSProp and Adam (Malladi et al., 2022). Empirically, a square-root scaling\nlaw for LAMB (You et al., 2020) has been observed, however, it has not been derived formally.\nProblematically, there is no known hyperparameter scaling law or SDE approximation known\nfor LARS (You et al., 2017), which has been used in Bootstrap Your Own Latent (BYOL)\n(Grill et al., 2020) and many other large-scale training procedures for convolution-based archi-\ntectures. Despite this, we are able to demonstrate in Appendix H.10 that a combination of the\nEMA Scaling Rule and progressive scaling can match, or surpass baseline BYOL performance\nat a batch size of 32,768 using LARS, indicating that although the theoretical guarantees may\nnot hold, there is still practical utility in the tools we provide in this work.\n18\n3. It may be the case that the optimal performance attainable by a given model setup exists at a\nlevel of discretization/gradient noise where no SDE exists. In this case, SDE-derived scaling\nrules can never be valid, and no scaling of this dynamics can be achieved with known tools.\nThe second issue is related to the case when the optimizer scaling rule is valid. In this case, the error\nfor the EMA Scaling Rule at finite learning rate \ud835\udf02 at large \ud835\udf05 can be considerable. In cases where the\nmodel EMA plays a role in the overall optimization, the error introduced by the EMA Scaling Rule\ncan break the preservation of model dynamics.\nPut another way, an optimizer scaling rule and the EMA Scaling Rule each introduce their own dis-\ncretization errors. In the case where EMA plays a role in optimization, as soon as the discretization\nerror of either the optimizer scaling rule or the EMA Scaling Rule is large, the error for the joint\noptimization procedure is large. This is at least as bad as cases that do not use a model EMA during\nthe optimization process.\nC\nThe scaling toolbox: practical methods for enabling systematic scaling\nThere are many different components involved in preserving optimization dynamics at different\nbatch sizes. In this appendix we collect into a single place the different concepts and values that we\nfound useful in practice, in an attempt to make the practice of scaling as accessible as possible.\nC.1\nThe continuous time/SDE perspective\nHere we discuss the mindset difference required when trying to preserve training dynamics. In\nML we typically use stochastic optimization, leading us to think of the optimization in terms of\nperforming updates, or stepping the optimizer. This notion has become more common in the era of\nlarge datasets, where it may be the case that we only see a fraction of the dataset during optimization.\nFor dynamics preservation under scaling, we suggest that it is simpler to consider the amount of data\nseen by the training process, or alternatively, the amount of continuous time in the discretization of\nSDEs view. The reason is the following. The SDE scaling rule results (Definition 1.2, Li et al.\n(2019, 2021); Malladi et al. (2022)) follow from showing that different discretizations of the SDE\nare close to that SDE, providing we appropriately scale hyperparameters (see Section 2.2). Each of\nthese discretizations shares the total continuous time\ud835\udc47 = \u02c6\ud835\udf02 \u00d7 b\n\ud835\udc41iter10 of the underlying SDE, but each\ndiscretization has a different number of iterations b\n\ud835\udc41iter = \ud835\udc41iter/\ud835\udf05.\nThis perspective is already adopted, perhaps by accident in some domains. For example, in Com-\nputer Vision (CV), it is typical to compare model performance after optimization on ImageNet1k\nafter a number of epochs, whilst also specifing a learning rate warmup after a number of epochs.\nThis transforms the schedule into the form wait until the process meets [condition], where here\n[condition] is when the process has seen sufficiently many samples.\nMore generally, we can specify any condition that is not a property of the discretization procedure\nitself. Instead, the discretization procedure should be viewed as a numerical approximation method\nfor the SDE we are evolving, and the properties of that discretization process (like number of steps)\nare not of specific interest in the world view where we do decouple optimization from the batch size.\nA specific example of this more general case is present in Section 3.3, where for scaling \ud835\udf05 > 2 we\nwait until the pre-training Word Error Rate (WER) is sufficiently low.\nThere may be cases where one is working with a setup that is explicitly defined in terms of quantities\nrelated to the discretization process. Indeed, the optimizer hyperparameters are examples of these,\nand need to be scaled accordingly with \ud835\udf05. The other typical example of this is conditions based on\nthe number of optimizer steps, rather than the number of epochs. In this case, these quantities should\nbe scaled to achieve the desired condition in the same amount of time, i.e. as above b\n\ud835\udc41iter = \ud835\udc41iter/\ud835\udf05,\nwhere \ud835\udc41iter is the number of iterations specified at the base batch size \ud835\udc35. Concretely, if training is\nspecified in a number of steps, then doubling the batch size implies you should train for half the\nnumber of steps.\n10This is in the case of SGD, for RMSProp and Adam one should use \ud835\udc47 = \u02c6\ud835\udf022 \u00d7 b\n\ud835\udc41iter (Malladi et al., 2022).\n19\nC.2\nScaling rules for optimization\nFor ease of reference, we collect all the scaling rules related to batch size modification we are aware\nof. We begin with the most well-known, the SGD Scaling Rule (Definitions 2.2 and C.1).\nDefinition C.1 (SGD Scaling Rule). When running SGD (Definition 2.1) with batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35,\nuse a learning rate \u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02 (Krizhevsky, 2014; Goyal et al., 2017).\nThe SGD Scaling Rule is also known as the Linear Scaling Rule (LSR), although for clarity, this\nwork adopts the naming convention [Algorithm Name] Scaling Rule, which means all parameters of\nthose algorithms are appropriately scaled from batch size \ud835\udc35 to \ud835\udf05\ud835\udc35.\nNext, we give the two scaling rules known for the adapative optimizers RMSProp (Tieleman et al.,\n2012) and Adam (Kingma & Ba, 2015) in Definition C.2 and Definition C.3 respectively.\nDefinition C.2 (RMSProp Scaling Rule). When running RMSProp (Tieleman et al., 2012) with\nbatch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35, use a learning rate \u02c6\ud835\udf02 = \u221a\ud835\udf05\ud835\udf02, beta coefficient \u02c6\ud835\udefd = 1 \u2212 \ud835\udf05 \u00d7 (1 \u2212 \ud835\udefd), and adaptivity\nparameter \u02c6\ud835\udf16 =\n\ud835\udf16\n\u221a\ud835\udf05 (Malladi et al., 2022).\nDefinition C.3 (Adam Scaling Rule). When running Adam (Kingma & Ba, 2015) with batch size\n\u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35, use a learning rate \u02c6\ud835\udf02 = \u221a\ud835\udf05\ud835\udf02, beta coefficients \u02c6\ud835\udefd1 = 1 \u2212 \ud835\udf05 \u00d7 (1 \u2212 \ud835\udefd1), \u02c6\ud835\udefd2 = 1 \u2212 \ud835\udf05 \u00d7 (1 \u2212 \ud835\udefd2),\nand adaptivity parameter \u02c6\ud835\udf16 =\n\ud835\udf16\n\u221a\ud835\udf05 (Malladi et al., 2022).\nNext, we present a contribution of this work, the EMA Scaling Rule (Definitions 1.2 and C.4), which\nextends the above scaling rules to allow the presence of a model EMA which is able to contribute to\nthe overall optimization (see Appendices D and E.1 for derivations).\nDefinition C.4 (EMA Scaling Rule). When computing the EMA update (Definition 1.1) of a model\nundergoing stochastic optimization with batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35, use a momentum \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05 and scale other\noptimizers according to their own scaling rules.\nConcretely, if we are using SGD in the presence of a model EMA, Definitions C.1 and C.4 state that\nwe should take \u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02 and \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05 when scaling by \ud835\udf05 = \u02c6\ud835\udc35/\ud835\udc35.\nThe final scaling rule is for weight decay, and follows from the scaling logic discussed in Ap-\npendix C.1 and Krizhevsky (2014). If we take the weight decay regularization penalty \ud835\udf06 defined at\nbatch size \ud835\udc35, what should the weight decay \u02c6\ud835\udf06 be for batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35? For simplicity, consider \ud835\udf05\nupdates of optimization of parameters \ud835\udec9\ud835\udc61 in the presence of weight decay only\n\ud835\udec9\ud835\udc61+\ud835\udf05 = \ud835\udec9\ud835\udc61+\ud835\udf05\u22121 \u2212 \ud835\udf02 \ud835\udf06 \ud835\udec9\ud835\udc61+\ud835\udf05\u22121 = (1 \u2212 \ud835\udf02 \ud835\udf06) \ud835\udec9\ud835\udc61+\ud835\udf05\u22121 = (1 \u2212 \ud835\udf02 \ud835\udf06)\ud835\udf05 \ud835\udec9\ud835\udc61.\n(11)\nTherefore, to match the effect of weight decay with a single iteration step, we need to match\n1 \u2212 \u02c6\ud835\udf02 \u02c6\ud835\udf06 = (1 \u2212 \ud835\udf02 \ud835\udf06)\ud835\udf05.\n(12)\nSolving for \u02c6\ud835\udf06 and expanding around \ud835\udf02 \u2248 0 gives\n\u02c6\ud835\udf06 = 1 \u2212 (1 \u2212 \ud835\udf02 \ud835\udf06)\ud835\udf05\n\u02c6\ud835\udf02\n\u2248 \ud835\udf02\n\u02c6\ud835\udf02 \u00d7 \ud835\udf05 \ud835\udf06 + O(\ud835\udf02).\n(13)\nThis leads to the Weight Decay Scaling Rule (Definition C.5).\nDefinition C.5 (Weight Decay Scaling Rule). When using weight decay with batch size \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35, use\na penalty term \u02c6\ud835\udf06 = (\ud835\udf05 \u02c6\ud835\udf02/\ud835\udf02) \ud835\udf06, where \u02c6\ud835\udf02 and \ud835\udf02 represent the scaled and unscaled learning rates of the\ncorresponding optimizer (Krizhevsky, 2014; Li et al., 2018; Loshchilov & Hutter, 2019).\nThe Weight Decay Scaling Rule implies that using linear scaling for the learning rate \ud835\udf02 then the\nweight decay penalty is automatically scaled, and when using square-root scaling for the learning\nrate \ud835\udf02 (e.g. in the case of the Adam Scaling Rule (Definition C.3)) then the weight decay penalty\nshould also be scaled with a square-root as is proposed in Loshchilov & Hutter (2019).\nFinally, we see that if the implementation of weight decay does not have an update scaled by the\nlearning rate, i.e. the update is \ud835\udec9\ud835\udc61+1 = (1 \u2212 \ud835\udf06) \ud835\udec9\ud835\udc61, then the scaling rule is optimizer-independent, and\nbecomes linear for small weight decay, i.e. \u02c6\ud835\udf06 = \ud835\udf05\ud835\udf06, and for arbitrary \ud835\udf06 takes the form \u02c6\ud835\udf06 = 1\u2212(1\u2212\ud835\udf06)\ud835\udf05.\n20\nTable 2: Scaled learning rates \u02c6\ud835\udf02 at different batch sizes \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35 given reference learning rates \ud835\udf02 defined at batch\nsize \ud835\udc35. The reference values of each column are boldened. Note that this is only valid when there is a notion of\nsingle sample. In the sequence learning setup (for example, in Section 3.3), the notion of batch size should be\nappropriately replaced with the dynamic batch size, i.e. total sequence length.\n\u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02 [SGD]\n\u02c6\ud835\udf02 = \u221a\ud835\udf05\ud835\udf02 [RMSProp, Adam]\n\ud835\udc35 = 256\n\ud835\udc35 = 512\n\ud835\udc35 = 256\n\ud835\udc35 = 4096\nBatch size \u02c6\ud835\udc35\n\ud835\udf02 = 0.1\n\ud835\udf02 = 0.3\n\ud835\udf02 = 0.1\n\ud835\udf02 = 10\u22123\n\ud835\udf02 = 4.8\n\ud835\udf02 = 10\u22123\n32\n0.0125\n0.0375\n0.00625\n0.00035\n0.42426\n0.00009\n64\n0.025\n0.075\n0.0125\n0.0005\n0.6\n0.00013\n128\n0.05\n0.15\n0.025\n0.00071\n0.84853\n0.00018\n256\n0.1\n0.3\n0.05\n0.001\n1.2\n0.00025\n512\n0.2\n0.6\n0.1\n0.00141\n1.69706\n0.00035\n1024\n0.4\n1.2\n0.2\n0.002\n2.4\n0.0005\n2048\n0.8\n2.4\n0.4\n0.00283\n3.39411\n0.00071\n4096\n1.6\n4.8\n0.8\n0.004\n4.8\n0.001\n8192\n3.2\n9.6\n1.6\n0.00566\n6.78823\n0.00141\n16384\n6.4\n19.2\n3.2\n0.008\n9.6\n0.002\n32768\n12.8\n38.4\n6.4\n0.01131\n13.57645\n0.00283\n65536\n25.6\n76.8\n12.8\n0.016\n19.2\n0.004\nTable 3: Scaled EMA momenta \u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05 at different batch sizes \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35 given reference momenta \ud835\udf0c defined at\nbatch size \ud835\udc35. The reference values of each column are boldened. Again in the sequence learning setup, batch\nsize should be appropriately replaced with a notion of sequence length.\n\ud835\udc35 = 256\n\ud835\udc35 = 4096\nBatch size \u02c6\ud835\udc35\n\ud835\udf0c = 0.9999\n\ud835\udf0c = 0.999\n\ud835\udf0c = 0.99\n\ud835\udf0c = 0.996\n\ud835\udf0c = 0.992\n\ud835\udf0c = 0.99\n\ud835\udf0c = 0.97\n32\n0.99999\n0.99987\n0.99874\n0.99997\n0.99994\n0.99992\n0.99976\n64\n0.99997\n0.99975\n0.99749\n0.99994\n0.99987\n0.99984\n0.99952\n128\n0.99995\n0.9995\n0.99499\n0.99987\n0.99975\n0.99969\n0.99905\n256\n0.9999\n0.999\n0.99\n0.99975\n0.9995\n0.99937\n0.9981\n512\n0.9998\n0.998\n0.9801\n0.9995\n0.999\n0.99874\n0.9962\n1024\n0.9996\n0.99601\n0.9606\n0.999\n0.99799\n0.99749\n0.99241\n2048\n0.9992\n0.99203\n0.92274\n0.998\n0.99599\n0.99499\n0.98489\n4096\n0.9984\n0.98412\n0.85146\n0.996\n0.992\n0.99\n0.97\n8192\n0.9968\n0.96849\n0.72498\n0.99202\n0.98406\n0.9801\n0.9409\n16384\n0.99362\n0.93798\n0.5256\n0.9841\n0.96838\n0.9606\n0.88529\n32768\n0.98728\n0.8798\n0.27625\n0.96844\n0.93776\n0.92274\n0.78374\n65536\n0.97472\n0.77405\n0.07632\n0.93788\n0.8794\n0.85146\n0.61425\nC.3\nCommonly used values of hyperparameters at different batch sizes\nIn the literature it is common to give a base learning rate \ud835\udf02 defined at batch size 256, implicitly\nusing the SGD Scaling Rule, even when using the Adam optimizer. Because the scaling of other\noptimization hyperparameters was not understood until recently, it is also common to just present\nthese for the experiment, e.g. the Adam betas and epsilon, and the EMA momentum, implicitly\ndefined at the scale of the experiment, for example at batch size 4096. One way to deal with this\nin practice is to define a single reference batch size \ud835\udc35 at which all hyperparameters are defined, and\nthen scale from there. In this case, it is easiest to compute using linear scaling the learning rate at\nthe redefined base batch size \ud835\udf02 = \u02dc\ud835\udf05 \ud835\udf02orig, where \u02dc\ud835\udf05 = \ud835\udc35/\ud835\udc35orig, and then scale this new reference \ud835\udf02 as\n\u02c6\ud835\udf02 = \ud835\udf05\ud835\udf02, \ud835\udf05 = \u02c6\ud835\udc35/\ud835\udc35, along with e.g. the momentum defined at \ud835\udc35.\nAs this process can be slightly frustrating, we have provided tables of typical learning rates in Table 2\nand momenta in Table 3.\nC.4\nProgressive scaling\nIn Section 3.4 we introduced Progressive Scaling (Definition 3.2) to test our hypothesis that early\nin the BYOL training procedure, there are dynamics that are challenging to replicate at larger batch\n21\nAlgorithm 1 Stochastic Gradient Descent with Progressive Scaling\nRequire: Base learning rate \ud835\udf02, base momentum \ud835\udf0c for base batch size \ud835\udc35\nRequire: Initial target model parameters \ud835\udec9 and model EMA parameters \ud835\udec7\nRequire: Epochs \ud835\udc38 and schedule of batch sizes B = \ud835\udc351, \ud835\udc352, . . . , \ud835\udc35\ud835\udc38\nRequire: Loss function L\nfor \ud835\udc52 in 1, 2 . . . , \ud835\udc38 do\n\u02c6\ud835\udc35 \u2190 B[\ud835\udc52]\n\u22b2 Get current batch size\n\ud835\udf05 \u2190 \u02c6\ud835\udc35/\ud835\udc35\n\u22b2 Compute scaling factor\n\u02c6\ud835\udf02 \u2190 \ud835\udf05\ud835\udf02\n\u22b2 Get scaled learning rate\n\u02c6\ud835\udf0c \u2190 \ud835\udf0c\ud835\udf05\n\u22b2 Get scaled momentum\nfor \ud835\udc4f in 1, 2 . . . , floor(\ud835\udc38/ \u02c6\ud835\udc35) do\nSample a minibatch of \u02c6\ud835\udc35 samples X = {x(1), . . . , x( \u02c6\ud835\udc35)}\n\ud835\udec9 \u2190 \ud835\udec9 \u2212 ( \u02c6\ud835\udf02/ \u02c6\ud835\udc35) \u00cd\n\ud835\udc65 \u2208X \u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9, \ud835\udec7)\n\u22b2 SGD Update\n\ud835\udec7 \u2190 \u02c6\ud835\udf0c \ud835\udec7 + (1 \u2212 \u02c6\ud835\udf0c) \ud835\udec9\n\u22b2 EMA Update\nend for\nend for\nsizes. To remove ambiguity, in Algorithm 1 we provide pseudo-code for how to use Progressive\nScaling.\nIn Algorithm 1, the prefactor of the SGD update could also have been written \ud835\udf02/\ud835\udc35, although an\nequivalent use of the base momentum is not possible.\nFinally, we outline how to extend Algorithm 1 to more complex setups, like those presented in\nSection 3.4:\n1. Optimizer scaling rules are used appropriately, for example the Adam scaling rule in case of\nusing the Adam optimizer to update parameters \ud835\udec9.\n2. Schedules for hyperparameters are computed using the base hyperparameters, and are then\nmodified by application of the scaling law in epoch (outer) loop.\n3. Schedules for hyperparameters at the step rather than epoch level can be achieved in practice\nthrough recomputing the schedule and updating the notion of minibatch index appropriately\nthroughout training.\nAll of the above techniques are used in Section 3.4. In addition, scheduling batch sizes within epoch\nis possible, providing one maintains a notion of computation within some fixed continuous time\n\ud835\udc47fixed. We did not investigate this scenario.\nD\nEMA approximation theorems with SDEs\nD.1\nSGD with model EMA\nWe will now derive the EMA scaling rule when tracking model parameters and the model is trained\nusing SGD. We employ a strategy similar to Malladi et al. (2022), where we associate to each\niterative process a Stochastic Differential Equation (SDE). In order to control the distance between\nthe SDE and the discrete process, we use the tools from Li et al. (2019).\nDefinition D.1 (Polynomial growth, Definition 1 in (Li et al., 2019)). The set \ud835\udc3a is the set of con-\ntinuous functions R\ud835\udc51 \u2192 R with at most polynomial growth, i.e., for \ud835\udc54 \u2208 \ud835\udc3a there exists two scalars\n\ud835\udf051,\ud835\udf052 > 0 such that for all x \u2208 R\ud835\udc51, we have |\ud835\udc54(x)| \u2264 \ud835\udf051(1 + \u2225x\u2225\ud835\udf052).\nFor an integer \ud835\udefc > 0, \ud835\udc3a\ud835\udefc is the set of functions R\ud835\udc51 \u2192 R that are \ud835\udefc-times continuously differentiable\nand such that all their derivatives up to order \ud835\udefc are in \ud835\udc3a.\nSimilarly to Malladi et al. (2022), we use Noisy Gradient Oracle with Scale Parameter (NGOS) to\ndefine the update rules on the parameters.\nDefinition D.2 (Noisy Gradient Oracle with Scale Parameter (NGOS), adaptation of (Malladi et al.,\n2022)). A NGOS is a tuple G\ud835\udf0e = (\ud835\udc53 , \u03a3, Z\ud835\udf0e). Given a noise scale parameter \ud835\udf0e > 0, the NGOS G\ud835\udf0e\n22\ntakes as input the parameters \u03b8 and outputs a random vector g = \u2207\ud835\udc53 (\u03b8, \u03b6) + \ud835\udf0e\ud835\udedc where \u2207\ud835\udc53 (\u03b8, \u03b6) is\nthe gradient of \ud835\udc53 with respect to \u03b8 at (\u03b8, \u03b6), and \ud835\udedc is a random vector drawn from the distribution\nZ\ud835\udf0e (\u03b8, \u03b6) with zero mean and covariance \u03a3(\u03b8, \u03b6).\nNote that in the above definition, the probability distribution Z\ud835\udf0e (\u03b8, \u03b6) is allowed to change with the\nscale \ud835\udf0e, but its first two moments \u2014 its mean and its covariance \u2014 are fixed with \ud835\udf0e. We have the\nfollowing theorem for model EMA under optimization with SGD:\nTheorem D.1 (SDE for SGD + EMA). Consider the couple x\ud835\udc58 = (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) where \ud835\udec9\ud835\udc58 are the iterates\nof SGD with a NGOS (Definition D.2) and \ud835\udec7\ud835\udc58 is an EMA of \ud835\udec9\ud835\udc58, defined, starting from x0 = x0, by\n\ud835\udec9\ud835\udc58+1 = \ud835\udec9\ud835\udc58 \u2212 \ud835\udf02g\ud835\udc58, with g\ud835\udc58 = \u2207\ud835\udc53 (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) + \ud835\udf0e\ud835\udedc\ud835\udc58, and \ud835\udedc\ud835\udc58 \u223c Z\ud835\udf0e (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58),\n(14)\n\ud835\udec7\ud835\udc58+1 = \ud835\udf0c\ud835\udec7\ud835\udc58 + (1 \u2212 \ud835\udf0c)\ud835\udec9\ud835\udc58 .\n(15)\nDefine \ud835\udefd0 = (1 \u2212 \ud835\udf0c)/\ud835\udf02, \ud835\udf0e0 = \ud835\udf0e\u221a\ud835\udf02, and define the SDE for \ud835\udc4b\ud835\udc61 = (\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61), starting from \ud835\udc4b0 = x0, by\n\ud835\udc51\u0398\ud835\udc61 = \u2212\u2207\ud835\udc53 (\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)\ud835\udc51\ud835\udc61 + \ud835\udf0e0\u03a3(\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)\n1\n2\ud835\udc51\ud835\udc4a\ud835\udc61, with \ud835\udc4a\ud835\udc61 a Wiener process\n(16)\n\ud835\udc51\ud835\udc4d\ud835\udc61 = \ud835\udefd0(\u0398\ud835\udc61 \u2212 \ud835\udc4d\ud835\udc61)\ud835\udc51\ud835\udc61 .\n(17)\nAssume that \ud835\udc53 is continuously differentiable, with \ud835\udc53 \u2208 \ud835\udc3a3 and \u03a3\n1\n2 \u2208 \ud835\udc3a2 (Definition D.1). Then, for\nany time horizon \ud835\udc47 > 0 and test function \ud835\udc54 \u2208 \ud835\udc3a2 , there exists a constant \ud835\udc50 > 0 such that\nmax\n\ud835\udc58=0,...,\u230a\ud835\udc47/\ud835\udf02\u230b |E[\ud835\udc54(\ud835\udc4b\ud835\udf02\ud835\udc58)] \u2212 E[\ud835\udc54(x\ud835\udc58)]| \u2264 \ud835\udc50 \u00d7 \ud835\udf02 .\n(18)\nProof. The proof uses the same tools as in Li et al. (2019). Define \u0394(\u03b8, \u03b6) = \ud835\udf02(\u2212\u2207\ud835\udc53 (\u03b8, \u03b6) +\n\ud835\udf0e\ud835\udedc, \ud835\udefd0(\u03b8 \u2212 \u03b6)) with \ud835\udedc \u223c Z\ud835\udf0e (\u03b8, \u03b6) the one-step update for the SGD + EMA update, such that\nx\ud835\udc58+1 = x\ud835\udc58 + \u0394(x\ud835\udc58). We have the first two moments:\nE[\u0394(\u03b8, \u03b6)] = \ud835\udf02(\u2212\u2207\ud835\udc53 (\u03b8, \u03b6), \ud835\udefd0(\u03b8 \u2212 \u03b6))\n(19)\nV[\u0394(\u03b8, \u03b6)] = \ud835\udf02\ud835\udf0e2\n0\n\u0014\n\u03a3(\u03b8, \u03b6)\n0\n0\n0\n\u0015\n(20)\nand the higher-order moments are \ud835\udc42(\ud835\udf022). Similarly, let \u02dc\u0394(\u03b8, \u03b6) be the solution at time \ud835\udf02 of the SDE\ndefined by Equation 6 starting from \ud835\udc4b0 = (\u03b8, \u03b6). From Ito\u2019s formula, we also obtain\nE[ \u02dc\u0394(\u03b8, \u03b6)] = \ud835\udf02(\u2212\u2207\ud835\udc53 (\u03b8), \ud835\udefd0(\u03b8 \u2212 \u03b6))\n(21)\nV[ \u02dc\u0394(\u03b8, \u03b6)] = \ud835\udf02\ud835\udf0e2\n0\n\u0014\n\u03a3(\u03b8, \u03b6)\n0\n0\n0\n\u0015\n(22)\nand the higher-order moments are \ud835\udc42(\ud835\udf022). Hence, the moments of the discrete iteration and of the\nSDE match up to second order. Following the same proof technique as in Li et al. (2019) then leads\nto the advertized theorem.\n\u25a1\nThis theorem is a simple adaptation of the results of Li et al. (2019). Intuitively, it is expected that\n\ud835\udc4b\ud835\udc61 and x\ud835\udc58 are close since x\ud835\udc58 is the Euler-Maruyama discretization of \ud835\udc4b\ud835\udc61 with learning rate \ud835\udf02. We\nthen have the corollary.\nCorollary D.1.1 (Validity of the EMA Scaling Rule). Assume that \ud835\udc53 is continuously differentiable,\nwith \ud835\udc53 \u2208 \ud835\udc3a3 and \u03a3\n1\n2 \u2208 \ud835\udc3a2. Let \ud835\udec9\ud835\udc35\n\ud835\udc58, \ud835\udec7\ud835\udc35\n\ud835\udc58 the iterates of the Equation 5 with batch size \ud835\udc35 and hyper-\nparameters \ud835\udf02, \ud835\udf0c. Let \ud835\udec9\ud835\udf05\ud835\udc35\n\ud835\udc58 , \ud835\udec7\ud835\udf05\ud835\udc35\n\ud835\udc58\nbe iterates with batch size \ud835\udf05\ud835\udc35, learning rate \ud835\udf02 determined by the SGD\nScaling Rule (Definition 2.2) and momentum determined by the EMA Scaling Rule, linear version\n(Definition 1.2). Then, for any time horizon \ud835\udc47 > 0 and function \ud835\udc54 \u2208 \ud835\udc3a2, there exists a constant \ud835\udc51 > 0\nsuch that\nmax\n\ud835\udc58=0,...,\u230a\ud835\udc47/\ud835\udf02\u230b |E[\ud835\udc54(\ud835\udec9\ud835\udf05\ud835\udc35\n\u230a\ud835\udc58/\ud835\udf05\u230b, \ud835\udec7\ud835\udf05\ud835\udc35\n\u230a\ud835\udc58/\ud835\udf05\u230b)] \u2212 E[\ud835\udc54(\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58)]| \u2264 \ud835\udc51 \u00d7 \ud835\udf02 .\n(23)\nProof. The proof is similar to Malladi et al. (2022). Under the scaling rule, both x\ud835\udc58 = (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) and\n\u02c6x\u230a\ud835\udc58/\ud835\udf05\u230b = (\ud835\udec9\ud835\udf05\ud835\udc35\n\u230a\ud835\udc58/\ud835\udf05\u230b, \ud835\udec7\ud835\udf05\ud835\udc35\n\u230a\ud835\udc58/\ud835\udf05\u230b) have the same limiting SDE. Hence we have from the previous theorem\nthat for all test function \ud835\udc54, we can find \ud835\udc50,\ud835\udc50\u2032 such that\nmax\n\ud835\udc58=0,...,\u230a\ud835\udc47/\ud835\udf02\u230b |E[\ud835\udc54(\ud835\udc4b\ud835\udf02\ud835\udc58)] \u2212E[\ud835\udc54(x\ud835\udc58)]| \u2264 \ud835\udc50 \u00d7\ud835\udf02 and\nmax\n\ud835\udc58=0,...,\u230a\ud835\udc47/\ud835\udf02\u230b |E[\ud835\udc54(\ud835\udc4b\ud835\udf02\ud835\udc58)] \u2212E[\ud835\udc54(\u02c6x\u230a\ud835\udc58/\ud835\udf05\u230b)]| \u2264 \ud835\udc50\u2032\u00d7\ud835\udf02. (24)\n23\nThe triangle inequality then gives\nmax\n\ud835\udc58=0,...,\u230a\ud835\udc47/\ud835\udf02\u230b |E[\ud835\udc54(\u02c6x\u230a\ud835\udc58/\ud835\udf05\u230b)] \u2212 E[\ud835\udc54(x\ud835\udc58)]| \u2264 (\ud835\udc50 + \ud835\udc50\u2032) \u00d7 \ud835\udf02.\n(25)\nHence, taking \ud835\udc51 = \ud835\udc50 + \ud835\udc50\u2032 gives the expected result.\n\u25a1\nD.2\nAdaptive gradient methods with model EMA\nWe now turn to the case where one uses an adaptive gradient method rather than SGD to train the\nmodel. We follow derivations similar to those of Malladi et al. (2022), with an added EMA. Like\nabove, we consider that the loss function \ud835\udc53 also depends on the EMA tracking parameter \ud835\udec7\ud835\udc58. We\nbegin with RMSProp with EMA, which iterates:\nv\ud835\udc58+1 = \ud835\udefev\ud835\udc58 + (1 \u2212 \ud835\udefe)g2\n\ud835\udc58, with g\ud835\udc58 = \u2207\ud835\udc53 (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) + \ud835\udf0e\ud835\udedc\ud835\udc58, and \ud835\udedc\ud835\udc58 \u223c Z\ud835\udf0e (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58),\n(26)\n\ud835\udec9\ud835\udc58+1 = \ud835\udec9\ud835\udc58 \u2212 \ud835\udf02(\u221av\ud835\udc58 + \ud835\udf00)\u22121 \u00d7 g\ud835\udc58\n(27)\n\ud835\udec7\ud835\udc58+1 = \ud835\udf0c\ud835\udec7\ud835\udc58 + (1 \u2212 \ud835\udf0c)\ud835\udec9\ud835\udc58.\n(28)\nLike in Malladi et al. (2022), we place ourselves in the high noise regime, in which the term g2\n\ud835\udc58 in\nEquation 26 is approximated by g2\n\ud835\udc58 \u2243 \ud835\udf0e2diag(\u03a3(\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58)). We use the same scaling rules, with an\nadditional one for \ud835\udf0c:\n\ud835\udefe0 = (1 \u2212 \ud835\udefe)/\ud835\udf022, \ud835\udf0e0 = \ud835\udf0e\ud835\udf02, \ud835\udf000 = \ud835\udf00\ud835\udf02, and \ud835\udefd0 = (1 \u2212 \ud835\udf0c)/\ud835\udf022,\n(29)\nand we let u\ud835\udc58 = v\ud835\udc58/\ud835\udf0e2. The equations for RMSProp with EMA then become, using only these new\nvariables and \ud835\udf02:\nu\ud835\udc58+1 \u2212 u\ud835\udc58 = \ud835\udf022\ud835\udefe0(diag(\u03a3(\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58)) \u2212 u\ud835\udc58),\n(30)\n\ud835\udec9\ud835\udc58+1 \u2212 \ud835\udec9\ud835\udc58 = \u2212(\u221au\ud835\udc58 + \ud835\udf000)\u22121 \u0000\ud835\udf022\u2207\ud835\udc53 (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) + \ud835\udf02\ud835\udedc\ud835\udc58\n\u0001\n(31)\n\ud835\udec7\ud835\udc58+1 \u2212 \ud835\udec7\ud835\udc58 = \ud835\udf022\ud835\udefd0(\ud835\udec9\ud835\udc58 \u2212 \ud835\udec7\ud835\udc58).\n(32)\nThis formulation makes it clear that these iterations can be seen as the discretization of the SDE\n\ud835\udc51\ud835\udc48\ud835\udc61 = \ud835\udefe0(diag(\u03a3(\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)) \u2212 \ud835\udc48\ud835\udc61)\ud835\udc51\ud835\udc61,\n(33)\n\ud835\udc51\u0398\ud835\udc61 = \u2212(\ud835\udf0e0\n\u221a\ufe01\n\ud835\udc48\ud835\udc61 + \ud835\udf000)\u22121(\u2207\ud835\udc53 (\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)\ud835\udc51\ud835\udc61 + \ud835\udf0e0\u03a3(\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)1/2\ud835\udc51\ud835\udc4a\ud835\udc61)\n(34)\n\ud835\udc51\ud835\udc4d\ud835\udc61 = \ud835\udefd0(\u0398\ud835\udc61 \u2212 \ud835\udc4d\ud835\udc61)\ud835\udc51\ud835\udc61,\n(35)\nwith step size \ud835\udf022. Of course, we recover the SDE of Malladi et al. (2022) in the case where \ud835\udefd0 = 0.\nA formal proof of closeness between the iterates and the SDE trajectory is out of the scope of the\npresent paper since it would imply redoing much of the theoretical work developed in Malladi et al.\n(2022). Still, the previous informal analysis hints that for RMSProp, the scaling rule in Equation 29\nshould be used. In other words, given a certain set of hyperparameters \ud835\udefe,\ud835\udf02 and \ud835\udf0c, if the batch\nsize goes from \ud835\udc35 to \u02c6\ud835\udc35 = \ud835\udf05 \u00d7 \ud835\udc35, the noise level becomes \u02c6\ud835\udf0e = \ud835\udf0e/\u221a\ud835\udf05, and keeping the quantities in\nEquation 29 constant means that we should use as new hyperparameters\n\u02c6\ud835\udefe = 1 \u2212 (1 \u2212 \ud835\udefe) \u00d7 \ud835\udf05,\n\u02c6\ud835\udf02 = \ud835\udf02 \u00d7 \u221a\ud835\udf05, and \u02c6\ud835\udf0c = 1 \u2212 (1 \u2212 \ud835\udf0c) \u00d7 \ud835\udf05 .\nThe linear rule \u02c6\ud835\udf0c = 1 \u2212 (1 \u2212 \ud835\udf0c) \u00d7 \ud835\udf05 is at the first order equivalent to the exponential scaling rule\n\u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05. Hence, even though the limiting SDE differs greatly from that of SGD, and even though the\nscaling rule regarding the learning rate differs, we recover for the momentum term \ud835\udf0c the exact same\nscaling rule as for SGD.\nWe finish the discussion with the case of Adam, which leads once again to the same rule as for SGD.\nAdam with EMA tracking of the network parameters iterates\nm\ud835\udc58+1 = \ud835\udefd1m\ud835\udc58 + (1 \u2212 \ud835\udefd1)g\ud835\udc58, with g\ud835\udc58 = \u2207\ud835\udc53 (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58) + \ud835\udf0e\ud835\udedc\ud835\udc58, and \ud835\udedc\ud835\udc58 \u223c Z\ud835\udf0e (\ud835\udec9\ud835\udc58, \ud835\udec7\ud835\udc58),\n(36)\nv\ud835\udc58+1 = \ud835\udefd2v\ud835\udc58 + (1 \u2212 \ud835\udefd2)g2\n\ud835\udc58\n(37)\n\u02dcm\ud835\udc58+1 = m\ud835\udc58+1/(1 \u2212 \ud835\udefd\ud835\udc58+1\n1\n)\n(38)\n\u02dcv\ud835\udc58+1 = v\ud835\udc58+1/(1 \u2212 \ud835\udefd\ud835\udc58+1\n2\n)\n(39)\n\ud835\udec9\ud835\udc58+1 = \ud835\udec9\ud835\udc58 \u2212 \ud835\udf02(\n\u221a\ufe01\n\u02dcv\ud835\udc58 + \ud835\udf00)\u22121 \u00d7 \u02dcm\ud835\udc58+1\n(40)\n\ud835\udec7\ud835\udc58+1 = \ud835\udf0c\ud835\udec7\ud835\udc58 + (1 \u2212 \ud835\udf0c)\ud835\udec9\ud835\udc58 .\n(41)\n24\nHere, we use the same minor modification of the iterations as in Malladi et al. (2022), where we use\nv\ud835\udc58 instead of v\ud835\udc58+1 in the denominator of the \ud835\udec9\ud835\udc58 update.\nWe consider the following scaling for the hyperparameters\n\ud835\udc501 = (1 \u2212 \ud835\udefd1)/\ud835\udf022, \ud835\udc502 = (1 \u2212 \ud835\udefd2)/\ud835\udf022, \ud835\udf0e0 = \ud835\udf0e\ud835\udf02, \ud835\udf000 = \ud835\udf00\ud835\udf02, and \ud835\udefd0 = (1 \u2212 \ud835\udf0c)/\ud835\udf022,\n(42)\nand \ud835\udefe1(\ud835\udc61) = 1 \u2212 exp(\u2212\ud835\udc501\ud835\udc61), \ud835\udefe2(\ud835\udc61) = 1 \u2212 exp(\u2212\ud835\udc502\ud835\udc61), and u\ud835\udc58 = v\ud835\udc58/\ud835\udf0e2. The SDE for Adam + EMA is\ngiven by\n\ud835\udc51\ud835\udc40\ud835\udc61 = \ud835\udc501\n\u0010\n(\u2207\ud835\udc53 (\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61) \u2212 \ud835\udc40\ud835\udc61)\ud835\udc51\ud835\udc61 + \ud835\udf0e0\u03a3(\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)1/2\ud835\udc51\ud835\udc4a\ud835\udc61\n\u0011\n(43)\n\ud835\udc51\ud835\udc48\ud835\udc61 = \ud835\udc502(diag(\u03a3(\u0398\ud835\udc61,\ud835\udc4d\ud835\udc61)) \u2212 \ud835\udc48\ud835\udc61)\ud835\udc51\ud835\udc61\n(44)\n\ud835\udc51\u0398\ud835\udc61 = \u2212\n\u221a\ufe01\n\ud835\udefe2(\ud835\udc61)\n\ud835\udefe1(\ud835\udc61) (\ud835\udf0e0\n\u221a\ufe01\n\ud835\udc48\ud835\udc61 + \ud835\udf000\n\u221a\ufe01\n\ud835\udefe2(\ud835\udc61))\u22121 \u00d7 \ud835\udc40\ud835\udc61\ud835\udc51\ud835\udc61\n(45)\n\ud835\udc51\ud835\udc4d\ud835\udc61 = \ud835\udefd0(\u0398\ud835\udc61 \u2212 \ud835\udc4d\ud835\udc61)\ud835\udc51\ud835\udc61.\n(46)\nThis is once again the same SDE as in Malladi et al. (2022) with the added EMA term. Like\npreviously, this SDE hints at the fact that the scaling rule in eq. 42 should be used. In other words,\ngiven a set of hyperparameters \ud835\udefd1, \ud835\udefd2,\ud835\udf02, and \ud835\udf0c, if the batch size goes from \ud835\udc35 to \ud835\udf05 \u00d7 \ud835\udc35, then the noise\nlevel becomes \u02c6\ud835\udf0e = \ud835\udf0e/\u221a\ud835\udf05 and keeping quantities in eq. 42 constant means that we should use as new\nhyperparameters\n\u02c6\ud835\udefd1 = 1 \u2212 (1 \u2212 \ud835\udefd1) \u00d7 \ud835\udf05,\n\u02c6\ud835\udefd2 = 1 \u2212 (1 \u2212 \ud835\udefd2) \u00d7 \ud835\udf05,\n\u02c6\ud835\udf02 = \ud835\udf02 \u00d7 \u221a\ud835\udf05, and \u02c6\ud835\udf0c = 1 \u2212 (1 \u2212 \ud835\udf0c) \u00d7 \ud835\udf05.\nWe once again recover a linear rule for 1 \u2212 \ud835\udf0c which is equivalent to the exponential scaling rule\n\u02c6\ud835\udf0c = \ud835\udf0c\ud835\udf05 in the limit \ud835\udf0c \u2192 0.\nE\nAdditional proofs\nE.1\nIterations of SGD + EMA\nHere we derive a critical component of the EMA Scaling Rule, the matrix equation of Equation 4\nfrom which the EMA Scaling Rule (Definition 1.2) follows.\nTheorem E.1 (Iterations of SGD + EMA). Assuming that gradients change slowly over iterations\nof SGD (Definition 2.1) and EMA (Definition 1.1): \u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9\ud835\udc61+\ud835\udc57, \ud835\udec7\ud835\udc61+\ud835\udc57) \u2248 \u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9\ud835\udc61, \ud835\udec7\ud835\udc61) \u2248 g, for\n\ud835\udc57 = 1, 2, . . . ,\ud835\udf05 and representative gradient g, iterating over \ud835\udf05 independent minibatches produces\nmodel states \"\ud835\udec9\ud835\udc61+\ud835\udf05\n\ud835\udec7\ud835\udc61+\ud835\udf05\ng\n#\n=\n\"\n1\n0\n\u2212\ud835\udf02\n1 \u2212 \ud835\udf0c\n\ud835\udf0c\n0\n0\n0\n1\n#\ud835\udf05\n\u00b7\n\"\ud835\udec9\ud835\udc61\n\ud835\udec7\ud835\udc61\ng\n#\n=\n\"\n\ud835\udec9\ud835\udc61 \u2212 \ud835\udf02 \ud835\udf05 g\n\ud835\udf0c\ud835\udf05 \ud835\udec7\ud835\udc61 + (1 \u2212 \ud835\udf0c\ud835\udf05) \ud835\udec9\ud835\udc61 + O \u0000\ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c\n\u0001\ng\n#\n.\n(47)\nProof. First note that for matrices of the form\nA =\n\"\n1\n0\n\ud835\udc4e0,2\n1 \u2212 \ud835\udc4e1,1\n\ud835\udc4e1,1\n0\n0\n0\n1\n#\n,\n(48)\ntheir multiplication follows\nA B =\n\"\n1\n0\n\ud835\udc4e0,2\n1 \u2212 \ud835\udc4e1,1\n\ud835\udc4e1,1\n0\n0\n0\n1\n# \"\n1\n0\n\ud835\udc4f0,2\n1 \u2212 \ud835\udc4f1,1\n\ud835\udc4f1,1\n0\n0\n0\n1\n#\n=\n\"\n1\n0\n\ud835\udc4e0,2 + \ud835\udc4f0,2\n1 \u2212 \ud835\udc4e1,1 \ud835\udc4f1,1\n\ud835\udc4e1,1 \ud835\udc4f1,1\n(1 \u2212 \ud835\udc4e1,1) \ud835\udc4f0,2\n0\n0\n1\n#\n,\n(49)\nand\nA B C =\n\"\n1\n0\n\ud835\udc4e0,2 + \ud835\udc4f0,2\n1 \u2212 \ud835\udc4e1,1 \ud835\udc4f1,1\n\ud835\udc4e1,1 \ud835\udc4f1,1\n(1 \u2212 \ud835\udc4e1,1) \ud835\udc4f0,2\n0\n0\n1\n# \"\n1\n0\n\ud835\udc500,2\n1 \u2212 \ud835\udc501,1\n\ud835\udc501,1\n0\n0\n0\n1\n#\n=\n\"\n1\n0\n\ud835\udc4e0,2 + \ud835\udc4f0,2 + \ud835\udc500,2\n1 \u2212 \ud835\udc4e1,1 \ud835\udc4f1,1 \ud835\udc501,1\n\ud835\udc4e1,1 \ud835\udc4f1,1 \ud835\udc501,1\n(1 \u2212 \ud835\udc4e1,1) \ud835\udc4f0,2 + (1 \u2212 \ud835\udc4e1,1 \ud835\udc4f1,1) \ud835\udc500,2\n0\n0\n1\n#\n.\n(50)\n25\nBy induction\nA\ud835\udf05 =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n\ud835\udf05 \u00d7 \ud835\udc4e0,2\n1 \u2212 \ud835\udc4e\ud835\udf05\n1,1\n\ud835\udc4e\ud835\udf05\n1,1\n\ud835\udeff(\ud835\udc4e0,2,\ud835\udc4e1,1,\ud835\udf05)\n0\n0\n1\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n(51)\nwhere\n\ud835\udeff(\ud835\udc4e0,2,\ud835\udc4e1,1,\ud835\udf05) = \ud835\udc4e0,2\n\ud835\udf05\u22121\n\u2211\ufe01\n\ud835\udc56=1\n\u00001 \u2212 \ud835\udc4e\ud835\udc56\n1,1\n\u0001 = \ud835\udc4e0,2\n\u0012\n\ud835\udf05 \u2212\n1 \u2212 \ud835\udc4e\ud835\udf05\n1,1\n1 \u2212 \ud835\udc4e1,1\n\u0013\n,\nfor\ud835\udc4e1,1 \u2260 1.\n(52)\nIt follows that\n\"\n1\n0\n\u2212\ud835\udf02\n1 \u2212 \ud835\udf0c\n\ud835\udf0c\n0\n0\n0\n1\n#\ud835\udf05\n=\n\"\n1\n0\n\u2212\ud835\udf05 \ud835\udf02\n1 \u2212 \ud835\udf0c\ud835\udf05\n\ud835\udf0c\ud835\udf05\n\ud835\udeff(\ud835\udf02, \ud835\udf0c,\ud835\udf05)\n0\n0\n1\n#\n(53)\nwhere the EMA Scaling Rule error\n\ud835\udeff(\ud835\udf02, \ud835\udf0c,\ud835\udf05) = (\u2212\ud835\udf02)\n\u0012\n\ud835\udf05 \u2212 1 \u2212 \ud835\udf0c\ud835\udf05\n1 \u2212 \ud835\udf0c\n\u0013\n\u2248 (\u2212\ud835\udf02) \u0000\ud835\udf05 \u2212 \ud835\udf05 + O(\ud835\udefd\ud835\udf0c)\u0001 = 0 + O(\ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c),\n(54)\nwhere \ud835\udefd\ud835\udf0c \u2261 1 \u2212 \ud835\udf0c and the approximation is around \ud835\udf0c = 1.\n\u25a1\nE.2\nLimiting behavior of Polyak-Ruppert averaging\nHere we sketch the asymptotic behavior of a target model \u03b8 and its EMA \u03b6. Let us assume that \u03b8\nconverges to the stationary distribution lim\ud835\udc61\u2192\u221e \u03b8\ud835\udc61 = \u03b8\u2217, \u03b8\u2217 \u223c \ud835\udc5d\u221e(\u03b8). We are interested in statistical\nproperties of \u03b6\u2217 = lim\ud835\udc61\u2192\u221e \u03b6\ud835\udc61, as this will formalize the notion of how the EMA depends on the a\ntime-horizon defined by its momentum \ud835\udf0c as discussed in Table 1.\nAs a warm-up, for \ud835\udc5b independent random variables x1, . . . , x2, we know that the sample mean \u00af\ud835\udc65 =\n1\n\ud835\udc5b (\ud835\udc651,\ud835\udc652, . . . ,\ud835\udc65\ud835\udc5b) has the statistical properties\nE[\u00af\ud835\udc65] = \ud835\udf07,\nVar[\u00af\ud835\udc65] = \ud835\udf0e2\n\ud835\udc5b ,\n(55)\nwhere \ud835\udf07 and \ud835\udf0e are the population mean and variance. This gives us an idea of what to expect. As we\nwill now show, the expectation of \u03b6\u2217 should have no time-horizon dependence, whereas the variance\nof \u03b6\u2217 will depend on its time horizon (i.e. the number of samples it integrates over) which is defined\nby \ud835\udf0c.\nIn the case of a weighted sum\n\u00af\ud835\udc65 (\ud835\udc64) =\n\ud835\udc5b\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc64\ud835\udc56 \ud835\udc65\ud835\udc56,\n(56)\nthen if the \ud835\udc65\ud835\udc56 are Independent and Identically Distributed (i.i.d.), then\nE[\u00af\ud835\udc65 (\ud835\udc64)] =\n\ud835\udc5b\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc64\ud835\udc56 E[\ud835\udc65\ud835\udc56] = \ud835\udc5b \u00af\ud835\udc64 \ud835\udf07,\n\u00af\ud835\udc64 = 1\n\ud835\udc5b\n\ud835\udc5b\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc64\ud835\udc56,\n(57)\nand for the variance (Kish, 1965)\nVar[\u00af\ud835\udc65 (\ud835\udc64)] = \ud835\udc5b \u00b7 \ud835\udc642 \u00b7 \ud835\udf0e2\n\ud835\udc642 = 1\n\ud835\udc5b\n\ud835\udc5b\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc642\n\ud835\udc56 ,\n\ud835\udf0e2 = Var[\ud835\udc65\ud835\udc56].\n(58)\nWe can verify that we reproduce the well-known result in Equation 55 in the case where all weights\nare equal to 1\n\ud835\udc5b as follows\n\u2200\ud835\udc56 : \ud835\udc64\ud835\udc56 = 1\n\ud835\udc5b =\u21d2 \ud835\udc642 = 1\n\ud835\udc5b \u00b7\n\ud835\udc5b\n\u2211\ufe01\n\ud835\udc56=1\n\u0012 1\n\ud835\udc5b\n\u00132\n= 1\n\ud835\udc5b2 =\u21d2 Var[\u00af\ud835\udc65 (\ud835\udc64)] = \ud835\udc5b \u00b7 1\n\ud835\udc5b2 \u00b7 \ud835\udf0e2 = \ud835\udf0e2\n\ud835\udc5b .\n(59)\n26\nIn the case of an exponential moving average we have\n\u03b6\ud835\udc61+1 = \ud835\udf0c \u03b6\ud835\udc61 + (1 \u2212 \ud835\udf0c) \u03b8\ud835\udc61 = \ud835\udf0c\ud835\udc61 \u03b61 + (1 \u2212 \ud835\udf0c)\n\ud835\udc61\u22121\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udf0c\ud835\udc56\u03b8\ud835\udc61\u2212\ud835\udc56.\n(60)\nLet\u2019s consider the specific case where we are at iteration \ud835\udc58 which is sufficiently large that \u03b6 and \u03b8\nhave converged to their stationary distributions. From \ud835\udc58, the iterations unfold as\n\u03b6\ud835\udc61+1 = \ud835\udf0c\ud835\udc61+1\u2212\ud835\udc58 \u03b6\ud835\udc58 + (1 \u2212 \ud835\udf0c)\n\ud835\udc61\u2212\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udf0c\ud835\udc56\u03b8\ud835\udc61\u2212\ud835\udc56.\n(61)\nWe rearrange for terms in \u03b6\n\u03b6\ud835\udc61+1 \u2212 \ud835\udf0c\ud835\udc61+1\u2212\ud835\udc58 \u03b6\ud835\udc58 = (1 \u2212 \ud835\udf0c)\n\ud835\udc61\u2212\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udf0c\ud835\udc56 \u03b8\ud835\udc61\u2212\ud835\udc56,\n(62)\nand before proceeding to the final result, using \ud835\udc5b = \ud835\udc61 + 1 \u2212 \ud835\udc58, we compute the convenient quantities\n\u00af\ud835\udf0c = 1\n\ud835\udc5b\n\ud835\udc5b\u22121\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udf0c\ud835\udc56 = 1\n\ud835\udc5b \u00d7 1 \u2212 \ud835\udf0c\ud835\udc5b\n1 \u2212 \ud835\udf0c\n(63)\n\ud835\udf0c2 = 1\n\ud835\udc5b\n\ud835\udc5b\u22121\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udf0c2\ud835\udc56 = 1\n\ud835\udc5b \u00d7 1 \u2212 \ud835\udf0c2\ud835\udc5b\n1 \u2212 \ud835\udf0c2 .\n(64)\nTaking expectation of Equation 62 and setting statistics to their stationary values, we have\n(1 \u2212 \ud835\udf0c\ud835\udc5b) E[\u03b6\u2217] = (1 \u2212 \ud835\udf0c) \ud835\udc5b \u00af\ud835\udf0c E[\u03b8\u2217] = (1 \u2212 \ud835\udf0c\ud835\udc5b) E[\u03b8\u2217],\n(65)\nwhere we have used the result in Equation 57. It follows that for \ud835\udf0c \u2260 1 we have\nE[\u03b6\u2217] = E[\u03b8\u2217],\n(66)\nindependent of \ud835\udf0c. Finally, we can take the variance of Equation 62. First the left hand side\nVar [\u03b6\ud835\udc61+1 \u2212 \ud835\udf0c\ud835\udc5b \u03b6\ud835\udc58] = Var [\u03b6\ud835\udc61+1] + \ud835\udf0c2\ud835\udc5b Var [\u03b6\ud835\udc58] = \u00001 + \ud835\udf0c2\ud835\udc5b\u0001 Var [\u03b6\u2217] .\n(67)\nNext the right hand side\nVar\n\"\n(1 \u2212 \ud835\udf0c)\n\ud835\udc5b\u22121\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udf0c\ud835\udc56 \u03b8\ud835\udc61\u2212\ud835\udc56\n#\n= (1 \u2212 \ud835\udf0c)2 Var\n\"\ud835\udc5b\u22121\n\u2211\ufe01\n\ud835\udc56=0\n\ud835\udf0c\ud835\udc56 \u03b8\ud835\udc61\u2212\ud835\udc56\n#\n= (1 \u2212 \ud835\udf0c)2 \u00b7\n\u00121 \u2212 \ud835\udf0c2\ud835\udc5b\n1 \u2212 \ud835\udf0c2\n\u0013\n\u00b7 Var[\u03b8\u2217].\n(68)\nFinally, equating left and right hand sizes and rearranging for Var[\u03b6\u2217] gives\nVar [\u03b6\u2217] = 1 \u2212 \ud835\udf0c2\ud835\udc5b\n1 + \ud835\udf0c2\ud835\udc5b \u00b7 1 \u2212 \ud835\udf0c\n1 + \ud835\udf0c \u00b7 Var [\u03b8\u2217]\n(69)\nIn the limit \ud835\udc61 \u2192 \u221e, the momentum-dependent prefactor becomes\nlim\n\ud835\udc61\u2192\u221e\n\u00121 \u2212 \ud835\udf0c2\ud835\udc5b\n1 + \ud835\udf0c2\ud835\udc5b \u00b7 1 \u2212 \ud835\udf0c\n1 + \ud835\udf0c\n\u0013\n= 1 \u2212 \ud835\udf0c\n1 + \ud835\udf0c =\u21d2 lim\n\ud835\udc61\u2192\u221e Var [\u03b6\u2217] = 1 \u2212 \ud835\udf0c\n1 + \ud835\udf0c \u00b7 Var [\u03b8\u2217] .\n(70)\nEquations 69 and 70 validate our intuition. When \ud835\udf0c \u2192 0, then \u03b6 behaves like \u03b8 independent of \ud835\udc47,\nwith their variance and expectation matching. When \ud835\udf0c > 0, the momentum-dependent prefactor\nserves as an aggregator over the history when \ud835\udc61 is sufficiently large compared to \ud835\udc58, reducing the\nvariance Var[\ud835\udf01 \u2217] but preserving its expectation. This formalizes the notion of time horizon discussed\nin Table 1.\nF\nAdditional details and results for Polyak-Ruppert averaging\nAdditional background\nPolyak-Ruppert averaging (Definition 3.1) is a simplification of Stochas-\ntic Weight Averaging (SWA) (Izmailov et al., 2018) which uses a more complex multi-cycle sched-\nule based weighting of the model parameters. Both Definition 3.1 and SWA present similar favor-\nable properties like wider minima and better generalization (Izmailov et al., 2018). For example,\nHe et al. (2022) observed that a supervised ViT-H/14 overfits on ImageNet1k (Russakovsky et al.,\n2014) without a model EMA, achieving an accuracy of 80.9%. Equipping a Polyak-Ruppert average\n(\ud835\udf0c = 0.9999) alleviated overfitting and gave a 83.1% accuracy.\n27\nOrganization\nIn this appendix, we look at additional momenta for one-dimensional noisy\nparabola, as well as extensions to \ud835\udc37-dimensions (Appendix F.1), provide a more detailed view\nof the results of Section 3.2 (Appendix F.2), and investigate the scenario where the EMA Scal-\ning Rule (Definition 1.2) is applied to batch normalization (Ioffe & Szegedy, 2015) coefficients\n(Appendix F.3).\nF.1\nNoisy parabola\nAdditional one-dimensional examples\nFirst we consider additional one-dimensional examples,\ninvestigating the effect of modifying the base momentum \ud835\udf0c\ud835\udc35. We present \ud835\udf0c\ud835\udc35 = 0.99 in Figure 7, and\n\ud835\udf0c\ud835\udc35 = 0.999 in Figure 8. The results for \ud835\udf0c\ud835\udc35 = 0.9999 are presented in main text in Figure 1.\n0.0\n0.5\n1.0\nContinuous Time\n0.4\n0.6\n0.8\n1.0\nEMA Position \u03b6t\nScaling \u03ba = 8\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 8, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 8, \u03c1 = \u03c1B\n0.0\n0.5\n1.0\nContinuous Time\nScaling \u03ba = 256\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 256, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 256, \u03c1 = \u03c1B\n(a) Trajectory of the model EMA \ud835\udec7 under different\nscalings \ud835\udf05, with \ud835\udf0c\ud835\udc35 = 0.99, \ud835\udf02\ud835\udc35 = 10\u22124.\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n0.0\n0.5\n1.0\nMomentum \u03c1\nEMA, \u03c1 = \u03c1B\nEMA, \u03c1 = \u03c1\u2217\nEMA, \u03c1 = \u03c1\u03ba\nB\nModel\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n10\u22122\n100\nApproximation Error\n(b) Choices for momentum (left) with corresponding ap-\nproximation errors (Equation 10) (right).\nFigure 7: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\ud835\udf05 = 1, black\ndashed) to \ud835\udf05 = 8 (left) and \ud835\udf05 = 256 (right), with (\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, blue) and without (\ud835\udf0c = \ud835\udf0c\ud835\udc35, red) the EMA Scaling Rule.\n(b, left) The momentum according for different scaling rules and the empirically optimal \ud835\udf0c\u2217 (Equation 10). (b,\nright) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange). Error for\n\ud835\udf0c\u2217 is computed using a hold-out to mitigate overfitting.\n0.0\n0.5\n1.0\nContinuous Time\n0.4\n0.6\n0.8\n1.0\nEMA Position \u03b6t\nScaling \u03ba = 8\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 8, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 8, \u03c1 = \u03c1B\n0.0\n0.5\n1.0\nContinuous Time\nScaling \u03ba = 256\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 256, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 256, \u03c1 = \u03c1B\n(a) Trajectory of the model EMA \ud835\udec7 under different\nscalings \ud835\udf05, with \ud835\udf0c\ud835\udc35 = 0.999, \ud835\udf02\ud835\udc35 = 10\u22124.\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n0.25\n0.50\n0.75\n1.00\nMomentum \u03c1\nEMA, \u03c1 = \u03c1B\nEMA, \u03c1 = \u03c1\u2217\nEMA, \u03c1 = \u03c1\u03ba\nB\nModel\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n10\u22122\n100\nApproximation Error\n(b) Choices for momentum (left) with corresponding ap-\nproximation errors (Equation 10) (right).\nFigure 8: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\ud835\udf05 = 1, black\ndashed) to \ud835\udf05 = 8 (left) and \ud835\udf05 = 256 (right), with (\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, blue) and without (\ud835\udf0c = \ud835\udf0c\ud835\udc35, red) the EMA Scaling Rule.\n(b, left) The momentum according for different scaling rules and the empirically optimal \ud835\udf0c\u2217 (Equation 10). (b,\nright) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange). Error for\n\ud835\udf0c\u2217 is computed using a hold-out to mitigate overfitting.\nAs described by the scaling error term in Equation 54, the approximation error at a given \ud835\udf05 is higher\nfor lower momenta \ud835\udf0c. For a large range of scalings \ud835\udf05, the EMA Scaling Rule and the optimal\nmomenta \ud835\udf0c\u2217 are consistent. In summary, we see the synthetic experiments validate the results of\nSection 3.1 for a range of momenta \ud835\udf0c.\nExamples in higher dimensions\nOur final use of the synthetic noisy parabola will consider an\nextension to \ud835\udc37 dimensions. Consider the optimization of \ud835\udec9 \u2208 R\ud835\udc37 in a noisy parabola at the origin:\nL(\ud835\udec9) = \ud835\udc4e\n2 \ud835\udec9\u22ba\ud835\udec9,\n\ud835\udec9\ud835\udc58+1 = \ud835\udec9\ud835\udc58 \u2212 \ud835\udf02 g\ud835\udc58,\ng\ud835\udc58 = \ud835\udc4e \ud835\udec9\ud835\udc58 + \ud835\udedc\ud835\udc58,\n\ud835\udedc\ud835\udc58 \u223c N\n\u0010\n0,\n\ud835\udc4f g2\n\ud835\udc58+\ud835\udc50\n\ud835\udf05\n\u0011\n,\n(71)\nfor curvature \ud835\udc4e > 0, scaled additive \ud835\udc4f > 0, and additive \ud835\udc50 > 0 noise coefficients. The scaling factor\n\ud835\udf05 in the covariance denominator implements the reduction in gradient noise as the scaling (i.e., the\nbatch size) increases (Jastrzebski et al., 2017). Let \ud835\udec9 \u2208 R\ud835\udc37 be optimized with SGD (Definition 2.1)\n28\nand let there be a Polyak-Ruppert average (Definition 3.1) \ud835\udec7 \u2208 R\ud835\udc37 with momentum \ud835\udf0c = 1 \u2212 \ud835\udefd for \ud835\udec9.\nWe consider dimensionalities \ud835\udc37 = 2 (Figure 9), \ud835\udc37 = 16 (Figure 10), and \ud835\udc37 = 100 (Figure 11). We\nobserve no significant differences in the EMA scaling behavior as we vary dimensions.\n0.0\n0.5\n1.0\nContinuous Time\n0.8\n0.9\n1.0\nEMA Norm ||\u03b6t||2\nScaling \u03ba = 8\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 8, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 8, \u03c1 = \u03c1B\n0.0\n0.5\n1.0\nContinuous Time\nScaling \u03ba = 256\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 256, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 256, \u03c1 = \u03c1B\n(a) Norm of the model EMA \ud835\udec7 under different\nscalings \ud835\udf05, with \ud835\udf0c\ud835\udc35 = 0.9999, \ud835\udf02\ud835\udc35 = 10\u22124, \ud835\udc37 = 2.\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n0.90\n0.95\n1.00\nMomentum \u03c1\nEMA, \u03c1 = \u03c1B\nEMA, \u03c1 = \u03c1\u2217\nEMA, \u03c1 = \u03c1\u03ba\nB\nModel\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n10\u22122\n100\nApproximation Error\n(b) Choices for momentum (left) with corresponding ap-\nproximation errors (Equation 10) (right).\nFigure 9: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\ud835\udf05 = 1, black\ndashed) to \ud835\udf05 = 8 (left) and \ud835\udf05 = 256 (right), with (\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, blue) and without (\ud835\udf0c = \ud835\udf0c\ud835\udc35, red) the EMA Scaling Rule.\n(b, left) The momentum according for different scaling rules and the empirically optimal \ud835\udf0c\u2217 (Equation 10). (b,\nright) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange). Error for\n\ud835\udf0c\u2217 is computed using a hold-out to mitigate overfitting.\n0.0\n0.5\n1.0\nContinuous Time\n0.8\n0.9\n1.0\nEMA Norm ||\u03b6t||2\nScaling \u03ba = 8\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 8, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 8, \u03c1 = \u03c1B\n0.0\n0.5\n1.0\nContinuous Time\nScaling \u03ba = 256\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 256, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 256, \u03c1 = \u03c1B\n(a) Norm of the model EMA \ud835\udec7 under different\nscalings \ud835\udf05, with \ud835\udf0c\ud835\udc35 = 0.9999, \ud835\udf02\ud835\udc35 = 10\u22124, \ud835\udc37 = 16.\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n0.90\n0.95\n1.00\nMomentum \u03c1\nEMA, \u03c1 = \u03c1B\nEMA, \u03c1 = \u03c1\u2217\nEMA, \u03c1 = \u03c1\u03ba\nB\nModel\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n10\u22122\n100\nApproximation Error\n(b) Choices for momentum (left) with corresponding ap-\nproximation errors (Equation 10) (right).\nFigure 10: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\ud835\udf05 = 1,\nblack dashed) to \ud835\udf05 = 8 (left) and \ud835\udf05 = 256 (right), with (\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, blue) and without (\ud835\udf0c = \ud835\udf0c\ud835\udc35, red) the EMA\nScaling Rule. (b, left) The momentum according for different scaling rules and the empirically optimal \ud835\udf0c\u2217\n(Equation 10). (b, right) The approximation error (Equation 10) of trajectories in (b, left) and the target model\n(orange). Error for \ud835\udf0c\u2217 is computed using a hold-out to mitigate overfitting.\n0.0\n0.5\n1.0\nContinuous Time\n0.8\n0.9\n1.0\nEMA Norm ||\u03b6t||2\nScaling \u03ba = 8\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 8, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 8, \u03c1 = \u03c1B\n0.0\n0.5\n1.0\nContinuous Time\nScaling \u03ba = 256\n\u03ba = 1, \u03c1 = \u03c1B\n\u03ba = 256, \u03c1 = \u03c1\u03ba\nB\n\u03ba = 256, \u03c1 = \u03c1B\n(a) Norm of the model EMA \ud835\udec7 under different\nscalings \ud835\udf05, with \ud835\udf0c\ud835\udc35 = 0.9999, \ud835\udf02\ud835\udc35 = 10\u22124, \ud835\udc37 = 100.\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n0.90\n0.95\n1.00\nMomentum \u03c1\nEMA, \u03c1 = \u03c1B\nEMA, \u03c1 = \u03c1\u2217\nEMA, \u03c1 = \u03c1\u03ba\nB\nModel\n1\n4\n16\n64\n256 1024\nScaling Factor \u03ba\n10\u22122\n100\nApproximation Error\n(b) Choices for momentum (left) with corresponding ap-\nproximation errors (Equation 10) (right).\nFigure 11: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\ud835\udf05 = 1,\nblack dashed) to \ud835\udf05 = 8 (left) and \ud835\udf05 = 256 (right), with (\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, blue) and without (\ud835\udf0c = \ud835\udf0c\ud835\udc35, red) the EMA\nScaling Rule. (b, left) The momentum according for different scaling rules and the empirically optimal \ud835\udf0c\u2217\n(Equation 10). (b, right) The approximation error (Equation 10) of trajectories in (b, left) and the target model\n(orange). Error for \ud835\udf0c\u2217 is computed using a hold-out to mitigate overfitting.\nCompute\nThe compute usage for the noisy parabola experiments is relatively small, with each run\ntaking less than one minute on a single CPU, and so we do not detail this compute usage as we do\nin the other experimental sections.\n29\nTable 4: Supervised ResNetv2-50 hyperparameters used in Polyak-Ruppert Averaging experiments.\nSupervised ResNetv2-50\nImageNet1k Test Top-1\n76.27 \u00b1 0.10%\nImageNet1k EMA Test Top-1\n76.55 \u00b1 0.07%\nWeight initialization\nkaiming_normal(relu)\nBackbone normalization\nBatchNorm\nSynchronized BatchNorm over replicas\nNo\nLearning rate schedule\nMulti step: \u00d70.1 at [30, 60, 80] epochs\nLearning rate warmup (epochs)\n5\nLearning rate minimum value\n1 \u00d7 10\u22126\nTraining duration (epochs)\n90\nOptimizer\nSGD + Momentum\nSGD momentum\n0.9\nOptimizer scaling rule\nLinear\nBase learning rate\n0.4\nBase batch size\n1024\nBase Polyak momentum\n0.9999\nWeight decay\n1 \u00d7 10\u22124\nWeight decay scaling rule\nNone\nWeight decay skip bias\nYes\nNumerical precision\nbf16\nAugmentation stack\nImageNet\nLabel smoothing rate\n0.1\nF.2\nImage Classification\nHyperparameters\nWe present the base hyperparameters for our image experiments in Table 4.\nData\nFor large scale vision evaluation, we use the ImageNet1k dataset (Russakovsky et al., 2014),\na widely used dataset containing approximately 1.2 million labeled images, distributed almost uni-\nformly across 1000 different object classes, like animals, plants, and vehicles.\nThe images in ImageNet1k are are not consistent in resolution. To handle this, they are resized and\ncropped to a standard size (in our case, 224 \u00d7 224), before further processing. This is part of the\nstandard ImageNet augmentation stack for convolutional networks mentioned in Table 4.\nCompute usage\nThe compute usage image classification Polyak-Ruppert averaging is summa-\nrized in Table 5.\nTable 5: Compute usage for image classification Polyak-Ruppert averaging in Figures 2 and 13. The three runs\nfor the batch size 1,024 baseline correspond to three seeds, and the nine runs for all other batch sizes correspond\nto using and not using the EMA Scaling Rule shown in Figure 2, and its application to Batch Normalization\nshown in Figure 13. All experiments conducted are using 80Gb A100s.\nBatch Size\nGPUs\nTime (h)\nCompute/Run (GPUh)\nRuns\nCompute (GPUh)\n512\n8\n35.3\n282.4\n9\n2,541.6\n1,024\n8\n17.1\n137.0\n3\n410.9\n2,048\n8\n13.3\n106.7\n9\n960.6\n4,096\n8\n4.2\n33.5\n9\n301.9\n8,192\n16\n2.8\n44.8\n9\n403.6\nAll other compute, e.g. code development, runs with errors, and debugging\n25,768.3\nTotal\n30386.8\nAdditional results\nIn Figure 12 we present a more detailed view of the results in Section 3.2. First,\nwe see that for all train metrics, model trajectories match, and that a learning rate step schedule after\nwarmup is present. As discussed in Figure 12, a gap in EMA Test Top-1 trajectories begins at scaling\n\ud835\udf05 = 4, with a more pronounced effect visible at \ud835\udf05 = 8. From Figure 12 it is clear that the (non-EMA)\n30\nTest Top-1 performance trajectory is no longer matching at these scalings, demonstrating that the\nproblem is not due to a breakdown of the EMA Scaling Rule, but rather, that the model is overfitting\nat larger batch sizes due to batch normalization (Ioffe & Szegedy, 2015).\n2\n4\n6\nTrain Model Loss\nScaling \u03ba = 1/2\nB = 512, \u03c1 = \u03c1\u03ba\nB\nB = 512, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 8\nB = 8192, \u03c1 = \u03c1\u03ba\nB\nB = 8192, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n20\n40\n60\n80\nTrain Top-1 (%)\n0\n20\n40\n60\n80\nTest Top-1 (%)\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\n0\n20\n40\n60\n80\nEMA Test Top-1 (%)\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\nFigure 12: ResNetv2-50 Polyak-Ruppert averaging on ImageNet1k for different scalings \ud835\udf05. The baseline model\n(\ud835\udf05 = 1, black dashed) uses batch size 1024 and momentum \ud835\udf0c\ud835\udc35 = 0.9999, is scaled down to a batch size of 512\n(left), and up to a batch size of 4096 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling\nRule (Definition 1.2). Bands indicate the mean and standard deviation across three runs.\nF.3\nApplying the EMA Scaling Rule to Batch Normalization\nIn Section 3.2 and Appendix F.2, we investigated a range of scalings \ud835\udf05, with and without applying the\nEMA Scaling Rule to the Polyak momentum. In those experiments, we maintained Batch Normal-\nization (Ioffe & Szegedy, 2015) coefficients of \ud835\udf0cBN = 0.9 throughout11, i.e. the EMA Scaling Rule\nis not applied. The running statistics of Batch Normalization are an EMA with values determined\nby \ud835\udf0cBN and so it is reasonable to suspect we should apply the EMA Scaling Rule to \ud835\udf0cBN also.\nIn Figure 13 we investigate the effect of applying the EMA Scaling Rule to Batch Normalization\ncoefficients, using \u02c6\ud835\udf0cBN = \ud835\udf0c\ud835\udf05\nBN. We observe that the Test Top-1 trajectories with the EMA Scaling\nRule applied are slightly closer to the reference trajectories for scalings \ud835\udf05 \u2265 2 than those trajectories\nwithout the EMA Scaling Rule. As the effect is not particularly large, at least in this setup, we do\npursue further ablating applications of the EMA Scaling Rule to batch normalization coefficients,\nand always use \ud835\udf0cBN = 0.1 for Batch Normalization, independent of \ud835\udf05.\n11In many ML frameworks, this value is defined using \ud835\udefd\ud835\udf0c = 1 \u2212 \ud835\udf0c, i.e. the default is 0.1 and corresponds to\n\ud835\udefdBN rather than 0.9 corresponding to \ud835\udf0cBN. We use \ud835\udf0cBN to maintain consistency across this work.\n31\n2\n4\n6\nTrain Model Loss\nScaling \u03ba = 1/2\nB = 512, \u03c1 = \u03c1\u03ba\nB (\u2020)\nB = 512, \u03c1 = \u03c1\u03ba\nB\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB (\u2020)\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB (\u2020)\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 8\nB = 8192, \u03c1 = \u03c1\u03ba\nB (\u2020)\nB = 8192, \u03c1 = \u03c1\u03ba\nB\nB = 1024, \u03c1 = \u03c1B\n0\n20\n40\n60\n80\nTrain Top-1 (%)\n0\n20\n40\n60\n80\nTest Top-1 (%)\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\n0\n20\n40\n60\n80\nEMA Test Top-1 (%)\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\n0\n15\n30\n45\n60\n75\n90\nTrain Epochs\nFigure 13: ResNetv2-50 Polyak-Ruppert averaging on ImageNet1k for different scalings \ud835\udf05. The baseline model\n(\ud835\udf05 = 1, black dashed) uses batch size 1024 and momentum \ud835\udf0c\ud835\udc35 = 0.9999, is scaled down to a batch size of\n512 (left), and up to a batch size of 4096 (right) with the EMA Scaling Rule applied to only model parameters\n(blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35), and model parameters and buffers (orange, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35 (\u2020)). Bands indicate the mean and standard\ndeviation across three runs.\nG\nAdditional details and results for Automatic Speech Recognition (ASR)\nIn this section we provide additional details for the speech recognition experiments in both the\nsupervised and semi-supervised case.\nData\nWe use the LibriSpeech dataset (Panayotov et al., 2015) which is a dataset of audio-\ntranscription pairs. For supervised Polyak-Ruppert averaging experiments, we use train-clean-100\nas training data, and for semi-supervised pseudo-labeling experiments, we use train-clean-100 as\nthe labeled and train-clean-360 and train-other-500 as the unlabeled data. The standard LibriSpeech\nvalidation sets (dev-clean and dev-other) are used to tune all hyperparameters, as well as to select\nthe best models. Test sets (test-clean and test-other) are only used for reporting final model per-\nformance, measured in WER without an external language model. We maintain the original 16kHz\nsampling rate, and compute log-mel filterbanks with 80 coefficients for a 25ms sliding window,\nstrided by 10ms, later normalized to zero mean and unit variance for each input sequence.\nAcoustic model\nWe employ a vanilla encoder-based only transformer model trained with the Con-\nnectionist Temporal Classification (CTC) loss (Graves et al., 2006). We use the training configura-\ntion from Likhomanenko et al. (2021a), which has three stages: i) 1D convolutions to perform strid-\ning (kernel of 7 with stride of 3); ii) a Transformer encoder with 36 layers, post-LayerNorm, four\nattention heads, an embedding dimension of 768, an MLP dimension of 3072, a dropout frequency\nof 0.3, and a layer drop frequency of 0.3; and iii) a linear layer to map to the target vocabulary12.\nTo reduce model training time by a factor of approximately 2 \u2212 3\u00d7, and to reduce memory footprint,\n12The token set of this vocabulary consists of the 26 English alphabet letters augmented with the apostrophe\nand a word boundary token.\n32\nTable 6: Hyperparameters summary for speech recognition task for supervised (left) and semi-supervised\npseudo-labeling (right) training with a vanilla transformer. The 0.3 \u2192 0.1 in the dropout and layer drop rates\nindicates that a rate of 0.3 is used during pre-training, and a rate of 0.1 is used during pseudo-labeling.\nSupervised\nPseudo-Labeling\nLibrispeech test-clean / test-other WER\n7.8/19.1\n4.8/11.5\nOptimizer\nAdam\nAdam\nOptimizer scaling rule\nAdam\nAdam\nBase (\ud835\udefd1, \ud835\udefd2)\n(0.995, 0.999)\n(0.995, 0.999)\nBase learning rate\n0.0001\n0.0001\nBase learning rate warmup (steps)\n64k\n64k\nLearning rate schedule\nFixed (no decay)\nFixed (no decay)\nLearning rate minimum value\n0\n0\nBase training duration (steps)\n400k\n500k\nBase batch size (dynamic)\n8 \u00d7 290\ud835\udc60\n8 \u00d7 290\ud835\udc60\nBase teacher momentum\n0.99995\n0.9999\nWeight decay\nNone\nNone\nNumerical precision\nbf16\nbf16\nAugmentation stack\nSpecAug\nSpecAug\nDropout\n0.3\n0.3 \u2192 0.1\nLayer drop\n0.3\n0.3 \u2192 0.1\nGradient clipping\n1\n1\nLabeled:unlabeled data ratio\nN/A\n1:3\nBase pre-training steps\nN/A\n20k\nBase start of EMA accumulation (steps)\nN/A\n19k\nwe use CAPE positional embeddings (Likhomanenko et al., 2021b) instead of relative positional\nembeddings (Shaw et al., 2018): both models perform similarly.\nTraining\nHere we discuss our training procedure for base batch size \ud835\udc35 = 8\u00d7290\ud835\udc60, which is adapted\nfrom Likhomanenko et al. (2021a), and is summarized in Table 6. We use SpecAugment (Park et al.,\n2019) activated after 5k steps of training: two frequency masks with frequency mask parameter\n\ud835\udc39 = 30, ten time masks with maximum time-mask ratio \ud835\udc5d = 0.1 and time mask parameter \ud835\udc47 = 50 are\nused; time warping is not used.\nOne difference in setup is we use the Adam optimizer, whereas Likhomanenko et al. (2021a) used\nAdagrad (Duchi et al., 2010). Even though Adagrad can be viewed as a particular limit (\ud835\udefd1 = 0 and\n\ud835\udefd2 \u2192 1) of Adam (Kingma & Ba, 2015), we were unable to produce reasonable optimization in\npractice when applying the Adam Scaling Rule of Malladi et al. (2022) in this limit. As a conse-\nquence, we chose to work with the Adam optimizer, where its scaling rule has been shown to work\n(Malladi et al., 2022), and we take \ud835\udefd1 = 0.995, \ud835\udefd2 = 0.999, and \ud835\udf16 = 10\u22128. We obtained similar results\nfor \ud835\udefd1 = 0.99. Finally, we use a linear learning rate warmup (64k steps) after which the learning rate\nis kept constant until convergence. This performance can be improved further by using a step decay\nschedule as shown in prior work. We also apply gradient clipping of 1, and do not use weight decay.\nPseudo-Labeling\nThe pseudo-labeling process comprises of two stages: i) The pre-training phase,\nwhere we train model on labeled data for 20k steps with model EMA accumulation starting after\n19k steps; and ii) the pseudo-labeling phase, where we involve unlabeled data by generating pseudo-\nlabels from the model EMA (teacher) and provide them to the model (student) as if they were\nground-truth labels. Pseudo-labels are generated without any dropout applied to the teacher, and\nno data augmentation is applied for the corresponding inputs. To produce the pseudo-label, we use\nhard transcription (Definition G.1)\nDefinition G.1 (Hard Transcription). For a sequence of frames, select the most probable token\nper frame, removing repetitions and the CTC blank token. For example, \u201ch##eelll##ll###oo\u201d is\ntransformed into \u201chello\u201d, where \u201c#\u201d is the CTC blank token.\nThese hard transcriptions are then used as transcription for student optimization. We use a 1:3\nproportion of labeled to unlabeled data as this was found to be optimal in Likhomanenko et al.\n(2021a), and we decrease model dropout and layer drop rates to 0.1 after pre-training phase. As\nwe have access to the ground-truth labels on the data being treated as unlabeled, we can track\n33\npseudo-label quality by computing pseudo-labels on this data, and compute the WER against their\nground-truth. Pseudo-label quality is the primary metric to evaluate progress on unlabeled data, as\nloss on pseudo-labeled data is unreliable when a teacher model and pseudo-labels are evolving with\neach time step.\nScaling of batch size\nSequential data is typically processed using dynamic batching as it is more\ncomputationally efficient than using a fixed number of sequences (Ott et al., 2019). In our work, we\nuse dynamic batching of \u223c290s audio per GPU. Moreover, for CTC we do not apply any additional\nsequence normalization. We experimented with fixed batching, but did not observe any significant\ndifferences in conclusions compared with the dynamic batching.\nWe note that dynamic batching is a more challenging setting for achieving systematic scaling, as the\nnumber of independent sequences in any given batch may change, and the i.i.d. assumption does\nnot hold at the frame level. Despite these violations of the assumptions of Section 2.2, our results\ndemonstrate that the Adam Scaling Rule (Definition C.3, Malladi et al. (2022)) holds in the case of\ndynamic batches, as does our EMA Scaling Rule (Definition 1.2).\nThe base batch size is set to \ud835\udc35 = 8 \u00d7 290\ud835\udc60, and in our experiments we scale down to batch size of\n\ud835\udc35 = 2 \u00d7 290\ud835\udc60 and up to batch size of \ud835\udc35 = 128 \u00d7 290\ud835\udc60. The number of warmup and pre-training\nsteps, steps before SpecAugment is turn on and model EMA is accumulated are scaled according to\nAppendix C.1.\nCompute\nAll experiments are done using A100 80GB 8GPU nodes with bfloat16 precision\ntraining. While for supervised training evaluation of different EMA decay values is done in parallel\nduring a single run, for pseudo-labeling every EMA decay value needs separate training. Final mod-\nels training compute is detailed in Tables 7 and 8. Total compute, including e.g. code development,\nruns with errors, and debugging, is 61k GPUh.\nTable 7: Compute usage for supervised model for speech recognition task in Figure 14. Values include node\nallocation times (typically a small % of corresponding total runtime), giving a practical estimate of reproduction\ncost. All experiments conducted are using 80Gb A100s with fast interconnect.\nBatch Size\nGPUs\nTime (h)\nCompute/Run (GPUh)\nRuns\nCompute (GPUh)\n2 \u00d7 290s\n2\n222\n444\n1\n222\n4 \u00d7 290s\n4\n108\n432\n1\n432\n8 \u00d7 290s\n8\n64\n512\n1\n512\n16 \u00d7 290s\n16\n54\n864\n1\n896\n32 \u00d7 290s\n32\n37\n1,184\n1\n1,184\nTotal\n3,436\nTable 8: Compute usage for continuous pseudo-labeling for the speech recognition task in Figure 16. Values\ninclude node allocation times (typically a small % of corresponding total runtime), giving a practical estimate\nof reproduction cost. All experiments conducted are using 80Gb A100s with fast interconnect.\nBatch Size\nGPUs\nTime (h)\nCompute/Run (GPUh)\nRuns\nCompute (GPUh)\n2 \u00d7 290s\n2\n225\n550\n2\n1,110\n4 \u00d7 290s\n4\n120\n480\n2\n960\n8 \u00d7 290s\n8\n72\n576\n1\n576\n16 \u00d7 290s\n16\n45\n720\n2\n1,440\n32 \u00d7 290s\n32\n33\n1,056\n4\n4,224\n64 \u00d7 290s\n64\n25\n1,600\n2\n3,200\nTotal\n11,510\nG.1\nAdditional experimental settings and detailed metrics\nWe present detailed comparison between models trained with and without EMA Scaling Rule in Fig-\nures 14 and 15 for supervised training and in Figures 16 and 17 for semi-supervised training.\nFirst, we observe that if the Adam Scaling Rule does not hold perfectly13 (there is a mismatch\nbetween trajectories for the model before pseudo-labels are involved) the EMA Scaling Rule also\n13See Malladi et al. (2022) for a discussion on scenarios that lead to a breakdown of the Adam Scaling Rule.\n34\n0\n200\n400\n600\nTrain Loss\nScaling \u03ba = 1/4\nScaling \u03ba = 1/2\nScaling \u03ba = 2\nScaling \u03ba = 4\n20\n40\n60\n80\n100\ntest-other WER\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\n20\n40\n60\n80\n100\nEMA test-other WER\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nFigure 14: Transformer Polyak-Ruppert averaging on LibriSpeech (trained on train-clean-100) with different\nscalings \ud835\udf05. The baseline (\ud835\udf05 = 1, black dashed) is trained with Adam and momentum \ud835\udf0c\ud835\udc35 = 0.99995 at a dynamic\nbatch size \ud835\udc35 = 8 \u00d7 290\ud835\udc60, which corresponds to a single train step on the \ud835\udc65-axis. We investigate dynamic batch\nsizes down to \ud835\udc35 = 2 \u00d7 290\ud835\udc60 (left) and up to \ud835\udc35 = 32 \u00d7 290\ud835\udc60 (right), with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35), and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35)\nthe EMA Scaling Rule (model non-EMA is marked by orange). The Adam Scaling Rule (Malladi et al. (2022),\nDefinition C.3) is used throughout. For momentum \ud835\udf0c\ud835\udc35 = 0.9999 we observe similar trajectories for all models.\n0\n200\n400\n600\nTrain Loss\nScaling \u03ba = 1/4\nScaling \u03ba = 1/2\nScaling \u03ba = 2\nScaling \u03ba = 4\n20\n40\n60\n80\n100\ntest-other WER\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\n20\n40\n60\n80\n100\nEMA test-other WER\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n1\n2\n3\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nFigure 15: Transformer Polyak-Ruppert averaging on LibriSpeech (trained on train-clean-100) with different\nscalings \ud835\udf05. The baseline (\ud835\udf05 = 1, black dashed) is trained with Adam and momentum \ud835\udf0c\ud835\udc35 = 0.999 at a dynamic\nbatch size \ud835\udc35 = 8 \u00d7 290\ud835\udc60, which corresponds to a single train step on the \ud835\udc65-axis. We investigate dynamic batch\nsizes down to \ud835\udc35 = 2 \u00d7 290\ud835\udc60 (left) and up to \ud835\udc35 = 32 \u00d7 290\ud835\udc60 (right), with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35), and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35)\nthe EMA Scaling Rule (model non-EMA is marked by orange). The Adam Scaling Rule (Malladi et al. (2022),\nDefinition C.3) is used throughout. If momentum \ud835\udf0c\ud835\udc35 is small and accumulation history is short we observe no\nany significant difference between models which all are matching the reference trajectory despite scaling \ud835\udf05.\ngives discrepancies with the reference trajectory, however they are negligible compared to models\ntrained without EMA Scaling Rule. For the semi-supervised training, to alleviate the difficulties with\na breakdown of the Adam Scaling Rule for large \ud835\udf05 we postpone the pseudo-labeling process until\nthe model reaches similar WER as the baseline. This allows us to align the initial model conditions\nfor pseudo-labeling. In this scenario we are able to match the reference trajectory up to \ud835\udf05 = 8.\n35\n200\n400\n600\nSupervised Train Loss\nScaling \u03ba = 1/4\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nScaling \u03ba = 1/2\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nScaling \u03ba = 2\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nScaling \u03ba = 4\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1B\nB = 32\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nScaling \u03ba = 8\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 64\u00d7290s,\u03c1 = \u03c1B\nB = 64\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n20\n40\n60\n80\n100\nPL WER\n0\n2\n4\nTrain Steps\n\u00d7105\n0\n25\n50\n75\n100\ntest-other WER\n0\n2\n4\nTrain Steps\n\u00d7105\n0\n2\n4\nTrain Steps\n\u00d7105\n0\n2\n4\nTrain Steps\n\u00d7105\n0\n2\n4\nTrain Steps\n\u00d7105\nFigure 16: Transformer pseudo-labeling on LibriSpeech (trained on train-clean-100 as labeled and the rest\nof LibriSpeech as unlabeled) with different scalings \ud835\udf05. The baseline (\ud835\udf05 = 1, black dashed) is trained with\nAdam at a dynamic batch size of 8 \u00d7 290 seconds, which corresponds to a single train step on the \ud835\udc65-axis. The\nmodel EMA (teacher) is updated with momentum \ud835\udf0c\ud835\udc35 = 0.9999. We investigate dynamic batch sizes down to\n\ud835\udc35 = 2 \u00d7 290\ud835\udc60 (left) and up to \ud835\udc35 = 64 \u00d7 290\ud835\udc60 (right), with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA\nScaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. For \ud835\udf05 \u2264 2, we\nstart pseudo-labeling after 20k/\ud835\udf05 training steps; while for \ud835\udf05 > 2, we start when pre-training WER matches the\nbaseline WER (24k/\ud835\udf05 for \ud835\udf05 = 4 and 29k/\ud835\udf05 for \ud835\udf05 = 8). For \ud835\udf05 = 4 we experimented with both variants: we start\npseudo-labeling after 20k/\ud835\udf05 (dashed) and when pre-training WER matches the baseline WER (solid, 24k/\ud835\udf05).\n200\n400\n600\nSupervised Train Loss\nScaling \u03ba = 1/4\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1B\nB = 2\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nScaling \u03ba = 1/2\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1B\nB = 4\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nScaling \u03ba = 2\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1B\nB = 16\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n20\n40\n60\n80\nPL WER\n0\n1\n2\n3\n4\n5\nTrain Steps\n\u00d7105\n0\n20\n40\n60\n80\n100\ntest-other WER\n0\n1\n2\n3\n4\n5\nTrain Steps\n\u00d7105\n0\n1\n2\n3\n4\n5\nTrain Steps\n\u00d7105\nFigure 17: Transformer pseudo-labeling on LibriSpeech (using train-clean-100 as labeled) with different scal-\nings \ud835\udf05. The baseline (\ud835\udf05 = 1, black dashed) is trained with Adam at a dynamic batch size of 8 \u00d7 290 seconds,\nwhich corresponds to a single train step on the \ud835\udc65-axis. The model EMA (teacher) is updated with momentum\n\ud835\udf0c\ud835\udc35 = 0.999. We investigate dynamic batch sizes down to \ud835\udc35 = 2\u00d7290\ud835\udc60 (left) and up to \ud835\udc35 = 16\u00d7290\ud835\udc60 (right), with\n(blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling Rule. The Adam Scaling Rule is used throughout.\nWe note that this result reveals that errors for the Adam Scaling Rule and the EMA Scaling Rule\nare contributing, although the way in which they contribute is different, and one can dominate the\nother. We observe in Figure 16 that if the initial conditions of the models are similar (attained by\nusing the same WER as a condition to begin pseudo-labeling) then the error from the EMA Scaling\nRule dominates over that of the Adam Scaling Rule, causing a divergence in training dynamics.\n36\nSecond, we observe in practice that the EMA Scaling Rule holds for both fixed batching (a sequence\nlength in the batch can vary significantly) and for dynamic batching (when total number of frames\nin the batch is fixed, though padding still is accounted to the this amount). This shows that EMA\nScaling Rule is applicable to sequential data too.\nThird, we observe in Figures 15 and 17 that for smaller values of \ud835\udf0c\ud835\udc35, scaling with or without\nEMA Scaling Rule behave similarly, and reference trajectories match in the supervised and semi-\nsupervised cases. However, if the momentum is too large, the teacher moves slowly and is uninfor-\nmative, whereas if the momentum is too low, the teacher and the student are effectively be the same\nmodel, implying: i) the student will self-predict with high confidence, removing any benefits of dis-\ntillation14; and ii) training instability or model divergence will happen in the low-resource settings\n(Likhomanenko et al., 2021a; Higuchi et al., 2022).\nG.2\nScaling to \ud835\udf05 = 16 with Progressive Scaling\nFinally, we aim to scale semi-supervised pseudo-labeling further to \ud835\udf05 = 16. In this case we observe\nthat Adam Scaling Rule does not hold in the pre-training phase and there is no model convergence.\nTo overcome this, we apply Progressive Scaling (Definition 3.2). We pre-train models on supervised\ndata with \ud835\udf05 = 8 for 29k of reference steps (model EMA accumulation starts at 28k steps). We then\nscale to \ud835\udf05 = 16 and begin pseudo-labeling. We see in Figure 18 that Progressive Scaling enables us\nto scale pseudo-labeling to \ud835\udf05 = 16 with (middle) and without (left) the EMA Scaling Rule. Second,\nmodels with the EMA Scaling Rule track the baseline much closer than models without the EMA\nScaling Rule, although a small gap is present. We further experimented with Progressive Scaling,\npostponed the transition condition to the \ud835\udf05 = 16 until 75k reference steps. In Figure 18 (right), we\nsee this scaled model tracks the reference trajectory, and so using a combination of the EMA Scaling\nRule and Progressive Scaling, we are able to scale pseudo-labeling to \ud835\udf05 = 16, corresponding to a\ndynamic batch size of 128 \u00d7 290\ud835\udc60.\n0\n2\n4\nLearning Rate\n\u00d710\u22124 Scaling \u03ba = 8 \u2192 16\nScaling \u03ba = 8 \u2192 16\nScaling \u03ba = 8 \u2192 16\n0\n20\n40\n60\n80\nPL WER\n0\n2\n4\nTrain Steps\n\u00d7105\n0\n50\n100\ntest-other WER\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 64\u00d7290s,\u03c1 = \u03c1B\nB = 64 \u2192 128\u00d7290s,\u03c1 = \u03c1B\n0\n2\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 64\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nB = 64 \u2192 128\u00d7290s,\u03c1 = \u03c1\u03ba\nB\n0\n2\n4\nTrain Steps\n\u00d7105\nB = 8\u00d7290s,\u03c1 = \u03c1B\nB = 64\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nB = 64 \u2192 128\u00d7290s,\u03c1 = \u03c1\u03ba\nB\nFigure 18: Transformer pseudo-labeling on LibriSpeech (trained on train-clean-100 as labeled and the rest of\nLibriSpeech as unlabeled) with different Progressive Scaling from \ud835\udf05 = 8 to \ud835\udf05 = 16 (\ud835\udf05 = 8 \u2192 16). The baseline\n(\ud835\udf05 = 1, black dashed) is trained with Adam at a dynamic batch size of 8 \u00d7 290 seconds, which corresponds\nto a single train step on the \ud835\udc65-axis. The model EMA (teacher) is updated with momentum \ud835\udf0c\ud835\udc35 = 0.9999. The\nscaling with \ud835\udf05 = 8 is shown with lighter color for reference from Figure 16. We investigate dynamic batch\nsizes progressively from \ud835\udc35 = 64 \u00d7 290\ud835\udc60 to \ud835\udc35 = 128 \u00d7 290\ud835\udc60, with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35)\nthe EMA Scaling Rule. For reference (top) we show the learning rate schedule with Progressive Scaling. The\nAdam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. Left and middle correspon to\nProgressive Scaling with scale from \ud835\udf05 = 8 to \ud835\udf05 = 16 at 29k steps, while right corresponds to 75k steps.\n14He et al. (2020) alleviated the problem with the proper amount of noise during student model training,\nwhilst Xu et al. (2020) used beam-search decoding with a language model.\n37\nH\nAdditional details and results for self-supervised image representation\nlearning\nOrganization\nThis appendix is structured into three sections. We first give an overview of our\nchosen SSL method BYOL (Appendix H.1), our recipe for training BYOL using ResNet 18s (Ap-\npendix H.2), our recipe for training BYOL using Vision Transformers (ViTs) (Appendix H.3), abla-\ntions of normalization approaches that lead to the development of this recipe (Appendix H.4), and\nadditional results corresponding to longer training duration (Appendix H.5) and further understand-\ning the impact of Progressive Scaling (Appendix H.6).\nSecond, we demonstrate that the EMA Scaling Rule combined with Progressive Scaling can scale\na ResNet-50 BYOL model trained with LARS to batch size 32,768 without performance drop,\ndemonstrating the empirical utility of the tools we provide outside of their theoretical validity (Ap-\npendix H.10).\nFinally, we show that it is possible to systematically scale DINO (Caron et al., 2021) using a com-\nbination of Progressive Scaling and the EMA Scaling Rule, providing a solution for researchers and\npractitioners wanting to train DINO at scale.\nH.1\nComponents of self-supervised learning\nFirst, a key component of many SSL methods is the stop-gradient or StopGrad (Definition H.1).\nDefinition H.1 (Stop Gradient/StopGrad( \u00b7 )). The stop-gradient operator StopGrad( \u00b7 ) prevents the\nflow of gradient information\n\ud835\udc51\ud835\udc53 (StopGrad(\u210e(\ud835\udc65; \ud835\udeda)); \ud835\udec9)\n\ud835\udc51\ud835\udeda\n\u2261 0\n(72)\nfor all parametric functions \u210e and \ud835\udc53 and for all parameters \ud835\udec9 and \ud835\udeda.\nApplying a stop-gradient is sometimes called detaching in the literature. Now, we introduce the\nupdate rule of our representative SSL method BYOL in Definition H.2.\nDefinition H.2 (BYOL Update). BYOL learns unsupervised features by minimizing the cosine dis-\ntance between the predictions of a student backbone \ud835\udc53 ( \u00b7 ; \ud835\udec9) (typically a ResNet or Vision Trans-\nformer), projected through \u210e( \u00b7 ; \ud835\udeda) (typically a Multi-Layer Perceptron (MLP)), and the predictions\nof an EMA teacher \ud835\udc53 ( \u00b7 ; \ud835\udec7) (Grill et al., 2020). The update for the parameters of BYOL is then\n(\ud835\udec9\ud835\udc61+1, \ud835\udeda\ud835\udc61+1) = (\ud835\udec9\ud835\udc61, \ud835\udeda\ud835\udc61) \u2212 \ud835\udf02 \u00d7 1\n\ud835\udc35\n\u2211\ufe01\n\ud835\udc65 \u2208B\n\u2207(\ud835\udec9,\ud835\udeda)L(\ud835\udc65; \ud835\udec9\ud835\udc61, \ud835\udeda\ud835\udc61, \ud835\udec7\ud835\udc61)\n(73)\n\ud835\udec7\ud835\udc61+1 = \ud835\udf0c \ud835\udec7\ud835\udc61 + (1 \u2212 \ud835\udf0c) \ud835\udec9\ud835\udc61+1\n(74)\nwith L(\ud835\udc65; \ud835\udec9\ud835\udc61, \ud835\udeda\ud835\udc61, \ud835\udec7\ud835\udc61) = 1\n2 cos\n\u0002\n\u210e(\ud835\udc53 (\ud835\udc651; \ud835\udec9\ud835\udc61); \ud835\udeda\ud835\udc61), StopGrad(\ud835\udc53 (\ud835\udc652; \ud835\udec7\ud835\udc61))\n\u0003\n+ (\ud835\udc651 \u2194 \ud835\udc652),\n(75)\nwhere cos(a, b) \u2261 1\u2212a\u00b7b/(||a|| ||b||) is the cosine distance, and \ud835\udc651 and \ud835\udc652 are two views of a single\nvariate \ud835\udc65, often produced by augmentations, and \ud835\udc651 \u2194 \ud835\udc652 denotes symmetrization over \ud835\udc651 and \ud835\udc652.\nAs noted in Section 3.4,the BYOL EMA update (Equation 74) uses \ud835\udec9\ud835\udc61+1 instead of our analyzed \ud835\udec9\ud835\udc61\n(Equation 4). The effect upon the overall EMA update is O(\ud835\udf02 \u00d7 \ud835\udefd\ud835\udf0c) and so is captured by the EMA\nScaling Rule (Definition 1.2).\nOne more piece of technology typically employed in SSL is a tracking probe (Definition H.3) which\nwe will use to evaluate the performance of BYOL on downstream tasks of interest, for example,\nimage classification.\nDefinition H.3 (Tracking Probe/Linear Probe). When optimizing model parameters \ud835\udeda\ud835\udc61 of an SSL\nmethod, simultaneously optimize the parameters \ud835\udecf of a probe model \ud835\udc5f ( \u00b7 ; \ud835\udecf) under a downstream\nobjective L(\ud835\udc51). For example, in classification, with data \ud835\udc65 and samples \ud835\udc66\nL(\ud835\udc51) (\ud835\udc65,\ud835\udc66, \ud835\udec9\ud835\udc61, \ud835\udecf\ud835\udc61) = \u2212 log \ud835\udc43(\ud835\udc66|\ud835\udc5f (StopGrad(\u210e(\ud835\udc65; \ud835\udeda\ud835\udc61)); \ud835\udecf))\n(76)\nL(total) (\ud835\udc65,\ud835\udc66; \ud835\udec9\ud835\udc61, \ud835\udeda\ud835\udc61, \ud835\udec7\ud835\udc61, \ud835\udecf\ud835\udc61) = L(\ud835\udc65; \ud835\udec9\ud835\udc61, \ud835\udeda\ud835\udc61, \ud835\udec7\ud835\udc61) + L (\ud835\udc51) (\ud835\udc65,\ud835\udc66, \ud835\udeda\ud835\udc61, \ud835\udecf\ud835\udc61),\n(77)\nThe is a probe for the teacher, which is typically the better choice due to Polyak-Ruppert averaging\neffects (see Section 3.2). When the \ud835\udc5f is a linear model, the tracking probe is called a linear probe.\n38\nTable 9: BYOL ResNet-18 hyperparameters for CIFAR10\nResNet-18\nWeight initialization\nkaiming_uniform (He et al., 2015)\nBackbone normalization\nBatchNorm\nHead normalization\nBatchNorm\nSynchronized BatchNorm over replicas\nYes\nLearning rate schedule\nSingle Cycle Cosine\nLearning rate warmup (epochs)\n20\nLearning rate minimum value\n0\nTraining duration (epochs)\n100\nOptimizer\nSGD\nOptimizer scaling rule\nSGD\nOptimizer momentum\n0.9\nGradient clipping\n0.1\nBase learning rate\n0.02\nBase batch size\n1024\nBase teacher momentum\n0.992\nWeight decay\n1 \u00d7 10\u22126\nWeight decay scaling rule\nNone\nWeight decay skip bias\nYes\nNumerical precision\ntf32\nAugmentation stack\nBYOL CIFAR10\nIt is also typical to use a Batch Normalization layer without trainable affine terms before this linear\nlayer as in He et al. (2022) to stabilize probe training. In this case, the running statistics can be\nabsorbed into a definition of the linear layer weights and biases, and so this is still a linear probe,\nalthough we will call this a pre-bn linear probe to remove ambiguity.\nH.2\nA ResNet-18 recipe for BYOL\nHyperparameters\nWe present the base hyperparameters for training BYOL with a ResNet-18\nbackbone using SGD in Table 9. This recipe was developed by starting from a well-known BYOL\nResNet-50 recipe (Grill et al., 2020), adapting the input augmentations for CIFAR10, and performing\na search over learning rate choices for an SGD optimizer.\nH.3\nA Vision Transformer recipe for BYOL\nHyperparameters\nWe present the base hyperparameters for training BYOL with a ViT-B/16 back-\nbone in Table 10. This recipe was developed by starting from a well-known supervised ViT-B/16\nrecipe (He et al., 2022) and performing a search over weight decay and learning rate hyperparameter\nchoices. We find that BYOL performs well with heavy weight decay (\ud835\udf06 = 0.3) and a low learning\nrate (\ud835\udf02 = 10\u22123) at a base batch size \ud835\udc35 = 4096. The AdamW optimizer is used, and so for scaling to\nother batch sizes \u02c6\ud835\udc35 = \ud835\udf05\ud835\udc35 we use the Adam Scaling Rule (Definition C.3)15 We use a pre-bn linear\nprobe as discussed in Appendix H.1. Finally, the performance of BYOL can be further improved\nby employing multicrop (Caron et al., 2020) by \u2248 +2% in absolute test top-1 performance on Im-\nageNet1k compared to without multicrop, however, as this is not our focus, we omit this from the\npresented recipe.\nAdditional background\nAchieving large scale SSL training with ViTs to large scale SSL train-\ning has been a long standing goal in the community. MoCo-v3 (Chen et al., 2021) enables the\nuse of ViTs with contrastive learning, but achieves this through modifications of the ViT training\nprocedures, including gradient freezing on the image patching layer, and re-introducing Batch Nor-\nmalization to post-attention MLP layers. Despite these modifications, MoCo-v3 was only trained up\nto a batch size of 6144, where model performance begins to suffer (Chen et al., 2021). In Figure 6\nwe demonstrate that combining dynamic batch scaling (Appendix C.4) with the EMA Scaling Rule\n15We note that Adam (Kingma & Ba, 2015) and AdamW (Loshchilov & Hutter, 2019) are equivalent in the\nlimit of zero weight decay, and that the Adam Scaling Rule (Definition C.3) was derived with zero weight decay\n(Malladi et al., 2022).\n39\nTable 10: BYOL ViT-B/16 hyperparameters.\nBYOL ViT-B/16\nImageNet1k Linear Probe Test Top-1\n74.47% (Figure 19)\nWeight initialization\ntrunc_normal(.02)\nBackbone normalization\nLayerNorm\nHead normalization\nBatchNorm\nSynchronized BatchNorm over replicas\nNo\nLearning rate schedule\nSingle Cycle Cosine\nLearning rate warmup (epochs)\n40\nLearning rate minimum value\n1 \u00d7 10\u22126\nTraining duration (epochs)\n480\nOptimizer\nAdamW\nOptimizer scaling rule\nAdam\nBase (\ud835\udefd1, \ud835\udefd2)\n(0.9, 0.95)\nBase learning rate\n1 \u00d7 10\u22123\nBase batch size\n4096\nBase teacher momentum\n0.99\nWeight decay\n0.3\nWeight decay scaling rule\nNone\nWeight decay skip bias\nYes\nNumerical precision\nbf16\nAugmentation stack\nBYOL (Grill et al., 2020)\nStochastic depth\n0.1\n(Definition 1.2) enables BYOL to be trained using ViTs to batch sizes of 24,576 without any drop\nin performance compared to the reference batch size of 4096. We emphasize that the piecewise\ntransitions in the schedules are important for preserving training dynamics.\nH.4\nThe role of Batch Normalization and Layer Normalization in BYOL with ViTs\n0\n200\n400\nTrain Epochs\n0\n10\n20\n30\n40\n50\n60\n70\nTest Top-1 (%)\n0\n200\n400\nTrain Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\nTrain Loss\nBN, B=3072\nBN, B=24576\nLN, B=3072\nLN, B=24576\n0\n200\n400\nTrain Epochs\n0.990\n0.992\n0.994\n0.996\n0.998\n1.000\nTeacher Momentum\n0\n200\n400\nTrain Epochs\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLearning Rate\n\u00d710\u22123\nFigure 19: BYOL ViT-B/16 on ImageNet1k for different scalings \ud835\udf05. We present runs comparing LayerNorm\n(blue) to BatchNorm (red) in the projection and prediction heads of BYOL ViT models for batch size 3072\n(dashed) and 24,576 (solid) without the EMA Scaling Rule. \ud835\udf05 = 1 corresponds to \ud835\udc35 = 4096. In all scenarios\nthe transformer backbone only uses LayerNorm. We truncate the training of the large batch size LayerNorm\nvariant to preserve compute (indicated by \u00d7).\nHere we compare the roles of Batch Normalization (BatchNorm, Ioffe & Szegedy (2015)) and Layer\nNormalization (LayerNorm, Ba et al. (2016)) in the projection and prediction heads of BYOL (Grill\net al., 2020) using ViTs.\nIt has been observed that BatchNorm plays a critical role in BYOL predictor and projector dynam-\nics (Fetterman & Albrecht, 2020), and using either LayerNorm or no normalization significantly\ndecreases model performance. Subsequently, it was demonstrated (Richemond et al., 2020) that\ncompetitive BYOL performance could be achieved through a combination of Group Normaliza-\n40\ntion (GroupNorm, Wu & He (2018)) and Weight Standardization (Qiao et al., 2019). Additionally,\nRichemond et al. (2020) showed that if BatchNorm is used in the backbone, one can use LayerNorm\nor no normalization in the predictor and projector without any performance drop.\nIn this work, we we show it is possible to train BYOL ViT using only LayerNorm across the back-\nbone, projector and predictor (see Figure 19), decoupling BYOL\u2019s reliance on batch statistics, a\ndesirable trait for a representation learning algorithm (Brock et al., 2021). At batch size 3072, using\nLayerNorm in the predictor and projector achieves competitive performance (74.10%), performing\nslightly worse than using BatchNorm (74.47%). At the larger batch size of 24,576, runs perform\nsignificantly worse as the EMA Scaling Rule was not applied.\nH.5\nLonger training duration with incremental Progressive Scaling\n0\n100 200 300 400 500\nTrain Epochs\n0\n10\n20\n30\n40\n50\n60\n70\nTest Top-1 (%)\nB = 3072\nB = 6144\nB = 9216\nB = 12288\nB = 15360\nB = 18432\nB = 21504\nB = 24576\n0\n100 200 300 400 500\nTrain Epochs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTrain Loss\n0\n100 200 300 400 500\nTrain Epochs\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00\nTeacher Momentum\n0\n100 200 300 400 500\nTrain Epochs\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLearning Rate\n\u00d710\u22123\nFigure 20: BYOL ViT-B/16 on ImageNet1k for different scalings \ud835\udf05. The baseline model (\ud835\udf05 = 0.75, black dashed)\nuses batch size 3072 and teacher momentum \ud835\udf0c\ud835\udc35 = 0.99. We increment the batch size by 3072 every 60 epochs\nto a final batch size of 24,576 using Progressive Scaling (Definition 3.2).\nHere we use the same base hyperparameters as Table 10, except that we train for 480 instead of\n300 epochs. To mitigate the student impulse phenomena discussed in Section 3.4, in Figure 20 we\ninvestigate increasing the batch size every 60 epochs using Progressive Scaling (Definition 3.2). We\nobserve that this more gradual procedure enables closer tracking of the baseline train loss trajec-\ntory. Additionally, this procedure results in a scaled linear probe performance that outperforms the\nbaseline (75.64% compared to the baseline performance of 74.47%). The same procedure can be ap-\nplied to the LayerNorm variant discussed in Appendix H.4, which produces a similar result (75.09%\ncompared to the baseline performance of 74.10%).\nH.6\nBuilding intuition around Progressive Scaling and momentum sensitivity\nOur final BYOL ViT results are to help build intuition around Progressive Scaling (Definition 3.2),\nas well as when the EMA Scaling Rule is most important. In Figure 21 we explore transition-\ning from the baseline batch size 4096 model to batch size 24,576 in a single transition after\n60 epochs. After this transition, we continue training for 240 epochs for a range of momenta:\n\ud835\udf0c \u2208 {0.8, 0.9, 0.95, 0.97, 0.9867, 0.994, 0.999} without the EMA Scaling Rule.\nWe observe that after the transition, any 0.9 \u2264 \ud835\udf0c \u2264 0.994 produces a linear probe performance that\nmatches or outperforms the baseline at the end of training. This indicates that after the initial training\nperiod, BYOL becomes less sensitive to the choice of teacher momentum. Note that without the\ninitial 60 epochs of training with batch size 4096, all models, including those employing the EMA\nScaling Rule diverge (see \ud835\udc35 = 24, 576 in Figure 6).\nWe present an illustration for why this might happen in Figure 22. First, we see that using the EMA\nScaling Rule always keeps the model within the acceptable momentum region. We also wee that\nnot using the EMA Scaling Rule can keep the model within the acceptable momentum region for a\nrange of batch sizes, depending on how large wide in momenta the acceptable region is at the base\nbatch size. Finally, we see that the momentum value matters much more at low values of momenta\n41\n0\n100\n200\n300\nTrain Epochs\n0\n10\n20\n30\n40\n50\n60\n70\nTest Top-1 (%)\nBaseline\n\u03c1 = 0.800\n\u03c1 = 0.900\n\u03c1 = 0.950\n\u03c1 = 0.970\n\u03c1 = 0.987\n\u03c1 = 0.994\n\u03c1 = 0.999\n0\n100\n200\n300\nTrain Epochs\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTrain Loss\n0\n100\n200\n300\nTrain Epochs\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nTeacher Momentum\n0\n100\n200\n300\nTrain Epochs\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLearning Rate\n\u00d710\u22123\nFigure 21: BYOL ViT-B/16 on ImageNet1k for different momenta \ud835\udf0c. The baseline model (\ud835\udf0c = 0.99, black\ndashed) uses batch size 4096. At the 60th epoch we apply Progressive Scaling (Definition 3.2) and transition\nto batch size 24576. We train for a further 240 epochs without EMA scaling for a range of momenta: \ud835\udf0c \u2208\n{0.9, 0.95, 0.97, 0.9867, 0.994}.\n(the acceptable momentum region shrinks), whereas at large momenta, this region of acceptability\nwidens.\n128\n256\n512\n1024\n2048\n4096\nBatch Size\n0.80\n0.85\n0.90\n0.95\n1.00\nMomentum\n\u03c1min\n\u03c1max\n\u03c1 = \u03c1B\n\u03c1 = \u03c1\u03ba\nB\nFigure 22: A hypothetical scenario where there is an upper and lower limit for momenta qualitatively leading\nto the same result. We assume at base batch size \ud835\udc35 = 1024 there is an upper (\ud835\udf0cmax, black dashdot) and lower\n(\ud835\udf0cmin, black dashed) limit for valid momenta. We show what happens if we start with \ud835\udf0c\ud835\udc35 = 0.95 at a batch size\nof 4096, and scale with (\ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35, blue) and without (\ud835\udf0c = \ud835\udf0c\ud835\udc35, red) the EMA Scaling Rule.\nH.7\nCompute usage for ViT BYOL investigation\nWe now summarize the compute usage for the BYOL ViT experiments. First we detail cost of\nreproducing Figure 6 in Table 11, Figure 19 in Table 12, and Figure 20 in Table 13.\nTable 11: Compute usage for baseline ViT BYOL investigation in Figure 6. Values include node allocation\ntimes (typically a small % of corresponding total runtime), giving a practical estimate of reproduction cost. All\nexperiments conducted are using 80Gb A100s, and experiments indicated by (\u2020) have a faster interconnect.\nBatch Size\nGPUs\nTime (h)\nCompute/Run (GPUh)\nRuns\nCompute (GPUh)\n4,096\n32\n16.6\n531.4\n2\n1,062.7\n8,192\n48\n14.1\n678.0\n2\n1,356.1\n16,384\n96\n8.3\n800.3\n2\n1,600.6\n24,576\n128\n6.6\n850.1\n4\n3,400.4\n32,768\u2020\n176\n4.1\n721.7\n2\n1,443.4\nTotal\n8,863.1\n42\nTable 12: Compute usage for ViT BYOL investigation into BatchNorm and LayerNorm variants in Figure 19.\nValues include node allocation times (typically a small % of corresponding total runtime), giving a practical\nestimate of reproduction cost. All experiments conducted are using 80Gb A100s, and were run for 480 epochs,\nexcept those indicated by (\u2217) were truncated early (see Figure 19 for more details).\nBatch Size\nNormalization\nGPUs\nTime (h)\nCompute (GPUh)\n3,072\nBatchNorm\n16\n47.9\n766.0\n3,072\nLayerNorm\n16\n48.0\n768.7\n24,576\nBatchNorm\n128\n14.8\n1900.4\n24,576\u2217\nLayerNorm\n128\n3.5\n451.1\nTotal\n3,886.2\nTable 13: Compute usage for ViT BYOL investigation into incremental scaling in Figure 20. Values include\nnode allocation times (typically a small % of corresponding total runtime), giving a practical estimate of re-\nproduction cost. All experiments conducted are using 80Gb A100s for 60 epochs. Stage 0 corresponding to\nthe baseline in Figure 20 is the run detailed in the first row of Table 12, using a batch size of 3,072, Batch\nNormalization, and 16 GPUs. Computing only the first 60 epochs of stage 0 corresponds to approximately\n127.7 GPUh, which would bring the total cost of Figure 20 to 1,432.9 GPUh.\nStage\nBatch Size\nGPUs\nTime (h)\nCompute (GPUh)\n1\n6,144\n32\n3.5\n113.0\n2\n9,216\n48\n3.1\n149.8\n3\n12,288\n64\n2.8\n176.0\n4\n15,360\n80\n2.3\n186.5\n5\n18,432\n96\n2.1\n202.9\n6\n21,504\n112\n2.1\n235.8\n7\n24,576\n128\n1.9\n241.3\nTotal\n1,305.2\nNext, the cost of a single momentum ablation presented in Figure 21 is 240 epochs at batch size\n24,576, which is \u2248 240/480 \u00d7 1900.4 GPUh = 950.2 GPUh, giving a total cost over seven runs of\n6651.4 GPUh.\nFinally, providing a full view of the investigations carried out for the ViT BYOL is given in Table 14.\nTable 14: Total compute usage for ViT BYOL investigations.\nCompute (GPUh)\nBaselines (Figure 6 and Table 11)\n8,863.1\nBatchNorm and LayerNorm (Figure 19 and Table 12)\n3,886.2\nIncremental scaling (Figure 20 and Table 13)\n1,305.2\nMomentum ablations (Figure 21)\n6,651.4\nAll other compute, e.g. code development, runs with errors, and debugging\n84,984.1\nTotal\n105,690.0\nH.8\nResNet-18 hyperparameter sensitivity analysis\nTo demonstrate that the EMA Scaling Rule works for a broad range of optimization hyperparameters\n(i.e. beyond those presented in Figure 5 and Section 3.4), we provide a sensitivity analysis for base\nteacher momentum \ud835\udf0c\ud835\udc35 and base learning rate \ud835\udf02\ud835\udc35 in the challenging setting of BYOL.\nBase teacher momentum\nIn Figure 23 we show the effect of changing the base teacher momen-\ntum \ud835\udf0c\ud835\udc35, defined at batch size 1024. The EMA Scaling Rule is robust to modifications of momentum\ndown to \ud835\udf0c\ud835\udc35 \u2248 0.946 in this particular setting. Below \ud835\udf0c\ud835\udc35 \u2248 0.946, matching is poor, although the\nsmallest momentum in this setting corresponds to 0.8414 \u2248 0.5, which is a particularly small teacher\nmomentum, and is unlike to provide utility over the using the target model (see Appendix E.2).\n43\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(a) \ud835\udf0c\ud835\udc35 = 0.841\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(b) \ud835\udf0c\ud835\udc35 = 0.946\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(c) \ud835\udf0c\ud835\udc35 = 0.974\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(d) \ud835\udf0c\ud835\udc35 = 0.987\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(e) \ud835\udf0c\ud835\udc35 = 0.992\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(f) \ud835\udf0c\ud835\udc35 = 0.997\nFigure 23: ResNet-18 BYOL on CIFAR10 teacher momentum sensitivity (\ud835\udf02\ud835\udc35 = 0.08) for scalings \ud835\udf05 \u2208 {2, 4} and\nbase teacher momenta \ud835\udf0c\ud835\udc35 \u2208 {0.841, 0.946, 0.974, 0.987, 0.992, 0.997} defined at \ud835\udf05 = 1. The baseline (\ud835\udf05 = 1, black\ndashed) uses batch size 1024, and is scaled from batch size 2048 (left) to 4096 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and\nwithout (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling Rule. Bands indicate mean and standard deviation across three runs.\nBase learning rate\nIn Figure 24 we show the effect of changing the base learning rate \ud835\udf02\ud835\udc35, defined\nat batch size 1024. The EMA Scaling Rule is robust over a wide range of learning rates. At the\nlargest learning rate \ud835\udf02\ud835\udc35 = 0.5 matching starts to become poor at scaling \ud835\udf05 = 4.\n44\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(a) \ud835\udf02\ud835\udc35 = 0.01\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(b) \ud835\udf02\ud835\udc35 = 0.02\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(c) \ud835\udf02\ud835\udc35 = 0.04\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(d) \ud835\udf02\ud835\udc35 = 0.15\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(e) \ud835\udf02\ud835\udc35 = 0.2\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 2\nB = 2048, \u03c1 = \u03c1\u03ba\nB\nB = 2048, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 4\nB = 4096, \u03c1 = \u03c1\u03ba\nB\nB = 4096, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n(f) \ud835\udf02\ud835\udc35 = 0.5\nFigure 24: ResNet-18 BYOL on CIFAR10 learning rate sensitivity (\ud835\udf0c\ud835\udc35 = 0.992) for scalings \ud835\udf05 \u2208 {2, 4} and base\nlearning rates \ud835\udf02\ud835\udc35 \u2208 {0.01, 0.02, 0.04, 0.15, 0.20, 0.50} defined at \ud835\udf05 = 1. The baseline (\ud835\udf05 = 1, black dashed) uses\nbatch size 1024, and is scaled from batch size 2048 (left) to 4096 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red,\n\ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling Rule. Bands indicate mean and standard deviation across three runs.\nH.9\nResNet-18 additional scaling analysis\nTo demonstrate that the EMA Scaling Rule works for a broad range of scalings \ud835\udf05 (i.e. beyond those\npresented in Figure 5 and Section 3.4), we investigate scaling down to \ud835\udf05 = 1/8 in Figure 25. We\nsee that the EMA Scaling Rule works well down to the small batch size of 128, although matching\nis not perfect. We suspect this is due to the presence of Batch Normalization layers in the ResNet-\n18 architecture, which underperform at small batch sizes (Ioffe & Szegedy, 2015). The synthetic\nanalysis of Section 3.1 instead demonstrated the EMA Scaling Rule holding for scalings spanning\nfactors of \ud835\udf05 that differ by 1024, with scaling error insensitive to the value of \ud835\udf05 for sufficiently low \ud835\udf05\n(see Figure 1b).\n45\n0\n25\n50\n75\nTest Top-1 (%)\nScaling \u03ba = 1/8\nB = 128, \u03c1 = \u03c1\u03ba\nB\nB = 128, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 1/4\nB = 256, \u03c1 = \u03c1\u03ba\nB\nB = 256, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\nScaling \u03ba = 1/2\nB = 512, \u03c1 = \u03c1\u03ba\nB\nB = 512, \u03c1 = \u03c1B\nB = 1024, \u03c1 = \u03c1B\n0\n25\n50\n75\n100\nTrain Epochs\n0.2\n0.4\n0.6\nTrain Loss\n0\n25\n50\n75\n100\nTrain Epochs\n0\n25\n50\n75\n100\nTrain Epochs\nFigure 25: ResNet-18 BYOL on CIFAR10 lower scaling analysis (\ud835\udf02\ud835\udc35 = 0.08, \ud835\udf0c\ud835\udc35 = 0.992) for scalings \ud835\udf05 \u2208\n{1/8, 1/4, 1/2}. The baseline (\ud835\udf05 = 1, black dashed) uses batch size 1024, and is scaled from batch size 128\n(left) to 512 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling Rule. Bands indicate mean\nand standard deviation across three runs.\nH.10\nScaling a ResNet-50 BYOL using LARS and Progressive Scaling\nHere we investigate whether Progressive Scaling and the EMA Scaling Rule can be used in practice\nwhere there is no known optimizer SDE approximation. We use the default 300 epoch configuration\nfor BYOL (Grill et al., 2020) in Figure 26. We see that although trajectories during training do not\nmatch, we are able to match or surpass the linear probe performance of the BYOL baseline at the\nlarger batch size if 32,768. This indicates that the contributions of our work have practical utility\nbeyond the theoretical constraints.\nThe compute usage for the BYOL ResNet using LARS is detailed in Table 15.\nTable 15: Compute usage for ResNet 50 LARS investigation in Figure 26. Values include node allocation\ntimes (typically a small % of corresponding total runtime), giving a practical estimate of reproduction cost. All\nexperiments conducted are using 80Gb A100s.\nBatch Size\nGPUs\nTime (h)\nCompute (GPUh)\n4, 096 \u2192 32, 768 (120 Epochs)\n128\n14.1\n1809.8\n4, 096 \u2192 32, 768 (60 Epochs)\n128\n12.9\n1655.9\n4, 096\n16\n32.8\n524.9\nAll other compute, e.g. code development, runs with errors, and debugging\n17,654.6\nTotal\n21645.2\n46\n0\n20\n40\n60\nTest Top-1 (%)\nProgressive Scaling for 60 Epochs\nB = 4096 \u2192 32,768 (60 Epochs)\nB = 4096\nProgressive Scaling for 120 Epochs\nB = 4096 \u2192 32,768 (120 Epochs)\nB = 4096\n250\n300\n350\nParam Norm\n0.2\n0.4\n0.6\nTeacher STD\n0.2\n0.4\n0.6\nTrain Loss\n0.96\n0.98\n1.00\nTeacher Momentum\n0\n10\n20\nLearning Rate\n0\n50\n100\n150\n200\n250\n300\nTrain Epochs\n4096\n8192\n16384\n32768\nBatch Size\n0\n50\n100\n150\n200\n250\n300\nTrain Epochs\nFigure 26: ResNet50 BYOL on ImageNet1k using LARS for different configurations of progressive scaling. The\nbaseline (black dashed) uses batch size 4096 and momentum \ud835\udf0c\ud835\udc35 = 0.99. We consider progressive scaling (blue)\nsmoothly from epoch 60 for 60 epochs (left) and 120 epochs (right) up until batch size 32,768, scaling the\nlearning rate linearly, and applying the EMA Scaling Rule.\nH.11\nPreventing collapse phenomena in DINO at scale\nUntil now, our representatives SSL method has been BYOL for reasons discussed in Section 3.4.\nHere, we will turn our attention to DIstillation with NO labels (DINO) (Caron et al., 2021), which\nhas the update rule presented in Definition H.4.\nDefinition H.4 (DINO Update). DINO learns unsupervised features by matching predictions over\nemergent pseudo-labels of a student backbone and head \ud835\udc53 ( \u00b7 ; \ud835\udec9) to those of an EMA teacher \ud835\udc53 ( \u00b7 ; \ud835\udec7)\nthrough a cross-entropy guided distillation procedure. DINO has a additional centering procedure,\nwhich is a form of batch normalization with momentum \ud835\udf0c\ud835\udc50 = 0.9 which we do not scale using the\n47\nTable 16: DINO ViT-B/16 Training hyperparameters.\nDINO ViT-B/16\nCIFAR10 Linear Probe Top-1 (\ud835\udf0c\ud835\udc35 = 0.996)\n85.38%\nCIFAR10 Linear Probe Top-1 (\ud835\udf0c\ud835\udc35 = 0.992)\n86.96%\nWeight initialization\ntrunc_normal(.02)\nNormalization\nLayer Norm\nLearning rate schedule\nSingle Cycle Cosine\nLearning rate warmup (epochs)\n50\nLearning rate minimum value\n1 \u00d7 10\u22126\nTraining duration (epochs)\n280\nOptimizer\nAdamW\nOptimizer scaling rule\nAdam\nBase (\ud835\udefd1, \ud835\udefd2)\n(0.9, 0.95)\nBase learning rate\n3 \u00d7 10\u22124\nBase batch size (\ud835\udc35)\n1024\nBase teacher momentum (\ud835\udf0c\ud835\udc35)\n0.992 or 0.996\nBase weight decay\n0.04\nWeight decay scaling rule\nLinear\nWeight decay skip bias\nYes\nCenter Momentum\n0.9\nCenter Momentum Scaling Rule\nNone\nPrecision\nbf16\nAugmentation stack\nDINO multi-crop (Caron et al., 2020)\nEMA Scaling Rule. The update for the parameters of DINO is\n\ud835\udec9\ud835\udc61+1 = \ud835\udec9\ud835\udc61 \u2212 \ud835\udf02 \u00d7 1\n\ud835\udc35\n\u2211\ufe01\n\ud835\udc65 \u2208B\n\u2207\ud835\udec9L(\ud835\udc65; \ud835\udec9\ud835\udc61, \ud835\udec7\ud835\udc61, c\ud835\udc61)\n(78)\n\ud835\udec7\ud835\udc61+1 = \ud835\udf0c \ud835\udec7\ud835\udc61 + (1 \u2212 \ud835\udf0c) \ud835\udec9\ud835\udc61+1\n(79)\nc\ud835\udc61+1 = \ud835\udf0c\ud835\udc50 c\ud835\udc61 + (1 \u2212 \ud835\udf0c\ud835\udc50) E\ud835\udc65\u2032\ud835\udec7(\ud835\udc65\u2032)\n(80)\nwith L(\ud835\udc65; \ud835\udec9\ud835\udc61, \ud835\udec7\ud835\udc61, c\ud835\udc61) = \ud835\udc3b \u0000\ud835\udc53 (\ud835\udc651, \ud835\udec9\ud835\udc61), \ud835\udc53 (\ud835\udc652, \ud835\udec7\ud835\udc61) \u2212 c\ud835\udc61\n\u0001 + (\ud835\udc651 \u2194 \ud835\udc652),\n(81)\nwhere \ud835\udc3b (a, b) \u2261 \u2212 \u00cd\ud835\udc40\n\ud835\udc5a=1 \ud835\udc5d\ud835\udc5a(a) log\ud835\udc5d\ud835\udc5a(b) is the cross-entropy between categorical distributions\nover \ud835\udc40 (emergent pseudo-)classes given logits a, b \u2208 R\ud835\udc40, \ud835\udc651 and \ud835\udc652 are two views of a single\nvariate \ud835\udc65, often produced by augmentations, and \ud835\udc651 \u2194 \ud835\udc652 denotes symmetrization over \ud835\udc651 and \ud835\udc652.\nIn practice, DINO employs multi-crop (Caron et al., 2021). We omit this detail for clarity of presen-\ntation, although we do use multi-crop in the experiments that follow.\nOur interest DINO is due to the difficulty in its optimization16, and in particular, preventing collapse\nphenomena in DINO at batch sizes above 1024, which is an open research problem. In this section,\nwe will show that a combination of the EMA Scaling Rule (Definition 1.2) and Progressive Scaling\n(Definition 3.2) enable training of DINO beyond batch size 1024 without sacrificing performance.\nHyperparameters\nBase hyperparameters are presented in Table 16.\nResults\nIn Figures 27 and 28 we show the results obtained training DINO on CIFAR-10 with\n\ud835\udf0c\ud835\udc35 = 0.996 and \ud835\udf0c\ud835\udc35 = 0.992 respectively at the reference batch size of 1024. We employ smooth\nProgressive Scaling (Definition 3.2) between epochs 120 and 180.\nAt batch size 2048, the training loss matches the reference only when the EMA Scaling Rule is\napplied, whereas the run without the scaling rule diverges from the reference. The impact of this\ndivergence is emphasized as we consider the larger batch size of 4096. Here. there is also a gap with\nthe EMA Scaling Rule, however is approximately three times smaller than the gap without the EMA\nScaling Rule.\n16For an example, see https://github.com/facebookresearch/dino/issues/43#\nissuecomment-881453515.\n48\nAdditionally, we observe that using \ud835\udf0c\ud835\udc35 = 0.992 yields higher Top-1 accuracy over \ud835\udf0c\ud835\udc35 = 0.996, and\nin our experiments, using the EMA Scaling Rule always performs better in terms of linear probe\nperformance than not using the scaling rule.\n0\n25\n50\n75\nTest Top-1 (%)\nProgressive Scaling \u03ba = 2\nB = 1024,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1\u03ba\nB\nProgressive Scaling \u03ba = 3\nB = 1024,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1\u03ba\nB\nProgressive Scaling \u03ba = 4\nB = 1024,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1\u03ba\nB\n0\n100\n200\nTrain Epochs\n7\n8\n9\n10\n11\nTrain Loss\n0\n100\n200\nTrain Epochs\n0\n100\n200\nTrain Epochs\nFigure 27: DINO ViT-B/16 on CIFAR-10 for different scalings \ud835\udf05 and base teacher momentum \ud835\udf0c\ud835\udc35 = 0.996. The\nbaseline model (\ud835\udf05 = 1, black dashed) uses batch size 1024 and center momentum \ud835\udf0c\ud835\udc50 = 0.9, and is scaled up\nfrom batch size 2048 (left) to 4096 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling\nRule. Between epochs 100 and 180 we scale the batch size using progressive scaling (Definition 3.2).\n0\n25\n50\n75\nTest Top-1 (%)\nProgressive Scaling \u03ba = 2\nB = 1024,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1\u03ba\nB\nProgressive Scaling \u03ba = 3\nB = 1024,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1\u03ba\nB\nProgressive Scaling \u03ba = 4\nB = 1024,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1\u03ba\nB\n0\n100\n200\nTrain Epochs\n6\n8\n10\nTrain Loss\n0\n100\n200\nTrain Epochs\n0\n100\n200\nTrain Epochs\nFigure 28: DINO ViT-B/16 on CIFAR-10 for different scalings \ud835\udf05 and base teacher momentum \ud835\udf0c\ud835\udc35 = 0.992. The\nbaseline model (\ud835\udf05 = 1, black dashed) uses batch size 1024 and center momentum \ud835\udf0c\ud835\udc50 = 0.9, and is scaled up\nfrom batch size 2048 (left) to 4096 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling\nRule. Between epochs 100 and 180 we scale the batch size using progressive scaling (Definition 3.2).\n49\nIn Figure 29 we show how the hyperparameters \ud835\udf0c, \ud835\udc35 and learning rate change with the progressive\nscaling in Definition 3.2.\n0\n200\nTrain Epochs\n0.980\n0.985\n0.990\n0.995\n1.000\nTeacher Momentum (\u03c1)\n0\n200\nTrain Epochs\n1000\n2000\n3000\n4000\nBatch Size (B)\n0\n200\nTrain Epochs\n0\n1\n2\n3\n4\n5\nLearning Rate\n\u00d710\u22124\nB = 1024,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1\u03ba\nB\nB = 3072,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1\u03ba\nB\nB = 4096,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1\u03ba\nB\nFigure 29: DINO ViT-B/16 on CIFAR-10 for different scalings \ud835\udf05 and base teacher momentum \ud835\udf0c\ud835\udc35 = 0.992. We\nshow how the hyperparameters \ud835\udf0c, \ud835\udc35 and learning rate change with the Progressive Scaling in Definition 3.2.\nThese hyperparameters correspond to the training runs in Figure 28. Those for Figure 27 are identical, with the\nexception of \ud835\udf0c that starts at 0.996 instead of 0.992.\nWe also attempted to use a sharp batch size transition (Figures 30 and 31), which leads to the\ncollapse pheonomena observed in prior work. This collapse happens with and without the EMA\nScaling Rule. We suspect this is due to dynamics specific to DINO\u2019s early phase that are even more\nchallenging to replicate under discretization than those of BYOL.\n0\n25\n50\n75\nTest Top-1 (%)\nProgressive Scaling \u03ba = 2\nB = 1024,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1\u03ba\nB\nProgressive Scaling \u03ba = 3\nB = 1024,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1\u03ba\nB\nProgressive Scaling \u03ba = 4\nB = 1024,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1\u03ba\nB\n0\n100\n200\nTrain Epochs\n6\n8\n10\nTrain Loss\n0\n100\n200\nTrain Epochs\n0\n100\n200\nTrain Epochs\nFigure 30: DINO ViT-B/16 on CIFAR-10 for different scalings \ud835\udf05 and base teacher momentum \ud835\udf0c\ud835\udc35 = 0.992. The\nbaseline model (\ud835\udf05 = 1, black dashed) uses batch size 1024 and center momentum \ud835\udf0c\ud835\udc50 = 0.9, and is scaled up\nfrom batch size 2048 (left) to 4096 (right) with (blue, \ud835\udf0c = \ud835\udf0c\ud835\udf05\n\ud835\udc35) and without (red, \ud835\udf0c = \ud835\udf0c\ud835\udc35) the EMA Scaling\nRule. Progressive Scaling is employed with a sharp transition at epoch 100, leading to a collapse phenomenon.\n50\n0\n200\nTrain Epochs\n0.990\n0.992\n0.994\n0.996\n0.998\n1.000\nTeacher Momentum (\u03c1)\n0\n200\nTrain Epochs\n1000\n2000\n3000\n4000\nBatch Size (B)\n0\n200\nTrain Epochs\n0\n1\n2\n3\nLearning Rate\n\u00d710\u22124\nB = 1024,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1B\nB = 2048,\u03c1 = \u03c1\u03ba\nB\nB = 3072,\u03c1 = \u03c1B\nB = 3072,\u03c1 = \u03c1\u03ba\nB\nB = 4096,\u03c1 = \u03c1B\nB = 4096,\u03c1 = \u03c1\u03ba\nB\nFigure 31: DINO ViT-B/16 on CIFAR-10 with \ud835\udf0c\ud835\udc35 = 0.992 and a sharp transition in batch size at epoch 100.\nWe show how the hyperparameters \ud835\udf0c, \ud835\udc35 and learning rate change with sudden scaling. These hyperparameters\ncorrespond to the training runs in Figure 30.\nCompute\nThe compute usage for the DINO investigations is detailed in Table 17.\nTable 17: Compute usage for DINO investigations. Values include node allocation times (typically a small %\nof corresponding total runtime), giving a practical estimate of reproduction cost. All experiments conducted\nare using 80Gb A100s.\nBatch Size\nGPUs\nTime (h)\nCompute/Run (GPUh)\nRuns\nCompute (GPUh)\n1, 024\n24\n6.8\n163.5\n2\n327.0\n1, 024 \u2192 2, 048\n40\n4.6\n182.4\n1\n182.4\n1, 024 \u2192 3, 072\n48\n4.0\n189.9\n1\n189.9\n1, 024 \u2192 4, 096\n64\n3.3\n212.3\n1\n212.3\n1, 024 \u2192 2, 048 (100 Epochs)\n40\n4.8\n190.6\n4\n762.3\n1, 024 \u2192 3, 072 (100 Epochs)\n48\n4.0\n192.5\n4\n769.9\n1, 024 \u2192 4, 096 (100 Epochs)\n64\n3.6\n232.1\n4\n928.2\nAll other compute, e.g. code development, runs with errors, and debugging\n38239.2\nTotal\n41,611.1\nOur results in this section show it is possible to scale DINO to large batch sizes without sacrificing\nperformance by using both the EMA Scaling Rule and Progressive Scaling, providing the batch size\nschedule of Progressive Scaling is not sudden.\nI\nAdditional details on numerical stability\nA general analysis of overflow and underflow of the EMA Update (Definition 1.1) or EMA Scaling\nRule (Definition 1.2) for different momenta \ud835\udf0c, particularly for IEE-754 floating point values, is\nbeyond the scope of this work due to non-linearity from mechanisms like gradual underflow (IEEE,\n2019).\nIn our setting, do not suffer from practical overflow or underflow issues through exponentiation when\napplying the EMA Scaling Rule, as FP32 precision allows a maximum \ud835\udf0c = 1\u2212\ud835\udf00, or minimum \ud835\udf0c = \ud835\udf00\nwith \ud835\udf00 \u2248 1.2 \u00d7 10\u22127. Take self-supervised image representation learning (Section 3.4) as a baseline,\nwith \ud835\udf05 = 1 corresponding to batch size \ud835\udc35 = 4096 with momentum \ud835\udf0c\ud835\udc35 = 0.996. The maximum value\nof \ud835\udf0c corresponds to scaling \ud835\udf05 = log(\ud835\udf0c\ud835\udc35)/log(\ud835\udf00) \u2248 1/(32\ud835\udc3e), give a batch size less than one, while the\nminimum value of \ud835\udf0c corresponds to scaling \ud835\udf05 = log(\ud835\udf0c\ud835\udc35)/log(1\u2212\ud835\udf00) \u2248 4\ud835\udc3e, giving a batch size \ud835\udc35 \u2248 8\ud835\udc40\nwhich is beyond current hardware feasibility, and beyond the breakdown of known optimizer scaling\nrules (Li et al., 2021).\nTo examine how momentum may induce numerical errors in practice during training, we train a\nlinear regression model with a Polyak-Ruppert average Definition 3.1, and and track the difference\nbetween FP32 model weights and weights in i) BF16; ii) FP16; and iii) a second FP32 run,\nwhich act as a proxy for overflow and underflow. In Figure 32 we plot these differences using\n51\n0.99\n0.999\n0.9999\n0.99999\nMomentum \u03c1\n0\n2\n4\n6\nmaxi\n\f\f\f\u03b8FP32\ni\n\u2212\u03b8dtype\ni\n\f\f\f\n\u00d710\u22122 Scaling \u03ba = 1\n0.99\n0.999\n0.9999\n0.99999\nMomentum \u03c1\nScaling \u03ba = 8\nTarget at bf16\nEMA at bf16\nTarget at fp16\nEMA at fp16\nTarget at fp32\nEMA at fp32\nFigure 32: Numerical precision of target and EMA networks compared to an FP32 reference on a regression\ntask for a range of momenta.\nthe maximum absolute difference between model parameters, where the maximum is taken over\nindividual weights\nMaxAbsDiff(dtype) =\n\ud835\udc43\nmax\n\ud835\udc56=1\n\f\f\f\ud835\udf03FP32\n\ud835\udc56\n\u2212 \ud835\udf03dtype\n\ud835\udc56\n\f\f\f,\n(82)\nwhere \ud835\udc43 is the number of parameters in the model. We observe that when model weights and EMA\nweights are FP16 (never done in practice), an increasing variance happens for FP16 as the value of\nthe momentum \ud835\udf0c approaches 0.99999, whereas BF16 and FP32 are stable. We stress that all exper-\niments presented in the paper store weights for target model and EMA in FP32 and use automatic-\nmixed precision to cast them to BF16 during training, and so do not encounter momentum-induced\noverflow or underflow.\nJ\nContributions\nAll authors contributed to writing this paper, designing the experiments, discussing results at each\nstage of the project.\nPreliminary work\nDerivation of the EMA Scaling Rule with learning rate \ud835\udf02 = 0, initial synthetic\nand self-supervised ImageNet1k experiments done by Dan Busbridge.\nEMA scaling rules for constant gradients\nOriginal proof of Equation 4 and the form of \ud835\udeff(\ud835\udf02, \ud835\udf0c,\ud835\udf05)\nin Equation 54 done by Eeshan Gunesh Dhekane. Final proof presented in Appendix E.1 done by\nDan Busbridge, verified by Eeshan Gunesh Dhekane and Pierre Ablin.\nEMA approximation theorems with SDEs\nProofs of validity of EMA Scaling Rule in the SDE\nlimit presented in Section 2.2 and Appendix D done by Pierre Ablin.\nPolyak-Ruppert averaging in a simple setting\nDesign of noisy parabola setting of Section 3.1\nand initial experiments done by Russ Webb. Design of \ud835\udf0c\u2217-optimality search (Equation 10), final\nexperiments and analysis of Section 3.1 and Appendix F.1 done by Dan Busbridge.\nPolyak-Ruppert averaging on image classification\nResNetv2-50 reproduction (Table 4) and\nbaseline momentum identification done by Jason Ramapuram. Final ImageNet1k experiments and\nanalysis of Section 3.2 and Appendices F.2 and F.3 done by Dan Busbridge.\nAutomatic speech recognition\nExperiments and analysis of automatic speech recognition us-\ning Polyak-Ruppert averaging (Section 3.2) and continuous pseudo-labeling (Section 3.3 and Ap-\npendix G), as well as design choice of a seed model to start pseudo-labeling (aligning quality of\nthe seed models for different batch size settings before pseudo-labeling process) done by Tatiana\nLikhomanenko.\nSelf-supervised image representation learning\nBYOL ResNet-18 recipe (Table 9) and experi-\nments on CIFAR10 using SGD (Figure 5), and BYOL ResNet-50 experiments using LARS (Ap-\npendix H.10) done by Dan Busbridge. BYOL ResNet 50 baseline implementation and BYOL ViT\n52\nrecipe (Table 10) done by Jason Ramapuram. BYOL ViT exploratory ablations done by Eeshan\nGunesh Dhekane and Jason Ramapuram. All final BYOL ViT experiments and analysis (Figure 6\nand Appendices H.4 to H.6) done by Jason Ramapuram. Baseline DINO reproduction done by Dan\nBusbridge. DINO experiments and analysis (Appendix H.11) done by Xavier Suau Cuadros.\nProgressive Scaling\nProgressive Scaling (Definition 3.2 and Algorithm 1) is proposed by Dan\nBusbridge based on discussions with Xavier Suau Cuadros, Tatiana Likhomanenko, Jason Ramapu-\nram, Russ Webb, and the authors of Malladi et al. (2022). Adaptation of progressive scaling to semi-\nsupervised learning in automatic speech recognition (Appendix G.2) done by Tatiana Likhoma-\nnenko, and to self-supervised learning in vision done by Dan Busbridge and Jason Ramapuram for\nBYOL (Figures 5 and 6 and Appendices H.4, H.5 and H.10) and Xavier Suau Cuadros for DINO\n(Appendix H.11).\nLimiting behavior of Polyak-Ruppert averaging\nOriginal proof of limiting behavior of Polyak-\nRuppert averaging done by Eeshan Gunesh Dhekane. Final proof presented in Appendix E.2 done\nby Dan Busbridge, verified by Eeshan Gunesh Dhekane.\nNumerical stability analysis\nPolyak-Ruppert experiment (Figure 32) using linear regression for\nvarious floating point precisions done by Jason Ramapuram.\nImplementation details\nInvestigations carried out in two distributed, scalable frameworks: Jax\nfor automatic speech recognition experiments, done by Tatiana Likhomanenko; and PyTorch for all\nremaining investigations, done by Dan Busbridge, Xavier Suau Cuadros, Eeshan Gunesh Dhekane,\nJason Ramapuram and Russ Webb. Initial implementation of progressive scaling experiments for\nincremental-style strategies (e.g. Appendix H.5) showing feasibility done by Jason Ramapuram,\nand subsequent progressive scaling implementations for smooth strategies (e.g. Appendices H.10\nand H.11) done by Dan Busbridge and Xavier Suau Cuadros.\n53\n"
  },
  {
    "title": "MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation",
    "link": "https://arxiv.org/pdf/2307.14460.pdf",
    "upvote": "6",
    "text": ""
  },
  {
    "title": "TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis",
    "link": "https://arxiv.org/pdf/2307.15042.pdf",
    "upvote": "6",
    "text": "TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis\nZIHAN ZHANG, University of Chicago\nRICHARD LIU, University of Chicago\nKFIR ABERMAN, Google Research\nRANA HANOCKA, University of Chicago\nFig. 1. Inspired by the gradual nature of the diffusion process along a diffusion time-axis (left), our approach (right) entangles the temporal-axis of motion\nwith the time-axis of the diffusion process (right), enabling a new mechanism for synthesizing arbitrarily long motion sequences.\nThe gradual nature of a diffusion process that synthesizes samples in small\nincrements constitutes a key ingredient of Denoising Diffusion Probabilistic\nModels (DDPM), which have presented unprecedented quality in image\nsynthesis and been recently explored in the motion domain. In this work, we\npropose to adapt the gradual diffusion concept (operating along a diffusion\ntime-axis) into the temporal-axis of the motion sequence. Our key idea is\nto extend the DDPM framework to support temporally varying denoising,\nthereby entangling the two axes. Using our special formulation, we itera-\ntively denoise a motion buffer that contains a set of increasingly-noised poses,\nwhich auto-regressively produces an arbitrarily long stream of frames. With\na stationary diffusion time-axis, in each diffusion step we increment only\nthe temporal-axis of the motion such that the framework produces a new,\nclean frame which is removed from the beginning of the buffer, followed\nby a newly drawn noise vector that is appended to it. This new mechanism\npaves the way towards a new framework for long-term motion synthesis\nwith applications to character animation and other domains. Our code and\nproject are publicly available. 1\n1\nINTRODUCTION\nLong-term generation of a motion sequence is a difficult and long\nstanding problem in character animation with myriad applications\nin computer animation, motion control, human-computer interac-\ntion, and more. Generating long-term motion entails producing\nrealistic, non-repetitive sequences which avoids degenerate outputs\n(i.e., frozen motion).\nA promising avenue for generating high-quality motion is through\nDenoising Diffusion Probabilistic Models (DDPM), which have pro-\nduced unprecedented quality in image synthesis [Ho et al. 2020] and\n1Project page: https://threedle.github.io/TEDi/\nhave been recently adapted to motion synthesis [Tevet et al. 2022;\nZhang et al. 2022; Kim et al. 2022]. A typical adaptation of DDPM\nto motion synthesis generates a fixed-length motion sequence (i.e.,\na \u201cmotion image\u201d) from randomly sampled Gaussian noise.\nA fixed-length output is limiting in the context of long-term mo-\ntion synthesis for a couple of reasons. First, there is no satisfactory\napproach for creating long-sequences from short-sequences out-\nputs. Simply chaining together motions and blending them may\ncreate stitching artifacts. Second, a typical diffusion process has\nlimited interactive controllability. Diffusion requires several hun-\ndred denoising iterations before producing a short sequence of clean\nmotions.\nWe are inspired by the time-dependent nature of the diffusion\nprocess, where samples are synthesized from pure noise gradually\nin small time increments along the diffusion time-axis. In this work,\nwe propose to adapt diffusion to the temporal-axis of the motion.\nOur method, referred to as TEDi (Temporally-Entangled Diffusion),\nextends the DDPM framework by enabling injection of temporally-\nvarying noise levels during each step of the diffusion process, instead\nof a Gaussian noise with a fixed, temporally-invariant variance.\nBy entangling the temporal-axis of the motion sequence with the\ntime-axis of the diffusion process, we enable the production of a\ncontinuous stream of clean motion frames during each step of the\ndiffusion process.\nAt the core of our framework lies a motion buffer, which encodes\nnoisy future motion frames with varied noise levels. During the\ntraining phase, we add temporally varied noise to clean motion se-\nquences, such that each frame has a random level. However, during\narXiv:2307.15042v2  [cs.CV]  29 Jul 2023\n2\n\u2022\nZihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka\ninference the motion buffer is initialized with a motion primer - a se-\nquence of clean motion frames that are being noised with increasing\nnoise levels, such that adjacent frames contains consecutive noise\nlevels. TEDi recursively denoises the increasingly-noised future\nframes. In order to constantly maintain the progressively-noised\nmotion buffer structure during each denoising step, we insert a noisy\nframe at the end of the motion buffer and remove a single clean\nframe at the beginning.\nThis recursive mechanism enables motion sequence frames to\nbe continuously generated, and avoids stitching problems which\ncurrent motion diffusion models suffer from (see 4.5.1).\nDuring inference, we can guide the generation with specific mo-\ntions by intervening in the process and persistently injecting clean\nframes, called guiding motions. This injection enables us to control\nand influence the current set of generated frames to prepare and\nplan for the upcoming motion guides. This strategy causes a pre-\nmeditated and calculated transition between the current frames and\nthe future guiding motions.\nOur network continues to denoise an ever-evolving motion buffer,\nwhich contains vague information about the future trajectory of the\nmotion sequence. This formulation opens the door to more direct\ncontrol, and better planning, of the generated motion via manipula-\ntion of the motion buffer. We demonstrate that our framework is\ncapable of producing different types of long motion sequences, and\ndue to its random nature, can provide diverse results even for the\nsame initialization. In addition, we evaluate the model against other\nlong-term generation models. Our experiments show that TEDi is a\nnatural framework for generating long-term motion sequences.\n2\nRELATED WORK\n2.1\nDiffusion Models\nDenoising diffusion probalistic models (DDPMs) and its variants\n[Ho et al. 2020; Dhariwal and Nichol 2021; Ho et al. 2022] have\nachieved unprecedented quality on conditional and unconditional\nimage generation, generally surpassing GAN-based [Dhariwal and\nNichol 2021] methods both in visual quality and sampling diversity.\nIn particular, diffusion models have demonstrated remarkable fi-\ndelity and semantic control for text-to-image synthesis and editing\ntasks when large models are trained on text and image pairs [Ramesh\net al. 2022; Saharia et al. 2022b; Rombach et al. 2021; Ruiz et al. 2022;\nHertz et al. 2022]. In addition, diffusion has been successfully ap-\nplied in adjacent domains such as text-to-video and image-to-image\ntranslation [Saharia et al. 2022a]. Moreover, diffusion models are\nbeginning to see increased usage in generative tasks with 3D data.\nSome recent work enable 3D data generation by reducing it to a 2D\ntask, while others directly train the entire diffusion pipeline on 3D\ndata. More recently, in the animation domain, Zhang et al. [Zhang\net al. 2022], Kim et al. [Kim et al. 2022], Tevet et al. [Tevet et al.\n2022], and Shafir et al. [Shafir et al. 2023] have suggested adapting\ndiffusion models for motion generation by directly applying the\ndiffusion framework, namely by treating the entire motion as an\nimage and denoising all frames in parallel. This adaptation can only\ngenerate fixed-length motion sequences which makes long-term\ngeneration and interactive control infeasible. In contrast, our frame-\nwork combines the diffusion framework with an auto-regressive\ngeneration scheme, thus enabling generation of arbitrary length\nsequences by design.\n2.2\nDeep Motion Synthesis\nBefore the advent of modern deep learning architectures, earlier\nworks attempted to model motion and styles of motion with tech-\nniques such as restricted Boltzmann machines Taylor and Hinton\n[2009]. Later on, the seminal set of works by Holden et al. [2015;\n2016] applied convolutional neural networks (CNN) to motion data\nand learned a motion manifold which can then be used to perform\nmotion editing by, for instance, projection onto the motion mani-\nfold. Concurrently, Fragkiadaki et al. [2015] chose to use recurrent\nneural networks (RNN) for motion modeling. RNN based works\nalso succeeded in short-term motion prediction [Fragkiadaki et al.\n2015; Pavllo et al. 2018], interactive motion generation [Lee et al.\n2018], and music-driven motion synthesis [Aristidou et al. 2021].\nHolden et al. [2017] propose phase-functioned neural networks\n(PFNN) for locomotion generation and introduce phase to neural\nnetworks for motion synthesis. Similar ideas are used in quadruped\nmotion generation by Zhang et al. [2018]. Starke et al. [2020] ex-\ntended phase to local joints to cope with more complex motion\ngeneration. Henter et al. [2020] proposed another generative model\nfor motion based on normalizing flow. Additionally, deep neural\nnetworks have succeeded in a variety of other motion synthesis\ntasks such as motion retargeting [Villegas et al. 2018; Aberman et al.\n2020a, 2019], motion style transfer [Aberman et al. 2020b; Mason\net al. 2022], key-frame based motion generation [Harvey et al. 2020],\nmotion matching [Holden et al. 2020], animation layering [Starke\net al. 2021] and motion synthesis from a single sequence [Li et al.\n2022].\n2.3\nLong-Term Motion Synthesis\nDeep learning models for long term motion synthesis are mostly\nbased on RNNs as they naturally enable auto-regressive genera-\ntion and capture the time dependencies between animation frames.\nIn general, RNNs have shown much success in natural language\nprocessing (NLP) for generating text [Sutskever et al. 2011], hand\nwritten characters [Gregor et al. 2015], and even captioning im-\nages [Vinyals et al. 2015]. They have also been proposed for spatio-\ntemporal prediction where [Ranzato et al. 2014; Srivastava et al.\n2015] integrated 2D convolutions into the recurrent state transi-\ntions of a standard LSTM and proposed the convolutional LSTM\nnetwork, which can model the spatial correlations and temporal\ndynamics in a unified recurrent unit. Wang et al. extended convolu-\ntional LSTMs with pairwise memory cells to capture both long-term\ndependencies and short-term variations to improve the prediction\nquality [Wang et al. 2017]. Zhou et al. tackled the problem of error\naccumulation in long-term random generation by alternating the\nnetwork\u2019s output and ground truth as the input of RNN during\ntraining [Zhou et al. 2018]. This method, called acRNN, is able to\ngenerate long and stable motion similar to the training set. However,\ndespite the modified training procedure, acRNN still fails to produce\nvery long motions. One speculation is that acRNN, and RNNs in\ngeneral, rely on a memory component that is being eroded with\ntime. In contrast, our framework explicitly utilizes frames within our\nTEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis\n\u2022\n3\n[b]\nFig. 2. TEDi Training. We train our diffusion-based model to remove\ntemporally-varying noise that is applied to clean sequences during training.\nIn each iteration we fetch a motion sequence of \ud835\udc3e frames [\ud835\udc531, \ud835\udc532, . . . , \ud835\udc53\ud835\udc3e ]\nfrom the dataset, apply noise to it according to a noise level schedule\n[\ud835\udefd\ud835\udc611, \ud835\udefd\ud835\udc612, . . . , \ud835\udefd\ud835\udc61\ud835\udc3e ], and train our network to predict the clean motion se-\nquence in a supervised fashion as described in (1).\ncontext window which only needs to be the same size temporally\nas the diffusion time-axis, producing the motion autoregressively\nin small increments that complies with the successful mechanism\nof the diffusion process.\n3\nMETHOD\nWe propose a new approach to synthesize long motion sequences\nusing diffusion models. Our approach extends the classic DDPM\nframework to support injection of temporally-varying noise levels\nduring the diffusion process. This extension enables entangling\nthe temporal-axis of the motion sequence with the time-axis of the\ndiffusion process. In the particular case where the first frame in\nthe sequence is mapped into the lowest noise level, the last frame\nto the highest level, and the mapping function is linear, we can\ncontinuously synthesize arbitrarily many frames during inference -\nakin to a motion buffer. In each diffusion step we get a clean frame\nat the beginning of the sequence, shift the frames in the stack by\npopping the clean frame, and append a new noisy frame (drawn from\na Gaussian distribution) to the end of the sequence. Repeating this\nprocess during inference results in a new mechanism for long term\nmotion synthesis. We describe below the motion representation\n(3.1), novel diffusion framework (3.2), training (3.3) and inference\nprocedure (3.4).\n3.1\nMotion representation\nWe represent a motion sequence by a temporal set of \ud835\udc3e poses that\nconsists of root joint displacements with respect to the xz-plane\nOxz \u2208 R\ud835\udc3e\u00d72, root joint height Oy \u2208 RK, and joint rotations R \u2208\nR\ud835\udc3e\u00d7\ud835\udc3d \ud835\udc44, where \ud835\udc3d is the number of joints and \ud835\udc44 is the number of\nrotation features. The rotations are defined in the coordinate frame\nof their parent in the kinematic chain, and represented by the 6D\nrotation features (\ud835\udc44 = 6) proposed by Zhou et al. [2019]. To mitigate\nfoot sliding artifacts, we incorporate foot contact labels as a \ud835\udc36 \u00b7 \ud835\udc3e\nbinary values L \u2208 {0, 1}\ud835\udc3e\u00d7\ud835\udc36, which correspond to the contact labels\nof the foot joints. In our work, we let \ud835\udc36 = 4, where the joints\nare the left(right) heels and toes. All the features are concatenated\nalong the channel axis and we denote the full representation by\nM \u2261 [O\ud835\udc65\ud835\udc67, O\ud835\udc66, R, L] \u2208 R\ud835\udc3e\u00d7(\ud835\udc3d \ud835\udc44+\ud835\udc36+3).\n3.2\nDiffusion Models\nDiffusion Denoising Probabilistic Models (DDPM) [Sohl-Dickstein\net al. 2015; Ho et al. 2020] are generative models that aim to ap-\nproximate a given data distribution \ud835\udc5e(\ud835\udc5a0) with an easy and intu-\nitive sampling mechanism that is inspired by diffusion processes\nin physics. In the particular case of motion synthesis, the data\nconsists of fixed-length motion sequences. During training, the\nprocess starts by sampling a clean motion sequence \ud835\udc5a0 from the\ndataset, then an IID Gaussian noise is added gradually to form a se-\nquence of noisy motions which constitute the latent variables of the\nprocess {\ud835\udc5a1, . . . ,\ud835\udc5a\ud835\udc47 }. The latent sequence follows \ud835\udc5e(\ud835\udc5a1, . . . ,\ud835\udc5a\ud835\udc61 |\n\ud835\udc5a0) = \u00ce\ud835\udc61\n\ud835\udc56=1 \ud835\udc5e(\ud835\udc5a\ud835\udc56 | \ud835\udc5a\ud835\udc56\u22121), where a sampling step in the forward\nprocess (clean data to noise) is defined as a Gaussian transition\n\ud835\udc5e(\ud835\udc5a\ud835\udc61 | \ud835\udc5a\ud835\udc61\u22121) := N (\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc5a\ud835\udc61\u22121, \ud835\udefd\ud835\udc61\ud835\udc3c) parameterized by a schedule\n\ud835\udefd0, . . . , \ud835\udefd\ud835\udc47 \u2208 (0, 1). When the total diffusion time step \ud835\udc47 is large\nenough, the last noise vector \ud835\udc5a\ud835\udc47 nearly follows an isotropic Gauss-\nian distribution.\nIn order to sample from the distribution \ud835\udc5e(\ud835\udc5a0), we define the dual\n\u201creverse process\u201d \ud835\udc5d(\ud835\udc5a\ud835\udc61\u22121 | \ud835\udc5a\ud835\udc61) from isotropic Gaussian noise \ud835\udc5a\ud835\udc47 to\ndata by sampling the posteriors \ud835\udc5e(\ud835\udc5a\ud835\udc61\u22121 | \ud835\udc5a\ud835\udc61). Since the intractable\nreverse process\ud835\udc5e(\ud835\udc5a\ud835\udc61\u22121 | \ud835\udc5a\ud835\udc61) depends on the unknown data distribu-\ntion \ud835\udc5e(\ud835\udc5a0), we approximate it with a parameterized Gaussian tran-\nsition network \ud835\udc5d\ud835\udf03 (\ud835\udc5a\ud835\udc61\u22121 | \ud835\udc5a\ud835\udc61) := N (\ud835\udc5a\ud835\udc61\u22121 | \ud835\udf07\ud835\udf03 (\ud835\udc5a\ud835\udc61,\ud835\udc61), \u03a3\ud835\udf03 (\ud835\udc5a\ud835\udc61,\ud835\udc61)).\nAs suggested by [Tevet et al. 2022], instead of predicting the noise\nas formulated by [Ho et al. 2020], we follow [Ramesh et al. 2022] and\nthe network predicts the signal itself while solving the following\noptimization problem:\nmin\n\ud835\udf03\n\ud835\udc3f(\ud835\udf03) := min\n\ud835\udf03\n\ud835\udc38\ud835\udc5a0\u223c\ud835\udc5e(\ud835\udc5a0),\ud835\udc64\u223c\ud835\udc41 (0,\ud835\udc3c ),\ud835\udc61 \u2225\ud835\udc5a0 \u2212 \ud835\udf07\ud835\udf03 (\ud835\udc5a\ud835\udc61,\ud835\udc61)\u22252\n2 ,\n(1)\nwhich maximizes a variational lower bound. In addition, we find\nthat it is best to fix the variance schedule on the reverse process,\nnamely setting \u03a3\ud835\udf03 = \ud835\udefd\ud835\udc61\ud835\udc3c for all time steps, so our model only needs\nto learn to predict the clean motion. For more details about DDPMs\nplease refer to [Sohl-Dickstein et al. 2015; Ho et al. 2020].\n3.3\nTeporally-Entangled Diffusion\nNext, we extend the DDPM framework to support injection of\ntemporally-varying noise levels during the diffusion process. The\nnoise level becomes a function of the frame index and we discard\nthe notion of the diffusion time-axis during training. Effectively,\nwe are setting \ud835\udc47 = \ud835\udc3e and identifying the diffusion time-axis and\n4\n\u2022\nZihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka\nthe motion temporal-axis. We propose two schemes for noise injec-\ntion: 1) random schedule, and 2) monotonic schedule (we avoid the\nterm linear schedule as it is commonly used to indicate a type of\nvariance schedule [Nichol and Dhariwal 2021]). Note that these are\nnot variance schedules. Concretely, given a fixed variance sched-\nule \ud835\udefd\ud835\udc61\ud835\udc56 \u2208 (0, 1),\ud835\udc61\ud835\udc56 \u2208 {0, 1, . . . ,\ud835\udc47 }, at each training step the random\nschedule is given by\n[\ud835\udefd\ud835\udc611, \ud835\udefd\ud835\udc612, . . . , \ud835\udefd\ud835\udc61\ud835\udc3e ], \ud835\udc61\ud835\udc56 \u223c U(0,\ud835\udc47).\n(2)\nOn the other hand, the monotonic schedule is given by\n[\ud835\udefd\ud835\udc611, \ud835\udefd\ud835\udc612, . . . , \ud835\udefd\ud835\udc61\ud835\udc3e ], \ud835\udc61\ud835\udc56 = \ud835\udc56.\n(3)\nThe former gives a temporally-varying noise level while the latter\ngives a monotonically increasing noise level.\nIn practice, we use a mix of these two noise injection schemes\nduring training, so the model learns to completely denoise a motion\nsequence with varying noise levels across frames. This enables us to\ncreate explicit entanglement between the time axis of the diffusion\nprocess and the temporal-axis of the motion - a unique property\nwhich will be exploited during inference.\nFor each iteration during training, we sample from the dataset\na motion sequence of length \ud835\udc3e, [\ud835\udc531, \ud835\udc532, . . . , \ud835\udc53\ud835\udc3e]. The model is given\nthe noise injected motion [ \u02dc\ud835\udc531, \u02dc\ud835\udc532, . . . , \u02dc\ud835\udc53\ud835\udc3e] as input where\n\u02dc\ud835\udc53\ud835\udc56 \u223c N (\n\u221a\ufe01\n\u00af\ud835\udefc(\ud835\udc61\ud835\udc56)\ud835\udc53\ud835\udc56, (1 \u2212 \u00af\ud835\udefc(\ud835\udc61\ud835\udc56)\ud835\udc3c),\nfor \u00af\ud835\udefc(\ud835\udc61\ud835\udc56) = \u00ce\ud835\udc61\ud835\udc57\n\ud835\udc57=1(1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc57 ), and is tasked to predict the clean mo-\ntion [\ud835\udc531, \ud835\udc532, . . . , \ud835\udc53\ud835\udc3e] directly. To give the network a mixture of the\ntwo types of noise injection, we assign [\ud835\udefd\ud835\udc61\ud835\udc57 ]\ud835\udc3e\n\ud835\udc57=1 using the random\nschedule or monotonic schedule with fixed probabilities \ud835\udc5d and 1 \u2212\ud835\udc5d.\nWe set \ud835\udc5d = 2\n3 in practice. In particular, the training objective with\nthe random schedule is similar to those of a pose-oriented diffusion\nmodel, where we view the entire motion sequence as a batch of\nposes with batch size \ud835\udc3e. And at each frame index, the model tries\nto learn a posterior\n\ud835\udc5e\u2217(\ud835\udc53 \ud835\udc61\u22121 | \ud835\udc53 \ud835\udc61)\nwhere the superscript indicates time in the diffusion time-axis, and\n\ud835\udc5e\u2217(\ud835\udc53 0) is the data distribution of individual poses. Then, the objec-\ntive with monotonic noise schedule serves to provide additional\nsupervision to ensure smooth transitions across frames during in-\nference.\n3.3.1\nLoss functions. As previously mentioned, the benefit of pre-\ndicting the clean motions directly is that it gives access to regu-\nlarizations that otherwise would be ill-defined for the mollified\ndistributions. For instance, joint velocities cannot be properly regu-\nlarized with loss terms for noisy motions. Due to the hierarchical\nnature of the human model, errors accumulate along the kinematic\nchains, thus errors on joint rotations should be weighted appropri-\nately with respect to their positions in the hierarchy. Therefore, we\nadd a positional loss loss defined as follows:\nLpos = 1\n\ud835\udc3e\ud835\udc3d\n\ud835\udc3e\n\u2211\ufe01\n\ud835\udc61=1\n\r\rFKS( \u02c6R\ud835\udc61, \u02c6O\ud835\udc61) \u2212 FKS(R\ud835\udc61, O\ud835\udc61)\n\r\r2\n2 ,\n(4)\nwhere FKS : R\ud835\udc3d \ud835\udc44 \u00d7 R3 \u2192 R3\ud835\udc3d is a forward kinematics operator for\na fixed skeleton \ud835\udc46, and \u02c6R, \u02c6O are the model predicted joint rotation\nand displacements and R, O are the corresponding ground truth. In\nFig. 3. TEDi Recursive Generation. TEDi is capable of generating an\narbitrarily long motion sequence. First, we initialize our motion buffer with\na a set of increasingly-noised motion frames. Then (step 1) we denoise the\nentire motion buffer, (step 2) pop the new, clean frame in the beginning of\nthe motion buffer, and then (step 3) push noise into the end of the motion\nbuffer. This process is repeated recursively.\naddition, since foot contact is vital to generating natural motions\nand enables using inverse kinematics as post-process, we further\npenalize errors accumulated at the foot joint with the following foot\ncontact loss:\nLcontact =\n1\n\ud835\udc3e\ud835\udc36\n\u2211\ufe01\n\ud835\udc57\n\ud835\udc3e\u22121\n\u2211\ufe01\n\ud835\udc61=1\n\r\rFKS(R\ud835\udc61+1, O\ud835\udc61+1)\ud835\udc57 \u2212 FKS(R\ud835\udc61, O\ud835\udc61)\ud835\udc57\n\r\r2\n2 \u00b7\ud835\udc60(L\ud835\udc61 \ud835\udc57),\n(5)\nwhere\n\ud835\udc60 =\n1\n1 + \ud835\udc52\u221212(\ud835\udc65\u22120.5) .\nThis penalizes high foot velocity while having true contact labels,\nthus ensuring self-consistency of the generated motions.\n3.3.2\nTraining. In summary, our full training loss is\nL = \ud835\udf06diffLdiff + \ud835\udf06posLpos + \ud835\udf06contactLcontact\n(6)\nwhere Ldiff corresponds to the diffusion loss specified by equation\n(1), and the \ud835\udf06 parameters determine the weights of the losses.\nOur diffusion network is inspired by the typical U-Net model\nused in the 2D image diffusion domain [Rombach et al. 2022]. In\norder for the network to process 1D signals, we use 1D convolutions\nstriding over the temporal axis. We also use 1D attention blocks\nand skip connections so long term frame correlations are captured\nwithin the motion data.\nTEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis\n\u2022\n5\n3.4\nInference\nDuring inference, we take advantage of the monotonic noise sched-\nule that our model trained on. We use a typewriting-like system,\nas depicted in Fig. 3. Our model maintains a buffer of frames with\nmonotonically increasing noise, where the first frame in the buffer\nis mapped to the lowest noise level, and and the last to the high-\nest, as described in (3). The model is designed to generate motion\nautoregressively. At the beginning, the buffer is initialized with\na given motion sequence that is noised with increasing variance.\nThen, at each iteration, the model processes all the frames in the\nmotion sequence in parallel and produces a progressively denoised\nsequence. At this point, the first frame in the sequence is completely\nclean and can be popped from the buffer. We sample a new frame\nfrom standard Gaussian distribution and push it into the motion\nsequence at the end of the buffer. The model can then iteratively\nperform this denoising mechanism. This mode of generation can be\ncontinued indefinitely as desired, and the resulting motion frames\nare collected frame by frame from the model output.\nConcretely, let \ud835\udc40\ud835\udf03 be our model and let \ud835\udc3c = [ \u02dc\ud835\udc531, . . . , \u02dc\ud835\udc53\ud835\udc3e] be the\ninitialization (clean motion that is noised with increasing variance)\nand let \ud835\udc39out denote the (initially empty) set of output frames. At\ntime step \ud835\udc61, \ud835\udc61 \u2208 {1, 2, . . . }, we have the update\n\ud835\udc39out = [\ud835\udc39out, \ud835\udc40\ud835\udf03 (\ud835\udc3c)1],\n\u02dc\ud835\udc53\ud835\udc56\u22121 = \ud835\udc40\ud835\udf03 (\ud835\udc3c)\ud835\udc56, \ud835\udc56 \u2208 {2, . . . \ud835\udc3e},\n\u02dc\ud835\udc53\ud835\udc3e = \ud835\udc4b \u223c N (0, \ud835\udc3c),\n\ud835\udc3c = [ \u02dc\ud835\udc531, . . . , \u02dc\ud835\udc53\ud835\udc3e],\nwhere \ud835\udc40\ud835\udf03 (\ud835\udc3c)\ud835\udc56 denotes the \ud835\udc56-th frame in the output of our model.\nWe highlight the distinction from a typical inference pass in the\nstandard diffusion process, which samples Gaussian noise using\nthe full motion length and repeatedly denoises the entire motion.\nFor such a generation scheme, all the frames are required to pass\nthrough the model \ud835\udc47 times. Here, our inference scheme is able to\noutput a new clean frame after only one forward pass of the model.\nAt the same time, a newly sampled frame (pure noise) that gets\npushed into the motion buffer will stay in the motion buffer for\n\ud835\udc47 iterations, going through all diffusion time steps before getting\nadded to the output. In short, our inference method enables faster\nautoregressive generation yet ensures that each frame of motion\ngoes through the full diffusion process.\n4\nEXPERIMENTS\nIn this section, we demonstrate the effectiveness of TEDi on several\nlong-term generation tasks. We show several unique applications of\nour method, including the ability to plan for upcoming motion using\nguided generation. We also evaluate our method through various\ncomparisons and ablations. For additional qualitative results, please\nrefer to the supplemental video.\n4.1\nImplementation details\nOur TEDi framework is implemented with PyTorch, and the training\nand inference are done on the NVIDIA A40 GPU. We use Adam\nas our optimizer. For the training data, we use motions from the\nCMU motion dataset and downsample them from 120fps to 30fps.\nFig. 4. Long-term Generation. Our method synthesizes arbitrarily long\nmotion sequences. In the above figure, we summarize 33 seconds of motion\nby visualizing the pose every 100-frames (\u22483 seconds). Our model is able to\ngenerate plausible motions throughout the entire motion sequence.\nWe then sample windows of 500 frames with a stride of 100 frames.\nThe CMU dataset contains frames that are shorter than 500 frames\nafter downsampling, and those are not used for training. Training\ntakes approximately three days for 500k iterations.\n4.2\nLong-term Generation\nOur TEDi framework is able to generate long-term motions condi-\ntioned on a clean primer motion which is used to populate the initial\nmotion buffer. The model is given as input a primer of \ud835\udc3e frames\n{ \u02dc\ud835\udc531, \u02dc\ud835\udc532, . . . , \u02dc\ud835\udc53\ud835\udc3e } which are progressively noised with a monotonic\nnoise schedule. Our iterative inference strategy can then produce\nan arbitrarily long sequence of new frames. We highlight some of\nthe frames from a long-term sequence generated by our method in\nFig. 4. The key to maintaining long-term generation is that at each\niteration, the newly sampled noise frame ensures that our \"buffer\"\nis able to explore new potential motions in the near future, and the\niterative denoising process ensures framewise consistency across\nthe motion. In addition, we show in Fig. 5 and 6 that our method is\ncapable of generating diverse motion sequences. Full video results\ncan be found in the supplementary video.\n4.3\nGuided generation\nFor a character in motion, it is often desired for the character to\nperform a set of predefined motions which will occur at a point and\ntime in the future. We refer to these frames are motion guides. Our\nframework maintains a motion buffer which contains information\nabout the motions to be performed in the future. In order to influence\nthe set of currently-generated frames, we directly modify the motion\nbuffer using the motion guide. Specifically, we remove the current\nset of frames and replace them with a noised version of the motion\nguide. Then, we discard the predicted denoised frames and replace\nthem with the noised version of the motion guide at the appropriate\ndiffusion time.\nSuppose we have a motion buffer of \ud835\udc3e frames \ud835\udc3c = [ \u02dc\ud835\udc531, . . . , \u02dc\ud835\udc53\ud835\udc3e]\nand a set of motion guides Q1, Q2, . . . each with length \ud835\udc591,\ud835\udc592, . . .\nframes that we wish to perform starting at frame number \ud835\udc5b1,\ud835\udc5b2, . . .,\n6\n\u2022\nZihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka\nFig. 5. Diverse Motions. Our method is capable of producing a wide variety of long motion sequences. From left to right: Boxing, shuffling, and hand-gestures.\nFig. 6. Motion Variations. Due to the stochastic nature of diffusion models, our method is able to generate variations using the same motion primer as input.\nWe show four motions generated from a single primer, from left to right, we can see that the motions begins to differ significantly as time goes on.\nFig. 7. Guided Generation. Given a set of motion guides Q\ud835\udc56 (shown in yellow), we are able to perform them in sequence at desired points while generating\nplausible motion in the interactively generated frames (blue). From top-left to bottom-right, our method generates an entire motion sequence that contains\nthe desired motion guides and the interactively synthesized motion. The interactively generated motions will \u201cprepare and plan\u201d for the upcoming motion\nguides. See the supplementary video.\nFig. 8. Trajectory Control. Similar to guided generation, given the desired trajectory information P (shown in yellow), our method can generate natural\nmotions that adhere to the given trajectory.\nTEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis\n\u2022\n7\nFig. 9. Ablations: Here we show the average motion variance over 500\nframes for our method with and without the random schedule. It can be\nseen that our random schedule helps avoid motion-collapse.\n\ud835\udc5b\ud835\udc56 \u2265 \ud835\udc3e. Assuming we start with the frame number \ud835\udc5b = 1 (e.g. the\nend of the current motion buffer would be frame number \ud835\udc3e), for\neach predefined motion Q\ud835\udc56, if any of its frames Q\ud835\udc56\ud835\udc57 , where \ud835\udc57 \u2208\n{1, 2, . . . ,\ud835\udc59\ud835\udc56} and \ud835\udc5b\ud835\udc56 \u2264 Q\ud835\udc56\ud835\udc57 \u2264 \ud835\udc5b\ud835\udc56 +\ud835\udc59\ud835\udc56 is such that \ud835\udc5b+5 \u2264 \ud835\udc5b\ud835\udc56 + \ud835\udc57 \u2264 \ud835\udc5b+\ud835\udc3e,\nthen we recursively replace it into the motion buffer. Note that\nthere are five frames right before the start of the motion buffer\nwhere we don\u2019t recursively replace. This enables the network to\nsmooth out the transitions between the generated frames and the\nmotion guide. We have detailed the procedure for guided generation\nthrough recursive replacement in Algorithm 1. We demonstrate\nguided generation in Fig. 7 and the supplementary material.\nAlgorithm 1 Guided generation\nRequire:\n\ud835\udc40\ud835\udf03: Denoising model\n\ud835\udc3c = [ \u02dc\ud835\udc531, . . . , \u02dc\ud835\udc53\ud835\udc3e]: motion buffer\n{Q1, Q2, . . .}: motion guides\n{\ud835\udc591,\ud835\udc592, . . .}: motion guide lengths\n{\ud835\udc5b1,\ud835\udc5b2, . . .}: starting frame numbers for guidance\n\ud835\udc39out = \u2205: ouput frames\nfor \ud835\udc5b in 1, 2, . . . do\nEvaluate \ud835\udc40\ud835\udf03 (\ud835\udc3c)\nfor all frames \ud835\udc44\ud835\udc56\ud835\udc57 do\nif \ud835\udc5b + 5 \u2264 \ud835\udc5b\ud835\udc56 + \ud835\udc57 \u2264 \ud835\udc5b + \ud835\udc3e then\n\ud835\udc40\ud835\udf03 (\ud835\udc3c)\ud835\udc5b\ud835\udc56+\ud835\udc57\u2212\ud835\udc5b \u2190 \ud835\udc44\ud835\udc56\ud835\udc57\nend if\nend for\n\ud835\udc39out \u2190 [\ud835\udc39out, \ud835\udc40\ud835\udf03 (\ud835\udc3c)1]\n\u02dc\ud835\udc53\ud835\udc56\u22121 \u2190 \ud835\udc40\ud835\udf03 (\ud835\udc3c)\ud835\udc56 \u2200\ud835\udc56 \u2208 {2, . . . \ud835\udc3e}\n\u02dc\ud835\udc53\ud835\udc3e \u2190 \ud835\udc4b \u223c N (0, \ud835\udc3c)\n\ud835\udc3c \u2190 [ \u02dc\ud835\udc531, . . . , \u02dc\ud835\udc53\ud835\udc3e]\nend for\n4.4\nTrajectory Control\nOur work can be applied to perform trajectory control during infer-\nence without additional training. Similar to the mechanism of guided\ngeneration in the previous section, trajectory control also utilizes\nthe inpainting strategy by modifying the motion buffer. Specifically,\nlet \ud835\udc3c = [ \u02dc\ud835\udc531, . . . , \u02dc\ud835\udc53\ud835\udc3e] be a motion buffer of \ud835\udc3e frames, and let P \u2208 R3\u00d7\ud835\udc41\nbe the trajectory information (root displacements with respect to\nthe xz-plane and root height), where \ud835\udc41 is the desired number of\nframes to be generated. During inference, we recursively overwrite\nthe trjactory information in the motion buffer with frames in \ud835\udc43. The\ndetailed procedure is similar to the one presented in Algorithm 1.\nWe demonstrate trajectory control generation in Fig. 8.\n4.5\nComparison and Ablation\nWe next evaluate our approach against alternative baselines, and\nassess our framework through an ablation study. We refer the reader\nto the supplementary video attached to this work to assess the re-\nsults qualitatively. For quantitative evaluation, we assess our ability\nto avoid collapses in the motion sequence by measuring the variance\nacross all generated frames. In order to measure how non-stationary\ngenerated motions are, and to detect the time-point where they col-\nlapse, we measure the average variance of poses in a local window.\n4.5.1\nComparison. In this experiment, we focus on comparing our\nframework to other works on the task of long-term generation. We\ncompare our method with ACRNN [Zhou et al. 2018] and the Human\nMotion Diffusion Model (MDM) [Tevet et al. 2022]. In particular,\nthe ACRNN work [Zhou et al. 2018] is an RNN-based work that\nreceives part of the model\u2019s output frames during training, to imitate\nthe inference setting and mitigate motion collapse. MDM is an\nadaptation of the classic DDPM network for motion generation.\nWhile ACRNN is designed to be trained on a subset of samples\nfrom the CMU dataset and has long-term generation as default for\ninference, MDM does not have a default implementation for long\nterm generation. Thus we use a pretrained checkpoint for MDM\nand implement an inpainting-based scheme to enable long-term\ngeneration for MDM. This implementation is the same as the popular\n\"outpainting\" technique used in 2D image generation, where we take\nthe latter part of the generated motion and in-paint it to the first part\nof the generated motion on the next iteration. As in Fig. 10, it can be\nseen that ACRNN is not able to perform well on a large and diverse\ndataset, producing motions that quickly collapse after initialization.\nIn contrast, TEDi can produce infinitely long sequences that is\nrobust to collapses. On the other hand, MDM produces significant\nstitching artifacts along the in-painting boundary. Please refer to\nthe supplemental video for more details.\n4.6\nPerceptual Study\nWe conduct a perceptual study to evaluate the perceived diver-\nsity and quality of the generated motions. In addition to MDM\nand ACRNN, we also add Motion VAE [Ling et al. 2020], a recent\nautoregressive motion generation model with VAE, as a baseline\ncomparison. Following the setup of DALLE-2 [Ramesh et al. 2022],\nwe show users 3x3 grids of randomly sampled motions from our\nmodel, MDM, Motion VAE, and ACRNN, and ask them to choose 1)\nthe set with the most diverse motions and 2) the set with the highest\n8\n\u2022\nZihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka\nquality motions (only from ours, MDM, or ACRNN). Visual exam-\nples of the generated motions in the perceptual study is provided in\nAppendix A\nWe had 55 respondents for our study, and we report the results\nin Tab. 1. We conclude from our perceptual study that our method\nproduces motion of equivalent or better quality compared to MDM\nwhile significantly outperforming in terms of diversity.\nOurs\nMDM\nACRNN\nMVAE\nDiversity\n34\n12\n8\n1\nQuality\n33\n17\n5\nN/A\nTable 1. Perceptual study results for our method and baselines.\n4.6.1\nAblation. In Fig. 9, we demonstrate the advantage of our\ntraining scheme, by training a version of our model with temporally-\ninvariant noise levels. Without temporally varying noise, the net-\nwork diminishes in both diversity and stability of long-range motion\ngeneration.\n5\nCONCLUSION\nIn this paper, we proposed TEDi, an adaptation of diffusion models\nfor motion synthesis which entangles the motion temporal-axis\nwith the diffusion time-axis. This mechanism enables synthesizing\narbitrarily long motion sequences in an autoregressive manner us-\ning a U-Net architecture. A unique aspect of our work is the notion\nof a stationary motion buffer. Our framework continues to produce\nclean frames (i.e., progressing along the diffusion-time axis), with-\nout actually incrementing the diffusion time. The ability of our\npipeline to continually generate motion along the diffusion axis is\nwhat enables our framework to robustly and continuously produce\nnovel frames. Interestingly, the ability to naturally use diffusion\nin such an autoregressive fashion may have implications for other\ntypes of sequential data beyond motion, such as audio and video,\nor modalities where a sequential order can be defined, such as a\npatch-by-patch order for images.\nOur system enables partially-clean-frame to be immediately (or\nnear immediately) popped-off the motion buffer stack. However, a\ncurrent limitation of our system is that computing a clean from from\npure noise requires going through the chain of denoising diffusion.\nIn the future we are interested in leveraging ideas from DDIM [Song\net al. 2020] to skip ahead during the denoising process to achieve\neven lower latency. In addition, our framework may enable future\nresearch in long-term text-conditioned motion generation. We are\ninterested in exploring how high-level control may be coupled with\nlow-level user-guidance for the task of long-term generation.\nACKNOLWEDGEMENTS\nWe thank the 3DL lab for their invaluable feedback and support.\nThis work was supported in part through Uchicago\u2019s AI Cluster\nresources, services, and staff expertise. This work was also partially\nsupported by the NSF under Grant No. 2241303, and a gift from\nGoogle Research.\nREFERENCES\nKfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung, Daniel Cohen-Or,\nand Baoquan Chen. 2020a. Skeleton-aware networks for deep motion retargeting.\nACM Transactions on Graphics (TOG) 39, 4 (2020), 62\u20131.\nKfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, and Baoquan Chen.\n2020b. Unpaired motion style transfer from video to animation. ACM Transactions\non Graphics (TOG) 39, 4 (2020), 64\u20131.\nKfir Aberman, Rundi Wu, Dani Lischinski, Baoquan Chen, and Daniel Cohen-Or.\n2019. Learning Character-Agnostic Motion for Motion Retargeting in 2D. ACM\nTrans. Graph. 38, 4 (2019), 75.\nAndreas Aristidou, Anastasios Yiannakidis, Kfir Aberman, Daniel Cohen-Or, Ariel\nShamir, and Yiorgos Chrysanthou. 2021. Rhythm is a Dancer: Music-Driven Motion\nSynthesis with Global Structure. arXiv preprint arXiv:2111.12159 (2021).\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image\nsynthesis. Advances in Neural Information Processing Systems 34 (2021), 8780\u20138794.\nKaterina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. 2015. Recur-\nrent network models for human dynamics. In Proceedings of the IEEE International\nConference on Computer Vision. 4346\u20134354.\nKarol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. 2015.\nDraw: A recurrent neural network for image generation. In International conference\non machine learning. PMLR, 1462\u20131471.\nF\u00e9lix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. Robust\nmotion in-betweening. ACM Transactions on Graphics (TOG) 39, 4 (2020), 60\u20131.\nGustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. Moglow: Probabilistic\nand controllable motion synthesis using normalising flows. ACM Transactions on\nGraphics (TOG) 39, 6 (2020), 1\u201314.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-Or. 2022. Prompt-to-prompt image editing with cross attention control.\narXiv preprint arXiv:2208.01626 (2022).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and\nTim Salimans. 2022. Cascaded Diffusion Models for High Fidelity Image Generation.\nJ. Mach. Learn. Res. 23 (2022), 47\u20131.\nDaniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. Learned\nmotion matching. ACM Transactions on Graphics (TOG) 39, 4 (2020), 53\u20131.\nDaniel Holden, Taku Komura, and Jun Saito. 2017. Phase-functioned neural networks\nfor character control. ACM Transactions on Graphics (TOG) 36, 4 (2017), 1\u201313.\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A deep learning framework for\ncharacter motion synthesis and editing. ACM Transactions on Graphics (TOG) 35, 4\n(2016), 1\u201311.\nDaniel Holden, Jun Saito, Taku Komura, and Thomas Joyce. 2015. Learning motion\nmanifolds with convolutional autoencoders. In SIGGRAPH Asia 2015 Technical Briefs.\n1\u20134.\nJihoon Kim, Jiseob Kim, and Sungjoon Choi. 2022. Flame: Free-form language-based\nmotion synthesis & editing. arXiv preprint arXiv:2209.00349 (2022).\nKyungho Lee, Seyoung Lee, and Jehee Lee. 2018. Interactive character animation by\nlearning multi-objective control. ACM Transactions on Graphics (TOG) 37, 6 (2018),\n1\u201310.\nPeizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung.\n2022. Ganimator: Neural motion synthesis from a single sequence. ACM Transactions\non Graphics (TOG) 41, 4 (2022), 1\u201312.\nHung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. 2020. Character\ncontrollers using motion VAEs. ACM Transactions on Graphics (2020).\nIan Mason, Sebastian Starke, and Taku Komura. 2022. Real-Time Style Modelling of\nHuman Locomotion via Feature-Wise Transformations and Local Motion Phases.\narXiv preprint arXiv:2201.04439 (2022).\nAlex Nichol and Prafulla Dhariwal. 2021. Improved Denoising Diffusion Probabilistic\nModels. (2021). arXiv:cs.LG/2102.09672\nDario Pavllo, David Grangier, and Michael Auli. 2018. Quaternet: A quaternion-based\nrecurrent model for human motion. arXiv preprint arXiv:1805.06485 (2018).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nMarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert,\nand Sumit Chopra. 2014. Video (language) modeling: a baseline for generative\nmodels of natural videos. arXiv preprint arXiv:1412.6604 (2014).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.\narXiv:cs.CV/2112.10752\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2022. Dreambooth: Fine tuning text-to-image diffusion models for subject-\ndriven generation. arXiv preprint arXiv:2208.12242 (2022).\nTEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis\n\u2022\n9\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans,\nDavid Fleet, and Mohammad Norouzi. 2022a. Palette: Image-to-image diffusion\nmodels. In ACM SIGGRAPH 2022 Conference Proceedings. 1\u201310.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton,\nSeyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gon-\ntijo Lopes, Tim Salimans, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad\nNorouzi. 2022b. Photorealistic Text-to-Image Diffusion Models with Deep Language\nUnderstanding. arXiv preprint arXiv:2205.11487 (2022).\nYonatan Shafir, Guy Tevet, Roy Kapon, and Amit H. Bermano. 2023. Human Motion\nDiffusion as a Generative Prior. arXiv:cs.CV/2303.01418\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015.\nDeep unsupervised learning using nonequilibrium thermodynamics. In International\nConference on Machine Learning. PMLR, 2256\u20132265.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit\nModels. In International Conference on Learning Representations.\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. 2015. Unsupervised\nlearning of video representations using lstms. In International conference on machine\nlearning. PMLR, 843\u2013852.\nSebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. 2020. Local motion phases\nfor learning multi-contact character movements. ACM Transactions on Graphics\n(TOG) 39, 4 (2020), 54\u20131.\nSebastian Starke, Yiwei Zhao, Fabio Zinno, and Taku Komura. 2021. Neural animation\nlayering for synthesizing martial arts movements. ACM Transactions on Graphics\n(TOG) 40, 4 (2021), 1\u201316.\nIlya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with\nrecurrent neural networks. In ICML.\nGraham W Taylor and Geoffrey E Hinton. 2009. Factored conditional restricted Boltz-\nmann machines for modeling motion style. In Proceedings of the 26th annual inter-\nnational conference on machine learning. 1025\u20131032.\nGuy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H\nBermano. 2022. Human motion diffusion model. arXiv preprint arXiv:2209.14916\n(2022).\nRuben Villegas, Jimei Yang, Duygu Ceylan, and Honglak Lee. 2018. Neural kinematic\nnetworks for unsupervised motion retargetting. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition. 8639\u20138648.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell:\nA neural image caption generator. In Proceedings of the IEEE conference on computer\nvision and pattern recognition. 3156\u20133164.\nYunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S Yu. 2017.\nPredrnn: Recurrent neural networks for predictive learning using spatiotemporal\nlstms. Advances in neural information processing systems 30 (2017).\nHe Zhang, Sebastian Starke, Taku Komura, and Jun Saito. 2018. Mode-adaptive neural\nnetworks for quadruped motion control. ACM Transactions on Graphics (TOG) 37, 4\n(2018), 1\u201311.\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang,\nand Ziwei Liu. 2022. Motiondiffuse: Text-driven human motion generation with\ndiffusion model. arXiv preprint arXiv:2208.15001 (2022).\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. 2019. On the continuity\nof rotation representations in neural networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 5745\u20135753.\nYi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. 2018. Auto-\nConditioned Recurrent Networks for Extended Complex Human Motion Synthesis.\nIn International Conference on Learning Representations.\n10\n\u2022\nZihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka\nMDM\nACRNN\nFig. 10. Long-term motion synthesis baseline comparisons. Top: We\nshow two pairs of consecutive frames generated through an in-painting\nimplementation with MDM [Tevet et al. 2022]. Classic in-painting shows\nvisible discontinuity that happens along the border of in-painting. Bottom:\nACRNN [Zhou et al. 2018] when trained on a large dataset is not stable, as\nseen by the foot levitation and penetration artifacts.\nA\nPERCEPTUAL STUDY\nHere we provide screenshots of our perceptual study in Fig. 11 and\nFig. 12, as shown for respondents.\nFig. 11. Questions from perceptual study.\nFig. 12. Example motions from perceptual study. From top to bottom: Ours,\nACRNN, MDM, and Motion VAE.\n"
  }
]