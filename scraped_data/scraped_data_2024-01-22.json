[
  {
    "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
    "link": "https://arxiv.org/pdf/2401.10891.pdf",
    "upvote": "53",
    "text": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data\nLihe Yang1\nBingyi Kang2\u2020\nZilong Huang2\nXiaogang Xu3,4\nJiashi Feng2\nHengshuang Zhao1\u2020\n1The University of Hong Kong\n2TikTok\n3Zhejiang Lab\n4Zhejiang University\n\u2020 corresponding authors\nhttps://depth-anything.github.io\nFigure 1. Our model exhibits impressive generalization ability across extensive unseen scenes. Left two columns: COCO [35]. Middle two:\nSA-1B [27] (a hold-out unseen set). Right two: photos captured by ourselves. Our model works robustly in low-light environments (1st and\n3rd column), complex scenes (2nd and 5th column), foggy weather (5th column), and ultra-remote distance (5th and 6th column), etc.\nAbstract\nThis work presents Depth Anything1, a highly practical\nsolution for robust monocular depth estimation. Without pur-\nsuing novel technical modules, we aim to build a simple yet\npowerful foundation model dealing with any images under\nany circumstances. To this end, we scale up the dataset by\ndesigning a data engine to collect and automatically anno-\ntate large-scale unlabeled data (\u223c62M), which significantly\nenlarges the data coverage and thus is able to reduce the\ngeneralization error. We investigate two simple yet effective\nstrategies that make data scaling-up promising. First, a more\nchallenging optimization target is created by leveraging data\naugmentation tools. It compels the model to actively seek\nextra visual knowledge and acquire robust representations.\nSecond, an auxiliary supervision is developed to enforce\nthe model to inherit rich semantic priors from pre-trained\nencoders. We evaluate its zero-shot capabilities extensively,\nincluding six public datasets and randomly captured photos.\nIt demonstrates impressive generalization ability (Figure 1).\nFurther, through fine-tuning it with metric depth information\nfrom NYUv2 and KITTI, new SOTAs are set. Our better depth\nmodel also results in a better depth-conditioned ControlNet.\nOur models are released here.\nThe work was done during an internship at TikTok.\n1While the grammatical soundness of this name may be questionable,\nwe treat it as a whole and pay homage to Segment Anything [27].\n1. Introduction\nThe field of computer vision and natural language processing\nis currently experiencing a revolution with the emergence of\n\u201cfoundation models\u201d [6] that demonstrate strong zero-/few-\nshot performance in various downstream scenarios [44, 58].\nThese successes primarily rely on large-scale training data\nthat can effectively cover the data distribution. Monocular\nDepth Estimation (MDE), which is a fundamental problem\nwith broad applications in robotics [65], autonomous driv-\ning [63, 79], virtual reality [47], etc., also requires a foun-\ndation model to estimate depth information from a single\nimage. However, this has been underexplored due to the\ndifficulty of building datasets with tens of millions of depth\nlabels. MiDaS [45] made a pioneering study along this di-\nrection by training an MDE model on a collection of mixed\nlabeled datasets. Despite demonstrating a certain level of\nzero-shot ability, MiDaS is limited by its data coverage, thus\nsuffering disastrous performance in some scenarios.\nIn this work, our goal is to build a foundation model for\nMDE capable of producing high-quality depth information\nfor any images under any circumstances. We approach this\ntarget from the perspective of dataset scaling-up. Tradition-\nally, depth datasets are created mainly by acquiring depth\ndata from sensors [18, 54], stereo matching [15], or SfM [33],\nwhich is costly, time-consuming, or even intractable in partic-\nular situations. We instead, for the first time, pay attention to\nlarge-scale unlabeled data. Compared with stereo images or\n1\narXiv:2401.10891v1  [cs.CV]  19 Jan 2024\nlabeled images from depth sensors, our used monocular unla-\nbeled images exhibit three advantages: (i) (simple and cheap\nto acquire) Monocular images exist almost everywhere, thus\nthey are easy to collect, without requiring specialized de-\nvices. (ii) (diverse) Monocular images can cover a broader\nrange of scenes, which are critical to the model generaliza-\ntion ability and scalability. (iii) (easy to annotate) We can\nsimply use a pre-trained MDE model to assign depth labels\nfor unlabeled images, which only takes a feedforward step.\nMore than efficient, this also produces denser depth maps\nthan LiDAR [18] and omits the computationally intensive\nstereo matching process.\nWe design a data engine to automatically generate depth\nannotations for unlabeled images, enabling data scaling-up\nto arbitrary scale. It collects 62M diverse and informative im-\nages from eight public large-scale datasets, e.g., SA-1B [27],\nOpen Images [30], and BDD100K [81]. We use their raw\nunlabeled images without any forms of labels. Then, in or-\nder to provide a reliable annotation tool for our unlabeled\nimages, we collect 1.5M labeled images from six public\ndatasets to train an initial MDE model. The unlabeled im-\nages are then automatically annotated and jointly learned\nwith labeled images in a self-training manner [31].\nDespite all the aforementioned advantages of monocular\nunlabeled images, it is indeed not trivial to make positive use\nof such large-scale unlabeled images [72, 89], especially in\nthe case of sufficient labeled images and strong pre-training\nmodels. In our preliminary attempts, directly combining la-\nbeled and pseudo labeled images failed to improve the base-\nline of solely using labeled images. We conjecture that, the\nadditional knowledge acquired in such a naive self-teaching\nmanner is rather limited. To address the dilemma, we pro-\npose to challenge the student model with a more difficult\noptimization target when learning the pseudo labels. The\nstudent model is enforced to seek extra visual knowledge\nand learn robust representations under various strong pertur-\nbations to better handle unseen images.\nFurthermore, there have been some works [9, 21] demon-\nstrating the benefit of an auxiliary semantic segmentation\ntask for MDE. We also follow this research line, aiming to\nequip our model with better high-level scene understanding\ncapability. However, we observed when an MDE model is\nalready powerful enough, it is hard for such an auxiliary\ntask to bring further gains. We speculate that it is due to\nsevere loss in semantic information when decoding an im-\nage into a discrete class space. Therefore, considering the\nexcellent performance of DINOv2 in semantic-related tasks,\nwe propose to maintain the rich semantic priors from it with\na simple feature alignment loss. This not only enhances the\nMDE performance, but also yields a multi-task encoder for\nboth middle-level and high-level perception tasks.\nOur contributions are summarized as follows:\n\u2022 We highlight the value of data scaling-up of massive,\ncheap, and diverse unlabeled images for MDE.\n\u2022 We point out a key practice in jointly training large-\nscale labeled and unlabeled images. Instead of learning\nraw unlabeled images directly, we challenge the model\nwith a harder optimization target for extra knowledge.\n\u2022 We propose to inherit rich semantic priors from pre-\ntrained encoders for better scene understanding, rather\nthan using an auxiliary semantic segmentation task.\n\u2022 Our model exhibits stronger zero-shot capability than\nMiDaS-BEiTL-512 [5]. Further, fine-tuned with metric\ndepth, it outperforms ZoeDepth [4] significantly.\n2. Related Work\nMonocular depth estimation (MDE). Early works [23, 36,\n50] primarily relied on handcrafted features and traditional\ncomputer vision techniques. They were limited by their re-\nliance on explicit depth cues and struggled to handle complex\nscenes with occlusions and textureless regions.\nDeep learning-based methods have revolutionized monoc-\nular depth estimation by effectively learning depth represen-\ntations from delicately annotated datasets [18, 54]. Eigen\net al. [17] first proposed a multi-scale fusion network to\nregress the depth. Following this, many works consistently\nimprove the depth estimation accuracy by carefully design-\ning the regression task as a classification task [3, 34], in-\ntroducing more priors [32, 53, 75, 82], and better objective\nfunctions [67, 77], etc. Despite the promising performance,\nthey are hard to generalize to unseen domains.\nZero-shot depth estimation. Our work belongs to this re-\nsearch line. We aim to train an MDE model with a diverse\ntraining set and thus can predict the depth for any given im-\nage. Some pioneering works [10, 66] explored this direction\nby collecting more training images, but their supervision is\nvery sparse and is only enforced on limited pairs of points.\nTo enable effective multi-dataset joint training, a mile-\nstone work MiDaS [45] utilizes an affine-invariant loss to\nignore the potentially different depth scales and shifts across\nvarying datasets. Thus, MiDaS provides relative depth infor-\nmation. Recently, some works [4, 22, 78] take a step further\nto estimate the metric depth. However, in our practice, we\nobserve such methods exhibit poorer generalization ability\nthan MiDaS, especially its latest version [5]. Besides, as\ndemonstrated by ZoeDepth [4], a strong relative depth es-\ntimation model can also work well in generalizable metric\ndepth estimation by fine-tuning with metric depth informa-\ntion. Therefore, we still follow MiDaS in relative depth\nestimation, but further strengthen it by highlighting the value\nof large-scale monocular unlabeled images.\nLeveraging unlabeled data. This belongs to the research\narea of semi-supervised learning [31, 55, 89], which is pop-\nular with various applications [70, 74]. However, existing\n2\nworks typically assume only limited images are available.\nThey rarely consider the challenging but realistic scenario\nwhere there are already sufficient labeled images but also\nlarger-scale unlabeled images. We take this challenging di-\nrection for zero-shot MDE. We demonstrate that unlabeled\nimages can significantly enhance the data coverage and thus\nimprove model generalization and robustness.\n3. Depth Anything\nOur work utilizes both labeled and unlabeled images to\nfacilitate better monocular depth estimation (MDE). For-\nmally, the labeled and unlabeled sets are denoted as Dl =\n{(xi, di)}M\ni=1 and Du = {ui}N\ni=1 respectively. We aim to\nlearn a teacher model T from Dl. Then, we utilize T to\nassign pseudo depth labels for Du. Finally, we train a stu-\ndent model S on the combination of labeled set and pseudo\nlabeled set. A brief illustration is provided in Figure 2.\n3.1. Learning Labeled Images\nThis process is similar to the training of MiDaS [5, 45].\nHowever, since MiDaS did not release its code, we first\nreproduced it. Concretely, the depth value is first transformed\ninto the disparity space by d = 1/t and then normalized\nto 0\u223c1 on each depth map. To enable multi-dataset joint\ntraining, we adopt the affine-invariant loss to ignore the\nunknown scale and shift of each sample:\nLl =\n1\nHW\nHW\nX\ni=1\n\u03c1(d\u2217\ni , di),\n(1)\nwhere d\u2217\ni and di are the prediction and ground truth, respec-\ntively. And \u03c1 is the affine-invariant mean absolute error loss:\n\u03c1(d\u2217\ni , di) = | \u02c6d\u2217\ni \u2212 \u02c6di|, where \u02c6d\u2217\ni and \u02c6di are the scaled and\nshifted versions of the prediction d\u2217\ni and ground truth di:\n\u02c6di = di \u2212 t(d)\ns(d)\n,\n(2)\nwhere t(d) and s(d) are used to align the prediction and\nground truth to have zero translation and unit scale:\nt(d) = median(d), s(d) =\n1\nHW\nHW\nX\ni=1\n|di \u2212 t(d)|.\n(3)\nTo obtain a robust monocular depth estimation model, we\ncollect 1.5M labeled images from 6 public datasets. Details\nof these datasets are listed in Table 1. We use fewer labeled\ndatasets than MiDaS v3.1 [5] (12 training datasets), because\n1) we do not use NYUv2 [54] and KITTI [18] datasets to\nensure zero-shot evaluation on them, 2) some datasets are\nnot available (anymore), e.g., Movies [45] and WSVD [60],\nand 3) some datasets exhibit poor quality, e.g., RedWeb (also\nlow resolution) [66]. Despite using fewer labeled images,\nDataset\nIndoor\nOutdoor\nLabel\n# Images\nLabeled Datasets\nBlendedMVS [76]\n\u2713\n\u2713\nStereo\n115K\nDIML [13]\n\u2713\n\u2713\nStereo\n927K\nHRWSI [67]\n\u2713\n\u2713\nStereo\n20K\nIRS [61]\n\u2713\nStereo\n103K\nMegaDepth [33]\n\u2713\nSfM\n128K\nTartanAir [62]\n\u2713\n\u2713\nStereo\n306K\nUnlabeled Datasets\nBDD100K [81]\n\u2713\nNone\n8.2M\nGoogle Landmarks [64]\n\u2713\nNone\n4.1M\nImageNet-21K [49]\n\u2713\n\u2713\nNone\n13.1M\nLSUN [80]\n\u2713\nNone\n9.8M\nObjects365 [52]\n\u2713\n\u2713\nNone\n1.7M\nOpen Images V7 [30]\n\u2713\n\u2713\nNone\n7.8M\nPlaces365 [87]\n\u2713\n\u2713\nNone\n6.5M\nSA-1B [27]\n\u2713\n\u2713\nNone\n11.1M\nTable 1. In total, our Depth Anything is trained on 1.5M labeled\nimages and 62M unlabeled images jointly.\nour easy-to-acquire and diverse unlabeled images will com-\nprehend the data coverage and greatly enhance the model\ngeneralization ability and robustness.\nFurthermore, to strengthen the teacher model T learned\nfrom these labeled images, we adopt the DINOv2 [42] pre-\ntrained weights to initialize our encoder. In practice, we\napply a pre-trained semantic segmentation model [69] to de-\ntect the sky region, and set its disparity value as 0 (farthest).\n3.2. Unleashing the Power of Unlabeled Images\nThis is the main point of our work. Distinguished from prior\nworks that laboriously construct diverse labeled datasets,\nwe highlight the value of unlabeled images in enhancing\nthe data coverage. Nowadays, we can practically build a\ndiverse and large-scale unlabeled set from the Internet or\npublic datasets of various tasks. Also, we can effortlessly\nobtain the dense depth map of monocular unlabeled images\nsimply by forwarding them to a pre-trained well-performed\nMDE model. This is much more convenient and efficient\nthan performing stereo matching or SfM reconstruction for\nstereo images or videos. We select eight large-scale public\ndatasets as our unlabeled sources for their diverse scenes.\nThey contain more than 62M images in total. The details are\nprovided in the bottom half of Table 1.\nTechnically, given the previously obtained MDE teacher\nmodel T, we make predictions on the unlabeled set Du to\nobtain a pseudo labeled set \u02c6Du:\n\u02c6Du = {(ui, T(ui))|ui \u2208 Du}N\ni=1.\n(4)\nWith the combination set Dl \u222a \u02c6\nDu of labeled images and\npseudo labeled images, we train a student model S on it.\n3\nlabeled image\nunlabeled image\nencoder\ndecoder\nmanual label\npseudo label\nencoder\nteacher\nmodel\nLiDAR, \nmatching, \nSfM, etc\nsemantic\npreservation\nlabeled prediction\nunlabeled prediction\nsup\nsup\nHRWSI: 102684_LookInStereoDotComDSCF0486\nSA1B: sa_10000139\nS\nfeature \nalignment\nFigure 2. Our pipeline. Solid line: flow of labeled images, dotted line: unlabeled images. We especially highlight the value of large-scale\nunlabeled images. The S denotes adding strong perturbations (Section 3.2). To equip our depth estimation model with rich semantic priors,\nwe enforce an auxiliary constraint between the online student model and a frozen encoder to preserve the semantic capability (Section 3.3).\nFollowing prior works [73], instead of fine-tuning S from T,\nwe re-initialize S for better performance.\nUnfortunately, in our pilot studies, we failed to gain im-\nprovements with such a self-training pipeline, which indeed\ncontradicts the observations when there are only a few la-\nbeled images [55]. We conjecture that, with already suffi-\ncient labeled images in our case, the extra knowledge ac-\nquired from additional unlabeled images is rather limited.\nEspecially considering the teacher and student share the\nsame pre-training and architecture, they tend to make similar\ncorrect or false predictions on the unlabeled set Du, even\nwithout the explicit self-training procedure.\nTo address the dilemma, we propose to challenge the stu-\ndent with a more difficult optimization target for additional\nvisual knowledge on unlabeled images. We inject strong per-\nturbations to unlabeled images during training. It compels\nour student model to actively seek extra visual knowledge\nand acquire invariant representations from these unlabeled\nimages. These advantages help our model deal with the open\nworld more robustly. We introduce two forms of perturba-\ntions: one is strong color distortions, including color jittering\nand Gaussian blurring, and the other is strong spatial dis-\ntortion, which is CutMix [83]. Despite the simplicity, the\ntwo modifications make our large-scale unlabeled images\nsignificantly improve the baseline of labeled images.\nWe provide more details about CutMix. It was originally\nproposed for image classification, and is rarely explored in\nmonocular depth estimation. We first interpolate a random\npair of unlabeled images ua and ub spatially:\nuab = ua \u2299 M + ub \u2299 (1 \u2212 M),\n(5)\nwhere M is a binary mask with a rectangle region set as 1.\nThe unlabeled loss Lu is obtained by first computing\naffine-invariant losses in valid regions defined by M and\n1 \u2212 M, respectively:\nLM\nu = \u03c1\n\u0000S(uab) \u2299 M, T(ua) \u2299 M\n\u0001\n,\n(6)\nL1\u2212M\nu\n= \u03c1\n\u0000S(uab) \u2299 (1 \u2212 M), T(ub) \u2299 (1 \u2212 M)\n\u0001\n, (7)\nwhere we omit the P and pixel subscript i for simplicity.\nThen we aggregate the two losses via weighted averaging:\nLu =\nP M\nHW LM\nu +\nP(1 \u2212 M)\nHW\nL1\u2212M\nu\n.\n(8)\nWe use CutMix with 50% probability. The unlabeled\nimages for CutMix are already strongly distorted in color,\nbut the unlabeled images fed into the teacher model T for\npseudo labeling are clean, without any distortions.\n3.3. Semantic-Assisted Perception\nThere exist some works [9, 21, 28, 71] improving depth es-\ntimation with an auxiliary semantic segmentation task. We\nbelieve that arming our depth estimation model with such\nhigh-level semantic-related information is beneficial. Be-\nsides, in our specific context of leveraging unlabeled images,\nthese auxiliary supervision signals from other tasks can also\ncombat the potential noise in our pseudo depth label.\nTherefore, we made an initial attempt by carefully assign-\ning semantic segmentation labels to our unlabeled images\nwith a combination of RAM [85] + GroundingDINO [37] +\nHQ-SAM [26] models. After post-processing, this yields a\nclass space containing 4K classes. In the joint-training stage,\nthe model is enforced to produce both depth and segmenta-\ntion predictions with a shared encoder and two individual\ndecoders. Unfortunately, after trial and error, we still could\nnot boost the performance of the original MDE model. We\nspeculated that, decoding an image into a discrete class space\nindeed loses too much semantic information. The limited\ninformation in these semantic masks is hard to further boost\nour depth model, especially when our depth model has es-\ntablished very competitive results.\nTherefore, we aim to seek more informative semantic sig-\nnals to serve as auxiliary supervision for our depth estimation\ntask. We are greatly astonished by the strong performance\nof DINOv2 models [42] in semantic-related tasks, e.g., im-\nage retrieval and semantic segmentation, even with frozen\nweights without any fine-tuning. Motivated by these clues,\nwe propose to transfer its strong semantic capability to our\n4\nMethod\nEncoder\nKITTI [18]\nNYUv2 [54]\nSintel [7]\nDDAD [20]\nETH3D [51]\nDIODE [59]\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nMiDaS v3.1 [5]\nViT-L\n0.127\n0.850\n0.048\n0.980\n0.587\n0.699\n0.251\n0.766\n0.139\n0.867\n0.075\n0.942\nDepth Anything\nViT-S\n0.080\n0.936\n0.053\n0.972\n0.464\n0.739\n0.247\n0.768\n0.127\n0.885\n0.076\n0.939\nViT-B\n0.080\n0.939\n0.046\n0.979\n0.432\n0.756\n0.232\n0.786\n0.126\n0.884\n0.069\n0.946\nViT-L\n0.076\n0.947\n0.043\n0.981\n0.458\n0.760\n0.230\n0.789\n0.127\n0.882\n0.066\n0.952\nTable 2. Zero-shot relative depth estimation. Better: AbsRel \u2193 , \u03b41 \u2191. We compare with the best model from MiDaS v3.1. Note that MiDaS\ndoes not strictly follow the zero-shot evaluation on KITTI and NYUv2, because it uses their training images. We provide three model scales\nfor different purposes, based on ViT-S (24.8M), ViT-B (97.5M), and ViT-L (335.3M), respectively. Best, second best results.\ndepth model with an auxiliary feature alignment loss. The\nfeature space is high-dimensional and continuous, thus con-\ntaining richer semantic information than discrete masks. The\nfeature alignment loss is formulated as:\nLfeat = 1 \u2212\n1\nHW\nHW\nX\ni=1\ncos(fi, f \u2032\ni),\n(9)\nwhere cos(\u00b7, \u00b7) measures the cosine similarity between two\nfeature vectors. f is the feature extracted by the depth model\nS, while f \u2032 is the feature from a frozen DINOv2 encoder.\nWe do not follow some works [19] to project the online\nfeature f into a new space for alignment, because a randomly\ninitialized projector makes the large alignment loss dominate\nthe overall loss in the early stage.\nAnother key point in feature alignment is that, semantic\nencoders like DINOv2 tend to produce similar features for\ndifferent parts of an object, e.g., car front and rear. In depth\nestimation, however, different parts or even pixels within the\nsame part, can be of varying depth. Thus, it is not beneficial\nto exhaustively enforce our depth model to produce exactly\nthe same features as the frozen encoder.\nTo solve this issue, we set a tolerance margin \u03b1 for the\nfeature alignment. If the cosine similarity of fi and f \u2032\ni has\nsurpassed \u03b1, this pixel will not be considered in our Lfeat.\nThis allows our method to enjoy both the semantic-aware\nrepresentation from DINOv2 and the part-level discrimina-\ntive representation from depth supervision. As a side effect,\nour produced encoder not only performs well in downstream\nMDE datasets, but also achieves strong results in the seman-\ntic segmentation task. It also indicates the potential of our\nencoder to serve as a universal multi-task encoder for both\nmiddle-level and high-level perception tasks.\nFinally, our overall loss is an average combination of the\nthree losses Ll, Lu, and Lfeat.\n4. Experiment\n4.1. Implementation Details\nWe adopt the DINOv2 encoder [42] for feature extraction.\nFollowing MiDaS [5, 45], we use the DPT [46] decoder for\ndepth regression. All labeled datasets are simply combined\ntogether without re-sampling. In the first stage, we train a\nteacher model on labeled images for 20 epochs. In the second\nstage of joint training, we train a student model to sweep\nacross all unlabeled images for one time. The unlabeled\nimages are annotated by a best-performed teacher model\nwith a ViT-L encoder. The ratio of labeled and unlabeled\nimages is set as 1:2 in each batch. In both stages, the base\nlearning rate of the pre-trained encoder is set as 5e-6, while\nthe randomly initialized decoder uses a 10\u00d7 larger learning\nrate. We use the AdamW optimizer and decay the learning\nrate with a linear schedule. We only apply horizontal flipping\nas our data augmentation for labeled images. The tolerance\nmargin \u03b1 for feature alignment loss is set as 0.15. For more\ndetails, please refer to our appendix.\n4.2. Zero-Shot Relative Depth Estimation\nAs aforementioned, this work aims to provide accurate\ndepth estimation for any image. Therefore, we compre-\nhensively validate the zero-shot depth estimation capability\nof our Depth Anything model on six representative unseen\ndatasets: KITTI [18], NYUv2 [54], Sintel [7], DDAD [20],\nETH3D [51], and DIODE [59]. We compare with the best\nDPT-BEiTL-512 model from the latest MiDaS v3.1 [5], which\nuses more labeled images than us. As shown in Table 2,\nboth with a ViT-L encoder, our Depth Anything surpasses\nthe strongest MiDaS model tremendously across extensive\nscenes in terms of both the AbsRel (absolute relative error:\n|d\u2217 \u2212d|/d) and \u03b41 (percentage of max(d\u2217/d, d/d\u2217) < 1.25)\nmetrics. For example, when tested on the well-known au-\ntonomous driving dataset DDAD [20], we improve the Ab-\nsRel (\u2193) from 0.251 \u2192 0.230 and improve the \u03b41 (\u2191) from\n0.766 \u2192 0.789.\nBesides, our ViT-B model is already clearly superior to\nthe MiDaS based on a much larger ViT-L. Moreover, our\nViT-S model, whose scale is less than 1/10 of the MiDaS\nmodel, even outperforms MiDaS on several unseen datasets,\nincluding Sintel, DDAD, and ETH3D. The performance\nadvantage of these small-scale models demonstrates their\ngreat potential in computationally-constrained scenarios.\nIt is also worth noting that, on the most widely used MDE\n5\nMethod\nHigher is better \u2191\nLower is better \u2193\n\u03b41\n\u03b42\n\u03b43\nAbsRel\nRMSE\nlog10\nAdaBins [3]\n0.903\n0.984\n0.997\n0.103\n0.364\n0.044\nDPT [46]\n0.904\n0.988\n0.998\n0.110\n0.357\n0.045\nP3Depth [43]\n0.898\n0.981\n0.996\n0.104\n0.356\n0.043\nSwinV2-L [39]\n0.949\n0.994\n0.999\n0.083\n0.287\n0.035\nAiT [41]\n0.954\n0.994\n0.999\n0.076\n0.275\n0.033\nVPD [86]\n0.964\n0.995\n0.999\n0.069\n0.254\n0.030\nZoeDepth\u2217 [4]\n0.951\n0.994\n0.999\n0.077\n0.282\n0.033\nOurs\n0.984\n0.998\n1.000\n0.056\n0.206\n0.024\nTable 3. Fine-tuning and evaluating on NYUv2 [54] with our\npre-trained MDE encoder. We highlight best, second best results,\nas well as most discriminative metrics. \u2217: Reproduced by us.\nbenchmarks KITTI and NYUv2, although MiDaS v3.1 uses\nthe corresponding training images (not zero-shot anymore),\nour Depth Anything is still evidently superior to it without\ntraining with any KITTI or NYUv2 images, e.g., 0.127 vs.\n0.076 in AbsRel and 0.850 vs. 0.947 in \u03b41 on KITTI.\n4.3. Fine-tuned to Metric Depth Estimation\nApart from the impressive performance in zero-shot relative\ndepth estimation, we further examine our Depth Anything\nmodel as a promising weight initialization for downstream\nmetric depth estimation. We initialize the encoder of down-\nstream MDE models with our pre-trained encoder parameters\nand leave the decoder randomly initialized. The model is\nfine-tuned with correponding metric depth information. In\nthis part, we use our ViT-L encoder for fine-tuning.\nWe examine two representative scenarios: 1) in-domain\nmetric depth estimation, where the model is trained and\nevaluated on the same domain (Section 4.3.1), and 2) zero-\nshot metric depth estimation, where the model is trained on\none domain, e.g., NYUv2 [54], but evaluated in different\ndomains, e.g., SUN RGB-D [56] (Section 4.3.2).\n4.3.1\nIn-Domain Metric Depth Estimation\nAs shown in Table 3 of NYUv2 [54], our model outperforms\nthe previous best method VPD [86] remarkably, improving\nthe \u03b41 (\u2191) from 0.964 \u2192 0.984 and AbsRel (\u2193) from 0.069\nto 0.056. Similar improvements can be observed in Table 4\nof the KITTI dataset [18]. We improve the \u03b41 (\u2191) on KITTI\nfrom 0.978 \u2192 0.982. It is worth noting that we adopt the\nZoeDepth framework for this scenario with a relatively ba-\nsic depth model, and we believe our results can be further\nenhanced if equipped with more advanced architectures.\n4.3.2\nZero-Shot Metric Depth Estimation\nWe follow ZoeDepth [4] to conduct zero-shot metric depth\nestimation. ZoeDepth fine-tunes the MiDaS pre-trained en-\nMethod\nHigher is better \u2191\nLower is better \u2193\n\u03b41\n\u03b42\n\u03b43\nAbsRel\nRMSE\nRMSE log\nAdaBins [3]\n0.964\n0.995\n0.999\n0.058\n2.360\n0.088\nDPT [46]\n0.959\n0.995\n0.999\n0.062\n2.573\n0.092\nP3Depth [43]\n0.953\n0.993\n0.998\n0.071\n2.842\n0.103\nNeWCRFs [82]\n0.974\n0.997\n0.999\n0.052\n2.129\n0.079\nSwinV2-L [39]\n0.977\n0.998\n1.000\n0.050\n1.966\n0.075\nNDDepth [53]\n0.978\n0.998\n0.999\n0.050\n2.025\n0.075\nGEDepth [75]\n0.976\n0.997\n0.999\n0.048\n2.044\n0.076\nZoeDepth\u2217 [4]\n0.971\n0.996\n0.999\n0.054\n2.281\n0.082\nOurs\n0.982\n0.998\n1.000\n0.046\n1.896\n0.069\nTable 4. Fine-tuning and evaluating on KITTI [18] with our\npre-trained MDE encoder. \u2217: Reproduced by us.\ncoder with metric depth information from NYUv2 [54] (for\nindoor scenes) or KITTI [18] (for outdoor scenes). There-\nfore, we simply replace the MiDaS encoder with our bet-\nter Depth Anything encoder, leaving other components un-\nchanged. As shown in Table 5, across a wide range of unseen\ndatasets of indoor and outdoor scenes, our Depth Anything\nresults in a better metric depth estimation model than the\noriginal ZoeDepth based on MiDaS.\n4.4. Fine-tuned to Semantic Segmentation\nIn our method, we design our MDE model to inherit the\nrich semantic priors from a pre-trained encoder via a sim-\nple feature alignment constraint. Here, we examine the\nsemantic capability of our MDE encoder. Specifically, we\nfine-tune our MDE encoder to downstream semantic segmen-\ntation datasets. As exhibited in Table 7 of the Cityscapes\ndataset [15], our encoder from large-scale MDE training\n(86.2 mIoU) is superior to existing encoders from large-scale\nImageNet-21K pre-training, e.g., Swin-L [38] (84.3) and\nConvNeXt-XL [40] (84.6). Similar observations hold on the\nADE20K dataset [88] in Table 8. We improve the previous\nbest result from 58.3 \u2192 59.4.\nWe hope to highlight that, witnessing the superiority of\nour pre-trained encoder on both monocular depth estimation\nand semantic segmentation tasks, we believe it has great\npotential to serve as a generic multi-task encoder for both\nmiddle-level and high-level visual perception systems.\n4.5. Ablation Studies\nUnless otherwise specified, we use the ViT-L encoder for\nour ablation studies here.\nZero-shot transferring of each training dataset. In Ta-\nble 6, we provide the zero-shot transferring performance of\neach training dataset, which means that we train a relative\nMDE model on one training set and evaluate it on the six\nunseen datasets. With these results, we hope to offer more\ninsights for future works that similarly aim to build a general\n6\nMethod\nSUN RGB-D [56]\niBims-1 [29]\nHyperSim [48]\nVirtual KITTI 2 [8]\nDIODE Outdoor [59]\nAbsRel (\u2193)\n\u03b41 (\u2191)\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nZoeDepth [4]\n0.520\n0.545\n0.169\n0.656\n0.407\n0.302\n0.106\n0.844\n0.814\n0.237\nDepth Anything\n0.500\n0.660\n0.150\n0.714\n0.363\n0.361\n0.085\n0.913\n0.794\n0.288\nTable 5. Zero-shot metric depth estimation. The first three test sets in the header are indoor scenes, while the last two are outdoor scenes.\nFollowing ZoeDepth, we use the model trained on NYUv2 for indoor generalization, while use the model trained on KITTI for outdoor\nevaluation. For fair comparisons, we report the ZoeDepth results reproduced in our environment.\nTraining set\nKITTI [18]\nNYUv2 [54]\nSintel [7]\nDDAD [20]\nETH3D [51]\nDIODE [59]\nMean\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nBlendedMVS [76]\n0.089\n0.918\n0.068\n0.958\n0.556\n0.689\n0.305\n0.731\n0.148\n0.845\n0.092\n0.921\n0.210\n0.844\nDIML [13]\n0.099\n0.907\n0.055\n0.969\n0.573\n0.722\n0.381\n0.657\n0.142\n0.859\n0.107\n0.908\n0.226\n0.837\nHRWSI [67]\n0.095\n0.917\n0.062\n0.966\n0.502\n0.731\n0.270\n0.750\n0.186\n0.775\n0.087\n0.935\n0.200\n0.846\nIRS [61]\n0.105\n0.892\n0.057\n0.970\n0.568\n0.714\n0.328\n0.691\n0.143\n0.845\n0.088\n0.926\n0.215\n0.840\nMegaDepth [33]\n0.217\n0.741\n0.071\n0.953\n0.632\n0.660\n0.479\n0.566\n0.142\n0.852\n0.104\n0.910\n0.274\n0.780\nTartanAir [62]\n0.088\n0.920\n0.061\n0.964\n0.602\n0.723\n0.332\n0.690\n0.160\n0.818\n0.088\n0.928\n0.222\n0.841\nAll labeled data\n0.085\n0.934\n0.053\n0.971\n0.492\n0.748\n0.245\n0.771\n0.134\n0.874\n0.070\n0.945\n0.180\n0.874\nTable 6. Examine the zero-shot transferring performance of each labeled training set (left) to six unseen datasets (top). Better performance:\nAbsRel \u2193 , \u03b41 \u2191. We highlight the best, second, and third best results for each test dataset in bold, underline, and italic, respectively.\nMethod\nEncoder\nmIoU (s.s.)\nm.s.\nSegmenter [57]\nViT-L [16]\n-\n82.2\nSegFormer [69]\nMiT-B5 [69]\n82.4\n84.0\nMask2Former [12]\nSwin-L [38]\n83.3\n84.3\nOneFormer [24]\nSwin-L [38]\n83.0\n84.4\nOneFormer [24]\nConvNeXt-XL [40]\n83.6\n84.6\nDDP [25]\nConvNeXt-L [40]\n83.2\n83.9\nOurs\nViT-L [16]\n84.8\n86.2\nTable 7. Transferring our MDE pre-trained encoder to Cityscapes\nfor semantic segmentation. We do not use Mapillary [1] for pre-\ntraining. s.s./m.s.: single-/multi-scale evaluation.\nmonocular depth estimation system. Among the six training\ndatasets, HRWSI [67] fuels our model with the strongest\ngeneralization ability, even though it only contains 20K im-\nages. This indicates the data diversity counts a lot, which\nis well aligned with our motivation to utilize unlabeled im-\nages. Some labeled datasets may not perform very well, e.g.,\nMegaDepth [33], however, it has its own preferences that\nare not reflected in these six test datasets. For example, we\nfind models trained with MegaDepth data are specialized at\nestimating the distance of ultra-remote buildings (Figure 1),\nwhich will be very beneficial for aerial vehicles.\nEffectiveness of 1) challenging the student model when\nlearning unlabeled images, and 2) semantic constraint.\nAs shown in Table 9, simply adding unlabeled images with\npseudo labels does not necessarily bring gains to our model,\nMethod\nEncoder\nmIoU\nSegmenter [57]\nViT-L [16]\n51.8\nSegFormer [69]\nMiT-B5 [69]\n51.0\nMask2Former [12]\nSwin-L [38]\n56.4\nUperNet [68]\nBEiT-L [2]\n56.3\nViT-Adapter [11]\nBEiT-L [2]\n58.3\nOneFormer [24]\nSwin-L [38]\n57.4\nOneFormer [24]\nConNeXt-XL [40]\n57.4\nOurs\nViT-L [16]\n59.4\nTable 8. Transferring our MDE encoder to ADE20K for semantic\nsegmentation. We use Mask2Former as our segmentation model.\nsince the labeled images are already sufficient. However,\nwith strong perturbations (S) applied to unlabeled images\nduring re-training, the student model is challenged to seek\nadditional visual knowledge and learn more robust repre-\nsentations. Consequently, the large-scale unlabeled images\nenhance the model generalization ability significantly.\nMoreover, with our used semantic constraint Lfeat, the\npower of unlabeled images can be further amplified for the\ndepth estimation task. More importantly, as emphasized in\nSection 4.4, this auxiliary constraint also enables our trained\nencoder to serve as a key component in a multi-task visual\nsystem for both middle-level and high-level perception.\nComparison with MiDaS trained encoder in downstream\ntasks. Our Depth Anything model has exhibited stronger\nzero-shot capability than MiDaS [5, 45]. Here, we further\n7\nFigure 3. Qualitative results on six unseen datasets.\nLl Lu\nS\nLfeat\nKI\nNY\nSI\nDD\nET\nDI\n\u2713\n0.085 0.053 0.492 0.245 0.134 0.070\n\u2713\n\u2713\n0.085 0.054 0.481 0.242 0.138 0.073\n\u2713\n\u2713\n\u2713\n0.081 0.048 0.469 0.235 0.134 0.068\n\u2713\n\u2713\n\u2713\n\u2713\n0.076 0.043 0.458 0.230 0.127 0.066\nTable 9. Ablation studies of: 1) challenging the student with strong\nperturbations (S) when learning unlabeled images, and 2) semantic\nconstraint (Lfeat). Limited by space, we only report the AbsRel\n(\u2193) metric, and shorten the dataset name with its first two letters.\nMethod\nNYUv2\nKITTI\nCityscapes ADE20K\nAbsRel\n\u03b41\nAbsRel\n\u03b41\nmIoU\nmIoU\nMiDaS\n0.077\n0.951\n0.054\n0.971\n82.1\n52.4\nOurs\n0.056\n0.984\n0.046\n0.982\n84.8\n59.4\nTable 10. Comparison between our trained encoder and MiDaS [5]\ntrained encoder in terms of downstream fine-tuning performance.\nBetter performance: AbsRel \u2193 , \u03b41 \u2191 , mIoU \u2191 .\ncompare our trained encoder with MiDaS v3.1 [5] trained\nencoder in terms of the downstream fine-tuning performance.\nAs demonstrated in Table 10, on both the downstream depth\nestimation task and semantic segmentation task, our pro-\nduced encoder outperforms the MiDaS encoder remarkably,\ne.g., 0.951 vs. 0.984 in the \u03b41 metric on NYUv2, and 52.4\nvs. 59.4 in the mIoU metric on ADE20K.\nComparison with DINOv2 in downstream tasks. We have\ndemonstrated the superiority of our trained encoder when\nfine-tuned to downstream tasks. Since our finally produced\nencoder (from large-scale MDE training) is fine-tuned from\nDINOv2 [42], we compare our encoder with the original\nDINOv2 encoder in Table 11. It can be observed that our\nencoder performs better than the original DINOv2 encoder\nin both the downstream metric depth estimation task and\nsemantic segmentation task. Although the DINOv2 weight\nhas provided a very strong initialization (also much better\nthan the MiDaS encoder as reported in Table 10), our large-\nscale and high-quality MDE training can further enhance it\nOurs\nMiDaS\nOurs\nMiDaS\nFigure 4. We compare our depth prediction with MiDaS. Meantime,\nwe use ControlNet to synthesize new images from the depth map\n(the last row). First row: input image, second row: depth prediction.\nEncoder\nNYUv2\nKITTI\nADE20K\nAbsRel (\u2193)\n\u03b41 (\u2191)\nAbsRel\n\u03b41\nmIoU (\u2191)\nDINOv2\n0.066\n0.973\n0.058\n0.971\n58.8\nOurs\n0.056\n0.984\n0.046\n0.982\n59.4\nTable 11. Comparison between the original DINOv2 and our pro-\nduced encoder in terms of downstream fine-tuning performance.\nimpressively in downstream transferring performance.\n4.6. Qualitative Results\nWe visualize our model predictions on the six unseen datasets\nin Figure 3. Our model is robust to test images from various\ndomains. In addition, we compare our model with MiDaS\nin Figure 4. We also attempt to synthesis new images con-\nditioned on the predicted depth maps with ControlNet [84].\nOur model produces more accurate depth estimation than\nMiDaS, as well as better synthesis results, although the Con-\ntrolNet is trained with MiDaS depth. For more accurate\nsynthesis, we have also re-trained a better depth-conditioned\nControlNet based on our Depth Anything, aiming to pro-\nvide better control signals for image synthesis and video\nediting. Please refer to our project page or the following\nsupplementary material for more qualitative results,\n5. Conclusion\nIn this work, we present Depth Anything, a highly practical\nsolution to robust monocular depth estimation. Different\nfrom prior arts, we especially highlight the value of cheap\nand diverse unlabeled images. We design two simple yet\nhighly effective strategies to fully exploit their value: 1)\nposing a more challenging optimization target when learning\nunlabeled images, and 2) preserving rich semantic priors\nfrom pre-trained models. As a result, our Depth Anything\nmodel exhibits excellent zero-shot depth estimation ability,\nand also serves as a promising initialization for downstream\nmetric depth estimation and semantic segmentation tasks.\n8\nDepth Anything: Unleashing the Power of Large-Scale Unlabeled Data\nSupplementary Material\n6. More Implementation Details\nWe resize the shorter side of all images to 518 and keep the\noriginal aspect ratio. All images are cropped to 518\u00d7518\nduring training. During inference, we do not crop images\nand only ensure both sides are multipliers of 14, since the\npre-defined patch size of DINOv2 encoders [42] is 14. Eval-\nuation is performed at the original resolution by interpolating\nthe prediction. Following MiDaS [5, 45], in zero-shot eval-\nuation, the scale and shift of our prediction are manually\naligned with the ground truth.\nWhen fine-tuning our pre-trained encoder to metric depth\nestimation, we adopt the ZoeDepth codebase [4]. We merely\nreplace the original MiDaS-based encoder with our stronger\nDepth Anything encoder, with a few hyper-parameters mod-\nified. Concretely, the training resolution is 392\u00d7518 on\nNYUv2 [54] and 384\u00d7768 on KITTI [18] to match the\npatch size of our encoder. The encoder learning rate is\nset as 1/50 of the learning rate of the randomly initialized\ndecoder, which is much smaller than the 1/10 adopted for\nMiDaS encoder, due to our strong initialization. The batch\nsize is 16 and the model is trained for 5 epochs.\nWhen fine-tuning our pre-trained encoder to semantic seg-\nmentation, we use the MMSegmentation codebase [14]. The\ntraining resolution is set as 896\u00d7896 on both ADE20K [88]\nand Cityscapes [15]. The encoder learning rate is set as\n3e-6 and the decoder learning rate is 10\u00d7 larger. We use\nMask2Former [12] as our semantic segmentation model. The\nmodel is trained for 160K iterations on ADE20K and 80K\niterations on Cityscapes both with batch size 16, without\nany COCO [35] or Mapillary [1] pre-training. Other training\nconfigurations are the same as the original codebase.\n7. More Ablation Studies\nAll ablation studies here are conducted on the ViT-S model.\nThe necessity of tolerance margin for feature alignment.\nAs shown in Table 12, the gap between the tolerance margin\nof 0 and 0.15 or 0.30 clearly demonstrates the necessity of\nthis design (mean AbsRel: 0.188 vs. 0.175).\nApplying feature alignment to labeled data. Previously,\nwe enforce the feature alignment loss Lfeat on unlabeled\ndata. Indeed, it is technically feasible to also apply this\nconstraint to labeled data. In Table 13, apart from applying\nLfeat on unlabeled data, we explore to apply it to labeled\ndata. We find that adding this auxiliary optimization target\nto labeled data is not beneficial to our baseline that does not\ninvolve any feature alignment (their mean AbsRel values are\nalmost the same: 0.180 vs. 0.179). We conjecture that this is\n\u03b1\nKITTI NYU Sintel DDAD ETH3D DIODE Mean\n0.00\n0.085\n0.055 0.523\n0.250\n0.134\n0.079\n0.188\n0.15\n0.080\n0.053 0.464\n0.247\n0.127\n0.076\n0.175\n0.30\n0.079\n0.054 0.482\n0.248\n0.127\n0.077\n0.178\nTable 12. Ablation studies on different values of the tolerance\nmargin \u03b1 for the feature alignment loss Lfeat. Limited by space,\nwe only report the AbsRel (\u2193) metric here.\nLfeat\nUnseen datasets (AbsRel \u2193)\nMean\nU\nL\nKITTI NYU Sintel DDAD ETH3D DIODE\n0.083\n0.055 0.478\n0.249\n0.133\n0.080\n0.180\n\u2713\n0.080\n0.053 0.464\n0.247\n0.127\n0.076\n0.175\n\u2713\n0.084\n0.054 0.472\n0.252\n0.133\n0.081\n0.179\nTable 13. Ablation studies of applying our feature alignment loss\nLfeat to unlabeled data (U) or labeled data (L).\nbecause the labeled data has relatively higher-quality depth\nannotations. The involvement of semantic loss may interfere\nwith the learning of these informative manual labels. In com-\nparison, our pseudo labels are noisier and less informative.\nTherefore, introducing the auxiliary constraint to unlabeled\ndata can combat the noise in pseudo depth labels, as well as\narm our model with semantic capability.\n8. Limitations and Future Works\nCurrently, the largest model size is only constrained to ViT-\nLarge [16]. Therefore, in the future, we plan to further scale\nup the model size from ViT-Large to ViT-Giant, which is\nalso well pre-trained by DINOv2 [42]. We can train a more\npowerful teacher model with the larger model, producing\nmore accurate pseudo labels for smaller models to learn, e.g.,\nViT-L and ViT-B. Furthermore, to facilitate real-world ap-\nplications, we believe the widely adopted 512\u00d7512 training\nresolution is not enough. We plan to re-train our model on a\nlarger resolution of 700+ or even 1000+.\n9. More Qualitative Results\nPlease refer to the following pages for comprehensive quali-\ntative results on six unseen test sets (Figure 5 for KITTI [18],\nFigure 6 for NYUv2 [54], Figure 7 for Sintel [7], Figure 8\nfor DDAD [20], Figure 9 for ETH3D [51], and Figure 10\nfor DIODE [59]). We compare our model with the strongest\nMiDaS model [5], i.e., DPT-BEiTL-512. Our model exhibits\nhigher depth estimation accuracy and stronger robustness.\n9\nInput image\nOur prediction\nMiDaS v3.1 prediction\nFigure 5. Qualitative results on KITTI. Due to the extremely sparse ground truth which is hard to visualize, we here compare our prediction\nwith the most advanced MiDaS v3.1 [5] prediction. The brighter color denotes the closer distance.\n10\nInput image\nOur prediction\nMiDaS v3.1 prediction\nFigure 6. Qualitative results on NYUv2. It is worth noting that MiDaS [5] uses NYUv2 training data (not zero-shot), while we do not.\n11\nInput image\nOur prediction\nMiDaS v3.1 prediction\nFigure 7. Qualitative results on Sintel.\n12\nInput image\nOur prediction\nMiDaS v3.1 prediction\nFigure 8. Qualitative results on DDAD.\n13\nInput image\nOur prediction\nMiDaS v3.1 prediction\nFigure 9. Qualitative results on ETH3D.\n14\nInput image\nOur prediction\nMiDaS v3.1 prediction\nFigure 10. Qualitative results on DIODE.\n15\nReferences\n[1] Manuel L\u00b4opez Antequera, Pau Gargallo, Markus Hofinger,\nSamuel Rota Bul`o, Yubin Kuang, and Peter Kontschieder.\nMapillary planet-scale depth dataset. In ECCV, 2020. 7, 9\n[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers. In ICLR, 2022. 7\n[3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\nAdabins: Depth estimation using adaptive bins. In CVPR,\n2021. 2, 6\n[4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\nand Matthias M\u00a8uller. Zoedepth: Zero-shot transfer by com-\nbining relative and metric depth. arXiv:2302.12288, 2023. 2,\n6, 7, 9\n[5] Reiner Birkl, Diana Wofk, and Matthias M\u00a8uller. Midas v3.\n1\u2013a model zoo for robust monocular relative depth estimation.\narXiv:2307.14460, 2023. 2, 3, 5, 7, 8, 9, 10, 11\n[6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S Bern-\nstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,\net al. On the opportunities and risks of foundation models.\narXiv:2108.07258, 2021. 1\n[7] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J\nBlack. A naturalistic open source movie for optical flow\nevaluation. In ECCV, 2012. 5, 7, 9\n[8] Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-\ntual kitti 2. arXiv:2001.10773, 2020. 7\n[9] Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-\nChiang Frank Wang.\nTowards scene understanding: Un-\nsupervised monocular depth estimation with semantic-aware\nrepresentation. In CVPR, 2019. 2, 4\n[10] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\nimage depth perception in the wild. In NeurIPS, 2016. 2\n[11] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\ndense predictions. In ICLR, 2023. 7\n[12] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander\nKirillov, and Rohit Girdhar. Masked-attention mask trans-\nformer for universal image segmentation. In CVPR, 2022. 7,\n9\n[13] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon\nSohn. Diml/cvl rgb-d dataset: 2m rgb-d images of natural\nindoor and outdoor scenes. arXiv:2110.11590, 2021. 3, 7\n[14] MMSegmentation\nContributors.\nMMSegmenta-\ntion:\nOpenmmlab\nsemantic\nsegmentation\ntoolbox\nand benchmark.\nhttps : / / github . com / open -\nmmlab/mmsegmentation, 2020. 9\n[15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\nStefan Roth, and Bernt Schiele. The cityscapes dataset for\nsemantic urban scene understanding. In CVPR, 2016. 1, 6, 9\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 7,\n9\n[17] David Eigen, Christian Puhrsch, and Rob Fergus. Depth\nmap prediction from a single image using a multi-scale deep\nnetwork. In NeurIPS, 2014. 2\n[18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. IJRR, 2013.\n1, 2, 3, 5, 6, 7, 9\n[19] Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\nto self-supervised learning. In NeurIPS, 2020. 5\n[20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos,\nand Adrien Gaidon. 3d packing for self-supervised monocular\ndepth estimation. In CVPR, 2020. 5, 7, 9\n[21] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien\nGaidon. Semantically-guided representation learning for self-\nsupervised monocular depth. In ICLR, 2020. 2, 4\n[22] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus,,\nand Adrien Gaidon. Towards zero-shot scale-aware monocu-\nlar depth estimation. In ICCV, 2023. 2\n[23] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recover-\ning surface layout from an image. IJCV, 2007. 2\n[24] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita\nOrlov, and Humphrey Shi. Oneformer: One transformer to\nrule universal image segmentation. In CVPR, 2023. 7\n[25] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,\nZhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp:\nDiffusion model for dense visual prediction. In ICCV, 2023.\n7\n[26] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing\nTai, Chi-Keung Tang, and Fisher Yu. Segment anything in\nhigh quality. In NeurIPS, 2023. 4\n[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 1, 2, 3\n[28] Marvin Klingner, Jan-Aike Term\u00a8ohlen, Jonas Mikolajczyk,\nand Tim Fingscheidt. Self-supervised monocular depth es-\ntimation: Solving the dynamic object problem by semantic\nguidance. In ECCV, 2020. 4\n[29] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco\nKorner. Evaluation of cnn-based single-image depth estima-\ntion methods. In ECCVW, 2018. 7\n[30] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings,\nIvan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov,\nMatteo Malloci, Alexander Kolesnikov, et al. The open im-\nages dataset v4: Unified image classification, object detection,\nand visual relationship detection at scale. IJCV, 2020. 2, 3\n[31] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient\nsemi-supervised learning method for deep neural networks.\nIn ICMLW, 2013. 2\n[32] Bo Li, Chunhua Shen, Yuchao Dai, Anton Van Den Hen-\ngel, and Mingyi He. Depth and surface normal estimation\nfrom monocular images using regression on deep features and\nhierarchical crfs. In CVPR, 2015. 2\n[33] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In CVPR, 2018.\n1, 3, 7\n16\n[34] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.\nBinsformer: Revisiting adaptive bins for monocular depth\nestimation. arXiv:2204.00987, 2022. 2\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 1, 9\n[36] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and\nWilliam T Freeman. Sift flow: Dense correspondence across\ndifferent scenes. In ECCV, 2008. 2\n[37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv:2303.05499,\n2023. 4\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 6, 7\n[39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\nSwin transformer v2: Scaling up capacity and resolution. In\nCVPR, 2022. 6\n[40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In CVPR, 2022. 6, 7\n[41] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang\nGeng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying\noutput space of visual tasks via soft token. In ICCV, 2023. 6\n[42] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy Vo,\nMarc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel\nHaziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:\nLearning robust visual features without supervision. TMLR,\n2023. 3, 4, 5, 8, 9\n[43] Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and\nLuc Van Gool. P3depth: Monocular depth estimation with a\npiecewise planarity prior. In CVPR, 2022. 6\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nIn ICML, 2021. 1\n[45] Ren\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. TPAMI, 2020. 1, 2, 3, 5, 7, 9\n[46] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In ICCV, 2021. 5,\n6\n[47] Alex Rasla and Michael Beyeler. The relative importance\nof depth cues and semantic edges for indoor mobility using\nsimulated prosthetic vision in immersive virtual reality. In\nVRST, 2022. 1\n[48] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit\nKumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,\nand Joshua M Susskind. Hypersim: A photorealistic synthetic\ndataset for holistic indoor scene understanding. In ICCV,\n2021. 7\n[49] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large scale\nvisual recognition challenge. IJCV, 2015. 3\n[50] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:\nLearning 3d scene structure from a single still image. TPAMI,\n2008. 2\n[51] Thomas Schops, Johannes L Schonberger, Silvano Galliani,\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and An-\ndreas Geiger. A multi-view stereo benchmark with high-\nresolution images and multi-camera videos. In CVPR, 2017.\n5, 7, 9\n[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\nlarge-scale, high-quality dataset for object detection. In ICCV,\n2019. 3\n[53] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and\nZhengguo Li. Nddepth: Normal-distance assisted monocular\ndepth estimation. In ICCV, 2023. 2, 6\n[54] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus. Indoor segmentation and support inference from rgbd\nimages. In ECCV, 2012. 1, 2, 3, 5, 6, 7, 9\n[55] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\nZhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,\nAlexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying\nsemi-supervised learning with consistency and confidence. In\nNeurIPS, 2020. 2, 4\n[56] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.\nSun rgb-d: A rgb-d scene understanding benchmark suite. In\nCVPR, 2015. 6, 7\n[57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmentation.\nIn ICCV, 2021. 7\n[58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models.\narXiv:2302.13971, 2023. 1\n[59] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,\nHaochen Wang, Falcon Z Dai, Andrea F Daniele, Mo-\nhammadreza Mostajabi, Steven Basart, Matthew R Walter,\net al. Diode: A dense indoor and outdoor depth dataset.\narXiv:1908.00463, 2019. 5, 7, 9\n[60] Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver\nWang. Web stereo video supervision for depth prediction\nfrom dynamic scenes. In 3DV, 2019. 3\n[61] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiy-\nong Zhao, and Xiaowen Chu. Irs: A large naturalistic indoor\nrobotics stereo dataset to train deep models for disparity and\nsurface normal estimation. In ICME, 2021. 3, 7\n[62] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,\nYuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and\nSebastian Scherer. Tartanair: A dataset to push the limits of\nvisual slam. In IROS, 2020. 3, 7\n[63] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariha-\nran, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar\nfrom visual depth estimation: Bridging the gap in 3d object\ndetection for autonomous driving. In CVPR, 2019. 1\n17\n[64] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\nGoogle landmarks dataset v2-a large-scale benchmark for\ninstance-level recognition and retrieval. In CVPR, 2020. 3\n[65] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman,\nand Vivienne Sze. Fastdepth: Fast monocular depth estima-\ntion on embedded systems. In ICRA, 2019. 1\n[66] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,\nRuibo Li, and Zhenbo Luo. Monocular relative depth per-\nception with web stereo data supervision. In CVPR, 2018. 2,\n3\n[67] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,\nand Zhiguo Cao. Structure-guided ranking loss for single\nimage depth prediction. In CVPR, 2020. 2, 3, 7\n[68] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understanding.\nIn ECCV, 2018. 7\n[69] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo.\nSegformer: Simple and\nefficient design for semantic segmentation with transformers.\nIn NeurIPS, 2021. 3, 7\n[70] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan\nWang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end\nsemi-supervised object detection with soft teacher. In ICCV,\n2021. 2\n[71] Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam\nLim, and Antonio Torralba. Mtformer: Multi-task learning\nvia transformer and cross-task reasoning. In ECCV, 2022. 4\n[72] I Zeki Yalniz, Herv\u00b4e J\u00b4egou, Kan Chen, Manohar Paluri, and\nDhruv Mahajan. Billion-scale semi-supervised learning for\nimage classification. arXiv:1905.00546, 2019. 2\n[73] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao.\nSt++: Make self-training work better for semi-supervised\nsemantic segmentation. In CVPR, 2022. 4\n[74] Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and\nYinghuan Shi. Revisiting weak-to-strong consistency in semi-\nsupervised semantic segmentation. In CVPR, 2023. 2\n[75] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren. Gedepth:\nGround embedding for monocular depth estimation. In ICCV,\n2023. 2, 6\n[76] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\nLei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-\nscale dataset for generalized multi-view stereo networks. In\nCVPR, 2020. 3, 7\n[77] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-\nforcing geometric constraints of virtual normal for depth pre-\ndiction. In ICCV, 2019. 2\n[78] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaix-\nuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\nTowards zero-shot metric 3d prediction from a single image.\nIn ICCV, 2023. 2\n[79] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Ge-\noff Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Q\nWeinberger. Pseudo-lidar++: Accurate depth for 3d object\ndetection in autonomous driving. In ICLR, 2020. 1\n[80] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\nlarge-scale image dataset using deep learning with humans in\nthe loop. arXiv:1506.03365, 2015. 3\n[81] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying\nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\nrell. Bdd100k: A diverse driving dataset for heterogeneous\nmultitask learning. In CVPR, 2020. 2, 3\n[82] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and\nPing Tan. New crfs: Neural window fully-connected crfs for\nmonocular depth estimation. arXiv:2203.01502, 2022. 2, 6\n[83] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In ICCV, 2019. 4\n[84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV, 2023. 8\n[85] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\nZhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian\nLi, Shilong Liu, et al. Recognize anything: A strong image\ntagging model. arXiv:2306.03514, 2023. 4\n[86] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie\nZhou, and Jiwen Lu. Unleashing text-to-image diffusion\nmodels for visual perception. In ICCV, 2023. 6\n[87] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. TPAMI, 2017. 3\n[88] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-\nriuso, and Antonio Torralba. Scene parsing through ade20k\ndataset. In CVPR, 2017. 6, 9\n[89] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-\niao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-\ntraining and self-training. In NeurIPS, 2020. 2\n18\n"
  },
  {
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
    "link": "https://arxiv.org/pdf/2401.10774.pdf",
    "upvote": "47",
    "text": "MEDUSA: Simple LLM Inference Acceleration\nFramework with Multiple Decoding Heads\nTianle Cai\u22171,2, Yuhong Li\u22173, Zhengyang Geng4, Hongwu Peng5,\nJason D. Lee1, Deming Chen3, Tri Dao1,2\n1Princeton University, 2Together AI, 3University of Illinois Urbana-Champaign,\n4Carnegie Mellon University, 5University of Connecticut\nAbstract\nThe inference process in Large Language Models (LLMs) is often limited due to\nthe absence of parallelism in the auto-regressive decoding process, resulting in\nmost operations being restricted by the memory bandwidth of accelerators. While\nmethods such as speculative decoding have been suggested to address this issue,\ntheir implementation is impeded by the challenges associated with acquiring and\nmaintaining a separate draft model. In this paper, we present MEDUSA, an ef-\nficient method that augments LLM inference by adding extra decoding heads to\npredict multiple subsequent tokens in parallel. Using a tree-based attention mech-\nanism, MEDUSA constructs multiple candidate continuations and verifies them si-\nmultaneously in each decoding step. By leveraging parallel processing, MEDUSA\nintroduces only minimal overhead in terms of single-step latency while substan-\ntially reducing the number of decoding steps required.\nWe present two levels of fine-tuning procedures for MEDUSA to meet the needs\nof different use cases:\n\u2022 MEDUSA-1: MEDUSA is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference acceleration.\n\u2022 MEDUSA-2: MEDUSA is fine-tuned together with the backbone LLM, enabling\nbetter prediction accuracy of MEDUSA heads and higher speedup but needing a\nspecial training recipe that preserves the backbone model\u2019s capabilities.\nMoreover, we propose several extensions that improve or expand the utility of\nMEDUSA, including a self-distillation to handle situations where no training data\nis available and a typical acceptance scheme to boost the acceptance rate while\nmaintaining generation quality.\nWe evaluate MEDUSA on models of various sizes and training procedures. Our\nexperiments demonstrate that MEDUSA-1 can achieve over 2.2\u00d7 speedup with-\nout compromising generation quality, while MEDUSA-2 further improves the\nspeedup to 2.3-3.6\u00d7. The code for this implementation is available at https:\n//github.com/FasterDecoding/Medusa.\n\u2217Equal contribution.\narXiv:2401.10774v1  [cs.LG]  19 Jan 2024\nContents\n1\nIntroduction\n3\n2\nRelated Work\n4\n2.1\nLLM Inference Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nSampling Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nMEDUSA\n6\n3.1\nKey Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.1.1\nMEDUSA Heads\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.1.2\nTree Attention\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.2\nTraining Strategies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2.1\nMEDUSA-1: Frozen Backbone . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2.2\nMEDUSA-2: Joint Training . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.3\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.3.1\nTypical Acceptance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.3.2\nSelf-Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.3.3\nSearching for the Optimized Tree Construction . . . . . . . . . . . . . . .\n10\n4\nExperiments\n11\n4.0.1\nShared Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n4.1\nCase Study: MEDUSA-1 v.s. MEDUSA-2 on Vicuna 7B and 13B . . . . . . . . . .\n11\n4.1.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n4.1.2\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.2\nCase Study: Training with Self-Distillation on Vicuna-33B and Zephyr-7B . . . . .\n12\n4.2.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.2.2\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4.3\nAblation Study\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.3.1\nConfiguration of Tree Attention . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.3.2\nThresholds of Typical Acceptance . . . . . . . . . . . . . . . . . . . . . .\n14\n4.3.3\nEffectiveness of Two-stage Fine-tuning . . . . . . . . . . . . . . . . . . .\n15\n4.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2\n1\nIntroduction\nThe recent advancements in Large Language Models (LLMs) have demonstrated that the quality\nof language generation significantly improves with an increase in model size, reaching billions of\nparameters [Brown et al., 2020, Chowdhery et al., 2022, Zhang et al., 2022, Hoffmann et al., 2022,\nOpenAI, 2023, Google, 2023, Touvron et al., 2023]. However, this growth has led to an increase\nin inference latency, which poses a significant challenge in practical applications. From a system\nperspective, LLM inference is predominantly memory-bound [Shazeer, 2019, Kim et al., 2023],\nwith the main latency bottleneck stemming from accelerators\u2019 memory bandwidth rather than arith-\nmetic computations. This bottleneck is inherent to the sequential nature of auto-regressive decoding,\nwhere each forward pass requires transferring the complete model parameters from High-Bandwidth\nMemory (HBM) to the accelerator\u2019s cache. This process, which generates only a single token, un-\nderutilizes the arithmetic computation potential of modern accelerators, leading to inefficiency.\nTo address this, one approach to speed up LLM inference involves increasing the arithmetic intensity\n(the ratio of total floating-point operations (FLOPs) to total data movement) of the decoding process\nand reducing the number of decoding steps. In line with this idea, speculative decoding has been\nproposed [Leviathan et al., 2022, Chen et al., 2023, Xia et al., 2023, Miao et al., 2023]. This method\nuses a smaller draft model to generate a sequence of tokens at each step, which is then refined\nby the original, larger model for acceptable continuation. However, obtaining an appropriate draft\nmodel remains challenging, and things become even harder when integrating the draft model into a\ndistributed system [Chen et al., 2023].\nInstead of using a separate draft model to sequentially generate candidate outputs, in this paper, we\nrevisit and refine the concept of using multiple decoding heads on top of the backbone model to\nexpedite inference [Stern et al., 2018]. We find that when applied effectively, this technique can\novercome the challenges of speculative decoding, allowing for seamless integration into existing\nLLM systems. Specifically, we introduce MEDUSA, a method that enhances LLM inference by\nintegrating additional decoding heads capable of concurrently predicting multiple tokens. These\nheads are fine-tuned in a parameter-efficient manner and can be added to any existing model. With\nno requirement for a new model, MEDUSA offers easy and automatic integration into current LLM\nsystems, including those in distributed environments, ensuring a user-friendly experience.\nWe further enhance MEDUSA with two key insights. Firstly, the current approach of generating\na single candidate continuation at each decoding step leads to a restricted acceptance length and\ninefficient use of computational resources. To address this, we propose generating multiple can-\ndidate continuations using the MEDUSA heads and verifying them concurrently through a simple\nadjustment to the attention mask. Secondly, we can use the rejection sampling scheme similar to\nthat used in speculative decoding to generate responses with the same distribution as the original\nmodel, but it is usually unnecessary for many LLM applications. Alternatively, we also introduce a\ntypical acceptance scheme that selects reasonable candidates from the MEDUSA head outputs. We\nuse temperature as a threshold to manage deviation from the original model\u2019s predictions, providing\nan efficient alternative to the rejection sampling method. This approach effectively addresses its\nlimitations, such as decreased speed at higher temperatures.\nTo equip LLMs with predictive MEDUSA heads, we propose two distinct fine-tuning procedures tai-\nlored to various scenarios. For situations with limited computational resources or when the objective\nis to incorporate MEDUSA into an existing model without affecting its performance, we recommend\nMEDUSA-1. This method requires minimal memory and can be further optimized with quantization\ntechniques akin to those in QLoRA [Dettmers et al., 2023], without compromising the generation\nquality due to the fixed backbone model. However, in MEDUSA-1, the full potential of the backbone\nmodel is not utilized. We can further fine-tune it to enhance the prediction accuracy of MEDUSA\nheads, which can directly lead to a greater speedup. Therefore, we introduce MEDUSA-2, which\nis suitable for scenarios with ample computational resources or for direct Supervised Fine-Tuning\n(SFT) from a base model. The key to MEDUSA-2 is a training protocol that enables joint training of\nthe MEDUSA heads and the backbone model without compromising the model\u2019s next-token predic-\ntion capability and output quality. We propose different strategies for obtaining the training datasets\ndepending on the model\u2019s training recipe and dataset availability. When the model is fine-tuned on\na public dataset, it can be directly used for MEDUSA. If the dataset is unavailable or the model\nunderwent a Reinforcement Learning with Human Feedback (RLHF) [Ouyang et al., 2022] process,\nwe suggest a self-distillation approach to generate a training dataset for the MEDUSA heads.\n3\nOur experiments primarily focus on scenarios with a batch size of one, which is representative\nof the use case where LLMs are locally hosted for personal use2 We test MEDUSA on models\nof varying sizes and training settings, including Vicuna-7B, 13B (trained with a public dataset),\nVicuna-33B [Chiang et al., 2023] (trained with a private dataset3), and Zephyr-7B (trained with both\nsupervised fine-tuning and alignment). MEDUSA can achieve a speedup of 2.3 to 3.6 times across\ndifferent prompt types without compromising on the quality of generation.\nIt is difficult\nIt is difficult not \u2705 \nIt' difficult a \u274c\nIt is' not \u274c  ...\nTransformer\nLayers\nEmbedding\nLM Head\n\u2744/\ud83d\udd25\nOriginal Model\nMedusa Head 1\nMedusa Head 2\nMedusa Head 3\n\ud83d\udd25Medusa Heads\nLast Hidden\nis, ', the\nIt, I, As\ndifficult, is, '\nnot, difficult, a\nWhat will happen if\nMedusa meets a llama?\n\ud83d\udd1dTop-k Predictions\n\ud83d\udcddInput\n\ud83d\udcdcCandidates\n/Single step prediction\nFigure 1: Overview of MEDUSA. MEDUSA introduces multiple heads on top of the last hidden\nstates of the LLM, enabling the prediction of several subsequent tokens in parallel (Section 3.1.1).\nFor training MEDUSA heads, the original model is either frozen (MEDUSA-1, Section 3.2.1) or\ntrained together (MEDUSA-2, Section 3.2.2) with MEDUSA heads. During inference, each head\ngenerates multiple top predictions for its designated position. These predictions are assembled into\ncandidates, which are subsequently processed in parallel using a tree-based attention mechanism\n(Section 3.1.2). The final step is to verify the candidates and accept a continuation. Besides the\nstandard rejection sampling scheme, a typical acceptance scheme (Section 3.3.1) can also be used\nhere to select reasonable continuations, and the longest accepted candidate prefix will be used for\nthe next decoding phase. The efficiency of the decoding process is enhanced by accepting more\ntokens simultaneously, thus reducing the number of required decoding steps.\n2\nRelated Work\n2.1\nLLM Inference Acceleration\nThe inefficiency of Large Language Model (LLM) inference is primarily attributed to the memory-\nbound nature of the auto-regressive decoding process. Several methods have been proposed to alle-\nviate this issue, improving inference latency and throughput. Traditionally, batch inference has been\n2It\u2019s important to note that while MEDUSA can be seamlessly used in a batched inference setting, it requires\nadditional engineering efforts to integrate into a serving engine like vLLM [Kwon et al., 2023]. We are working\non this and also welcome community contributions to help us.\n3Upon contacting the authors, this version is experimental and used some different data than Vicuna 7B and\n13B.\n4\nemployed as a straightforward method to enhance arithmetic intensity and escape memory-bound\nlimitations. However, with LLMs, both model parameters and the Key-Value (KV) cache consume\nsubstantial accelerator memory, hindering the utilization of large batch sizes. Existing methods to\ntackle this problem can be conceptually divided into two main categories: (1) Reducing memory\nconsumption, thereby minimizing memory transfer overhead and enabling larger batch sizes, and\n(2) Minimizing the number of decoding steps to decrease latency directly.\nReducing KV Cache.\nMethods such as Multi-query attention [Shazeer, 2019] and Grouped-query\nattention [Ainslie et al., 2023] adopt a direct approach to diminish the KV cache. By utilizing fewer\nkey and value heads in the attention modules relative to query heads, these strategies substantially\ncut the KV\u2019s memory consumption, thereby facilitating larger batch sizes and enhanced accelera-\ntor utilization [Pope et al., 2022]. Additionally, Zhang et al. [2023] proposes to selectively retain\nthe most critical KV tokens, further reducing the KV cache. From a system perspective, Kwon\net al. [2023] introduces a paged memory management scheme for reducing fragmentation of the KV\ncache.\nQuantization.\nQuantization techniques are extensively used to shrink LLMs\u2019 memory consump-\ntion. Xiao et al. [2023a] apply rescaling between activations and parameters to eliminate outliers\nand simplify the quantization process. Dettmers et al. [2022] breaks down matrix multiplications\ninto predominantly 8-bit and a minority of 16-bit operations. Frantar et al. [2022] iteratively round\nweight columns into 3/4 bits, while Lin et al. [2023] present an activation-aware quantization scheme\nto protect salient weights and compress LLMs to 3/4 bits. Kim et al. [2023] introduce a sparse plus\nlow-precision pattern to handle a minor portion of vital weights, among other techniques.\nSpeculative Decoding.\nAs an approach orthogonal to the aforementioned methods, speculative\ndecoding [Leviathan et al., 2022, Chen et al., 2023] aims to execute several decoding steps in paral-\nlel, thus reducing the total number of steps required. This parallelization is realized by employing a\nsmaller draft model to conjecture several subsequent words, which the LLMs then collectively evalu-\nate and accept as appropriate. While resonating with non-autoregressive generation literature [Xiao\net al., 2023b], this method is specifically tailored for LLMs to address the aforementioned ineffi-\nciency. Unlike previous works, we propose leveraging the original model to make predictions rather\nthan introducing an additional draft model. This approach is more straightforward and seamlessly\nintegrates into existing systems without the complexities of managing two models. Independently,\nMiao et al. [2023], Spector and Re [2023] propose the use of tree-structured attention to generate\nmultiple candidates in parallel, where Miao et al. [2023] suggest employing an ensemble of models\nto propose candidates, and Spector and Re [2023] advocate adding another hierarchy for the draft\nmodel. After the first release of MEDUSA, we have seen many new works improving speculative\ndecoding from the perspective of distillation [Liu et al., 2023, Zhou et al., 2023], making draft model\ntraining-free [He et al., 2023, Fu et al., 2023].\n2.2\nSampling Scheme\nThe manner in which text is sampled from Large Language Models (LLMs) can significantly influ-\nence the quality of the generated output. Recent studies have revealed that direct sampling from a\nlanguage model may lead to incoherent or nonsensical results [Pillutla et al., 2021, Holtzman et al.,\n2020]. In response to this challenge, truncation sampling schemes have been introduced [Fan et al.,\n2018, Basu et al., 2021, Meister et al., 2022, Hewitt et al., 2022, Meister et al., 2023]. These ap-\nproaches aim to produce high-quality and diverse samples by performing sampling on a truncated\ndistribution over a specific allowed set at each decoding step.\nDifferent strategies define this allowed set in various ways. For example, top-k sampling [Fan et al.,\n2018] retains the k most likely words, whereas top-p sampling [Holtzman et al., 2020] incorporates\nthe minimal set of words that account for p percent of the probability. Another method, known as\ntypical decoding [Meister et al., 2023], employs the entropy of the predicted distribution to establish\nthe threshold for inclusion. Hewitt et al. [2022] offers a unified framework to understand truncation\nsampling techniques comprehensively.\nDrawing inspiration from these methods, our typical acceptance scheme aligns with the concept of\ndefining an allowed set to exclude improbable candidates from the sampling process. However, we\ndiverge because we do not insist on an exact correspondence between the output and language model\n5\ndistribution. This deviation allows us to facilitate more diverse yet high-quality outputs, achieving\ngreater efficiency without compromising the integrity of the generated text.\n3\nMEDUSA\nMEDUSA follows the same framework as speculative decoding, where each decoding step primarily\nconsists of three substeps: (1) generating candidates, (2) processing candidates, and (3) accepting\ncandidates. For MEDUSA, (1) is achieved by MEDUSA heads, (2) is realized by tree attention, and\nsince MEDUSA heads are on top of the original model, the logits calculated in (2) can be used\nfor substep (1) for the next decoding step. The final step (3) can be realized by either rejection\nsampling [Leviathan et al., 2022, Chen et al., 2023] or typical acceptance (Section 3.3.1). The\noverall pipeline is illustrated in Figure 1.\nIn this section, we first introduce the key components of MEDUSA, including MEDUSA heads, and\ntree attention. Then, we present two levels of fine-tuning procedures for MEDUSA to meet the needs\nof different use cases. Finally, we propose two extensions to MEDUSA, including self-distillation\nand typical acceptance, to handle situations where no training data is available for MEDUSA and to\nimprove the efficiency of the decoding process, respectively.\n3.1\nKey Components\n3.1.1\nMEDUSA Heads\nIn speculative decoding, subsequent tokens are predicted by an auxiliary draft model. This draft\nmodel must be small yet effective enough to generate continuations that the original model will ac-\ncept. Fulfilling these requirements is a challenging task, and existing approaches [Spector and Re,\n2023, Miao et al., 2023] often resort to separately pre-training a smaller model. This pre-training\nprocess demands substantial additional computational resources. For example, in [Miao et al., 2023],\na reported 275 NVIDIA A100 GPU hours were used. Additionally, separate pre-training can poten-\ntially create a distribution shift between the draft model and the original model, leading to continua-\ntions that the original model may not favor. Chen et al. [2023] have also highlighted the complexities\nof serving multiple models in a distributed environment.\nTo streamline and democratize the acceleration of LLM inference, we take inspiration from Stern\net al. [2018] and introduce MEDUSA heads. These are additional decoding heads appended to the\nlast hidden states of the original model. Specifically, given the original model\u2019s last hidden states\nht at position t, we add K decoding heads to ht. The k-th head is used to predict the token in the\n(t+k+1)-th position of the next tokens (the original language model head is used to predict the (t+\n1)-th position). The prediction of the k-th head is denoted as p(k)\nt\n, representing a distribution over\nthe vocabulary, while the prediction of the original model is denoted as p(0)\nt . Following the approach\nof Stern et al. [2018], we utilize a single layer of feed-forward network with a residual connection\nfor each head. We find that this simple design is sufficient to achieve satisfactory performance. The\ndefinition of the k-th head is outlined as:\np(k)\nt\n= softmax\n\u0010\nW (k)\n2\n\u00b7\n\u0010\nSiLU(W (k)\n1\n\u00b7 ht) + ht\n\u0011\u0011\n, where W (k)\n2\n\u2208 Rd\u00d7V , W (k)\n1\n\u2208 Rd\u00d7d.\nWe initialize W (k)\n1\nidentically to the original language model head, and W (k)\n2\nto zero. This aligns\nthe initial prediction of MEDUSA heads with that of the original model. The SiLU activation func-\ntion [Elfwing et al., 2017] is employed following the Llama models [Touvron et al., 2023].\nUnlike a draft model, MEDUSA heads are trained in conjunction with the original backbone model,\nwhich can remain frozen during training (MEDUSA-1) or be trained together (MEDUSA-2). This\nmethod allows for fine-tuning large models even on a single GPU, taking advantage of the powerful\nbase model\u2019s learned representations. Furthermore, it ensures that the distribution of the MEDUSA\nheads aligns with that of the original model, thereby mitigating the distribution shift problem. Ad-\nditionally, since the new heads consist of just a single layer akin to the original language model\nhead, MEDUSA does not add complexity to the serving system design and is friendly to distributed\nsettings. We will discuss the training recipe for MEDUSA heads in Section 3.2.\n6\n3.1.2\nTree Attention\nThrough MEDUSA heads, we obtain probability predictions for the subsequent K + 1 tokens. These\npredictions enable us to create length-K + 1 continuations as candidates. While the speculative\ndecoding studies [Leviathan et al., 2022, Chen et al., 2023] suggest sampling a single continuation as\nthe candidate, leveraging multiple candidates during decoding can enhance the expected acceptance\nlength within a decoding step. Nevertheless, more candidates can also raise computational demands.\nTo strike a balance, we employ a tree-structured attention mechanism to process multiple candidates\nconcurrently. This attention mechanism diverges from the traditional causal attention paradigm.\nFigure 2: Tree Attention Illustrated. This visualization demonstrates the use of tree attention\nto process multiple candidates concurrently. As exemplified, the top-2 predictions from the first\nMEDUSA head and the top-3 from the second result in a total of 2\u00d73 = 6 candidates. Each of these\ncandidates corresponds to a distinct branch within the tree structure. To guarantee that each token\nonly accesses its predecessors, we devise an attention mask that exclusively permits attention flow\nfrom the current token back to its antecedent tokens. The positional indices for positional encoding\nare adjusted in line with this structure.\nWithin this framework, only tokens from the same continuation are regarded as historical data.\nDrawing inspiration from the concept of embedding graph structures into attention as proposed\nin the graph neural network domain [Ying et al., 2021], we incorporate the tree structure into our\nattention mask, visualized in Figure 2. For a given k-th head, its top-sk predictions serve as the basis\nfor candidate formation, where sk is a designated hyperparameter. These candidates are established\nby determining the Cartesian product of the top-sk predictions from each head. For instance, in\nFigure 2, with s1 = 2 and s2 = 3, each first head prediction can be succeeded by any prediction from\nthe second head. This leads to a tree structure where sk branches exist at the k-th level (considering\na virtual root as the 0-level, in practice, this 0-level is for the prediction of the language model\nhead of the original model, which can be sampled independently). Within this tree, only a token\u2019s\npredecessors are seen as historical context, and our attention mask ensures that the attention is only\napplied on a token\u2019s predecessors. By employing this mask and properly setting the positional\nindices for positional encoding, we can process numerous candidates simultaneously without the\nneed to expand the batch size. The cumulative number of new tokens is calculated as PK\nk=1\nQk\ni=1 si.\nIn this section, we demonstrate the most simple and regular way to construct the tree structure\nby taking the Cartesian product. However, it is possible to construct the tree structure in a more\nsophisticated way and exploit the unbalanced accuracy of different top predictions of different heads.\nWe will discuss this in Section 3.3.3.\n7\n3.2\nTraining Strategies\nAt the most basic level, we can train MEDUSA heads by freezing the backbone model and focusing\nsolely on the MEDUSA heads. This approach is straightforward and requires minimal computational\nresources. However, training the backbone in conjunction with the MEDUSA heads can significantly\nenhance the accuracy of the MEDUSA heads. Depending on the computational resources and the\nspecific requirements of the use case, we propose two levels of training strategies for MEDUSA\nheads.\nIn this section, we assume the availability of a training dataset that aligns with the target model\u2019s\noutput distribution. This could be the dataset used for Supervised Fine-Tuning (SFT) of the target\nmodel. We will discuss how to eliminate the need for such a dataset using a self-distillation approach\nin Section 3.3.2.\n3.2.1\nMEDUSA-1: Frozen Backbone\nTo train MEDUSA heads with a frozen backbone model, we can use the cross-entropy loss between\nthe prediction of MEDUSA heads and the ground truth. Specifically, given the ground truth token\nyt+k+1 at position t + k + 1, the loss for the k-th head is Lk = \u2212 log p(k)\nt\n(yt+k+1) where p(k)\nt\n(y)\ndenotes the probability of token y predicted by the k-th head. We also observe that Lk is larger\nwhen k is larger, which is reasonable since the prediction of the k-th head is more uncertain when k\nis larger. Therefore, we can add a weight \u03bbk to Lk to balance the loss of different heads. And the\ntotal MEDUSA loss is:\nLMEDUSA-1 =\nK\nX\nk=1\n\u2212\u03bbk log p(k)\nt\n(yt+k+1).\n(1)\nIn practice, we set \u03bbk as the k-th power of a constant like 0.8. Since we only use the backbone model\nfor providing the hidden states, we can use a quantized version of the backbone model to reduce the\nmemory consumption. This introduces a more democratized way to accelerate LLM inference, as\nwith the quantization, MEDUSA can be trained for a large model on a single consumer GPU similar\nto QLoRA [Dettmers et al., 2023]. The training only takes a few hours (e.g., 5 hours for MEDUSA-1\non Vicuna 7B model with a single NVIDIA A100 PCIE GPU to train on 60k ShareGPT samples).\n3.2.2\nMEDUSA-2: Joint Training\nTo further improve the accuracy of MEDUSA heads, we can train MEDUSA heads together with the\nbackbone model. However, this requires a special training recipe to preserve the backbone model\u2019s\nnext-token prediction capability and output quality. To achieve this, we propose three strategies:\n\u2022 Combined loss: To keep the backbone model\u2019s next-token prediction capability, we need to add\nthe cross-entropy loss of the backbone model LLM = \u2212 log p(0)\nt (yt+1) to the MEDUSA loss.\nWe also add a weight \u03bb0 to balance the loss of the backbone model and the MEDUSA heads.\nTherefore, the total loss is:\nLMEDUSA-2 = LLM + \u03bb0LMEDUSA-1.\n(2)\n\u2022 Differential learning rates: Since the backbone model is already well-trained and the MEDUSA\nheads need more training, we can use separate learning rates for them to enable faster convergence\nof MEDUSA heads while preserving the backbone model\u2019s capability.\n\u2022 Heads warmup: Noticing that at the beginning of training, the MEDUSA heads have a large loss,\nwhich leads to a large gradient and may distort the backbone model\u2019s parameters. Following the\nidea from Kumar et al. [2022], we can employ a two-stage training process. In the first stage, we\nonly train the backbone model as MEDUSA-1. In the second stage, we train the backbone model\nand MEDUSA heads together with a warmup strategy. Specifically, we first train the backbone\nmodel for a few epochs, then train the MEDUSA heads together with the backbone model. Besides\nthis simple strategy, we can also use a more sophisticated warmup strategy by gradually increasing\nthe weight \u03bb0 of the backbone model\u2019s loss. We find both strategies work well in practice.\nPutting these strategies together, we can train MEDUSA heads together with the backbone model\nwithout hurting the backbone model\u2019s capability. Moreover, this recipe can be applied together with\nSupervised Fine-Tuning (SFT), enabling us to get a model with native MEDUSA support.\n8\n3.3\nExtensions\n3.3.1\nTypical Acceptance\nIn speculative decoding papers [Leviathan et al., 2022, Chen et al., 2023], authors employ rejection\nsampling to yield diverse outputs that align with the distribution of the original model. However,\nsubsequent implementations [Joao Gante, 2023, Spector and Re, 2023] reveal that this sampling\nstrategy results in diminished efficiency as the sampling temperature increases. Intuitively, this can\nbe comprehended in the extreme instance where the draft model is the same as the original one. Here,\nwhen using greedy decoding, all output of the draft model will be accepted, therefore maximizing\nthe efficiency. Conversely, rejection sampling introduces extra overhead, as the draft model and the\noriginal model are sampled independently. Even if their distributions align perfectly, the output of\nthe draft model may still be rejected.\nHowever, in real-world scenarios, sampling from language models is often employed to generate\ndiverse responses, and the temperature parameter is used merely to modulate the \u201ccreativity\u201d of the\nresponse. Therefore, higher temperatures should result in more opportunities for the original model\nto accept the draft model\u2019s output. We ascertain that it is typically unnecessary to match the distri-\nbution of the original model. Thus, we propose employing a typical acceptance scheme to select\nplausible candidates rather than using rejection sampling. This approach draws inspiration from\ntruncation sampling studies [Hewitt et al., 2022] (refer to Section 2 for an in-depth explanation).\nOur objective is to choose candidates that are typical, meaning they are not exceedingly improbable\nto be produced by the original model. We use the prediction probability from the original model as\na natural gauge for this and establish a threshold based on the prediction distribution to determine\nacceptance. Specifically, given x1, x2, \u00b7 \u00b7 \u00b7 , xn as context, when evaluating the candidate sequence\n(xn+1, xn+2, \u00b7 \u00b7 \u00b7 , xn+T +1) (composed by top predictions of the original language model head and\nMEDUSA heads), we consider the condition\nporiginal(xn+k|x1, x2, \u00b7 \u00b7 \u00b7 , xn+k\u22121) > min (\u03f5, \u03b4 exp (\u2212H(poriginal(\u00b7|x1, x2, \u00b7 \u00b7 \u00b7 , xn+k\u22121)))) ,\nwhere H(\u00b7) denotes the entropy function, and \u03f5, \u03b4 are hyperparameters. This criterion is adapted\nfrom Hewitt et al. [2022] and rests on two observations: (1) tokens with relatively high probabil-\nity are meaningful, and (2) when the distribution\u2019s entropy is high, various continuations may be\ndeemed reasonable. During decoding, every candidate is evaluated using this criterion, and a prefix\nof the candidate is accepted if it satisfies the condition. To guarantee the generation of at least one\ntoken at each step, we apply greedy decoding for the first token and unconditionally accept it while\nemploying typical acceptance for subsequent tokens. The final prediction for the current step is\ndetermined by the longest accepted prefix among all candidates.\nExamining this scheme leads to several insights. Firstly, when the temperature is set to 0, it reverts\nto greedy decoding, as only the most probable token possesses non-zero probability. As the temper-\nature surpasses 0, the outcome of greedy decoding will consistently be accepted with appropriate\n\u03f5, \u03b4, since those tokens have the maximum probability, yielding maximal speedup. Likewise, in gen-\neral scenarios, an increased temperature will correspondingly result in longer accepted sequences,\nas corroborated by our experimental findings.\n3.3.2\nSelf-Distillation\nIn Section 3.2, we assume the existence of a training dataset that matches the target model\u2019s output\ndistribution. However, this is not always the case. For example, the model owners may only release\nthe model without the training data, or the model may have gone through a Reinforcement Learning\nwith Human Feedback (RLHF) procedure, which makes the output distribution of the model differ-\nent from the training dataset. To tackle this issue, we propose an automated self-distillation pipeline\nto use the model itself to generate the training dataset for MEDUSA heads, which matches the output\ndistribution of the model.\nThe dataset generation process is straightforward. We first take a public seed dataset from a domain\nsimilar to the target model; for example, using the ShareGPT [ShareGPT, 2023] dataset for chat\nmodels. Then, we simply take the prompts from the dataset and ask the model to reply to the\nprompts. In order to obtain multi-turn conversation samples, we can sequentially feed the prompts\nfrom the seed dataset to the model. Or, for models like Zephyr 7B [Tunstall et al., 2023], which are\ntrained on both roles of the conversation, they have the ability to self-talk, and we can simply feed\nthe first prompt and let the model generate multiple rounds of conversation.\n9\nFor MEDUSA-1, this dataset is sufficient for training MEDUSA heads. However, for MEDUSA-2, we\nobserve that solely using this dataset for training the backbone and MEDUSA heads usually leads\nto a lower generation quality. In fact, even without training MEDUSA heads, training the backbone\nmodel with this dataset will lead to performance degradation. This suggests that we also need to use\nthe original model\u2019s probability prediction instead of using the ground truth token as the label for the\nbackbone model, similar to classic knowledge distillation works [Kim and Rush, 2016]. Concretely,\nthe loss for the backbone model is:\nLLM-distill = KL(p(0)\noriginal,t||p(0)\nt ),\nwhere p(0)\noriginal,t denotes the probability distribution of the original model\u2019s prediction at position t.\nHowever, naively, to obtain the original model\u2019s probability prediction, we need to maintain two\nmodels during training, increasing the memory requirements. To further alleviate this issue, we\npropose a simple yet effective way to exploit the self-distillation setup. We can use a parameter-\nefficient adapter like LoRA [Hu et al., 2021] for fine-tuning the backbone model. In this way, the\noriginal model is simply the model with the adapter turned off. Therefore, the distillation does not\nrequire additional memory consumption. Together, this self-distillation pipeline can be used to train\nMEDUSA-2 without hurting the backbone model\u2019s capability and introduce almost no additional\nmemory consumption. Lastly, one tip about using self-distillation is that it is preferable to use\nLoRA without quantization in this case, otherwise, the teacher model will be the quantized model,\nwhich may lead to a lower generation quality.\nFigure 3: Visualization of a sparse tree setting for MEDUSA-2 Vicuna-7B. The tree has depth 4\nwhich indicates 4 MEDUSA heads involved in calculation. Each node indicates a token from a top-k\nprediction of a MEDUSA head, and the edges show the connections between them. The red lines\nhighlight the path that correctly predicts the future tokens.\n3.3.3\nSearching for the Optimized Tree Construction\nIn Section 3.1.2, we present the simplest way to construct the tree structure by taking the Cartesian\nproduct. However, with a fixed number of total nodes in the tree, a regular tree structure may not be\nthe best choice. Intuitively, those candidates composed of the top predictions of different heads may\nhave different accuracies. Therefore, we can leverage an estimation of the accuracy to construct the\ntree structure.\nSpecifically, we can use a calibration dataset and calculate the accuracies of the top predictions of\ndifferent heads. Let a(i)\nk\ndenote the accuracy of the i-th top prediction of the k-th head. Assuming\nthe accuracies are independent, we can estimate the accuracy of a candidate sequence composed\nby the top [i1, i2, \u00b7 \u00b7 \u00b7 , ik] predictions of different heads as Qk\nj=1 a(ij)\nj\n. Let I denote the set of all\npossible combinations of [i1, i2, \u00b7 \u00b7 \u00b7 , ik] and each element of I can be mapped to a node of the tree\n10\n(not only leaf nodes but all nodes are included). Then, the expectation of the acceptance length of a\ncandidate sequence is:\nX\n[i1,i2,\u00b7\u00b7\u00b7 ,ik]\u2208I\nk\nY\nj=1\na(ij)\nj\n.\nThinking about building a tree by adding nodes one by one, the contribution of a new node to the\nexpectation is exactly the accuracy associated with the node. Therefore, we can greedily add nodes\nto the tree by choosing the node that is connected to the current tree and has the highest accuracy.\nThis process can be repeated until the total number of nodes reaches the desired number. In this\nway, we can construct a tree structure that maximizes the expectation of the acceptance length.\nFig. 3 illustrates the structure of a sparsely constructed tree for the MEDUSA-2 Vicuna-7B model.\nThis tree structure extends four levels deep, indicating the engagement of four MEDUSA heads in\nthe computation. The tree is initially formed through a Cartesian product approach and subsequently\nrefined by pruning based on the statistical expectations of the top-k predictions from each MEDUSA\nhead measured on the Alpaca-eval dataset Dubois et al. [2023]. The tree\u2019s lean towards the left\nvisually represents the algorithm\u2019s preference for nodes with higher probabilities on each head.\n4\nExperiments\nIn this section, we present two sets of experiments to demonstrate the effectiveness of MEDUSA\nunder different settings. First, we evaluate MEDUSA on the Vicuna-7B and 13B models [Chiang\net al., 2023] to show the performance of MEDUSA-1 and MEDUSA-2. Second, we evaluate MEDUSA\non the Vicuna-33B and Zephyr-7B [Tunstall et al., 2023] models to study the effectiveness of self-\ndistillation because for Vicuna-33B model, the training dataset is not publicly available, and for\nZephyr-7B model, the model is trained with RLHF.\nWe clarify three commonly used terms: a) Acceleration rate: This refers to the average number of\ntokens decoded per decoding step. In a standard auto-regressive model, this rate is 1.0. b) Overhead:\nThis is used to characterize the per decoding step overhead compared to classic decoding, and is\ncalculated by dividing the average per step latency of the MEDUSA models by that of the vanilla\nmodel. c) Speedup: This refers to the wall-time acceleration rate. Following these definitions, we\nhave the relation: Speedup = Acceleration rate / Overhead.\n4.0.1\nShared Settings\nFor all the experiments, we use the Axolotl [Axolotl, 2023] framework for training. We use a cosine\nlearning rate scheduler with warmup and use 8-bit AdamW [Dettmers et al., 2021] optimizer. We\ntrain 5 MEDUSA heads with 1 layer and set \u03bbk in Eq. (1) to be 0.8k. For MEDUSA-2, we use either\nLoRA [Hu et al., 2021] or QLoRA [Dettmers et al., 2023] for fine-tuning and set the learning rate\nof MEDUSA heads to be 4 times larger than the backbone model. LoRA is applied to all the linear\nlayers of the backbone model, including the language model head. The rank of LoRA adapter is set\nto 32, and \u03b1 is set to 16. A dropout of 0.05 is added to the LoRA adapter.\n4.1\nCase Study: MEDUSA-1 v.s. MEDUSA-2 on Vicuna 7B and 13B\n4.1.1\nExperimental Setup\nWe use the Vicuna model class [Chiang et al., 2023], which encompasses chat models of varying\nsizes (7B, 13B, 33B) that are fine-tuned from the Llama model [Touvron et al., 2023]. Among them,\nthe 7B and 13B models are trained on the ShareGPT [ShareGPT, 2023] dataset, while the 33B model\nis an experimental model and is trained on a private dataset. In this section, we use the ShareGPT\ndataset to train the MEDUSA heads on the 7B and 13B models for 2 epochs. We use the v1.5 version\nof Vicuna models, which are fine-tuned from Llama-2 models with sequence length 4096. We use\na global batch size of 64 and a peak learning rate of 5e\u22124 for the backbone and 2e\u22123 for MEDUSA\nheads and warmup for 40 steps. We use 4-bit quantized backbone models for both models. We first\ntrain the models with MEDUSA-1 and use these trained models as initialization to train MEDUSA-2.\nWe employ QLoRA for MEDUSA-2 and the \u03bb0 in Eq. (2) is set to be 0.2.\n11\n7B\n13B\nModel Size\n0\n20\n40\n60\n80\n100\n120\nTokens per Second\n2.18x\n2.33x\n2.83x\n2.83x\nSpeedup on different model sizes\nw/o Medusa\nMedusa-1\nMedusa-2\n(a)\nHumanities\nReasoning\nRoleplay\nWriting\nStem\nMath\nCoding\nExtraction\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nSpeedup\n2.58x\n2.58x\n2.7x\n2.72x\n2.77x\n3.01x\n3.29x\n3.62x\nSpeedup on different categories for 7B model\n(b)\nFigure 4: Left: Speed comparison of baseline, MEDUSA-1 and MEDUSA-2 on Vicuna-7B/13B.\nMEDUSA-1 achieves more than 2\u00d7 wall-time speedup compared to the baseline implementation\nwhile MEDUSA-2 further improves the speedup by a significant margin. Right: Detailed speedup\nperformance of Vicuna-7B on 8 categories from MT-Bench.\n4.1.2\nResults\nWe collect the results and show them in Fig. 4. The baseline is the vanilla Huggingface implemen-\ntation. In Fig. 4a, we can see that for the 7B models, MEDUSA-1 and MEDUSA-2 configurations\nlead to a significant increase in speed, measuring in tokens processed per second. MEDUSA-1 shows\na 2.18\u00d7 speedup, while MEDUSA-2 further improves this to a 2.83\u00d7. When applied to the larger\n13B model, MEDUSA-1 results in a 2.33\u00d7 speed increase, while MEDUSA-2 maintains a similar\nperformance gain of 2.83\u00d7 over the baseline. We also plot the speedup per category for MEDUSA-2\nVicuna-7B model. We observe that the \u201cCoding\u201d category benefits from a 3.29\u00d7 speedup, suggest-\ning that MEDUSA is particularly effective for tasks in this domain. This points to a significant po-\ntential for optimizing coding LLMs, widely used in software development and other programming-\nrelated tasks. The \u201cExtraction\u201d category shows the highest speedup at 3.62\u00d7, indicating that this\ntask is highly optimized by the MEDUSA. Overall, the results suggest that the MEDUSA significantly\nenhances inference speed across different model sizes and tasks.\n4.2\nCase Study: Training with Self-Distillation on Vicuna-33B and Zephyr-7B\n4.2.1\nExperimental Setup\nIn this case study, we focus on the cases where self-distillation is needed. We use the Vicuna-33B\nmodel [Chiang et al., 2023] and the Zephyr-7B model [Tunstall et al., 2023] as examples. Following\nthe procedure described in Section 3.3.2, we first generate the datasets with some seed prompts. We\nuse ShareGPT [ShareGPT, 2023] and UltraChat [Ding et al., 2023] as the seed datasets and collect\na dataset at about 100k samples for both cases. Interestingly, we find that the Zephyr model can\ncontinue to generate multiple rounds of conversation with a single prompt, which makes it easy\nto collect a large dataset. For Vicuna-33B, we generate the multi-turn conversations by iteratively\nfeeding the prompts from each multi-turn seed conversation. Both models are trained with sequence\nlength 2048 and batch size 128. We use MEDUSA-2 for both models and instead of using a two-stage\ntraining procedure, we use a sine schedule for the \u03b80 to gradually increase the value to its peak at the\nend of the training, we find this approach is equally effective. We set the peak learning rate of the\nbackbone LoRA adapter to be 1e\u22124 and the warmup steps to be 20. Since the self-distillation loss is\nrelatively small, we set the \u03bb0 in Eq. (2) to be 0.01.\n4.2.2\nResults\nTable 1 complements these findings by comparing various MEDUSA-2 models in terms of their\nacceleration rate, overhead, and quality on MT-Bench. Notably, while the MEDUSA-2 Vicuna-33B\n12\nmodel shows a lower acceleration rate, it maintains a comparable quality. We hypothesize that this\nis due to a mismatch between the hidden training dataset and the dataset we used for self-distillation.\nThese results underscore the complex interplay between speed and performance when scaling up\nmodel sizes and applying self-distillation techniques. The findings also highlight the potential of the\nMEDUSA-2 configuration to boost efficiency in processing while carefully preserving the quality\nof the model\u2019s outputs, suggesting a promising direction for co-optimizing LLMs with MEDUSA\nheads.\nVicuna-7B\nZephyr-7B\nVicuna-13B\nVicuna-33B\nModel Size\n0\n20\n40\n60\n80\n100\n120\nTokens per Second\n2.83x\n2.66x\n2.83x\n2.35x\nSpeedup on different model sizes\nw/o Medusa\nMedusa-2\nFigure 5: Speedup of various models with MEDUSA-2. MEDUSA-2 shows significant speed im-\nprovement over all the models, while models trained with self-distillation have weaker speedup due\nto the trade-off between preserving model quality and boosting model speed.\nModel Name\nVicuna-7B\nZephyr-7B\nVicuna-13B\nVicuna-33B\nAcc. rate\n3.47\n3.14\n3.51\n3.01\nOverhead\n1.22\n1.18\n1.23\n1.27\nQuality\n6.18 (+0.01)\n7.25 (-0.07)\n6.43 (-0.14)\n7.18 (+0.05)\nTable 1: Comparison of various MEDUSA-2 models. The quality denotes the average scores on the\nMT-Bench benchmark [Zheng et al., 2023]. MEDUSA-2 achieves promising acceleration rate with\nmild overhead and preserves the model quality.\n4.3\nAblation Study\n4.3.1\nConfiguration of Tree Attention\nThe ablation study of tree attention is conducted on the writing and roleplay categories from the MT-\nBench dataset [Zheng et al., 2023] using MEDUSA-2 Vicuna-7B. We target to depict tree attention\u2019s\nmotivation and its performance.\nFig. 6a compares the acceleration rate of randomly sampled dense tree configurations (Section. 3.1.2,\ndepicted by blue dots) against optimized sparse tree settings (Section. 3.3.3, shown with red stars).\nThe sparse tree configuration with 64 nodes shows a better acceleration rate than the dense tree\nsettings with 256 nodes. Fig. 6b presents the speed for both dense and sparse tree settings. The\ntrend observed here indicates a notable decrease in speed as the additional length increases. This\nsuggests that while sparse tree attention is beneficial for maintaining a steady acceleration rate, it\ncomes with the trade-off of reduced speed, particularly at higher additional lengths.\nThe observed decline in speed is attributed to the increased overhead introduced by the hardware\narchitecture. While a more complex tree can improve the acceleration rate, it does so at the cost of\nspeed due to the hardware-imposed overhead.\n13\n0\n50\n100\n150\n200\n250\nAdditional Length\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nAcc. Rate\nw/o Medusa\nSparse Tree Attention\n(a)\n0\n50\n100\n150\n200\n250\nAdditional Length\n50\n60\n70\n80\n90\n100\n110\n120\nSpeed (token/s)\nw/o Medusa\nSparse Tree Attention\n(b)\nFigure 6: Evaluation of additional length introduced by trees. Left: The acceleration rate for ran-\ndomly sampled dense tree settings (blue dots) and optimized sparse tree settings (red stars). Right:\nThe speed (tokens/s) for both settings. The trend lines indicate that while the acceptance rate re-\nmains relatively stable for sparse trees, there is a notable decrease in speed as the additional length\nincreases.\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nPosterior Thresholds\n3.0\n3.1\n3.2\n3.3\n3.4\n3.5\nAcc. Rate\nGreedy\nRS\n7.0\n7.1\n7.2\n7.3\n7.4\n7.5\n7.6\nScores\nGreedy\nRS\nFigure 7: Performance comparison of MEDUSA using proposed typical sampling. The plot illus-\ntrates the acceleration rate (Acc. Rate) and averaged scores on the \u201cWriting\u201d and \u201cRoleplay\u201d (MT\nbenchmark) with a fixed temperature of 0.7 for 3 different settings: greedy sampling with MEDUSA,\nrandom sampling, and typical sampling under different thresholds. The model is fully fine-tuned\nVicuna-7B.\n4.3.2\nThresholds of Typical Acceptance\nThe thresholds of typical acceptance are studied on the \u201cWriting\u201d and \u201cRoleplay\u201d categories from\nthe MT-Bench dataset [Zheng et al., 2023] using MEDUSA-2 Vicuna 7B. Utilizing the Vicuna 7B\nmodel, we aligned our methodology with the approach delineated by Hewitt et al. [2022] setting the\n\u03b1 = \u221a\u03f5. Fig. 7 presents a comparative analysis of our model\u2019s performance across various sampling\nsettings. These settings range from a threshold \u03f5 starting at 0.01 and incrementally increasing to 0.25\nin steps of 0.01. Our observations indicate a discernible trade-off: as \u03f5 increases, there is an elevation\nin quality at the expense of a reduced acceleration rate. Furthermore, for tasks demanding creativity,\nit is noted that the default random sampling surpasses greedy sampling in performance, and the\nproposed typical sampling is comparable with random sampling when \u03f5 increases.\n14\nBaseline\nDirect Fine-tuning\nMEDUSA-1\nMEDUSA-2\nQuality\n6.17\n5.925\n6.23\n6.18\nSpeed Up\nN/A\nN/A\n2.18\n2.83\nTable 2: Comparison of Different Settings Vicuna-7B. Quality is obtained by evaluating models on\nMT-Bench.\n4.3.3\nEffectiveness of Two-stage Fine-tuning\nWe examine the performance differences between two fine-tuning strategies for the Vicuna-7B\nmodel in Table 2. We provided the comparison of directly fine-tuning the model with the MEDUSA\nheads vs. MEDUSA-2 that involves two-stage fine-tuning described in Section 3.2.2. The findings\nindicate that implementing our MEDUSA-2 for fine-tuning maintains the model\u2019s quality and con-\ncurrently improves the speedup versus MEDUSA-1.\n4.4\nDiscussion\nIn conclusion, we have proposed MEDUSA, a novel method to accelerate large language model in-\nference by equipping models with additional predictive decoding heads. MEDUSA allows models\nto generate multiple tokens per step, overcoming the bottleneck of sequential auto-regressive de-\ncoding. We have demonstrated two procedures, MEDUSA-1 and MEDUSA-2, for efficiently training\nthese extra heads while preserving model performance. Experiments on models of varying sizes and\ntraining methods show consistent speedups of 2.3-3.6\u00d7 on single prompt inference across different\nprompt types and models.\nKey advantages of MEDUSA include its simplicity, parameter efficiency, and ease of integration into\nexisting systems. By building on top of speculative decoding concepts, MEDUSA avoids the need for\nspecialized draft models. The typical acceptance scheme also removes complications from rejection\nsampling while still providing reasonable outputs. Finally, the fine-tuning procedures ensure high-\nquality generations without affecting the performance of the original model.\nAcknowledgements\nWe extend our heartfelt gratitude to several individuals whose contributions were invaluable to this\nproject:\n\u2022 Zhuohan Li, for his invaluable insights on LLM serving. If you haven\u2019t already, do check out\nZhuohan\u2019s vLLM project\u2014it\u2019s nothing short of impressive.\n\u2022 Shaojie Bai, for engaging in crucial discussions that helped shape the early phases of this work.\n\u2022 Denny Zhou, for introducing the truncation sampling scheme to Tianle and encouraging Tianle to\nexplore the area of LLM serving.\n\u2022 Yanping Huang, for pointing out the memory-bound challenges associated with LLM serving to\nTianle.\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\u00b4on, and Sumit\nSanghai. Gqa: Training generalized multi-query transformer models from multi-head check-\npoints. arXiv preprint arXiv:2305.13245, 2023.\nAxolotl.\nAxolotl.\nhttps://github.com/OpenAccess-AI-Collective/axolotl,\n2023.\nSourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R. Varsh-\nney. {MIROSTAT}: A {neural} {text} {decoding} {algorithm} {that} {directly} {controls}\n{perplexity}. In International Conference on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=W1G1JZEIy5_.\n15\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. Accelerating large language model decoding with speculative sampling. February 2023.\ndoi: 10.48550/ARXIV.2302.01318.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nTim Dettmers, M. Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise\nquantization. International Conference on Learning Representations, 2021.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations, 2023.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback, 2023.\nStefan Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function\napproximation in reinforcement learning. Neural Networks, 2017. doi: 10.1016/j.neunet.2017.\n12.012.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics, 2018. doi: 10.18653/v1/p18-1082.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\nYichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the sequential dependency of llm\ninference using lookahead decoding, November 2023. URL https://lmsys.org/blog/\n2023-11-21-lookahead-decoding/.\nGoogle. Palm 2 technical report, 2023. URL https://ai.google/static/documents/\npalm2techreport.pdf.\nZhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative\ndecoding. arXiv preprint arXiv: 2311.08252, 2023.\nJohn Hewitt, Christopher D. Manning, and Percy Liang. Truncation sampling as language model\ndesmoothing. October 2022. doi: 10.48550/ARXIV.2210.15191.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n16\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=rygGQyrFvH.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. ICLR, 2021.\nJoao Gante. Assisted generation: a new direction toward low-latency text generation, 2023. URL\nhttps://huggingface.co/blog/assisted-generation.\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W\nMahoney, and Kurt Keutzer.\nSqueezellm: Dense-and-sparse quantization.\narXiv preprint\narXiv:2306.07629, 2023.\nYoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. EMNLP, 2016.\nAnanya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can\ndistort pretrained features and underperform out-of-distribution. International Conference on\nLearning Representations, 2022.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\ndecoding. November 2022. doi: 10.48550/ARXIV.2211.17192.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.\nAwq:\nActivation-aware weight quantization for llm compression and acceleration.\narXiv preprint\narXiv:2306.00978, 2023.\nXiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang.\nOnline speculative decoding. arXiv preprint arXiv: 2310.07177, 2023.\nClara Meister, Gian Wiher, Tiago Pimentel, and Ryan Cotterell. On the probability-quality paradox\nin language generation. March 2022. doi: 10.48550/ARXIV.2203.17217.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally typical sampling. Transac-\ntions of the Association for Computational Linguistics, 11:102\u2013121, 2023.\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,\nZhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating\ngenerative llm serving with speculative inference and token tree verification.\narXiv preprint\narXiv:2305.09781, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using\ndivergence frontiers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, 2021. URL https://openreview.\nnet/forum?id=Tqx7nJp7PR.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Lev-\nskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans-\nformer inference. November 2022. doi: 10.48550/ARXIV.2211.05102.\n17\nShareGPT.\nShareGPT.\nhttps://huggingface.co/datasets/Aeala/ShareGPT_\nVicuna_unfiltered, 2023.\nNoam Shazeer.\nFast transformer decoding: One write-head is all you need.\narXiv preprint\narXiv:1911.02150, 2019.\nBenjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. arXiv\npreprint arXiv:2308.04623, 2023.\nMitchell Stern, Noam M. Shazeer, and Jakob Uszkoreit.\nBlockwise parallel decoding for deep\nautoregressive models. Neural Information Processing Systems, 2018.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada,\nShengyi Huang, Leandro von Werra, Cl\u00b4ementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar\nSanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment,\n2023.\nHeming Xia, Tao Ge, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Lossless\nspeedup of autoregressive translation, 2023. URL https://openreview.net/forum?\nid=H-VlwsYvVi.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models. In International\nConference on Machine Learning, pages 38087\u201338099. PMLR, 2023a.\nYisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey\non non-autoregressive generation for neural machine translation and beyond. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2023b.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and\nTie-Yan Liu. Do transformers really perform badly for graph representation? Advances in Neural\nInformation Processing Systems, 34:28877\u201328888, 2021.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,\nYuandong Tian, Christopher R\u00b4e, Clark Barrett, et al. H 2 o: Heavy-hitter oracle for efficient\ngenerative inference of large language models. arXiv preprint arXiv:2306.14048, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\nYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh,\nSanjiv Kumar, Jean-Franc\u00b8ois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative\ndecoding via knowledge distillation. arXiv preprint arXiv: 2310.08461, 2023.\n18\n"
  },
  {
    "title": "Zero Bubble Pipeline Parallelism",
    "link": "https://arxiv.org/pdf/2401.10241.pdf",
    "upvote": "19",
    "text": "ZERO BUBBLE PIPELINE PARALLELISM\nPenghui Qi\u2217, Xinyi Wan\u2217, Guangxing Huang & Min Lin\nSea AI Lab\n{qiph,wanxy,huanggx,linmin}@sea.com\nABSTRACT\nPipeline parallelism is one of the key components for large-scale distributed train-\ning, yet its efficiency suffers from pipeline bubbles which were deemed inevitable.\nIn this work, we introduce a scheduling strategy that, to our knowledge, is the\nfirst to successfully achieve zero pipeline bubbles under synchronous training se-\nmantics. The key idea behind this improvement is to split the backward compu-\ntation into two parts, one that computes gradient for the input and another that\ncomputes for the parameters. Based on this idea, we handcraft novel pipeline\nschedules that significantly outperform the baseline methods.\nWe further de-\nvelop an algorithm that automatically finds an optimal schedule based on spe-\ncific model configuration and memory limit. Additionally, to truly achieve zero\nbubble, we introduce a novel technique to bypass synchronizations during the op-\ntimizer step. Experimental evaluations show that our method outperforms the\n1F1B schedule up to 23% in throughput under a similar memory limit. This\nnumber can be further pushed to 31% when the memory constraint is relaxed.\nWe believe our results mark a major step forward in harnessing the true po-\ntential of pipeline parallelism. We open sourced our implementation based on\nthe popular Megatron-LM repository on https://github.com/sail-sg/\nzero-bubble-pipeline-parallelism.\n1\nINTRODUCTION\nThe realm of distributed model training has become a focal point in the deep learning community,\nespecially with the advent of increasingly large and intricate models. Training these behemoths\noften requires a vast amount of GPUs interconnected with various topologies. Various parallelism\ntechniques have been proposed for training DNN in the past years. Data parallelism (DP) (Goyal\net al., 2017; Li et al., 2020) is the default strategy for models of small to moderate sizes due to\nits simplicity. Beyond a model size, it is no longer possible to fit the model parameters in one\nsingle GPU. This is when model parallelism comes to the rescue (Harlap et al., 2018; Huang et al.,\n2019; Fan et al., 2021; Zheng et al., 2022). There are two main model parallel schemes, tensor\nparallelism (TP) and pipeline parallelism (PP). TP splits the matrix multiplication in one layer to\nseveral devices, while PP segments the entire model into different stages which can be processed\nacross different devices. Notably, ZeRO (Rajbhandari et al., 2020) provides a strong alternative to\nmodel parallelism by sharding parameters across devices, while keeping the simplicity of DP.\nRecent research indicates that achieving optimal performance in large-scale training scenarios re-\nquires a non-trivial interaction of DP, TP and PP strategies. In the abundance of interconnection\nresources, e.g. NVLink between GPUs within one compute node, a hybrid of DP, TP and ZeRO\nstrategies works efficiently. Whereas there are numerous empirical evidences Fan et al. (2021);\nZheng et al. (2022); Narayanan et al. (2021) showing PP is particularly advantageous for utilizing\ncross-server connections, especially at the scale of thousands of GPUs. This highlights the primary\naim of our work: enhancing the efficiency of PP.\nGoing deeper into the intricacies of PP, the efficiency of its implementation relies heavily on the\namount of device idle time referred to as pipeline bubbles. Due to the dependency between lay-\ners, bubbles seem inevitable. A prominent early work to address this issue is GPipe (Huang et al.,\n2019), which attempts to reduce the bubble ratio by increasing the number of concurrent batches\n\u2217Equal Contributors\n1\narXiv:2401.10241v1  [cs.DC]  30 Nov 2023\nin the pipeline. However, a direct consequence of this is an increase in peak memory demands.\nTo mitigate this, GPipe discards part of the intermediate activations while recomputing them dur-\ning the backward pass. Yet, this approach introduced a computation overhead of around 20% (Fan\net al., 2021). One line of work that improves over GPipe focuses on asynchronous PP, including\nPipeDream (Harlap et al., 2018), PipeMare (Yang et al., 2021). Asynchronous PP is theoretically\nbubble free, they greatly improve pipeline efficiency, however, at the sacrifice of exact optimiza-\ntion semantics. On the other hand, improvements are also made under synchronous settings. A\nnotable scheduling strategy to address the limitation of GPipe is called one-forward-one-backward\n(1F1B). It was first proposed in PipeDream (Harlap et al., 2018) under the asynchronous setting,\nand later introduced under synchronous settings (Fan et al., 2021; Narayanan et al., 2021). 1F1B\noffers faster memory clearance by early scheduling the backward passes. With the same number of\nmicrobatches, it yields similar bubble ratios but with a distinct advantage in peak memory. Based\non 1F1B, Narayanan et al. (2021) introduced the 1F1B interleaved strategy. By assigning multiple\nstages to the same device, it further reduces the bubble size at the cost of more communication and\nhigher peak memory.\nDespite various efforts, to this date the remaining bubbles still pose the largest issue for PP under\nsynchronous training semantics. In this work, we spotted the opportunity that PP can be further\noptimized by representing and scheduling the computation graph at a finer granularity. Classical\ndeep learning frameworks are designed at the granularity of layers, whereas modern deep learning\ncompilers use different intermediate representations for optimizations at various levels. (Chen et al.,\n2018; Roesch et al., 2018; Sabne, 2020; Tillet et al., 2019; Lattner et al., 2020). Although a finer\ngranularity always means a larger space for searching, it is often impeded by the lack of optimization\ntools to navigate the space. Therefore, choosing a suitable granularity is crucial.\n\u01c1\u01be\u06c2\u01de\u01b6\n\u06c2\u01dc\u01b6\n\u01c8\u063f(\u01de)\n\u01c8\u01de\u06c2\u01dd\u01b6\n\u06c2\u01de\u01b6\n\u06c2\u01de\u01b6\u01dc\u01be\n\u06c2\u01de\u01b6\u06c2\u01c1\u01b6\n\u06c2\u01dd\u01b6\n\u01c1\u01dc\n\u01dc\n\u063f(\u01de)\n\u01de\n\u01dd\n\u01c1\nB\nF\nW\nForward\nBackward\n\u01b8 \u00d7\n\u00d7 \u01b8\nFigure 1: Computation Graph for MLP.\nTraditionally, neural networks are granularized as stacked layers. There are two functions associated\nwith each layer, forward and backward. In the forward pass, the input x is transformed into the out-\nput y with the parameterized mapping f(x, W ). The backward pass, crucial for training, involves\ntwo computations: \u2207xf(x, W )\u22a4 d\u2113\ndy and \u2207W f(x, W )\u22a4 d\u2113\ndy. Correspondingly, they compute the\ngradient with respect to the input x and the layer\u2019s parameters W . For convenience, we use single\nletters B and W to denote these two computations respectively, and F to denote forward pass (Figure\n1). Traditionally, B and W are grouped and provided as a single backward function. This design is\nconceptually friendly to the user, and it happens to work well for DP, because the communication\nof the weights\u2019 gradient at layer i can be overlapped with the backward computation at layer i \u2212 1.\nHowever, in PP, this design unnecessarily increases the sequentially dependent computations, i.e. B\nat the layer i \u2212 1 depends on W at the layer i, which is usually detrimental for the efficiency of the\npipeline.\nBased on split B and W, we present new pipeline schedules that greatly improve pipeline efficiency.\nThe remainder of this paper is organized as follows: In Section 2, we introduce handcrafted sched-\n2\nules based on an ideal assumption that the execution times of F, B and W are identical. Subsequently,\nin Section 3, we remove this assumption and propose an automatic scheduling algorithm that works\nunder more realistic conditions. To achieve zero bubble, Section 4 details a method that sidesteps the\nneed for synchronization during the optimizer step, yet preserves synchronous training semantics.\nWe conduct empirical evaluations of our methods against baseline methods under diverse settings\nin Section 5. In addition, to further reduce the memory requirements to achieve zero bubble, we\npropose a novel scheduling mechanism, and evaluate its performance in Section 6.\nWe should note that we do not aim to explore general mixed strategies for large scale distributed\ntraining. Instead, we specifically target to improve the efficiency of pipeline scheduling, supported\nwith apple to apple comparisons with baselines. Our method is orthogonal to DP, TP and ZeRO\nstrategies, and it can be used as a parallel replacement for the PP part in large scale training.\n2\nHANDCRAFTED PIPELINE SCHEDULES\nBased on the key observation that splitting B and W could reduce sequential dependency and thus\nimprove efficiency, we redesign the pipeline starting from the commonly utilized 1F1B schedule. As\ndepicted in Figure 2, 1F1B initiates with a warm-up phase. In this phase, workers conduct varying\nnumbers of forward passes, with each stage typically performing one more forward pass than its\nimmediately subsequent stage. Following the warm-up phase, each worker transits to a steady state\nwhere they alternately execute one forward pass and one backward pass, ensuring an even workload\ndistribution among stages. In the final phase, each worker processes the backward passes for the\noutstanding in-flight microbatches, completing the batch.\nIn our improved version we split the backward pass into B and W passes, it is imperative that F\nand B from the same microbatch must still remain sequentially dependent across pipeline stages.\nHowever, W can be flexibly scheduled anywhere after the corresponding B of the same stage. This\nallows for strategic placement of W to fill the pipeline bubbles. There are many possible schedules\nthat improve over 1F1B, trading off differently on the bubble size and the memory footprint. We\nintroduce two particularly interesting handcrafted schedules in this section to show the great poten-\ntial of finer granularity at reducing pipeline bubbles (see Figure 3). For the sake of clarity in our\ninitial design, we assume that the time costs for F, B, and W are identical, an assumption shared by\nearlier studies (Narayanan et al., 2021; Huang et al., 2019). However, in Section 3, we re-evaluate\nthis assumption to optimize scheduling efficiency in real-world scenarios.\nDevice 1\n1\n2\n3\n4\n1\n5\n2\n6\n3\n7\n4\n8\n5\n6\n7\n8\n1\n2\n3\n4\nDevice 2\n1\n2\n3\n1\n4\n2\n5\n3\n6\n4\n7\n5\n8\n6\n7\n8\n1\n2\n3\nDevice 3\n1\n2\n1\n3\n2\n4\n3\n5\n4\n6\n5\n7\n6\n8\n7\n8\n1\n2\nDevice 4\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\nTime\n\u2192\nForward\nBackward\nOptimizer step\nFigure 2: 1F1B pipeline schedule.\nDevice 1\n1\n2\n3\n4\n1\n1\n5\n2\n2\n6\n3\n3\n7\n4\n4\n8\n5\n5\n6\n6\n7\n7\n8\n8\n1\n2\n3\n4\n1\n1\n5\nDevice 2\n1\n2\n3\n1\n4\n2\n1\n5\n3\n2\n6\n4\n3\n7\n5\n4\n8\n6\n5\n7\n6\n8\n7\n8\n1\n2\n3\n1\n4\n2\n1\nDevice 3\n1\n2\n1\n3\n2\n4\n3\n1\n5\n4\n2\n6\n5\n3\n7\n6\n4\n8\n7\n5\n8\n6\n7\n8\n1\n2\n1\n3\n2\n4\n3\nDevice 4\n1\n1\n2\n2\n3\n3\n4\n4\n1\n5\n5\n2\n6\n6\n3\n7\n7\n4\n8\n8\n5\n6\n7\n8\n1\n1\n2\n2\n3\n3\n4\nTime\n\u2192\nDevice 1\n1\n2\n3\n4\n5\n6\n7\n1\n1\n8\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\n2\n3\n4\n5\n6\n7\n1\n1\n8\n2\n2\n3\nDevice 2\n1\n2\n3\n4\n5\n1\n6\n2\n7\n3\n1\n8\n4\n2\n5\n3\n6\n4\n7\n5\n8\n6\n7\n8\n1\n2\n3\n4\n5\n1\n6\n2\n7\n3\n1\n8\nDevice 3\n1\n2\n3\n1\n4\n2\n5\n3\n6\n4\n7\n5\n1\n8\n6\n2\n7\n3\n8\n4\n5\n6\n7\n8\n1\n2\n3\n1\n4\n2\n5\n3\n6\n4\n7\nDevice 4\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n1\n8\n8\n2\n3\n4\n5\n6\n7\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\nTime\n\u2192\nF\nB\nW\nOptimizer step\nFigure 3: Handcrafted pipeline schedules, top: ZB-H1; bottom: ZB-H2\n3\n2.1\nMEMORY EFFICIENT SCHEDULE\nOur first handcrafted schedule, named ZB-H1, ensures that the maximum peak memory usage over\nall workers doesn\u2019t exceed that of 1F1B. ZB-H1 generally follows the 1F1B schedule, but it adjusts\nthe starting points of W depending on the number of warm-up microbatches. This ensures all work-\ners maintain the same number of in-flight microbatches. As a result, as seen in Figure 3 (top), the\nbubble size is reduced to a third of 1F1B\u2019s size. This reduction is because B is initiated earlier across\nall workers compared to 1F1B, and the tail-end bubbles are filled by the later-starting W passes. As\nW typically uses less memory than B (Table 1), the first worker has the maximum peak memory\nusage which is consistent with 1F1B.\n2.2\nZERO BUBBLE SCHEDULE\nWhen we permit a larger memory footprint than 1F1B and have a sufficient number of microbatches,\nit\u2019s possible to achieve a zero bubble schedule, which we label as ZB-H2. As illustrated in Fig-\nure 3 (bottom), we introduce more F passes during the warm-up phase to fill the bubble preceding\nthe initial B. We also reorder the W passes at the tail, which changes the layout from trapezoid\ninto a parallelogram, eliminating all the bubbles in the pipeline. It is important to highlight that the\nsynchronization between the optimizer steps is removed here, we discuss how this is safely done in\nSection 4.\n2.3\nQUANTITATIVE ANALYSES\nWe use p to denote the number of stages and b to denote the size of each microbatch. For transformer\narchitecture, we denote the number of attention heads as a, the sequence length as s and the hidden\ndimension size as h. We use the notations MB/MW to represent the memory required to store\nactivations for one B/W pass, and TF /TB/TW to represent the running time for one F/B/W pass.\nFor simplicity, we only do quantitative analyses on transformer architecture (Vaswani et al., 2017),\nusing a typical setting similar to GPT-3 (Brown et al., 2020) where the hidden dimension size inside\nfeedforward is 4h and the dimension size for each attention head is h/a.\nAs in Narayanan et al. (2021), we only consider matmul operations when calculating FLOPs because\nthey contribute most of the computations in a transformer layer. For each matmul operation in the\nforward pass, there are two matmul operations with the same FLOPs in corresponding backward pass\n(see Figure 1), each of which belongs to either B or W. The approximate formula for calculating the\nFLOPs of a transformer layer is in Table 1. We can see that TW < TF < TB and TB + TW = 2TF .\nWe use the same method in Korthikanti et al. (2023) to estimate activations memory required for\nB. After B completes, it releases some activations not used anymore but keeps some extra gradients\n(\u2207zL in Figure 1) for W. The total memory required by W, as in Table 1, is less than B.\nTable 1: FLOPs and activations memory required per transformer layer for each pass\nPass\nFLOPs\nActivations Memory Required\nF\nsbh(24h + 4s)\n0\nB\nsbh(24h + 8s)\nsb(34h + 5as)\nW\nsbh(24h)\n32sbh\nWithout the assumption of TF = TB = TW , the peak activations memory and bubble size of ZB-\nH1 and ZB-H2 are quantified in Table 2. Notably, the activations memory of worker i is (p \u2212 i +\n1)MB + (i \u2212 1)MW for ZB-H1 and (2p \u2212 2i + 1)MB + (2i \u2212 2)MW for ZB-H2. As in Table\n1, the activations memory required for W is smaller than that for B. Therefore, the peak activations\nmemory is pMB and (2p \u2212 1)MB, for ZB-H1 and ZB-H2 respectively.\n3\nAUTOMATIC PIPELINE SCHEDULING\nWhile handcrafted schedules offer simplicity and better comprehensibility, they face several issues\nin practical applications. For one, scheduling under the assumption that TF = TB = TW introduces\n4\nTable 2: Comparison between 1F1B and our handcrafted schedules.\nSchedule\nBubble size\nPeak activations memory\n1F1B\n(p \u2212 1)(TF + TB + TW )\npMB\nZB-H1\n(p \u2212 1)(TF + TB \u2212 TW )\npMB\nZB-H2\n(p \u2212 1)(TF + TB \u2212 2TW )\n(2p \u2212 1)MB\nunwanted bubbles, especially for models where these values differ significantly. Moreover, com-\nmunication time (denoted as Tcomm) required to transfer activation/gradient between stages is often\nignored in handcrafted schedules, leading to noticeable latencies in the pipeline stream. Finally,\nstriking a balance between minimizing bubble size and adhering to memory limit becomes particu-\nlarly challenging when the available memory is insufficient to accommodate enough microbatches\nfor a bubble-free schedule.\nTo address these challenges and ensure generalization to practical scenarios, we propose algorithms\nto automatically search the optimal schedule given the number of pipeline stages p, the number of\nmicrobatches m, the activations memory limit Mlimit, and the running time estimations TF , TB,\nTW and Tcomm. We design a heuristic strategy, which always generates an optimal or near optimal\nsolution especially when m is large enough. We also systematically formulate the problem as Integer\nLinear Programming (for more details see Appendix G), which can be solved by an off-the-shelf\nILP solver (Forrest & Lougee-Heimer, 2005) when the problem is under a certain scale. These two\napproaches can be combined: first, use the heuristic solution as initialization, and then optimize it\nfurther with ILP.\n3.1\nTHE HEURISTIC ALGORITHM\nWe present our heuristic algorithm in the following steps:\n\u2022 In the warm-up phase, within the memory limit, we schedule as many F passes as possible to\nminimize the bubble before the first B. The resulting schedule may still have a small bubble (less\nthan TF ) before the first B if not reaching memory limit, where scheduling another F may delay\nthe following B. We use a binary hyperparameter to control whether to do it or not.\n\u2022 After the warm-up phase, we adhere to the pattern where one F and one B are scheduled itera-\ntively. We insert W to fill the bubble when there is a gap larger than TW . When a bubble occurs\nbut the size is less than TW , we still insert a W if the current bubble makes the largest cumulative\nbubble size among all stages become larger. We also insert W to recycle some memory when the\nmemory limit is hit. Typically, our heuristic strategy enters a steady state that follows 1F-1B-1W\npattern.\n\u2022 Throughout this process, pipeline stage i is always guaranteed to schedule at least one more F\nthan stage i + 1 anytime before F is used up. When this difference exceeds one, we use another\nbinary hyperparameter to decide whether to skip one F in pipeline stage i if it doesn\u2019t cause more\nbubbles. We perform a grid search to find the best combination of hyperparameters.\n\u2022 In each stage, when F and B passes run out, we schedule all the left W passes one by one.\n4\nBYPASSING OPTIMIZER SYNCHRONIZATIONS\nIn most practices of PP, synchronizations over pipeline stages are usually performed in optimizer\nstep for the sake of numerical robustness. For example, a global gradient norm needs to be com-\nputed for gradient norm clipping (Pascanu et al., 2013); a global check for NAN and INF values\nare performed in the mixed precision settings (Micikevicius et al., 2017); both of them require an\nall-reduce communication across all stages. However, synchronization at the optimizer step de-\nstroys the parallelogram (Figure 3) and makes zero bubble impossible. In this section, we propose\nan alternative mechanism to bypass these synchronizations, while still maintaining a synchronous\noptimization semantics.\n5\nIn existing implementations, an all-reduce communication is first launched to collect the global\nstates, followed by the optimizer steps which are conditioned on the global states. However, we\nnoticed that most of the time the global states have no effects, e.g., the global check for NAN and\nINF rarely trigger because in a robust setting most iterations shouldn\u2019t have numerical issues; the\ngradient clipping rate is also quite low empirically to justify a synchronization of global gradient\nnorm at every iteration.\nBased on these observations, we propose to replace the before-hand synchronizations with a post\nupdate validation. The idea is illustrated in Figure 4, at each stage before the optimizer step, a\npartially reduced global state is received from the previous stage, combined with the current stage\u2019s\nlocal state, and passed on to the next stage. The optimizer step of each stage is controlled by\nthe partially reduced state, e.g. skip the update when a NAN is spotted or the partially reduced\ngradient norm exceeds the clipping threshold. During the warm-up phase of the next iteration, the\nfully reduced global state is then propagated back from the last stage to first stage. Upon receiving\nthe global state, each stage performs a validation to decide whether the previous optimizer step is\nlegitimate. If an amendment to the gradient is required, a rollback will be issued (for more details\nsee Appendix C) and then we redo the optimizer step based on the fully reduced global state.\nRollback if validation fails\n5 - 8\nPropagate globally reduced\nvalue to each stage\nReduce local values by\npropagating from 1 to 4\n1 2 3 4\nOptimizer step\n1\n2\n3\n4\n5\n6\n8\n7\nFigure 4: The post-validation strategy to replace optimizer synchronization.\n5\nEXPERIMENTS\n5.1\nSETUP\nWe base our implementation on the open-source Megatron-LM project (Narayanan et al., 2021)\nand assess its performance using models analogous to GPT-3 (Brown et al., 2020), as detailed in\nTable 3. During our experiments, we first conducted a specific number of iterations for profiling,\ncollecting empirical measurements for TF , TB, TW , and Tcomm. After obtaining these values, we fed\nthem into our automatic pipeline scheduling algorithm to determine the optimal schedule. It\u2019s worth\nnoting that both the initial and final pipeline stages possess one fewer transformer layer compared\nto the intermediate stages. This design is to compensate for the extra embedding lookup and loss\ncomputations in the initial and final stages so that they won\u2019t become the bottleneck and cause\nbubbles to other stages.\nTable 3: Models and fixed settings used in experiments\nModel\nLayers\nAttention\nHidden\nSequence\nPipelines\nMicrobatch\nNumber of\nHeads\nSize\nLength\n(GPUs)\nSize\nMicrobatches\n1.5B\n22\n24\n2304\n1024\n8\n6\n24 / 32 / 64\n6.2B\n30\n32\n4096\n1024\n8\n3\n24 / 32 / 64\n14.6B\n46\n40\n5120\n1024\n16\n1\n48 / 64 / 128\n28.3B\n62\n48\n6144\n1024\n32\n1\n96 / 128 / 256\nCompared methods:\n\u2022 ZB-1p: Automatically searched schedule with the activation memory limited to pMB, which\ntheoretically has the same peak memory as 1F1B.\n\u2022 ZB-2p: Automatically searched schedule with the activation memory limited to 2pMB, which is\nthe least amount of memory to empirically achieve close to zero bubble (see Figure 7).\n6\n\u2022 1F1B and 1F1B-I: 1F1B and interleaved 1F1B methods introduced by Harlap et al. (2018) and\nNarayanan et al. (2021) with implementation from Megatron-LM. For interleaved 1F1B, the\nentire model is divided into a sequence of chunks, which are cyclically taken by each stage,\nforming an interleaved pipeline. In our interleaved experiments, we always use the maximum\nnumber of chunks to ensure least bubble, i.e. each transformer layer serves as a chunk.\nOur experiments utilize up to 32 NVIDIA A100 SXM 80G GPUs distributed across 4 nodes inter-\nconnected by a RoCE RDMA network. The running time of each iteration is recorded after several\nwarm-up iterations. Thanks to the reproducibility provided by Megatron-LM implementation, we\ncan verify the correctness of ZB-1p and ZB-2p without running models until convergence. We use\na fixed random seed to initialize the model, record the loss after every iteration for ZB-1p, ZB-2p,\nand 1F1B, and then verify that they\u2019re bit-to-bit identical.\n5.2\nMAIN RESULTS\n24 microbatches\n32 microbatches\n64 microbatches\n11\n12\n13\n14\n15\n16\n17\nsamples per second per gpu\n1.5B Model, 8GPUs\n1F1B\n1F1B-I\nZB-1p\nZB-2p\nupper bound\n24 microbatches\n32 microbatches\n64 microbatches\n3.25\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\nsamples per second per gpu\n6.2B Model, 8GPUs\n1F1B\n1F1B-I\nZB-1p\nZB-2p\nupper bound\n48 microbatches\n64 microbatches\n128 microbatches\n1.4\n1.6\n1.8\n2.0\nsamples per second per gpu\n14.6B Model, 16GPUs\n1F1B\n1F1B-I\nZB-1p\nZB-2p\nupper bound\n96 microbatches\n128 microbatches\n256 microbatches\n0.7\n0.8\n0.9\n1.0\n1.1\nsamples per second per gpu\n28.3B Model, 32GPUs\n1F1B\n1F1B-I\nZB-1p\nZB-2p\nupper bound\nFigure 5: Comparison of throughput across different pipeline schedules.\nTable 4: Experiment result details\nModel\n1.5B\n6.2B\n14.6B\n28.3B\nSetup\n#GPU\n8\n8\n16\n32\n#Microbatch\n24\n32\n64\n24\n32\n64\n48\n64\n128\n96\n128\n256\nSamples\nZB-2p\n14.5 14.8 14.9 4.32 4.35 4.39 1.81 1.83 1.85 0.99 1.00 1.00\nper GPU\nZB-1p\n12.9 13.4 14.2 3.88 4.00 4.20 1.61 1.67 1.76 0.87 0.90 0.96\nper second\n1F1B\n11.8 12.5 13.6 3.50 3.70 4.03 1.40 1.49 1.64 0.76 0.80 0.88\n1F1B-I\n13.1 13.4 13.9 4.01 4.08 4.19 1.54 1.59 1.66 0.82 0.85 0.90\nZB-2p\n59\n59\n59\n70\n70\n70\n51\n51\n51\n74\n74\n74\nMemory\nZB-1p\n32\n32\n32\n42\n42\n42\n33\n33\n33\n44\n44\n44\n(GB)\n1F1B\n30\n30\n30\n39\n39\n39\n32\n32\n32\n43\n43\n43\n1F1B-I\n40\n40\n40\n48\n48\n48\n39\n39\n39\n58\n58\n58\nWe present the throughput of all methods in Figure 5, and leave the additional details for each setup\nin Table 4. Our experiments demonstrate that ZB-2p consistently outperforms all other methods\nacross various settings. Notably, the throughput of 1F1B, 1F1B-I and ZB-1p show a strong positive\ncorrelation with the number of microbatches. In contrast, ZB-2p maintains the efficiency even with\nfewer microbatches. This is because the bubble rate in ZB-2p has almost reached zero (Table 5),\n7\nand its throughput is already close to the upper bound. Here the upper bound is roughly estimated\nby multiplying the throughput of 1F1B and\n1\n1\u2212bubble rate of 1F1B (for more details see Section 5.3).\nAs mentioned before, the improved efficiency of ZB-2p comes at the cost of a higher memory\nconsumption compared to the 1F1B baseline. We also compare ZB-2p with 1F1B under the same\nmemory consumption in Appendix F, and the experimental results also show that ZB-2p achieves a\nhigher throughput even with half microbatch size compared to 1F1B.\nIn contrast, ZB-1p is designed to have a peak memory cost similar to the 1F1B baseline. It shows a\ncomparable throughput to 1F1B-I in the 8 GPUs setups. In multi-node setups where communication\nbandwidth is more of a bottleneck, ZB-1p clearly outperforms 1F1B-I, highlighting its advantage in\nreducing pipeline bubbles without incurring extra communication cost.\nIn most of our settings we set number of microbatches m larger than number of stages p because\nthey\u2019re more common use cases of pipeline parallelism. However we conducted experiments listed\nin Appendix H for m \u2264 p cases which shows 20% to 30% improvements with a similar memory\nconsumption.\n5.3\nEFFICIENCY OF AUTOMATIC SCHEDULING\nTable 5: Bubble rates of 1F1B, 1F1B-I, ZB-H1, ZB-H2, ZB-1p, ZB-2p under different settings.\nModel\n#Stage (p)\n#Microbatch (m)\n1F1B\n1F1B-I\nZB-H1\nZB-H2\nZB-1p\nZB-2p\n1.5B\n8\n24\n0.2431\n0.1055\n0.1585\n0.1083\n0.1585\n0.0433\n32\n0.1985\n0.0818\n0.1242\n0.0837\n0.1242\n0.0039\n64\n0.1240\n0.0443\n0.0674\n0.0444\n0.0674\n0.0026\n6.2B\n8\n24\n0.2347\n0.0808\n0.1323\n0.0698\n0.1323\n0.0029\n32\n0.1898\n0.0628\n0.1045\n0.0559\n0.1045\n0.0022\n64\n0.1091\n0.0320\n0.0554\n0.0294\n0.0554\n0.0010\n14.6B\n16\n48\n0.2552\n0.1104\n0.1397\n0.0672\n0.1397\n0.0066\n64\n0.2082\n0.0852\n0.1088\n0.0516\n0.1088\n0.0054\n128\n0.1251\n0.0445\n0.0576\n0.0266\n0.0576\n0.0028\n28.3B\n32\n96\n0.2646\n0.1493\n0.1421\n0.0641\n0.1421\n0.0038\n128\n0.2168\n0.1164\n0.1106\n0.0490\n0.1106\n0.0029\n256\n0.1352\n0.0624\n0.0594\n0.0257\n0.0594\n0.0018\nWe study the efficiency of the schedules generated from our automatic scheduling algorithm. The\nsame setups as our main experiments are used, however, since our purpose is to study the efficiency\nof the automatic scheduling algorithm, the numbers here are based on theoretical calculations instead\nof real experiments. To quantify the efficiency of a pipeline schedule, we introduce the concept of\nbubble rate, which is calculated as (cost \u2212 m(TF + TB + TW ))/cost. The cost here is defined as\nthe largest execution time of all stages, calculated for each schedule using profiled TF , TB, TW and\nTcomm values. The m(TF + TB + TW ) is the optimal execution time when all communications are\noverlapped with computations and hence no bubbles in the pipeline.\nThe bubble rates for different schedules are presented in Table 5. We include the handcrafted sched-\nules ZB-H1 and ZB-H2 as baselines to the automatically searched schedules. In most of the settings,\nZB-2p produces a bubble rate of less than 1%, which is the best among all schedules. In contrast,\nZB-H2 consistently performs worse than ZB-2p. This provides a strong evidence that our automatic\nscheduling algorithm adapts better to realistic scenarios by using more accurate estimates of TF ,\nTB, TW and Tcomm. On the contrary, this improvement is not observed in ZB-1p vs ZB-H1, hy-\npothetically because the memory limit becomes the dominate factor. Notably, all of our methods\nsignificantly outperform 1F1B.\nWe also plot ZB-2p and its profiled real execution on 16 GPUs to provide a direct visual evidence\nthat it is truly a zero bubble schedule. As shown in Figure 6, the automatically generated ZB-2p\nschedule has almost no bubble. The profiled execution has slightly more bubbles but retains a good\noverall alignment.\n8\nDevice 0\nDevice 1\nDevice 2\nDevice 3\nDevice 4\nDevice 5\nDevice 6\nDevice 7\nDevice 8\nDevice 9\nDevice 10\nDevice 11\nDevice 12\nDevice 13\nDevice 14\nDevice 15\nDevice 0\nDevice 1\nDevice 2\nDevice 3\nDevice 4\nDevice 5\nDevice 6\nDevice 7\nDevice 8\nDevice 9\nDevice 10\nDevice 11\nDevice 12\nDevice 13\nDevice 14\nDevice 15\nTime\nF\nB\nW\nOptimizer Step\nFigure 6: A schedule produced by ZB-2p (top) and its profiled execution process (bottom).\n1.0pMB\n2.0pMB\n3.0pMB\nMlimit\n0.00\n0.05\n0.10\n0.15\nBubble rate\n1.5B Model, p = 8\n#Microbatches = 24\n#Microbatches = 32\n#Microbatches = 64\n1.0pMB\n2.0pMB\n3.0pMB\nMlimit\n6.2B Model, p = 8\n#Microbatches = 24\n#Microbatches = 32\n#Microbatches = 64\n1.0pMB\n2.0pMB\n3.0pMB\nMlimit\n14.6B Model, p = 16\n#Microbatches = 48\n#Microbatches = 64\n#Microbatches = 128\n1.0pMB\n2.0pMB\n3.0pMB\nMlimit\n28.3B Model, p = 32\n#Microbatches = 96\n#Microbatches = 128\n#Microbatches = 256\nFigure 7: The relation between memory limit and bubble rate using our heuristic algorithm.\n5.4\nMEMORY LIMIT\nTo better understand the effect of memory limit, we study the relationship of the bubble rate to\nMlimit. We run our heuristic algorithm with a series of Mlimit and plot them in Figure 7. Initially, the\nbubble rate shows a close-to-linear decreasing trend as we increase the value of Mlimit. Theoretically,\nthe curve should plateau around (p\u22121)(TB+2Tcomm)+pTF\nTF\nMB. Empirically, we find 2pMB a good\nthreshold for achieving close to zero bubble rate when TF \u2248 TB and Tcomm is relatively small.\nBeyond the inflection point, although a sufficiently large memory limit does result in a theoretically\nzero bubble rate, in general the cost outweighs the gain. For more details see Appendix B.\n6\nMEMORY EFFICIENT ZERO BUBBLE SCHEDULE\nDevice 1\n1 2 3 4 5 6 7 1 1 1 2 2 2 3 3 3 4 4 4 8 1 1 5 5 5 2 2 6 6 6 3 3 7 7 7 4 4 8 8 8 5 5 6 6 7 7 8 8\n1 2 3 4 5 6 7 1 1 1 2 2 2 3 3 3 4 4 4 8 1 1 5 5 5 2\nDevice 2\n1 2 3 4 5 1 6 2 1 1 3 2 2 4 3 3 7 1 1 5 4 4 8 2 2 6 5 5 3 3 7 6 6 4 4 8 7 7 5 8 6 5 7 8 8 6 7 8\n1 2 3 4 5 1 6 2 1 1 3 2 2 4 3 3 7 1 1 5 4 4 8 2 2\nDevice 3\n1 2 3 1 4 2 5 3 1 1 4 2 2 6 1 1 5 3 3 7 2 2 6 4 4 8 3 3 7 5 5 4 4 8 6 6 5 7 6 8 7 5 8 7 6 8 7 8\n1 2 3 1 4 2 5 3 1 1 4 2 2 6 1 1 5 3 3 7 2 2 6 4\nDevice 4\n1 1 2 2 3 3 4 4 1 1 5 1 1 5 2 2 6 2 2 6 3 3 7 3 3 7 4 4 8 4 4 8 5 5 5 6 6 7 7 8 8 5 6 6 7 7 8 8\n1 1 2 2 3 3 4 4 1 1 5 1 1 5 2 2 6 2 2 6 3 3 7\nTime\n\u2192\nF\nB\nW\nOptimizer step\nFigure 8: ZB-V schedule. Each device is assigned to exactly 2 chunks, where white text colors\nrepresent the first chunk and black text colors represent the second chunk. The sequence of depen-\ndencies among model chunks follows a \u201dV\u201d shape pattern for both the forward and backward passes.\n9\nWhile ZB-2p can effectively achieve nearly zero bubble, it comes at the cost of doubling the mem-\nory consumption compared to 1F1B. This increased memory requirement poses limitations on its\npractical applicability in real-world scenarios. To address this concern, we design ZB-V, a schedul-\ning approach that achieves minimal idle time within the same memory constraints as 1F1B. Inspired\nby the interleaved 1F1B strategy proposed by Narayanan et al. (2021), our method evenly divides\nthe entire model into exactly 2p chunks, assigning two chunks to each worker. In contrast to an\ninterleaved scheme, our method involves sequentially allocating model chunks to workers, starting\nfrom the first worker and progressing to the last, then reversing the order from the last worker back\nto the first, creating a distinctive \u201dV\u201d shape (see the forward passes of the first microbatch in Figure\n8). For instance, in partitioning a 16-layer transformer model for a 4-stage pipeline, we allocate\nlayers 1-2 and layers 15-16 to worker 1, layers 3-4 and layers 13-14 to worker 2, and so forth.\nThis approach ensures that both the forward pass and backward pass for each microbatch originate\nfrom the same worker, which differentiates from previous methods like 1F1B and interleaved 1F1B,\nwhere the forward pass starts from the first worker while the backward pass begins from the last\nworker. This distinction offers two notable advantages: firstly, the first worker can initiate the back-\nward pass promptly without waiting for backward passes from the last worker to return, resulting\nin faster memory clearance and reduced memory requirements to achieve minimal idle time. Un-\nder the condition TF = TB = TW , ZB-V achieves zero bubble with a peak activations memory\nof pMB, aligning with the maximum peak memory usage of 1F1B. Notably, this is nearly half the\nmemory requirement compared to ZB-H2, which utilizes (2p \u2212 1)MB. Secondly, the peak memory\nusage is inherently balanced across all workers. This equilibrium arises due to uniform computation\nworkloads and consistent memory consumption across all model chunks.\nIn Figure 8, the scheduling strategy of ZB-V unfolds in three distinct phases. In the initial warm-up\nphase, each worker (denoted as i) performs a total of 2p \u2212 1 forward passes, comprising 2p \u2212 i\npasses for the first chunk and i\u22121 passes for the second chunk. Following the warm-up, all workers\ntransition into a steady phase characterized by a repetitive 1F-1B-1W pattern. During the steady\nphase, workers execute groups of computations, specifically F-B-W, with each group corresponding\nto a specific chunk. For a given worker i, the process initiates with the execution of p \u2212 i groups\nfor the second chunk. Subsequently, the worker alternates between processing one group for the\nsecond chunk and one group for the first chunk. This pattern continues until all forward passes are\nprocessed. In the final phase, each worker focuses on handling the remaining B and W computations,\nwith B being prioritized and W filling the bubbles.\nWe employ a similar heuristic algorithm as described in Section 3.1 to automatically search for the\noptimal schedule, considering parameters such as the number of pipeline stages p, the number of\nmicrobatches m, the activations memory limit Mlimit, and the profiled running times TF , TB, TW ,\nand Tcomm. As the memory distribution is inherently balanced across all workers during the warm-up\nand steady phases, we can straightforwardly shift all W to the right, within the memory constraint.\nThis modification enables the effective utilization of additional W to fill the bubbles in the schedule\u2019s\ntail, primarily arising from the comparatively shorter duration of W compared to F and B (for more\ndetails see Appendix D).\n6.1\nEVALUATION\nIn Table 6, we conduct a comprehensive performance comparison among 1F1B, ZB-1p, ZB-2p\nand ZB-V. To ensure fair memory consumption assessments, we adjust the ZB-2p configuration by\nhalving the microbatch size and doubling the number of microbatches (denoted as ZB-2p*), thus\nmaintaining a consistent global batch size across all methods.\nThe experimental results indicate that ZB-V consistently outperforms 1F1B and ZB-1p across di-\nverse settings, demonstrating comparable performance with ZB-2p*.\nTo delve deeper into the\ncomparison between ZB-2p* and ZB-V, we conduct an ablation study examining how throughput\nchanges with increasing the microbatch size in Table 7. Larger batch sizes empirically enhance GPU\nutilization and overall efficiency. The results show a noteworthy 8% improvement for the 14.6B and\n28.3B models when increasing the microbatch size from 1 to 2. However, the improvement is more\nmodest (less than 3%) for the 6.2B model, as the microbatch size is already sufficiently large. This\nexplains why ZB-2p* outperforms ZB-V in this scenario. In conclusion, there exists a trade-off\n10\nTable 6: Comparison between 1F1B, ZB-1p, ZB-2p and ZB-V under the same memory consump-\ntion. It\u2019s important to note that we adopt a distinct configuration for ZB-2p, where we set the\nmicrobatch size as b/2 and the number of microbatches as 2m. To emphasize this variation, we\ndenote this particular setting as ZB-2p*.\nSetup\nModel\n6.2B\n14.6B\n28.3B\n#GPU\n16\n24\n32\nb\n6\n2\n2\nm\n48\n64\n128\n72\n96\n192\n96\n128\n256\nSamples\nper GPU\nper\nsecond\nZB-V\n4.15\n4.21\n4.35\n1.85\n1.88\n1.93\n1.01\n1.02\n1.06\nZB-2p*\n4.36\n4.37\n4.45\n1.84\n1.84\n1.85\n1.00\n1.00\n1.01\nZB-1p\n3.87\n4.00\n4.29\n1.72\n1.78\n1.89\n0.94\n0.97\n1.03\n1F1B\n3.38\n3.57\n3.91\n1.52\n1.61\n1.76\n0.82\n0.87\n0.95\nMemory\n(GB)\nZB-V\n64\n64\n64\n45\n45\n45\n71\n71\n71\nZB-2p*\n63\n64\n65\n46\n46\n46\n72\n72\n72\nZB-1p\n62\n62\n62\n46\n46\n46\n73\n73\n73\n1F1B\n61\n61\n61\n44\n44\n44\n69\n69\n69\nTable 7: Improvement when double the size of each microbatch.\nSetup\nModel\n6.2B\n14.6B\n28.3B\n#GPU\n16\n24\n32\nm\n64\n96\n128\nb\n3\n6\n\u2206\n1\n2\n\u2206\n1\n2\n\u2206\nSamples per\nGPU per\nsecond\nZB-V\n4.13\n4.21\n1.94%\n1.75\n1.88\n7.43%\n0.95\n1.02\n6.32%\nZB-1p\n3.91\n4.00\n2.30%\n1.65\n1.78\n7.88%\n0.90\n0.97\n5.56%\n1F1B\n3.48\n3.57\n2.59%\n1.47\n1.61\n9.52%\n0.80\n0.87\n8.75%\nbetween a larger microbatch size and a reduced bubble rate. When the benefit of a smaller bubble\nrate outweighs that of a larger microbatch size, sacrificing the latter may be a strategic choice.\n6.2\nSCHEDULE EFFICIENCY\nTable 8: Bubble rates of 1F1B, 1F1B-I, ZB-H1, ZB-H2 and ZB-V under different settings.\nModel\n#Stage (p)\n#Microbatch (m)\n1F1B\n1F1B-I\nZB-H1\nZB-H2\nZB-V\n6.2B\n16\n48\n0.2668\n0.1499\n0.1536\n0.0823\n0.0697\n64\n0.2206\n0.1169\n0.1198\n0.0630\n0.0533\n128\n0.1390\n0.0621\n0.0637\n0.0325\n0.0274\n14.6B\n24\n72\n0.2699\n0.1519\n0.1439\n0.0628\n0.0638\n96\n0.2229\n0.1184\n0.1121\n0.0480\n0.0483\n192\n0.1403\n0.0630\n0.0595\n0.0247\n0.0250\n28.3B\n32\n96\n0.2676\n0.1509\n0.1429\n0.0629\n0.0593\n128\n0.2204\n0.1177\n0.1111\n0.0478\n0.0451\n256\n0.1362\n0.0626\n0.0593\n0.0251\n0.0236\nIn Table 8, we calculate the bubble rate, as introduced in Section 5.3, for 1F1B, 1F1B-I, ZB-H1, ZB-\nH2, and ZB-V. The calculations are based on the profiled values of TF , TB, TW , and Tcomm obtained\nin the experiments for ZB-V. The results indicate that the bubble rate of ZB-V is significantly smaller\nthan that of 1F1B, 1F1B-I, and ZB-H1. Moreover, it is comparable to ZB-H2 but with only half the\n11\n0.00\n0.05\n0.10\n0.15\nBubble rate\n6.2B, p = 16, m = 48\nZB\nZB-V\n14.6B, p = 24, m = 72\nZB\nZB-V\n28.3B, p = 32, m = 96\nZB\nZB-V\n0.00\n0.05\n0.10\n0.15\nBubble rate\n6.2B, p = 16, m = 64\nZB\nZB-V\n14.6B, p = 24, m = 96\nZB\nZB-V\n28.3B, p = 32, m = 128\nZB\nZB-V\n1.0pMB\n2.0pMB\n3.0pMB\nMlimit\n0.00\n0.05\n0.10\n0.15\nBubble rate\n6.2B, p = 16, m = 128\nZB\nZB-V\n1.0pMB\n2.0pMB\n3.0pMB\nMlimit\n14.6B, p = 24, m = 192\nZB\nZB-V\n1.0pMB\n2.0pMB\n3.0pMB\nMlimit\n28.3B, p = 32, m = 256\nZB\nZB-V\nFigure 9: The relation between memory limit and bubble rate for ZB-V, compared with the heuristic\nmethod in Section 3.1.\nmemory consumption. Notably, in this comparison, 1F1B, ZB-H1, and ZB-V have similar memory\nconsumption, while 1F1B-I and ZB-H2 require more memory compared to the other methods.\nIn Figure 9, we explore the relationship between the bubble rate and the memory limit. Our observa-\ntions align with the trends presented in Section 5.4. Initially, the bubble rate exhibits a close-to-linear\ndecrease as the value of Mlimit increases, eventually reaching a plateau close to zero bubble rate be-\nyond a certain threshold. Notably, when the memory limit is below 2pMB, ZB-V demonstrates a\nsignificant advantage compared to the heuristic algorithm that does not leverage ZB-V(denoted as\nZB in Figure 9).\n7\nCONCLUSION AND DISCUSSION\nIn this work, we introduced a novel strategy to improve the efficiency of pipeline parallelism by\nsplitting the activation gradient and parameter gradient in backward computation, and we design an\nautomatic pipeline scheduling algorithm that can minimize the pipeline bubble rate under different\nmemory budgets. The schedules produced by this algorithm consistently outperform 1F1B and even\nachieve close to zero bubble rate. To further reduce the memory consumption, we proposed a novel\nscheduling mechanism named ZB-V, capable of achieving zero bubble when TF = TB = TW ,\nwhile adhering to the same memory limit as 1F1B. Another advantage of our methods is that it can\nachieve optimal efficiency with a smaller number of microbatches (typically 3p is enough), which\nmeans more microbatches can be partitioned over data parallelism dimension. This brings a better\nscalability for the training of large models.\nREFERENCES\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n12\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan\nCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. {TVM}: An automated {End-to-End} op-\ntimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI 18), pp. 578\u2013594, 2018.\nShiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping\nLong, Jun Yang, Lixue Xia, et al. Dapple: A pipelined data parallel approach for training large\nmodels. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of\nParallel Programming, pp. 431\u2013445, 2021.\nJohn Forrest and Robin Lougee-Heimer. Cbc user guide. In Emerging theory, methods, and appli-\ncations, pp. 257\u2013277. INFORMS, 2005.\nPriya Goyal, Piotr Doll\u00b4ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\ndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nAaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg\nGanger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377, 2018.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong\nLee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural\nnetworks using pipeline parallelism. Advances in neural information processing systems, 32,\n2019.\nVijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mo-\nhammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer\nmodels. Proceedings of Machine Learning and Systems, 5, 2023.\nChris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River\nRiddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. Mlir: A compiler infras-\ntructure for the end of moore\u2019s law. arXiv preprint arXiv:2002.11054, 2020.\nShen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff\nSmith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating\ndata parallel training. arXiv preprint arXiv:2006.15704, 2020.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740, 2017.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vi-\njay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al.\nEfficient large-scale language model training on gpu clusters using megatron-lm. In Proceed-\nings of the International Conference for High Performance Computing, Networking, Storage and\nAnalysis, pp. 1\u201315, 2021.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\nnetworks. In International conference on machine learning, pp. 1310\u20131318. Pmlr, 2013.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20: International Conference for High Perfor-\nmance Computing, Networking, Storage and Analysis, pp. 1\u201316. IEEE, 2020.\nJared Roesch, Steven Lyubomirsky, Logan Weber, Josh Pollock, Marisa Kirisame, Tianqi Chen, and\nZachary Tatlock. Relay: A new ir for machine learning frameworks. In Proceedings of the 2nd\nACM SIGPLAN international workshop on machine learning and programming languages, pp.\n58\u201368, 2018.\n13\nAmit Sabne. Xla : Compiling machine learning for peak performance, 2020.\nPhilippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler\nfor tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International\nWorkshop on Machine Learning and Programming Languages, pp. 10\u201319, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nBowen Yang, Jian Zhang, Jonathan Li, Christopher R\u00b4e, Christopher Aberger, and Christopher De Sa.\nPipemare: Asynchronous pipeline parallel dnn training. Proceedings of Machine Learning and\nSystems, 3:269\u2013296, 2021.\nLianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida\nWang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating inter-and {Intra-\nOperator} parallelism for distributed deep learning. In 16th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 22), pp. 559\u2013578, 2022.\n14\nA\nOVERLAP COMMUNICATION IN DATA PARALLELISM\nWhen data parallelism is taken into consideration, an all-reduce communication is launched to col-\nlect gradients before optimizer step. Generally, such communication is poorly overlapped with com-\nputation pass, resulting in a latency especially when the communication bandwidth is limited. As in\nFigure 3, usually a number of W passes are scheduled at the tail of an iteration. For each W pass,\nit consists of several independent computations calculating gradients for different parameters. As in\nFigure 10, We can reorder all of these computations to cluster those calculating the gradients for the\nsame parameter, thus achieving the optimal overlapping between computation and communication.\n1\n1\n1\n1\n2\n2\n2\n2\n3\n3\n3\n3\n4\n4\n4\n4\nAR\nAR\nAR\nAR\n(a)\nThe schedule grouped by W\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nAR\nAR\nAR\nAR\n(b)\nThe schedule grouped by parameter\nFigure 10: Comparison between the original schedule grouped by W with poor overlapping (top)\nand the reordered schedule grouped by parameters with optimal overlapping (bottom). The number\ni represents the computation belongs to i-th W, and different colors represent computations for\ndifferent paramters.\nB\nTHE MEMORY LIMIT FOR AUTOMATIC SCHEDULING ALGORITHM\nThe relation between memory limit and bubble rate is highly affected by the bubbles preceding\nthe first B in the initial stage. For the first microbatch, the forward pass needs to go through from\nthe initial stage to final stage, and the backward pass reverses this process until it eventually goes\nback to the initial stage. The total time for the first microbatch from start to complete takes at least\np(TF + TB) + 2(p \u2212 1)Tcomm and it can not be squeezed due to the dependency chains. We denote\nthe number of F passes as k(\u2265 1) and the bubble size as \u03b2(\u2265 0), preceding the first B pass in the\ninitial stage. Then we have:\nMlimit \u2265 kMB\n(1)\n\u03b2 \u2265 p(TF + TB) + 2(p \u2212 1)Tcomm \u2212 kTF \u2212 TB = (p \u2212 1)(TB + 2Tcomm) + (p \u2212 k)TF\n(2)\nThe lower bound of Mlimit is in proportion to k (see Formula 1), and \u03b2 is inversely proportional to k\n(see Formula 2). When increasing k and keeping k < \u230a (p\u22121)(TB+2Tcomm)+pTF\nTF\n\u230b, \u03b2 decreases linearly,\nmeanwhile the lower bound of Mlimit increases linearly. When k = \u230a (p\u22121)(TB+2Tcomm)+pTF\nTF\n\u230b, \u03b2\nreaches its minimum value without delaying B and its value is less than TF , with a peak activation\nmemory at least \u230a (p\u22121)(TB+2Tcomm)+pTF\nTF\n\u230bMB. Beyond this point, further reducing pipeline bubbles\nto zero is not easy. This is because there is a small bubble less than TF in each stage (see Figure\n6), and scheduling another F will delay the starting time of B thus causing more requirements on\nF in previous stages. Theoretically, another p \u2212 1 F passes are required in the initial stage to fully\neliminate bubbles preceding the first B for all stages (see Figure 11), which also means a total\nactivation memory usage at least \u230a (p\u22121)(TB+2Tcomm)+(2p\u22121)TF\nTF\n\u230bMB.\nDevice 0\nDevice 1\nDevice 2\nDevice 3\nDevice 4\nDevice 5\nDevice 6\nDevice 7\nTime\nF\nB\nW\nFigure 11: Zero bubble schedule for 1.5B model with 32 microbatches.\n15\nC\nIN-PLACE OPTIMIZER ROLLBACK\nWhen we need to rollback an optimizer step, a typical method is to store a historic version of param-\neters and optimizer states, and revert to this historic version when needed. However, this method is\nmemory inefficient and lots of copy operations are needed, which definitely hurts the training per-\nformance. For most optimizers, we notice that the step function is arithmetically reversible. Under\nthis observation, we propose a novel technique to perform in-place optimizer rollback, which avoids\nallocating extra memory and requires extra computations only when the rollback is performed. As\nin Algorithm 1, we show how to rollback the step function for AdamW optimizer (Loshchilov &\nHutter, 2017).\nAlgorithm 1 In-place rollback for AdamW\n1: Optimizer States:\n2:\n\u03b3(lr), \u03b21, \u03b22(betas), \u03f5 (epsilon), \u03bb(weight decay),\n3:\nm (first moment), v ( second moment), \u03b8 (parameters),\n4:\nt(time stamp).\n5: function STEP(g)\n\u25b7 In-place step\n6:\nt = t + 1\n7:\nm = \u03b21m + (1 \u2212 \u03b21)g\n8:\nv = \u03b22v + (1 \u2212 \u03b22)g2\n9:\nm\u2032 = m/(1 \u2212 \u03b2t\n1)\n10:\nv\u2032 = v/(1 \u2212 \u03b2t\n2)\n11:\n\u03b8 = \u03b8 \u2212 \u03b3\u03bb\u03b8 \u2212 \u03b3m\u2032/(\n\u221a\nv\u2032 + \u03f5)\n12: end function\n13: function ROLLBACK(g)\n\u25b7 In-place rollback\n14:\nm\u2032 = m/(1 \u2212 \u03b2t\n1)\n15:\nv\u2032 = v/(1 \u2212 \u03b2t\n2)\n16:\n\u03b8 = (\u03b8 + \u03b3m\u2032/(\n\u221a\nv\u2032 + \u03f5))/(1 \u2212 \u03b3\u03bb)\n17:\nm = (m \u2212 (1 \u2212 \u03b21)g)/\u03b21\n18:\nv = (v \u2212 (1 \u2212 \u03b22)g2)/\u03b22\n19:\nt = t \u2212 1\n20: end function\nD\nPROFILED TIME IN EXPERIMENTS\nFor the experiments in Section 5, we record the profiled time of TF , TB, TW , and Tcomm in ZB-2p\nacross different settings. These values are then used to calculate bubble rates for all the methods\nconsidered in Section 5.3 and 5.4. These values can be found in Table 9.\nTable 9: Profiled time of TF , TB, TW , and Tcomm.\nModel\n#Stage (p)\n#Microbatch (m)\nTF\nTB\nTW\nTcomm\n1.5B\n8\n24\n18.522\n18.086\n9.337\n0.601\n32\n18.513\n18.086\n9.331\n0.626\n64\n18.546\n18.097\n9.321\n0.762\n6.2B\n8\n24\n29.718\n29.444\n19.927\n0.527\n32\n29.802\n29.428\n19.530\n0.577\n64\n29.935\n29.621\n19.388\n0.535\n14.6B\n16\n48\n11.347\n11.248\n8.132\n0.377\n64\n11.307\n11.254\n8.101\n0.379\n128\n11.325\n11.308\n8.109\n0.378\n28.3B\n32\n96\n10.419\n10.207\n7.715\n0.408\n128\n10.408\n10.204\n7.703\n0.408\n256\n10.402\n10.248\n7.698\n0.460\n16\nE\nABLATION STUDY ON OPTIMIZER POST-VALIDATION STRATEGY\nIn this section, we provide an ablation study on the effectiveness of the optimizer post-validation\nstrategy. The study compares the throughput of ZB-2p under two conditions: with post-validation\nand with all-reduce synchronization. According to the experimental results in Table 7, the synchro-\nnized version of ZB-2p demonstrates a performance decrease of approximately 8% compared to\nZB-2p with optimizer post-validation.\nTable 10: Throughput (Samples per GPU per second) comparison between ZB-2p and synchronized\nZB-2p\nModel\n#Stage (p)\n#Microbatch (m)\nPost-validation\nAll-reduce synchronization\n1.5B\n8\n24\n14.5\n13.11\n6.2B\n8\n24\n4.32\n4.00\n14.6B\n16\n48\n1.81\n1.68\n28.3B\n32\n96\n0.99\n0.91\nF\nCOMPARE ZB-2P WITH 1F1B UNDER THE SAME MEMORY CONSUMPTION\nUnder the same memory consumption, we double the size of each microbatch for 1F1B and ZB-1p\nand compare their throughput with ZB-2p in Table 11. The experimental results show that ZB-2p\nalso holds a better performance even with a half microbatch size compared to 1F1B. Empirically, a\nlarger batch size increases the utilization rate of GPU and thus improves the efficiency. However, it\nis less of a concern for large models because the hidden dimension is large enough to saturate device\nutilization. Based on this consideration and our experimental results, we believe ZB-2p is more\npreferred than increasing the batch size for 1F1B. In some experiments where the device utilization\nis less saturated and m/p is relatively large, ZB-1p with a doubled microbatch size may slightly\noutperform than ZB-2p.\nG\nILP FORMULATION\nAny pass in the pipeline can be uniquely indexed by (i, j, c), where i \u2208 {1, 2, ..., p} indexes the\nstage, j \u2208 {1, 2, ..., m} indexes the microbatch, and c \u2208 {F, B, W} denotes the specific pass of the\nmicrobatch. We define the variable T(i,j,c) as the time cost and E(i,j,c) as the ending time of a pass.\nWe introduce \u2206M(i,j,c) to denote the memory increment incurred by the pass (i, j, c). For example,\n\u2206M(\u00b7,\u00b7,F ) = MB because the forward pass leads to a net increase of MB of activation stored for\nthe backward pass. \u2206M(\u00b7,\u00b7,B) = MW \u2212 MB which removes the memory stored for B while adding\nthose required by W, and \u2206M(\u00b7,\u00b7,W ) = \u2212MW . Finally, the variable that we want to search is the\nordering of the passes in the schedule, for which we introduce the variable O(i,j,c)\u2192(i,j\u2032,c\u2032) \u2208 {0, 1},\nwhich is an indicator whether the pass index by (i, j, c) is scheduled before (i, j\u2032, c\u2032).\nmin\nO,E\nmax\ni\nE(i,m,W) \u2212 E(i,1,F) + T(i,1,F)\n(3)\ns.t.\nE(i,j,F) \u2265 E(i\u22121,j,F) + Tcomm + T(i,j,F)\n(4)\nE(i,j,B) \u2265 E(i+1,j,B) + Tcomm + T(i,j,B)\n(5)\nE(i,j,c) \u2265 E(i,j\u2032,c\u2032) + T(i,j,c) \u2212 O(i,j,c)\u2192(i,j\u2032,c\u2032)\u221e\n(6)\nMlimit \u2265 \u2206M(i,j\u2032,c\u2032) +\nX\nj,c\n\u2206M(i,j,c)O(i,j,c)\u2192(i,j\u2032,c\u2032)\n(7)\nOverall, the optimization target (3) is to minimize the time spent by the longest stage. Constraints (4)\nand (5) add the sequential dependency requirements on the F and B passes of the same microbatch\nin adjacent stages. Additionally, (6) adds the dependency constraint imposed by our decision of the\nscheduling order. Finally, (7) limits the peak activations memory to be below Mlimit.\n17\nTable 11: Comparison between 1F1B, ZB-1p and ZB-2p under the same memory consumption.\nModel\np\nm\nb\nSamples per GPU per second\nMemory(GB)\nSchedule\n1.5B\n8\n24\n12\n12.0\n57\n1F1B\n12\n13.0\n61\nZB-1p\n6\n14.5\n59\nZB-2p\n32\n12\n12.6\n57\n1F1B\n12\n13.6\n61\nZB-1p\n6\n14.8\n59\nZB-2p\n64\n12\n13.8\n57\n1F1B\n12\n14.4\n61\nZB-1p\n6\n14.9\n59\nZB-2p\n6.2B\n8\n24\n6\n3.56\n66\n1F1B\n6\n3.95\n71\nZB-1p\n3\n4.32\n70\nZB-2p\n32\n6\n3.76\n66\n1F1B\n6\n4.05\n71\nZB-1p\n3\n4.35\n70\nZB-2p\n64\n6\n4.09\n66\n1F1B\n6\n4.24\n71\nZB-1p\n3\n4.39\n70\nZB-2p\n14.6B\n16\n48\n2\n1.53\n50\n1F1B\n2\n1.73\n51\nZB-1p\n1\n1.81\n51\nZB-2p\n32\n2\n1.62\n50\n1F1B\n2\n1.79\n51\nZB-1p\n1\n1.83\n51\nZB-2p\n128\n2\n1.78\n50\n1F1B\n2\n1.89\n51\nZB-1p\n1\n1.85\n51\nZB-2p\n28.3B\n32\n96\n2\n0.81\n72\n1F1B\n2\n0.93\n74\nZB-1p\n1\n0.99\n74\nZB-2p\n32\n2\n0.85\n72\n1F1B\n2\n0.96\n74\nZB-1p\n1\n1.00\n74\nZB-2p\n256\n2\n0.94\n72\n1F1B\n2\n1.02\n74\nZB-1p\n1\n1.00\n74\nZB-2p\nH\nCOMPARE ZB METHODS WITH 1F1B ON SMALL NUMBER OF\nMICROBATCHES\nBy nature of PP when the number of microbatches m is less then number of stages p, there\u2019ll be a\nlarge bubble rate. However zerobubble methods can still boost performance under these rare settings\nby approximately 20% to 30%. In a rough analysis ignoring communication and assuming m <= p\nand TW < TB, an 1F1B iteration takes (m + p \u2212 1) \u2217 (TF + TB + TW ) to complete, while a\nZB iteration takes (m + p \u2212 1) \u2217 (TF + TB) + TW . The experiment result is shown in Table 12.\nNoticeably when m <= p ZB-1p and ZB-2p are essentially the same and consumes similar memory\nas 1F1B.\n18\nTable 12: Comparison between 1F1B and ZB-2p on small number of microbatches.\nModel\np\nm\nb\nSamples per GPU per second\nMemory(GB)\nSchedule\n1.5B\n8\n2\n6\n3.56\n11\n1F1B\n6\n4.25\n12\nZB-2p\n4\n6\n5.74\n18\n1F1B\n6\n6.92\n19\nZB-2p\n8\n6\n8.26\n29\n1F1B\n6\n9.90\n34\nZB-2p\n6.2B\n8\n2\n3\n1.04\n21\n1F1B\n3\n1.33\n21\nZB-2p\n4\n3\n1.69\n28\n1F1B\n3\n2.16\n29\nZB-2p\n8\n3\n2.45\n39\n1F1B\n3\n3.07\n44\nZB-2p\n14.6B\n16\n4\n1\n0.39\n19\n1F1B\n1\n0.52\n20\nZB-2p\n8\n1\n0.65\n24\n1F1B\n1\n0.85\n24\nZB-2p\n16\n1\n0.95\n32\n1F1B\n1\n1.25\n33\nZB-2p\n19\n"
  },
  {
    "title": "Synthesizing Moving People with 3D Control",
    "link": "https://arxiv.org/pdf/2401.10889.pdf",
    "upvote": "11",
    "text": "Synthesizing Moving People with 3D Control\nBoyi Li\nJathushan Rajasegaran\nYossi Gandelsman\nAlexei A. Efros\nJitendra Malik\nUC Berkeley\nImitator\nActor\nImitator\nActor\nActor\nFigure 1. The Imitation Game: Given a video of a person \"The Actor\", we want to transfer their motion to a new person \"The Imitator\".\nIn this figure, the first row shows a sequence of frames of the actor (Michelle Kwan), doing her Olympics \u201998 performance. The inset row\nshows the 3D poses extracted from this video. Now, given any single image of a new person The Imitator, our model can synthesize new\nrenderings of the imitator, to copy the actions of the actor in 3D.\nAbstract\nIn this paper, we present a diffusion model-based frame-\nwork for animating people from a single image for a given\ntarget 3D motion sequence. Our approach has two core\ncomponents: a) learning priors about invisible parts of the\nhuman body and clothing, and b) rendering novel body poses\nwith proper clothing and texture. For the first part, we learn\nan in-filling diffusion model to hallucinate unseen parts of a\nperson given a single image. We train this model on texture\nmap space, which makes it more sample-efficient since it\nis invariant to pose and viewpoint. Second, we develop a\ndiffusion-based rendering pipeline, which is controlled by\n3D human poses. This produces realistic renderings of novel\nposes of the person, including clothing, hair, and plausible in-\nfilling of unseen regions. This disentangled approach allows\nour method to generate a sequence of images that are faithful\nto the target motion in the 3D pose and, to the input image\nin terms of visual similarity. In addition to that, the 3D con-\ntrol allows various synthetic camera trajectories to render a\nperson. Our experiments show that our method is resilient in\ngenerating prolonged motions and varied challenging and\ncomplex poses compared to prior methods. Please check our\nwebsite for more details: 3DHM.github.io.\n1. Introduction\nGiven a random photo of a person, can we accurately an-\nimate that person to imitate someone else\u2019s action? This\narXiv:2401.10889v1  [cs.CV]  19 Jan 2024\nproblem requires a deep understanding of how human poses\nchange over time, learning priors about human appearance\nand clothing. For example, in Figure 1 the Actor can do a\ndiverse set of actions, from simple actions such as walking\nand running to more complex actions such as fighting and\ndancing. For the Imitator, learning a visual prior about\ntheir appearance and clothing is essential to animate them\nat different poses and viewpoints. To tackle this problem,\nwe propose 3DHM, a two-stage framework (see Figure 2)\nthat synthesizes 3D Human Motions by completing a texture\nmap from a single image and then rendering the 3D humans\nto imitate the actions of the actor.\nWe use state-of-the-art 3D human pose recovery model\n4DHumans [9, 19] for extracting motion signals of the actor,\nby reconstructing and tracking them over time. Once we\nhave a motion signal in 3D, as a sequence of meshes, one\nwould think we can simply re-texture them with the texture\nmap of the imitator to get an intermediate rendering of the\nimitation task. However, this requires a complete texture\nmap of the imitator. When given only a single view image\nof the imitator, we see only a part of their body, perhaps\nthe front side, or the backside but never both sides. To get\nthe complete texture map of the imitator from a single view\nimage, we learn a diffusion model to in-fill the unseen re-\ngions of the texture map. This essentially learns a prior about\nhuman clothing and appearance. For example, a front-view\nimage of a person wearing a blue shirt would usually have\nthe same color at the back. With this complete texture map,\nnow we can get an intermediate rendering of the imitator\ndoing the actions of the actor. Intermediate rendering means,\nwrapping the texture map on top of the SMPL [17] mesh to\nget a body-tight rendering of the imitator.\nHowever, the SMPL [17] mesh renderings are body-tight\nand do not capture deformations on clothing, like skirts or\nvarious hairstyles. To solve this, we learn a second model,\nthat maps from mesh renderings to more realistic images,\nby controlling the motion with 3D poses. We find out such\na simple framework could successfully synthesize realis-\ntic and faithful human videos, particularly for long video\ngenerations. We show that the 3D control provides a more\nfine-grained and accurate flow of motion and captures the\nvisual similarities of the imitator faithfully.\nWhile there has been a lot of work on rewriting the motion\nof an actor [4, 14, 28], each requires either large amounts\nof data, supervised control signals, or requires careful cura-\ntions of the training data. For example, Make-a-video [24]\ncan generate decent results while for human videos, it often\ngenerates incomplete or nonconsequential videos and fails\nat faithful reconstruction of humans. Some works [8] use\nOpenpose [6] as intermediate supervision. However, Open-\npose primarily contains the anatomical key points of humans,\nit can not be used to indicate the body shape, depth, or other\nrelated human body information. DensePose [10] aims to\nrecover highly accurate dense correspondences between im-\nages and the body surface to provide dense human pose\nestimation. However, it can not reflect the texture informa-\ntion from the original inputs. Compared to this line of work,\nours fully utilizes the 3D models to control the motion, by\nproviding an accurate dense 3D flow of the motion, and the\ntexture map representation makes it easy to learn appearance\nprior from a few thousand samples.\n2. Related Works\nControllable Human Generation. Human generation is\nnot an easy task. Unlike image translation [16], generating\ndifferent humans requires the model to understand the 3D\nstructure of the human body. Given arbitrary text prompts or\npose conditions [5, 15], we often find out that existing gener-\native models often generate unreasonable human images or\nvideos. Diffusion-HPC [31] proposes a diffusion model with\nHuman Pose Correction and finds that injecting human body\nstructure priors within the generation process could improve\nthe quality of generated images. ControlNet [34] is designed\non neural network architecture to control pre-trained large\ndiffusion models to support additional input conditions, such\nas Openpose [6]. GestureDiffuCLIP [3] designs a neural\nnetwork to generate co-speech gestures. However, these\ntechniques are not tailored for animating humans, which can-\nnot guarantee the required human appearance and clothing.\nSynthesizing Moving People. Synthesizing moving peo-\nple is very challenging. For example, Make-a-Video [24]\nor Imagen Video [23] could synthesize videos based on a\ngiven instruction. However, the generated video cannot ac-\ncurately capture human properties correctly and may cause\nthe weird composition of generated humans. Prior meth-\nods [8, 29] learn pose-to-pixels mapping directly. However,\nthese designs could only be trained and used for one per-\nson. Recent works such as SMPLitex [7] consider human\ntexture estimation from a single image to animate a person.\nHowever, there is a visual gap between rendered people via\npredicted texture map and real humans. Many works start\nto directly predict pixels based on diffusion models, such as\nDreampose [14] and DisCO [28]. DreamPose is controlled\nby DensePose [10], it aims to synthesize a video contain-\ning both human and fabric motion based on a sequence of\nhuman body poses. DisCO is directly controlled by Open-\npose [6], and it aims to animate the human based on the 2D\npose information. However, the approach of aligning output\npixels for training regularization often leads these models to\nbecome overly specialized to certain training data. Moreover,\nthis methodology limits the models\u2019 generalization capabili-\nties, as they often perform well on a few people whose data\ndistribution closely matches that of the training dataset.\nStage 1\n(Inpainting)\nStage 2\n(Rendering)\nPredicted\nComplete  \nTexture map\n3D Poses\n(from Actor)\nTexture mapped\n(Imitator)\nFinal Rendering\n(Imitator)\nA photo of \na person\n(Imitator)\n28-7\uff0c 15\uff0c 22\nFigure 2. Overview of 3DHM: we show an overview of our model pipeline. Given an image of the imitator and a sequence of 3D poses\nfrom the actor, we first generate a complete full texture map of the imitator, which can be applied to the 3D pose sequences extracted from\nthe actor to generate texture-mapped intermediate renderings of the imitator. Then we pass these intermediate renderings to the Stage-2\nmodel to project the SMPL mesh rendering to more realistic renderings of real images. Note: red boxes represent inputs, yellow boxes\nrepresent intermediate predictions from stage 1, and blue boxes represent the final outputs from stage 2. To create a moving person animation\nwith variable duration and any number of 3D poses, it is only necessary to execute stage 1 once in order to acquire a complete texture map.\n3. Synthesizing Moving People\nIn this section, we discuss our two-stage approach for imitat-\ning a motion sequence. Our 3DHM framework embraces the\nadvantage of accurate 3D pose prediction from the state-of-\nthe-art predicting models 4DHumans [9, 19], which could\naccurately track human motions and extracts 3D human\nposes of the actor videos. For any given video of the actor\nwe want to imitate, we use 3D reconstruction-based tracking\nalgorithms to extract 3D mesh sequences of the actor. For\nthe inpainting and rendering part, we rely on the pre-trained\nStable Diffusion [22] model, which is one of the most recent\nclasses of diffusion models that achieve high competitive\nresults over various generative vision tasks.\nOur approach 3DHM is composed of two core parts: In-\npainting Diffusion for texture map in-painting as Stage-1\nand Rendering Diffusion for human rendering as Stage-2.\nFigure 2 shows a high-level overview of our framework. In\nStage-1, first, for a given single view image, we extract a\nrough estimate of the texture map by rendering the meshes\nonto the image and assigning pixels to each visible mesh tri-\nangle such that when rendered again it will produce a similar\nimage as the input image. This predicted texture map has\nonly visible parts of the input image. The Stage-1 Diffusion\nin-painting model takes this partial texture map and gener-\nates a complete texture map including the unseen regions.\nGiven this completed texture map, we generate intermediate\nrenderings of SMPL [17] meshes and use Stage-2 model to\nproject the body-tight renderings to more realistic images\nwith clothing. For the Stage-2 Diffusion model, we apply\n3D control to animate the imitator to copy the actions of the\nactor.\n3.1. Texture map Inpainting\nThe goal of Stage-1 model is to produce a plausible complete\ntexture map by inpainting the unseen regions of the imitator.\nWe extract a partially visible texture map by first rendering a\n3D mesh onto the input image and sample colors for each\nvisible triangle following 4DHumans [9].\nInput. We first utilize a common approach to infer pixel-\nto-surface correspondences to build an incomplete UV tex-\nturemap [7, 32] for texturing 3D meshes from a single RGB\nimage. We also compute a visibility mask to indicate which\npixels are visible in 3D and which ones are not.\nTarget. Since the objective of this modeling is to generate\ncomplete texture maps, we generate a pseudo-complete tex-\nture map using video data. Since the 4DHumans can track\npeople over time, it continually updates its internal texture\nmap representations as a moving average of visible regions.\nHowever to produce more sharp images, for the generative\ntask we found that a median filtering is more suitable than\na moving average. While this technique can be applied to\nany video, in this stage we rely on 2,205 human videos. For\neach human video, we first extract a partial texture map from\neach frame. Since each video contains 360 degrees of human\nviews, we calculate a pseudo-complete texture map from a\nwhole video and set it as the target output for Stage 1. In\ndetail, we take the median overall visible parts of texture\nmaps of a video.\nModel. We finetune directly on the Stable Diffusion In-\npainting model [21] that shows great performance on image\ncompletion tasks. We input a partial texture map and corre-\nsponding visibility mask and obtain the recovered predicted\nmap for the human. We lock the text encoder branch and\nStable Diffusion \nInpainting\nA photo of \na person\n(Imitator)\nExtracted  \nTexture map\nVisibility Mask\nPredicted \nComplete \nTexture map\n28,60\uff0c -03794\nFigure 3. Stage-1 of 3DHM: In the first stage, given a single view\nimage of an imitator, we first apply 4Dhumans [9] style sampling\napproach to extract partial texture map and its corresponding visi-\nbility map. These two inputs are passed to the in-painting diffusion\nmodel to generate a plausible complete texture map. In this exam-\nple, while we only see the back view of the imitator, the model\nwas able to hallucinate a plausible front region that is consistent\nwith their clothing.\nalways feed \u2018real human\u2019 as input text of fixed Stable Dif-\nfusion models. We refer to our trained model as Inpainting\nDiffusion. See Figure 3 for the model architecture.\n3.2. Human Rendering\nIn Stage 2, we aim to obtain a realistic rendering of a human\nimitator doing the actions of the actor. While the interme-\ndiate renderings (rendered with the poses from the actor\nand texture map from Stage-1) can reflect diverse human\nmotion, these SMPL mesh renderings are body-tight and\ncannot represent realistic rendering with clothing, hairstyles,\nand body shapes. For example, if we input a scene where\na girl is wearing a dress and she is dancing, the intermedi-\nate renderings might be able to \u201cdance\" but it is impossible\nto animate the skirt with SMPL mesh rendering. To train\nthis model, in a fully self-supervised fashion, we assume\nthe actor is the imitator, after all a good actor should be a\ngood imitator. This way, we can take any video, and get a\nsequence of poses from 4DHumans [9] and take any single\nframe, and get a complete texture map from Stage-1, then get\nthe intermediate renderings by rendering the texture maps\non the 3D poses. Now, we have paired data of intermediate\nrenderings and real RGB images. Using this, we collect a\nlarge amount of paired data and train our Stage-2 diffusion\nmodel with conditioning.\nInput: We first apply the generated texture map (fully com-\nplete) from Stage 1 to actor 3D body mesh sequences to an\nintermediate rendering of the imitator performing the actions\nof the actor. Note at this time, intermediate rendering can\nonly reflect the clothing that fits the 3D mesh (body-tight\nclothing) but fails to reflect the texture outside the SMPL\nbody, such as the puffed-up region of a skirt, winter jacket, or\nhat. To obtain the human with complete clothing texture, we\ninput the obtained intermediate renderings and the original\nInput Image\n      (t=0)\n   Texture mapped\n                  (at time t)\nStableDiffusion \nEncoder\nStableDiffusion \nDecoder\n3D Controllable\nBranch\nInput Latents \n(64 x 64)\nOutput Latents \n(64 x 64)\nFinal Rendering\n(at time t)\ndecode latents\ncvpr1 - 106\nFigure 4. Stage-2 of 3DHM: This figure shows the inference of our\nStage-1 approach. Given an intermediate rendering of the imitator\nwith the pose of the actor and the actual RGB image of the imitator,\nour model can synthesize realistic renderings of the imitator on the\npose of the actor.\nimage of the person into Rendering Diffusion to render the\nhuman in a novel pose with a realistic appearance.\nTarget: Since we collected the data by assuming the actor\nis the imitator, we have the paired data of the intermediate\nrenderings and the real RGB images. This allows us to train\nthis model on lots of data, without requiring any direct 3D\nsupervision.\nModel. Similar to ControlNet, we directly clone the weights\nof the encoder of the Stable Diffusion [20] model as our Con-\ntrollable branch (\"trainable copy\") to process 3D conditions.\nWe freeze the pre-trained Stable Diffusion and input noisy la-\ntents (64\u00d764). In the meanwhile, we input a texture mapped\n3D human at time t and original human photo input into a\nfixed VAE encoder and obtain texture mapped 3D human\nlatents (64 \u00d7 64) and appearance latents (64 \u00d7 64) as condi-\ntioning latents. We feed these two conditioning latents into\nRendering Diffusion Controllable branch. The key design\nprinciple of this branch is to learn textures from human input\nand apply them to the texture mapped 3D human during\ntraining through the denoising process. The goal is to render\na real human with vivid textures from the generated(texture\nmapped) 3D human from Stage 1. We obtain the output\nlatent and process it to the pixel space via diffusion step pro-\ncedure and fixed VAE decoder. Same to Stage 1, we lock the\ntext encoder branch and always feed \u2018a real human is acting\u2019\nas input text of fixed Stable Diffusion models. We refer to\nour trained model as Rendering Diffusion. In Rendering\nDiffusion, we predict outputs frame by frame. We show the\nStage 2 workflow in Figure 4.\n4. Experiments\n4.1. Experimental Setup\nDataset.\nWe collect 2,524 3D human videos from\n2K2K [11], THuman2.0 [33] and People-Snapshot [2]\ndatasets. 2K2K is a large-scale human dataset with 3D\nhuman models reconstructed from 2K resolution images.\nTHuman2.0 contains 500 high-quality human scans captured\nby a dense DLSR rig. People-Snapshot is a smaller human\ndataset that captures 24 sequences. We convert the 3D hu-\nman dataset into videos and extract 3D poses from human\nvideos using 4DHumans. We use 2,205 videos for training\nand other videos for validation and testing. See the Appendix\nfor more details on the dataset distribution on clothing.\nEvaluation Metrics. We evaluate the quality of generated\nframes of our method with image-based and video-based\nmetrics. For image-based evaluation, we follow the evalua-\ntion protocol of DisCO [28] to evaluate the generation qual-\nity. We report the average PSNR [13], SSIM [30], FID [12],\nLPIPS [35], and L1. For video-based evaluation, we use\nFVD [26]. For pose evaluating 3D pose accuracy we use\nMPVPE and PA-MVPVE. MPVPE [18], or Mean Per-Vertex\nPosition Error, is a critical metric in 3D human pose estima-\ntion, which quantifies the average distance between predicted\nand actual 3D vertices across a model. This measurement\nis essential for evaluating the accuracy of 3D reconstruc-\ntions and pose estimations, with a lower MPVPE indicat-\ning higher precision. Complementing this, PA-MPVPE, or\nProcrustes-Aligned Mean Per-Vertex Position Error, adds\nanother dimension to this evaluation. It involves aligning\nthe predicted and ground truth data using Procrustes Analy-\nsis, which neutralizes differences in orientation, scale, and\nposition before calculating the mean error. This alignment\nallows PA-MPVPE to focus on the structural accuracy of\npredictions, making it a valuable metric for assessing the\nrelative positioning of vertices in a model, independent of\ntheir absolute spatial coordinates.\nImplementation Details. As for training all the datasets, we\nset the constant learning rate as 5e-05 and use the pre-trained\ndiffusion models from diffusers [27] for both Stage-1 and\nStage-2. As for Stage 1 Inpainting Diffusion, we finetune on\nStable Diffusion Inpainting models [21], which has an 859M\ntotal number of trainable parameters and 206M total number\nof non-trainable parameters, since the VAE is frozen during\nthis stage. We train Rendering Diffusion for 50 epochs and it\ntakes about 2 weeks to run our model on our soup of training\ndatasets. As for Stage 2 Rendering Diffusion, we train the\nControllable branch and freeze Stable Diffusion backbones.\nThe total number of trainable parameters in this case is 876M\nand the total number of non-trainable parameters is 1.1B.\nWe train Rendering Diffusion for 30 epochs and it takes\nMethod\nPSNR\u2191\nSSIM \u2191\nFID \u2193\nLPIPS \u2193\nL1 \u2193\nDreamPose\n35.06\n0.80\n245.19\n0.18\n2.12e-04\nDisCO\n35.38\n0.81\n164.34\n0.15\n1.44e-04\nOurs\n36.18\n0.86\n154.75\n0.12\n9.88e-05\nTable 1. Quantitative comparison on frame-wise generation\nquality : We compare our method with prior works on pose con-\ndition generation tasks and measure the generation quality of the\nsamples.\nabout 2 weeks to run our model on training datasets based\non 8 NVIDIA A100 GPUs with a batch size of 4. As for\ninference, we only need to run Stage-1 once to reconstruct\nthe full texture map of the imitator, and it is used for all other\nnovel poses and viewpoints. We run Stage-2 inference for\neach frame independently, however since the initial RGB\nframe of the imitator is conditioned for all frames, the Stage-\n2 model is able to produce samples that are temporarily\nconsistent.\n4.2. Quantitative Results\nBaselines. We compare our approaches with past and state-\nof-the-art methods: DreamPose [14], DisCo [28] and Con-\ntrolNet [34] (for pose accuracy comparisons)1. We set infer-\nence steps as 50 for all the approaches for fair comparisons.\nComparisons on Frame-wise Generation Quality. We\ncompare 3DHM with other methods on 2K2K test dataset,\nwhich is composed of 50 unseen human videos, at 256\u00d7256\nresolution. For each human video, we take 30 frames that\nrepresent the different viewpoints of each unseen person.\nThe angles range from 0\u25e6 to 360\u25e6, we take one frame every\n12\u25e6 to better evaluate the prediction and generalization abil-\nity of each model. As for DisCO, we strictly follow their\nsetting and extract OpenPose for inference. As for Dream-\nPose, we extract DensePose for inference. We evaluate the\nresults and calculate the average score over all frames of each\nvideo. We set the background as black for all approaches for\nfair comparisons. We report the average score overall of the\nsame 50 videos and show the comparisons in Table 1. We\nobserve that 3DHM outperforms all the baselines in different\nmetrics.\nComparisons on Video-level Generation Quality. To ver-\nify the temporal consistency of 3DHM, we also report the re-\nsults following the same test set and baseline implementation\nas in image-level evaluation. Unlike image-level compar-\nisons, we concatenate every consecutive 16 frames to form\na sample of each unseen person on challenging viewpoints.\nThe angles range from 150\u25e6 to 195\u25e6, we take one frame\n1We utilize the open-source official code and models provided by the\nauthors to implement these baselines. We use diffusers [27] for ControlNet\nand Openpose extraction, and Detectron2 for DensePose extraction for\nDisCO. Since Chan et al. [8] can only work for animating a specific person,\nwe don\u2019t compare with it in this paper.\nMethod\nFID-VID\u2193\nFVD \u2193\nDreamPose\n113.96\n950.40\nDisCO\n83.91\n629.18\nOurs\n55.40\n422.38\nTable 2. Quantitative comparison on video-level generation quality.\nMethod\nMPVPE \u2193\nPA-MPVPE \u2193\nDreamPose\n123.07\n82.75\nDisCO\n112.12\n63.33\nControlNet\n108.32\n59.80\nOurs\n41.08\n31.86\nTable 3. Quantitative comparison on pose accuracy.\nevery 3\u25e6 to better evaluate the prediction and generalization\nability of each model. We report the average score overall\nof 50 videos and show the comparisons in Table 2. We ob-\nserve that 3DHM, though trained and tested by per frame,\nstill embrace significant advantage over prior approaches,\nindicating superior performance on preserving the temporal\nconsistency with 3D control.\nComparisons on Pose Accuracy. To further evaluate the\nvalidity of our model, we estimate 3D poses from generated\nhuman videos from different approaches via a state-of-the-\nart 3D pose estimation model 4DHumans. We use the same\ndataset setting mentioned above and compare the extracted\nposes with 3D poses from the target videos. Following the\nsame comparison settings with generation quality, we evalu-\nate the results and calculate the average score over all frames\nof each video. Beyond DreamPose and DisCO, we also com-\npare with ControlNet, which achieves the state-of-the-art\nin generating images with conditions, including openpose\ncontrol. Since ControlNet does not input images, we input\nthe same prompts as ours \u2018a real human is acting\u2019 and the\ncorresponding openpose as conditions. We report the aver-\nage score overall of 50 test videos and show the comparisons\nin Table 3. We could notice that 3DHM could synthesize\nmoving people following the provided 3D poses with very\nhigh accuracy. At the same time, previous approaches might\nnot achieve the same performance by directly predicting the\npose-to-pixel mapping. We also notice that 3DHM could\nachieve superior results on both 2D metrics and 3D metrics,\neven if DisCO and ControlNet are controlled by Openpose\nand DreamPose is controlled by DensePose.\n4.3. Qualitative Results\nOur work focuses on synthesizing moving people, primar-\nily for clothing and the human body. With the aid of 3D\nassistance, our approach has the potential to produce human\nSettings\nPSNR\u2191\nSSIM \u2191\nFID \u2193\nLPIPS \u2193\nL1 \u2193\nDefault\n36.18\n0.86\n154.75\n0.12\n9.88e-05\nw/o Texture map\n35.00\n0.78\n237.42\n0.20\n2.35e-04\nw/o Appearance Latents\n36.07\n0.86\n167.58\n0.12\n1.03e-04\nadding SMPL parameters\n36.42\n0.87\n157.60\n0.12\n8.87e-05\nTable 4. Ablation study of Rendering Diffusion. We compare the\nframe-wise generation quality under different settings. We notice\nboth texturemap reconstruction and appearance latents are critical\nto the model performance.\nmotion videos in various scenarios. We consider challenging\n3D poses and motions from 3 sources: 3D human videos,\nrandom YouTube videos, and text input.\nPoses from Unseen 3D Human Videos. We test our model\non different 3D human videos with different human appear-\nances and 3D poses from the 2K2K dataset. We verify that\nthe tested video has never appeared in training data. We\ndisplay the results in Figure 5a.\nMotions from Random YouTube Videos. We test our\nmodel on very different motions from randomly downloaded\nYoutube videos for an unseen human. We display the results\nin Figure 5b.\nMotions from Text Inputs. We test our model on motions\nfrom arbitrary text prompts. We randomly input an unseen\nhuman photo and motions from random text inputs via a\nwidely used human motion generative model (MDM [25]).\nWe display the results in Figure 5c.\n5. Analysis and Discussion\n5.1. Ablation Study\nTo further verify the components of our methods, we train\non training dataset and test on test datasets. We extract the\n3D rendered pose from these 50 test video tracks. Same\nwith the settings in quantitative comparison, we calculate\nthe average scores of PSNR, SSIM, VGG, L1, LPIPS among\nall the generated frames and targeted original frames and\nreport the results on both frame-wise metric (Table 4), video-\nlevel metric (Table 5) and pose accuracy (Table 6). We find\nthat both texture map reconstruction and appearance latents\nare critical to the model performance. Also, we notice that\ndirectly adding SMPL parameters into the model during\ntraining may not bring improved performance considering\nall evaluation metrics. This is presumably due to the impreci-\nsion of SMPL parameters, which could provide contradictory\ninformation throughout the diffusion training process if they\nare not incorporated correctly.\n5.2. 2D Control and 3D Control\nWe also compare the results of the official model from\nDreamPose and DisCO on a random person on a random\nreal human photo which ensures distinct data distribution.\nVarious\nViewpoints\n(a) 3DHM with a random human photo and a random 3D pose of various viewpoints. We show that even if the person\u2019s photo is from a side angle, our stage 1\ncan help reconstruct the full texture map, which could be used to obtain full body information. Stage 2 can add texture information based on a given input.\nMotions\u00a0from\nrandom\u00a0videos\n(b) 3DHM with a random human photo and motions from random YouTube Videos. This example is from Gene Kelly\u2019s dancing video.\nA\u00a0person\u00a0turns\u00a0to\u00a0his\u00a0right\u00a0and\u00a0paces\u00a0back\u00a0and\u00a0forth.\nMotions\nfrom text\n(c) 3DHM with a random human photo and motions generated from text inputs by MDM, a Human Motion Diffusion Model [25].\nFigure 5. Qualitative results on different viewpoints of the same pose; motions from random videos and motions from text input.\nWe display the qualitative results of various viewpoints in\nFigure 6. DreamPose, DisCO, and 3DHM are all initialize\nthe U-Net model with the pre-trained weights of Stable Dif-\nfusion. We notice that 3DHM can generalize well to unseen\nreal humans though it is only trained by limited 3D humans.\nSince DreamPose requires subject-specific finetuning of the\nUNet to achieve better results, it cannot directly generalize\nwell on a random human photo. As for DisCO, though it has\nbeen trained with an effective human attribute pre-training\non multiple public datasets for better generalizability to un-\nseen humans, still fails to synthesize people without the\ntarget pose. We assume this is because 3DHM adds rigid\n3D control to better correlate the appearance to the poses,\nand preserve the body shape. Training with OpenPose or\nDensePose cannot guarantee the mapping between textures\nand poses, which makes it hard for the models to generalize.\nInputs\nVarious\u00a03D\u00a0Poses\nDreamPose\nDisCO\nOurs\nVarious\u00a0Viewpoints\nFigure 6. Qualitative comparison with other 2D control approaches on a random real human photo (a Korean actress). We apply various 3D\nposes or the same 3D pose from different viewpoints. It could be noticed that 2D poses may not be able to capture the folding motion, and\ndetails of the human body. We could notice that our approach 3DHM could bridge this gap with 3D control.\nMethod\nFID-VID\u2193\nFVD \u2193\nDefault\n55.40\n422.38\nw/o Texture map\n113.97\n632.67\nw/o Appearance Latents\n93.21\n715.51\nadding SMPL parameters\n72.35\n579.90\nTable 5. Ablation study of Rendering Diffusion. We compare the\nvideo-level generation quality under different settings. We notice\nthat although adding SMPL parameters achieve better performance\non frame-wise setting but may yield worse temporal consistency\nthan default settings.\n5.3. Limitations\nAs 3DHM generates the frames of the human motion videos\nindependently, there is no guarantee of consistency in time.\nFor example, the clothing light may change between consec-\nutive frames. One possible solution is to train the model to\npredict multiple frames simultaneously. Another possible\nsolution is to condition the generation process on previously\ngenerated frames via stochastic conditioning [1]. Addition-\nMethod\nMPVPE \u2193\nPA-MPVPE \u2193\nDefault\n41.08\n31.86\nw/o Texture map\n92.94\n59.18\nw/o Appearance Latents\n41.99\n32.82\nadding SMPL parameters\n39.16\n29.67\nTable 6. Ablation study of Rendering Diffusion. We compare the\npose accuracy under different settings.\nally, since 3DHM is trained on a dataset of 2K people, not all\nthe detailed textures can be reconstructed completely during\ninference (e.g. unique logos on the clothes). We hypothesize\nthis could be alleviated by training with more human data.\n6. Conclusion\nIn this paper, we propose 3DHM, a two-stage diffusion\nmodel-based framework that enables synthesizing moving\npeople based on one random photo and target human poses.\nA notable aspect of our approach is that we employ a cutting-\nedge 3D pose estimation model to generate human motion\ndata, allowing our model to be trained on arbitrary videos\nwithout necessitating ground truth labels. Our method is\nsuitable for long-range motion generation, and can deal with\narbitrary poses with superior performance over previous ap-\nproaches.\nAcknowledgement\nWe thank the Machine Common Sense project and ONR\nMURI award number N00014-21-1-2801. We also thank\nGoogle\u2019s TPU Research Cloud (TRC) for providing cloud\nTPUs. We thank Georgios Pavlakos, Shubham Goel, and\nJane Wu for the constructive feedback and helpful discus-\nsions.\nAppendices\nA. Dataset Analysis\nFigures 7a and 7b present the clothing type statistics of the\ntraining data (2,205 humans) and test data (50 humans). We\ncount people based on four clothing categories: skirted attire,\nsuit, casual wear, and others. In some cases, the clothing\nbelongs to skirted attire and suits or casual wear, we will\ncount this as skirted attire. For each clothing category, we\ntally two styles: tight-fitting and loose-fitting.\nIn this paper, we only train on limited human videos, we\nassume training with more human videos could largely boost\nthe model generalization on the fly. Given that 3DHM makes\nuse of a cutting-edge 3D pose estimation model and only\nrequires human videos without additional labels for training,\nit could be trained with numerous and any human videos\nsuch as movies, etc.\nB. 3DHM Training Features\nAs has been mentioned in the paper, 3DHM is in a fully\nself-supervised fashion. Here we summarize the key training\nfeatures of our approach:\n\u2022 3DHM training pipeline (for both stages) is self-\nsupervised.\n\u2022 3DHM does not use any additional annotations. It is\ntrained with pseudo-ground-truth as we use cutting-edge\nsoftware which can detect, segment, track and 3Dfy hu-\nmans (H4D).\n\u2022 3DHM is scalable and its scaling can be done readily in\nthe future given additional videos of humans in motion\nand computing resources.\n128\n7\n571\n2\n0\n250\n500\n750\n1000\n1250\nSkirted Attire\nSuit\nCasual Wear\nOthers\nLoose-fitting\nTight-fitting\n(a) Training data distribution.\n8\n0\n5\n0\n0\n5\n10\n15\n20\n25\nSkirted Attire\nSuit\nCasual Wear\nOthers\nLoose-fitting\nTight-fitting\n(b) Testing data distribution.\nFigure 7. Data distribution. We split the clothing type into 4\ncategories: skirted attire, suit, casual wear, and others. We split\neach category into two types: loose and tight. We report the number\nof each category and type and display the overall distribution. We\ncould notice that most clothing is casual wear and a large portion\nbelongs to tight-fitting.\nReferences\n[1] Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu\nSalzmann, Lars Petersson, and Stephen Gould.\nA\nstochastic conditioning scheme for diverse human mo-\ntion prediction. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 5223\u20135232, 2020. 8\n[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Chris-\ntian Theobalt, and Gerard Pons-Moll. Video based\nreconstruction of 3d people models. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8387\u20138397, 2018. 5\n[3] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturedif-\nfuclip: Gesture diffusion model with clip latents. arXiv\npreprint arXiv:2303.14613, 2023. 2\n[4] Christoph Bregler, Michele Covell, and Malcolm\nSlaney. Video rewrite: Driving visual speech with au-\ndio. In Seminal Graphics Papers: Pushing the Bound-\naries, Volume 2, pages 715\u2013722. 2023. 2\n[5] Tim Brooks and Alexei A Efros. Hallucinating pose-\ncompatible scenes. In European Conference on Com-\nputer Vision, 2022. 2\n[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser\nSheikh.\nRealtime multi-person 2d pose estimation\nusing part affinity fields. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npages 7291\u20137299, 2017. 2\n[7] Dan Casas and Marc Comino Trinidad.\nSmplitex:\nA generative model and dataset for 3d human tex-\nture estimation from single image.\narXiv preprint\narXiv:2309.01855, 2023. 2, 3\n[8] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and\nAlexei A Efros. Everybody dance now. In Proceedings\nof the IEEE/CVF international conference on computer\nvision, pages 5933\u20135942, 2019. 2, 5\n[9] Shubham Goel, Georgios Pavlakos, Jathushan Ra-\njasegaran, Angjoo Kanazawa, and Jitendra Malik. Hu-\nmans in 4D: Reconstructing and tracking humans with\ntransformers. In ICCV, 2023. 2, 3, 4\n[10] R\u0131za Alp G\u00fcler, Natalia Neverova, and Iasonas Kokki-\nnos. Densepose: Dense human pose estimation in the\nwild. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 7297\u20137306,\n2018. 2\n[11] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-\nMi Kang, Young-Jae Park, and Hae-Gon Jeon. High-\nfidelity 3d human digitization from single 2k resolution\nimages. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\n2023. 5\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local\nnash equilibrium. Advances in neural information pro-\ncessing systems, 30, 2017. 5\n[13] Alain Hore and Djemel Ziou. Image quality metrics:\nPsnr vs. ssim. In 2010 20th international conference\non pattern recognition, pages 2366\u20132369. IEEE, 2010.\n5\n[14] Johanna Karras, Aleksander Holynski, Ting-Chun\nWang, and Ira Kemelmacher-Shlizerman. Dreampose:\nFashion image-to-video synthesis via stable diffusion.\narXiv preprint arXiv:2304.06025, 2023. 2, 5\n[15] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu,\nJimei Yang, Jingwan Lu, Alexei A. Efros, and Kr-\nishna Kumar Singh.\nPutting people in their place:\nAffordance-aware human insertion into scenes. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[16] Boyi Li, Yin Cui, Tsung-Yi Lin, and Serge Belongie.\nSitta: Single image texture translation for data augmen-\ntation. In European Conference on Computer Vision,\npages 3\u201320. Springer, 2022. 2\n[17] Matthew Loper, Naureen Mahmood, Javier Romero,\nGerard Pons-Moll, and Michael J Black. Smpl: A\nskinned multi-person linear model. In Seminal Graph-\nics Papers: Pushing the Boundaries, Volume 2, pages\n851\u2013866. 2023. 2, 3\n[18] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu\nLee. Accurate 3d hand pose estimation for whole-\nbody 3d human mesh estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2308\u20132317, 2022. 5\n[19] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\nKanazawa, and Jitendra Malik. Tracking people by pre-\ndicting 3d appearance, location and pose. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2740\u20132749, 2022. 2, 3\n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution im-\nage synthesis with latent diffusion models. 2022 ieee.\nIn CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10674\u201310685, 2021. 4\n[21] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution im-\nage synthesis with latent diffusion models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 10684\u201310695,\n2022. 3, 5\n[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution im-\nage synthesis with latent diffusion models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10684\u201310695, 2022. 3\n[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sal-\nimans, et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding. Advances\nin Neural Information Processing Systems, 35:36479\u2013\n36494, 2022. 2\n[24] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin,\nJie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-\nto-video generation without text-video data.\narXiv\npreprint arXiv:2209.14792, 2022. 2\n[25] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir,\nDaniel Cohen-or, and Amit Haim Bermano. Human\nmotion diffusion model. In The Eleventh International\nConference on Learning Representations, 2023. 6, 7\n[26] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Ku-\nrach, Raphael Marinier, Marcin Michalski, and Syl-\nvain Gelly. Towards accurate generative models of\nvideo: A new metric & challenges. arXiv preprint\narXiv:1812.01717, 2018. 5\n[27] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pe-\ndro Cuenca, Nathan Lambert, Kashif Rasul, Mishig\nDavaadorj, and Thomas Wolf. Diffusers: State-of-\nthe-art diffusion models. https://github.com/\nhuggingface/diffusers, 2022. 5\n[28] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,\nZhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang. Disco: Disentangled control for referring\nhuman dance generation in real world. arXiv preprint\narXiv:2307.00040, 2023. 2, 5\n[29] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin\nLiu, Andrew Tao, Jan Kautz, and Bryan Catan-\nzaro.\nVideo-to-video synthesis.\narXiv preprint\narXiv:1808.06601, 2018. 2\n[30] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and\nEero P Simoncelli. Image quality assessment: from er-\nror visibility to structural similarity. IEEE transactions\non image processing, 13(4):600\u2013612, 2004. 5\n[31] Zhenzhen Weng, Laura Bravo-S\u00e1nchez, and Serena Ye-\nung. Diffusion-hpc: Generating synthetic images with\nrealistic humans. arXiv preprint arXiv:2303.09541,\n2023. 2\n[32] Xiangyu Xu and Chen Change Loy. 3d human texture\nestimation from a single image with transformers. In\nProceedings of the IEEE/CVF international conference\non computer vision, pages 13849\u201313858, 2021. 3\n[33] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu,\nQionghai Dai, and Yebin Liu. Function4d: Real-time\nhuman volumetric capture from very sparse consumer\nrgbd sensors. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR2021), 2021. 5\n[34] Lvmin Zhang and Maneesh Agrawala. Adding condi-\ntional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023. 2, 5\n[35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli\nShechtman, and Oliver Wang. The unreasonable ef-\nfectiveness of deep features as a perceptual metric. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 586\u2013595, 2018.\n5\n"
  },
  {
    "title": "ActAnywhere: Subject-Aware Video Background Generation",
    "link": "https://arxiv.org/pdf/2401.10822.pdf",
    "upvote": "11",
    "text": "ActAnywhere: Subject-Aware Video Background Generation\nBoxiao Pan1,2*\nZhan Xu2\nChun-Hao Paul Huang2\nKrishna Kumar Singh2\nYang Zhou2\nLeonidas J. Guibas1\nJimei Yang2\n1Stanford University\n2Adobe Research\nAbstract\nGenerating video background that tailors to foreground\nsubject motion is an important problem for the movie indus-\ntry and visual effects community. This task involves synthe-\nsizing background that aligns with the motion and appear-\nance of the foreground subject, while also complies with\nthe artist\u2019s creative intention. We introduce ActAnywhere,\na generative model that automates this process which tradi-\ntionally requires tedious manual efforts. Our model lever-\nages the power of large-scale video diffusion models, and is\nspecifically tailored for this task. ActAnywhere takes a se-\nquence of foreground subject segmentation as input and an\nimage that describes the desired scene as condition, to pro-\nduce a coherent video with realistic foreground-background\ninteractions while adhering to the condition frame.\nWe\ntrain our model on a large-scale dataset of human-scene\ninteraction videos. Extensive evaluations demonstrate the\nsuperior performance of our model, significantly outper-\nforming baselines. Moreover, we show that ActAnywhere\ngeneralizes to diverse out-of-distribution samples, includ-\ning non-human subjects. Please visit our project webpage\nat https://actanywhere.github.io.\n1. Introduction\nCompositing an acting video of a subject onto a novel back-\nground is central for creative story-telling in filmmaking\nand VFX. The key requirement is seamlessly integrating the\nforeground subject with the background in terms of cam-\nera motions, interactions, lighting and shadows, so that the\ncomposition looks realistic and vivid as if the subject acts\nphysically in the scene. In movie industry, this process is of-\nten conducted by virtual production [1] that requires artists\nto first create a 3D scene and then to film the acting video in\nan LED-walled studio or to render the video in 3D engines.\nThis process is not only tedious and expensive, but most im-\nportantly, prevents artists from quickly iterating their ideas.\nInspired by this artistic workflow, we study a novel prob-\nlem of automated subject-aware video background genera-\ntion. As shown in Fig. 1, given a foreground segmentation\n* Work done during an internship at Adobe.\nsequence that provides the subject motion, as well as a con-\ndition frame that describes a novel scene, we aim to gener-\nate a video that adapts the person to the novel scene with re-\nalistically synthesized foreground-background interactions.\nThis condition frame can be either a background-only im-\nage, or a composite frame consisting of both background\nand foreground, which can be created manually using photo\nediting tools such as Adobe Photoshop [3] or via automated\nimage outpainting methods such as Dall-E [32].\nThis problem poses significant challenges, as the human-\nscene interactions need to be correctly inferred and extrap-\nolated into an extended space-time volume given only the\ntwo input signals. The model also needs to implicitly reason\nabout the camera motion from the sequence of foreground\nsegmentation, which is an inherently ambiguous problem.\nFor example, in the first case of Fig. 1, the model needs to\ngenerate a background that moves according to the direc-\ntion that the woman runs towards. Last but not the least,\nto support various applications, we aim to have a model\nwith strong generalization capability, allowing for the re-\nalistic and creative integration of different subjects into di-\nverse background scenes.\nExisting works on video generation and editing, de-\nspite achieving impressive progress, are not able to solve\nthis task.\nRecent approaches generally focus on uncon-\nditional video generation [15, 19, 45], text-conditioned\nvideo generation [6, 15, 17, 19, 39], or simple outpaint-\ning masked regions [44, 47].\nMeanwhile, video editing\nmethods assume a source video as input and make edits\nbased on some condition signals, most commonly natural\nlanguage [5, 8, 13, 16, 22, 24, 41]. However, the edits these\nmethod make are mostly limited to stylization, which means\nthey preserve the spatial structure in the source video and\nperform only stylizing changes. On the other hand, simply\npropagating image outpainted results [9, 17] does not nec-\nessarily respect the guidance from the foreground subject\nmotion, and hence is under-constrained (as shown later in\nSec. 4.2). In this paper, we aim to completely generate the\nstructure and texture of the video background, while keep-\ning it coherent with the foreground subject motion.\nTo this end, we propose a diffusion-based model that\narXiv:2401.10822v1  [cs.CV]  19 Jan 2024\nFigure 1. Given a sequence of foreground segmentation as input, and one frame that describes the background as the condition, Ac-\ntAnywhere generates coherent video background that adapts to the subject motion. We show two subjects here, each with two generated\nsamples. ActAnywhere is able to generate videos consistent with the condition frame with highly realistic details such as splatting water,\nmoving smoke and flame, shadows, duck feet, etc. It generalizes to a diverse distribution of subjects and backgrounds, including non-\nhuman subjects. Our method works with both composited frames and background-only images as the condition.\nleverages cross-frame attention for temporal reasoning.\nSpecifically, our model takes as input a sequence of seg-\nmented foreground subject, the corresponding masks, and a\nsingle condition frame of the background, to generate the\ncomposited video with a hallucinated video background.\nSince temporal attention is currently the standard de facto\nfor diffusion-based video generation [10, 13, 16, 17, 24, 41]\ndue to the ability to generate temporally coherent videos, we\nalso perform temporal self-attention to frame-wise features,\nwhile conditioning the diffusion process on the features of\nthe background frame.\nWe train our model on a large-scale dataset [26] that con-\nsists of 2.4M videos of human-scene interactions in a self-\nsupervised fashion, and evaluate both on a held-out set as\nwell as on videos from DAVIS [30]. ActAnywhere is able\nto generate highly realistic videos that follow the condition\nframe, and at the same time synthesizes video background\nthat conforms to the foreground motion. Notably, despite\ntrained solely on videos of humans, ActAnywhere general-\nizes to non-human subjects in a zero-shot manner.\nIn summary, our contributions are:\n1. We introduce a novel problem of automated subject-\naware video background generation.\n2. We propose ActAnywhere, a video diffusion-based\nmodel to solve this task, and train it on a large-\nscale human-scene interaction video datatset in a self-\nsupervised manner.\n3. Extensive evaluations demonstrate that our model gen-\nerates coherent videos with realistic subject-scene inter-\nactions, camera motions, lighting and shadows, and gen-\neralizes to out-of-distribution data including non-human\nsubjects, such as animals and man-made objects.\n2. Related Work\nVideo generation. There have been a long thread of works\non video generation. The core architecture has evolved from\nGANs [11, 38, 40] to more recent transformers [15, 39, 44,\n47] and diffusion models [6, 9, 17, 19, 21, 24, 45]. Be-\nlow we review most related diffusion-based works. Most of\nthese works leverage temporal self-attention blocks inside\nthe denoising U-Net in order to acquire temporal aware-\nness. On top of that, Text2Video-Zero [24] introduces ad-\nditional noisy scheduling to correlate the latents in a video.\nLVDM [19] and Align Your Latents [6] both design a hierar-\nchical approach to generate longer-term videos. Align Your\nLatents additionally fine-tunes a spatial super-resolution\nmodel for high-resolution video generation.\nAnimateD-\niff [17] proposes to train the temporal attention blocks on\na large-scale video dataset, which can then be inserted into\nany text-to-image diffusion models (given that the archi-\ntecture fits) to turn that into a text-to-video model, in a\nzero-shot manner. VideoCrafter1 [9] further uses dual atten-\ntion to enable joint text and image-conditioned generation.\nThese works focus on unconditional generation or with text\nor image conditioning, but are not able to follow the guid-\nance of additional foreground motion.\nVideo editing. Another thread studies the problem of video\nediting, where a source video is given as input, and ed-\nits are performed according to some condition signals.\nText2Live [5] uses pre-trained video atlases of the input\nvideo, and performs text-guided edits on the foreground or\nbackground. Gen1 [13] leverages depth maps estimated by\na pre-trained network [33] as an additional condition to im-\nprove the structure consistency. Tune-A-Video [41] pro-\nposes to finetune only part of the spatial-attention blocks\nand all of the temporal-attention blocks on a single input\nvideo. TokenFlow [16] uses latent nearest neighbor fields\ncomputed from the input video to propagate edited features\nacross all frames. Both VideoControlNet [22] and Control-\nA-Video [10] adopt a ControlNet-like approach [46] to con-\ndition the video diffusion process with additional signals\nsuch as depth maps or Canny edges extracted from the in-\nput video. One downside of these works is that the gener-\nated videos tend to keep the spatial structure from the source\nvideo, which greatly limits the edits that the model is able\nto perform. In our work, we propose to condition on the\nforeground segmentation for the motion, while extract the\nbackground information only from one condition frame. In\nparticular, using the masked foreground as input endows a\nnice separation as in what to preserve and what to generate.\nImage and video inpainting.\nImage / video inpainting\naims to fill a missing region, often expressed as a mask.\nThese methods either take condition signals such as natu-\nral language and image [34, 42, 43], or rely solely on the\ncontext outside the masked region [14, 36, 44, 47]. Recent\ndiffusion-based image inpainting methods use a combina-\ntion of masked image and the mask itself, and condition\nthe diffusion process either on natural language [34, 42] or\nan image of the condition object [43], or perform uncondi-\ntional diffusion [36]. For video in-painting, MAGVIT [44]\nproposes a generative video transformer trained through\nmasked token prediction, and is able to inpaint small\nmasked regions afterwards. ProPainter [47] designs a flow-\nbased method by propagating pixels and features through\ncompleted flows. M3DDM [14] leverages a video diffusion\nmodel, and conditions the diffusion process on global video\nfeatures extracted by a video encoder. Different from these\nworks, we aim to generate large background regions that\nstrictly follow the condition frame. Moreover, the gener-\nated background needs to adapt to the foreground subject\nmotion in a coherent way. This poses significant challenges\nthat previous inpainting methods cannot tackle.\n3. Method\nWe first provide essential preliminary background on latent\ndiffusion in Sec. 3.1. We then formally define our problem\nin Sec. 3.2 and delve into our model design in Sec. 3.3. Fi-\nnally, we specify the training details in Sec. 3.4.\n3.1. Preliminaries on Latent Diffusion Models\nDiffusion models such as DDPM [20], encapsulate a for-\nward process of adding noise and a backward process of\ndenoising. Given a diffusion time step \u03c4, the forward pro-\ncess incrementally introduces Gaussian noises into the data\ndistribution x0 \u223c q(x0) via a Markov chain, following a\npredefined variance schedule denoted as \u03b2:\nq(x\u03c4|x\u03c4\u22121) = N(x\u03c4;\np\n1 \u2212 \u03b2\u03c4x\u03c4\u22121, \u03b2\u03c4I)\n(1)\nFor the backward process, a U-Net [35] \u03f5\u03b8 is trained to\ndenoise x\u03c4 and recover the original data distribution:\np\u03b8(x\u03c4\u22121|x\u03c4) = N(x\u03c4\u22121; \u00b5\u03b8(x\u03c4, \u03c4), \u03a3\u03b8(x\u03c4, \u03c4))\n(2)\n\u00b5\u03b8 and \u03a3\u03b8 are parametrized by \u03f5\u03b8. The discrepancy be-\ntween the predicted noise and the ground-truth noise is min-\nimized as the training objective.\nStable Diffusion [34] further proposes to train the dif-\nfusion model in the latent space of a VAE [25]. Specif-\nically, an encoder E learns to compress an input image x\ninto latent representations z = E(x), and a decoder D\nlearns to reconstruct the latents back to pixel space, such\nthat x = D(E(x)). In this way, the diffusion is performed\nin the latent space of the VAE.\nFigure 2. Architecture overview. During training, we take a randomly sampled frame from the training video to condition the denoising\nprocess. At test time, the condition can be either a composited frame of the subject with a novel background, or a background-only image.\n3.2. Problem Formulation\nGiven an input video X \u2208 RT \u00d7H\u00d7W \u00d73 featuring a fore-\nground subject, we first deploy a segmentation algorithm,\nsuch as Mask R-CNN [18], to obtain a subject segmentation\nsequence, S \u2208 RT \u00d7H\u00d7W \u00d73, along with the corresponding\nmasks, M \u2208 RT \u00d7H\u00d7W \u00d71. Both S and M serve as input to\nour model. S contains the segmentation of the foreground\nsubject, with background pixels set to 127 (grey). M has\nthe foreground pixels set to 0 and background to 1. Across\nall our experiments, H = W = 256 and T = 16.\nAdditionally, we also incorporate a single condition\nframe c \u2208 RH\u00d7W \u00d73 describing the background that we\nwant to generate.\nAs shown in Fig. 2, c is a randomly\nsampled frame from X at training time, while can be either\na frame showing foreground-background composition or a\nbackground-only image at inference time. The goal is thus\nto generate an output video V with the subject dynamically\ninteracting with the synthesized background. The motiva-\ntion of using an image not language as the condition is that\nimage is a more straightforward media to carry detailed and\nspecific information of the intended background, especially\nwhen users already have a pre-defined target scene image.\n3.3. Subject-Aware Latent Video Diffusion\nWe build our model based on latent video diffusion mod-\nels [17]. In our architecture design, we address two main\nquestions: 1) providing the foreground subject sequence to\nthe network to enable proper motion guidance, and 2) in-\njecting the condition signal from the background frame to\nmake the generated video adhere to the condition.\nWe present our pipeline as shown in Fig. 2. For the fore-\nground segmentation sequence S, we use the pre-trained\nVAE [34] encoder E to encode the foreground segmentation\ninto latent features \u02c6S \u2208 R16\u00d732\u00d732\u00d74. We downsample the\nforeground mask sequence M 8 times to obtain the resized\nmask sequence \u02c6\nM \u2208 R16\u00d732\u00d732\u00d71 to align with the latent\nfeatures \u02c6S. To train the denoising network \u03f5\u03b8, we encode\nthe original frames X with the same VAE encoder into la-\ntent representation Z \u2208 R16\u00d732\u00d732\u00d74, and add noises at\ndiffusion time step \u03c4 with the forward diffusion process de-\nnoted in Eq. (1) to get noisy latent feature Z\u03c4. We subse-\nquently concatenate \u02c6S, \u02c6\nM and Z\u03c4 along the feature dimen-\nsion, forming a 9-channel input feature Fi\n\u03c4 \u2208 R16\u00d79\u00d732\u00d732\nto the U-Net. During inference, Z0 is initialized as Gaus-\nsian noises, and gets auto-regressively denoised for multiple\ntime steps to sample a final result, according to the back-\nward diffusion process described in Eq. (2). The denoised\nlatents are then decoded to a video via the VAE decoder D.\nWe build our 3D denoising U-Net based on AnimateD-\niff [17]. AnimateDiff works by inserting a series of motion\nmodules in between the spatial attention layers in the de-\nnoising U-Net of a pre-trained T2I diffusion model. These\nmotion modules consist of a few feature projection layers\nfollowed by 1D temporal self-attention blocks.\nFor the condition image c, we follow prior works [26]\nto encode it with the CLIP image encoder [31], and take the\nfeatures from the last hidden layer as its encoding Fc. These\nfeatures are then injected into the UNet \u03f5\u03b8 through its cross-\nattention layers, similar to [26, 34]. We empirically find that\nthis method achieves better temporal consistency compared\nto other alternatives, such as using VAE features for either\ncross-attention or concatenation with other input features.\n3.4. Training\nModel training is supervised by a simplified diffusion ob-\njective, namely predicting the added noise [20]:\nL = ||\u03f5 \u2212 \u03f5\u03b8(Fi\n\u03c4, \u03c4, Fc)||2\n2\n(3)\nwhere \u03f5 is the ground-truth noise added.\nDataset. We train on the large-scale dataset compiled and\nprocessed by [26], which we refer to as HiC+. The resulting\ndataset contains 2.4M videos of human-scene interactions.\nIt also provides foreground segmentation and masks. We\nrefer the reader to the original paper for more details.\nPre-trained weights. We initialize the weights of our de-\nnoising network \u03f5\u03b8 with the pre-trained weights from the\nStable Diffusion image inpainting model [34], which is fine-\ntuned on top of the original Stable Diffusion on the text-\nconditioned image inpainting task. We initialize the weights\nof the inserted motion modules with AnimateDiff v2*.\nFor the CLIP image encoder, we use the \u201cclip-vit-large-\npatch14\u201d variant\u2020 provided by OpenAI, whose features\nfrom the last hidden layer have a dimension of 1024, while\nthe pre-trained U-Net takes in features of dimension 768 as\nthe condition, which are also in the text feature space. To\naccount for this, we train an additional two-layer MLP to\nproject the features into the desired space.\nDuring training, we freeze the shared VAE and the CLIP\nencoder, and fine-tune the U-Net with the motion modules.\nData processing and augmentation.\nObtaining perfect\nsegmentation masks from videos is challenging. The masks\nmay be incomplete, missing some parts of the foreground,\nor be excessive such that they include leaked background\nnear the boundary. To deal with incomplete segmentation,\nduring training, we apply random rectangular cut-outs to the\nforeground segmentation and masks. To reduce information\nleak from excessive segmentation, we perform image ero-\nsion to the segmentation and masks with a uniform kernel\nof size 5 \u00d7 5, both during training and inference.\nRandom condition dropping. In order to enable classifier-\nfree guidance at test time, we randomly drop the segmenta-\ntion and the mask, the condition frame, or all of them at\n10% probability each during training. In these cases we set\nthem to zeros before passing into the respective encoders.\nOther details.\nWe use the AdamW [27] optimizer with\na constant learning rate of 3e-5. We train our model on\n*https://github.com/guoyww/animatediff/\n\u2020https://huggingface.co/openai/clip-vit-large-patch14\n8 NVIDIA A100-80GB GPUs at a batch size of 4, which\ntakes approximately a week to fully converge.\n4. Experiments\nWe start by describing the data used for evaluation. We\nthen show diverse samples generated from our method\nin Sec. 4.1,\nboth using an inpainted frame and a\nbackground-only frame as conditioning. In Sec. 4.2, we\ncompare with various baselines. We provide additional re-\nsults and analysis in Sec. 4.3. Specifically, we show that cer-\ntain general video inpainting capability emerges from our\nmodel once trained. We also demonstrate that our model is\nrobust to inaccurate foreground segmentation at test time.\nFinally, we analyze the model runtime.\nFollowing prior works [5, 10, 13, 16, 41], we com-\npare with previous works on videos sampled from the\nDAVIS [30] dataset. We select videos with both human and\nnon-human subjects. We also evaluate on held-out samples\nfrom the HiC+ dataset. Samples with our method are gen-\nerated with 50 denoising steps, with a guidance scale [34]\nof 5.\n4.1. Diverse Generation with ActAnywhere\nIn Fig. 3, we show results on the held-out segmentation se-\nquences from the HiC+ dataset, using an inpainted frame or\na background-only frame as condition. ActAnywhere gen-\nerates highly realistic foreground-background interactions\nboth at coarse and fine-grained levels. At a coarse level, our\nmodel synthesizes road structure, pumpkin field, city views,\nwaves, etc. that align with the subject\u2019s motion. While at a\nfine-grained level, our method also generates small mov-\ning objects that are in close interaction with the subject,\nsuch as the buckets, bed sheets, horses and dune buggies,\nas well as the dog. Moreover, these generation stay consis-\ntent across frames, and tightly follow the guidance in the\ncondition frame. The synthesized backgrounds also exhibit\ncoherent scale, lightning, and shadows (also see Fig. 1).\n4.2. Comparison with Baselines\nBaselines. We first clarify that since we study a novel prob-\nlem, there is no prior work operating under the exact same\nsetting to the best of our knowledge. We hence compare to\nclosest works and adapt some, i.e. AnimateDiff [17], if nec-\nessary. Nonetheless, we emphasize that the formulation and\npipeline are the core contribution of this work.\nWe compare ActAnywhere to a number of baselines,\nwhich we classify based on whether they do (Fig. 4 top)\nor do not (Fig. 4 bottom) take a video as input. For the\nmethods taking a video as input, Gen1 [13] takes an addi-\ntional image as condition, and also leverages a pre-trained\ndepth-estimation network [33]. Given pre-trained neural at-\nlases [23], Text2LIVE [5] assumes a text prompt as condi-\ntion to synthesize the edited video. TokenFlow [16] also\nFigure 3. Additional results with our method. The top part shows examples using inpainted frames as condition, while bottom contains\nexamples with background-only conditioning. Foreground sequences are from the held-out set of HiC+.\nFigure 4. Comparison with baselines. We provide results on two videos sampled from the DAVIS [30] dataset. For each example, we\nshow three representative frames (top) and their corresponding condition signal (left). Note that different methods assume different input,\nconditioning or pre-trained models, as specified in Sec. 4.2.\nuses text conditioning. Control-A-Video [10] first extracts\nCanny edges from the input video, then synthesizes the out-\nput video conditioned jointly on the edges and text.\nFor baselines without a video as input, we use the strat-\negy contributed by a public pull request\u2021 to make Animate-\nDiff [17] take additional image conditioning. Specifically,\n\u2021https://github.com/guoyww/AnimateDiff/pull/8\nFigure 5. Zero-shot video inpainting with our model. We show two cases from DAVIS, each with four sampled frames. The yellow\nregions denote the masked areas to be inpainted.\nFigure 6. Our method is robust to inaccurate masks. We show\ntwo examples from HiC+, each with its foreground segmentation\nfollowed by two generated outputs with different condition frames.\nWe only show one frame and do not show the condition frame due\nto space limit. Please see supplement for full examples in videos.\nat test time, latent features are first extracted from the con-\ndition image with the pre-trained SD VAE encoder [34],\nwhich are then merged with the original per-frame Gaus-\nsian noises through linear blending. The diffusion process\nis later conditioned on a text prompt too. VideoCrafter1 [9]\nprovides both a text-to-video and an image-to-video model.\nWe use the latter for a closer comparison setting.\nThe qualitative comparison on two examples from the\nDAVIS [30] dataset is shown in Fig. 4. Our method gener-\nates temporally coherent videos that follow the foreground\nmotion with highly realistic details, e.g. falling snow and\nsnow on the car windshield, while strictly follows the guid-\nance and constraints given by the condition frame. Baseline\nmethods in the first category generally inherit the structure\npresent in the input video, e.g. road direction, horse, etc.,\nand hence they completely fail when fine-grained edits are\ndesired, e.g. horse changes to motorcycle in the second case.\nMethods in the second category generate unconstrained mo-\ntion due to lack of guidance (VideoCrafter1 in the second\nexample generates backward motion, which is more evident\nin the supplementary video).\n4.3. Additional Results and Analysis\nGeneral video inpainting. Interestingly, once trained, cer-\ntain general video inpainting capability emerges from our\nmodel. We perform preliminary experiments by manually\ncreating a mask sequence, and pass those with the fore-\nground sequence as the input to our model, and we disable\nthe condition signal by setting it to 0. Two cases are shown\nin Fig. 5, where our model is able to inpaint the missing re-\ngions, despite not explicitly trained so. This may suggest\nthat our model learns to approximate the underlying data\ndistribution to a certain degree, possibly benefiting from the\nrandom condition dropping during training (Sec. 3.4). We\nfind similar results with general video outpainting, which\nwe show in supplement.\nRobust to inaccurate masks. As stated in Sec. 3.4, masks\ncreated or extracted in practice are often imperfect, being ei-\nther incomplete or excessive. Here we show that our model\ntrained in our designed procedure is robust to imperfect\nmasks. In Fig. 6, we showcase two examples of this. De-\nspite a large region of the guitar (top) and both feet (bottom)\nmissing, our model is able to hallucinate them in a reason-\nable way by considering the global context.\nRuntime. Generating one video on an NVIDIA A100 GPU\ntakes about 8.5 seconds, thus enables much faster idea iter-\nation compared to traditional workflows.\n5. Conclusion\nWe present ActAnywhere, a video diffusion-based model\nthat generates videos with coherent and vivid foreground-\nbackground interactions, given an input foreground seg-\nmentation sequence and a condition frame describing the\nbackground. Our model synthesizes highly realistic details\nsuch as moving or interacting objects and shadows. The\ngenerated videos also exhibit consistent camera scales and\nlighting effects. We believe our work contributes a useful\ntool for the movie and visual effects community, as well as\nfor the general public to realize novel ideas of situating an\nacting subject in diverse scenes, in a simple and efficient\nway that is not previously possible.\n6. Acknowledgment\nWe thank the authors of [26] for compiling and processing\nthe dataset HiC+, especially Sumith Kulal for the code and\ninstructions on accessing the data. We also thank Jiahui\n(Gabriel) Huang from Adobe Research for helping set up\nthe Adobe Firefly GenFill API.\nReferences\n[1] Virtual Production.\nhttps://en.wikipedia.org/\nwiki/On-set_virtual_production. 1\n[2] Adobe.\nFirefly.\n2023.\nhttps://www.adobe.com/\nsensei/generative-ai/firefly.html. 11, 13\n[3] Adobe.\nPhotoshop, version.\n2023.\nhttps://www.\nadobe.com/products/photoshop.html. 1, 11\n[4] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, 2021. 12\n[5] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In ECCV, 2022. 1, 3, 5\n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 1, 3\n[7] Tim Brooks and Alexei A Efros.\nHallucinating pose-\ncompatible scenes. In ECCV, 2022. 13\n[8] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In ICCV,\n2023. 1\n[9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideocrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 1, 3, 8\n[10] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models.\narXiv preprint arXiv:2305.13840, 2023. 2, 3, 5, 7\n[11] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adver-\nsarial video generation on complex datasets. arXiv preprint\narXiv:1907.06571, 2019. 3\n[12] Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri,\nJ\u00a8urgen Gall, Rainer Stiefelhagen, and Luc Van Gool. Large\nscale holistic video understanding. In ECCV, 2020. 12\n[13] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, 2023. 1, 2, 3, 5, 13\n[14] Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng\nGe, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. Hierar-\nchical masked 3d diffusion model for video outpainting. In\nACM MM, 2023. 3\n[15] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic vqgan and time-\nsensitive transformer. In ECCV, 2022. 1, 3\n[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 1, 2, 3, 5\n[17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 1, 2, 3, 4, 5,\n7\n[18] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 4\n[19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation.\narXiv preprint arXiv:2211.13221,\n2023. 1, 3\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 3, 5\n[21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 3\n[22] Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided\nvideo-to-video translation framework by using diffusion\nmodel with controlnet.\narXiv preprint arXiv:2307.14073,\n2023. 1, 3\n[23] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM TOG,\n40(6), 2021. 5\n[24] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 1, 2, 3\n[25] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[26] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei\nYang, Jingwan Lu, Alexei A Efros, and Krishna Kumar\nSingh. Putting people in their place: Affordance-aware hu-\nman insertion into scenes. In CVPR, 2023. 2, 4, 5, 9, 12,\n13\n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[28] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ra-\nmakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown,\nQuanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments\nin time dataset: one million videos for event understanding.\nIEEE TPAMI, 42(2), 2019. 12\n[29] OpenAI. ChatGPT. 2023. https://chat.openai.\ncom/. 11\n[30] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017. 2, 5, 7, 8\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 5\n[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1, 11\n[33] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE TPAMI, 44(3), 2020. 3, 5\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 3, 4, 5,\n8\n[35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015. 3\n[36] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In ACM\nSIGGRAPH, 2022. 3\n[37] Gunnar A Sigurdsson, G\u00a8ul Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta.\nHollywood in\nhomes: Crowdsourcing data collection for activity under-\nstanding. In ECCV, 2016. 12\n[38] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz.\nMocogan: Decomposing motion and content for\nvideo generation. In CVPR, 2018. 3\n[39] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\n1, 3\n[40] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. In NeurIPS, 2016.\n3\n[41] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, 2023. 1, 2, 3, 5\n[42] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun\nZhang. Smartbrush: Text and shape guided object inpainting\nwith diffusion model. In CVPR, 2023. 3\n[43] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In CVPR, 2023. 3\n[44] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked\ngenerative video transformer. In CVPR, 2023. 1, 3\n[45] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace. In CVPR, 2023. 1, 3\n[46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nCVPR, 2023. 3\n[47] Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and\nChen Change Loy. Propainter: Improving propagation and\ntransformer for video inpainting. In ICCV, 2023. 1, 3\nFigure 7. Zero-shot video outpainting with our model. We show two examples from the DAVIS dataset.\nIn this supplement, we first provide more examples of\nthe general video outpainting application and of that our\nmodel is robust to inaccurate masks in Appendix A, fol-\nlowing Sec. 4.3 of the main manuscript. We then describe\nessential processing steps for training and evaluation data\nin Appendix B. We show failure cases and discuss limita-\ntions of our model in Appendix C. Lastly, we conclude by\ndiscussing the ethical impact of this work in Appendix D.\nWe strongly encourage the reader to check our project\nwebpage, where we show extensive video results on video\nbackground generation with diverse generated contents and\ncamera motions, and under various conditioning scenarios.\nIt also contains the video version of the comparison with\nbaselines.\nA. Full Examples for Sec. 4.3 of the Main Paper\nGeneral video outpainting.\nAs stated in Sec. 4.3 of\nthe main manuscript, preliminary capability on general\nvideo inpainting / outpainting emerges from our model once\ntrained. In Fig. 5 of the main paper, we show results on gen-\neral video inpainting. Here we present the results for apply-\ning the same model on general video outpainting in Fig. 7.\nWe resize the original sequence of frames by 0.75, and pad\nthem with gray boundaries. Associated masks are also cre-\nated to indicate the missing regions. We then randomly se-\nlect one frame and use Adobe Firefly [2] to outpaint it, with\nwhich as condition we outpaint the entire sequence. Along\nwith the general video inpainting results shown in Fig. 5 of\nthe main paper, these suggest that our model learns to ap-\nproximate the underlying data distribution to a certain de-\ngree.\nRobust to inaccurate masks. We show the full version of\nthe first example in Fig. 6 of the main manuscript in Fig. 8.\nOur model fills in realistic details of the guitar and the man\u2019s\nhand, which are missing from the segmentation due to inac-\ncurate masks.\nB. Data Processing\nTraining. In Sec. 3.4 of the main manuscript, we described\nour data processing and augmentation strategies for training\ndata. Specifically, to deal with incomplete segmentation, we\napply random rectangular cut-outs to the segmentation and\nmasks. We show two examples in Fig. 9.\nEvaluation.\nAs mentioned in Sec. 1 of the main\nmanuscript, at test time, the composited foreground-\nbackground frames used as condition can be created with\nvarious methods, such as photo editing tools (e.g. Adobe\nPhotoshop [3]) or automated image outpainting methods\n(e.g. Dall-E [32]). In our experiments, we adopt ChatGPT\n4 [29] and Adobe Firefly [2] to automatically synthesize\nthese composited frames for use at test time. Specifically,\nwe first sample the 0-th, 8-th, and 15-th frames from the in-\nput video, and ask ChatGPT the following question \u201cBased\non these three images, can you give me a description as\nFigure 8. Our model is robust to inaccurate masks. Here we show the full version of the first example in Fig. 6 of the main manuscript.\nFigure 9. Data augmentation. We apply random cut-outs to the\nperson segmentation during training. Here we show two examples\nof cut-outs with their corresponding original frames.\nprompt, less than 10 words, about one contextual scenario\nwe can put this human in?\u201d. We then use Firefly to synthe-\nsize the outpainted frames, given the foreground segmenta-\ntion and the text prompt. We use the \u201cgpt4-vision-preview\u201d\nversion of ChatGPT 4 \u00a7, and the official Firefly GenFill \u00b6.\nC. Limitations\nWe show two failure cases of our method in Fig. 10. In\nthe first example, the grass-like texture on the dress is ex-\ncluded from the segmentation, hence the model mistakenly\nperceives it to be an independent object growing outside the\ndress. While in the second example, the Firefly-inpainted\nframe has the broomstick facing towards the wrong direc-\ntion. Although the model tries to generate something rea-\nsonable, it fails to correct this mistake to produce a coherent\nvideo. Despite certain fault tolerance from our model, pro-\nviding proper input segmentation and condition signal helps\nensure high-quality generation results.\n\u00a7https://platform.openai.com/docs/overview\n\u00b6https://firefly.adobe.com/generate/inpaint\nFigure 10. Failure cases. Foreground sequences from DAVIS.\nD. Data Ethics\nThe HiC+ dataset [26] includes public datasets HVU [12],\nCharades [37], Moments [28], and WebVid10M [4]. Read-\ners are encouraged to refer to Section A.1 in the supplemen-\ntary material of [7] for the license of these datasets. More-\nover, same as [26], more internal videos are incorporated\nduring training and evaluation. We have conducted the nec-\nessary legal review process, and can provide more details of\nthis data upon request.\nWe present a method that can synthesize highly realis-\ntic videos that place human and non-human subjects into\ndiverse background scenes. While our work has implica-\ntions and values for a wide range of fields, we note that\nour model can be misused to generate malicious content.\nSimilar to Firefly GenFill [2] and Gen1 [13], the generated\ncontent can be watermarked to prevent misuse. Moreover,\nour model inherits the demographic biases presented in the\ntraining data. We make our best effort to demonstrate im-\npartiality in all the results shown.\n"
  },
  {
    "title": "Rambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation",
    "link": "https://arxiv.org/pdf/2401.10838.pdf",
    "upvote": "8",
    "text": "Rambler: Supporting Writing With Speech via LLM-Assisted Gist\nManipulation\nSusan Lin\nUC Berkeley\nBerkeley, CA, USA\nJeremy Warner\nUC Berkeley\nBerkeley, CA, USA\nJ.D. Zamfirescu-Pereira\nUC Berkeley\nBerkeley, CA, USA\nMatthew G. Lee\nUC Berkeley\nBerkeley, CA, USA\nSauhard Jain\nUC Berkeley\nBerkeley, CA, USA\nMichael Xuelin Huang\nGoogle\nMountain View, CA, USA\nPiyawat Lertvittayakumjorn\nGoogle\nMountain View, CA, USA\nShanqing Cai\nGoogle\nMountain View, CA, USA\nShumin Zhai\nGoogle\nMountain View, CA, USA\nBj\u00f6rn Hartmann\nUC Berkeley\nBerkeley, CA, USA\nCan Liu\u2217\nSchool of Creative Media, City\nUniversity of Hong Kong\nHong Kong, China\nFigure 1: Supporting long-form text composition via speech with a new dictation interface that consists of three components:\n1) dictating trains of thought into Ramble boxes, 2) reviewing the spoken text through automatically generated keywords and\nsummaries, and 3) performing manual or LLM-assisted macro-revisions operated at conceptual levels.\n\u2217Corresponding author\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\n\u00a9 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0330-0/24/05.\nhttps://doi.org/10.1145/3613904.3642217\nABSTRACT\nDictation enables efficient text input on mobile devices. However,\nwriting with speech can produce disfluent, wordy, and incoherent\ntext and thus requires heavy post-processing. This paper presents\nRambler, an LLM-powered graphical user interface that supports\ngist-level manipulation of dictated text with two main sets of func-\ntions: gist extraction and macro revision. Gist extraction generates\nkeywords and summaries as anchors to support the review and\ninteraction with spoken text. LLM-assisted macro revisions allow\nusers to respeak, split, merge, and transform dictated text without\narXiv:2401.10838v2  [cs.HC]  8 Mar 2024\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nspecifying precise editing locations. Together they pave the way\nfor interactive dictation and revision that help close gaps between\nspontaneously spoken words and well-structured writing. In a com-\nparative study with 12 participants performing verbal composition\ntasks, Rambler outperformed the baseline of a speech-to-text editor\n+ ChatGPT, as it better facilitates iterative revisions with enhanced\nuser control over the content while supporting surprisingly diverse\nuser strategies.\nCCS CONCEPTS\n\u2022 Human-centered computing \u2192 Interactive systems and\ntools; Interaction paradigms; Interaction techniques; Natural lan-\nguage interfaces.\nKEYWORDS\ndictation, speech, speech-to-text, STT, text composition, writing,\nLLM, AI\nACM Reference Format:\nSusan Lin, Jeremy Warner, J.D. Zamfirescu-Pereira, Matthew G. Lee, Sauhard\nJain, Michael Xuelin Huang, Piyawat Lertvittayakumjorn, Shanqing Cai,\nShumin Zhai, Bj\u00f6rn Hartmann, and Can Liu. 2024. Rambler: Supporting\nWriting With Speech via LLM-Assisted Gist Manipulation. In Proceedings\nof the CHI Conference on Human Factors in Computing Systems (CHI \u201924),\nMay 11\u201316, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 19 pages.\nhttps://doi.org/10.1145/3613904.3642217\n1\nINTRODUCTION\nSpeech, regarded as a natural input modality, has attracted much\nattention following the breakthroughs of automatic speech recog-\nnition (ASR) and natural language processing (NLP). Speech tran-\nscription software products like Otter.ai, Google Voice Typing, and\nDragon are gaining popularity. Dictation is now an integral key-\nboard function on most smartphones and tablets. Indeed, dictation\ncan be three to four times faster than typing for text input under the\nbest circumstances [26]. This makes it particularly advantageous\nfor inputting long text, such as writing memos or drafting ideas.\nMobile devices benefit even more, given the challenges of using a\nkeyboard and cursor on a touchscreen [31].\nCurrent interfaces for dictating long text largely mirror tradi-\ntional text editors with an added microphone. However, writing\nwith speech can be more than just dictation. Previous research [20]\nshowed that speaking differs significantly from typing in terms of\nboth production and memory retention. Transcribed spoken text\ncan be much more verbose, erroneous, and disorganized, leading\nto high-effort editing [13]. The verbatim memory of spoken con-\ntent is also weaker, leading to a likely higher effort in reading the\ntranscripts. Therefore, treating speaking simply as a faster \u201ctyping\u201d\nmethod can be problematic, especially for long texts. Similarly, re-\nsearchers of voice user interfaces (VUI) have long argued against\ndirectly transplanting GUI guidelines to VUI design [22]. All these\ncall for new affordances and interface solutions for writing with\nspeech. Further, writing is a highly iterative activity and thought\ndevelopment process. Typically, digital writing requires people\u2019s\nfull attention on a screen while sitting in front of a computer with\na mouse and keyboard, but this requires time and an environment,\nwhich creates a high barrier to starting. While speech has the po-\ntential to be used for faster and lower-barrier writing, its adoption\nremains limited and the effort to edit resulting transcripts is large.\nThis work sets out to tackle that challenge and answer the ques-\ntion: Can we identify and build stepping stones towards closing the\ngap between speaking and writing, by designing a dictation inter-\nface that addresses the challenges inherent in speech? We build on\nexisting solutions in recent publications that use NLP and large\nlanguage models (LLMs) to generate summaries to assist writing\n[6] and review spoken dialogue [15, 16]. These interfaces extract\nthe gist of text to support visualization and/or interaction with that\ntext, a paradigm we call gist-based interfaces. They move beyond\nautomatic text summarization to semantic manipulation, with the\ninteraction centered around the \u201cgist\u201d of each piece of text, rather\nthan the sequences of characters in each word of that text. While\nsuch concepts have been explored in traditional writing [6], we\noperationalize these ideas in the context of interacting with spoken\ntext, which is perhaps an even more compelling use given the liter-\nature that shows spoken content more often remains in memory in\nthe form of gists, rather than verbatim [20].\nWe introduce Rambler, an LLM-powered graphical user inter-\nface (GUI) that implements a gist-based interface tailored to dictated\ntext (see Figure 1). Users can dictate their extemporaneous thoughts\nand place each segment of recording (i.e., a fragment of an idea)\ninto a Ramble. Rambler provides Semantic Zoom [23] to visualize\nmultiple summarization levels in the form of gists for each Ramble,\nwhich users can review on demand. Users then focus on iterating\non higher-level ideas by manipulating the Rambles, through res-\npeaking, splitting, merging and reorganizing them, activities that\nwe collectively call macro revision. Behind the scenes, Rambler\nuses an LLM to automatically clean up any disfluencies and punctu-\nation errors from the raw transcript, as well as to complete broken\nsentences and to smooth transitions, activities that we call micro\nrevision. In addition, Rambler extracts keywords from text to aid\nvisual skimming across Rambles and Semantic Zoom levels. Indi-\nvidual keywords can also be activated and deactivated by the user\nto indicate importance; these keywords then serve as parameters to\ncustomize summary (gist) generation and other LLM-based word\nprocessing.\nRambler aims to aid the review and revision of spoken text and\nease interactions with text on mobile devices. To evaluate Ram-\nbler\u2019s effectiveness, we conducted a comparative study with 12\nparticipants using a long-form verbal text composition task. The\nbaseline is a standard speech-to-text editor interface with Chat-\nGPT provided on the side. While participants demonstrated varied\nstrategies and feature usage, most of them expressed a preference\nof Rambler over the baseline and acceptance of using it for their\nown tasks. Their feedback shows Rambler provides better support\nfor developing and reviewing spoken content, supporting iteration,\nand improving user control over content revision.\nOur work makes the following contributions: First, Rambler: a\nnovel tool that implements the concept of a gist-based interface\nto support writing with speech on mobile devices. We motivate\nour design goals and provide implementation details for Rambler.\nSecond, we contribute empirical findings from a user study reveal-\ning the potential of Rambler in helping close the gap between\nrambling and writing. Our study also reveals several advantages of\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nour specialized LLM-backed GUI over a one-size-fits-all dialogue-\nbased chatbot UI for LLMs; we thus also offer insights, based on\nour findings, for designing and building LLM-supported direct ma-\nnipulation interfaces.\n2\nRELATED WORK\nTo provide more context for our contributions, we discuss three\nareas of highly related work: voice dictation and editing, supporting\nwriters with LLMs, and semantic manipulations on text.\n2.1\nVoice Dictation and Editing\nAlthough speaking is faster than typing or writing, it can be time-\nconsuming to correct errors caused by speech artifacts, such as\ndisfluencies and repetitions. Modern speech and NLP research has\nmade much progress in cleaning disfluency and speech recognition\nerrors [17, 19, 30], mostly for post-processing. Researchers have\nalso proposed multimodal interaction methods to reduce editing\neffort, especially on mobile devices. For example, EyeSayCorrect\nuses gaze to infer users\u2019 editing intentions and allows re-dictation\nto edit the phrase near eye fixations [38]. Similar ideas were applied\nto other modalities, such as touch [37].\nOther work has explored speech-based editing through auto-\nmatic removal of colloquial or filler words [1] as well as voice\nediting with and without explicit command keywords [10]. Ghosh\net al. [10] found that using explicit command words (e.g., \"replace\"\nand \"delete\") were suitable for one-word edits, and re-dictation with-\nout command words was suitable for complex edits. To leverage\nthe advantages of both cases, \"Just speak it\" supports commanding\nwithout explicit command words [8], which are instead induced\nfrom existing context and the edit command. Li et al. [14] further\nexplored auto segmentation and classification to support the seam-\nless transition between voice dictation and editing with natural\nlanguage. While this line of work lays the groundwork for natu-\nral and effortless voice-based text editing, it primarily focuses on\nediting one or a few words. Generalizing to sentence-level macro\nrevision remains largely unexplored.\n2.2\nWriting with LLMs\nA number of studies have demonstrated that text generation by\nLLMs can be beneficial for writing assistance. For example, Yuan\net al. [33] developed Wordcraft, an editor that allows users to col-\nlaborate with an LLM to write stories. Users can replace selected\ntext with LLM suggestions or prompt the model for text genera-\ntion. Yang et al. [32] investigated an LLM-powered writing tool\nto shorten, edit, summarize, and generate text. Gero et al. [9] cus-\ntomized the decoding method of an LLM to encourage diverse\noutputs specifically for scientific writings.\nWhile LLMs have made significant progress in the field of writing,\nthere is still room for improvement. LLMs may struggle to retain\nthe writing style and align with the writer\u2019s goal. Ippolito et al. [11]\nfound that LLMs can struggle to preserve the style and authorial\nvoice of experienced writers. This is supported by Biermann et\nal.\u2019s [2] study, which found that authors prefer text generation\ntools that respect their styles and strategies. Rahman et al. [24]\nstudied academic writing with ChatGPT. Their findings revealed\nthat ChatGPT can be useful to generate initial ideas, but falls short\nin tasks that demand critical thinking, such as literature synthesis,\ncitations, problem statements, research gaps, and data analysis.\nTaken together, LLMs are a promising tool for writing, but they\nalone are not yet perfect.\n2.3\nSemantic Manipulations on Text\nIn addition to text generation discussed in the previous section,\nLLMs can also support macro-level structural revision. However, as\nStrobl et al. [28] recently surveyed, writing tools that support gram-\nmar and word-level editing are common, but tools for macro-level\nstructural revision are relatively rare. Arnold et al. [1] highlighted\nthe opportunities to return semantic control to the writers. One of\ntheir proposals is to support within-sentence structural manipula-\ntion, where users can drag and drop words to rephrase the sentence.\nAlongside that, semantic zooming [23] is another promising feature\nfor collaborative human-AI interfaces. It allows users to explore\nand navigate complicated data (e.g. long texts and nested tables)\nby zooming in and out to check details of different levels. This\ncan be particularly useful for interacting with LLMs in a conver-\nsational paradigm, as it reduces the cognitive and physical load of\nscrolling back and forth to linearly search for information from a\nlengthy conversation. Several studies have demonstrated that se-\nmantic zooming is an effective way to help users navigate and digest\ncomplex content. Sensescape [29] and Graphologue [12] introduce\nzoomable visual representations of different concepts involved in a\nconversation with LLMs. Li et al. [15, 16] developed a system that\ngenerates hierarchical summaries of spoken dialog. As the closest\nwork to ours, Beyond Text Generation [6] is a writing tool that\nprovides on-the-fly paragraph summarization along with the origi-\nnal text. This work also provides interaction with the summaries\nof paragraphs, such as reorganization via drag and drop, which\nmanipulates the original text in parallel. Building on this work, we\nleverage the benefit of summaries as one way of extracting gists.\nWhile its focus is on keyboard typing and ours is on speech input,\nour interface also provides keywords extraction and interaction as\nwell as other LLM-assisted operations in order to support macro\nrevision.\n3\nRAMBLER: A GIST-BASED INTERFACE FOR\nWRITING WITH SPEECH\n3.1\nDesign Rationale and Goals\nAs explained in the introduction, we aim to design a gist-based\ndictation interface to support writing with speech, by helping users\novercome the challenges of speech production and shortcomings\nof memory. The challenges and shortcomings identified in previ-\nous empirical and theoretical work [18, 20] can be summarized as\nfollows: 1) speech is spontaneous as it is produced in real-time, so\nwhile the gists may be planned, the wording is not; 2) looking at\na live transcript while speaking can be distracting, so people tend\nto have less visual engagement speaking than they would typing,\nwhich leads to less micro-revision when speaking; 3) remembering\nspoken content verbatim is a challenge memory task; combined\nwith reduced visual engagement, these challenges may also make\nreviewing content harder.\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nFigure 2: A labeled screenshot of the Rambler UI. (1) Ramble in default state, with revision functions accessible through\nbuttons on (2) and (3). (4) Ramble in re-speaking mode, where voice input is transcribed so that it can be appended to current\ntext, replace the current text, or to be discarded using the buttons on (5). Fixed at the bottom of the UI is (6), with the Semantic\nMerge button, New Ramble button, and Semantic Zoom slider.\n3.1.1\nRambles as the unit of interaction. With these characteristics,\na text editor that transcribes everything linearly will also include\nall the disfluencies, repetition, and disorganization of raw speech.\nAlthough advanced NLP techniques can help correct some of these\nissues, the user still needs to direct the composition\u2019s meaning and\norganization. Successful LLM interfaces (e.g., ChatGPT,1 Claude2)\nhave demonstrated the ability to refine unorganized text into high-\nquality writing. However, such general tools not only rely on users\u2019\nability to prompt the LLM, which has been observed to be challeng-\ning [34, 35], but also make it harder for users to precisely control\nwhere and how their text is edited.\nBuilding on previous work that shows the benefits of using NLP\nor LLMs in direct manipulation interfaces [6, 12], we investigate\nhow to leverage recent LLM and NLP techniques to inject intelligent\nword processing into the GUI for our specific task of writing with\nspeech, with the goal of preserving user control over the composi-\ntion. With these considerations, we aim to improve the text editor\ninterface for better interaction with spoken text by establishing the\nfollowing design goals:\n\u2022 Support non-linear composition. The tool should allow users\nto ramble spontaneously while providing them with struc-\ntures to support iterative organizing and editing their spoken\ntext.\n\u2022 Support iterative drafting. Leverage the fast production of\nspeech while recognizing that the first attempt may not be\nwell articulated as written text. Our interface should encour-\nage and support users easily iterating on drafts by respeaking\nlarge chunks of the text before diving into micro-editing.\n1https://chat.openai.com/\n2https://claude.ai/\n\u2022 Aid the review and navigation of spoken text. The interface\nshould help reduce user effort in reading and comprehend-\ning the transcript, which could be verbose, repetitive, and\ncontain errors or disfluencies.\n\u2022 Leverage LLMs while preserving user control. The interface\nshould leverage the capacity of LLMs in cleaning up, smooth-\ning, and transforming text while allowing users to control\nwhere and how changes will be applied.\n\u2022 Optimize interactions for mobile devices. Although speech-\ntor-text input can be used on any device, mobile devices\nparticularly benefit, given the known challenges of inter-\nacting with text on these devices without keyboards. For\nour evaluation, we target a tablet interface for designing our\ncurrent interface, as it is a commonly used mobile device\nsuitable for long-form writing.\nIn the next subsection, we will explain how we designed the\ninterface of our tool to achieve these design goals.\n3.2\nRambler Interface Design\nRambler is built around the creation and manipulation of Rambles,\nillustrated in Figure 1 and 2. Each Ramble is a container (visually\nseparated from other Rambles) that holds its content and multiple\nbuttons for users to interact with. Our intention is for the content\nof a Ramble to roughly correspond to a single train of thought.\nWithout the pressure of producing a decent piece of text in one\nshot, users may either speak everything into one Ramble and split\nit later, or start a new Ramble when they feel what they will say\nnext is about a different point or subject matter. Rambler should\nsupport various writing strategies, including those starting both\nwith or without an outline in mind. Here, we explain the design of\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nFigure 3: From left to right, example text in three Rambles is presented at all four Semantic Zoom levels: full transcript, 50%\nlength, 25% length, and 10% length.\n(a)\n(b)\nFigure 4: Example Rambles before and after (a) Semantic Merge and (b) Semantic Split. In (a), the user select all the Rambles to\ninclude, then press the Semantic Merge button (shown in Figure 2). In (b), the user taps the scissors icon in a Ramble to ask\nLLM to split it based on content.\nRambler and its main features supporting macro revision and text\nreviewing, respectively.\n3.2.1\nSupporting text reviewing with clean transcript and gist extrac-\ntion. Rambler is designed to help users review dictated text more\nefficiently with three key features (transcript cleaning, semantic\nzooming, and highlighting keywords) that leverage NLP and LLMs.\nFirst, Rambler uses an LLM to automatically clean Ramble tran-\nscripts when they are added or edited, in order to minimize small\nspeech-to-text (STT) typos (like messy punctuation or misheard\nwords) that impact reading. To avoid distraction, our interface only\nshows raw speech for real-time feedback when the user is actively\ndictating; if the user stops dictating, the interface immediately\ncleans the spoken text, and only displays the clean transcript.\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nSecond, we implement Semantic Zoom using a slider that con-\ntrols the degree of summarization shown across all the Rambles in\na composition. As shown in Figures 2 (component 6) and 3, users\ncan move the slider at the bottom of the UI to view text summaries\nat different levels, specifically, at 50%, 25%, and 10% of the original\nlength.3 We designed this feature to help visualize the core ideas of\nlengthy text and to allow users to quickly gain a sense of how their\nideas should be re-organized and iterated upon.\nLast, to aid visual skimming across Rambles, Rambler automat-\nically highlights keywords after a Ramble is added or edited (see\nFigure 5-top). Users can also tap any word in a Ramble to manually\ndeselect or select the word as a keyword. Keywords are not only\na visual aid for continuity across zoom levels; users can also tap\nthe regenerate button on a Ramble to re-summarize their transcript\ntext based on their modified keyword selection. This helps address\nan issue we discovered where summaries generated by LLMs may\nnot be aligned with the users\u2019 main ideas in a Ramble.\n3.2.2\nSupporting macro revision with LLM assistance. We distin-\nguish between conceptual-level edits, which we call macro-revisions,\nfrom fine-grained edits, which we call micro-revisions. Macro-\nrevisions operate on ideas, and in our prototypes happen at a Ram-\nble level \u2013 consider functionalities like reorganizing Rambles, and\nmerging or splitting Rambles. Micro-revisions, in contrast, operate\non a much smaller scale and, in our prototypes, happen primarily\nthrough keyboard editing \u2013 consider fixing a single typo or some\nword choice in a sentence. We consider macro-revisions and micro-\nrevisions complementary, thus our interface also provides keyboard\nediting, activated by clicking the edit button in a Ramble.\nWe designed five macro-revision operations: Ramble respeaking,\nRamble re-ordering through drag and drop, Ramble merging (con-\ncatenation of text) by dragging one Ramble onto another, Ramble\nsplitting (by pressing return during keyboard editing), and Ramble\ntransformation. Although macro-revisions can also be done in a\nnormal text editor, we provide an interface with a stronger affor-\ndance for doing so, with the goal of encouraging and supporting\nusers to apply these operations on spoken text. We carefully de-\nsigned a re-speaking mechanism and interface (Figure 2, zone 4),\nwhich is activated when the user re-activates the mic of an existing\nRamble. The user talks further and then chooses whether to append\nthe new text to the Ramble, replace the old text in the Ramble with\nit, or discard the new text (Figure 2, zone 4). During the respeaking\nprocess, the original text remains above in grey color as a reminder.\nWe do so to aid the user\u2019s memory, as in our design process we\nlearned that without the original text, it is hard to remember what\nto say.\nWhile all of the macro revisions can be performed manually,\nwe provide an LLM-assisted version for three of the operations\n\u2013 Ramble merge, split, and transformation \u2013 and named them Se-\nmantic Merge, Semantic Split, and Magic Custom Prompt. Semantic\nmerging utilizes the LLM to intelligently merge the content of mul-\ntiple selected Rambles into the content of one Ramble by clicking a\nSemantic Merge button (see Figure 2.6). Semantic Split, represented\nby a scissors button, is placed on individual Rambles, which prompts\nthe LLM to divide one particular Ramble into N Rambles based on\n3Rambler\u2019s prompts for these summary levels were tuned to consistently achieve\nthese levels, but are not explicit about these percentages; see appendix A.2.4.\nFigure 5: Example transforming a ramble with Magic Custom\nPrompt: (top) keywords highlighted in light green; (bottom)\ncustom prompt window triggered by the magic wand button\non a Ramble. The user inputs an example prompt, and option-\nally ticks the checkbox for including keywords as context.\nits content. These two mechanisms are visualized in Figure 4 with\nexample text. The Magic Custom Prompt feature is triggered by\na magic wand button on a Ramble. It opens up the possibility for\nexpert LLM users to define any custom transformation on Ramble\ncontent by directly inputting an LLM prompt (see Figure 5).\n3.3\nExample User Workflow\nTo better illustrate the functionality of Rambler, consider the fol-\nlowing example of a simple user workflow: Alex, an artist, is drafting\na post for his personal blog and wants to describe how he finds\ncreative inspiration. He has many ideas on the topic, but isn\u2019t sure\nthe best way to write it: hence, he reasons that he should simply\ntalk it out. Opening up Rambler, Alex presses the + Ramble button\nto begin recording. After finishing his thoughts and stopping the\nrecording, Rambler automatically cleans his dictated text into the\nfirst Ramble. Alex senses that he wandered somewhat from his\ninitial topic; since the resulting text is fairly long, he slides the\nSemantic Zoom slider to see a summarized version of his text. Then\nhe takes a look at the highlighted keywords in all the zoom levels\nto get a better sense of what he talked about. After glancing back\nat the transcript, Alex decides to use Semantic Split to revise the\nvarious ideas in his Ramble separately.\nRambler segments his cleaned spoken text into multiple rambles\nalong three main ideas, 1) \u201cfinding personal inspiration,\u201d 2) \u201cinter-\naction between self expression and audience interpretation,\u201d and 3)\n\u201cdifference between art and design.\u201d With a clearer understanding of\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nFigure 6: Rambler System Architecture Diagram consisting\nof a web-based frontend, a cloud server mediating between\nuser requests and the OpenAI API for LLM functionality. The\nAssemblyAI API is used for real-time speech transcription.\nthe ideas he\u2019s just verbalized, Alex begins to alternate between com-\nposing new Rambles, and revising with the provided functionalities\n(e.g., respeaking, reordering, merging, and/or splitting Rambles)\n\u2013 until he feels like each Ramble roughly resembles a somewhat\nconcrete idea. Once he feels that he has added all his thoughts and\ndeveloped them into a cohesive flow of ideas, he uses the built-in\nonscreen keyboard to fix word choices here and there to his liking,\nand then exports his composition to share with the world.\n3.4\nPrototype Implementation\nRambler is implemented as a React.js web application with a node.js\nbackend and is hosted in the cloud. LLM-based text manipulation\nfunctionality is enabled using OpenAI\u2019s GPT-4 API, which provides\nhigh quality output for our desired LLM functionality. We consid-\nered other models such as GPT-3.5 Turbo for response time reasons;\ntheir output on LLM interactions (most notably, Semantic Split)\nwas less precise. Rambler\u2019s voice transcription uses AssemblyAI\u2019s\nreal time Speech-to-Text API. We made this engineering choice\nbecause AssemblyAI\u2019s API allowed us to transcribe text indefinitely\nin a single instance. Automatic keyword selection is implemented\nvia the rapid automatic keyword extraction (RAKE) method [25].\nCompared to an LLM based approach for keyword selection akin\nthe method used in Graphologue [12], the RAKE method was or-\nders of magnitude faster (\u223c4s vs nearly instant). Our overall system\narchitecture is illustrated with Figure 6. Our project source code is\navailable at https://github.com/BerkeleyHCI/rambler.\nWe used GPT-4 to power the four key LLM-based gist-related\nfeatures: text summarization for Semantic Zoom (including tran-\nscription cleaning), merging multiple Rambles, splitting a single\nRamble into multiple, and open-ended editing using customized\nLLM prompts. We created pre-defined prompts for zoom, merging,\nand splitting; the open-ended editing prompts are entirely user\ndefined. One-shot prompting was employed for the transcription\ncleaning prompt and zero-shot was used for the rest. Keywords are\nappended to prompts when marked for inclusion through selection.\nAll prompts are attached in Appendix A.2 for reference.\n4\nEVALUATION\nTo evaluate the effectiveness of Rambler for supporting writing\nwith speech, we designed a within-subjects study to compare Ram-\nbler with a state-of-the-art solution for dictation and word process-\ning. We chose, as our baseline, raw dictation into a text editor with\nChatGPT provided on the side. Participants completed two long-form\nwriting tasks through dictation, one using Rambler\u2019s full suite of\navailable functionality, and the other using the baseline. Through\nthis comparison, we seek answers to the following questions:\n(1) Is writing with speech better supported by Rambler than\nthe baseline? If so, how?\n(2) How effectively does our approach of embedding LLMs in a\nGUI support the review and revision of spoken text?\n4.1\nBaseline Interface\nAs mentioned above, the baseline uses a plain text editor interface\nplus dictation. Raw transcripts populate the text content area as\na user speaks, after the microphone is activated. The transcript\ncan be edited using iOS\u2019s built-in keyboard and cursor interface.\nTo avoid potential confounding effects resulting from differing\nspeech recognition engines, we implemented our baseline interface\nusing the same speech recognition engine used in our prototype\n(Assembly.ai), as a web interface. For a fair comparison between\nthe baseline and Rambler, in our baseline condition we provided\nChatGPT through a separate tab in the same web browser\u2014thus\nensuring that our results are primarily driven by the affordances\nof Rambler rather than the availability of an LLM. However, we\ndid limit the use of ChatGPT to revising content produced by par-\nticipants, by discouraging participants from asking ChatGPT to\ngenerate lengthy compositions.\n4.2\nParticipants\nTwelve participants (\ud835\udc41 = 12), seven who self-reported as female\nand five as male, were recruited from our university campus com-\nmunity through various mailing lists and student organizations.\nAll participants were native-fluent English speakers, and there was\nvaried experience with dictation and LLM use. P3 and P10 were\naged between 25 and 34; all other participants were aged between\n18 and 24. No participant had motor impairments that impacted\nkeyboard or dictation usage. Participant details are presented in\nTable 1.\n4.3\nExperiment Task\nParticipants were asked to compose and revise a blog post using the\ngiven interface in each condition. We chose this task to encourage\nparticipants to compose long-form personal writing and revise it\nextensively for an audience, in order to address a potential lack of\nmotivation, in a lab study setting, to improve text the participants\nmay not have any reason to feel invested in. Thus participants were\nasked to choose a topic they were personally interested in, from\na provided list of open-ended writing tasks (e.g., \u201cThink of two\ndream jobs of yours.\u201d and \u201cThink of two academic concepts you\nfind interesting.\u201d). For each of the two conditions, participants were\nasked to create a \u201cblog post that would be posted on your social\nmedia\u201d about one chosen answer to the topic. Note that we had\nparticipants use the same topic across conditions (e.g., two dream\njobs, not one dream job and one academic concept), to mitigate the\npotential impact of topic selection on participants\u2019 compositions.\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nTable 1: Demography and dictation + LLM usage of study\nparticipants.\nID\nDegree\nDomain\nDictation Usage\nLLM Usage\nP1\nPh.D.\nComp. Sci.\nYearly\nWeekly\nP2\nUndergrad\nComp. Sci.\nNever\nEveryday\nP3\nPh.D.\nComp. Sci.\nNever\nNever\nP4\nUndergrad\nComp. Sci.\nMonthly\nWeekly\nP5\nUndergrad\nBusiness\nWeekly\nMonthly\nP6\nUndergrad\nCog. Sci.\nNever\nWeekly\nP7\nPh.D.\nElec. Eng.\nNever\nYearly\nP8\nUndergrad\nData. Sci.\nNever\nMonthly\nP9\nUndergrad\nEnglish\nMonthly\nMonthly\nP10\nPh.D.\nPhysics\nYearly\nMonthly\nP11\nUndergrad\nEnglish\nYearly\nNever\nP12\nUndergrad\nComp. Sci.\nNever\nNever\n4.4\nProcedure\nA lab study session for each participant lasted about 90 minutes,\ndivided into three parts: the pre-study survey, the main study itself,\nand the post-study interview. The study session was conducted\nin a quiet room, in person, with an experimenter. Participants ac-\ncessed Rambler via Safari on a researcher-provided 11-inch iPad\nPro running iOS 16. There was no hardware keyboard, however,\nparticipants were allowed to use the on-screen keyboard as needed.\nPre-study survey. Participants were first given a brief, initial\ndemographic survey including questions about their familiarity\nwith writing, dictation tools, voice-interface tools, and LLMs.\nMain study. Participants were not told we made the interfaces.\nIn a counterbalanced order across participants, each participant\nused both interfaces in two conditions. For each condition, the par-\nticipants engaged in 1) a tutorial for training, 2) the task, and 3) a\npost-task survey. In the training trial, the interviewer spent about 5\nto 15 minutes explaining all the functionality of the given condition\nand provided time for the participant to try and familiarize them-\nselves with each feature. Next, to perform the task, the participants\nwere given up to 20 minutes to improve their composition until\nthe time is up, or until they felt satisfied, whichever came first. If\nthe participants hit a word count of 800 words or more, or had not\ndone any editing by the 10-minute mark, the experimenter would\nencourage the participant to begin editing (to ensure enough time\nfor revisions). In the post-task survey, the participants were asked\nto assess the usefulness of each feature in the given condition using\na five-point Likert scale. They were also asked to estimate what\npercentage of their time was spent in micro-revision (specifically\nkeyboard-editing), compared with macro-revision. Finally, partici-\npants were asked to assess the quality of their text composition on\na five-point Likert scale.\nPost-study interview. Following the two conditions, participants\nwere interviewed regarding their overall experience, for around 25\nminutes. Specifically, they were asked about their workflows for\neach condition and their experiences with the features in both con-\nditions. Finally, participants were asked for their overall preference\nbetween the baseline and the Rambler condition. Each participant\nwas compensated $30 USD for their participation.\n4.5\nData Collection and Analysis\nWe captured three types of data for analysis: a screen recording\nof the iPad running the Rambler application; audio recording of\nparticipant and interviewer speech during each study session; and\napplication logs of user interactions with the interfaces. We per-\nformed descriptive quantitative analysis of the ratings and logs,\nand summarized our interview findings in qualitative analysis.\nQualitatively, we engaged in exploratory data analysis, transcrib-\ning all audio recordings, and coding the screen-recording videos to\nobserve participants\u2019 workflows and strategies. Two of the authors\nthen compared these approaches across participants and catego-\nrized their usage through a thematic analysis, relying on a modified\nform of affinity diagrams [21] and service blueprints [4] to docu-\nment the specific workflows and strategies each participant engaged\nin and the experiences they communicated with the interviewer.\nAdditionally, we evaluated participant output text for fluency\nand grammaticality using the GRUEN automated benchmark [39],\nand compared our participants\u2019 final text with similar \u201cblog post\u201d\ntext sampled from a popular corpus of weblog posts [27] to match\nparticipant demographics on age and gender.\n5\nRESULTS\nThis section synthesizes the findings from our quantitative and\nqualitative analysis. We analyzed the overall workflow based on\nscreen recordings and identified a broad diversity of ways Ram-\nbler was used. The questionnaires and interviews compared the\neffectiveness of Rambler in supporting writing with speech, and\ncollected feedback on user acceptance. The usage of each feature\nin Rambler was reported in reference to system logs, ratings, and\ninterviews.\n5.1\nText Output Quality\nAcross both subjective and objective assessments, text output qual-\nity did not differ significantly between Rambler and the baseline.\nSubjective assessment. We asked participants to rate the quality\nof the text they submitted as the final outcome for each task. As\nshown in Figure 7, more than half of the participants rated the\noutcome quality the same across both conditions. Two participants\nrated higher for the baseline and three rated higher for Rambler.\nAn independent t-test of the Likert ratings revealed no significant\ndifference in perceived text quality across conditions, consistent\nwith the overview we get from the boxplot in Figure 7.\nObjective assessment. We also performed an objective text quality\nassessment using an established computational method, GRUEN [39],\nwhich calculates a score of fluency based on grammaticality, non-\nredundancy, and focus. We compared the GRUEN scores of the\nparticipants\u2019 text output using a paired t-test across the two con-\nditions, and found no statistical difference (\ud835\udc5d = 0.46, \ud835\udc61(11) = 0.77)\nbetween Rambler (\ud835\udc40 = 0.75, \ud835\udc46\ud835\udc37 = 0.16) and the baseline (\ud835\udc40 = 0.70,\n\ud835\udc46\ud835\udc37 = 0.26). This is consistent with the subjective assessment, fur-\nther validating that the text output quality of Rambler is compara-\nble with the baseline.\nAdditionally, we used GRUEN scores to further assess the text\noutput quality of writing with speech versus traditional writing.\nWe compared our participants\u2019 final text output quality across\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nFigure 7: Self-assessed text quality Likert rating by condition\n(technique)\u2014Baseline and Rambler. Left: individual ratings\nper participant; Right: aggregated box plot per condition.\nboth conditions with an author-demographic-matched sample of\n74 Internet blog post texts [27]. Here, we found that our partic-\nipants\u2019 GRUEN scores were significantly higher than the blog\nposts. Two paired t-tests were performed separately comparing\nthe GRUEN scores of Rambler and the blog posts (\ud835\udc5d < 0.0001,\n\ud835\udc61(84) = 4.25), and comparing the baseline and the blog posts\n(\ud835\udc5d = 0.0008, \ud835\udc61(84) = 3.48). The GRUEN scores were 83% higher us-\ning Rambler (\ud835\udc40 = 0.75, \ud835\udc46\ud835\udc37 = 0.16), and 71% higher using our base-\nline (\ud835\udc40 = 0.70, \ud835\udc46\ud835\udc37 = 0.26), than in the blog posts sample (\ud835\udc40 = 0.41,\n\ud835\udc46\ud835\udc37 = 0.26). This shows, in terms of fluency, non-redundancy and\nfocus, that LLM-assisted writing with speech in both methods can\noutperform traditional writing.\n5.2\nAnalysis of Workflows and Strategies\nParticipants exhibited a large diversity of approaches in using the\nvarious features offered in Rambler; no single consistent work-\nflow emerged as the most common. This section summarizes and\nhighlights our findings about participants\u2019 workflows and strate-\ngies, which came from coding the screen recording videos with\nthe themes of editing goals. We excluded P6 from the workflow\nanalysis due to a technical issue that prevented us from capturing\ntheir screen recording. Appendix A.1 illustrates detailed operations\nperformed by individual participants for each task based on sys-\ntem logs and describes each participant\u2019s workflow based on our\nobservation.\n5.2.1\nOverall Composition and Revision Workflow. There are two\nknown writing styles identified in the writing literature, struc-\ntured and freeform, distinguishing whether authors have a narra-\ntive planned ahead of time [3] (structured). In writing with speech,\nwe observed two potentially related styles: content-first\u2014dictating\nmost content first\u2014and intertwined\u2014revising as they compose. This\nwas also observed in previous work about spoken composition [18].\nAs we can see in Figure 10, three participants (P8, P10, and P11)\nfollowed a content-first approach by dictating most of their text\nfirst, before editing through macro- and micro- revisions. Eight\nparticipants (P1, P2, P3, P4, P5, P7, P9, and P12) adopted an inter-\ntwined approach and switched between dictation and revision more\nfrequently.\n5.2.2\nStrategies for Individual Editing Goals. Our analysis also\nsheds some light on how participants achieved individual compo-\nsition and editing goals, including adding, reviewing, reorganizing,\nrephrasing, and removing content.\nAdding Content. All 11 participants used dictation to compose\ntext, through creating a new Ramble, as their main way of adding\nnew content. Seven participants (P1, P3, P4, P5, P7, P8, 12) also\nutilized Respeaking to add content to an existing Ramble. P1 used\nManual Split to separate a specific sentence out of a Ramble so that\nthey could add onto it.\nReviewing content. Eight participants used either Semantic Zoom,\nHighlighting Keywords, or Regenerating Summaries to explicitly\nhelp review their composition. Four participants (P4, P8, P10, P12)\nused Semantic Zoom to help review during their process, while\nthree (P2, P7, P11) used Semantic Zoom to only review at the very\nend of their composition task. One participant (P12) also used High-\nlighting Keywords by itself as a form of review, while P2 and P10\nused Highlighting Keywords combined with Regenerating Sum-\nmaries to review.\nReorganizing Content. As expected, participants made use of the\nManual (onscreen keyboard-based) and Semantic (LLM-based) Split\nand Merge functions to segment content. But we also observed\ninteresting mix-and-match strategies combining manual and se-\nmantic operations. Three participants (P1, P4, P12) used Semantic\nSplit first, before using Manual Merge on some of the resulting\nRambles to achieve a partially Semantic, partially Manual Split.\nP3 used Semantic Merge and then Manual Split to control some\nof the uncertainty of what they would get from Semantic Merge.\nP1 also paired Manual Split and Manual Merge to more precisely\nresegment their Ramble (by moving the last sentence of the first\nRamble into the second Ramble). More uniquely, P10 got rid of\nall their segmentation through a combination of Manual Merge\nand Semantic Merge, and P4 experimented on a Ramble through\nManual Merge, then Semantic Split, and then used Semantic Merge\non the resulting pieces.\nFurthermore, three participants (P1, P3, P4) utilized Magic Cus-\ntom Prompt to attempt to split their composition: P3 (\u201cmake into\nbulleted list\u201d) and P4 (\u201cReformat like a design proposal\u201d) did so\nat the beginning of the task, while P1 (\u201cCan you... organize it\ninto paragraphs\u201d) did so at the end of their task. As Magic Custom\nPrompt is designed to only modify a single Ramble and Rambles do\nnot store paragraph breaks, these prompts would just create a block\nof text within a Ramble, and participants would go on to either\nreplace through Respeak Ramble (in the case of P3) or segment it\nthrough Semantic Split.\nSeven out of 11 participants (P1, P2, P3, P4, P9, P12) rearranged\ntheir Rambles through drag and drop Reordering, with P1 and P4\nalso altering the order of text by using Manual Merge on non-\nconsecutive Rambles. Two participants (P1, P3) also more specif-\nically rearranged content within their Ramble: P1 did so through\nManual Split and then either reordered or did a non-consecutive\nManual Merge, while P3 asked Magic Custom Prompt (on a single\nRamble) to \u201creflow this text to make sense\u201d.\nRephrasing Content. All participants used keyboard editing to\nmake minor changes like fixing typos or rewording phrases. Ten out\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\n(a)\n(b)\nFigure 8: Stacked bar graph of feature usage counts associated with (a) text reviewing and (b) macro-revision. In (a), one usage\ncount for Semantic Zoom represents a single transition from a Semantic Zoom level to an adjacent level (e.g., 50% summary to\n25% summary). All other usage counts in (a) and (b) represent a single tap, drag, etc. on their respective interfaces.\nof 11 participants utilized Magic Custom Prompt to do a more broad\nrephrasing. Three participants (P2 P8, P12) only sought specific\nstyle changes like \u201cmake it more formal\u201d or \u201clike a tumblr post\u201d. Five\nparticipants (P1, P5, P7, P9, P11) looked for a more general revision\nof the text through less specific requests like \u201crevise text,\u201d \u201cclean the\ntext,\u201d or \u201cmake this writing more readable\u201d, while two participants\n(P3, P4) did both. Furthermore, five participants tried to achieve a\nmore uniform style across their entire composition: three partic-\nipants (P2, P3, P8) used similar prompts across all their Rambles,\nwhile two participants (P1, P5) used Manual Merge to combine all\ntheir Rambles into a single Ramble so that they could apply Magic\nCustom Prompt onto their entire text in one go. Two participants\n(P3, P12) also used Respeaking to replace entire Rambles.\nRemoving Content. Ten out of 11 participants deleted content\nfrom their composition. Seven participants (P1, P2, P3, P4, P5, P8,\nP12) utilized the Delete Ramble functionality, and three of those\nparticipants (P3, P5, P8) specifically combined Manual Split with\nDelete Ramble to delete words and sentences from the beginning or\nend of a Ramble. Three participants (P1, P2, P11) utilized keyboard\nediting to delete whole sentences from their Rambles.\n5.3\nUsage of Rambler Functions\nHere, we analyze participants\u2019 feature usages from both the system\nlogs and interview data. Overall, the participants in the study found\nRambler to be a useful tool for writing. As detailed in Section 5.2.1,\nparticipants invented diverse and creative strategies when using\nRambler. These diverse strategies led to equally diverse usage of\nfunctionalities. Figure 8 visualizes the usage count of each function\nlogged in our system, per participant. As we can see from Figure\n8a, Semantic Zoom and Highlighting Keywords were the most\nfrequently used features. Overall, the use of features varied largely\nacross participants. Figure 9 shows the subjective ratings of each\nfunction in the post-task survey. The following analysis assesses the\nuse of each feature by referring to these ratings and summarizing\nrelevant insights revealed from the interview.\n5.3.1\nThe Ramble Structure. The Ramble structure is the most\nwell-received feature. Participants made good use of the Ramble\nstructure to organize and discretize their writing. While they had\nvarying opinions on how it impacted their writing process, all 12\nparticipants found it useful for its organizational benefits. Some\nparticipants found the structure more efficient when they had ideas\nprepared beforehand, while others preferred it for freely brain-\nstorming. P3 and P5 also appreciated that the sections of Rambles\nmimicked their usual writing style of outlining first, which helped\nthem get used to the structure. Furthermore, eight participants men-\ntioned that Rambles helped them to focus and iterate on specific\nsections of their work. P12 noted that the structure allowed them to\nseparate their ideas and feel more in control of their organization,\nwhile P9 found that it enabled them to work in a more methodical\nmanner. Overall, participants found that Rambles helped them to\nbetter discretize their composition, which made it easier to visualize\nand revise.\n5.3.2\nManual Operation Usage for Macro Revision. Participants\nfound the manual operations helpful, while individual feature usage\ndiffers based on their workflow.\nRespeaking. Seven participants used Respeaking to add new text\nto a Ramble. Two of those participants used Respeaking to replace\nentire Rambles. Half of the participants found this feature to be a\nnice addition, and more than half of the participants rated Respeak-\ning to be useful. While some mentioned that they could simulate it\nby deleting and restarting the Ramble, one participant noted that\nRespeaking is quicker and more convenient.\nReordering Rambles. Participants\u2019 usage of Reordering Rambles\ndepended on their individual workflows. Five participants did not\nuse the Reordering feature.However, most participants felt Reorder-\ning was a useful feature, which is supported in the Likert ratings.\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nInterestingly, a couple of participants felt more comfortable ram-\nbling knowing that they could easily adjust their ideas afterwards.\nAs P4 put it, \u201cI was able to just kind of get my ideas on the screen\nwithout necessarily like a linear progression, because I knew I could...\nmove things around [after]... the Rambles... [help let] me... freely\nbrainstorm, but then allowing me to kind of put it all together in a\nway that made sense.\u201d While several participants pointed out that\nthis can also be done in a normal editor by copy and paste, P12\nnoticed in a \u201cmobile device [context]...it was a lot easier... copying\nand pasting text [in the baseline]... was a little bit more clunky\u201d.\nManual Merge and Split. Manual Merge (via drag-and-drop) and\nManual Split (via an onscreen keyboard\u2019s return key) were used\nby participants depending on their workflows. While many par-\nticipants found Manual Split and Merge to be helpful and useful\nas further indicated in the Likert ratings, these two features were\nnot considered essential by all. Some participants used them to\ncombine Rambles, take breaks in between dictation, or section out\nparts of the text for Respeaking. P3 utilized Manual Split as a way\nfor them to regain control over and \u201crein in\u201d their composition\nafter using Semantic Merge. Some participants also found tension\nbetween Manual Split and Merge and Semantic Split and Merge, as\nthey preferred one or the other depending on their workflow.\n5.3.3\nSemantic Operation Usage for Macro Revision. Rambler pro-\nvides a number of LLM-assisted operations, some of which are more\nwell-received than others. Cleaning transcripts is a valuable feature\nthat the majority of participants found helpful. Semantic Zoom is a\nversatile feature that can be used for a variety of purposes, but it is\nnot as widely used as part of people\u2019s strategies. Semantic Merge\nand Manual Split are less commonly used, and some participants\nstopped using them after a few attempts.\nCleaning Transcripts. Most participants found the live cleaning\nof the raw transcript to be helpful and useful. P4 and P12 specifi-\ncally mentioned how much they appreciated the feature after using\nthe baseline (where they didn\u2019t have the automatically cleaned\ntranscript). Participants P11 and P12 were also pleased with how\nthe clean transcript preserved their voice better than what they\ncould get using ChatGPT in the baseline. The accuracy of the clean\ntranscript also made it easier for participants to immediately and\nactively review, understand, and iterate on their composition. As\nP11 expressed, the cleaned transcript was so high quality that they\nwere willing to do keyboard edits to it (whereas in baseline they\njust handed it off to ChatGPT).\nSemantic Zoom. The usage of Semantic Zoom varied depending\non each participant\u2019s strategy. Most participants used or explored\nusing this feature according to Fig. 8a. It was used for a variety\nof purposes, such as gauging whether they could query the LLM\nfor more succinctness (P4), review what they talked about (P7, P8,\nP11), checking if they missed any main ideas (P7), using generated\nsummaries as part of the final text (P8), or simply out of curiosity\n(P10). The relatively high count of usage in Fig. 8a is partially due\nto the way logs were generated \u2013 if a participant scrolls through the\nslider it gets registered at each level change. Semantic Zoom was ob-\nserved to be less used when participants did more keyboard editing,\nperhaps because participants can gain a good understanding of the\ntext while micro-editing and thus need less assistance for reviewing.\nFigure 9: Usefulness Likert Rating per Feature, collected with\nthe statement \u201cI found <feature_name> useful.\u201d The total\nnumber of ratings per feature varies because some partici-\npants indicated that they did not use a particular feature.\nP3 found Semantic Zoom less helpful because their composition\nwas short enough to not need summaries and felt that the shortest\nlevel was too summarized to be useful. On the other hand, P7 was\nvery impressed with it, and P8, who used Semantic Zoom as a tool\nfor editing instead of reviewing, expressed that it was the most\nuseful tool of them all. P8 used the 50% level as the final submission\nbecause it felt like a \u201cmore clean or more postable version\u201d of their\nwork, and mentioned how they \u201cliked having more information to\ngo off when I\u2019m reviewing my work.\u201d They also appreciated how\neasy it was to regenerate the summaries by customizing keywords.\nSemantic Merge and Split. The usage of Semantic (LLM-assisted)\nMerge and Split varied depending on participants\u2019 workflows. Seven\nout of 12 participants used Semantic Merge and three used Semantic\nSplit. Participants used Semantic Split to partition long Rambles\n(P4), and participants used Semantic Merge to achieve cohesive\ncombinations (P2, P4) or out of curiosity (P3, P10). P3 mentioned it\nis good for low-stakes where word choice precision doesn\u2019t matter\nas much. When it comes to the reasons why Semantic Split was\nless used, we observed that participants used Manual Split more\noften than Semantic Split, because it warrants more certainty. As\nP3 said, \u201cI sort of use the Manual Splitting as a way to like rein in\nthe Semantic Merge, which I really liked.\u201d Some (P7, P9) felt the\ntask wasn\u2019t long enough for Semantic Split to be useful. P10 was\nworried about unexpected results, and thus was reluctant to use it.\nWe also learned that some participants stopped using these features\nafter a few attempts due to receiving unexpected results (P1).\nMagic Custom Prompt. Most participants made extensive use of\nMagic Custom Prompt to revise within Rambles: changing tone,\nreformatting structure, further cleaning up their text composition,\nand even reducing the size of a Ramble. One participant spoke with\nthe intention of constructing an outline, then used Magic Custom\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nPrompt to expand their outline into a \u201cdesign document\u201d (P4), which\nthey then split into multiple Rambles for further revision. Four\nparticipants had positive outlooks on the Magic Custom Prompt,\nthough P2 felt that they were relying on it too much. P11 and\nP12 also liked the Magic Custom Prompt, but primarily because it\npreserved their writing voice more than ChatGPT in the baseline did\nwhen they used keywords to influence the Magic Custom Prompt\noutcome. In contrast, P8 did not like this feature, as they felt that it\nwas not helpful in getting the style or language that they wanted.\nHighlighting Keywords and Regenerating Summaries. Highlight-\ning Keywords (and Regenerating Summaries) was used in different\nways by participants, but those who adopted it agreed or strongly\nagreed it was useful. Participants used keywords in a variety of\nways: to understand their readers (P7), improve their focus and\nunderstanding of their own writing (P2, P12), highlight keywords\nto regenerate summaries (P8, P10), and influence their Magic Cus-\ntom Prompt (P9, P11). P4 found that highlighting keywords could\nbe extra work, as they could change during editing. P11, on the\nother hand, found that providing keywords as context to the system\nhelped them retain their writing voice.\n5.4\nEffectiveness of Rambler Compared to\nBaseline\nThe questionnaire and interview reveal that Rambler is an effective\ntool for supporting text composition and revision via speech, espe-\ncially for tasks that require organizing one\u2019s thoughts, iterating on\ncontent, and maintaining one\u2019s writing voice. We explicitly asked\nparticipants about Rambler with respect to reviewing composi-\ntion, organizing thoughts, supporting iteration, and granting user\ncontrol.\n5.4.1\nSupport for Reviewing the Composition. Eight out of 12 par-\nticipants found Rambler to be more helpful for reviewing text\ncomposition than the baseline system. Half of these participants\nattributed this to the Ramble structure, which made it easier to\ndiscretize, visualize, and review their text composition. The other\nhalf found Semantic Zoom to be useful for grasping the main idea\nof their Rambles and checking that they included or emphasized all\nthe points they wanted to. Additionally, the automatically gener-\nated clean transcript made it easier for participants to review their\ntext.\n5.4.2\nSupport for Organizing Thoughts. Eleven out of 12 partici-\npants felt that Rambler was better for organizing their thought\nprocess. Most participants attributed this to the Ramble structure\nand its inherent support for reorganization. For example, P6 said\nthat the Ramble structure made it \u201ca lot easier... to keep track of\nwhat I was doing.\u201d P1 explicitly said that Rambler was \u201cclearly\nbetter for organizing... I was able to move the ideas around. And\nthat was very helpful.\u201d P4 also found it \u201csuper useful for dragging\nmy thoughts around and kind of restructuring.\u201d P3 was the only\nparticipant who felt that the baseline was better for organizing their\nthought process. They rationalized this by the free-form nature of\nthe baseline\u2019s text editor. Their preferred workflow depends on a\nnested structure which Rambler doesn\u2019t support.\n5.4.3\nSupport for Iteration. Seven out of 12 participants felt that\nthey iterated more in Rambler. This was due to the Ramble struc-\nture, the support for macro-revisions, and the automatically gen-\nerated clean transcript. The Ramble structure made it easier to\ndiscretize text composition and allowed for more flexibility, which\nmade iteration faster. P3 said that Rambler was \"a lot more ...invit-\ning of ... merging and smashing things together. So I could definitely\niterate a lot more.\" The clean transcript made it easier to review\non the spot and enabled iteration on the go. P10 and P11 explicitly\nmentioned that they iterated more in Rambler because they did\nnot feel motivated to iterate at all in the baseline.\n5.4.4\nUser Control. Ten out of 12 participants felt that Rambler\ngave users more control over the content iteration. While some\nfunctionality like Manual Merge, Manual Split, and Reorganization\nwere pointed out as mimicable in the baseline, five participants\nemphasized how the support within Rambler made it faster to\nperform macro-revisions. Knowing that there was support for it-\neration made participants feel more in control. Furthermore, the\nstructure of the Rambles helped localize changes. Four participants\nmentioned that this made it easier to know what part of the text\nthe intelligent text processing was going to be operating on. P8 said\nthat \"when you figure out what [Rambler] does, it is much easier\nto really customize your composition... If you want more detail,\nRambler is worth learning.\" Some users also noted how they felt\nmore in control of their writing voice. P11 and P12 were pleased at\nhow the clean transcript more accurately preserved their voice than\nChatGPT could. P11 explicitly mentioned how keywords helped\npreserve their voice when they utilized the LLM through Magic\nCustom Prompt. Among many other participants, they expressed\nthat Rambler made it possible for them to \u201csee [the composition]\nbetter and edit it better\u201d.\n5.5\nUser Acceptance and Use Cases\n5.5.1\nOn using speech to write. Dictation is a powerful tool for\ncapturing thoughts for writing, but it can take time to learn how\nto use it effectively. Our post-study interviews revealed mixed\nopinions on the experience of writing with speech, indicating a\nlearning curve to overcome. Three participants who used dictation\na few times a month or year (P9, P10, and P11) found it to be \u201ca really\ngood way\u201d to get started and \u201ccapture [your] loose thoughts before\nthey run away\u201d (P9). P4 (a few times a month) felt both tensions:\nit \u201ckind of just let your ideas flow\u201d to use dictation, but there\u2019s\nalso \u201ca little build up in my brain of... what should I say first.\u201d One\nparticipant who reportedly used dictation weekly (P5) described\ntheir process as \u201cI speak before I think\u201d but \u201cthat\u2019s not exactly a\ngood thing\u201d because their \u201cwords are definitely not clean\u201d\u2014 and\nthat Rambler encouraged them to \u201cramble a lot more\u201d. Finally, some\nparticipants with no experience (three who had never used it before:\nP2, P3, and P7) felt that speaking made them think about what to say\nfirst and dictation slowed them down. P6 also mentioned that they\ndid not like having to manually clean up STT transcripts. Overall\nwe did not find a consistent trend between dictation experiences\nand their opinion about it being positive or negative, perhaps due\nto our limited sample size.\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\n5.5.2\nOn using Rambler for their own writing. Despite the learning\ncurve of using speech for writing, 10 out of 12 participants pre-\nferred Rambler over the baseline and envisioned themselves using\nit in the future. The participant feedback suggests that Rambler\nis a versatile tool well-suited for a variety of tasks. First, we have\ntext editing tasks: four participants mentioned situations where it\nwould be useful to edit or break down text using Rambler. One\nparticipant said that Semantic Zoom would be useful for digest-\ning content more quickly. Another participant said, \u201cWhat makes\nRambler like better for certain use cases is being able to have like\nthis higher level control of ideas. So I really enjoyed using those\nmacro tools... I feel like this process of just like moving that you\nfeel like you can actually like move an idea... in an abstract sense. I\nreally enjoyed that part of it\u201d (P4). Then, we have spoken-language\ntasks: Five participants mentioned using Rambler for transcribing\nconversations, interviews, or lectures, or composing speeches, pre-\nsentations, or scripts. Five participants mentioned using Rambler\nfor brainstorming, describing it as a way to \u201ccapture the essence of\nthings\u201d. Three participants mentioned using Rambler for blogging\nor journaling (e.g., low-stake/shorter tasks).\n6\nDISCUSSION\n6.1\nSummary of Findings\nWe found that writing with speech can be better supported by\nRambler compared to the state-of-the-art practice, which is an\nSTT text editor with the help of ChatGPT. Rambler is shown to be\nan effective tool for supporting text composition and revision via\nspeech. Specifically, it showed advantages in helping users review\nspoken compositions and organize their thoughts, encouraging and\nsupporting iteration while enhancing user control. Furthermore,\nparticipants used Rambler in highly creative and diverse ways,\nsuggesting that Rambler is a versatile tool that can be adapted to\ndifferent writing styles and workflows. The most frequently used\nfeatures were Highlighting Keywords and Semantic Zoom. Macro\nrevision features were mostly well received. Some LLM-assisted\noperations like Semantic Merge and Semantic Split were used less\nfrequently but creatively. Participants who did use them found\nthem helpful. The Magic Custom Prompt served as a lubricating\ncustom feature used to achieve a variety of editing goals in both\nrephrasing and reorganizing content. Finally, writing with speech\nwas considered to be a good way to get started and capture loose\nthoughts. Despite the learning curve for getting used to writing\nwith speech, almost all participants in the study preferred Rambler\nover the baseline and envisioned themselves using it in the future.\n6.2\nEffects of Gist-based Interface Features\nThe concept of gist-based interface was suggested for spoken text\nin previous theoretical work [20] and explored in a few existing\ninterfaces [6, 12] although not in this exact term. In this work, we\nfurther frame this concept in the context of dictation interfaces\nfor long-form writing. It is operationalized with a few features,\nincluding: gist extraction through Semantic Zoom and Highlighting\nKeywords; gist manipulation through macro revision operations\nincluding Ramble respeaking, Ramble reorganization, Ramble merg-\ning, splitting, and custom transformation.\nOur data showed that gist extraction features were the most\nfrequently used. Keywords appeared to be highly versatile: visual\ngrounding, users\u2019 self-annotation, customizing summaries, and in-\nfluencing custom LLM transformation to align content with users\u2019\nvoice. Semantic Zoom had mixed ratings and was actively employed\nby fewer participants despite being tried by most. When employed,\nit was used in multiple ways including supporting review, serving as\na checklist of ideas, and generating concise versions of participants\u2019\nwriting to be included in their final draft.\nMacro revision in essence refers to interaction with text in larger\nchunks, in particular to support the conceptual level of text manip-\nulation, as opposed to micro revision which focuses on verbatim\nediting word-by-word. In theory, it should serve well for speech-to-\ntext input, considering the speed of speech input, weaker verbatim\nmemory, good contextual correction of ASR, and the difficulty of\nperforming small corrections on mobile. In our study, manual oper-\nations for macro revisions were well-used, including Respeaking\nand Manual Splitting / Merging. Although LLM-assisted Splitting\nand Merging were less used, participants came up with creative\nstrategies to appropriate these features, showing promising new\nways of interaction with text via gist manipulation. Uncertainty\nabout the outcome of these features remains an obstacle and affects\nusers\u2019 trust.\nOverall, Rambler sets a starting point for exploring the design\nspace of gist-based interfaces for text review and manipulation.\nOur findings suggest that such interfaces can support useful GUI\nmanipulations without always requiring high input precision.\n6.3\nDesign Implications\nOur findings from the design and evaluation of Rambler suggest a\nnumber of implications for the future design of systems for writing\nwith speech. First, our findings reveal that the greatest benefit\nof using Rambler is the affordance of the Ramble structure and\nmacro revisions. Being able to manipulate text in chunks serves\nmobile platforms well, given that precise input is a challenge in\nthat environment. Spoken text in particular benefits from support\nfor iteration, such as automatic cleaning of disfluency and both\nLLM-assisted and manual support for reorganizing chunks of text.\nSecond, the popularity and high rating of Rambler\u2019s Magic Cus-\ntom Prompt reveals a need for customized requests in AI-assisted\nword processing. As also found in other research efforts embedding\nLLM capabilities in graphical user interfaces [5, 7, 36], Rambler is\nwell-served by having a mixture of deterministic shortcuts (buttons)\nand custom prompts, offering users control on multiple levels. Users\ncan rely on common operations without worrying about prompt\nquality, while still being able to take full advantage of AI support\nvia natural language requests. Some participants used Magic Cus-\ntom Prompt to split or reformat one large Ramble into multiple\nparagraphs or other structures, indicating the potential usefulness\nof custom prompting for scopes larger than a single Ramble. How-\never, part of the benefit of having operations applied to individual\nRambles is that changes are localized: word processing through a\nconversational UI (as in our baseline condition) makes it difficult to\napply changes at specific text locations and observe changes.\nThird, both macro revision and micro revision are critical compo-\nnents of Rambler; they go hand-in-hand, and are complementary\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nin serving users\u2019 goals. In our user study, keyboard editing was still\nused by most participants in the Rambler condition, even if visibly\nless than in the baseline condition (see Figure A.1).\nThese implications together also suggest a future of writing\ninterfaces that combine input modalities\u2014systems that can draw\non the affordances of speech, keyboard, and other input modes\nas appropriate, relying on LLM underpinnings to help mitigate\nimpedance mismatches (e.g., by cleaning up disfluencies) and create\nnew affordances (e.g., Semantic Zoom).\n6.4\nChallenges Embedding LLM API Calls in a\nLatency-Sensitive GUI\nUnder the hood, many of Rambler\u2019s system architecture decisions\nwere driven by the latency capabilities and limitations of the under-\nlying LLM (GPT-4) and transcription (AssemblyAI) APIs. We found\nGPT-4\u2019s latency to be roughly linear to output length; for example,\nproducing a 60-word summary took about 7 seconds.\nGiven that this would impact user experience in features like\nSemantic Zoom, we opted to pre-generate all summaries in parallel\n(and cache them for later) as soon as dictation ended, rather than\nwaiting until users sought them out. We used the streaming form of\nthe GPT-4 API to receive summaries. This approach showed users\nat least some summary text right away at each level, even if it would\ntake several seconds to fully complete, improving interactivity.\nAs an illustration of how latency limitations and LLM capa-\nbilities interact to steer interfaces and interactions, consider the\nfollowing anecdote from our development process: We initially\nattempted a mechanism for summary levels to update in real-time\nas the user was still dictating text, by sending that dictated-text-\nin-progress through our GPT-4 summary pipeline. However, this\npipeline yielded a new summary with nearly zero word-level con-\nsistency with the old summary, even if the difference between new\nand old transcript was minor. A summary that completely changes\nevery few seconds is not useful, but we found a way to specifically\nprompt GPT-4 to build on the previous existing summary based on\nthe changes to the transcribed text from one summary requested to\nthe next. Though this approach now generated useful \u201cappend-style\u201d\nsummaries, the prompts grew three times as fast as the transcript\ndid\u2014rapidly reaching latencies above 20 seconds, a delay so long\nas to render them essentially useless.\n7\nLIMITATIONS\nOne limitation of this work is that the study was conducted in a lab\nwith an assigned task to participants. This artificial setting does not\nstrongly motivate participants to feel invested in the task outcome.\nGiven the nature of Rambler, this poses a challenge as participants\nare more likely to claim completion rather than continue editing,\nlimiting the data we can collect. Although we designed the task\nspecifically to try to mitigate this, there are likely still differences\nbetween what we could find in a 90-minute long session where\nparticipants were given a topic to write, in comparison with a more\nrealistic task with greater benefit for the participant, in which the\nparticipant would feel more invested in the outcome. Additionally, a\nrelatively small sample size leads to challenges in concluding some\nof the potential correlations, e.g. potential effects of participants\u2019\ndictation experiences.\nFurther, our choice of study device was an iPad tablet, represent-\ning one of the mobile platforms suitable for long-form writing. We\nbelieve our general concept of supporting writing with speech via\nrambles, gist extraction, and macro revision can be applied to other\nmobile devices such as smartphones. However, a smaller screen\nwould demand some UI adjustments. Not all our findings could\ndirectly generalize to a smartphone platform. For instance, the fea-\nture usages may differ as a smaller screen renders more challenges\nin content reviewing and manual editing, which points to a poten-\ntially greater need for support. Our current system already offers a\nresponsive web interface, we plan to make UI adjustments and run\nfuture studies on other mobile devices in particular smartphones.\nIn addition, we used a computational method for objective as-\nsessment of text quality produced by participants. This method is\nlimited to evaluating fluency, redundancy and focus, therefore does\nnot encompass complete matrices of text quality. It serves as an\nadditional result to triangulate with the subjective assessment of\ntext quality.\nLastly, Rambler itself, as a research prototype, also comes with\ncertain limitations. Rambler semantic features relying on the GPT-\n4 API experience some unavoidable latency in the order of seconds.\nOur reliance on large language models for operations like Seman-\ntic Split and Semantic Merge might increase complications about\nproper attribution of authorship. Rambler also does not yet support\nmultiple files or syncing across devices.\n8\nFUTURE WORK\nPromising directions for future work include a longitudinal, diary\nstudy, which would provide participants with Rambler for longer\nand in a more realistic setting with their own mobile devices, po-\ntentially yielding insights into use less skewed by novelty effects.\nThis diary study should also include a more diverse participant\npool, addressing the limitation of our study being focused on an\nacademic context.\nWhile Rambler showed a decent degree of versatility by sur-\nfacing highly diverse participant strategies in our lab study, future\nwork shall investigate how to further support these diverse strate-\ngies, perhaps by making the features modular and user-customizable\nby choosing their own combinations. Future development of the\napproach described here could, for example, explore how to sup-\nport personalization of LLM-powered GUIs through user-defined\ncomponents for users\u2019 own LLM operations. Lastly, here we merely\nscratched the surface of the design space of possible Gist-based\nInterfaces, supporting only a subset of the possible semantic oper-\nations made possible by LLMs. Extending this work to other use\ncases and other form factors beyond writing with speech on mobile\ndevices could, in combination with this work, shed substantial light\non new ways of writing more broadly.\n9\nCONCLUSION\nIn this paper, we describe the design, implementation, and evalua-\ntion of Rambler, a versatile gist-based user interface that supports\nusers to iteratively transform spontaneous \u201crambling\u201d text into\nwell-structured writing. With Rambler, users can start writing\neasily and quickly by speaking \u201crambling\u201d ideas into our interface,\nseeing the content in clean text, grasping its gist with Semantic\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nZoom and Highlighted Keywords, and iterating on text content via\nLLM-assisted macro revisions (i.e., respeaking, merging, splitting,\nand rearranging). Our evaluation shows several major advantages\nof Rambler compared to a baseline speech-to-text editor with\nChatGPT, especially in supporting reviewing, manipulating, and\nrevising the spoken text. The supported macro revision interac-\ntions are especially suitable for mobile devices as they relax the\nconstant need for high-precision input for editing text. Our work\nalso contributes one example, in the context of interacting with\nspoken text, in support of the notion that injecting LLM capabilities\ninto a GUI tailored for a specific task\u2014and which also supports\nuser-defined prompts\u2014can outperform relying solely on chat-based\nLLM interfaces like ChatGPT.\nACKNOWLEDGMENTS\nThis project was funded in part by the Berkeley Artificial Intelli-\ngence Research Lab - Open Research Commons, and by the National\nNatural Science Foundation of China - Young Scientists Fund (CityU\n62202397). We thank the Android Input Research team at Google\nfor discussions and feedback.\nREFERENCES\n[1] Kenneth C Arnold, April M Volzer, and Noah G Madrid. 2021. Generative Models\ncan Help Writers without Writing for Them.. In IUI Workshops.\n[2] Oloff C. Biermann, Ning F. Ma, and Dongwook Yoon. 2022. From Tool to Compan-\nion: Storywriters Want AI Writers to Respect Their Personal Values and Writing\nStrategies. Proceedings of the 2022 ACM Designing Interactive Systems Conference\n(2022). https://api.semanticscholar.org/CorpusID:249578888\n[3] Oloff C. Biermann, Ning F. Ma, and Dongwook Yoon. 2022. From Tool to Com-\npanion: Storywriters Want AI Writers to Respect Their Personal Values and\nWriting Strategies. In Proceedings of the 2022 ACM Designing Interactive Systems\nConference (Virtual Event) (DIS \u201922). Association for Computing Machinery, New\nYork, NY, USA, 1209\u20131227. https://doi.org/10.1145/3532106.3533506\n[4] Mary Jo Bitner, Amy L Ostrom, and Felicia N Morgan. 2008. Service blueprinting:\na practical technique for service innovation. California management review 50, 3\n(2008), 66\u201394.\n[5] Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, and Tovi Grossman.\n2023. Promptify: Text-to-Image Generation through Interactive Prompt Explo-\nration with Large Language Models. In Proceedings of the 36th Annual ACM\nSymposium on User Interface Software and Technology (San Francisco, CA, USA)\n(UIST \u201923). Association for Computing Machinery, New York, NY, USA, Article\n96, 14 pages. https://doi.org/10.1145/3586183.3606725\n[6] Hai Dang, Karim Benharrak, Florian Lehmann, and Daniel Buschek. 2022. Beyond\nText Generation: Supporting Writers with Continuous Automatic Text Summaries.\nIn Proceedings of the 35th Annual ACM Symposium on User Interface Software and\nTechnology (Bend, OR, USA) (UIST \u201922). Association for Computing Machinery,\nNew York, NY, USA, Article 98, 13 pages. https://doi.org/10.1145/3526113.3545672\n[7] Hai Dang, Frederik Brudy, George Fitzmaurice, and Fraser Anderson. 2023.\nWorldSmith: Iterative and Expressive Prompting for World Building with a\nGenerative AI. In Proceedings of the 36th Annual ACM Symposium on User\nInterface Software and Technology (San Francisco, CA, USA) (UIST \u201923). Asso-\nciation for Computing Machinery, New York, NY, USA, Article 63, 17 pages.\nhttps://doi.org/10.1145/3586183.3606772\n[8] Jiayue Fan, Chenning Xu, Chun Yu, and Yuanchun Shi. 2021. Just Speak It:\nMinimize Cognitive Load for Eyes-Free Text Editing with a Smart Voice Assistant.\nIn The 34th Annual ACM Symposium on User Interface Software and Technology\n(Virtual Event, USA) (UIST \u201921). Association for Computing Machinery, New York,\nNY, USA, 910\u2013921. https://doi.org/10.1145/3472749.3474795\n[9] Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks: Inspiration\nfor Science Writing Using Language Models. In Proceedings of the 2022 ACM\nDesigning Interactive Systems Conference (Virtual Event, Australia) (DIS \u201922).\nAssociation for Computing Machinery, New York, NY, USA, 1002\u20131019. https:\n//doi.org/10.1145/3532106.3533533\n[10] Debjyoti Ghosh, Can Liu, Shengdong Zhao, and Kotaro Hara. 2020. Commanding\nand Re-Dictation: Developing Eyes-Free Voice-Based Interaction for Editing\nDictated Text. ACM Trans. Comput.-Hum. Interact. 27, 4, Article 28 (aug 2020),\n31 pages. https://doi.org/10.1145/3390889\n[11] Daphne Ippolito, Ann Yuan, Andy Coenen, and Sehmon Burnam. 2022. Creative\nWriting with an AI-Powered Writing Assistant: Perspectives from Professional\nWriters. ArXiv abs/2211.05030 (2022). https://api.semanticscholar.org/CorpusID:\n253420678\n[12] Peiling Jiang, Jude Rayan, Steven P Dow, and Haijun Xia. 2023. Graphologue:\nExploring Large Language Model Responses with Interactive Diagrams. arXiv\npreprint arXiv:2305.11473 (2023).\n[13] Clare-Marie Karat, Christine Halverson, Daniel Horn, and John Karat. 1999.\nPatterns of Entry and Correction in Large Vocabulary Continuous Speech Recog-\nnition Systems. In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems (Pittsburgh, Pennsylvania, USA) (CHI \u201999). Association for\nComputing Machinery, New York, NY, USA, 568\u2013575. https://doi.org/10.1145/\n302979.303160\n[14] Belinda Z. Li, Jason Eisner, Adam Pauls, and Sam Thomson. 2023. Toward\nInteractive Dictation. arXiv:2307.04008 [cs.CL]\n[15] Daniel Li, Thomas Chen, Albert Tung, and Lydia B Chilton. 2021.\nHierar-\nchical Summarization for Longform Spoken Dialog. In The 34th Annual ACM\nSymposium on User Interface Software and Technology (Virtual Event, USA)\n(UIST \u201921). Association for Computing Machinery, New York, NY, USA, 582\u2013597.\nhttps://doi.org/10.1145/3472749.3474771\n[16] Daniel Li, Thomas Chen, Alec Zadikian, Albert Tung, and Lydia B Chilton. 2023.\nImproving Automatic Summarization for Browsing Longform Spoken Dialog. In\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems.\n1\u201320.\n[17] Junwei Liao, Sefik Eskimez, Liyang Lu, Yu Shi, Ming Gong, Linjun Shou, Hong Qu,\nand Michael Zeng. 2023. Improving readability for automatic speech recognition\ntranscription. ACM Transactions on Asian and Low-Resource Language Information\nProcessing 22, 5 (2023), 1\u201323.\n[18] Can Liu, Siying Hu, Li Feng, and Mingming Fan. 2022. Typist Experiment: An\nInvestigation of Human-to-Human Dictation via Role-Play to Inform Voice-Based\nText Authoring. Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article 338 (nov\n2022), 33 pages. https://doi.org/10.1145/3555758\n[19] Paria Jamshid Lou and Mark Johnson. 2020. End-to-End Speech Recognition and\nDisfluency Removal. In Findings of the Association for Computational Linguistics:\nEMNLP 2020. 2051\u20132061.\n[20] Brinda Mehra, Kejia Shen, Hen Chen Yen, and Can Liu. 2023. Gist and Verbatim:\nUnderstanding Speech to Inform New Interfaces for Verbal Text Composition. In\nProceedings of the 5th International Conference on Conversational User Interfaces\n(Eindhoven, Netherlands) (CUI \u201923). Association for Computing Machinery, New\nYork, NY, USA, Article 15, 11 pages. https://doi.org/10.1145/3571884.3597134\n[21] Bill Moggridge. 2006. Designing Interactions. The MIT Press.\n[22] Christine Murad, Cosmin Munteanu, Benjamin R. Cowan, and Leigh Clark. 2019.\nRevolution or Evolution? Speech Interaction and HCI Design Guidelines. IEEE\nPervasive Computing 18, 2 (2019), 33\u201345.\nhttps://doi.org/10.1109/MPRV.2019.\n2906991\n[23] Ken Perlin and David Fox. 1993. Pad: An Alternative Approach to the Computer\nInterface. In Proceedings of the 20th Annual Conference on Computer Graphics and\nInteractive Techniques (Anaheim, CA) (SIGGRAPH \u201993). Association for Computing\nMachinery, New York, NY, USA, 57\u201364. https://doi.org/10.1145/166117.166125\n[24] Mizanur Rahman, Harold Jan Redonda Terano, Nafizur Rahman, Aidin\nSalamzadeh, and Saidur Rahaman. 2023. ChatGPT and Academic Research:\nA Review and Recommendations Based on Practical Examples. Journal of Educa-\ntion, Management and Development Studies (2023). https://api.semanticscholar.\norg/CorpusID:257845986\n[25] Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. 2010.\nAu-\ntomatic Keyword Extraction from Individual Documents.\nJohn Wiley &\nSons, Ltd, Chapter 1, 1\u201320.\nhttps://doi.org/10.1002/9780470689646.ch1\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470689646.ch1\n[26] Sherry Ruan, Jacob O. Wobbrock, Kenny Liou, Andrew Ng, and James A. Landay.\n2018. Comparing Speech and Keyboard Text Entry for Short Messages in Two\nLanguages on Touchscreen Phones. Proc. ACM Interact. Mob. Wearable Ubiquitous\nTechnol. 1, 4, Article 159 (jan 2018), 23 pages. https://doi.org/10.1145/3161187\n[27] J Schler, M Koppel, S Argamon, and JW Pennebaker. 2006. Effects of Age and\nGender on Blogging in Proceedings of 2006 AAAI Spring Symposium on Compu-\ntational Approaches for Analyzing Weblogs. In Proceedings of 2006 AAAI Spring\nSymposium on Computational Approaches for Analyzing Weblogs, Vol. 1.\n[28] Carola Strobl, Emilie Ailhaud, Kalliopi Benetos, Ann Devitt, Otto Kruse, Antje\nProske, and Christian Rapp. 2019. Digital support for academic writing: A review\nof technologies and pedagogies. Computers & education 131 (2019), 33\u201348.\n[29] Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023. Sensecape: Enabling\nMultilevel Exploration and Sensemaking with Large Language Models. arXiv\npreprint arXiv:2305.11483 (2023).\n[30] Tomohiro Tanaka, Ryo Masumura, Hirokazu Masataki, and Yushi Aono. 2018.\nNeural Error Corrective Language Models for Automatic Speech Recognition.. In\nINTERSPEECH. 401\u2013405.\n[31] Daniel Vogel and Patrick Baudisch. 2007. Shift: a technique for operating pen-\nbased interfaces using touch. In Proceedings of the SIGCHI conference on Human\nfactors in computing systems. 657\u2013666.\n[32] Daijin Yang, Yanpeng Zhou, Zhiyuan Zhang, Toby Jia-Jun Li, and LC Ray.\n2022.\nAI as an Active Writer: Interaction Strategies with Generated Text\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nin Human-AI Collaborative Fiction Writing 56-65. In IUI Workshops.\nhttps:\n//api.semanticscholar.org/CorpusID:248301902\n[33] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. 2022. Wordcraft:\nStory Writing With Large Language Models. (2022), 841\u2013852. https://doi.org/10.\n1145/3490099.3511105\n[34] J.D. Zamfirescu-Pereira, Heather Wei, Amy Xiao, Kitty Gu, Grace Jung, Matthew G\nLee, Bjoern Hartmann, and Qian Yang. 2023. Herding AI Cats: Lessons from\nDesigning a Chatbot by Prompting GPT-3. In Proceedings of the 2023 ACM\nDesigning Interactive Systems Conference (Pittsburgh, PA, USA) (DIS \u201923). As-\nsociation for Computing Machinery, New York, NY, USA, 2206\u20132220.\nhttps:\n//doi.org/10.1145/3563657.3596138\n[35] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny Can\u2019t Prompt: How Non-AI Experts Try (and Fail) to Design\nLLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing\nMachinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/\n3544548.3581388\n[36] Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, and Toby Jia-Jun Li. 2023. VISAR:\nA Human-AI Argumentative Writing Assistant with Visual Programming and\nRapid Draft Prototyping. In Proceedings of the 36th Annual ACM Symposium\non User Interface Software and Technology (San Francisco, CA, USA) (UIST \u201923).\nAssociation for Computing Machinery, New York, NY, USA, Article 5, 30 pages.\nhttps://doi.org/10.1145/3586183.3606800\n[37] Maozheng Zhao, Wenzhe Cui, IV Ramakrishnan, Shumin Zhai, and Xiaojun\nBi. 2021. Voice and Touch Based Error-tolerant Multimodal Text Editing and\nCorrection for Smartphones. In The 34th Annual ACM Symposium on User Interface\nSoftware and Technology. 162\u2013178.\n[38] Maozheng Zhao, Henry Huang, Zhi Li, Rui Liu, Wenzhe Cui, Kajal Toshniwal,\nAnanya Goel, Andrew Wang, Xia Zhao, Sina Rashidian, Furqan Baig, Khiem\nPhi, Shumin Zhai, IV Ramakrishnan, Fusheng Wang, and Xiaojun Bi. 2022. Eye-\nSayCorrect: Eye Gaze and Voice Based Hands-Free Text Correction for Mobile\nDevices. In 27th International Conference on Intelligent User Interfaces (Helsinki,\nFinland) (IUI \u201922). Association for Computing Machinery, New York, NY, USA,\n470\u2013482. https://doi.org/10.1145/3490099.3511103\n[39] Wanzheng Zhu and Suma Bhat. 2020. GRUEN for Evaluating Linguistic Quality\nof Generated Text. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: Findings. 94\u2013108.\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nA\nAPPENDIX\nA.1\nParticipant Workflows\nSee Figure 10, later, for a visualization of participant workflows.\nA.1.1\nDetailed Description of Participant Workflows.\n\u2022 P1 typically started new rambles with a few words, re-speaking\nto add on, and then keyboard editing it. At the very end, they\nmanually merged all rambles together to apply magic custom\nprompt, and then they used semantic split to get paragraph\nbreaks.\n\u2022 P2 thought about what to say before actually starting speak-\ning out loud, by pre-dividing their thoughts into Rambles.\nThey then edited the composition at a higher level through\nsemantic merges and magic custom prompt, before diving\ninto more keyboard editing.\n\u2022 P3 used Rambles like subpoints, creating an outline for them-\nselves. They first recorded a couple of Rambles, and then used\na series of magic custom prompts to clean the entire compo-\nsition, before recording more rambles, and using semantic\nmerge and manual split to restructure the composition.\n\u2022 P4 first produced a short paragraph and used magic custom\nprompt to reformat it into a design document (which inher-\nently generated more text). They then used semantic split\nand manual merge as they continued to add content through\nspeech. They used more keyboard editing in the latter half\nof the task.\n\u2022 P5 did not pre-plan, and spoke as if telling a story; they\nkeyboard-edited each section without much macro revision,\nbut at the end merged everything into a single Ramble to\napply the magic custom prompt \u201cclean up text\u201d.\n\u2022 P6 added content as it came to mind. They reorganized, man-\nually merged and split, applied magic custom prompt to\nrevise tone, and keyboard edited text.\n\u2022 P7 spoke in a sequential way\u2014for each ramble, they would\nspeak, then keyboard edit, and occasionally use magic cus-\ntom prompt to help revise. At the very end they took a look\nat the semantic zoom summaries.\n\u2022 P8 recorded most of their rambles first, then visited 50% se-\nmantic zoom level to see what was there. They indirectly\nedited the 50% zoom level by regenerating summaries, se-\nmantically merging, and adding onto rambles until they were\nhappy with the text in the 50% zoom level summary, which\nthey used for submission.\n\u2022 P9 worked in a fairly linear order. After recording each ram-\nble, they would sometimes choose to keyboard edit or alter\nkeywords, and then use magic custom prompt (with key-\nwords as context) to \u201cclean the text\u201d. At the very end, they\ndid some reordering.\n\u2022 P10 recorded all their content first, then highlighted key-\nwords in all the rambles and regenerated all the summaries.\nThen, they semantically merged all of them together to see\nwhat this would produce; because the context shifted due\nto LLM variability, they had to do some keyboard editing at\nthe end.\n\u2022 P11 recorded all of their content first, and then keyboard\nedited before using magic custom prompt (using keywords\nas context). At the very end, they took a look at the semantic\nzoom summaries.\n\u2022 P12 rambled a lot to branch off ideas and then restructured us-\ning reordering, semantic split and merge, and manual merge.\nThey highlighted keywords for their own reference.\nA.2\nList of Rambler Prompts\nAs mentioned in Section 3.4, we define custom prompts for our LLM-\nenabled functionality and embody them within our UI elements for\nease of use. We list these prompts below.\nA.2.1\nSplit Text Prompt.\nSystem: Split the paragraph the user enters into log-\nical, cohesive paragraphs and return the result as a\nJSON array. Analyze the content and break it up into\nat least two separate paragraphs (but more where it\nmakes sense). Try to split it into the appropriate num-\nber of paragraphs based on the content. Add each\nparagraph as a separate string element in the JSON\narray.\nResponse format: [\"Paragraph 1 text\", \"Paragraph 2\ntext\", \"Paragraph 3 text\"]\nUser: [User Ramble text]\nA.2.2\nMerge Text Prompt.\nSystem: You are a paragraph merger bot, capable of\nmerging paragraphs. Please merge the following text\ninto one paragraph of roughly median length as the\noriginals:\n[Text from selected user Rambles, concatenated with\nnewline]\nYou may use the following keywords to help you\nmerge the text. Ensure that each keyword is in the\nmerged paragraph.\nThe keywords are: [list of keywords].\nAgain, the resulting paragraph should be roughly the\naverage length of the original paragraphs.\nA.2.3\nClean Text Prompt.\nSystem: You are a text cleaning bot that cleans up the\ntext the user enters by correcting obviously incorrect\npunctuation and formatting, but otherwise keeping\nthe user\u2019s text the exact same. You never ask your\nown questions. For example, if the user enters:\nGoogle. Followed by the neural style transfer method,\nwhich became really popular. In a lot of cell phone\napps. And these things started to. Show up in art gal-\nleries and art exhibitions. And more and more artists\nstart playing with it. And now there\u2019s a number of\nfairly. Um, significant contemporary artists who are\nalso playing, experimented with AI techniques. This\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nLin, et al.\nwork all comes out of the, the academic research lit-\nerature, and whatnot. So these are images from one\nacademic paper which has really\nyou should return:\nGoogle. Followed by the neural style transfer method,\nwhich became really popular in a lot of cell phone\napps. And these things started to show up in art gal-\nleries and art exhibitions and more and more artists\nstart playing with it. And now there\u2019s a number of\nfairly, um, significant contemporary artists who are\nalso playing, experimented with AI techniques. This\nwork all comes out of the, the academic research lit-\nerature, and whatnot. So these are images from one\nacademic paper which has really\nUser: [User Ramble text]\nA.2.4\nSummarize Text Prompt.\nSystem: You are a professional writer specializing\nin text summarization. Make a summary of [LEVEL\nTEXT] of the chunk of the text provided by the user.\nThe summary should reflect the main idea and the\nmost important relationships of the text. You must\npreserve the same point of view, grammar and tense\nas the original text. If the text is in the first person, us-\ning words like I, you must use the first person as well.\nIf the tone was conversational, you must be human\nconversational as well. You should use the follow-\ning keywords to help you determine what to focus\nthe summary on. Ensure that each keyword is in the\nsummary. Try to fit as many as makes sense. Do not\ninclude anything else in the response other than the\nsummary.\nThe keywords are: [list of keywords].\nUser: [User Ramble text]\nwhere the possible values of [LEVEL TEXT], given that \ud835\udc3f = the\nnumber of words in the user paragraph, are\n\u2022 5 words or less\n\u2022 \ud835\udc3f / 4 words or less\n\u2022 \ud835\udc3f / 2 words or less\nRambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation\nCHI \u201924, May 11\u201316, 2024, Honolulu, HI, USA\nFigure 10: Timelines visualizing participants\u2019 workflow for each task. The icons are manually annotated based on the coding\nof the screen-recordings of the tasks. The timelines are a qualitative observation of feature usage: consecutive triggers of\nfunctions may be annotated as single uses, and obvious accidental triggers for keyword highlighting were omitted.\n"
  },
  {
    "title": "Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution",
    "link": "https://arxiv.org/pdf/2401.10404.pdf",
    "upvote": "8",
    "text": "Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video\nSuper-Resolution\nXin Yuan1*, Jinoo Baek2, Keyang Xu2, Omer Tov2, Hongliang Fei2\n1University of Chicago 2Google\nyuanx@uchicago.edu {jinoo,keyangxu,omertov,hongliangfei}@google.com\nAbstract\nWe propose an efficient diffusion-based text-to-video\nsuper-resolution (SR) tuning approach that leverages the\nreadily learned capacity of pixel level image diffusion model\nto capture spatial information for video generation. To ac-\ncomplish this goal, we design an efficient architecture by in-\nflating the weightings of the text-to-image SR model into our\nvideo generation framework. Additionally, we incorporate a\ntemporal adapter to ensure temporal coherence across video\nframes. We investigate different tuning approaches based\non our inflated architecture and report trade-offs between\ncomputational costs and super-resolution quality. Empirical\nevaluation, both quantitative and qualitative, on the Shut-\nterstock video dataset, demonstrates that our approach is\nable to perform text-to-video SR generation with good visual\nquality and temporal consistency. To evaluate temporal co-\nherence, we also present visualizations in video format in\ngoogle drive.\n1. Introduction\nDiffusion model [5, 14], as a deep generative model,\nhas achieved a new state-of-the-art performance, surpassing\nGANs [3,6,16,18] in generative tasks [9,10]. In a diffusion-\nbased text-conditioned generation system, a base model ini-\ntially generates a low-resolution image/video, which is sub-\nsequently refined by a super-resolution module [4,10,12] to\nproduce high-quality samples. Numerous existing diffusion-\nbased text-to-image super-resolution models [9,10], trained\non billion-scale text-image dataset, have demonstrated out-\nstanding generation capability. However, training text-to-\nvideo spatial super-resolution is challenging due to the\nscarcity of high-resolution video data. This scenario mo-\ntivates the inflation of off-the-shelf image models to video\ngeneration tasks [2,7,15,19]. Furthermore, training a video\ngeneration model needs exceedingly high computational and\nmemory requirements, which drives techniques that offer\n*This work has been done during the first author\u2019s internship at Google.\n2x\n4x\n8x\n16x\n16x\n8x\n4x\n2x\nTemporal Adapter\nInject\nTunable Module\nFrozen Weights\nFigure 1: Overall architecture of our approach. Up: we inflate\nthe UNet weights from a text-to-image model into a text-to-video\nmodel to perform a diffusion-based super-resolution task. Bottom:\nwe inject and tune a temporal adapter in the inflated architecture\nwhile maintaining the UNet weights frozen.\ncost-effective alternatives to optimize the video models.\nSeveral recently proposed methods [1] also focus on gen-\nerating high-quality videos using pretrained latent diffusion\nmodels. Temporal attention mechanisms are also commonly\nused in [4, 12]. Yet, investigating the trade-offs between\nvideo quality and the resource requirements in a fine-tuning\nstage is not the focus of those works. [1] typically requires\narXiv:2401.10404v1  [cs.CV]  18 Jan 2024\nfull tuning of all computational modules to generate high-\nquality videos, even with pretrained image weights inflated\nin the video architectures. In contrast, our approach lies in\nthe applied domain and investigates how tuning efficiency\naffects the video super-resolution quality. More importantly,\ninstead of investigating model inflation in latent space [1],\nour approach is the first to directly work on pixels. Note that\nour goal is not to achieve state-of-the-art generation quality.\nInstead, we aim to establish a practical and efficient tuning\nsystem to generate high-resolution videos with reasonable\nvisual quality and temporal consistency.\nIn this paper, we aim to leverage the readily learned spa-\ntial capacity of image weights for efficient and effective\ntext-to-video super-resolution, as shown in the upper of Fig-\nure 1. To capture the coherence across video frames, we\ninject an attention-based temporal adapter into the video\narchitecture. This adapter can be fine-tuned independently\nwhile keeping inflated weights frozen, as shown in the bot-\ntom of Figure 1. We perform the spatial super-resolution\ntask on the Shutterstock video dataset and validate that our\napproach is capable of generating videos with good visual\nquality and temporal consistency. We also demonstrate the\ntrade-off between tuning complexity and generation quality.\n2. Related Work\nDiffusion-based SR model is conditioned on low-\nresolution samples, generated by a base generation model,\nto further produce high-resolution images [10] or videos [4].\nWith the success of image generation models pre-trained\non billion-scale image data, recent research efforts have\nbeen made to directly borrow off-the-shelf image models for\nvideo tasks. For example, [7] load image weights for video\ngeneration in a zero-shot manner. [2,15,19] adopt model\ninflation and DDIM [13] inversion for text-to-video editing.\nWhile these studies may not directly apply to video spatial\nsuper-resolution task, they provide insightful hints on the\nfeasibility of adopting an image model without necessitating\nre-training from scratch.\nTemporal attention mechanisms that operate on the time\naxis are commonly adopted in video diffusion approaches [4,\n12]. Our method shares the same concept with [1] in the spirit\nof borrowing image diffusion models for video generation.\nHowever, our approach focus on the applied domain for text-\nto-video super resolution. More importantly, with facilitating\npartial tuning of the video architectures, we qualitatively and\nquantitatively evaluate how different tuning methods affect\nthe generation quality, including visual quality and temporal\nconsistency.\n3. Approach\nConsider a video clip represented as a sequence of n\nimage frames, denoted as I : [I1, ..., In], with low spatial\nresolution s, and a text description t for this clip, our objec-\ntive is to generate a new video clip of the same length but\nwith an enhanced resolution s\u2032 while preserving the corre-\nlation between text and video. We aim to exploit the robust\nspatial understanding of a pre-trained and fixed large-scale\nimage diffusion model, repurposing it for video generation\nthat remains temporally consistent. This removes the need\nfor extensive training from scratch on limited high-resolution\nvideo data, which is both time- and resource-consuming. We\nachieve this goal by inflating the weights of image diffusion\nmodel into a video generation architecture (as detailed in\nSection 3.1) and further tuning an efficient temporal adapter\nto ensure the continuity and coherence across video frames,\ndiscussed in Section 3.2.\n3.1. Inflation with Image Weights\nWe build a one-on-one mapping between image and video\narchitectures through \u2018upgrade\u2019 Imagen [10] text-to-image\nsuper-resolution model to accommodate video tasks. We\nfirst revisit the U-Net architecture in Imagen, composed of\nresidual and cross-attention blocks, as shown in Figure 2\n(left).\nGiven a batch of input static images with shape\nB \u00d7 C \u00d7 H \u00d7 W, the residual blocks capture the spatial\ninformation while cross-attention ensures that the generated\nimage aligns with the given text prompt. In the context of our\ntext-to-video super-resolution, the input batch of video clips\nis in the shape of B \u00d7F \u00d7C \u00d7H \u00d7W, where F is the num-\nber of frames. As shown in Figure 2 (right), each individual\nframe is processed through a parallel scheme, each branch\ncontains a residual block and cross-attention layer for text-\nvisual feature extraction. At the end of the UNet block, we\nhave a temporal adapter for feature aggregation to maintain\nconsistency and smoothness across frames. The processing\nunits of the residual block and cross-attention layer share the\nsame weights during training, in which case we can simply\nreshape the video data into (BF) \u00d7 C \u00d7 H \u00d7 W. Given\nthis nice property of weight sharing scheme, we can directly\ninflate the pre-trained image model weights into the video\nUNet without any architectural modification.\n3.2. Temporal Adaptater with Frame-wise Atten-\ntion\nTo capture the coherence among video frames, we apply\nthe temporal adapter after the residual and cross-attention\nblocks. Figure 3 depicts the design of the attention-based\ntemporal adapter. We first reshape video data I into I\u2032 with\nthe shape of B\u00d7F \u00d7(CHW) and then adopt a conventional\nself attention module:\nSelf-Attention(Q, K, V ) = Softmax(QKT\n\u221a\nd\n) \u00b7 V.\n(1)\nSuch an attention mechanism is effective in determining\nthe overall structure and the coherence of the video frames.\nText-to-Image SR UNet\nText-to-Video SR UNet\nResidual Block \n(Spatial Convs)\nImage\nCross-Attention\nResidual Block \n(Spatial Convs)\nVideo\nCross-Attention\nText\nFrame 1\nFrame 2\nFrame N\nTemporal Adapter\nResidual Block \n(Spatial Convs)\nCross-Attention\nResidual Block \n(Spatial Convs)\nCross-Attention\n\u2026\u2026\nText\nShape: (B, C, H, W)\nShape: (B, F, C, H, W)\nInflate Weights\nFigure 2: Weights inflation from a text-to-image SR UNet to a text-to-video SR UNet.\nFigure 3: Temporal adapter with attention that ensures temporal coherence across a video clip.\nSpecifically, a weighted sum over the \u2018token axis\u2019 F is cal-\nculated to learn the frame-wise correlation. We employ\nend-to-end optimization of either the full or partial model\nweights, aligning with the simple denoising objective in\nDDPM [5]. As such, the model weights are optimized by\nminimizing the MSE loss of noise prediction, conditioned\non the low-resolution frames.\n4. Experiments\nWe validate our approach on the Shutterstock dataset. We\ninflate a version of the Imagen diffusion model pre-trained\non our internal data sources for 8x super-resolution, into\nour video UNet. This UNet consists of four stages each\nfor downsampling and upsampling, denoted as 2\u00d7, 4\u00d7, 8\u00d7,\n16\u00d7. T5-xxl encoder [8] is used to extract text embedding,\nthe output of which is fed into the cross-attention layers\nwithin the 16\u00d7 stage. We train our video model on the Shut-\nterstock text-to-video dataset with 7 million video clips in\na resolution of 256 \u00d7 256-resolution and frame rate of 8\nFPS. The duration for each clip is 1 second, i.e. F = 8.\nThe supper-resolution scale is 4\u00d7, elevating the resolution\nfrom 64 \u00d7 64 to 256 \u00d7 256. We investigate several base-\nline optimization approaches, including (1) Zero-shot (ZS):\nGT\nZS\nFull\nTemporal\nFigure 4: Visualization of different tuning methods after image model inflation, conditioned on text prompt \u201cDog dachshund on chromakey\u201d.\nZS\nFull\nTemporal\nGT\nFigure 5: Text prompt: Camera follows cooking mezze machine rigate pasta in tomato sauce.\nwe directly evaluate the video model after inflation with-\nout further training. (2) Full-ft (Full): After integrating the\ntemporal adapter, all modules undergo optimization. This\nstrategy aims to showcase the potential \u2019upper bound\u2019 perfor-\nmance in the super-resolution task. (3) Temporal: we only\ntune the temporal adapter to capture the temporal consistency\nwhile maintaining superior generation quality efficiently. We\nfinetune for 1 epoch, using Adafactor [11] with initial LR of\n10\u22125 and batch size of 256 on 64 TPUv3.\nZS\nFull\nTemporal\nGT\nFigure 6: Text prompt: Little beautiful ducklings on green screen.\nZS\nFull\nTemporal\nGT\nFigure 7: Text prompt: Brazil northeast beaches.\n4.1. Quantitative Results\nWe evaluate different optimization approaches using var-\nious metrics. As shown in Table 1, the Full-ft approach\nachieves the best visual quality in terms of Peak signal to\nnoise ratio (PSNR) and structural index similarity (SSIM) by\ntuning all 628.89 million parameters of UNet. The efficient\ntemporal adapter tuning still yields reasonable visual qual-\nity while achieving an approximate 2\u00d7 wall-clock training\nacceleration and halving memory usage by adjusting only\n(a) Ground Truth\nWithout Inflated Image Weights\nWith Inflated Image Weights\n(b) Train with 40 % video data\nWithout Inflated Image Weights\nWith Inflated Image Weights\n(c) Train with 50 % video data\nWithout Inflated Image Weights\nWith Inflated Image Weights\n(d) Train with 60 % video data\nFigure 8: Visualizations of methods with and without image model inflation. Text prompt: Tourists visiting the old town.\nTable 1: Quantitative results for different tuning approaches.\nVisual Quality\nTemporal Consistency\nEfficiency\nMethod\nPSNR (% \u2191) SSIM (\u2191)\nTCC (\u2191)\nTunable Params (M) \u2193 Train Speed (steps/s) \u2191 Memory (G) \u2193\nZero-shot\n18.1\n0.42\n0.70\n-\n-\n-\nFull-ft\n28.7\n0.77\n0.86\n628.89\n1.05\n15\nTemporal\n24.3\n0.62\n0.82\n67.24\n2.02\n8\none-tenth of the typical parameter quantity. The zero-shot\napproach performs the worst.\nWe also validate that the efficient tuning approach can\nmaintain temporal consistency, i.e. the motions among con-\nstructive frames remain smooth in the super-resolution re-\nsults. We adopt the quantitative evaluation metric in [17]:\ntemporal change consistency (TCC), which is defined as:\nTCC(H, G) =\nPn\u22121\ni=1 SSIM(|hi \u2212 hi+1|, |gi \u2212 gi+1|)\nn \u2212 1\n(2)\nwhere H = {h1, h2, ..., hn} and G = {g1, g2, ..., gn} are\nhigh-resolution ground-truth and generated video frames,\nrespectively. Table 1 shows a clear trade-off between train-\ning efficiency and temporal consistency, in which efficient\ntemporal tuning still yields reasonable results. We also ob-\nserve that zero-shot approach fails to maintain the consistent\nchanges among adjacent frames due to the lack of a temporal\nmodule that operate exclusively on the time axis.\n4.2. Qualitative Results\nAs shown in Figure 4, when compared with the ground\ntruth high-resolution video (GT), both Full and Temporal\nproduce good super-resolution results, marked by high visual\nquality and temporal smoothness. The ZS approach manages\nto generate frames with decent visual content without any\nfine-tuning on video data, but it falls short in maintaining\ntemporal coherence \u2014 a limitation due to its pre-training\nsolely on static images. This demonstrates the effectiveness\nof our temporal adapter in capturing the coherence across\nvideo frames. We provide addtional visualizations in Fig-\nure 5, 6 and 7.\n4.3. Inflation is Data Efficient\nA straightforward baseline for image weight inflation is\nto randomly initialize the video UNet and fully fine-tune it\nusing only video data. As observed in Figure 9, the image\ninflation-based approach can achieve high visual quality\neven when leveraging only 10% of 7M video data. This\ntrend becomes more evident in Figure 8, demonstrating the\ndata efficiency of the image weight inflation approach.\n5. Conclusion\nIn this paper, we proposed a practical diffusion system for\ninflating text-to-image model weights to text-to-video spatial\n(a) PSNR\n(b) SSIM\nFigure 9: Training data efficiency evaluated by PSNR and SSIM.\nsuper-resolution model. This is the first work to study the\nweight inflation on the pixel level diffusion model. We have\ninvestigated different tuning methods for efficient temporal\nadaptation. We also demonstrated a good trade-off between\nthe super-resolution quality with temporal consistency and\ntuning efficiency. As a future investigation, we aim to scale\nup our target resolution from 256 to 512 (e.g. from 4\u00d7 to\n8\u00d7SR) and generate videos with longer time frames, which\nwould yield a more obvious trade-off between generation\nquality and computational resources.\nReferences\n[1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 1, 2\n[2] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J. Mi-\ntra. Pix2video: Video editing using image diffusion. CoRR,\nabs/2303.12688, 2023. 1, 2\n[3] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\nand Yoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 1\n[4] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Salimans.\nImagen video: High definition video generation with diffusion\nmodels. CoRR, abs/2210.02303, 2022. 1, 2\n[5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 1, 3\n[6] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of StyleGAN. In CVPR, 2020. 1\n[7] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto Henschel,\nZhangyang Wang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators.\nCoRR, abs/2303.13439, 2023. 1, 2\n[8] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. J. Mach. Learn. Res., 2020. 3\n[9] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 1\n[10] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\nJonathan Ho, David J. Fleet, and Mohammad Norouzi. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. In NeurIPS, 2022. 1, 2\n[11] Noam Shazeer and Mitchell Stern.\nAdafactor: Adaptive\nlearning rates with sublinear memory cost. In Jennifer G. Dy\nand Andreas Krause, editors, ICML, 2018. 4\n[12] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. In ICLR, 2023. 1, 2\n[13] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In ICLR, 2021. 2\n[14] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\nIn ICLR, 2021. 1\n[15] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of im-\nage diffusion models for text-to-video generation. CoRR,\nabs/2212.11565, 2022. 1, 2\n[16] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe\nGan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In CVPR, 2018. 1\n[17] Haokui Zhang, Ying Li, Yuanzhouhan Cao, Yu Liu, Chunhua\nShen, and Youliang Yan. Exploiting temporal consistency for\nreal-time video depth estimation. In ICCV, 2019. 7\n[18] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris Metaxas. Stack-\nGAN: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In ICCV, 2017. 1\n[19] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han,\nTiande Guo, and Luoqi Liu. Towards consistent video editing\nwith text-to-image diffusion models. CoRR, 2023. 1, 2\n"
  },
  {
    "title": "Understanding Video Transformers via Universal Concept Discovery",
    "link": "https://arxiv.org/pdf/2401.10831.pdf",
    "upvote": "6",
    "text": "Understanding Video Transformers via Universal Concept Discovery\nMatthew Kowal1,3*\nAchal Dave3\nRares Ambrus3\nAdrien Gaidon3\nKonstantinos G. Derpanis1,2\nPavel Tokmakov3\n1York University, 2Samsung AI Centre Toronto, 3Toyota Research Institute\nyorkucvil.github.io/VTCD\nAbstract\nThis paper studies the problem of concept-based inter-\npretability of transformer representations for videos. Con-\ncretely, we seek to explain the decision-making process\nof video transformers based on high-level, spatiotemporal\nconcepts that are automatically discovered. Prior research\non concept-based interpretability has concentrated solely\non image-level tasks.\nComparatively, video models deal\nwith the added temporal dimension, increasing complex-\nity and posing challenges in identifying dynamic concepts\nover time. In this work, we systematically address these\nchallenges by introducing the first Video Transformer Con-\ncept Discovery (VTCD) algorithm. To this end, we pro-\npose an efficient approach for unsupervised identification of\nunits of video transformer representations - concepts, and\nranking their importance to the output of a model. The re-\nsulting concepts are highly interpretable, revealing spatio-\ntemporal reasoning mechanisms and object-centric repre-\nsentations in unstructured video models. Performing this\nanalysis jointly over a diverse set of supervised and self-\nsupervised representations, we discover that some of these\nmechanism are universal in video transformers. Finally, we\ndemonstrate that VTCD can be used to improve model per-\nformance for fine-grained tasks.\n1. Introduction\nUnderstanding the hidden representations within neural net-\nworks is essential for addressing regulatory concerns [11,\n31], preventing harms during deployment [5, 30], and can\naid innovative model designs [13]. This problem has been\nstudied extensively in the image world, both for convolu-\ntional neural networks (CNNs) [4, 22, 26, 35] and, more\nrecently, for transformers [51, 64], resulting in a number\nof key insights. For example, image classification models\nextract low-level positional and texture cues at early lay-\ners and gradually combine them into higher-level, semantic\n*Work completed during an internship at Toyota Research Institute\n9-7\n11-0\n2-4\n6-7\nWhat concepts are important for object permanence?\nContainer\nPositions\nTracking\nEarly Layers\nDeep Layers\nSpatial\nOcclusion\nCollisions\nframes \n1 7 14 20\nMiddle Layers\nFigure 1. Heatmap predictions of the TCOW model [62] for track-\ning through occlusions (top), together with concepts discovered\nby our VTCD (bottom). We can see that the model encodes po-\nsitional information in early layers, identifies containers and col-\nlisions events in mid-layers and tracks through occlusions in late\nlayers. Only one video is shown, but the discovered concepts are\nshared between many dataset samples (see video for full results).\nconcepts at later layers [4, 24, 45].\nHowever, while video transformers do share their over-\nall architecture with image-level ViTs, the insights obtained\nin existing works do very little to explain their inner mecha-\nnisms. Consider, for example, the recent approach for track-\ning occluded objects [62] shown in Figure 1 (top). To ac-\ncurately reason about the trajectory of the invisible object\ninside the pot, texture or semantic cues alone would not suf-\nfice. What, then, are the spatiotemporal mechanisms used\nby this approach? And are any of these mechanisms univer-\nsal across video models trained for different tasks?\nTo answer these questions, in this work we propose the\n1\narXiv:2401.10831v1  [cs.CV]  19 Jan 2024\nVideo Transformer Concept Discovery algorithm (VTCD)\n- the first concept-discovery methodology for interpreting\nthe representations of deep video transformers. We focus\non concept-based interpretability [21, 22, 26, 69] due to its\ncapacity to explain the decision-making process of a com-\nplex model\u2019s distributed representations in high-level, intu-\nitive terms. Our goal is to decompose a representation at\nany given layer into human-interpretable \u2018concepts\u2019 with-\nout any labelled data (i.e., concept discovery) and then rank\nthem in terms of their importance to the model output.\nConcretely, we first group model features at a given layer\ninto spatio-temporal tubelets via SLIC clustering [1], which\nserve as a basis for our analysis (Section 3.1.1). Next, we\ncluster these tubelets across videos to discover high-level\nconcepts [14, 21, 22, 39, 69] (Section 3.1.2) . The result-\ning concepts for an occluded object tracking method [62]\nare shown in Figure 1 (bottom) and span a broad range of\ncues, including spatiotamporal ones that detect events, like\ncollisions, or track the containers.\nTo better understand the decision-making mechanisms of\nvideo transformers, we then quantify the importance of con-\ncepts for the model\u2019s predictions. Inspired by previous work\non saliency maps [49], we propose a novel, noise-robust ap-\nproach to estimate concept importance (Section 3.2). Un-\nlike existing techniques that rely on gradients [35], or con-\ncept occlusion [21], our approach effectively handles redun-\ndancy in self-attention heads in transformer architectures.\nNext, we use VTCD to study whether there are any\nuniversal mechanisms in video transformer models, that\nemerge irrespective of their training objective. To this end,\nwe extend the recent work by Dravid et al. [16] to automat-\nically identify important concepts that are shared between\nseveral models in Section 4.1. We then analyze a diverse\nset of representations (e.g. supervised, self-supervised, or\nvideo-language) and make a number of discoveries: (i)\nmany concepts are indeed shared between models trained\nfor different tasks; (ii) early layers tend to form a spatiotem-\nporal basis that underlines the rest of the information pro-\ncessing; (iii) later layers form object-centric video represen-\ntations even in models trained in a self-supervised way.\nFinally, VTCD can be used to turn a pre-trained video\ntransformer into an efficient and effective fine-grained\nrecognition model by pruning least important units.\nWe\ndemonstrate in Section 5.4 that removing one third of the\nheads from an action classification model results in a 4.3%\nincrease in accuracy while reducing computation by 33%.\n2. Related work\nOur work proposes a novel concept-based interpretabil-\nity algorithm that focuses on transformer-based represen-\ntations for video understanding. Below, we review the most\nrelevant works in each of these fields.\nConcept-based interpretability is a family of neural net-\nwork interpretability methods used to understand, post-hoc,\nthe representations that a model utilizes for a given task.\nClosed-world interpretability operates under the premise of\nhaving a labeled dataset of concepts [4, 35]. However, for\nvideos, it is unclear what concepts may exist and also diffi-\ncult to densely label videos even if they were known a priori.\nIn contrast, unsupervised concept discovery makes no\nassumptions on the existence of semantic concepts and\nuses clustering to partition data into interpretable compo-\nnents within the model\u2019s feature space.\nACE [26] and\nCRAFT [22] segment input images into superpixels and\nrandom crops, before applying clustering at a given layer. In\nvideos, however, the potential tubelets far outnumber image\ncrops, prompting us to introduce a more efficient method\nfor segmenting videos into proposals in Section 3.1.1.\nA necessary component of concept-based interpretability\nis measuring the importance (i.e. fidelity) of the discovered\nconcepts to the model. However, the aforementioned meth-\nods [21, 22, 26, 35, 69] were developed for CNNs, and are\nnot readily applicable to transformers. The main challenge\nof ranking concepts in attention heads is due to the trans-\nformers\u2019 robustness to minor perturbations in self-attention\nlayers. To address this limitation, we introduce a new al-\ngorithm to rank the significance of any architectural unit,\ncovering both heads and intra-head concepts in Section 3.2.\nRecent work [16] identifies neurons that produce sim-\nilar activation maps across various image models (includ-\ning transformers). However, neurons are unable to explain\na full extent of a models\u2019 representation due to it being\ndistributed across many dimensions [18]. In contrast, our\nmethod works on any dimensional features and is applied\nto video-based models.\nInterpretability of transformers has received a signifi-\ncant amount of attention recently, due to the success of\nthis architecture in variety of computer vision problems.\nEarly work [51] contrasted vision transformer representa-\ntions with CNNs (representational differences per layer, re-\nceptive fields, localization of information, etc). Other work\naims to generate saliency heatmaps based on attention maps\nof a model [8, 9].\nLater works focused on understand-\ning the impact of different training protocols [47, 64] (e.g.\nself-supervised learning (SSL) vs. supervised) and robust-\nness [50, 70]. The features of a specific SSL vision trans-\nformer, DINO [2], were explored in detail and shown to\nhave surprising utility for part-based segmentation tasks.\nHowever, none of these works address concept-based inter-\npretability or study video representations.\nIndependently, studies in natural language processing\n(NLP) have analyzed self-attention layers [17, 63] and\nfound that heads are often specialized to capture different\nlinguistic or grammatical phenomenon. This is qualitatively\nseen in works that shows dissimilar attention maps for dif-\nferent self-attention heads [12, 34]. Moreover, other NLP\n2\nVideo Dataset\nConcept\nClustering\nConcepts\nTubelet proposals\n...\nSpatiotemporal Tubelets\n...\n...\n...\n...\n...\nFigure 2. Video Transformer Concept Discovery (VTCD) takes a dataset of videos, X, as input and passes them to a model, f[1,l] (shown\nin yellow). The set of video features, Z, are then parsed into spatiotemporal tubelet proposals, T (shown in red), via SLIC clustering in the\nfeature space. Finally, tubelets are clustered across the videos to discover high-level units of network representation - concepts, C (right).\nworks [44, 63] explore the impact of removing heads and\nfind that only a small number need to be kept to produce\nsimilar performance. Our findings agree with evidence from\nthese works, and in Section 5.4 we further demonstrate that\npruning unimportant heads from video transformers can ac-\ntually improve model\u2019s performance.\nVideo model interpretability is an under-explored area of\nresearch considering the recent successes of deep learning\nmodels in action recognition [36], video object segmenta-\ntion [40, 48, 59, 62], or self-supervised approaches [20, 52,\n58, 60, 65]. Efforts have used proxy tasks to measure the de-\ngree to which models use dynamic information [25, 29, 33]\nor scene bias [10, 41, 42]. One method quantifies the static\nand dynamic information contained in a video model\u2019s in-\ntermediate representation [37, 38]. However, these meth-\nods can only measure one or two predefined concepts (i.e.\nstatic, dynamic, or scene information) while our approach\nis not restricted to a subset of concepts. Another work vi-\nsualizes videos that activate single neurons (or filters) in 3D\nCNN\u2019s via activation maximization with temporal regular-\nization [19]. While this method has no restrictions on what a\nneuron can encode, it only applies to 3D CNNs and does not\ntruly capture \u2018concepts\u2019 across distributed representations\n(i.e. feature space directions that generalize across videos).\n3. Video transformer concept discovery\nWe study the problem of decomposing a video representa-\ntion into a set of high-level open-world concepts and rank-\ning their importance for the model\u2019s predictions. We are\ngiven a set of (RGB) videos, X \u2208 RN\u00d73\u00d7T \u00d7H\u00d7W , where\nN, T, H, and W denote the dataset size, time, height,\nand width, respectively, and an L layer pretrained model,\nf.\nLet f[r,l] denote the model from layer r to l, with\nf[1,l](X) = Zl \u2208 RN\u00d7C\u00d7T \u2032\u00d7H\u2032\u00d7W \u2032 being the intermedi-\nate representation at layer l. To decompose Zl into a set of\nhuman-interpretable concepts, Cl = {c1, ..., cQ}, existing,\nimage-level approaches [22, 26] first parse the N feature\nmaps into a set of M proposals, T \u2208 RM\u00d7C (M > N),\nwhere each Tm corresponds to a region of the input im-\nage. These proposals are then clustered into Q << M\nconcepts in the feature space of the model to form an as-\nsignment matrix W \u2208 RM\u00d7Q. Finally, the importance of\neach concept ci to the model\u2019s prediction is quantified by\na score si \u2208 [0, 1]. Performing this analysis over all the\nlayers in f produces the entire set of concepts for a model,\nC = {C1, ..., CL}, together with their corresponding im-\nportance scores.\nHowever, existing approaches are not immediately appli-\ncable to video transformers because they do not scale well\nand are focused on 2D CNN architectures. In this work,\nwe extend concept-based interpretability to video represen-\ntations. To this end, we first describe a computationally\ntractable proposal generation method (Section 3.1.1) that\noperates over space-time feature volumes and outputs spa-\ntiotemporal tublets. Next (Section 3.1.2), we adapt existing\nconcept clustering techniques to video transformer repre-\nsentations. Finally, in Section 3.2 we propose CRIS - a\nnovel concept importance estimation approach applicable\nto any architecture units, including transformer heads.\n3.1. Concept discovery\n3.1.1\nTubelet proposals\nPrevious methods [21, 22, 26] use superpixels or crops\nin RGB space to propose segments; however, the number\nof possible segments is exponentially greater for videos.\nMoreover, proposals in color space are unrestricted and may\nnot align with the model\u2019s encoded information, leading to\nmany irrelevant or noisy segments. To address these draw-\nbacks, we instantiate proposals in feature space, which nat-\nurally partitions a video based on the information contained\nwithin each layer (shown in Figure 2, left).\nMore specifically, we construct tubelets per video via\nSimple Linear Iterative Clustering [1, 32] (SLIC) on the\n3\nFigure 3. A visual representation of concept masking for a single\nconcept. Given a video xi and a concept, cl, we mask the tokens of\nthe intermediate representation zi = f[1,l](xi) with the concepts\u2019\nbinary support masks, Bcl, to obtain the perturbed prediction, \u02c6yi.\nspatiotemporal features via\nT = GAP(B \u2299 Z) = GAP(SLIC(Z) \u2299 Z),\n(1)\nwhere T \u2208 RM\u00d7C is the set of tubelets for the dataset,\nB \u2208 {0, 1}C\u00d7M\u00d7N\u00d7T \u2032\u00d7H\u2032\u00d7W \u2032 are spatiotemporal binary\nsupport masks obtained from the SLIC clustering, M is the\ntotal number of tubelets for all N videos (M >> N), and\nGAP is a global average pooling operation over the space\nand time dimensions.\nSLIC is an extension of the K-Means algorithm that\ncontrols a trade-off between cluster support regularity and\nadaptability, and also constrains cluster support masks to be\nconnected. Together these properties produce non-disjoint\ntubelets that are easier to interpret for humans because they\nreduce the need to attend to multiple regions in a video at\na time. Further, the pruning step in SLIC makes it more\nrobust to the hyperparameter that controls the desired num-\nber of clusters, as it automatically prunes spurious, discon-\nnected tubelets. Next, we describe our approach for group-\ning individual tubelets into higher-level concept clusters.\n3.1.2\nConcept clustering\nRecent work [21, 22, 69] has used Non-Negative Matrix\nFactorization (NMF) [14] to cluster proposals into concepts.\nGiven a non-negative data matrix, T+ \u2208 RM\u00d7C, NMF\naims to find two non-negative matrices, W+ \u2208 RM\u00d7Q and\nC+ \u2208 RQ\u00d7C, such that T+ = W+C+, where W+ is the\ncluster assignment matrix. Unfortunately, NMF cannot be\napplied to transformers as they use GeLU non-linearities,\nrather than ReLU, resulting in negative activations.\nWe solve this problem by leveraging Convex Non-\nnegative Matrix Factorization [14] (CNMF). Despite the\nname, CNMF extends NMF and allows for negative input\nvalues. This is achieved by constraining the factorization\nsuch that the columns of W are convex combinations of the\ncolumns of T, i.e. each column of W is a weighted average\nof the columns of T. This constraint can be written as\nW = TG,\n(2)\nwhere G \u2208 [0, 1]C\u00d7Q and P\nj Gi,j = 1. To cluster a set of\ntublets, T, into corresponding concepts, we optimize\n(G\u2217, C\u2217) = arg min\nC>0,G>0\n||T \u2212 TGC||2,\n(3)\nwhere the final set of concepts are represented by the rows\nof the matrix C, i.e. concept centroid ci is the ith row of C\n(Figure 2, right).\n3.2. Concept importance\nGiven a set of discovered concepts, we now aim to quan-\ntify their impact on model performance.\nOne approach,\nshown in Figure 3, is to mask out each concept indepen-\ndently and rank the importance based on the drop in perfor-\nmance [21]. Formally, let cl be a single target concept, and\nBcl \u2208 {0, 1}C\u00d7M\u00d7N\u00d7T \u2032\u00d7H\u2032\u00d7W \u2032 the corresponding binary\nsupport masks over X. It can then be masked in layer l via\n\u02c6y = f[l,L](Zl \u2299 (1 \u2212 Bcl)).\n(4)\nHowever, while this approach works well for CNNs [21],\ntransformers are robust to small perturbations within self-\nattention layers [44, 63]. Therefore, single concept mask-\ning has little effect on performance (shown by results in\nFigure 4). Instead, we mask a high percentage of sampled\nconcepts in parallel (across all layers and heads) and then\nempirically validate in Section 5.1 that averaging the results\nover thousands of samples produces valid concept rankings.\nFormally, we propose Concept Randomized Importance\nSampling (CRIS), a robust method to compute importance\nfor any unit of interest. To this end, we first randomly sam-\nple K different concept sets, such that each Ck \u2282 C. We\nthen define Ck\nl as the set of concepts in Ck discovered at\nlayer l, with BCk\nl denoting the corresponding binary sup-\nport masks. We mask out every concept at every layer of\nthe model via\n\u02c6yk = g(\u02dcBck\nL \u2299 f[L\u22121,L](\u00b7 \u00b7 \u00b7 (\u02dcBCk\n1 \u2299 f[0,1](X)))),\n(5)\nwhere g(\u00b7) is the prediction head (e.g. an MLP) and \u02dcB de-\nnotes the inverse mask (i.e. 1\u2212B). Finally, we calculate the\nimportance of each concept, ci, via\nsi = 1\nK\nK\nX\nk\n(D(\u02dcy, y) \u2212 D(\u02c6yk, y))1ci\u2208Ck,\n(6)\nwhere \u02dcy is the original prediction without any masking and\nD is a metric quantifying performance (e.g. accuracy).\n4. Understanding transformers with VTCD\nOur algorithm facilitates the identification of concepts\nwithin any unit of a model and quantifying their signifi-\ncance in the final predictions. However, this is not enough\n4\nto fully represent the computations performed by a video\ntransformer. It is also crucial to understand how these con-\ncepts are employed in the model\u2019s information flow.\nAs several recent works have shown [17, 46], the resid-\nual stream of a transformer serves as the backbone of the\ninformation flow. Each self-attention block then reads in-\nformation from the residual stream with a linear projec-\ntion, performs self-attention operations to process it, and\nfinally writes the results back into the residual stream. Cru-\ncially, self-attention processing is performed individually\nfor each head and several studies have shown, both in vi-\nsion [2, 15, 34] and NLP [63], that different self-attention\nheads capture distinct information. In other word, heads\nform the basis of the transformer representation.\nA closer analysis of the concepts found in the heads of\nTCOW with VTCD allows us to identify several patterns\nin the information processing of that model. In particular,\nFigure 1 shows that the heads in early layers group input to-\nkens based on their spatiotemporal positions. This informa-\ntion is then used to track objects and identify events in mid-\nlayers, and later layers utilize mid-layer representations to\nreason about occlusions. Next, we study whether any of\nthese mechanisms are universal across video transformers\ntrained on different datasets with varying objectives.\n4.1. Rosetta concepts\nInspired by [16], we propose to mine for Rosetta concepts\nthat are shared between models and represent the same in-\nformation. The key to identifying Rosetta units is a robust\nmetric, R, where a higher R-score corresponds to the two\nunits having a larger amount of shared information. Pre-\nvious work [16] focused on finding such neurons in image\nmodels based on correlating their activation maps. We in-\nstead propose to measure the similarity between concepts\n(i.e. distributed representations) via the mean Intersection\nover Union (mIoU) of the concepts\u2019 support.\nFormally, we mine Rosetta concepts by first applying\nVTCD to a set of D models {f 1, ..., f D}, resulting in\ndiscovered concepts, Cj = {cj\n1, ..., cj\ni}, and importance\nscores, Sj = {sj\n1, ..., sj\ni}, for each model f j. We then aim\nto measure the similarity between all concept D-tuples from\nthe models. Given a set of D concepts, {c1\ni , ..., cD\ni } and cor-\nresponding binary support masks, {B1\ni , ..., BD\ni }, we define\nthe similarity score of these concepts as\nRD\ni = |B1\ni \u2229 \u00b7 \u00b7 \u00b7 \u2229 BD\ni |\n|B1\ni \u222a \u00b7 \u00b7 \u00b7 \u222a BD\ni |.\n(7)\nNaively computing the similarity between all D-tuples\nresults in an exponential number of computations and is in-\ntractable for even small D. To mitigate this issues, we ex-\nclude two types of concepts: (i) unimportant ones and (ii)\nthose with a low R-score among d-tuples where d < D.\nMore specifically, we only consider the most important\n\u03f5% of concepts from each model.\nWe then iterate over\nd \u2208 {2, ..., D} and filter out any concept that has an R-score\nless than \u03b4 for all d-tuples in which it participates. Formally,\nthe filtered Rosetta d-concept scores are defined as\nRd\n\u03f5,\u03b4 = {Rd\ni |Rd\ni > \u03b4 \u2200Rd\ni \u2208 Rd\n\u03f5},\n(8)\nwhere Rd\n\u03f5 is the set of all R-scores among d concepts af-\nter the \u03f5 importance filtering. This results in a significantly\nsmaller pool of candidates for the next stage d+1, reducing\nthe overall computational complexity of the algorithm. Fi-\nnally, as some concepts may reside in a subset of the models\nbut are still interesting to study, we examine the union of all\nimportant and confident Rosetta d-concepts corresponding\nto R-scores R2\n\u03f5,\u03b4 \u222a \u00b7 \u00b7 \u00b7 \u222a RD\n\u03f5,\u03b4.\n5. Experiments\nWe evaluate the quality of our concept discovery algorithm\nquantitatively and qualitatively across a variety of models\ntrained for different tasks.\nDatasets. We use two datasets in our experiments: TCOW\nKubric [62] and Something-Something-v2 (SSv2) [27]. The\nformer is a synthetic, photo-realistic dataset of 4,000 videos\nwith randomized object location and motion, based on\nthe Kubric synthetic video generator [28].\nThis dataset\nis intended for semi-supervised video object segmentation\n(semi-VOS) through occlusions.\nSSv2 contains 220,847\nreal videos intended for finegrained action recognition.\nEach sample is a crowdsourced video of a person-object\ninteraction (i.e. doing something to something).\nUnlike\nmany other video classification benchmarks [6, 56], tem-\nporal reasoning is fundamental for distinguishing SSv2 ac-\ntions, making it an ideal choice for analyzing spatiotempo-\nral mechanisms in transformers.\nModels. We evaluate four models with public pretrained\ncheckpoints: (i) TCOW [62] trained on Kubric for semi-\nVOS, (ii) VideoMAE [60] trained on SSv2 for action clas-\nsification (Supervised VideoMAE), (iii) VideoMAE self-\nsupervised on SSv2 (SSL VideoMAE), and (iv) Intern-\nVideo [66], a video-text foundation model trained con-\ntrastively on 12M video clips from eight video datasets\nand 100M image-text pairs from LAION-400M [54]. As\nTCOW requires a segmentation mask as input, when ap-\nplying it to SSv2, we hand label the most salient object in\nthe initial frame to use as the query. We focus our analysis\non the first two models, and use the last two to validate the\nuniversality of our Rosetta concepts.\nImplementation details.\nFor all experiments, we run\nVTCD for 30 randomly sampled videos, and discover con-\ncepts from every head at every model layer. Prior work [2]\nshows Keys produce the most meaningful clusters in self-\nattention heads, so we focus here on Keys and present re-\nsults with Queries and Values, together with the rest of the\n5\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\nmIoU (%)\nPositive Perturbation \u2193\nOcc\nIG\nCRIS (Ours)\nRandom\n0\n0.2\n0.4\n0.6\n0.8\nNegative Perturbation \u2191\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\nAccuracy (%)\n0\n0.2\n0.4\n0.6\n0.8\nFigure 4. Concept attribution curves for every layer of TCOW\ntrained on Kubric (top) and VideoMAE trained on SSv2 (bottom).\nWe remove concepts ordered from most-to-least (left) or least-to-\nmost important (right). CRIS produces better concept importance\nrankings than methods based on occlusion (Occ) or gradients (IG).\nhyperparameters, in the appendix. Our code for the VTCD\ntoolbox allowing to analyze any video transformer repre-\nsentation will be released.\nVTCD target metrics.\nTo discover and rank concepts,\nVTCD requires a target evaluation metric.\nFor TCOW\nKubric, we use the intersection-over-union (IoU) between\nthe predicted and the groundtruth masks. For SSv2, we use\nclassification accuracy for a target class.\n5.1. Quantitative concept evaluation\nTo quantitatively confirm the effectiveness of our method,\nwe follow the standard evaluation protocol for concept-\nbased interpretability, and measure the fidelity of the dis-\ncovered concepts [22, 26, 69]. To this end, we calculate\nattribution curves [21, 22, 26], where concepts (across all\nlayers) are removed from a model in either most-to-least\norder (positive perturbation), or least-to-most (negative per-\nturbation). The intuition is that concepts with higher fidelity\nand more accurate importance scores will have a steeper\nperformance decrease when removing the most important\nconcepts, and vice-versa for the reverse order.\nIn Figure 4, we plot concept attribution curves of our\nmethod for TCOW for Kubric (top) and Supervised Video-\nMAE for SSv2, targeting 10 randomly sampled classes and\naveraging results (bottom). In addition, we report results\nfor several baselines: (i) concept removal in a random or-\nder, (ii) standard, occlusion-based [21] concept importance\nestimation, and (iii) a gradient based approach [21, 35]. In\nall cases, CRIS produces a more viable importance ranking,\ndramatically outperforming both random ordering and the\nocclusion baseline. Integrated gradients approach performs\nsimilarly to ours for TCOW, but is significantly worse for\nthe action classification VideoMAE model.\nNotably, we observe that the performance actually in-\ncreases for VideoMAE when up to 70% of the least im-\nportant concepts are removed. Recall that SSv2 is a fine-\ngrained action classification dataset. Our method removes\nconcepts that are irrelevant for the given class, hence in-\ncreasing the robustness of the model\u2019s predictions. In Sec-\ntion 5.4, we elaborate on this observation and demonstrate\nhow VTCD can be used improve performance and effi-\nciency of any fine-grained video transformer.\n5.2. Qualitative analysis\nWe have seen that the importance assigned to concepts dis-\ncovered by VTCD aligns well with the accuracy of the\nmodel. We now turn to assessing the concepts themselves\nqualitatively. To this end, in Figure 5 we show two repre-\nsentative videos for the top three most important concepts\nfor the TCOW and VideoMAE models for the class drop-\nping something into something.\nFor TCOW, the most important concept occurs in layer\nfive and tracks the target object. Interestingly, the same con-\ncept highlights objects with similar appearance and 2D po-\nsition to the target. This suggests that the model solves the\ndisambiguation problem by first identifying possible dis-\ntractors in mid-layers (i.e. five) and then using this infor-\nmation to more accurately track the target in final layers. In\nfact, the second most important concept, occurring in layer\nnine, tracks the target object throughout the video.\nFor VideoMAE, the most important concepts highlights\nthe object being dropped until the dropping event, at which\npoint both the object and the container are highlighted. The\nsecond most important concept clearly captures the con-\ntainer being dropped into, notably not capturing the object\nitself and making a ring-like shape. These concepts identify\nan important mechanism of the model that helps it differ-\nentiating between similar classes (e.g. dropping something\ninto/behind/in-front of something).\nThe third most important concept for each model capture\nsimilar information, occurring in early layers: a temporally\ninvariant, spatial support. This observation corroborates re-\nsearch [2, 24] suggesting that positional information pro-\ncessing occurs early in the model and acts as a reference\nframe between the semantic information and the tokens\nthemselves. We now set out to discover concepts that are\nshared across multiple models, termed Rosetta concepts.\n5.3. Rosetta concepts\nWe begin by applying VTCD to the four models, target-\ning two classes chosen due to their dynamic nature: rolling\nsomething across a flat surface and dropping something be-\nhind something. We then mine for Rosetta concepts using\nthe method described in Section 4.1 and setting \u03b4 = 0.15\nand \u03f5 = 15% in all experiments.\nThe resulting set of\nRosetta 4-concepts contains 40 tuples with an average R\n6\nLayer 5 - Head 8 - Concept 7\nTCOW - Kubric\nLayer 9 - Head 12 - Concept 3\nMost Important\n\u00a0\nMost Important\nLayer 3 - Head 11 - Concept 3\nMost Important\n\u00a0\nLayer 11 - Head 9 - Concept 5\nVideoMAE - SSv2\nLayer 8 - Head 1 - Concept 5\nLayer 4 - Head 3 - Concept 9\nVideo 1\nVideo 2\nVideo 1\nVideo 2\nVideo 1\nVideo 2\nVideo 1\nVideo 2\nVideo 1\nVideo 2\nVideo 1\nVideo 2\nFigure 5. The top-3 most important concepts for the TCOW model trained on Kubric (left) and VideoMAE trained on SSv2 for the target\nclass dropping something into something (right). Two videos are shown for each concept and the query object is denoted with a green\nborder in Kubric. For TCOW, the 1st and 2nd (top-left, middle-left) most important concepts track multiple objects including the target\nand the distractors. For VideoMAE, the top concept (top-right) captures the object and dropping event (i.e. hand, object and container)\nwhile the 2nd most important concept (middle-right) captures solely the container. Interestingly, for both models and tasks, the third most\nimportant concept (bottom) is a temporally invariant tubelet. See further discussion in Section 5.3 (and video for full results).\nvid10\nvid24\nvid13\nvid9\nVOS\nVideo  \nLanguage\nAction \nRecognition\nSSL\nLayer 9 - Head 8 - Concept 3\nLayer 11 - Head 4 - Concept 3\nLayer 10 - Head 5 - Concept 2\nLayer 12 - Head 7 - Concept 9\nFigure 6. A sample Rosetta concept found in four models trained\nfor different tasks. Interestingly, we find object-centric represen-\ntations in all the models (see video for full results).\nscore (Equation 7) of 17.1. For comparison, the average R\nscore between all possible 4-concepts is 0.6, indicating the\nsignificance of the selected matches. We visualize one of\nthe mined Rosetta 4-concepts in Figure 6, which captures\nan object tracking representation in the late layers. This re-\nsult demonstrates that universal representations indeed exist\nbetween all four models. We show many more examples of\nshared concepts on the project web page in video format.\nNext, we qualitatively analyze all Rosetta d-concept with\nd \u2208 {2, 3, 4} at various layers, and show a representative\nsample in Figure 7. Firstly, we observe that in early layers\nthe models learn spatiotemporal basis representations (Fig-\nure 7, left). That is, they decompose the space-time volume\nof a video into connected regions that facilitate higher-level\nreasoning in later layers. This is consistent with prior works\nthat showed spatial position is encoded in the early layers of\nimage transformers [2, 24].\nIn the mid-layers (Figure 7, middle), we find that, among\nother things, all the models learn to localize and track in-\ndividual objects.\nThis result introduces a new angle to\nthe recently developed field of object-centric representation\nlearning [3, 23, 43, 55]: it invites us to explore how special-\nized approaches contribute, given that object concepts natu-\nrally emerge in video transformers. In addition, all models,\nexcept for synthetically trained TCOW, develop hand track-\ning concepts, confirming the importance of hands for action\nrecognition from a bottom-up perspective [57, 68].\nFinally, in deeper layers we find concepts that build on\ntop of an object-centric representation to capture specific\nspatiotemporal events. For example, three out of the four\n7\nSpatiotemporal object-centric\nRolling object\nSpatial basis\nTemporal basis\nSpatiotemporal basis\nObject-centric\nHands\nContainer before drop\nEarly layers\nDeep layers\nHands+object before drop\nFigure 7. Universal concepts emerge in video transformers despite being trained for different tasks. Early layers encode spatiotemporal\npositional information. Middle layers track various objects. Deep layers capture fine-grained spatiotemporal concepts, e.g. related to\nocclusion reasoning (see video for full results). Legend:\nTCOW,\nSupervised VideoMAE,\nSSL VideoMAE,\nInternVideo.\nModel\nAccuracy \u2191\nGFLOPs \u2193\nBaseline\n37.1\n180.5\nVTCD 33% Pruned\n41.4\n121.5\nVTCD 50% Pruned\n37.8\n91.1\nTable 1. Pruning unimportant heads in VideoMAE results in im-\nproved efficiency and accuracy when targeting a subset of classes.\nHere, we target the six SSv2 classes containing types of spills.\nmodels learn to identify containers before an object has\nbeen dropped into them and two models track the object in\nthe hand until it gets dropped. One notable exception here\nis InternVideo [66], which is primarily trained on static im-\nages and has a limited spatiotemporal modeling capacity.\nOn the other hand, we emphasize that these concepts are\nalso found in the self-supervised VideoMAE [60] that was\nnever trained to reason about object-container relationships.\nThis intriguing observation raises the question: can intuitive\nphysics models [7, 67] be learned via large-scale training of\nvideo representations?\n5.4. Application: Concept pruning with VTCD\nConsider a video transformer model trained for K classes,\nbut during deployment, the user is only interested in a sub-\nset of classes, Kd \u2282 K. Typically, they can either utilize the\nmodel as is, with many parameters dedicated to classes out-\nside Kd, or enhance performance by finetuning the model\nsolely on the subset. Motivated by the findings that dif-\nferent self-attention heads focus on class-specific concepts\n(Section 5.1), we instead propose a simple method leverag-\ning VTCD to prune unimportant heads to Kd and improve\nmodel performance and efficiency without finetuning.\nWe choose the VideoMAE model trained on the SSv2\ndataset and focus on the six classes where a \u2018spill\u2019 takes\nplace (listed in the appendix). We then use CRIS to rank all\nthe heads in VideoMAE according to their effect on perfor-\nmance on those six classes using the training set and report\nthe results for pruning the least important of them on the\nvalidation set in Table 1. Pruning the 33% least important\nheads actually improves the accuracy by 4.3% while reduc-\ning FLOPS from 180.5 to 121.5. Further removing 50% of\nthe heads retains the full performance of the original model\n(+0.7%) and reduces FLOPs to 91.1. These results suggest\nthat one can control the trade-off between performance and\ncomputation by choosing the number of heads to remove.\n6. Conclusion\nIn this work, we introduced VTCD, the first algorithm for\nconcept discovery in video transformers. We experimen-\ntally demonstrated that it is capable of extracting human-\ninterpretable concepts from video understanding models\nand quantifying their importance for the final predictions.\nUsing VTCD, we discovered shared concepts among sev-\neral models with varying objectives, revealing common pro-\ncessing patterns like a spatiotemporal basis in early layers.\nIn later layers, useful, higher-level representations univer-\nsally emerge, such as those responsible for object tracking.\nLarge-scale video representation learning is an active area\nof research at the moment and our approach can serve as a\nkey to unlocking its full potential.\nAcknowledgements. We acknowledge financial support from the\nCanadian NSERC Discovery Grants. K.G.D. contributed to this\nwork in their personal capacity as Associate Professor at York Uni-\nversity. Thanks to Greg Shakhnarovich for feedback on the paper.\n8\nUnderstanding Video Transformers via Universal Concept Discovery\nAppendix\nIn this appendix, we report additional results, visualiza-\ntions and implementation details. Note that we include ad-\nditional video results and corresponding discussions on the\nproject web page. We begin by ablating the tubelet gen-\neration component of VTCD and comparing to an existing\nconcept discovery approach in Section 7.1. We then pro-\nvide statistics of concepts importance distribution between\nlayers in Section 7.2. Next, in Section 7.3, we provide fur-\nther discussion and qualitative results showing how differ-\nent concepts are captured in different self-attention heads\nfrom the same layer. Finally, we provide further implemen-\ntation details in Section 8.\n7. Additional results\n7.1. Tubelet validation\nRecall that, unlike previous methods that partition the in-\nputs into proposals in the pixel space, VTCD generates the\nproposals via SLIC [1] clustering in the model\u2019s feature\nspace. We ablate this design choice by comparing VTCD\nwith CRAFT [22] - a recent concept discovery approach\nthat uses random cropping for proposal generation, in Ta-\nble 2.\nIn particular, we report concept attribution results for\nthe TCOW [62] and VideoMAE [60] models for both our\nmethod and CRAFT. In all cases, VTCD result in concepts\nthat are more faithful to the model\u2019s representation. To fur-\nther isolate the effect of proposals on the performance, we\nthen equip CRAFT with our concept importance estima-\ntion approach (shown as \u2018CRAFT [22] + CRIS\u2019 in the ta-\nble). The results confirm our observation from Figure 4 in\nthe main paper that CRIS is superior to the occlusion-based\nmasking used in CRAFT. However, VTCD still outperforms\nthis strong baseline in all settings, validating that generating\ntubelet proposals in the feature space of the transformer in-\ndeed results in concepts that are more faithful to the model\u2019s\nrepresentation.\n7.2. Quantitative analysis of per-layer concept im-\nportance\nWe now quantify the importance of each model layer for the\ntwo target models analyzed in Section 5.2 in the main paper.\nTo this end, we calculate the average concept importance\nranking per-layer and then normalize this value, which re-\nsults in a [0 \u2212 1] score, where higher values indicate more\nimportant layers, and plot the results in Figure 8.\nWe immediately see similarities and differences between\nthe two models. For example, the first two layers are less\nimportant than mid layers for both models.\nFor Video-\nTCOW\nVideoMAE\nModel\nPositive \u2193\nNegative \u2191\nPositive \u2193\nNegative \u2191\nCRAFT [22]\n0.174\n0.274\n0.240\n0.300\nCRAFT [22] + CRIS\n0.166\n0.284\n0.157\n0.607\nVTCD (Ours)\n0.102\n0.288\n0.094\n0.625\nTable 2. Ablation of our tubelet proposal approach via compari-\nson to CRAFT [22] for both TCOW [62] and VideoMAE [60]. Our\ntubelets result in concepts that are more faithful to the model\u2019s rep-\nresentations even when the baseline is equipped with our concept\nscoring algorithm (CRAFT [22] + CRIS).\n1 2 3 4 5 6 7 8 9 10 11 12\n0.4\n0.5\n0.6\n0.7\nLayer\nImportance (%)\nConcept Importance\nTCOW - VOS\nVideoMAE - AR\nFigure 8. The average concept importance over all model lay-\ners for a VOS model (TCOW) and an action recognition model\n(VideoMAE). Interestingly, while VideoMAE encodes important\nconcepts both in the middle and late in the model, TCOW encodes\nmost important concepts at layer three and the least important in\nthe final layer.\nMAE, the middle (6) and end layer (12) are the most im-\nportant. Interestingly, for TCOW, the most important layer\nby far is layer 3, while the final layer is the least impor-\ntant. This makes intuitive sense since TCOW is an object\ntracking model, hence it most heavily utilizes spatiotempo-\nral positional information and object-centric representations\nin early-to-mid layers. In contrast, VideoMAE is trained\nfor action classification, which requires fine-grained, spa-\ntiotemporal concepts in the last layers.\n7.3. Uniqueness of head concepts\nAs discussed in Section 4 in the main paper, we qualita-\ntively visualize the concepts from the same layer but dif-\nferent heads of a model to demonstrate that the heads en-\ncode diverse concepts. For example, Figure 9 shows that\ndiscovered concepts in heads one and six in layer five of\nthe TCOW [62] model encode unrelated concepts (e.g. po-\nsitional and falling objects).\nThis corroborates existing\nwork [2, 17, 34, 46, 63] that heads capture independent in-\nformation and are therefore a necessary unit of study using\nVTCD.\n1\nLayer 5 - Head 1\nLayer 5 - Head 6\nFigure 9. Different heads in the same layer capture independent\nconcepts. In layer 5 of TCOW [62], head 6 (top two rows) high-\nlights falling objects, while head 1 (bottom two rows) captures\nspatial position.\n8. Implementation details\nConcept discovery.\nWhen generating tubelets (Section\n3.1.1), we use 12 segments and set all other hyperparame-\nters to the Scikit-Image [61] defaults, except for the com-\npactness parameter, which is tuned on a held-out set for\neach model to the following values: TCOW - 0.01, Video-\nMAE - 0.1, SSL-VideoMAE - 0.1, InternVideo - 0.15.\nWhen clustering concepts using CNMF (Section 3.1.2) we\nfollow the same protocol as [2] and use the Elbow method,\nwith the Silhouette metric [53] as the distance, to select the\nnumber of clusters with a threshold of 0.9.\nConcept importance. For all importance rankings using\nCRIS, we use the original loss the models were trained with.\nFor InternVideo, we use logit value for the target class by\nencoding the class name with the text encoder, and then\ntaking the dot product between the text and video features.\nWe use 4,000 masking iterations for all models, except for\nTCOW [62], where we empirically observe longer conver-\ngence times and use 8,000 masks.\nConcept pruning with VTCD. The six classes targeted in\nthe concept pruning application (Section 5.4 in the main pa-\nper) are as follows:\n1. Pouring something into something until it overflows\n2. Spilling something behind something\n3. Spilling something next to something\n4. Spilling something onto something\n5. Tipping something with something in it over, so some-\nthing in it falls out\n6. Trying to pour something into something, but missing so\nit spills next to it\n2\nReferences\n[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien\nLucchi, Pascal Fua, and Sabine S\u00a8usstrunk. SLIC superpix-\nels compared to state-of-the-art superpixel methods. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n34(11):2274\u20132282, 2012. 2, 3, 1\n[2] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.\nDeep VIT features as dense visual descriptors.\nIn ECCV\nWorkshops, 2022. 2, 5, 6, 7, 1\n[3] Zhipeng Bao, Pavel Tokmakov, Allan Jabri, Yu-Xiong Wang,\nAdrien Gaidon, and Martial Hebert. Discovering objects that\ncan move. In CVPR, 2022. 7\n[4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and\nAntonio Torralba. Network dissection: Quantifying inter-\npretability of deep visual representations. In CVPR, 2017. 1,\n2\n[5] Joy Buolamwini and Timnit Gebru. Gender shades: Inter-\nsectional accuracy disparities in commercial gender classifi-\ncation. In ACM FAccT, 2018. 1\n[6] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? A new model and the kinetics dataset. In CVPR,\n2017. 5\n[7] Michael Chang, Tomer Ullman, Antonio Torralba, and\nJoshua Tenenbaum. A compositional object-based approach\nto learning physical dynamics. In ICLR, 2016. 8\n[8] Hila Chefer, Shir Gur, and Lior Wolf.\nGeneric attention-\nmodel explainability for interpreting bi-modal and encoder-\ndecoder transformers. In ICCV, 2021. 2\n[9] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-\npretability beyond attention visualization. In CVPR, 2021.\n2\n[10] Jinwoo Choi, Chen Gao, C. E. Joseph Messou, and Jia-Bin\nHuang. Why can\u2019t I dance in the mall? Learning to mitigate\nscene bias in action recognition. In NeurIPS, 2019. 3\n[11] European Commision. Laying down harmonised rules on ar-\ntificial intelligence (artificial intelligence act) and amending\ncertain union legislative acts. European Commision, 2021. 1\n[12] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin\nJaggi. On the relationship between self-attention and con-\nvolutional layers. In ICLR, 2020. 2\n[13] Timoth\u00b4ee Darcet, Maxime Oquab, Julien Mairal, and Pi-\notr Bojanowski. Vision transformers need registers. arXiv\npreprint arXiv:2309.16588, 2023. 1\n[14] Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and\nsemi-nonnegative matrix factorizations. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 32(1):45\u201355,\n2008. 2, 4\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 5\n[16] Amil Dravid, Yossi Gandelsman, Alexei A Efros, and Assaf\nShocher. Rosetta neurons: Mining the common units in a\nmodel zoo. In ICCV, 2023. 2, 5\n[17] Nelson Elhage,\nNeel Nanda,\nCatherine Olsson,\nTom\nHenighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai,\nAnna Chen,\nTom Conerly,\nNova Das-\nSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Andy Jones, Jackson Kernion, Liane\nLovitt,\nKamal Ndousse,\nDario Amodei,\nTom Brown,\nJack Clark, Jared Kaplan, Sam McCandlish, and Chris\nOlah. A mathematical framework for transformer circuits.\nTransformer Circuits Thread, 2021.\nhttps://transformer-\ncircuits.pub/2021/framework/index.html. 2, 5, 1\n[18] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas\nSchiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-\nDodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger\nGrosse, Sam McCandlish, Jared Kaplan, Dario Amodei,\nMartin Wattenberg, and Christopher Olah.\nToy models\nof superposition.\nTransformer Circuits Thread, 2022.\nhttps://transformer-circuits.pub/2022/toy model/index.html.\n2\n[19] Christoph Feichtenhofer, Axel Pinz, Richard P Wildes, and\nAndrew Zisserman.\nDeep insights into convolutional net-\nworks for video recognition. International Journal of Com-\nputer Vision, 128:420\u2013437, 2020. 3\n[20] Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al.\nMasked autoencoders as spatiotemporal learners. NeurIPS,\n2022. 3\n[21] Thomas Fel, Victor Boutin, Mazda Moayeri, R\u00b4emi Cad`ene,\nLouis Bethune, Mathieu Chalvidal, Thomas Serre, et al. A\nholistic approach to unifying automatic concept extraction\nand concept importance estimation. NeurIPS, 2023. 2, 3, 4,\n6\n[22] Thomas Fel, Agustin Picard, Louis Bethune, Thibaut\nBoissin, David Vigouroux, Julien Colin, R\u00b4emi Cad`ene, and\nThomas Serre. Craft: Concept recursive activation factoriza-\ntion for explainability. In CVPR, 2023. 1, 2, 3, 4, 6\n[23] Ruohan Gao,\nDinesh Jayaraman,\nand Kristen Grau-\nman. Object-centric representation learning from unlabeled\nvideos. In ACCV, 2017. 7\n[24] Amin Ghiasi, Hamid Kazemi, Eitan Borgnia, Steven Reich,\nManli Shu, Micah Goldblum, Andrew Gordon Wilson, and\nTom Goldstein. What do vision transformers learn? a visual\nexploration. arXiv preprint arXiv:2212.06727, 2022. 1, 6, 7\n[25] Amir Ghodrati, Efstratios Gavves, and Cees G. M. Snoek.\nVideo time: Properties, encoders and evaluation. In BMVC,\n2018. 3\n[26] Amirata Ghorbani, James Wexler, James Y Zou, and Been\nKim.\nTowards automatic concept-based explanations.\nIn\nNeurIPS, 2019. 1, 2, 3, 6\n[27] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax,\nand Roland Memisevic. The \u201csomething something\u201d video\ndatabase for learning and evaluating visual common sense.\nIn ICCV, 2017. 5\n[28] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, et al. Kubric: A\nscalable dataset generator. In CVPR, 2022. 5\n[29] Isma Hadji and Richard P Wildes. A new large scale dynamic\n3\ntexture dataset with application to convnet understanding. In\nECCV, 2018. 3\n[30] Sven Ove Hansson, Matts- \u02daAke Belin, and Bj\u00a8orn Lundgren.\nSelf-driving vehicles-An ethical overview.\nPhilosophy &\nTechnology, pages 1\u201326, 2021. 1\n[31] The White House. President biden issues executive order\non safe, secure, and trustworthy artificial intelligence. The\nWhite House, 2023. 1\n[32] Filip Ilic and Axel Pinz. Representing objects in video as\nspace-time volumes by combining top-down and bottom-up\nprocesses. In WACV, 2020. 3\n[33] Filip Ilic, Thomas Pock, and Richard P Wildes. Is appear-\nance free action recognition possible? In ECCV, 2022. 3\n[34] Rezaul Karim, He Zhao, Richard P. Wildes, and Mennatullah\nSiam. MED-VT: Multiscale encoder-decoder video trans-\nformer with application to object segmentation. In CVPR,\n2023. 2, 5, 1\n[35] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai,\nJames Wexler, and Fernanda Viegas. Interpretability beyond\nfeature attribution: Quantitative testing with concept activa-\ntion vectors (TCAV). In ICML, 2018. 1, 2, 6\n[36] Yu Kong and Yun Fu. Human action recognition and predic-\ntion: A survey. International Journal of Computer Vision,\n130(5):1366\u20131401, 2022. 3\n[37] Matthew Kowal, Mennatullah Siam, Md Amirul Islam,\nNeil DB Bruce, Richard P Wildes, and Konstantinos G Der-\npanis.\nA deeper dive into what deep spatiotemporal net-\nworks encode: Quantifying static vs. dynamic information.\nIn CVPR, 2022. 3\n[38] Matthew Kowal, Mennatullah Siam, Md Amirul Islam,\nNeil DB Bruce, Richard P Wildes, and Konstantinos G Der-\npanis.\nQuantifying and learning static vs. dynamic infor-\nmation in deep spatiotemporal networks.\narXiv preprint\narXiv:2211.01783, 2022. 3\n[39] Daniel D Lee and H Sebastian Seung. Learning the parts\nof objects by non-negative matrix factorization. Nature, 401\n(6755):788\u2013791, 1999. 2\n[40] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and\nJames M Rehg. Video segmentation by tracking many figure-\nground segments. In ICCV, 2013. 3\n[41] Yi Li and Nuno Vasconcelos. Repair: Removing representa-\ntion bias by dataset resampling. In CVPR, 2019. 3\n[42] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: To-\nwards action recognition without representation bias.\nIn\nECCV, 2018. 3\n[43] Francesco Locatello,\nDirk Weissenborn,\nThomas Un-\nterthiner, Aravindh Mahendran, Georg Heigold, Jakob\nUszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-\ncentric learning with slot attention. NeurIPS, 2020. 7\n[44] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\nheads really better than one? NeurIPS, 2019. 3, 4\n[45] Chris\nOlah,\nNick\nCammarata,\nLudwig\nSchubert,\nGabriel\nGoh,\nMichael\nPetrov,\nand\nShan\nCarter.\nZoom in:\nAn introduction to circuits.\nDistill, 2020.\nhttps://distill.pub/2020/circuits/zoom-in. 1\n[46] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph,\nNova DasSarma,\nTom Henighan,\nBen Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,\nDawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny\nHernandez, Scott Johnston, Andy Jones, Jackson Kernion,\nLiane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,\nJack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.\nIn-context learning and induction heads. Transformer Cir-\ncuits Thread, 2022. https://transformer-circuits.pub/2022/in-\ncontext-learning-and-induction-heads/index.html. 5, 1\n[47] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim,\nand Sangdoo Yun. What do self-supervised vision transform-\ners learn? In ICLR, 2023. 2\n[48] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc\nVan Gool, Markus Gross, and Alexander Sorkine-Hornung.\nA benchmark dataset and evaluation methodology for video\nobject segmentation. In CVPR, 2016. 3\n[49] Vitali Petsiuk, Abir Das, and Kate Saenko. RISE: Random-\nized input sampling for explanation of black-box models. In\nBMVC, 2018. 2\n[50] Yao Qin, Chiyuan Zhang, Ting Chen, Balaji Lakshmi-\nnarayanan, Alex Beutel, and Xuezhi Wang.\nUnderstand-\ning and improving robustness of vision transformers through\npatch-based negative augmentation. NeurIPS, 2022. 2\n[51] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-\nformers see like convolutional neural networks?\nNeurIPS,\n2021. 1, 2\n[52] Kanchana Ranasinghe, Muzammal Naseer, Salman Khan,\nFahad Shahbaz Khan, and Michael S Ryoo. Self-supervised\nvideo transformer. In CVPR, 2022. 3\n[53] Peter J Rousseeuw. Silhouettes: A graphical aid to the in-\nterpretation and validation of cluster analysis.\nJournal of\nComputational and Applied Mathematics, 20:53\u201365, 1987.\n2\n[54] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 5\n[55] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Do-\nminik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel,\nTong He, Zheng Zhang, Bernhard Sch\u00a8olkopf, Thomas Brox,\net al. Bridging the gap to real-world object-centric learning.\nIn ICLR, 2023. 7\n[56] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUCF101:\nA dataset of 101 human actions classes from\nvideos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n5\n[57] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Mur-\nphy, Rahul Sukthankar, and Cordelia Schmid. Actor-centric\nrelation network. In ECCV, 2018. 7\n[58] Xinyu Sun, Peihao Chen, Liangwei Chen, Changhao Li,\nThomas H Li, Mingkui Tan, and Chuang Gan. Masked mo-\ntion encoding for self-supervised video representation learn-\ning. In CVPR, 2023. 3\n[59] Pavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the\u201d\nobject\u201d in video object segmentation. In CVPR, 2023. 3\n[60] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideoMAE: Masked autoencoders are data-efficient learners\n4\nfor self-supervised video pre-training. NeurIPS, 2022. 3, 5,\n8, 1\n[61] Stefan Van der Walt, Johannes L Sch\u00a8onberger, Juan Nunez-\nIglesias, Franc\u00b8ois Boulogne, Joshua D Warner, Neil Yager,\nEmmanuelle Gouillart, and Tony Yu. scikit-image: image\nprocessing in python. PeerJ, 2:e453, 2014. 2\n[62] Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li,\nand Carl Vondrick. Tracking through containers and occlud-\ners in the wild. In CVPR, 2023. 1, 2, 3, 5\n[63] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,\nand Ivan Titov. Analyzing multi-head self-attention: Spe-\ncialized heads do the heavy lifting, the rest can be pruned. In\nACL, 2019. 2, 3, 4, 5, 1\n[64] Matthew Walmer, Saksham Suri, Kamal Gupta, and Abhi-\nnav Shrivastava. Teaching matters: Investigating the role of\nsupervision in vision transformers. In CVPR, 2023. 1, 2\n[65] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yi-\nnan He, Yi Wang, Yali Wang, and Yu Qiao. VideoMAE v2:\nScaling video masked autoencoders with dual masking. In\nCVPR, 2023. 3\n[66] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun\nWang, et al. InternVideo: General video foundation models\nvia generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022. 5, 8\n[67] Jiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and\nJosh Tenenbaum. Galileo: Perceiving physical object prop-\nerties by integrating a physics engine with deep learning.\nNeurIPS, 2015. 8\n[68] Chuhan Zhang, Ankush Gupta, and Andrew Zisserman.\nHelping hands: An object-aware ego-centric video recogni-\ntion model. In ICCV, 2023. 7\n[69] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A\nEhinger, and Benjamin IP Rubinstein. Invertible concept-\nbased explanations for cnn models with non-negative con-\ncept activation vectors. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, 2021. 2, 4, 6\n[70] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Ani-\nmashree Anandkumar, Jiashi Feng, and Jose M Alvarez. Un-\nderstanding the robustness in vision transformers. In ICML,\n2022. 2\n5\n"
  }
]