[
  {
    "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack",
    "link": "https://arxiv.org/pdf/2309.15807.pdf",
    "upvote": "30",
    "text": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack\nXiaoliang Dai\u2217, Ji Hou\u2217, Chih-Yao Ma\u2217, Sam Tsai\u2217, Jialiang Wang\u2217, Rui Wang\u2217, Peizhao Zhang\u2217,\nSimon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic,\nDhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song,\nRoshan Sumbaly\u2020, Vignesh Ramanathan\u2020, Zijian He\u2020, Peter Vajda\u2020, Devi Parikh\u2020\nGenAI, Meta\n{xiaoliangdai, jihou, cyma, sstsai, jialiangw, ruiw, stzpz}@meta.com\nFigure 1. With quality-tuning, Emu generates highly aesthetic images. Prompts: (top) a glass of orange juice; a woman in an apron works\nat a local bar; a coffee mug; (bottom) an egg and a bird made of wheat bread; a corgi; a shake is next to a cake.\nAbstract\nTraining text-to-image models with web scale image-text\npairs enables the generation of a wide range of visual con-\ncepts from text. However, these pre-trained models often\nface challenges when it comes to generating highly aes-\nthetic images. This creates the need for aesthetic alignment\npost pre-training. In this paper, we propose quality-tuning\nto effectively guide a pre-trained model to exclusively gen-\nerate highly visually appealing images, while maintaining\ngenerality across visual concepts. Our key insight is that\nsupervised fine-tuning with a set of surprisingly small but\n\u2217 Core contributors: equal contribution, alphabetical order.\n\u2020 Equal last authors.\nextremely visually appealing images can significantly im-\nprove the generation quality. We pre-train a latent diffusion\nmodel on 1.1 billion image-text pairs and fine-tune it with\nonly a few thousand carefully selected high-quality images.\nThe resulting model, Emu, achieves a win rate of 82.9%\ncompared with its pre-trained only counterpart. Compared\nto the state-of-the-art SDXLv1.0, Emu is preferred 68.4%\nand 71.3% of the time on visual appeal on the standard\nPartiPrompts and our Open User Input benchmark based\non the real-world usage of text-to-image models. In addi-\ntion, we show that quality-tuning is a generic approach that\nis also effective for other architectures, including pixel dif-\nfusion and masked generative transformer models.\n1\narXiv:2309.15807v1  [cs.CV]  27 Sep 2023\n1. Introduction\nRecent advances in generative models have enabled them\nto generate various high-quality content, such as text [2,33],\nimage [21, 25], music [16], video [32], and even 3D\nscenes [19, 22, 34], which has fundamentally revolution-\nized generative artificial intelligence (AI). In this paper, we\npresent a recipe we have found to be effective for train-\ning highly aesthetic text-to-image models. It involves two\nstages: a knowledge learning stage, where the goal is to\nacquire the ability to generate virtually anything from text,\nwhich typically involves pre-training on hundreds of mil-\nlions of image-text pairs; and a quality learning stage, which\nis necessary to restrict the output to a high-quality and aes-\nthetically pleasing domain. We refer to the process of fine-\ntuning for the purpose of improving quality and promoting\naesthetic alignment as quality-tuning for short.\nOur key insight is that to effectively perform quality-\ntuning, a surprisingly small amount \u2013 a couple of thousand\n\u2013 exceptionally high-quality images and associated text is\nenough to make a significant impact on the aesthetics of\nthe generated images without compromising the generality\nof the model in terms of visual concepts that can be gen-\nerated. Although having more data while maintaining the\nsame level of quality may be helpful, any attempts to priori-\ntize quantity over quality may result in a compromise of the\nquality of generated images.\nThis is an interesting finding in the broader landscape\nof fine-tuning generative models.\nQuality-tuning for vi-\nsual generative models can be thought of as analogous\nto instruction-tuning for large language models (LLMs)\nin terms of improving generation quality.\nFirst, before\ninstruction-tuning, language models are more prone to gen-\nerating low-quality text, which may be inconsistent in tone,\noverly verbose or concise, or simply unhelpful or even\ntoxic [6, 7, 13, 20, 35]; ChatGPT-level [2] performance is\nachieved with effective instruction-tuning [20]. Similarly,\nwe find that quality-tuning significantly improves the gen-\neration quality of text-to-image models. Second, the recipe\nfor effectively performing instruction-tuning and quality-\ntuning is similar: use high-quality data, even if the quan-\ntity has to be small to maintain quality. Llama2 [33] has\nbeen fine-tuned on 27K high-quality prompts, which can\nbe considered a very small quantity compared to the bil-\nlions or trillions of pre-training tokens. Similarly, we find\nthat strong text-to-image performance can be achieved by\nfine-tuning with even less data \u2013 a few thousand carefully\nselected images. Lastly, the knowledge obtained from pre-\ntraining is mostly retained after both instruction-tuning and\nquality-tuning. Like instruction-tuned LLMs, quality-tuned\ntext-to-image models retain their generality in terms of the\nvisual concepts that can be generated.\nThese post pre-\ntraining stages align the knowledge to downstream user\nvalue \u2013 improving text quality and following instructions\nin the case of LLMs, and promoting aesthetic alignment in\nthe case of text-to-image models.\nConcretely, we pre-train a latent diffusion model (LDM)\non 1.1 billion image-text pairs and quality-tune the model\non a few thousand hand-picked exceptionally high-quality\nimages selected from a large corpus of images.\nBy its\nnature, the selection criterion is subjective and culturally\ndependent.\nWe follow some common principles in pho-\ntography, including but not limited to composition, light-\ning, color, effective resolution, focus, and storytelling to\nguide the selection process. With a few optimizations to\nthe latent diffusion architecture, we start with a strong pre-\ntrained model and dramatically improve the visual appeal\nof our generated images through quality-tuning. In fact,\nit significantly outperform a state-of-the-art publicly avail-\nable model SDXLv1.0 [21] on visual appeal. We call our\nquality-tuned LDM Emu. We show example generations\nfrom Emu in Figure 1 and Figure 2.\nFurthermore, we show that quality-tuning is a generic\napproach that is also effective for pixel diffusion and\nmasked generative transformer models.\nOur main contributions are:\n\u2022 We build Emu, a quality-tuned latent diffusion model\nthat significantly outperforms a publicly available\nstate-of-the-art model SDXLv1.0 on visual appeal.\n\u2022 To the best of our knowledge, this is the first work to\nemphasize the importance of a good fine-tuning recipe\nfor aesthetic alignment of text-to-image models. We\nprovide insights and recommendations for a key ingre-\ndient of this recipe \u2013 supervised fine-tuning with a sur-\nprisingly small amount of exceptionally high-quality\ndata can have a significant impact on the quality of the\ngenerated images. Image quality should always be pri-\noritized over quantity.\n\u2022 We show that quality-tuning is a generic approach that\nalso works well for other popular model architectures\nbesides LDM, including pixel diffusion and masked\ngenerative transformer models.\n2. Related Work\nText-to-Image Models.\nGenerating an image from a\ntextual description has been explored using various ap-\nproaches. Diffusion-based methods learn a denoising pro-\ncess to gradually generate images from pure Gaussian\nnoise [14]. The denoising process can occur either in pixel\nspace or latent space, resulting in pixel diffusion models [5,\n25, 30] or latent diffusion models [21, 27], which feature\nhigher efficiency by reducing the size of spatial features.\nGenerative transformer methods usually train autoregres-\nsive [4,10,11,26,37,38] or non-autoregressive (masked) [9]\ntransformers in discrete token space to model the generation\n2\nFigure 2. Selected Examples. Selected images generated by our quality-tuned model, Emu.\n3\nprocess. Generative adversarial network [17,31] also show\nremarkable capability of generating realistic images. While\nthese models exhibit unprecedented image generation abil-\nity, they do not always generate highly aesthetic images.\nFine-Tuning Text-to-Image Models.\nGiven a pre-trained\ntext-to-image model, different methods have been devel-\noped to enable specific tasks.\nA number of techniques\nhave been developed to personalize or adapt text-to-image\nmodels to a new subject or style [12, 15, 28].\nControl-\nNet [39] provides additional control to the generation pro-\ncess by additionally conditioning on pose, sketch, edges,\ndepth, etc. InstructPix2Pix [8] makes text-to-image mod-\nels follow editing instructions by fine-tuning them on a set\nof generated image editing examples. To the best of our\nknowledge, this is the first work highlighting fine-tuning for\ngenerically promoting aesthetic alignment for a wide range\nof visual domains.\nFine-Tuning Language Models.\nFine-tuning has become\na critical step in building high-quality LLMs [1, 20, 33].\nIt generically improves output quality while enabling\ninstruction-following capabilities. Effective fine-tuning of\nLLMs can be achieved with a relatively small but high-\nquality fine-tuning dataset, e.g., using 27K prompts in [33].\nIn this work, we show that effective fine-tuning of text-to-\nimage models can be also achieved with a small but high-\nquality fine-tuning dataset.\nThis finding shows an inter-\nesting connection between fine-tuning vision and language\nmodels in generative AI.\n3. Approach\nAs discussed earlier, our approach involves a knowledge\nlearning stage followed by a quality-tuning stage. This may\nseem like a well-known recipe when mapped to pre-training\nand fine-tuning. That said, the key insights here are: (i) the\nfine-tuning dataset can be surprisingly small, on the order of\na couple of thousand images, (ii) the quality of the dataset\nneeds to be very high, making it difficult to fully automate\ndata curation, requiring manual annotation, and (iii) even\nwith a small fine-tuning dataset, quality-tuning not only sig-\nnificantly improves the aesthetics of the generated images,\nbut does so without sacrificing generality as measured by\nfaithfulness to the input prompt. Note that the stronger the\nbase pre-trained model, the higher the quality of the gener-\nated images after quality-tuning. To this end, we made sev-\neral modifications to the latent diffusion architecture [27] to\nfacilitate high-quality generation. That said, quality-tuning\nis general enough and can be applied to a variety of archi-\ntectures.\nIn this section, we first introduce the latent diffusion ar-\nchitecture we use. Then, we discuss the pre-training stage,\nFigure 3. Autoencoder. The visual quality of the reconstructed\nimages for autoencoders with different channel sizes. While keep-\ning all other architecture layers the same, we only change the latent\nchannel size. We show that the original 4-channel autoencoder de-\nsign [27] is unable to reconstruct fine details. Increasing channel\nsize leads to much better reconstructions. We choose to use a 16-\nchannel autoencoder in our latent diffusion model.\nfollowed by the protocol for collecting the high-quality fine-\ntuning dataset, and finally the quality-tuning stage. Later in\nSection 4, we demonstrate that quality-tuning is not limited\nto latent diffusion models but also improve other models\nsuch as pixel diffusion [30] and masked generative trans-\nformer [9] models.\n3.1. Latent Diffusion Architecture\nWe design a latent diffusion model that outputs 1024 \u00d7\n1024 resolution images. Following standard latent diffusion\narchitecture design, our model has an autoencoder (AE) to\nencode an image to latent embeddings and a U-Net to learn\nthe denoising process.\nWe find that the commonly used 4-channel autoencoder\n(AE-4) architecture often results in a loss of details in the\nreconstructed images due to its high compression rate. The\nissue is especially noticeable in small objects. Intuitively, it\ncompresses the image resolution by 64\u00d7 with three 2 \u00d7 2\ndownsampling blocks but increases the channel size only\n4\nfrom 3 (RGB) to 4 (latent channels). We find that increasing\nthe channel size to 16 significantly improves reconstruction\nquality (see Table 1). To further improve the reconstruc-\ntion performance, we use an adversarial loss and apply a\nnon-learnable pre-processing step to RGB images using a\nFourier Feature Transform to lift the input channel dimen-\nsion from 3 (RGB) to a higher dimension to better capture\nfine structures. See Figure 3 for qualitative results of au-\ntoencoders of different channel size.\nmodel\nchannel\nSSIM\nPSNR\nFID\n4\n0.80\n28.64\n0.35\nAE\n8\n0.86\n30.95\n0.19\n16\n0.92\n34.00\n0.06\nFourier-AE\n16\n0.93\n34.19\n0.04\nTable 1. While keeping all other architecture design choices fixed,\nwe first only change the latent channel size and report their recon-\nstruction metrics on ImageNet [29]. We see that AE-16 signifi-\ncantly improves over AE-4 on all reconstruction metrics. Adding\na Fourier Feature Transform and an adversarial loss further im-\nproves the reconstruction performance.\nWe use a large U-Net with 2.8B trainable parameters.\nWe increase the channel size and number of stacked residual\nblocks in each stage for larger model capacity. We use text\nembeddings from both CLIP ViT-L [23] and T5-XXL [24]\nas the text conditions.\n3.2. Pre-training\nWe curate a large internal pre-training dataset consist-\ning of 1.1 billion images to train our model. The model\nis trained with progressively increasing resolutions, similar\nto [21]. This progressive approach allows the model to ef-\nficiently learn high-level semantics at lower resolutions and\nimprove finer details at the highest resolutions We also use\na noise-offset [3] of 0.02 in the final stage of pre-training.\nThis facilitates high-contrast generation, which contributes\nto the aesthetics of the generated images.\n3.3. High-Quality Alignment Data\nAs discussed before, in order to align the model to-\nwards highly aesthetic generations \u2013 quality matters signif-\nicantly more than quantity in the fine-tuning dataset (see\nSection 4.3 for an ablation study on quality vs quantity).\nAs also discussed, the notion of aesthetics is highly subjec-\ntive. Here we discuss in detail what aesthetics we chose\nand how we curated our fine-tuning dataset by combining\nboth automated filtering and manual filtering. The general\nquality-tuning strategy will likely apply to other aesthetics\nas well.\nAutomatic Filtering. Starting from an initial pool of bil-\nlions of images, we first use a series of automatic filters\nto reduce the pool to a few hundreds of millions. These\nFigure 4. Visual Appealing Data. Examples of visually appealing\ndata that can meet our human filtering criterion.\nfilters include but are not limited to offensive content re-\nmoval, aesthetic score filter, optical character recognition\n(OCR) word count filter to eliminate images with too much\noverlaying text on them, and CLIP score filter to eliminate\nsamples with poor image-text alignment, which are stan-\ndard pre-filtering steps for sourcing large datasets. We then\nperform additional automated filtering via image size and\naspect ratio. Lastly, to balance images from various do-\nmains and categories, we leverage visual concept classifi-\ncation [36] to source images from specific domains (e.g.,\nportrait, food, animal, landscape, car, etc). Finally, with ad-\nditional quality filtering based on proprietary signals (e.g.,\nnumber of likes), we can further reduce the data to 200K.\nHuman Filtering. Next, we perform a two-stage human\nfiltering process to only retain highly aesthetic images. In\nthe first stage, we train generalist annotators to downselect\nthe image pool to 20K images. Our primary goal during\nthis stage is to optimize recall, ensuring the exclusion of\nmedium and low quality that may have passed through the\nautomatic filtering. In the second stage, we engage spe-\ncialist annotators who have a good understanding of a set\nof photography principles. Their task is to filter and se-\nlect images of the highest aesthetic quality (see Figure 4 for\nexamples). During this stage, we focus on optimizing pre-\ncision, meaning we aim to select only the very best images.\nA brief annotation guideline for photorealistic images is as\nfollows. Our hypothesis is that following basic principles\nof high quality photography leads to generically more aes-\nthetic images across a variety of styles, which is validated\n5\nvia human evaluation.\n1. Composition.\nThe image should adhere to certain\nprinciples of professional photography composition,\nincluding the \u201cRule Of Thirds\u201d, \u201cDepth and Layering\u201d,\nand more. Negative examples may include imbalance\nin visual weight, such as when all focal subjects are\nconcentrated on one side of the frame, subjects cap-\ntured from less flattering angles, or instances where the\nprimary subject is obscured, or surrounding unimpor-\ntant objects are distracting from the subject.\n2. Lighting. We are looking for dynamic lighting with\nbalanced exposure that enhances the image, for ex-\nample, lighting that originates from an angle, casting\nhighlights on select areas of the background and sub-\nject(s). We try to avoid artificial or lackluster lighting,\nas well as excessively dim or overexposed light.\n3. Color and Contrast. We prefer images with vibrant\ncolors and strong color contrast. We avoid monochro-\nmatic images or those where a single color dominates\nthe entire frame.\n4. Subject and Background. The image should have a\nsense of depth between the foreground and background\nelements. The background should be uncluttered but\nnot overly simplistic or dull.\nThe focused subjects\nmust be intentionally placed within the frame, ensur-\ning that all critical details are clearly visible without\ncompromise. For instance, in a portrait, the primary\nsubject of image should not extend beyond the frame\nor be obstructed. Furthermore, the level of detail on\nthe foreground subject is extremely important.\n5. Additional Subjective Assessments.\nFurthermore,\nwe request annotators to provide their subjective as-\nsessments to ensure that only images of exceptionally\naesthetic quality are retained by answering a couple of\nquestions, such as: (i) Does this image convey a com-\npelling story? (ii) Could it have been captured signifi-\ncantly better? (iii) Is this among the best photos you\u2019ve\never seen for this particular content?\nThrough this filtering process, we retained a total of 2000\nexceptionally high-quality images. Subsequently, we com-\nposed ground-truth captions for each of them. Note that\nsome of these handpicked images are below our target reso-\nlution of 1024 \u00d7 1024. We trained a pixel diffusion upsam-\npler inspired by the architecture proposed in [30] to upsam-\nple these images when necessary.\n3.4. Quality-Tuning\nWe can think of the visually stunning images (like the\n2000 images we collected) as a subset of all images that\nshare some common statistics.\nOur hypothesis is that a\nstrongly pre-trained model is already capable of generat-\ning highly aesthetic images, but the generation process is\nnot properly guided towards always producing images with\nthese statistics. Quality-tuning effectively restricts outputs\nto a high-quality subset.\nWe fine-tune the pre-trained model with a small batch\nsize of 64. We use a noise-offset of 0.1 at this stage. Note\nthat early stopping is important here as fine-tuning on a\nsmall dataset for too long will result in significant overfit-\nting and degradation in generality of visual concepts. We\nfine-tune for no more than 15K iterations despite the loss\nstill decreasing. This total iteration number is determined\nempirically.\n4. Experiments\nWe compare our quality-tuned model to our pre-trained\nmodel to demonstrate the effectiveness of quality-tuning.\nTo place the visual appeal of our generated images in con-\ntext with a current state-of-the-art model, we compare our\nmodel to SDXLv1.0 [21]. Due to lack of access to training\ndata of SDXL and their underlying model, we leveraged\ntheir corresponding APIs for our comparison. Note that un-\nlike SDXL, we use a single stage architecture, and do not\nuse a subsequent refinement stage. As stated earlier, we\nalso show that quality-tuning is not specific to LDMs, and\ncan be applied to other architectures \u2013 pixel diffusion and\nmasked generative transformer models.\n4.1. Evaluation Setting\nPrompts.\nWe evaluate on two large sets of prompts: 1600\nPartiPrompts [37] which is commonly used for text-to-\nimage generation benchmarking, and our 2100 Open User\nInput (OUI) Prompts. The OUI prompt set is based on real-\nworld user prompts. It captures prompts that are popular\nwith text-to-image models, reflecting visual concepts that\nare relevant to real-world use cases (Figure 5), paraphrased\nby LLMs to be closer to how regular users might input\nprompts (as opposed to being highly prompt engineered).\nThe overall motivation was to capture the creativity and in-\ntricacies of popular prompts for text-to-image models so\nwe are pushing the capabilities of models, while also being\ngrounded in likely real world use cases.\nMetrics.\nWe use two separate evaluation metrics: visual\nappeal and text faithfulness.\nVisual appeal refers to the\noverall aesthetic quality of a generated image. It combines\nvarious visual elements such as color, shape, texture, and\ncomposition that creates a pleasing and engaging look. The\nconcept of visual appeal is subjective and varies from per-\nson to person, as what may be aesthetically pleasing to one\nperson may not be to another. Therefore, we ask five anno-\ntators to rate each sample. Concretely, we show annotators\n6\nAnimals\nFood\nArt & Fashion\nCulture\nTransportation\nNature\nEmotion\nActivity\nArchitecture\nObjects\nHuman\nLocation\nFigure 5. Prompt distributions. The distribution of different con-\ncepts in our Open User Input prompts. We cover a comprehensive\nlist of common concepts people typically use to generate images.\ntwo images A and B, side-by-side, each generated by a dif-\nferent model using the same caption. The text captions are\nnot displayed. Annotators choose which image is more vi-\nsually appealing by selecting \u201cA\u201d, \u201cB\u201d or \u201cTie\u201d.\nText faithfulness refers to the degree of similarity be-\ntween a generated image and a text caption. In this task,\nwe again display two generated images A and B, side-by-\nside, but with the caption alongside the images. The annota-\ntors are asked to ignore the visual appeal of the images and\nchoose which ones best describe the caption with choices\n\u201cA \u201d, \u201cB\u201d, \u201cBoth\u201d, and \u201cNeither\u201d, where \u201cBoth\u201d and \u201cNei-\nther\u201d are considered as \u201cTie\u201d. In this task, we have three\nannotators to annotate each sample pair.\nWe do not report \u201cstandard\u201d metrics such as FID scores.\nAs argued in many recent papers (e.g., [18,21]), FID scores\ndo not correlate well with human assessment of the perfor-\nmance of generative models.\n4.2. Results\nEffectiveness of Quality-Tuning.\nFirst, we compare our\nquality-tuned model, Emu, with the pre-trained model. See\nFigure 7 for random (not cherry-picked) qualitative exam-\nples before and after quality-tuning. Note the highly aes-\nthetic non-photorealistic image as well, validating our hy-\npothesis that following certain photography principles in\ncurating the quality-tuning dataset leads to improved aes-\nthetics for a broad set of styles. We show more examples of\ngenerated images using Emu in Figure 8 and Figure 9.\nQuantitatively, as shown in Figure 6 (top), after quality-\ntuning, Emu is preferred in both visual appeal and text faith-\nfulness by a significant margin. Specifically, Emu is pre-\nferred 82.9% and 91.2% of the time for visual appeal, and\n36.7% and 47.9% of the time for text faithfulness on Par-\ntiPrompts and OUI Prompts, respectively. In contrast, the\n(a) All Prompts\n(b) Stylized Prompts\nFigure 6. Quality-Tuning vs Pre-training. Human evaluation on\nboth the PartiPrompts and Open User Input prompts shows that\nour quality-tuned model, Emu, significantly outperforms the pre-\ntrained model on visual appeal, without loss of generality of visual\nconcepts or styles that can be generated.\npre-trained model is preferred only 15.4% and 7.9% of the\ntime for visual appeal, and 21.0% and 18.5% of the time for\ntext faithfulness on PartiPrompts and OUI Prompts. The re-\nmaining cases result in ties. From the two large sets of eval-\nuation data that covers various domains and categories, we\ndid not observe degradation of generality across visual con-\ncepts. In fact, as seen, text-faithfulness also improved. We\nhypothesize this is because the captions of the 2000 quality-\ntuning images were manually written, while the captions in\nthe pre-training dataset tend to be noisy. Finally, we ana-\nlyze the results on non-photorealistic stylized prompts (e.g.,\nsketches, cartoons, etc.). We find that the improvements\nbroadly apply to a variety of styles, see Figure 6 (bottom).\nVisual Appeal in the Context of SoTA.\nTo place the vi-\nsual appeal of our generated images in the context of current\nstate-of-the-art, we compare Emu with SDXLv1.0 [21]. As\nshown in Table 2, our model is preferred over SDXLv1.0 in\nvisual appeal by a significant margin \u2013 including on stylized\n(non-photorealistic) prompts.\n7\nFigure 7. Qualitative Comparison. a comparison of images gen-\nerated by the pre-trained and quality-tuned model.\nQuality-Tuning Other Architectures.\nNext, we show\nour quality-tuning can also improve other popular architec-\ntures, such as pixel diffusion and masked generative trans-\nformer models. Specifically, we re-implement and re-train\nfrom scratch a pixel diffusion model, similar to Imagen [30]\nEval data\nwin (%)\ntie (%)\nlose (%)\nParti (All)\n68.4\n2.1\n29.5\nOUI (All)\n71.3\n1.2\n27.5\nParti (Stylized)\n81.7\n1.9\n16.3\nOUI (Stylized)\n75.5\n1.4\n23.1\nTable 2. Emu vs SDXL on Visual Appeal. Our model is pre-\nferred over SDXL by a large margin, including on stylized, non-\nphotorealistic prompts.\narchitecture, and a masked generative transformer model,\nsimilar to Muse [9] architecture, and then quality-tune them\non 2000 images. We evaluate both quality-tuned models\non 1/3 randomly sampled PartiPrompts. As shown in Fig-\nure 10, both architectures show significant improvement af-\nter quality-tuning on both visual appeal and text faithfulness\nmetrics.\n4.3. Ablation Studies\nWe do ablation studies on the fine-tuning dataset with a\nfocus on visual appeal. We first investigate the impact of\nthe dataset size. We report results of quality-tuning on ran-\ndomly sampled subsets of sizes 100, 1000 and 2000 in Ta-\nble 3. With even just 100 fine-tuning images, the model can\nalready be guided towards generating visually-pleasing im-\nages, jumping from a win rate of 24.8% to 60% compared\nwith SDXL.\nfine-tune data\nwin (%)\ntie (%)\nlose (%)\nw/o quality-tuning\n24.8\n1.4\n73.9\n100\n60.3\n1.5\n38.2\n1000\n63.2\n1.9\n35.0\n2000\n67.0\n2.6\n30.4\nTable 3. Visual Appeal by Fine-Tuning Dataset Size. All the\nnumbers are against SDXL as baseline. With merely 100 high-\nquality images as fine-tuning data, our quality-tuned model can\nalready outperform SDXL in visual appeal. Our model\u2019s visual\nappeal further improves as more images are used.\n5. Limitation\nLimitation of Human Evaluation. Relative to most pub-\nlished works, the prompt sets we evaluate on are reasonably\nlarge (1600 Parti prompts and our 2100 Open User Input\nprompts), under a multi-review annotation setting. Even\nthen, the evaluation results may not fully reflect real-world\nusage of the models. In addition, human evaluation of text-\nto-image models, especially when it comes to aesthetics, is\ninherently subjective and noisy. As a result, evaluation on\na different set of prompts or with different annotators and\nguidelines may lead to different results.\nLimitations of Small-Scale Fine-Tuning.\nThe role of\nquality-tuning is to restrict the output distribution to a high-\nquality domain. However, issues rooted from pre-training\n8\nFigure 8. Generated Examples. Images generated by our quality-tuned model, Emu.\n9\nFigure 9. More Examples. Images generated by our quality-tuned model, Emu.\n10\nFigure 10. Quality-Tuning vs Pre-training on Pixel Diffusion\nand Masked Generative Transformer. We adapt our quality-\ntuning to other popular text-to-image model architectures. Our re-\nsults indicate that the success of quality-tuning can be transferred\nto other architectures, beyond latent diffusion models.\nmay still persist. For instance, the models may struggle to\ngenerate certain objects that were not sufficiently learned\nduring pre-training.\nLimitations of Text-to-Image Models in General. Like\nother text-to-image models, our models may generate bi-\nased, misleading, or offensive outputs. We\u2019ve invested a\nsignificant amount in fairness and safety of our models -\nstarting with balanced dataset construction, creating dedi-\ncated evaluation sets for these risk categories and investing\nmultiple hours of redteaming.\n6. Conclusion\nIn this paper, we demonstrated that manually selecting\nhigh quality images that are highly aesthetically-pleasing\nis one of the most important keys to improving the aesthet-\nics of images generated by text-to-image generative models.\nWe showed that with just a few hundred to thousand fine-\ntuning images, we were able to improve the visual appeal\nof generated images without compromising on the general-\nity of visual concepts depicted. With this finding, we build\nEmu, a LDM for high-quality image synthesis. On the com-\nmonly used PartiPrompts and our Open User Input Prompts,\nwe carefully evaluate our model against a publicly avail-\nable state-of-the-art text-to-image model (SDXLv1.0 [21])\nas well as our pre-trained LDM. We also show that quality-\ntuning not only improves LDMs, but also pixel diffusion\nand masked generative transformer models.\n7. Acknowledgement\nThis work would not have been possible without a\nlarge group of collaborators who helped with the un-\nderlying infrastructure, data, privacy, and the evaluation\nframework.\nWe extend our gratitude to the following\npeople for their contributions (alphabetical order): Eric\nAlamillo, Andr\u00b4es Alvarado, Giri Anantharaman, Stuart An-\nderson, Snesha Arumugam, Chris Bray, Matt Butler, An-\nthony Chen, Lawrence Chen, Jessica Cheng, Lauren Co-\nhen, Jort Gemmeke, Freddy Gottesman, Nader Hamekasi,\nZecheng He, Jiabo Hu, Praveen Krishnan, Carolyn Krol,\nTianhe Li, Mo Metanat, Vivek Pai, Guan Pang, Albert\nPumarola, Ankit Ramchandani, Stephen Roylance, Kalyan\nSaladi, Artsiom Sanakoyeu, Dev Satpathy, Alex Schneid-\nman, Edgar Schoenfeld, Shubho Sengupta, Hardik Shah,\nShivani Shah, Yaser Sheikh, Karthik Sivakumar, Lauren\nSpencer, Fei Sun, Ali Thabet, Mor Tzur, Mike Wang, Mack\nWard, Bichen Wu, Seiji Yamamoto, Licheng Yu, Hector\nYuen, Luxin Zhang, Yinan Zhao, and Jessica Zhong.\nFinally, thank you Connor Hayes, Manohar Paluri and\nAhmad Al-Dahle for your support and leadership.\nReferences\n[1] https://cdn.openai.com/papers/gpt-4.pdf.\n[2] https://openai.com/blog/chatgpt/.\n[3] https://www.crosslabs.org/blog/diffusion-with-offset-noise/.\n[4] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir\nKarpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Man-\ndar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal\nmasked multimodal model of the internet.\narXiv preprint\narXiv:2201.07520, 2022.\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al.\nediffi:\nText-to-\nimage diffusion models with an ensemble of expert denois-\ners. arXiv preprint arXiv:2211.01324, 2022.\n[6] Emily M Bender, Timnit Gebru, Angelina McMillan-Major,\nand Shmargaret Shmitchell.\nOn the dangers of stochastic\nparrots: Can language models be too big?\nIn Proceedings\nof the 2021 ACM conference on fairness, accountability, and\ntransparency, pages 610\u2013623, 2021.\n[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021.\n[8] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n[9] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023.\n[10] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image gen-\neration via transformers. Advances in Neural Information\nProcessing Systems, 34:19822\u201319835, 2021.\n[11] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors.\nIn\n11\nEuropean Conference on Computer Vision, pages 89\u2013106.\nSpringer, 2022.\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022.\n[13] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin\nChoi, and Noah A Smith. Realtoxicityprompts: Evaluating\nneural toxic degeneration in language models. arXiv preprint\narXiv:2009.11462, 2020.\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020.\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[16] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,\nAndy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,\nJiahui Yu, Christian Frank, et al.\nNoise2music:\nText-\nconditioned music generation with diffusion models. arXiv\npreprint arXiv:2302.03917, 2023.\n[17] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park.\nScal-\ning up gans for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10124\u201310134, 2023.\n[18] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023.\n[19] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023.\n[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al.\nTraining\nlanguage models to follow instructions with human feed-\nback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[21] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023.\n[22] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022.\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020.\n[25] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[26] Aditya Ramesh,\nMikhail Pavlov,\nGabriel Goh,\nScott\nGray,\nChelsea Voss,\nAlec Radford,\nMark Chen, and\nIlya Sutskever.\nZero-shot text-to-image generation.\nIn\nInternational Conference on Machine Learning, pages 8821\u2013\n8831. PMLR, 2021.\n[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022.\n[28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. 2022.\n[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115:211\u2013252, 2015.\n[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022.\n[31] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger,\nand Timo Aila. Stylegan-t: Unlocking the power of gans\nfor fast large-scale text-to-image synthesis. arXiv preprint\narXiv:2301.09515, 2023.\n[32] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022.\n[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023.\n[34] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023.\n[35] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Grif-\nfin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia\nGlaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and\nsocial risks of harm from language models. arXiv preprint\narXiv:2112.04359, 2021.\n12\n[36] I Zeki Yalniz, Herv\u00b4e J\u00b4egou, Kan Chen, Manohar Paluri, and\nDhruv Mahajan. Billion-scale semi-supervised learning for\nimage classification. arXiv preprint arXiv:1905.00546, 2019.\n[37] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022.\n[38] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning.\narXiv\npreprint arXiv:2309.02591, 2023.\n[39] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023.\n13\n"
  },
  {
    "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
    "link": "https://arxiv.org/pdf/2309.15505.pdf",
    "upvote": "21",
    "text": "FINITE SCALAR QUANTIZATION:\nVQ-VAE MADE SIMPLE\nFabian Mentzer1, David Minnen1, Eirikur Agustsson1, Michael Tschannen2,\u25e6\n1Google Research\n2Google DeepMind\nABSTRACT\nWe propose to replace vector quantization (VQ) in the latent representation of\nVQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where\nwe project the VAE representation down to a few dimensions (typically less than\n10). Each dimension is quantized to a small set of fixed values, leading to an\n(implicit) codebook given by the product of these sets. By appropriately choosing\nthe number of dimensions and values each dimension can take, we obtain the same\ncodebook size as in VQ. On top of such discrete representations, we can train the\nsame models that have been trained on VQ-VAE representations. For example,\nautoregressive and masked transformer models for image generation, multimodal\ngeneration, and dense prediction computer vision tasks. Concretely, we employ\nFSQ with MaskGIT for image generation, and with UViM for depth estimation,\ncolorization, and panoptic segmentation. Despite the much simpler design of FSQ,\nwe obtain competitive performance in all these tasks. We emphasize that FSQ\ndoes not suffer from codebook collapse and does not need the complex machinery\nemployed in VQ (commitment losses, codebook reseeding, code splitting, entropy\npenalties, etc.) to learn expressive discrete representations. Code on GitHub.\n1\nINTRODUCTION\nVector quantization (VQ), initially introduced by Gray (1984), has recently seen a renaissance in the\ncontext of learning discrete representations with neural networks. Spurred by the success of VQ-\nVAE (Van Den Oord et al., 2017), Esser et al. (2020) and Villegas et al. (2022) showed that training\nan autoregressive transformer on the representations of a VQ-VAE trained with a GAN loss enables\npowerful image and video generation models, respectively. At the same time, VQ has become\npopular component in image (Bao et al., 2021; Li et al., 2023) and audio (Baevski et al., 2019)\nrepresentation learning, and is a promising building block for the next generation of multimodal\nlarge language models (Aghajanyan et al., 2022; Kim et al., 2023; Aghajanyan et al., 2023).\nWhen training VQ-VAE, the goal is to learn a codebook C whose elements induce a compressed,\nsemantic representation of the input data (typically images). In the forward pass, an image x is en-\ncoded into a representation z (typically a sequence of feature vectors), and each vector in z quantized\nto (i.e., replaced with) the closest vector in C. The quantization operation is not differentiable. When\ntraining a VAE with VQ in the latent representation, Van Den Oord et al. (2017) use the straight-\nthrough estimator (STE) (Bengio et al., 2013), copying the gradients from the decoder input to the\nencoder output, resulting in gradients to the encoder. Since this still does not produce gradients\nfor the codebook vectors, they further introduce two auxiliary losses to pull the codeword vectors\ntowards the (unquantized) representation vectors and vice-versa.\nThe above formulation is challenging to optimize, and leads to the well-documented problem of un-\nderutilized codebooks (\u0141a\u00b4ncucki et al., 2020; Takida et al., 2022; Dhariwal et al., 2020; Huh et al.,\n2023): as the size of C is increased, many codewords will be unused. Subsequent works aimed to im-\nprove this with various tricks such as reinitializing the entire codebook or some codewords Dhariwal\net al. (2020); \u0141a\u00b4ncucki et al. (2020), stochastic formulations Takida et al. (2022), etc. (see Sec. 2).\n\u25e6Significant technical contributions.\n1\narXiv:2309.15505v2  [cs.CV]  12 Oct 2023\nFSQ\n(1, 0, -1)\n1\n0\n-1\nVQ\nFigure 1:\nFSQ (left): the final encoder layer projects to d dimensions (d = 3 shown). We bound\neach dimension of the encoder output z to L values (L = 3 shown), and then round to integers,\nresulting in the quantized \u02c6z, the nearest point in this hypercube. VQ (right): The final encoder layer\nprojects to d dimensions (d = 7 shown, as d is typically much larger for VQ). The resulting vector\nz is replaced with the closest vector from the codebook, \u02c6z, by nearest neighbor lookup.\nHere, we are interested in simplifying the original VQ-VAE formulation (Van Den Oord et al., 2017)\nwith the following goals: i) remove auxiliary losses, ii) achieve high codebook utilization by design,\nand iii) keep the functional setup the same to the extent that we obtain a drop-in replacement for VQ.\nTo this end, we draw inspiration from the neural compression literature, where discrete codes are\ntypically obtained with scalar quantization, following initial work (Ball\u00b4e et al., 2016; Theis et al.,\n2017): Each (scalar) entry in the representation z is independently quantized to the nearest integer\nby rounding. The majority of the current compression literature uses unbounded scalar quantization,\nwhere the range of integers is not limited by the encoder, only by constraining the entropy of the\nrepresentation. Other compression work relied on bounding the range of the quantizer (Mentzer\net al., 2018; Tschannen et al., 2018; Agustsson et al., 2019).\nWe call this approach finite scalar quantization (FSQ). The important insight is that by care-\nfully choosing how to bound each channel, we can get an implicit codebook of (almost) any de-\nsired size: Consider a vector z with d channels.\nIf we map each entry zi to L values (e.g.,\nvia zi 7\u2192 \u230aL/2\u230btanh(zi) followed by rounding to integers), we obtain a quantized \u02c6z, where \u02c6z\nis one of Ld unique possible vectors. Fig. 1 shows FSQ for d=3, L=3, implying a codebook\nC = {(\u22121, \u22121, \u22121), (\u22121, \u22121, 0), (\u22121, \u22121, 1), . . . , (1, 1, 1)}, where |C| = Ld = 27.\nTo get gradients through the rounding operation, we use the STE like VQ-VAE. Thus, using FSQ in-\nside an autoencoder trained with a reconstruction loss, we get gradients to the encoder that force the\nmodel to spread the information into multiple quantization bins, as that reduces the reconstruction\nloss. As a result, we obtain a quantizer that uses all codewords without any auxiliary losses.\nTo the best of our knowledge, FSQ has not been used for vision tasks outside of compression, where\nVQ remains dominant. We aim to change this by revisiting FSQ in conjunction with powerful\ntransformers/language models. In summary, our contributions are:\n1. We show that FSQ can serve as a drop-in replacement for VQ in various architectures, for dif-\nferent datasets and tasks, by applying it to MaskGIT (Chang et al., 2022) for image generation,\nand in UViM (Kolesnikov et al., 2022) for depth estimation, colorization, and panoptic segmen-\ntation. We observe a reduction of only 0.5 - 3% in the respective metrics, and correspondingly\nget highly similar visual results. We emphasize that the two model families have very differ-\nent designs (convolutional vs. transformer-based autoencoders, masked vs. fully autoregressive\ntransformers, decoder-only vs. encoder-decoder transformers, etc.).\n2. We analyze the trade-offs for VQ vs. FSQ, characterize the scaling behaviors w.r.t. codebook size\nof the two models, and analyze the representation complexity from a compression angle. We find\nthat FSQ is able to leverage large codebooks for better reconstruction metrics, and better sample\nquality. The codebook usage is very high for FSQ (\u2248100% for most models), without relying on\nany auxiliary losses.\n3. We show that the full generality of the VQ formulation gives little benefits over our simpler FSQ\nmethod (VQ is actually worse for large codebooks C). This can be attributed to VQ being difficult\nto optimize, whereas FSQ can be viewed as the standard VQ formulation changed such that a)\nthe encoder output is bounded and b) C is fixed. We note that the (implicit) FSQ C has much\nsmaller dimensionality vs. VQ (typically d < 10 for FSQ, vs. d \u2265 512 for VQ).\n2\nVQ\nFSQ\nQuantization\narg minc\u2208C ||z \u2212 c||\nround(f(z))\nGradients\nSTE\nSTE\nAux. Losses\nCommitment, codebook,\nentropy loss\n-\nTricks\nEMA on codebook,\ncodebook splitting\nprojections, ...\n-\nParameters\nCodebook\n-\n3\n2\n1\n0\n1\n2\n3\nz\n2\n1\n0\n1\n2\nf(z)\nround_ste(f(z))\nFigure 2:\nLeft: VQ made simple: comparing implementation and optimization of VQ vs. FSQ.\nRight: Bounding z with f, and rounding the output, shown for a single channel with L = 5.\n2\nRELATED WORK\nVQ-VAE and improvements\nVan Den Oord et al. (2017) introduced the initial formulation in\nVQ-VAE, including a commitment loss and EMA for improved codebook learning. Roy et al. (2018)\nuse soft expectation maximization (EM) to train VQ-VAE. They also report success in tuning the\ncodebook size for the target tasks. Dhariwal et al. (2020) use VQ-VAE for audio generation. To\nprevent codebook collapse, they use \u201crandom restarts\u201d, where vectors are reset to encoder outputs\nwhen their usage becomes low. They also introduce a multi-scale variant of VQ. \u0141a\u00b4ncucki et al.\n(2020) aim to improve codebook learning by periodically reinitializing it using offline clustering\nalgorithms. Yu et al. (2021) introduce a vision transformer (ViT) based VQ-GAN. To improve\nlearning of the quantizer, they l2-normalize all vectors and map codes to a lower dimensional space\nfor lookup. Takida et al. (2022) propose a stochastic quantization approach to avoid codebook\ncollapse, adding Gaussian noise to the encoder output to imitate quantization, which is annealed\nduring training. Williams et al. (2020) also explore stochastic quantizers, in addition to a hierarchical\nrepresentation. Huh et al. (2023) examines challenges in training the vanilla VQ formulation. They\npropose various improvements, including a re-parameterization, alternating optimization, and an\nimproved commitment loss.\nVQ Alternatives\nResidual quantization (RVQ) has been used for image (Lee et al., 2022) and\naudio (Zeghidour et al., 2021) generation. There, quantized codes are refined by additionally storing\n(quantized) residuals. In Product quantization (PQ) (Chen et al., 2020; El-Nouby et al., 2022), the\ncodebook is factored into a product of smaller codebooks. In a similar spirit, there is a body of\nliterature around reducing the number of tokens output by VQ-VAEs for more efficient inference,\nsee, e.g., Huang et al. (2023). Outside of vision tasks and compression, FSQ has been applied to\naudio tasks by Donahue et al. (2019) and Dieleman et al. (2021). The authors use a \u201cmargin loss\u201d\nto encourage the encoder to produce a bounded representation. Hsu et al. (2023) use per channel\ncodebooks, leading to a learned grid. The optimization uses the same losses as vanilla VQ.\nNeural compression\nMany works (Ball\u00b4e et al., 2016; Minnen et al., 2018; Lu et al., 2019; Mentzer\net al., 2020; Cheng et al., 2020) rely on unbounded scalar quantization and constrain the entropy of\nthe quantized representation to prevent spreading to all integers. Bounded scalar quantization (i.e.,\nFSQ), has been used to represent images with high fidelity (Mentzer et al. (2018) use d=16, L=5),\nand for \u201cextreme compression\u201d (Tschannen et al. (2018); Agustsson et al. (2019) used d=5, L=5).\nTo the best of our knowledge, FSQ has not been used outside of compression. Neural image com-\npression generally targets \u201chigh bitrate\u201d reconstructions, and the challenge is to reduce the entropy\nof the complex representations, whereas in representation learning with VQ-VAE, the goal is usually\nthe opposite: increase the entropy of a heavily constrained representation to maximally use it.\n3\nMETHOD\nWe start with some high-level intuition. VQ defines a learnable Voronoi partition in the high-\ndimensional latent space of VQ-VAE, which leads to a complex non-linear partitioning of the VQ-\nVAE input space (e.g., images). FSQ, by contrast, relies on a simple, fixed grid partition in a much\nlower-dimensional space. Intuitively this is feasible because VAEs have a relatively high model ca-\npacity in typical applications (see Sec. 2), and thus the non-linearity of VQ can be \u201cabsorbed\u201d into\n3\nencoder and decoder, so that FSQ enables partitions of the VAE input space of similar complexity\nas VQ.\n3.1\nFINITE SCALAR QUANTIZATION\nGiven a d-dimensional representation z \u2208 Rd, our goal is to quantize z to a finite set of codewords.\nTo this end, we first apply a bounding function f, and then round to integers. We chose f such that\neach channel/entry in \u02c6z = round(f(z)) takes one of L unique values (e.g., f : z 7\u2192 \u230aL/2\u230btanh(z)).\nThereby, we have \u02c6z \u2208 C, where C is the implied codebook, given by the product of these per-channel\ncodebook sets, with |C| = Ld. The vectors in C can simply be enumerated leading to a bijection from\nany \u02c6z to an integer in {1, . . . , Ld}. Therefore, VQ can be replaced with FSQ in any neural network-\nrelated setup where VQ is commonly used, e.g., to train transformers, after appropriately adapting\nthe output and input dimension of the layers before and after VQ, respectively. We generalize the\nabove exposition to the case where the i-th channel is mapped to Li values and get |C| = Qd\ni=1 Li.\nWe visualize FSQ in Fig. 1 (left) and in Fig. 2. Since quantization is performed by round to integers,\nsupporting even L requires an asymmetric f. We show the general f used throughout this paper\nas code in App. A.1. To propagate gradients throughout the round operation, we use the STE\nthroughout, replacing the gradients with 1. In ML frameworks, this can easily be implemented via\nthe \u201cstop gradient\u201d (sg) operation as round ste : x 7\u2192 x + sg(round(x) \u2212 x).\n3.2\nHYPERPARAMETERS\nFSQ has the following hyper-parameters: the number of channels d and the number of levels per\nchannel, L = [L1, . . . , Ld]. In most of our experiments, to obtain fair comparisons, we will choose\ntarget codebook sizes |C| based on the VQ codebooks we aim to replace with FSQ. However, various\nconfigurations of d and Li can approximate a given |C| (i.e., any L where Q\ni Li \u2248 |C| is a candidate).\nWe explore various configurations in our study, and find that not all choices lead to optimal results.\nHowever, we found a simple heuristic that performs well in all considered tasks: Use Li \u2265 5 \u2200i. In\nTable 1 we tabulate L for common target |C|.\n3.3\nPARAMETER COUNT\nWe note that FSQ has fewer parameters than VQ, since in VQ, a codebook of size |C| \u00b7 d is learned.\nFor example, for a typical |C|=212=4096 and d=512, this results in 2M parameters, which FSQ\nlacks. Additionally, since for FSQ, d tends to be much smaller than for VQ (e.g., d=5 for FSQ\nfor this |C|, see Tab. 1), the final encoder layer also has fewer parameters when training FSQ. To\ncompensate for this, we explored adding more dense layers at the end of the VAE encoder, resp. at\nthe start of the decoder, but found no further gains from doing so. Thus, in all models in this paper,\nFSQ with the same codebook size has fewer parameters.\n4\nEXPERIMENTS\n4.1\nREVIEW OF MASKGIT AND UVIM\nWe start with a brief review of MaskGIT (Chang et al., 2022) and UViM (Kolesnikov et al., 2022).\nIn MaskGIT, the authors first train a (convolutional) VQ-GAN autoencoder (Esser et al., 2020) for\nreconstruction (Stage I). They then freeze the autoencoder, and train a masked transformer BERT-\nstyle (Devlin et al., 2018) to predict the quantized representations (Stage II): Given a representation\n\u02c6z, a fraction of tokens is randomly \u201cmasked out\u201d, i.e., replaced with a special MASK token. The\nresulting sequence \u02c6zM is fed to a transformer in addition to a class token, and the transformer\npredicts a distribution for each masked token. During inference, initially only MASK tokens along\nTarget Size |C|\n28\n210\n212\n214\n216\nProposed L\n[8, 6, 5]\n[8, 5, 5, 5]\n[7, 5, 5, 5, 5]\n[8, 8, 8, 6, 5]\n[8, 8, 8, 5, 5, 5]\nTable 1: Recommended sets of FSQ levels L to approximately match a given codebook size |C|.\n4\n24\n26\n28\n210\n212\n214\n216\nCodebook Size\n6\n8\n10\n12\n14\n16\na) Reconstruction FID\nFSQ\nVQ\n24\n26\n28\n210\n212\n214\n216\nCodebook Size\n9\n10\n11\n12\n13\n14\n15\nb) Sampling FID\nFSQ\nVQ\n24\n26\n28\n210\n212\n214\n216\nCodebook Size\n25\n27\n29\n211\n213\n215\nc) Codebook Usage\nFSQ\nVQ\nMaximum\n50%\n24\n26\n28\n210\n212\n214\n216\nCodebook Size\n4\n6\n8\n10\n12\n14\n16\nd) Compression Cost [bits]\nFSQ\nVQ\nUniform\nFigure 3:\nCharacteristics and trade-offs for VQ and FSQ for 128 \u00d7 128 ImageNet. We see that\nReconstruction FID correlates with codebook size for FSQ, and improves as we scale the codebook\nsize. FSQ gets better Sampling FID and higher codebook usage for codebook size exceeding 210,\nwhile the metrics start deteriorating for VQ.\nwith the class token are fed to the transformer. Then, some of the token locations are selected based\non prediction confidence, and corresponding tokens are sampled (see (Chang et al., 2022, Sec 3.2)).\nThese tokens are used to replace mask tokens at the input, and the model is ran again, until all input\ntokens have been uncovered.\nUViM (Kolesnikov et al., 2022) is a general architecture to tackle various (dense) prediction tasks in\ncomputer vision. In the first stage a transformer-based VQ-VAE is trained to model the label space of\nthe target task. Optionally, both the VQ-VAE encoder and decoder can rely on the task input (RGB\nimage for depth estimation and segmentation, grayscale image for colorization) as side information\nor \u201ccontext\u201d, which was found beneficial for some tasks. In the second stage, an encoder-decoder\ntransformer is trained to predict the dense label as quantized tokens produced by the VQ-VAE en-\ncoder, given the task input. For inference, a code is sampled autoregressively using the transformer\nconditioned on the input and then fed to the VQ-VAE decoder. The architecture is shared for the\nthree tasks, but different weights are learned for each task.\n4.2\nCHARACTERISTICS AND TRADE-OFFS FOR VQ AND FSQ REPRESENTATIONS\nWe start with a study, where we train MaskGIT models on lower resolution 128 \u00d7 128 ImageNet\nimages and for shorter time compared to the paper Chang et al. (2022) (100 epochs for Stage I,\n200 epochs for Stage II. Please see Appendix A.4.1 for more hyperparameters). This allows us to\nsweep the codebook size and other hyperparameters. For VQ, we use the auxiliary entropy loss from\nMaskGIT, that aims to increase the entropy of the codebook (to increase utilization). We only sweep\nthe codebook size. For FSQ, we explore various d and Li to match these codebook sizes.\nWe track the following metrics: Reconstruction FID, the FID obtained by the GAN-trained au-\ntoencoder when the 50k validation images are fed through the quantized autoencoder. This is the\nFID that the Stage II transformer would achieve if it would perfectly model the data. We use the\nwell established ADM TensorFlow Suite (Dhariwal & Nichol, 2023), which computes FID from 50k\nreconstructions w.r.t. the training set. Codebook Usage: The fraction of the codewords that are used\nat least once when encoding the validation set.\nWith the transformer trained in Stage II, we additionally report Sampling FID, the FID obtained\nwhen decoding representations \u02c6z sampled (class-conditionally) with the transformer. We addition-\n5\nModel\nSource\nCFG\nSampling FID\u2020\u2193\nPrecision\u2020\u2191\nRecall\u2020\u2191\nUsage\u2191\nMaskGIT (VQ)\nOurs\n0.1\n4.509\n0.860\n0.465\n81%\nMaskGIT (FSQ)\nOurs\n0.2\n4.534\n0.864\n0.453\n100%\nMaskGIT (VQ)\nGitHub\n-\n4.916\n0.836\n0.489\nADM (Dhariwal & Nichol, 2021)\n1.5\n4.59\n0.83\n0.52\n0.45\n0.50\n0.55\n0.60\nRecall\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nPrecision\n= 0\n= 0\nMaskGIT (FSQ)\nMaskGIT (VQ)\nMaskGIT (github)\nADM\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nCFG Weight  (inference)\n4.6\n4.8\n5.0\n5.2\n5.4\n5.6\nSampling FID\nFigure 4:\nMASKGIT results on ImageNet 256. Top: We show the best classifier-free guidance\n(CFG) setting for each MaskGIT model. As a reference, we show the well established diffusion\nbased ADM model (Dhariwal & Nichol, 2021). Bottom Left: Precision vs. Recall for various CFG\nweights. Bottom Right: Sampling FID for various CFG weights. We show ADM as a horizontal\nline, because the CFG weight 1.5 used for ADM is not comparable with our \u03b1 in absolute terms.\n\u2020We use the ADM TensorFlow Suite to evaluate all shown models, see text.\nally propose studying Compression Cost as a proxy for how hard it is to model the discrete distri-\nbution underlying the representations (i.e., modelling complexity): Note that any transformer that\npredicts a distribution over discrete codes can be used to losslessly compress the corresponding rep-\nresentation. For masked transformers, the only requirement is a deterministic masking schedule,\nthat gradually uncovers the input. Using such a schedule, we can compress any \u02c6z to bits, by pairing\nthe transformer outputs with entropy coding. We use the deterministic masking schedule employed\nin M2T (Mentzer et al., 2023) and refer to Section 1 in that work for further details on the theory.\n4.3\nMASKGIT\nWe train MaskGIT models on ImageNet 256 based on the public GitHub code, training Stage I for\n1M steps with batch size 512, and Stage II for 2.5M steps with batch size 256. For inference, we\nuse 12 steps with the cosine to sample an image. Initial experiments with the public code showed a\nslight instability in the Stage II transformer loss, which we were able to mitigate by lower bounding\nthe minimal masking ratio used during training. Please see Appendix A.4.3 for further details and\nhyper parameters. We train VQ with codebook size 1024 (10 bits) and the entropy loss, as in the\npublished model. For FSQ, we use L = [8, 5, 5, 5] as suggested in Tab. 1.\nFollowing the paper, we report Sampling FID as well as Precision and Recall (Sajjadi et al., 2018)\nto assess the quality of the generative model. Additionally, we also report Codebook usage. We\nagain use the well-established ADM TensorFlow Suite, leading to an (ADM-)-FID-train of 4.916 for\nthe official checkpoint published in the MaskGIT GitHub, vs. 6.19 reported in the MaskGIT paper.\nEarly experiments showed that FSQ lands at a different Precision & Recall point compared to VQ\n(FSQ had higher recall, lower precision). Inspired by the diffusion literature, we thus add classifier\nfree guidance (CFG) (Ho & Salimans, 2022) to MaskGIT: During training, we replace 10% of\nthe class labels with the MASK token to let the model learn the unconditional distribution. During\ninference, we interpolate logits: Let lc be the logits obtained when conditioning on the class label\nc, and l\u2205 be unconditional logits. During inference, we compute new logits l\u2032 = lc + \u03b1(lc \u2212 l\u2205),\nwhere \u03b1 is the CFG inference weight. Intuitively, this pulls the predicted distribution towards the\n6\nFigure 5: Non-cherry-picked samples from our FSQ (top) and VQ (bottom) MaskGIT models for\n4 imagenet classes (330, 320, 510, 454). We show two samples per model per category. Both\nmodels get very comparable sample quality, as reflected by the metrics in Fig. 4.\nunconditional one. We emphasize that this has previously been explored in the context of masked\ntransformers, e.g., by (Chang et al., 2023, Sec. 2.7).\n4.4\nUVIM\nWe retrain the public UViM GitHub code for all three tasks (panoptic segmentation, depth estima-\ntion, colorization). As in the paper, we train each Stage II transformer 3 times, and report averaged\nmetrics. For VQ, we use 4096 codewords (12 bits), and we use the codebook splitting (described\nbelow), as in the published results. We obtain similar metrics to what is reported in the GitHub repo,\nsee Sec. 5. For FSQ, we use L = [7, 5, 5, 5, 5] from Tab. 1.\nFollowing the UViM paper, we report panoptic quality (PQ) for panoptic segmentation, RMSE for\ndepth estimation, and FID-5k for colorization. For all tasks, we use the evaluation suite provided\nby the UViM github repository. We refer to (Kolesnikov et al., 2022) for more details on these tasks\nand corresponding data sets.\nWe ablate the effect of VAE context input (i.e., the RGB image, see above) on the performance\nof VQ and FSQ in the panoptic segmentation task. Further, we investigate the codebook splitting\nemployed by UViM to avoid unused codewords in VQ-VAE. Specifically, they adopt the algorithm\nfrom Linde et al. (1980), where throughout training, unused vectors are detected. These are then\nreplaced by splitting most frequently used embeddings into two new embeddings, adding noise to\neach. Since we observe training instabilities when deactivating codebook splitting in the panoptic\nsegmentation task, we use the depth estimation task for this ablation.\n5\nRESULTS\n5.1\nTRADEOFF STUDY\nIn Fig. 3 we show the results for the trade-off study. On the x-axis, we always show the codebook\nsize |C|, representing the maximal amount of information the codebook can store. We observe the\nfollowing:\nCodebook size correlates with Reconstruction FID for FSQ\nIn Fig. 3 a), we see that as we\nincrease the codebook size, the reconstruction FID for FSQ keeps improving. This is what one\nwould expect from a compression perspective: as we have more bits to store information, we should\nget better reconstruction metrics. However, we see that VQ struggles with utilizing large codebooks\n(despite entropy regularization of the codes), and reconstruction FID achieves a minimum at 211\ncodes, co-inciding with the point where the codebook usage starts decreasing (cf. Fig. 3 c)). We\nnote that for low codebook sizes (Fig. 3 a), left), VQ marginally outperforms FSQ, likely owning to\nthe its more expressive nature (see Contribution 3 in the Section 1).\nFSQ gets better Sampling FID\nA similar picture emerges in Fig. 3 b), where we see that the\nbetter Stage I behavior of FSQ translates to better Sampling FID as we scale the codebook.\nFSQ gets high codebook usage\nIn Fig. 3 c) we see that FSQ uses almost all codewords for a\ncodebook size of 214=16k, without employing any tricks. At the same time, VQ starts dropping\n7\nNYU Depth v2\nSource\nRMSE\u2020 \u2193\nCodebook Usage\nUViM (VQ)\nOurs\n0.468 \u00b1 0.012\n99%\nUViM (FSQ)\nOurs\n0.473 \u00b1 0.012\n99%\nUViM (VQ without splitting)\nOurs\n0.490 \u00b1 0.0037\n0.78%\nUViM (VQ)\nGitHub\n0.463\nDenseDepth (Alhashim & Wonka, 2018)\n0.465\nCOCO Panoptic\nSource\nPQ\u2020 \u2191\nCodebook Usage\nUViM (VQ)\nOurs\n43.4 \u00b1 0.0008\n100%\nUViM (FSQ)\nOurs\n43.2 \u00b1 0.0014\n100%\nUViM (VQ without context)\nOurs\n39.0 \u00b1 0.0023\n99%\nUViM (FSQ without context)\nOurs\n40.2 \u00b1 0.0019\n99%\nUViM (VQ)\nGitHub\n43.1\nDETR-R101 (Carion et al., 2020)\n45.1\nImageNet Colorization\nSource\nFID-5k\u2020 \u2193\nCodebook Usage\nUViM (VQ)\nOurs\n16.90 \u00b1 0.056\n100%\nUViM (FSQ)\nOurs\n17.55 \u00b1 0.057\n100%\nUViM (VQ)\nGithub\n16.99 \u00b1 0.057\nColTran (Kumar et al., 2021)\n19.37\nTable 2: UVIM results for the three tasks. For each, we show results in the corresponding metric\naveraged over three runs with std. dev. (as in UViM). We show the numbers reported by the reference\nGitHub repository, as well as one well established baseline per task. For our models, we show\nCodebook usage. For Depth Estimation, we train an ablation where we do not employ the codebook\nsplitting in VQ. Overall, FSQ obtains competitive but marginally worse results on all tasks. \u2020We use\nthe UViM GitHub evaluation suite.\nbelow 50% usage for codebooks larger than 211 and is not able to utilize more than 210 codewords\nfor larger codebooks. In contrast, for FSQ usage continues growing with more than 215 codewords\nutilized for a codebook of size 216.\nDiminishing gains from codebook scaling\nOne might wonder whether just scaling the codebook\nsize more would lead to ever lower sampling FID. However, as shown in Fig. 3 d), the compression\ncost of the representation keeps increasing. This indicates that the quantized representations get\nmore complex to model for the transformer. Indeed, we see in Fig. 3 b) that the Sampling FID\nsaturates for FSQ starting when using about 212 codewords. We note that in general, for this task,\nthe discrete distribution underlying the FSQ representations are slightly harder to model (as seen by\nthe higher Compression Cost when training the same transformer on different VAEs, Fig. 3 d)). We\nalso note how the Compression Cost for VQ correlates with the codebook usage: when the usage\ndrops, the code becomes easier to model again. Similarly, within a model group (i.e., considering\nonly FSQ or VQ models), the compression cost is anti-correlated with sampling FID.\nSelecting the number of levels per channel L\nIn Appendix A.4.1 we also show the effect of\ndifferent L on the Sampling FID. We find that Li < 5 leads to subpar performance.\n5.2\nMASKGIT\nIn Fig. 4 we show the metrics for MaskGIT on 256\u00d7256 ImageNet. We sweep the CFG weight for\nboth VQ and FSQ. The following can be observed:\nFSQ and VQ achieve comparable metrics and visual results\nFig. 4 shows that both quantizers\nachieve very comparable FID, as well as precision and recall. To put the numbers in context, we\nshow the well established diffusion-based ADM model (Dhariwal & Nichol, 2021). When inspect-\ning the visual results in Fig. 5, we see that both quantizers lead to qualitatively similar samples.\nMotivated by the tradeoff study (sec. 5.1), we explored a larger codebook for these models, but did\nnot observe further gains.\n8\nInput\nGround Truth\nFSQ\nVQ w/o code splitting\nVQ\nFigure 6:\nSamples from UViM for the depth estimation task. Other tasks in Appendix A.2. We\nobserve that VQ and FSQ lead to comparable samples. VQ without splitting leads to jagged edges.\nSemantics\nIt is commonly argued in the literature that the codebook in VQ-VAEs and VQ-GANs\nlearns semantically meaningful codes. Yet, we see that we get similar samples from both VQ and\nFSQ, even though FSQ does not learn an explicit codebook (and thus has less parameters). We\nperformed a small study to see whether either representation is more semantically meaningful than\nthe other, shown in Appendix A.3. We found no evidence that a particular code represents a fixed\nvisual concept in either quantizer. Indeed, both behave very similary in that study.\nPrecision-Recall trade-offs\nNote that precision is a measure for the \u201cquality\u201d of the samples,\nwhile recall measures the proportion of the true distribution that is covered by the samples (Sajjadi\net al., 2018). When we sweep the CFG weight \u03b1 during inference, we obtain models that cover a\nvery similar space in Precision & Recall (bottom, left), and that obtain very similar minimal FID\n(bottom, right).\n5.3\nUVIM\nTable 2 shows the results for the three tasks trained with UViM along with some baselines from the\nliterature.\nFSQ is competitive with VQ on all tasks\nWe can see that across all tasks, FSQ obtains compet-\nitive metrics compared to VQ. This is also reflected in the visual results shown in Fig. 6 (for depth\nestimation) and App. A.2 (for panoptic segementation and colorization).\nFSQ performs better in absence of side information (context)\nTable 2 also shows removing the\nVAE context in UViM (panoptic segmentation), i.e., removing the original RGB image input to the\nVAE encoder and decoder (see Sec. 4.1). In this setting, both the FSQ and VQ-based models obtain\nlower PQ numbers than with context, but the performance of the FSQ-based model degrades less.\nFSQ does not rely on codebook splitting\nWe explore disabling the codebook splitting on the\nNYU Depth task, and we observe signficantly worse RMSE, while Codebook usage drops by more\nthan two orders of magnitude to 0.78%. In the predictions, we observe jagged edges, see Fig. 6\n(right most column). At the same time, FSQ does not rely on any auxiliary algorithms to obtain\n99% codebook usage.\n6\nCONCLUSION\nIn this work, we showed that we can replace the vector quantizer in VQ-VAEs with a simple\nscalar quantization scheme, where the representation is projected to very few dimensions which\nare bounded and rounded. We studied and compared the behavior of FSQ and VQ as a function of\nthe codebook size and observed that FSQ achieves much better codebook utilization for large code-\nbook sizes. Despite the much more constrained setup, we were able to obtain comparable metrics\non image generation with MaskGIT, and dense computer vision tasks with UViM. We hope future\nwork will explore FSQ in even more applications.\nAcknowledgements\nWe thank Andr\u00b4e Susano Pinto, Basil Mustafa and Alexander Kolesnikov for\nthe feedback on the text and method, as well as for insightful discussions.\n9\nReproducibility\nWe refer to Section A.1 for reference code.\nEthics Statement\nThis work proposes a drop-in replacement for VQ, and can thus be applied\nin all domains where VQ is used. A domain where care w.r.t. biases has to be taken is generative\nmodels. However, no new ethical concern arises from our method that would not be a concern for\nVQ-based methods.\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multi-\nmodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan\nZhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for gen-\nerative mixed-modal language models. arXiv preprint arXiv:2301.03728, 2023.\nEirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Gener-\native adversarial networks for extreme learned image compression. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 221\u2013231, 2019.\nIbraheem Alhashim and Peter Wonka. High quality monocular depth estimation via transfer learn-\ning. arXiv preprint arXiv:1812.11941, 2018.\nAlexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of\ndiscrete speech representations. In International Conference on Learning Representations, 2019.\nJohannes Ball\u00b4e, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.\narXiv preprint arXiv:1611.01704, 2016.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.\nIn International Conference on Learning Representations, 2021.\nYoshua Bengio, Nicholas L\u00b4eonard, and Aaron Courville.\nEstimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:\n//github.com/google/jax.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European conference on\ncomputer vision, pp. 213\u2013229. Springer, 2020.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative\nimage transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 11315\u201311325, 2022.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\neration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nTing Chen, Lala Li, and Yizhou Sun. Differentiable product quantization for end-to-end embedding\ncompression. In International Conference on Machine Learning, pp. 1617\u20131626. PMLR, 2020.\nZhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with\ndiscretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 7939\u20137948, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n10\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021.\nPrafulla Dhariwal and Alexander Nichol.\nADM TensorFlow Suite, 2023.\nURL https://\ngithub.com/openai/guided-diffusion/tree/main/evaluations.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\nSander Dieleman, Charlie Nash, Jesse Engel, and Karen Simonyan. Variable-rate discrete represen-\ntation learning. arXiv preprint arXiv:2103.06089, 2021.\nChris Donahue, Ian Simon, and Sander Dieleman. Piano genie. In Proceedings of the 24th Interna-\ntional Conference on Intelligent User Interfaces, pp. 160\u2013164, 2019.\nAlaaeldin El-Nouby, Matthew J Muckley, Karen Ullrich, Ivan Laptev, Jakob Verbeek, and Herv\u00b4e\nJ\u00b4egou.\nImage compression with product quantized masked image modeling.\narXiv preprint\narXiv:2212.07372, 2022.\nPatrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming transformers for high-resolution image\nsynthesis. 2021 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 12868\u201312878, 2020.\nRobert Gray. Vector quantization. IEEE Assp Magazine, 1(2):4\u201329, 1984.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nKyle Hsu, Will Dorrell, James CR Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement via\nlatent quantization. arXiv preprint arXiv:2305.18378, 2023.\nMengqi Huang, Zhendong Mao, Quan Wang, and Yongdong Zhang. Not all image regions matter:\nMasked vector quantization for autoregressive image generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 2002\u20132011, 2023.\nMinyoung Huh, Brian Cheung, Pulkit Agrawal, and Phillip Isola. Straightening out the straight-\nthrough estimator: Overcoming optimization challenges in vector quantized networks.\narXiv\npreprint arXiv:2305.08842, 2023.\nSungwoong Kim, Daejin Jo, Donghoon Lee, and Jongmin Kim. Magvlt: Masked generative vision-\nand-language transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 23338\u201323348, 2023.\nAlexander Kolesnikov, Andr\u00b4e Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and\nNeil Houlsby. Uvim: A unified modeling approach for vision with learned guiding codes. Ad-\nvances in Neural Information Processing Systems, 35:26295\u201326308, 2022.\nManoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. Colorization transformer. arXiv preprint\narXiv:2102.04432, 2021.\nAdrian \u0141a\u00b4ncucki, Jan Chorowski, Guillaume Sanchez, Ricard Marxer, Nanxin Chen, Hans JGA\nDolfing, Sameer Khurana, Tanel Alum\u00a8ae, and Antoine Laurent. Robust training of vector quan-\ntized bottleneck models. In 2020 International Joint Conference on Neural Networks (IJCNN),\npp. 1\u20137. IEEE, 2020.\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image\ngeneration using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11523\u201311532, 2022.\nTianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage:\nMasked generative encoder to unify representation learning and image synthesis. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2142\u20132152, 2023.\nYoseph Linde, Andres Buzo, and Robert Gray. An algorithm for vector quantizer design. IEEE\nTransactions on communications, 28(1):84\u201395, 1980.\n11\nGuo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao.\nDvc: An\nend-to-end deep video compression framework. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 11006\u201311015, 2019.\nFabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Condi-\ntional probability models for deep image compression. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 4394\u20134402, 2018.\nFabian Mentzer, George D Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity gen-\nerative image compression.\nAdvances in Neural Information Processing Systems, 33:11913\u2013\n11924, 2020.\nFabian Mentzer, Eirikur Agustsson, and Michael Tschannen. M2t: Masking transformers twice for\nfaster decoding. arXiv preprint arXiv:2304.07313, 2023.\nDavid Minnen, Johannes Ball\u00b4e, and George D Toderici. Joint autoregressive and hierarchical priors\nfor learned image compression. Advances in neural information processing systems, 31, 2018.\nAurko Roy, Ashish Vaswani, Arvind Neelakantan, and Niki Parmar. Theory and experiments on\nvector quantized autoencoders. arXiv preprint arXiv:1805.11063, 2018.\nMehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing\ngenerative models via precision and recall. Advances in neural information processing systems,\n31, 2018.\nYuhta Takida, Takashi Shibuya, WeiHsiang Liao, Chieh-Hsin Lai, Junki Ohmura, Toshimitsu Ue-\nsaka, Naoki Murata, Shusuke Takahashi, Toshiyuki Kumakura, and Yuki Mitsufuji. Sq-vae: Vari-\national bayes on discrete representation with self-annealed stochastic quantization. arXiv preprint\narXiv:2205.07547, 2022.\nLucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Husz\u00b4ar. Lossy image compression with\ncompressive autoencoders. arXiv preprint arXiv:1703.00395, 2017.\nMichael Tschannen, Eirikur Agustsson, and Mario Lucic. Deep generative models for distribution-\npreserving lossy compression. Advances in Neural Information Processing Systems, 31, 2018.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\nlength video generation from open domain textual descriptions. In International Conference on\nLearning Representations, 2022.\nWill Williams, Sam Ringer, Tom Ash, David MacLeod, Jamie Dougherty, and John Hughes. Hier-\narchical quantized autoencoders. Advances in Neural Information Processing Systems, 33:4524\u2013\n4535, 2020.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.\narXiv preprint arXiv:2110.04627, 2021.\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec.\nIEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 30:495\u2013507, 2021.\n12\nA\nAPPENDIX \u2014 FINITE SCALAR QUANTIZATION: VQ-VAE MADE SIMPLE\nA.1\nCODE\nWe refer to the MaskGIT GitHub and the UViM GitHub for the model code used in this paper. The\nFSQ method is implemented in full generality for Jax (Bradbury et al., 2018) in the following listing,\nand in the Colab on GitHub.\ndef round_ste(z):\n\"\"\"Round with straight through gradients.\"\"\"\nzhat = jnp.round(z)\nreturn z + jax.lax.stop_gradient(zhat - z)\nclass FSQ:\ndef __init__(self, levels: list[int]):\nself._levels = levels\nself._levels_np = np.asarray(levels)\nself._basis = np.concatenate(\n([1], np.cumprod(self._levels_np[:-1]))\n).astype(np.uint32)\ncodebook_size = np.prod(levels)\nself.implicit_codebook = self.indexes_to_codes(\nnp.arange(codebook_size))\ndef bound(self, z):\n\"\"\"Bound \u2018z\u2018, an array of shape (..., d).\"\"\"\neps = 1e-3\nhalf_l = (self._levels_np - 1) * (1 - eps) / 2\noffset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\nshift = jnp.tan(offset / half_l)\nreturn jnp.tanh(z + shift) * half_l - offset\ndef quantize(self, z):\n\"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\nquantized = round_ste(self.bound(z))\nhalf_width = self._levels_np // 2\n# Renormalize to [-1, 1].\nreturn quantized / half_width\ndef _scale_and_shift(self, zhat_normalized):\nhalf_width = self._levels_np // 2\nreturn (zhat_normalized * half_width) + half_width\ndef _scale_and_shift_inverse(self, zhat):\nhalf_width = self._levels_np // 2\nreturn (zhat - half_width) / half_width\ndef codes_to_indexes(self, zhat):\n\"\"\"Converts a \u2018code\u2018 to an index in the codebook.\"\"\"\nassert zhat.shape[-1] == len(self._levels)\nzhat = self._scale_and_shift(zhat)\nreturn (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\ndef indexes_to_codes(self, indices):\n\"\"\"Inverse of \u2018indexes_to_codes\u2018.\"\"\"\nindices = indices[..., jnp.newaxis]\ncodes_non_centered = np.mod(\nnp.floor_divide(indices, self._basis), self._levels_np\n)\nreturn self._scale_and_shift_inverse(codes_non_centered\n13\nInput\nGround Truth\nFSQ\nVQ\nFigure 7: Visualization for panoptic segmentation (first two rows) and colorization (last two rows).\nA.2\nADDITIONAL UVIM VISUALS\nWe show visual results for segmentation and colorization in Fig. 7. Results for depth estimation are\nin Fig. 6 in the main text.\n14\nTop\nBottom\nStited Pixels\nStitched FSQ\nStitched VQ\nFigure 8: Analyzing representations: we take two random images A, B from the validation set (first\ntwo columns). We compare stitching the top half of A to the bottom half of B in pixel space (center)\nto stitching the corresponding representations obtained by the FSQ-GAN and VQ-GAN (last two\ncolumns) in latent space. Note how the GAN decoder maps the sharp transitions in representation\nspace to smooth transitions in pixel-space.\nFSQ\nFSQ\nVQ\nVQ\nFigure 9: Analysing \u201cfake\u201d representations: Left 3 columns: randomly sampling codes according to\nthe marginal histogram, for FSQ (top) and VQ (bottom). Right 3 columns: Creating a representation\nsharing code across all spatial location, where we pick the 3 most common codes according to the\nmarginal histogram (left-to-right).\nA.3\nVISUALIZING VQ AND FSQ REPRESENTATIONS\nWe are interested in what the representations of our MaskGIT autoencoders store. In Fig. 9, we\nvisualize \u201caverage\u201d representations: for each autoencoder (FSQ-GAN and VQ-GAN), we create\nmarginal histograms by encoding the entire ImageNet validation set. We then sample 3 16\u00d716\nrepresentations from each histogram, and decode the representation with the resp. decoders. Both\n15\nproduce similar \u201csoup of patches\u201d. We also visualize representations sharing a single code across\nall spatial locations.\nWe further stitch together representations obtained by encoding real images in Fig. 8. We see that\nboth decoders smoothly blend the the stitched representations when decoding to RGB space.\nOverall, this investigation seems to imply that individual codes do not learn very abstract concepts.\nInstead it is the combination of codes decoder weights which determine the final RGB image.\nA.4\nTRAINING DETAILS\nA.4.1\nTRADEOFF STUDY\nWe use MaskGIT and train stages I and II on 128 \u00d7 128 ImageNet. We explore a range of config-\nurations for the quantization levels L in FSQ models and show the results in Fig. 10. We find that\nLi \u2265 5 leads to the best performance. Motivated by this we recommend the following codebook\nsizes for L for FSQ:\n24\n26\n28\n29\n210\n211\n212\n214\n216\n[5, 3]\n[8, 8]\n[8, 6, 5]\n[8, 8, 8]\n[8, 5, 5, 5]\n[8, 8, 6, 5]\n[7, 5, 5, 5]\n[8, 8, 8, 6, 5]\n[8, 8, 8, 5, 5, 5]\nWe use 100 epochs for Stage I, split into \u2248 500k steps of batch size 256, and 200 epochs split into\n\u2248 1M steps for Stage II, also using batch size 256.\nAs mentioned in the main text, we employ a minimal masking ratio to stabilize Stage II training\ndescribed in Sec A.4.2. All other hyperparameters are copied from the vqgan config.py and\nmaskgit class cond config.py configs from the MaskGIT GitHub. We emphasize that for\nVQ we use the entropy loss from MaskGIT with weight 0.1.\nA.4.2\nLOWERBOUNDING THE MASKGIT MASKING RATIO\nMaskGIT uses a cosine schedule to sample masking ratios during training, where first a ratio r \u223c\nU[0, 1] is sampled, and then NM = \u2308cos(\u03c0/2(1 \u2212 r))S\u2309 randomly selected tokens are masked for\neach example in the mini batch. S is the sequence length, which is 162 = 256 for models trained on\nImageNet 256. We found that this causes instability, likely because there are training steps, where\nNM = 1, i.e., only one token is masked, and we only get a loss from the corresponding prediction.\nInstead, we lower-bound r to rmin = 1 \u2212 (arccos(0.45)2/\u03c0), which results in NM > 0.45S for\nevery training step. We later explored various alternatives to 0.45 and found that any value above\n0.2 helps with stabilization, but use 0.45 throughout.\nA.4.3\nMASKGIT ON IMAGENET256\nAgain,\nwe\nbase\nall\nexperiments\non\nthe\nvqgan config.py\nand\nmaskgit class cond config.py configs from the MaskGIT GitHub repo.\nTo speed\nup iteration, we change the VQGAN config to use 1M steps with batch size 512 (for Stage I),\ninstead of 2M steps with batch size 256. We again lower bound the masking ratio as described in\nSec. A.4.2.\n8\n9\n10\n11\n12\n13\n14\n15\n16\nLog of Codebook size [bits]\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\nSampling FID\n3\n4\n5\n8\nFigure 10: Exploring different configurations of quantization levels per channel L. The color and\nmarker indicate the smallest Li used for a given model (see legend).\n16\n"
  },
  {
    "title": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation",
    "link": "https://arxiv.org/pdf/2309.15818.pdf",
    "upvote": "18",
    "text": "SHOW-1: MARRYING PIXEL AND LATENT DIFFUSION\nMODELS FOR TEXT-TO-VIDEO GENERATION\nDavid Junhao Zhang*\nJay Zhangjie Wu*\nJia-Wei Liu*\nRui Zhao\nLingmin Ran\nYuchao Gu\nDifei Gao\nMike Zheng Shou \u0000Show Lab, National University of Singapore\nhttps://showlab.github.io/Show-1/\nClose up of mystic cat, like a buring\nphoenix, red and black colors.\nA panda besides the waterfall is\nholding a sign that says \u201cShow 1\u201d.\nToad practicing karate.\nGiant octopus invades new york\ncity.\nA medieval knight in gleaming ar-\nmor rides confidently on a majestic\nhorse, the sun setting behind them.\nMotorcyclist with space suit riding\non moon with storm and galaxy in\nbackground.\nFigure 1: Given text description, our approach generates highly faithful and photorealistic videos.\nClick the image to play the video clips. Best viewed with Adobe Acrobat Reader.\nABSTRACT\nSignificant advancements have been achieved in the realm of large-scale pre-\ntrained text-to-video Diffusion Models (VDMs).\nHowever, previous methods\neither rely solely on pixel-based VDMs, which come with high computational\ncosts, or on latent-based VDMs, which often struggle with precise text-video\nalignment. In this paper, we are the first to propose a hybrid model, dubbed\nas Show-1, which marries pixel-based and latent-based VDMs for text-to-video\ngeneration. Our model first uses pixel-based VDMs to produce a low-resolution\nvideo of strong text-video correlation.\nAfter that, we propose a novel expert\ntranslation method that employs the latent-based VDMs to further upsample the\nlow-resolution video to high resolution.\nCompared to latent VDMs, Show-1\ncan produce high-quality videos of precise text-video alignment; Compared to\npixel VDMs, Show-1 is much more efficient (GPU memory usage during in-\nference is 15G vs 72G). We also validate our model on standard video gen-\neration benchmarks.\nOur code and model weights are publicly available at\nhttps://github.com/showlab/Show-1.\n*Equal Contribution. \u0000 Corresponding Author.\n1\narXiv:2309.15818v2  [cs.CV]  17 Oct 2023\n1\nINTRODUCTION\nPixel-based VDM\n64 x 40\nLatent-based VDM\n 64 x 40\nLatent-based VDM\n256 x 160\nA blue tiger in the grass in the sunset, surrounded by butterflies.\n\u201cA wolf drinking coffee in a caf\u00e9.\u201d\nPixel-based VDM\n64 x 40\nLatent-based VDM\n 64 x 40\nLatent-based VDM\n256 x 160\nFigure 2: Text-Video alignment comparisons among pixel-based VDM at low resolution, latent-\nbased VDM at low resolution and latent-based at relatively high resolution.\nRemarkable progress has been made in developing large-scale pre-trained text-to-Video Diffusion\nModels (VDMs), including closed-source ones (e.g., Make-A-Video (Singer et al., 2022), Imagen\nVideo (Ho et al., 2022a), Video LDM (Blattmann et al., 2023a), Gen-2 (Esser et al., 2023)) and open-\nsourced ones (e.g., VideoCrafter (He et al., 2022), ModelScopeT2V (Wang et al., 2023a). These\nVDMs can be classified into two types: (1) Pixel-based VDMs that directly denoise pixel values,\nincluding Make-A-Video (Singer et al., 2022), Imagen Video (Ho et al., 2022a), PYoCo (Ge et al.,\n2023), and (2) Latent-based VDMs that manipulate the compacted latent space within a variational\nautoencoder (VAE), like Video LDM (Blattmann et al., 2023a) and MagicVideo (Zhou et al., 2022).\nHowever, both of them have pros and cons. Pixel-based VDMs can generate motion accurately\naligned with the textual prompt but typically demand expensive computational costs in terms of time\nand GPU memory, especially when generating high-resolution videos. Latent-based VDMs are\nmore resource-efficient because they work in a reduced-dimension latent space. But it is challenging\nfor such small latent space (e.g., 8\u00d75 for 64\u00d740 videos) to cover rich yet necessary visual semantic\ndetails as described by the textual prompt. Therefore, as shown in Fig. 2, the generated videos often\nare not well-aligned with the textual prompts. On the other hand, if the generated videos are of\nrelatively high resolution (e.g., 256 \u00d7 160 videos), the latent model will focus more on spatial\nappearance but may also ignore the text-video alignment.\nTo marry the strength and alleviate the weakness of pixel-based and latent-based VDMs, we intro-\nduce Show-1, an efficient text-to-video model that generates videos of not only decent video-text\nalignment but also high visual quality. Further, Show-1 can be trained on large-scale datasets with\nmanageable computational costs. Specifically, we follow the conventional coarse-to-fine video gen-\neration pipeline (Ho et al., 2022a; Blattmann et al., 2023a) which starts with a module to produce\nkeyframes at a low resolution and a low frame rate. Then we employs a temporal interpolation\nmodule and super-resolution module to increase temporal and spatial quality respectively.\nIn these modules, prior studies typically employ either pixel-based or latent-based VDMs across\nall modules. While purely pixel-based VDMs tend to have heavy computational costs, exclusively\nlatent-based VDMs can result in poor text-video alignment and motion inconsistencies. In contrast,\nwe combine them into Show-1 as shown in Fig. 3. To accomplish this, we employ pixel-based VDMs\nfor the keyframe module and the temporal interpolation module at a low resolution, producing key\nframes of precise text-video alignment and natural motion with low computational cost. Regarding\nsuper-resolution, we find that latent-based VDMs, despite their inaccurate text-video alignment, can\nbe re-purposed to translate low-resolution video to high-resolution video, while maintaining the\noriginal appearance and the accurate text-video alignment of low-resolution video. Inspired by this\nfinding, for the first time, we propose a novel two-stage super-resolution module that first employs\npixel-based VDMs to upsample the video from 64 \u00d7 40 to 256 \u00d7 160 and then design a novel\nexpert translation module based on latent-based VDMs to further upsample it to 572\u00d7320 with low\ncomputation cost.\nIn summary, our paper makes the following key contributions:\n2\nLow-resolution Pixel-based Diffusion\na) Key Frame Generation\n\u2026\nUNet\n(base)\n\u201cToad practicing \nkarate.\u201d\n\u2026\nUNet\n(interp.)\n\u2026\nb) Frame Interpolation\nUNet\n(super-res)\nc) Super Resolution\n\u2026\nHigh-resolution Latent-based Diffusion\nUNet\n(super-res)\nEncoder\n\u2026\nDecoder\nd) Super Resolution\nFigure 3: Overview of Show-1. Pixel-based VDMs produce videos of lower resolution with better\ntext-video alignment, while latent-based VDMs upscale these low-resolution videos from pixel-\nbased VDMs to then create high-resolution videos with low computation cost.\n\u2022 Upon examining pixel and latent VDMs, we discovered that: 1) pixel VDMs excel in gen-\nerating low-resolution videos with more natural motion and superior text-video synchro-\nnization compared to latent VDMs; 2) when using the low-resolution video as an initial\nguide, conventional latent VDMs can effectively function as super-resolution tools by sim-\nple expert translation, refining spatial clarity and creating high-quality videos with greater\nefficiency than pixel VDMs.\n\u2022 We are the first to integrate the strengths of both pixel and latent VDMs, resulting into a\nnovel video generation model that can produce high-resolution videos of precise text-video\nalignment at low computational cost (15G GPU memory during inference).\n\u2022 Our approach achieves state-of-the-art performance on standard benchmarks including\nUCF-101 and MSR-VTT.\n2\nPREVIOUS WORK\nText-to-image generation. (Reed et al., 2016) stands as one of the initial methods that adapts\nthe unconditional Generative Adversarial Network (GAN) introduced by (Goodfellow et al., 2014)\nfor text-to-image (T2I) generation. Later versions of GANs delve into progressive generation, as\nseen in (Zhang et al., 2017) and (Hong et al., 2018). Meanwhile, works like (Xu et al., 2018)\nand (Zhang et al., 2021) seek to improve text-image alignment. Recently, diffusion models have\ncontributed prominently to advancements in text-driven photorealistic and compositional image\nsynthesis (Ramesh et al., 2022; Saharia et al., 2022). For attaining high-resolution imagery, two\nprevalent strategies emerge. One integrates cascaded super-resolution mechanisms within the RGB\ndomain (Nichol et al., 2021; Ho et al., 2022b; Saharia et al., 2022; Ramesh et al., 2022). In contrast,\nthe other harnesses decoders to delve into latent spaces (Rombach et al., 2022; Gu et al., 2022).\nOwing to the emergence of robust text-to-image diffusion models, we are able to utilize them as\nsolid initialization of text to video models.\nText-to-video generation.\nPast research has utilized a range of generative models, including\nGANs (Vondrick et al., 2016; Saito et al., 2017; Tulyakov et al., 2018; Tian et al., 2021; Shen et al.,\n2023), Autoregressive models (Srivastava et al., 2015; Yan et al., 2021; Le Moing et al., 2021; Ge\net al., 2022; Hong et al., 2022), and implicit neural representations (Skorokhodov et al., 2021; Yu\net al., 2021). Inspired by the notable success of the diffusion model in image synthesis, several recent\nstudies have ventured into applying diffusion models for both conditional and unconditional video\nsynthesis (Voleti et al., 2022; Harvey et al., 2022; Zhou et al., 2022; Wu et al., 2022b; Blattmann\net al., 2023b; Khachatryan et al., 2023; H\u00a8oppe et al., 2022; Voleti et al., 2022; Yang et al., 2022;\nNikankin et al., 2022; Luo et al., 2023; An et al., 2023; Wang et al., 2023b). Several studies have\ninvestigated the hierarchical structure, encompassing separate keyframes, interpolation, and super-\nresolution modules for high-fidelity video generation. Magicvideo (Zhou et al., 2022) and Video\nLDM (Blattmann et al., 2023a) ground their models on latent-based VDMs. On the other hand,\nPYoCo (Ge et al., 2023), Make-A-Video (Singer et al., 2022), and Imagen Video (Ho et al., 2022a)\nanchor their models on pixel-based VDMs. Contrary to these approaches, our method seamlessly\nintegrates both pixel-based and latent-based VDMs.\n3\n3\nShow-1\n3.1\nPRELIMINARIES\nDenoising Diffusion Probabilistic Models (DDPMs). DDPMs, as detailed in (Ho et al., 2020), rep-\nresent generative frameworks designed to reproduce a consistent forward Markov chain x1, . . . , xT .\nConsidering a data distribution x0 \u223c q(x0), the Markov transition q(xt|xt\u22121) is conceptualized as\na Gaussian distribution, characterized by a variance \u03b2t \u2208 (0, 1). Formally, this is defined as:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI),\nt = 1, . . . , T.\n(1)\nApplying the principles of Bayes and the Markov characteristic, it\u2019s feasible to derive the conditional\nprobabilities q(xt|x0) and q(xt\u22121|xt, x0), represented as:\nq(xt|x0) = N(xt; \u221a\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I),\nt = 1, . . . , T,\n(2)\nq(xt\u22121|xt, x0) = N(xt\u22121; \u02dc\u00b5t(xt, x0), \u02dc\u03b2tI),\nt = 1, . . . , T,\n(3)\nwhere \u03b1t = 1 \u2212 \u03b2t, \u00af\u03b1t = Qt\ns=1 \u03b1s, \u02dc\u03b2t = 1\u2212\u00af\u03b1t\u22121\n1\u2212\u00af\u03b1t \u03b2t, \u02dc\u00b5t(xt, x0) =\n\u221a\u00af\u03b1t\u03b2t\n1\u2212\u00af\u03b1t x0 +\n\u221a\u03b1t(1\u2212\u00af\u03b1t\u22121)\n1\u2212\u00af\u03b1t\nxt.\nIn order to synthesize the chain x1, . . . , xT , DDPMs utilize a reverse approach, characterized by a\nprior p(xT ) = N(xT ; 0, I) and Gaussian transitions. This relation is:\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)),\nt = T, . . . , 1.\n(4)\nThe model\u2019s adaptable parameters \u03b8 are optimized to ensure the synthesized reverse sequence aligns\nwith the forward sequence.\nIn their essence, DDPMs adhere to the variational inference strategy, focusing on enhancing the\nvariational lower bound of the negative log-likelihood. Given the KL divergence among Gaussian\ndistributions, this approach is practical. In practice, this framework resembles a series of weight-\nshared denoising autoencoders \u03f5\u03b8(xt, t), trained to render a cleaner version of their respective input\nxt. This is succinctly represented by: Ex,\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2\n\u0003\n.\nUNet architecture for text to image model. The UNet model is introduced by (Spr, 2015) for\nbiomedical image segmentation. Popular UNet for text-to-image diffusion model usually contains\nmultiple down, middle, and up blocks. Each block consists of a resent2D layer, a self-attention\nlayer, and a cross-attention layer. Text condition c is inserted into to cross-attention layer as keys\nand values. For a text-guided Diffusion Model, with the text embedding c the objective is given by:\nEx,\u03f5\u223cN (0,1),t,c\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t, c)\u22252\n2\n\u0003\n.\n(5)\n3.2\nTURN IMAGE UNET TO VIDEO\nWe incorporate the spatial weights from a robust text-to-image model. To endow the model with\ntemporal understanding and produce coherent frames, we integrate temporal layers within each UNet\nblock. Specifically, after every Resnet2D block, we introduce a temporal convolution layer consist-\ning of four 1D convolutions across the temporal dimension. Additionally, following each self and\ncross-attention layer, we implement a temporal attention layer to facilitate dynamic temporal data\nassimilation. Specifically, a frame-wise input video x \u2208 RT \u00d7C\u00d7H\u00d7W , where C is the number of\nchannels and H and W are the spatial latent dimensions. The spatial layers regard the video as a\nbatch of independent images (by transposing the temporal axis into the batch dimension), and for\neach temporal layer, the video is reshaped back to temporal dimensions.\n3.3\nPIXEL-BASED KEYFRAME GENERATION MODEL\nGiven a text input, we initially produce a sequence of keyframes using a pixel-based Video UNet at\na very low spatial and temporal resolution. This approach results in improved text-to-video align-\nment. The reason for this enhancement is that we do not require the keyframe modules to prioritize\nappearance clarity or temporal consistency. As a result, the keyframe modules pays more attention\nto the text guidance. The training objective for the keyframe modules is following Eq. 5.\nWhy we choose pixel diffusion over latent diffusion here? Latent diffusion employs an encoder\nto transform the original input x into a latent space. This results in a reduced spatial dimension, for\n4\nResNet2D\nTemporal Conv1D\nSelf A7en8on\nCross A7en8on\nInsert\nInsert\nTemporal A7en8on\n\u2744\n\u2744\n\u2744\n\ud83d\udd25\n\ud83d\udd25\na)\nb)\nSuper Resolu*on\nbilinear upsampling\n1\nx!\"#$\nx!\"\nz!\"\nz!\"#$\n1\n0\n0\n0\n0\n0\n0\nInterpola*on\nnoise\ncondi(on\ncondi(on  \nmask\nnoise\ncondi(on\nLast frame\nof previous\nsegment\nFigure 4: 3D UNet and input of the UNet of interpolation and super-resolution modules. (a) shows\nthat how we insert temporal modules into 2D UNet. (b) explains the input for interpolation and first\nsuper-resolution UNet.\nexample, H/8, W/8, while concentrating the semantics and appearance into this latent domain. For\ngenerating keyframes, our objective is to have a smaller spatial dimension, like 64\u00d740. If we opt for\nlatent diffusion, this spatial dimension would shrink further, perhaps to around 8 \u00d7 5, which might\nnot be sufficient to retain ample spatial semantics and appearance within the compacted latent space.\nOn the other hand, pixel diffusion operates directly in the pixel domain, keeping the original spatial\ndimension intact. This ensures that necessary semantics and appearance information are preserved.\nFor the following low resolution stages, we all utilize pixel-based VDMs for the same reason.\n3.4\nTEMPORAL INTERPOLATION MODEL\nTo enhance the temporal resolution of videos we produce, we suggest a pixel-based temporal in-\nterpolation diffusion module. This method iteratively interpolates between the frames produced by\nour keyframe modules. The pixel interpolation approach is built upon our keyframe modules, with\nall parameters fine-tuned during the training process. We employ the masking-conditioning mech-\nanism, as highlighted in (Blattmann et al., 2023a), where the target frames for interpolation are\nmasked. In addition to the original pixel channels C, as shown in Fig. 4 we integrate 4 supplemen-\ntary channels into the U-Net\u2019s input: 3 channels are dedicated to the RGB masked video input, while\na binary channel identifies the masked frames. As depicted in the accompanying figure, during a\nspecific noise timestep, we interpolate three frames between two consecutive keyframes, denoted\nas xi\nt and xi+1\nt\n. For the added 3 channels, values of zi and zi+1 remain true to the original pixel\nvalues, while the interpolated frames are set to zero. For the final mask channel, the mask values\nmi and mi+1 are set to 1, signifying that both the initial and concluding frames are available, with\nall others set to 0. In conclusion, we merge these components based on the channel dimension and\ninput them into the U-Net. For zi and zi+1, we implement noise conditioning augmentation. Such\naugmentation is pivotal in cascaded diffusion models for class-conditional generation, as observed\nby (Ho et al., 2022a), and also in text-to-image models as noted by (He et al., 2022). Specifically,\nthis method aids in the simultaneous training of diverse models in the cascade. It minimizes the vul-\nnerability to domain disparities between the output from one cascade phase and the training inputs\nof the following phase. Let the interpolated video frames be represented by x, \u2208 R4T \u00d7C\u00d7H\u00d7W .\nBased on Eq. 5, we can formulate the updated objective as:\nEx,,z,m,\u03f5\u223cN (0,1),t,c\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8([x,\nt, z, m], t, c)\u22252\n2\n\u0003\n.,\n(6)\n3.5\nSUPER-RESOLUTION AT LOW SPATIAL RESOLUTION\nTo improve the spatial quality of the videos, we introduce a pixel super-resolution approach uti-\nlizing the video UNet. For this enhanced spatial resolution, we also incorporate three additional\nchannels, which are populated using a bilinear upscaled low-resolution video clip, denoted as\nx,,\nu \u2208 R4T \u00d7C\u00d74H\u00d74W through bilinear upsampling. In line with the approach of(Ho et al., 2022c),\n5\nwe employ Gaussian noise augmentation to the upscaled low resolution video condition during its\ntraining phase, introducing a random signal-to-noise ratio. The model is also provided with this\nsampled ratio. During the sampling process, we opt for a consistent signal-to-noise ratio, like 1 or\n2. This ensures minimal augmentation, assisting in the elimination of artifacts from the prior phase,\nyet retaining a significant portion of the structure.\nGiven that the spatial resolution remains at an upscaled version throughout the diffusion process,\nit\u2019s challenging to upscale all the interpolated frames, denoted as x\n\u2032 \u2208 R4T \u00d7C\u00d7H\u00d7W , to x\n\u2032\u2032 \u2208\nR4T \u00d7C\u00d74H\u00d74W simultaneously on a standard GPU with 24G memory. Consequently, we must\ndivide the frames into four smaller segments and upscale each one individually.\nHowever, the continuity between various segments is compromised. To rectify this, as depicted in\nthe Fig. 4, we take the upscaled last frame of one segment to complete the three supplementary\nchannels of the initial frame in the following segment.\n3.6\nSUPER-RESOLUTION AT HIGH SPATIAL RESOLUTION\nThrough our empirical observations, we discern that a latent-based VDM can be effectively utilized\nfor enhanced super-resolution with high fidelity. Specifically, we design a distinct latent-based VDM\nthat is tailored for high-caliber, high-resolution data. We then apply a noising-denoising procedure,\nas outlined by SDEdit (Meng et al., 2021), to the samples from the preliminary phase. As pointed\nout by (Balaji et al., 2022), various diffusion steps assume distinct roles during the generation pro-\ncess. For instance, the initial diffusion steps, such as from 1000 to 900, primarily concentrate on\nrecovering the overall spatial structure, while subsequent steps delve into finer details. Given our\nsuccess in securing well-structured low-resolution videos, we suggest adapting the latent VDM to\nspecialize in high-resolution detail refinement. More precisely, we train a UNet for only the 0 to 900\ntimesteps (with 1000 being the maximum) instead of the typical full range of 0 to 1000, directing\nthe model to be a expert emphasizing high-resolution nuances. This strategic adjustment signifi-\ncantly enhances the end video quality, namely expert translation. During the inference process, we\nuse bilinear upsampling on the videos from the prior stage and then encode these videos into the\nlatent space. Subsequently, we carry out diffusion and denoising directly in this latent space using\nthe latent-based VDM model, while maintaining the same text input. This results in the final video,\ndenoted as x\n\u2032\u2032\u2032 \u2208 R4T \u00d7C\u00d716H\u00d716W .\nWhy we choose latent-based VDM over pixel-based VDM here? Pixel-based VDMs work di-\nrectly within the pixel domain, preserving the original spatial dimensions. Handling high-resolution\nvideos this way can be computationally expensive. In contrast, latent-based VDMs compress videos\ninto a latent space (for example, downscaled by a factor of 8), which results in a reduced computa-\ntional burden. Thus, we opt for the latent-based VDMs in this context.\n4\nEXPERIMENTS\n4.1\nIMPLEMENTATION DETAILS\nFor the generation of pixel-based keyframes, we utilized DeepFloyd1 as our pre-trained Text-to-\nImage model for initialization, producing videos of dimensions 8\u00d764\u00d740\u00d73(T \u00d7H \u00d7W \u00d73). In\nour interpolation model, we initialize the weights using the keyframes generation model and produce\nvideos with dimensions of 29\u00d764\u00d740\u00d73. For our initial model, we employ DeepFloyd\u2019s SR model\nfor spatial weight initialization, yielding videos of size 29 \u00d7 256 \u00d7 160. In the subsequent super-\nresolution model, we modify the ModelScope text-to-video model and use our proposed expert\ntranslation to generate videos of 29 \u00d7 576 \u00d7 320.\nThe dataset we used for training is WebVid-10M (Bain et al., 2021). Training and hyperparameter-\ndetails can be found in appendix Table 5.\n4.2\nQUANTITATIVE RESULTS\nUCF-101 Experiment. For our preliminary evaluations, we employ IS and FVD metrics. UCF-\n101 stands out as a categorized video dataset curated for action recognition tasks. When extracting\n1https://github.com/deep-floyd/IF\n6\nTable 1: Zero-shot text-to-video generation on UCF-101. Our approach achieves competitive results\nin both inception score and FVD metrics.\nMethod\nIS (\u2191)\nFVD (\u2193)\nCogVideo (Hong et al., 2022) (English)\n25.27\n701.59\nMake-A-Video (Singer et al., 2022)\n33.00\n367.23\nMagicVideo (Zhou et al., 2022)\n-\n655.00\nVideo LDM (Blattmann et al., 2023a)\n33.45\n550.61\nVideoFactory (Wang et al., 2023b)\n-\n410.00\nShow-1 (ours)\n35.42\n394.46\nTable 2: Quantitative comparison with state-of-the-art models on MSR-VTT. Our approach achieves\nthe state-of-the-art performance.\nModels\nFID-vid (\u2193)\nFVD (\u2193)\nCLIPSIM (\u2191)\nN \u00a8UWA (Wu et al., 2022a)\n47.68\n-\n0.2439\nCogVideo (Chinese) (Hong et al., 2022)\n24.78\n-\n0.2614\nCogVideo (English) (Hong et al., 2022)\n23.59\n1294\n0.2631\nMagicVideo (Zhou et al., 2022)\n-\n1290\n-\nVideo LDM (Blattmann et al., 2023a)\n-\n-\n0.2929\nMake-A-Video (Singer et al., 2022)\n13.17\n-\n0.3049\nModelScopeT2V (Wang et al., 2023a)\n11.09\n550\n0.2930\nShow-1 (ours)\n13.08\n538\n0.3072\nsamples from the text-to-video model, following PYoCo (Ge et al., 2023), we formulate a series\nof prompts corresponding to each class name, serving as the conditional input. This step becomes\nessential for class names like jump rope, which aren\u2019t intrinsically descriptive. We generate 20 video\nsamples per prompt to determine the IS metric. For FVD evaluation, we adhere to methodologies\npresented in prior studies (Le Moing et al., 2021; Tian et al., 2021) and produce 2,048 videos.\nFrom the data presented in Table 1, it\u2019s evident that Show-1\u2019s zero-shot capabilities outperform or\nare on par with other methods. This underscores Show-1\u2019s superior ability to generalize effectively,\neven in specialized domains. It\u2019s noteworthy that our keyframes, interpolation, and initial super-\nresolution models are solely trained on the publicly available WebVid-10M dataset, in contrast to\nthe Make-A-Video models, which are trained on other data.\nMSR-VTT Experiment. The MSR-VTT dataset (Xu et al., 2016) test subset comprises 2, 990\nvideos, accompanied by 59, 794 captions. Every video in this set maintains a uniform resolution\nof 320 \u00d7 240. We carry out our evaluations under a zero-shot setting, given that Show-1 has not\nbeen trained on the MSR-VTT collection. In this analysis, Show-1 is compared with state-of-the-art\nmodels, on performance metrics including FID-vid (Heusel et al., 2017), FVD (Unterthiner et al.,\n2018), and CLIPSIM (Wu et al., 2021). For FID-vid and FVD assessments, we randomly select\n2,048 videos from the MSR-VTT testing division. CLIPSIM evaluations utilize all the captions\nfrom this test subset, following the approach (Singer et al., 2022). All generated videos consistently\nuphold a resolution of 256 \u00d7 256.\nTable 2 shows that, Show-1 achieves the best performance in both FID-vid (a score of 13.08) and\nFVD (with a score of 538). This suggests a remarkable visual congruence between our generated\nvideos and the original content. Moreover, our model secures a notable CLIPSIM score of 0.3076,\nemphasizing the semantic coherence between the generated videos and their corresponding prompts.\nIt is noteworthy that our CLIPSIM score surpasses that of Make-A-Video (Singer et al., 2022),\ndespite the latter having the benefit of using additional training data beyond WebVid-10M.\nHuman evaluation. We gather an evaluation set comprising 120 prompts that encompass camera\ncontrol, natural scenery, food, animals, people, and imaginative content. The survey is conducted on\nAmazon Mechanical Turk. Following Make a Video (Singer et al., 2022), we assess video quality,\nthe accuracy of text-video alignment and motion fidelity. In evaluating video quality, we present\ntwo videos in a random sequence and inquire from annotators which one possesses superior quality.\nWhen considering text-video alignment, we display the accompanying text and prompt annotators to\n7\nModelScope\nZeroScope\nShow-1\n(Ours)\n\u201cA blue tiger in the grass in the sunset, surrounded by butterflies.\u201d\nModelScope\nZeroScope\nShow-1\n(Ours)\n\u201cToad practicing karate.\u201d\nImagen \nVideo\nShow-1\n(Ours)\n\u201cA panda taking a selfie.\u201d\n\u201cA panda playing on a swing set.\u201d\nMake-A-Video\nShow-1\n(Ours)\nFigure 5: Qualitative comparison with existing video generative models. Words in red highlight the\nmisalignment between text and video in other open-source approaches (i.e., ModelScope and Ze-\nroScope), whereas our method maintains proper alignment. Videos from closed-source approaches\n(i.e., Imagen Video and Make-A-Video) are obtained from their websites.\n8\nTable 3: Human evaluation on state-of-the-art open-sourced text-to-video models.\nVideo Quality\nText-Video alignment\nMotion Fidelity\nOurs vs. ModelScope\n62%vs.38%\n63%vs.37%\n63%vs.37%\nOurs vs. ZeroSope\n62%vs.38%\n58%vs.42%\n59%vs.41%\nTable 4: Comparisons of different combinations of pixel-based and latent-based VDMs in terms of\ntext-video similarity and memory usage during inference.\nLow resolution stage\nHigh resolution stage\nCLIPSIM\nMax memory\nlatent-based\nlatent-based\n0.2934\n15GB\nlatent-based\npixel-based\n\u2013\n72GB\npixel-based\npixel-based\n\u2013\n72GB\nlatent-based\npixel-based\n0.3072\n15GB\nw/ export\ntranslation\nw/o export\ntranslation\n\u201cA married couple embraces in front of a burning house.\u201d\nFigure 6: Qualitative comparison for our expert translation. With expert translation, the visual\nquality is significantly improved.\ndetermine which video aligns better with the given text, advising them to overlook quality concerns.\nFor motion fidelity, we let annotators to determine which video has the most natural notion. As\nshown in Table 3, our method achieves the best human preferences on all evaluation parts.\n4.3\nQUALITATIVE RESULTS\nAs depicted in Fig. 5, our approach exhibits superior text-video alignment and visual fidelity com-\npared to the recently open-sourced ModelScope (Wang et al., 2023a) and ZeroScope2. Additionally,\nour method matches or even surpasses the visual quality of the current state-of-the-art methods,\nincluding Imagen Video and Make-A-Video.\n4.4\nABLATION STUDIES.\nImpact of different combinations of pixel-based and latent-based VDMs. To assess the integra-\ntion method of pixel and latent-based VDMs, we conduct several ablations. For fair comparison, we\nemploye the T5 encoder (Raffel et al., 2020) for text embedding in all low-resolution stages and the\nCLIP text encoder (Radford et al., 2021) for high-resolution stages. As indicated in Tab. 4, utiliz-\ning pixel-based VDMs in the low-resolution stage and latent diffusion for high-resolution upscaling\nresults in the highest CLIP score with reduced computational expenses. On the other hand, imple-\nmenting pixel-based VDMs during the high-resolution upscaling stage demands significant compu-\ntational resources. These findings reinforce our proposition that combining pixel-based VDMs in the\nlow-resolution phase and latent-based VDMs in the high-resolution phase can enhance text-video\nalignment and visual quality while minimizing computational costs.\nImpact of expert translation of latent-based VDM as super-resolution model. We provide visual\ncomparison between models with and without expert translation. As elaborated in Section 3.6, \u201cwith\nexpert translation\u201d refers to training the latent-based VDMs using timesteps 0-900 (with a maximum\n2https://huggingface.co/cerspense/zeroscope-v2-576w\n9\ntimestep of 1000), while \u201cw/o expert translation\u201d involves standard training with timesteps 0-1000.\nAs evident in Fig. 6, the model with expert translation produces videos of superior visual quality,\nexhibiting fewer artifacts and capturing more intricate details.\n5\nCONCLUSION\nWe introduce Show-1, an innovative model that marries the strengths of pixel and latent based\nVDMS. Our approach employs pixel-based VDMs for initial video generation, ensuring precise\ntext-video alignment and motion portrayal, and then uses latent-based VDMs for super-resolution,\ntransitioning from a lower to a higher resolution efficiently. This combined strategy offers high-\nquality text-to-video outputs while optimizing computational costs.\n6\nETHICS STATEMENT\nOur pretrained T2I model, Deep-IF, is trained using web data, and our models utilize WebVid-10M.\nGiven this, there\u2019s a potential for our method to not only learn but also amplify societal biases, which\ncould include inappropriate or NSFW content. To address this, we can integrate the CLIP model to\ndetect NSFW content and filter out such instances.\n7\nREPRODUCIBILITY STATEMENT\nWe take the following steps to guarantee reproducibility: (1) Our codes, along with model weights,\nwill be public available. (2) The training and hyperparameter details can be found in appendix\nTable 5.\nREFERENCES\nJie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-\nshift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint\narXiv:2304.08477, 2023. 3\nMax Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 1728\u20131738, 2021. 6\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten\nKreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. eDiff-I: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n6\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\nand Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 22563\u201322575, 2023a. 2, 3, 5, 7\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\nand Karsten Kreis.\nAlign your latents: High-resolution video synthesis with latent diffusion\nmodels. In CVPR, 2023b. 3\nPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Ger-\nmanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint\narXiv:2302.03011, 2023. 2\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and\nDevi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer.\narXiv preprint arXiv:2204.03638, 2022. 3\nSongwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs,\nJia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior\nfor video diffusion models. arXiv preprint arXiv:2305.10474, 2023. 2, 3, 7\n10\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. NIPS, 2014. 3\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pp. 10696\u2013\n10706, 2022. 3\nWilliam Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible\ndiffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022. 3\nYingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion mod-\nels for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221,\n2022. 2, 5\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017. 7\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:\n6840\u20136851, 2020. 4\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. 2, 3, 5\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sal-\nimans. Cascaded diffusion models for high fidelity image generation. JMLR, 23:47\u20131, 2022b.\n3\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022c. 5\nSeunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for\nhierarchical text-to-image synthesis. In CVPR, pp. 7986\u20137994, 2018. 3\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pre-\ntraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3,\n7\nTobias H\u00a8oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models\nfor video prediction and infilling. arXiv preprint arXiv:2206.07696, 2022. 3\nLevon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models\nare zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 3\nGuillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs: Context-aware controllable video\nsynthesis. NeurIPS, 2021. 3, 7\nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao,\nJingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video\ngeneration. In CVPR, 2023. 3\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations. In International\nConference on Learning Representations, 2021. 6\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3\nYaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion: Training diffusion models on a single\nimage or video. arXiv preprint arXiv:2211.11743, 2022. 3\n11\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021. 9\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-\ntext transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http:\n//jmlr.org/papers/v21/20-074.html. 9\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 3\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. In ICML, pp. 1060\u20131069. PMLR, 2016. 3\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pp. 10684\u201310695, 2022. 3\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022. 3\nMasaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with sin-\ngular value clipping. In ICCV, 2017. 3\nXiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal\nmotion styles. In CVPR, 2023. 3\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video\ndata. arXiv preprint arXiv:2209.14792, 2022. 2, 3, 7\nIvan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video\ngenerator with the price, image quality and perks of stylegan2. arXiv preprint arXiv:2112.14683,\n2021. 3\nU-net: Convolutional networks for biomedical image segmentation, 2015. Springer. 4\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video\nrepresentations using lstms. In ICML, 2015. 3\nYu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey\nTulyakov. A good image generator is what you need for high-resolution video synthesis. In\nICLR, 2021. 3, 7\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion\nand content for video generation. In CVPR, June 2018. 3\nThomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski,\nand Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges.\narXiv preprint arXiv:1812.01717, 2018. 7\nVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion\nfor prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022. 3\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.\nNIPS, 2016. 3\nJiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Mod-\nelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a. 2, 7, 9\n12\nWenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu.\nVideofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv\npreprint arXiv:2305.10874, 2023b. 3, 7\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and\nNan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint\narXiv:2104.14806, 2021. 7\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00a8uwa: Visual\nsynthesis pre-training for neural visual world creation. In ECCV, pp. 720\u2013736. Springer, 2022a.\n7\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models\nfor text-to-video generation. arXiv preprint arXiv:2212.11565, 2022b. 3\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging\nvideo and language. In CVPR, 2016. 7\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial net-\nworks. In CVPR, pp. 1316\u20131324, 2018. 3\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\nvq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3\nRuihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video\ngeneration. arXiv preprint arXiv:2203.09481, 2022. 3\nSihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin.\nGenerating videos with dynamics-aware implicit generative adversarial networks. In ICLR, 2021.\n3\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-\nitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adver-\nsarial networks. In ICCV, pp. 5907\u20135915, 2017. 3\nHan Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive\nlearning for text-to-image generation. In CVPR, pp. 833\u2013842, 2021. 3\nDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo:\nEfficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.\n2, 3, 7\n13\nA\nAPPENDIX\nTable 5: Hyperparameters for our all models presented.\nHyperparameter\nKeyframe Module\nInterpolation Module\nFirst Superresolution\nSecond Superresolution\nSpace\npixel\npixel\npixel\nlatent\nfps\n2\n8\n8\n8\nChannels\n320\n320\n128\n320\nDepth\n4\n4\n5\n4\nChannel multiplier\n1,2, 4,4\n1,2,4,4\n1,2,4,6,6\n1,2,4,4\nHead channels\n64\n64\n64\n64\nTraining\nParameterization\n\u03b5\n\u03b5\nv\n\u03b5\n# train steps\n120K\n40K\n40K\n120K\nLearning rate\n10\u22124\n10\u22124\n10\u22124\n10\u22124\nBatch size per GPU\n1\n2\n1\n1\n# GPUs\n48\n16\n16\n24\nGPU-type\nA100-40GB\nA100-40GB\nA100-40GB\nA100-40GB\npdrop\n0.1\n0.1\n0.1\n0.1\nDiffusion Setup\nDiffusion steps\n1000\n1000\n1000\n1000\nNoise schedule\nLinear\nLinear\nLinear\nLinear\n\u03b20\n10\u22124\n10\u22124\n10\u22124\n0.0015\n\u03b2T\n0.02\n0.02\n0.02\n0.0195\nSampling Parameters\nSampler\nDPM++\nDPM++\nDPM++\nDDIM\nSteps\n75\n50\n125\n40\n\u03b7\n1.0\n1.0\n1.0\n1.0\nA.1\nTRAINING AND HYPERPARAMETER DETAILS.\nWe list details of our models in Table 5.\n14\n"
  },
  {
    "title": "NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions",
    "link": "https://arxiv.org/pdf/2309.15426.pdf",
    "upvote": "14",
    "text": "NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions\nZhang Chen1\u2020\nZhong Li1\u2020\nLiangchen Song2\nLele Chen1\nJingyi Yu3\nJunsong Yuan2\nYi Xu1\n1 OPPO US Research Center\n2 University at Buffalo\n3 ShanghaiTech University\n{zhang.chen,zhong.li,lele.chen,yi.xu}@oppo.com\n{lsong8,jsyuan}@buffalo.edu\nyujingyi@shanghaitech.edu.cn\nhttps://oppo-us-research.github.io/NeuRBF-website/\nAbstract\nWe present a novel type of neural fields that uses gen-\neral radial bases for signal representation. State-of-the-art\nneural fields typically rely on grid-based representations for\nstoring local neural features and N-dimensional linear ker-\nnels for interpolating features at continuous query points.\nThe spatial positions of their neural features are fixed on\ngrid nodes and cannot well adapt to target signals. Our\nmethod instead builds upon general radial bases with flex-\nible kernel position and shape, which have higher spatial\nadaptivity and can more closely fit target signals. To further\nimprove the channel-wise capacity of radial basis functions,\nwe propose to compose them with multi-frequency sinusoid\nfunctions. This technique extends a radial basis to multiple\nFourier radial bases of different frequency bands without\nrequiring extra parameters, facilitating the representation\nof details. Moreover, by marrying adaptive radial bases\nwith grid-based ones, our hybrid combination inherits both\nadaptivity and interpolation smoothness. We carefully de-\nsigned weighting schemes to let radial bases adapt to dif-\nferent types of signals effectively. Our experiments on 2D\nimage and 3D signed distance field representation demon-\nstrate the higher accuracy and compactness of our method\nthan prior arts. When applied to neural radiance field re-\nconstruction, our method achieves state-of-the-art render-\ning quality, with small model size and comparable training\nspeed.\n1. Introduction\nNeural fields (also termed implicit neural representation)\nhave gained much popularity in recent years due to their ef-\nfectiveness in representing 2D images, 3D shapes, radiance\nfields, etc. [50, 44, 15, 70, 62, 47, 48]. Compared to tra-\n\u2020 Corresponding author.\nSDF\nNeRF\nGigapixel Image\nFigure 1. NeuRBF provides an accurate and compact neural fields\nrepresentation for 2D images, 3D SDF, and neural radiance fields.\nditional discrete signal representations, neural fields utilize\nneural networks to establish a mapping from continuous in-\nput coordinates to the corresponding output value. Owing\nto their concise and efficient formulation, neural fields have\nbeen applied to various areas ranging from signal compres-\nsion [66, 22, 12, 86], 3D reconstruction [49, 83, 79], neu-\nral rendering [47, 48, 9, 39, 63, 25, 64, 14], medical imag-\ning [24, 81, 75], acoustic synthesis [11] and climate predic-\ntion [31].\nEarly neural fields [50, 44, 15, 47] use neural fea-\ntures that are globally shared in the input domain.\nDe-\nspite the compactness of the models, they have difficulty\nin representing high-frequency details due to the inductive\nbias [5, 70] of MLPs. To tackle this problem, local neu-\nral fields have been proposed and widely adopted [7, 32,\n51, 41, 26, 67, 48, 9], where each local region in the in-\nput domain is assigned with different neural features. A\ncommon characteristic in this line of work is to use ex-\nplicit grid-like structures to spatially organize neural fea-\ntures and apply N-dimensional linear interpolation to ag-\ngregate local neural features. However, grid-like structures\nare not adaptive to the target signals and cannot fully exploit\nthe non-uniformity and sparsity in various tasks, leading to\narXiv:2309.15426v1  [cs.CV]  27 Sep 2023\npotentially sub-optimal accuracy and compactness. While\nmulti-resolution techniques [69, 16, 58, 84, 28] have been\nexplored, it can still be expensive to achieve fine granularity\nwith excessive resolution levels. Some works [47, 70, 62]\nuse frequency encoding to address the low-frequency induc-\ntive bias. However, this technique is only applied on either\ninput coordinates or deep features.\nIn this work, we aim to increase the representation ac-\ncuracy and compactness of neural fields by equipping the\ninterpolation of basis functions with both spatial adaptiv-\nity and frequency extension.\nWe observe that the grid-\nbased linear interpolation, which is the fundamental build-\ning block in state-of-the-art local neural fields, is a spe-\ncial case of radial basis function (RBF). While grid-based\nstructures typically grow quadratically or cubically, general\nRBFs can require fewer parameters (sometimes even con-\nstant number) to represent patterns such as lines and ellip-\nsoids. Based upon this observation, we propose NeuRBF,\nwhich comprises of a combination of adaptive RBFs and\ngrid-based RBFs. The former uses general anisotropic ker-\nnel function with high adaptivity while the latter uses N-\ndimensional linear kernel function to provide interpolation\nsmoothness.\nTo further enhance the representation capability of\nRBFs, we propose to extend them channel-wise and com-\npose them with multi-frequency sinusoid function.\nThis\nallows each RBF to encode a wider range of frequencies\nwithout requiring extra parameters. This multi-frequency\nencoding technique is also applicable to the features in the\nMLP, which further improves accuracy and compactness.\nTo effectively adapt radial bases to target signals, we\nadopt the weighted variant of K-Means to initialize their\nkernel parameters, and design a weighting scheme for each\nof the three tasks (see Fig. 1): 2D image fitting, 3D signed\ndistance field (SDF) reconstruction, and neural radiance\nfield (NeRF) reconstruction. For NeRF, since it involves in-\ndirect supervision, traditional K-Means cannot be directly\napplied. To address this, we further propose a distillation-\nbased approach.\nIn summary, our work has the following contributions:\n\u2022 We present a general framework for neural fields based\non radial basis functions and propose a hybrid combi-\nnation of adaptive RBFs and grid-based RBFs.\n\u2022 We extend radial bases with multi-frequency sinu-\nsoidal composition, which substantially enhances their\nrepresentation ability.\n\u2022 To effectively adapt RBFs to different target signals,\nwe devise tailored weighting schemes for K-Means\nand a distillation-based approach.\n\u2022 Extensive experiments demonstrate that our method\nachieves state-of-the-art accuracy and compactness on\n2D image fitting, 3D signed distance field reconstruc-\ntion, and neural radiance field reconstruction.\n2. Related Work\nGlobal Neural Fields.\nEarly neural fields [50, 44, 15, 77,\n45, 21] are global ones and primarily focus on represent-\ning the signed distance field (SDF) of 3D shapes. They di-\nrectly use spatial coordinates as input to multi-layer per-\nceptrons (MLPs) and optionally concatenate a global la-\ntent vector for generalized or generative learning. These\nmethods have concise formulation and demonstrate supe-\nrior flexibility over convolutional neural networks (CNN)\nand traditional discrete representations in modeling signals\nin the continuous domain. However, these methods are un-\nable to preserve the high-frequency details in target signals.\nMildenhall et al. [47] pioneeringly proposed NeRF,\nwhich incorporates neural fields with volumetric rendering\nfor novel view synthesis. They further apply sine trans-\nform to the input coordinates (i.e., positional encoding),\nenabling neural fields to better represent high-frequency\ndetails.\nSimilar ideas are also adopted in RFF [70] and\nSIREN [62], which use random Fourier features or peri-\nodic activation as frequency encoding. These works also\npromote neural fields to be a general neural representation\napplicable to different types of signals and tasks. More re-\ncently, other encoding functions or architectures have been\nexplored [23, 72, 40, 60, 74, 73, 19, 36, 87, 52, 53, 18, 85,\n57, 80]. For example, MFN [23] replaces MLPs with the\nmultiplication of multiple linear functions of Fourier or Ga-\nbor basis functions, and WIRE [57] uses Gabor wavelet as\nactivation function in MLPs. Radial basis functions (RBF)\nhave also been discussed in [52, 53]. However, unlike our\nwork, they only consider simplified forms of RBFs and do\nnot explore spatial adaptivity, leading to nonideal perfor-\nmance.\nLocal Neural Fields.\nIn parallel to frequency encoding,\nlocal neural fields improve representation accuracy by lo-\ncality. Early attempts [7, 32, 51, 17, 13, 67] uniformly sub-\ndivide the input domain into dense grids and assign a neu-\nral feature vector to each grid cell. During point querying,\nthese local neural features are aggregated through nearest-\nneighbor or N-dimensional linear interpolation and then\nused as input to the following MLPs. Due to feature lo-\ncality, the depth and width of the MLPs can be largely re-\nduced [67, 26, 33], leading to higher training and inference\nspeed than global neural fields. Apart from neural features,\nthe locality can also be implemented on the network weights\nand biases [54, 58, 29], where each grid cell has a different\nMLP. Dense grids can be further combined with RFF [70]\nor SIREN [62] to improve accuracy on high-frequency de-\ntails [30, 43]. However, a significant drawback of dense\ngrids is that they are parameter-intensive.\n \n  \n \n   \n   \n  \nRGB\nSDF\nDensity\n\u2026\nMLP Network\nAdaptive RBFs\nMulti-Frequency\nSinusoidal Composition\nFeature\nAggregation\nFigure 2. Illustration of NeuRBF. Each adaptive RBFs can have different position and shape parameters ci, \u03a3i, leading to high spatial\nadaptivity. With multi-frequency sinusoidal composition, each adaptive RBF is further extended to multiple radial bases with different\nfrequencies, which are then combined with neural features wi through Hadamard product. The resulting features are aggregated within the\nneighborhood U(x) of query point x, and then be mapped to the output domain by the MLP network gm.\nTo improve model compactness, numerous techniques\nhave been proposed, such as multi-resolution tree (and/or\nresidual) structures [41, 84, 16, 42, 58, 82, 76, 26], hash\ngrids [48], dictionary optimization [68], permutohedral lat-\ntices [56], tensor decomposition [9], orthogonal planes [51,\n8, 61, 6, 25], wavelet [55], and multiplicative fields compo-\nsition [10]. Among them, Instant NGP [48] achieves high\naccuracy, compactness, and efficiency across different sig-\nnal types. Despite the additional data structures or oper-\nations, these methods still rely on basic grid-based linear\ninterpolation as the building block for neural feature aggre-\ngation. Another line of work [27, 38, 78] relaxes the grid\nstructures and allows neural features to be freely positioned\nin the input domain. However, they use simple interpolation\nkernel functions, which still have limited spatial adaptivity.\nTheir performance is also inferior to state-of-the-art grid-\nbased ones.\nUnlike prior local neural fields, we seek a general frame-\nwork consisting of hybrid radial bases and enhance their\nrepresentation capability by simultaneously exploiting spa-\ntial adaptivity and frequency extension.\n3. Our Method\n3.1. Local Neural Fields As Radial Basis Functions\nLocal neural fields represent a signal f in the form of a\nfunction \u02c6f : RD \u2192 RO, which maps a coordinate x in the\ncontinuous D-dimensional space to an output of O dimen-\nsions. The function f can be considered as a composition\nof two stages, i.e., f = gm \u25e6 gb, where gb extracts the lo-\ncal neural features at input location x from a neural repre-\nsentation (e.g., feature grid), and gm decodes the resulting\nfeature to the final output. Now we consider grid-based lin-\near interpolation for gb, which is a common building block\nin state-of-the-art neural fields. It has the following form:\ngb(x) = P\ni\u2208U(x) \u03c6(x, ci)wi. U(x) is the set of grid cor-\nner nodes that enclose x, ci \u2208 RD and wi \u2208 RF are the\nposition and neural feature of node i. \u03c6(x, ci) \u2208 R is the\ninterpolation weight of node i, and is computed as:\n\u03c6(x, ci) =\nD\nY\nj=1\nmax(0, 1 \u2212 |xj \u2212 ci,j|\n\u03c3\n),\n(1)\nwhere \u03c3 is the sidelength of each grid cell, and xj, ci,j\nare the jth element of x, ci. Note that Eq. (1) is a spe-\ncial case of radial basis function (RBF) with the form of\n\u03c6(x, ci, \u03c3i), where each RBF has its own position parame-\nter ci and shape parameter \u03c3i. From the perspective of RBF,\nwe use the following formulation for gb(x):\ngb(x) =\nX\ni\u2208U(x)\n\u03c6(x, ci, \u03c3i)wi.\n(2)\n3.2. Neural Radial Basis Fields\nCompared to grid-based linear interpolation, the advan-\ntages of RBFs originate from the additional position and\nshape parameters ci, \u03c3i. As illustrated in Fig. 2, our frame-\nwork makes extensive use of adaptive RBFs. To fully ex-\nploit their adaptivity, we propose to use anisotropic shape\nparameters \u03a3i \u2208 RD\u00d7D. The first row of Fig. 3 shows that\nwith anisotropic shape parameters, the shape of an RBF\u2019s\nlevel set can be either circular, elliptical, or even close to a\nline. This allows an RBF to be more adaptive to target sig-\nnals. For the kernel function \u03c6, we use the inverse quadratic\nfunction as an example, which is computed as:\n\u03c6(x, ci, \u03a3i) =\n1\n1 + (x \u2212 ci)T \u03a3\u22121\ni (x \u2212 ci).\n(3)\nNote that \u03a3i is a covariance matrix, which is symmetric.\nHence, each \u03a3i only has D\u00b7(D\u22121)\n2\nparameters. We can op-\nGabor Basis\nExtended Radial Bases\nFourier Basis\nRadial Bases\nFigure 3. Comparison of Bases. For the right 3 columns: the first\nrow shows radial bases with different shape parameters; the bot-\ntom row shows extended radial bases with different frequencies.\ntionally normalize the radial basis value at each point:\n\u02dc\u03c6(x, ci, \u03a3i) =\n\u03c6(x, ci, \u03a3i)\nP\nk\u2208U(x) \u03c6(x, ck, \u03a3k).\n(4)\nNote that our framework is not limited to a specific func-\ntion type but supports any others that have the radial basis\nform. The choice of the function type can thus be finetuned\nper task.\nSinusoidal Composition on Radial Basis.\nWe notice that\nwhile traditional RBF is a scalar function, wi \u2208 RF is a\nvector with multiple channels (recall Eq. (2)). Our moti-\nvation is to let each channel of wi linearly combine with a\ndifferent variant of the RBF so that the channel-wise capac-\nity of RBF can be exploited. To achieve this, we propose\nto compose RBF with a multi-frequency sinusoid function,\nwhere a radial basis is extended into multiple channels with\ndifferent frequencies:\n\u03c6(x, ci, \u03a3i) = sin( \u02dc\u03c6(x, ci, \u03a3i) \u00b7 m + b),\n(5)\nwhere m, b \u2208 RF are the global multiplier and bias ap-\nplied to \u02dc\u03c6(x, ci, \u03a3i) before sine transform. The resulting\n\u03c6(x, ci, \u03a3i) has F channels and is then multiplied with wi\nthrough Hadamard product. Fig. 2 illustrates this computa-\ntion process. gb(x) is thus computed as:\ngb(x) =\nX\ni\u2208U(x)\n\u03c6(x, ci, \u03a3i) \u2299 wi.\n(6)\nWith Eq. (5), the number of bases encoded by a single\npair of ci, \u03a3i is increased from 1 to F, leading to higher\nrepresentation ability. Note that m, b are globally shared\nacross RBFs. We set b as a learnable parameter and m as a\nfixed parameter. We determine the value of m by specify-\ning the lowest and highest frequencies ml, mh. The rest of\nthe elements are obtained by log-linearly dividing the range\nbetween ml and mh.\nOur sinusoidal composition technique differs from po-\nsitional encoding [47] and random Fourier features [70] in\nthat we apply sine transform to radial bases instead of input\ncoordinates. This allows the composited bases to have ellip-\ntical periodic patterns as shown in Fig. 3 second row, while\nthe bases created by [47, 70] are limited to linear periodic\npatterns. Our technique is also related to the Gabor filter,\nwhich combines a Gaussian function and a sinusoidal func-\ntion using multiplication. Still, the Gabor filter can only\nproduce bases with linear patterns.\nSinusoidal Composition on Feature Vector.\nWe also ap-\nply our sinusoidal composition technique to the output fea-\ntures h0 of the first fully-connected (FC) layer in gm:\nf0 = sin(h0 \u2299 m0) + h0,\n(7)\nwhere h0, m0, f0 \u2208 RF0 and \u2299 is Hadamard product. The\nbias term is omitted since it is already contained in FC layer.\nThe reason to apply this sinusoidal composition to h0 in-\nstead of gb(x) is to let the network first combines the multi-\nfrequency bases in gb(x) via an FC layer. Here, we also\ninclude a residual connection, which slightly benefits per-\nformance. The resulting feature vector f0 is input to the\nnext layer in gm. m0 is set in a similar manner as m by\nspecifying its lowest and highest frequency ml0 and mh0.\nCompared to sinusoid activation [62],\nour multi-\nfrequency approach can produce features of wide frequency\nrange with one sine transform. In addition, it does not re-\nquire specialized initialization for the FC layers. We exper-\nimentally observe that our technique achieves higher per-\nformance under radial basis framework. Table 5 shows a\nquantitative comparison with positional encoding [47] and\nsinusoid activation [62].\nHybrid Radial Bases.\nTo balance between fitting accu-\nracy and interpolation smoothness, we propose to use a\ncombination of adaptive RBFs and grid-based RBFs. The\nposition and shape parameters of adaptive RBFs can be\nfreely assigned while those of grid-based RBFs are fixed\nto a grid structure. Adaptive RBFs tend to produce sharp\ndiscontinuities when U(x) (the set of neighboring RBFs of\nthe point x) changes. On the other hand, grid-based RBFs\ndo not exhibit such discontinuity and can better preserve\nfunction smoothness. Please refer to our supplementary for\nan illustration. We combine adaptive and grid-based RBFs\nthrough feature concatenation, which allows the network to\nselect features accordingly.\n3.3. Initialization of Position and Shape Parameters\nMotivated by [59], we adapt RBFs to target signals\nby initializing their position and shape parameters with\nweighted K-Means clustering. Intuitively, this biases RBF\ndistribution towards data points with higher weights. This\ntechnique is simple and effective, and can be applied to dif-\nferent tasks by changing the weighting scheme.\nPosition Initialization.\nLet x1, ..., xm be the coordinates\nof input points and w1, ..., wm be the weight of each point\n(weight calculation will be described later).\nGiven ini-\ntial cluster centers c1, ..., cn, weighted K-Means optimizes\nthese cluster centers with:\nmin\nc1,...,cn\nn\nX\ni=1\nm\nX\nj=1\naijwj\u2225xj \u2212 ci\u22252,\n(8)\nwhere aij is an indicator variable. Following common prac-\ntice, we solve Eq. (8) with an expectation\u2013maximization\n(EM)-style algorithm.\nShape\nInitialization.\nInspired\nby\nGaussian\nmixture\nmodel, we initialize the shape parameters \u03a3i as the follow-\ning:\n\u03a3i =\nP\nj aijwj(xj \u2212 ci)(xj \u2212 ci)T\nP\nj aijwj\n.\n(9)\nWeighting Schemes.\nThe weights w1, ..., wm control\nhow RBFs will be distributed after initialization.\nData\npoints with higher importance should be assigned with\nhigher weights.\nFor 2D images, we use the spatial gradient norm of pixel\nvalue as the weight for each point: wj = \u2225\u2207I(xj)\u2225.\nFor 3D signed distance field, we use the inverse of abso-\nlute SDF value as point weight: wj = 1 / (|SDF(xj)| +\n1e\u22129). The inclusion of 1e\u22129 is to avoid division by zero.\nFor neural radiance field, it is a task with indirect su-\npervision.\nThe input signal is a set of multi-view 2D\nimages while the signal to be reconstructed lies in 3D\nspace. Therefore, we cannot directly obtain the weights.\nTo tackle this problem, we propose a distillation method.\nWe first use grid-based neural fields to train a model for\n1000 \u223c 2000 training steps. Then we uniformly sample\n3D points and use the trained model to predict the density\n\u03c3(x) and color feature vector fc(x) at these points.\nFi-\nnally, we convert density to alpha and multiply with the\nspatial gradient norm of the color feature vector as point\nweight: wj = (1\u2212exp(\u2212\u03c3(xj)\u03b4))\u2225\u2207fc(xj)\u2225. This weight-\ning scheme takes both density and appearance complexity\ninto account. Compared to 3D Gaussian Splatting [34] and\nPoint-NeRF [78], our approach does not require external\nstructure-from-motion or multi-view stereo methods to re-\nconstruct the point cloud, but distills information from a\nvolumetric model. Hence, our initialization can handle both\nsurface-based objects and volumetric objects.\n4. Implementation\nIn this section, we describe the keypoints of our imple-\nmentation. More details can be found in our supplementary.\nWe implement our adaptive RBFs using vanilla PyTorch\nwithout custom CUDA kernels. For the grid-based part in\nour framework, we adopt Instant NGP [48] for 2D image fit-\nting and 3D signed distance field (SDF). We use a PyTorch\nimplementation of Instant NGP from [1]. For neural radi-\nance field (NeRF) reconstruction, we explored TensoRF [9]\nand K-Planes [25] as the grid-based part. We reduce the\nspatial resolution and feature channel of the grid-based part,\nand allocate parameters to the adaptive RBFs accordingly.\nFor sinusoidal composition, we use ml = 2\u22123, mh =\n212, ml0 = 1, mh0 = 1000 in the image experiments on\nDIV2K dataset [3, 71], and ml = 20, mh = 23, ml0 =\n30, mh0 = 300 in SDF experiments. In NeRF task, we do\nnot apply sinusoidal composition since the improvement is\nsmall.\nTraining is conducted on a single NVIDIA RTX A6000\nGPU. We use Adam optimizer [35] where \u03b21 = 0.9, \u03b22 =\n0.99, \u03f5 = 10\u221215. The learning rates for neural features are\n5\u00d710\u22123, 1\u00d710\u22124, 2\u00d710\u22122 for image, SDF and NeRF task\nrespectively. In addition, we apply learning rate schedulers\nthat gradually decrease learning rates during training. The\nposition and shape parameters of RBFs can be optionally\nfinetuned via gradient backpropagation. However, we do\nnot observe significant performance gain and therefore fix\nthese parameters during training.\nWe use L2 loss when fitting 2D images and reconstruct-\ning neural radiance field, and use MAPE loss [48] when re-\nconstructing 3D SDF. For SDF task, we use the same point\nsampling approach as Instant NGP [48]. For NeRF task,\nwe follow the training approaches in TensoRF [9] and K-\nPlanes [25] respectively. In all experiments, both competing\nmethods and our method are trained per scene.\n5. Experiment\n5.1. 2D Image Fitting\nWe first evaluate the effectiveness of fitting 2D images.\nWe use the validation split of DIV2K dataset [3, 71] and\n6 additional images of ultra-high resolution as evaluation\nbenchmark. DIV2K validation set contains 100 natural im-\nages with resolution around 2040 \u00d7 1356. The resolution\nof the 6 additional images ranges from 6114 \u00d7 3734 to\n56718 \u00d7 21450.\nWe first compare with MINER [58] and Instant NGP (\u201cI-\nNGP\u201d) [48], which exhibit state-of-the-art performance for\nhigh-resolution image fitting. We let our method use fewer\nparameters than the other two. During timing, the time for\ninitializing RBFs is also taken into account.\nTable 1 top half shows the comparison on the DIV2K\ndataset. For our method, we include two additional setups:\nSteps\nTime\u2193\n# Tr. Params\u2193\nPSNR\u2191\nDIV2K\nMINER [58]\n35k\n16.7m\n5.49M\n46.92\nI-NGP [48]\n35k\n1.3m\n4.91M\n47.56\nOurs\n35k\n7.9m\n4.31M\n58.56\nOurs3.5k\u2212steps\n3.5k\n48s\n4.31M\n51.53\nOurs2.2M\n35k\n7.7m\n2.20M\n49.26\nDIV2K 256\u00d7256\u00d73\nBACON [40]\n5k\n78.2s\n268K\n38.51\nPNF [80]\n5k\n483.9s\n287K\n38.99\nOurs\n5k\n28.5s\n128K\n54.84\nTable 1. 2D Image Fitting. We quantitatively compare our method\nwith MINER [58], Instant NGP (\u201cI-NGP\u201d) [48], BACON [40]\nand PNF [80] on the validation set of DIV2K dataset [3, 71].\n\u201cDIV2K\u201d: original image resolution; \u201cDIV2K 256\u00d7256\u00d73\u201d: cen-\nter cropped and downsampled to 256\u00d7256\u00d73.\nMINER\nInstant NGP\nOurs\n0.01\n0\n# Tr. Params\u2193 : 43.87M\nPSNR\u2191 : 51.77 dB\n37.15M\n50.89 dB\n34.62M\n60.52 dB\n67.24M\n50.66 dB\n61.45M\n48.53 dB\n57.46M\n53.86 dB\n6114 \u00d7 3734 \u00d7 3\n8000 \u00d7 8000 \u00d7 3\nFigure 4. 2D Image Fitting. Leftmost column shows the fitted\nimages of our method and the resolution of the images. The other\ncolumns show the error maps of each method, along with the num-\nber of trainable parameters (\u201c# Tr. Params\u201d) and PSNR.\none using fewer training steps and one using fewer train-\nable parameters. When using the same number of training\nsteps, our method outperforms the other two by over 10 dB\nin Peak Signal-to-Noise Ratio (PSNR) with less trainable\nparameters. Although Instant NGP has faster training speed\ndue to their heavily-optimized CUDA implementation, our\nmethod is implemented with vanilla PyTorch and is easily\nextensible. In addition, with only 3.5k training steps (1/10\nof that of Instant NGP), our method already reaches a PSNR\nof 51.53 dB, which is almost 4 dB higher than Instant NGP.\nMeanwhile, the training time is only 48s and even faster\nthan Instant NGP. The time for RBF initialization is around\n2s. \u201cOurs2.2M\u201d additionally demonstrates the high com-\npactness of our method. After reducing trainable parame-\nters to be over 50% fewer than the competing methods, our\napproach still retains a higher fitting accuracy.\nIn Fig. 4, we show the fitting results on 2 ultra-high res-\nolution images. Besides achieving higher PSNR than the\nother two, our method also has a more uniform error distri-\n128K / 50.34\nGT\nOurs\nBACON\nPNF\n268K / 40.74\n287K / 40.86\n# Params\u2193 / PSNR\u2191\n(a)\n(b)\nOurs\nI-NGP\nMINER\nBACON\nPNF\n2500\n500\n100\nSteps\n(c)\nFigure 5. 2D Image Fitting on an image from Kodak dataset [20].\n(a) Final results after 5k training steps. (b) Training curves. (c)\nIntermediate results.\nbution. This reflects the adaptivity of RBFs, which allows\na more accurate representation of details. Results on other\nimages can be found in our supplementary material.\nWe additionally compare with BACON [40] and\nPNF [80] on the 100 images in DIV2K validation set. In this\nexperiment, the images are center cropped and downsam-\npled to 256\u00d7256\u00d73 following the practice of BACON [40].\nWe use their official codes and settings for BACON and\nPNF, and let our method use the same batch size (65,536)\nand training steps (5k) as them. The results are shown in\nTable 1 bottom half. We further conduct comparisons on a\nsample image from Kodak dataset [20], and show the qual-\nitative results and training curves in Fig. 5. The image is\nsimilarly center cropped and resized to 256\u00d7256\u00d73. The\nresults show that our method has both fast convergence and\nhigh fitting accuracy. Higher PSNR demonstrates the ability\nto more precisely represent target signals, and implies fewer\nparameters and training steps to reach a specified PSNR.\nFor the image in Fig. 5, Instant NGP and MINER reach\n45.34 dB and 45.23 dB PSNR with 140K parameters and\n5k steps. Our method instead can reach 45.59 dB PSNR\nwith only 72K parameters and 3.5k steps.\nNGLOD6\nInstant NGP\nOurs\nGT\n# Tr. Params\u2193 : 78.84M\nNAE\u2193 : 14.64\u00b0\n1.62M\n9.23\u00b0\n1.80M\n10.09\u00b0\n78.84M\n6.46\u00b0\n856K\n5.30\u00b0\n950K\n6.00\u00b0\nFigure 6. 3D Signed Distance Field Reconstruction. Leftmost column shows the reconstructed geometry of our method. The other\ncolumns show qualitative and quantitative comparisons of reconstruction results. \u201c# Tr. Params\u201d is the number of trainable parameters and\n\u201cNAE\u201d is the normal angular error.\nSteps\n# Tr. Params\u2193\nIoU\u2191\nNAE\u2193\nNGLOD5 [69]\n245k\n10.15M\n0.9962\n6.58\nNGLOD6 [69]\n245k\n78.84M\n0.9963\n6.14\nI-NGP [48]\n20k\n950K\n0.9994\n5.70\nOurs\n20k\n856K\n0.9995\n4.93\nI-NGP400K [48]\n20k\n498K\n0.9992\n6.39\nOurs400K\n20k\n448K\n0.9994\n5.53\nTable 2. 3D Signed Distance Field Reconstruction. We quanti-\ntatively compare our method with NGLOD [69] and Instant NGP\n(\u201cI-NGP\u201d) [48].\n5.2. 3D Signed Distance Field Reconstruction\nWe use 10 3D models from the Stanford 3D Scanning\nRepository [65], the Digital Michelangelo Project [37], and\nTurboSquid [2] as benchmark data.\nThese models con-\ntain delicate geometric details and challenging topologies.\nWe compare our method with NGLOD [69] and Instant\nNGP [48]. For evaluation metrics, we use Intersection over\nUnion (IoU) and normal angular error (NAE). NAE mea-\nsures the face normal difference of corresponding points\nand can better reflect the accuracy of reconstructed surface\nthan IoU.\nFig. 6 demonstrates example results on 3 objects. Our\nmethod produces more accurate geometry, with sharp edges\nand smooth surfaces. Comparatively, the results of NGLOD\nare overly smooth while those of Instant NGP contain\nnoises.\nIn Table 2, we compare the performance under differ-\nent numbers of trainable parameters. Our approach consis-\ntently has higher IoU and lower NAE. The advantages of\nour method are larger when using fewer parameters, which\nis also demonstrated in Fig. 7.\nGT\nI-NGP\nOurs\n# Tr. Params\u2193 : 137K\nNAE\u2193 : 4.93\u00b0\n260K\n4.11\u00b0\n498K\n3.20\u00b0\n124K\n3.75\u00b0\n235K\n3.13\u00b0\n448K\n2.89\u00b0\nFigure 7. 3D Signed Distance Field Reconstruction. We com-\npare the reconstruction accuracy of Instant NGP (\u201cI-NGP\u201d) [48]\nand ours under different parameter count.\n5.3. Neural Radiance Field Reconstruction\nWe evaluate our approach on both 360\u25e6 scenes and\nforward-facing scenes. Metrics of the comparison methods\nare taken from their paper whenever available. Full per-\nscene results are available in our supplementary material.\n360\u25e6 Scenes.\nWe use the Synthetic NeRF dataset [47]\nwhich is a widely adopted benchmark for neural radiance\nfield reconstruction. We utilize TensoRF [9] as the grid-\nbased part in this experiment. We compare with numer-\nous representative methods in this area, as listed in Table 3.\nAmong them, Instant NGP [48] and TensoRF [9] represent\nstate-of-the-art performance while Factor Fields [10] is con-\ncurrent to our work. For Point-NeRF [78], their SSIM met-\nrics are recomputed with a consistent SSIM implementation\nas other work.\nBatch Size\nSteps\nTime\u2193\n# Params\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPSV GG \u2193\nLPIPSAlex \u2193\nNeRF [47]\n4096\n300k\n\u223c 35h\n1.25M\n31.01\n0.947\n0.081\n-\nMip-NeRF 360 [4]\n16384\n250k\n\u223c 3.4h\n3.23M\n33.25\n0.962\n0.039\n-\nPoint-NeRF [78]\n-\n200k\n\u223c 4.5h\n-\n33.31\n0.962\n0.050\n0.028\nPlenoxels [26]\n5000\n128k\n11.4m\n194.5M\n31.71\n0.958\n0.049\n-\nInstant NGP [48]\n262144\n35k\n3.8m\n12.21M\n33.18\n0.963\n0.051\n0.028\nTensoRF [9]\n4096\n30k\n17.4m\n17.95M\n33.14\n0.963\n0.047\n0.027\nFactor Fields [10]\n4096\n30k\n12.2m\n5.10M\n33.14\n0.961\n-\n-\nK-Planes [25]\n4096\n30k\n38m\n33M\n32.36\n0.962\n0.048\n0.031\nOurs\n4096\n30k\n33.6m\n17.74M\n34.62\n0.975\n0.034\n0.018\nOurs3.66M\n4096\n30k\n29.3m\n3.66M\n33.97\n0.971\n0.039\n0.022\nTable 3. Neural Radiance Field Reconstruction. We quantitatively compare our method with numerous state-of-the-art methods on the\nSynthetic NeRF dataset [47]. Best 3 scores in each metric are marked with gold , silver\nand bronze . \u201c-\u201d denotes the information is\nunavailable in the respective paper.\nGround Truth\nOurs\nK-Planes\nTensoRF\nInstant NGP\nPoint-NeRF\nFigure 8. Neural Radiance Field Reconstruction. Qualitative comparisons on the Synthetic NeRF Dataset [47]. Leftmost column shows\nthe full-image results of our method.\nTable 3 comprehensively compares training time, num-\nber of parameters and novel view rendering metrics. Our\nmethod surpasses competing methods by a noticeable mar-\ngin in rendering accuracy. Fig. 8 reflects the higher qual-\nity of our results, which contain more accurate details and\nfewer artifacts. Meanwhile, our method retains a moderate\nmodel size (same as TensoRF [9]) and comparable training\ntime. After reducing to 3.66M parameters, our model still\nachieves high rendering accuracy and outperforms other\nmethods that use more parameters (Plenoxels [26], Instant\nNGP [48], TensoRF [9], Factor Fields [10], K-Planes [25]).\nFig. 9 compares the novel view synthesis accuracy with rep-\nresentative methods (Instant NGP [48], TensoRF [9]) under\nsimilar parameter count. Our method consistently performs\nbetter than the other two and also achieves higher PSNR\nthan vanilla NeRF [47] when using the same number of pa-\nrameters.\nForward-Facing Scenes.\nWe use the LLFF dataset [46]\nwhich contains 8 real unbounded forward-facing scenes. In\nthis experiment, we explore using K-Planes [25] as the grid-\nbased part . As shown in Table 4, our approach achieves\nthe highest PSNR and second-best SSIM. Although Mip-\nNeRF 360 has a higher score in SSIM, its training time\nis 7 times longer than ours. Compared to Plenoxels and\nTensoRF, our method has higher rendering accuracy, fewer\nparameters and comparable training speed. Fig. 10 shows\nexample novel view synthesis results, where ours contain\nfewer visual artifacts.\nI-NGP TensoRF Ours\nI-NGP TensoRF Ours\nGT\n# Params: 1M\n18M\nFigure 9. Neural Radiance Field Reconstruction. We compare\nthe novel view synthesis quality under different parameter count\non the \u201cMaterials\u201d scene. Top is a quantitative comparison of ren-\ndering PSNR. Bottom is a qualitative comparison between Instant\nNGP (\u201cI-NGP\u201d) [48], TensoRF [9] and ours at 1M and 18M pa-\nrameters.\nGround Truth\nOurs\nK-Planes\nTensoRF\nFigure 10. Neural Radiance Field Reconstruction. Qualitative\ncomparisons on the LLFF Dataset [46].\nTime\u2193\n# Params\u2193\nPSNR\u2191\nSSIM\u2191\nNeRF [47]\n36h\n1.25M\n26.50\n0.811\nMip-NeRF 360 [4]\n3.8h\n3.23M\n26.86\n0.858\nPlenoxels [26]\n24m\n\u223c 500M\n26.29\n0.839\nTensoRF [9]\n25m\n45M\n26.73\n0.839\nK-Planes [25]\n33m\n18.7M\n26.92\n0.847\nOurs\n31m\n18.7M\n27.05\n0.849\nTable 4. Neural Radiance Field Reconstruction. Quantitative\ncomparisons on the LLFF Dataset [46].\n5.4. Ablation Study\nIn Table 5, we conduct ablation study on adaptive\nRBFs (A-RBF) and multi-frequency sinusoidal composition\n(MSC) using the DIV2K validation set [3, 71] and the 3D\nshapes in Sec. 5.2. All image models are trained for 3500\n2D Images\n3D SDF\nPSNR\u2191\nSSIM\u2191\nIoU\u2191\nNAE\u2193\nNo A-RBF\n42.37\n0.9918\n0.9994\n5.70\nNo MSC on RBF\n48.19\n0.9940\n0.9995\n5.04\nNo MSC on Feat.\n48.46\n0.9935\n0.9995\n5.09\nNo MSC on Both\n43.81\n0.9870\n0.9995\n5.16\nOurs Full\n51.53\n0.9961\n0.9995\n4.93\nOurs-PE\n43.72\n0.9870\n0.9994\n5.46\nOurs-SIREN\n45.98\n0.9920\n0.9994\n5.69\nTable 5.\nAblation Study. We ablate on the adaptive RBFs (A-\nRBF) and multi-frequency sinusoidal composition (MSC). \u201cOurs-\nPE\u201d replaces MSC with positional encoding [47]. \u201cOurs-SIREN\u201d\nreplaces MSC with sinusoid activation [62].\nsteps and all SDF models are trained for 20000 steps. To\ndemonstrate the effectiveness of sinusoidal composition in\nour framework, we further include variants that replace it\nwith positional encoding [47] (Ours-PE) and sinusoid ac-\ntivation [62] (Ours-SIREN). For Ours-PE, we apply posi-\ntional encoding [47] (PE) on input coordinate x and con-\ncatenate the features with gb(x) before input to the decoder\nnetwork gm. For Ours-SIREN, we apply sinusoidal activa-\ntion [62] to the hidden layers in gm, and use the method\nin [62] to initialize fully-connected layers. As shown in Ta-\nble 5, without adaptive RBFs and sinusoidal composition,\nthere is a noticeable drop in accuracy. Compared to PE and\nSIREN, our multi-frequency sinusoidal composition tech-\nnique achieves higher performance.\n6. Conclusion\nWe have proposed NeuRBF, which provides accurate\nand compact neural representations for signals. We demon-\nstrate that by simultaneously exploiting the spatial adaptiv-\nity and frequency extension of radial basis functions, the\nrepresentation ability of neural fields can be greatly en-\nhanced. To effectively adapt radial basis functions to tar-\nget signals, we further devise tailored weighting schemes.\nOur method achieves higher accuracy than state-of-the-arts\non 2D shape fitting, 3D signed distance field reconstruction,\nand neural radiance field reconstruction, while using same\nor fewer parameters. We believe our framework is a valu-\nable step towards more expressive neural representations.\nBy far, we have not explored generalized learning, which\nwould be a promising extension of our framework. Another\nfuture direction would be incorporating dictionary learning\nto further increase model compactness.\nAcknowledgements\nThe authors thank the anonymous reviewers for their\nvaluable feedback, and Anpei Chen and Zexiang Xu for\nhelpful discussions.\nReferences\n[1] https://github.com/ashawkey/torch-ngp. 5\n[2] https://www.turbosquid.com. 7\n[3] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In The\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) Workshops, July 2017. 5, 6, 9, 14, 15\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 8, 9, 21, 24\n[5] Alberto Bietti and Julien Mairal. On the inductive bias of\nneural tangent kernels. Advances in Neural Information Pro-\ncessing Systems, 32, 2019. 1\n[6] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 130\u2013141, 2023. 3\n[7] Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt,\nJulian Straub, Steven Lovegrove, and Richard Newcombe.\nDeep local shapes: Learning local sdf priors for detailed 3d\nreconstruction. In Computer Vision\u2013ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23\u201328, 2020, Pro-\nceedings, Part XXIX 16, pages 608\u2013625. Springer, 2020. 1,\n2\n[8] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 3\n[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Is-\nrael, October 23\u201327, 2022, Proceedings, Part XXXII, pages\n333\u2013350. Springer, 2022. 1, 3, 5, 7, 8, 9, 14, 21, 24\n[10] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su,\nand Andreas Geiger. Factor fields: A unified framework for\nneural fields and beyond. arXiv preprint arXiv:2302.01226,\n2023. 3, 7, 8, 16, 21\n[11] Changan Chen, Alexander Richard, Roman Shapovalov,\nVamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman,\nand Andrea Vedaldi. Novel-view acoustic synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6409\u20136419, 2023. 1\n[12] Hao Chen, Bo He, Hanyu Wang, Yixuan Ren, Ser Nam Lim,\nand Abhinav Shrivastava. Nerv: Neural representations for\nvideos. Advances in Neural Information Processing Systems,\n34:21557\u201321568, 2021. 1\n[13] Yinbo Chen, Sifei Liu, and Xiaolong Wang.\nLearning\ncontinuous image representation with local implicit image\nfunction. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8628\u20138638,\n2021. 2\n[14] Zhang Chen, Anpei Chen, Guli Zhang, Chengyuan Wang, Yu\nJi, Kiriakos N Kutulakos, and Jingyi Yu. A neural rendering\nframework for free-viewpoint relighting. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5599\u20135610, 2020. 1\n[15] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5939\u20135948, 2019. 1, 2\n[16] Zhang Chen, Yinda Zhang, Kyle Genova, Sean Fanello,\nSofien Bouaziz, Christian H\u00a8ane, Ruofei Du, Cem Keskin,\nThomas Funkhouser, and Danhang Tang.\nMultiresolution\ndeep implicit functions for 3d shape representation.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 13087\u201313096, 2021. 2, 3\n[17] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.\nImplicit functions in feature space for 3d shape reconstruc-\ntion and completion. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6970\u20136981, 2020. 2, 16\n[18] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and\nSimon Lucey. Gaussian activated neural radiance fields for\nhigh fidelity reconstruction and pose estimation. In Com-\nputer Vision\u2013ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXIII,\npages 264\u2013280. Springer, 2022. 2\n[19] Junwoo Cho, Seungtae Nam, Daniel Rho, Jong Hwan Ko,\nand Eunbyung Park. Streamable neural fields. In Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XX, pages\n595\u2013612. Springer, 2022. 2\n[20] Eastman Kodak Company. Kodak lossless true color image\nsuite. https://r0k.us/graphics/kodak/. 6\n[21] Yueqi Duan, Haidong Zhu, He Wang, Li Yi, Ram Nevatia,\nand Leonidas J Guibas. Curriculum deepsdf. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part VIII 16, pages\n51\u201367. Springer, 2020. 2\n[22] Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam\nGolinski, Y Whye Teh, and Arnaud Doucet. Coin++: Neu-\nral compression across modalities. Transactions on Machine\nLearning Research, 2022(11), 2022. 1\n[23] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico\nKolter. Multiplicative filter networks. In International Con-\nference on Learning Representations, 2021. 2\n[24] Jie Feng, Ruimin Feng, Qing Wu, Zhiyong Zhang, Yuyao\nZhang, and Hongjiang Wei. Spatiotemporal implicit neural\nrepresentation for unsupervised dynamic mri reconstruction.\narXiv preprint arXiv:2301.00127, 2022. 1\n[25] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12479\u201312488, 2023. 1,\n3, 5, 8, 9, 15, 21, 24\n[26] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 1, 2, 3, 8, 9, 21, 24\n[27] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\nand Thomas Funkhouser. Local deep implicit functions for\n3d shape.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4857\u2013\n4866, 2020. 3\n[28] Kang Han and Wei Xiang. Multiscale tensor decomposition\nand rendering equation encoding for view synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4232\u20134241, 2023. 2, 14\n[29] Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu.\nImplicit neural representations with levels-of-experts. In Ad-\nvances in Neural Information Processing Systems, 2022. 2\n[30] Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung,\nand Daniel Cohen-Or. Sape: Spatially-adaptive progressive\nencoding for neural optimization. Advances in Neural Infor-\nmation Processing Systems, 34:8820\u20138832, 2021. 2\n[31] Langwen Huang and Torsten Hoefler. Compressing multi-\ndimensional weather and climate data into neural networks.\narXiv preprint arXiv:2210.12538, 2022. 1\n[32] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei\nHuang, Matthias Nie\u00dfner, Thomas Funkhouser, et al. Local\nimplicit grid representations for 3d scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6001\u20136010, 2020. 1, 2\n[33] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy\nMitra. Relu fields: The little non-linearity that could. In\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20139,\n2022. 2\n[34] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(TOG), 42(4):1\u201314, 2023. 5\n[35] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[36] Zoe Landgraf, Alexander Sorkine Hornung, and Ricardo Sil-\nveira Cabral. Pins: progressive implicit networks for multi-\nscale neural representations.\nIn Proceedings of the Inter-\nnational Conference on Machine Learning (ICML), pages\n11969\u201311984, 2022. 2\n[37] Marc\nLevoy,\nKari\nPulli,\nBrian\nCurless,\nSzymon\nRusinkiewicz, David Koller, Lucas Pereira, Matt Ginz-\nton, Sean Anderson, James Davis, Jeremy Ginsberg, et al.\nThe digital michelangelo project:\n3d scanning of large\nstatues.\nIn Proceedings of the 27th annual conference\non Computer graphics and interactive techniques, pages\n131\u2013144, 2000. 7\n[38] Tianyang Li, Xin Wen, Yu-Shen Liu, Hua Su, and Zhizhong\nHan. Learning deep implicit functions for 3d shapes with\ndynamic code clouds. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n12840\u201312850, 2022. 3\n[39] Zhong Li, Liangchen Song, Celong Liu, Junsong Yuan, and\nYi Xu. Neulf: Efficient novel view synthesis with neural 4d\nlight field. In Eurographics Symposium on Rendering, 2022.\n1\n[40] David B Lindell, Dave Van Veen, Jeong Joon Park, and\nGordon Wetzstein.\nBacon: Band-limited coordinate net-\nworks for multiscale scene representation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 16252\u201316262, 2022. 2, 6, 17\n[41] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. Advances\nin Neural Information Processing Systems, 33:15651\u201315663,\n2020. 1, 3\n[42] Julien NP Martel, David B Lindell, Connor Z Lin, Eric R\nChan, Marco Monteiro, and Gordon Wetzstein.\nAcorn:\nAdaptive coordinate networks for neural scene representa-\ntion.\nACM Transactions on Graphics (TOG), 40:1 \u2013 13,\n2021. 3\n[43] Ishit Mehta, Micha\u00a8el Gharbi, Connelly Barnes, Eli Shecht-\nman, Ravi Ramamoorthi, and Manmohan Chandraker. Mod-\nulated periodic activations for generalizable local functional\nrepresentations. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 14214\u201314223,\n2021. 2\n[44] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4460\u20134470, 2019. 1, 2\n[45] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack,\nMahsa Baktashmotlagh, and Anders Eriksson. Implicit sur-\nface representations as layers in neural networks. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 4743\u20134752, 2019. 2\n[46] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 38(4):1\u201314, 2019. 8, 9, 15, 16, 17,\n24, 25\n[47] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 1,\n2, 4, 7, 8, 9, 14, 16, 17, 21, 22, 23, 24\n[48] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 1, 3, 5, 6, 7, 8, 9, 15, 16, 17, 19,\n20, 21\n[49] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUnisurf:\nUnifying neural implicit surfaces and radiance\nfields for multi-view reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 5589\u20135599, 2021. 1\n[50] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 165\u2013174, 2019. 1, 2\n[51] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger.\nConvolutional occupancy\nnetworks. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part III 16, pages 523\u2013540. Springer, 2020. 1, 2, 3\n[52] Sameera Ramasinghe and Simon Lucey.\nLearning po-\nsitional embeddings for coordinate-mlps.\narXiv preprint\narXiv:2112.11577, 2021. 2\n[53] Sameera Ramasinghe and Simon Lucey. Beyond periodicity:\ntowards a unifying framework for activations in coordinate-\nmlps. In Computer Vision\u2013ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XXXIII, pages 142\u2013158. Springer, 2022. 2\n[54] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 14335\u2013\n14345, 2021. 2\n[55] Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan\nLee, Jong Hwan Ko, and Eunbyung Park. Masked wavelet\nrepresentation for compact neural radiance fields. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 20680\u201320690, 2023. 3\n[56] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast\nmulti-view reconstruction with implicit surfaces using per-\nmutohedral lattices. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8466\u20138475, 2023. 3\n[57] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha\nBalakrishnan, Ashok Veeraraghavan, and Richard G Bara-\nniuk. Wire: Wavelet implicit neural representations. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18507\u201318516, 2023. 2, 16\n[58] Vishwanath Saragadam, Jasper Tan, Guha Balakrishnan,\nRichard G Baraniuk, and Ashok Veeraraghavan.\nMiner:\nMultiscale implicit neural representation.\nIn Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Is-\nrael, October 23\u201327, 2022, Proceedings, Part XXIII, pages\n318\u2013333. Springer, 2022. 2, 3, 5, 6, 15\n[59] Friedhelm Schwenker, Hans A Kestler, and G\u00a8unther Palm.\nThree learning phases for radial-basis-function networks.\nNeural networks, 14(4-5):439\u2013458, 2001. 4\n[60] Shayan Shekarforoush, David Lindell, David J Fleet, and\nMarcus A Brubaker. Residual multiplicative filter networks\nfor multiscale reconstruction. Advances in Neural Informa-\ntion Processing Systems, 35:8550\u20138563, 2022. 2\n[61] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3d neural field generation\nusing triplane diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 20875\u201320886, 2023. 3\n[62] Vincent Sitzmann, Julien Martel, Alexander Bergman, David\nLindell, and Gordon Wetzstein. Implicit neural representa-\ntions with periodic activation functions. Advances in Neural\nInformation Processing Systems, 33:7462\u20137473, 2020. 1, 2,\n4, 9\n[63] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\nNerf-\nplayer: A streamable dynamic scene representation with de-\ncomposed neural radiance fields. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(5):2732\u20132742, 2023.\n1\n[64] Liangchen Song, Zhong Li, Xuan Gong, Lele Chen, Zhang\nChen, Yi Xu, and Junsong Yuan. Harnessing low-frequency\nneural fields for few-shot view synthesis.\narXiv preprint\narXiv:2303.08370, 2023. 1\n[65] Stanford University.\nThe Stanford 3d scanning repos-\nitory.\nhttps://graphics.stanford.edu/data/\n3Dscanrep. 7\n[66] Yannick Str\u00a8umpler, Janis Postels, Ren Yang, Luc Van Gool,\nand Federico Tombari. Implicit neural representations for\nimage compression. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXVI, pages 74\u201391. Springer, 2022. 1\n[67] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5459\u2013\n5469, 2022. 1, 2\n[68] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas\nM\u00a8uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.\nVariable bitrate neural fields. In ACM SIGGRAPH 2022 Con-\nference Proceedings, pages 1\u20139, 2022. 3\n[69] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten\nKreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson,\nMorgan McGuire, and Sanja Fidler. Neural geometric level\nof detail: Real-time rendering with implicit 3d shapes. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 11358\u201311367, 2021. 2,\n7, 17, 19, 20\n[70] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. Advances in Neural Information Processing\nSystems, 33:7537\u20137547, 2020. 1, 2, 4\n[71] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-\nHsuan Yang, Lei Zhang, Bee Lim, et al. Ntire 2017 chal-\nlenge on single image super-resolution: Methods and results.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops, July 2017. 5, 6, 9, 14, 15\n[72] Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, and Xin Tong.\nSpline positional encoding for learning 3d implicit signed\ndistance fields. 2021. 2\n[73] Francis Williams, Zan Gojcic, Sameh Khamis, Denis Zorin,\nJoan Bruna, Sanja Fidler, and Or Litany. Neural fields as\nlearnable kernels for 3d reconstruction. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18500\u201318510, 2022. 2\n[74] Francis Williams, Matthew Trager, Joan Bruna, and Denis\nZorin. Neural splines: Fitting 3d surfaces with infinitely-\nwide neural networks. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9949\u20139958, 2021. 2\n[75] Jelmer M Wolterink, Jesse C Zwienenberg, and Christoph\nBrune. Implicit neural representations for deformable image\nregistration. In International Conference on Medical Imag-\ning with Deep Learning, pages 1349\u20131359. PMLR, 2022. 1\n[76] Zhijie Wu, Yuhe Jin, and Kwang Moo Yi. Neural fourier\nfilter bank.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 14153\u2013\n14163, 2023. 3\n[77] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir\nMech, and Ulrich Neumann. Disn: Deep implicit surface\nnetwork for high-quality single-view 3d reconstruction. Ad-\nvances in neural information processing systems, 32, 2019.\n2\n[78] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 3, 5, 7, 8, 21\n[79] Xiangyu Xu, Lichang Chen, Changjiang Cai, Huangying\nZhan, Qingan Yan, Pan Ji, Junsong Yuan, Heng Huang, and\nYi Xu. Dynamic voxel grid optimization for high-fidelity\nrgb-d supervised surface reconstruction.\narXiv preprint\narXiv:2304.06178, 2023. 1\n[80] Guandao Yang, Sagie Benaim, Varun Jampani, Kyle Genova,\nJonathan Barron, Thomas Funkhouser, Bharath Hariharan,\nand Serge Belongie. Polynomial neural fields for subband\ndecomposition and manipulation. Advances in Neural Infor-\nmation Processing Systems, 35:4401\u20134415, 2022. 2, 6\n[81] Runzhao Yang, Tingxiong Xiao, Yuxiao Cheng, Qianni Cao,\nJinyuan Qu, Jinli Suo, and Qionghai Dai.\nSci: A spec-\ntrum concentrated implicit neural compression for biomedi-\ncal data. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 37, pages 4774\u20134782, 2023. 1\n[82] Runzhao Yang, Tingxiong Xiao, Yuxiao Cheng, Jinli Suo,\nand Qionghai Dai.\nTinc: Tree-structured implicit neural\ncompression. pages 18517\u201318526, 2023. 3\n[83] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. Advances in Neu-\nral Information Processing Systems, 34:4805\u20134815, 2021. 1\n[84] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2, 3\n[85] Gizem Y\u00a8uce, Guillermo Ortiz-Jim\u00b4enez, Beril Besbinar, and\nPascal Frossard.\nA structured dictionary perspective on\nimplicit neural representations.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19228\u201319238, 2022. 2\n[86] Yunfan Zhang, Ties van Rozendaal, Johann Brehmer,\nMarkus Nagel, and Taco Cohen. Implicit neural video com-\npression. arXiv preprint arXiv:2112.11312, 2021. 1\n[87] Jianqiao\nZheng,\nSameera\nRamasinghe,\nand\nSimon\nLucey.\nRethinking positional encoding.\narXiv preprint\narXiv:2107.02561, 2021. 2\nA. Illustration of the Discontinuity in Adaptive\nRBFs\nAs shown in Fig. 11, consider a simple 1D case where\nx1 \u2248 x2 are two points located on the boundary where\nU(x) changes. Let U(x1) = {1}, U(x2) = {2} be the\nsets of their closest RBF. From Eq. (2) in the paper, the\naggregated neural feature gb(x) is computed as gb(x) =\nP\ni\u2208U(x) \u03c6(x, ci, \u03a3i)wi.\nGenerally, for adaptive RBFs,\n\u03c6(x1, c1, \u03a31) \u0338\u2248 \u03c6(x2, c2, \u03a32) and w1 \u0338\u2248 w2. Therefore,\ngb(x1) \u0338\u2248 gb(x2). This reveals a discontinuity in gb(x)\nwhen x changes from x1 to x2. On the other hand, for grid-\nbased RBFs that use linear interpolation as kernel function,\nboth \u03c6(x1, c1) and \u03c6(x2, c2) are close to 0, so gb(x) does\nnot contain such discontinuity. We combine adaptive and\ngrid-based RBFs through feature concatenation to balance\nfitting accuracy and interpolation smoothness.\nB. Details on RBF Initialization\nWe utilize the EM-style Lloyd\u2019s K-Means algorithm to\ninitialize RBF positions using all points. The number of\nRBFs is calculated based on parameter budget. The initial-\nization is conducted only once per scene, before the start of\ntraining. We do not split or merge RBFs during training.\nDuring weighted K-Means, the initial centers are generated\nby weighted random sampling. We do not repeat this ran-\ndom sampling for multiple times because we observe it does\nnot have major influence on final performance. The E-M\nsteps are the following:\naij =\n\uf8f1\n\uf8f2\n\uf8f3\n1, if i = arg min\nk\n\u2225xj \u2212 ck\u22252,\n0, otherwise.\n(10)\nci =\nP\nj aijwjxj\nP\nj aijwj\n.\n(11)\naij is an indicator variable: aij = 1 if xj is assigned to\ncluster i and aij = 0 otherwise. For efficiency, we iterate\nEq. (10)(11) for only 10 steps as the results are already close\nto convergence and sufficient for our use case. We imple-\nment the E-M steps with parallel KD Tree and vectorized\ncentroid update.\nC. More Ablation Study\nC.1. RBF Initialization\nTo evaluate the effects of RBF initialization, we compare\nweighted K-Means with grid initialization, random initial-\nization and weighted random initialization. As shown in\nFig. 12, we use an image from DIV2K dataset [3, 71] and\nconduct 2D image fitting. To facilitate visualization, we\nonly use 15129 RBFs in each baseline. We visualize the po-\nsition and shape parameters of RBFs as yellow ellipses, and\nGrid-Based RBFs\nAdaptive RBFs\nFigure 11. Illustration of the discontinuity in adaptive RBFs.\nshow the fitting error maps and PSNR. As demonstrated in\nthe top two rows, weighted K-Means initialization achieves\nthe highest fitting accuracy. Among the other three base-\nlines, weighted random initialization has a competitve per-\nformance while random initialization leads to the worst re-\nsult.\nIn the bottom four rows, we further evaluate the effec-\ntiveness of using gradient backpropagation to finetune RBF\nparameters during training. We first use a set of reasonable\nlearning rates for position and shape parameters, which are\nobtained through grid search on the baseline with weighted\nrandom initialization. As shown in the middle two rows,\ngradient backpropagation (with only L2 loss on pixel value)\nonly provides minor improvement compared to the first two\nrows. Besides, the update to the RBF parameters is barely\nnoticeable for grid initialization. Then, we experiment with\nlarge learning rates in the last two rows. It can be seen that\nthe RBF parameters can be largely changed from their ini-\ntialization. However, this leads to significant performance\ndrop for all baselines. The above results validate the bene-\nfits of RBF initialization.\nC.2. Adaptive Positions and Generalized Interpola-\ntion\nHere, we evaluate the effects of using adaptive positions\nfor RBFs and generalizing N-dimensional linear interpola-\ntion to RBFs with shape parameters. We conduct this abla-\ntion study on image, SDF and NeRF tasks, and the results\nare shown in Fig. 13. The parameter count of each model\nis 567K, 856K and 17.7M respectively for the three tasks.\nBased on the results, both adaptive positions and general-\nized interpolation are beneficial to performance.\nD. More Implementation Details\nArchitecture.\nFor the decoder network gm, except NeRF\ntask, we use a 3-layer MLP (2 hidden layers + 1 output\nlayer) with a network width of 64 neurons, where rectified\nlinear unit (ReLU) activation function is applied to the sec-\nond hidden layer. The MLP uses a very small part of the pa-\nrameters (e.g., only 7K in image fitting). For the NeRF ex-\nperiments on the Synthetic NeRF dataset [47], we use a sin-\ngle Softplus layer as density decoder (same as TensoRF [9])\nand use the rendering equation encoding from NRFF [28] as\nWeighted K-Means Init.\nWeighted Random Init.\nRandom Init.\nGrid Init.\nNo Finetuning\nGrid-Searched\nLearning Rates\nLarge\nLearning Rates\nPSNR\u2191 : 32.73 dB\n31.61 dB\n34.17 dB\n35.24 dB\n32.56 dB\n31.57 dB\n34.79 dB\n35.71 dB\n22.93 dB\n22.80 dB\n23.40 dB\n23.56 dB\n0.05\n0\nFigure 12. Evaluation on RBF Initialization. We compare different RBF initialization methods in columns: grid initialization, random\ninitialization, weighted random initialization and weighted K-Means initialization. We also evaluate different RBF finetuning strategies\nusing gradient backpropagation (with only L2 loss on pixel value) in rows: no finetuning, grid-searched learning rates, large learning rates.\nFor each result, we visualize the RBF parameters as yellow ellipses and show fitting error maps.\ncolor decoder. For the NeRF experiments on the real LLFF\nForward-Facing dataset [46], we adopt the same network\narchitecture as K-Planes-hybrid [25], which uses a 2-layer\nMLP for density decoder and a 3-layer MLP for color de-\ncoder.\nExperiments on 2D Image Fitting.\nThe neural features\nwi of adaptive RBFs have a channel dimension of 32. The\nneighboring RBFs U(x) of a point x is its 4 nearest neigh-\nbors.\nFor Instant NGP [48], we use their official open-\nsourced codes and hyper-parameters in the comparison ex-\nperiments. Note that some results in their paper use smaller\nhash table sizes, hence fewer parameters but also lower\nPSNR. For MINER [58], we use the implementation from\nMINER pl1. The original MINER paper does not report\ntheir results on the DIV2K dataset [3, 71]. All methods use\na batch size of 262144 and are trained for a same number of\nsteps.\n1https://github.com/kwea123/MINER_pl\nImage\nPSNR\u2191 : 34.35 dB\n31.97 dB\n33.14 dB\nGT\nFull\nNo Adaptive Positions\nNo Generalized Interp.\n2040 \u00d7 1356 \u00d7 3\nNAE\u2193 : 5.30\u00b0\n6.81\u00b0\n6.48\u00b0\nPSNR\u2191 : 37.56 dB\n37.42 dB\n37.44 dB\nSDF\nNeRF\nFigure 13. Evaluation on the adaptive positions and generalized interpolation of RBFs. \u201cNo Adaptive Positions\u201d: the positions of\nRBFs are fixed to a grid structure. \u201cNo Generalized Interp.\u201d: the interpolation function is N-dimensional linear interpolation.\nExperiments on 3D SDF Reconstruction.\nThe neural\nfeatures wi of adaptive RBFs have a channel dimension of\n16, and the size of neighborhood U(x) is 8. We use a grid\nresolution of 10243 for marching cubes. IoU is evaluated\non these grid points. For normal angular error (NAE), it\nis computed similarly as normal consistency [17], but is in\nunit of degree. Specifically, let P1, P2 be randomly sam-\npled points on two mesh surfaces, NN(x, P) be the closest\npoint to x in point set P, and nf(x) \u2208 R3\u00d71 be the unit face\nnormal of point x, NAE is calculated as:\nNAE(P1, P2) = 1\n2 \u00b7 180\n\u03c0\n\u00b7 (\n1\n|P1|\nX\np1\u2208P1\narccos\n\u0000nf(p1)T nf(NN(p1, P2))\n\u0001\n+\n1\n|P2|\nX\np2\u2208P2\narccos\n\u0000nf(p2)T nf(NN(p2, P1))\n\u0001\n).\n(12)\nExperiments on Neural Radiance Field Reconstruction.\nFor the Synthetic NeRF dataset [47], the channel dimension\nof adaptive RBFs is 32 and the size of neighborhood U(x) is\n5. We first train the grid-based part for 1000 steps, which is\nthen used to distill scene information and conduct RBF ini-\ntialization. For the real LLFF Forward-Facing dataset [46],\nthe channel dimension of adaptive RBFs is 16 and the size\nof neighborhood U(x) is 8. The grid-based model used for\ndistillation is trained for 2000 steps while the full model is\ntrained for 38000 steps.\nE. Limitations and Future Work\nIn this work, we have primarily focused on local neu-\nral representation.\nIt could be promising to explore the\ncombination with other activation functions in MLP (e.g.,\nWIRE [57]). Besides, in our current implementation, the\nmultipliers m, m0 are treated as hyper-parameters and are\nnot trainable. We tried training them along with other pa-\nrameters, but observed little improvement. A possible rea-\nson is that they act as frequencies and would require tailored\noptimization techniques.\nOur method demonstrates high representation accuracy\nin spatial domains; however, similar to Instant NGP [48]\nand Factor Fields [10], we have not explored spatial-\ntemporal tasks such as dynamic novel view synthesis. By\nextending radial basis functions into higher dimensions or\nusing dimension decomposition techniques, our method can\npotentially be applied to these tasks. We also observe that it\nis difficult to represent large-scale complicated signals with\nboth high accuracy and small model size, which is a com-\nAvg.\nArmadillo\nBunny\nDragon\nBuddha\nLucy\nXYZ Dragon\nStatuette\nDavid\nChameleon\nMechanism\nNAE\u2193\nNGLOD5 [69]\n6.58\n3.60\n4.81\n2.85\n3.28\n4.73\n5.66\n7.53\n3.43\n15.91\n14.00\nNGLOD6 [69]\n6.14\n3.35\n4.47\n2.76\n3.02\n4.28\n5.15\n6.46\n3.22\n14.64\n14.03\nI-NGP [48]\n5.70\n2.89\n1.96\n2.30\n2.73\n3.57\n4.51\n6.00\n2.88\n11.96\n18.21\nOurs\n4.93\n2.83\n2.00\n2.22\n2.69\n3.36\n4.14\n5.30\n2.62\n10.42\n13.73\nI-NGP400K [48]\n6.39\n3.20\n2.22\n2.64\n3.18\n4.29\n4.96\n6.82\n3.27\n13.04\n20.31\nOurs400K\n5.53\n2.89\n2.14\n2.35\n2.88\n3.70\n4.44\n6.07\n2.85\n11.96\n16.00\nIoU\u2191\nNGLOD5 [69]\n0.9962\n0.99974\n0.97664\n0.99964\n0.99977\n0.99979\n0.99981\n0.99969\n0.99960\n0.99456\n0.99237\nNGLOD6 [69]\n0.9963\n0.99979\n0.97696\n0.99969\n0.99977\n0.99986\n0.99983\n0.99980\n0.99963\n0.99528\n0.99237\nI-NGP [48]\n0.9994\n0.99997\n0.99968\n0.99995\n0.99996\n0.99997\n0.99996\n0.99993\n0.99993\n0.99893\n0.99605\nOurs\n0.9995\n0.99994\n0.99943\n0.99995\n0.99996\n0.99996\n0.99996\n0.99995\n0.99993\n0.99765\n0.99837\nI-NGP400K [48]\n0.9992\n0.99995\n0.99974\n0.99994\n0.99994\n0.99996\n0.99995\n0.99990\n0.99990\n0.99820\n0.99448\nOurs400K\n0.9994\n0.99996\n0.99964\n0.99995\n0.99995\n0.99997\n0.99995\n0.99991\n0.99992\n0.99706\n0.99767\nTable 6. 3D Signed Distance Field Reconstruction. Per-object breakdown of the quantitative metrics (NAE\u2193 and IoU\u2191) in Table 2 of the\npaper.\nmon challenge for local neural fields methods. An interest-\ning future direction would be to design basis functions with\nmore adaptive shapes and long-range support.\nF. Additional Results\nF.1. 2D Image Fitting\nFig. 14 compares the results on 4 ultra-high resolution\nimages that are not displayed in the paper due to page limit.\nFor the error maps, we calculate the mean absolute error\nacross color channels for each pixel. To highlight the differ-\nence among methods, we set the color bar range as 0 \u223c 0.01\n(the range of pixel value is 0 \u223c 1).\nFor the Pluto image (Fig. 4 row 2 in the paper), when\nfitting the 16 megapixel version of it, our method can reach\n44.13 dB PSNR with 7.8M parameters and 50s training.\nF.2. 3D SDF Reconstruction\nTable 6 shows per-object breakdown of the quantita-\ntive metrics (NAE\u2193 and IoU\u2191) in Table 2 of the paper.\nFig. 15, 16 show the qualitative results, where the numbers\nof trainable parameters for Instant NGP and ours are 950K\nand 856K.\nWe further compare with BACON [40] and let our\nmethod use the same training settings as them.\nBA-\nCON uses 531K parameters while our models only use\n448K. Averaging over 4 scenes (Armadillo, Lucy, XYZ\nDragon, Statuette), the normal angular errors (NAE\u2193) are\n5.89\u25e6(BACON) vs. 4.53\u25e6(Ours).\nF.3. Neural Radiance Field Reconstruction\nTable 7 and 8 demonstrate the per-scene quantitative\ncomparisons (PSNR\u2191, SSIM\u2191, LPIPSV GG\u2193, LPIPSAlex\u2193)\non the Synthetic NeRF dataset [47] and the real LLFF\nForward-Facing dataset [46].\nFig. 17 and Fig. 18 show\nmore close-up and full-image comparisons on the Synthetic\nNeRF dataset [47]. Fig. 19 shows full-image comparisons\non the real LLFF Forward-Facing dataset [46].\n0.01\n0\n# Tr. Params\u2193 : 45.18M | PSNR\u2191 : 49.91 dB\nSSIM\u2191 : 0.997 | LPIPSAlex\u2193 : 6.55\u00d710-4\n35.85M | 50.02 dB\n0.997 | 8.07\u00d710-4\n33.43M | 56.86 dB\n0.999 | 2.20\u00d710-4\n6000 \u00d7 4000 \u00d7 3\nMINER\nInstant NGP\nOurs\nOurs\n90.89M | 39.12 dB\n0.982 | 1.61\u00d710-2\n71.86M | 39.59 dB\n0.980 | 1.46\u00d710-2\n67.00M | 44.19 dB\n0.988 | 1.22\u00d710-2\n8000 \u00d7 9302 \u00d7 3\n137.38M | 44.08 dB\n0.989 | 1.73\u00d710-2\n29164 \u00d7 8592 \u00d7 3\n147.34M | 40.87 dB\n0.982 | 3.09\u00d710-2\n188.43M | 39.91 dB\n0.985 | 2.43\u00d710-2\nInstant NGP\nMINER\n178.06M | 38.59 dB\n0.926\n56718 \u00d7 21450 \u00d7 3\n181.42M | 37.74 dB\n0.915\n196.22M | 37.64 dB\n0.918\nInstant NGP\nOurs\nMINER\nFigure 14. 2D Image Fitting. Leftmost column or top left quarter shows the fitted images of our method and the resolution of the images.\nThe other columns or quarters show the error maps of each method, along with the number of trainable parameters (\u201c# Tr. Params\u201d)\u2193,\nPSNR\u2191, SSIM\u2191 and LPIPSAlex\u2193. For the last image, its resolution is too high to compute LPIPSAlex. \u201cGirl With a Pearl Earring\u201d\nrenovation \u00a9Koorosh Orooj (CC BY-SA 4.0).\nNGLOD6\nInstant NGP\nOurs\nGT\nArmadillo\nBunny\nDragon\nBuddha\nLucy\nFigure 15. 3D SDF Reconstruction. Qualitative comparisons between NGLOD6 [69], Instant NGP [48] and ours. For the results in this\nfigure, the number of trainable parameters of Instant NGP is 950K, while that of ours is 856K. (To be continued in the next page.)\nXYZ Dragon\nStatuette\nDavid\nChameleon\nMechanism\nNGLOD6\nInstant NGP\nOurs\nGT\nFigure 16. 3D SDF Reconstruction. Qualitative comparisons between NGLOD6 [69], Instant NGP [48] and ours. For the results in this\nfigure, the number of trainable parameters of Instant NGP is 950K, while that of ours is 856K.\nMethods\nAvg.\nChair\nDrums\nFicus\nHotdog\nLego\nMaterials\nMic\nShip\nPSNR\u2191\nNeRF [47]\n31.01\n33.00\n25.01\n30.13\n36.18\n32.54\n29.62\n32.91\n28.65\nMip-NeRF 360 [4]\n33.25\n-\n-\n-\n-\n-\n-\n-\n-\nPoint-NeRF [78]\n33.31\n35.40\n26.06\n36.13\n37.30\n35.04\n29.61\n35.95\n30.97\nPlenoxels [26]\n31.71\n33.98\n25.35\n31.83\n36.43\n34.10\n29.14\n33.26\n29.62\nInstant NGP [48]\n33.18\n35.00\n26.02\n33.51\n37.40\n36.39\n29.78\n36.22\n31.10\nTensoRF [9]\n33.14\n35.76\n26.01\n33.99\n37.41\n36.46\n30.12\n34.61\n30.77\nFactor Fields [10]\n33.14\n-\n-\n-\n-\n-\n-\n-\n-\nK-Planes [25]\n32.36\n34.99\n25.66\n31.41\n36.78\n35.75\n29.48\n34.05\n30.74\nOurs\n34.62\n36.74\n26.47\n35.14\n38.65\n37.53\n34.30\n36.17\n31.94\nOurs3.66M\n33.97\n35.82\n26.19\n34.08\n38.11\n36.75\n34.32\n35.49\n31.03\nSSIM\u2191\nNeRF [47]\n0.947\n0.967\n0.925\n0.964\n0.974\n0.961\n0.949\n0.980\n0.856\nMip-NeRF 360 [4]\n0.962\n-\n-\n-\n-\n-\n-\n-\n-\nPoint-NeRF [78]\n0.962\n0.984\n0.935\n0.987\n0.982\n0.978\n0.948\n0.990\n0.892\nPlenoxels [26]\n0.958\n0.977\n0.933\n0.976\n0.980\n0.976\n0.949\n0.985\n0.890\nInstant NGP [48]\n0.963\n0.985\n0.940\n0.982\n0.982\n0.982\n0.949\n0.989\n0.893\nTensoRF [9]\n0.963\n0.985\n0.937\n0.982\n0.982\n0.983\n0.952\n0.988\n0.895\nFactor Fields [10]\n0.961\n-\n-\n-\n-\n-\n-\n-\n-\nK-Planes [25]\n0.962\n0.983\n0.938\n0.975\n0.982\n0.982\n0.950\n0.988\n0.897\nOurs\n0.975\n0.988\n0.946\n0.987\n0.987\n0.986\n0.980\n0.992\n0.930\nOurs3.66M\n0.971\n0.985\n0.942\n0.984\n0.985\n0.984\n0.980\n0.990\n0.919\nLPIPSV GG\u2193\nNeRF [47]\n0.081\n0.046\n0.091\n0.044\n0.121\n0.050\n0.063\n0.028\n0.206\nMip-NeRF 360 [4]\n0.039\n-\n-\n-\n-\n-\n-\n-\n-\nPoint-NeRF [78]\n0.050\n0.023\n0.078\n0.022\n0.037\n0.024\n0.072\n0.014\n0.124\nPlenoxels [26]\n0.049\n0.031\n0.067\n0.026\n0.037\n0.028\n0.057\n0.015\n0.134\nInstant NGP [48]\n0.051\n0.023\n0.076\n0.027\n0.038\n0.021\n0.065\n0.020\n0.137\nTensoRF [9]\n0.047\n0.022\n0.073\n0.022\n0.032\n0.018\n0.058\n0.015\n0.138\nFactor Fields [10]\n-\n-\n-\n-\n-\n-\n-\n-\n-\nK-Planes [25]\n0.062\n0.027\n0.089\n0.056\n0.034\n0.047\n0.068\n0.029\n0.148\nOurs\n0.034\n0.015\n0.059\n0.014\n0.021\n0.015\n0.031\n0.008\n0.110\nOurs3.66M\n0.039\n0.019\n0.065\n0.019\n0.025\n0.018\n0.034\n0.010\n0.124\nLPIPSAlex\u2193\nPoint-NeRF [78]\n0.028\n0.010\n0.055\n0.009\n0.016\n0.011\n0.041\n0.007\n0.070\nInstant NGP [48]\n0.028\n0.0097\n0.0540\n0.0174\n0.0142\n0.0085\n0.0296\n0.0072\n0.0863\nTensoRF [9]\n0.027\n0.010\n0.051\n0.012\n0.013\n0.007\n0.026\n0.009\n0.085\nK-Planes [25]\n0.031\n0.0125\n0.0527\n0.0209\n0.0170\n0.0096\n0.0303\n0.0091\n0.0968\nOurs\n0.018\n0.0067\n0.0409\n0.0085\n0.0085\n0.0057\n0.0106\n0.0044\n0.0614\nOurs3.66M\n0.022\n0.0088\n0.0454\n0.0101\n0.0109\n0.0070\n0.0119\n0.0060\n0.0735\nTable 7. Neural Radiance Field Reconstruction. Per-scene quantitative comparisons (PSNR\u2191, SSIM\u2191, LPIPSV GG\u2193, LPIPSAlex\u2193) on\nthe Synthetic NeRF dataset [47]. Best 3 scores in each scene are marked with gold , silver\nand bronze . \u201c-\u201d denotes scores that are\nunavailable in prior work. For LPIPSAlex, since the scores of NeRF [47], Mip-NeRF 360 [4], Plenoxels [26] and Factor Fields [10] are\nunavailable in prior work, we exclude these methods in this metric.\nGround Truth\nOurs\nK-Planes\nTensoRF\nInstant NGP\nPoint-NeRF\nFigure 17. Neural Radiance Field Reconstruction. More close-up comparisons on the Synthetic NeRF Dataset [47]. Leftmost column\nshows the full-image results of our method.\nGround Truth\nOurs\nK-Planes\nTensoRF\nInstant NGP\nPoint-NeRF\nFigure 18. Neural Radiance Field Reconstruction. Full-image comparisons on the Synthetic NeRF Dataset [47].\nMethods\nAvg.\nRoom\nFern\nLeaves\nFortress\nOrchids\nFlower\nT-Rex\nHorns\nPSNR\u2191\nNeRF [47]\n26.50\n32.70\n25.17\n20.92\n31.16\n20.36\n27.40\n26.80\n27.45\nMip-NeRF 360 [4]\n26.86\n-\n-\n-\n-\n-\n-\n-\n-\nPlenoxels [26]\n26.29\n30.22\n25.46\n21.41\n31.09\n20.24\n27.83\n26.48\n27.58\nTensoRF [9]\n26.73\n32.35\n25.27\n21.30\n31.36\n19.87\n28.60\n26.97\n28.14\nK-Planes [25]\n26.92\n32.64\n25.38\n21.30\n30.44\n20.26\n28.67\n28.01\n28.64\nOurs\n27.05\n32.80\n25.48\n21.81\n30.98\n20.03\n28.57\n28.06\n28.68\nSSIM\u2191\nNeRF [47]\n0.811\n0.948\n0.792\n0.690\n0.881\n0.641\n0.827\n0.880\n0.828\nMip-NeRF 360 [4]\n0.858\n-\n-\n-\n-\n-\n-\n-\n-\nPlenoxels [26]\n0.839\n0.937\n0.832\n0.760\n0.885\n0.687\n0.862\n0.890\n0.857\nTensoRF [9]\n0.839\n0.952\n0.814\n0.752\n0.897\n0.649\n0.871\n0.900\n0.877\nK-Planes [25]\n0.847\n0.957\n0.828\n0.746\n0.890\n0.676\n0.872\n0.915\n0.892\nOurs\n0.849\n0.955\n0.822\n0.769\n0.891\n0.675\n0.868\n0.916\n0.895\nLPIPSV GG\u2193\nNeRF [47]\n0.250\n0.178\n0.280\n0.316\n0.171\n0.321\n0.219\n0.249\n0.268\nPlenoxels [26]\n0.210\n0.192\n0.224\n0.198\n0.180\n0.242\n0.179\n0.238\n0.231\nTensoRF [9]\n0.204\n0.167\n0.237\n0.217\n0.148\n0.278\n0.169\n0.221\n0.196\nK-Planes [25]\n0.194\n0.147\n0.223\n0.242\n0.154\n0.250\n0.165\n0.199\n0.173\nOurs\n0.179\n0.134\n0.209\n0.238\n0.128\n0.271\n0.147\n0.158\n0.149\nLPIPSAlex\u2193\nTensoRF [9]\n0.124\n0.082\n0.155\n0.153\n0.075\n0.201\n0.106\n0.099\n0.123\nK-Planes [25]\n0.102\n0.066\n0.130\n0.153\n0.068\n0.151\n0.088\n0.071\n0.092\nOurs\n0.090\n0.059\n0.111\n0.127\n0.056\n0.160\n0.072\n0.057\n0.075\nTable 8. Neural Radiance Field Reconstruction. Per-scene quantitative comparisons (PSNR\u2191, SSIM\u2191, LPIPSV GG\u2193, LPIPSAlex\u2193) on\nthe real LLFF Forward-Facing dataset [46]. Best 3 scores in each scene are marked with gold , silver\nand bronze . \u201c-\u201d denotes scores\nthat are unavailable in prior work. For LPIPSAlex, since the scores of NeRF [47], Mip-NeRF 360 [4] and Plenoxels [26] are unavailable in\nprior work, we exclude these methods in this metric.\nGround Truth\nOurs\nK-Planes\nTensoRF\nFigure 19. Neural Radiance Field Reconstruction. Full-image comparisons on the real LLFF Forward-Facing dataset [46].\n"
  },
  {
    "title": "Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition",
    "link": "https://arxiv.org/pdf/2309.15223.pdf",
    "upvote": "12",
    "text": "LOW-RANK ADAPTATION OF LARGE LANGUAGE MODEL RESCORING FOR\nPARAMETER-EFFICIENT SPEECH RECOGNITION\nYu Yu\u2217, Chao-Han Huck Yang, Jari Kolehmainen, Prashanth G. Shivakumar, Yile Gu, Sungho Ryu\nRoger Ren, Qi Luo, Aditya Gourav, I-Fan Chen, Yi-Chieh Liu, Tuan Dinh, Ankur Gandhe\nDenis Filimonov, Shalini Ghosh, Andreas Stolcke, Ariya Rastow, Ivan Bulyko\nAmazon, USA\n\u2217Stevens Institute of Technology, USA\nABSTRACT\nWe propose a neural language modeling system based on\nlow-rank adaptation (LoRA) for speech recognition out-\nput rescoring. Although pretrained language models (LMs)\nlike BERT have shown superior performance in second-pass\nrescoring, the high computational cost of scaling up the pre-\ntraining stage and adapting the pretrained models to specific\ndomains limit their practical use in rescoring. Here we present\na method based on low-rank decomposition to train a rescor-\ning BERT model and adapt it to new domains using only a\nfraction (0.08%) of the pretrained parameters. These inserted\nmatrices are optimized through a discriminative training ob-\njective along with a correlation-based regularization loss. The\nproposed low-rank adaptation RescoreBERT (LoRB) archi-\ntecture is evaluated on LibriSpeech and internal datasets with\ndecreased training times by factors between 5.4 and 3.6.\nIndex Terms\u2014 Low-rank adaptation, neural language\nmodel rescoring, parameter-efficient speech recognition\n1. INTRODUCTION\nSecond-pass rescoring is a widely explored technique to\nimprove the performance of automatic speech recognition\n(ASR) systems [1, 2, 3, 4, 5]. Language models in different\narchitectures, such as long short-term memory (LSTM) [6]\nand transformer [7], have proven effective as N-best rescor-\ners [8] to boost the performance of first-pass decoding. No-\ntably, transformers stand out among other language model\narchitectures due to their exceptional ability to model long-\nrange dependencies and context within the input. Addition-\nally, large language models (LLMs) such as GPT-2 [9] and\nBERT [10], which are based on transformers, have the advan-\ntage of incorporating both linguistic and world knowledge.\nAs a result, LLMs have been used in extensive applications\nacross many natural language processing tasks.\nLLMs are conventionally pretrained on massive unla-\nbelled data sets and fine-tuned on some smaller labelled\n\u2217Work done as an applied scientist intern at Amazon Alexa.\ndatasets for adaptation to downstream tasks.\nHowever, as\nthe size of the pretrained models increases, the cost as-\nsociated with fine-tuning and deploying these models for\nreal-world applications also escalates. To address this prac-\ntical challenge, a range of parameter-efficient methods (e.g.,\nadapters, model reprogramming, and prompts) have been pro-\nposed [11, 12, 13, 14, 15, 16, 17, 18] to alleviate the compu-\ntation and memory demands of fine-tuning LLMs. Low-rank\nadaptation (LoRA) [19] freezes all pretrained parameters in\nthe LLM and inserts a trainable pair of matrices (acting as a\nlow-rank decomposition of a full matrix) additively into each\nlayer of the Transformer architecture.\nCompared to other\nparameter-efficient training methods, such as adapters [12],\nLoRA has two distinct advantages: 1) it employs a simple\narchitecture and has the potential to reduce the number of\ntrainable parameters compared to alternatives; 2) LoRA does\nnot introduce any additional inference latency, making it an\nexcellent choice for deployment in production environments.\nIn this work, we explore low-rank adaptation for lan-\nguage model rescoring to achieve a favorable trade-off be-\ntween computational efficiency and speech recognition per-\nformance. Specifically, we follow the discriminative training\nobjective proposed in [20] to directly optimize the minimum\nword error rate, as described in Section 3.1. During train-\ning, we freeze all layers in BERT and only update low-rank\nmatrices inserted at each transformer layer, as discussed in\nSection 3.2. As a result, the memory required to store the\ntrainable parameters and the backward-pass computation are\nboth reduced. Meanwhile, it is worth noting that we have\nobserved that LoRA can lead to a degraded representation,\nsimilar to full fine-tuning [21], which can consequently af-\nfect performance on unseen test domains. To mitigate this\nnegative effect, we further apply a correlation-based regular-\nization in addition to the minimum word error loss, as shown\nin Section 3.3.\nThe proposed Low-rank Rescoring for BERT (LoRB) is\nevaluated on both a public dataset and internal datasets cov-\nering a range of domains. We show that LoRB can achieve\ncomparable performance on the target domain and even bet-\narXiv:2309.15223v2  [cs.CL]  10 Oct 2023\nter performance on non-target domains, as compared to full\nfine-tuning and other parameter-efficient methods, using only\n0.08% of the trainable parameters updated in fine-tuning. Ad-\nditionally, LoRB can save up to 32% training memory utiliza-\ntion and achieve up to 6-fold reduction in training times, by\nallowing training with a larger learning rate.\n2. RELATED WORK\n2.1. Low-rank adaptation\nLoRA has been widely investigated in the natural language\nprocessing (NLP) domain. For example, [22] explores an\nautomatic way to select the optimal rank value of LoRA ma-\ntrices. [23, 24] discuss the most effective transformer mod-\nules in which to insert LoRA matrices, while [25] examines\nthe parameter allocation among weight matrices. Some stud-\nies have investigated the underlying reasons for the effective-\nness of LoRA. [26, 27] discovered that the sparsity of learned\nweights imposes a regularization effect on the original model,\nresulting in improved generalization. [28] demonstrated that\nconstraining the dimensionality of the optimization problem\ncan effectively mitigate catastrophic forgetting. Beyond NLP,\nlow-rank adaptation has also been applied in vision tasks by\nfine-tuning of vision transformers [28, 29, 30]. However, it\nremains to be seen whether the findings for NLP and vision\ntasks can be transferred to second-pass rescoring in automatic\nspeech recognition.\n2.2. Domain adaptation for ASR\nIn the domain adaptation research for ASR, the focus has\nbeen largely on first-pass acoustic models. Strategies such as\ncontextual biasing have been widely used for RNN-T mod-\nels [31, 32]. Additionally, for low-resource target domains,\nself-supervised training and semi-supervised training strate-\ngies have been explored [33, 34, 35] using speech model re-\nprogramming or adapters.\nFor second-pass models, [36] explored fine-tuning a gen-\neral rescoring model for new domains and incorporating a\ndomain classifier to switch between domain-specific models.\n[37] proposed training of prompt embeddings for target do-\nmains and attaching them to the N-best list before scoring\nwith the rescoring GPT2 model. However, this method in-\ntroduces additional inference latency due to the prepended\nprompts. Our work, by contrast, aims to explore the gen-\neralization effects of low-rank parameter-efficient fine-tuning\nmethods, while reducing the computational cost of domain\nadaptation without introducing additional inference latency.\n3. APPROACH\n3.1. Discriminative training for second-pass rescoring\n3.1.1. Second-pass rescoring\nIn this section, we formulate the second-pass rescoring task.\nGiven an N-best hypothesis list E = {E1, E2, . . . , En} ob-\ntained from the beam search in the decoder based on the first-\npass acoustic model, the rescoring model will generate scores\nfor each hypothesis. For any hypothesis Ei \u2208 E, denote by sa\ni\nthe score given by the first pass, and by sl\ni the score produced\nby the second pass. For both passes, the score of a hypothe-\nsis represents the negative log likelihood, thus a lower score\nrepresents a more likely hypothesis.\nThe language model, such as BERT, takes a hypothesis\nand outputs a hidden representation gi, then the feed-forward\nnetwork takes the representation of the task-specific [CLS]\ntoken as input and derives the second-pass score sl\ni, as shown\nby Equation (2):\ngi = BERT(Ei)\n(1)\nsl\ni = FFNN(gCLS\ni\n)\n(2)\nThe final score of a hypothesis is the linear combination\nof the first- and second-pass scores:\nsi = sa\ni + \u03b2 \u00b7 sl\ni\n(3)\n3.1.2. Discriminative training objective\nDiscriminative training has been widely explored for second-\npass rescoring.\nSpecifically, BERT as a masked language\nmodel has been applied to second-pass rescoring [20] by\ntraining with a discriminative objective of minimum word er-\nror rate (MWER) [38]. Given a hypothesis Ei \u2208 E, denote by\n\u03f5i the number of word errors (edit distance) from the ground\ntruth transcription. The MWER loss function is defined as the\nexpected number of word errors for the N-best hypothesis, as\nshown by Equation (6):\nPi =\ne\u2212si\nPn\nj=1 e\u2212sj\n(4)\n\u00af\u03f5H = 1\nn\nn\nX\ni=1\n\u03f5i\n(5)\nLMWER =\nn\nX\ni=1\nPi \u00b7 (\u03f5i \u2212 \u00af\u03f5H)\n(6)\n3.2. Low-rank adaptation to ASR rescoring\nIn the previous modification of BERT for the rescoring task,\nthe pretrained weights \u03a60 of BERT are updated to \u03a60+\u2206\u03a6 by\nfollowing the gradient for minimizing the MWER loss. The\nprocess of learning task-relevant parameters \u2206\u03a6 is known as\nthe full fine-tuning process. In the full fine-tuning process,\n...\n[CLS] fishing and fusion  \n[CLS] fishing and fasion  \n[CLS] fission and fusion \n...\nMulti-head\nAttention\nFeed Forward\nLayer Norm\nLow-Rank\nRescoreBERT\n(LoRB)\nLayer Norm\nQ\nK\nV\nLoRA\nWA\nWB\nTrainable\nFrozen (not updating)\nFeed Forward\nN-best hypotheses\n(from 1st pass ASR)\nCLS1\nCLS2\nCLS-N\n\u2026\nTransformer Block\nFig. 1. Illustration of the Low-Rank adaptation based Rescoring BERT (LoRB).\nthe dimension of the learned parameters |\u2206\u03a6| equals that of\nthe pretrained weights |\u03a60|.\nAs shown by [39], pretrained language models have a low\nintrinsic dimension and can learn efficiently through a low-\ndimensional reparameterization. Inspired by this finding and\nthe success of low-rank adaptation of large language models\nin NLP tasks [19], we propose adapting BERT for the rescor-\ning task by learning a low-rank representation \u0398 that has a\nmuch smaller dimension than \u03a60, or |\u0398| \u226a |\u03a60|.\nFormally, for any dense layer in the transformer blocks\nwith input x and output h, denote the pretrained weight as\nW0 \u2208 Rd\u00d7k, and the updates to the weight as \u2206W. We\nperform a low-rank decomposition to the updates \u2206W =\nWBWA, where WB\n\u2208 Rd\u00d7r, WA \u2208 Rr\u00d7k and r \u226a\nmin(d, k). The forward pass is modified to be\nh = W0x + \u2206Wx = W0x + WBWAx\n(7)\nDuring training, W0 is frozen and only WA and WB are up-\ndated. In BERT, LoRA can be applied to any subset of weight\nmatrices, for example, W0 could be Wq, Wk, Wv or Wo in-\nside a self-attention module, or be the weight matrices in the\ntwo-layer feed-forward network, i.e., Wf1 and Wf2.\n3.3. Multi-loss training with regularization\nFine-tuning large pretrained models often leads to overfitting\non the training data for downstream tasks [21, 40].\nEven\nthough some parameter-efficient fine-tuning methods are\nshown to be helpful in alleviating the overfitting issues by\nconstraining the number of trainable parameters [41, 42, 43],\nin some of our experiments a marginal degradation of perfor-\nmance on unseen test sets is observed when evaluating the\nLoRA fine-tuned rescoring model.\nIn order to obtain a hidden representation from the pre-\ntrained BERT with better generalization performance, we\nadd a correlation-based regularization loss Lcor besides the\nMWER loss:\nL = LMWER + \u03bbLcor\n(8)\nThe correlation-based regularization [44] has been pro-\nposed to alleviate the representation degeneration [45] prob-\nlem caused by fine-tuning on pretrained language models.\nBy forcing the feature space of representations to be more\nisotropic (uniformly variable in all directions), the expres-\nsiveness of the learned representation can be preserved better.\nFormally, the correlation-based regularization loss is defined\nso as to penalize the correlation matrix for sentence represen-\ntations for deviating from the identity:\nLcor = \u2225\u03a3 \u2212 I\u2225\n(9)\nwhere \u2225\u00b7\u2225 denotes the Frobenius norm, I \u2208 Rdh\u00d7dh is the\nidentity matrix, \u03a3 \u2208 Rdh\u00d7dh is the correlation matrix with\n\u03a3ij being the Pearson correlation coefficient between the ith\ndimension and the jth dimension of the hidden representation\nof the [CLS] token gCLS \u2208 Rdh. In the case of LoRB, only the\nLoRA matrices that contribute to the hidden representation of\nthe [CLS] token in each BERT layer are regularized by the\ncorrelation-matrix loss.\nTable 1. Relative WER improvement of LoRB, full fine-tuning (FT), Adapter and BitFit when fine-tuning on messaging data.\nTarget Domain\nNon-Target Domain\nMethod\n% Trainable\nMessagingTest\nGeneral\nShopping\nKnowledge\nParameters\nRescoreBERTpretrained 170M\nnon-adapted\nbaseline\nbaseline\nbaseline\nbaseline\nw/ Fine-Tuning (FT)\n100%\n3.30%\n-2.33%\n-1.17%\n-0.34%\nw/ Residual Adapter\n1.27%\n3.72%\n-16.60%\n-17.33%\n-17.07%\nw/ BitFit\n0.01%\n3.30%\n-18.83%\n-17.57%\n-20.90%\nw/ Prefix\n0.05%\n3.30%\n-1.98%\n-1.53%\n-1.39%\nLoRB\n0.08%\n6.06%\n0.27%\n0.23%\n0.34%\nLoRB + Lcor\n0.08%\n5.65%\n-0.51%\n0.82%\n0.01%\nLoRB\nLoRB+\ncor\nFT\nMethod\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nTime (h)\nFig. 2. Wall-clock training time of LoRB, LoRB+Lcor and\nFine-Tuning (FT) when training on messaging data.\n4. EXPERIMENTS\n4.1. Datasets\nThe training datasets for domain adaptation include one pub-\nlic dataset, LibriSpeech [46], and two internal datasets: Mes-\nsaging (350 hours) and Music (150 hours). Furthermore, we\nexplore the scaling behavior with regard to the sizes of the\npretrained model and the training data, using an internal con-\nversational domain dataset.\nWe evaluate the low-rank adaptation of the language\nmodel on three internal datasets drawn from from de-identified,\nfar-field English-language conversations with a voice assis-\ntant. The internal General domain set contains 194 hours, the\nShopping domain set contains 20 hours, and the Knowledge\ndomain set contains 5 hours of training data, respectively.\n4.2. Implementation\nIn the adaptation experiments, we vary the LoRA rank over\nthe values {4,8,16,32} and apply LoRA to two sets of tar-\nget modules: [Wq, Wv] and [Wq, Wk, Wv, Wf1, Wf2].\nIn the LoRA layer, we set the dropout rate to 0.01 and\n\u03b1 = 32. When fine-tuning RescoreBERT, we initialize the\nfeed-forward network in RescoreBERT from the pretrained\nmodel checkpoints and continuously update the parameters\nin the feed-forward network, as shown in Figure 1.\nFor\nall parameter-efficient training methods and full fine-tuning,\nwe use early stopping to evaluate the checkpoint with best\nperformance on an in-domain validation set.\nFor LibriSpeech, we fine-tune the cased BERTbase model\nfor fair comparison with previous work. For other internal\ntraining datasets, we fine-tune an in-house 170M Rescore-\nBERT model with 16 layers and 1024-dimensional hidden\nlayers, which was trained on internal data with the discrim-\ninative training objective for 435K steps.\n4.3. Baselines\nThe word error rate (WER) of the first-pass RNN-Transducer\nspeech recognition baseline system used is below 10%. We\ncompare the fine-tuning results of low-rank adaptation with\nfull fine-tuning and three other parameter-efficient fine-tuning\nmethods. Here the \u201cAdapter\u201d method refers to the standard\nresidual adapter proposed in [12], which has a latent dimen-\nsion that is half of its encoder dimension, 768. Adapter lay-\ners are inserted into the self-attention module and the subse-\nquent residual connection, as well as into the MLP module\nand its subsequent residual connection. Each adapter layer\nincludes two fully connected layers, bias vectors, and a non-\nlinearity placed between them. The \u201cBitFit\u201d method, pro-\nposed in [13], involves training the bias vectors in each mod-\nule while freezing all other parameters. The \u201cPrefix\u201d method\nrefers to prefix-tuning [11], which inserts trainable tokens into\ninput sequence.\n5. RESULTS AND ANALYSIS\n5.1. Low-rank domain adaptation\n5.1.1. Messaging data as continuous domain adaptation\nTable 1 shows the evaluation results on four internal datasets.\nWe fine-tune a 170M RescoreBERT model with the MWER\ntraining objective on an internal messaging (MSG) dataset.\nThe fine-tuned models are evaluated on both in-domain mes-\nsaging test set and out-of-distribution data from the General,\nShopping and Knowledge domains. The first row shows the\ntest evaluation results of the 170M RescoreBERT model with-\nout any fine-tuning. All parameter-efficient fine-tuning meth-\nods achieves performance comparable to or better than full\nfine-tuning (FT) on the target domain Messaging. However,\nFT, Adapter and BitFit suffer from performance degradation\non out-of-distribution data, while LoRB performs robustly in\nboth target domain and nontarget domains.\n5.1.2. Case Study 1: Effect of regularization\nTable 2 presents the performance comparison of LoRB and\nLoRB with correlation-based regularization against baseline\nmethods on three internal test sets from nontarget domains.\nOur experiments reveal that the Music domain data is prone\nto overfitting when fine-tuning is applied, resulting in degra-\ndation on other domain data. This can be attributed to the lim-\nited dataset size and the presence of challenging rare words\nlike artist names. While both Adapter and LoRB techniques\nexhibit some level of improvement in mitigating the degra-\ndation across most domains, the combination of LoRB with\ncorrelation-based regularization results in the most substan-\ntial improvement in performance.\nTable 2. Relative WER improvement of LoRB170M, full fine-\ntuning (FT) and Adapter when fine-tuning on Music data.\nNon-Target\nMethod\nGeneral\nShopping\nKnowledge\nAverage\nFine-Tuning (FT)\nbaseline\nbaseline\nbaseline\nbaseline\nResidual Adapter\n-0.14%\n0.49%\n0.3%\n0.22%\nLoRB170M\n-0.5%\n0.21%\n0.90%\n0.20%\nLoRB170M + Lcor\n0.22%\n0.71%\n1.21%\n0.71%\n5.1.3. Case Study 2: Public dataset\nTable 3 shows the WER on test-Clean and test-Other portions\nof the LibriSpeech dataset. We follow a Whisper setup [47]\nfor first-pass decoding.\nOn both test sets, LoRB achieves\nthe largest reduction in WER compared to other parameter-\nefficient training methods. Specifically, in test-Other, LoRB\ncan achieve results comparable to FT with only 0.27% of the\nparameters, and the correlation-based loss brings further im-\nprovements, which aligns with our findings in Case Study 1.\nTable 3. Absolute WER on the two standard test sets of pub-\nlic LibriSpeech [46] baseline decoded by Whisper-tiny. The\n170M BERT base model is retrieved from official public re-\nlease [48] for reproducible evaluation under Apache License.\nModel & Method\n% Params\ntest-Clean\ntest-Other\nBERTbase-cased\nnon-adapted\n6.17\n13.81\nw/ FT\n100%\n4.37\n10.80\nw/ Residual Adapter\n2.15%\n5.29\n12.01\nw/ BitFit\n0.01%\n5.60\n12.43\nw/ Prefix\n0.34%\n5.30\n12.05\nLoRB170M\n0.27%\n4.50\n10.81\nLoRB170M + Lcor\n0.27%\n4.47\n10.78\n5.1.4. Analysis: Training stability\nTable 4 shows the word error rate after full fine-tuning and\nLoRB under different training hyper-parameter settings. We\nobserved that FT is brittle for various combinations of warm-\nup steps and learning rate schedules, while LoRB is more ro-\nbust to changes in hyperparameters.\n5.1.5. Analysis: Training time and GPU memory utilization\nA training time comparison is shown in Figure 2. We find\nthat, while LoRB takes longer to converge compared to FT\nat the same learning rate, the performance of FT degrades\ngreatly when the learning rate is increased. As a result, we\ncan utilize LoRB to achieve a similar WER as FT with shorter\ntraining time by benefiting from the larger learning rate, as\nshown in Figure 2. Furthermore, we find that LoRB can re-\nduce the GPU memory percentage used during training sub-\nstantially, from 87% to 52%.\nTable 4.\nRelative WER improvement on nontarget Shop-\nping domain compared to 170M RescoreBERT without fine-\ntuning, under different warm-up steps and learning rate com-\nbinations.\nWER\nwarmup=5k\nwarmup=10k\nlr=1e-5\nlr=1e-7\nlr=1e-5\nlr=1e-7\nRescoreBERT\nbaseline\nbaseline\nbaseline\nbaseline\nFT\n-72.2%\n-2.0%\n-6.48%\n-1.17%\nLoRB170M\n0\n0\n+0.23%\n+0.11%\n5.1.6. LLM scaling results\nIn this section, we show how the scale of the underlying pre-\ntrained language model and the scale of the training dataset\ncan affect the performance of LoRB. We use an internal con-\nversational dataset (roughly 60M utterances) as the training\n5M\n170M\n1B\nModel Size\n5\n10\n15\n20\n25\n30\nWERR [%]\nMethod\nLoRB\nFT\nFig. 3.\nWER on a conversational test set evaluated by\nRescoreBERT of size 5M, 170M and 1B, fine-tuned with\n\u201cconversational domain\u201d data using FT and LoRA.\nsource.\nTo evaluate the scaling behavior for varying pre-\ntrained model sizes, we fine-tune in-house RescoreBERT\nmodels with 5M, 170M and 1B parameters, respectively,\non a set of 150K conversational training utterances. To in-\nvestigate the scaling behavior for data sizes, we split the\nconversational training data into five log scales with roughly\n20M/5M/1500K/500K/150K utterances, respectively.\nFigure 3 shows the scaling with regard to model size.\nWith the size of the pretrained language model increasing,\nthe performance gap between FT and LoRB shrinks. With\nthe increase in total pretrained parameters of the backbone\nmodel, the performance gap between FT and LoRB is re-\nduced from -22.3% (at the scale of 170M) to +2.4% (at the\n1B scale) in terms of WER relative (WERR) difference. In\nour ASR rescoring model experiments, we found that a larger\nBERT model size improves the convergence speed of LoRB\nby a factor of 2.74, which has benefits for production-size de-\nployments.\nFigure 4 shows the WER on the same conversational test\nset for models trained on different amount of data. In general,\nwe observe that a larger data size correlates with greater im-\nprovement in performance. Notably, the improvement result-\ning from a change in data scale from 150K to 500K is nearly\nfour times that observed when transitioning from 500K to\n20M for LoRB. Unlike the linear scaling law observed in full\nfine-tuning [49], LoRB follows a logarithmic scaling curve,\napproaching a fixed value as the data size reaches a certain\nthreshold. Figure 5 shows the scaling of LoRB across var-\nious rank sizes. While there is no obvious correlation be-\ntween rank value and word error rate across different data\nscale settings, the general trend remains consistent: larger\ndataset sizes lead to a more substantial performance gap com-\npared to full fine-tuning (FT).\n10^5.3\n10^5.8\n10^6.3\n10^6.8\n10^7.3\nData Size\n4\n6\n8\n10\n12\ntestWERR [%]\nLoRB rank=8\nFT\nFig. 4. WER evaluated by 1B RescoreBERT, fine-tuned with\nvarious sizes of \u201cconversational domain\u201d data using FT and\nLoRA.\n150K\n500K\n1500K\n5M\n20M\nData Size\n4\n6\n8\n10\n12\ntestWERR [%]\nFT\nLoRB rank=4\nLoRB rank=8\nLoRB rank=16\nLoRB rank=32\nFig. 5.\nWER as a function of data size, evaluated by\n1B RescoreBERT, fine-tuned with FT and various ranks of\nLoRA.\n6. CONCLUSION\nWe have introduced LoRB, an efficient and scalable low-rank\ndecomposition for domain-adaptation of BERT-based rescor-\ning models with low computation cost and no performance\ndegradation when trained on limited-size in-domain data. By\ninserting weight matrices amounting to only 0.08% of the pa-\nrameters of the pretrained models and freezing all other pa-\nrameters, we achieve speech recognition performance com-\nparable to full fine-tuning with a 6-fold speedup in training.\nExperimental rescoring results on public and internal datasets\ndemonstrate the effectiveness and generalization of the LoRB\nframework and a correlation-based multi-loss training. The\nscaling results highlight the importance of large pretrained\nmodels for best speech recognition rescoring results.\n7. REFERENCES\n[1] Neeraj Gaur, Tongzhou Chen, Ehsan Variani, Parisa Haghani,\nBhuvana Ramabhadran, and Pedro J. Moreno, \u201cMultilingual\nsecond-pass rescoring for automatic speech recognition sys-\ntems,\u201d in Proc. IEEE ICASSP, 2022, pp. 6407\u20136411.\n[2] Ke Hu, Ruoming Pang, Tara N. Sainath, and Trevor Strohman,\n\u201cTransformer based deliberation for two-pass speech recogni-\ntion,\u201d in Proc. IEEE SLT Workshop, 2021, pp. 68\u201374.\n[3] Ankur Gandhe and Ariya Rastrow, \u201cAudio-attention discrim-\ninative language model for asr rescoring,\u201d\nin Proc. IEEE\nICASSP, 2020, pp. 7944\u20137948.\n[4] Tara N. Sainath, Ruoming Pang, David Rybach, Yanzhang\nHe, Rohit Prabhavalkar, Wei Li, Mirk\u00b4o Visontai, Qiao Liang,\nTrevor Strohman, Yonghui Wu, et al., \u201cTwo-pass end-to-end\nspeech recognition,\u201d\nin Proc. Interspeech, 2019, pp. 2773\u2013\n2777.\n[5] Yun-Ning Hung, Chao-Han Huck Yang, Pin-Yu Chen, and\nAlexander Lerch,\n\u201cLow-resource music genre classification\nwith cross-modal neural model reprogramming,\u201d\nin Proc.\nIEEE ICASSP, 2023, pp. 1\u20135.\n[6] Sepp Hochreiter and J\u00a8urgen Schmidhuber, \u201cLong short-term\nmemory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780,\n1997.\n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, et al., \u201cAttention is all you need,\u201d Advances in Neural\nInformation Processing Systems, vol. 30, 2017.\n[8] Chao-Han Huck Yang, Linda Liu, Ankur Gandhe, Yile Gu,\nAnirudh Raju, Denis Filimonov, and Ivan Bulyko, \u201cMulti-task\nlanguage modeling for improving speech recognition of rare\nwords,\u201d in Proc. IEEE ASRU Workshop, 2021, pp. 1087\u20131093.\n[9] Xianrui Zheng, Chao Zhang, and Philip C Woodland, \u201cAdapt-\ning GPT, GPT-2 and BERT language models for speech recog-\nnition,\u201d in Proc. IEEE ASRU Workshop, 2021, pp. 162\u2013168.\n[10] Joonbo Shin, Yoonhyung Lee, and Kyomin Jung, \u201cEffective\nsentence scoring method using BERT for speech recognition,\u201d\nin Proc. Asian Conference on Machine Learning. PMLR, 2019,\npp. 1081\u20131093.\n[11] Xiang Lisa Li and Percy Liang, \u201cPrefix-tuning: Optimizing\ncontinuous prompts for generation,\u201d in Proc. ACL, 2021, vol.\n1: Long papers, p. 4582\u20134597.\n[12] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly,\n\u201cParameter-efficient transfer\nlearning for NLP,\u201d in Proc. ICML. PMLR, 2019, pp. 2790\u2013\n2799.\n[13] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg, \u201cBitFit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models,\u201d in Proc. ACL, 2022, vol. 2: Short\npapers, pp. 1\u20139.\n[14] Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Rohit\nPrabhavalkar, Tara N. Sainath, and Trevor Strohman, \u201cFrom\nEnglish to more languages: Parameter-efficient model repro-\ngramming for cross-lingual speech recognition,\u201d in Proc. IEEE\nICASSP, 2023, pp. 1\u20135.\n[15] Hao Yen,\nPin-Jui Ku,\nChao-Han Huck Yang,\nHu Hu,\nSabato Marco Siniscalchi, Pin-Yu Chen, and Yu Tsao, \u201cNeu-\nral model reprogramming with similarity based mapping for\nlow-resource spoken command classification,\u201d in Proc. Inter-\nspeech, 2023, pp. 3317\u20133321.\n[16] Chun-Wei Ho, Chao-Han Huck Yang, and Sabato Marco Sinis-\ncalchi, \u201cDifferentially private adapters for parameter efficient\nacoustic modeling,\u201d in Proc. Interspeech, 2023, pp. 839\u2013843.\n[17] Kai-Wei Chang, Wei-Cheng Tseng, et al., \u201cSpeechprompt: An\nexploration of prompt tuning on generative spoken language\nmodel for speech processing tasks,\u201d in Proc. Interspeech, 2022.\n[18] Kai-Wei Chang, Yu-Kai Wang, et al.,\n\u201cSpeechprompt v2:\nPrompt tuning for speech classification tasks,\u201d arXiv preprint\narXiv:2303.00733, 2023.\n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, \u201cLoRA:\nLow-rank adaptation of large language models,\u201d in Proc. ICLR,\n2021.\n[20] Liyan Xu, Yile Gu, Jari Kolehmainen, Haidar Khan, Ankur\nGandhe, Ariya Rastrow, Andreas Stolcke, and Ivan Bulyko,\n\u201cRescoreBERT: Discriminative speech recognition rescoring\nwith BERT,\u201d in Proc. IEEE ICASSP, 2022, pp. 6117\u20136121.\n[21] Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu,\nJianfeng Gao, and Tuo Zhao, \u201cSMART: Robust and efficient\nfine-tuning for pre-trained natural language models through\nprincipled regularized optimization,\u201d in Proc. ACL, 2020, p.\n2177\u20132190.\n[22] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and\nAli Ghodsi,\n\u201cDyLoRA: Parameter efficient tuning of pre-\ntrained models using dynamic search-free low-rank adapta-\ntion,\u201d in Proc. ACL, 2023, p. 3274\u20133287.\n[23] George Pu, Anirudh Jain, Jihan Yin, and Russell Kaplan, \u201cEm-\npirical analysis of the strengths and weaknesses of PEFT tech-\nniques for LLMs,\u201d in Proc. ICLR Workshop on Mathematical\nand Empirical Understanding of Foundation Models, 2023.\n[24] Neal Lawton, Anoop Kumar, Govind Thattai, Aram Galstyan,\nand Greg Ver Steeg, \u201cNeural architecture search for parameter-\nefficient fine-tuning of large pre-trained language models,\u201d in\nProc. ACL Findings of the ACL, 2023, p. 8506\u20138515.\n[25] Qingru\nZhang,\nMinshuo\nChen,\nAlexander\nBukharin,\nPengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao,\n\u201cAdaptive budget allocation for parameter-efficient fine-\ntuning,\u201d in Proc. ICLR, 2023.\n[26] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam,\nLidong Bing, and Nigel Collier,\n\u201cOn the effectiveness of\nparameter-efficient fine-tuning,\u201d Proc. AAAI, vol. 37, no. 11,\npp. 12799\u201312807, 2023.\n[27] Zih-Ching Chen, Yu-Shun Sung, and Hung-yi Lee,\n\u201cChap-\nter: Exploiting convolutional neural network adapters for self-\nsupervised speech models,\u201d in Proc. IEEE ICASSP Workshop.\nIEEE, 2023, pp. 1\u20135.\n[28] Xuehai He, Chunyuan Li, et al.,\n\u201cParameter-efficient\nfine-tuning\nfor\nvision\ntransformers,\u201d\narXiv\npreprint\narXiv:2203.16329, 2022.\n[29] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and\nZhiqiang Shen, \u201cOne-for-all: Generalized lora for parameter-\nefficient fine-tuning,\u201d arXiv preprint arXiv:2306.07967, Mar.\n2023.\n[30] Parth Kothari, Danya Li, Yuejiang Liu, and Alexandre Alahi,\n\u201cMotion style transfer: Modular low-rank adaptation for deep\nmotion forecasting,\u201d in Proc. Conference on Robot Learning.\nPMLR, 2023, pp. 774\u2013784.\n[31] Chhavi Choudhury, Ankur Gandhe, Xiaohan Ding, and Ivan\nBulyko, \u201cA likelihood ratio based domain adaptation method\nfor E2E models,\u201d\nin Proc. IEEE ICASSP, 2022, pp. 6762\u2013\n6766.\n[32] Rahul Pandey, Roger Ren, Qi Luo, Jing Liu, Ariya Rastrow,\nAnkur Gandhe, Denis Filimonov, Grant Strimel, Andreas Stol-\ncke, and Ivan Bulyko,\n\u201cPROCTER: PROnunciation-aware\nContextual adaptER for personalized speech recognition in\nneural transducers,\u201d in Proc. IEEE ICASSP, 2023, pp. 1\u20135.\n[33] Dongseong Hwang, Ananya Misra, Zhouyuan Huo, Nikhil\nSiddhartha, et al., \u201cLarge-scale ASR domain adaptation using\nself-and semi-supervised learning,\u201d\nin Proc. IEEE ICASSP,\n2022, pp. 6627\u20136631.\n[34] Zih-Ching Chen, Chao-Han Huck Yang, Bo Li, Yu Zhang,\nNanxin Chen, Shou-Yiin Chang, Rohit Prabhavalkar, Hung-yi\nLee, and Tara Sainath, \u201cHow to estimate model transferability\nof pre-trained speech models?,\u201d in Proc. Interspeech, 2023, pp.\n456\u2013460.\n[35] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen,\n\u201cVoice2Series: Reprogramming acoustic models for time se-\nries classification,\u201d in Proc. ICML. PMLR, 2021, pp. 11808\u2013\n11819.\n[36] Linda Liu, Yile Gu, Aditya Gourav, Ankur Gandhe, Shashank\nKalmane, Denis Filimonov, Ariya Rastrow, and Ivan Bulyko,\n\u201cDomain-aware neural language models for speech recogni-\ntion,\u201d in Proc. IEEE ICASSP, 2021, pp. 7373\u20137377.\n[37] Saket Dingliwal, Ashish Shenoy, Sravan Bodapati, Ankur\nGandhe, Ravi Teja Gadde, and Katrin Kirchhoff,\n\u201cDomain\nprompts:\nTowards memory and compute efficient domain\nadaptation of ASR systems,\u201d in Proc. Interspeech, 2022, pp.\n684\u2013688.\n[38] Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick\nNguyen, Zhifeng Chen, Chung-Cheng Chiu, and Anjuli Kan-\nnan, \u201cMinimum word error rate training for attention-based\nsequence-to-sequence models,\u201d in Proc. IEEE ICASSP, 2018,\npp. 4839\u20134843.\n[39] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta, \u201cIn-\ntrinsic dimensionality explains the effectiveness of language\nmodel fine-tuning,\u201d in Proc. ACL/IJCNLP, 2021, vol. 1: Long\npapers, p. 7319\u20137328.\n[40] Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Na-\nman Goyal, Luke Zettlemoyer, and Sonal Gupta, \u201cBetter fine-\ntuning by reducing representational collapse,\u201d in Proc. ICLR,\n2021.\n[41] Peng Xu, Mostofa Patwary, Shrimai Prabhumoye, Virginia\nAdams, Ryan Prenger, Wei Ping, Nayeon Lee, Mohammad\nShoeybi, and Bryan Catanzaro,\n\u201cEvaluating parameter effi-\ncient learning for generation,\u201d\nin Proc. EMNLP, 2022, p.\n4824\u20134833.\n[42] Li-Jen Yang, Chao-Han Huck Yang, and Jen-Tzung Chien,\n\u201cParameter-efficient learning for text-to-speech accent adapta-\ntion,\u201d in Proc. Interspeech, 2023, pp. 4354\u20134358.\n[43] Zih-Ching Chen,\nChin-Lun Fu,\nChih-Ying Liu,\nShang-\nWen Daniel Li, and Hung-yi Lee, \u201cExploring efficient-tuning\nmethods in self-supervised speech models,\u201d in Proc. IEEE SLT\nWorkshop, 2023, pp. 1120\u20131127.\n[44] Haode Zhang, Haowen Liang, et al., \u201cFine-tuning pre-trained\nlanguage models for few-shot intent detection: Supervised\npre-training and isotropization,\u201d\nin Proc. NAACL, 2022, p.\n532\u2013542.\n[45] Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tieyan\nLiu, \u201cRepresentation degeneration problem in training natural\nlanguage generation models,\u201d in Proc. ICLR, 2019.\n[46] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev\nKhudanpur,\n\u201cLibriSpeech: an ASR corpus based on public\ndomain audio books,\u201d in Proc. IEEE ICASSP, 2015, pp. 5206\u2013\n5210.\n[47] Prashanth Gurunath Shivakumar, Jari Kolehmainen, Yile Gu,\nAnkur Gandhe, Ariya Rastrow, and Ivan Bulyko, \u201cDistillation\nstrategies for discriminative speech recognition rescoring,\u201d in\nProc. Interspeech, 2023, pp. 4084\u20134088.\n[48] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, \u201cBERT: Pre-training of deep bidirectional trans-\nformers for language understanding,\u201d in Proc. NAACL, 2019,\npp. 4171\u20134186.\n[49] Yi Gu, Prashanth Gurunath Shivakumar, Jari Kolehmainen,\nAnkur Gandhe, Ariya Rastrow, and Ivan Bulyko,\n\u201cScaling\nlaws for discriminative speech recognition rescoring models,\u201d\nin Proc. Interspeech, 2023, pp. 471\u2013475.\n"
  },
  {
    "title": "Jointly Training Large Autoregressive Multimodal Models",
    "link": "https://arxiv.org/pdf/2309.15564.pdf",
    "upvote": "8",
    "text": "Preprint\nJOINTLY TRAINING LARGE AUTOREGRESSIVE MULTI-\nMODAL MODELS\nEmanuele Aiello1\u2217\nLili Yu2\nYixin Nie2\nArmen Aghajanyan2\nBarlas Oguz2\n1Politecnico di Torino, 2Meta AI\nABSTRACT\nIn recent years, advances in the large-scale pretraining of language and text-to-\nimage models have revolutionized the field of machine learning. Yet, integrating\nthese two modalities into a single, robust model capable of generating seamless\nmultimodal outputs remains a significant challenge. To address this gap, we\npresent the Joint Autoregressive Mixture (JAM) framework, a modular approach\nthat systematically fuses existing text and image generation models. We also\nintroduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-\nmodal generation tasks. Our final instruct-tuned model demonstrates unparalleled\nperformance in generating high-quality multimodal outputs and represents the first\nmodel explicitly designed for this purpose.\n1\nINTRODUCTION\nAutoregressive text-to-image models, as exemplified by works such as Yu et al. (2023; 2022), have\nmade remarkable strides in generating highly detailed images, paralleling the achievements of\nDiffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models\nbear architectural resemblance to Large Language Models (LLMs), yet their training regimen is\ntailored for paired image-text data. LLMs on the other hand (Brown et al., 2020; Zhang et al., 2022;\nTouvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities\ndespite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged\nin recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to\nbring together the disparate strengths of vision and language models. Despite important advances\nin this direction, these models still predominantly generate one modality, thereby constraining their\nexpressiveness. This study aspires to break this limitation by developing a multimodal model capable\nof generating integrated text and image outputs.\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two\nspecialized autoregressive, decoder-only, large transformer models, each designed for unique tasks\n(one for text-to-image and a text only model). We introduce a set of methods under the umbrella of\nthe Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of\nthe inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing\nus to do deep model fusion and joint training in ways which would otherwise not be possible. Our\nmodular and data-efficient solution allows for deep, rapid and effective integration of continually\nevolving large models, using less than 1% of the original pretraining data for both parent models.\nOur contributions to this study are twofold. First, we establish the feasibility of blending autoregres-\nsive text-to-image models with LLMs into a unified architecture that retains the core strengths of each\nwhile revealing new, emergent capabilities. Second, we present innovative strategies for multimodal\ninstruction tuning, utilizing text-based instructions and a custom-curated dataset designed explicitly\nfor image generation. The result is a first-of-its-kind large multimodal model which can coherently\ngenerate long-form content with interleaved text and images.\n2\nMETHODS\nTo tackle the challenge of creating a unified model that excels at vision-language generative tasks, we\npropose to combine two autoregressive decoder-only architectures. Our primary image-text model is\n\u2217work done as an intern in Meta AI, correspondence <emanuele.aiello@polito.it>\n1\narXiv:2309.15564v2  [cs.LG]  28 Sep 2023\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I make homemade jam?\nThere are many different kinds of jam, spread, or preserves that you can make at home. Some are simple and take little time, while others are more complicated and\ntake a little longer. Most require some berries, fruits or herbs to make, but many of them also have other ingredients you may already have in your kitchen.\nHere are some simple and easy ways to make jam, spread, or preserves:\nSimple and easy jam\n1. Bring a kettle of water to boil. Pour in 3 cups (710 ml) water into a kettle or pot and put it on the stove to boil. If you don't have a kettle, you can boil water in\na pot on the stove.\n2. Prepare the berries. Wash the berries and then strain them to remove the seeds. You can use any kind of berries, such as raspberries, blueberries,\nblackberries,\u00a0 strawberries,\u00a0gooseberries, and mirabelle berries. You can also use other fruits, such as apricots, peaches, and plums.\n3.\u00a0Poke the berries through a cheesecloth. Place the berries through the cheesecloth so that the seeds get caught on the cloth.\nWash and peel the apples. Cut the core out of the apple. Then cut the apple into small slices. These slices should be about half an inch thick. Peel the skins off\nthe apples. Cut the core\u00a0 out of the peaches. Then peel the peaches and cut them into slices. Cut the gooseberries into small pieces. This process is called\ncutting the berries. You can also use other fruits, such as mangoes, in place of the apples and peaches.\n4.\u00a0Add all the ingredients to a saucepan. Pour in the water, sugar, lemon juice, and lemon peel.\u00a0 Stir the ingredients together with a spoon or rubber spatula until\nthey are well dissolved.\n5.\u00a0Bring the mixture to a boil. Place the saucepan on the stove and bring the mixture to a boil over high heat. Then reduce the heat to low and simmer the\ningredients, covered, for 10 minutes.\n6.\u00a0Remove the lid and stir the mixture. Remove the cover from the saucepan and use a wooden spoon to stir the mixture frequently. This will help the sugar\nbreakdown and make the jam smoother. It will also help the flavors meld together.\nFigure 1: Selected sample generated by our instruction-tuned JAM-Cross model. The model can\ncomplex mixed-modal outputs with coherent alignment between generated text and images.\nCM3leon (Yu et al., 2023), trained on 2.4T image-text caption tokens. In contrast, using the same\narchitecture, our LLM (Molybog et al., 2023) has been trained on 1.4T text tokens. Both models have\n7B parameters, we provide additional architectural details in Section 3.1. Our overall methodology\ndevelops in two stages. In the first stage (Sect. 2.1), we first combine and align the models. In the\nsecond stage (Sect. 2.2), we explore new directions for instruction tuning focused on interleaved\nimage-text generation.\n2.1\nCONTINUED PRETRAINING\nWe combine the two pretrained models into a singular, cohesive structure in our proposed framework.\nThis composite model is fine-tuned using a hybrid dataset comprising both text-only and image-text\nsamples within our continued pretraining phase. The central motivation behind this approach is to\nseamlessly merge the capabilities of two pretrained models, capitalizing on the unique strengths of\neach.\n2.1.1\nMODEL MERGING\nThe concept of model merging has been previously utilized to combine models that share identical\noptimization trajectories (Kaddour et al., 2022), or models that are trained on identical datasets but\nhave independent optimizations (for instance, Matena & Raffel (2022); Wortsman et al. (2022);\nAinsworth et al. (2022)). A consistent approach across these studies is to combine models without\nany training. Our approach diverges from this convention; we view the merged model as a powerful\ninitialization for subsequent training on mixed-modal data. The weights of the averaged model are\ndefined as:\n\u03b8average = 1\n2\u03b8llm + 1\n2\u03b8img\n(1)\nWhere \u03b8llm and \u03b8img represent the weights of the LLM and the text-to-image model respectively. In\nthis study, we explore weights merging specifically to multimodal decoder-only large transformer\nmodels, and notably, on an unprecedented scale, involving models trained on trillions of tokens from\ndiverse datasets. In the following sections, we refer to our average model as JAM-Uniform.\n2\nPreprint\nLLM block\nCross Attention block\nShared Embedding\nShared Embedding\nLinear\nT-I block\nCross Attention block\nNx\nNx\nFigure 2: JAM-Cross, architecture overview. The cross-attention blocks are interleaved between the\noriginal LLM block and the Text-Image blocks, and the output embedding between the two branches\nare concatenated and then projected to the output embedding dimension.\n2.1.2\nWIDTH CONCATENATION\nOur second approach employs the pretrained weights to initialize a wider architecture. Our new\nmodel has hidden dimensions djoint = 8192, which is doubled with respect to one of the two original\nmodels dllm = dimg = 4096. We keep the same number of layers of the original architectures. The\nresulting architecture has 26B parameters, initialized starting from the pretrained weights of our\nbackbones. The token embedding input/output projections and the learned positional embeddings of\nthe two initial models are concatenated on the hidden dimension. The attention weights (e.g query\nprojection) Wq,combined \u2208 Rdjoint\u00d7djoint are initialized as:\nWq,combined =\n\u0012\nWq,llm\nWq,llm\nWq,img\nWq,img\n\u0013\n(2)\nWhere Wq,llm, Wq,img \u2208 Rdllm\u00d7dllm represent the weights for the query projection of a generic\nattention layer. All the other weights (FFNs and output projections) are initialized following the same\nlogic. We also experiment with slight variations of the approach:\nWq,combined =\n\u0012\nWq,llm\nWq,average\nWq,img\nWq,average\n\u0013\n(3)\nInstead of copying the two models\u2019 parameters, we use the average to initialize half of the new\nparameters. We name the resulting model JAM-Width.\n2.1.3\nCROSS MODEL FUSION\nWe propose to embed cross-attention layers between the foundational models to facilitate seamless\ninformation interchange while preserving the original models\u2019 knowledge. Given two decoder-only\ntransformers models Tllm and Timg, we introduce a bi-directional cross-attention mechanism that\nenables the layers of one model to attend to the corresponding layer\u2019s output of the other model. This\napproach allows for a progressive exchange of information at different representation levels. For a\nspecific layer l, let the models produce sequences of hidden states Hllm,l for Tllm and Himg,l for\nTllm where these hidden states are outputs from layer l. The output of the cross-attention mechanism\n(Hcross,l) from Timg \u2192 Tllm for a given layer is evaluated as:\nQcross,l = Wq,lHllm,l\u22121,\nKcross,l = Wk,lHimg,l\u22121,\nVcross,l = Wv,lHimg,l\u22121\n(4)\nHcross,l = Softmax\n \nQcross,lKT\ncross,l\n\u221adk\n!\n(5)\nWhere Wq, Wk, Wv represent the query, key, and value projection weights of the newly inserted\ncross-attention layers. A symmetric process is applied for the reverse direction Tllm \u2192 Timg. We\nuse a shared input-output projection layer, initializing the weights of the text tokens from the LLM\ninput embedding and the weights of the image tokens from the image-text model. We insert a new\nlinear projection layer that takes the concatenation of the two model\u2019s output embeddings as input.\n3\nPreprint\nFigure 2 illustrates a schematic of our model configuration. We refer to the model resulting from this\napproach as JAM-Cross. Additional architectural details and the underlying design choices can be\nfound in Sect. 3.1. The ablation study for the optimal frequency of inserting new layers is presented\nin Sect. 3.3.\n2.2\nMULTIMODAL CONVERSATIONAL INSTRUCT TUNING\nSupervised fine-tuning is a fundamental tool to leverage the abilities of large pretrained models.\nRecently, instruct tuning has been extended to a multimodal setting (Liu et al., 2023; Dai et al., 2023);\nhowever, all the existing approaches are focused on visual understanding abilities. In this work, we\nstudy instruction tuning tailored to interleaved image-text generation. We collect a small and curated\nmixed-modal dataset to teach our JAM model to support textual explanations with coherent images.\nSince in the first stage, the model has been trained on image-text captions and text-only data; we train\non interleaved image-text data during this phase. In line with the superficial alignment hypothesis\nintroduced in LIMA (Zhou et al., 2023), we demonstrate that the model can quickly learn the style\nof images and text from a small curated dataset. Our results suggest that the Superficial Alignment\nHypothesis introduced in LIMA holds not only for learning the text style but also for images. In\nour experiments, we consider two slightly different instruction tuning settings, we introduce a small\nportion of the image-text Shutterstock data with retrieval augmentation and we find this approach\nbeneficial to preserve the generated image quality when generating with retrieval augmentation. Sect\n3 presents a comparison between these two strategies. We train using a standard supervised procedure\nwithout leveraging any reinforcement learning or human preference strategy. In this instruction-tuning\nphase, we leverage interleaved image-text data in contrast to previous methods (Koh et al., 2023a)\nthat rely only on image-text caption and no instruction tuning, our experimental results confirm the\nbenefits of training with interleaved image-text data.\n3\nEXPERIMENTS\n3.1\nEXPERIMENTAL DETAILS\nTokenizers\nFor images, we use the VQ-VAE tokenizer from Gafni et al. (2022). The image\nresolution is set to 256 \u00d7 256, 1024 tokens represent each image, and the vocabulary has a size of\n8192. Our text tokenizer is the same that have been used to train the two parent models, trained over\nthe Zhang et al. (2022) data for text. We introduce the additional <break> token used by CM3leon to\nidentify a modality break.\nImage-Text Autoregressive Model\nWe adopt CM3leon as the image-text autoregressive backbone.\nThe model has a standard decoder-only architecture with some peculiarities: no bias terms, dropout,\nand learnable parameters for layer norms. It has been trained on 2.4T image-text tokens and uses a\nsequence length 4096.\nLLM\nAs an LLM backbone, we select a model with the same architecture as CM3leon, trained\nin Molybog et al. (2023) this allows us to experiment with a broader range of approaches, such as\nweight averaging and width concatenation. The model is trained on 1.4T text data with a 2048 context\nlength, and we further fine-tuned it with a 4096 context length using only 30B text tokens.\nObjective\nIn all our experiments, we employ the CM3 objective introduced in Aghajanyan et al.\n(2022); this objective accepts the original sequence as input or transforms it into an infilling instance\nby masking specific spans and relocating them to the end. Then, the model is optimized for minimizing\nthe standard autoregressive loss \u2212 log p(xinput). This objective allows for optional bidirectionally\nand increases the versatility of the model that can be used for both infilling or standard autoregressive\ngeneration. We prevent the objective from masking across the modality <break> tokens.\nRetrieval Augmentation\nWe employ multimodal retrieval augmentation introduced in Yasunaga\net al. (2022) for our training procedure. We leverage our text-to-image backbone\u2019s modifications\nintroduced in Yu et al. (2023). The retrieval procedure employs a dense retriever and a specifically\nselected retrieval strategy. The retriever takes an input query x and returns a relevance score r(q, m)\nfor each candidate document m in our memory bank M. Each multimodal document is split between\n4\nPreprint\nWhat is the best way to start practicing yoga?\u00a0\nYoga is a great way to start practicing physical and mental health. There are\nmany styles of yoga to choose from; each with its own focus, practice methods,\nand intensity levels. That said, if you're new to yoga or if you're interested in\ntrying it out, you may want to start with a beginner's class, choose a class that\nfocuses on relaxation, or begin with a gentler practice until your body can\nhandle more rigorous routines.\nTo find a beginner's class, search for \"beginner's class\" or \"introduction to\nyoga\" in your area. You may also want to ask your doctor or a physical therapist\nabout local yoga instructors who know how to work with people who are new to\nyoga. If you have a friend who already practices yoga, ask them if they can go\nwith you to your first class; they may be able to show you some of the basics.\nAs you become more experienced, you may find that you want to specialize in a\nparticular style of yoga or in a particular path within yoga; you may also find\nthat you want to teach yoga. If you decide that you want to teach yoga, you will\nneed to get certified by the Yoga Alliance or by a similar organization\nHow can I improve my physical fitness?\u00a0\nIt is important to understand that fitness is not just about how much you can lift or\nhow fast you can run, there is a holistic approach to health and fitness that takes into\naccount your habits, stress levels and diet.To improve your physical fitness, you should\nfirst assess your current level of fitness and identify areas where you can improve.\nThere are a number of ways to do this, such as assessing your heart rate during\nexercise or doing a BMI test.\nAfter you have done an assessment, you should develop a specific plan to improve your\nfitness. This should involve not only specific exercises and workouts, but also changing\nyour overall lifestyle, such as by getting more active and adopting a healthier diet.\nCoordinating your workouts with your regular schedule. Be sure to take time for rest\nand relaxation. Give yourself a few days off between workouts to prevent injury and\nhelp your body repair and grow. Work out at a time of day when you will be energetic.\nYou will be able to exert yourself at your best and achieve the best results. Many\npeople find that morning is the best time for them, while others prefer to work out at\nnight. It is important to experiment with different working out schedules to see what\nworks best for you.]\nTry new workouts and exercise techniques to keep your workout routine exciting and\nfun. This will help you stay motivated and prevent you from getting bored with your\nroutine. Always talk to your doctor before trying a new workout or technique,\nespecially if you have any medical conditions. You may want to seek advice from a\npersonal trainer, who can develop a specific workout program for you.\nBe patient with yourself and your progress. Remember that it takes time to improve\nphysical fitness, and you should be patient with yourself and avoid beating yourself up\nover what you perceive are your shortcomings. Focus on the positives and use your\nfailures as learning opportunities. You may have to adapt your goals over time as you\nprogress.\nHow can I plan my trip to Hawaii?\nHawaii is a stunningly beautiful state in the United States, and it's only fitting that\nyou want to explore all that it has to offer. The island paradise is made up of six\ndifferent islands- Hawaii, Molokai, Lanai, Kauai, Oahu, Maui, and Hawaii Island -each\nwith their own unique culture, traditions, and attractions. To really make the most of\nyour Hawaiian adventure, you'll need to do some serious planning, especially if you\nwant to see the big picture and travel to different islands.\nSet your travel goals\n1. List your priorities. Before you start planning your trip to Hawaii, you need to\nfigure out what you want out of it. Are you looking for a relaxing, family-friendly\nexperience? Or are you more interested in exploring Hawaii's natural wonders, such\nas its lush vegetation and beautiful beaches? Once you know what you want, you can\nstart narrowing down your options and focusing on the activities that best fit your\ngoals. For example, if you want to take in some of Hawaii's most popular attractions,\nsuch as the Rainbow Falls and the Duke Kahanamoku Beach, you'll probably want to\nstay on the island of Oahu,\nthen you can make sure to see the main attractions on your list, like the Pearl Harbor\nand the Lone Star State Park. If you're interested in nature, you might want to\nexplore one of Hawaii's wilderness areas, such as the Kaumakani Recreation Area on\nHawaii Island. Another great option is the Hawaii Tropical Botanical Garden, which\nfeatures a wide array of exotic plants and flowers.\n2. Research travel time and costs. Depending on where you live, you may have to fly\nto Hawaii to experience all that it has to offer. If you're interested in seeing as\nmuch of Hawaii as you can, you'll probably want to stay on the island of Oahu, which\nis the biggest and busiest of the Hawaiian islands. Once you fly to Hawaii, you'll need\nto figure out how much time you have and how you'd like to spend it. Also, take into\naccount the cost of different travel methods, such as flights, trains, buses, and\nrentals. If you're planning a family trip, you may want to research attractions that\nare kid-friendly.\nI want to visit Yosemite National Park, can you give me some suggestions for my trip?\nYosemite National Park is one of the most beautiful parks in the world. There are so\nmany different things to do and see in the park, that it's really hard to make\nsuggestions. The park is huge, and travels long distances. You could visit Venice\nBeach, and then drive up to Mammoth Lakes. From there, you can go back down\nthrough Glacier Point and Bear Yosemite to Mariposa Grove. You can explore the\ngrove, then head back to Yahoo Dome. From there, you can just keep driving and\ndrive back into Oakland. It's a long journey, and there are many areas you can\nexplore.\nOne of the best ways to explore the park is to take a shuttle bus. The park provides\nfree shuttle buses between Yosemite Village and several locations in the valley. The\nshuttles run every day, but they are not available on Thanksgiving Day, Christmas\nDay, or New Year's Day. The bus system is fantastic. It's easy to use, and it allows\nyou to explore different parts of the valley that you may not have otherwise visited.\nIn the summer time the seasons are not as distinct as they are in other areas of\nCalifornia. It can be warm most of the time, and it rains fairly often. The water level\nin the springs and lakes can vary a lot. Sometimes, it's low enough to walk across. In\nother cases, it's deep enough to swim in. The weather can change quickly in Yosemite.\nIt's important to pack clothing for all sorts of weather, and to be prepared for rainy,\ncooler weather. On the other hand, you may not get to see a lot of snow. If you want\nto snow bike, then you may be able to find someone to rent you equipment.\nYosemite is amazing, and it will be a fantastic trip. Just remember to bring water,\nclothe, and sunblock\nFigure 3: Selected samples generated by our JAM-Cross instruct tuned model. (Top - generated\nwithout retrieval augmentation; Bottom - generated with retrieval augmentation)\ntext and images and fed to the corresponding modality-specific VIT-B-32 CLIP encoder (Radford\net al., 2021). The two embeddings are then averaged to form the documents\u2019 vector representation. We\nthen use Maximum Inner Product Search (MIPS) over the memory bank to obtain a list of candidates.\nWhen sampling retrieved documents, we prioritize the diversity of the sampled documents by skipping\ncandidates with a score r(q, m) \u2264 0.9. Query dropout is applied to regularize the training, dropping\n20% of tokens from the input sequence x.\nTraining - Alignment Phase\nDuring the continued pretraining, we train for approximately 50B\nmultimodal tokens. Our initial learning rate is lr = 3 \u00d7 10\u22125 we use 500 warm-up steps. We\nset our optimal batch size to 8M tokens, this hyperparameter is borrowed from the mixed-modal\nscaling laws introduced in Aghajanyan et al. (2023). The total number of training steps is 5960. This\ntraining procedure takes approximately one day on 256 80GB A100s for all models. We select the\nlast checkpoint for all the different JAM models, which is always the one with the lowest average\nvalidation PPL. All our training procedures are implemented using Metaseq1.\nTraining - Instruct Tuning\nOur instruct tuning training procedure is data efficient we train with\nour instruction tuning mixed corpora. The initial learning rate is set to 1 \u00d7 10\u22125, and we use 300\nwarm-up steps and a batch size of 1M. The instruction tuning procedure takes less than 2 hours\non 64 80GB A100s, we train for 15 epochs over our mixture of datasets and manually select the\nbest checkpoint corresponding to the 9th epoch. Following Zhou et al. (2023), we notice that the\nvalidation PPL doesn\u2019t correlate with the quality of the responses.\n1https://github.com/facebookresearch/metaseq\n5\nPreprint\nDecoding Strategies\nWe implement a mixed-modal decoding strategy for our interleaved generation.\nThe model starts generating text tokens until a modality <break> token is detected, then an image\nis sampled, and the generation continues until a <eos> token is sampled. We employ temperature\nsampling, a common technique used in autoregressive model (e.g Ramesh et al. (2022)) to control\nthe randomness of the prediction by modifying the softmax temperature \u03c4. We pair this technique\nwith TopP sampling introduced in Holtzman et al. (2019) consisting of sampling from the top-ranked\ntokens with a cumulative probability exceeding a predefined threshold \u03c4P . We also employ classifier-\nfree guidance (CFG (Gafni et al., 2022)) for sampling images. This technique allows to condition\nthe sampling procedure, blending the logits from an unconditional sample with the logits from a\nconditional sample. The procedure is mathematically described as\nlogitscf = logitsuncond + \u03b1c(logitscond \u2212 logitsuncond)\n(6)\nwhere logitscond = T (ty|tx) and logitsuncond = T (ty| < mask >); T represent the transformer\nmodel, < mask > represent the absence of the input text, tx are the conditional input tokens, ty are\nthe output tokens and \u03b1c is the scaling factor for CFG. Thanks to the CM3 objective, our training\nprocedure allows our models to sample with CFG without further fine-tuning. Inspired by Yu et al.\n(2023) we complement this technique to boost the generation quality. Our samples are generated\nusing a temperature value \u03c4 = 1, \u03c4P is set between 0.8 and 1, and we use classifier-free guidance\nwith values 3.5 and 4. In contrast to other approaches, we don\u2019t make use of the computationally\nexpensive clip-reranking (Ramesh et al., 2021; Yu et al., 2022; Gafni et al., 2022) or constrastive\ndecoding (Li et al., 2022b; Yu et al., 2023).\n3.1.1\nDATASETS\nShutterstock\nWe randomly sample a subset of 30B tokens from CM3leon (Yu et al., 2023) pretrain-\ning data. The data consists of legally acquired image-caption pairs from Shutterstock, a commercial\nonline platform offering images with ownership attribution and clear licensing terms.\nText corpora\nWe use 30B text tokens sampled from a mixture of several publicly available data, and\nwe reuse the data used for training other common open-source LLM following the same preprocessing\nof (Touvron et al., 2023). The datasets are: English CommonCrawl (Touvron et al., 2023), C4 (Raffel\net al., 2020), Wikipedia, Books3 from ThePile (Gao et al., 2020), and arXiv.\nLIMA\nWe use the 1k dataset present in Zhou et al. (2023), which features various curated prompts\nand responses.\nwikiHow\nWe collect an interleaved image-text dataset sampling 3000 articles from WikiHow, an\nonline wiki publication that usually curates apposite images for each article. We sample balanced\narticles from each category to ensure diversity; moreover, we leverage the platform\u2019s community\nratings to filter each article\u2019s quality, sampling only those with a score greater than 90/100. For each\narticle, we use the title (e.g., \u2019How to make ..?\u2019) as prompt, we modify the phrase \u2019This article...\u2019 with\n\u2019The following answer..\u2019. Furthermore, we restrict the number of images as 3 per sample, to fit our\n4096 context length.\n3.2\nCONTINUED PRETRAINING RESULTS\nIn the initial stage of continued pretraining, we evaluate the performance across various JAM models.\nOur primary objective is to ensure minimal performance degradation post-merging, relative to the\nparent models. Managing both image and text processing within a single model poses significant\nchallenges. This evaluation seeks to quantify the retention of original performance in our different\nJAM models, benchmarked against the two parent models specialized in individual modalities.\n3.2.1\nTEXT MODALITY\nFor the text modality, we compare the zero-shot performance on some common sense reasoning tasks:\nPIQA (Bisk et al., 2020), ARC-Challenge, ARC-Easy (Clark et al., 2018), StoryCloze (Mostafazadeh\net al., 2016), Winograd, and Winogrande (Sakaguchi et al., 2021). We also report some recent\ninfluential LLM (Brown et al., 2020; Touvron et al., 2023), and our LLM (Molybog et al., 2023)\n6\nPreprint\nTable 1: Zero Shot Text Comparison on Common Sense Reasoning Tasks\nModel\nSize PIQA ARC-C ARC-E StoryCloze Winograd Winogrande\nGPT-3\n175B 81.0\n51.4\n68.8\n-\n-\n70.1\nLLaMa\n7B\n79.8\n47.6\n72.8\n-\n-\n70.1\nLLM-4k\n7B\n76.7\n45.9\n67.7\n79.3\n83.9\n66.2\nJAM-Uniform\n7B\n62.4\n28.5\n42.6\n63.5\n47.8\n49.7\nJAM-Width\n26B\n57.8\n31.4\n31.6\n54.7\n50.2\n51.9\nJAM-Cross\n19B\n75.4\n41.6\n67.2\n79.8\n81.0\n66.0\nTable 2: Image-Text Comparison\nModel\nSize\nMS-COCO PPL\nCM3\n2.7B\n200.1\nRA-CM3\n2.7B\n193.1\nCM3leon\n760M\n168.8\nCM3leon\n7B\n149.0\nJAM-Uniform\n7B\n177.5\nJAM-Width\n26B\n159.5\nJAM-Cross\n19B\n147.6\nTable 3: Ablations - JAM-Width Model\nInit.\nWikipedia PPL MS-COCO PPL\nCopy\n7.34\n159.5\nAverage\n9.0\n175.4\nTable 4: Ablations - JAM-Cross Model\nC-Attn Size Wikipedia PPL MS-COCO PPL\n\u2717\n13B\n7.86\n153.2\n1\n26B\n7.53\n152.4\n2\n19B\n7.18\n149.0\n4\n16B\n8.55\n151.7\nTable 5: Ablations - Instruction Tuning\nShutterstock MS-COCO PPL\n\u2717\n190.2\n\u2713\n164.5\nfine-tuned with 4k context as a reference. Results are presented in Table 1. The JAM-Uniform\nreaches slightly better text-only performance than JAM-Width however, it is crucial to remark that\nthis approach consolidates the functionalities of both parent models within a constrained 7B parameter\nspace. Our findings reveal that the intrinsic knowledge of the parent models can be recovered mainly\nfrom the parameter average utilizing only a minimal portion of the original pretraining data. The\nJAM-Cross model yields the best results, aligning with our primary LLM. This highlights the strength\nof our bidirectional cross-attention mechanism against other baselines.\n3.2.2\nIMAGE-TEXT MODALITY\nTo assess the performance of our different baselines over the image-text modality, we compare them\nusing the validation PPL on MS-COCO dataset (Lin et al., 2014). We believe this metric robustly\ncorrelates with performance on subsequent tasks, such as image generation and captioning. Further-\nmore, it provides a reliable reference point for comparing different autoregressive models sharing an\nidentical tokenizer. Results are reported in Table 2. Diverging from results on the text-only modality,\nthe JAM-Width model exhibits enhanced performance over the JAM-Uniform model in the image-text\ndomain. Specifically, the JAM-Width model demonstrates superior efficacy in retaining image-text\nperformance relative to text-only performance. Conversely, despite a decline in performance, the\nJAM-Uniform model remains a good parameters-performance trade-off. Interestingly, our JAM-Cross\nmodel not only reaches the best PPL between the JAM strategies but also surpasses our foundational\nimage-text model, CM3leon. We hypothesize that such advancement can be attributed to integrating\nnovel textual capabilities coupled with an augmented parameter count inherent to the combined\narchitecture. Based on empirical evidence, the JAM-Cross emerges as the best strategy to combine\ntwo pretrained autoregressive models.\n7\nPreprint\nHi, I'm looking for ideas for a customized cake design for my daughter's birthday.\nI'm not sure what you mean by customised.\nJAM\nGILL\nHere are some custom cake ideas for your daughter's birthday:\n Pink and lavender themed cake with edible glitter.\n Multicolored frosted cake with white icing and pink and lavender decorations.\n Baker's chest themed cake.\n Princess themed cake.\n Panda themed cake.\n Tomato themed cake.\n Hawaiian themed cake.\n Fluffy cake with layers.\n 3D decorated cake.\n Colorful sprinkles or glitter.\n New Year themed cake.\n Book themed cake.\n(Generated)\n(Retrieved)\nI need inspiration for my bridal hairstyle in a garden wedding.\nI love the idea of a garden wedding, but I think you should go with\nJAM\nGILL\nHere are some hairstyles you can try for a garden wedding. You can do 1 large bun, 2\nside buns, a low side part, or part your hair in the middle and go slightly half up half\ndown.\n 1 large bun: Put some styling mousse in your hair. Then gather your hair into a large\nponytail at the nape of your neck. Tuck the hair at the back of your head under so\nit looks like a large braid. Then use bobby pins to hold it in place.\n 2 side buns: Part your hair down the middle from the crown of your head. Then,\nuse a comb to separate a half of your hair to the left and the other half to the\nright. Put the half that is on the left in a small ponytail and pin it up. Repeat the\nsame thing with the other half.\n A low side part: Part your hair from just behind one ear to the bottom of your\nchin. Let some of your hair fall on the sides and part the rest in the middle.\n Part your hair in the middle: Take a comb and start combing from the midsection\ndown to your feet. Part the hair in the middle so that\n(Generated)\n(Generated)\nI want to paint a landscape with mountains and a river. Can you help me find some inspiration?\nI'm not sure\nJAM\nGILL\nHere are some inspiration ideas for you to paint a landscape with mountains and a river.\nYou can start by exploring your own hometown and look around at the scenery you are\nfamiliar with. Take pictures of the mountains and the river if you can. Look at the\npictures and try to find ways to incorporate them into your painting. You can also find\ninspiration by looking at paintings and drawings of mountains and rivers by other artists.\nYou might go to a museum or an art store and look at some paintings. If you can't visit a\nmuseum or an art store, you can at least look online. You can also find inspiration by\nwatching movies and videos that feature beautiful landscapes with mountains and rivers.\n(Generated)\nWhat would a pizza with bacon look like?\nJAM\nGILL\nPizza with bacon is a savory dish that contains both savory and sweet flavors. The sweet\nflavors come from the sauce, the cheese, and the toppings, while the savory flavors\ncome from the bacon. There are various methods of making this dish. Some people\nchoose to make a thin crust pizza, which contains less cheese and more bacon. Others\nmake a thicker crust pizza that has more cheese and less bacon.\n(Generated)\n(Generated)\n(Generated)\nFigure 4: Qualitative comparison with previous interleaved generation models. Compared to GILL,\nour model is able to generate more complete and precise answers. Results for GILL are sourced from\nKoh et al. (2023a).\n3.2.3\nINTERLEAVED GENERATION\nOur instruct-tuned JAM-Cross model reaches a high-quality level of image-text generated output. To\ndemonstrate its ability to generate coherent modality interleaved responses, we show an extensive set\nof generated samples in Figure 3 and Section B. The samples generated with retrieval are obtained\nfrom the model instruct-tuned with a mixture of pretraining image-text Shutterstock data along with\nour corpora of instruct-tuning datasets, while the samples generated without retrieval are obtained\nfrom the model instruct tuned only on our instruct-tuning set. The generated samples show coherent\nimage and text integration, demonstrating unprecedented abilities at this novel task. Overall we find\nour retrieval augmented solution to be more effective than standard image sampling, boosting image\nquality. We further report several qualitative comparisons with the most relevant previous work GILL\n(Koh et al., 2023a) that features mixed-modal generation. We use our retrieval-augmented JAM-Cross\nmodel and source generations for the GILL model from the original paper. From this comparison\n(Figure 4), it\u2019s immediate to notice how our model has a better overall quality of responses. The\ngenerated text is more complete and exhaustive, while the generated images are more relevant to\nthe text context. We remark that our method is the first capable of such coherent and interleaved\ngeneration with a focus on instruction tuning and that our fine-tuning procedure is effective in\nefficiently learning the style of the dataset, not only for text but even for images. Our model paves the\nway toward a larger adaption of mixed-modal generation in real-world use cases.\n3.3\nABLATION STUDY\nWe compare the two approaches for the width concatenation model: copying the original models\u2019\nweight or using the average to initialize the new parameters. Results (Table 3) show that copying the\n8\nPreprint\nweights is more effective than averaging them to retain the original model capabilities. The ablation\nstudy for the Cross-attention model is presented in Table 4. We ablate the frequency of inserting\ncross-attention layers and the impact of not using any cross-attention layers. These experiments are\nperformed training with 25B tokens, all the other parameters are the same as reported in Sect. 3.1. We\nremark that this is an even shorter training setting concerning our 50B tokens total training and that\nthe difference in performance increases as the training progresses. We further ablate the contribution\nof image-text pretraining data in the instruction tuning procedure in Table 5. The results indicate\nthe importance of using pretraining data mixed in the instruction tuning procedure to preserve the\nMS-COCO PPL. We do not report WikiHow PPL since analyzing the models shows that it doesn\u2019t\ncorrelate with generation quality similarly to Zhou et al. (2023).\n4\nRELATED WORKS\nGenerative Text-to-Image Models\nThe field of generative text-to-image models has recently been\ndominated by diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020). Recent enhancements\nhave used pretrained text representations (Ramesh et al., 2022; Nichol et al., 2022) like CLIP (Radford\net al., 2021) to improve the generation quality. Concurrently to developing diffusion-based generative\nmodels, significant steps have been made by autoregressive token models (Esser et al., 2021; Gafni\net al., 2022). These models encode images into a discrete latent space (Van Den Oord et al., 2017)\nand can be processed as a standard sequence-to-sequence modeling task, enabling the borrowing\nof techniques used from Large Language Models. A critical element that has been found beneficial\nin boosting text-to-image generative models is retrieval augmentation (Chen et al., 2022; Yasunaga\net al., 2022). Yasunaga et al. (2022) propose to prefix decoder-only models, such as Aghajanyan\net al. (2022), with retrieved images during training, resulting in a huge efficiency gain for the\ntraining procedure. Yu et al. (2023), scale this strategy to reach state-of-art performance in image\ngeneration using 5x less training compute. In this work, we borrow their model as our text-to-image\nautoregressive backbone.\nMultimodal Language Models\nThe multimodal language model field has recently seen consid-\nerable development. Several prior works have focused on connecting language models to visual\nencoders. (Tsimpoukelli et al., 2021; Mokady et al., 2021; Najdenkoska et al., 2023; Li et al., 2023).\nThese methods typically train a mapping network between a pretrained image encoder and a language\nmodel. Flamingo (Alayrac et al., 2022) introduces cross attention into a frozen LLM to inject\nvisual features and trains a large corpus of image-text pairs. In this work, we similarly use cross\nattention to bridge the two models; however, our mechanism is bidirectional between the vision and\nlanguage models, while for Flamingo, the visual knowledge is injected in the language model and\nnot vice-versa. CM3 (Aghajanyan et al., 2022) is trained on a large corpus of structured HTML; it\nintroduces the Casually Masked Language Modeling objective we adopt to train our models. Koh\net al. (2023b) propose a multimodal language model capable of processing arbitrarily interleaved\nimage and text inputs and generating interleaved output of text and retrieved image. Subsequently,\non the same line of work, GILL Koh et al. (2023a) proposes to ground an LLM to a text-to-image\nmodel, using a mapping network and freezing the pretrained models, introducing the possibility of\ngenerating or retrieving images as output.\nInstruction Tuning\nInstruction tuning aims to teach language models to follow natural language\ninstructions. Several methods have been proposed for instruction tuning, using existing NLP datasets\nconverted in instruction formats Wei et al. (2021) Chung et al. (2022), or using LLMs like GPT-4 to\ngenerate instruction data with better diversity Wang et al. (2022) Honovich et al. (2022). Recently,\nLIMA Zhou et al. (2023) demonstrated that 1,000 carefully curated samples are enough to reach\ncompetitive results compared to bigger instruction-tuning datasets. The authors hypothesize that most\nof the knowledge is learned during the pretraining, and the instruction tuning teaches the style to\ninteract with the users. In this work, we explore using a small set of multimodal instruction tuning\ndata to fine-tune our model, verifying the effectiveness of a small dataset in this multimodal setting\ntailored to image generation. Several vision language works adopt instruction tuning for multimodal\ntasks-focused user interactions optimized for visual content understanding Liu et al. (2023) Dai et al.\n(2023) Ye et al. (2023) Zhu et al. (2023). Unlike previous works, we explore instruction tuning\nfocused mixed-modal generation, paving the way for more significant adaptation of multimodal\nmodels that can generate interleaved image-text output.\n9\nPreprint\n5\nCONCLUSIONS\nIn this work, we have presented novel methodologies for combining pretrained autoregressive models,\ndemonstrating the viability of synthesizing the knowledge of two distinct models into a cohesive\nstructure with extended capabilities. Our exploration validates that the integrated model can be adeptly\nfine-tuned using our tailored instruction-tuning procedure for interleaved image-text generation. To\nthis end, we pioneered creating a specialized dataset centered on instruction tuning for this particular\ntask. Nevertheless, the proposed study is limited to 7B parameter models with the same architecture.\nFuture works may consider scaling the models\u2019 size and asymmetrically applying our cross-fusion\nmethod to bridge models of varying sizes. Increasing the context length and delving into multi-turn\nconversations could further represent an interesting exploration direction. In conclusion, our study\nsets the foundation for substantial advancements in the realm of multimodal autoregressive models.\nThe fusion of text-to-image generation with large language models paves the way for sophisticated\nsystems capable of interleaved image-text interactions, enriching the landscape of conversational AI.\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked\nmultimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang,\nStephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative\nmixed-modal language models. arXiv preprint arXiv:2301.03728, 2023.\nSamuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo\npermutation symmetries. In The Eleventh International Conference on Learning Representations,\n2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nWenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented\ntext-to-image generator. arXiv preprint arXiv:2209.14491, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npp. 12873\u201312883, 2021.\n10\nPreprint\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-\na-scene: Scene-based text-to-image generation with human priors. In European Conference on\nComputer Vision, pp. 89\u2013106. Springer, 2022.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027, 2020.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations, 2019.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.\nJean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner. Questions for flat-minima optimization\nof modern neural networks. arXiv preprint arXiv:2202.00661, 2, 2022.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023a.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for\nmultimodal generation. arXiv preprint arXiv:2301.13823, 2023b.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference on\nMachine Learning, pp. 12888\u201312900. PMLR, 2022a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.\narXiv preprint arXiv:2210.15097, 2022b.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740\u2013755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\nMichael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. Advances in\nNeural Information Processing Systems, 35:17703\u201317716, 2022.\nRon Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv\npreprint arXiv:2111.09734, 2021.\nIgor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh\nKoura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. A theory on adam instability in\nlarge-scale machine learning. arXiv preprint arXiv:2304.09871, 2023.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Van-\nderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper\nunderstanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016.\nIvona Najdenkoska, Xiantong Zhen, and Marcel Worring. Meta learning to bridge vision and language\nmodels for multimodal few-shot learning. arXiv preprint arXiv:2302.14794, 2023.\n11\nPreprint\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models. In International Conference on Machine Learning, pp.\n16784\u201316804. PMLR, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pp. 8821\u20138831. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv e-prints, pp. arXiv\u20132204, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learning,\npp. 2256\u20132265. PMLR, 2015.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. Advances in Neural Information\nProcessing Systems, 34:200\u2013212, 2021.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations, 2021.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model\nsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing\ninference time. In International Conference on Machine Learning, pp. 23965\u201323998. PMLR,\n2022.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling.\narXiv preprint arXiv:2211.12561, 2022.\n12\nPreprint\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. Transactions on Machine Learning Research, 2022.\nLili Yu, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,\nMandar Joshi, Gargi Ghosh, Mike Lewis, et al. Scaling autoregressive multi-modal models:\nPre-training and instruction tuning. arXiv preprint arXiv:2201.07520, 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nA\nLIMITATIONS\nThe generation quality of our proposed model presents some limitations. Our JAM frameworks rely\non LLMs and text-to-image autoregressive models, inheriting their strengths and limitations, such as\nthe potential for hallucinations and biases in image generation. These limitations may be addressed\nby improving and leveraging better autoregressive backbones.\nMoreover, our instruct-tuning procedure focuses on a specific wiki-style single-turn question answer-\ning. Most of the time, the model generates a single or, at most, two images, while a longstanding goal\ncould be to generate entire multimodal documents with >2 images. This is partially bottlenecked\nby the training data and the context length of 4k tokens; given that each image takes 1024 tokens,\ngenerate longer outputs it\u2019s still challenging.\nFurthermore, our model is more effective in generating interleaved image text when the question\nstyle aligns with the dataset style and when the images refer to concrete objects. While the model\nusually struggles to represent more abstract concepts. We showcase some examples of this behavior\nin Figure 5. When the subject of the text dialogue are more abstract concepts such as \"time travel\",\n\"music\", or \"research\", the generated images fail to capture the textual context and enrich the model\noutput faithfully.\nB\nGENERATED EXAMPLES\nThis section shows several samples generated by our instruction-tuned models using both retrieval\naugmentation and standard image generation.\n13\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nYou are a scientist who just invented a Time Machine. Where do you travel first?\nI would travel back in time to when I was a teenager. There are a few people I want to meet, and I have a lot of work to do!\nFirst, I would travel to about the year 2000, and spend some time getting to know my family. I think it would be really cool to talk to my parents when\nthey were young, and see how their lives turned out. I'd also like to meet my grandparents, and see how they changed over the course of their lives.\nI would also travel to the past to try to influence history in ways that I think would be beneficial. I would travel to historical events and try to change\nthe course of history for the better, or at least record what I do and let people see what I did. I would also travel to ancient times and try to make\ndiscoveries that have never been made before.\nI would also go to the future to see how society has changed and what kind of changes it has undergone. I would also like to see what kind of\ntechnological advances people have made in the future and what kind of impact those advances have had on society.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I start playing guitar and become a better musician?\nThere are several ways. Start with basic lessons. Many websites offer free lessons, including Google, YouTube, and Encyclopedia Britannica. Focus on learning the basics, playing\nsongs, and becoming familiar with the guitar's neck and how the strings are positioned. Practice your chords and scales. When you have a basic foundation of knowledge, you can\nmove on to more advanced techniques. Search for videos and lessons online. Check out popular websites such as YouTube, Vimeo, and Google Play. You can also find books at your\nlocal library that offer guidance.\nCreate your own routine. When you first start learning how to play the guitar, you'll probably feel overwhelmed and not make any progress at all. This is normal; everyone was once a\nbeginner. To help you overcome this, create a practice routine. Here are a few things to consider for your routine:\n Set aside a set time and day to practice. This will help you establish a routine and stay motivated. Since you're a beginner, start with 15-20 minutes a day and 30-60 minutes a\nfew times a week. As you progress, you can increase your practice time.\n Review what you learned in your lesson and practice a few songs\n\u00a0\u00a0\u00a0\u00a0\nWhy are you holding the guitar this way? What is the best technique you can find for this? Go slow and repeat until you get it right. Review and practice until you can do it\nwithout taking a break. This will help you build endurance and become a more efficient player.\n Start with the basics and work your way up. Don't try to learn too much too fast. Focus on becoming a competent player who can play a few fun songs and review the basics\nalong the way. You'll progress faster if you take your time and learn each aspect properly. There are plenty of resources that outline the basics of guitar playing. You can find\nthem online or at your local music store.\n Take your guitar with you. This way you can practice whenever you have free time. You'll also be able to take your practice further. For example, if you're in the middle of a\nmovie, practice a few chords. If you're bored in between episodes of your favorite TV show, hit the practice pedal. Finally, if you're out on a walk or running an errand, riff\naway. Any time you have a few free moments, take out your guitar and practice a few chords. Your patience will pay off.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I write a research paper?\nA research paper is an academic essay that researches and evaluates a topic and presents the results in the form of written content. Usually, a thesis statement (the main idea of\nthe paper) will guide the research paper; the thesis statement will also be supported by the research and appear at the end of the paper. There are lots of different ways to write\na research paper; the basic structure, however, stays essentially the\nsame.\nResearch paper introduction\n1. Give a definition of the topic you are researching. A good research paper topic should be interesting, worthy of investigation, and have an aspect of immediacy. For example,\nyou could research whether teenage \npregnancy rates have decreased over the past 15 years in your area. You might choose to research homelessness in your area, or early\nchildhood education programs in your city. How does obesity in children affect \u00a0 \u00a0 \ntheir growth and development? Do pregnant women in your area get screenings for\ngestational diabetes?\n2. Describe why your topic is important. Your topic is important because people's lives are at stake. If you are researching homelessness in your area, people are living in\nunsanitary conditions, maybe they lack access \nto basic necessities like food and shelter, they may be infected with diseases\nRead about the different styles and include what styles the professional uses in their work. You might want to research how different authors in your field have written their\nintroductory chapters. Some research papers\u00a0 \u00a0 \u00a0 \neven include a chapter titled \"Acknowledgments\" where the author acknowledges all of the people and resources that went into\ncreating the work. Remember to cite all sources you use.\n3. Provide context and weigh the severity of the issue. You want to tell your reader who or what will be affected by your topic, why that would be bad, and how your topic ties\nin to other issues and fields. How will your \nresearch paper affect real people? If you are writing about homelessness, your research will either help homeless people get shelter\nand basic necessities or it will show that the current system of homelessness in your \narea is unsustainable and needs to be altered. What will the outcomes of your research be?\nIf you are researching obesity in children, you might find that children are not getting enough exercise, which can lead to health problems later in life, or you might find that\nparents are not doing a good enough job at watching their children's eating habits, which may lead to lifelong health problems and/or dependence on others.\nFigure 5: Generated samples - Failure Cases\n14\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nI need to train my dog.\u00a0How can I teach him how to sit and other basic instructions?\nStart with easy commands like \"Sit\" and \"Stay\". These are the first steps for someone to learn how to control their dog. They are also the first steps\nfor your dog to learn how to associate his body with certain commands.\nTo get your dog to sit, raise your hand in the air and make a demanding motion with your hand. Often called a \"hands command\", your dog will associate\nthe command with your hand motion and with the positioning of your arm. Start using the word \"sit\" as you are making the hand motion. After a few\nattempts, your dog should understand that the command \"sit\" corresponds to the hand motion you are making.\nIn addition to using the hand motion, be consistent with your tone of voice. Positive reinforcement will help him learn the hand motion, but your tone of\nvoice will let him know whether or not he has succeeded.\nStart with a gentle voice and gradually raise your tone of voice when he follows the command. If you are happy with his performance, tell him \"Good\ndog!\" in a cheerful tone of voice. If he needs more work, use a firmer tone of voice and let him know that he has not succeeded yet. However, always\nstay positive.\nBeginning commands with the hand motion will help you to establish the movement as the command.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I improve my mathematical knowledge?\nThere are several things you can do to improve your mathematical knowledge. Here are some suggestions.\n\u2022 Read books. A good way to learn mathematical concepts is to read about them. You can find books in your local library. Some popular books that might be helpful are:\n Introduction to Algebra\u00a0\n Introduction to Geometry\u00a0\n Mathematics: A Very Basic Guide\u00a0\n\u2022 Play with numbers. You can find some math games online. Playing these games can help you practice calculations and recognize patterns. Some games you might try are:\n Arithmetic Bee\n Hyperbole and Parabola\n Pathfinder\n Solving Equations\u00a0\n\u2022\u00a0Take classes. You can take classes in math at your local community college or university. Most colleges and universities offer classes in algebra, geometry, and calculus.\nYou can also take classes in advanced math, such as abstract algebra, harmonic analysis, and number theory.\n\u2022 Speak with a tutor. If you are having trouble understanding a concept, you can hire a math tutor. A tutor can help you explain the material in a simpler way. You can\nfind a tutor online or by asking around at\nFigure 6: Generated samples - retrieval augmentation\n15\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I prepare for college admission?\nThere are a lot of things to consider when preparing for college admission. Here are a few suggestions:\n Take college preparation classes, visit college campuses, and talk to counselors and admissions officers. Doing these things will help you\nchoose the right college and understand what you need to do to get admitted.\nSet aside time to think about what you want to study and why. Don't settle on a major just because your parents or teachers told you to.\nChoose a major that you're interested in and that will help you find a job that you'll like.\n Join clubs, organizations, and sports teams. These can help you make friends, get to know other people, and have fun. They're also a good\nway to build skills and get immediate experience that colleges will value.\n Work hard and get good grades. A grade point average of 3.0 or better is usually needed to get into a good college. Some schools may\nrequire a higher GPA, so ask them when you visit. If you can't achieve a 3.0 grade point average, talk to your teachers and counselors, and\nsee if there are any special programs you can join to help you get good grades.\n Take notes in class\u00a0\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow to prepare a tasty hot dog?\nHot dogs are a classic comfort food that you can enjoy at home or at a festival. They're easy to make at home with common ingredients, and\nyour friends and family may be surprised at how delicious they can be.\nYou can mix up your own filler to save money and add healthy ingredients; you can also sausage or use turkey bacon to make a veggie-friendly hot\ndog. Get started by choosing a topping that suits your preferences.\nChoose the types of hot dog\n\u0401.  Get a homemade hot dog for a more nutritious option. Homemade hot dogs are healthier than store-bought versions since they don't\ncontain primarily processed ingredients. They're also fresher, so you're less likely to get sick from eating them. While you can buy a kit\nthat contains all the ingredients you need to make a homemade hot dog, you can also make your own using common ingredients. The hot dog\nusing the basics of a homemade turkey sausage\n\u0402.  Try a veggie hot dog for a plant based option. Many people eat vegetarian or vegan hot dogs for various reasons. Kosher and soy-based hot\ndogs are available in bulk or at most grocery stores, and they can be substituted for meat in most recipes. Purchase\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nVeggie hot dogs cook just as well as bacon-filled hot dogs, and they generally aren't filled with a lot of processed meat (or at all). For\nmeat lovers, you can cook your veggie hot dog using delicious mushrooms, seitan, or tempeh.\n3. Choose a hot dog seasoning mix for a quick fix. Seasoning a hot dog at home is simple and fast, and it's less expensive than buying a\npremade seasoning mix. While you can buy hot dog seasoning mixes at most grocery stores, it's easy to make your own using a few spices.\nTaste your hot dogs as they come out of the oven and add more seasoning if you prefer. Use your finger or a spoon to spread the seasoning\nevenly over the hot dog. For example, you can use 1 tablespoon (15 ml) each of garlic powder, onion powder, cayenne pepper, lemon pepper\nseasoning, and herb seasoning. You can try flavoring your hot dogs with different spices, or use a combination of seasonings to create your\nown unique taste.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I start to snowboard?\nSnowboarding is a lot of fun. Here are some steps to help you learn how to snowboard.\n\u0401.  Get up on the chair lift to go to the top of the mountain. You'll need to buy a ski or snowboard lift ticket.\n\u0402.  Begin riding the chair lift to the top of the mountain. Once you've arrived at the top, you can turn around and go back the way you came. You\ndidn't actually have to ride the chair lift all the way to the top --you could have walked, but it's much faster to ride the lift.\n\u0403.  Now that you've arrived at the top of the mountain, you can ride your snowboard down the mountain. To go downhill, you need to keep your\nweight over the front of your snowboard and release the grip that you have on the back of the board. Engage your brakes so that you can stop\nwhen you need to. To go faster, you can push off the side of the snowboard. If you want to turn, you can turn your feet the same way that you\nwould on a skateboard. Releases the pressure on your back foot to start the turn and then catches it with your front foot\nFigure 7: Generated samples - without retrieval augmentation\n16\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I order a large pizza from Domino's?\nHere is the solution, Domino's has a \"Large Pizza\" menu item that is 12\" and this can be ordered online.\n1.  Visit the Domino's website.\n2.  Click on the \"Pizza\" menu and select \"Large Pizza\".\n3.  Enter the desired quantity in the \"Quantity\" box. For example, 2.\n4.  Check the \"Grab a Package\" box to get a box of 12 slices.\n5.  Click on the \"Order Now\" button.\n6.  Provide your name, phone number, address, and the payment method.\n7.  Confirm the order and wait for the pizza to be delivered.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I build a wood table for my kitchen?\nHere are some options to consider for your kitchen table:\n Build a table out of panels and a top (this is probably the most common style, and probably the cheapest)\n Build a table out of whole logs (good for a rustic look, and you get to choose the color of the wood)\n Build a table out of distressed wood (my personal favorite, can be a little more expensive)\n Build a table with a tile top (not sure why, but there are people that love it)\n Build a pool table (not really a kitchen table, but can be used in the kitchen)\n Build a buffet table (these are usually made of brass and wood, and can be quite heavy. Good if you have a lot of guests)\nRemember, when building any table, the most important part is the frame. The legs are just going to hold up the frame, so choose your legs based on\nthe frame, not the other way around.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I prepare an avocado toast?\nAvocado toast is delicious and relatively easy to make. To make avocado toast, toast bread slices until they're crispy, then scoop chopped avocado on\ntop. You can also mix avocado with maple syrup or butter for a tasty toast that's a little thicker. If you want to cut down on the calories, you can use\nbread that's made with whole wheat flour or rye grain. If you want to go vegan, you can use vegan butter or mousse instead.\nToast and scoop avocado\n1.  Preheat the oven to 250 \u00b0F (120 \u00b0C). Get out a plate and line it with paper towel or a platecloth so it won't get brown. Set the plate aside while\nyou toast the bread.\n2.  Heat a pan on the stove until it's hot. Place a pan on the stove and heat it over medium heat. You can use a pan with a toaster oven on the bottom\nif you want to toast the bread in the oven as well. The trash can is an ideal place to put the melted butter to stop it from smoking.\n3.  Toast the bread until it's crispy\nFigure 8: Generated samples - without retrieval augmentation\n17\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow to make shakshuka?\nShakshuka is a tasty and healthy breakfast dish made with eggs, beans, mushrooms, onions, lettuce and tomato sauce. The shaken eggs are scrambled in a pan before\nbeing poured over anything else. The dish can be served warm or cold. Read on for some tips on how to make your own shakshuka.\nMake basic shakshuka\n1. Heat olive oil or a tasty oil (like coconut or pistachio) in a pan on medium-high heat. A cast iron pan is best, but you can also use a non-stick pan. You'll also need a\nmedium-sized saucepan.\n2. Add onion, garlic and/or mushrooms to the pan and saute them for 5-10 minutes, depending on your taste. Onion is the most common addition, but you can experiment\nwith other vegetables, like za'atar (middle eastern spice mix), chopped chili pepper, or garlic powder. You can also mix in some spices like cumin and\u00a0coriander.\n3. Crush the tomatoes (you can crull them up, or simply chop them into small pieces)\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nI am going with my friend for an hiking. How should I prepare my backpack?\nHere are some tips to get you started preparing your backpack for a trek.\n\u0401.  Pack your essentials in a lightweight, compact bag.\n\u0402.  Place your care items-including snacks, clothes, and toiletries-in a separate bag.\n\u0403.  Pack your gear in a strap bag, or a pack pouch attached to the main bag.\n\u0404.  Use breathable, lightweight materials in your gear bags.\n\u0405.  Opt for removable straps, bags, or other accessories so you can lighten your load while hiking.\n\u0406.  Pack your clothes in a tarp or a collapsible bag so they'll take up minimal space.\n\u0407.  Use a pack that allows you to easily adjust your carrying capacity according to the terrain and distance of your trek.\n\u0408.  Pack your gear in plastic bags to keep it clean and preserve its durability.\n\u0409.  Use a waterproof bag that can hold everything you need, or create a waterproof bag calculated for your trek.\n\u0401\u0400.  Start your trek with fresh clothes so you don't get sweaty or dirty while hiking.\nFigure 9: Generated samples - without retrieval augmentation\n18\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nWhat can I visit in Torino?\nHere are some suggestions for your visit to Torino, Italy:\n The Palazzo Savona, formerly the palace of the Savoys, is a spectacular building located in the city center, constructed in the 17th century.\n The Muse Piemonte, located in the Old Palazzo, houses an impressive collection of art and antiquities, and is one of the largest museums in Italy.\n Church of the Ges\u00f9, designed by Brunelleschi and completed in 1558, is a Gothic-style church located in the church district of the city.\n Piazza San Marco, or St. Mark's Square, is a beautiful square located in the heart of the city, facing the Basilica.\n The Torino Dome, constructed in 1878, is an extraordinary example of Italian architecture, created by architects Guglielmo and Achille Villa.\n The Biblioteca Albertina, located in the University of Torino, houses one of the largest collections of rare books and manuscripts in Italy.\n The Botanical Garden, located in the heart of the city, is one of the largest and most beautiful gardens\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I learn skateboarding?\nSkateboarding is fun, involves a lot of movement and is a good exercise. There are a lot of ways to learn how to skateboard. You can take lessons from\na professional skater, watch instructional videos, read how-to articles, etc. Try to find a friend that skates and asks to ride with you, or visit a\nskatepark and practice on the bumpy terrain with others.\nLearn from a pro skater\n1.  Find a professional skater to teach you. Look online for skater camps or try asking around at your local skatepark. You might even know a skater\nthat's willing to teach you how to skate. Organize a lesson with the person and make sure you know what you expect to get out of it. Some\nskaters may be willing to help you for free if they like you, or if you help them with something.\n2.  Organize a lesson. Make sure you know exactly what you need to learn. For example, if you want to learn how to kick turn, you need to know how\nto move your foot from the front to the back, and you need to know how to rotate the deck. You probably also need to know how to balance on a\nskateboard\nFigure 10: Generated samples - retrieval augmentation\n19\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow to fly to Niagara Falls?\nNiagara Falls is one of the most popular tourist destinations in the world. You can easily travel to this region from the Canadian or American side, although you may want to explore\nthe whole area. The best way to get to Niagara Falls is by plane, as there are several airports in the area. Once you're at Niagara Falls, you can take a number of different activities\nto make the most of your visit.## Fly to niagara falls\n1. Use the Google Flight Search website to book your flight. Google Flight Search is an online flight booking website. You can browse flight prices from a number of different\nairlines. You can also input vacation dates and see what flight prices come up. You don't have to log in or register in any way. Within the flight search box, you can enter the\nfollowing information:\n Year of travel\n Month of travel\n Day of travel.\n You can also input where you want to go, and the website will find flights that meet your budget.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow to make mango lassi?\nMango lassi is a tasty drink that's made with mango juice and yogurt. It's a good choice for a hot summer day, as it's cold and creamy. You can enjoy it plain, or you can add some\nspices to it. If you want to know how to make it, just follow these steps.\nMake plain mango lassi\n1.  Wash the mango. Use your hands to scrub the mango to remove any dirt. Rinse the mango and let it dry.\n2.  Prepare the yogurt. Heat the yogurt in a microwave-safe bowl. Place the bowl in the microwave and heat it on medium heat for 1 to 2 minutes. You can use any kind of yogurt.\nPlain yogurt will have less sugar and will be sweetened nicely by the mango juice. You can also use plain cottage cheese instead of yogurt.\n3.  Mix the yogurt and mango juice. Use a spoon to mix the yogurt and juice together until the mixture is smooth. You may need to stop and scrape down the sides of the bowl\noccasionally.\n4.  Pour the mixture into a glass. You can use any kind of glass or mug.\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I get better at chess? I would like to improve my skills.\nThere are several ways you can improve your chess skills.\n Play more chess. Practice several times a week and increase your play time as you get better. You could join a chess club to get better practiced, or play online chess with\nfriends or other players.\n Analyze your games. After you have played a game, observe it and think about how you could have done things differently. This will help you grow as a player and evaluate your\nmoves in a more objective\nway.\n Study the chess tactics. There are some interesting strategies and concepts in chess that you can learn about by studying them. Some good books to reference are The Art\nof Chess Strategy by Alexander Navolynets, The Chess Improvement Book by Nikola Kodrina, The Chess Triple Threat by Adrian Menendez.\n Understand the chess openings. The way you begin a game of chess depends on whether you want to counteract your opponent's moves, or try to establish your control over\nthe chessboard early on. There are several kinds of openings, such as the Anglicane, Russian, Sicilian, Zusovsky, and Germans. Learn about the strategies behind them and\nunderstand which kind of opening you should\nFigure 11: Generated samples - retrieval augmentation\n20\nPreprint\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I start playing basketball?\nBasketball is one of the most popular sports in the world. There are more kids playing basketball now than ever before. There are a lot of ways to start playing basketball, and a lot\nof resources available to help you learn. Checking out some new basketball shoes and adding some new basketball pants to your wardrobe might be a good place to start. Next, you\nmight want to consider joining a basketball class or league.\nFinally, to start playing basketball, you'll need to learn the game by playing it. Basketball is a one-on-one sport, but it's also a team sport, and learning how to work as a team and\nplay individual games will help you become a better basketball player.\nGet into basketball\n1. Try basketball activities for kids. There are a lot of different activities you can do to get into basketball. Most activities are geared towards kids ages 8-12, but many of them\nmight be appropriate for younger children as well. Many of these activities involve playing basketball in a tested environment, under the supervision of trained coaches. Some of the\nmore popular activities include:\n* Youth Basketball Program (YBP): YBP is one of the most popular youth\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I improve my bouldering technique?\nPushing too hard is a common problem for beginners in bouldering. Here are a few things to keep in mind as you develop your technique:\nStart with small, shallow moves. You need to get your technique down before you start trying big, difficult moves. Start with some simple strategies like\njust getting your foot on the rock, or pushing a grip onto a particular hold. These small strategies will help you build strength and confidence before you\ntackle bigger moves.\nFocus on getting through one grip at a time. When you're just getting started, you'll probably be working on just getting your grip on the rock. Focus on\njust working on a single grip at a time, and get good at that before moving on. For example, you might start by working on a double overhand grip, and only\nwork on that grip for the first few sessions. Once you feel more comfortable with the grip, you can start looking at trying a different one.\nTake your time. You really can't learn proper technique in a single session. You need to practice consistently in order to develop your skills. Set aside 30\nminutes or so to practice every day, and make sure you take breaks occasionally. You also need to practice multiple times\nhttps://t3.ftcdn.net/jpg/05/17/79/88/360_F_517798849_WuXhHTpg2djTbfNf0FQAjzFEoluHpnct.jpg\nHow can I improve my rock climbing technique?\nThere are a few fundamental concepts that you need to get better at first, before you start getting better at climbing. So, in no particular order:\nForm an essential training routine. If you want to improve your climbing technique, you have to practice it. A lot. And you have to do it consistently. There are a lot\nof different exercises you can do, but they need to be done daily, and they need to be done for a long period of time (at least 3 months, preferably 6 months)\nbefore you start seeing improvements. If you're just starting out, 3x weekly is good. If you're already more experienced, 2x weekly is good.\nPractice good technique. This includes things like using your arms in synchrony with your legs, using your whole body weight when you can, not pushing too hard, and\navoiding bad habits (like bouncing). A good way to ensure you're using good technique is to do your exercises in front of a mirror. That way you can see whether or\nnot you're actually using your body in the most efficient way.\nLearn to fail safely. Rock climbing is a dangerous activity, so you need to learn how to fail safely.\nFigure 12: Generated samples - retrieval augmentation\n21\n"
  },
  {
    "title": "DECO: Dense Estimation of 3D Human-Scene Contact In The Wild",
    "link": "https://arxiv.org/pdf/2309.15273.pdf",
    "upvote": "7",
    "text": "DECO: Dense Estimation of 3D Human-Scene Contact In The Wild\nShashank Tripathi1*\u2020\nAgniv Chatterjee1*\nJean-Claude Passy1\nHongwei Yi1\nDimitrios Tzionas2\nMichael J. Black1\n1Max Planck Institute for Intelligent Systems, T\u00a8ubingen, Germany\n2University of Amsterdam, the Netherlands\n{stripathi, achatterjee, jpassy, hyi, black}@tue.mpg.de\nd.tzionas@uva.nl\nFigure 1: Given an RGB image, DECO infers dense vertex-level 3D contacts on the full human body. To this end, it reasons\nabout the contacting body parts, human-object proximity, and the surrounding scene context to infer 3D contact for diverse\nhuman-object and human-scene interactions. Blue areas show the inferred contact on the body, hands, and feet for each image.\nAbstract\nUnderstanding how humans use physical contact to inter-\nact with the world is key to enabling human-centric artificial\nintelligence. While inferring 3D contact is crucial for mod-\neling realistic and physically-plausible human-object inter-\nactions, existing methods either focus on 2D, consider body\njoints rather than the surface, use coarse 3D body regions,\nor do not generalize to in-the-wild images. In contrast, we\nfocus on inferring dense, 3D contact between the full body\nsurface and objects in arbitrary images. To achieve this,\nwe first collect DAMON, a new dataset containing dense\nvertex-level contact annotations paired with RGB images\ncontaining complex human-object and human-scene con-\ntact. Second, we train DECO, a novel 3D contact detector\nthat uses both body-part-driven and scene-context-driven\nattention to estimate vertex-level contact on the SMPL body.\nDECO builds on the insight that human observers recognize\ncontact by reasoning about the contacting body parts, their\nproximity to scene objects, and the surrounding scene con-\n* equal technical contribution\n\u2020 project lead\ntext. We perform extensive evaluations of our detector on\nDAMON as well as on the RICH and BEHAVE datasets. We\nsignificantly outperform existing SOTA methods across all\nbenchmarks. We also show qualitatively that DECO gen-\neralizes well to diverse and challenging real-world human\ninteractions in natural images. The code, data, and models\nare available at https://deco.is.tue.mpg.de.\n1. Introduction\nHumans rely on contact to interact with the world. While\nwe use our hands and feet to support grasping and locomo-\ntion, we also leverage our entire body surface in our daily\ninteractions with the world; see Fig. 1. We sit on our but-\ntocks and thighs, lie on our backs, kneel on our knees, carry\nbags on our shoulders, and move heavy objects by holding\nthem against our bodies. Executing everyday tasks involves\ndiverse full-body and object contact. Thus, modeling and\ninferring contact from images or videos is essential for ap-\nplications such as human activity understanding, robotics,\nbiomechanics, and augmented or virtual reality.\narXiv:2309.15273v1  [cs.CV]  26 Sep 2023\nInferring contact from images has recently received at-\ntention. While some methods infer contact for hands [49],\nfeet [52], self contact [15, 48], or person-person contact [14],\nothers focus on human-scene or human-object contact for the\nfull body [8, 29]. HOT [8] infers contact in 2D by training\non in-the-wild images with crowd-sourced 2D contact areas,\nwhile BSTRO [29] infers 3D contact on a body mesh and is\ntrained on images paired with 3D body and scene meshes\nreconstructed with a multi-camera system.\nIn contrast to prior work, we seek to represent detailed\nscene contacts across the full body and to infer these from in-\nthe-wild images as illustrated in Fig. 1. To that end, we need\nboth an appropriate training dataset and an inference method.\nNote that manipulating objects is fundamentally 3D. Thus,\nwe must capture, model, and understand contact in 3D. Also\nnote that some contacts support the body, while others do not.\nWhen sitting on a chair and drinking a cup of coffee, the body\nis supported by the buttocks on the chair and feet on the floor,\nwhile the coffee cup does not support the body. The former is\ncritical for physical reasoning about human pose and motion,\nwhile the latter is important to understand how we interact\nwith objects. The type of contact is therefore important to\nrepresent. For a method to robustly estimate contact for\narbitrary images we need a rich dataset that combines in-the-\nwild images with precise 3D annotations; see Fig. 2. This is\na huge challenge.\nTo address this challenge, we present a novel method and\na new dataset. We first collect a dataset with 3D contact\nannotations for in-the-wild images using a novel interactive\n3D labelling tool (Fig. 2). We then train a novel 3D contact\ndetector that takes a single image as input and produces\ndense contact labels on a 3D body mesh (Fig. 1). Training\non our new dataset means that the method generalizes well.\nContact data: To train a 3D contact detector that is both\naccurate and robust, we need appropriate training data. How-\never, existing datasets for 3D contact [3, 25, 29] involve\npre-scanning a 3D scene and estimating 3D human pose\nand shape (HPS) of people in the scene. These approaches\nare limited in the complexity of the human-scene interac-\ntions, the size of the dataset, and very few methods capture\nhuman-object interactions paired with image data [4, 30].\nAn alternative is to use synthetic data [60], but getting realis-\ntic synthetic data of complex human contacts is challenging,\ncausing a domain gap between the dataset and real images.\nIn contrast, crowdsourced image annotations support\nmany tasks in computer vision such as image classifica-\ntion [12], object detection [42, 73], semantic segmenta-\ntion [28, 42], 2D human pose estimation [1, 6], and 3D body\nshape estimation [10, 62]. HOT [8] takes this approach for\nhuman-object contact, but the labels are all in 2D, while con-\ntact is fundamentally 3D. Consequently, we collect a large\ndataset with dense 3D contact annotations for in-the-wild\nimages, called DAMON (Dense Annotation of 3D huMan\nBENCH\nSUPPORT\nLAPTOP\nCELL PHONE\nBOTTLE\nBED\nCOUCH\nCHAIR\nTEDDY BEAR\nBICYCLE\nSUPPORT\nSUPPORT\nSUPPORT\nSUPPORT\nCAR\nBICYCLE\nCELL PHONE\nHANDBAG\nFigure 2: Sample contact annotations from the DAMON\ndataset. Left to Right: RGB image, two views showing\nhuman-supported contact (color-coded by object labels), and\ntwo views showing scene-supported contact.\nObject contact in Natural images). We enable this with a\nnew interactive software tool that lets people \u201cpaint\u201d con-\ntact areas on a 3D body mesh such that these reflect the\nobserved contact in images. We use Amazon Mechanical\nTurk, train human annotators for our task, and collect a rich\ncorpus of 3D contact annotations for standard datasets of\nin-the-wild images of diverse human-object interactions, i.e.,\nV-COCO [22] and HAKE [38]; Fig. 2 shows samples of our\ndataset. Note how contact and support regions are distin-\nguished as are the semantic labels related to object contact.\nContact detection: As noted in the literature [8, 29], con-\ntact areas are ipso facto occluded in images, thus, detecting\ncontact requires reasoning about the involved body-parts and\nscene elements. To this end, BSTRO [29] uses a transformer\n[40] with positional encoding based on body-vertex posi-\ntions to implicitly learn the context around these, but has\nno explicit attention over body or scene parts. HOT [8, 29],\non the other hand, focuses only on 2D, pulls image features,\nand processes them with two branches in parallel, a contact\nbranch and a body-part attention branch; the latter helps the\ncontact features attend areas on and around body parts.\nWe go beyond prior work to estimate detailed 3D contact\non the body. Our method, DECO (Dense Estimation of 3D\nhuman-scene COntact in the wild), introduces two technical\nnovelties: (1) DECO uses not only body-part-driven atten-\ntion, but also adds scene-context-driven attention, as well as\na cross-attention module; this explicitly encourages contact\nfeatures computed from the image to attend to meaningful\nareas both on (and near) body parts and scene elements.\n(2) DECO uses a new 2D Pixel Anchoring Loss (PAL) that\nrelates the inferred 3D contacts to the respective image pix-\nels. For this, we infer a 3D body mesh with CLIFF [39]\n(SOTA for HPS), detect which vertices of this are in con-\ntact with DECO, project the 3D contact vertices onto the\nimage, and encourage them to lie in HOT\u2019s corresponding\n2D contact-area annotations. Note that this brings together\nboth crowd-sourced 2D and 3D contact annotations.\nExperiments: We perform detailed quantitative experi-\nments and find that DECO outperforms BSTRO on the test\nsets of RICH and DAMON, when both are trained on the\nsame data. Ablation studies show that our two-branch archi-\ntecture effectively combines body part and scene information.\nWe also provide ablation studies of the backbone and training\ndata. We show that the inferred contact from DECO signif-\nicantly outperforms methods that compute the geometric\nvertex distance between a reconstructed object and human\nmesh [74, 83]. Finally, we use DECO\u2019s estimated contact\nin the task of 3D human pose and shape estimation and find\nthat exploiting estimated contact improves accuracy.\nContributions: In summary, our contributions are (1)\nWe collect DAMON, a large-scale dataset with dense vertex-\nlevel 3D contact annotations for in-the-wild images of\nhuman-object interactions. (2) Using DAMON, we train\nDECO, a novel regressor that cross-attends to both body\nparts and scene elements to predict 3D contact on a body.\nDECO outperforms existing contact detectors, and all its\ncomponents contribute to performance. This shows that\nlearning 3D contact estimation from natural images is possi-\nble. (3) We integrate DECO\u2019s inferred 3D contacts into\na 3D HPS method and show that this boosts accuracy.\n(4) Our data, models, and code are available at https:\n//deco.is.tue.mpg.de.\n2. Related Work\n2.1. 2D contact in images\nThere exist multiple ways of representing human-object\ninteractions (HOI) and human-scene interactions (HSI) in\n2D. Several HOI methods [34, 50, 70, 76, 88] localize hu-\nmans and objects as bounding boxes and assign a semantic\nlabel to indicate the interactions between them. However,\nthe interaction labels focus on action and do not support con-\ntact inference. Chen et al. [8] output image-aligned contact\nheatmaps and body-part labels directly from the RGB image\nby training a regressor on approximate 2D polygon-level\ncontact annotations. Some approaches learn part-specific\ncontact regressors for hand [49, 58] and foot [53] contact\nbut only detect rough bounding boxes around contacting re-\ngions or joint-level labels. Such coarse image-based contact\nannotations are ambiguous and not sufficient for many down-\nstream tasks. We address these limitations by collecting a\nlarge-scale dataset of paired images and accurate vertex-level\ncontact annotations directly on the 3D SMPL mesh.\nSeveral methods estimate properties related to contact\nsuch as affordances [37, 55, 71], contact forces [61, 80, 87]\nand pressure [17, 20, 57]. However, collecting large datasets\nwith ground-truth object affordances, forces, or pressure is\nchallenging. Clever et al. [11] use simulation and a virtual\npressure mat to generate synthetic pressure data for lying\nposes. Tripathi et al. [67] exploit interpenetration of the\nbody mesh with the ground plane as a heuristic for pressure.\nRecent work [18, 61, 80] uses a physics simulator to infer\ncontact forces. In contrast, we focus on annotating and\nestimating 3D contact, which is universal in HOI and is\nintuitively understood by annotators.\n2.2. Joint- & patch-level 3D contact\nJoint-level contact. 3D contact information is useful for\n3D human pose estimation [53, 61, 74], 3D hand pose esti-\nmation [7, 21, 27], 3D body motion generation [52, 64, 84\u2013\n86] and 3D scene layout estimation [79]. 3D pose esti-\nmation approaches use joint-level contact to ground the\nestimated 3D human mesh [16, 25, 78, 81, 83] or en-\ncourage realistic foot-ground contact to avoid foot-skating\nartefacts [31, 52, 59, 84, 89].\nPhysCap [61] and oth-\ners [52, 53, 81, 89] constrain the human pose by predicting\nskeleton joint-level foot-ground contact from video. Sev-\neral approaches predict 3D contact states of 2D foot joints\ndetected from RGB images by manually annotating con-\ntact labels [89] or computing contact labels from MoCap\ndatasets [53, 61]. Rempe et al. [52] extend joint-level contact\nestimation to the toe, heel, knee and hands, but use heuristics\nsuch as a zero-velocity constraint to estimate contact from\nAMASS [46]. Zhang et al. [84] estimate contact between\nfoot-ground vertices using alignment of normals between\nfoot and scene surface points. Such joint-level annotations\ncannot represent the richness of how human bodies contact\nthe world. In contrast DECO captures dense vertex-level\ncontact across the full body.\nDiscrete patch-level contact. Pre-defined contact re-\ngions or \u201cpatches\u201d on the 3D body provide an interme-\ndiate representation for modeling surface-level contact.\nM\u00a8uller et al. [48] and Fieraru et al. [15] crowdsource patch-\nlevel self-contact annotations between discrete body-parts\npatches on the same individual. Fieraru et al. [14] also\ncollect patch-level contact between two interacting people.\nWhile richer than joint-level contact, patches do not model\nfine-grained contact. In contrast, the DAMON dataset and\nDECO model contact on the vertex level, significantly in-\ncreasing the contact resolution.\n2.3. Dense vertex-level contact\nDense ground-truth contact can be computed if one has\naccurate 3D bodies in 3D scenes. For instance, PROX [25],\nInterCap [30], and BEHAVE [3] use RGB-D cameras to\ncapture humans interacting with objects and scenes whereas\nHPS [23] uses a head-mounted camera and IMU data to lo-\ncalize a person in a pre-scanned 3D scene. RICH uses a laser\nscanner to capture high-quality 3D scenes and the bodies are\nreconstructed using multi-view cameras. GRAB [65] cap-\ntures hand-object interactions using marker-based MoCap\nbut lacks images paired with the ground-truth scene. Such\ndatasets require a constrained capture setup and are difficult\nto scale. An alternative uses synthetic 3D data. HULC [60]\ngenerates contact by fitting SMPL to 3D joint trajectories in\nthe GTA-IM [5] dataset. The contacts, however, lack detail\nand the domain gap between the video game and the real\nworld limits generalization to natural images.\nSeveral methods infer 3D bodies using dense 3D contact.\nPHOSA [83] jointly estimates 3D humans, objects and con-\ntacts for a limited set of objects for which there are predeter-\nmined, hand-crafted, contact pairs on the human and object.\nOther methods optimize the body and scene together using\ninformation about body-scene contact [56, 72, 74, 75, 79].\nSome methods predict dense contact on the body mesh.\nPOSA [26] learns a body-centric prior over contact. Given a\nposed 3D body, POSA predicts which vertices are likely to\ncontact the world and what they are likely to contact. It as-\nsumes the pose is given. Closest to our work are BSTRO [29]\nand HULC [60], which infer dense contact on the body from\nan image. We go beyond these methods by providing a rich\ndataset of images in the wild with dense contact labels. More-\nover we exploit contextual cues from body parts as well as\nthe scene and objects using a novel attentional architecture.\n3. DAMON Dataset\nDAMON is a collection of vertex-level 3D contact la-\nbels on SMPL paired with color images of people in uncon-\nstrained environments with a wide diversity of human-scene\nand human-object interactions. We source our images from\nthe HOT dataset [8] for the following reasons: (1) HOT cu-\nrates valid human contact images from existing HOI datasets\nlike V-COCO [22] and HAKE [38] by removing indirect\nhuman-object interactions, heavily cropped humans, mo-\ntion blur, distortion or extreme lighting conditions; (2) HOT\ncontains 15082 images containing 2D image-level contact\nannotations, which are complementary to the dense 3D con-\ntact annotations in our dataset. Example images and contact\nannotations from the DAMON dataset are shown in Fig. 2.\n3.1. Types of contact\nWhile existing HOI methods and datasets typically treat\nall contacts the same way, human contact is more nuanced.\nPhysical contact can be classified into 3 categories: (1) scene-\nsupported contact, i.e., humans supported by scene objects;\n(2) human-supported contact, i.e., objects supported by a\nhuman; and (3) unsupported contact, e.g., self-contact [15,\n48] and human-human contact [14, 16]. Since datasets for\nthe latter already exist, we focus on the first two categories,\ni.e., contact that involves support. Note that labeling contact\nin images is challenging. Focusing on support helps reduce\nambiguous cases where humans are close to scene objects\nbut not actually in contact.\nWe use Amazon Mechanical\nTurk (AMT) to crowd-source annotations for DAMON; we\nask people to annotate both human-supported contact for\neach individual object and scene-supported contact.\n3.2. Annotation procedure\nWe create a novel user-friendly interface and tool that\nenables annotators to \u201cpaint\u201d 3D vertex-level contact areas\ndirectly on the human mesh; see the interface in Sup. Mat.\nWe show the original image with the type of contact to be an-\nnotated on the left and the human mesh to the right. We then\nask annotators to \u201cpaint\u201d contact labels on the NV = 6890\nvertices of the SMPL [44] template mesh, \u00af\nM \u2208 R6890\u00d73.\nThe tool has features such as mesh rotation, zoom in/out,\npaint-brush size selection, an eraser, and a reset button. De-\npending on the selected brush size, the tool \u201cpaints\u201d contact\nannotations by selecting a geodesic neighborhood of vertices\naround the vertex currently under the mouse pointer. For a\ndetailed description of the tool, see video in Sup. Mat.\nThe tool lets annotators label contact with multiple ob-\njects in addition to the scene-supported contact. For example\nannotations, see Fig. 2. For every image, to label human-\nsupported contact, we cycle through object labels provided\nin the V-COCO and HAKE datasets. For scene-supported\ncontact, we ask annotators to label contact with all support-\ning scene objects, including the ground. We automatically\nget body-part labels for contact vertices using SMPL\u2019s part\nsegmentation. To support amodal contact estimation, we ask\nannotators to also label contact regions that may not be visi-\nble in the image but can be guessed confidently. We filter out\nambiguous contact in images such as human-human contact,\nhuman-animal contact, and indirect human-object interac-\ntions, such as pointing; for details about data collection and\nhow we limit ambiguity in the task, see Sup. Mat.\nWe ensure a high annotation quality with two quality\nchecks: (1) We detect and filter out the inconsistent annota-\ntors; out of 100 annotators we keep only 14 good ones. (2)\nWe have meta-annotators curate the collected annotations;\nimages with noisy annotations are then pushed for a re-\nannotation. For details about quality control, see Sup. Mat.\nWe access DAMON\u2019s quality by computing two metrics:\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n# images\nsurfboard\nchair\nsnowboard\nlaptop\ncell phone\nsports ball\ndining table\nskateboard\ntennis racket\nbook\nfrisbee\ncup\ncar\nbed\nbottle\nbench\ncouch\nhandbag\nbackpack\nknife\nbowl\nskis\nbaseball glove\npotted plant\ntv\nmouse\npizza\nsuitcase\nfork\nkeyboard\ntie\nspoon\nremote\ncake\ntruck\nclock\nsandwich\nbicycle\numbrella\nwine glass\n4.5%\n7.3%\n17.8%\n25.0%\n32.7%\n2.9%\n9.8%\ntransportation\naccessories\nfurniture\neveryday\nobjects\nsports\nequipment\nfood\nitems\nkitchen\nappliances\nFigure 3: DAMON dataset statistics. Histogram: contact\nobject labels (y-axis) and the number of images in which\nthey are present (x-axis). We crop the plot in the interest of\nspace; for the full long-tailed plot see Sup. Mat. Pie chart:\nobject labels are grouped into 7 main categories; inner colors\ncorrespond to the colors in the histogram. \u00fc Zoom in.\n(1) Label accuracy: We manually curate from RICH [29]\nand PROX [25] 100 images that have highly-accurate 3D\nposes and contact labels. We treat these as ground-truth\ncontact, and compute the IoU of our collected annotations.\n(2) Level of annotators\u2019 agreement: We ask annotators to la-\nbel the same set of 100 images, and compute Fleiss\u2019 Kappa\n(\u03ba) metric. For a detailed analysis of results, see Sup. Mat.\n3.3. Dataset statistics\nOut of HOT\u2019s 15082 images we annotate 5522 images via\nour annotation tool (Sec. 3.2); we \u201cpaint\u201d contact vertices,\nand assign to each vertex an appropriate label out of 84\nobject (Fig. 3) and 24 body-part labels. An image has on\naverage 3D contacts for 1.5 object labels. We use HOT\u2019s\ntrain/test/val data splits.\nWe also show aggregate vertex-level contact probabili-\nties on the SMPL mesh across the whole DAMON dataset in\nFig. 4. The individual body-part close-ups in Fig. 4 show nor-\nmalized contact probabilities for that body part. It is evident\nthat, while we typically use our hands and feet for contact,\nwe also frequently use the rest of our body, especially the\nbuttocks, back of the head, chest, lips, and ears to interact\nwith everyday objects. To our knowledge, no such analysis\nof full-body contact for in-the-wild images has previously\nbeen reported. This motivates the need for modeling dense\nfull-body contact.\nHead\nLeft Hand\nFront\nBack\nRight Hand\nBack\nFront\nFeet\nFigure 4: Aggregate statistics showing contact probabilities\nacross the body vertices in the DAMON dataset. The body\npart closeups show the contact probabilities normalized for\nthat body part. Red implies higher probability of contact\nwhile blue implies lower probability. \u00fc Zoom in.\n4. Method: DECO\nContact regions in images are ipso facto occluded. This\nmakes human-object contact estimation from in-the-wild\nimages a challenging and ill-posed problem. We tackle this\nwith a new DEnse COntact estimator, DECO, which uses\nscene and part context.\nOur contributions are two fold: (1) To reason about the\ncontacting body parts, human-object proximity, and the sur-\nrounding scene context, we use a novel architecture with\nthree branches, i.e., a scene-context, a part-context, and a\nper-vertex contact-classification branch. (2) We use a novel\n2D pixel-anchoring loss that constrains the solution space\nby grounding the inferred 3D contact to the 2D image space.\n4.1. Model architecture\nGiven an image I \u2208 RH\u00d7W \u00d73, DECO predicts contact\nprobabilities on the SMPL [44] mesh. We use SMPL as it\nis widely used for HPS estimation [32, 33, 35, 36, 39, 82].\nSMPL parameterizes the human body with pose and shape\nparameters, \u0398 = [\u03b8 \u2208 R72, \u03b2 \u2208 R10] and outputs a 3D\nmesh M(\u03b8, \u03b2) \u2208 R6890\u00d73. SMPL\u2019s template mesh\n\u00af\nM\nis segmented into J = 24 parts, Pk \u2208 P, which allows\npart-labeling of contact vertices. Moreover, SMPL\u2019s mesh\ntopology is consistent with the SMPL-H [54] model and has\nthe same vertices below the neck as the SMPL-X model [29],\nmaking our contact representation widely applicable.\nFigure 5 shows DECO\u2019s architecture. Intuitively, contact\nestimation relies on both part and scene features as they are\ncomplementary. We use two separate encoders Es and Ep\nto extract scene features Fs and body-part features Fp. For\nthe encoder backbone, we use both the transformer-based\nSWIN [43] and the CNN-based HRNET [69]. We integrate\nscene features Fs and body-part features Fp via a cross-\n`\nScene \nEncoder\nPart \nEncoder\nScene \nDecoder\nPart \nDecoder\nCross\nAttention\nMLP\nScene Context Branch \nContact Branch \nPart Context Branch \nGT \nContact\nPred \nContact\nGT 2D \nContact\nFigure 5: DECO architecture (Sec. 4.1). DECO reasons about body parts, human-object proximity, and the surrounding scene\ncontext. To this end, it uses three branches, i.e., a scene-context, a part-context, and a per-vertex contact-classification branch.\nCross attention guides the features to focus attention on (and around) body parts and scene elements that are relevant for\ncontact.\nattention module inspired by [45, 68]. Previous methods ei-\nther concatenate multi-modal features [47], use channel-wise\nmultiplication [35], adopt trainable fusion [66] or use bilin-\near interpolation between multi-modal features [63]. How-\never, such methods simply combine the multi-modal features\nwithout explicitly exploiting their interactions. In contrast,\nDECO\u2019s cross-attention guides the network to \u201cattend\u201d to\nrelevant regions in Fs and Fp to reason about contact.\nTo implement cross-attention, we exchange the key-value\npairs in the multi-head attention block between the two\nbranches. Specifically, we initialize the query, key, and value\nmatrices for each branch i.e. {Qs, Ks, Vs} = {Fs, Fs, Fs}\nfor the scene branch and {Qp, Kp, Vp} = {Fp, Fp, Fp} for\nthe part branch. Then we obtain the contact features Fc after\nmulti-head attention as\nF \u2032\ns = softmax(QpKT\ns /\np\nCt)Vs,\n(1)\nF \u2032\np = softmax(QsKT\np /\np\nCt)Vp,\n(2)\nFc = LN (F \u2032\ns \u2299 F \u2032\np),\n(3)\nwhere Ct is a scaling factor [68], \u2299 is the Hadamard operator\nand LN represents layer-normalization [2]. We obtain final\ncontact predictions \u00afyc \u2208 R6890\u00d71 after filtering Fc via a\nshallow MLP followed by sigmoid activation.\nThe DECO architecture encourages the scene and part\nencoders, Es and Ep, to focus on relevant features by up-\nsampling Fs and Fp using scene decoder Ds and part de-\ncoder Dp respectively. The output of Ds is a predicted scene\nsegmentation map, \u00af\nXs \u2208 RH\u00d7W \u00d7No, where No are the\nnumber of objects in MS COCO [41]. Similarly, we obtain\nthe part features \u00af\nXp \u2208 RH\u00d7W \u00d7(J+1) from Dp, where J are\nthe number of body parts and the extra channel is for the\nSMPL\nForward \nKinematics\nDifferentiable \nRendering\nFigure 6: The Pixel Anchoring Loss (PAL) grounds 3D\ncontact predictions to image pixels by rendering the contact-\ncolored posed mesh on the image plane. The rendered con-\ntact mask is compared with 2D contact ground truth contact\nfrom HOT [8]\n.\nbackground class.\nWe train DECO end-to-end (Fig. 5) with the loss:\nL = wcL3D\nc\n+ wpalL2D\npal + wsL2D\ns\n+ wpL2D\np ,\n(4)\nwhere L3D\nc\nis the binary-cross entropy loss between per-\nvertex predicted contact \u00afyc and ground-truth contact labels\nygt\nc . L2D\ns\nand L2D\np\nare segmentation losses between the\npredicted and the ground-truth masks. We describe L2D\npal in\nthe following section. Steering weights w are set empirically.\n4.2. 2D Pixel Anchoring Loss (PAL)\nTo relate contact on the 3D mesh with image pixels, we\npropose a novel pixel anchoring loss (PAL); see Fig. 6. We\nrun the SOTA HPS network CLIFF [39] on input image I to\ninfer the camera scale s, camera translation, tc, and SMPL\nparameters, \u03b8 and \u03b2, in the camera coordinates assuming\ncamera rotation, Rc = I3 and body translation, tb = 0.\nUsing the estimated SMPL parameters, we obtain the posed\nmesh M(\u03b8, \u03b2, tb), which is colored using DECO-predicted\nper-vertex contact probability, \u00afyc, in a continuous and dif-\nferentiable manner. We denote the posed mesh colored with\ncontact probability by Mc. We use the PyTorch3D [51]\ndifferentiable renderer to render Mc on the image under\nweak perspective, resulting in the 2D contact probability\nmap, \u00af\nX2D\nc\n. L2D\npal is computed as the binary-cross entropy\nloss between \u00af\nX2D\nc\nand the ground-truth 2D contact mask\nfrom HOT [8], X2D\nc\n.\n5. Experiments\nImplementation Details. We experiment with both Swin\nTransformer [43] and HRNET [69] as backbone architec-\ntures for Es and Ep. We initialize the two encoder con-\nfigurations with ImageNet and HRNET pretrained weights\nrespectively. We obtain pseudo ground-truth scene segmen-\ntation masks, Xs \u2208 RH\u00d7W \u00d7No, containing semantic labels\nfor No = 133 categories, by running inference using the\nSOTA image segmentation network, Mask2Former [9]. To\nget ground-truth part segmentations, \u00af\nXp \u2208 RH\u00d7W \u00d7(J+1),\nwe follow [35] to use the SMPL part segmentation and seg-\nment the posed ground-truth mesh when available (e.g. in\nRICH and PROX) into J = 24 parts, rendering each part\nmask as a separate channel. Since there are no ground-\ntruth 3D meshes in DAMON, we obtain pseudo ground-truth\nmeshes by running the SOTA human pose and shape net-\nwork, CLIFF [39]. This strategy works better in practice than\nusing a human-parsing network (e.g. Graphonomy [19]). It\nhas the advantage of left-right sided part labels, which helps\nin circumventing left-right ambiguity. It also retains full-\nvisibility under occlusion, which allows reasoning about\nparts not visible in the original image.\nTraining and Evaluation. To train DECO, we use the\nDAMON dataset along with existing datasets with 3D con-\ntact labels: RICH [29] and PROX [25]. We evaluate our\nmethod on the test splits of DAMON and RICH. To evaluate\nout-of-domain generalization performance, we also show\nevaluation on the test split of BEHAVE [3], which is not\nused in training. We follow [29] and report both count-\nbased evaluation metrics: precision, recall and F1 score and\ngeodesic error (in cm, see [29] for details). For additional\nimplementation and training details, please refer to Sup. Mat.\n5.1. 3D Contact Estimation\nWe compare DECO with BSTRO [29] and POSA [26],\nboth of which give dense vertex-level contact on the body\nmesh. Since POSA needs a posed body mesh as input, we\nshow POSA results when given ground-truth meshes, called\nPOSAGT and meshes reconstructed by PIXIE [13], called\nPOSAPIXIE. For a fair comparison, we make sure to use the\nsame training data splits in all our evaluations.\nWe report results on RICH-test, BEHAVE-test, and\nDAMON-test in Tab. 1. For evaluation on RICH-test, we\ntrain both BSTRO and DECO on the RICH training split\nonly. This ablates the effect of the DAMON dataset, allow-\ning us to isolate the contribution of the DECO architecture.\nAs shown in Tab. 1, we outperform all baselines across all\nmetrics. Specifically, we report a significant \u223c11% improve-\nment in F1 score and 7.93 cm improvement in the geodesic\nerror over the closest baseline, BSTRO. Further, we observe\nthat adding L2D\npal improves the geodesic error considerably\nwith only a slight trade-off in F1 score. Here, we reiterate\nthe observation in [29] that, while POSA matches DECO in\nrecall, it comes at the cost of precision, resulting in worse\nF1 scores. Since POSA does not rely on image evidence and\nonly takes the body pose as input, it tends to predict false\npositives. For qualitative results, see Fig. 7 and Sup. Mat.\nNext, we retrain both BSTRO and DECO on all available\ntraining datasets, RICH, PROX and DAMON, and evaluate\non the DAMON test split. POSA training needs a GT body\nwhich is not available in DAMON. This evaluation tests\ngeneralization to unconstrained Internet images. Note that\nto train with L2D\npal, we include HOT images with 2D contact\nannotations even if they do not have 3D contact labels from\nDAMON. For these images, we simply turn off L3D\nc . This\nis because DECO, unlike BSTRO, is compatible with both\n3D and 2D contact labels. DECO significantly outperforms\nall baselines and results in an F1 score of 0.55 vs 0.46 for\nBSTRO with a 16.18 cm improvement in geodesic error.\nNotably, the improvement over baselines when including\nPROX and DAMON in training is higher compared with\ntraining only on RICH, which indicates that DECO scales\nbetter with more training images compared to BSTRO.\nFinally, we evaluate out-of-domain generalization on the\nunseen BEHAVE [3] dataset. BEHAVE focuses on a single\nhuman-object contact per image, even if multiple contacting\nobjects may be present. The focus on single object-contact\nin the GT contact annotations partly explains why most\nmethods struggle with this dataset. Further, since BEHAVE\ndoes not label contact with the ground, for the purpose of\nevaluation, we mask out contact predictions on the feet. As\nreported in Tab. 1, we outperform all baselines on both F1\nand geodesic error, which indicates that DECO has a better\ngeneralization ability.\n5.2. Ablation Study\nIn Tab. 2 we evaluate the impact of our design choices.\nFirst, we analyze the effect of using a shared encoder for\nthe scene and the part branch vs separate encoders for\nboth. Compared to having separate encoders without branch-\nspecific losses, a single encoder performs better, which can\nbe attributed to having fewer training parameters. How-\never, any configuration using L2D\ns\nor L2D\np\noutperforms the\nshared encoder. While L2D\np\ncontributes improvements to\nprecision, L2D\ns\ncontributes to better recall. This is expected\nsince, intuitively, attending to body parts helps with inferring\nfine-grained contact, whereas scene context helps to reason\nMethods\nRICH [29]\nDAMON\nBEHAVE [3]\nPrecision \u2191\nRecall \u2191\nF1 \u2191\ngeo. (cm)\u2193\nPrecision \u2191\nRecall \u2191\nF1 \u2191\ngeo. (cm) \u2193\nPrecision \u2191\nRecall \u2191\nF1 \u2191\ngeo. (cm) \u2193\nBSTRO [29]\n0.65\n0.66\n0.63\n18.39\n0.51\n0.53\n0.46\n38.06\n0.13\n0.03\n0.04\n50.45\nPOSAPIXIE [13, 26]\n0.31\n0.69\n0.39\n21.16\n0.42\n0.34\n0.31\n33.00\n0.11\n0.07\n0.06\n54.29\nPOSAGT [13, 26]\n0.37\n0.76\n0.46\n19.96\n-\n-\n-\n-\n0.10\n0.09\n0.06\n55.43\nDECO\n0.71\n0.76\n0.70\n17.92\n0.64\n0.57\n0.55\n21.32\n0.25\n0.21\n0.18\n46.33\nDECO +L2D\npal\n0.71\n0.74\n0.69\n10.46\n0.65\n0.57\n0.55\n21.88\n0.27\n0.18\n0.18\n44.51\nTable 1: Comparison of DECO with SOTA models on RICH [29], DAMON, and BEHAVE [3]. See discussion in Sec. 5.1.\nGT\nDECO\nBSTRO\nPOSAPIXIE\nHOT\nRICH\nFigure 7: Qualitative evaluation of DECO (green), BSTRO (red) and POSAPIXIE (blue), alongside Ground Truth (black).\nEs\nEp\nL2D\ns\nL2D\np\nBack.\nPre. \u2191\nRec. \u2191\nF1 \u2191\ngeo. (cm) \u2193\nshared\n\u2717\n\u2717\nHR\n0.68\n0.76\n0.68\n20.85\n\u2713\n\u2713\n\u2717\n\u2717\nHR\n0.67\n0.76\n0.67\n23.54\n\u2713\n\u2713\n\u2713\n\u2717\nHR\n0.68\n0.75\n0.68\n18.44\n\u2713\n\u2713\n\u2717\n\u2713\nHR\n0.70\n0.74\n0.68\n18.37\n\u2713\n\u2713\n\u2713\n\u2713\nSW\n0.68\n0.71\n0.66\n18.54\n\u2713\n\u2713\n\u2713\n\u2713\nHR\n0.71\n0.76\n0.70\n17.92\nTable 2: Ablation study for DECO design choices (Sec. 5.2).\nWe ablate: (1) using separate or joint encoders for the scene\nand body parts, (2) using branch-specific losses, (3) using an\nHRNET (HR) or Swin (SW) backbone. Bold denotes best\nperformance.\nabout the existence of contact regions. Each one separately\nhelps with geodesic error, but the best performance comes\nwhen used together, in terms of both F1 score and geodesic\nerror. Finally, we see that the HRNET backbone outperforms\nthe Swin backbone. This is likely because HRNET is pre-\ntrained on human-centric tasks (like our task), whereas Swin\nin pretrained on ImageNet image classification.\n5.3. Inferred versus geometric contact\nAn alternative to directly inferring contact, as DECO\ndoes, is to first recover the 3D body and scene and then com-\npute contact geometrically using the distance between the\nbody and scene [74, 83]. If 3D human and scene recovery\nwere accurate, this could be a viable alternative to DECO\u2019s\ninferred contact. To test this hypothesis we perform an ex-\nperiment using the two SOTA techniques for 3D human and\nobject estimation, PHOSA [83] and CHORE [74]. PHOSA\nworks only on 8 objects, and CHORE works on 13. In con-\ntrast, DECO supports all 80 object classes in MS-COCO.\nBecause they are optimization based, PHOSA and CHORE\nare slow, taking 4 mins and 66 secs per image respectively.\nDECO is real-time and takes 0.012 secs for inference. For\nfair comparison, we split the DAMON dataset and evalu-\nate using test sets that include only objects supported by\neither PHOSA or CHORE. We reconstruct the human and\nobject and then recover contact using thresholded distance.\nCHORE achieves an F1 score of 0.08 as opposed to DECO\u2019s\nscore of 0.48. Similarly, PHOSA achieves an F1 score of\n0.18 as opposed to DECO\u2019s score of 0.60. Given the current\nstate of 3D human pose and scene estimation, DECO sig-\nnificantly outperforms geometry-based contact estimation.\n6. HPS using DECO contacts\nNext we evaluate whether contact information inferred by\nDECO can be used to improve human pose and shape (HPS)\nregression; we do so using the PROX \u201cquantitative\u201d dataset\n[25]. PROX uses an optimization method to fit SMPL-X\nbodies to images. It further assumes a-priori known 3D\nscenes and uses manually-annotated contact regions on the\nbody to encourage these body vertices to be in contact with\nthe scene if they are sufficiently close, while penalizing\nbody-scene penetration.\nSpecifically, we replace the manually-annotated contact\nvertices with the inferred SMPL-X body-part contact ver-\ntices from baseline methods as well as the detailed contact\nestimated by DECO. For a fair comparison, we follow the\nsame experimental setup as HOT [8] and evaluate all meth-\nods using the Vertex-to-Vertex (V2V) error. For the \u201cNo\ncontact\u201d setup, we turn off all contact constraints in the op-\ntimization process. PROX uses the contact regions on the\nbody from the original method [25]. HOT uses the body-\npart vertices from the body-part labels predicted by the HOT\ndetector. We also report V2V errors when using the ground-\ntruth (GT) contact vertices. The results in Tab. 3 illustrate\nthe value of inferring detailed contact on the body.\nAll baselines in Tab. 3 use PROX\u2019s [25] hyperparame-\nters for a fair comparison. PROX uses a Geman-McClure\nrobust error function (GMoF) for the contact term (see Eq.4\nin [25]), so that the manually-defined contact areas that lie\n\u201cclose enough\u201d to the scene are snapped onto it. The robust\nscale term, \u03c1C = 5e\u221202, is tuned for PROX\u2019s naive contact\nprediction; this is relatively conservative as PROX uses no\nimage contact for this prediction. Since DECO takes into ac-\ncount the image features, and makes a much more informed\ncontact prediction, we we can \u201crelax\u201d this robustness term,\nand trust the output of DECO regressor more. In Tab. 4 we\nreport a sensitivity analysis by varying \u03c1C with DECO\u2019s con-\ntact predictions. The results verify that we can trust DECO\u2019s\nMethod\nNo\nPROX\nHOT\nDECO\nGT\nContact\n[25]\n[8]\nContact\nContact\nV2V \u2193\n183.3\n174.0\n172.3\n171.6\n163.0\nTable 3: HPS estimation performance using contact derived\nfrom different sources.\nGMoF \u03c1C\n1e-03\n5e-02\n1e-01\n1.0\n2.0\n3.0\n5.0\nV2V \u2193\n180.07\n171.6\n170.0\n169.0\n176.5\n179.6\n183.5\nTable 4: Sensitivity analysis for the \u03c1C value in the Geman-\nMcClure error function (GMoF) of the contact term.\ncontacts more, and that there is a sweet spot for \u03c1C = 1.0.\nThis suggests that exploiting inferred contact is a promising\ndirection for improving HPS estimates.\n7. Conclusion\nWe focus on detecting 3D human-object contact from a\nsingle image taken in the wild; existing methods perform\npoorly for such images. To this end, we use crowd-sourcing\nto collect DAMON, a rich dataset of in-the-wild images\npaired with pseudo ground-truth 3D contacts on the vertex\nlevel, as well as labels for the involved objects and body parts.\nUsing DAMON, we train DECO, a novel model that detects\ncontact on a 3D body from a single color image. DECO\u2019s\nnovelty lies in cross-attending to both the relevant body parts\nand scene elements, while it also anchors the inferred 3D\ncontacts to the relevant 2D pixels. Experiments show that\nDECO outperforms existing work by a good margin, and\ngeneralizes reasonably well in the wild. To enable further\nresearch, we release our data, models and code.\nFuture work: DECO currently reasons about contact\nbetween a single person, the scene, and multiple objects. Our\nlabelling tool and DECO could be extended to fine-grained\nhuman-human, human-animal and self-contact. Another\npromising, but challenging, direction would be to leverage\ncaptions in existing datasets, or methods that infer captions\nfor unlabeled images, via large language models (LLM).\nAcknowledgements: We sincerely thank Alpar Cseke\nfor his contributions to DAMON data collection and PHOSA\nevaluations, Sai K. Dwivedi for facilitating PROX down-\nstream experiments, Xianghui Xie for help with CHORE\nevaluations, Lea M\u00a8uller for her help in initiating the con-\ntact annotation tool, Chun-Hao P. Huang for RICH discus-\nsions and Yixin Chen for details about the HOT paper. We\nare grateful to Mengqin Xue and Zhenyu Lou for their col-\nlaboration in BEHAVE evaluations and Tsvetelina Alexi-\nadis for valuable data collection guidance. Their invalu-\nable contributions enriched this research significantly. This\nwork was funded by the International Max Planck Research\nSchool for Intelligent Systems (IMPRS-IS).\nDisclosure:\nhttps://files.is.tue.mpg.de/black/CoI ICCV 2023.txt\nSupplementary Material\nA. DAMON Data Collection and Quality\nWe select images for annotation from the HOT [8] curated\nsubset of V-COCO [22] and HAKE [38] by filtering out\nimages containing multiple people or images with a single\nperson but fewer than 10 visible keypoints. For keypoint\nestimation, we use the transformer-based SOTA 2D keypoint\nestimator ViTPose [77].\nWe take several steps to limit ambiguity in the contact\nannotation task.\nHere, we focus on scene- and human-\nsupported contact. The requirement for support resolves\nambiguous cases, e.g. humans close to scene objects but not\nin contact. We use the object labels in V-COCO and HAKE\nto filter out images containing unsupported human-human\nand human-animal contact. V-COCO and HAKE also con-\ntain action labels that we leverage to filter out ambiguous\nindirect contact which does not involve physical touch, such\nas direct, greet, herd, hose, point, teach, etc. The training\nvideo (in Sup. Mat.) advises workers to orient the 3D mesh\nand to visualize themselves in the same posture as the per-\nson in the image. This helps infer contact while avoiding\nleft-right ambiguity. Our Fleiss\u2019 Kappa score indicates sig-\nnificant agreement between annotators (see Appendix A.3),\nsuggesting that our protocol effectively minimizes task am-\nbiguities.\nTo facilitate crowd-sourced 3D contact annotation using\nAmazon Mechanical Turk (AMT), we build a new annotation\ntool which we describe in detail in the following section.\nPlease see the Supplemental Video.\nA.1. Dense Contact Annotation Tool\nWe built a dense contact annotation tool to collect anno-\ntations from the DAMON dataset images. The code for the\ntool is written using Dash, a popular Python framework for\nbuilding web applications. This application is deployed in-\nside a Docker container under an uWSGI application server,\neventually served by a NGINX web server acting as a reverse\nproxy. The annotation tool is accessible under a public URL\nused to create the Human Intelligent Tasks on AMT.\nInterface and use. As seen in Fig. S.1, the application is\nmade of four parts. The top part contains a title and general\ninstructions about how to use the annotation tool. The left\npart is made of the image and a label describing which\ncontact should be annotated (object or supporting contact).\nThe right part contains the mesh to be annotated by hovering\nover it. The mesh can be translated, rotated, and zoomed-\nin/out. A slider allows the user to select the size of the brush,\nand buttons are available for switching modes (draw/erase),\nerasing the full selection, and resetting the camera. Finally,\na confirmation button is located at the bottom of the window\nto submit an annotation to the server. The user must provide\none annotation for several human-object contacts and for\nthe supporting contact. Once the last annotation has been\nsubmitted, a dialog box appears to ask for optional feedback\nabout the annotation task for the current image. This helps\nworkers report ambiguous contact scenarios.\nCallbacks. Dash applications work with callbacks. Call-\nbacks are functions that are fired when an input component\nis updated (e.g., a button is clicked) and that update output\ncomponents. Regular callbacks are executed on the server-\nside: they are simpler to implement, but slower to execute.\nOn the other hand, client-side callbacks are faster but require\na more complex implementation. The user will spend most\nof their time annotating the high-resolution mesh. It should\ntherefore be smooth and fast. As such, we implemented this\nlogic in JavaScript as a client-side callback. Other callbacks,\nfor instance when the camera is reset or the brush size is\nupdated, rarely happen and do not require a fast response.\nTherefore, they have been implemented as server-side call-\nbacks. During their execution, a spinner appears to let the\nworker know that the application is updating.\nCaching. When a vertex is annotated, vertices belonging\nto a neighboring region are also annotated. The extent of this\nneighboring region is correlated with the brush size that the\nuser chooses. When we start the application, we compute,\nfor each vertex and for each brush size, all of its neighboring\nvertices. As the mesh is static, this has to be done only once.\nTherefore, we cache this result and use it for all annotations.\nVideo. Please watch the Supplementary Video for an\nin-depth tour of our tool, its features and the annotation\nprotocol. Note that this is the same video we showed AMT\nworkers for training purposes during qualification.\nA.2. DAMON Additional Statistics\nFigure S.3 shows the full version of Fig. 3 in the main pa-\nper. The DAMON dataset is long-tailed and it covers contact\nscenarios with a wide variety of objects and scenes. Please\nrefer to the sunburst plot in Fig. S.3 for a full breakdown.\nFigure S.2 shows the number of images per object label.\nWe see that contact with feet, hands, and the bigger body\nparts (torso, hips, upper arms) prevails; this makes sense\nas humans interact with objects mostly with these (e.g., for\nwalking, grasping, sitting, lying down). However, interac-\ntions are highly varied, thus, the distribution is long-tailed\nand includes all body parts.\nWorkers take on average 3.48 min/image and we pay\n$0.5/image. The total cost is $3313.20 with AMT fees. The\nDAMON contact annotations are not prohibitively expensive\ngiven that it provides a stepping stone for future research.\nA.3. Quality Control and Evaluation\nWe adopt two strategies to ensure quality and avoid noisy\nannotations in the DAMON dataset. First, we conduct quali-\nFigure S.1: AMT interface design for our annotation tool. We show a an example of an annotator collecting human-supported\ncontact for the object label \u201cBook\u201d. The application cycles through all available object labels in the image and the scene-\nsupported contact. Please refer to the Supplemental Video for a detailed description of the tool. \u00fc Zoom in\nfication tasks to shortlist high-quality annotator candidates.\nThis qualification task has two parts: (i) watching a detailed\ntutorial video (see Supplementary Video) explaining the\ntask and annotator tool step-by-step by showing three exam-\nple annotations with varying degrees of contact complexity,\n(ii) annotating 10 sample images for contact annotations.\nFor the sample images, we had a set of author-annotated\npseudo-ground-truth (pseudo GT) labels. The responses\nof candidates were evaluated using Intersection-over-Union\n(IoU) with the pseudo-GT labels. Workers who responded\nsatisfactorily were allowed to annotate the DAMON dataset\nimages. We qualified 14 out of 100 participants after the\nqualification round. The second strategy involved hiring\nMaster\u2019s students as meta-annotators to visually inspect the\nquality of contact annotations. Annotations that were flagged\nas incorrect or low-quality were sent for re-annotation with\nspecific feedback to the annotators on how to avoid mistakes.\nWe assess the quality of the DAMON dataset by measur-\ning the label accuracy and the level of annotator\u2019s agree-\nment.\nWe evaluate label accuracy by manually selecting 100 im-\nages with contact labels from the RICH [29] and PROX [25]\ndatasets. Note that the pseudo-ground-truth contact labels in\nthese datasets are obtained by thresholding the Signed Dis-\ntance Field (SDF) between the reconstructed human mesh\nand the 3D scene. We evaluate annotations from qualified\nworkers on these images and compute IoU w.r.t. the pseudo-\nground-truth contact labels. With this, we obtain an IOU\nscore of 0.512 on RICH, 0.263 on PROX, and a mean IOU\n(mIOU) score of 0.450.\nFigure S.5 visualizes the DAMON annotation earning the\nlowest IoU scores. Scanned datasets that rely on thresholding\nSDF values for estimating contact labels fail to take into\naccount the soft-tissue deformation of the human body when\nit interacts with rigid objects. The vertices in the \u201csoft\u201d body\nparts such as buttocks, thighs, etc interpenetrate far enough\nleftFoot\nrightFoot\nrightHand\nleftHand\nspine\nhips\nleftUpLeg\nrightUpLeg\nrightForeArm\nrightLeg\nhead\nrightShoulder\nleftShoulder\nleftForeArm\nleftLeg\nrightArm\nleftArm\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n# images\nFigure S.2: DAMON dataset statistics. The number of im-\nages (y-axis) for which each body part (x-axis) has at least\n10 vertices in contact. For visualization purposes here we\ncombine the fingers into the hands category, the toes into\nthe feet category, and several spine parts into a single spine\ncategory (SMPL has 24 parts but here we show 17 bars).\n\u00fc Zoom in.\nfrom the scan surface to overshoot the heuristic threshold,\nleading to noisy GT annotation and a \u201cring\u201d like contact\nprofile. DAMON is annotated by human annotators and\ntherefore does not suffer from this issue. This produces a\nmismatch between these two types of ground truth. Note\nthat DAMON ground truth is closer to reality.\nWe also compare annotations on a randomly-selected set\nof 10 images from all the qualified workers against author-\nannotated labels, resulting in mIOU = 0.510.\nTo determine the agreement between annotators, qualified\nworkers annotate the same set of 10 images and we report\nthe Fleiss\u2019 Kappa (\u03ba) metric. Fleiss\u2019 Kappa is a statistical\nmeasure used to evaluate the agreement level among a fixed\nnumber of annotators when assigning categorical labels to\ndata. It considers the possibility of chance agreement and\nprovides a standardized measure of inter-rater reliability that\nranges from 0 (no agreement) to 1 (perfect agreement). In\nthis study, we obtain a Fleiss\u2019 Kappa \u03ba = 0.656 which is\nconsidered \u201csubstantial agreement\u201d between workers [24].\nNote, \u03ba of 1 means \u201cperfect agreement\u201d, 0 means \u201cchance\nagreement\u201d and -1 means \u201cperfect disagreement\u201d. To build\nintuition on the significance of \u03ba, Fig. S.4 shows example\nannotations with low and high \u03ba scores.\nB. DECO Experiments\nB.1. Implementation Details\nFor training DECO, we resize input images, the scene\nsegmentation mask and the part segmentation mask such\nthat I \u2208 R3\u00d7256\u00d7256, Xs \u2208 R133\u00d7256\u00d7256 and Xp \u2208\nR25\u00d7256\u00d7256. Fs and Fs are of size R480\u00d764\u00d764S. We de-\ntermine the loss weights in Eqn. 4 empirically and set it to\nwc = 10.0, wpal = 0.05, ws = 1.0 and wp = 1.0. We use\nthe Adam optimizer with a learning rate of 5 \u00d7 10\u22125 and\nbatch size of 4, and training takes 12 epochs (\u223c 31 hours)\non an Nvidia Tesla V100 GPU.\nFor evaluation on RICH-test in Tab. 1 in main, we sub-\nsample every 10th frame from the released test set.\nThe base model without context branches has 90.19M\nparameters. Adding context branches (L2D\ns\nand L2D\np ) adds\nanother 853K parameters. This improves the geodesic error\nby \u223c24% (see Tab. 1 in main), at the cost of \u223c1% increase in\ncomplexity. We will release both models, with and without\ncontext branches.\nB.2. Additional Qualitative results\nFigure S.6 shows DECO estimated contact and compari-\nson with baseline methods from the test subset of DAMON.\nFigure S.7 shows DECO contacts on some randomly sam-\npled images from the internet.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n# images\nsurfboard\nchair\nsnowboard\nlaptop\ncell phone\nsports ball\ndining table\nskateboard\ntennis racket\nbook\nfrisbee\ncup\ncar\nbed\nbottle\nbench\ncouch\nhandbag\nbackpack\nknife\nbowl\nskis\nbaseball glove\npotted plant\ntv\nmouse\npizza\nsuitcase\nfork\nkeyboard\ntie\nspoon\nremote\ncake\ntruck\nclock\nsandwich\nbicycle\numbrella\nwine glass\nboat\ndonut\noven\nteddy bear\nsink\nrefrigerator\nscissors\nvase\nbaseball bat\nkite\nbanana\nhot dog\ntraffic light\nmotorcycle\nmicrowave\napple\nfire hydrant\ncarrot\nbus\nbroccoli\norange\ntoilet\nstop sign\nhair drier\ntrain\nairplane\nparking meter\ntoothbrush\ntoaster\n5.3%\n6.7%\n16.5%\n25.6%\n30.5%\n4.6%\n10.8%\ntransportation\naccessories\nfurniture\neveryday\nobjects\nsports\nequipment\nfood\nitems\nkitchen\nappliances\nFigure S.3: Full version plot for DAMON dataset statistics (Fig. 3 in Main). Histogram: contact object labels (y-axis) and the\nnumber of images in which they are present (x-axis). Pie chart: object labels are grouped into 7 main categories; inner colors\ncorrespond to the colors in the histogram. \u00fc Zoom in.\nTennis Racket\nSupporting\nWorker A \nWorker B \nWorker C \nFleiss\u2019 Kappa 0.80\nSupporting\nFleiss\u2019 Kappa 0.54\nWorker A \nWorker B \nWorker C \nFigure S.4: Annotator agreement indicated by \u03ba \u00fc Zoom in.\n0.17\n0.09\n0.18\n0.25\nFigure S.5: DAMON annotations (in magenta) earning the lowest IOU scores compared to GT contact in PROX and RICH (in\nblack). IOU scores are reported to the left of RGB images for each row. Scanned datasets (e.g. PROX/RICH) infer contact by\nthresholding the SDF between body and scene, which can be sub-optimal due to soft-tissue deformation of the body (see text).\nGT\nPOSAPIXIE\nBSTRO\nDECO\nFigure S.6: Additional qualitative evaluation of DECO (green), BSTRO (red) and POSAPIXIE (blue), alongside Ground Truth\n(black) on images from the DAMON dataset.\nFigure S.7: DECO predictions (in green) on Internet images, not seen during training.\nReferences\n[1] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid\nPishchulin, Anton Milan, Juergen Gall, and Bernt Schiele.\nPoseTrack: A benchmark for human pose estimation and\ntracking.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 5167\u20135176, 2018. 2\n[2] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization. CoRR, abs/1607.06450, 2016. 6\n[3] Bharat Lal Bhatnagar, Xianghui Xie, Ilya A Petrov, Cristian\nSminchisescu, Christian Theobalt, and Gerard Pons-Moll.\nBEHAVE: Dataset and method for tracking human object\ninteractions. In Computer Vision and Pattern Recognition\n(CVPR), pages 15935\u201315946, 2022. 2, 4, 7, 8\n[4] Samarth Brahmbhatt, Chengcheng Tang, Christopher D.\nTwigg, Charles C. Kemp, and James Hays. ContactPose:\nA dataset of grasps with object contact and hand pose. In\nEuropean Conference on Computer Vision (ECCV), volume\n12358, pages 361\u2013378. Springer, 2020. 2\n[5] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh\nVo, and Jitendra Malik. Long-term human motion prediction\nwith scene context. In European Conference on Computer\nVision (ECCV), volume 12346, pages 387\u2013404, 2020. 4\n[6] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and\nYaser Sheikh. OpenPose: Realtime multi-person 2D pose\nestimation using part affinity fields. Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 43(1):172\u2013186,\n2021. 2\n[7] Zhe Cao, Ilija Radosavovic, Angjoo Kanazawa, and Jitendra\nMalik. Reconstructing hand-object interactions in the wild.\nInternational Conference on Computer Vision (ICCV), pages\n12397\u201312406, 2021. 3\n[8] Yixin Chen, Sai Kumar Dwivedi, Michael J. Black, and Dim-\nitrios Tzionas. Detecting human-object contact in images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 2, 3, 4, 6, 7, 9,\n10\n[9] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander\nKirillov, and Rohit Girdhar. Masked-attention mask trans-\nformer for universal image segmentation. In Computer Vision\nand Pattern Recognition (CVPR), pages 1290\u20131299, 2022. 7\n[10] Vasileios Choutas, Lea M\u00a8uller, Chun-Hao P. Huang, Siyu\nTang, Dimitrios Tzionas, and Michael J. Black. Accurate 3D\nbody shape regression using metric and semantic attributes.\nIn Computer Vision and Pattern Recognition (CVPR), pages\n2718\u20132728, 2022. 2\n[11] Henry M. Clever, Zackory M. Erickson, Ariel Kapusta, Greg\nTurk, C. Karen Liu, and Charles C. Kemp. Bodies at Rest: 3D\nhuman pose and shape estimation from a pressure image using\nsynthetic data. In Computer Vision and Pattern Recognition\n(CVPR), pages 6214\u20136223, 2020. 3\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image database.\nIn Computer Vision and Pattern Recognition (CVPR), pages\n248\u2013255, 2009. 2\n[13] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\nTzionas, and Michael J. Black. Collaborative regression of\nexpressive bodies using moderation. In International Confer-\nence on 3D Vision (3DV), 2021. 7, 8\n[14] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut\nPopa, Vlad Olaru, and Cristian Sminchisescu.\nThree-\ndimensional reconstruction of human interactions. In Com-\nputer Vision and Pattern Recognition (CVPR), pages 7212\u2013\n7221, 2020. 2, 3, 4\n[15] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut\nPopa, Vlad Olaru, and Cristian Sminchisescu. Learning com-\nplex 3D human self-contact. In AAAI Conference on Artificial\nIntelligence, 2021. 2, 3, 4\n[16] Mihai Fieraru, Mihai Zanfir, Teodor Szente, Eduard Bazavan,\nVlad Olaru, and Cristian Sminchisescu. REMIPS: Physi-\ncally consistent 3D reconstruction of multiple interacting\npeople under weak supervision. In Conference on Neural\nInformation Processing Systems (NeurIPS), volume 34, pages\n19385\u201319397. Curran Associates, Inc., 2021. 3, 4\n[17] Christopher Funk, Savinay Nagendra, Jesse Scott, Bharadwaj\nRavichandran, John H Challis, Robert T Collins, and Yanxi\nLiu. Learning dynamics from kinematics: Estimating 2D foot\npressure maps from video frames. arXiv:1811.12607, 2018.\n3\n[18] Erik G\u00a8artner, Mykhaylo Andriluka, Erwin Coumans, and Cris-\ntian Sminchisescu. Differentiable dynamics for articulated\n3D human motion reconstruction. In Computer Vision and\nPattern Recognition (CVPR), pages 13180\u201313190, 2022. 3\n[19] Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng\nWang, and Liang Lin. Graphonomy: Universal human parsing\nvia graph transfer learning. In Computer Vision and Pattern\nRecognition (CVPR), pages 7450\u20137459, 2019. 7\n[20] Patrick Grady, Chengcheng Tang, Samarth Brahmbhatt,\nChristopher D Twigg, Chengde Wan, James Hays, and\nCharles C Kemp. PressureVision: Estimating hand pres-\nsure from a single RGB image. In European Conference on\nComputer Vision (ECCV), 2022. 3\n[21] Patrick Grady, Chengcheng Tang, Christopher D. Twigg,\nMinh Vo, Samarth Brahmbhatt, and Charles C. Kemp. Contac-\ntopt: Optimizing contact to improve grasps. 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 1471\u20131481, 2021. 3\n[22] Saurabh Gupta and Jitendra Malik. Visual semantic role\nlabeling. arXiv:1505.04474, 2015. 2, 4, 10\n[23] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard\nPons-Moll. Human POSEitioning System (HPS): 3D hu-\nman pose estimation and self-localization in large scenes\nfrom body-mounted sensors. In Computer Vision and Pattern\nRecognition (CVPR), pages 4318\u20134329, 2021. 4\n[24] Kilem L Gwet. Handbook of inter-rater reliability: The\ndefinitive guide to measuring the extent of agreement among\nraters. Advanced Analytics, LLC, 2014. 12\n[25] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and\nMichael J. Black. Resolving 3D human pose ambiguities\nwith 3D scene constraints. In International Conference on\nComputer Vision (ICCV), pages 2282\u20132292, 2019. 2, 3, 4, 5,\n7, 9, 11\n[26] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios\nTzionas, and Michael J. Black. Populating 3D scenes by\nlearning human-scene interaction. In Computer Vision and\nPattern Recognition (CVPR), pages 14708\u201314718, 2021. 4, 7,\n8\n[27] Yana Hasson, G\u00a8ul Varol, Dimitrios Tzionas, Igor Kalevatykh,\nMichael J. Black, Ivan Laptev, and Cordelia Schmid. Learning\njoint reconstruction of hands and manipulated objects. In\nComputer Vision and Pattern Recognition (CVPR), pages\n11807\u201311816, 2019. 3\n[28] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross B.\nGirshick. Mask R-CNN. In International Conference on\nComputer Vision (ICCV), pages 2980\u20132988, 2017. 2\n[29] Chun-Hao Huang, Hongwei Yi, Markus H\u00a8oschle, Matvey\nSafroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel\nScharstein, and Michael Black. Capturing and inferring dense\nfull-body human-scene contact. In Computer Vision and\nPattern Recognition (CVPR), 2022. 2, 4, 5, 7, 8, 11\n[30] Yinghao Huang, Omid Taheri, Michael J. Black, and Dim-\nitrios Tzionas. InterCap: Joint markerless 3D tracking of\nhumans and objects in interaction. In German Conference on\nPattern Recognition (GCPR), volume 13485, pages 281\u2013299,\n2022. 2, 4\n[31] Leslie Ikemoto, Okan Arikan, and David Forsyth. Knowing\nwhen to put your foot down. In Proceedings of the 2006\nSymposium on Interactive 3D Graphics and Games, page\n49\u201353, 2006. 3\n[32] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In Computer Vision and Pattern Recognition (CVPR),\npages 7122\u20137131, 2018. 5\n[33] Rawal Khirodkar, Shashank Tripathi, and Kris Kitani. Oc-\ncluded human mesh recovery. In Computer Vision and Pattern\nRecognition (CVPR), pages 1705\u20131715, 2022. 5\n[34] Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim,\nand Hyunwoo J. Kim. HOTR: End-to-end human-object\ninteraction detection with transformers. In Computer Vision\nand Pattern Recognition (CVPR), 2021. 3\n[35] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,\nand Michael J. Black. PARE: Part attention regressor for\n3D human body estimation. In International Conference on\nComputer Vision (ICCV), pages 11127\u201311137, 2021. 5, 6, 7\n[36] Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, and\nKostas Daniilidis. Learning to reconstruct 3D human pose\nand shape via model-fitting in the loop. In International\nConference on Computer Vision (ICCV), pages 2252\u20132261,\n2019. 5\n[37] Hema S Koppula and Ashutosh Saxena. Physically grounded\nspatio-temporal object affordances. In European Conference\non Computer Vision (ECCV), pages 831\u2013847. Springer, 2014.\n3\n[38] Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu,\nShiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, and\nCewu Lu. Pastanet: Toward human activity knowledge engine.\nIn Computer Vision and Pattern Recognition (CVPR), pages\n382\u2013391, 2020. 2, 4, 10\n[39] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,\nand Youliang Yan. CLIFF: Carrying location information\nin full frames into human pose and shape estimation. In\nEuropean Conference on Computer Vision (ECCV), volume\n13665, pages 590\u2013606, 2022. 3, 5, 6, 7\n[40] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human\npose and mesh reconstruction with transformers. In Computer\nVision and Pattern Recognition (CVPR), pages 1954\u20131963,\n2021. 2\n[41] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C. Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEuropean Conference on Computer Vision (ECCV), volume\n8693, pages 740\u2013755, 2014. 6\n[42] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C. Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision (ECCV), 2014. 2\n[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nInternational Conference on Computer Vision (ICCV), pages\n10012\u201310022, 2021. 5, 7\n[44] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\nmulti-person linear model. Transactions on Graphics (TOG),\n34(6):248:1\u2013248:16, 2015. 4, 5\n[45] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In Conference on Neural Infor-\nmation Processing Systems (NeurIPS), pages 13\u201323, 2019.\n6\n[46] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael J. Black. AMASS: Archive of\nmotion capture as surface shapes. In International Conference\non Computer Vision (ICCV), pages 5441\u20135450, 2019. 3\n[47] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,\nHelge Rhodin, Mohammad Shafiei, Hans-Peter Seidel,\nWeipeng Xu, Dan Casas, and Christian Theobalt. VNect:\nReal-time 3D human pose estimation with a single RGB cam-\nera. Transactions on Graphics (TOG), 36(4):44:1\u201344:14,\n2017. 6\n[48] Lea M\u00a8uller, Ahmed A. A. Osman, Siyu Tang, Chun-Hao P.\nHuang, and Michael J. Black. On self-contact and human\npose. In Computer Vision and Pattern Recognition (CVPR),\npages 9990\u20139999, 2021. 2, 3, 4\n[49] Supreeth Narasimhaswamy, Trung Nguyen, and Minh Hoai\nNguyen. Detecting hands and recognizing physical contact\nin the wild. Conference on Neural Information Processing\nSystems (NeurIPS), 33:7841\u20137851, 2020. 2, 3\n[50] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,\nand Song-Chun Zhu. Learning human-object interactions by\ngraph parsing neural networks. In European Conference on\nComputer Vision (ECCV), pages 401\u2013417, 2018. 3\n[51] Nikhila Ravi, Jeremy Reizenstein, David Novotn\u00b4y, Taylor\nGordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari.\nAccelerating 3D deep learning with PyTorch3D.\nCoRR,\nabs/2007.08501, 2020. 7\n[52] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\nSrinath Sridhar, and Leonidas J Guibas. HuMoR: 3D human\nmotion model for robust pose estimation. In International\nConference on Computer Vision (ICCV), pages 11488\u201311499,\n2021. 2, 3\n[53] Davis Rempe, Leonidas J Guibas, Aaron Hertzmann, Bryan\nRussell, Ruben Villegas, and Jimei Yang. Contact and human\ndynamics from monocular video. In European Conference on\nComputer Vision (ECCV), pages 71\u201387. Springer, 2020. 3\n[54] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Em-\nbodied hands: Modeling and capturing hands and bodies to-\ngether. Transactions on Graphics (TOG), 36(6):245:1\u2013245:17,\n2017. 5\n[55] Anirban Roy and Sinisa Todorovic. A multi-scale CNN for\naffordance segmentation in RGB images. In European Con-\nference on Computer Vision (ECCV), pages 186\u2013201, 2016.\n3\n[56] Nadine R\u00a8uegg,\nShashank Tripathi,\nKonrad Schindler,\nMichael J. Black, and Silvia Zuffi. BITE: Beyond priors\nfor improved three-D dog pose estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 8867\u20138876, June 2023. 4\n[57] Jesse Scott, Bharadwaj Ravichandran, Christopher Funk,\nRobert T Collins, and Yanxi Liu. From image to stability:\nlearning dynamics from human pose. In European Conference\non Computer Vision (ECCV), pages 536\u2013554, 2020. 3\n[58] Dandan Shan, Jiaqi Geng, Michelle Shu, and David F Fouhey.\nUnderstanding human hands in contact at internet scale. In\nComputer Vision and Pattern Recognition (CVPR), pages\n9869\u20139878, 2020. 3\n[59] Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura,\nDani Lischinski, Daniel Cohen-Or, and Baoquan Chen. Mo-\ntionet: 3D human motion reconstruction from monocular\nvideo with skeleton consistency. Transactions on Graphics\n(TOG), 40(1):1\u201315, 2020. 3\n[60] Soshi Shimada, Vladislav Golyanik, Zhi Li, Patrick P\u00b4erez,\nWeipeng Xu, and Christian Theobalt. HULC: 3D human\nmotion capture with pose manifold sampling and dense con-\ntact guidance. In European Conference on Computer Vision\n(ECCV), pages 516\u2013533, 2022. 2, 4\n[61] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Chris-\ntian Theobalt. PhysCap: Physically plausible monocular 3D\nmotion capture in real time. Transactions on Graphics (TOG),\n39(6):1\u201316, 2020. 3\n[62] Stephan Streuber, M. Alejandra Quiros-Ramirez, Matthew Q.\nHill, Carina A. Hahn, Silvia Zuffi, Alice O\u2019Toole, and\nMichael J. Black. Body Talk: Crowdshaping realistic 3D\navatars with words. Transactions on Graphics (TOG), 35(4),\n2016. 2\n[63] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao Mei.\nHuman mesh recovery from monocular images via a skeleton-\ndisentangled representation. In International Conference on\nComputer Vision (ICCV), pages 5348\u20135357, 2019. 6\n[64] Omid Taheri, Vasileios Choutas, Michael J. Black, and Dim-\nitrios Tzionas. GOAL: Generating 4D whole-body motion\nfor hand-object grasping. In Computer Vision and Pattern\nRecognition (CVPR), 2022. 3\n[65] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-\nitrios Tzionas. GRAB: A dataset of whole-body human grasp-\ning of objects. In European Conference on Computer Vision\n(ECCV), volume 12349, pages 581\u2013600, 2020. 4\n[66] Bugra Tekin, Pablo M\u00b4arquez-Neila, Mathieu Salzmann, and\nPascal Fua. Learning to fuse 2D and 3D image cues for\nmonocular body pose estimation. In International Conference\non Computer Vision (ICCV), pages 3961\u20133970, 2017. 6\n[67] Shashank Tripathi, Lea M\u00a8uller, Chun-Hao P. Huang, Taheri\nOmid, Michael J. Black, and Dimitrios Tzionas.\n3D hu-\nman pose estimation via intuitive physics. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n4713\u20134725, 2023. 3\n[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Conference on\nNeural Information Processing Systems (NeurIPS), pages\n5998\u20136008, 2017. 6\n[69] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, Wenyu Liu, and Bin Xiao.\nDeep\nhigh-resolution representation learning for visual recognition.\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI), 43(10):3349\u20133364, 2021. 5, 7\n[70] Tiancai Wang, Rao Muhammad Anwer, Muhammad Haris\nKhan, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao, and\nJorma Laaksonen.\nDeep contextual attention for human-\nobject interaction detection.\nIn International Conference\non Computer Vision (ICCV), pages 5694\u20135702, 2019. 3\n[71] Xiaolong Wang, Rohit Girdhar, and Abhinav Gupta. Binge\nwatching: Scaling affordance learning from sitcoms. In Com-\nputer Vision and Pattern Recognition (CVPR), pages 2596\u2013\n2605, 2017. 3\n[72] Zhenzhen Weng and Serena Yeung. Holistic 3D human and\nscene mesh estimation from single view images. In Computer\nVision and Pattern Recognition (CVPR), pages 334\u2013343, 2020.\n4\n[73] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 2\n[74] Xianghui Xie, Bharat Lal Bhatnagar, and Gerard Pons-Moll.\nCHORE: Contact, human and object reconstruction from a\nsingle rgb image. In European Conference on Computer\nVision (ECCV), pages 125\u2013145. Springer, 2022. 3, 4, 8, 9\n[75] Xianghui Xie, Bharat Lal Bhatnagar, and Gerard Pons-Moll.\nVisibility aware human-object interaction tracking from single\nrgb camera. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 4757\u20134768, June 2023. 4\n[76] Bingjie Xu, Yongkang Wong, Junnan Li, Qi Zhao, and Mo-\nhan S Kankanhalli. Learning to detect human-object inter-\nactions with knowledge. In Computer Vision and Pattern\nRecognition (CVPR), 2019. 3\n[77] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.\nViTPose: Simple vision transformer baselines for human pose\nestimation. In Conference on Neural Information Processing\nSystems (NeurIPS), 2022. 10\n[78] M. Yamamoto and K. Yagishita. Scene constraints-aided\ntracking of human body. In Computer Vision and Pattern\nRecognition (CVPR), pages 151\u2013156 vol.1, 2000. 3\n[79] Hongwei Yi, Chun-Hao P. Huang, Shashank Tripathi, Lea\nHering, Justus Thies, and Michael J. Black. MIME: Human-\naware 3D scene generation. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 12965\u201312976,\nJune 2023. 3, 4\n[80] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason\nSaragih. SimPoE: Simulated character control for 3D human\npose estimation. In Computer Vision and Pattern Recognition\n(CVPR), pages 7159\u20137169, 2021. 3\n[81] Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchis-\nescu. Monocular 3D pose and shape estimation of multiple\npeople in natural scenes \u2013 the importance of multiple scene\nconstraints. In Computer Vision and Pattern Recognition\n(CVPR), pages 2148\u20132157, 2018. 3\n[82] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D human\npose and shape regression with pyramidal mesh alignment\nfeedback loop. In International Conference on Computer\nVision (ICCV), pages 11426\u201311436, 2021. 5\n[83] Jason Y Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan,\nJitendra Malik, and Angjoo Kanazawa. Perceiving 3D human-\nobject spatial arrangements from a single image in the wild.\nIn European Conference on Computer Vision (ECCV), pages\n34\u201351. Springer, 2020. 3, 4, 8, 9\n[84] Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys, and\nSiyu Tang. Learning motion priors for 4D human body cap-\nture in 3D scenes. In International Conference on Computer\nVision (ICCV), pages 11343\u201311353, 2021. 3\n[85] Siwei Zhang, Yan Zhang, Qianli Ma, Michael J. Black, and\nSiyu Tang. PLACE: Proximity learning of articulation and\ncontact in 3D environments. In International Conference on\n3D Vision (3DV), pages 642\u2013651, 2020.\n[86] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J.\nBlack, and Siyu Tang. Generating 3D people in scenes with-\nout people. In Computer Vision and Pattern Recognition\n(CVPR), pages 6193\u20136203, 2020. 3\n[87] Yixin Zhu, Chenfanfu Jiang, Yibiao Zhao, Demetri Terzopou-\nlos, and Song-Chun Zhu. Inferring forces and learning human\nutilities from videos. In Computer Vision and Pattern Recog-\nnition (CVPR), pages 3823\u20133833, 2016. 3\n[88] Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu\nZhao, Boxun Li, Chenguang Zhang, Chi Zhang, Yichen Wei,\nand Jian Sun. End-to-end human object interaction detec-\ntion with HOI transformer. In Computer Vision and Pattern\nRecognition (CVPR), pages 11825\u201311834, 2021. 3\n[89] Yuliang Zou, Jimei Yang, Duygu Ceylan, Jianming Zhang,\nFederico Perazzi, and Jia-Bin Huang. Reducing footskate in\nhuman motion reconstruction with ground contact constraints.\nIn Winter Conference on Applications of Computer Vision\n(WACV), pages 459\u2013468, 2020. 3\n"
  },
  {
    "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
    "link": "https://arxiv.org/pdf/2309.15129.pdf",
    "upvote": "5",
    "text": "Evaluating Cognitive Maps and Planning in Large\nLanguage Models with CogEval\nIda Momennejad\u2217\nMicrosoft Research\nNew York, NY\nidamo\nHosein Hasanbeig\u2217\nMicrosoft Research\nNew York, NY\nhosein.hasanbeig\nFelipe Vieira Frujeri\u2217\nMicrosoft\nRedmond, WA\nfelipe.frujeri\nHiteshi Sharma\nMicrosoft\nRedmond, WA\nhiteshi.sharma\nRobert Osazuwa Ness\nMicrosoft Research\nRedmond, WA\nrobertness\nNebojsa Jojic\nMicrosoft Research\nRedmond, WA\njojic\nHamid Palangi\nMicrosoft Research\nRedmond, WA\nhpalangi\nJonathan Larson\nMicrosoft Research\nRedmond, WA\njolarso\n@microsoft.com\nAbstract\nRecently an influx of studies claim emergent cognitive abilities in large language\nmodels (LLMs). Yet, most rely on anecdotes, overlook contamination of training\nsets, or lack systematic Evaluation involving multiple tasks, control conditions,\nmultiple iterations, and statistical robustness tests. Here we make two major\ncontributions. First, we propose CogEval, a cognitive science-inspired protocol for\nthe systematic evaluation of cognitive capacities in Large Language Models. The\nCogEval protocol can be followed for the evaluation of various abilities. Second,\nhere we follow CogEval to systematically evaluate cognitive maps and planning\nability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-\n175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B,\nand Alpaca-7B). We base our task prompts on human experiments, which offer\nboth established construct validity for evaluating planning, and are absent from\nLLM training sets. We find that, while LLMs show apparent competence in a few\nplanning tasks with simpler structures, systematic evaluation reveals striking failure\nmodes in planning tasks, including hallucinations of invalid trajectories and getting\ntrapped in loops. These findings do not support the idea of emergent out-of-the-box\nplanning ability in LLMs. This could be because LLMs do not understand the\nlatent relational structures underlying planning problems, known as cognitive maps,\nand fail at unrolling goal-directed trajectories based on the underlying structure.\nImplications for application and future directions are discussed.\n1\nIntroduction\nLarge language models (LLMs) are generatively pre-trained and display apparent competence on\nsome cognitive tasks [9]. This has led to a recent surge in studies claiming LLMs have emergent\nhuman-level cognitive abilities, which may encourage applications that interact with LLMs in a\nzero-shot or few-shot manner with expectations of human-level cognition. However, most claims\nof competence are based on anecdotes rather than systematic evaluation. In response, we make\ntwo contributions. First, we propose CogEval, a Cognitive Science-Inspired [14, 6, 47] protocol for\nMeasurement and Evaluation of cognitive abilities in LLMs (Figure 1, top), such as planning, theory\n\u2217Equal contribution\nPreprint. Under review.\narXiv:2309.15129v1  [cs.AI]  25 Sep 2023\nof mind, causal inference, or other abilities. Second, we apply this evaluation protocol to the domain\nof cognitive maps and planning, and systematically evaluate these capacities across eight LLMs. We\nbuild our task prompts according to established human experiments, but our goal is not a comparison\nwith human performance nor any assumptions of LLMs being \"human-like\" [34]. We evaluate LLMs\u2019\nfunctional as opposed to formal linguistic abilities [27], and by that we have both a functionalist and\nmultiple-realizability-based notion of cognitive ability [10] in mind.\nWe investigated whether LLMs (OpenAI GPT-4, GPT-3.5-175B, and davinci-003-175B, Google\nBard, Cohere-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B) understand the latent\nstructure of planning problems (cognitive maps). We hypothesized that failure in planning may relate\nto cognitive map deficits. To address these questions, we followed the CogEval protocol (Figure 1).\nFirst, we operationalized the latent ability (cognitive map and planning) in terms of multiple tasks\nwith variations in three factors: (a) the latent structure of the tasks\u2019 environment (different Markov\nDecision Processes (MDPs) or graphs), (b) the domain (spatial vs. social ties vs. object relations), and\n(c) multiple planning tasks for each latent graph structure (c.f. Methods for detail). These domains\nwere selected due to their prevalence in everyday problems as well as the cognitive science literature\non cognitive maps [5, 53, 43]. We then generated repeated measurements across small and large\nLLMs (c.f. Methods for choice of LLMs) and conducted statistical analysis to compare the results.\nWe found that LLMs only show apparent competence in simpler tasks, where route memorization\nwas sufficient to find a solution, but fail on closer systematic observation. Our evidence suggests\nagainst out-of-the-box emergent planning capacities in recently introduced LLMs.\nWhat is a cognitive map? A cognitive map is a representation of latent relational structures that\nunderlies a task or environment, and facilitates planning, reasoning, and inference in biological and\nartificial problems [55, 5, 33, 8]. The concept originated from Tolman\u2019s latent learning experiments,\ndemonstrating rodents\u2019 ability to learn maze structures without rewards [55]. This challenged\nthe dominant behaviorist view that learning only occurs with reinforcement; and paved the way\nfor a cognitive revolution. Decades later, discoveries of hippocampal place cells [39, 38, 40] and\nentorhinal cortex grid cells [15, 17, 36], together referred to as \"the brain\u2019s GPS,\" further substantiated\ncognitive maps and earned the 2014 Nobel Prize [1]. Cognitive maps have since been studied\nbehaviorally, computationally, and neurally; revealing that multi-step, multi-scale, and compressed\nneural representations are crucial for inference in both memory and planning [5, 33, 8]. Over the\npast decades, a number of Reinforcement Learning (RL) and deep neural network models have been\nproposed to capture the computations involved in cognitive maps and planning in the hippocampus\nand the prefrontal cortex of humans, rodents, bats, monkeys, and birds [5, 45, 8].\nWhy would LLMs plan with a cognitive map? It has been suggested that the transformer architecture\nand its learned representations, which lie at the heart of modern LLMs, are comparable to the\nhippocampus of the brain and the representations it learns [66]. Other studies show that GPT-3 is\ncapable of event segmentation of narrative transcripts similar to human evaluators [29], and evaluate\nsome cognitive capacities of GPT-3 using cognitive science and psychological methods applied in the\nevaluation of human cognition [6, 46, 57, 67]. Other cognitive scientists have distinguished formal\nlinguistic ability (e.g., the ability to form grammatically correct sentences) from functional cognitive\ncapacities (e.g., theory of mind, sequential planning, etc) and call for a meticulous evaluation of\nLLMs\u2019 functional competence without conflating them with their formal linguistic competence -\nmuch like the dissociation of language and thought [27]. Taken together, these studies raise the\nhypothesis that LLMs would be able to extract and use cognitive maps from text, and second, that\nLLMs\u2019 failure in capturing cognitive maps could underlie failure modes in planning.\nTo test these hypotheses, we designed prompts to measure behavioral signatures of extraction\nand use of cognitive maps in a set of tasks adapted from existing human behavioral experiments\n[32, 31, 35, 42, 44]. We operationalized cognitive maps and planning with a number of tasks\n(Figure 1) with variations in environmental structures or graphs, varying items from different domains\n(spatial, social, object relations), and across a number of different conditions (e.g., value-based\nplanning, reward and transition revaluation, shortcut, and detour).\nNotably, the corresponding human experiments that inspired our prompts were never in linguistic\nform, and this is the first adaptation of them to prompts to the best of our knowledge. This is an\nimportant consideration since contamination of the training data with the test set is one of the most\nchallenging obstacles to testing LLM capacities. To avoid potential contamination, we avoided\nBIG-bench [49], which has been flagged by OpenAI for contamination [2], and a planning benchmark\n2\nFigure 1: The CogEval protocol, Experiment 1 task structure, and example task prompt. (top)\nIn the CogEval protocol, a latent ability can be evaluated by first, being operationalized as tasks, and\nsecond, be measured multiple times and with variations and controls. We followed this protocol to\nevaluate cognitive map and planning. To robustly evaluate these abilities, multiple task prompts were\ngenerated with varying task structures (graph), the item domains (e.g., spatial or social), and task\nconditions (e.g., value-based path, detour). LLM responses were generated 30 times per task prompt\nand temperature for the three OpenAI models studied in this work and once per task and temperature\nfor other LLMs. The results were compared across task configurations, LLMs, and temperatures\nusing statistical analysis. (middle) The prompts\u2019 underlying task structures were six graphs based on\nhuman experiments. A: simple line graph from [32]. B: simple tree graphs based on [31]. C: graph A\nwith double depth and stochastic transitions. D, E, and F represent community graphs from [44], [35],\nand [42] respectively. (bottom) An example prompt for graph A. This procedure evaluates planning\nbehavior in value-based navigation (see Table 1). The colored transitions in the figure are for clarity,\nshowing different stages of the latent transition structure (cognitive map or graph).\nfor GPT-3 [58] as both pre-date GPT-4 and raise data contamination issues. Here we introduce and\ngenerate novel prompts inspired by human experiments with established validity in cognitive science.\nTo our knowledge, a systematic evaluation of planning and cognitive map capacities in GPT-4 and\ncomparison to other LLMs remain unexplored. In what follows we elaborate on a protocol and two\nrelated experiments to address this.\n3\n2\nMethods\nThe CogEval protocol. In order to evaluate cognitive-map-related planning and navigation in LLMs,\nwe propose and use the CogEval protocol (Figure 1). Please note that CogEval is not a benchmark\nnor limited to cognitive maps, it is a general protocol for evaluating any cognitive capacity, such as\nplanning, theory of mind, causal reasoning, etc. As an example, here we have applied it to the domain\nof cognitive maps and planning.\nCogEval adheres to four methodological best practices suggested by cognitive scientists [14]. First,\nthe latent construct or ability: here we evaluate cognitive maps, which are representations that capture\na model of the task structure, and adaptive planning, which requires an internal representation of\ntask structures (similar to model-based RL [51] or task-oriented model-free RL [18\u201321]). Second,\noperationalization with construct validity: we operationalize planning ability by generating unique\nvariations of established experimental tasks that measure the comprehension and use of cognitive\nmaps in multi-step planning behavior [32, 44, 31]. Third, multiple tasks and multiple response\ngenerations: we generated many tasks for each condition of interest varying graph structure, and\ndomain (spatial with ordered states such as room numbers, spatial with unordered states, social ties,\nobject relations). Most task conditions include a partial change in the environment to test adaptive\nplanning (e.g., changing the location of rewards or the structure of the environment, see Table 1).\nCollectively, these tasks allow us to robustly measure the latent construct: cognitive map and planning\nability. Fourth, including multiple task conditions allows us to control for multiple factors when\nmaking inference about the ability.\nThus, we evaluate the construct using multiple environments with different graph structures (based\non existing human experiments on cognitive maps [32, 44, 31], see graphs in Figure 1), controlling\nfor robustness to variations in graphs, task conditions, and item domains (e.g., rooms, people, objects,\nrandom letters), using multiple generations (30 generations per condition), and across different\ntemperatures (0, 0.5, and 1).\nLLMs evaluated. We compared the following LLMs: GPT-4-* [2], GPT-3.5-turbo-175B [41],\ntext-Davinci-3-175B [7] (Azure OpenAI API), Bard-* [54], Anthropic Claude-1-52B [4], LLaMA-\n13B [56], Cohere-52.4B [11], Alpaca-7B [52] (nat.dev API), where * means the number of parameters\nis undisclosed.\nExperiments. We conducted two incremental experiments. Experiment 1 systematically compares\nthe performance of all LLMs across different temperatures and conditions created with 3 factors of\ngraph structure, domain, and tasks. Experiment 2 evaluates whether simple Chain of Thought (CoT)\ninstructions can mitigate the observed failure modes in GPT-4.\n2.1\nExperiment 1: A cognitive science inspired evaluation of cognitive maps and planning\ncapacity in LLMs\nWe designed our experiment prioritizing robustness and control conditions. Model performance on\ncognitive tasks can be influenced by various factors beyond the primary cognitive capacity, such as\nthe specific prompts, the temperature parameter, experimental conditions (Table 1, Figure 1, bottom),\nthe specific items the task is presented with or domain (e.g., spatial connections vs. social ties),\nand the specific relational graph underlying the problem (e.g., this could be a graph structure such\nas line graphs, trees, community graphs with different size and density). For instance, perhaps an\nLLM performs better when the items in a task are rooms that are numbered in order in a line graph\n(item or domain effect), or when the graph structure is finite rather than a community graph with\npotential loops (graph effect). Thus, we implemented measures to mitigate such effects, like potential\nperformance variations due to task item selection or its underlying graph structure. We measured the\nresults for each combination of factors and parameters 30 times for OpenAI models (for which we\nhad API access) and once for the remaining models with no API access. We compared the results\nacross 10 LLMs.\nWhy vary temperature? Temperature in LLMs determines randomness in the generated response, by\nmanipulating the probabilities of the next word in a sequence. Thus, temperature can be thought of\nas a parameter controlling the diversity of the output. When temperature is set to 0, this results in\ndeterministic or greedy responses with less variance (Note: OpenAI has made it known that even a\ntemperature of 0 is not entirely deterministic, though this is as close as we can get). When temperature\n4\nis set higher, especially closer to 1, the LLM creates more diverse and varied text upon repetition,\nakin to exploration. While a higher temperature may be helpful for tasks that require varied responses\nor creativity, it could go either way for planning: on the one hand, precision in planning trajectories\nmay seem more in line with deterministic temperature, and on the other, a higher temperature leads\nto exploration, which may improve getting stuck in local minima and improving behavior. Thus,\nrepeating the experiments with varying temperature can help address its possible effect in either\ndirection.\nStatistical analysis. We evaluated the robustness of each LLM\u2019s performance by applying a statistical\nmodel of how each of the factors and their combinations contribute to variance in performance.\nSpecifically, we fit a logistic regression analysis with domain, condition, and graph types as categorical\nregressors, and include second and third-order interaction terms between these three terms. We make\nsure that each combination of domain, condition, and graph has several replicates, though the approach\nis robust to imbalance issues. We include model version and temperature as separate independent\nvariables that account for technical variation distinct from our conditions of interest.\nWe choose a logistic regression to model the number of items the LLM answers correctly in a given\ndialog out of a total number of possible correct answers. We aggregate the results into an analysis\nof deviance table (the generalized linear model equivalent of Analysis of Variance or ANOVA),\nwhich highlights the contributions of each factor and their interactions to performance, along with\nsignificance statistics. See supplement for full details on analysis and results.\n2.1.1\nExperiment 1: Example prompts\nNavigating cognitive maps requires adaptive multi-step planning using compressed representations of\nthe environment, not mere memorization of all routes. Thus, cognitive map experiments test flexible\nadaptivity to local changes in the environment to evaluate biological and reinforcement learning\nagents [32, 32, 33, 16]. Latent learning experiments found that rodents who explored a maze with\nno reward could quickly find the shortest route to a newly introduced reward, i.e., find an optimal\npolicy in RL context. This was taken as their ability to learn the cognitive maps of their mazes [55],\nbut various additional experimental conditions were then designed and evaluated to confirm that\nthey could flexibly adapt their cognitive map and planning to local environment alterations such as\nreward relocation (revaluation), changes to the map (transition revaluation) [32], or the introduction\nof shortcuts and detours [50]. Previous research has adapted these experiments to investigating the\nrobustness and flexibility of deep model-based RL in the face of local changes to the reward structure\n(LoCA), and shown that deep model-based RL agents such as Dreamer v2, muZero, and PlaNet failed\nat flexible planning in reward revaluation scenarios [61]. Here we operationalized our tasks inspired\nby similar conditions in human reinforcement learning and deep MBRL experiments on learning,\nupdating, and using cognitive maps for adaptive and flexible planning [32, 61].\nImportantly, the corresponding human experiments were never conducted using texts, but were\npresented either as videos or a sequence of images that human participants moved forward by\nchoosing an action (e.g. pressing left, right, up, or down). We believe this mitigates the risks\nof contamination. Moreover, when possible, we set the date earlier than our pilot studies to avoid\npotential contamination due to our experiments in the past month. To also ensure that the model cannot\ninfer any answers from the papers, we asked GPT-4 to explain the experimental paradigm and draw\nthe map of the environments after providing it a reference to a specific figure in a corresponding paper,\nand it failed. Thus, we believe our prompts have a negligible to no chance of having contaminated\nthe training sets.\nBelow we provide examples of task prompts for graph A and a spatial domain (number ordered rooms).\nAll prompts are available in the supplementary material and on https://tinyurl.com/cogmaps-in-llm.\nI. Value-based or goal-driven planning. Below is an example prompt for value-driven or goal-\ndirected planning in graph A in Figure 1. Success requires an understanding of the start and goal\npositions, comparison of the paths to find the shortest path that leads to the highest rewards, and\nplanning a multi-step navigation or traversal of the underlying graph structure of the task.\n5\nImagine a world with six rooms. From the lobby you have two choices, room 1 and room 2.\nYou enter room 1, at the end there\u2019s a door that leads to room 3, and room 3 leads to room 5.\nThere\u2019s a chest in room 5. You open it and there\u2019s 10 dollars. Then you exit and start over.\nThis time in the lobby you choose room 2, then enter room 4, which leads to room 6. There\u2019s\na chest with 50 dollars. You return to the lobby. Which room will you choose to make the\nmost money?\nII. Transition Revaluation, after prompt I. This condition occurs when the structure of the\nenvironment (e.g., an edge of the graph or Markov decision process) locally changes, and planning\nrequires integrating or \u2018piecing together\u2019 different parts of the cognitive map to update one\u2019s plan or\npolicy.\nNow you\u2019re dropped in room 3 and the door at its end suddenly leads to room 6, and then\nyou\u2019re dropped in room 4 and the door at its end suddenly leads to room 5. you return to the\nlobby. Which room will lead to more rewards?\nIII. Reward Revaluation, after prompt I. A common local change in any environment is when the\nlocation of rewards or goals change, without any changes to the map or structure of the states (or the\ncognitive map). This is known as Reward Revaluation or retrospective revaluation of rewards [32].\nNow you\u2019re dropped into room 3, then you enter room 5 and the chest has 100 dollars. Then\nyou\u2019re taken out, and dropped into room 4, then you enter room 6 and the chest has the same\namount as before. When you return to the lobby, which room do you choose to make the most\nreward?\nV. Shortcut prompts with and without teleportation, after prompt I. Tolman\u2019s experiments on\ncognitive maps [55] included a condition evaluating the animal\u2019s ability to discover shortcuts. Since\nthe early 1990s, evaluating the ability of various Dyna architectures [51] in discovering shortcuts has\nbeen an important part of evaluating planning behavior. Below are two different shortcut prompts.\nIn the lobby you\u2019re presented with a portal, and you can choose which room to teleport into.\nWhich room do you choose to maximize rewards?\nIn the lobby you\u2019re presented with a new door which leads to a new room, room 7. Room\n7\u2019s door leads directly to room 6. Remember that you will only be able to choose one path\nthat leads to the most money. Which room from the lobby will lead to the path where one can\nmake the most money?\nV. Detour prompts with and without Teleportation, after prompt I.\nYou enter the lobby and this time you encounter a new room, room 7. Room 7\u2019s door leads to\nroom 8, and room 8 leads to room 9. From room 9 you can teleport anywhere. You return to\nthe lobby, and choose the room that leads to the most reward, but the door to the next room is\nblocked. You go back to the lobby. Which room do you choose to reach the most rewards?\nYou enter the lobby and this time you encounter a new room, room 7. Room 7\u2019s door leads to\nroom 8, and room 8 leads to room 6. When you return to the lobby and choose the previous\npath that led to the most reward, you discover that the regular door to the room with the most\nmoney is now blocked. You go back to the lobby. You will only be able to choose one path\nthat leads to the most money. Which room from the lobby will lead to the path where one can\nmake the most money?\nVI. Drawing a map.\n6\nPlease draw a text-based map of the environment.\nTable 1: Brief descriptions of the task conditions applied to varying graphs and domains\nCondition\nDescription\nGroup\nvaluePath\nThe optimal solution is to find the optimal policy, or shortest path, which yields the highest reward\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\nTraversal\n1stepPath\nThe optimal solution is a 1-hop policy, i.e., goal is adjacent to the starting state\n2stepPath\nThe optimal solution is a 2-step policy\n3stepPath\nThe optimal solution is a 3-step policy\nnstepPath\nThe optimal solution is an n-step policy, where max n is the diameter of the graph (longest shortest path)\nrewardReval\nUpon a local change in the reward structure, the goal has changed and the optimal solution requires finding a new path\n\u001b\nRewReval\npolicyReval\nUpon a local change in the reward structure, the optimal solution requires finding a new policy\ntransReval\nUpon a local change in the transition structure, the goal is the same but the optimal solution requires finding a new policy \u001b\nTransReval\ntransRevalStochastic\nUpon a local change in the transition structure, the goal is the same but the optimal solution requires finding a new policy\nin a stochastic environment\nnonteleShortcut\nUpon a change in the graph structure, the optimal solution requires finding a shortcut\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\nShortcut\nnonteleShortcutCoT\nUpon a change in the graph structure, the optimal solution requires finding a shortcut, an additional CoT prompt is given\nteleShortcut\nUpon a local change in the transition structure, the optimal solution requires finding a shortcut using a teleportation portal\nteleShortcutCoT\nUpon a local change in the graph or transition structure, the optimal solution requires finding a shortcut using\na teleportation portal, an additional CoT prompt is given\nnonteleDetour\nUpon a change in the graph structure, the optimal solution requires finding a detour\n\u001b\nDetour\nteleDetour\nUpon a local change in the transition structure, the optimal solution requires finding a detour using a teleportation step\nTable 2: Step-wise contribution of adding each factor to the logistic regression fit of LLM model\nperformance (number of successes out of max possible successes in dialog).\nterm\nChi-squared Stat (Deviance)\ndf\np value\n1\nLLM\n2357.87\n7\n<0.001\n2\ngraph\n3431.53\n5\n<0.001\n3\ndomain\n458.74\n2\n<0.001\n4\ntemperature\n1.28\n2\n0.53\n5\ncondition\n2080.04\n4\n<0.001\n6\nLLM and temperature\n10.69\n14\n0.71\n7\ngraph and domain\n334.41\n10\n<0.001\n8\ngraph and condition\n1651.33\n20\n<0.001\n9\ndomain and condition\n310.53\n8\n<0.001\n10\ngraph, domain, condition\n1133.16\n44\n<0.001\n2.2\nExperiment 2: Evaluating the effect of Chain of Though (CoT) instructed prompts\nLLM evaluation is usually performed within the in-context learning framework [3, 30], where\nthe input to the LLM is the text of the problem to be solved, preceded with several examples of\nrelated problems and their solutions, possibly worked out step-by-step. The rationale is that a\nsingle problem may be ambiguously stated (as far as the LLM is concerned), and a few examples,\npossibly with explanations, may be all that is needed to disambiguate the intent. However, the\nchoice and even the order of examples impacts the performance [26], as does the incorporation of\nauxiliary knowledge, [48, 68, 37], particularly in the form of Chain-of-Thought (CoT) reasoning\n[65, 63, 69, 12, 62, 25, 23, 24].\nWhile CoT prompting is not a rigorously defined concept, a prompt with a small number of worked\nout examples, serving as an instance of few-shot learning, may qualify as a CoT prompt and has\nbeen shown to improve performance considerably on cognitive tasks (e.g., Theory of Mind [30]).\nHowever, such a prompt can be so regimented that they effectively turn an LLM into a Turing Machine\nexecuting a given algorithm the way a computer would [22].In this view, careful CoT prompts could\nhave a significant effect both on the performance and on our interpretation of how it is achieved.\nHere wetried breadth first and depth first instructions as follows:\n2.3\nBFS (Breadth First Search) instruction:\n\u201cThink carefully before you respond. You can try using Breadth-first search (BFS), it is a graph\ntraversal algorithm that visits all the vertices of a graph in breadth-first order, starting from a given\nsource vertex. In BFS, vertices are visited in layers, where the vertices at distance 1 from the source\n7\nFigure 2: Experiment 1 results. (top) Mean and standard error of performance on all tasks for each\nof the different graphs (see Figure 1 for graph details) across different LLMs studied in this work.\n(bottom) Mean performance compared across per main task category (see Table 3 for details).\nvertex are visited first, followed by the vertices at distance 2, and so on. BFS uses a queue data\nstructure to keep track of the vertices to be visited, and it ensures that no vertex is visited more than\nonce. BFS is useful for finding the shortest path between two vertices in an unweighted graph, or for\nexploring all the vertices in a graph.\u201d\n8\nTable 3: Mean and standard errors for planning performance across all task conditions in all 10\nLLMs. ARI scores closer to zero represent poor performance by the LLM and ARI scores reaching\n1.0 represent performance matching Leiden.\ngpt-4-32k\ngpt-35\ndavinci-003\nclaude-v1\npythia-20b\ncohere\nllama-13b\nalpaca-7b\nbard\nCondition\n1stepPath\n0.99, 0.08\n0.76, 0.32\n0.52, 0.45\n0.57, 0.37\n0.64, 0.41\n0.27, 0.42\n0.23, 0.38\n0.27, 0.41\n0.05, 0.10\n2stepPath\n0.82, 0.35\n0.73, 0.38\n0.16, 0.25\n0.61, 0.41\n0.67, 0.42\n0.29, 0.44\n0.22, 0.37\n0.35, 0.47\n0.25, 0.50\n3stepPath\n0.55, 0.38\n0.37, 0.37\n0.58, 0.43\n0.27, 0.31\n0.35, 0.49\n0.04, 0.11\n0.04, 0.07\n0.06, 0.20\n0.11, 0.10\nnonteleDetour\n0.55, 0.39\n0.51, 0.35\n0.55, 0.43\n0.50, 0.41\n0.51, 0.37\n0.21, 0.35\n0.19, 0.33\n0.26, 0.38\n0.29, 0.48\nnonteleShortcut\n0.56, 0.40\n0.52, 0.39\n0.49, 0.40\n0.62, 0.43\n0.40, 0.36\n0.16, 0.27\n0.11, 0.18\n0.20, 0.30\n0.29, 0.48\nnonteleShortcutCoT\n1.00, 0.00\n1.00, 0.00\n0.09, 0.07\n0.58, 0.38\n0.36, 0.38\n0.37, 0.49\n0.17, 0.29\n0.37, 0.37\n-\nnstepPath\n0.47, 0.38\n0.31, 0.34\n0.17, 0.27\n0.33, 0.37\n0.27, 0.42\n0.05, 0.11\n0.06, 0.08\n0.12, 0.32\n0.00, 0.00\npolicyReval\n0.21, 0.18\n0.18, 0.23\n0.13, 0.04\n0.28, 0.30\n0.00, 0.00\n0.00, 0.00\n0.04, 0.07\n0.05, 0.22\n0.00, 0.00\nrewardReval\n0.67, 0.40\n0.57, 0.36\n0.34, 0.25\n0.48, 0.35\n0.60, 0.45\n0.31, 0.44\n0.28, 0.43\n0.33, 0.44\n0.14, 0.14\nteleDetour\n0.47, 0.35\n0.34, 0.30\n0.53, 0.44\n0.37, 0.33\n0.44, 0.41\n0.21, 0.35\n0.23, 0.37\n0.23, 0.38\n0.29, 0.48\nteleShortcut\n0.54, 0.39\n0.35, 0.33\n0.44, 0.41\n0.45, 0.39\n0.27, 0.33\n0.16, 0.27\n0.16, 0.22\n0.12, 0.24\n0.29, 0.48\nteleShortcutCoT\n0.50, 0.00\n0.50, 0.00\n0.04, 0.01\n0.50, 0.50\n0.39, 0.36\n0.19, 0.40\n0.83, 0.29\n0.35, 0.36\n-\ntransReval\n0.60, 0.42\n0.59, 0.40\n0.49, 0.38\n0.55, 0.36\n0.47, 0.42\n0.19, 0.28\n0.22, 0.33\n0.27, 0.37\n0.08, 0.17\ntransRevalStochastic\n0.73, 0.36\n0.52, 0.36\n0.91, 0.24\n0.78, 0.34\n0.36, 0.32\n0.00, 0.00\n0.11, 0.19\n0.22, 0.39\n-\nvaluePath\n0.58, 0.41\n0.66, 0.40\n0.66, 0.39\n0.44, 0.41\n0.49, 0.46\n0.31, 0.40\n0.27, 0.39\n0.33, 0.45\n0.29, 0.48\nOriginal +BFS\n+DFS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nCluster Count\nTemperature=0\nTemperature=0.5\nTemperature=1\nOriginal +BFS\n+DFS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nShortest Path\nTemperature=0\nTemperature=0.5\nTemperature=1\nOriginal +BFS\n+DFS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nShortest Path\n 1 Cluster Away\nTemperature=0\nTemperature=0.5\nTemperature=1\nFigure 3: Experiment 2 results. (Bottom) BFS and DFS instructions marginally enhance perfor-\nmance on community graphs. In the Cluster counting task (graph D) adding BFS or DFS is beneficial\nat temperatures 0 and 0.5 but less at 1. For finding shortest paths within a cluster, BFS or DFS help\nwith BFS being effective at temperature 0. However, for finding the shortest path 1-cluster away, only\nBFS at temperature 0.5 yields slight improvements.\n2.4\nDFS (Depth First Search) instruction:\n\u201cThink carefully before you respond. You can try using Depth-first search (DFS), it is a graph traversal\nalgorithm that visits all the vertices of a graph in depth-first order, starting from a given source vertex.\nIn DFS, the algorithm traverses as far as possible along each branch before backtracking. DFS uses a\nstack data structure to keep track of the vertices to be visited, and it ensures that all vertices connected\nto a visited vertex are explored before backtracking. DFS is useful for finding cycles in a graph, for\nexploring all the vertices in a graph, or for finding a path between two vertices. However, unlike BFS,\nDFS does not guarantee that the shortest path is found.\u201d\nWe explored how the simple instructions impact LLM performance for different temperatures to\ninvestigate if the effectiveness of a given prompt can be impacted by the level of uncertainty caused\nby the temperature parameter. We find that while in some cases the performance improves, the effects\nare not consistent nor monotonic. This is an interesting phenomenon that needs further investigation\nto be better understood.\n9\n3\nResults\n3.1\nExperiment 1: Repeated measures comparison of planning across LLMs\nWe evaluated out-of-the-box emergent or native ability of different LLMs on the cognitive map\ntasks. Table 2 shows the statistical analysis highlighting the contributions of each factor to a logistic\nregression model\u2019s fit of LLM model performance. The magnitude of chi-square test statistics indicate\ncontribution to overall model fit. Figure 2 compares the performance of all LLMs across all latent\ngraph structures. Table 3 shows mean and standard error for planning performance across tasks and\nLLMs.\nThe results in Table 2 indicate that the LLM (\u03c72(11) = 2357.87, p < .001), graph (\u03c72(11) = 3431.53,\np < .001), condition (\u03c72(11) = 2080.04, p < .001), and domain (\u03c72(11) = 304.06, p < .001) each\nyielded significant chi-squared statistics. This means that not only did different LLMs performed\ndifferently, but performance varied as a result of varying graphs, domains, and conditions. Conversely,\nthe temperature showed a non-significant chi-squared statistic (\u03c72(11) = 1.28, p = .53) and the\ninteraction between the LLM and temperature was also non-significant (\u03c72(11) = 10.69, p =\n.71). Noteworthy, the interactions among graph-domain, graph-condition, domain-condition, and\ngraph-domain-condition were all significant (all p\u2019s < .001). The interactions among graph-domain\n(\u03c72(11) = 334.41, p < .001), graph-condition (\u03c72(50) = 1651.33, p < .001), domain-condition\n(\u03c72(39) = 310.53, p < .001), and graph-domain-condition (\u03c72(108) = 1133.16, p < .001) were all\nsignificant. A full table of regression coefficient estimates is included in the supplement.\nIn summary, while the \u2019temperature\u2019 and the interaction of \u2019LLM\u2019 and \u2019temperature\u2019 do not show\nsignificant effects in Experiment 1 (but show difference in Experiments 2), all other factors and\ntheir interactions significantly contribute to the variations in the dependent variable. Considering\nboth individual and interactive effects, this finding shows that LLM performance on cognitive map\nand planning tasks was not robust to the graph structure of the problems, the domain, or the task\nconditions, and it also varied across models (see Tables 2 and 3 and Figure 2).\n3.2\nExperiment 2: The effect of Chain of Thought (CoT) Instructions\nWe explored the impact of instructing GPT-4 with graph traversal methods\u2014Breadth First Search\n(BFS) and Depth First Search (DFS). Even though explained at a high level, they resulted in\nperformance improvement on several experiments with complex community graphs. For the Cluster\ncounting task on community graph D in Figure 1, adding BFS or DFS improved results at temperatures\n0 and 0.5, but less so at 1. Finding shortest paths within a cluster was improved across all temperatures\nwhen using BFS or DFS, with BFS being most effective at temperature 0. However, for finding the\nshortest path 1-cluster away, only BFS at temperature 0.5 improved performance. Each experiment\nwas repeated 30 times per prompt and temperature (Figure 3).\n3.3\nFailure modes\nWe note three main failure modes when the task had an underlying graph with a dense com-\nmunity structure.\nNotably, when we probe the LLMs to list connected rooms or items as\n(state, actions, state) tuples, they do well (e.g., (room1, opendoor, room3) is a tuple for graph\nA in Figure 1). However, when asked to do any tasks with a community graph structure using this\ntuple knowledge, LLMs display the following failure modes; (1) hallucinate edges that do not exist,\nor (2) fall into longer trajectories instead of shortest paths, or (3) get trapped in loops. For example\nin the task of finding the shortest path to a state that is 1 cluster away, out of 30 runs GPT-4 has a\nsuccess rate of 0 at temperature 0. Even with changing the temperature to 0.5 or 1 and repeating the\nsame 30 runs its success rate can not exceed 10%. Please refer to Figure 4 for examples of above\nfailure modes.\n4\nDiscussion and future directions\nThis paper makes two main contributions. First, we introduce CogEval, a cognitive science inspired\nprotocol for systematic and robust evaluation of functional [27] cognitive abilities in LLMs. Second,\nwe follow the CogEval protocol to evaluate multiple LLMs\u2019 native or emergent ability to extract\n10\nFigure 4: Examples of three failure modes. (left) Edge hallucination. (middle) Failure at finding a\n1-step policy within the same cluster. (right) Failure at multi-hop path by both getting trapped in a\nloop and hallucinating edges. In each example the blue box is the task prompt, the grey box shows\nthe model response, and the green arrows demonstrate the correct response on the graph.\ncognitive maps for sequential planning, navigation, or graph inference. All tasks and prompts are\nbased on non-linguistic human cognitive science experiments that we adapted into text prompts for\nthe first time. We test for robustness of the findings by varying task conditions, graph structure,\ndomains (spatial, social, and object-relational), and LLM temperature. Our systematic evaluation\nreveals that while LLMs display apparent competence on some tasks in smaller graphs, they do not\nhave out-of-the-box emergent cognitive map comprehension or planning competence.\nMethodological contribution. We provide a cognitive-science inspired protocol [14] for systematic\nand careful evaluation of LLMs, CogEval, as follows. (1) We avoid the reuse of contaminated\nstandardized benchmarks by creating novel prompts based on non-text-based experiments that are\nknown to evaluate cognitive maps and planning in humans, animals, and RL. (2) We use multiple tasks\nto probe the cognitive constructs (cognitive maps and planning) and repeat each interaction multiple\ntimes and across different temperatures. (3) We use statistical analysis to evaluate the robustness and\nreliability of each effect, with three main factors of graph structure, item domain (spatial vs. social vs.\nobject relations), and task condition (e.g., value-based decision making, shortcut, detour, see Table 1).\n(4) We employ chain of thought and instruction prompts to evaluate the limits of out-of-the-box\ncognitive abilities of LLMs and (5) analyze different types of failure modes. Please note that CogEval\nis not a benchmark nor limited to evaluating cognitive maps and planning, it is a general protocol for\nevaluating any cognitive capacity in LLMs. As an example, here we have applied it to the domain of\ncognitive maps and planning.\nNo evidence for understanding cognitive maps or planning. Our systematic and incremental evalu-\nations reveal limited to no cognitive map capacities in the current generation of LLMs - including\nGPT-4. Specifically, we find that LLMs only show apparent competence on simple sequential infer-\nence tasks where route memorization can help, and given LLMs have received all possible trajectories\nin the text prompt. We also observe that the sparsity of graph structure drove performance. However,\nwhen 1-step and multi-step traversal and planning require understanding the underlying relational\nstructure of the environment graph, LLMs including GPT-4 fail by hallucinations, suboptimally long\nroutes, or falling in loops.\nHow did LLMs solve the simpler tasks? Without access to full architectures or training sets, we can\nonly form hypotheses based on our behavioral observations. We observe that LLMs do better in\nproblems where the entire trajectories are explicitly available in the text prompts, and they only need\nto retrieve and piece together partial changes. Planning behavior in larger graphs is far worse than the\nsmaller ones, and this is not just due to graph size: LLMs often perform worse on the graph with 15\nnodes and 3 dense clusters compared to the 16-node (4-cluster) graph that has more nodes, but better\ncross-cluster connectivity. The difference is that there are fewer paths among clusters in the 15-node\ngraph.\nThese observations suggest that LLMs may fail at planning problems where they need to use the\ntransition structure to unroll the trajectories and find the correct path, which is closer to the notion\n11\nof planning in model-based RL and in cognitive science. Capturing the underlying structure and\nusing it to unroll trajectories are quintessential to cognitive maps and planning ability. Thus, the\napparent competence in simpler tasks may be due to using cached or memorized routes rather than\nunderstanding the cognitive map, planning, or inference ability.\nLLMs may do better in smaller graphs because the prompt already expands all the possible paths or\ntrajectories. When there is a change in the rewards or transition structure, the LLM only needs to\nchange one step in an already laid out path. However, in more clustered graphs only the one-step\nconnections are laid out in the prompt, but not all paths or trajectories between any given two nodes.\nWe observed that failures significantly increase in tasks with these larger graphs and a community\nstructure, even though LLMs can list the pairwise tuples of connected states (see failure modes,\nFigure 4), especially for paths that LLMs could not explicitly memorize by reading the prompts.\nInterpreting the results. The incremental experiments in the paper, notably, are not meant to be\ninterpreted as a benchmark for planning. They probe the same construct in different ways, evaluating\nthe ability to use information about (state, actions, state) tuples, e.g., (room1, opendoor, room3)\nto piece together policies in response to task prompts. A reader may wonder why we claim that\nLLMs do not display emergent planning in spite of high performance for some tasks in experiment 1\n(Figure 2. We interpret the findings against emergent planning or understanding of cognitive maps in\nLLMs due to various inconsistencies in success and failure cases (Figure 4). For instance, a common\nfailure mode is generating sequences with hallucinated (state, actions, state) tuples that do not exist.\nAnother common failure mode is that they fall into loops when prompted to find the shortest path\nbetween two states (Figure 4, left and right). Moreover, LLMs can even fail to identify 1-step paths\nand sometimes suggest multi-hop trajectories for traversing to an adjacent state (Figure 4, middle).\nThese observations stand in contrast to LLMs\u2019 ability to generate a list of tuples based on the text\nprompt. It shows that, while LLMs appear to solve planning problems with simple routes that can be\nexplicitly memorized, they have not emerged the ability to generalize from route memory solutions\nto using the tuples to adaptively generate routes. Together, these inconsistent observations are in line\nwith the hypothesis that LLMs do not understand cognitive maps and therefore cannot consistently\nplan. We acknowledge that they can be augmented with various tricks to improve their planning, but\nthese findings point to the absence of out-of-the-box planning ability.\nLimitations. First, we lack knowledge of LLMs like GPT-4\u2019s architecture or training. Thus, we did\nnot use existing text-based benchmarks that could be in the training data, and instead generated novel\nprompts not in their training sets. Second, in the human experiments that influenced our prompts,\nparticipants learn gradually, experiencing states one-by-one, but were only tested after they showed\nsigns of learning, similar to a model-based RL agent having the transition structure and using it\nfor inference and planning. To address this difference, we present the environment\u2019s structure in\nlinguistic format. In all cases, the participant or model had to identify the goal location based on\ninstructions and infer the policy towards the goal, which is the room with the maximum reward. Thus,\nwe believe that our approach sufficiently addresses these differences.\nImplication for applications. LLMs are expected to be applied in fields like gaming, planning, and\nsocial reasoning, with tasks that require understanding the inherent relational structure of the problem\nfrom the input for flexible reasoning and planning. However, here we show various failure modes\nin the understanding of the underlying cognitive maps or planning abilities, including hallucination\nand falling in loops. Even when provided instructions and Chain of Thought (CoT) prompts like\nbreadth-first search (BFS), we observe that GPT-4 struggles to process multi-hop paths it has not\nexperienced, which it needs to infer from the task\u2019s underlying graph. These findings suggest caution\nin the application of LLMs in problems that involve planning or complex structures. However,\naugmenting the LLMs and CoT design may mitigate these challenges for problems with simpler\nstructures.\nLLMs as programmable machines rather than emergent intelligence? While some prefer to regard\nLLMs as agents with emergent intelligence comparable to humans and animals, our results reveal no\nemergent cognitive map or planning capacities. These findings are more consistent with the view that\nLLMs are programmable machines with natural language as their programming language [22]. This\nis why here we evaluated planning in LLMs in a functionalist and multiple-realizability sense rather\nthan requiring any assumptions of them being \"human-like\" [34].\n12\nAnother implication of this view regards the role of scale in future directions. Scale might have\nbeen a computationally costly and carbon-expensive shortcut to certain capacities and skills in the\nabsence of an architecture with energy- or efficiency- constraints. However, smaller models with\nwell-thought-out architecture, augmentation, and energy constraints could potentially achieve the\nsame skills. This will especially be more achievable in smaller models with specialized domain\ntraining. While more hypothesis-driven and brain-inspired architectures would take more time and\ndomain knowledge, they may lead to more ecologically friendly AI with efficiency constraints and\nadaptive skills.\nFuture directions. A future direction is to analyze representational similarities in the embeddings\nand test hypotheses about representations underlying success and failure modes. This mirrors how\nneuroscience analyzes neural data to understand representations in model-based and predictive\nplanning and decision-making [32, 8]. Moreover, we observed that while LLMs struggled with\nplanning, some could list pairwise tuples or recognize the item that was associated with the goal.\nThus, an interesting direction is to study the limits of LLMs\u2019 transitive inference using pair-wise\nassociations [45, 42]. Another direction is to study whether the use of schemas, i.e., overused,\ngeneralized cognitive maps such as \"airport\" [59, 28, 13, 60], can improve performance on real-world\nscenarios, given LLMs can apply analogical reasoning to solve problems [64]. Finally, some have\nsuggested ways to improve planning by augmenting LLMs with algorithms that enable executive\ncontrol, an interesting direction that can contribute to the future of augmenting both larger and\nespecially smaller language models.\nLLMs need a hippocampus and prefrontal cortex. Practical applications may benefit from adding\nmemory, planning, and executive control augmentations to LLMs. The failure modes we observed in\ndense community graphs included hallucinating edges, inability to find shortest paths, and falling in\nloops. It is possible that they can be mitigated with careful augmentations for memory and planning\nthat, similarly to the role of the hippocampus and prefrontal cortex in the brain, can extract the\nrelational structure from sequential data and flexibly reflect [47] to plan at multiple scales [8, 33].\nSummary. We\u2019ve made two contributions. We introduce CogEval, a cognitive science inspired\nprotocol for systematic and robust evaluation of LLMs. Following CogEval, we show that LLMs do\nnot have emergent planning capacities, possibly because LLMs do not understand cognitive maps:\nthe relational structures underlying planning problems.\n5\nAcknowledgement\nWe are extremely grateful to Hiteshi Sharma for providing further analysis after submission, and to\nPeter Lee, Michael Frank, John Krakauer, Joshua Tenenbaum, Paul Bennett, Alison Gopnik, and\nMelanie Mitchell for early feedback on methods and results. We would also like to acknowledge\nDerek Worthen and Richard Ciapala for engineering support.\n6\nSupplementary Material\nThe CogEval protocol as well as all the conversations generated in this paper are available at\nhttps://tinyurl.com/cogmaps-in-llmin json format.\n13\nReferences\n[1] The nobel prize in physiology or medicine 2014. https://www.nobelprize.org/prizes/\nmedicine/2014/press-release/. Accessed: 2023-5-10.\n[2] OpenAI 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[3] Ekin Akyrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning\nalgorithm is in-context learning? investigations with linear models. November 2022.\n[4] Anthropic.\nIntroducing\nClaude.\nhttps://www.anthropic.com/index/\nintroducing-claude, 2023. [Online].\n[5] Timothy E J Behrens, Timothy H Muller, James C R Whittington, Shirley Mark, Alon B\nBaram, Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map? organizing\nknowledge for flexible behavior. Neuron, 100(2):490\u2013509, October 2018.\n[6] Marcel Binz and Eric Schulz. Using cognitive psychology to understand GPT-3. Proceedings\nof the National Academy of Sciences, 120(6):e2218523120, 2023.\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[8] Iva K Brunec and Ida Momennejad. Predictive representations in hippocampal and prefrontal\nhierarchies. J. Neurosci., November 2021.\n[9] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith GPT-4. March 2023.\n[10] Rosa Cao. Multiple realizability and the spirit of functionalism. Synthese, 200(6):506, December\n2022.\n[11] Cohere.\nIntroducing\nCohere.\nhttps://txt.cohere.com/\ncohere-launches-extremely-large-beta-2, 2022. [Online].\n[12] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\nlanguage models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.\n[13] Delaram Farzanfar, Hugo J Spiers, Morris Moscovitch, and R Shayna Rosenbaum. From\ncognitive maps to spatial schemas. Nat. Rev. Neurosci., 24(2):63\u201379, February 2023.\n[14] Michael C Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke,\nRobert Hawkins, Maya Mathur, and Rondeline Williams.\nExperimentology.\nhttps://\nexperimentology.io/, 2023. Accessed: 2023-5-9.\n[15] Marianne Fyhn, Sturla Molden, Menno P Witter, Edvard I Moser, and May-Britt Moser. Spatial\nrepresentation in the entorhinal cortex. Science, 305(5688):1258\u20131264, August 2004.\n[16] Mona M Garvert, Raymond J Dolan, and Timothy Ej Behrens. A map of abstract relational\nknowledge in the human hippocampal-entorhinal cortex. Elife, 6, 2017.\n[17] Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I Moser. Mi-\ncrostructure of a spatial map in the entorhinal cortex. Nature, 436(7052):801\u2013806, August\n2005.\n[18] Hosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Logically-constrained neural fitted\nQ-iteration. In Proceedings of the 18th International Conference on Autonomous Agents and\nMultiAgent Systems, pages 2012\u20132014. International Foundation for Autonomous Agents and\nMultiagent Systems, 2019.\n14\n[19] Hosein Hasanbeig, Yiannis Kantaros, Alessandro Abate, Daniel Kroening, George J Pappas,\nand Insup Lee. Reinforcement learning for temporal logic control synthesis with probabilistic\nsatisfaction guarantees. In Proceedings of the 58th Conference on Decision and Control, pages\n5338\u20135343. IEEE, 2019.\n[20] Hosein Hasanbeig, Daniel Kroening, and Alessandro Abate. Certified reinforcement learning\nwith logic guidance. Artificial Intelligence, page 103949, 2023.\n[21] Hosein Hasanbeig, Natasha Yogananda Jeppu, Alessandro Abate, Tom Melham, and Daniel\nKroening. Symbolic task inference in deep reinforcement learning. Journal of Artificial\nIntelligence Research (JAIR), 2023.\n[22] Ana Jojic, Zhen Wang, and Nebojsa Jojic. GPT is becoming a turing machine: Here are some\nways to program it. March 2023.\n[23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n[24] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.\nOn the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336,\n2022.\n[25] Zihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai Prabhumoye, Wei Ping, Mohammad\nShoeybi, and Bryan Catanzaro. Multi-stage prompting for knowledgeable dialogue generation.\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 1317\u20131337,\nDublin, Ireland, May 2022. Association for Computational Linguistics.\n[26] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically\nordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8086\u20138098, 2022.\n[27] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and\nEvelina Fedorenko. Dissociating language and thought in large language models: a cognitive\nperspective. January 2023.\n[28] Rolando Mas\u00eds-Obando, Kenneth A Norman, and Christopher Baldassano. Schema representa-\ntions in distinct brain networks support narrative memory during encoding and retrieval. Elife,\n11, April 2022.\n[29] Sebastian Michelmann, Manoj Kumar, Kenneth A Norman, and Mariya Toneva. Large language\nmodels can segment narrative events similarly to humans. ArXiv, January 2023.\n[30] Shima Rahimi Moghaddam and Christopher J Honey. Boosting Theory-of-Mind performance\nin large language models via prompting. April 2023.\n[31] I Momennejad, A R Otto, N D Daw, and K A Norman. Offline replay supports planning in\nhuman reinforcement learning. Elife, 2018.\n[32] I Momennejad, E M Russek, J H Cheong, M M Botvinick, N D Daw, and S J Gershman. The\nsuccessor representation in human reinforcement learning. Nat Hum Behav, 1(9):680\u2013692,\nSeptember 2017.\n[33] Ida Momennejad. Learning structures: Predictive representations, replay, and generalization.\nCurrent Opinion in Behavioral Sciences, 32:155\u2013166, April 2020.\n[34] Ida Momennejad. A rubric for human-like agents and NeuroAI. Philos. Trans. R. Soc. Lond. B\nBiol. Sci., 378(1869):20210446, December 2022.\n[35] Ida Momennejad, Ajua Duker, and Alin Coman. Bridge ties bind collective memories. Nat.\nCommun., 10(1):1578, April 2019.\n[36] Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the brain\u2019s\nspatial representation system. Annu. Rev. Neurosci., 31:69\u201389, 2008.\n15\n[37] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show\nyour work: Scratchpads for intermediate computation with language models. arXiv preprint\narXiv:2112.00114, 2021.\n[38] J O\u2019Keefe. Place units in the hippocampus of the freely moving rat. Exp. Neurol., 51(1):78\u2013109,\nApril 1976.\n[39] J O\u2019Keefe and J Dostrovsky. The hippocampus as a spatial map. preliminary evidence from\nunit activity in the freely-moving rat. Brain Res., 34(1):171\u2013175, November 1971.\n[40] John O\u2019Keefe and Lynn Nadel. The Hippocampus as a Cognitive Map. Oxford: Clarendon\nPress, 1978.\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[42] Athula Pudhiyidath, Neal W Morton, Rodrigo Viveros Duran, Anna C Schapiro, Ida Momenne-\njad, Demitrius M Hinojosa-Rowland, Robert J Molitor, and Alison R Preston. Representations\nof temporal community structure in hippocampus and precuneus predict inductive reasoning\ndecisions. J. Cogn. Neurosci., 34(10):1736\u20131760, September 2022.\n[43] Matthew Schafer and Daniela Schiller. Navigating social space. Neuron, 100(2):476\u2013489,\nOctober 2018.\n[44] Anna C Schapiro, Timothy T Rogers, Natalia I Cordova, Nicholas B Turk-Browne, and\nMatthew M Botvinick. Neural representations of events arise from temporal community\nstructure. Nat. Neurosci., 16(4):486\u2013492, April 2013.\n[45] Anna C Schapiro, Nicholas B Turk-Browne, Matthew M Botvinick, and Kenneth A Norman.\nComplementary learning systems within the hippocampus: a neural network modelling approach\nto reconciling episodic memory with statistical learning. Philos. Trans. R. Soc. Lond. B Biol.\nSci., 372(1711), 2017.\n[46] Richard Shiffrin and Melanie Mitchell. Probing the psychology of AI models. Proc. Natl. Acad.\nSci. U. S. A., 120(10):e2300963120, March 2023.\n[47] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamic memory and self-reflection. March 2023.\n[48] Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised\ncommonsense question answering with self-talk. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 4615\u20134629, Online,\nNovember 2020. Association for Computational Linguistics.\n[49] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka\nKluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexan-\nder W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain,\nAmanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, An-\nders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew Dai, Andrew\nLa, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta,\nAnna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul\nMenezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka\u00b8s, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart\u0142omiej\nBojanowski, Batuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno\nStein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine\nStinson, Cedrick Argueta, C\u00e9sar Ferri Ram\u00edrez, Chandan Singh, Charles Rathkopf, Chenlin\nMeng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christo-\npher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin\nRaffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks,\n16\nDan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed\nGonz\u00e1lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa,\nDavid Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar\nBuzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk,\nElad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele\nRodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan\nDyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue\nXia, Fatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca Happ\u00e9, Francois Chollet, Frieda\nRong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambat-\ntista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L\u00f3pez, Gregor Betz,\nGuy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh\nMehta, Hayden Bogar, Henry Shevlin, Hinrich Sch\u00fctze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob\nHilton, Jaehoon Lee, Jaime Fern\u00e1ndez Fisac, James B Simon, James Koppel, James Zheng,\nJames Zou, Jan Koco\u00b4n, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein,\nJason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh,\nJeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian\nTang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Berant, J\u00f6rg Frohberg,\nJos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B Tenenbaum,\nJoshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakr-\nishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D Dhole, Kevin Gimpel, Kevin Omondi,\nKory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle\nRichardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt,\nLuheng He, Luis Oliveros Col\u00f3n, Luke Metz, L\u00fctfi Kerem \u00b8Senel, Maarten Bosma, Maarten Sap,\nMaartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco\nMarelli, Marco Maru, Maria Jose Ram\u00edrez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha\nLewis, Martin Potthast, Matthew L Leavitt, Matthias Hagen, M\u00e1ty\u00e1s Schubert, Medina Orduna\nBaitemirova, Melody Arnaud, Melvin McElrath, Michael A Yee, Michael Cohen, Michael Gu,\nMichael Ivanitskiy, Michael Starritt, Michael Strube, Micha\u0142 Sw\u02dbedrowski, Michele Bevilacqua,\nMichihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mohit\nBansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Varma T Mukund, Nanyun Peng,\nNathan Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick\nDoiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S\nIyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,\nOmer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu\nLiang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter\nEckersley, Phu Mon Htut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil, Pouya Pezeshkpour,\nPriti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer\nGabriel, Rahel Habacker, Ram\u00f3n Risco Delgado, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard\nBarnes, Rif A Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand,\nRoman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan\nSalakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh,\nSaif M Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruet-\nter, Samuel R Bowman, Samuel S Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A Rous,\nSarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian\nSchuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi,\nShikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal,\nShyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanis-\nlas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad,\nSteven T Piantadosi, Stuart M Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop\nMishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Th\u00e9o\nDesbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick,\nTimofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang,\nTrishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria\nNyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek\nSrikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu\n17\nTong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu\nSong, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\nYu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J Wang, Zirui\nWang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities\nof language models. June 2022.\n[50] K L Stachenfeld, M Botvinick, and others. Design principles of the hippocampal cognitive map.\nAdv. Neural Inf. Process. Syst., 2014.\n[51] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press,\nNovember 2018.\n[52] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following LLaMA model,\n2023.\n[53] Rita Morais Tavares, Avi Mendelsohn, Yael Grossman, Christian Hamilton Williams, Matthew\nShapiro, Yaacov Trope, and Daniela Schiller. A map for social navigation in the human brain.\nNeuron, 87(1):231\u2013243, July 2015.\n[54] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for\ndialog applications. arXiv preprint arXiv:2201.08239, 2022.\n[55] E C Tolman. Cognitive maps in rats and men. Psychol. Rev., 55(4):189\u2013208, July 1948.\n[56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[57] Tomer Ullman. Large language models fail on trivial alterations to Theory-of-Mind tasks.\nFebruary 2023.\n[58] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large\nlanguage models still can\u2019t plan (a benchmark for LLMs on planning and reasoning about\nchange). June 2022.\n[59] Marlieke T R van Kesteren, Sarah F Beul, Atsuko Takashima, Richard N Henson, Dirk J\nRuiter, and Guill\u00e9n Fern\u00e1ndez. Differential roles for medial prefrontal and medial temporal\ncortices in schema-dependent encoding: from congruent to incongruent. Neuropsychologia,\n51(12):2352\u20132359, October 2013.\n[60] Marlieke Tina Ren\u00e9e van Kesteren and Martijn Meeter. How to optimize knowledge construction\nin the brain. NPJ Sci Learn, 5:5, May 2020.\n[61] Yi Wan, Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Sarath Chandar,\nand Harm van Seijen. Towards evaluating adaptivity of Model-Based reinforcement learning\nmethods. April 2022.\n[62] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-\naugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022.\n[63] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\nSelf-\nconsistency improves chain of thought reasoning in language models.\narXiv preprint\narXiv:2203.11171, 2022.\n[64] Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large\nlanguage models. December 2022.\n[65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[66] James C R Whittington, Joseph Warren, and Timothy E J Behrens. Relating transformers to\nmodels and neural representations of the hippocampal formation. December 2021.\n18\n[67] Eunice Yiu, Eliza Kosoy, and Alison Gopnik. Imitation versus innovation: What children can\ndo that large language and language-and-vision models cannot (yet)? May 2023.\n[68] Eric Zelikman, Yuhuai Wu, and Noah D Goodman. STaR: Bootstrapping reasoning with\nreasoning. arXiv preprint arXiv:2203.14465, 2022.\n[69] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Olivier Bousquet, Quoc Le, and Ed Chi.\nLeast-to-most prompting enables\ncomplex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n19\nEvaluating Cognitive Maps in Large Language\nModels: No Emergent Planning\n(Supplementary Materials)\nAnonymous Author(s)\nAffiliation\nAddress\nemail\n1\nSupplementary Experiment: Systematic graph explorations\n1\nTo systematically evaluate GPT-4\u2019s planning or graph traversal failure modes, we created a three-block\n2\ncommunity graph structures where each block contains five vertices. Using this approach, we vary\n3\nthe connection density within each community block and ask GPT-4 to perform reasoning tasks over\n4\neach permutation of the graph structure as block density is varied. For the graph community block\n5\nmodel, example graphs are shown in Figure 1 with the community graphs starting as simple line\n6\ngraphs on the left - representing the sparsest level of connectivity. We then create a new edge within\n7\neach block for each iteration of the experiment until each community block forms a clique structure\n8\nas seen on the right of Figure 1. To measure performance, the LLM is asked to assign partitions\n9\nfor each vertex such as to maximize each graph\u2019s modularity. Modularity is chosen as the task as it\n10\nrequires a non-trivial understanding of the graph beyond the local network of any single vertex in\n11\norder to detect the boundaries between communities. The LLM\u2019s vertex assignment is then compared\n12\nto the vertex assignments obtained from a Leiden [? ] modularity maximization process. The results\n13\nare compared using Adjusted Rand Index (ARI), which gives a similarity score between the actual\n14\nmodularity-maximized partitioning scheme and the observed partitioning that the LLM returned. ARI\n15\nscores closer to zero represent poor performance by the LLM and ARI scores reaching 1.0 represent\n16\nperformance matching Leiden. This is performed for temperatures 0.05, 0.5, 0.95 and TopP 0.05, 0.5,\n17\n0.95. Each configured test is executed thirty times through the GPT-4 chat completion API.\n18\nFigure 1: Systematic investigation of line graphs to full-clique structures. Each graph is composed\nof three blocks, where each block is its own sub-graph structure with five interconnected vertices\non which we vary the edge density. Each block has a bridge node linked to other blocks\u2019 bridge\nnodes. The left graph displays the sparsest density with a line graph for each block, while the right\ngraph shows the densest blocks with each forming a fully interconnected clique. Vertex partition\nassignments are color-coded via Leiden modularity maximization.\nSubmitted to 37th Conference on Neural Information Processing Systems (NeurIPS 2023). Do not distribute.\narXiv:2309.15129v1  [cs.AI]  25 Sep 2023\nFigure 2: Supplementary experiment results. Systematic investigation of line graphs to full-clique\nstructures. y-axis: accuracy of the LLM as measured using the Adjusted Rand Index (ARI) between\na maximal modularity partitioning from the LLM as compared to observed maximal modularity\nvia Leiden. An ARI of 1.0 indicates the LLM matched Leiden. x-axis: the density of each block,\n0.4 represents a line graph of the five vertices and 1.0 represents a fully connected clique structure.\nTemperature: 0.05, 0.5, 0.95,TopP: 0.95.\n1.1\nSupplementary Experiment Evaluation: Evaluating the systematic effect of graph\n19\nstructure\n20\nFigure 2 shows the result of systematic evaluation over the community based graph structures using\n21\nGPT-4. The y-axis measures how well the LLM performed (higher is better) and the x-axis measures\n22\nthe density of each of the community block structure. Outlier points are added directly to each plot\n23\nand each panel shows how performance varies with temperature. These findings suggest that the LLM\n24\nperforms more poorly in the sparse community structures, but better as the edge density increases.\n25\nAdditionally, a higher temperature results in poorer performance with higher variance. These results\n26\nmay appear in contrast to the results shown in Experiment 1, where LLMs show apparent success in\n27\nlocal traversal of a small graph. However, as the task changed from local traversal (Experiment 1)\n28\ntowards optimization and reasoning of a non-local graph structure (Supplementary Experiment), we\n29\nobserve the LLMs fail when the structures are sparse. This points to the LLM\u2019s struggle to reason\n30\nover local neighborhood and community structures, and is consistent with failure on community\n31\ngraphs as demonstrated in Experiment 1. Moreover, these findings show how the LLMs\u2019 behavior is\n32\nnot consistent over community structures vs. simple traversals, varying across overall sparsity. This\n33\nis in line with the hypothesis that LLMs lack functional understanding of underlying structures of the\n34\nproblems, which may contribute to their failure in multi-step planning.\n35\nIn order to better understand failure modes, this experiment systematically evaluates the observed\n36\npoor performance of GPT-4 on community graphs on gradually more dense community graphs.\n37\n1.2\nSupplementary Experiment: Evaluating the systematic effect of graph structure as TopP\n38\nis varied\n39\nBelow are the graphs for each TopP configuration:Systematic graph explorations and the variance of\n40\nTopP\n41\n2\nFigure 3: Results of systematic investigation of line graphs to full-clique structures. TopP = 0.05.\nFigure 4: Results of systematic investigation of line graphs to full-clique structures. TopP = 0.5.\nFigure 5: Results of systematic investigation of line graphs to full-clique structures. TopP = 0.95.\n1.3\nSupplementary Experiment: Prompt templates\n42\nFigure 6 shows the prompt templates that were used in this supplementary experiment.\n43\n3\nFigure 6: Prompt template for maximizing modularity\n2\nCoT prompts used for BFS and DFS\n44\nWe have used quite general and simple description of graph traversal algorithms in the prompt to\n45\nmeasure how much the LLM can leverage the information in the prompt. The prompts that we have\n46\nused for BFS and DFS are as follows and they are directly appended to the existing prompt for each\n47\ntask.\n48\n\u2022 BFS: \"Think carefully before you respond. You can try using Breadth-first search (BFS), it\n49\nis a graph traversal algorithm that visits all the vertices of a graph in breadth-first order,\n50\nstarting from a given source vertex. In BFS, vertices are visited in layers, where the vertices\n51\nat distance 1 from the source vertex are visited first, followed by the vertices at distance 2,\n52\nand so on. BFS uses a queue data structure to keep track of the vertices to be visited, and\n53\nit ensures that no vertex is visited more than once. BFS is useful for finding the shortest\n54\npath between two vertices in an unweighted graph, or for exploring all the vertices in a\n55\ngraph.|endofprompt|\"\n56\n\u2022 DFS: \"Think carefully before you respond. You can try using Depth-first search (DFS), it is\n57\na graph traversal algorithm that visits all the vertices of a graph in depth-first order, starting\n58\nfrom a given source vertex. In DFS, the algorithm traverses as far as possible along each\n59\nbranch before backtracking. DFS uses a stack data structure to keep track of the vertices to\n60\nbe visited, and it ensures that all vertices connected to a visited vertex are explored before\n61\nbacktracking. DFS is useful for finding cycles in a graph, for exploring all the vertices in\n62\na graph, or for finding a path between two vertices. However, unlike BFS, DFS does not\n63\nguarantee that the shortest path is found.|endofprompt|\"\n64\n4\n3\nBrief descriptions of the task conditions applied to varying graphs and\n65\ndomains\n66\nTable 1: Brief descriptions of the task conditions applied to varying graphs and domains\nCondition\nDescription\nGroup\nvaluePath\nThe optimal solution is to find the optimal policy, or shortest path, which yields the highest reward\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\nTraversal\n1stepPath\nThe optimal solution is a 1-hop policy, i.e., goal is adjacent to the starting state\n2stepPath\nThe optimal solution is a 2-step policy\n3stepPath\nThe optimal solution is a 3-step policy\nnstepPath\nThe optimal solution is an n-step policy, where max n is the diameter of the graph (longest shortest path)\nrewardReval\nUpon a local change in the reward structure, the goal has changed and the optimal solution requires finding a new path\n\u001b\nRewReval\npolicyReval\nUpon a local change in the reward structure, the optimal solution requires finding a new policy\ntransReval\nUpon a local change in the transition structure, the goal is the same but the optimal solution requires finding a new policy \u001b\nTransReval\ntransRevalStochastic\nUpon a local change in the transition structure, the goal is the same but the optimal solution requires finding a new policy\nin a stochastic environment\nnonteleShortcut\nUpon a change in the graph structure, the optimal solution requires finding a shortcut\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\nShortcut\nnonteleShortcutCoT\nUpon a change in the graph structure, the optimal solution requires finding a shortcut, an additional CoT prompt is given\nteleShortcut\nUpon a local change in the transition structure, the optimal solution requires finding a shortcut using a teleportation portal\nteleShortcutCoT\nUpon a local change in the graph or transition structure, the optimal solution requires finding a shortcut using\na teleportation portal, an additional CoT prompt is given\nnonteleDetour\nUpon a change in the graph structure, the optimal solution requires finding a detour\n\u001b\nDetour\nteleDetour\nUpon a local change in the transition structure, the optimal solution requires finding a detour using a teleportation step\n4\nSummary of high-level statistical analysis\n67\nIn the presented study, the \"score\" of a dialog is the number of correct answers provided by the LLM\n68\nout of a total number of correct answers possible for that dialog. We modeled the score using a\n69\nlogistic regression approach; the score follows a binomial distribution with a probability parameter\n70\ndetermined by the three categorical variables (graph structure, condition, and domain) as well as\n71\nmodel and temperature. We our regression model included second and third-order interaction terms\n72\nbetween levels of these three terms.\n73\nOur initial strategy was to assume that for a particular combination of the three factors (graph\n74\nstructure, condition, and domain), the conjunction of model and temperature could be likened to\n75\na \u2019subject\u2019 in a repeated measures analysis. With the inability to set a seed in these LLMs, we\n76\nposited that each repeated measurement for the engine and temperature variables could be akin to a\n77\nrepeated replicate measure in a longitudinal or panel study, where there would be \"within-subject\n78\nvariation\" across replicates. We introduced a nested random effect (temperature nested within model)\n79\nto the linear component of a linear regression model. Note that we did not have an equal number of\n80\nreplicates across each combination of graph structure, condition, domain, model, and temperature\n81\n(the minimum number of replicates for a combination was 1, the maximum was 30, and the mean\n82\nwas 7.1). However, the logistic regression approach is robust to replicate imbalance.\n83\nWe used a two-step fitting process. In the first step, we used elastic net to fit the model using the R\n84\npackage glmnet. We relied on elastic net\u2019s mix of L1 and L2 regularization to address cases of LLMs\n85\nwhere we collected less data, and to address the multicollinearity introduced by the interaction terms.\n86\nThe parameter estimates are shown in Table 2. Next, we refit a new model using non-regularized\n87\nlogistic regression on the predictors with non-zero coefficient estimates in the first model, and used\n88\nthis model to generate the analysis of variance (deviance) table in Table ??. Deviance is a measure of\n89\ngoodness of fit; it quantifies the discrepancy between the observed scores and the scores predicted\n90\nby the model. Each row in the Chi-squared statistic column quantifies the reduction in deviance by\n91\nadding the categorical variables associated with the term in that row, given the variables from the\n92\nprevious rows are included in the model.\n93\n5\nExperiment 1, statics of results: Repeated measures comparison of\n94\nplanning across LLMs\n95\nWe evaluated out-of-the-box emergent or native ability of different LLMs on the cognitive map tasks.\n96\nTable 2 shows the statistical analysis highlighting the contributions of each factor to regression model\u2019s\n97\n5\nTable 2: Parameter estimates from the regularized logistic regression model. Baselines are condi-\ntion:traversal, graph:n7line, domain:ordRooms, LLM:replicate-alpaca-7b, temp:0. NA values indicate\nthe data was not sufficient to fit the parameters.\nfactor\nlevel\nestimate\nodds multiple\np value\n1\n(Intercept)\n(Intercept)\n0.85\n2.33\n<0.001\n2\nLLM\nbard\n-0.41\n0.66\n0.01\n3\nLLM\ncohere-xlarge\n-2.25\n0.11\n<0.001\n4\nLLM\ngpt-35-turbo\n-0.40\n0.67\n<0.001\n5\nLLM\ngpt-4-32k\n1.25\n3.50\n<0.001\n6\nLLM\nreplicate-alpaca-7b\n-3.57\n0.03\n<0.001\n7\nLLM\nreplicate-llama-13b\n-2.67\n0.07\n<0.001\n8\nLLM\ntext-davinci-003\n-0.73\n0.48\n<0.001\n9\ngraph\nn13line\n-1.14\n0.32\n<0.001\n10\ngraph\nn7tree\n-2.59\n0.07\n<0.001\n11\ngraph\nn15star\n-1.47\n0.23\n<0.001\n12\ngraph\nn21star\n-1.78\n0.17\n<0.001\n13\ngraph\nn16cluster\n-0.62\n0.54\n<0.001\n14\ndomain\nsocialTies\n0.58\n1.78\n<0.001\n15\ndomain\nunordSpatial\n-1.25\n0.29\n<0.001\n16\ntemperature\n0.5\n-0.00\n1.00\n0.96\n17\ntemperature\n1\n-0.06\n0.95\n0.12\n18\ncondition\nDetour\n-2.35\n0.10\n<0.001\n19\ncondition\nRewReval\n-2.56\n0.08\n<0.001\n20\ncondition\nShortcut\n-2.01\n0.13\n<0.001\n21\ncondition\nTransReval\n-2.23\n0.11\n<0.001\n22\nmodel and temp\ngpt-4-32k.0.5\n0.04\n1.04\n0.52\n23\nmodel and temp\ngpt-4-32k.1\n0.05\n1.05\n0.37\n24\ngraph and domain\nn7line & ordRooms\n2.11\n8.27\n<0.001\n25\ngraph and domain\nn7tree & ordRooms\n1.01\n2.75\n<0.001\n26\ngraph and domain\nn7line & socialTies\n0.77\n2.16\n<0.001\n27\ngraph and domain\nn7line & unordSpatial\nNA\nNA\nNA\n28\ngraph and condition\nn7line & Detour\n1.95\n7.02\n<0.001\n29\ngraph and condition\nn13line & RewReval\n3.18\n23.99\n<0.001\n30\ngraph and condition\nn16cluster & RewReval\n1.46\n4.30\n<0.001\n31\ngraph and condition\nn7line & Shortcut\n1.66\n5.28\n<0.001\n32\ngraph and condition\nn13line & Traversal\n2.06\n7.88\n<0.001\n33\ngraph and condition\nn16cluster & Traversal\n0.13\n1.14\n0.19\n34\ngraph and condition\nn7line & Traversal\n1.32\n3.76\n<0.001\n35\ngraph and condition\nn7tree & Traversal\n1.97\n7.17\n<0.001\n36\ndomain and condition\nsocialTies & Shortcut\n-0.02\n0.98\n0.90\n37\ndomain and condition\nordRooms & Traversal\n-0.48\n0.62\n<0.001\n38\ndomain and condition\nsocialTies & Traversal\n-0.99\n0.37\n<0.001\n39\ngraph, domain, condition\nn16cluster & ordRooms & Detour\n0.70\n2.02\n<0.001\n40\ngraph, domain, condition\nn7line & ordRooms & Detour\n0.50\n1.65\n0.10\n41\ngraph, domain, condition\nn7tree & ordRooms & Detour\n2.39\n10.94\n<0.001\n42\ngraph, domain, condition\nn15star & socialTies & Detour\n1.30\n3.69\n<0.001\n43\ngraph, domain, condition\nn21star & socialTies & Detour\n1.01\n2.73\n<0.001\n44\ngraph, domain, condition\nn7line & socialTies & Detour\n-1.06\n0.35\n<0.001\n45\ngraph, domain, condition\nn15star & unordSpatial & Detour\n1.78\n5.92\n<0.001\n46\ngraph, domain, condition\nn21star & unordSpatial & Detour\n2.24\n9.40\n<0.001\n47\ngraph, domain, condition\nn13line & ordRooms & RewReval\n-0.57\n0.57\n0.01\n48\ngraph, domain, condition\nn7tree & ordRooms & RewReval\n2.58\n13.20\n<0.001\n49\ngraph, domain, condition\nn13line & socialTies & RewReval\n-0.20\n0.82\n0.38\n50\ngraph, domain, condition\nn15star & socialTies & RewReval\n2.58\n13.25\n<0.001\n51\ngraph, domain, condition\nn16cluster & socialTies & RewReval\n0.80\n2.23\n<0.001\n52\ngraph, domain, condition\nn15star & unordSpatial & RewReval\n3.61\n36.98\n<0.001\n53\ngraph, domain, condition\nn16cluster & unordSpatial & RewReval\n2.07\n7.94\n<0.001\n54\ngraph, domain, condition\nn7line & unordSpatial & RewReval\n2.58\n13.21\n<0.001\n55\ngraph, domain, condition\nn7line & ordRooms & Shortcut\n-1.76\n0.17\n<0.001\n56\ngraph, domain, condition\nn21star & socialTies & Shortcut\n0.91\n2.48\n<0.001\n57\ngraph, domain, condition\nn7line & socialTies & Shortcut\n-0.48\n0.62\n0.06\n58\ngraph, domain, condition\nn7tree & socialTies & Shortcut\n3.59\n36.40\n<0.001\n59\ngraph, domain, condition\nn15star & unordSpatial & Shortcut\n1.95\n7.05\n<0.001\n60\ngraph, domain, condition\nn16cluster & unordSpatial & Shortcut\n1.28\n3.59\n<0.001\n61\ngraph, domain, condition\nn13line & ordRooms & TransReval\n2.31\n10.12\n<0.001\n62\ngraph, domain, condition\nn15star & ordRooms & TransReval\n1.04\n2.82\n<0.001\n63\ngraph, domain, condition\nn7tree & ordRooms & TransReval\n2.84\n17.13\n<0.001\n64\ngraph, domain, condition\nn7tree & socialTies & TransReval\n3.20\n24.49\n<0.001\n65\ngraph, domain, condition\nn13line & unordSpatial & TransReval\n3.37\n29.19\n<0.001\n66\ngraph, domain, condition\nn16cluster & unordSpatial & TransReval\n1.78\n5.92\n<0.001\n67\ngraph, domain, condition\nn7line & unordSpatial & TransReval\n2.65\n14.15\n<0.001\n68\ngraph, domain, condition\nn13line & ordRooms & Traversal\n1.70\n5.47\n<0.001\n69\ngraph, domain, condition\nn7line & ordRooms & Traversal\n-0.67\n0.51\n0.01\n70\ngraph, domain, condition\nn13line & socialTies & Traversal\n0.77\n2.15\n<0.001\n71\ngraph, domain, condition\nn16cluster & socialTies & Traversal\n-0.02\n0.98\n0.87\n72\ngraph, domain, condition\nn7line & socialTies & Traversal\nNA\nNA\nNA\n73\ngraph, domain, condition\nn21star & unordSpatial & Traversal\n0.65\n1.91\n<0.001\n74\ngraph, domain, condition\nn7tree & unordSpatial & Traversal\n0.72\n2.06\n<0.001\nfit of LLM model performance. The magnitude of chi-square test statistics indicate contribution to\n98\noverall model fit. Figure 3 compares the performance of all LLMs across all latent graph structures.\n99\nTable 3 shows mean and standard error for planning performance across tasks and LLMs.\n100\nThe results in Table 2 indicate that the model engine (\u03c72(11) = 689.36, p < .001.), graph (\u03c72(11) =\n101\n7247.30, p < .001.), condition (\u03c72(11) = 1475.03, p < .001.), and domain (\u03c72(11) = 304.06,\n102\np < .001.) each yielded significant chi-squared statistics. This means that not only did different\n103\nLLMs performed differently, but each LLM\u2019s performance varied as a result of varying graphs,\n104\ndomains, and conditions. Conversely, the temperature showed a non-significant chi-squared statistic\n105\n(\u03c72(11) = 1.01, p = .6022) and the interaction between the model and temperature was also\n106\n6\nnon-significant (\u03c72(11) = 4.65, p = .997). Noteworthy, the interactions among graph-domain,\n107\ngraph-condition, domain-condition, and graph-domain-condition were all significant (all p\u2019s < .001).\n108\nThe interactions among graph-domain (\u03c72(11) = 689.36, p < .001), graph-condition (\u03c72(50) =\n109\n1392.82, p < .001), domain-condition (\u03c72(39) = 524.48, p < .001), and graph-domain-condition\n110\n(\u03c72(108) = 1002.93, p < .001) were all significant.\n111\nIn summary, while the \u2019temperature\u2019 and the interaction of \u2019model\u2019 and \u2019temperature\u2019 do not show\n112\nsignificant effects in Experiment 1 (but show difference in Experiments 2 and 3), all other factors\n113\nand their interactions significantly contribute to the variations in the dependent variable. Considering\n114\nboth individual and interactive effects, this finding shows that LLM performance on cognitive map\n115\nand planning tasks was not robust to the graph structure of the problems, the domain, or the task\n116\nconditions, and it also varied across models (see Tables 2 and 3 and Figure 3 .\n117\n7\n"
  },
  {
    "title": "VPA: Fully Test-Time Visual Prompt Adaptation",
    "link": "https://arxiv.org/pdf/2309.15251.pdf",
    "upvote": "4",
    "text": "VPA: Fully Test-Time Visual Prompt Adaptation\nJiachen Sun\u2217\nUniversity of Michigan\nAnn Arbor, MI, USA\njiachens@umich.edu\nMark Ibrahim\nMeta AI\nNew York, NY, USA\nmarksibrahim@meta.com\nMelissa Hall\nMeta AI\nNew York, NY, USA\nmelissahall@meta.com\nIvan Evtimov\nMeta AI\nSeattle, WA, USA\nivanevtimov@meta.com\nZ. Morley Mao\nUniversity of Michigan\nAnn Arbor, MI, USA\nzmao@umich.edu\nCristian Canton Ferrer\nMeta AI\nSeattle, WA, USA\nccanton@meta.com\nCaner Hazirbas\nMeta AI\nNew York, NY, USA\nhazirbas@meta.com\nABSTRACT\nTextual prompt tuning has demonstrated significant performance\nimprovements in adapting natural language processing models to a\nvariety of downstream tasks by treating hand-engineered prompts\nas trainable parameters. Inspired by the success of textual prompt-\ning, several studies have investigated the efficacy of visual prompt\ntuning. In this work, we present Visual Prompt Adaptation (VPA),\nthe first framework that generalizes visual prompting with test-time\nadaptation. VPA introduces a small number of learnable tokens,\nenabling fully test-time and storage-efficient adaptation without\nnecessitating source-domain information. We examine our VPA de-\nsign under diverse adaptation settings, encompassing single-image,\nbatched-image, and pseudo-label adaptation. We evaluate VPA on\nmultiple tasks, including out-of-distribution (OOD) generalization,\ncorruption robustness, and domain adaptation. Experimental results\nreveal that VPA effectively enhances OOD generalization by 3.3%\nacross various models, surpassing previous test-time approaches.\nFurthermore, we show that VPA improves corruption robustness\nby 6.5% compared to strong baselines. Finally, we demonstrate that\nVPA also boosts domain adaptation performance by relatively 5.2%.\nOur VPA also exhibits marked effectiveness in improving the ro-\nbustness of zero-shot recognition for vision-language models.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Machine learning approaches.\n\u2217This project was mainly conducted during Jiachen Sun\u2019s internship at Meta AI.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConference\u201917, July 2017, Washington, DC, USA\n\u00a9 2018 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nKEYWORDS\nTest-Time Adaptation, Out-of-Distribution Generalization, Corrup-\ntion Robustness, Domain Adaptation\nACM Reference Format:\nJiachen Sun, Mark Ibrahim, Melissa Hall, Ivan Evtimov, Z. Morley Mao,\nCristian Canton Ferrer, and Caner Hazirbas. 2018. VPA: Fully Test-Time\nVisual Prompt Adaptation. In Proceedings of ACM Conference (Conference\u201917).\nACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nVisual recognition, a crucial component in multimedia systems,\nplays an essential role in various applications. As technology evolves\nand the demand for intelligent multimedia systems increases, the im-\nportance of effective and robust visual recognition techniques can-\nnot be overstated. Although various deep neural networks achieve\nstate-of-the-art (SOTA) performance on test sets drawn from the\nsame distribution as the training set [9], these expertly-trained mod-\nels may struggle to generalize when faced with distribution shifts,\nleading to substantial performance drops [23, 26]. These shifts en-\ncompass common corruption [24], adversarial attacks [5, 52, 53, 55],\nconceptual changes [23, 30], and even out-of-distribution (OOD)1\nvariations [26], and can emerge in numerous real-world applica-\ntions such as autonomous driving [51, 57, 64] and facial recognition\nsystems [59], where accurate and robust performance is critical.\nTherefore, addressing the vulnerabilities to distribution shifts is es-\nsential for enhancing the robustness and generalization capabilities\nof deep learning models.\nNumerous architectural improvements and training techniques\nhave been proposed to address the challenges associated with\nachieving robustness against various domain variations [12, 21, 45].\nFor instance, recent advances in Vision Transformer (ViT) architec-\ntures [12] have demonstrated significant improvements on many\nout-of-distribution (OOD) generalization and corruption robustness\nbenchmarks [23, 26]. Pretraining and fine-tuning strategies, such as\nCLIP [45] and WiSE [62], have further enhanced the generalization\n1In this study, we refer out-of-distribution (OOD) to covariate shift but not concept\nshift of the test/validation dataset.\narXiv:2309.15251v1  [cs.CV]  26 Sep 2023\nConference\u201917, July 2017, Washington, DC, USA\nJiachen Sun et al.\nperformance of ViT models [12]. In addition to these general meth-\nods, a plethora of specialized training recipes have been presented\nto address specific objectives, including OOD generalization [61, 63],\ncorruption robustness [8, 25], domain adaptation [25]. However,\nit remains challenging to address these generalization problems\nsolely during the training phase, as a single training recipe cannot\nencompass all underlying distributions. Test-time updates serve as\nvaluable complements, focusing on tailored adaptation for specific\ntest data [61]. Such a scheme is particularly important for multime-\ndia systems, where content may come from unseen domains.\nHumans typically begin with their existing knowledge and ex-\ntrapolate from it when learning a new skill. Prompting is a similar\nparadigm that aids machine learning models in adapting to various\ncontexts or even new tasks through specific textual input. This\napproach has gained popularity in the field of natural language pro-\ncessing (NLP) [41]. Recent studies have shown that prompt tuning\ncan enhance model generalization across different domains, where\npredefined prompts evolve as trainable parameters [66, 67]. Test-\ntime Prompt Tuning (TPT) is a pioneering technique that leverages\ntextual prompting during testing to improve the generalization\ncapability of vision-language models [45, 49]. Prompt tuning is\nefficient in adapting a pretrained model, as it does not modify\nthe original model parameters. In addition to prompting in NLP,\nrecent studies have explored visual prompting during the train-\ning phase [3, 29], yielding substantial improvements on numerous\nvision benchmarks. However, there is still a scarcity of research\nexamining the application of visual prompting in online test-time\nadaptation, indicating an area ripe for further exploration.\nOur Contributions. In this paper, we propose visual prompt adap-\ntation (VPA) to bridge the gap between visual prompting and online\ntest-time adaptation, drawing inspiration from the success of textual\nprompting in NLP. VPA is a simple yet effective framework that gen-\neralizes existing prompt designs and adaptation objectives. Given a\npretrained model, we attach additive and prependitive prompts to\nthe frozen model during the adaptation phase. VPA is highly storage-\nefficient. Rather than adapting all the parameters of the model, VPA\nrequires only a small number of prompts to be stored. This efficiency\ngreatly reduces the storage overhead while maintaining the ability\nto effectively adapt the model to new contexts and tasks, making\nVPA a practical and appealing solution for real-world applications.\nIn contrast to the pixel-space prompts in [3] and the randomized\ninitialization for embedding-space prompts in [29], we design a\nstraightforward but intuitive paradigm using zero attention to ini-\ntialize our prompts, ensuring that the original performance remains\nunaffected. We combine VPA with various adaptation settings, in-\ncluding batched-image adaptation (BIA), single-image adaptation\n(SIA), and pseudo-label adaptation (PLA). For BIA, we input a batch\nof images belonging to different classes into the model simulta-\nneously and leverage self-entropy minimization as the adaptation\nobjective. In SIA, we employ marginal entropy minimization as the\nobjective by enriching a single image into a batch through various\naugmentations. Additionally, we employ confidence selection to\nidentify images with high confidence, which allows for more ef-\nfective adaptation. By focusing on these high-confidence images,\nthe VPA framework can better leverage the information contained\nwithin them. Pseudo-labels have been shown to be effective in en-\nhancing test-time adaptation performance. As such, we incorporate\na memory queue that stores pseudo-labels for historical data to\nassist incoming images during adaptation. Our VPA combines the\nstrengths of VPA, BIA, SIA, and PLA to achieve both fully test-time\nand storage-efficient adaptation.\nWe conduct extensive evaluations of VPA across three critical\naxes in real-world vision systems: OOD generalization (\u00a7 4.1), cor-\nruption robustness (\u00a7 4.2), and domain adaptation (\u00a7 4.3). In our\nOOD generalization experiments, we are the first to evaluate visual\nprompting on a variety of large-scale datasets. We primarily use the\nViT architecture fine-tuned on the ImageNet training set as our base\nmodel. Our results indicate that VPA enhances the average accuracy\nof ImageNet-scale OOD generalization benchmarks [23, 26] by 3.3%.\nIn contrast, existing state-of-the-art methods, such as TENT [61]\nand DDA [16], struggle to perform effectively under challenging\nOOD scenarios. Moreover, VPA achieves a similar 2.6% improve-\nment as MEMO [63] under SIA without updating the parameters\nof the frozen model. Notably, we also demonstrate that VPA en-\nhances corruption robustness and domain adaptation performance\nby relative margins of 6.5% and 5.2%, respectively, compared to\nstrong baselines [6, 32]. Moreover, we have shown that VPA could\neffectively improve the robustness of zero-shot recognition for\nvision-language models [45]. It is important to note that the goal\nof our study is not solely to pursue state-of-the-art results but to\nhighlight the potential applications of visual prompting in test-time\nadaptation. Our promising results will encourage future research\nto develop new adaptation schemes using visual prompting.\nWe summarize our main contributions as three-fold:\n\u2022 We propose visual prompt adaptation (VPA), a fully test-time and\nstorage-efficient adaptation framework that introduces both additive\nand prependitive adaptable tokens to improve the robustness of\nvision models.\n\u2022 We conduct a rigorous taxonomy of VPA under different adap-\ntation setups, including batched-image, single-image, and pseudo-\nlabel adaptation.\n\u2022 We perform an extensive evaluation of VPA on various tasks,\nincluding out-of-distribution (OOD) generalization, corruption ro-\nbustness, and domain adaptation. Our VPA consistently improves\nperformance on these benchmarks by a significant margin.\n2\nRELATED WORK\nIn this section, we review topics related to our study, including\nprompting in foundation models, test-time adaptation, and out-of-\ndistribution (OOD) robustness and domain adaptation.\nPrompting in Foundation Models. Deep learning has made sig-\nnificant strides in natural language processing and computer vision\ntasks [36, 37]. In addition to architectural improvements [12, 22, 60],\nrecent efforts have focused on fine-tuning foundational models with\nlarge-scale data to enable transfer learning across multiple down-\nstream tasks [10, 45]. Prompting, which originated in language\nmodels, involves using human-engineered texts to improve the\ncontext-specific learning of a given task [33, 41]. Prompt tuning has\nbeen advanced for different goals in language models [19, 20, 38],\nwhile visual prompting has been explored for zero-shot recogni-\ntion tasks [66, 67]. For instance, CoOp [67] and CoCoOp [66] both\nemploy trainable prompts to improve zero-shot recognition perfor-\nmance, while TPT [49] leverages test-time adaptation of language\nVPA: Fully Test-Time Visual Prompt Adaptation\nConference\u201917, July 2017, Washington, DC, USA\nTransformer Block                             \nLinear Projection of Flattened Patches \nTransformer Block\nL1\nLm\nLN\nTransformer Block\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n \n\u2026\n\u2026\n\u2026\n\u2026\nCls Token\n;\n\u2026\n\u2026\n\u2026\nAdditive Prompt\nPrependitive Prompt\n\u2026\n\u2026\n\u2026\n\u2026\nHead\n+\n+\n+\nVision \nModel\nPrompt\nPredictions\nBatched/Single-Image Adaptation\nMarginal Entropy \nMinimization\nBackward Propagation\nBatched Input\nAugmented Single Input\nConfidence\nSelection\nBackward Propagation\nSelf-Entropy Minimization\nVision \nModel\nPrompt\nMemory \nQueue\nEnqueue\nPredictions\nBatched/Single Input\nAugW\nAugS\nBackward Propagation\nPseudo Label\nCross Entropy\nPseudo-Label Adaptation\nFigure 1: Overview of Visual Prompt Adaptation Pipeline. Our VPA supports both batched- and single-image and pseudo-label\nadaptation settings as shown in the left figure. The visual prompt designs are illustrated in the right figure based on the ViT\narchitecture.\nprompts to improve out-of-distribution robustness. Visual prompt-\ning has also been proposed to reprogram recognition tasks [14] and\nto enhance model performance on various downstream tasks [3].\nRecent research has also introduced memory-efficient prompt tun-\ning purely for vision models [29] to improve model generalization.\nIn this work, we propose visual prompt adaptation that achieves\nfully test-time and storage-efficient adaptation to improve model\ngeneralization.\nTest-Time Adaptation. Adapting machine learning models to dif-\nferent test domains has been applied to numerous tasks [35, 39,\n40, 56, 58]. Among various adaptation techniques, we focus on\ntest-time adaptation, which is particularly beneficial as it does not\nrequire label information from the test data [13, 39, 58]. Source-\nfree adaptation allows models to adapt without any source-domain\nknowledge, adhering to real-world deployment constraints related\nto computation and privacy [13, 30]. Fully test-time adaptation is\nmore rigorous, as it necessitates on-the-fly model updates without\ndelaying inference.[42, 48] represent initial efforts towards achiev-\ning fully test-time adaptation, which involves updating or replacing\nthe statistics of batch normalization (BatchNorm) layers[27] during\ninference. TENT accomplishes fully test-time adaptation by updat-\ning the model parameters in BatchNorm layers, using self-entropy\nminimization as its objective [61]. MEMO takes advantage of input\naugmentations to achieve single image adaptation, circumventing\nthe batch-level adaptation requirement in TENT [63]. In contrast to\noptimizing model parameters, an alternative approach is to adapt\ninput with minor modifications. DDA employs diffusion models\nto purify input data, although it is limited to specific types of cor-\nruption [16]. TPT introduces test-time adaptation via language\nprompting to enhance the OOD robustness of the CLIP model [49].\nModel Robustness against Distribution Shifts. A trustwor-\nthy machine learning model should exhibit robust performance\nunder data distribution shifts in real-world applications [54, 55].\nDistribution shift refers to the differences between the underlying\ndistributions of test and training data for a model trained on a spe-\ncific dataset. Distribution shifts can naturally occur in the physical\nworld due to environmental and conceptual variations [11, 31].In\nour study, we address three types of distribution shifts. Firstly, we\nconsider natural variations such as object size, occlusion, and ren-\ndition changes as an OOD generalization problem, where the test\nset does not adhere to a specific pattern or concept. For instance,\nHendrycks et al. proposed ImageNet-A [26], which serves as natural\n\"adversarial\" examples that challenge vision systems. ImageNet-\nR [23] was introduced to encompass a variety of patterns, including\nart, cartoons, deviantart, and graffiti etc. Secondly, we examine\ncommon corruptions of visual data that frequently occur in every-\nday life. For example, autonomous driving vehicles may encounter\nvarious weather changes, such as snow, fog, and rain [24]. Lastly,\nwe explore the domain adaptation problem, where the test set dif-\nfers from the training set but follows a specific pattern or concept,\nsuch as DomainNet [30] and VisDA-C [44]. Numerous methods\nhave been investigated to enhance the OOD robustness of machine\nlearning models at different stages. These approaches include pre-\ntraining techniques [21, 45, 65], finetuning methods [34, 62], and\ntest-time strategies [16, 61, 63]. By exploring various techniques\nthroughout the model\u2019s lifecycle, researchers aim to develop more\nrobust models capable of handling distribution shifts problems. By\ntackling these distribution shifts, our study aims to improve the ro-\nbustness and adaptability of machine learning models in real-world\napplications.\nConference\u201917, July 2017, Washington, DC, USA\nJiachen Sun et al.\n3\nVPA: VISUAL PROMPT ADAPTATION\nIn this section, we introduce VPA, which leverages visual prompting\nfor fully test-time adaptation. We first motivate our design. Next,\nwe describe our visual prompt and adaptation designs in \u00a73.1 and\n\u00a73.2, respectively. Finally, we present our adaptation setups in \u00a73.3.\nWhy Fully Test-Time? Although various training-phase (i.e., pre-\ntraining and fine-tuning) methods have been proposed to improve\nmodel performance and generalization, there is no overarching\ncombination that achieves the best performance. Therefore, test-\ntime adaptation is a desirable complement. As briefly mentioned\nin \u00a71 and \u00a72, fully test-time adaptation updates the model with-\nout preventing inference nor accessing source domain information.\nBesides, it does not require any supporting dataset [28] and en-\nsures that the adaptation solely relies on the current input (i.e., the\nbatched or single image). We believe such a setup is realistic as it\naccounts for protecting the privacy and intellectual property of the\nmodel and supports domain switches during inference.\nWhy Visual Prompting? Visual prompting has shown great po-\ntential in adapting and even reprogramming the model during the\ntraining phase [7, 29], making it memory and storage efficient. Vi-\nsual prompt tuning is also beneficial in terms of faster convergence.\nTraining a language model from scratch can be a time-consuming\nprocess; however, incorporating visual prompts can accelerate the\ntraining process by providing the model with relevant visual cues.\nThis enables the model to learn and converge more quickly, result-\ning in improved efficiency and reduced training time. However,\nthere are very few studies that have researched the application of\nvisual prompting in test-time adaptation. Our study serves as a\nfirst step towards exploring the effectiveness of visual prompting\nin this area. Additionally, we integrate visual prompting into ex-\nisting adaptation frameworks and demonstrate its superiority in\nimproving model generalization and robustness.\n3.1\nPrompt Design\nTaxonomy of Visual Prompts. In this study, we introduce a novel\nadaptation design that utilizes visual prompting. While prompt-\ning has been extensively studied in language models for various\ntasks, as discussed in \u00a7 2, there exist only a few visual prompt\ndesigns aimed at improving recognition performance in the train-\ning phase [7, 29]. We formally define these designs as additive\nand prependitive prompts and illustrate them using the architec-\nture of the Vision Transformer (ViT) model. Consider a ViT model\nwith \ud835\udc41 layers, where an input image is divided into \ud835\udc5a patches\n{\ud835\udc3c\ud835\udc56 |1 \u2264 \ud835\udc56 \u2264 \ud835\udc5a}. Each patch is then fed into the linear projection\nlayer with positional encoding: \ud835\udc86\ud835\udc56\n0 = Linear(\ud835\udc3c\ud835\udc56). We denote the input\nto the \ud835\udc56-th Transformer layer as \ud835\udc6c\ud835\udc56\u22121 = {\ud835\udc86\ud835\udc57\n\ud835\udc56\u22121|1 \u2264 \ud835\udc57 \u2264 \ud835\udc5a}, and the\n\ud835\udc56-th Transformer layer output as [\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc56; \ud835\udc6c\ud835\udc56] = \ud835\udc3f\ud835\udc56 ([\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc56\u22121; \ud835\udc6c\ud835\udc56\u22121]),\nwhere the classification head takes the final \ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc41 token for pre-\ndiction: \ud835\udc66 = Head(\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc41 ). Each Transformer layer consists of a\nself-attention module, an MLP layer with LayerNorm, and residual\nconnections. The additive prompting is defined as:\n[\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc56; \ud835\udc6c\ud835\udc56] = \ud835\udc3f\ud835\udc56 ([\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc56\u22121; \ud835\udc77\ud835\udc56\u22121 + \ud835\udc6c\ud835\udc56\u22121])\n(1)\nwhere \ud835\udc77\ud835\udc56 = {\ud835\udc91\ud835\udc57\n\ud835\udc56 |1 \u2264 \ud835\udc57 \u2264 \ud835\udc5a} and + denotes element-wise addition.\nSimilarly, our prependitive prompting is formulated as:\n[\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc56; \ud835\udc81\ud835\udc56; \ud835\udc6c\ud835\udc56] = \ud835\udc3f\ud835\udc56 ([\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc56\u22121; \ud835\udc77\ud835\udc56\u22121; \ud835\udc6c\ud835\udc56\u22121])\n(2)\nwhere the prompts \ud835\udc77 play as additional tokens and \ud835\udc81 is the output\ncorresponding to the input prompts. In the rest of this paper, we\nuse \u2295 to denote the attachment of visual prompts i.e., \ud835\udc77 \u2295 \ud835\udc6c. The\nnumbers of visual prompts and layers prompted are configurable.\nWhile we use the ViT architecture to demonstrate our visual\nprompt designs, it\u2019s important to note that additive prompting is\ncompatible with most model architectures, as it only modifies the\nnumeric values without altering the input size. In contrast, prependi-\ntive prompting changes the input dimension, which is better suited\nfor ViT-based models, as Transformer blocks are insensitive to the\nlength of the input [12]. In contrast to randomized initialization,\nwe design a zero attention scheme to initialize the prompt with\nzero tensors. This approach ensures that the initialization process\ndoes not impact the original performance of the frozen model.\n3.2\nPrompt Adaptation\nIn this section, we present the test-time adaptation procedure. Our\nstudy investigates two setups, namely episodic and continual adap-\ntations. Episodic adaptation only applies to incoming data, and the\nmodel will be reset afterward. In contrast, continual adaptation\nlasts throughout the entire inference procedure.\nA recent study by Goyal et al. [18] has demonstrated that self-\nentropy minimization is an almost-optimal objective function for\nepisodic test-time adaptation on models trained with cross-entropy\nloss. Therefore, we adopt the objective of self-entropy minimization\nin our study. Let \ud835\udc53 denote a well-trained classifier, and the self-\nentropy \ud835\udc3b (\u00b7) of a prediction is formulated as follows:\n\ud835\udc3b (\ud835\udc9b,\ud835\udf0f) = \u2212\n\ud835\udc50\u2211\ufe01\n\ud835\udc56=1\n\ud835\udf0e(\ud835\udc9b/\ud835\udf0f)\ud835\udc56 log\ud835\udf0e(\ud835\udc9b/\ud835\udf0f)\ud835\udc56\n\ud835\udc9b = \ud835\udc53 (\ud835\udc99 \u2295 \ud835\udc77)\n(3)\nHere \ud835\udc50 is the number of classes, \ud835\udf0e(\ud835\udc9b)\ud835\udc56 =\nexp \ud835\udc9b(\ud835\udc56)\n\u00cd\ud835\udc50\n\ud835\udc57=1 exp \ud835\udc9b(\ud835\udc57) is the softmax\nfunction, and \ud835\udf0f is a tunable hyper-parameter that controls the soft-\nmax temperature. Self-entropy is an unsupervised loss function as\nit relies only on predictions and not on ground-truth information.\nHowever, since entropy reflects the prediction confidence, it can\nserve as an indicator of the model\u2019s performance on the supervised\ntask [61]. As VPA is a general adaptation framework, we leverage\nboth batched- and single-image adaptation settings in our work, as\nintroduced below.\nBatched-Image Adaptation (BIA). Real-world machine learning\nservices usually aggregate input data for batched predictions to\nsave computation resources and time [1]. Therefore, we mainly\nfocus on BIA in our study, whose objective is formulated as:\n\u02c6\ud835\udc77 = arg min\n\ud835\udc77\n1\n\ud835\udc3e\n\ud835\udc3e\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc3b (\ud835\udc53 (\ud835\udc99\ud835\udc56 \u2295 \ud835\udc77),\ud835\udf0f)\n(4)\nwhere \ud835\udc3e is the batch size. In BIA, VPA optimizes a visual prompt\nfor all the test images in a given batch, which is the same setup as\nTENT [61], the SOTA method under BIA.\nSingle-Image Adaptation (SIA). As self-entropy minimization\nrequires batched inputs to function [61], we utilize the setups in\nMEMO [63] to expand a single image to a batch via augmentations.\nVPA: Fully Test-Time Visual Prompt Adaptation\nConference\u201917, July 2017, Washington, DC, USA\nTable 1: OOD Generalization Evaluation Results (%) of VPA on ImageNet Variants. The OOD average accuracy is calculated\nfrom the evaluation of ImageNet-A, ImageNet-R, and ObjectNet datasets.\nAccuracy (\u2191)\nMethod\nImageNet\nImageNet-V2\nImageNet-A\nImageNet-R\nObjectNet\nOOD Average\nSource\nCLIP-ViT-LPFT\n81.2\n71.1\n49.3\n71.1\n52.3\n57.6\nEpisodic BIA\nTENT (Norm Layer)\n81.3\n71.3\n49.6\n71.8\n52.6\n58.0\nTENT (Cls Token)\n81.2\n71.0\n49.4\n71.5\n52.3\n57.7\nTENT (All Parameters)\n81.2\n71.2\n49.7\n71.6\n52.4\n57.9\nDDA\n77.2\n65.2\n38.5\n65.4\n46.5\n50.1\nAdditiv VPA\n81.3\n71.4\n50.4\n72.0\n52.8\n58.4\nPrependitive VPA\n81.3\n71.3\n50.1\n72.0\n52.5\n58.2\nEpisodic SIA\nMEMO\n81.3\n72.3\n52.0\n72.2\n52.9\n59.0\nAdditive VPA\n81.2\n72.3\n49.5\n72.5\n52.3\n58.1\nPrependitive VPA\n81.2\n72.9\n52.4\n72.6\n52.8\n59.3\nSource\nCLIP-ViT-WiSE\n79.8\n70.5\n49.7\n71.9\n52.4\n58.0\nEpisodic BIA\nTENT (Norm Layer)\n79.6\n70.7\n49.8\n72.2\n52.8\n58.3\nTENT (Cls Token)\n79.7\n70.5\n50.0\n72.2\n52.4\n58.2\nTENT (All Parameters)\n79.8\n70.6\n50.3\n72.5\n52.5\n58.4\nDDA\n70.1\n62.2\n41.4\n64.8\n46.0\n50.7\nAdditive VPA\n79.8\n71.2\n52.1\n72.5\n52.8\n59.2\nPrependitive VPA\n79.8\n71.0\n51.2\n72.4\n52.5\n58.7\nEpisodic SIA\nMEMO\n80.1\n72.0\n53.9\n72.6\n53.0\n59.8\nAdditive VPA\n80.0\n72.1\n50.3\n72.3\n52.5\n58.4\nPrependitive VPA\n80.0\n72.5\n54.2\n72.7\n53.2\n60.0\nSource\nCLIP-ResNet50\u00d74\n78.9\n67.5\n36.7\n64.0\n49.5\n50.1\nEpisodic BIA\nBN\n78.3\n67.5\n27.2\n55.1\n40.5\n40.9\nTENT (Norm Layer)\n78.1\n67.5\n27.3\n55.2\n40.4\n41.0\nTENT (All Parameters)\n79.1\n68.2\n37.2\n64.4\n49.9\n50.5\nDDA\n69.0\n61.1\n24.6\n56.3\n41.0\n40.6\nAdditive VPA\n79.1\n68.6\n37.9\n65.0\n49.9\n50.9\nThe objective of VPA under SIA is formulated as:\n\u02c6\ud835\udc77 = arg min\n\ud835\udc77\n\ud835\udc3b ( 1\n\ud835\udf02\ud835\udc3e\n\ud835\udc3e\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc53 (\ud835\udc46(A\ud835\udc56 (\ud835\udc99\ud835\udc56),\ud835\udf02) \u2295 \ud835\udc77,\ud835\udf0f)\n\ud835\udc46(\ud835\udc99\ud835\udc56,\ud835\udf02) = \ud835\udc99\ud835\udc56 \u00b7 1[\ud835\udc3b (\ud835\udc9b\ud835\udc56,\ud835\udf0f) \u2264 arg top-\ud835\udf02\ud835\udc3e{\ud835\udc3b (\ud835\udc9b,\ud835\udf0f)}]\n(5)\nwhere A denotes a random augmentation function, \ud835\udc3e is the aug-\nmented batch size, and \ud835\udc46() is a confidence selection function to\npick augmented images with high confidence with a percentile of\n\ud835\udf02, following the setting in [49]. The intuition behind SIA is to use\nmarginal entropy minimization on the augmented input to optimize\nthe prompt and enhance generalization.\nDuring both BIA and SIA, the visual prompt \ud835\udc77 is optimized by\ncomputing the gradient of the entropy loss w.r.t. \ud835\udc77 (i.e., \ud835\udf15\ud835\udc3b\n\ud835\udf15\ud835\udc77 ) during\nthe backward pass. As self-entropy only relies on the network\u2019s\npredictions without labels or source domain information, and our\nvisual prompt \ud835\udc77 is independent of model parameters, VPA achieves\nfully test-time adaptation. Additionally, besides episodic test-time\nadaptation, we also explore the application of continual online\nlearning in test-time adaptation.\nPseudo-Labeling Adaptation (PLA). Pseudo-labeling has been\nwidely adopted in semi- and self-supervised learning. In this work,\nwe adopt pseudo-labeling for test-time adaptation, which requires\nmore setup than BIA and SIA. To implement this approach, we\nuse a memory queue M with size \ud835\udc60 that stores the final \ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc41\ntoken, along with the prediction of historic data \ud835\udc9b\ud835\udc56 for reference\nwhen processing an incoming batch, i.e., M = {\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc41 \ud835\udc56, \ud835\udc9b\ud835\udc56}\ud835\udc60\n1. During\nadaptation, we generate reference labels for the incoming test data\nusing its \ud835\udc58 nearest-neighbor (\ud835\udc58NN) predictions on the \ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc41 token\nbefore feeding into the head classifier. We then average the \ud835\udc58NN\npredictions to produce the eventual pseudo label. We apply weak\nand strong augmentations to every incoming data sample inspired\nby FixMatch [50]. Specifically, we obtain the soft predictions for the\nweakly and strongly augmented samples, denoted as \ud835\udc9bW and \ud835\udc9bS,\nrespectively. We then generate the pseudo label for the incoming\ndata based on the soft voting mechanism in our memory queue:\n\u02c6\ud835\udc9b\ud835\udc56 = 1\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc57 \u2208\ud835\udc58NN\ud835\udc56\n\ud835\udc9bW \ud835\udc57\n(6)\nWe apply cross-entropy minimization as our objective, utilizing a\ntemperature hyper-parameter on the generated soft pseudo label:\n\ud835\udc3b (\ud835\udc9bS,\ud835\udf0f) = \u2212\n\ud835\udc50\u2211\ufe01\n\ud835\udc56=1\n\ud835\udf0e(\u02c6\ud835\udc9b/\ud835\udf0f)\ud835\udc56 log\ud835\udf0e(\ud835\udc9bS)\ud835\udc56\n\ud835\udc9b = \ud835\udc53 (\ud835\udc99 \u2295 \ud835\udc77)\n(7)\n\u02c6\ud835\udc77 = arg min\n\ud835\udc77\n\ud835\udc3b (\ud835\udc53 (\ud835\udc99\ud835\udc56 \u2295 \ud835\udc77),\ud835\udf0f)\n(8)\nFinally, we dynamically update the \ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc41 token along with its pre-\ndiction \ud835\udc9bW into our memory queue for next-round adaptation.\nSimilarly, the visual prompt \ud835\udc77 is optimized by computing the gradi-\nent of the cross-entropy loss.\nConference\u201917, July 2017, Washington, DC, USA\nJiachen Sun et al.\nTable 2: Corruption Robustness Evaluation Results (%) of VPA on ImageNet-C with the Highest Severity Level.\nError Rate (\u2193)\nMethod\nGauss. Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixelate JPEG Average\nSource\nViT\n52.5\n51.7\n51.5\n56.0\n69.2\n51.0\n56.1\n46.3 50.3 45.6 24.9\n69.1\n55.1\n34.2\n33.7\n49.1\nEpisodic SIA\nMEMO\n48.5\n47.8\n45.8\n58.2\n68.0\n53.5\n57.5\n42.7 48.2 41.8 22.2\n68.9\n52.0\n32.1\n27.9\n47.7\nPrependitive VPA\n48.2\n47.5\n45.4\n58.0\n67.6\n53.2\n56.0\n41.5 48.3 41.7 22.5\n67.5\n52.2\n31.9\n27.5\n47.3\nContinual BIA\nTENT (Norm Layer) 47.7\n49.0\n45.5\n59.4\n68.1\n49.0\n55.9\n49.5 48.6 42.0 22.3\n62.1\n52.1\n32.3\n27.5\n47.4\nAdditive VPA\n48.5\n49.1\n45.7\n58.9\n67.5\n48.8\n55.8\n56.1 49.7 42.1 22.1\n61.8\n52.5\n32.3\n27.7\n47.9\nPrependitive VPA\n47.0\n48.5\n44.2\n56.8\n65.7\n48.2\n55.5\n48.2 48.0 40.1 21.8\n61.4\n51.3\n31.2\n27.2\n46.5\nContinual PLA\nAdaContrast\n45.8\n44.7\n44.5\n47.2\n57.8\n41.8\n46.0\n35.2 39.8 34.8 22.8\n47.5\n40.2\n28.5\n29.5\n40.4\nCFA\n43.1\n42.0\n41.9\n45.6\n51.1\n40.1\n43.4\n33.6 35.9 32.3 21.0\n41.2\n35.7\n28.3\n29.8\n37.6\nPrependitive VPA\n46.7\n44.7\n43.9\n42.0\n44.5\n38.9\n43.0\n31.0 33.2 28.5 22.9\n37.1\n31.8\n28.4\n30.0\n36.6\n3.3\nAdaptation Setups\nIn our study, we primarily utilize the ViT-B/16 model architecture,\nwhich is now considered a standard benchmarking model [12].\nAdditionally, we use ResNet [22] to demonstrate the general effec-\ntiveness of additive VPA. We attach additive prompts to the 1st and\n6th Transformer layers in ViT, which are in total 196 \u00d7 2 = 392\ntokens. Similarly. we insert 50 adaptable prompts into every other\nlayer of the ViT-B architecture, resulting in 300 tokens, for prepen-\nditive prompting. By default, we set the value of \ud835\udf0f to 1.0 in episodic\nadaptation, and we analyze its impact in Section 4.4 to demon-\nstrate that an optimal \ud835\udf0f can lead to further improvements. We\nfollow TENT [61] and set 10 adaptation steps for each batch. We\nalso ablate the number of steps in Section 4.4. We empirically set\nthe learning rate to 4.0 and 0.001 with SGD [47] for additive and\nprependitive prompting, respectively. We use a batch size of 64 for\nall experiments in Section 4. For SIA, we utilize random cropping [2]\nas the augmentation function A and W. In the experimentation of\nPLA, we set \ud835\udf0f = 0.07 and the memory queue size to 1% of the test\ndataset by default. We utilize \ud835\udc58 = 11 similarly to AdaContrast [6].\nMoreover, we leverage random cropping and RandAugment [8] as\nthe weak (W) and strong (S) augmentations, respectively. Most im-\nportantly, we compare both episodic and continual learning settings\nin TENT [61] with our VPA in this study.\n4\nEXPERIMENTS AND RESULTS\nThis section reports on the experimental results of VPA and several\nother baseline methods across multiple benchmarks. As previously\nnoted in \u00a7 1, we commence by assessing the performance of VPA in\nthe presence of challenging distribution shifts (i.e., OOD generaliza-\ntion) in \u00a7 4.1. Following this, we delve into exploring the potential\nof VPA in enhancing robustness against common corruptions in\n\u00a7 4.2. Lastly, we evaluate the performance of VPA on the domain\nadaptation task, alongside other baseline methods, in \u00a7 4.3.\n4.1\nEvaluation of OOD Generalization\nExperimental Setups. We select models pre-trained with CLIP [45],\nand leverage two SOTA robust fine-tuning methods (i.e., LPFT [34]\nand WiSE [62]) to train them on the ImageNet training set. To\nstudy model robustness to realistic OOD data that naturally occurs\nin the physical world, we leverage ImageNet-A [26], ImageNet-\nR [23], and ObjectNet [4]. ImageNet-A consists of 7,500 test images\ndenoted as \u201cnatural adversarial examples\u201d that are misclassified\nby a collection of standard models overlapped with 200 ImageNet\ncategories. ImageNet-R collects 30,000 images of 200 ImageNet cat-\negories with artistic renditions. ObjectNet is a large real-world test\nset for object recognition with control where object backgrounds,\nrotations, and imaging viewpoints are random. We chose the Object-\nNet subset that overlaps 113 classes with ImageNet in our study. We\nalso use ImageNet-V2 [46], a robustness benchmark with mild dis-\ntribution shift, to further validate our results. As these challenging\ndatasets do not follow specific distribution patterns, research has\nshown that continual learning may not be effective in improving ro-\nbustness [6, 61]. Therefore, we evaluate VPA under episodic BIA and\nSIA using different prompt types and measure the in-distribution\n(ID) and OOD accuracy as the metrics for our evaluation.\n4.1.1\nEvaluation on Foundation Models. We present our large-scale\nevaluation of models fine-tuned with different methods in this\nstudy. Specifically, we utilize additive and prependitive prompting\nfor VPA under BIA and SIA, respectively. As ViT models do not\nuse BatchNorm layers, we default to adapting the LayerNorm lay-\ners for TENT. We also experiment with adapting the classification\ntoken \ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc41 and all model parameters for additional comparisons,\nfollowing the settings in [32]. Table 1 shows the experimental re-\nsults of different methods under BIA and SIA. We observe that\nTENT (Norm) only achieves slight improvements against natural\ndistribution shifts compared to the source-only baseline. While\nthe LayerNorm layer is a linear module that is preferred by the\nadaptation assumption in TENT [61], it is independent of the in-\nput data. Additionally, natural OOD data does not follow a clear\ndistributional pattern, unlike synthesized corruptions. Therefore,\nthe benefits of linear module adaptation do not transfer to our set-\nting [61]. Similarly, applying TENT to other model parameters does\nnot show sensible improvements over the baseline either, which is\nconsistent with prior studies [32]. In contrast, our VPA achieves\nvisible enhancements over the source-only method. Specifically,\nVPA relatively improves OOD robustness by 3.8% on average, while\nmaintaining or improving the (near) ID accuracy on ImageNet and\nImageNet-V2 for all chosen pre-trained models. Conversely, TENT\ndegrades the original performance on (near) ID data in some cases.\nWe also evaluate VPA and MEMO under SIA and find that both\nmethods achieve around 2.7% improvements over the source-only\nmethod (Table 1). While MEMO adapts the whole backbone, VPA\nachieves more efficient prompt-level adaptation.\nOur results show that, unlike under BIA where additive and\nprependitive prompting achieve similar adaptation performance,\nthe prependitive VPA performs better under SIA. As discussed in\nVPA: Fully Test-Time Visual Prompt Adaptation\nConference\u201917, July 2017, Washington, DC, USA\nTable 3: Domain Adaptation Evaluation Results (%) of VPA on DomainNet-126 (S: Sketch, R: Real, C: Clipart, I: Infograph, Q:\nQuickdraw, and P: Painting).\nAccuracy (\u2191)\nMethod\nS\u2192R\nS\u2192C\nS\u2192I\nS\u2192Q\nS\u2192P\nR\u2192S\nR\u2192C\nR\u2192I\nR\u2192Q\nR\u2192P\nAverage\nSource\nCLIP-ViT-LPFT\n67.5\n69.2\n35.5\n16.4\n52.8\n55.2\n68.6\n44.4\n9.7\n60.9\n48.0\nEpisodic SIA\nMEMO\n67.8\n69.5\n35.4\n16.8\n53.2\n55.5\n68.9\n44.8\n10.1\n61.3\n48.3\nPrependitive VPA\n68.1\n70.1\n36.5\n17.3\n53.6\n55.8\n69.5\n45.0\n10.6\n61.5\n48.8\nContinual BIA\nTENT (Norm Layer)\n67.7\n69.0\n35.8\n16.6\n53.0\n55.3\n68.9\n44.5\n9.9\n61.0\n48.2\nPrependitive VPA\n68.7\n69.8\n36.3\n16.8\n53.2\n55.8\n69.0\n44.8\n10.1\n61.3\n48.6\nContinual PLA\nAdaContrast\n70.2\n72.0\n36.5\n18.0\n54.2\n58.4\n71.8\n45.5\n10.5\n63.3\n50.0\nCFA\n69.8\n72.2\n35.9\n17.8\n54.4\n58.7\n71.6\n45.7\n10.3\n63.0\n49.9\nPrependitive VPA\n70.8\n73.0\n37.0\n17.8\n55.3\n58.8\n71.9\n46.4\n10.4\n63.8\n50.5\n\u00a7 3.2 and illustrated in Figure 1, SIA realizes test-time adaptation\nvia augmentations over a single image. The workflow of SIA is as\nfollows: 1) expand a single image as a mini-batch through augmen-\ntations for self-entropy minimization, 2) discard the expanded batch\nafter adaptation, and 3) use the original single image and adapted\nmodel/prompt for inference. This setting is designed for model-level\nadaptation, as the model is trained to be insensitive to input aug-\nmentations. However, additive prompts are directly added on top of\nthe input with a small magnitude, which makes their effectiveness\nsensitive to any change in the images. The prompt is adapted to\nthe augmented batch but added to the original image in SIA, re-\nsulting in reduced effectiveness. On the other hand, prependitive\nprompting does not directly modify the semantics of the input, and\nthe optimization over a single image is easier to converge than BIA\nwith an appropriate prompt length. Our experiments show that\nprependitive prompting achieves a 3.3% improvement under SIA,\nwhile additive prompting does not provide a tangible enhancement\ncompared to the source-only baseline method.\nWe conduct another experiment of VPA on the ResNet architec-\nture. Specifically, we use ResNet50x4 pre-trained with CLIP, and\nTable 1 presents the evaluation results, where VPA achieves similar\nimprovement on the ResNet model [62]. Surprisingly, we find that\nTENT has an around 20% performance drop on ResNet50x4. TENT,\nby default, replaces the original statistics (i.e., \ud835\udf07 and \ud835\udf0e) in BatchNorm\nlayers with the statistics of the input data. This setting is useful\nwhen the input batch is from one specific domain (e.g., synthesized\ncorruptions). However, natural distribution shifts do not follow\nsuch assumptions, rendering significant performance degradation.\nIn comparison, VPA consistently achieves better OOD robustness\non different fine-tuning methods and model architectures.\n4.2\nEvaluation of Corruption Robustness\nExperimental Setups. In this section, we evaluate the perfor-\nmance of VPA against common corruptions using the ImageNet-C\ndataset [24]. ImageNet-C is designed to assess the robustness and\ngeneralization capabilities of computer vision models by introduc-\ning 15 different corruptions at five severity levels to the original\nImageNet validation dataset. These corruptions include various\ntypes of noise, blur, and distortion, making ImageNet-C a more\nrealistic and challenging test of model robustness and generaliza-\ntion. We adopt the methodology from [6], focusing on the highest\ncorruption severity level\u2019s performance. Our primary interest lies\nin the continual adaptation setting, as it has been shown to be\nmore effective in enhancing corruption robustness; this is because\neach corruption can be considered as being drawn from a similar\ndistribution shift. Based on our experiments, which demonstrated\nthat the prependitive prompt outperforms the additive design un-\nder SIA, we employ only prependitive prompts for this evaluation.\nAs foundation models do not show visible improvements on cor-\nruption robustness benchmarks, we use the ViT model pretrained\non ImageNet in this section. In addition, we compare VPA with\nMEMO, a SOTA episodic adaptation method. Error rate serves as\nour evaluation metric for assessing corruption robustness.\nTable 2 presents the experimental results for the highest sever-\nity level, demonstrating that our prependitive VPA consistently\nachieves the best robustness improvement among all the baselines.\nImportantly, VPA attains the highest robustness under episodic SIA\nand provides greater storage efficiency compared to MEMO. This\nefficiency results from VPA adapting only the additional prompts,\nwhereas MEMO adapts all the model parameters. For the evaluation\nunder continual BIA, our prependitive VPA outperforms TENT and\nthe additive design by 1.9% and 3.0%, respectively. This outcome\nmay be associated with the nature of corruption benchmarks. Addi-\ntive prompts are directly added to the embedding of the corrupted\ninput, while prependitive prompts do not directly mix with the\noriginal corrupted embedding. Instead, they leverage the attention\nmechanism to interact with the original embedding, leading to a\nbetter robustness gain. Continual PLA achieves the largest gain,\nbenefiting from the pseudo labels generated by the memory queue.\nOur prependitive VPA outperforms AdaContrast [6] and CFA [32]\nby relative margins of 10.4% and 2.7%, respectively.\n4.3\nEvaluation of Domain Adaptation\nExperimental Setups. In this section, we discuss our experiments\nand results related to the domain adaptation task. We employ the\nDomainNet-126 dataset [43] for this purpose. DomainNet encom-\npasses common objects from six domains (i.e., sketch, real, clipart,\ninfograph, quickdraw, and painting) and 345 categories. In our study,\nwe empirically use the sketch (S) and real (R) images as the training\nsets and evaluate the adaptation performance on the remaining\nsubsets. We leverage the ViT model pretrained with CLIP and fine-\ntuned by LPFT in this evaluation. For other setups, we follow the\nsame configuration as used in the corruption robustness evaluation.\nTable 3 displays the evaluation results for DomainNet-126, where\nwe observe that VPA consistently achieves the best robustness\nConference\u201917, July 2017, Washington, DC, USA\nJiachen Sun et al.\n(a) Different Number of Adaptation Steps.\n(b) Different Softmax Temperatures \ud835\udf0f.\nFigure 2: Ablation Studies of Visual Prompt Adaptation on OOD Generalization. We show that the first adaptation step\ncontributes the most in VPA and additional improvements can be achieved with an optimal temperature hyperparameter \ud835\udf0f.\nTable 4: Ablation Study of the Prompt Size in Episodic VPA.\nPerformance (%)\nPrompt Size\nOOD (\u2191)\nCorruption (\u2193)\nDA (\u2191)\nAdditive VPA\n196\n58.5\n48.3\n-\n392\n58.8\n47.9\n-\n588\n58.7\n48.1\n-\nPrependitive VPA\n150\n58.1\n47.0\n48.2\n300\n58.5\n46.5\n48.6\n450\n58.3\n46.8\n48.4\nacross various adaptation settings. DomainNet shares certain simi-\nlarities with ImageNet-C, as each domain exhibits a specific pattern.\nOn average, our VPA outperforms the source-only baseline by 5.2%.\nFurthermore, PLA-based VPA demonstrates more substantial im-\nprovements, with relative margins of 1.0% and 1.1% for AdaContrast\nand CFA, respectively. This highlights the effectiveness of our VPA\napproach in addressing domain adaptation challenges while main-\ntaining robust performance across different settings.\n4.4\nAblation Studies\nBesides visual prompt designs, this section provides an empiri-\ncal analysis of VPA on different hyper-parameter settings under\ndifferent adaptation settings.\nPrompt Size. We begin by conducting ablation studies on the\nprompt size utilized in additive and prependitive VPA. As outlined\nin \u00a7 3.3, we utilize 50 adaptable prompts in 6 layers as the default\nfor our prependitive VPA and 196 adaptable prompts in 2 layers\nof the ViT-B model for our additive VPA, leading to 300 and 392\nlearnable tokens, respectively. We vary the number of prompts\nadapted and assess the VPA\u2019s performance on OOD generalization\nand corruption robustness. Specifically, we vary the number of\nprompted layers for the additive VPA. The results are presented in\nTable 4. The evaluation highlights the existence of an optimal point\nfor the number of prompting tokens. Having too many prompts\ncan make it difficult to optimize, while a relatively small number of\nprompts may restrict the capability of VPA.\nAdaptation Steps and Temperature \ud835\udf0f. We then ablate the ef-\nfect of adaptation steps and the temperature parameter of additive\nepisodic adaptation. In Figure 2(a), the average OOD accuracy gen-\nerally improves as the number of steps increases. Encouragingly, we\nfind the first step of VPA contributes the most to the OOD robust-\nness improvement, where the relative improvements are 1.2% and\nTable 5: OOD Generalization Evaluation Results (%) of VPA\nfor Zero-Shot Recognition in the Vision-Language Model.\nImageNet ImageNet-V2 ImageNet-A ImageNet-R\nCLIP\n66.7\n60.9\n47.9\n74.0\nCLIP+TPT\n69.0\n63.4\n53.5\n76.5\nCLIP+TPT+VPA\n69.1\n63.7\n53.9\n77.0\n1.9% for models fine-tuned with LPFT and WiSE, respectively. In\ncontrast, one-step TENT adaptation shows no improvements over\nthe source-only baseline, demonstrating the effectiveness of VPA.\nFigure 2(b) shows that there is a sweet point for the temperature\nparameter for OOD robustness improvement: We find that with\nan optimal temperature, the OOD robustness of the LPFT model\ncould further improve by 1.5% on average. However, selecting an\noptimal \ud835\udf0f requires an additional validation set with access to the\nlabel, so we do not tune the temperature \ud835\udf0f in the central part of our\nevaluation. We leave this as a future study to automatically select\nthe temperature parameter for VPA.\nVision-Language Model. We evaluate the performance of our pro-\nposed VPA combined with TPT [49] on the zero-shot recognition\nof the CLIP model. We adopt the SIA experimental setup from [49].\nThe results in Table 5 demonstrate that our VPA consistently en-\nhances the zero-shot recognition performance on the challenging\nOOD generalization benchmarks by approximately 0.5%.\n5\nCONCLUDING REMARKS\nTo conclude, we propose VPA, a pioneering framework for general-\nizing visual prompting with test-time adaptation. VPA effectively\nimproves OOD generalization, corruption robustness, and domain\nadaptation performance across diverse settings and tasks. The ef-\nfectiveness of VPA highlights the potential of incorporating visual\nprompting in future research to address a wide array of adaptation\nchallenges.\nACKNOWLEDGMENTS\nWe appreciate the constructive feedback from the anonymous re-\nviewers. Jiachen Sun and Z. Morley Mao were partially supported\nby NSF under CNS-1930041, CMMI-2038215, and the National AI\nInstitute for Edge Computing Leveraging Next Generation Wireless\nNetworks, Grant # 2112562.\nVPA: Fully Test-Time Visual Prompt Adaptation\nConference\u201917, July 2017, Washington, DC, USA\nREFERENCES\n[1] 2022.\nMachine\nlearning\ninference\nduring\ndeployment.\nhttps:\n//learn.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/best-\npractices/ml-deployment-inference#batch-inference.\n[2] 2022. Random Cropping in Pytorch. https://pytorch.org/vision/main/generated/\ntorchvision.transforms.RandomCrop.html.\n[3] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. 2022.\nExploring visual prompts for adapting large-scale models.\narXiv preprint\narXiv:2203.17274 1, 3 (2022), 4.\n[4] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang,\nDan Gutfreund, Josh Tenenbaum, and Boris Katz. 2019. ObjectNet: A large-\nscale bias-controlled dataset for pushing the limits of object recognition\nmodels. In Advances in Neural Information Processing Systems, H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (Eds.),\nVol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/\n97af07a14cacba681feacf3012730892-Paper.pdf\n[5] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of\nNeural Networks. In 2017 IEEE Symposium on Security and Privacy (SP). 39\u201357.\nhttps://doi.org/10.1109/SP.2017.49\n[6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. 2022. Contrastive\nTest-Time Adaptation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 295\u2013305.\n[7] Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 15750\u201315758.\n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. 2020. Randaug-\nment: Practical automated data augmentation with a reduced search space. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\nworkshops. 702\u2013703.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\nA large-scale hierarchical image database. In 2009 IEEE conference on computer\nvision and pattern recognition. Ieee, 248\u2013255.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[11] Akshay Raj Dhamija, Manuel G\u00fcnther, and Terrance Boult. 2018. Reducing\nnetwork agnostophobia. Advances in Neural Information Processing Systems 31\n(2018).\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[13] Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Sch\u00f6lkopf.\n2021. Source-free adaptation to measurement shift via bottom-up feature restora-\ntion. arXiv preprint arXiv:2107.05446 (2021).\n[14] Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein. 2018. Ad-\nversarial reprogramming of neural networks. arXiv preprint arXiv:1806.11146\n(2018).\n[15] Yulu Gan, Yan Bai, Yihang Lou, Xianzheng Ma, Renrui Zhang, Nian Shi, and Lin\nLuo. 2023. Decorate the newcomers: Visual domain prompt for continual test\ntime adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence,\nVol. 37. 7595\u20137603.\n[16] Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, and Dequan\nWang. 2022. Back to the Source: Diffusion-Driven Test-Time Adaptation. arXiv\npreprint arXiv:2207.03442 (2022).\n[17] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu\nLi, and Dimitris N Metaxas. 2022. Visual Prompt Tuning for Test-time Domain\nAdaptation. arXiv preprint arXiv:2210.04831 (2022).\n[18] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and Zico Kolter. 2022. Test-time\nadaptation via conjugate pseudo-labels. arXiv preprint arXiv:2207.09640 (2022).\n[19] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt: Pre-trained\nprompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332 (2021).\n[20] Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. Ptr:\nPrompt tuning with rules for text classification. arXiv preprint arXiv:2105.11259\n(2021).\n[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 16000\u201316009.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition. 770\u2013778.\n[23] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan\nDorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob\nSteinhardt, and Justin Gilmer. 2021. The Many Faces of Robustness: A Critical\nAnalysis of Out-of-Distribution Generalization. ICCV (2021).\n[24] Dan Hendrycks and Thomas Dietterich. 2019.\nBenchmarking neural net-\nwork robustness to common corruptions and perturbations. arXiv preprint\narXiv:1903.12261 (2019).\n[25] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and\nBalaji Lakshminarayanan. 2019. Augmix: A simple data processing method to\nimprove robustness and uncertainty. arXiv preprint arXiv:1912.02781 (2019).\n[26] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.\n2021. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 15262\u201315271.\n[27] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In International conference\non machine learning. PMLR, 448\u2013456.\n[28] Yusuke Iwasawa and Yutaka Matsuo. 2021. Test-time classifier adjustment mod-\nule for model-agnostic domain generalization. Advances in Neural Information\nProcessing Systems 34 (2021), 2427\u20132440.\n[29] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie,\nBharath Hariharan, and Ser-Nam Lim. 2022. Visual prompt tuning. arXiv preprint\narXiv:2203.12119 (2022).\n[30] Youngeun Kim, Donghyeon Cho, Kyeongtak Han, Priyadarshini Panda, and\nSungeun Hong. 2021. Domain adaptation without source data. IEEE Transactions\non Artificial Intelligence 2, 6 (2021), 508\u2013518.\n[31] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin\nZhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas\nPhillips, Irena Gao, et al. 2021. Wilds: A benchmark of in-the-wild distribution\nshifts. In International Conference on Machine Learning. PMLR, 5637\u20135664.\n[32] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Robustifying Vision\nTransformer without Retraining from Scratch by Test-Time Class-Conditional\nFeature Alignment. arXiv preprint arXiv:2206.13951 (2022).\n[33] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan\nGulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me\nanything: Dynamic memory networks for natural language processing. In Inter-\nnational conference on machine learning. PMLR, 1378\u20131387.\n[34] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.\n2022. Fine-tuning can distort pretrained features and underperform out-of-\ndistribution. arXiv preprint arXiv:2202.10054 (2022).\n[35] Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. 2020. Universal\nsource-free domain adaptation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 4544\u20134553.\n[36] Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks for images,\nspeech, and time series. The handbook of brain theory and neural networks 3361,\n10 (1995), 1995.\n[37] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature\n521, 7553 (2015), 436\u2013444.\n[38] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for\nparameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021).\n[39] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. 2020. Model\nadaptation: Unsupervised domain adaptation without source data. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9641\u2013\n9650.\n[40] Jian Liang, Dapeng Hu, and Jiashi Feng. 2020. Do we really need to access the\nsource data? source hypothesis transfer for unsupervised domain adaptation. In\nInternational Conference on Machine Learning. PMLR, 6028\u20136039.\n[41] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Gra-\nham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompt-\ning methods in natural language processing. arXiv preprint arXiv:2107.13586\n(2021).\n[42] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D\u2019Amour, Balaji Lakshmi-\nnarayanan, and Jasper Snoek. 2020. Evaluating prediction-time batch normal-\nization for robustness under covariate shift. arXiv preprint arXiv:2006.10963\n(2020).\n[43] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.\n2019. Moment matching for multi-source domain adaptation. In Proceedings of\nthe IEEE International Conference on Computer Vision. 1406\u20131415.\n[44] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and\nKate Saenko. 2017. Visda: The visual domain adaptation challenge. arXiv preprint\narXiv:1710.06924 (2017).\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from natural language supervision.\nIn International Conference on Machine Learning. PMLR, 8748\u20138763.\n[46] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019.\nDo imagenet classifiers generalize to imagenet?. In International Conference on\nMachine Learning. PMLR, 5389\u20135400.\n[47] Sebastian Ruder. 2016. An overview of gradient descent optimization algorithms.\narXiv preprint arXiv:1609.04747 (2016).\n[48] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel,\nand Matthias Bethge. 2020. Improving robustness against common corruptions\nby covariate shift adaptation. Advances in Neural Information Processing Systems\n33 (2020), 11539\u201311551.\nConference\u201917, July 2017, Washington, DC, USA\nJiachen Sun et al.\n[49] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anand-\nkumar, and Chaowei Xiao. 2022. Test-time prompt tuning for zero-shot general-\nization in vision-language models. arXiv preprint arXiv:2209.07511 (2022).\n[50] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,\nColin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. 2020.\nFixmatch: Simplifying semi-supervised learning with consistency and confidence.\nAdvances in neural information processing systems 33 (2020), 596\u2013608.\n[51] Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z. Morley Mao. 2020.\nTo-\nwards Robust LiDAR-based Perception in Autonomous Driving: General Black-\nbox Adversarial Sensor Attack and Countermeasures. In 29th USENIX Secu-\nrity Symposium (USENIX Security 20). USENIX Association, 877\u2013894.\nhttps:\n//www.usenix.org/conference/usenixsecurity20/presentation/sun\n[52] Jiachen Sun, Yulong Cao, Christopher B Choy, Zhiding Yu, Anima Anandku-\nmar, Zhuoqing Morley Mao, and Chaowei Xiao. 2021. Adversarially robust 3d\npoint cloud recognition using self-supervisions. Advances in Neural Information\nProcessing Systems 34 (2021), 15498\u201315512.\n[53] Jiachen Sun, Karl Koenig, Yulong Cao, Qi Alfred Chen, and Z Morley Mao. 2020.\nOn adversarial robustness of 3d point cloud classification under adaptive attacks.\narXiv preprint arXiv:2011.11922 (2020).\n[54] Jiachen Sun, Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, Dan Hendrycks,\nJihun Hamm, and Z Morley Mao. 2022. A Spectral View of Randomized Smoothing\nUnder Common Corruptions: Benchmarking and Improving Certified Robustness.\nIn European Conference on Computer Vision. Springer, 654\u2013671.\n[55] Jiachen Sun, Weili Nie, Zhiding Yu, Z Morley Mao, and Chaowei Xiao. 2022.\nPointdp: Diffusion-driven purification against adversarial attacks on 3d point\ncloud recognition. arXiv preprint arXiv:2208.09801 (2022).\n[56] Jiachen Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, and\nZ Morley Mao. 2022. Benchmarking robustness of 3d point cloud recognition\nagainst common corruptions. arXiv preprint arXiv:2201.12296 (2022).\n[57] Jiachen Sun, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Z Morley Mao,\nand Chaowei Xiao. 2023. CALICO: Self-Supervised Camera-LiDAR Contrastive\nPre-training for BEV Perception. arXiv preprint arXiv:2306.00349 (2023).\n[58] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz\nHardt. 2019. Test-time training for out-of-distribution generalization. (2019).\n[59] Fatemeh Vakhshiteh, Ahmad Nickabadi, and Raghavendra Ramachandra. 2021.\nAdversarial attacks against face recognition: A comprehensive study. IEEE Access\n9 (2021), 92735\u201392756.\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[61] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor\nDarrell. 2020. Tent: Fully test-time adaptation by entropy minimization. arXiv\npreprint arXiv:2006.10726 (2020).\n[62] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Korn-\nblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi,\nHongseok Namkoong, et al. 2022. Robust fine-tuning of zero-shot models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n7959\u20137971.\n[63] Marvin Zhang, Sergey Levine, and Chelsea Finn. 2021. Memo: Test time robust-\nness via adaptation and augmentation. arXiv preprint arXiv:2110.09506 (2021).\n[64] Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred Chen, and Z Morley\nMao. 2022. On adversarial robustness of trajectory prediction for autonomous\nvehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 15159\u201315168.\n[65] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and\nTao Kong. 2021. ibot: Image bert pre-training with online tokenizer. arXiv preprint\narXiv:2111.07832 (2021).\n[66] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Conditional\nprompt learning for vision-language models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 16816\u201316825.\n[67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning\nto prompt for vision-language models. International Journal of Computer Vision\n130, 9 (2022), 2337\u20132348.\n[68] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired\nimage-to-image translation using cycle-consistent adversarial networks. In Pro-\nceedings of the IEEE international conference on computer vision. 2223\u20132232.\nVPA: Fully Test-Time Visual Prompt Adaptation\nConference\u201917, July 2017, Washington, DC, USA\nAPPENDICES\nA\nDISCUSSION\nIn this paper, we have investigated the application of visual prompt-\ning in fully online test-time adaptation. Although prompting has\nbeen extensively studied in NLP tasks and has recently gained\nsignificant attention for improving the zero-shot performance of\nvision-language models, its exploration in vision systems remains\nlimited. In NLP tasks, prompted embedding tends to lose seman-\ntics after tuning, unlike na\"ive prompt engineering [67]. Similarly,\nexplaining the operational principle of visual prompting remains\nchallenging. As introduced in [14], a more general interpretation\nof visual prompting involves reprogramming a well-trained vi-\nsion model to achieve any deterministic goal. Despite the inherent\nlimitations of fully test-time visual prompting, we have explored\nits application across three critical aspects of real-world machine\nlearning systems: OOD generalization, corruption robustness, and\ndomain adaptation, demonstrating its effectiveness. Our experimen-\ntal results generally show that prependitive VPA is more effective,\nwhereas additive VPA is more universal to different model architec-\ntures. We believe visual prompting could also be applied to other\ntasks as ViT architectures advance and become more dominant.\nAnother avenue for future research is prompt design. In this work,\nwe have explored four combinations of prompting setups in \u00a7 3.1;\nadditional design choices, including image-to-image models [68]\nand embedding-space prompting [29], could yield intriguing appli-\ncations in the future. It is important to note that our study\u2019s focus\nis not solely on achieving state-of-the-art results but rather on ex-\namining how visual prompting performs within the fully test-time\nadaptation framework. We are aware of a concurrent, yet unpub-\nlished work, DePT [17], which investigates a similar topic. DePT\nprimarily focuses on offline test-time adaptation, where the model\nis adapted offline. Offline adaptation offers more room for sophis-\nticated prompting designs. We also noticed another concurrent\nwork [15] that studies a similar problem. We believe that all three\nstudies are complementary, each providing valuable insights into\ndifferent aspects of test-time adaptation using visual prompting.\nB\nABLATION STUDIES\nWe further conducted two ablation studies on the augmentation\nand \ud835\udc58NN soft majority voting in PLA.\nTable 6: Ablation Study on Augmentation Method in VPA.\nPLA (%)\nImageNet-C (\u2193)\nDomainNet-126 (\u2191)\nRandAugment\n36.6\n50.5\nAugMix\n35.7\n51.1\nIn single-image adaptation (SIA), we employ random cropping\nas the preferred augmentation technique. The weak augmentation\nemployed in pseudo-label adaptation also utilizes random crop-\nping, whereas the strong augmentation is constructed based on\nRandAugment [8]. We here utilize AugMix [25] to evaluate the\nimpact of distinct augmentation techniques. Our findings reveal\nthat the usage of AugMix can further boost the performance of\nVisual Prompting Adaptation (VPA) within the pseudo-label adapta-\ntion framework by approximately 0.8%, as shown in Table 6. These\nfindings illustrate the considerable potential for performance en-\nhancement through the thoughtful selection and application of\nimage augmentation techniques.\nWe default to leverage \ud835\udc58 = 11 in the soft majority voting. We\nfurther ablate the importance of \ud835\udc58 in this experiment. As presented\nin Table 7, \ud835\udc58 is indeed an essential hyper-parameter in pseudo-label\nadaptation. We find that \ud835\udc58 \u2208 [11, 15] generally achieves the highest\nperformance gain.\nTable 7: Ablation Study on \ud835\udc58NN Soft Majority Voting.\nPLA (%)\nImageNet-C (\u2193)\n\ud835\udc58 = 3\n44.1\n\ud835\udc58 = 7\n39.0\n\ud835\udc58 = 11\n36.6\n\ud835\udc58 = 15\n36.4\n\ud835\udc58 = 21\n38.2\n"
  }
]