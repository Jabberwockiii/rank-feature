[
  {
    "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
    "link": "https://arxiv.org/pdf/2310.18356.pdf",
    "upvote": "22",
    "text": "LoRAShear: Efficient Large Language Model Structured Pruning and\nKnowledge Recovery\nTianyi Chen 1 Tianyu Ding 1 Badal Yadav 1 Ilya Zharkov 1 Luming Liang 1\n1Microsoft\n{tiachen,tianyuding,bayadav,zharkov,lulian}@microsoft.com\nAbstract\nLarge Language Models (LLMs) have trans-\nformed the landscape of artificial intelligence,\nwhile their enormous size presents significant\nchallenges in terms of computational costs. We\nintroduces LoRAShear, a novel efficient ap-\nproach to structurally prune LLMs and recover\nknowledge. Given general LLMs, LoRAShear at\nfirst creates the dependency graphs over LoRA\nmodules to discover minimally removal struc-\ntures and analyze the knowledge distribution.\nIt then proceeds progressive structured pruning\non LoRA adaptors and enables inherent knowl-\nedge transfer to better preserve the information\nin the redundant structures. To recover the lost\nknowledge during pruning, LoRAShear metic-\nulously studies and proposes a dynamic fine-\ntuning schemes with dynamic data adaptors to\neffectively narrow down the performance gap to\nthe full models. Numerical results demonstrate\nthat by only using one GPU within a couple of\nGPU days, LoRAShear effectively reduced foot-\nprint of LLMs by 20% with only 1.0% perfor-\nmance degradation and significantly outperforms\nstate-of-the-arts. (Code will be public soon.)\n1. Introduction\nThe advent of Large Language Models (LLMs) (Zhao\net al., 2023; Hadi et al., 2023) has marked a significant\nmilestone in evolution of artificial intelligence. These mod-\nels, distinguished by their extensive parameter sizes, have\ndemonstrated emergent abilities (Wei et al., 2022), cat-\nalyzing breakthroughs not only in the realm of natural\nlanguage processing but also across tasks in various do-\nmains (Driess et al., 2023). This has opened up new pos-\n1Microsoft, Redmond WA 98052, United States.\nCorre-\nspondence to: Tianyi Chen <tiachen@microsoft.com>, Luming\nLiang <lulian@microsoft.com>.\nPreprint and ongoing work.\nsibilities for advancing towards Artificial General Intelli-\ngence (AGI) (Everitt et al., 2018; Bubeck et al., 2023).\nHowever, the enormous size of LLMs, typically ranging\nfrom tens to hundreds of billions of parameters (Touvron\net al., 2023), incurs substantial computational costs of both\nprocessing power and memory requirements.\nStructured pruning is an effective way to deliver compact\nDNNs via identifying and removing redundant structures\nthen recovering the lost knowledge (Han et al., 2015; Chen\net al., 2021b).\nHowever, its application onto LLMs is\nfacing significant challenges, due to the requirements of\nmassive computational resources and the unavailable train-\ning datasets of both pretraining and instructed fine-tuning\ndatasets (Brown et al., 2020). Consequently, the paradigms\ncould be largely categorized as pruning under limited or\nfull resources. For the limited-resource setup, recent prun-\ning works (Ma et al., 2023; Zhang et al., 2023; Sun et al.,\n2023) uses Low-Rank-Adaptor (LoRA) (Hu et al., 2021)\nduring either pruning and instructed fine-tuning stage to\nreduce the resource requirements, yet still face significant\nperformance degradation to the full LLMs. For the full-\nresouce setup, Sheared-LLaMA (Xia et al., 2023) conducts\nstructured pruning on the original LLMs to directly achieve\ncompact counterparts outperforming the equal sizes of\nLLMs trained from scratch, while requires significant GPU\npowers that might be not feasible for the public users.\nWe propose LoRAShear, a novel structured pruning frame-\nwork for LLMs in the limited-resource setup to signifi-\ncantly bring down the performance gap between pruned\nLLMs to their full versions.\nCompared with the exist-\ning works, LoRAShear has two main advantages to better\npreserve and recover the lost knowledge during pruning.\nFirstly, we proposed a novel Lora Half-Space Projected\nGradient (LHSPG) to enable progressive structured prun-\ning with inherent knowledge transfer over LoRA modules.\nSecondly, we propose a dynamic knowledge recovery stage\nto perform multi-stage fine-tuning in manner of both pre-\ntraining and instructed fine-tuning. Additionally, LoRAS-\nhear is applicable onto general LLMs with conducting de-\npendency graph analysis over LLMs with LoRA modules\n1\narXiv:2310.18356v2  [cs.CL]  31 Oct 2023\nPreprint version.\nMinimally Removal\nStructures Discovery\nProgressive Structured\nPruning via LHSPG\nKnowledge Distribution\nAnalysis\nDynamic Knowledge\nRecovery\n20%\n80%\n30%\nFigure 1. Overview of LoRAShear. Given a general LLM, LoRAShear at first discovers the minimally removal structures, then analyzes\nthe knowledge distribution to mark the crucial ones as unprunable, then performs progressive structurally pruning over the prunable\nstructures via LHSPG, and finally recovers the lost knowledge to recap the performance gap to the full LLM.\nupon (Chen et al., 2023b).\nWe now summarize our main contributions as follows.\n\u2022 Dependency Graph Analysis on LLMs with LoRA\nModules.\nTo automatically structurally prune general\nLLMs, discovering the minimally removal structures is\nnecessary via dependency graph analysis. LLMs with\nLoRA poses additional challenges, since target structures\nare non-trainable yet auxiliary LoRA modules are learn-\nable, which are ignored by existing algorithms (Chen\net al., 2023b; Ma et al., 2023). We propose a novel graph\nalgorithm to construct dependency graphs composed by\noverlapping node groups and composed node groups, and\npartition the trainable variables accordingly.\n\u2022 Progressive Structured Pruning via LHSPG. We pro-\npose a novel structured sparsity optimization algorithm\nLoRA Half-Space Projected Gradient (LHSPG) to per-\nform progressive structured pruning. LHSPG leverages\nthe information from LoRA modules and effectively pro-\nduces desired structured sparsity over the original vari-\nables. LHSPG transfers the knowledge stored in the rela-\ntively redundant structures to the important structures to\nbetter preserve the knowledge of the pretrained LLMs.\n\u2022 Dynamic Knowledge Recovery.\nTo further recover\nthe knowledge after progressive pruning, we propose a\ndynamic knowledge recovery mechanism. Rather than\nonly engaging the instructed fine-tuning as the existing\nlimited-resource pruning works, we adaptively construct\na subset from pretraining datasets upon the performance\ndistribution to recover the lost general knowledge dur-\ning pruning. We then perform the usual instructed fine-\ntuning to recover domain-specific expertise and the in-\nstruction capacity of pruned LLMs.\n\u2022 Experimental Results. We demonstrate the effective-\nness of LoRAShear on open-source LLAMAv1. By us-\ning one A100 GPU within a couple of GPU days, com-\npared to the full model, the 20% pruned LLAMAv1 neg-\nligibly regresses 1% performance, and the 50% pruned\nLLAMAv1 preserves 82% performance on the evalua-\ntion benchmarks. Meanwhile, our results significantly\noutperform the existing state-of-the-arts.\n2. Related Work\nWhile pruning (Han et al., 2015) is well-established in tra-\nditional Deep Neural Networks (DNNs), its application to\nLLMs presents unique challenges. Unlike the smaller, task-\nspecific DNNs (Ding et al., 2021; 2022), LLMs have a\nlarge number of parameters and require significant compu-\ntational resources (Brown et al., 2020). Moreover, it\u2019s cru-\ncial for them to generalize well across multiple tasks (Xia\net al., 2023). Recently, various pruning methods have been\ndeveloped specifically for LLMs, generally falling into two\nmain categories: unstructured and structured.\nUnstructured Pruning.\nUnstructured pruning meth-\nods (Dong et al., 2017; Chen et al., 2020; 2021a) focus\non setting unimportant individual weights in the model to\nzero.\nThis fine-grained approach is straightforward and\noften maintains good performance, even with high com-\npression rates. However, it results in sparse weight matri-\nces that aren\u2019t well-suited for hardware accelerators, mak-\ning them less efficient in real-world deployment. In the\nrealm of LLMs, several new techniques have emerged.\nSparseGPT (Frantar & Alistarh, 2023) uses a sophisticated\nweight update process involving synchronized second-\norder Hessian updates, bypassing traditional retraining. In\ncontrast, Wanda (Sun et al., 2023) achieves high sparsity\nwithout any retraining, simply by pruning weights with\nthe smallest magnitudes multiplied by their corresponding\ninput activations.\nPST (Li et al., 2022), however, com-\nbines unstructured pruning with efficient fine-tuning, prun-\ning both LoRA and pre-trained model weights. A drawback\nof this method is the need for a memory-intensive mask that\nmatches the shape of the pre-trained weights.\nStructured Pruning. Structured pruning methods (Chen\net al., 2021b; 2023a;b) focus on removing entire groups\nof parameters, such as neurons or layers, rather than in-\n2\nPreprint version.\ndividual weights. This group-level approach is hardware-\nfriendly as it maintains dense weight matrices. The main\nchallenge is selecting which structures to remove with-\nout compromising model performance. In the context of\nLLMs, several recent techniques aim for more efficient de-\nployment and inference acceleration. For example, LLM-\nPruner (Ma et al., 2023) proposes a dependency detec-\ntion algorithm to identify and remove non-critical coupled\nstructures, followed by a rapid post-training phase with\nlimited data. However, this method is memory-intensive\nas it requires full gradient information and is not compati-\nble with LoRA, necessitating a separate post-training phase\nfor knowledge recovery. In contrast, LoRAPrune (Zhang\net al., 2023) integrates LoRA with iterative structured prun-\ning, achieving both parameter-efficient fine-tuning and di-\nrect hardware acceleration. This approach is also memory-\nefficient, relying only on LoRA\u2019s weights and gradients for\npruning criteria, unlike LLM-Pruner, which uses full gra-\ndients. Most recently, Sheared-LLaMA (Xia et al., 2023)\naims to prune the model to a target architecture defined by\nexisting pre-trained models. It then trains the pruned model\nusing dynamically loaded data, based on each domain\u2019s\nrate of loss reduction, leading to more efficient data usage\nand faster performance improvement. However, Sheared-\nLLaMA allocates considerable computational resources to\nsubsequent pre-training for performance recovery.\nIn this work, we present LoRAShear, a method for efficient\nstructured pruning of LLMs while recovers knowledge.\nCompared to the existing methods, our approach uniquely\nleverages a novel graph algorithm to create dependency\ngraphs for both the original LLM and LoRA modules. We\nfurther introduce a structured sparsity optimization algo-\nrithm that utilizes information from LoRA modules to up-\ndate weights, thereby enhancing knowledge preservation.\nFollowing pruning, we employ a dual-phase training ap-\nproach involving both pre-training and fine-tuning to re-\ncover general and domain-specific knowledge effectively.\n3. LoRAShear\nLoRAShear dedicately designs a comprehensive end-to-\nend pipeline to compress pretrained LLMs and deliver effi-\ncient knowledge recovery. The outlined is stated as Algo-\nrithm 1. Given a general LLM M, we at first analyze its\narchitecture, create its dependency graph, and partition its\ntrainable variables into a group set G following the discov-\nered minimally removal structures (Section 3.1). We then\nanalyze the knowledge distributed over the minimally re-\nmoval structures to exclude the ones that highly impact the\nmodel performance from pruning (Section 3.2). Next, pro-\ngressive structured pruning is performed over the prunable\nstructures via our proposed LHSPG to identify redundant\nstructures and transfer the knowledge stored in the redun-\ndant structures back onto the important counterparts (Sec-\ntion 3.3), and construct a compressed LLM M\u2217 via auto-\nmatically removing redundant structures (Section 3.4). The\nlost knowledge during pruning is then recovered via a dy-\nnamic knowledge recovery stage to recap the performance\nof the compressed M\u2217 to the full LLM (Section 3.5).\nAlgorithm 1 Outline of LoRAShear.\n1: Input. A general pretraining LLM M.\n2: Discover minimal removal structures of M via cre-\nating and analyzing dependency graph (V, E). Parti-\ntion trainable variables of M into G.\n3: Analyze knowledge distribution over each node\ngroup in the dependency graph.\n4: Progressive structured pruning by LHSPG to iden-\ntify redundant structures and transfer lost knowledge.\n5: Construct compressed model to erasing redundancy\nto form compressed compact LLM M\u2217.\n6: Dynamic fine-tuning to recover lost knowledge.\n7: Output. The compact high-performing LLM M\u2217.\n3.1. Minimally Removal Structure Discovery\nAlgorithm 2 Minimally Removal Structure Discovery.\n1: Input. A LLM M to be compressed and fine-tuned.\n2: Construct the trace graph (E, V) of M.\n3: Establish node groups Ncomposed for composed opera-\ntors via traversing (E, V) and the module tree of M.\n4: Establish node groups Nbasic for remaining operators.\n5: Build dependancy across Ncomposed and Nbasic.\n6: Partition trainable variables into minimally removal\nstructures and form G.\n7: Return the trainable variable groups G.\nGiven a target LLM M, the foremost step is to discover\nthe minimally removal structures, which are defined as the\nunits that can be directly removed without affecting the\nfunctionality of the remaining DNNs. Such discovery was\nachieved by analyzing the trace graphs and creating the de-\npendency graphs over the basic operators in OTOv2 (Chen\net al., 2023b).\nIn these dependency graphs, each node\ngroup indicates the operators that are dependent and needs\nto be pruned together if with trainable variables and are dis-\njoint to each other in the normal DNNs. However, LLMs\nwith LoRA modules easily disrupt those algorithms since\nin such models, only LoRA modules are trainable, and\nthe original LLM variables are fixed yet prunable. To ad-\ndress issue, we dedicately introduce composed operator\nand overlapping node groups. Composed operator refers\nto the operators that are assembled by multiple basic op-\nerators such as LoRA modules consisting of two linear\noperators, yet needs to be considered as an entirety. Each\n3\nPreprint version.\nLinear1-LoRA-B\nW1\nB \u2208 R4096\u00d78\nLinear1-LoRA-A\nW1\nA \u2208 R8\u00d74096\nLinear1-Original\nW1 \u2208 R4096\u00d74096\nLinear2-LoRA-B\nW2\nB \u2208 R4096\u00d78\nLinear2-LoRA-A\nW2\nA \u2208 R8\u00d74096\nLinear2-Original\nW2 \u2208 R4096\u00d74096\nSigmoid\nW1\nB\nW1\nA\nW2\nB\nW2\nA\n\u2208 Gprunable\n\u2208 Gunprunable\nLayerNorm\n(a) Dependency graph for MLP layers.\nLinearK-LoRA-B\nWK\nB \u2208 R4096\u00d78\nLinearK-LoRA-A\nWK\nA \u2208 R8\u00d74096\nLinearK-Original\nWK \u2208 R4096\u00d74096\nLinearV-LoRA-B\nWV\nB \u2208 R4096\u00d78\nLinearV-LoRA-A\nWV\nA \u2208 R8\u00d74096\nLinearV-Original\nWV \u2208 R4096\u00d74096\nWK\nB\nWK\nA\nWV\nB\nWV\nA\n\u2208 Gprunable\n\u2208 Gunprunable\nLayerNorm\nLinearQ-LoRA-B\nWQ\nB \u2208 R4096\u00d78\nLinearQ-LoRA-A\nWQ\nA \u2208 R8\u00d74096\nLinearQ-Original\nWQ \u2208 R4096\u00d74096\nWQ\nA\nWQ\nB\nWK\nB\nWV\nB\nWQ\nB\n(b) Dependency graph for Attention layers.\nFigure 2. Dependency graph in LLAMAv1 and trainable variable partitions.\nsuch composed operator should form one node group,\nsuch as Linear-1-LoRA-A-Linear-1-LoRA-B in\nFigure 2a.\nThe overlapping node groups Ncomposed ex-\nist because their outgoing nodes still need to obey the\ndependency across other adjacent operators in the trace\ngraph, e.g., Linear-1-LoRA-B belonging to two node\ngroups marked as green and blue simultaneously. We then\njointly consider basic and composed node groups Nbasic\nand Ncomposed and partition the trainable variables of M\ninto a set of groups G, wherein each group g \u2208 G corre-\nsponds to one minimally removal structure.\n3.2. Knowledge Distribution Analysis\nDue to the universal training process, the knowledge is un-\nevenly distributed across all the node groups in the depen-\ndency graph. Some node groups serve remarkably more\nsignificant roles than others, resulting in performance col-\nlapse if pruning them. Meanwhile, in the limited resources\nsetting, the knowledge after collapse would not be easily\nrecovered. Therefore, before engaging into the progres-\nAlgorithm 3 Knowledge Distribution Analysis.\n1: Input.\nTrainable variable partition G, node groups\nNcomposed \u222a Nbasic, a set of pruning ratios P, an eval-\nuation dataset Deval, and a unprunable ratio \u03b3.\n2: for each node group in Ncomposed \u222a Nbasic do\n3:\nPrune groups upon some specified proxy and P.\n4:\nCompute performance deviation upon Deval.\n5:\nRecover pruned groups to the original status.\n6: end for\n7: Sort the perform deviation over Ncomposed \u222a Nbasic.\n8: Mark the groups in G regarding the node groups with\nthe largest deviation upon \u03b3 as unprunable Gunprunable.\n9: Mark the remaining groups in G as prunable Gprunable.\n10: Return prunable and unprunable variable groups\nGprunable \u222a Gunprunable.\nsive structured pruning stage, we analyze the knowledge\ndistribution to locate the node groups that should be ex-\ncluded from pruning. As stated in Algorithm 3, we iter-\n4\nPreprint version.\nxt+1\nO\n[x]1\n[x]2\nxt\n\u01ebxt\n\u02dcxt+1\n\u01eb > 0\n\u03b8 < 90\u25e6\n\u03b3Bt+1At+1\n\u2212\u03b1\u03bbgxt/\u2225xt\u2225\nFigure 3. Half-Space step over LoRA modules.\natively traverse all node groups, and prune each of them\nupon some specified pruning ratio yet keep the remaining\ngroups unchanged. We then evaluate the output deviation\nbetween each pruned LLM against the full model upon\nsome pre-selected evaluation dataset. The ones with the\nlargest \u03b3|Ncomposed \u222a Nbasic| deviations are marked as un-\nprunable, which corresponding groups of variables to form\nGunprunable. The remaining ones are marked as prunable,\nwhere trainable variables form Gprunable.\n3.3. Progressive Structured Pruning via LHSPG\nThe next step is to proceed progressive structured prun-\ning over the prunable groups of variables Gprunable.\nTo\nproceed it, we propose a novel structured sparsity opti-\nmizer LoRA Half-Space Projected Gradient (LHSPG) to\nyield structured sparsity over the original model parameters\nbased on the optimization information over auxiliary LoRA\nmodules. There exist two main takeaways of LHSPG, i.e.,\n(i) effectively identify and remove redundant structures via\nprojecting them onto zero, and (ii) transfer the knowledge\nstored in the relatively redundant structures to be pruned\nback to the important counterparts to better preserve the\nknowledge of full LLMs.\nTarget Problem. We formulate the progressive structured\npruning as the following structured sparsity optimization\nproblem (3) over LLMs with LoRA modules.\nminimize\nA,B\nf(A, B), s.t. Card{g \u2208 Gprunable|[x]g = 0} = K,\n(3)\nwhere A and B are the collections of LoRA decompos-\ning sub-matrices, which are trainable during the structured\npruning. We seek to yield group sparsity over the original\nvariables with the target sparsity level as K.\nOutline.\nThe outline of LHSPG is presented in Algo-\nrithm 4. We at first warm up the LoRA variables in the\nprunable groups Gprunable via stochastic gradient descent\nAlgorithm 4 Progressive Structured Pruning via LHSPG\n1: Input. pretraining variable x0, learning rate \u03b1, warm-\nup steps Tw, progressive period P, period length Tp,\ntarget group sparsity level K, and variable partition\nGprunable \u222a Gunprunable.\n2: Warm up Tw steps via SGD or its variants (AdamW).\n3: Initialize redundant groups Gredundant \u2190 \u2205.\n4: Initialize important groups Gimportant \u2190 G.\n5: Compute sparsity level for each pruning period bK :=\nK/Tp.\n6: for each pruning period p = 0, 1, \u00b7 \u00b7 \u00b7 , P \u2212 1 do\n7:\nPickup bGp in Gimportant with bK-least saliency scores.\n8:\nUpdate Gredundant \u2190 Gredundant \u222a bGp.\n9:\nUpdate Gimportant \u2190 Gimportant/ bGp.\n10:\nfor t = 0, 1, \u00b7 \u00b7 \u00b7 , Tp \u2212 1 do\n11:\nUpdate LoRA B and A via SGD or its variants.\nBt+1 \u2190 Bt \u2212 \u03b1k\u2207Btf\nAt+1 \u2190 At \u2212 \u03b1k\u2207Atf\n(1)\n12:\nCompute trial iterate [\u02dcxt+1] b\nGp for each g \u2208 bGp.\n[\u02dcxt+1]g \u2190 [xt + \u03b3Bt+1At+1]g \u2212 \u03bbg[xt]g\n\u2225[xt]g\u2225\n(2)\n13:\nPerform Half-Space projection over [\u02dcxt+1] b\nGp.\n14:\nUpdate [xt+1] b\nGp \u2190 [\u02dcxt+1] b\nGp.\n15:\nUpdate [Bt+1] b\nGp \u2190 0.\n16:\nif t = Tp \u2212 1 then\n17:\nMerge [Bt+1At+1]Gimportant into [x]Gimportant.\n18:\nend if\n19:\nend for\n20: end for\n21: Return the final iterate x\u2217\nLHSPG.\n(SGD) or its variants like AdamW to collect gradient in-\nformation. We then progressively identify redundant struc-\ntures within P periods of sparse optimization. To proceed,\nwe compute the target group sparsity level to be produced\nfor each period. In each period p, we sort the prunable\ngroups upon some prespecified saliency proxies and pick\nup the ones with least saliency scores as redundant groups\nfor the current period bGp. Then we compute trial iterate\nover the LoRA variables in A and B via SGD or its vari-\nants. For the redundant groups bGp, we proceed a gradi-\nent descent via LoRA approximation and penalize over the\nvariable magnitude proportionally to \u03bbg, which is selected\nupon the length of each pruning period. A Half-Space pro-\njection is next performed over the trial iterate to project\ngroups of variables with the least sacrificing over the objec-\ntive function. During the whole process, redundant groups\nare progressively projecting onto zero, during the projec-\n5\nPreprint version.\ntion, the LoRA modules for the important counterparts are\nabsorbing the knowledge via minimizing the loss functions.\nAs a result, the progressive structured pruning not only ef-\nfectively identifies and projects redundant groups of vari-\nables onto zero, but also preserve the knowledge stored in\nthe redundant structures to the largest extent. A final iterate\nx\u2217\nLHSPG is returned for the subsequent step.\n3.4. Compressed LLM Construction\nGiven the solution of LHSPG, LoRAShear automatically\nconstructs a structurally pruned LLM M\u2217 via automat-\nically erasing the structures corresponding to the redun-\ndant groups in Gprunable. The whole procedure is performed\nvia two pass dependency graph traversal. The first-pass\ntraversal iterates each node group and prunes the structures\nalong the primary dimension. The second-pass traversal\nerases the structures along the secondary dimension upon\nthe pruned status of the incoming structures.\n3.5. Dynamic Knowledge Recovery\nAlgorithm 5 Dynamic Knowledge Recovery.\n1: Input. pretraining dataset Dpretraining, instructed fine-\ntuning dataset Dinstruct, and a pruned LLM M\u2217.\n2: Establish validation datasets for Dpretraining and Dinstruct\nas Dval\npretraining and Dval\ninstruct, respectively.\n3: for D \u2208 {Dpretraining, Dinstruct} do\n4:\nwhile not converge do\n5:\nDynamically construct bD \u2286 D by evaluation.\n6:\nFine-tune M\u2217 with LoRA on bD.\n7:\nend while\n8: end for\n9: Return knowledge-recovered pruned LLM M\u2217.\nThe final step is recovering lost knowledge after pruning\nand restoring the capabilities of LLM. To achieve success-\nful recovery, it\u2019s essential to understand how the LLM ac-\nquires its knowledge. The knowledge is acquired through\na two-stage process: pretraining on extensive and diverse\ntext corpora, followed by fine-tuning with specific instruc-\ntion datasets. The acquired knowledge is stored as variables\nwithin the LLM, but these variables are removed during\nthe pruning process. Therefore, to regain the knowledge,\na post-training process is required, involving both the pre-\ntraining and instructed fine-tuning datasets.\nDue to the vast and diverse nature of the pretraining\ndatasets, existing structured pruning methods, especially\nthose designed for limited resources, only rely on the in-\nstructed fine-tuning datasets. However, this approach often\nleads to a significant degradation in performance. To mit-\nigate this challenge, we introduce a dynamic knowledge\nrecovery framework, presented as Algorithm 5.\nIn particular, given the pretraining and the instructured\nfine-tuning dataset collections Dpretraining and Dinstruct, we\nat first uniformly sample subsets from them for validation\nDval\npretraining and Dval\ninstruct. We then consider knowledge recov-\nery over pretraining datasets. To proceed, we at first evalu-\nate the performance degradation over the different sources\nvia Dval\npretraining. Upon on the performance deviation distri-\nbution, we construct a subset bDpretraining \u2286 Dpretraining. The\ncriteria for selecting samples involve prioritizing categories\nexperiencing more significant degradation while ensuring a\nbalanced representation of samples from sources with min-\nimal degradation to prevent overfitting. Subsequently, we\nemploy LoRA for fine-tuning the pruned model. If the eval-\nuation results do not converge, we repeat the process of\nconstructing the next subset from Dpretraining until conver-\ngence is achieved. Following the knowledge recovery from\nthe pretraining stage, we apply the same methodology to\nthe instructed fine-tuning datasets. This iterative approach\nultimately yields the highly optimized pruned LLM M\u2217.\n4. Numerical Experiments\nTo demonstrate the effectiveness of LoRAShear, we pro-\nvide our preliminary results on open-source LLAMAv1\n(Touvron et al., 2023).\nMore experimental results will\ncome in the future revisions.\n4.1. Dataset Selection\nPretraining Datasets. We follow Touvron et al. to col-\nlect pretraining datasets or the alternatives for English. In\nparticular, we select the OpenWebText (Aaron Gokaslan,\n2019) as an alternative to English CommonCrawl and C4\ndatasets. We select a processed Wikipedia dump on 2022\n(Foundation). Gutenberg (Gerlach & Font-Clos, 2020) and\nBookCorpus (Zhu et al., 2015) are also used in our collec-\ntion. For each datasets, we proceed standard pre-processing\nto erase irregular characters and only keep the paragraphs\nthat contains more than 64 tokens.\nInstructed Fine-Tuning Datasets. For fair comparison,\nwe follow the existing structured pruning LLM works (Ma\net al., 2023; Zhang et al., 2023) in the limited-resource set-\nting to use the Alpaca dataset (Taori et al., 2023), which\nconsists 52,000 instructions and demonstrations generated\nby OpenAI\u2019s text-davinci-003 engine. Alpaca is frequently\nused to conduct general instruction-tuning for language\nmodels and make the LLM follow instruction better.\n4.2. Experimental Results\nKnowledge Distribution Analysis. The analyzed knowl-\nedge distribution on LLAMAv1 is presented in Figure 4.\nGiven an evaluation dataset, we perform Algorithm 3 to\nanalyze the knowledge distributed across the minimally re-\n6\nPreprint version.\nFigure 4. Knowledge distribution analysis by measuring the perplexity deviation to the full LLAMAv1.\nTable 1. LoRAShear over LLAMAv1.\nPruning Ratio\nMethod\nBoolQ\nPIQA\nHellaSwag\nWinoGrande\nARC-e\nARC-c\nOBQA\nAverage\nRatio = 0%\nLLAMAv1 (Touvron et al., 2023)\n76.5\n79.8\n76.1\n70.1\n72.8\n47.6\n57.2\n68.59\n(Baseline)\nLLAMAv1 (Ma et al., 2023)\n73.18\n78.35\n72.99\n67.01\n67.45\n41.38\n42.40\n63.25\nRatio = 20%\nLLM-Pruner (Ma et al., 2023)\n66.79\n77.58\n68.48\n64.96\n64.06\n37.88\n39.00\n59.82\nLoRAPrune (Zhang et al., 2023)\n65.82\n79.31\n70.00\n62.76\n65.87\n37.69\n39.14\n60.05\nWANDA (Sun et al., 2023)\n65.75\n74.70\n64.52\n59.35\n60.65\n36.26\n39.40\n57.23\nLoRAShear\u2020\n70.17\n76.89\n68.69\n65.83\n64.11\n38.77\n39.97\n60.63\nLoRAShear\n72.78\n76.36\n69.49\n67.63\n69.02\n39.47\n40.78\n62.22\nRatio = 50%\nLLM-Pruner (Ma et al., 2023)\n61.56\n68.72\n46.62\n52.64\n47.94\n29.27\n35.40\n48.88\nLoRAPrune (Zhang et al., 2023)\n61.88\n71.53\n47.86\n55.01\n45.13\n31.62\n34.98\n49.71\nWANDA (Sun et al., 2023)\n50.90\n57.38\n38.12\n55.98\n42.68\n34.20\n38.78\n45.43\nLoRAShear\u2020\n62.12\n71.80\n48.01\n56.29\n47.68\n32.26\n34.61\n50.39\nLoRAShear\n63.40\n72.15\n49.83\n56.40\n49.45\n34.31\n35.86\n51.63\n\u2020 Knowledge recovery only on the instructured fine-tuning datasets as other works.\nmoval structures in each node group. After measuring the\noutput deviation, it is apparent that the knowledge is un-\nevenly distributed across different node groups. The first\nand last few node groups serve as more significant roles\nthan others to the model prediction.\nDuring pruning, it\nwould be better to avoid pruning these most sensitive node\ngroups since the saliency score calculation may still prune\nsome of their minimally removal structures which may re-\nsult in significant performance degradation.\nPruning Results.\nWe now show the quantitative re-\nsults of LoRAShear and compare with other meth-\nods\nover\nthe\nevaluation\nbenchmark\ncomputed\nvia\nlm-evaluation-harness (Gao et al., 2021).\nAs\nshown in Table 1, under the same pruning ratio 20%, Lo-\nRAShear significantly outperforms others by 2.2%-5.0%\naccuracy and negligibly regress 1% compared to the full\nLLAMAv1. We additionally conduct an ablation that only\nlevering the same instructured fine-tuning dataset, i.e., Al-\npaca, to recover the lost knowledge. The performance in\nthis setting still outperform other methods, implying the\neffectiveness of progressive structured pruning via LHSPG\nto transferring and preserving knowledge. Under the high\npruning ratio 50%, the outerperformance of LoRAShear\nstill holds. In particular, under both progressive structured\npruning followed by knowledge recovery via pretraining\nand instructured fine-tuning datasets, our performance is\nsignificantly better than the existing state-of-the-arts.\n5. Conclusion\nWe propose a novel LoRAShear to conduct efficient struc-\ntured pruning and knowledge recovery for general LLMs\nin the limited resources setup. LoRAShear has three take-\naways: (i) it automatically discovers minimally removal\nstructures over LLMs with LoRA modules; (ii) conducts\nprogressive structured pruning via a novel structured spar-\nsity optimizer LHSPG that yields structured sparsity over\noriginal variables via the information stored in LoRA mod-\nules; and (iii) equips with a dynamic knowledge recovery\nstage to gain knowledge back from both pretraining and in-\nstructured fine-tuning datasets. Numerical results validates\nthe efficacy, that negligibly regress 1% performance to the\nfull model under 20% pruning ratio and preserve 82% per-\nformance under 50% pruning ratio to the full LLMs. More\nexperiments will come in the updated versions.\n7\nPreprint version.\nReferences\nAaron Gokaslan, Vanya Cohen, E. P. S. T. Openwebtext\ncorpus, 2019.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sas-\ntry, G., Askell, A., et al. Language models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877\u20131901, 2020.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.,\nLundberg, S., et al.\nSparks of artificial general intel-\nligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712, 2023.\nChen, T., Ji, B., Shi, Y., Ding, T., Fang, B., Yi, S., and Tu,\nX. Neural network compression via sparse optimization.\narXiv preprint arXiv:2011.04868, 2020.\nChen, T., Ding, T., Ji, B., Wang, G., Shi, Y., Tian, J., Yi, S.,\nTu, X., and Zhu, Z. Orthant based proximal stochastic\ngradient method for l1-regularized optimization. In Ma-\nchine Learning and Knowledge Discovery in Databases:\nEuropean Conference, ECML PKDD 2020, Ghent, Bel-\ngium, September 14\u201318, 2020, Proceedings, Part III, pp.\n57\u201373. Springer, 2021a.\nChen, T., Ji, B., Ding, T., Fang, B., Wang, G., Zhu, Z.,\nLiang, L., Shi, Y., Yi, S., and Tu, X. Only train once:\nA one-shot neural network training and pruning frame-\nwork.\nIn Advances in Neural Information Processing\nSystems, 2021b.\nChen, T., Liang, L., Ding, T., and Zharkov, I. Towards au-\ntomatic neural architecture search within general super-\nnetworks. arXiv preprint arXiv:2305.18030, 2023a.\nChen, T., Liang, L., Ding, T., Zhu, Z., and Zharkov, I.\nOtov2: Automatic, generic, user-friendly. arXiv preprint\narXiv:2303.06862, 2023b.\nDing, T., Liang, L., Zhu, Z., and Zharkov, I.\nCdfi:\nCompression-driven network design for frame interpo-\nlation. arXiv preprint arXiv:2103.10559, 2021.\nDing, T., Liang, L., Zhu, Z., Chen, T., and Zharkov, I.\nSparsity-guided network design for frame interpolation.\narXiv preprint arXiv:2209.04551, 2022.\nDong, X., Chen, S., and Pan, S. Learning to prune deep\nneural networks via layer-wise optimal brain surgeon.\nAdvances in neural information processing systems, 30,\n2017.\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,\nA., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T.,\net al. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378, 2023.\nEveritt, T., Lea, G., and Hutter, M. Agi safety literature\nreview. arXiv preprint arXiv:1805.01109, 2018.\nFoundation, W. Wikimedia downloads. URL https://\ndumps.wikimedia.org.\nFrantar, E. and Alistarh, D. SparseGPT: Massive language\nmodels can be accurately pruned in one-shot. In Krause,\nA., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,\nand Scarlett, J. (eds.), Proceedings of the 40th Interna-\ntional Conference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pp. 10323\u2013\n10337. PMLR, 23\u201329 Jul 2023.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A.,\nFoster, C., Golding, L., Hsu, J., McDonell, K., Muen-\nnighoff, N., Phang, J., Reynolds, L., Tang, E., Thite,\nA., Wang, B., Wang, K., and Zou, A.\nA frame-\nwork for few-shot language model evaluation, Septem-\nber 2021.\nURL https://doi.org/10.5281/\nzenodo.5371628.\nGerlach, M. and Font-Clos, F.\nA standardized project\ngutenberg corpus for statistical analysis of natural lan-\nguage and quantitative linguistics. Entropy, 22(1):126,\n2020.\nHadi, M. U., Qureshi, R., Shah, A., Irfan, M., Zafar, A.,\nShaikh, M. B., Akhtar, N., Wu, J., Mirjalili, S., et al.\nA survey on large language models: Applications, chal-\nlenges, limitations, and practical usage. 2023.\nHan, S., Mao, H., and Dally, W. J.\nDeep compres-\nsion: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint\narXiv:1510.00149, 2015.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y.,\nWang, S., Wang, L., and Chen, W.\nLora: Low-rank\nadaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nLi, Y., Luo, F., Tan, C., Wang, M., Huang, S., Li, S., and\nBai, J. Parameter-efficient sparsity for large language\nmodels fine-tuning. In Raedt, L. D. (ed.), Proceedings\nof the Thirty-First International Joint Conference on Ar-\ntificial Intelligence, IJCAI-22, pp. 4223\u20134229. Interna-\ntional Joint Conferences on Artificial Intelligence Orga-\nnization, 7 2022.\nMa, X., Fang, G., and Wang, X. Llm-pruner: On the struc-\ntural pruning of large language models. arXiv preprint\narXiv:2305.11627, 2023.\nSun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and\neffective pruning approach for large language models.\narXiv preprint arXiv:2306.11695, 2023.\n8\nPreprint version.\nTaori,\nR.,\nGulrajani,\nI.,\nZhang,\nT.,\nDubois,\nY.,\nLi,\nX.,\nGuestrin,\nC.,\nLiang,\nP.,\nand Hashimoto,\nT. B. Stanford alpaca: An instruction-following llama\nmodel.\nhttps://github.com/tatsu-lab/\nstanford_alpaca, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al.\nLlama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971,\n2023.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language mod-\nels. arXiv preprint arXiv:2206.07682, 2022.\nXia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama:\nAccelerating language model pre-training via structured\npruning. arXiv preprint arXiv:2310.06694, 2023.\nZhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.,\net al. Pruning meets low-rank parameter-efficient fine-\ntuning. arXiv preprint arXiv:2305.18403, 2023.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou,\nY., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al.\nA survey of large language models.\narXiv preprint\narXiv:2303.18223, 2023.\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S.\nAligning books\nand movies: Towards story-like visual explanations by\nwatching movies and reading books. In The IEEE In-\nternational Conference on Computer Vision (ICCV), De-\ncember 2015.\n9\n"
  },
  {
    "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
    "link": "https://arxiv.org/pdf/2310.19773.pdf",
    "upvote": "18",
    "text": "MM-VID\n: Advancing Video Understanding with GPT-4V(ision)\nKevin Lin\u2217, Faisal Ahmed\u2217, Linjie Li\u2217, Chung-Ching Lin\u2217, Ehsan Azarnasab, Zhengyuan Yang,\nJianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, Lijuan Wang\u2217\u2660\nMicrosoft Azure AI\n\u2217 Core Contribution\n\u2660 Project Lead\nhttps://multimodal-vid.github.io/\n\u2026\n\u2026\n\u2026\u2026\nFaraon\nThut\nNefer\nNimona\nBallister \nBoldheart\nbaal bir\u01a1\n\u2026\u2026\nFigure 1. MM-VID allocates specialized vision, audio, speech experts with GPT-4V(ision) to address challenging video understanding\ntasks. For example, the system could associate information from multiple uploaded episodes and reason the storyline of the queried\ncharacters (\u201cMulti-Video Episodic Analysis\u201d). We highlight key information here and postpone full MM-VID responses to Figures 4-32.\nDemo videos are available at this link.\n1\narXiv:2310.19773v1  [cs.CV]  30 Oct 2023\nAbstract\nWe present MM-VID, an integrated system that har-\nnesses the capabilities of GPT-4V1, combined with special-\nized tools in vision, audio, and speech, to facilitate ad-\nvanced video understanding. MM-VID is designed to ad-\ndress the challenges posed by long-form videos and intri-\ncate tasks such as reasoning within hour-long content and\ngrasping storylines spanning multiple episodes. MM-VID\nuses a video-to-script generation with GPT-4V to transcribe\nmultimodal elements into a long textual script. The gener-\nated script details character movements, actions, expres-\nsions, and dialogues, paving the way for large language\nmodels (LLMs) to achieve video understanding. This en-\nables advanced capabilities, including audio description,\ncharacter identification, and multimodal high-level compre-\nhension. Experimental results demonstrate the effectiveness\nof MM-VID in handling distinct video genres with various\nvideo lengths. Additionally, we showcase its potential when\napplied to interactive environments, such as video games\nand graphic user interfaces.\n1. Introduction\nPeople around the world create numerous videos on a\ndaily basis [14, 27, 48, 55], including user-generated live\nstreams, video-game live streams, short clips, movies,\nsports broadcasts, advertising, and more. Videos serve as\na versatile medium for conveying information and content\nthrough various modalities [12, 59, 60, 74, 77, 81, 82], such\nas text, visuals, and audio. Developing methods that can\nlearn from diverse modalities will enable us to design cogni-\ntive machines with enhanced capabilities for analyzing un-\ncurated real-world videos, extending beyond the confines\nof hand-curated datasets. However, this rich representation\nintroduces many challenges for the study of video under-\nstanding, particularly when dealing with extended-duration\nvideos [62,70].\nUnderstanding long videos, especially those spanning\nover an hour, is a complex task that demands advanced\nmethods capable of analyzing sequences of images and\naudio across multiple episodes.\nThis challenge is com-\npounded by the need to extract information from various\nsources, such as distinguishing speakers [17,49,61], identi-\nfying characters [29, 46, 50], and maintaining narrative co-\nherence [26, 57]. Additionally, answering questions based\non video evidence [31] requires a deep comprehension of\nthe content, context, and subtitles. When it comes to live\nstreaming and gaming videos [1, 11, 55], there are chal-\nlenges in processing dynamic environments in real-time, re-\n1In this work, we explore GPT-4V(ision) with the vision capability and\nrefers to the model as \u201cGPT-4V,\u201d following the OpenAI reports [51, 52].\nWe refer to the text-only version of the model as \u201cGPT-4\u201d [51].\nquiring semantic understanding, and the ability of long-term\nstrategy planning [11,54,67,75,83].\nRecently, substantial advances have been made with\nlarge pre-trained video models [9, 13, 20, 21, 42, 69] and\nvideo-language models [10, 22, 23, 30, 36\u201340, 66], which\nhave demonstrated their reasoning capabilities for video\ncontent.\nHowever, these models are usually trained on\nshort clips (e.g., 10-second videos in Kinetics [15] and VA-\nTEX [68]) or pre-defined action classes (e.g., 174 classes in\nSomething-Something v1 [24]). Consequently, these mod-\nels may fall short in providing a detailed comprehension of\nintricate videos in real world [62, 70]. To achieve a more\ncomprehensive understanding of the videos we encounter in\ndaily life, we need methods capable of addressing complex\nchallenges. It involves not only identifying who are in the\nscene and what they do, but also pinpointing when and how\nthey act, while recognizing subtle nuances and visual cues\nacross different scenes. The aim of this work is to address\nthese challenges and explore methods that can be applied\ndirectly to real-world video understanding. Our approach\ninvolves breaking down extended video content into coher-\nent narratives and subsequently employing these generated\nstories for video analysis.\nRecent\nadvances\nin\nLarge\nMultimodal\nModels\n(LMMs) [7,8,19,47,51,52,78], such as GPT-4V(ision) [52],\nhave demonstrated significant breakthroughs in processing\nboth input images and text for multimodal understanding.\nThis has sparked interest in applying LMMs to the video\ndomain.\nIn this work, we present MM-VID, a system\nthat integrates specialized tools with GPT-4V for video\nunderstanding. Given an input video, MM-VID performs\nmultimodal pre-processing, including scene detection and\nautomatic speech recognition (ASR), to collect important\ninformation in the video. The input video is then split into\nmultiple clips according to the scene detection algorithm.\nThen, we employ GPT-4V, which takes the clip-level video\nframes as input and generates a detailed description for\neach video clip.\nFinally, GPT-4 is adopted to generate\na coherent script for the full video, conditioning on the\nclip-level video descriptions, ASR, and video metadata if\navailable. As shown in Figure 1, the generated script allows\nMM-VID to perform a diverse set of video tasks.\nExperimental results demonstrate the effectiveness of\nMM-VID in different challenging scenarios.\nMM-VID\nis able to comprehend hour-long videos through multiple\nmodalities, and localize specific events with correct times-\ntamps.\nMM-VID also demonstrates intriguing results in\nan interactive environment, such as predicting the possible\nnext steps when playing a video game [4] or interacting with\na graphical user interface (GUI) [78].\n2\nScript\nInput Video\nClip-Level Video \nDescription Generation\n(GPT-4V)\nScript Generation \nusing LLM (GPT-4)\nExternal Knowledge\n(Metadata, Title, Abstract,\nCharacter Snapshot, etc.)\nMultimodal \nPre-Processing\n(Scene Detection, ASR)\nTask List:\nAudio Description, Grounded QA, Summarization, Speaker \nIdentification, Character Identification, Multimodal Reasoning, etc.\nChat with \nReference (Script)\nText Prompt\nClip 1\nClip 2\nClip N\nText Prompt\nVisual Prompt\nPlease generate audio \ndescription for the input video\nVideo Tasks:\nAudio Description,\nGrounded QA,\nSummarization,\nSpeaker Identification,\nCharacter Identification,\netc.\nLLM with \nScript\nFigure 2. Overview of MM-VID. Our system takes a video file as input, and outputs a long textual script describing the video contents.\nMM-VID consists of four modules: (i) Multimodal Pre-Processing, (ii) External Knowledge Collection, (iii) Clip-Level Video Description\nGeneration, and (iv) Script Generation.\n2. Related Work\nConventional Video Understanding Methods.\nEarly\nwork in computer vision centered on building video founda-\ntion models [9,13,20,21,42,69]. These models, with differ-\nent neural network architecture designs and training meth-\nods, have achieved great breakthrough at analyzing short\nvideo clips [14,15,28,63], typically lasting less than 30 sec-\nonds. However, these models are typically pre-trained with\nvision modality only, and thus may require specific adjust-\nment or fine-tuning for multimodal downstream tasks.\nVideo-Language Models.\nRecent studies [10, 22, 23, 30,\n36\u201340, 66] have made remarkable improvements in multi-\nmodal representation learning for video-and-language un-\nderstanding. These advancements have been particularly\nevident in popular downstream tasks such as video ques-\ntion answering [31], text-video retrieval [32, 73] and video\ncaptioning [68]. Building on this momentum, researchers\ntypically embark on a pretrain-finetune paradigm: initially\npre-training a video-language foundation model on large-\nscale video-text pairs, followed by a fine-tuning process on\nspecific downstream datasets. However, these methods are\nusually trained on short video clips, often restricted to du-\nrations of around 10 seconds, posing potential challenges in\ncomprehending longer video sequences.\nVisual Instruction Tuning. Inspired by the breakthrough\nof Large Language Models (LLMs) [18, 19, 51, 64, 85], re-\ncent studies [35,43,44,62,84] suggest using a frozen LLM\ncombined with an image encoder and a few learnable mod-\nules for video understanding tasks. Specifically, researchers\npropose the visual instruction tuning [35, 41, 44], which\naims to fine-tune the learnable modules and thus enable\nLLMs to generate textual descriptions for the video content.\nWhile promising performance is presented, these models\nmay fall short when it comes to handling videos with ex-\ntended duration. Our work aims to fill this gap, exploring\nmethods that can be directly applied to the understanding of\nlong videos in real-world situations.\nPrompting LLMs for Video Understanding.\nRecently,\nresearchers [6, 33, 65, 72] explore the LangChain system\nparadigm [16], which aims to integrate expert tools with\nexisting LLMs to create new functionalities. For example,\nVLog [6] uses BLIP2 [34] and GRIT [71] as dense image\ncaptioners, Whisper [56] as ASR translator, and ChatGPT\nas a reasoner. By transcribing a given video to textual de-\nscriptions (e.g., document), it enables ChatGPT for video\nquestion-answering tasks. Inspired by the efficacy of these\ntool-using approaches [16, 65, 79], we explore integration\nwith GPT-4V for video understanding.\n3. Preliminary Study with GPT-4V(ision)\nRecent studies [3, 51, 52, 78] show that GPT-4V can ac-\ncept a range of inputs, such as textual descriptions, ques-\ntions, or even visual cues like images or short video clips.\nGPT-4V\u2019s inherent ability to comprehend visual inputs and\ngenerate contextually relevant text opens the door for a wide\nrange of applications. By introducing a sequence of frames\nas input, GPT-4V can grasp temporal relationships and in-\nteractions, aiding in the identification and interpretation of\ndynamic visual content.\n4. MM-VID\nFigure 2 shows the overview of our system pipeline.\nMM-VID takes the video file as input, and outputs a script\ndescribing the video contents. The generated script enables\nLLMs to achieve various video understanding capabilities.\nMM-VID consists of four modules: (i) Multimodal Pre-\nProcessing, (ii) External Knowledge Collection, (iii) Clip-\nLevel Video Description Generation, and (iv) Script Gener-\nation. We describe each module in detail below.\nMultimodal Pre-Processing. Starting with an input video\nfile, our process begins by using the established ASR tool\nto extract transcriptions from the video. Following this, we\ndivide the video into several short video clips. This pro-\ncess involves uniform sampling of video frames, with each\nclip consisting of 10 frames. To enhance the overall qual-\nity of frame sampling, we use established scene detection\n3\nStreaming Inputs\nAudio Description Generation, \nAgent\n(GPT-4V and Memory) \nEnvironment\n(Visual World, Game, GUI, etc.)\nAction\n(e.g., Gaming Console Control)\n      State \n(e.g., Video Frames, Scalar Signals)\nFigure 3. MM-VID for streaming inputs. MM-VID can serve as\nan agent in an interactive environment, continually receiving and\nprocessing the streaming video frames.\ntools like PySceneDetect [5] to help identify crucial scene\nboundaries.\nExternal Knowledge Collection. We incorporate external\nknowledge into our input prompts to GPT-4V. This involves\ngathering available information, such as metadata, title, ab-\nstract, and face photos of characters within the video. In\nour experiments, the metadata, title, and abstract are gath-\nered from YouTube.\nClip-Level Video Description Generation.\nDuring our\nmultimodal pre-processing, the input video is segmented\ninto multiple clips. For each clip, which typically consists\nof 10 frames, we employ GPT-4V to generate video descrip-\ntions. By feeding the video frames along with the associ-\nated text prompt into the model, GPT-4V utilizes the input\nto generate detailed descriptions that capture the visual ele-\nments, actions, and events depicted in those frames.\nIn addition, we explore the use of visual prompting,\nwhere the character\u2019s face photos are presented alongside\nthe character\u2019s name in the input to GPT-4V. Our empirical\nresults suggest that visual prompting is helpful to enhance\nthe quality of video descriptions, particularly for more accu-\nrate character identification. These findings align with the\ninsights from [78].\nScript Generation using LLM. After generating the de-\nscriptions for each video clip, we use GPT-4 to integrate\nthese clip-level descriptions into a coherent script.\nThis\nscript serves as a comprehensive description of the entire\nvideo, and is used by GPT-4 for a diverse set of video un-\nderstanding tasks.\n5. MM-VID for Streaming Inputs\nFigure 3 shows the diagram of MM-VID when applied to\nthe context of streaming inputs. Our system operates as an\nagent within a dynamic environment where streaming video\nframes serve as the primary input. In this context, the agent\ncontinually receives streaming video frames as states, rep-\nresenting the ongoing visual information unfolding in the\nenvironment. These states are then processed by GPT-4V\nto make informed decisions and generate responses.\nBy continually analyzing the streaming video frames,\nMM-VID plays a crucial role in transforming raw visual\ndata into meaningful insights, making it valuable for appli-\ncations such as video game play, the embodied agent, and\nGUI navigation.\n6. Experiments\n6.1. Experiment Setup\nWe implement MM-VID based on MM-REACT [79]\ncodebase.\nWe use the Automatic Speech Recognition\n(ASR) tool publicly available via the Azure Cognitive Ser-\nvices APIs [2], and utilize PySceneDetect [5] for scene de-\ntection.\n6.2. MM-VID Capabilities\nFigures 4-9 provide illustrative examples of MM-VID\u2019s\ncomplete execution flow. When a user uploads a video file,\nMM-VID initiates the process by first assessing the esti-\nmated video length. Subsequently, it performs multimodal\npre-processing by invoking expert tools, including scene de-\ntection and ASR. Additionally, MM-VID collects external\nknowledge, encompassing video metadata such as title and\nabstract.\nFollowing this preliminary stage, MM-VID proceeds to\ngenerate clip-level video descriptions for each segment of\nthe video. Finally, it invokes GPT-4, integrating these clip-\nlevel descriptions into a coherent script. Once the script is\ngenerated, it empowers LLMs to provide a summarized un-\nderstanding of the video content. That equips the system to\naddress users\u2019 questions with grounded answers. We dis-\ncuss MM-VID\u2019s distinct capabilities as below.\nGrounded Question-Answer (QA). The generation of a\ncomprehensive script empowers our system with the capa-\nbility of grounded QA. As shown in Figure 8, let us consider\na scenario where a user poses the question, \u201cShow me the\nmost exciting moment in this video.\u201d In response, MM-\nVID displays a highlight, specifically featuring a home run,\nand provides the corresponding timestamp. When a user\nasks \u201cWho are the best pitchers in this video?\u201d MM-VID\naddresses the question by referring to relevant evidence in\nthe generated script.\nThis grounding capability owes its\nsuccess to the extensive and detailed script generation pro-\ncess, which documents essential timestamps and significant\nevents within the video, enabling accurate and contextually\ngrounded responses to user inquiries.\nMultimodal Reasoning.\nMM-VID considers multimodal\ninputs, including video frames, speech transcriptions, and\nexternal knowledge if available.\nIn Figure 8, when a\nuser inquires, \u201cHow did you know the sound is different?\u201d\n4\nLoading video\u2026\nVideo length is  9 minutes 19 seconds.\nInvoke tools: Scene Detection, ASR, Metadata \nCollection\nScene Detection:\n00:00:00, 00:00:02, 00:00:20, 00:00:24, \n..., 00:08:42, 00:08:53, 00:09:01, 00:09:07\nASR:\n0:05\ngets ready for his second start of the\n0:08\nSeason began this home stand got the win\n:\n:\n:\n:\n9:04\nthat's the ball game Dodgers sweep the\n9:07\ntwo-game series and they go four and two\n9:09\non the season opening home stand\nMetadata Collection:\nTitle: Rockies vs. Dodgers Game Highlights\nAbstract: Rockies vs. Dodgers Game Highlights \n(4/4/23) | MLB Highlights\nClip-Level Video Description Generation \nUsing GPT-4V\nInput Prompt: \nYou are an expert in understanding scene \ntransitions based on visual features in a \nvideo. For the given sequence of images per \ntimestamp, identify different scenes in the \nvideo. Generate audio description for each \nscene with time ranges.\nScript Generation Using GPT-4:\nInput Prompt:\nYou are an expert at understanding audio \ndescriptions of different scenes in a video. Can you \nleverage the information provided, including title, \nabstract, audio descriptions and generate full \naudio description of each scene with non \noverlapping time ranges. Keep as many scenes \npossible covering all time ranges. You may find \ncharacter names in the title or abstract.  Use \ncharacter names wherever possible in the audio \ndescriptions. Keep the audio description for each \ntime range within one short sentence.\nOutput:\n0:00:00 - 0:00:14: Video begins with a montage \nof the Colorado Rockies and the Los Angeles \nDodgers players warming up for their game at \nDodger Stadium over the backdrop of the Los \nAngeles skyline.\n0:00:15 - 0:00:30: Dodge.rs pitcher, Julio Urias, \nmakes his second start of the season and deals a \npowerful pitch towards the Rocky's batter. As the \nbatter swings and makes contact, the ball heads \ndown the line under the glove of the Dodgers and \nis marked as a double for the Rockies.\n0:00:31 - 0:00:46: Urias successfully strikes out  \nrk heart \n00:00:00\n00:00:02\n00:09:07\nClip-Level Video Description Generation \nUsing GPT-4V\nOutput:\nTimestamp: 00:00:00 - 00:00:02\nScene: Opening graphics introducing the game \nbetween the Colorado Rockies and the Los Angeles \nDodgers.\nTimestamp: 00:00:20 - 00:00:24\nScene: Pitcher on the mound at Dodger Stadium.\n:\n:\n:\nTimestamp 00:09:01-00:09:07\nScene Description: In this scene, we see a baseball \ngame between the Los Angeles Dodgers and the \nColorado Rockies. The Dodgers are leading with a \nscore of 5-2. A player from the Dodgers hits the \nball, and it is fielded by a player from the Rockies.\nFigure 4. An example of MM-VID\u2019s execution flow. Given a baseball video, MM-VID provides an estimated video length, and then\ninvokes scene detection and ASR tools, and collects external knowledge. Then, we generate clip-level video descriptions by using GPT-4V.\nGPT-4V takes the video frames and the text prompt as input, and outputs the video descriptions. Finally, given the clip-level descriptions,\nvideo metadata and ASR, we use GPT-4 to generate a coherent script for the input video. Figures 7-9 show continued outputs. The original\nvideo is available at https://www.youtube.com/watch?v=-pNe0p4H8ec\n5\nMM-VID explains that this information was derived from\nthe commentator\u2019s remarks during the game. The exam-\nples illustrate MM-VID\u2019s multimodal reasoning capabili-\nties, where it integrates both visual and auditory cues to\nprovide contextually accurate responses to user queries.\nHour-Long Video Comprehension.\nFigures 10-13\ndemonstrate MM-VID\u2019s capabilities in processing lengthy\nvideos. In this example, MM-VID effectively analyzes a\ndocumentary video spanning approximately 50 minutes in\nduration. For simplicity, the intermediate outputs are omit-\nted in the figures, and only the final generated script is pre-\nsented. We observe that MM-VID is able to generate a long\nscript with the corresponding timestamps to represent the\ndocumentary video. By leveraging this generated script as\ncontextual information, MM-VID is equipped to perform\na range of tasks, including summarizing the lengthy video,\naddressing specific queries raised within the video, and in-\ndexing pivotal moments.\nMulti-Video Episodic Analysis.\nMM-VID\u2019s proficiency\nin handling extensive video content can be expanded to\nencompass multiple lengthy videos, as illustrated in Fig-\nures 14-16. In these examples, we upload multiple episodes\nto MM-VID, showcasing its ability to perform a variety of\ncomplex tasks. MM-VID exhibits the capability to sum-\nmarize the video series, engage in cross-episode reasoning,\nprovide detailed descriptions of character journeys across\nmultiple episodes, and facilitate grounded QA interactions.\nCharacter Identification.\nWe found that incorporating\nvisual prompts enhances the quality of script generation,\nparticularly with regards to character identification. In Fig-\nure 17, we illustrate this by providing MM-VID with ad-\nditional inputs consisting of characters\u2019 face photos and\ntheir corresponding names. MM-VID effectively utilizes\nthese visual prompts to identify the characters depicted in\nthe video, based on the provided face photos. As a result,\nthe script generation process is notably improved, ensur-\ning more accurate and contextually relevant descriptions of\ncharacters and their interactions within the video content.\nSpeaker Identification. Our exploration has revealed an-\nother valuable application of visual prompting in enhanc-\ning the quality of Automatic Speech Recognition (ASR). In\nFigures 18-19, we highlight a scenario where conventional\nASR struggles to accurately recognize the number of speak-\ners and their identities in the video. Visual prompting plays\na pivotal role in enhancing ASR performance by providing\ncontextual cues to identify individuals and attribute speech\nto specific speakers. This improvement ensures more pre-\ncise transcriptions, enabling a more accurate representation\nof the dialogue and interactions within the video content.\nAudio Description Generation.\nAudio descriptions [26,\n57] play a crucial role in making videos accessible to in-\ndividuals who are blind, have low vision, or face difficul-\nties in visually understanding the content. These descrip-\ntions provide contextual narration of meaningful visual el-\nements, clarify speakers, and convey the essence of visual\ninformation within a video. In our experiments, we also ex-\nplore MM-VID\u2019s performance in audio description genera-\ntion. We experiment with videos where there is limited or\nno speech content. In Figure 20, we showcase an example\nfeaturing a short film of Mr. Bean taking an exam, which\nprimarily lacks speech. Without ASR inputs, MM-VID pro-\ncesses the video and generates a detailed script. This shows\nMM-VID\u2019s versatility in handling various types of video\ncontent and its potential in creating inclusive and accessible\nmultimedia content.\nSelf-Refinement. While the generated script offers a com-\nprehensive understanding of video content, our experiments\nhave unveiled occasional inaccuracies, especially in cases\ninvolving blurry or low-resolution video frames, as demon-\nstrated in Figure 21. In this example, MM-VID mistak-\nenly identifies a bird as a rock due to the challenges posed\nby the video\u2019s visual quality. To address such inconsisten-\ncies and elevate the overall accuracy of the generated script,\nwe employ a self-refinement approach [45, 58, 80]. This\ninvolves revising the script based on both the initially gen-\nerated script and a concurrently generated video summary.\nThrough this process, MM-VID is able to rectify errors and\ninaccuracies, resulting in a more refined output.\nFast-Changing Short Videos. In Figure 22, we present an\nexample of our experimentation with fast-changing short-\nform videos, such as those found on platforms like TikTok.\nShort videos often feature non-standard frame sizes and\nsignificantly shorter durations compared to conventional\nvideos. Remarkably, MM-VID excels at accurately describ-\ning the cooking recipes depicted in these short videos, de-\nspite the distinct characteristics of such content.\nThese examples demonstrate the versatility of MM-VID\nin processing a diverse array of video content. Whether\ndealing with lengthy documentaries, episodic series, or\nshort-form clips, MM-VID adapts seamlessly to the unique\nattributes of each video type, consistently delivering mean-\ningful and contextually relevant descriptions.\n6.3. Applications to Interactive Environments\nIn the following section, we evaluate MM-VID when ap-\nplying to the context of streaming inputs. MM-VID serves\nas an agent in an interactive environment, continually re-\nceiving streaming video frames as the inputs.\nEmbodied Agent. Figure 23 illustrates an example where\nMM-VID is applied to an egocentric video captured by a\nhead-mounted camera. This video, collected from Ego4D\ndataset [25], provides a brief glimpse into the wearer\u2019s daily\nlife within their home environment. Remarkably, MM-VID\nshowcases its capability in understanding such video con-\ntent and assists the user in a few practical tasks. Specif-\nically, MM-VID helps the user locate items like the pink\n6\nTable 1. Questionnaire for the group with visual impairments. Par-\nticipants listen to a video and subsequently assign scores (ranging\nfrom 0 to 10) for distinct auditory criteria.\nEffectiveness of Delivery: If the original audio and the\nembedded AD are effectively presented?\nInformative: Is it easy to follow the storyline? Does the\nAD provide context and background information when\nnecessary?\nAudio Quality: Is the overall audio production quality\ngood?\nOverall Satisfaction: Are you satisfied with the overall\nAD experience?\njacket and the laptop within the home. Additionally, it gen-\nerates a list of the user\u2019s activities within a specified time\nrange, offering insights into the wearer\u2019s daily routine.\nPlaying Video Games.\nFigures 24-27 demonstrate the\nresults of applying MM-VID to a Mario video game [4].\nIn these experiments, our agent consistently receives three\nvideo frames as states and calculates the next possible con-\ntrol action. Remarkably, our agent displays an understand-\ning of the specific video game dynamics and generates rea-\nsonable action controls to play the game effectively. These\nexamples highlight MM-VID\u2019s ability to comprehend and\nnavigate in an interactive gaming environment. Interested\nreaders may find the full gameplay demonstration on our\nproject website.\nGUI Navigation.\nFigures 28-32 provide the demonstra-\ntion of MM-VID\u2019s performance in the GUI navigation sce-\nnario. In this context, the agent continually receives iPhone\nscreenshots and previous user actions as states. The agent\neffectively predicts the possible next steps in the user\u2019s jour-\nney, which may include clicking on the correct shopping\napps, initiating searches for items of interest, and ultimately\nplacing an order. These results demonstrate MM-VID\u2019s re-\nmarkable ability to interact with graphical user interfaces,\nfacilitating seamless and intelligent navigation through dig-\nital interfaces.\n6.4. User Study\nWe explore the potential of MM-VID for people who are\nblind or have low vision. Audio description (AD) [26, 57]\nprovides an auditory narration integrated into the video\u2019s\nsoundtrack, offering important visual details that may not\nbe discernible from the main video soundtrack. Such de-\nscriptions play a pivotal role in conveying essential visual\ncontent to those with visual impairments.\nTo assess the efficacy of MM-VID in generating audio\ndescriptions (AD), we conduct a user study. We invited 9\nparticipants for the evaluation. 4 participants were either\nblind or had low vision, while the remaining 5 had normal\nTable 2. Questionnaire for the group with normal vision. Partici-\npants view a video and subsequently assign scores (ranging from\n0 to 10) for various auditory and visual criteria.\nClarity: Are the visual elements clearly and accuratetly\ndescribed?\nConciseness: Does the AD convey essential visual infor-\nmation without overloading the user?\nTiming and Synchronization: Are the original audio\nand the embedded AD effectively presented? Does the\nAD properly synchronize with visual contents?\nInformative: Is it easy to follow the storyline? Does the\nAD provide context and background information when\nnecessary?\nAudio Quality: Is the overall audio production quality\ngood?\nOverall Satisfaction: Are you satisfied with the overall\nAD experience?\nvision. All the participants have normal hearing. For the\npurposes of the experiments, we segregated participants into\ntwo distinct groups: (i) Group with visual impairments, and\n(ii) Group with normal vision.\n6.4.1\nEvaluation Procedure\nOur experiments utilize a curated set of videos, which are\nmainly suggested by the American Council of the Blind2.\nWe also collected accessibility videos from YouTube3. For\nevery video used in our evaluation, participants are exposed\nto two versions: the first containing human-crafted AD and\nthe second powered by MM-VID-generated AD. Both ren-\nditions are narrated using text-to-speech (TTS) technology.\nWe have designed two questionnaires for the two groups,\nreferenced in Table 1 and Table 2, respectively. Participants\nwith visual impairments are instructed to base their evalu-\nation exclusively on auditory cues. In contrast, those with\nnormal vision are instructed to consider both visual and au-\nditory elements.\nThe assessment adopts the standardized Likert scale for\nratings. For each posed question, participants are guided to\nassign a score ranging from 0 to 10, with higher values indi-\ncating more favorable feedback. Furthermore, participants\nare urged to share feedback and remarks concerning their\noverall experience.\n6.4.2\nResults on the Group with Visual Impairments\nWe utilized 3 different videos for our evaluation, with\ndurations of 1 minute, 1 minute 42 seconds, and 2 minutes\n2The Audio Description Project: https://adp.acb.org/\n3Apple Accessibility: https://www.youtube.com/watch?v=\nSL7YSqlEd8k\n7\n42 seconds, respectively. Each of the 4 participants with\nvisual impairment was well versed with screen reader\nand other common accessibility tools. After listening to\nthe audio descriptions for each video, they were asked to\nrespond to the 4 questions outlined in Table 1.\nHypotheses and Results\nH1:\nThe MM-VID-generated audio description and\noriginal video dialogues are effectively presented to the\nparticipants.\nResults:\nUsing the Likert scale (0=Not Effective to\n10=Most Effective) the participants rated the effectiveness\nof the delivery of human-crafted AD and MM-VID-\ngenerated AD. On average, participants gave 7.14 \u00b1 1.39\nfor MM-VID-generated AD and 8.33 \u00b1 0.90 for human-\ncrafted AD, which shows a MM-VID-generated AD very\nclose to human-crafted one in terms of effective delivery\n(Figure 5).\nH2:\nParticipants are able to follow the main story\nline of the video based on MM-VID-generated audio\ndescription only.\nResults:\nUsing the Likert scale (0=Not Informative to\n10=Highly Informative) the participants rated the infor-\nmativeness of human-crafted AD and MM-VID-generated\nAD. On average, participants gave 7.14 \u00b1 1.16 for MM-\nVID-generated AD and 9.29 \u00b1 0.91 for human-crafted AD,\nwhich shows little difference in informativeness between\nMM-VID-generated AD and human-crafted one (Figure\n5).\nH3:\nMM-VID-generated AD and human-crafted AD\nare close in terms of voice and audio quality.\nResults:\nUsing the Likert scale (0=Low Quality to\n10=High Quality) the participants rated the voice and audio\nquality on average as 8.91 \u00b1 1.23 for MM-VID-generated\nAD and 9.07 \u00b1 0.65 for human-crafted AD. This minimal\ndifference between the scores indicates the close-to-human\nvoice and audio quality of MM-VID-generated AD (Figure\n5).\nDiscussion:\nThe results show that the participants\u2019 overall satisfaction\nof MM-VID-generated ADs was on average around 2\npoints less than human-crafted ones in the Likert scale\n(0=Not Satisfied to 10=Highly satisfied) (Figure 5). Some\nof the difficulties indicated by participants while listening\nto MM-VID-generated ADs were 1) occasional overlaps\nbetween AD audio and original video dialogues 2) wrong\ndescriptions due\nto hallucinations of\nGPT-4V(ision).\nRegardless of the difference in overall satisfaction, all the\nparticipants agreed that MM-VID-generated AD can pro-\nEffect. Delivery\nInformative\nAudio Quality\nSatisfaction\n0\n2\n4\n6\n8\n10\nRating\nResults on Group with Visual Impairments\nHuman-written AD\nMMVid-generated AD\nFigure 5. Results on the group with visual impairments. MM-\nVID-generated AD is close to human-generated ones in terms\nof audio quality and effectiveness of delivery. However, MM-\nVID\u2019s AD yields lower satisfaction levels compared to the human-\ngenerated ones. This was primarily attributed to occasional over-\nlaps between the audio descriptions and the video dialogues.\nvide a cost-effective and scalable solution. Thus, millions\nof videos that cannot afford to be professionally audio\ndescribed, can be auto-processed by a tool like MM-VID\nto make them accessible to the visual-impaired community.\n6.4.3\nResults on the Group with Normal Vision\nFor sighted individuals, we used the same set of videos as\nwe used for individuals with visual impairments. All of\nour 5 participants answered to 6 questions listed in Table 2\nafter watching videos embedded with MM-VID-generated\nAD as subtitles and audio track.\nHypotheses and Results\nH1: The MM-VID-generated AD is accurate and conveys\nessential information without overloading the listener.\nResults:\nThe sighted individuals rated the clarify and\naccuracy of MM-VID-generated AD as 7.83 \u00b1 1.24 and\nhuman-curated AD as 8.9 \u00b1 0.74 on average, using the\nLikert scale (0=Not Accurate to 10=Most Accurate).\nIn terms of conciseness, the participants on average gave\n8.73\u00b10.49 for the MM-VID-generated AD and 9.16\u00b10.54\nfor human-curated AD based on the Likert scale (0=Not\nconcise to 10=Most concise).\nThese results indicate\nMM-VID-generated ADs are close to human-curated ones\nin terms of accuracy and conciseness (Figure 6).\nH2:\nThe MM-VID-generated ADs are in sync with\nvisual content and do not overlap with other dialogues\nensuring listeners can follow the story line.\n8\nResults: Participants gave on average 8.90 \u00b1 0.90 and\n7.97 \u00b1 1.54 to human-crafted AD and MM-VID-generated\nAD respectively using the Likert scale (0=Not Infor-\nmative to 10=Highly Informative).\nHuman-crafted AD\nand MM-VID-generated AD received 8.59 \u00b1 0.95 and\n8.53 \u00b1 0.58 respectively on the aspect of timing and\nsynchronization using the Likert scale (0=Not Effective\nto 10=Most Effective). These indicates while listening to\nMM-VID-generated ADs participants were able to follow\nmain story line and found the audios are in sync with video\ncontent very close to that of human-crafted ADs (Figure 6).\nH3: The voice and audio quality of MM-VID-generated\nADs are close to human-crafted ADs.\nResults: The results are very similar to results on group\nwith visual impairments.\nSighted participants rated the\nvoice and audio quality on average as 8.30 \u00b1 0.89 for MM-\nVID-generated AD and as 8.93 \u00b1 0.32 for human-crafted\nAD. Therefore the voice and audio experience did not\ndegrade much while listening to MM-VID-generated ADs\ncompare to human-crafted ADs (Figure 6).\nDiscussion:\nThe evaluations on sighted individuals helped to cross\nverify the hypotheses of individuals with visual impair-\nments, that are based on audio cues only. Although the\noverall satisfaction points for sighted participants with\nMM-VID-generated ADs was on average <1 points lower\nthan human-generated ADs (Figure 6), the overall satis-\nfaction points for participants who were blind was worse.\nThis is expected because sighted individuals had access to\nboth audio and video modalities but individuals with visual\nimpairments did not. We also believe the reason for lower\noverall satisfaction, may have been the lack of practice\nlistening to auto generated ADs. Some of the users also\nmentioned they have preference between pitches of voice\nand number of concurrent audio channels. These may add\nto the reason of lower overall satisfaction.\n6.4.4\nParticipant Feedback\nWe present a collection of interview quotes from our par-\nticipants who were visually impaired, in which they share\ntheir personal experiences and insights about the audio de-\nscriptions (AD) generated by MM-VID. The participants\nexpressed a unanimous desire to continue utilizing this AD\ngeneration service in the future, highlighting its exceptional\nquality (\u201cNearly perfect\u201d), intricate details (\u201cfavorite was\nthe details\u201d), extensive applicability (\u201callowed me to follow\nanything visual\u201d), and the profound impact it has on them\n(\u201cI did not depend on someone else\u201d). Below, we provide\nadditional quotes for further insight.\nP1: \u201cI understand what is going on very quickly\nClarity\nConciseness\nInformative\nTiming and Sync.\nAudio Quality\nSatisfaction\n0\n2\n4\n6\n8\n10\nRating\nResults on Group with Normal Vision\nHuman-written AD\nMMVid-generated AD\nFigure 6. Results on the group with normal vision. MM-VID-\ngenerated AD was on average <1 points lower than human-\ngenerated ADs. The participants were able to follow the main\nstory line and the audios are in sync with the video content.\nand I did not depend on someone else.\u201d\nP2:\n\u201cIf it\u2019s AI-generated, there are so many\nplaces it\u2019s not available, and we need it there.\u201d\nP2: \u201cFirst time listening to auto-generated AD.\nAs a user, if I am offered this AD, I would take it.\u201d\nP3: \u201cNearly perfect. Most favorite was the de-\ntails.\u201d\nP3: \u201cMore information helped me follow the sto-\nryline.\u201d\nP3: \u201cIt allowed me to follow anything visual. It\nfelt natural the way AD describes how the actor\ninteracts with the environment.\u201d\nP3: \u201cI love animal kingdom, and I watch Wild\nEarth safari virtual tour. I would love to have au-\ndio descriptions of Wild Earth videos and daily\nsafaris.\u201d\nP4: \u201cI would like to have auto-generated au-\ndio description for live conferences in Microsoft\nTeams.\u201d\nP4: \u201cIt worked best as the original audio had not\nmuch value.\u201d\nDespite the positive feedback, not all responses were fa-\nvorable:\nP4: \u201cI am skeptical when it becomes subjective.\nSometimes I feel they make up stories which is not\ngood.\u201d\nP4: \u201cAfter listening to the human-generated AD,\nI figured I misunderstood parts of the original\nstory.\u201d\nP1: \u201cIt keeps referring to the same person using\ntheir names instead of pronouns.\u201d\n9\nP4: \u201cI don\u2019t deal well with overlapped or two\nparallel audios.\u201d\nInterestingly, even those participants who provided criti-\ncal feedback still rated the MM-VID-generated AD closely\nto human-generated AD, during the questionnaire sessions.\nThis indicates that, similar to human-curated AD, adapting\nto MM-VID-generated ADs might necessitate some prac-\ntice and acclimatization over time.\n7. Conclusion\nWe have presented MM-VID, a system that synergizes\nwith GPT-4V for advancing video understanding.\nMM-\nVID employs GPT-4V to transcribe video content into\nlong and detailed scripts, thereby enriching LLMs with ad-\nvanced video understanding capabilities. Experimental re-\nsults demonstrate the effectiveness of MM-VID in address-\ning challenging tasks, including comprehension of hour-\nlong videos, analysis across multiple episodes, identifica-\ntion of characters and speakers, and interaction with video\ngames and graphical user interfaces.\nBeyond the development of the MM-VID system, we\nconducted an extensive user study, drawing feedback from a\nvaried group of participants. The outcomes of this study in-\ndicated that the audio descriptions generated by MM-VID\nclosely mirror the quality of those crafted by humans. In\nour future work, we plan to explore SoM [76] and object\ntracking techniques to enhance various tasks and function-\nalities.\nAcknowledgment\nWe are deeply grateful to OpenAI for providing access\nto their exceptional tool [3, 51\u201353].\nWe are profoundly\nthankful to Misha Bilenko for his invaluable guidance and\nsupport. We also extend heartfelt thanks to our Microsoft\ncolleagues for their insights, with special acknowledgment\nto Cenyu Zhang, Saqib Shaikh, Ailsa Leen, Jeremy Curry,\nCrystal Jones, Roberto Perez, Ryan Shugart, Anne Taylor\nfor their constructive feedback.\nReferences\n[1] Dota 2. https://openai.com/research/dota-2,\n2017. 2\n[2] Azure cognitive services apis.\nhttps : / / azure .\nmicrosoft . com / products / cognitive -\nservices, 2023. 4\n[3] Chatgpt can now see, hear, and speak. https://openai.\ncom/blog/chatgpt-can-now-see-hear-and-\nspeak, 2023. 3, 10\n[4] Pygame library. https://www.pygame.org/, 2023. 2,\n7, 31, 32, 33, 34\n[5] Pyscenedetect: Video scene cut detection and analysis tool.\nhttps://www.scenedetect.com/, 2023. 4\n[6] Vlog: Video as a long document.\nhttps://github.\ncom/showlab/VLog, 2023. 3\n[7] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatie Millican, Malcolm Reynolds, et al. Flamingo: a vi-\nsual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022. 2\n[8] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 2\n[9] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid.\nVivit: A video\nvision transformer. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 6836\u20136846,\n2021. 2, 3\n[10] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 2, 3\n[11] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampe-\ndro, and Jeff Clune. Video pretraining (vpt): Learning to act\nby watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639\u201324654, 2022. 2\n[12] Tadas Baltru\u02c7saitis, Chaitanya Ahuja, and Louis-Philippe\nMorency. Multimodal machine learning: A survey and tax-\nonomy. IEEE transactions on pattern analysis and machine\nintelligence, 41(2):423\u2013443, 2018. 2\n[13] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\nIn ICML, volume 2, page 4, 2021. 2, 3\n[14] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zis-\nserman.\nA short note on the kinetics-700 human action\ndataset. arXiv preprint arXiv:1907.06987, 2019. 2, 3\n[15] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299\u20136308, 2017. 2, 3\n[16] Harrison Chase.\nLangchain.\nhttps://langchain.\nreadthedocs.io/, 2023. 3\n[17] Sanyuan Chen, Yu Wu, Zhuo Chen, Jian Wu, Jinyu Li,\nTakuya Yoshioka, Chengyi Wang, Shujie Liu, and Ming\nZhou.\nContinuous speech separation with conformer.\nIn ICASSP 2021-2021 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages\n5749\u20135753. IEEE, 2021. 2\n[18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, March 2023. 3\n[19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 2, 3\n10\n[20] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichten-\nhofer.\nMultiscale vision transformers.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 6824\u20136835, October 2021. 2, 3\n[21] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 6202\u20136211, 2019. 2, 3\n[22] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang\nWang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end\nvideo-language transformers with masked visual-token mod-\neling. arXiv preprint arXiv:2111.12681, 2021. 2, 3\n[23] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang\nWang, Lijuan Wang, and Zicheng Liu. An empirical study\nof end-to-end video-language transformers with masked vi-\nsual modeling. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22898\u2013\n22909, 2023. 2, 3\n[24] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, et al.\nThe\u201d something something\u201d video\ndatabase for learning and evaluating visual common sense.\nIn Proceedings of the IEEE international conference on com-\nputer vision, pages 5842\u20135850, 2017. 2\n[25] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:\nAround the world in 3,000 hours of egocentric video. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18995\u201319012, 2022. 6, 30\n[26] Tengda Han, Max Bain, Arsha Nagrani, G\u00a8ul Varol, Weidi\nXie, and Andrew Zisserman.\nAutoad: Movie description\nin context.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 18930\u2013\n18940, 2023. 2, 6, 7\n[27] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas\nLeung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video\nclassification with convolutional neural networks. In Pro-\nceedings of the IEEE conference on Computer Vision and\nPattern Recognition, pages 1725\u20131732, 2014. 2\n[28] Hildegard Kuehne, Hueihan Jhuang, Est\u00b4\u0131baliz Garrote,\nTomaso Poggio, and Thomas Serre. Hmdb: a large video\ndatabase for human motion recognition.\nIn 2011 Inter-\nnational conference on computer vision, pages 2556\u20132563.\nIEEE, 2011. 3\n[29] Anna Kukleva, Makarand Tapaswi, and Ivan Laptev. Learn-\ning interactions and relationships between movie characters.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9849\u20139858, 2020. 2\n[30] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg,\nMohit Bansal, and Jingjing Liu. Less is more: Clipbert for\nvideo-and-language learning via sparse sampling. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 7331\u20137341, June\n2021. 2, 3\n[31] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.\nTvqa: Localized, compositional video question answering.\nIn EMNLP, 2018. 2, 3\n[32] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr:\nA large-scale dataset for video-subtitle moment retrieval. In\nECCV, 2020. 3\n[33] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,\nLinjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-\ndation models: From specialists to general-purpose assis-\ntants. arXiv preprint arXiv:2309.10020, 2023. 3\n[34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 3\n[35] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint\narXiv:2305.06355, 2023. 3\n[36] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu,\nand Jingjing Liu.\nHero: Hierarchical encoder for video+\nlanguage omni-representation pre-training.\narXiv preprint\narXiv:2005.00200, 2020. 2, 3\n[37] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng\nLiu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-\nlanguage understanding as masked language modeling. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 23119\u201323129, 2023. 2,\n3\n[38] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen,\nRohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang,\nWilliam Yang Wang, et al.\nValue: A multi-task bench-\nmark for video-and-language understanding evaluation. In\n35th Conference on Neural Information Processing Systems\n(NeurIPS 2021) Track on Datasets and Benchmarks, 2021.\n2, 3\n[39] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe\nGan, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nSwin-\nbert: End-to-end transformers with sparse attention for video\ncaptioning.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 17949\u2013\n17958, 2022. 2, 3\n[40] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael\nWray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wen-\nzhe Zhao, Weijie Kong, et al.\nEgocentric video-language\npretraining. Advances in Neural Information Processing Sys-\ntems, 35:7575\u20137586, 2022. 2, 3\n[41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning, 2023. 3\n[42] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 3202\u20133211, 2022. 2, 3\n[43] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui\nQiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley:\nVideo assistant with large language model enhanced ability.\narXiv preprint arXiv:2306.07207, 2023. 3\n[44] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video\n11\nunderstanding via large vision and language models. arXiv\npreprint arXiv:2306.05424, 2023. 3\n[45] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-\nlinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\nShrimai Prabhumoye, Yiming Yang, et al.\nSelf-refine:\nIterative refinement with self-feedback.\narXiv preprint\narXiv:2303.17651, 2023. 6\n[46] EVERINGHAM Mark.\nHello!\nmy name is... buffy\u2019-\nautomatic naming of characters in tv video. In Proceedings\nof British Machine Vision Conference, 2006, 2006. 2\n[47] Microsoft. Bingchat. https://www.microsoft.com/\nen-us/edge/features/bing-chat, 2023. 2\n[48] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowTo100M:\nLearning\na\nText-Video\nEmbedding\nby\nWatching Hundred Million Narrated Video Clips. In ICCV,\n2019. 2\n[49] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman.\nVoxceleb: a large-scale speaker identification dataset. arXiv\npreprint arXiv:1706.08612, 2017. 2\n[50] Arsha Nagrani and Andrew Zisserman. From benedict cum-\nberbatch to sherlock holmes: Character identification in tv\nseries without a script.\narXiv preprint arXiv:1801.10442,\n2018. 2\n[51] OpenAI. Gpt-4 technical report. 2023. 2, 3, 10\n[52] OpenAI. Gpt-4v(ision) system card. 2023. 2, 3, 10\n[53] OpenAI. Gpt-4v(ision) technical work and authors. https:\n//cdn.openai.com/contributions/gpt- 4v.\npdf, 2023. 10\n[54] Joon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bernstein.\nGenerative agents: Interactive simulacra of human behavior.\narXiv preprint arXiv:2304.03442, 2023. 2\n[55] Karine Pires and Gwendal Simon. Youtube live and twitch:\na tour of user-generated live streaming systems. In Proceed-\nings of the 6th ACM multimedia systems conference, pages\n225\u2013230, 2015. 2\n[56] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,\nChristine McLeavey, and Ilya Sutskever.\nRobust speech\nrecognition via large-scale weak supervision.\nIn Inter-\nnational Conference on Machine Learning, pages 28492\u2013\n28518. PMLR, 2023. 3\n[57] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt\nSchiele. A dataset for movie description. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 3202\u20133212, 2015. 2, 6, 7\n[58] Noah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning, 2023.\n6\n[59] Mustafa Shukor, Corentin Dancette, Alexandre Rame, and\nMatthieu Cord. Unified model for image, video, audio and\nlanguage tasks. arXiv preprint arXiv:2307.16184, 2023. 2\n[60] Cees GM Snoek and Marcel Worring. Multimodal video in-\ndexing: A review of the state-of-the-art. Multimedia tools\nand applications, 25:5\u201335, 2005. 2\n[61] David Snyder, Daniel Garcia-Romero, Gregory Sell, Alan\nMcCree, Daniel Povey, and Sanjeev Khudanpur.\nSpeaker\nrecognition for multi-speaker conversations using x-vectors.\nIn ICASSP 2019-2019 IEEE International conference on\nacoustics, speech and signal processing (ICASSP), pages\n5796\u20135800. IEEE, 2019. 2\n[62] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng\nZhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan\nLu, Jenq-Neng Hwang, et al. Moviechat: From dense to-\nken to sparse memory for long video understanding. arXiv\npreprint arXiv:2307.16449, 2023. 2, 3\n[63] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 3\n[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 3\n[65] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai,\nLu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: A\ntracklet-centric multimodal and versatile video understand-\ning system. arXiv preprint arXiv:2304.14407, 2023. 3\n[66] Jinpeng\nWang,\nYixiao\nGe,\nRui\nYan,\nYuying\nGe,\nKevin\nQinghong\nLin,\nSatoshi\nTsutsui,\nXudong\nLin,\nGuanyu Cai, Jianping Wu, Ying Shan, et al.\nAll in one:\nExploring unified video-language pre-training. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6598\u20136608, 2023. 2, 3\n[67] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,\nJingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai\nLin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A\nsurvey on large language model based autonomous agents,\n2023. 2\n[68] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang. Vatex: A large-scale, high-\nquality multilingual dataset for video-and-language research.\nIn The IEEE International Conference on Computer Vision\n(ICCV), October 2019. 2, 3\n[69] Yunbo Wang, Mingsheng Long, Jianmin Wang, and Philip S\nYu. Spatiotemporal pyramid network for video action recog-\nnition. In Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pages 1529\u20131538, 2017. 2,\n3\n[70] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form\nvideo understanding. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1884\u20131894, 2021. 2\n[71] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,\nZicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-\nerative region-to-text transformer for object understanding.\narXiv preprint arXiv:2212.00280, 2022. 3\n[72] Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen\nBach, Ce Liu, and Michael Zeng. Visual clues: Bridging vi-\nsion and language foundations for image paragraph caption-\ning. Advances in Neural Information Processing Systems,\n35:17287\u201317300, 2022. 3\n12\n[73] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nCVPR, 2016. 3\n[74] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal\nlearning with transformers: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2023. 2\n[75] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong\nWang, Weidong Liu, and Yang Liu. Exploring large language\nmodels for communication games: An empirical study on\nwerewolf. arXiv preprint arXiv:2309.04658, 2023. 2\n[76] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan\nLi, and Jianfeng Gao.\nSet-of-mark prompting unleashes\nextraordinary visual grounding in gpt-4v.\narXiv preprint\narXiv:2310.11441, 2023. 10\n[77] Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant,\nDongdong Chen, Yu Shi, Yichong Xu, Yao Qian, Mei Gao,\nYi-Ling Chen, et al.\ni-code: An integrative and compos-\nable multimodal learning framework. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 37, pages\n10880\u201310890, 2023. 2\n[78] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn\nof lmms: Preliminary explorations with gpt-4v(ision). arXiv\npreprint arXiv:2309.17421, 2023. 2, 3, 4\n[79] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang.\nMm-react: Prompting\nchatgpt for multimodal reasoning and action. arXiv preprint\narXiv:2303.11381, 2023. 3, 4\n[80] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img:\nIterative self-refinement with gpt-4v (ision) for auto-\nmatic image design and generation.\narXiv preprint\narXiv:2310.08541, 2023. 6\n[81] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li,\nChunyuan Li,\net al.\nFlorence:\nA new\nfoundation model for computer vision.\narXiv preprint\narXiv:2111.11432, 2021. 2\n[82] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Mer-\nlot: Multimodal neural script knowledge models. Advances\nin Neural Information Processing Systems, 34:23634\u201323651,\n2021. 2\n[83] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou,\nYilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang\nGan. Building cooperative embodied agents modularly with\nlarge language models.\narXiv preprint arXiv:2307.02485,\n2023. 2\n[84] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv preprint arXiv:2306.02858, 2023. 3\n[85] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al.\nOpt: Open pre-trained\ntransformer language models, 2022.\nURL https://arxiv.\norg/abs/2205.01068. 3\n13\nScript Generation Using GPT-4 \n(continued):\nChris Bryant from the Rockies, employing a \nseries of breaking stuff and tempting \nfastballs at the top of the strike zone.\n0:00:47 - 0:01:02: Next up, KRON from the \nRockies pops the ball to the infield. Dodgers' \ncatcher, Will Smith, steps out and catches the \nball despite a one-out double, stopping the \nRockies from making a score.\n0:01:03 - 0:01:18: The video pans to Dodgers' \nlead-off hitter, Mookie Betts, who manages to \nhit a lead-off single to the centre field, setting \na positive start for the Dodgers.\n0:01:19 - 0:01:34: Will Smith steps up and \ndrives a fly ball to left which goes deep, \nresulting in him scoring a home run for the \nthird game in a row.\n0:01:35 - 0:01:50: Scene next highlights Max \nMuncy's performance who contributes to the \nDodgers scoring by cracking a ball to the \nleft-center field which is then picked up by the \nRockies' fielder.\n0:01:51 - 0:02:06: The scene changes to a duo \nof spectacular plays by the Dodgers. First, a \ndouble play managed by shortstop Tovar, \nswiftly eliminating JD Martinez to curb the \nfirst inning. Then in the second inning, we see \nUrias successfully striking out Rockies' Ryan \nMcMahon.\n0:02:07 - 0:02:22: Urias faces a setback as \nMcMahon bounces back with a high-quality \ndeep shot to the center field, netting himself \na double. \n0:02:23 - 0:02:38: Despite the Rockies \nimproving, Urias ends the inning by striking \nout the Rockies' third baseman, Ellen Trejo, \nleaving McMahon stranded at second base.  \n0:02:39 - 0:02:54: The tables turn as the \ndodgers regain their footing. Mookie Betts \nsmacks the ball into right field and makes his \nway safely to second base. \n0:02:55 - 0:03:10: Dodgers player Will Smith \ntakes a turn at bat. After a quick scuffle with \nthe ball, he blasts it into left field, scoring yet \nanother home run.\n0:03:11 - 0:03:26: The Dodgers are in a roll. Max \nMuncie adds another point to the Dodgers lead by \nsending the ball deep into the right field.\n0:03:27 - 0:03:42: The Rockies' are unfortunate to \nlose a run despite a successful hit due to swift \ncoordination between the Dodgers' players.\n0:03:43 - 0:03:58: Jason Hayward from Dodgers \nadds to Rockies' dismay by smacking a lead-off shot \nin the third inning of the game.\n0:03:59 - 0:04:14: Julio Urias, from the Dodgers, \nefficiently strikes out the Venezuelans' Luis \nMontero and Ryan McMahon in quick succession. \n0:04:15 - 0:04:30: A moment of respite for the \nRockies as Urias throws a pitch, allowing the batter \nto hit a ground ball into infield. However, it is \nquickly fielded, ending their chances. The innings \nend with the Dodgers in the lead by four runs. \n0:04:31 - 0:04:46: The scene captures dodgers' Max \nMuncy blasting a hit into the field, which bumps \nthe Dodgers' lead to four.\n0:04:47 - 0:05:02: Despite a promising start for the \nRockies at the top of the third, their momentum \nslows down as they lose a run due to a double play \ninduced by Urias.\n0:05:03 - 0:05:18: Hayward makes a fantastic \nlead-off shot for the Dodgers' in the third \ninning, continuing their advantageous streak.\n0:05:19 - 0:05:34: Back at the pitching mound, \nUrias expertly strikes out the Rockies' Luis Montero \nand leaves the Rockies scoreless for 6 games.\n0:05:35 - 0:05:50: Urias's superior pitching \ncontinues as he strikes out Ryan McMahon and \nskillfully fields a sharp grounder ending the fourth \ninning, serving a huge blow to the Rockies.\n0:05:51 - 0:06:06: Scene cuts to Dodgers' Mookie \nBetts, who despite an injury, hammers a hit into the \nfield, landing him a double.\n0:06:07 - 0:06:22: Despite leading the series, \nrt \nFigure 7. An example of MM-VID\u2019s execution flow (continued). We present the full script generated by MM-VID. The original video is\navailable at https://www.youtube.com/watch?v=-pNe0p4H8ec\n14\nHow did you know the sound is different?\nThe commentator mentions that there is a \ndifferent sound when the ball leaves Will \nSmith's bat, implying that he might have \nbecome stronger.\nEvidence in ASR:\n1:27\nthe track at the wall Smith goes deep\n1:29\nagain third game in a row\n1:34\nfrom Big Time trajectory on that one\n1:37\nWill Smith must have gotten stronger in\n1:39\nthe off season there's a different sound\n1:41\nnow when the ball leaves his Bat so the\n1:44\nDodgers on the board quickly two-run\n1:46\nhome run in the first inning for Will\n1:48\nSmith first time in his career he's gone\n1:50\ndeep three games in a row one two to\n1:52\nMuncie is crack the left center field\nWho won the game, and what are their final \nscores?\nShow me the most exciting moment in this \nvideo\nThe most exciting moment in this video is \nwhen Will Smith hits a home run in the first \ninning at timestamp 0:01:19 - 0:01:34 in \nthis video\nWill Smith hits a home run at 0:01:19 - 0:01:34 \nCompared to the last season, did Will Smith \nbecome stronger?\nBased on the video, it is suggested that Will \nSmith has become stronger compared to \nthe last season, as there is a different \nsound when the ball leaves his bat.\nrt \nCan you summarize the game?\nThe game was a thrilling and eventful match \nbetween the Colorado Rockies and the Los \nAngeles Dodgers. The Dodgers dominated \nthe game with superior pitching and \npowerful hits, while the Rockies showed \nsome fight towards the end but ultimately \nfell short.\nScript Generation Using GPT-4\n(continued):\nDodgers' catcher Will Smith lifts a high ball \ninto center field which brings Mookie Betts \nrunning home, netting the Dodgers another \npoint.\n0:06:23 - 0:06:38: The Rockies' show some \nfight as Chris Brian leads the ninth innings \nwith a double bringing the Rockies trailing the \nDodgers by five.\n0:06:39 - 0:06:54: A series of unfortunate \nevents for the Rockies as batters Montero and \nDiaz get directly hit by pitched balls \nconsecutively. Trejo from the Rockies finally \nmanages to hit a fly ball into left field, \nallowing Montero to make it to the home \nplate, closing the gap between the two teams \nto three.\n0:06:55 - 0:07:10: The game comes to a close \nas Charlie Blackman grounds to the first base, \nending the game in favor of Dodgers, after a \nthrilling and eventful match. \n0:07:11 - 0:09:09: The scene concludes with \nthe teams leaving the field. \n(continued)\nWill Smith hits a home run in the first inning \nat timestamp 0:01:19 - 0:01:34\nFigure 8. An example of MM-VID\u2019s execution flow (continued). Given the generated script as reference, MM-VID achieves various video\nunderstanding capabilities, including grounded QA and multimodal reasoning. The original video is available at https://www.\nyoutube.com/watch?v=-pNe0p4H8ec\n15\nJulio Urias had his first strikeout in the \nvideo at 0:00:31, striking out Chris Bryant \nfrom the Rockies.\nWhen did Julio Urias induce a double play?\nJulio Urias induced a double play at around \n0:02:07 in the video.\nJulio Urias induced a double play around 0:02:07\nrt \nThe Los Angeles Dodgers won the game \nwith a final score of 5-2.\nFinal scores were displayed at 0:07:11 - 0:09:09, \nand mentioned by the commentator at 0:08:53\nWho are the best pitchers in this video?\nJulio Urias was the best pitcher in this \nvideo, striking out multiple batters and \ninducing a double play.\nWhen did Julio Urias have his first strikeout?\nEvidence in the Generated Script:\n0:02:07 - 0:02:22: Urias faces a setback as \nMcMahon bounces back with a high-quality deep \nshot to the center field, netting himself a double.  \n0:03:59 - 0:04:14: Julio Urias, from the Dodgers, \nefficiently strikes out the Venezuelans' Luis \nMontero and Ryan McMahon in quick succession. \n0:04:47 - 0:05:02: Despite a promising start for the \nRockies at the top of the third, their momentum slows \ndown as they lose a run due to a double play \ninduced by Urias.\n0:05:35 - 0:05:50: Urias's superior pitching \ncontinues as he strikes out Ryan McMahon and \nskillfully fields a sharp grounder ending the \nfourth inning, serving a huge blow to the Rockies. \n(continued)\nJulio Urias had his first strikeout at 0:00:31\nFigure 9. An example of MM-VID\u2019s execution flow (continued). Given the generated script as reference, MM-VID achieves various video\nunderstanding capabilities, including grounded QA and multimodal reasoning. The original video is available at https://www.\nyoutube.com/watch?v=-pNe0p4H8ec\n16\nFigure 10. Case studies of MM-VID \u2019s capabilities and application scenarios: hour-long video comprehension. Figures 11-13 show\ncontinued outputs. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n17\nScene 31: 00:06:46\nAn elephant and a gorilla roam the jungle.\nScene 32: 00:06:57\nThe gorilla relocates deeper into the jungle.\nScene 33: 00:07:05\nThe gorilla is seen wading in a small body of \nwater.\nScene 34: 00:07:13\nTwo gorillas spotted playing in the grass.\nScene 35: 00:07:27\nAdmiring the grass, the gorilla takes a deserved \nbreak.\nScene 36: 00:07:32\nAerial view of gorillas and elephants in the \nmarsh.\nScene 37: 00:07:37\nClose-up of the elephant in the water.\nScene 38: 00:07:43\nTwo elephants are seen fighting and splashing in \nthe water.\nScene 39: 00:08:00\nA lone elephant is caught grazing in the \nlandscape.\nScene 40: 00:08:08\nScene transitions to a group of gorillas in their \nnatural habitat.\nScene 41: 00:08:16\nA herd of elephants is spotted in shallow water.\nScene 42: 00:08:22\nAerial view of a vast landscape with a mountain \npeak and flying birds.\nScene 43: 00:08:28\nAerial view of a wide, green landscape with \nscattered water bodies and flying birds.\nScene 44: 00:08:34\nAerial view of a mountainous range with a valley \nand a river.\nScene 45: 00:08:41\nClose-up of gorilla hanging from a tree branch.\nScene 46: 00:09:03\nClose-up of a bug on a leaf.\nScene 47: 00:09:43\nLandscape view of mountains with clouds hovering \noverhead.\nScene 48: 00:10:32\nClose-up of a bird as it takes flight.\nScene 49: 00:10:50\nClose-up of a gorilla's face hidden behind the leaves.\nScene 50: 00:11:23\nA gorilla is seen searching the dense forest floor.\nScene 51: 00:12:15\nClose-up of a black bird with large eyes.\nScene 52: 00:13:03\nA black bird is seen walking in a jungle path. \nScene 53: 00:13:29\nBird of prey perched on tree branches in the jungle. \nScene 54: 00:15:05\nClose-up of a bird's eye in the forest.\nScene 55: 00:16:38\nA shot of a gorilla hiding behind a tree in a dense \njungle.\nScene 56: 00:17:50\nTwo elephants engage in a playful fight near a water \nbody.\nScene 57: 00:19:20\nThe leopard is seen slowly moving through the \nfoliage.\nScene 58: 00:21:51\nBird seen entering a hole in a tree.\nScene 59: 00:22:22\nA red ant crawls on a fern.\nScene 60: 00:23:32\nA peculiar mushroom blooms in the darkness near \nthe forest floor.\nScene 61: 00:25:07\nA deer seen foraging in the foliage.\nFigure 11. Case studies of MM-VID \u2019s capabilities and application scenarios: hour-long video comprehension; We present the full script\ngenerated by MM-VID. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n18\nThe video is a documentary that explores the \ndiverse and rich ecosystems of jungles and \nrainforests around the world. It showcases various \nanimal species, their behaviors, and their roles in \nmaintaining the balance of these ecosystems. The \ndocumentary also highlights the threats faced by \nthese habitats, such as deforestation and \npoaching and emphasizes the importance of\nCan you summarize the video?\nCordyceps fungus is a parasitic fungus that \ninfects and takes control of its host, usually an \ninsect. The fungus manipulates the host's \nbehavior, eventually leading to the host's death. \nThe fungus then erupts from the host's body and \nreleases spores to infect more victims. Cordyceps \nfungus plays a role in maintaining the balance of \nWhat is Cordyceps fungus?\nFigure 12. Case studies of MM-VID \u2019s capabilities and application scenarios: hour-long video comprehension; MM-VID is able to\nsummarize the video and address specific questions with regard to the video contents. Blue highlights the correct answers. The original\nvideo is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n19\nPitcher plants can be found in the video around \nthe timestamp 00:18:18.\nFigure 13. Case studies of MM-VID \u2019s capabilities and application scenarios: hour-long video comprehension; MM-VID is able to\nlocalize the specific moments or topics in the lengthy documentary video. Blue highlights the correct answers. The original video is\navailable at https://www.youtube.com/watch?v=um2Q9aUecy0\n20\nLoading video\u2026\nVideo length is  46 minutes 54 seconds.\nLoading video\u2026\nVideo length is  48 minutes 30 seconds.\nCan you summarize the two episodes?\nEpisode 1 Summary: In the first episode of \nHouse of Cards Season 2, Frank Underwood \nprepares for his promotion to Vice President. \nDoug Stamper works on erasing any leads \nthat could connect him and Frank to their \npast actions. Claire faces a legal battle against \nGillian, while Zoe considers working with \nFrank again. Throughout the episode, various \ncharacters engage in tense conversations and \npolitical maneuvering, with Frank ultimately \nbeing sworn in as Vice President in a private \nceremony.\nEpisode 2 Summary: In the second episode, \nFrank is announced as Vice President and \nfaces obstacles in his new role. The US \nengages in talks with China, which fall apart \nover cyber warfare issues. Jackie Sharp plans \nto run for the new whip position, facing \nchallenges along the way. Meanwhile, Lucas \ninvestigates Zoe's death and delves into\nWhat are the main challenges Frank facing in \nthe two videos?\nMain challenges Frank faces in the two \nepisodes:\n- Erasing all leads connecting him and Doug to \ntheir past actions, particularly their involvement \nin Peter Russo's death.\n- Managing the political fallout from Zoe's \ndeath and preventing any further investigation \ninto his involvement.\n- Navigating the complex relationships and \npolitical strategies within the government, \nparticularly in relation to the Whip race and \nthe US-China talks.\n- Balancing his personal life with his new role \nas Vice President, including dealing with the \nincreased security measures and lack of privacy.\nWhen did Frank win the promotion? include \nthe timestamps \nFrank wins the promotion to Vice President in \nSeason 2 Episode 1, with the swearing-in \nceremony taking place between 0:02:24 and \n0:02:36 in Episode 2.\nDescribe Frank's journey in the two videos.\nFrank's journey in these two episodes involves \nhim preparing for his new role as Vice \nPresident while trying to maintain control over \nvarious situations. He deals with the \nconsequences of his past actions, such as \nmanaging Do g Stamper's attempts to erase\nrt \n(continued)\nthe deep web for information. The episode \nfeatures various tense conversations and \npolitical strategizing, with characters \nnavigating complex relationships and making \ncrucial decisions.\nFigure 14. Case studies of MM-VID \u2019s capabilities and application scenarios: multi-video episodic analysis; Given multiple video files as\ninputs, MM-VID can summarize the video series, perform cross-episodic reasoning, describe a specific character\u2019s journey across multiple\nvideos. Figure 15 shows continued outputs. Blue highlights the correct answers. The two videos are from House of Cards, Season 2:\nepisode 1 and episode 2.\n21\nDescribe Frank's journey in the two videos.\nFrank's journey in these two episodes \ninvolves him preparing for his new role as \nVice President while trying to maintain \ncontrol over various situations. He deals with \nthe consequences of his past actions, such as \nmanaging Doug Stamper's attempts to erase \ntheir tracks and handling the fallout from \nZoe's death. Frank also navigates the political \nlandscape, working with Jackie to secure her \nposition as Whip and engaging in tense \nnegotiations with China. Throughout these \nchallenges, Frank continues to demonstrate \nhis cunning and ruthlessness in achieving his \ngoals.\nFigure 15. Case studies of MM-VID \u2019s capabilities and application scenarios: multi-video episodic analysis; Given multiple video files as\ninputs, MM-VID can summarize the video series, perform cross-episodic reasoning, describe a specific character\u2019s journey across multiple\nvideos. Blue highlights the correct answers. The two videos are from House of Cards, Season 2: episode 1 and episode 2.\n22\nLoading video\u2026\nVideo length is  23 minutes 36 seconds.\nLoading video\u2026\nVideo length is  21 minutes 57 seconds.\nCan you summarize the two episodes?\nIn Episode 131, Ash and Leon face off in the \nfinals of the Pok\u00e9mon World Coronation \nSeries Masters Eight Tournament. The \nintense battle sees various Pok\u00e9mon, \nincluding Rillaboom, Dragonite, and \nCinderace, fighting fiercely. The combined \npower of Pikachu's Z-Move and Charizard's \nGigantamax attack causes Eternatus to \nappear unexpectedly.\nIn Episode 132, the final round of the \ntournament continues with the appearance \nof Eternatus, which recharges Leon and Ash's \nDynamax bands. Ash and Pikachu face Leon's \nGigantamax Cinderace and Charizard in a \nthrilling battle. With the support of their \nfriends and Pok\u00e9mon, Ash and Pikachu \nemerge victorious, making Ash the new \nWorld Champion.\nDid Pikachu use 10000000 Volts move?\nYes, Pikachu used the 10,000,000 Volts move \n(Timestamp: 00:20:09 - 00:20:14 in Episode \n131).\nDid Pikachu use Gigantamax? \nYes, Pikachu used Gigantamax. (Timestamp: \n00:03:45 - 00:03:49 in Episode 132)\nShow me the keyframe when Pikachu \ndefeats Charizard\nThe keyframe when Pikachu defeats \nCharizard is at timestamp 00:16:02-00:16:03 \nin Episode 132.\nrt \nFigure 16. Case studies of MM-VID \u2019s capabilities and application scenarios: multi-video episodic analysis; Given multiple video files\nas inputs, MM-VID can summarize the video series, and localize specific moments across multiple videos. Blue highlights the correct\nanswers. The two videos are from Pok\u00b4emon Journeys: episode 131 and episode 132.\n23\nLoading video\u2026\nVideo length is  2 minutes 43 seconds.\nVisual Prompting:\nFaraon\nThut\nNefer\n(continued)\n0:00:06 - 0:00:08: Nefer looks down the cliff's \nedge, revealing her concerned reaction.\n0:00:12 - 0:00:14: A shot of a rock with a golden \nankh necklace sets an ominous tone.\n0:00:15 - 0:00:16: Transition to a worried Nefer \nunder the open sky.\n0:00:17 - 0:00:18: Faraon overlooks Thut lying \non the rocky ground.\n0:00:18 - 0:00:24: Various interactions between \nThut, Nefer, and Faraon in the desert transpire.\n0:00:24 - 0:00:30: Nefer is hugged by Faraon as \nThut watches closely.\n0:00:30 - 0:00:36: Thut shares a moment with \nNefer before seating himself beside a frog as \nFaraon joins Nefer.\n0:00:36 - 0:00:42: Nefer and Faraon approach a \nslightly confused Thut.\n0:00:42 - 0:00:48: Thut and Nefer stand alone \nexchanging a few words.\n0:00:48 - 0:00:54: Faraon joins Thut and Nefer, \nlooking at a ring Thut holds.\n0:01:00 - 0:01:06: Thut presents a ring between \nhis and Nefer's fingers.\n0:01:06 - 0:01:12: Thut listens as Nefer talks \nexcitingly about the ring.\n0:01:12 - 0:01:18: Faraon oversees a close \nmoment between Thut and Nefer.\n0:01:18 - 0:01:24: Nefer and Thut stand in front \nof an audience as a small girl with a snake \nappears.\n0:01:24 - 0:01:30: Thut and Nefer perform in \nfront of a crowd.\n0:01:42 - 0:01:48: Thut and Nefer share a kiss as \nthe crowd cheers on, transitioning to the girl \nwith her crocodile.\n0:01:54 - 0:02:00: Thut and Nefer perform on \nstage with others in a palace courtyard.\n0:02:12 - 0:02:18: Multiple performances ensue \nin the courtyard, ending with Thut and Nefer \nsharing a gaze.\n0:02:18 - 0:02:24: Thut and Nefer continue their \nromantic interaction under the watchful eyes of \ntheir audience.\n0:02:24 - 0:02:30: The dancing in the courtyard \ncontinues.\n0:02:30 - 0:02:36: Thut, Nefer, and Faraon \nperform on stage as their performers enjoy \ntheir display.\n0:02:36 - 0:02:43: As the sun sets, Thut and \nNefer conclude their musical performance on a \nbalcony.\nInvoke tools: Scene Detection, Metadata \nCollection\nScene Detection:\n0:00:00, 0:00:01, 0:00:02, 0:00:03, 0:00:04, \n0:00:05, 0:00:06, 0:00:07, 0:00:08, 0:00:09,\n..., 0:02:39, 0:02:40, 0:02:41, 0:02:42\nMetadata Collection:\nTitle: Mummies\nAbstract: Thut is a young charioteer. Nefer is the \ndaughter of Faraon. In this video, Thut saves \nhimself from falling off a cliff. Thut and Nefer \nmeets Faraon. Thut and Nefer gets married in \nfront of the goddess statue and all people. Thut \nand Nefer celebrates by singing and dancing. \nScript Generation:\n0:00:00 - 0:00:06: Nefer, an Egyptian princess, is \nseen gazing at the desert landscape and \nexamining a rocky cliff.\nrt \nFigure 17. Case studies of MM-VID \u2019s capabilities and application scenarios: character identification; By providing the face photos\nof the characters and their names, MM-VID can generate the script with the identified characters. Blue highlights the correct character\nidentification. The original video is available at https://www.tokyvideo.com/video/mummies-online-free-film, from\n1:19:47 to 1:22:30.\n24\nLoading video\u2026\nVideo length is  2 minutes 57 seconds.\nVisual Prompting:\nNimona\nBallister \nBoldheart\nRegenerated ASR with Identified Speakers:\n0:00:01 Ballister Boldheart: Good trade flesh \nand we run and we hide.\n0:00:33 Unknown: That was weird.\n0:00:34 Nimona: Hey boss.\n0:00:39 Nimona: I love the secret layer.\n0:00:42 Nimona: The garbage and the smell of \nsadness really pull the whole thing together.\n0:00:45 Ballister Boldheart: Well, wait, wait, \nwait. Who? Who are you?\n0:00:47 Nimona: The names. Pneumonia. \nWhoa. Yeah. Sick arm.\n0:00:48 Ballister Boldheart: And how did you?\n0:00:51 Unknown: Did it bleed a lot? Did they \nlet you keep the old one?\n0:00:54 Ballister Boldheart: No, let's go. That's \nwrong with you.\n0:00:58 Nimona: Can I have it? What? Put that?\n0:00:59 Ballister Boldheart: Down that is.\n0:01:00 Ballister Boldheart: Not for little girls, \nlittle.\n0:01:02 Nimona: Girls. OK, how old do you \nthink I?\n0:01:05 Ballister Boldheart: AM 10.\n0:01:09 Unknown: Alright, home.\n0:01:09 Ballister Boldheart: We are more or \nless than 10.\n0:01:10 Nimona: Not a lot of kids in your life, \nhuh?\n0:01:12 Ballister Boldheart: You know what?\n0:01:13 Ballister Boldheart: No. And then like if \nyou stay that?\n0:01:14 Ballister Boldheart: Way you have to go \njob.\n0:01:15 Nimona: But I'm here about this.\n0:01:16 Unknown: What job?\n0:01:18 Nimona: Oh, it's all here.\n0:01:18 Nimona: It's all here in my application.\n0:01:22 Nimona: This is just a bunch of \ndrawings.\n0:01:26 Unknown: Very disturbing drawings.\n0:01:28 Ballister Boldheart: Oh, look, it's me on \na rhinoceros skewering.\n0:01:32 Ballister Boldheart: Guards like a \nhuman kebab.\n0:01:34 Nimona: Yeah. Do you like?\n0:01:35 Nimona: It I thought a visual aid.\n0:01:37 Unknown: Really embarrass me. Puff up \nhere. So about the job. What job?\n0:01:45 Nimona: To be your.\nInvoke tools: ASR, Metadata Collection\nASR:\n0:00:01 Guest_0:  Good trade flesh and we run and \nwe hide.\n0:00:33 Guest_2:  That was weird.\n0:00:34 Guest_3:  Hey boss.\n0:00:39 Guest_3:  I love the secret layer.\n\u2026\n0:02:53 Guest_10:  From you. No. Fine then from.\n0:02:55 Guest_10:  Who? The guy who cut off your \narms.\nMetadata Collection:\nTitle: Nimona\nAbstract: Ballister boldheart is a knight. Nimona is a \nmischievous teen who is a shapeshifting creature. \nThe video shows how Ballister met Nimona first \ntime.Nimona thought boldheart was the villain but \ndisappointed to know he is not.\nn\nco\nth\nFigure 18. Case studies of MM-VID \u2019s capabilities and application scenarios: speaker identification; By leveraging visual prompting,\nMM-VID can enhance ASR predictions with the speakers\u2019 identity. Blue and Red highlight the correct and incorrect predictions, respec-\ntively. Figure 19 shows continued outputs. The original video is available at https://www.netflix.com/title/81444554,\nfrom 9:52 to 12:52.\n25\n(continued):\n0:01:45 Nimona: Sidekick, you know.\n0:01:46 Nimona: To help you do whatever it \ntakes to.\n0:01:47 Nimona: Get revenge on.\n0:01:48 Nimona: The cold, cruel world that \nrejected you. Shall we pillage village? Lay low \nuntil they don't remember you, and then we \nrise like a phoenix. From the Ashes, overthrow \nthe government.\n0:01:58 Unknown: Or we could.\n0:01:59 Nimona: Just talk.\n0:02:02 Unknown: The the the point.\n0:02:03 Nimona: Is whatever your dark heart \ndesires. Boss, your sidekick has arrived.\n0:02:08 Ballister Boldheart: I don't need a.\n0:02:09 Ballister Boldheart: Side wait, I am not \na.\n0:02:09 Nimona: Every villain needs a sidekick.\n0:02:12 Unknown: Sweetener. Murdo. That's \nhow you want to kill first. Oh.\n0:02:14 Nimona: Are these your next targets?\n0:02:15 Nimona: Who do?\n0:02:16 Ballister Boldheart: I'm not killing \nanybody.\n0:02:18 Nimona: This one, this guy looks \nextremely punchable.\n0:02:19 Nimona: Right. He is actually extremely.\n0:02:21 Ballister Boldheart: You're right, he is \nactually.\n0:02:22 Ballister Boldheart: Punchable. Wait, \nthat is not a murder wall. It's a.\n0:02:26 Ballister Boldheart: The innocence wall, \nyeah.\n0:02:27 Nimona: Citizens will.\n0:02:30 Nimona: We are you saying that you're \nnot a villain? Yes.\n0:02:36 Nimona: Yes you are.\n0:02:37 Ballister Boldheart: No, no, no. I'm not \na villain, no.\n0:02:42 Ballister Boldheart: But you.\n0:02:43 Ballister Boldheart: Disappointed that \nI'm not a murderer.\n0:02:46 Nimona: I mean.\n0:02:46 Nimona: I mean a little. Why?\n0:02:48 Ballister Boldheart: You know what we.\n0:02:49 Ballister Boldheart: Don't know.\n0:02:50 Ballister Boldheart: The real villain is \nstill out there and I do need help not.\n0:02:53 Nimona: From you. No. Fine then from.\n0:02:55 Nimona: Who? The guy who cut off \nyour arms.\nFigure 19. Case studies of MM-VID \u2019s capabilities and application scenarios: speaker identification; By leveraging visual prompting,\nMM-VID can enhance ASR predictions with the speakers\u2019 identity. Blue and Red highlight the correct and incorrect predictions, respec-\ntively. The original video is available at https://www.netflix.com/title/81444554, from 9:52 to 12:52.\n26\nScript Generation:\n0:00:00 - 0:00:08 : Mr. Bean enters a classroom and \ntakes a seat next to another man. They both \nanxiously look at their exam papers as they begin \ntheir test.\n0:00:09 - 0:00:16 : Mr. Bean glances over at the other \nman's paper and tries to sneakily copy his answers. \nBoth men continue to work on their tests, unaware \nof their surroundings.\n0:00:17 - 0:00:23 : Close-up shot of Mr. Bean's face \nshowing his frustration as he struggles to figure out \nthe answers to the exam.\n0:00:24 - 0:00:27 : Mr. Bean continues to take the \nexam while holding a pink flamingo pen, struggling \nto concentrate.\n0:00:27 - 0:00:35 : Mr. Bean becomes more \ndesperate, sticking his nose into the exam paper as \nhe continues to copy answers from the man sitting \nnext to him.\n0:00:36 - 0:00:45 : Mr. Bean takes a break, rubbing \nhis eyes as the scene cuts back and forth between \nhim and his colleague.\n0:00:45 - 0:00:54 : The students continue to work on \ntheir task as the scene shows the peaceful and quiet \nenvironment of the exam room.\n0:00:54 - 0:01:00 : Mr. Bean is deep in thought as he \ntaps his pen on the desk, looking more worried and \nnervous about his exam performance.\n0:01:01 - 0:01:02 : Close-up of Mr. Bean struggling \nwith a problem and looking upward with a frustrated \nexpression.\nLoading video\u2026\nVideo length is  1 minutes 44 seconds.\nWhy Mr. Bean cheat in the exam? \nMr. Bean cheats in the exam because he is \nunprepared, lacks confidence in his ability to \nanswer the questions, and feels desperate \nto pass the test. His comedic nature and \ninability to cope with the pressure of the \nexam lead him to attempt cheating as a last \nresort. \nWhy do people find the video funny?\nPeople might find the video funny because of \nMr. Bean's exaggerated expressions and \nactions, such as sticking his nose into the \nexam paper and using a pink flamingo pen, \nwhich add humor to the otherwise tense \nand stressful situation of taking an exam.\n(continued)\n0:01:03 - 0:01:09 : Mr. Bean tries to regain \nfocus in this scene, tapping his fingers as he \nnervously attempts to recall information for \nthe exam.\n0:01:11 - 0:01:12 : Two men, including Mr. \nBean, are discussing a portion of the exam in a \nlibrary, pointing at a book while brainstorming \nanswers.\n0:01:12 - 0:01:21 : Mr. Bean and the man next \nto him become more desperate, whispering to \neach other and trying to cheat as their \nfrustration grows. The mood is tense, mirroring \ntheir anxiety.\nFigure 20. Case studies of MM-VID \u2019s capabilities and application scenarios: audio description generation; Without ASR inputs, MM-\nVID processes the video and generates a detailed script. Blue and Red highlight the correct and incorrect predictions, respectively. The\noriginal video is available at https://www.youtube.com/watch?v=29ZmXkICT1g, from 2:50 to 4:34.\n27\n\u2026\nrohammad examining a small\nrohammad examining a small\nbapl \nbiotects the\nbiotects the\n\u2026\u2026\nFigure 21. Case studies of MM-VID \u2019s capabilities and application scenarios: self-refinement; In this example, MM-VID mistakenly\nidentifies a bird as a rock due to low-resolution video frames. The self-refinement helps rectify the errors, resulting in a more refined\noutput. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www.\nyoutube.com/watch?v=o4UKvgBkO2Y, from 1:06 to 4:17.\n28\n\u2026\n\u2026\n\u2026\nFigure 22. Case studies of MM-VID \u2019s capabilities and application scenarios: fast-changing short videos. The original videos are\navailable at https://www.instagram.com/mealtimewithmummy/reels/\n29\n\u2026\n.\nFigure 23. Case studies of MM-VID \u2019s capabilities and application scenarios: embodied agent. Blue highlights the correct prediction.\nThe original video is collected from Ego4D dataset [25].\n30\nFigure 24. Case studies of MM-VID \u2019s capabilities and application scenarios: playing video game. Blue highlights the correct prediction.\nFigures 25-27 show continued outputs. The video is generated by Pygame library [4].\n31\nFigure 25. Case studies of MM-VID \u2019s capabilities and application scenarios: playing video game. Blue highlights the correct prediction.\nFigures 26-27 show continued outputs. The video is generated by Pygame library [4].\n32\nYou are expert in playing Super Mario Bros.\nWhat is the next controls to be pressed after \nthe last frame. Output JSON.\nAvailable controls are: Jump, ArrowLeft, \nArrowRight, Wait\nOutput:\n    {\n        \"controls\": \"ArrowLeft+Jump\",\n        \"reason\": \"Mario should\"\n    }\n{\n    \"control\": \"Jump\", \n    \"reason\": \"Mario should jump to                    \n                        avoid the goomba\"\n}\n(Agent presses control \u201cJump\u201d)\n{\n    \"con\n    \"rea\n           \n           \n}\nYou are expert in playing Super Mario Bros.\nWhat is the next controls to be pressed after \nthe last frame. Output JSON.\nAvailable controls are: Jump, ArrowLeft, \nArrowRight, Wait\nOutput:\n    {\n        \"controls\": \"ArrowLeft+Jump\",\n        \"reason\": \"Mario should\"\n    }\nFor this sequence of frames of super mario, \nwhat is the next controls.\nOutput:\nFigure 26. Case studies of MM-VID \u2019s capabilities and application scenarios: playing video game. Blue highlights the correct prediction.\nFigure 27 shows continued outputs. The video is generated by Pygame library [4].\n33\nFigure 27. Case studies of MM-VID \u2019s capabilities and application scenarios: playing video game. Blue highlights the correct prediction.\nThe video is generated by Pygame library [4].\n34\nFigure 28. Case studies of MM-VID \u2019s capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and\nincorrect predictions, respectively. Figures 29-32 show continued outputs.\n35\nImagine that you are a robot operating a \nphone. Like how humans operate the phone, \nyou can move your finger to an icon, click an \nicon with your finger, scroll down a screen, or \ntype some texts with the keyboard.\nYou are asked to shop for an ergonomic \nkeyboard, your budget is between $50 and \n$100.\nPrevious action: \"My immediate next move \nwould be to move my finger to the \"Filters\" \nbutton, which is located at the top right corner \nof the screen, next to the \"Windows\" button. I \nwould then click on the \"Filters\" button to open \nthe filter options and narrow down my search \nresults to only show ergonomic keyboards \nwithin my budget range of $50 to $100. \"\nYou have now completed the previous action as \ndescribed above. Below is what you see on the \nphone screen. Please predict your immediate \nnext move. If the action involves moving your \nfinger to an icon, describe the location of that \nicon as detailed as possible.\nMy immediate next move would be to move \nmy finger to the \"Price & Deals\" filter \noption, which is located under the \"Colors\" \nfilter option and above the \"Sort by\" filter \noption. I would then click on the \"Price & \nDeals\" filter option to open the price range \nImagine that you are a robot operating a \nphone. Like how humans operate the phone, \nyou can move your finger to an icon, click an \nicon with your finger, scroll down a screen, or \ntype some texts with the keyboard.\nYou are asked to shop for an ergonomic \nkeyboard, your budget is between $50 and \n$100.\nPrevious action: \"My next move would be to \nmove my finger to the Amazon icon, which is \nlocated in the third row and third column of \nthe screen. I would then click on the icon to \nopen the app and search for an ergonomic \nkeyboard within my budget of $50. \"\nYou have now opened the Amazon app. \nBelow is what you see on the phone screen. \nWhat will you do next? If the action involves \nmoving your finger to an icon, describe the \nlocation of that icon as detailed as possible.\nMy next move would be to move my \nfinger to the search bar, which is located \nat the top of the screen. I would then click \non the search bar and type in \"ergonomic \nkeyboard\" using the keyboard. After \ntyping in the search term, I would click on \nthe \"Search\" button, which is located on \nthe bottom right corner of the keyboard, \nto initiate the search. \nFigure 29. Case studies of MM-VID \u2019s capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and\nincorrect predictions, respectively. Figures 30-32 show continued outputs.\n36\nMy next move would be to move my finger \nto the \"Show 1,987 results\" button, which is \nlocated at the bottom of the screen, below \nthe \"Clear Filters\" button and above the \nnavigation bar. I would then click on the \n\"Show 1,987 results\" button to view the \nsearch results for ergonomic keyboards \nwithin my budget range. \n(continued)\nfilter and set my minimum price to $50 \nand my maximum price to $100. This \nwould further narrow down my search \nresults to only show ergonomic keyboards \nwithin my budget range. \nImagine that you are a robot operating a \nphone. Like how humans operate the phone, \nyou can move your finger to an icon, click an \nicon with your finger, scroll down a screen, or \ntype some texts with the keyboard.\nYou are asked to shop for an ergonomic \nkeyboard, your budget is between $50 and \n$100.\nPrevious action: \" My immediate next move \nwould be to move my finger to the \"Price & \nDeals\" filter option, which is located under the \n\"Colors\" filter option and above the \"Sort by\" \nfilter option. I would then click on the \"Price & \nDeals\" filter option to open the price range filter \nand set my minimum price to $50 and my \nmaximum price to $100. This would further \nnarrow down my search results to only show \nergonomic keyboards within my budget range. \"\nYou have now completed the previous action as \ndescribed above. Below is what you see on the \nphone screen. What will you do next? If the \naction involves moving your finger to an icon, \ndescribe the location of that icon as detailed as \npossible.\nImagine that you are a robot operating a \nphone. Like how humans operate the phone, \nyou can move your finger to an icon, click an \nicon with your finger, scroll down a screen, or \ntype some texts with the keyboard.\nYou are asked to shop for an ergonomic \nkeyboard, your budget is between $50 and \n$100.\nPrevious action: \" My next move would be to \nmove my finger to the \"Show 1,987 results\" \nbutton, which is located at the bottom of the \nscreen, below the \"Clear Filters\" button and \nabove the navigation bar. I would then click on \nthe \"Show 1,987 results\" button to view the \nsearch results for ergonomic keyboards within \nmy budget range. \"\nYou have now completed the previous action as \ndescribed above. Below is what you see on the \nphone screen. What will you do next? If the \naction involves moving your finger to an icon, \ndescribe the location of that icon as detailed as \npossible.\nFigure 30. Case studies of MM-VID \u2019s capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and\nincorrect predictions, respectively. Figures 31-32 show continued outputs.\n37\nFigure 31. Case studies of MM-VID \u2019s capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and\nincorrect predictions, respectively. Figure 32 shows continued outputs.\n38\nFigure 32. Case studies of MM-VID \u2019s capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and\nincorrect predictions, respectively.\n39\n"
  },
  {
    "title": "VideoCrafter1: Open Diffusion Models for High-Quality Video Generation",
    "link": "https://arxiv.org/pdf/2310.19512.pdf",
    "upvote": "14",
    "text": "VideoCrafter1: Open Diffusion Models for High-Quality Video Generation\nHaoxin Chen1*\nMenghan Xia1*\nYingqing He1,2*\nYong Zhang1* Xiaodong Cun1*\nShaoshu Yang1,3,4\nJinbo Xing1,5\nYaofang Liu1,6\nQifeng Chen2\nXintao Wang1\u2020 Chao Weng1\nYing Shan1\n1 Tencent AI Lab\n2 Hong Kong University of Science and Technology\n3 Center for Research on Intelligent Perception and Computing, CASIA\n4 School of Artificial Intelligence, UCAS\n5 The Chinese University of Hong Kong\n6 City University of Hong Kong\nProject page: https://ailab-cvc.github.io/videocrafter\nGitHub: https://github.com/AILab-CVC/VideoCrafter\nT2V\na robot holding a basketball in \nouter space with expressive \nface, cyberpunk, digital art\nPrompt:\nI2V\nPrompt:\nImage:\na car running fast on the road\nVideoCrafter1\nFigure 1. We have open-sourced two diffusion models for video generation in VideoCrafter1. The Text-to-Video (T2V) model takes a text\nprompt as input and generates a video accordingly. On the other hand, the Image-to-Video (I2V) model accepts either an image, a text\nprompt, or both as input for video generation.\nAbstract\nVideo generation has increasingly gained interest in both\nacademia and industry.\nAlthough commercial tools can\ngenerate plausible videos, there is a limited number of\nopen-source models available for researchers and engi-\nneers. In this work, we introduce two diffusion models for\nhigh-quality video generation, namely text-to-video (T2V)\nand image-to-video (I2V) models. T2V models synthesize a\nvideo based on a given text input, while I2V models incor-\nporate an additional image input. Our proposed T2V model\n* Equal contribution.\n\u2020 Corresponding author.\ncan generate realistic and cinematic-quality videos with a\nresolution of 1024 \u00d7 576, outperforming other open-source\nT2V models in terms of quality. The I2V model is designed\nto produce videos that strictly adhere to the content of the\nprovided reference image, preserving its content, structure,\nand style. This model is the first open-source I2V foundation\nmodel capable of transforming a given image into a video\nclip while maintaining content preservation constraints. We\nbelieve that these open-source video generation models will\ncontribute significantly to the technological advancements\nwithin the community.\n1\narXiv:2310.19512v1  [cs.CV]  30 Oct 2023\n1. Introduction\nWith the rapid development of generative models, partic-\nularly diffusion models [26, 47], numerous breakthroughs\nhave been achieved in fields such as image generation [11,\n12, 16, 19, 39, 41, 43, 57] and video generation [10,\n24, 27, 45, 50], as well as in recognition and detection\ntasks [13, 37]. The most well-known open-source text-to-\nimage (T2I) generative model is Stable Diffusion (SD) [42],\nwhich produces plausible results.\nSubsequently, its en-\nhanced version, SDXL [39], was released, offering im-\nproved concept composition and image quality. Another\nnotable T2I open-source model is IF [2], a cascaded model\nthat operates on pixels rather than latent features.\nRe-\ngarding text-to-video (T2V) models, Make-A-Video [45]\nand Imagen Video [27] are cascaded models, while most\nother works, such as LVDM [24], Magic Video [63], Mod-\nelScope [50], and Align your Latents [10], are SD-based\nmodels. These models extend the SD framework to videos\nby incorporating temporal layers to ensure temporal consis-\ntency among frames. The spatial parameters are inherited\nfrom the pretrained SD UNet.\nThe flourishing of successful T2I models and advance-\nments in downstream tasks can be largely attributed to the\nopen-source environment within the community. SD serves\nas a critical foundation, as it is trained on a vast collection of\ntext-image pairs using immense computing power. The cost\nassociated with this is often prohibitive for most academic\nresearch groups. In contrast, in the field of T2V, Make-A-\nVideo [45] and Imagen Video [27] demonstrate promising\nvideo results, but neither of them are open-sourced. Several\nstartups, such as Gen-2 [1], Pika Labs [6], and Moonval-\nley [5], can generate high-quality videos, but their models\nremain inaccessible to researchers for further exploration.\nCurrently, several open-source T2V models exist, i.e.,\nModelScope [50], Hotshot-XL [4], AnimateDiff [23], and\nZeroscope V2 XL [7]. The released ModelScope model\ncan only generate videos with a resolution of 256 \u00d7 256,\nand the image quality is unsatisfactory. Zeroscope V2 XL\nimproves its visual quality by tuning it on a small set of\nvideos, but flickers and visible noise persist in its gener-\nated videos. Hotshot-XL aims to extend SDXL into a video\nmodel and produce a gif with 8 frames and a resolution of\n512 \u00d7 512. AnimateDiff proposes to combine the tempo-\nral module with the spatial module of a LORA SD model.\nSince the temporal module is trained on Webvid-10M, the\nresults of the original T2V model of AnimateDiff are poor.\nThe combination with a high-quality LORA model can gen-\nerate high-quality videos. However, the scope is restricted\nby the LORA model in terms of style and concept com-\nposition ability.\nThere is still a lack of an open-source\ngeneric T2V foundation model capable of generating high-\nresolution and high-quality videos.\nRecently, Pika Labs and Gen-2 released their image-\nto-video (I2V) models, aiming to animate a given image\nwith a prompt while preserving its content and structure.\nSuch a technique is still in its early stages, as the gener-\nated motion is limited, and there are usually visible arti-\nfacts. The only open-source generic I2V foundation model,\nI2VGen-XL [15], is released in ModelScope. This model\nuses image embedding to replace text embedding for tuning\na pretrained T2V model. However, it does not satisfy the\ncontent-preserving constraints. The generated videos match\nthe semantic meaning in the given image but do not strictly\nfollow the reference content and structure. Hence, there is\nan urgent need for a good I2V model in the open-source\ncommunity.\nIn this work, we introduce two diffusion models for high-\nquality video generation: one for text-to-video (T2V) gen-\neration and the other for image-to-video (I2V) generation.\nThe T2V model builds upon SD 2.1 by incorporating tem-\nporal attention layers into the SD UNet to capture temporal\nconsistency. We employ a joint image and video training\nstrategy to prevent concept forgetting. The training dataset\ncomprises LAION COCO 600M [3], Webvid10M [8], and\na 10M high-resolution collected video dataset. The T2V\nmodel can generate videos with a resolution of 1024 \u00d7 576\nand a duration of 2 seconds. The I2V model, on the other\nhand, is based on a T2V model and accepts both text and\nimage inputs.\nThe image embedding is extracted using\nCLIP [14] and injected into the SD UNet through cross\nattention [54], similar to the injection of text embeddings.\nThe I2V model is trained on LAION COCO 600M and We-\nbvid10M. By releasing these models, we aim to make a\nsignificant contribution to the open-source community, en-\nabling researchers and practitioners to build upon our work\nand further advance the field of video generation.\nOur contributions can be summarized as follows:\n\u2022 We introduce a text-to-video model capable of generat-\ning high-quality videos with a resolution of 1024 \u00d7 576\nand cinematic quality. The model is trained on 20 million\nvideos and 600 million images.\n\u2022 We present an image-to-video model, the first open-\nsource generic I2V model that can strictly preserve the\ncontent and structure of the input reference image while\nanimating it into a video. This model allows for both im-\nage and text inputs.\n2. Related Works\nDiffusion models (DMs) [26, 46, 48] have recently shown\nunprecedented capability in content generation field, espe-\ncially in text-to-image (T2I) generation [9, 21, 25, 38, 39,\n41\u201343, 62].\nFollowing the success of T2I DMs, Video\nDiffusion Models (VDMs) are proposed to model the spa-\ntial and temporal distribution of videos under the con-\ndition of text prompts (T2V). The first VDM [28] uti-\nlizes a space-time factorized U-Net to model low-resolution\n2\nVideoCrafter1\nGen-2\nPika Labs\nI2VGen-XL\nZeroscope-XL\nPrompt: a girl with long curly blonde hair and sunglasses, camera pan from left to right \nVideoCrafter1\nGen-2\nPika Labs\nI2VGen-XL\nZeroscope-XL\nPrompt: a dog wearing vr goggles on a boat\nFigure 2. Visual comparison with Gen-2, Pika Labs, I2VGen-XL, and Zeroscope-XL.\n3\nvideos in pixel space, which is trained jointly on image and\nvideo data.\nTo generate high-definition videos, Imagen-\nVideo [27] introduces an effective cascaded paradigm of\nDMs with v-prediction parameterization method. To pro-\nmote the computational efficiency, subsequent studies [10,\n24, 50, 52, 63] mainly take the way of transferring T2I\nknowledge to T2V generation [33, 45] and learning DMs\nin latent space.\nMost recently, Zhang et al. [59] high-\nlight the issue of heavy computational cost in pixel-based\nVDM [20] and poor text-video alignment in latent-based\nVDM, proposing a hybrid-pixel-latent VDM framework to\naddress these issues.\nAlthough T2V models can generate high-quality videos,\nthey only accept text prompts as semantic guidance, which\ncan be verbose and may not accurately reflect users\u2019 inten-\ntions. Similar to adding controls in T2I models [31, 36,\n44, 55, 60], introducing conditional controls in T2V DMs\nhas increasingly attracted researchers\u2019 attention. Gen-1 [18]\nand Make-Your-Video [53] integrate structure control into\nVDMs by concatenating frame-wise depth map with input\nnoise sequences for video editing, while other control con-\nditions, including pose [34, 61] and canny edge [29, 61]\nare also investigated. However, visual conditions in VDMs,\nsuch as RGB images, remain under-explored.\nMost re-\ncently, image condition is examined in Seer [22] and Video-\nComposer [51] for text-image-to-video synthesis, by keep-\ning first-frame latent clean and concatenating image embed-\nding with noise in channel dimension, respectively. Nev-\nertheless, they either focus on the curated domain, i.e.,\nindoor objects [22], or fail to generate temporally coher-\nent frames and realistic motions [51] due to insufficient\nsemantic understanding of the input image.\nAlthough\nDragNUWA [56] further introduce trajectory control into\nimage-to-video generation, which can only mitigate the\nunrealistic-motion issue to some extent. Moreover, some\nrecent close-sourced text-to-video diffusion models [30, 35,\n45] or auto-regressive models [49, 58] successfully demon-\nstrate their extension applicability to image-to-video syn-\nthesis.\nHowever, their results rarely adhere to the input\nimage condition and suffers from the unrealistic tempo-\nral variation issue. While we build our model upon text-\nconditioned VDMs to leverage their rich dynamic prior for\nanimating open-domain images, incorporating tailored de-\nsigns for better conformity and semantic understanding of\nthe input image.\n3. Methodology\n3.1. VideoCrafter1: Text-to-Video Model\nStructure Overview.\nThe VideoCrafter T2V model is a\nLatent Video Diffusion Model (LVDM) [24] consisting of\ntwo key components: a video VAE and a video latent dif-\nfusion model, as illustrated in Fig. 3. The Video VAE is\nresponsible for reducing the sample dimension, allowing\nthe subsequent diffusion model to be more compact and\nefficient. First, the video data x0 is fed into the VAE en-\ncoder E to project it into the video latent z0, which exhibits\na lower data dimension with a compressed video represen-\ntation. Then, the video latent can be projected back into\nthe reconstructed video x\u2032\n0 via the VAE decoder D. We\nadopt the pretrained VAE from the Stable Diffusion model\nto serve as the video VAE and project each frame individu-\nally without extracting temporal information. After obtain-\ning the video latent z0, the diffusion process is performed\non z0 via:\nq(z1:T |z0) :=\nT\nY\nt=1\nq(zt|zt\u22121),\n(1)\nq(zt|zt\u22121) := N(zt;\np\n1 \u2212 \u03b2tzt\u22121, \u03b2tI),\n(2)\nwhere T is the number of diffusion timesteps, and \u03b2t is the\nnoise level at timestep t. Thus, we can obtain a series of\nnoisy video latents zt at arbitrary timesteps t.\nTo perform the denoising process, a denoiser U-Net is\nlearned to estimate the noise in the input noisy latent, which\nwill be discussed in the next section. After the progressive\ndenoising process, the latent sample transitions from noisy\nto clean, and it can finally be decoded by the VAE decoder\ninto a generated video in the pixel space.\nDenoising 3D U-Net.\nAs illustrated in Fig.3, the denois-\ning U-Net is a 3D U-Net architecture consisting of a stack of\nbasic spatial-temporal blocks with skip connections. Each\nblock comprises convolutional layers, spatial transformers\n(ST), and temporal transformers (TT), where\nST = Projin \u25e6 (Attnself \u25e6 Attncross \u25e6 MLP) \u25e6 Projout,\n(3)\nTT = Projin \u25e6 (Attntemp \u25e6 Attntemp \u25e6 MLP) \u25e6 Projout.\n(4)\nThe controlling signals of the denoiser include semantic\ncontrol, such as the text prompt, and motion speed control,\nsuch as the video fps. We inject the semantic control via the\ncross-attention:\nAttention(Q, K, V) = softmax\n\u0012QKT\n\u221a\nd\n\u0013\n\u00b7 V, where\n(5)\nQ = W(i)\nQ \u00b7 \u03c6i(zt), K = W(i)\nK \u00b7 \u03d5(y), V = W(i)\nV \u00b7 \u03d5(y).\n(6)\n\u03c6i(zt) \u2208 RN\u00d7di\n\u03f5 represents spatially flattened tokens of\nvideo latent, \u03d5 denotes the Clip text encoder, and y is the\n4\n\u2130\n\ud835\udc9f\n\ud835\udc9b\ud835\udfce\nDenoising\nfps\nt\nText prompt\n\ud835\udc9b\ud835\udc95\n\ud835\udc9b\ud835\udc95\u2212\ud835\udfcf\nDiffusion\nConv layers\nSpatial\nTransformer\nTemporal\nTransformer\n\ud835\udc99\ud835\udfce\n\ud835\udc99\ud835\udfce\n\u2032\nImage prompt\nText/Image\nEncoder\nEmbedding\nLayer\nElement-wise\naddition\nFigure 3. The framework of the video diffusion model in VideoCrafter1. We train the video UNet in the latent space of the auto-encoder.\nFPS is taken as a condition to control the motion speed of the generated video. For the T2V model, only the text prompt is fed into the\nspatial transformer via cross-attention, while for the I2V model, both the text and image prompts are taken as the inputs.\nProjection Net\nText \nprompt\nImage \nprompt\nText Encoder\n\ud835\udc05\ud835\udc56\ud835\udc5b\n\ud835\udc05\ud835\udc5c\ud835\udc62\ud835\udc61\nCross Attention\nImage Encoder\nCross Attention\nFigure 4. The diagram of image conditional branch. The U-Net\nbackbone features Fin are processed with the text and image em-\nbeddings via a dual cross-attention layer, the output of which are\nfused as Fout.\ninput text prompt. Motion speed control with fps is incor-\nporated through an FPS embedder, which shares the same\nstructure as the timestep embedder. Specifically, the FPS or\ntimestep is projected into an embedding vector using sinu-\nsoidal embedding. This vector is then fed into a two-layer\nMLP to map the sinusoidal embedding to a learned em-\nbedding. Subsequently, the timestep embedding and FPS\nembedding are fused via elementwise addition. The fused\nembedding is finally added to the convolutional features to\nmodulate the intermediate features.\n3.2. VideoCrafter1: Image-to-Video Model\nText prompts offer highly flexible control for content gener-\nation, but they primarily focus on semantic-level specifica-\ntions rather than detailed appearance. In the I2V model, we\n\u22ef\nSpatial\u00a0\nconvolution\nTemporal\u00a0\nconvolution\nTemporal\u00a0\nattention\nSpatial\u00a0\nattention\n\ud835\udc17\u0be7\n\ud835\udc17\u0b34\n\ud835\udc17\u0bcd\nText\u00a0prompt:\u00a0\u201ca girl is reading books in her bedroom\u201d\nDenoising\u00a0U\u2010Net\n(a)\n(b)\n(c)\nFigure 5. Image-conditioned text-to-video generation comparison.\n(a) Conditional image input. (b) Generation with the global se-\nmantic token conditioned. (c) Generation with the full patch visual\ntokens conditioned. The used text prompt is \u201da beautiful girl with\ncolorful hair\u201d.\naim to integrate an additional conditional input, i.e., image\nprompt, into the video diffusion model, which is expected\nto synthesize dynamic visual content based on the provided\nimage. For text-to-video diffusion models, the conditional\ntext embedding space plays a crucial role in determining\nthe visual content of the final output videos. To supply the\nvideo model with image information in a compatible man-\nner, it is essential to project the image into a text-aligned\nembedding space. We propose learning such an embedding\nwith rich details to enhance visual fidelity. Figure 4 illus-\ntrates the diagram of equipping the diffusion model with an\nimage conditional branch.\nText-Aligned Rich Image Embedding.\nSince the text\nembedding is constructed using the pretrained CLIP [40]\ntext encoder, we employ its image encoder counterpart to\nextract the image features from the input image. Although\n5\nVisual\nText-Video\nMotion\nTemporal\nQuality\nAlignment\nQuality\nConsistency\nI2VGen-XL\u2020\n55.23\n47.22\n59.41\n59.31\nZeroScope\n56.37\n46.18\n54.26\n61.19\nPikaLab\u2217\n63.52\n54.11\n57.74\n69.35\nGen2\u2217\n67.35\n52.30\n62.53\n69.71\nVideoCrafter23.04\n46.88\n41.56\n56.24*\n55.78\nVideoCrafter23.08\n59.53\n51.29\n51.97\n56.36\nVideoCrafter23.10\n61.64\n66.76\n56.06\n60.36\nTable 1. Human-preference aligned results from four different as-\npects, with the rank of each aspect in the brackets. * indicated\nthese models are not open-sourced.\nthe global semantic token fcls from the CLIP image encoder\nis well-aligned with image captions, it primarily represents\nvisual contents at a semantic level, while being less capa-\nble of capturing details. Inspired by existing visual condi-\ntioning works [44, 55], we utilize the full patch visual to-\nkens Fvis = {fi}K\ni=0 from the last layer of the CLIP image\nViT [17], which are believed to encompass much richer in-\nformation about the image.\nTo promote alignment with the text embedding, we uti-\nlize a learnable projection network P to transform Fvis into\nthe target image embedding Fimg = P(Fvis), enabling the\nvideo model backbone to process the image feature effi-\nciently. The text embedding Ftext and image embedding\nFimg are then used to compute the U-Net intermediate fea-\ntures Fin via dual cross-attention layers:\nFout = Softmax(QK\u22a4\ntext\n\u221a\nd\n)Vtext+Softmax(QK\u22a4\nimg\n\u221a\nd\n)Vimg,\n(7)\nwhere Q\n=\nFinWq, Ktext\n=\nFtextWk, Vtext\n=\nFtextWv, and Kimg = FimgW\u2032\nk, Vimg = FimgW\u2032\nv ac-\ncordingly. Note that we use the same query for image cross-\nattention as for text cross-attention.\nThus, only two pa-\nrameter matrices W\u2032\nk, W\u2032\nv are newly added for each cross-\nattention layer. Figure 5 compares the visual fidelity of the\ngenerated videos conditioned on the global semantic token\nand our adopted rich visual tokens, respectively.\n4. Experiments\n4.1. Implementation Details\nDatasets.\nWe employ an image and video joint training\nstrategy for model training.\nThe image dataset used is\nLAION COCO [3], a large text-image dataset consisting\nof 600 million generated high-quality captions for publicly\navailable web images. For video datasets, we utilize the\npublicly available WebVid-10M [8], a large-scale dataset of\nshort videos with textual descriptions sourced from stock\nfootage sites, offering diverse and rich content. Addition-\nally, we compile a large-scale high-quality video dataset\ncontaining 10 million videos with resolutions greater than\n1280 \u00d7 720 for the training of T2V models.\nFigure 6. The raw ratings from our user studies.\nTraining Scheme.\nTo train the T2V model, we employ\nthe training strategy used in Stable Diffusion, i.e., training\nfrom low resolution to high resolution. We first train the\nvideo model extended from the image model at a resolution\nof 256 \u00d7 256 for 80K iterations with a batch size of 256.\nNext, we resume from the 256 \u00d7 256 model and finetune it\nwith videos for 136K iterations at a resolution of 512\u00d7320.\nThe batch size is 128. Finally, we finetune the model for\n45K iterations at a resolution of 1024 \u00d7 576. The batch\nsize is 64. For the I2V model, we initially train the mapping\nfrom the image embedding to the embedding space used\nfor the cross attention. Subsequently, we fix the mappings\nof both text and image embeddings and finetune the video\nmodel for improved alignment.\nEvaluation Metrics.\nWe employ comprehensive metrics\nto assess video quality and the alignment between text and\nvideo using EvalCrafter [32], a benchmark for evaluat-\ning video generation models. EvalCrafter conducts com-\nparisons among our model, Gen-2, Pika Labs, and Mod-\nelScope, considering both quantitative metrics and user\nstudies. We present the main results in Table 1 and Fig-\nure 6. Our T2V model achieves the best visual quality and\nvideo quality among open-source models. Please refer to\nEvalCrafter for further details. For qualitative evaluation,\nwe provide several visual examples in Figures 2, 7, and 9\nfor illustration.\nRelations to Floor33.\nWe deploy the two open-source\nmodels on a Discord channel named Floor33, to allow users\nto explore the capability of the models online by just typing\nthe prompt. We add an optional function, prompt exten-\n6\nVideoCrafter1\nGen-2\nPika Labs\nI2VGen-XL\nZeroscope-XL\nPrompt: Macro len style, A tiny mouse in a dainty dress holds a parasol to shield from the sun.\nVideoCrafter1\nGen-2\nPika Labs\nI2VGen-XL\nZeroscope-XL\nPrompt: The old man the boat. in watercolor style\nFigure 7. Visual comparison with Gen-2, Pika Labs, I2VGen-XL, and Zeroscope-XL.\n7\n23.04\n23.08\n23.10\nFigure 8. The visual comparisons of the visual quality between different VideoCrafter text-to-video versions. The prompts are \u201cIn Marvel\nmovie style, supercute siamese cat as sushi chef\u201d, \u201dA wise tortoise in a tweed hat and spectacles reads a newspaper, Howard Hodgkin\nstyle\u201d and \u201chand-held camera, a politician giving a speech at a podium\u201d, respectively. The comparison video will be released on our\nGithub.\nsion, to enrich the information in the user\u2019s prompt. The\ndiscord channel can be accessed at https://discord.\ngg/rrayYqZ4tf.\n4.2. Performance Evaluation\nText-to-Video Results.\nWe compare our T2V model with\ncommercial models such as Gen-2 and Pika Labs, as well as\nthe open-source model I2VGen-XL. Since I2VGen-XL is\nan image-to-video model, we first generate an image using\nSDXL and then employ I2VGen-XL to create a video. The\nresults are displayed in Fig. 2 and 7. As shown in Table 1,\nour model outperforms open-source T2V models in terms\nof visual quality and text-alignment. Our model encour-\nages large object movements during training, resulting in\nmore significant motion in the generated videos compared\nto other models. However, larger motions can sometimes\nintroduce errors in temporal consistency. Fig. 2 and 7 also\ndemonstrate our superiority in visual quality and concept\ncomposition compared to open-source models. The image\nquality of Zeroscope is subpar, as it sometimes fails to gen-\nerate content or produces artifacts like repetitive grids.\nGen-2 and Pika Labs consistently generate videos with\nhigh aesthetic scores, and noise is suppressed in their re-\nsults. Nevertheless, Gen-2 occasionally struggles with con-\ncept composition, as seen in the two examples in Fig. 7, and\nits results are overly smooth. Pika Labs exhibits the best\ntext-alignment performance but does not always generate\nthe correct style, such as in the second example in Fig. 7.\nWe also compare the differences between VideoCrafter\nversions to verify our efforts. As shown in Fig. 8 and Ta-\nble. 1, our method has a great process this year in both\nvisual quality, text-video alignment, temporal consistency,\nand motion quality.\nWe also find that our latest ver-\nsion (23.10) has achieved the same quality as Pika Lab [6],\nwhich demonstrates the benefits of our training and datasets.\nImage-to-Video Results.\nWe evaluate our method against\nexisting state-of-the-art image-to-video approaches, includ-\ning two open-source conditional video diffusion models and\ntwo commercial product demos. VideoComposer [51] is\na recently released model for compositional video genera-\ntion that supports text-image-to-video synthesis. I2VGen-\nXL [15] is an open-source image-to-video generation\nproject.\nPika [6] and Gen-2 [1] are well-known text-to-\nvideo generation products developed by commercial com-\n8\nOurs\nI2VGen\u2010XL\nVideoComposer Image input\nPika\nGEN2\nFigure 9. Visual comparisons with image-to-video approaches: VideoComposer, I2VGen-XL, Pika, Gen-2 and our I2V model.\npanies, which also support image-to-video applications.\nThe visual comparison results are illustrated in Figure 9.\nWe observe that Pika, Gen-2, and our approach achieve rel-\natively better visual fidelity to the conditional image than\nVideoComposer and I2V-XL. Although the first frame is al-\nmost identical to the input image, VideoComposer suffers\nfrom severe temporal inconsistency, where the subsequent\nframes transform into entirely different appearances. I2V-\nXL exhibits good temporal consistency and motion mag-\nnitude, but the appearance deviates significantly from the\nconditional image.\nPika achieves the best visual fidelity\nand temporal consistency; however, it generally presents\nvery subtle motion magnitude. In contrast, Gen-2 can gen-\nerate satisfying motion magnitude and visual fidelity, but\nits performance is not stable, i.e., it sometimes suffers from\ntemporal drifting problems (as in the car case). Our I2V\nmodel demonstrates better performance in these cases, with\ngood temporal consistency and motion magnitude, as well\nas acceptable visual fidelity. However, our I2V model still\nhas several limitations such as the successful rate, unsatis-\nfactory facial artifacts, etc, requiring further efforts for im-\nprovement.\n5. Conclusion and Future Work\nWe introduce two diffusion models for video generation.\nOne is a text-to-video generation model capable of produc-\ning high-quality, high-resolution, cinematic-quality videos\nwith a resolution of 1024 \u00d7 576. It offers the best quality\namong open-source T2V models. The other is an image-\nto-video generation model, which is the first open-source\ngeneric I2V foundation model that can preserve the content\nand structure of the given reference image.\nThe existing open-source models merely represent the\nstarting point. Improvements in duration, resolution, and\nmotion quality remain crucial for future developments.\nSpecifically, the current duration of the two models is lim-\nited to 2 seconds; extending this to a longer duration would\nbe more beneficial. This can be accomplished by training\nwith additional frames and developing a frame-interpolation\nmodel. As for resolution, employing a spatial upscaling\nmodule or collaborating with ScaleCrafter [25] presents a\npromising strategy. Moreover, improvements in motion and\nvisual quality can be achieved by utilizing higher-quality\ndata.\n9\nReferences\n[1] Gen-2.\nAccessed October 22, 2023 [Online] https://\nresearch.runwayml.com/gen2.\n[2] If.\nAccessed October 22, 2023 [Online] https://\ngithub.com/deep-floyd/IF.\n[3] Laion-coco. Accessed October 22, 2023 [Online] https:\n//laion.ai/blog/laion-coco/.\n[4] Hotshot-xl. Accessed October 22, 2023 [Online] https:\n//github.com/hotshotco/Hotshot-XL.\n[5] Moonvalley. Accessed October 22, 2023 [Online] https:\n//moonvalley.ai/.\n[6] Pika labs.\nAccessed October 22, 2023 [Online] https:\n//www.pika.art/.\n[7] Zeroscope-xl. Accessed October 22, 2023 [Online] https:\n//huggingface.co/cerspense/zeroscope_v2_\nXL.\n[8] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE International Conference on\nComputer Vision, 2021.\n[9] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022.\n[10] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023.\n[11] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023.\n[12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al.\nPixart-\u03b1: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023.\n[13] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Dif-\nfusiondet: Diffusion model for object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 19830\u201319843, 2023.\n[14] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning laws for contrastive language-image learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2818\u20132829, 2023.\n[15] I2VGen-XL contributors. I2vgen-xl. ModelScope. Accessed\nOctober 15, 2023 [Online] https://modelscope.cn/\nmodels/damo/Image-to-Video/summary.\n[16] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023.\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2020.\n[18] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, 2023.\n[19] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors. In Eu-\nropean Conference on Computer Vision, pages 89\u2013106.\nSpringer, 2022.\n[20] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In ICCV, 2023.\n[21] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nCVPR, 2022.\n[22] Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer:\nLanguage instructed video prediction with latent diffusion\nmodels. arXiv preprint arXiv:2303.14897, 2023.\n[23] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023.\n[24] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022.\n[25] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,\nMenghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng\nChen, and Ying Shan.\nScalecrafter: Tuning-free higher-\nresolution visual generation with diffusion models.\narXiv\npreprint arXiv:2310.07702, 2023.\n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020.\n[27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022.\n[28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. In NeurIPS, 2022.\n[29] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023.\n[30] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu,\nQi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong\nWang. Videogen: A reference-guided latent diffusion ap-\nproach for high definition text-to-video generation.\narXiv\npreprint arXiv:2309.00398, 2023.\n10\n[31] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation.\nIn\nCVPR, 2023.\n[32] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong\nZhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond\nChan, and Ying Shan. Evalcrafter: Benchmarking and eval-\nuating large video generation models, 2023.\n[33] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-\nniu Tan.\nVideofusion: Decomposed diffusion models for\nhigh-quality video generation. In CVPR, 2023.\n[34] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023.\n[35] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023.\n[36] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023.\n[37] Cindy M Nguyen, Eric R Chan, Alexander W Bergman,\nand Gordon Wetzstein.\nDiffusion in the dark: A diffu-\nsion model for low-light text recognition.\narXiv preprint\narXiv:2303.04291, 2023.\n[38] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. 2022.\n[39] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023.\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. 2021.\n[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022.\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 2022.\n[44] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023.\n[45] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. In ICLR, 2023.\n[46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. 2015.\n[47] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020.\n[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR, 2021.\n[49] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. In ICLR, 2023.\n[50] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023.\n[51] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023.\n[52] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023.\n[53] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong\nZhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong\nCun, Xintao Wang, et al.\nMake-your-video: Customized\nvideo generation using textual and structural guidance. arXiv\npreprint arXiv:2306.00943, 2023.\n[54] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023.\n[55] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023.\n[56] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023.\n[57] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022.\n[58] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked\ngenerative video transformer. In CVPR, 2023.\n11\n[59] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation, 2023.\n[60] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023.\n[61] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023.\n[62] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-\nworld image variation by aligning diffusion inversion chain.\narXiv preprint arXiv:2305.18729, 2023.\n[63] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022.\n12\n"
  },
  {
    "title": "CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models",
    "link": "https://arxiv.org/pdf/2310.19784.pdf",
    "upvote": "9",
    "text": "CUSTOMNET: ZERO-SHOT OBJECT CUSTOMIZATION\nWITH\nVARIABLE-VIEWPOINTS\nIN\nTEXT-TO-IMAGE\nDIFFUSION MODELS\nZiyang Yuan1,2 \u2217\nMingdeng Cao2,3\nXintao Wang2\u2020\nZhongang Qi2\nChun Yuan1\u2020\nYing Shan2\n1Tsinghua Shenzhen International Graduate School\n2ARC Lab, Tencent PCG\n3The University of Tokyo\nhttps://jiangyzy.github.io/CustomNet/\nObject Image\nObject Image\nOn dirt road\nIn snow\nWith autumn leaves\nOn the street\nOn the Mountain\nOn wooden floor\nViewpoints\nLocation\nBackground-Text Background-Image\nMultiple Object Images\nFigure 1: We propose CustomNet, a zero-shot customization method that can generate harmonious\ncustomized images with explicit viewpoint, location, and background controls simultaneously while\nensuring identity preservation.\nABSTRACT\nIncorporating a customized object into image generation presents an attractive\nfeature in text-to-image generation. However, existing optimization-based and\nencoder-based methods are hindered by drawbacks such as time-consuming opti-\nmization, insufficient identity preservation, and a prevalent copy-pasting effect. To\novercome these limitations, we introduce CustomNet, a novel object customiza-\ntion approach that explicitly incorporates 3D novel view synthesis capabilities\ninto the object customization process. This integration facilitates the adjustment\nof spatial position relationships and viewpoints, yielding diverse outputs while ef-\nfectively preserving object identity. Moreover, we introduce delicate designs to\nenable location control and flexible background control through textual descrip-\ntions or specific user-defined images, overcoming the limitations of existing 3D\nnovel view synthesis methods. We further leverage a dataset construction pipeline\nthat can better handle real-world objects and complex backgrounds. Equipped\nwith these designs, our method facilitates zero-shot object customization with-\nout test-time optimization, offering simultaneous control over the viewpoints, lo-\ncation, and background. As a result, our CustomNet ensures enhanced identity\npreservation and generates diverse, harmonious outputs.\n\u2217Work done during an internship at ARC Lab, Tencent PCG. \u2020 Corresponding author.\n1\narXiv:2310.19784v2  [cs.CV]  7 Dec 2023\n1\nINTRODUCTION\nRecently, there has been an emerging trend of the diffusion model to become the new state-of-the-\nart model in text-to-image (T2I) generation (Nichol et al., 2021; Ramesh et al., 2022; Saharia et al.,\n2022; Rombach et al., 2022). The society also applies extra diverse control conditions to the T2I\ndiffusion models (Zhang & Agrawala, 2023; Mou et al., 2023; Li et al., 2023c), such as layout,\nstyle, and depth. Customization, as another control dimension in diffusion models, has received\nsignificant attention. It allows users to incorporate objects from reference images into the generated\nimages while preserving their identities. Pioneering works such as Dreambooth (Ruiz et al., 2023)\nand Textual Inversion (Gal et al., 2022) use a few images of the same object to finetune the diffu-\nsion model parameters or learn concept word embeddings through an iterative optimization process.\nAlthough these optimization-based techniques excel at maintaining object identity, they suffer from\ncertain drawbacks, such as time-consuming optimization and a tendency to overfit when only a\nsingle image is provided.\nConsequently, researchers have started exploring encoder-based methods (Yang et al., 2023; Li\net al., 2023c;a; Wei et al., 2023; Chen et al., 2023b). These methods only require training an encoder\nto explicitly represent visual concepts of objects. Once trained, the concept embeddings obtained by\nencoding the image can be directly fed into the denoising process during inference, achieving a speed\ncomparable to the standard diffusion model sampling process. However, simply injecting an image\ninto a compressed concept embedding often leads to inadequate identity preservation (Yang et al.,\n2023; Li et al., 2023a). To address this issue, several methods have been proposed to enhance detail\npreservation by introducing local features (Wei et al., 2023; Ma et al., 2023) or spatial details (Chen\net al., 2023b). Despite these improvements, a closer examination of the results produced by these\nmethods reveals a prevalent copy-pasting effect, i.e., the objects in the synthesized image and source\nimage are identical. The only variations observed stem from basic data augmentations applied during\ntraining, such as flipping and rotation. This limitation makes it difficult to achieve harmonized\nresults with the background and negatively impacts output diversity.\nWhen generating images with customized objects, it is necessary to consider the spatial position\nand viewpoint of the object in relation to the scene, i.e., the 3D properties of the object, in order to\nachieve harmonious and diverse results. Following this guidance, we introduce CustomNet, a novel\nobject customization method that facilitates diverse viewpoints in text-to-image diffusion models.\nUnlike previous encoder-based methods that either fail to maintain a strong object identity or suffer\nfrom apparent copy-pasting effects, CustomNet explicitly controls the viewpoint of the customized\nobject. This capability results in diverse viewpoint outputs while effectively preserving identity.\nThe primary advantage of CustomeNet stems from its utilization of 3D novel view synthesis, which\nallows for the prediction of outputs from other views based on a single image input. A representative\nwork on 3D novel view synthesis is Zero-1-to-3 (Liu et al., 2023a), which employs the massive\nsynthetic 3D object dataset Objaverse (Deitke et al., 2023) with multiple views to train a viewpoint-\nconditioned diffusion model featuring explicit viewpoint control. However, simply incorporating\nZero-1-to-3 into object customization tasks poses several challenges due to its inherent limitations:\n1) It is solely capable of generating centrally positioned objects, lacking the ability to place them in\nalternative locations; 2) It is unable to generate diverse backgrounds, being restricted to a simplistic\nwhite background. It also lacks the text control functionality to produce desired backgrounds. These\nconstraints significantly hinder its applicability in object customization tasks.\nIn this paper, we make delicate designs to incorporate 3D novel view synthesis capability for object\ncustomization while adhering to the requirements of the customization. Our proposed CustomNet\nfirst integrates viewpoint control ability and subsequently supports location control to place objects\nin user-defined positions and sizes. The location control is achieved by concatenating the trans-\nformed reference object image to the UNet input. For background generation, we introduce a dual\ncross-attention module that enables CustomNet to accept both text (for background generation) and\nobject images (for foreground generation). CustomNet also accommodates user-provided back-\nground images, generating harmonious results. Moreover, we have designed a dataset construction\npipeline that effectively utilizes synthetic multiview data and massive natural images to better han-\ndle real-world objects and complex backgrounds. Built upon those designs, CustomNet supports\nfine-grained object and background control within a unified framework and can achieve zero-shot\ncustomization with excellent identity preservation and diverse outcomes, as illustrated in Fig. 1.\n2\nWe summarize our contributions as follows: (1). In contrast to previous customization approaches\nthat predominantly rely on 2D input images, we propose CustomNet to explicitly incorporate 3D\nnovel view synthesis capabilities (e.g., Zero-1-to-3) into the object customization process. This al-\nlows for the adjustment of spatial position relationships and viewpoints, leading to improved identity\npreservation and diverse outputs. (2). Our approach features intricate designs that enable location\ncontrol and flexible background control, addressing inherent limitations in Zero-1-to-3, such as sim-\nplistic white backgrounds, exclusively centered objects, and overly synthetic effects. (3). We design\na dataset construction pipeline that effectively utilizes synthetic multiview data and massive natu-\nral images to better handle real-world objects and complex backgrounds. (4). Equipped with the\naforementioned designs, our method enables zero-shot object customization without test-time op-\ntimization while controlling location, viewpoints, and background simultaneously. This results in\nenhanced identity preservation and diverse, harmonious outputs.\n2\nRELATED WORKS\nObject customization in diffusion models. With the promising progress of text-to-image dif-\nfusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song & Ermon, 2019; Nichol et al.,\n2021; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022), researches explore to cap-\nture the information of a reference object image and maintain its identity throughout the diffusion\nmodel generation process, i.e., object customization. These methods can be broadly classified into\noptimization-based techniques (Ruiz et al., 2023; Gal et al., 2022; Chen et al., 2023a; Liu et al.,\n2023b) and encoder-based approaches (Yang et al., 2023; Song et al., 2023; Li et al., 2023c;a; Wei\net al., 2023). Optimization-based methods can achieve high-fidelity identity preservation; however,\nthey are time-consuming and may sometimes result in overfitting. In contrast, current encoder-based\nmethods enable zero-shot performance but may either lose the identity or produce trivial results re-\nsembling copy-pasting. In contrast, our proposed CustomNet aims to preserve high fidelity while\nsupporting controllable viewpoint variations, thereby achieving more diverse outcomes.\nImage harmonization.\nIn image composition, a foreground object is typically integrated into\na given background image to achieve harmonized results.\nVarious image harmonization meth-\nods (Sunkavalli et al., 2010; Chen & Kae, 2019; Cong et al., 2020; Guo et al., 2021) have been\nproposed to further refine the foreground region, ensuring more plausible lighting and color ad-\njustments (Xue et al., 2022; Cong et al., 2022; Chen et al., 2022). However, these methods focus\non low-level modifications and are unable to alter the viewpoint or pose of the foreground objects.\nIn contrast, our proposed CustomNet not only achieves flexible background generation using user-\nprovided images but also offers additional viewpoint control and enhanced harmonization.\n3D novel view synthesis aims to infer the appearance of a scene from novel viewpoints based on one\nor a set of images of a given 3D scene. Previous methods have typically relied on classical techniques\nsuch as interpolation or disparity estimation (Park et al., 2017; Zhou et al., 2018), as well as genera-\ntive models (Sun et al., 2018; Chan et al., 2022). More recently, approaches based on Scene Repre-\nsentation Networks (SRN) (Sitzmann et al., 2019) and Neural Radiance Fields (NeRF) (Mildenhall\net al., 2020; Yu et al., 2021; Jang & Agapito, 2021) have been explored. Furthermore, diffusion\nmodels have been introduced into novel view synthesis (Liu et al., 2023a; Watson et al., 2022). Zero-\n1-to-3 (Liu et al., 2023a) propose a viewpoint-conditioned diffusion model trained on large synthetic\ndatasets, achieving excellent performance in single-view 3D reconstruction and novel view synthe-\nsis tasks. Our CustomNet leverages the powerful 3D capabilities of diffusion models for object\ncustomization tasks, generating outputs with diverse viewpoints while preserving the identity\n3\nPROPOSED METHOD\nOverview. Given a reference background-free 1 object image x \u2208 RH\u00d7W \u00d73 with height H and\nwidth W, we aim to generate a customized image \u02c6x where the object of the same identity can\nbe seamlessly placed in a desired environment (i.e., background) harmoniously with appropriate\nviewpoint and location variations.\nAs illustrated in Fig. 2, we propose CustomNet, a novel architecture designed to achieve this cus-\ntomization conditioned on the viewpoint [R, T] (where R and T represent the relative camera ro-\n1Background-free images can be easily obtained by segmentation methods, e.g., SAM (Kirillov et al., 2023).\n3\n\u201csitting on\nthe beach\u201d\nDenoising U-Net\nObject\nImage\n(\ud835\udc45, \ud835\udc47)\nobject w/ loc\nlocation\n\ud835\udc9b\ud835\udc61\u22121\nGeneration\nbackground\n\ud835\udc9b\ud835\udc61\u22121\nComposition\nobject w/ loc\nViewpoint\nDUAL ATTN\nCross\nAttention\nCross\nAttention\nConcat\nImage Encoder\nMLP\nText Encoder\nFigure 2: Overview of CustomNet. CustomNet is able to simultaneously control viewpoint, location,\nand background in a unified framework, and can achieve harmonious customized image generation\nwhile effectively preserving object identity and texture details. The background generation can be\ncontrolled either through textual descriptions (the \u2018Generation\u2019 branch) or by providing a specific\nuser-defined image (the \u2018Composition\u2019 branch).\ntation and translation of the desired viewpoint, respectively), object location L, and background\ncondition B. The background can be controlled either through textual descriptions or by providing\na specific user-defined image xbg. :\n\u02c6x = CUSTOMNET(x, [R, T], L, B).\n(1)\nOur work is the first attempt to concurrently control viewpoint, location, and background in the\ncustomized synthesis task, thereby achieving a harmonious customized image while effectively\npreserving object identity and texture details (see Fig. 1 and Fig. 3 for representative results).\nSpecifically, we observe that explicit viewpoint control is the missing ingredient for customiza-\ntion that enables simultaneous viewpoint alteration and object identity preservation (more details\nare in Sec. 4.3). Guided by this insight, our proposed CustomNet builds on the existing powerful\nviewpoint-conditioned diffusion model, Zero-1-to-3 (Liu et al., 2023a), which facilitates accurate\nviewpoint manipulation of the reference object. We further extend the location control by concate-\nnating the reference object image transformed by the desired location and size into the UNet input.\nMoreover, we offer flexible background control through a textual description or a specific back-\nground image. This leads to improved identity preservation and diverse, harmonious outputs.\nDiscussions. There also exist simplistic approaches to achieving customization, which involve plac-\ning the object into a specific background while accounting for viewpoint and location variations in\ntwo distinct stages. Firstly, one can synthesize an object from the desired viewpoint using an existing\nsingle image-based novel view synthesis method, such as Zero-1-to-3. Subsequently, the synthesized\nobject can be placed into a given background at the desired location using textual background in-\npainting (e.g., SD-Inpainting model (RunwayML, 2022)) or exemplar image-based object inpainting\n(e.g., Paint-by-Example (Yang et al., 2023), AnyDoor (Chen et al., 2023b)). However, employing\nbackground inpainting often results in suboptimal harmonious outcomes with obvious artifacts. On\nthe other hand, foreground object-based inpainting is prone to issues such as copying-and-pasting\nartifacts and identity loss, and faces challenges in handling view variations (see Sec. 4.2). Moreover,\nadopting a two-stage approach necessitates the utilization of distinct methods to accomplish the de-\nsired customization. This requires alternating between multiple tools for refining effects to enhance\nperformance, thereby exacerbating the complexity of its usage. Instead, our CustomNet achieves\nthe desired customization in a unified framework with more precise controls.\n3.1\nCONTROL THE VIEWPOINT AND LOCATION OF OBJECTS\nViewpoint Control. To enable synthesizing a target customized image complied with the given\nviewpoint parameter [R, T], we follow the view-conditioned diffusion method introduced by Zero-\n1-to-3. As shown in Fig. 2 (the left-top part), we first apply a pre-trained CLIP (Radford et al.,\n4\n2021) image encoder to encode the reference background-free object image into an object embed-\nding, containing high-level semantic information of the input object. Then the object embedding is\nconcatenated with [R, T] and passed through a trainable lightweight multi-layer perception (MLP).\nThe fused object embedding further passes to the denoising UNet as a condition with the cross-\nattention mechanism to control viewpoints of the synthesized images.\nLocation Control. We further control the object location in the synthesized image by concatenating\nthe reference object image with the desired location and size to the UNet input. The process is\nillustrated in Fig. 2 (the \u2018Generation\u2019 branch). The desired location and size L, represented as a\nbounding box [x, y, w, h], are given by the user. Then, we resize the reference object image into the\nsize of [w, h] and place its left-top corner at the [x, y] coordinate of a background-free image (this\nimage is the same size as the target image being denoised but without background). The additional\nconcatenated reference object image helps the model synthesize the desired image while keeping\nthe identity and the texture details (Rombach et al., 2022; Liu et al., 2023a). Note that Zero-1-\nto-3 directly concatenates the centrally-located reference object to the UNet input, which can only\nsynthesize an image where the object is centered. Our method enables synthesizing the target object\nat the desired position with the proposed explicit location control.\n3.2\nFLEXIBLE BACKGROUND CONTROL BY TEXT OR REFERENCE IMAGES\nOur proposed framework exhibits flexibility in generating backgrounds after positioning the object.\nThere are two approaches. The first approach, referred to as Generation-based background control,\ninvolves synthesizing a target background using a textual description provided by the user, as shown\nin the \u2018Generation\u2019 branch in Fig. 2. The second approach, termed Composition-based background\ncontrol, employs a specific background image supplied by the user (as illustrated in the \u2018Composi-\ntion\u2019 branch in Fig. 2).\nGeneration-based\nBackground\nControl.\nIn\nthis\nmode,\nthe\ndiffusion\nmodel\ntakes\n[zt\u22121, object w/ loc] as inputs, where zt\u22121 represents the noisy latent at the time step t\u22121. The\ndiffusion model is required to generate an appropriate background based on the textual description.\nDifferent from Zero-1-to-3, which solely accepts the object embedding without textual descriptions\nfor background, we propose a novel dual cross-attention conditioning strategy that accepts both the\nfused object embedding with viewpoint control and textual descriptions for background. The dual\ncross-attention mechanism integrates the fused object embedding and the textual embedding through\ntwo distinct cross-attention modules. Specifically, we first employ the CLIP text encoder to obtain\nthe textual embeddings and subsequently inject them into the denoising UNet, along with the fused\nobject embedding, using the DUALATTN:\nDUALATTN(Q, Ko, Vo, Kb, Vb) = Softmax(QKT\no\n\u221a\nd\n)Vo + Softmax(QKT\nb\n\u221a\nd\n)Vb,\n(2)\nwhere the query features Q come from the UNet, while Ko, Vo are the object features projected\nfrom fused object embeddings with viewpoint control, and Kb, Vb are the background features from\nthe textural embeddings. d is the dimension of the aforementioned feature embeddings. During\ntraining, we randomly drop the background text description to disentangle the viewpoint control\nand background control. This straightforward yet effective design enables us to achieve accurate\nbackground control without affecting the object viewpoint control.\nComposition-based Background Control. In many practical scenarios, users desire to seamlessly\ninsert objects into pre-existing background images with specific viewpoints and locations. Our\nproposed CustomNet is designed to accommodate this functionality. More precisely, we extend\nthe input channels of the UNet by concatenating the provided background image channel-wise,\nadhering to the Stable Diffusion inpainting pipeline. Consequently, the diffusion model accepts\n[zt\u22121, object w/ loc, xbg] as inputs. Note that in this mode, the textual description is optional, al-\nlowing for straightforward input of NULL to the text prompt. In comparison to existing image\ncomposition methods (Yang et al., 2023; Chen et al., 2023b) which often struggle with copy-pasting\nartifacts or identity loss issues, our method offers viewpoint and location control over objects.\n3.3\nTRAINING STRATEGIES\nData Construction Pipeline.\nTo train the proposed CustomNet, paired data with object image x,\ntarget image xtgt, desired viewpoint parameter [R, T], location and size L, and background con-\n5\nBLIP-\nDiffusion\nELITE\nGLIGEN\nTextual \nInversion\nDreambooth\nCustomNet\n(Ours)\n\u201cOn top of wooden floor\u201d\nObject Image\nMultiple \nView\n\u201cIn the snow\u201d\n\u201cOn green grass\u201d\nFigure 3: Qualitative comparison. Our CustomNet demonstrates superior capacities in terms of\nidentity preservation, viewpoint control, and harmony of the customized image.\ndition B (i.e., a textual description or a background image) are required. Naturally, we can obtain\nmulti-view object images and corresponding view parameters from existing 3D datasets like Obja-\nverse (Deitke et al., 2023). However, these datasets only contain object images without a background\n(usually with a pure black or white background), which is not appropriate for the customization task.\nAs a result, we can simply perform mask-blending with the object image and collected background\nimages. In addition, we use BLIP2 (Li et al., 2023b) to caption the textual descriptions of the blended\nimages for the background control with text prompts. However, since the composition between the\nobject and background would be unreasonable (i.e., the object is placed into the background dishar-\nmoniously) and the blended target image is unrealistic, the model trained on them often generates a\ndisharmonious customized image, e.g., the objects float over the background (see Sec. 4.3).\nTo alleviate this problem, we propose a training data construction pipeline that is the reverse of\nthe above-mentioned way, i.e., directly utilizing natural images as the target image and extracting\nobjects from the image as the reference. Specifically, for a natural image, we first segment the\nforeground object using SAM model (Kirillov et al., 2023). Then we synthesize a novel view of the\nobject using Zero-1-to-3 with randomly sampled relative viewpoints. The textual description of the\nimage can be also obtained using the BLIP2 model. In this way, we can synthesize a large amount\nof data pairs from natural image datasets, like OpenImages (Kuznetsova et al., 2020). Meanwhile,\nthe model trained with these data can synthesize more harmonious results with these natural images.\nMore details are in the Appendix.\nModel Training.\nGiven paired images with their relative camera viewpoint, object locations, and\nbackground conditions (textual description or background image) {x, xtgt, [R, T], L, B}, we can\nfine-tune a pre-trained diffusion model condition on these explicit controls. We adopt the viewpoint-\nconditioned diffusion model from Zero-1-to-3 (Liu et al., 2023a) as our base model, which also\nutilizes the latent diffusion model (LDM) (Rombach et al., 2022) architecture. LDM contains a\nvariational auto-encoder with an encoder E, decoder D and an UNet denoiser \u03f5\u03b8. It performs the\ndiffusion-denoising process in the latent space rather than the image space for efficiency. The opti-\nmization objective is:\nmin\n\u03b8\nEztgt,\u03f5,t \u2225 \u03f5 \u2212 \u03f5\u03b8(ztgt, t, c(x, [R, t], L, B)) \u22252,\n(3)\nwhere ztgt = E(xtgt), and c(\u00b7) is the condition mechanism with explicit controls. Once the de-\nnoising UNet \u03f5\u03b8 is trained, we can perform harmonious customization conditioned on the target\nviewpoint, location, and background with CustomNet.\n4\nEXPERIMENTS\n4.1\nTRAINING DATASETS AND IMPLEMENTATION DETAILS\nWe use multi-view synthetic dataset Objaverse (Deitke et al., 2023), natural image dataset\nOpenImages-V6 (Kuznetsova et al., 2020) filtered as BLIP-Diffusion (Li et al., 2023a) to construct\n6\n\u201cOn the beach\u201d\nPaint-by-Example\nOurs\nObject image\nSD-Inpainting\nOurs\nBackground image\n\u201cOn the mountain\u201d\nFigure 4: Comparison to existing textual background inpainting method SD-Inpainting model and\nforeground object inpainting model Paint-by-Example. Our CustomNet can achieve a more harmo-\nnious output with diverse viewpoint changes while preserving identity.\nTable 1: Quantitative Comparison. We compute DINO-I, CLIP-I, CLIP-T following (Li et al.,\n2023a). We also conducted a user study to measure subjective metrics: ID, View, Text representing\nidentity preservation, viewpoints variation, and text alignment, respectively.\nMethod\nDINO-I \u2191\nCLIP-I \u2191\nCLIP-T \u2191\nID \u2191\nView \u2191\nText \u2191\nDreamBooth (Ruiz et al., 2023)\n0.6333\n0.8019\n0.2276\n0.1333\n0.0822\n0.1378\nTextual Inversion (Gal et al., 2022)\n0.5116\n0.7557\n0.2088\n0.0100\n0.0944\n0.0367\nBLIP-Diffusion (Li et al., 2023a)\n0.6079\n0.7928\n0.2183\n0.0522\n0.0878\n0.0422\nELITE (Wei et al., 2023)\n0.5101\n0.7675\n0.2310\n0.0056\n0.0722\n0.1056\nGLIGEN (Li et al., 2023c)\n0.5587\n0.8152\n0.1974\n0.0111\n0.0722\n0.0144\nCustomNet (Ours)\n0.7742\n0.8164\n0.2258\n0.7878\n0.5912\n0.6633\ndata pairs with the pipeline introduced in Sec. 3.3. A total of (250+500)K data pairs are constructed\nfor model training. We exploit the Zero-1-to-3 checkpoint as the model weight initialization. For\ntraining, we employ AdamW (Loshchilov & Hutter, 2017) optimizer with a constant learning rate\n2\u00d710\u22126 for 500K optimization steps. The total batch size is 96, and about 6 days are taken to finish\nthe training on 8 NVIDIA-V100 GPUs with 32GB VRAM.\n4.2\nCOMPARISON TO EXISTING METHODS\nWe compare our CustomNet to the optimization-based methods Textual Inversion (Gal et al., 2022),\nDreambooth (Ruiz et al., 2023), and encoder-based (zero-shot) method GLIGEN (Li et al., 2023c),\nELITE (Wei et al., 2023), BLIP-Diffusion (Li et al., 2023a). We use their official implementation\n(for GLIDEN, ELITE, and BLIP-Diffusion) or the diffuser implementations (von Platen et al., 2022)\n(for Textual Inversion, Dreambooth) to obtain the results. Note that Dreambooth requires several\nimages of the same object to finetune.\nFigure 3 shows the images generated with different methods (more results are in the Appendix).\nWe see that the zero-shot methods GLIGEN, ELITE, BLIP-Diffusion, and the optimization-based\nmethod Textual Inversion are far from the identity consistent with the reference object. Dreambooth\nand the proposed CustomNet achieve highly promising harmonious customization results, while our\nmethod allows the user to control the object viewpoint easily and obatain diverse results. In addition,\nour method does not require time-consuming model fine-tuning and textual embedding optimization.\nWe also evaluate the synthesized results quantitatively. All methods apply 26 different prompts\nto perform customizations 3 times randomly on 50 objects. We calculate the visual similarity with\nCLIP image encoder and DINO encoder, denoted as CLIP-I and DINO-I, respectively. We measure\nthe text-image similarity with CLIP directly, denoting CLIP-T. Tab. 1 shows the quantitative results,\nwhere CustomNet achieves better identity preservation (DINO-I and CLIP-I than other methods.\nMeanwhile, CustomNet shows comparable capacity to the state-of-the-art methods regarding textual\ncontrol (CLIP-T). We also conducted a user study and collected 2700 answers for Identity similar-\nity (ID), View variation (View), and Text alignment (Text), respectively. As shown in the right part\nof Tab. 1, most participants prefer CustomNet in all three aspects (78.78%, 64.67%, 67.84%).\nComparison to Inpainting-based Methods Existing inpainting-based methods (SD-Inpainting\nmodel, Paint-by-Example (Yang et al., 2023), AnyDoor (Chen et al., 2023b)) can also place a ref-\nerence object in the desired background in an inpainting pipeline. Given an object, the background\ncan be inpainted with textual descriptions in the SD-Inpainting model, while this kind of method\n7\nObject \nImage\nObject \nImage\nw/o RT\nw/ RT\n(a) Zero-1-to-3\n(b) CustomNet\nw/o RT\nw/ RT\nFigure 5: Explicit viewpoints control. Without the explicit viewpoint parameters [R, T], a) Zero-\n1-to-3 tends to generate images that cannot change the viewpoint or have undesired artifacts; b)\nCustomNet easily obtains copy-pasting effects, even though it is trained with the multi-view dataset.\n\u2464 w/o DualAttn\n\u2460 Object \nImage\n\u2462 w/o Concat \u2463 w/o DataPipeline\n\u2461 w/ SD ckpt\n\u2465 Ours (Full)\n\u201cAt Sunset\u201d\n\u201cOn the top of a purple rug\u201d\nFigure 6: Ablation Study. w/ SD ckpt: initialize model weights with Stable-Diffusion pretrained\ncheckpoints. w/o Concat: do not concatenate object with UNet input. w/o DataPipeline: do not\nuse the dataset construction pipeline for OpenImages. w/o DualAttn: concatenate image and text\nembedding together and use shared cross-attn modules.\neasily suffers from unreal and disharmonious results and cannot cast variations to the reference ob-\nject. Our CustomNet can obtain more harmonious customization with diverse viewpoint control.\nAnother line of methods Paint-by-Example and AnyDoor can inpaint the reference object to a given\nbackground image. AnyDoor has not open-sourced yet and we compare CustomNet with Paint-by-\nExample in Fig. 4. From Fig. 4, we see that Paint-by-Example cannot maintain the identity and\ndiffers significantly from the reference object.\n4.3\nABLATION STUDIES AND ANALYSIS\nWe conduct detailed ablation studies to demonstrate the effectiveness of each design in CustomNet\nand the necessity of explicit control for identity preservation in harmonious customization.\nExplicit viewpoint control is the key for customization that enables simultaneous viewpoint al-\nteration and object identity preservation. We conduct a comparison in terms of with and without\nexplicit viewpoint control parameters [R, T] on the original Zero-1-to-3 model. As shown in the\nleft part of Fig. 5, models trained without viewpoint conditions tend to generate images that cannot\nchange the viewpoint or have undesired artifacts. The same phenomenon can be observed in our\nCustomNet. To to specific, as shown in the right part of Fig. 5, without the explicit camera pose\ncontrol, our model can only obtain copying-and-pasting effects, even though it is trained with the\nmulti-view dataset. Note that in this setting, we also concatenate the object image into the UNet\ninput, otherwise, the model cannot preserve adequate identity.\nPretraining with massive multi-view data is important to preserve identity in CustomNet. We\nadopt Zero-1-to-3 as the initialization, i.e., our CustomNet is pre-trained with massive multi-view\nObjarverse data, so that view-consistency information has been already encoded into the model.\nWhen we train CustomNet from the SD checkpoint (see the 2nd column in Fig. 6), the synthesized\nimages cannot maintain view consistency from other viewpoints and suffer from quality degradation.\nInput concatenation helps better maintain texture details. Previous methods (Wei et al., 2023;\nChen et al., 2023b) also try to integrate local features to maintain texture details in synthesized\nimages. In our method, concatenating the reference object to the UNet input can also preserve\ntextures. Without the concatenation (see the 3rd column in Fig. 6), the color, shape, and texture\nof the generated images differ significantly from the reference object image. We also note that the\n8\nFigure 7: (a) Diverse background control with textual descriptions. (b) Precise location control with\nexplicit location condition.\nmodel would generate copying-and-pasting images without any view variations when we do not\nadopt explicit viewpoint control (see Fig. 5). This is to say, the combination of input concatenation\nand explicit view conditions enables precise and harmonious customization.\nOur data construction pipeline enables more harmonious outputs. We adopted a new data con-\nstruction pipeline for utilizing the OpenImages dataset in Sec. 3.3. Without this design, the model\ntrained with only the data constructed by the naive combination between multi-view object images in\nObjaverse and background images can result in unrealistic and unnatural customized results, usually\nleading to \u2018floating\u2019 artifacts (see the 4th column in Fig. 5).\nDual cross-attention enables disentangled object and background controls. We introduce dual-\nattention for the disentangled object-level control and background control with textual descriptions.\nWhen directly concatenating the text embedding and fused object embedding as the condition to be\ninjected into the UNet, the model tends to learn a coupled control with viewpoint parameters and\ntextual description. As a result, the viewpoint control capacity would degrade significantly and the\nmodel cannot generate desired background (see 5th column in Fig. 6).\n4.4\nMORE APPLICATIONS\nDiverse Background Control. As shown in the first row of Fig. 7, our method can also generate\nharmonious results with diverse backgrounds controlled by textual descriptions. Thanks to the dis-\nentangled dual-attention design, the viewpoint of the object in the synthesized image can remain the\nsame under different textual prompts.\nPrecise Location Control. With explicit location control design, our method also places an object\ninto the desired location shown in the second row of Fig. 7, where the location is specified by the\nuser and the background is synthesized directly by the textual description.\n5\nCONCLUSION\nWe present CustomNet, a novel object customization approach explicitly incorporating 3D view\nsynthesis for enhanced identity preservation and viewpoint control. We introduce intricate designs\nfor location control and flexible background control through textual descriptions or provided back-\nground images, and develop a dataset construction pipeline to handle real-world objects and complex\nbackgrounds effectively. Our experiments show that CustomNet enables diverse zero-shot object\ncustomization while controlling location, viewpoints, and background simultaneously.\nLimitations. CustomNet inherits Zero-1-to-3\u2019s resolution limitation (256 \u00d7 256) restricting the\ngeneration quality. Despite outperforming existing methods, CustomNet cannot perform non-rigid\ntransformations or change object styles. Future work aims to address these limitations.\n9\nREFERENCES\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware\n3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 16123\u201316133, 2022.\nBor-Chun Chen and Andrew Kae. Toward realistic image compositing with adversarial learning. In\nIEEE Conf. Comput. Vis. Pattern Recog., 2019.\nHaoxing Chen, Zhangxuan Gu, Yaohui Li, Jun Lan, Changhua Meng, Weiqiang Wang, and Huax-\niong Li. Hierarchical dynamic image harmonization. arXiv:2211.08639, 2022.\nHong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disen-\nbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation. arXiv\npreprint arXiv:2305.03374, 2023a.\nXi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-\nshot object-level image customization. arXiv preprint arXiv:2307.09481, 2023b.\nWenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, and Liqing Zhang. Dovenet:\nDeep image harmonization via domain verification. In IEEE Conf. Comput. Vis. Pattern Recog.,\n2020.\nWenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong Gao, Qihao Sun, and Liqing Zhang. High-\nresolution image harmonization via collaborative dual transformations. In IEEE Conf. Comput.\nVis. Pattern Recog., 2022.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 13142\u201313153, 2023.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. arXiv:2208.01618, 2022.\nZonghui Guo, Haiyong Zheng, Yufeng Jiang, Zhaorui Gu, and Bing Zheng. Intrinsic image harmo-\nnization. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nWonbong Jang and Lourdes Agapito.\nCodenerf: Disentangled neural radiance fields for object\ncategories. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n12949\u201312958, 2021.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\nSegment anything.\narXiv:2304.02643, 2023.\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.\nThe open images dataset v4: Unified image classification, object detection, and visual relationship\ndetection at scale. IJCV, 2020.\nDongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for\ncontrollable text-to-image generation and editing. arXiv:2305.14720, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023b.\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li,\nand Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22511\u201322521, 2023c.\n10\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023a.\nZhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou,\nand Yang Cao. Cones: Concept neurons in diffusion models for customized generation. arXiv\npreprint arXiv:2303.05125, 2023b.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nJian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized\ntext-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410, 2023.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European\nconference on computer vision, pp. 405\u2013421. Springer, 2020.\nChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv:2112.10741, 2021.\nEunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, and Alexander C Berg. Transformation-\ngrounded image generation network for novel 3d view synthesis.\nIn Proceedings of the ieee\nconference on computer vision and pattern recognition, pp. 3500\u20133509, 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In Int. Conf. Mach. Learn., 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE\nConf. Comput. Vis. Pattern Recog., 2023.\nRunwayML. Stable diffusion. https://github.com/runwayml/stable-diffusion,\n2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Adv. Neural Inform. Process.\nSyst., 2022.\nVincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wetzstein. Scene representation networks: Con-\ntinuous 3d-structure-aware neural scene representations. Advances in Neural Information Pro-\ncessing Systems, 32, 2019.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learn-\ning, pp. 2256\u20132265. PMLR, 2015.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019.\n11\nYizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and\nDaniel Aliaga. Objectstitch: Object compositing with diffusion model. In IEEE Conf. Comput.\nVis. Pattern Recog., 2023.\nShao-Hua Sun, Minyoung Huh, Yuan-Hong Liao, Ning Zhang, and Joseph J Lim.\nMulti-view\nto novel view: Synthesizing novel views with self-learned confidence. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pp. 155\u2013171, 2018.\nKalyan Sunkavalli, Micah K Johnson, Wojciech Matusik, and Hanspeter Pfister. Multi-scale image\nharmonization. ACM Trans. Graph., 2010.\nPatrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https:\n//github.com/huggingface/diffusers, 2022.\nDaniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mo-\nhammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628,\n2022.\nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding\nvisual concepts into textual embeddings for customized text-to-image generation. arXiv preprint\narXiv:2302.13848, 2023.\nBen Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, and Xing Tang. Dccf: Deep\ncomprehensible color filter learning framework for high-resolution image harmonization. In Eur.\nConf. Comput. Vis., 2022.\nBinxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and\nFang Wen. Paint by example: Exemplar-based image editing with diffusion models. In IEEE\nConf. Comput. Vis. Pattern Recog., 2023.\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from\none or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 4578\u20134587, 2021.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv:2302.05543, 2023.\nTinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification:\nLearning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018.\nA\nAPPENDIX\nA.1\nDATA CONSTRUCTION PIPELINE\nWe show our data construction pipeline in Fig. 8. For the 3D object dataset - Objarverse (Deitke\net al., 2023), we can obtain multi-view object images and corresponding view parameters. However,\nthese datasets only contain object images without a background (usually with a pure white back-\nground), which is not appropriate for the customization task. As a result, we can simply perform\nmask-blending with the object image and collected background images. However, since the compo-\nsition between the object and background would be unreasonable (i.e., the object is placed into the\nbackground disharmoniously) and the blended target image is unrealistic, the model trained on them\noften generates a disharmonious customized image, e.g., the objects float over the background.\nWe show our proposed data construction pipeline for natural images in Fig. 8 (b). It is the reverse of\nthe first way, i.e., directly utilizing natural images as the target image and extracting objects from the\nimage as the reference. Specifically, for a natural image, we first use BLIP-2 5T (Li et al., 2023b)\nto extract the foreground object with the instruction {\u201dimage\u201d: image, \u201dprompt\u201d: \u201dQuestion: What\nforeground objects are in the image? find them and separate them using commas. Answer:\u201d}. Then\nwe feed the object and its corresponding text to SAM, SAM can receive text as input and output\nthe segmentation mask of the corresponding object. Then we synthesize a novel view of the object\n12\nusing Zero-1-to-3 with randomly sampled relative viewpoints. In this way, we can synthesize a large\namount of data pairs from natural image datasets, like OpenImages (Kuznetsova et al., 2020). Fig. 9\nshows some data samples from the two dataset construction pipelines.\nFigure 8: Data construction pipeline from 3D objects (a) and single image (b).\nA.2\nDEFINITION OF CAMERA MODEL\nthe definition and design of RT is consistent with the setting in Zero-1-to-3. In Zero-1-to-3, they\nrender 3D objects to images with a predefined camera pose that always points at the center of the\nobject. This means the viewpoint is actually controlled by polar angle \u03b8, azimuth angle \u03d5, and\nradius r (distance away from the center), resulting all objects to be central of the rendered image.\nTherefore, the rotation matrix R can only control polar angle \u03b8, azimuth angle \u03d5, and the translation\nvector T can only control the radius r between the object and camera. Thus the Zero-1-to-3 can\nonly synthesize objects at the image center, lacking the capacity to place the object at the arbitrary\nlocation in the image. We tackle this problem by specifying the bounding box at the desired location\nto the latent.\nA.3\nIMPROVEMENTS OF ZERO-1-TO-3\nWe show our improvements of Zero-1-to-3 compared with the limitations of Zero-1-to-3 in Fig. 10.\nIn the 1st, 2nd row, We compare with Zero-1-to-3 the ability to control the object location generation.\nAs Zero-1-to-3 can only control polar angle \u03b8, azimuth angle \u03d5, and radius r, it can control the object\nlocation directly, so we apply our location control method on Zero-1-to-3. We resize the object and\nmove it to the location in different bounding boxes (top left and top right in 1st row, bottom left and\nbottom right in 2nd row.). We can see that Zero-1-to-3 generate distortion in the generated images.\nThis is because Zero-1-to-3 only trained on the object centred image, it can not really control the\nobject location.\nIn the 3rd, 4th row, We compare with Zero-1-to-3 the ability of novel-view synthesis. We can see\nfrom the figure that Zero-1-to-3 fails to generate the resonable geometry for the dog and cartoon\ncharacter. Our CustomNet, training with our real world constructed datasets, have a better com-\nprehension of object geometry. We can generate a normal dog and a resonable cartoon chracter in\nmulti-views.\nIn all rows, our CustomNet generate harmonious customized image with the control of different text\nprompts.\n13\n(a) Data samples from data construction pipeline 1.\n(b) Data samples from data construction pipeline 2.\nObject Image\nTarget Image\nObject Image\nTarget Image\nFigure 9: Data samples from our constructed synthetic datasets and real world datasets. (a) shows\nsynthetic data samples from data construction pipeline 1. (b) shows real world data samples from\ndata construction pipeline 2.\nA.4\nMORE COMPARISON RESULTS\nWe provide additional qualitative comparison results to demonstrate the superiority of the proposed\nCustomNet shown in Fig. 11.\nA.5\nMORE RESULTS OF CUSTOMNET\nWe provide more real world object customized results shown in Fig. 12, Fig. 13 and 14.\n14\nZero-1-to-3\nObject Image\nOurs\nFigure 10: The comparison of CustomNet and Zero-1-to-3. In the 1st, 2nd row, We compare with\nZero-1-to-3 the ability to control the object location generation. As Zero-1-to-3 can only control\npolar angle \u03b8, azimuth angle \u03d5, and radius r, it can control the object location directly, so we apply\nour location control method on Zero-1-to-3. We resize the object and move it to the location in\ndifferent bounding boxes (top left and top right in 1st row, bottom left and bottom right in 2nd row.).\nWe can see that Zero-1-to-3 generate distortion in the generated images. This is because Zero-1-to-\n3 only trained on the object centred image, it can not really control the object location.In the 3rd,\n4th row, We compare with Zero-1-to-3 the ability of novel-view synthesis. We can see from the\nfigure that Zero-1-to-3 fails to generate the resonable geometry for the dog and cartoon character.\nOur CustomNet, training with our real world constructed datasets, have a better comprehension of\nobject geometry. We can generate a normal dog and a resonable cartoon chracter in multi-views. In\nall rows, our CustomNet generate harmonious customized image with the control of different text\nprompts.\n15\nBLIP-\nDiffusion\nELITE\nGLIGEN\nTextual \nInversion\nDreambooth\nCustomNet\n(Ours)\nObject Image\nMultiple \nView\n\u201cWith autumn leaves in the background\u201d\n\u201cWith a city in the background\u201d\n\u201cIn the office\u201d\nFigure 11: More qualitative comparison results. Our CustomNet demonstrates superior capacities\nin terms of identity preservation, viewpoint control, and harmony of the customized image.\nIn the garden\nAt sunset\nIn the office\nIn outer space\nIn snow\nIn the bucket\nOn the moon\nIn the office\nOn the water\nIn the exhibition\nIn the office\nOn wooden floor\nWith autumn leaves\nOn dirt road\nOn the beach\nWith clouds\nFigure 12: More real world object customized results of the proposed CustomNet.\n16\nOn the street\nWith mountain\nOn the moon\nIn outer space\nOn green grass\nOn the beach\nWith mountain\nWith autumn leaves\nOn wooden floor\nOn dirt road\nIn the office\nOn the beach\nWith autumn leaves\nOn dirt road\nOn wooden floor\nOn the beach\nOn the beach\nIn desert\nOn the water\nIn outer space\nOn the street\nIn snow\nAt midnight\nOn the water\nFigure 13: More real world object customized results of the proposed CustomNet.\n17\nOn the moon\nOn dirt road\nWith city\nIn the jungle\nOn the beach\nAt midnight\nIn snow\nOn the street\nOn the beach\nOn the street\nIn the exhibition\nIn the jungle\nIn the jungle\nOn the moon\nIn desert\nUnder water\nOn the beach\nWith autumn leaves\nOn the street\nOn dirt road\nWith clouds\nOn dirt road\nWith mountain\nOn the street\nFigure 14: More real world object customized results of the proposed CustomNet.\n18\n"
  },
  {
    "title": "TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise",
    "link": "https://arxiv.org/pdf/2310.19019.pdf",
    "upvote": "9",
    "text": "TeacherLM: Teaching to Fish Rather Than Giving the Fish,\nLanguage Modeling Likewise\nNan He\u2217 1 Hanyu Lai\u2217 2 Chenyang Zhao\u2217 2 Zirui Cheng 2 Junting Pan 3 Ruoyu Qin 2 Ruofan Lu 2 Rui Lu 2\nYunchen Zhang 4 Gangming Zhao 5 Zhaohui Hou 6 Zhiyuan Huang 6 Shaoqing Lu 7 Ding Liang 7\nMingjie Zhan 7\nAbstract\nLarge Language Models (LLMs) exhibit impres-\nsive reasoning and data augmentation capabilities\nin various NLP tasks. However, what about small\nmodels? In this work, we propose TeacherLM-\n7.1B, capable of annotating relevant fundamen-\ntals, chain of thought, and common mistakes\nfor most NLP samples, which makes annotation\nmore than just an answer, thus allowing other\nmodels to learn \u201cwhy\u201d instead of just \u201cwhat\u201d.\nThe TeacherLM-7.1B model achieved a zero-shot\nscore of 52.3 on MMLU, surpassing most models\nwith over 100B parameters. Even more remark-\nable is its data augmentation ability. Based on\nTeacherLM-7.1B, we augmented 58 NLP datasets\nand taught various student models with differ-\nent parameters from OPT and BLOOM series\nin a multi-task setting. The experimental results\nindicate that the data augmentation provided by\nTeacherLM has brought significant benefits. We\nwill release the TeacherLM series of models and\naugmented datasets as open-source.\n1. Introduction\nLarge Language Models have recently revolutionized the\nNLP landscape (Brown et al., 2020; Rae et al., 2021; Chowd-\nhery et al., 2022; Hoffmann et al., 2022; Zeng et al., 2022;\nBlack et al., 2022; Wei et al., 2022a; Taylor et al., 2022).\nCompared to the increase in model size, an essential thing\nfor achieving a deeper understanding of language is to utilize\ndata effectively. Unfortunately, most NLP datasets have sim-\n*Equal contribution\n1University of the Chinese Academy of\nSciences 2Tsinghua University 3The Chinese University of Hong\nKong 4University of Electronic Science and Technology of China\n5The University of Hong Kong 6Beijing University of Posts and\nTelecommunications 7SenseTime Research. Correspondence to:\nMingjie Zhan <zmjdll@gmail.com>.\nple input-output formats, inconsistent with the data seen dur-\ning the pre-training of language models. As a result, directly\nfinetuning such data is insufficient for the model to fully\nunderstand the comprehensive content of samples, inducing\ndifficulty in fully utilizing its learning capabilities. Thus,\ntwo leading data augmentation strategies have emerged to\naddress this issue, including task-level and instance-level\napproaches.\nIn task-level data augmentation, the aim is to map any natu-\nral language task into multiple human-readable prompted\nforms with diverse wording (Wei et al., 2021; Bach et al.,\n2022; Wang et al., 2022b). Furthermore, combining task-\nlevel augmentation with multi-task training leads to a more\ncomprehensive ability. Finetuning a pre-trained model on\nthis multi-task mixture covering various tasks (Sanh et al.,\n2021; Chung et al., 2022; Muennighoff et al., 2022; Iyer\net al., 2022) induces the language model a more substantial\nzero-shot and few-shot capabilities.\nThough task-level augmentation has pushed the language\nmodel\u2019s generalizability to a new height, each sample repre-\nsents an individual entity, limiting the augmentation, which\ntreats the entire dataset using a unified data augmentation\nmethod. Hence a more effective approach is to augment\neach sample individually based on its unique characteristics,\nwith a new stage flourishing in instance-level data augmenta-\ntion. For this approach, retrieval-based pre-trained language\nmodels (Borgeaud et al., 2021; Izacard et al., 2022) utilize\nsmall models in conjunction with a massive database to\nintroduce more relevant knowledge for each sample. Mean-\nwhile, LLMs can use their solid zero-shot ability to augment\neach sample based on different prompts (Wang et al., 2021;\nHo et al., 2022). Nevertheless, these two methods bring\nhuge usage costs, and some of the current best-performing\nmodels are not open-source.\nIn order to reduce the cost of data augmentation, in this work,\nwe propose to open-source a series of small TeacherLM\nmodels, which rival human annotation and LLMs. As an\nadage goes, \u201cIt is better to teach someone how to fish than\nto give them the fish,\u201d which also applies to language mod-\nels. To achieve this purpose, our approach has two primary\narXiv:2310.19019v2  [cs.CL]  31 Oct 2023\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nQuestion: Determine following sentence\u2019s correctness. Boyle\u2019s law\ncorrelates the pressure and volume of a gas. (A) True (B) False\nAnswer: The answer is (A).\nInput\nTeacherLM\nStep 1: Boyle\u2019s law states that when the remaining conditions are\nconstant, then P1V1 = P2V2. Step 2: The given statement is Boyle\u2019s\nlaw correlates the pressure and volume of a gas. Hence the statement\nis correct.\nOutput by\nprompting Let\u2019s\nthink step by\nstep.\nThe mathematical expression for Boyle\u2019s law: P \u221d 1\nV, Here, P is\nthe pressure of the gas and V is the volume of the gas. It can also be\nexpressed as follows: P = K 1\nV. Here, K = Constant.\nOutput by\nprompting The\nfundamental of\nthis question is:\nDo not get confused between Boyles\u2019s law and Charles\u2019s law.\nCharles\u2019s law states the relationship between the volume and tem-\nperature of a gas under constant other conditions. It states as follows:\nV1T2 = V2T1.\nOutput by\nprompting The\ncommon mis-\ntakes are:\nDataset\nMultiple-Choice QA,\nClosed-Book QA,\nNatural Language\nInference, Sentiment,\nTopic Classification...\nTeacherLM\nAugmented Dataset\nStudent Model\n\u221210\n0\n10\n\u2206 Accuracy (%)\nAugmented v.s. Unaugmented\nAugmented > Unaugmented\nAugmented < Unaugmented\nhigh school statistics\nhigh school government politics\nsecurity studies\nhigh school macroeconomics\nmarketing\nus foreign policy\nconceptual physics\nhigh school chemistry\ncollege computer science\nhigh school biology\nprehistory\ncollege medicine\nhigh school european history\ncomputer security\npublic relations\ncollege biology\nhigh school microeconomics\ncollege physics\ninternational law\nnutrition\nelectrical engineering\neconometrics\nanatomy\nhigh school world history\ncollege chemistry\ncollege mathematics\nhigh school computer science\nmedical genetics\nhigh school psychology\nclinical knowledge\nformal logic\nhuman sexuality\nsociology\nmanagement\nmoral disputes\nphilosophy\nmiscellaneous\nhigh school geography\nvirology\nworld religions\nprofessional medicine\nprofessional accounting\nglobal facts\nhuman aging\nprofessional law\nhigh school us history\nprofessional psychology\nbusiness ethics\nmoral scenarios\nelementary mathematics\nlogical fallacies\nhigh school mathematics\nhigh school physics\njurisprudence\nabstract algebra\nmachine learning\nastronomy\nFigure 1: TeacherLMs can perform augmentation on a wide range of datasets. They can leverage three different prompts\nto generate augmentations, including fundamentals, CoT, and common mistakes, providing complete information needed\nto solve the problem. The results of multitask training using augmented and unaugmented P3-Sense-3K data show that\nthe augmented data can make the BLOOM-7.1B student model perform better in zero-shot performance on 47 tasks in the\nMMLU(57 tasks) benchmark.\nintuitions. Firstly, we believe that the real need for aug-\nmentation in a dataset lies in each sample\u2019s label, where\nthe model should learn \u201cwhy\u201d instead of just remember-\ning \u201cwhat\u201d. Our goal is to shift the learning objectives of\nlanguage models from results-oriented to process-oriented,\nmoving away from rote memorization towards a more holis-\ntic understanding to break through current limitations im-\nposed on language model capabilities. Secondly, language\nmodels should mimic humans\u2019 learning process by simul-\ntaneously grasping each sample\u2019s relevant fundamentals,\nchain of thought, and common mistakes to understand the\ntraining objectives more comprehensively.\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nTo achieve this, we define the learning process as com-\nprising three dimensions: fundamental, chain of thought,\nand common mistake. Our ultimate goal is to annotate\nthis information for every sample in any NLP dataset. To\nensure that TeacherLM has strong zero-shot capabilities\nfor most natural language processing tasks, we have col-\nlected 2 million samples from multiple domains for train-\ning. Furthermore, we combined manual annotation and\nSTaR (Zelikman et al., 2022) strategy to construct a com-\nplete \u201c{Question} {Answer} {Fundamentals} {Chain of\nThought} {Common Mistakes}\u201d five-element training ob-\nject for each sample.\nIn sum, our key contributions are:\n\u2022 Comprehensive TeacherLM can generate fundamen-\ntals, chain of thought, and common mistakes, providing\ncomprehensive information tailored to the task\u2019s char-\nacteristics and allowing each task to learn the most\nrelevant knowledge.\n\u2022 Generalizability TeacherLM is suitable for a vari-\nety of datasets and models.\nAs shown in Figure\n1, we augmented 58 NLP datasets and taught vari-\nous student models with different parameters from\nOPT (Zhang et al., 2022), and BLOOM (Scao et al.,\n2022) series in a multi-task setting. Compared to non-\naugmented versions, the experimental results indicate\nthat TeacherLM\u2019s data augmentation gains clear bene-\nfits.\n\u2022 Fight big with small TeacherLM-7.1B model achieved\na zero-shot score of 52.3 on MMLU, surpassing most\nmodels with over 100B parameters.\n\u2022 Cost friendly TeacherLM-7.1B has only 7.1 billion\nparameters; compared to models such as text-davinci-\n003 , it has efficient inference speed and lower running\nconfiguration requirements. Therefore, with the sig-\nnificant cost reduction, we can augment NLP datasets\nof millions of levels, further opening the door to the\nreasoning world.\n\u2022 Open source We will release the TeacherLM series of\nmodels and augmented datasets as open-source.\n2. Related Work\nThis paper explores the intersection of various NLP re-\nsearch fields, including multi-task learning, instruction tun-\ning, multi-step reasoning, and data augmentation. In this\nsection, we will introduce several key related works.\nReasoning via finetuning In this study, we construct a\nlarge-scale Reasoning dataset for training TeacherLM. Pre-\nvious works have utilized manually annotated multi-step\nreasoning to improve model performance (Ling et al., 2017;\nCamburu et al., 2018; Rajani et al., 2019; Talmor et al., 2020;\nCobbe et al., 2021; Nye et al., 2021; Zelikman et al., 2022;\nChung et al., 2022). Compared to these works, TeacherLM-\n7.1B has certain advantages when compared to models of\nthe same scale.\nUsing large models as zero-Shot data augmentation gen-\nerators Combining chain of thought and in-context learning\nhas unlocked more robust reasoning capabilities for LLMs\n(Wei et al., 2022b; Suzgun et al., 2022; Lampinen et al.,\n2022), guiding models to move from learning to rote infor-\nmation to now learning to think critically. Furthermore, by\nadding the sentence \u201cLet\u2019s think step by step\u201d before LLMs\ngenerate answers, the model can generate a step-by-step\nthought process and significantly improve accuracy in solv-\ning reasoning tasks(Kojima et al., 2022). Therefore, Large\nLanguage Models are zero-shot Reasoners and can also be\nconsidered as Zero-Shot data augmentation generators.\nInstruction finetuning Multi-task learning improves the\nperformance of language models in zero-shot settings. Many\nworks have found that designing elaborated natural lan-\nguage templates with instructions for each NLP task and\nconnecting them breaks down barriers between tasks and\nallows the language model to understand the data better\n(Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022;\nWang et al., 2022b; Scialom et al., 2022; Chung et al., 2022;\nMuennighoff et al., 2022; Iyer et al., 2022). In our work,\nwe combine instruction finetuning and reasoning to unlock\nmore potential in language models.\n3. Training TeacherLM\nA good teacher can be a beacon, guiding students toward\nmastering the methods to solve problems. We aim the same\nfor the TeacherLM. In this regard, we make two primary\nefforts. Firstly, we construct a dataset comprising two mil-\nlion detailed explanations. Secondly, we adopt a multi-stage\nprogressive training mode, moving from generality towards\nspecialization.\n3.1. Dataset Construction\nP3-Sense-3K We extract 58 supervised datasets from P3\n(Sanh et al., 2021). Each dataset contains multiple prompts,\nresulting in 529 tasks. To ensure sample balance, We select\nat most 3,000 samples of less than 1,200 tokens for each\ntask. There are 1,400,364 samples in total. Then we for-\nmat multiple choice tasks in the form of \u201cQ: {question}\n{options} A: {answer}\u201d. All other tasks are changed to the\nform of \u201cQ: {question} A: {answer}\u201d.\nMuffin-3W We extract 56 supervised datasets from Muffin\n(Wei et al., 2021), and each dataset includes ten prompts.\nSimilarly, We select at most 30,000 samples of less than\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\n0.5\n2\n10\n40\n160\n640\n20\n40\n60\n80\nAverage Human Expert\nAverage Human Rater\nours\nours\nours\nours\nours\nParameters (Billion)\nAccuracy (%)\nTeacherLM-560M\nTeacherLM-1.1B\nTeacherLM-3B\nTeacherLM-7.1B\nTeacherLM-176B\nFlan-PaLM-8B\nFlan-PaLM-62B\nFlan-PaLM-540B\nGopher-1.4B\nGopher-7.1B\nGopher-280B\nGPT-3-13B\nGPT-3-175B\nAtlas-11B\nGPT-Neox-20B\nChinchilla-70B\nGAL-120B\nGLM-130B\nBLOOM-176B\nFigure 2: Average MMLU scores (%) for 57 tasks with model and human accuracy comparisons. TeacherLMs are in the\n0-shot setting, and the rest are in the 5-shot setting.\n1,200 tokens for each task. There are 1,155,767 samples in\ntotal. All tasks are changed to the form of \u201cQ: {question}\nA: {answer}\u201d.\nTeacherData-2M Furthermore, we collect 2 million pieces\nof multi-domain data not included in any public datasets for\ntraining. We utilize manual annotation and the STaR (Zelik-\nman et al., 2022) strategy to construct a five-element fixed\nparadigm for each sample, including the question, answer,\nfundamentals, chain of thought, and common mistakes. We\nselect samples of less than 2048 tokens.\n3.2. Training Procedure\nTraining the teacher models aims to obtain checkpoints\nexcelling at generating comprehensive explanations with\nsolid generalization ability. In this section, we introduce our\nbase models and the details of the multi-stage training.\n3.2.1. MODELS\nIn this work, our base models are the BLOOM (Scao et al.,\n2022) series ranging from 560 million to 176 billion pa-\nrameters, which are pre-trained on the ROOTS (Laurenc\u00b8on\net al., 2022) corpus in 46 natural languages and 13 program-\nming languages. BLOOM models are large decoder-only\nlanguage models pre-trained for 350 billion tokens.\n3.2.2. MULTI-STAGE TRAINING\nFor each model, we adopt a multi-stage training procedure.\nMulti-task training Previous literature has shown that train-\ning on a large number of tasks with instructions can improve\nthe model\u2019s zero-shot ability and allow it to generalize well\non unseen tasks (Sanh et al., 2021; Wei et al., 2021). There-\nfore, we combine the mixture of P3-Sense-3K and Muffin-\n3W to conduct multi-task training.\nPersonalization training Using the TeacherData-2M\ndataset, the model simultaneously learns to analyze each\nsample\u2019s fundamentals, chain of thought, and common mis-\ntakes. Through this process, the models can generate three\ntypes of explanations.\nSpecialization learning We split the TeacherData-2M\ndataset and train three independent models to focus on\nlearning fundamentals, chain of thought, and common\nmistakes, respectively.\nThe resulting models are re-\nferred to as TeacherLM-Fundamental, TeacherLM-COT,\nand TeacherLM-CommonMistake.\nIn each stage of training, we use packing (Raffel et al.,\n2020) to combine multiple texts, and terminators sepa-\nrate different texts. The interaction between texts is elim-\ninated by setting the attention mask and resetting the po-\nsition id. Models of different sizes use different learning\nrates and batchsizes. According to the number of param-\neters from small to large, they are 3e-4/256(TeacherLM-\n560M), 1e-4/512(TeacherLM-1.1B), 4e-5/512(TeacherLM-\n3B), 2e-5/768(TeacherLM-7.1B), 6e-5/1024(TeacherLM-\n176B). More information can be seen in Appendix A.1. Af-\nter the first stage, we evaluate the models\u2019 performance on\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nthe MMLU (Hendrycks et al., 2020) benchmark for select-\ning checkpoints for further training. Then, after the second\nand third stages, apart from evaluating performance on the\nMMLU benchmark, we conduct a manual evaluation as a\nreference for selecting checkpoints.\n3.3. Evaluation Protocol\n3.3.1. BENCHMARK\nFor the teacher models, we focus on the performance on the\nheld-out tasks not included in the training procedure. Specif-\nically, we evaluate the teacher models on the MMLU(57\ntasks) benchmark in the zero-shot setting. The benchmark\nconsists of exam-like questions on academic subjects such\nas mathematics, history, law, and medicine.\n3.3.2. EVALUATION METHOD\nThe evaluation adopts the method of rank classification. In\nmore detail, assume answers are from a fixed set A, and\nansweri \u2208 A, where i = 1, ..., n, n indexes n candidate\nanswers. Calculate the probability of each candidate answer\nusing the Formula:\nP(answeri|q) = 1\nK\nK\nX\nk=1\nlog P(ak|q, a1, ..., ak\u22121)\n(1)\nHere log P(ak|q, a1, ..., ak\u22121) is the log probability of gen-\nerating the k-th token ak in answeri conditioned on the\nprevious tokens. K is the total number of tokens in answeri\nand q is the question. Choose the answer with the highest\nprobability and calculate average accuracy on all tasks.\n3.4. Results\n3.4.1. ZERO-SHOT SETTING\nWe show the MMLU scores with model and human compar-\nisons in Figure 2. There are some key points.\nFirst, models with few parameters can still benefit from\nour datasets. Even our 560M model exceeds BLOOM-\n176B. Moreover, scaling the CoT data to a larger size can\nimprove zero-shot performance. In FLAN-PaLM (Chung\net al., 2022), nine CoT datasets are added to improve the\nperformance on the held-out tasks. Further scaling the num-\nber of CoT samples to 2 million in this work shows fantastic\nimprovement. The TeacherLM-7.1B model has achieved a\nzero-shot score of 52.3 on the MMLU, surpassing the 5-shot\nperformance of most hundred billion parameter models. Ad-\nditionally, the zero-shot score of TeacherLM-176B is 59.8,\ncomparable to the 5-shot score of gopher-280B (Rae et al.,\n2021). The full results are in Appendix A.2.\n3.4.2. MULTI-STAGE TRAINING HELPS A LOT\nWe show the evaluation results during multi-stage training\nin Table 1. In contrast to multi-stage training, we also\nmixed all datasets and trained on them directly. The training\nhyperparameters and the number of steps are consistent. We\nfind that directly blending all datasets scores much lower\nthan multi-stage training. However, only the models using\nCoT data obtained continuous improvement in the third\nstage. This phenomenon may indicate that tasks of the\nMMLU benchmark require higher reasoning ability.\n3.4.3. ADD SUBJECT TO PROMPT\nInspired by the fact that the MMLU benchmark is composed\nof different subject tasks, we added the prompt \u201cThe fol-\nlowing are multiple choice questions (with answers) about\n{subject name}\u201d at the beginning of each text. We labeled\nthe subject of each sample of TeacherData-2M. This ap-\nproach increases the score of TeacherLM-7.1B by 2%. We\nthink such a prompt is more helpful for the model to use the\ncorrect knowledge to answer the question.\n4. Teaching Student\nIn this section, we select TeacherLM-7.1B as the teacher for\nthe following experiments in order to balance efficiency and\nperformance. Our experiment contains three dimensions to\nverify the ability of TeacherLM. The first dimension is the\nchoice of different training modes, divided into multi-task\ntraining followed by zero-shot testing and single-task fine-\ntuning followed by testing in the corresponding task. The\nsecond dimension is the scaling of the model size. The third\ndimension is the diversity of the test tasks.\n4.1. Datasets\nWe selected P3-Sense-3K for multi-task training. More-\nover, for single-task fine-tuning, we aimed to compare\nTeacherLM-7.1B\u2019s enhancement effects with manually an-\nnotated rationales. Thus we selected (1) StrategyQA (Geva\net al., 2021), a question-answering benchmark with implicit\nreasoning strategies. Due to the unavailability of the test\ndataset, the training dataset was was randomly redivided\ninto training, validation, and test sets in an 8:1:1 ratio. (2)\nCREAK (Onoe et al., 2021), a dataset for commonsense rea-\nsoning over entity knowledge. Since the test dataset lacks\nlabels, we directly evaluated the dev dataset. (3) ECQA\n(Aggarwal et al., 2021), a dataset containing explanations\nfor commonsenseQA.\n4.2. Models and training details\nWe used the three models in the TeacherLM-7.1B series\nto augment each sample in the above four datasets. The\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nTable 1: Average MMLU scores(%) for 57 tasks with multi-stage training and no-stage training comparisons. C, F, M\nrepresent CoT, fundamentals, common mistakes respectively. No stage represents directly mixing all datasets for training.\nTeacherLM-176B only completed part of the training process and only trained on TeacherData-2M.\nparameters\n1st stage\n2nd stage\n3rd stage(C)\n3rd stage(F)\n3rd stage(M)\nNo stage\n560M\n31.00\n36.47\n38.39\n34.65\n34.85\n35.27\n1.1B\n32.87\n41.36\n41.38\n39.25\n38.75\n40.26\n3B\n36.51\n46.34\n46.74\n45.22\n45.1\n46.01\n7.1B\n41.47\n51.11\n52.30\n50.32\n49.00\n47.00\n176B\n/\n59.80\n/\n/\n/\n/\nTable 2: Dataset settings in section 4. Here shows option amounts, example amounts, and average tokens in the training split\nof each dataset. Manual denotes human annotated rationales in original datasets. CoT, Fud, and Mis are TeacherLM\u2019s\ngenerated CoT, fundamentals, and common mistakes, while CoT-D, Fud-D, and Mis-D are text-davinci-003\u2019s generated\nCoT, fundamentals and common mistakes, through the same prompts inputted to TeacherLM.\nDATASET\nOPTIONS\nEXMAPLE AMOUNTS\nAVERAGE TOKENS\nTRAIN\nTEST\nMANUAL\nCOT\nFUD\nMIS\nCOT-D\nFUD-D\nMIS-D\nECQA\n5\n7598\n2194\n59\n75\n186\n53\n47\n27\n32\nSTRATEGYQA\n2\n1832\n228\n28\n90\n192\n55\n73\n48\n31\nCREAK\n2\n10174\n1371\n15\n71\n190\n48\n42\n25\n19\nP3-SENSE-3K\n/\n1400364\n/\n/\n49\n135\n54\n/\n/\n/\naugmented samples, as shown in Figure 4, include five parts\nof information: question, answer, fundamentals, chain of\nthought, and common mistakes. For more information on\nthe above datasets, please refer to Table 2.\nIn the multi-task training mode, we evaluated the benefit of\nthe P3-Sense-3K dataset augmented by TeacherLM-7.1B\non various models. The three parts of TeacherLM\u2019s explana-\ntions are concatenated into sequence with the answer. The\ncontrol group consisted of the original P3-Sense-3k dataset,\ncontaining only question and answer pairs. We increased the\nstudent model size from 1.1B to 7.1B and set the learning\nrate for all experiments in the multi-task training mode to\n2e-5 and the batch size to 256.\nFor the single-task fine-tuning, we select BLOOMZ-7.1B\nas the student model, which has been fine-tuned on xP3,\na composite of supervised datasets in 46 languages with\nEnglish and machine-translated prompts. In the single-task\nfine-tuning, we set the learning rate of all experiments to\n6e-6 and the batch size to 64.\n4.3. Comparison with human and text-davinci-003\nTo further validate model-generated explanations\u2019 qual-\nity, we include human annotation and text-davinci-003\nas control groups in our experiments, where text-davinci-\n003 serves as the teacher and augments the Strate-\ngyQA, CREAK, and ECQA datasets in the same way as\nTeacherLM-7.1B.\nApart from training results in section 4.4, we found that\nmanual explanations and text-davinci-003\u2019s augmentation\nboth read smoothly but are inherently less detailed than\nTeacherLM-7.1B\u2019s. Nevertheless, text-davinci-003 some-\ntimes reiterates the content of the question, rendering limited\naugmented information that could help the student model\nimprove its abilities.\n4.4. Results\nMulti-task training In this experiment section, the model\nunderwent extensive data training and thoroughly learned\nthe rationales of data augmentation. As shown in Figure\n3, models of different sizes were able to bring significant\nbenefits in general. The full experiment results can be seen\nin appendix B.\nSingle-task training In this section, we divided the experi-\nment into two parts. First, we directly fine-tune BLOOMZ-\n7.1B for each task. In the StrategyQA and CREAK datasets,\nTeacherLM-7.1B performs beyond manual annotation and\nhas a more robust augmentation ability than text-davinci-\n003 in the CREAK task.\nSecond, we train BLOOMZ-7.1B on the augmented P3-\nSense-3K dataset, obtaining P3-Augmented-BLOOMZ-\n7.1B, and then fine-tune it on single tasks. The experiment\nshows that this further improved the accuracy of the task\nand, to some extent, solved the problem of insufficient data.\nIn StrategyQA and CREAK, TeacherLM-7.1B also shows\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\n0\n2\n4\n25\n30\n+1.84\nBloom-1B1\nMMLU\n0\n2\n4\n50\n55\n60\n65\n+3.94\nCREAK\n0\n2\n4\n20\n30\n40\n50\n+3.42\nECQA\n0\n2\n4\n45\n50\n55\n60\n+3.07\nStrategyQA\n0\n2\n4\n25\n30\n35\n+1.97\nBloom-3B\n0\n2\n4\n50\n60\n70\n+1.09\n0\n2\n4\n20\n40\n60\n+2.14\n0\n2\n4\n55\n60\n65\n70\n-0.44\n0\n2\n4\n25\n30\n35\n40\n45\n+3.98\nBloom-7B1\n0\n2\n4\n50\n60\n70\n+5.32\n0\n2\n4\n20\n40\n60\n+1.96\n0\n2\n4\n50\n60\n70\n+3.95\n0\n2\n4\n25\n30\n35\n+2.66\nOPT-1B3\n0\n2\n4\n50\n60\n70\n+1.97\n0\n2\n4\n20\n40\n60\n+2.55\n0\n2\n4\n50\n60\n+3.95\n0\n2\n4\n25\n30\n35\n40\n+1.84\nOPT-2B7\n0\n2\n4\n50\n60\n70\n+3.28\n0\n2\n4\n20\n40\n60\n+0.77\n0\n2\n4\n50\n60\n70\n+3.07\n0\n2\n4\n25\n30\n35\n40\n+3.32\nOPT-6B7\n0\n2\n4\n50\n60\n70\n+2.12\n0\n2\n4\n20\n40\n60\n0.00\n0\n2\n4\n50\n60\n70\n+2.63\nNumber of Tokens (Million)\nAccuracy (%)\nPlain\nMulti-task Unaugmented\nMulti-task Augmented\nFigure 3: Different student models\u2019 performance in terms of augmentation of TeacherLM-7.1B. We augmented P3-Sense-\n3K(58 NLP datasets) and trained student models using the multi-task method. The x-axis indicates training token numbers,\nwhile the y-axis indicates zero-shot scores on various datasets. The gray line labeled \u201cPlain\u201d represents student models\u2019\naccuracy without training. The orange line represents accuracy fluctuation when student models are trained directly on the\noriginal P3-Sense-3K, and the blue line represents the effect of training on the augmented P3-Sense-3K dataset.\nthe ability to enhance data beyond human annotation and\ntext-davinci-003.\nIn addition, we found that including manual rationales dur-\ning training can harm some datasets, such as ECQA. Our\nmodel was able to alleviate some problems to some extent,\nbut it could not completely solve them.\n5. Discussion & Conclusion\nWhich part of data augmentation is the most important?\nIt cannot be generalized to all datasets. There are unavoid-\nable differences between datasets, which often lead to vary-\ning requirements for data augmentation content, as shown\nin Table 3. Therefore, CoT, fundamentals, and common mis-\ntakes form a complementary relationship, and these three\nelements can be separated or combined in different orders\nto enhance samples. We conducted detailed experiments, as\nshown in Appendix B. However, in general, CoT brings the\nmost benefits.\nHow can data augmentation effects be maintained in the\ncase of limited data?\nIn addition to the TeacherLM-7.1B model, we also released\nthe augmented P3-Sense-3K dataset. If the data is limited,\nmulti-task training can first be performed on this dataset and\nthen fine-tuned on the small dataset, as Table 3 shows.\nWhat did the student model learn from the augmented\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nTable 3: Single-task finetuning results for the BLOOMZ-7.1B model and the P3-Augmented-BLOOMZ-7.1B model. Text\nmeans no data augmentation for the corresponding task, and manual means using manual CoT. Each result of TeacherLM-\n7.1B and text-davinci-003, from left to right, represents the score after data augmentation using the corresponding model to\ngenerate CoT, fundamentals, and common mistakes.\nTASK\nTEXT\nMANUAL\nTEACHERLM-7.1B\nTEXT-DAVINCI-003\nBLOOMZ-7.1B\nECQA\n71.9\n61.9\n61.9 / 61.9 / 61.9\n64.0 / 64.0 / 64.4\nSTRATEGYQA\n71.1\n71.1\n65.8 / 73.3 / 57.0 (+2.2)\n66.7 / 79.0 / 69.3 (+7.9)\nCREAK\n64.0\n73.3 (+9.3)\n77.0 / 54.8 / 58.2 (+13.0)\n74.6 / 69.6 / 76.7 (+12.7)\nP3-AUGMENTED-BLOOMZ-7.1B\nECQA\n68.6\n53.1\n60.5 / 58.3 / 65.0\n60.1 / 67.7 / 56.6\nSTRATEGYQA\n72.4\n68.9\n74.1 / 77.2 / 74.1 (+4.8)\n65.8 / 75.4 / 71.9 (+3.0)\nCREAK\n76.7\n78.6 (+1.9)\n75.9 / 75.8 / 79.0 (+2.3)\n70.9 / 70.0 / 70.3\nQuestion: Fact 1: Climate is generally described in terms of temperature and moisture. Fact 2: Humidity is a measure of mois-\nture content of air. Climate is generally described in terms of temperature and what?\n(A) humidity (B) wind (C) warmer air (D) calibrated (E) percentages (F) matter (G) color (H) SI units\nAnswer: The answer is (A).\nManual Explanations\nClimate is generally described in terms of temperature and humidity.\ntext-davinci-003 by prompting Let\u2019s think step by step.\nFact 1 states that climate is generally described in terms of temperature and moisture. Fact 2 states that humidity is a measure of\nmoisture content of air. Therefore, the answer is (A) humidity.\ntext-davinci-003 by prompting The fundamental of this question is:\nClimate is generally described in terms of temperature and moisture. Humidity is a measure of moisture content of air, so the\ncorrect answer is (A) humidity.\ntext-davinci-003 by prompting The common mistakes are:\n(B) wind, (C) warmer air, (D) calibrated, (E) percentages, (F) matter, (G) color, and (H) SI units.\nTeacherLM-7.1B by prompting Let\u2019s think step by step.\nVarious physical properties describe the climate. One of them is humidity. The climate is generally described in terms of tempera-\nture and humidity. So, option humidity is the correct answer.\nTeacherLM-7.1B by prompting The fundamental of this question is:\nClimate refers to the weather pattern in a long-term condition in a specific region. Several factors influence the climate of a spe-\ncific region. Temperature, wind speed, humidity, precipitation, and solar radiation are the important significant factors that influ-\nence the climate of a specific region.\nTeacherLM-7.1B by prompting The common mistakes are:\nDo not think that climate is simply determined by temperature alone. Temperature, humidity, wind speed, and other factors also\nplay an important role in the climate of a region. Do not consider that climate is not measured in humid regions. Climate is mea-\nsured in terms of humidity also.\nFigure 4: Example showing a common sense question, with its manual explanations from the original dataset and CoT,\nfundamentals, and common mistakes generated by text-davinci-003 and TeacherLM-7.1B-7.1B, from top to bottom.\ndata?\nLearn by analogy. Though many of the rationales gener-\nated by TeacherLM are not always correct, in this paper,\nno measures were taken to filter the augmented content,\nand the student model still produces significant benefits.\nThe correctness of the augmented content is not the only\nimportant factor. The relevance of the content and the con-\nsistency of the reasoning logic are also significant (Wang\net al., 2022a), which allows the student models to learn to\nthink critically and truly enhance the model\u2019s generalization\nability on unseen tasks.\nIn comparison to human annotation and text-davinci-\n003, what are the characteristics and deficiencies of the\ngeneration content of TeacherLM-7.1B?\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nAs shown in Figure 4 and Appendix C, TeacherLM-7.1B\u2019s\nexplanations are generally more comprehensive and detailed\nthan human annotations and text-davinci-003\u2019s explanations.\nHowever, it falls behind text-davinci-003 in solving mathe-\nmatical problems, probably related to the model\u2019s size.\nReferences\nAggarwal, S., Mandowara, D., Agrawal, V., Khandelwal,\nD., Singla, P., and Garg, D. Explanations for common-\nsenseqa: New dataset and models. In Workshop on Com-\nmonsense Reasoning and Knowledge Bases, 2021.\nBach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C.,\nNayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T.,\net al. Promptsource: An integrated development environ-\nment and repository for natural language prompts. arXiv\npreprint arXiv:2202.01279, 2022.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., et al. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther-\nford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B.,\nDamoc, B., Clark, A., et al. Improving language mod-\nels by retrieving from trillions of tokens. arXiv preprint\narXiv:2112.04426, 2021.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nCamburu, O.-M., Rockt\u00a8aschel, T., Lukasiewicz, T., and\nBlunsom, P. e-snli: Natural language inference with\nnatural language explanations. Advances in Neural Infor-\nmation Processing Systems, 31, 2018.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nGeva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and\nBerant, J. Did aristotle use a laptop? a question answering\nbenchmark with implicit reasoning strategies. Transac-\ntions of the Association for Computational Linguistics, 9:\n346\u2013361, 2021.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J.\nMeasuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\nHo, N., Schmid, L., and Yun, S.-Y. Large language models\nare reasoning teachers. arXiv preprint arXiv:2212.10071,\n2022.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\nIyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig,\nD., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S.,\net al. Opt-iml: Scaling language model instruction meta\nlearning through the lens of generalization. arXiv preprint\narXiv:2212.12017, 2022.\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,\nF., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and\nGrave, E. Few-shot learning with retrieval augmented lan-\nguage models. arXiv preprint arXiv:2208.03299, 2022.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. Large language models are zero-shot reasoners. arXiv\npreprint arXiv:2205.11916, 2022.\nLampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson,\nK., Tessler, M. H., Creswell, A., McClelland, J. L., Wang,\nJ. X., and Hill, F. Can language models learn from ex-\nplanations in context? arXiv preprint arXiv:2204.02329,\n2022.\nLaurenc\u00b8on, H., Saulnier, L., Wang, T., Akiki, C., del Moral,\nA. V., Le Scao, T., Von Werra, L., Mou, C., Ponferrada,\nE. G., Nguyen, H., et al. The bigscience roots corpus:\nA 1.6 tb composite multilingual dataset. In Thirty-sixth\nConference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2022.\nLing, W., Yogatama, D., Dyer, C., and Blunsom, P. Pro-\ngram induction by rationale generation: Learning to solve\nand explain algebraic word problems. arXiv preprint\narXiv:1705.04146, 2017.\nMuennighoff, N., Wang, T., Sutawika, L., Roberts, A., Bi-\nderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X.,\nSchoelkopf, H., et al. Crosslingual generalization through\nmultitask finetuning. arXiv preprint arXiv:2211.01786,\n2022.\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,\nAustin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,\nM., Luan, D., et al. Show your work: Scratchpads for\nintermediate computation with language models. arXiv\npreprint arXiv:2112.00114, 2021.\nOnoe, Y., Zhang, M. J., Choi, E., and Durrett, G. Creak:\nA dataset for commonsense reasoning over entity knowl-\nedge. arXiv preprint arXiv:2109.01653, 2021.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,\nK., Ray, A., et al.\nTraining language models to fol-\nlow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann,\nJ., Song, F., Aslanides, J., Henderson, S., Ring, R.,\nYoung, S., et al. Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint\narXiv:2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\nRajani, N. F., McCann, B., Xiong, C., and Socher, R. Ex-\nplain yourself! leveraging language models for common-\nsense reasoning. arXiv preprint arXiv:1906.02361, 2019.\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\nAlyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,\nA., et al. Multitask prompted training enables zero-shot\ntask generalization. arXiv preprint arXiv:2110.08207,\n2021.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u00b4c, S., Hesslow,\nD., Castagn\u00b4e, R., Luccioni, A. S., Yvon, F., Gall\u00b4e, M.,\net al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\nScialom, T., Chakrabarty, T., and Muresan, S. Continual-t0:\nProgressively instructing 50+ tasks to language models\nwithout forgetting.\narXiv preprint arXiv:2205.12393,\n2022.\nSuzgun, M., Scales, N., Sch\u00a8arli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\nTalmor, A., Tafjord, O., Clark, P., Goldberg, Y., and Be-\nrant, J. Leap-of-thought: Teaching pre-trained models\nto systematically reason over implicit knowledge. Ad-\nvances in Neural Information Processing Systems, 33:\n20227\u201320237, 2020.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,\nA., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085, 2022.\nWang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer,\nL., and Sun, H. Towards understanding chain-of-thought\nprompting: An empirical study of what matters. arXiv\npreprint arXiv:2212.10001, 2022a.\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,\nMirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran,\nA. S., Naik, A., Stap, D., et al. Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+ nlp\ntasks. URL https://arxiv. org/abs/2204.07705, 2022b.\nWang, Z., Yu, A. W., Firat, O., and Cao, Y.\nTo-\nwards zero-label language learning.\narXiv preprint\narXiv:2109.09193, 2021.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\nguage models are zero-shot learners.\narXiv preprint\narXiv:2109.01652, 2021.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682, 2022a.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,\nLe, Q., and Zhou, D. Chain of thought prompting elic-\nits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022b.\nZelikman, E., Wu, Y., and Goodman, N. D. Star: Boot-\nstrapping reasoning with reasoning.\narXiv preprint\narXiv:2203.14465, 2022.\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,\nYang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b:\nAn open bilingual pre-trained model.\narXiv preprint\narXiv:2210.02414, 2022.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nA. TeacherLMs\nA.1. Training hyperparameters\nWe show the training hyperparameters of TeacheLMs and the amount of training data at each stage in Table 4.\nTable 4: Training hyperparameter settings for each model size. TeacherLM-176B completed part of the training process and\nonly trained on TeacherData-2M.\nparameters\nlearning rate\nbatch size\ntokens in 1st stage\ntokens in 2nd stage\ntokens in 3rd stage\n560M\n3e-4\n256\n5B\n7.5B\n0.5B\n1.1B\n1e-4\n512\n5B\n7.5B\n0.5B\n3B\n4e-5\n512\n5B\n7.5B\n0.5B\n7.1B\n2e-5\n768\n0.5B\n8B\n1.5B\n176B\n6e-5\n1024\n/\n1B\n/\nA.2. Full experimental results\nThe evaluation results of TeacherLMs can be seen at Table 5 and Table 6, where we show the MMLU individual task\nperformance of TeacherLM-560M, TeacherLM-1.1B, TeacherLM-3B, FLAN-PALM-8B, TeacherLM-7.1B, TeacherLM-\n176B, GLM-130B, BLOOM-176B and Gopher-280B. Here, we report the \u201cvalidation\u201d set performance of individual tasks\nin MMLU.\nB. Complete Benchmarks for All Augmented Trained Models\nWe evaluate all the augmented trained models on four benchmarks, MMLU, ECQA, CREAK and StrategyQA. We show the\nresults in Table 7. We evaluate ten kinds of checkpoints, including original pretrained models, models trained with original\nP3 dataset, models trained with P3-Sense-3K, and models trained with seven kinds of augmented dataset.\nC. More Comparison Examples\nFigure 4 shows TeacherLM-7.1B\u2019s explanations for a Physics problem. Apart from Physics, TeacherLM-7.1B also has a\nsound ability to analyze History and Geography problems, which we show in Figure 5.\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nTable 5: MMLU individual task performance of TeacherLM-7.1B, TacherLM-176B, GLM-130B, BLOOM-176B, and\nGopher-280B. Furthermore, we denote \u201ccollege\u201d as \u201cCO\u201d and \u201chigh school\u201d as \u201cHS\u201d.\nTASK\nTEACHERLM-7.1B\nTEACHERLM-176B\nGLM-130B\nBLOOM-176B\nGOPHER-280B\nABSTRACT ALGEBRA\n30.00\n32.00\n24.00\n24.00\n25.00\nANATOMY\n46.67\n54.81\n48.90\n38.52\n56.30\nASTRONOMY\n51.32\n67.76\n48.03\n34.87\n65.80\nBUSINESS ETHICS\n65.00\n67.00\n51.00\n34.00\n70.00\nCLINICAL KNOWLEDGE\n57.74\n62.26\n48.68\n35.85\n67.20\nCO BIOLOGY\n59.72\n66.67\n47.22\n37.50\n70.80\nCO CHEMISTRY\n42.00\n42.00\n34.00\n19.00\n45.00\nCO COMPUTER SCIENCE\n40.00\n35.00\n44.00\n1.00\n49.00\nCO MATHEMATICS\n34.00\n35.00\n27.00\n31.00\n37.00\nCO MEDICINE\n50.29\n58.96\n43.35\n28.90\n60.10\nCO PHYSICS\n41.18\n44.12\n30.39\n24.50\n34.30\nCOMPUTER SECURITY\n64.00\n76.00\n61.00\n40.00\n65.00\nCONCEPTUAL PHYSICS\n51.49\n51.06\n38.72\n31.49\n49.40\nECONOMETRICS\n35.09\n44.74\n26.32\n26.32\n43.00\nELECTRICAL ENGINEERING\n53.10\n60.00\n45.52\n32.41\n60.00\nELEMENTARY MATHEMATICS\n39.42\n42.86\n31.75\n29.63\n33.60\nFORMAL LOGIC\n38.10\n39.68\n27.78\n23.02\n35.70\nGLOBAL FACTS\n29.00\n32.00\n35.00\n23.00\n38.00\nHS BIOLOGY\n62.90\n70.97\n51.29\n27.42\n71.30\nHS CHEMISTRY\n43.35\n50.74\n34.98\n27.09\n47.80\nHS COMPUTER SCIENCE\n62.00\n64.00\n53.00\n30.00\n54.00\nHS EUROPEAN HISTORY\n61.21\n75.76\n58.18\n35.76\n72.10\nHS GEOGRAPHY\n67.68\n78.28\n53.54\n36.36\n76.80\nHS GOVERNMENT AND POLITICS\n68.39\n75.13\n62.18\n40.41\n83.90\nHS MACROECONOMICS\n55.64\n62.82\n42.56\n30.77\n65.10\nHS MATHEMATICS\n29.26\n33.33\n28.15\n25.93\n23.70\nHS MICROECONOMICS\n60.50\n72.27\n45.80\n26.89\n66.40\nHS PHYSICS\n30.46\n37.09\n29.80\n30.46\n33.80\nHS PSYCHOLOGY\n70.83\n82.20\n54.13\n39.27\n81.80\nHS STATISTICS\n44.91\n50.00\n38.43\n26.39\n50.00\nHS US HISTORY\n54.90\n66.18\n58.33\n40.69\n78.90\nHS WORLD HISTORY\n64.98\n73.42\n67.09\n32.07\n75.10\nHUMAN AGING\n56.95\n67.26\n45.29\n32.29\n66.40\nHUMAN SEXUALITY\n61.07\n67.18\n51.15\n35.11\n67.20\nINTERNATIONAL LAW\n71.90\n78.51\n56.20\n42.15\n77.70\nJURISPRUDENCE\n62.96\n70.37\n43.52\n35.19\n71.30\nLOGICAL FALLACIES\n52.15\n61.96\n57.06\n31.29\n72.40\nMACHINE LEARNING\n29.46\n42.86\n40.18\n29.46\n41.10\nMANAGEMENT\n75.73\n73.79\n56.31\n27.18\n77.70\nMARKETING\n79.06\n86.75\n67.52\n39.74\n83.30\nMEDICAL GENETICS\n57.00\n71.00\n48.00\n45.00\n69.00\nMISCELLANEOUS\n60.54\n71.14\n61.18\n40.23\n75.70\nMORAL DISPUTES\n55.20\n63.87\n47.11\n36.71\n66.80\nMORAL SCENARIOS\n22.35\n29.72\n24.25\n24.36\n40.20\nNUTRITION\n56.86\n64.38\n50.65\n32.35\n69.90\nPHILOSOPHY\n52.73\n66.24\n45.34\n35.37\n68.80\nPREHISTORY\n50.62\n69.14\n50.93\n40.43\n67.60\nPROFESSIONAL ACCOUNTING\n36.17\n45.74\n35.46\n28.72\n44.30\nPROFESSIONAL LAW\n34.16\n42.31\n37.94\n29.53\n44.50\nPROFESSIONAL MEDICINE\n47.79\n55.15\n43.38\n18.01\n64.00\nPROFESSIONAL PSYCHOLOGY\n47.06\n63.07\n42.48\n31.54\n68.10\nPUBLIC RELATIONS\n66.36\n64.55\n55.46\n33.64\n71.80\nSECURITY STUDIES\n63.67\n68.16\n44.90\n34.29\n64.90\nSOCIOLOGY\n70.15\n81.09\n51.74\n31.84\n84.10\nUS FOREIGN POLICY\n70.00\n78.00\n61.00\n46.00\n81.00\nVIROLOGY\n42.17\n46.99\n39.16\n28.31\n47.00\nWORLD RELIGIONS\n55.56\n73.68\n55.56\n42.11\n84.20\nAVERAGE\n52.30\n59.80\n45.70\n31.90\n60.00\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nTable 6: MMLU individual task performance of TeacherLM-560M, TacherLM-1.1B, TeacherLM-3B, Flan-PaLM-8B.\nFurthermore, we denote \u201ccollege\u201d as \u201cCO\u201d and \u201chigh school\u201d as \u201cHS\u201d.\nTASK\nTEACHERLM-560M\nTEACHERLM-1.1B\nTEACHERLM-3B\nFLAN-PALM -8B\nABSTRACT ALGEBRA\n29.00\n33.00\n32.00\n36.40\nANATOMY\n42.96\n39.26\n40.74\n42.90\nASTRONOMY\n44.08\n42.11\n46.71\n43.80\nBUSINESS ETHICS\n30.00\n44.00\n51.00\n36.40\nCLINICAL KNOWLEDGE\n47.92\n45.66\n53.21\n48.30\nCO BIOLOGY\n36.11\n35.42\n47.92\n56.20\nCO CHEMISTRY\n37.00\n31.00\n42.00\n25.00\nCO COMPUTER SCIENCE\n35.00\n40.00\n30.00\n54.50\nCO MATHEMATICS\n30.00\n32.00\n41.00\n18.20\nCO MEDICINE\n47.40\n40.46\n47.40\n50.00\nCO PHYSICS\n32.35\n30.39\n37.25\n45.50\nCOMPUTER SECURITY\n42.00\n49.00\n58.00\n72.70\nCONCEPTUAL PHYSICS\n37.02\n41.28\n44.68\n38.50\nECONOMETRICS\n28.07\n33.33\n26.32\n33.30\nELECTRICAL ENGINEERING\n46.20\n46.90\n49.66\n37.50\nELEMENTARY MATHEMATICS\n29.89\n34.13\n34.66\n34.10\nFORMAL LOGIC\n29.37\n26.98\n32.54\n21.40\nGLOBAL FACTS\n24.00\n30.00\n30.00\n30.00\nHS BIOLOGY\n40.32\n50.65\n57.42\n50.00\nHS CHEMISTRY\n39.90\n39.41\n43.35\n18.20\nHS COMPUTER SCIENCE\n40.00\n38.00\n47.00\n44.40\nHS EUROPEAN HISTORY\n42.42\n41.21\n52.73\n72.20\nHS GEOGRAPHY\n44.95\n51.52\n56.57\n68.20\nHS GOVERNMENT AND POLITICS\n40.93\n47.67\n53.37\n57.10\nHS MACROECONOMICS\n38.46\n41.28\n53.08\n44.20\nHS MATHEMATICS\n28.52\n27.04\n26.67\n17.20\nHS MICROECONOMICS\n41.60\n50.00\n60.08\n57.70\nHS PHYSICS\n29.14\n29.14\n29.14\n17.60\nHS PSYCHOLOGY\n44.22\n55.23\n63.85\n68.30\nHS STATISTICS\n35.65\n34.26\n41.20\n39.10\nHS US HISTORY\n39.22\n40.69\n47.55\n72.70\nHS WORLD HISTORY\n40.51\n45.15\n53.59\n61.50\nHUMAN AGING\n39.46\n40.81\n55.61\n52.20\nHUMAN SEXUALITY\n44.27\n43.51\n48.09\n66.70\nINTERNATIONAL LAW\n58.68\n65.29\n62.81\n76.90\nJURISPRUDENCE\n44.44\n49.07\n50.00\n72.70\nLOGICAL FALLACIES\n32.52\n34.97\n46.01\n61.10\nMACHINE LEARNING\n28.57\n27.68\n36.61\n45.50\nMANAGEMENT\n44.66\n63.11\n61.17\n81.80\nMARKETING\n50.85\n63.25\n67.95\n72.00\nMEDICAL GENETICS\n42.00\n39.00\n48.00\n63.60\nMISCELLANEOUS\n37.42\n43.93\n54.92\n68.60\nMORAL DISPUTES\n45.95\n45.09\n52.02\n39.50\nMORAL SCENARIOS\n27.15\n23.58\n24.69\n25.00\nNUTRITION\n45.42\n47.06\n50.33\n57.60\nPHILOSOPHY\n35.05\n44.37\n47.59\n61.80\nPREHISTORY\n37.04\n38.27\n42.90\n45.70\nPROFESSIONAL ACCOUNTING\n35.46\n32.98\n35.82\n35.50\nPROFESSIONAL LAW\n29.40\n32.40\n33.64\n32.40\nPROFESSIONAL MEDICINE\n29.78\n33.82\n42.65\n51.60\nPROFESSIONAL PSYCHOLOGY\n33.82\n40.36\n43.46\n46.40\nPUBLIC RELATIONS\n32.73\n42.73\n50.91\n50.00\nSECURITY STUDIES\n44.08\n47.76\n54.69\n40.70\nSOCIOLOGY\n54.72\n62.69\n65.67\n72.70\nUS FOREIGN POLICY\n52.00\n57.00\n67.00\n63.60\nVIROLOGY\n33.13\n36.14\n40.96\n44.40\nWORLD RELIGIONS\n35.67\n38.01\n50.29\n68.40\nAVERAGE\n38.39\n41.38\n46.74\n49.29\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nTable 7: Comparing different benchmarks of the original six models we use and the scores of each model after training\nwith P3 dataset, P3-Sense-3K, and seven augmented datasets. PRETRAINED in the figure represents the original models;\nORIGIN represents models trained on origin P3 dataset; SENSE represents models trained on P3-Sense-3K; The rest\nrepresent models trained on augmented P3 dataset in different prompt format. C represents the CoT field; M represents the\ncommon mistakes field; F represents the fundamental field.\nMMLU\nBL-1B1\nBL-3B\nBL-7B1\nBZ-1B1\nBZ-3B\nBZ-7B1\nOPT-1B3\nOPT-2B7\nOPT-6B7\nPRETRAINED\n23.1\n25.1\n24.7\n23.7\n35.2\n39.6\n24.8\n22.2\n24.9\nORIGIN\n25.8\n26.3\n32.8\n24.8\n28.9\n32.2\n27.8\n28.3\n31.0\nSENSE\n31.0\n36.1\n39.2\n32.2\n38.6\n41.3\n33.1\n37.0\n39.1\nC\n32.9\n38.1\n42.6\n32.8\n40.1\n44.0\n35.8\n38.4\n42.2\nC M\n32.5\n37.4\n42.3\n33.1\n40.2\n44.1\n34.9\n38.4\n41.3\nC M F\n33.0\n38.0\n43.0\n33.5\n40.3\n44.7\n34.3\n39.4\n42.7\nSHUFFLE\n33.1\n38.2\n41.0\n32.9\n40.1\n44.1\n35.3\n38.2\n41.6\nM\n31.9\n37.3\n41.5\n32.8\n39.0\n43.1\n34.8\n38.1\n39.9\nF\n31.7\n37.0\n42.0\n32.7\n39.7\n43.5\n33.2\n37.9\n40.9\nF C M\n32.5\n38.0\n43.2\n32.6\n40.2\n44.5\n33.4\n39.5\n42.2\nECQA\nBL-1B1\nBL-3B\nBL-7B1\nBZ-1B1\nBZ-3B\nBZ-7B1\nOPT-1B3\nOPT-2B7\nOPT-6B7\nPRETRAINED\n20.6\n19.3\n21.2\n20.2\n62.8\n74.3\n21.2\n20.4\n20.8\nORIGIN\n22.1\n46.0\n61.3\n24.8\n54.5\n64.3\n27.6\n48.5\n64.9\nSENSE\n50.0\n63.4\n70.0\n32.2\n76.0\n78.2\n59.5\n69.6\n71.9\nC\n51.5\n65.3\n71.9\n32.8\n75.6\n80.1\n59.5\n70.4\n68.8\nC M\n53.1\n64.7\n71.7\n33.1\n76.4\n79.5\n58.8\n70.2\n71.9\nC M F\n49.4\n64.8\n70.9\n33.5\n74.5\n78.2\n51.9\n69.7\n71.7\nSHUFFLE\n55.7\n67.9\n71.3\n32.9\n75.8\n78.2\n58.1\n71.6\n72.9\nM\n51.8\n65.5\n71.4\n32.8\n76.6\n78.6\n62.1\n70.2\n71.6\nF\n49.1\n63.4\n70.1\n32.7\n75.0\n78.6\n49.7\n69.3\n70.3\nF C M\n46.5\n64.8\n70.6\n32.6\n75.3\n78.2\n50.0\n68.4\n71.8\nCREAK\nBL-1B1\nBL-3B\nBL-7B1\nBZ-1B1\nBZ-3B\nBZ-7B1\nOPT-1B3\nOPT-2B7\nOPT-6B7\nPRETRAINED\n50.6\n49.6\n49.6\n50.4\n49.6\n49.6\n49.7\n53.7\n64.6\nORIGIN\n51.3\n54.6\n51.6\n51.3\n51.4\n51.1\n52.2\n51.2\n51.9\nSENSE\n59.2\n68.2\n67.9\n58.8\n66.7\n71.3\n67.1\n70.2\n73.6\nC\n61.4\n66.8\n71.3\n63.7\n67.0\n70.6\n68.8\n73.2\n74.4\nC M\n62.4\n68.8\n71.8\n63.7\n68.8\n72.2\n68.3\n72.9\n75.7\nC M F\n61.9\n70.2\n70.5\n63.9\n69.0\n70.8\n65.9\n73.5\n75.0\nSHUFFLE\n60.0\n68.6\n72.0\n63.7\n69.0\n70.8\n67.9\n72.6\n75.0\nM\n60.7\n67.0\n73.2\n62.7\n66.7\n71.7\n66.4\n71.9\n75.2\nF\n57.4\n66.9\n70.8\n60.7\n67.6\n71.1\n69.1\n72.6\n73.8\nF C M\n64.1\n69.3\n70.4\n62.9\n68.5\n70.9\n65.0\n73.2\n75.9\nSTRATEGYQA\nBL-1B1\nBL-3B\nBL-7B1\nBZ-1B1\nBZ-3B\nBZ-7B1\nOPT-1B3\nOPT-2B7\nOPT-6B7\nPRETRAINED\n55.3\n53.5\n52.2\n50.0\n52.2\n47.4\n53.5\n63.6\n71.5\nORIGIN\n57.0\n55.7\n60.5\n56.6\n52.6\n63.2\n57.9\n54.4\n63.2\nSENSE\n57.5\n69.7\n70.6\n61.8\n71.5\n73.7\n62.3\n65.8\n71.9\nC\n58.8\n68.9\n72.4\n64.0\n70.6\n73.2\n65.4\n66.7\n74.6\nC M\n59.6\n70.2\n69.7\n62.7\n71.9\n73.7\n65.8\n66.7\n73.7\nC M F\n58.8\n65.4\n72.4\n61.8\n69.3\n75.0\n62.7\n65.4\n72.4\nSHUFFLE\n62.3\n71.5\n71.9\n61.8\n71.1\n73.7\n64.0\n65.4\n73.2\nM\n61.8\n66.2\n74.6\n64.5\n72.4\n71.5\n66.2\n68.4\n74.1\nF\n60.1\n68.0\n71.9\n63.6\n70.2\n75.0\n63.2\n68.9\n70.6\nF C M\n60.1\n65.8\n71.1\n64.0\n70.2\n72.8\n60.5\n66.2\n71.5\nTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\nQuestion: Determine the correctness of the following sentence. Sam Cooke became a member of the Rock & Roll Hall of Fame.\n(A) True (B) False\nAnswer: The answer is (A).\nManual Explanations\nI know this to be true.\ntext-davinci-003 by prompting Let\u2019s think step by step.\nSam Cooke was an American singer, songwriter, and entrepreneur. He was one of the most influential musical artists of the 20th\ncentury. He was inducted into the Rock and Roll Hall of Fame in 1986. Therefore, the statement is true.\ntext-davinci-003 by prompting The fundamental of this question is:\nTrue. Sam Cooke was inducted into the Rock and Roll Hall of Fame in 1986.\ntext-davinci-003 by prompting The common mistakes are:\nSam Cooke became a member of the Rock and Roll Hall of Fame.\nTeacherLM-7.1B by prompting Let\u2019s think step by step.\nStep 1: The early 1970s saw the emergence of Rock & Roll as the leading music form in the United States. Step 2: Sam Cooke is\nknown as the King of Soul. He became a Rock & Roll Hall of Fame member in 1995.\nTeacherLM-7.1B by prompting The fundamental of this question is:\nThe United States established several organizations to recognize excellence in achievements. These organizations are the National\nFoundation for the Advancement of Arts and Sciences and the Rock and Roll Hall of Fame. In 1972, the Country Music Asso-\nciation introduced the first-ever Rock and Roll Hall of Fame. A rock music awards show named the BBC Radio 1 Big Weekend\nalso emerged. This award show had several celebrities competing for the top honor: the Rock and Roll Hall of Fame. The first\ninductee was the band, the Beach Boys. The recording industry\u2019s successes maintain the honor.\nTeacherLM-7.1B by prompting The common mistakes are:\nRock & Roll songs emerged in the 1950s and 1960s. Do not forget that Ray Charles and Sam Cooke are both in the Rock & Roll\nHall of Fame.\nQuestion: Determine the correctness of the following sentence. Turmeric grows in the sub-Asian region of the world.\n(A) True (B) False\nAnswer: The answer is (A).\nManual Explanations\nBecause they are facts.\ntext-davinci-003 by prompting Let\u2019s think step by step.\nTurmeric is a plant that is native to South Asia. Therefore, it grows in the sub-Asian region of the world. This means that the\nstatement is true.\ntext-davinci-003 by prompting The fundamental of this question is:Turmeric is a plant native to the subcontinent of India, which\nis part of the Asian region of the world. Therefore, the statement is true.\ntext-davinci-003 by prompting The common mistakes are:\nTrue.\nTeacherLM-7.1B by prompting Let\u2019s think step by step.\nStep 1: The botanical name of Turmeric is Curcuma Longa. The root of this plant is used for many purposes, like its anticancer-\nous, analgesic, and antimicrobial properties. Step 2: Turmeric is also known as Indian ginger because the herbaceous root veg-\netable is native to south Asia, also called sub-Asian. So the statement is true.\nTeacherLM-7.1B by prompting The fundamental of this question is:\nIndia has a tropical monsoon climate with four seasons throughout the year, a cool season from January to February, a summer\nseason from March to May, a southwest monsoon rainy season from June to September, and a northeast monsoon period from\nOctober to December. The temperature is as low as 150C in the north and as high as 27C in the south. Almost all year is frost-\nfree, crops can be grown throughout the year, and the heat resources are abundant. In this way, very many tropical plants can\nsurvive in India.\nTeacherLM-7.1B by prompting The common mistakes are:\nMake sure to distinguish the term sub-Asia from Asia. Although sub-Asia is part of Asia, people generally refer to India as sub-\nAsia.\nFigure 5: More examples generated by TeacherLM-7.1B and text-davinci-003, with its manual explanations from the\noriginal dataset and CoT, fundamentals, and common mistakes generated by text-davinci-003 and TeacherLM-7.1B, from\ntop to bottom.\n"
  },
  {
    "title": "Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V",
    "link": "https://arxiv.org/pdf/2310.19061.pdf",
    "upvote": "7",
    "text": "Multimodal ChatGPT for Medical Applications: an\nExperimental Study of GPT-4V\nZhiling Yan1\u2217\nKai Zhang1\u2217\nRong Zhou1\nLifang He1\nXiang Li2\nLichao Sun1\u2020\n1Lehigh University, 2Massachusetts General Hospital and Harvard Medical School\nAbstract\nIn this paper, we critically evaluate the capabilities of the state-of-the-art multi-\nmodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question\nAnswering (VQA) task. Our experiments thoroughly assess GPT-4V\u2019s proficiency\nin answering questions paired with images using both pathology and radiology\ndatasets from 11 modalities (e.g. Microscopy, Dermoscopy, X-ray, CT, etc.) and\nfifteen objects of interests (brain, liver, lung, etc.). Our datasets encompass a\ncomprehensive range of medical inquiries, including sixteen distinct question types.\nThroughout our evaluations, we devised textual prompts for GPT-4V, directing it\nto synergize visual and textual information. The experiments with accuracy score\nconclude that the current version of GPT-4V is not recommended for real-world\ndiagnostics due to its unreliable and suboptimal accuracy in responding to diagnos-\ntic medical questions. In addition, we delineate seven unique facets of GPT-4V\u2019s\nbehavior in medical VQA, highlighting its constraints within this complex arena.\nThe complete details of our evaluation cases are accessible at Github.\n1\nIntroduction\nMedicine is an intrinsically multimodal discipline. Clinicians often use medical images, clinical\nnotes, lab tests, electronic health records, genomics, and more when providing care [Tu et al.,\n2023]. However, although many AI models have demonstrated great potential for biomedical\napplications [Lee et al., 2023b, Nori et al., 2023, Lee et al., 2023a], majority of them focus on just\none type of data [Acosta et al., 2022, Singhal et al., 2023, Zhou et al., 2023]. Some need extra\nadjustments to work well for specific tasks [Zhang et al., 2023a, Chen et al., 2023], while others\nare not available for public use [Tu et al., 2023, Li et al., 2023]. Previously, ChatGPT [OpenAI,\n2022] has shown potential in offering insights for both patients and doctors, aiding in minimizing\nmissed or misdiagnoses [Yunxiang et al., 2023, Han et al., 2023, Wang et al., 2023]. A notable\ninstance was when ChatGPT accurately diagnosed a young boy after 17 doctors had been unable to\nprovide conclusive answers over three years1. The advent of visual input capabilities in the GPT-4\nsystem, known as GPT-4 with Vision (GPT-4V) [OpenAI, 2023], further amplifies this potential.\nThis enhancement allows for more holistic information processing in a question and answer format,\npossibly mitigating language ambiguities\u2014particularly relevant when non-medical experts, such as\npatients, might struggle with accurate descriptions due to unfamiliarity with medical jargon.\n*These authors contributed equally to this work.\n\u2020Corresponding author.\nLichao Sun (lis221@lehigh.edu)\n1https://www.today.com/health/mom-chatgpt-diagnosis-pain-rcna101843\nPreprint. Under review.\narXiv:2310.19061v1  [cs.CV]  29 Oct 2023\nWhat is the name of the \nechocardiography view?\nA: Basal 3-chamber view       \nB: Parasternal short-axis view\nC: Apical four-chamber view  \nD: Subcostal view \nC: Apical four-chamber view.\nWhat is the texture of \nthe tumor? \nHyperechoic.\nWhere is the stenosis \nlocated in the RCA? \nIn the mid segment.\nWhere was the tumor \nlocated?\nThe tumor was located \nat the superior pole of \nthe right kidney.\nIs this Hyaline \nmembrane disease?\nYes\nWhat implies impending \nperforation?\nAreas of translucency or \nthinning indicate a risk \nof impending perforation.\nNeurology\nIn which two ventricles \ncan calcifications be \nseen on this CT scan?\nThe 3rd ventricle and \nthe lateral ventricles.\nBrain CT\nBrain MRI\nAngiography\nECHO\nUltrasound\nAbdominal MRI\nEndoscopic videos\nCardiology\nOncology\nPathological \nimages\nPathology\nSurgery\nWhere is the abnormality?\nLeft temporal lobe.\nFigure 1: The diagram of medical departments and their corresponding objects of interest and\nmodalities. We comprehensively consider 11 modalities across 15 objects of interest in the paper.\n1.1\nAnalysis Dimensions\nIn this paper, we will systematically examine how GPT-4V operates in the medical field using the\nVQA approach. We believe this might become a predominant method for future medical AI like daily\nhealthcare assistant. We will address questions in a hierarchical manner, emphasizing the step-by-step\nprocess of medical progression for an intelligent machine / agent. This approach will help us uncover\nwhat GPT-4V can offer and also delve into its limitations and challenges in real-world medical\napplications.\n\u2022 In medical imaging, the fundamental aspect of machine understanding lies in recognizing the used\nmodalities, such as X-Ray, MRI, or microscopy. Following this recognition, an effective system\nshould discern specific objects within these images, from anatomical structures to distinct cellular\nconfigurations. The adeptness of GPT-4V in these preliminary tasks could offer insights into its\nproficiency in interpreting medical images across varied domains, setting the foundation for more\nadvanced evaluations.\n\u2022 Central to medical analysis is the notion of localization, or the precise pinpointing of specific\nregions or objects within an image. Such capability is instrumental when demarcating between\nhealthy and pathological tissues or discerning the positions of tumors, vessels, and other salient\nstructures. For a model like GPT-4V, mastering this skill could usher in nuanced analyses, bolstering\nclinical diagnostics, treatment design, and disease tracking.\n\u2022 Further deepening the analysis, the precision with which GPT-4V gauges the dimensions of regions\nof interest (ROIs) becomes paramount. Monitoring the dynamics of tumors, evaluating organ\ndimensions, or quantifying lesions holds clinical weight, aiding in diagnostics, surgical planning,\nand gauging ailment severity.\n\u2022 Another layer of analytical depth involves the identification of morphological patterns. Such\npatterns\u2014be it the systematic cellular structures in pathology or attributes such as density, form,\nand opacity in radiology\u2014are instrumental for diagnostic deliberations. A case in point is the\npalisade-like cellular organization around necrotic zones, characteristic of glioblastoma multiforme,\na specific brain malignancy.\n2\n\u2022 Expanding the purview beyond mere visual cues, an integrative diagnostic modality combines\nimagery with textual descriptions, offering a holistic view of a patient\u2019s status. However, the\nefficacy of GPT-4V in such vision-language synthesis warrants exploration, particularly given\nconcerns of potential over-reliance on singular modes, leading to possible incomplete or skewed\ndiagnostic outcomes.\n\u2022 A mere answer, devoid of context or clarity, often falls short in the medical domain. Therefore,\nassessing if GPT-4V elucidates its rationale, articulates clearly, and evinces assurance in its\nresponses becomes pivotal. Such a facet not only engenders trust among users but also aligns with\nthe gravity and precision the medical domain demands.\n\u2022 Lastly, shaping the user-AI interaction framework remains crucial. Crafting an optimal prompt\ntemplate when querying GPT-4V or similar AI entities can drastically influence response accuracy.\nPreliminary observations suggest that while GPT-4V\u2019s immediate answers might sometimes falter,\ncertain prompt structures channel its analytical depth more effectively. Sharing such findings can\nguide users in their engagement with GPT-4V, optimizing outcomes in personal healthcare inquiries\nand consultations.\n1.2\nHighlights\nIn this section, we provide a concise summary of our findings related to the characteristics of GPT-4V\nin the context of medical VQA. These characteristics, depicted in Section 5, directly correspond to\nthe research questions posed earlier:\n(1) GPT-4V consistently recognizes various medical imaging modalities and the objects within them.\n(2) For accurate localization, GPT-4V requires cues, particularly to consider the orientations of\nmedical images across different modalities.\n(3) GPT-4V finds it challenging to discern the size of Regions of Interest (ROI) or objects, especially\nwhen the assessment involves multiple slices, such as CT scans.\n(4) While GPT-4V has the capability to integrate both image and text inputs for diagnostic-related\nqueries, it displays tendencies towards visual and linguistic biases. Specifically, it might either\noveremphasize markings in images or rely excessively on text, neglecting the visual information in\nthe process.\n(5) GPT-4V typically offers cautious responses, emphasizing that it is not a medical professional\n(e.g., radiologist or pathologist). Nonetheless, its answers are thorough and come with detailed\nexplanations. It\u2019s important to note that these explanations, while informative, are not definitive facts\nand should be cross-checked by experts for accuracy.\n(6) Based on the statistical results concerning the accuracy of VQA, the current version of GPT-4V\nis not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in\nresponding to diagnostic medical questions (see Section 4 for details).\n1.3\nContributions\nThis report provides the following contributions to the community in the realm of medical AI:\n\u22c6 We meticulously assess GPT-4V\u2019s performance in responding to visually paired medical queries,\nleveraging datasets from a diverse array of seven imaging modalities such as Microscopy, Der-\nmoscopy, X-ray, and CT, and centering our analysis on fifteen different clinical objects including\nthe brain, liver, and lung. Our comprehensive dataset, uniquely tailored, encompasses sixteen\ndifferent types of medical questions, providing a broad basis for evaluation.\n\u22c6 The empirical results, derived from rigorous testing for accuracy, unequivocally suggest that\nthe current version of GPT-4V should not be employed for practical diagnostic purposes. Its\nperformance in responding to diagnostic medical questions demonstrates a lack of reliability and\naccuracy necessary for real-world application.\n\u22c6 Our study delineates seven distinct dimensions of GPT-4V\u2019s operational capabilities within the\nmedical VQA context. These dimensions highlight the model\u2019s operational boundaries and shed\nlight on its adaptability and limitations in the demanding realm of medical inquiry.\n3\n2\nExperimental Setup\nWe outline the experimental setup or case studies employed to address the aforementioned questions\nand objectives. Since GPT-4V has not officially released APIs, we evaluated its capability for medical\nVQA through its dedicated chat interface (the ChatGPT webpage version), initiating our dialogue\nwith image inputs. To eliminate any interference, hints, or biases from a multi-round conversation, we\nbegan a new chat session for each new Q&A case. This ensured that GPT-4V did not unintentionally\nreference information from previous conversations related to different cases. In this report, we\nprioritize evaluating zero-shot performance using the accuracy metric. For closed-ended questions\nwith limited choices, this metric gauges the consistency of GPT-4V\u2019s answers with factual accuracy.\nFor open-ended queries, it assesses how often GPT-4V\u2019s responses contain the correct information.\nWe provide examples on the prompts utilized and the criteria for determining the correctness of the\nanswers in the following:\nClosed-ended Pathology VQA\nPrompt: [image] Is atrophy seen in the brain?\nGPT-4V: I am not a pathologist, but based on the image provided, it appears to be a represen-\ntation of two human brains. The key characteristic of brain atrophy is a noticeable reduction\nin the size or volume of the brain tissue. Comparing the two images, there might be some\ndifferences in the appearance of the gyri (raised portions) and sulci (grooves) between the\ntwo brains, suggesting potential volume changes.\nCriteria: The response exhibits considerable uncertainty, characterized by terms such as\nmight be\u2019 and potential\u2019. Furthermore, the answer lacks clarity and directness. For instance,\nwhile the phrase \u2018a noticeable reduction in size\u2019 suggests atrophy, the term \u2018volume changes\u2019\nis ambiguous regarding whether it denotes expansion or reduction. Consequently, we classify\nthis response from GPT-4V as incorrect.\nOpen-ended Radiology VQA\nPrompt: [image] What type of imaging does this not represent?\nGPT-4V: This is an X-ray image of the chest. Thus, it does not represent imaging modalities\nsuch as MRI (Magnetic Resonance Imaging), CT (Computed Tomography), Ultrasound, or\nPET (Positron Emission Tomography).\nCriteria: The reference answer is exclusively Ultrasound. In this instance, GPT-4V offers\na more comprehensive response that encompasses the reference answer. Thus, this case is\ncategorized as correct for accuracy computation. Notably, even if GPT-4V mentions CT or\nother correct type only instead of ultrasound, it is still deemed accurate, as GPT-4V distinctly\nidentifies the image as an X-ray.\nCriteria for assessing the accuracy of GPT-4V\u2019s responses are as follows:\n\u2022 GPT-4V should directly answer the question and provide the correct response.\n\u2022 GPT-4V does not refuse to answer the question, and its response should encompass key points or\nsemantically equivalent terms. Any additional information in the response must also be manually\nverified for accuracy. This criterion is particularly applicable to open-ended questions.\n\u2022 Responses from GPT-4V should be devoid of ambiguity. While answers that display a degree of\ncaution, like \u201cIt appears to be atrophy\u201d, are acceptable, ambiguous answers such as \u201cIt appears to\nbe volume changes\u201d are not permitted, as illustrated by the closed-ended pathology VQA example.\n\u2022 GPT-4V needs to provide comprehensive answers. For instance, if the prompt is \u201cIn which two\nventricles ...\u201d and GPT-4V mentions only one, the answer is considered incorrect.\n\u2022 Multi-round conversations leading to the correct answer are not permissible. This is because they\ncan introduce excessive hints, and the GPT model is intrinsically influenced by user feedback, like\nstatements indicating \u201cYour answer is wrong, ...\u201d will mislead the response easily.\n4\n\u2022 OpenAI has documented inconsistent medical responses within the GPT-4V system card 2. This\nindicates that while GPT-4V may offer correct answers sometimes, it might falter in others. In\nour study, we permit only a single response from GPT-4V. This approach mirrors real-life medical\nscenarios where individuals have just one life, underscoring the notion that a virtual doctor like\nGPT-4V cannot be afforded a second chance.\nTo comprehensively assess GPT-4V\u2019s proficiency in medicine, and in light of the absence of an\nAPI which necessitates manual testing (thus constraining the scalability of our evaluation), we\nmeticulously selected 133 samples. These comprise 56 radiology samples sourced from VQA-RAD\n[Lau et al., 2018] and PMC-VQA [Zhang et al., 2023b], along with 77 samples from PathVQA [He\net al., 2020]. Detailed information about the data, including sample selection and the distribution of\nquestion types, can be found in Section 3.\n3\nData Collection\n3.1\nPathology\nThe pathology data collection process commences with obtaining question-answer pairs from\nPathVQA set [He et al., 2020]. These pairs involve tasks such as recognizing objects in the image\nand giving clinical advice. Recognizing objects holds fundamental importance for AI models to\nunderstand the pathology image. This recognition forms the basis for subsequent assessments.\nTo be more specific, we randomly select 63 representative pathology images, and manually select 77\nhigh quality questions from the corresponding question set. To ensure the diversity of the data, we\nselect images across microscopy, dermoscopy, WSI, with variety objects of interest: brain, liver, skin,\ncell, Heart, lung, vessel, and kidney, as shown in Table 1. On average, each image has 1.22 questions.\nThe maximum and minimum number of questions for a single image is 5 and 1 respectively. Figure 2\nshows some examples.\nThere are eight categories of questions: \"Anatomical Structures,\" \"Lesion & Abnormality Detec-\ntion,\" \"Disease Diagnosis,\" \"Temporal & History-Related Effects,\" \"Spatial Relationships,\" \"Contrast\nAgents & Staining,\" \"Microscopic Features,\" and \"Pathophysiological Mechanisms & Manifesta-\ntions.\" Table 2 shows the number of questions and percentage of each category. The eight categories\nencompass a comprehensive range of medical inquiries. \"Anatomical Structures\" pertain to specific\norgans or tissues within the body. \"Lesion & Abnormality Detection\" focus on the identification\nof unusual imaging findings or pathological abnormalities. \"Disease Diagnosis\" aims to determine\nspecific medical conditions from given symptoms or findings. \"Temporal & History-Related Effects\"\ndelve into the progression or changes over time, linking them to past medical events. \"Spatial Rela-\ntionships\" address the relative positioning of structures or abnormalities within the body. \"Contrast\nAgents & Staining\" relate to the use and interpretation of imaging contrasts or histological stains.\n\"Microscopic Features\" detail observations made at a cellular level, often in histology or cytology.\nFinally, \"Pathophysiological Mechanisms & Manifestations\" explore the underpinnings and outcomes\nof diseases, touching on both their causes and effects.\nThe questions are also defined into three difficulty levels: \"Easy,\" \"Medium,\" and \"Hard,\" as shown in\nTable 4 Questions about recognizing objects in the image are tend to be considered as easy samples.\nRecognizing objects holds fundamental importance for AI models to understand the pathology image.\nThis recognition forms the basis for subsequent assessments. Medium questions also ask GPT-4V to\nrecognize objects, but with more complicated scenario and less background information. Questions\nabout giving clinical advice are often categorized as challenging due to their demand for a holistic\nunderstanding.\nOverall, this approach allows us to comprehensively assess GPT-4V\u2019s performance across a range of\npathological scenarios, question types and modes.\n3.2\nRadiology\nThe radiology data collection process commences with obtaining modality-related question-answer\npairs from VQA-RAD dataset [Lau et al., 2018]. These pairs involve tasks such as determining\n2https://openai.com/research/gpt-4v-system-card\n5\nFigure 2: VQA samples from both pathology set and radiology set. Samples of pathology set are in\ngreen boxes, while radiology samples are in red boxes. Each question comes with a prompt, directing\nGPT-4V to consider both the visual and textual data. Questions and their corresponding ground truth\nanswers are denoted with [Question] and [GT] respectively.\n6\nTable 1: Dataset evaluated in this paper. \"Num. Pairs\" refers to the number of text-image pairs of\neach dataset.\nDataset\nSource Data\nImage Modality\nObjects of interest Text Category Num. Pairs\nPathVQA\nPathology Questions\nfor Medical\nVisual Question Answering\n[He et al., 2020]\nMicroscopy,\nDermoscopy,\nWSI,\nEndoscopic Video\nBrain, Liver,\nSkin, Cell,\nHeart, Lung,\nVessel, Kidney\nClosed-ended,\nOpen-ended\n77\nVQA-RAD\nClinicians asked naturally\noccurring questions of\nradiology images and\nprovided reference answers.\n[Lau et al., 2018]\nX-Ray,\nCT,\nMRI\nChest, Head,\nAbdomen\nClosed-ended,\nOpen-ended\n37\nPMC-VQA\nMixture of medical VQAs\nfrom PubmedCentral\u00ae\n[Zhang et al., 2023b].\nWe only select\nradiology-related pairs\nin this report.\nECHO,\nAngiography,\nUltrasound,\nMRI,\nPET\nNeck, Heart,\nKidney, Lung,\nHead, Abdomen,\nPelvic, Jaw,\nVessel\nClosed-ended,\nOpen-ended\n19\nTable 2: Statistics of the pathology data based on question type.\nQuestion Type\nTotal Number\nPercentage\nAnatomical Structures\n9\n11.69%\nLesion & Abnormality Detection\n10\n12.99%\nDisease Diagnosis\n12\n15.58%\nTemporal & History-Related Effects\n6\n7.79%\nSpatial Relationships\n3\n3.90%\nContrast Agents & Staining\n8\n10.39%\nMicroscopic Features\n16\n20.78%\nPathophysiological Mechanisms & Manifestations\n13\n16.88%\nthe imaging type and identifying the medical devices employed for capturing radiological images.\nRecognizing imaging types holds fundamental importance in the development of radiology AI models.\nThis recognition forms the basis for subsequent assessments, including evaluations of imaging density,\nobject size, and other related parameters. To ensure the diversity of modality-related data, we select\n10 images across X-ray, CT and MRI, and different anatomies: head, chest, and abdomen.\nTo ensure the diversity of modality-related data, we selected 10 images from various anatomical\nregions, including the head, chest, and abdomen, representing different imaging modalities such\nas X-ray, CT, and MRI. In our continued exploration of GPT-4V\u2019s capabilities, we employed three\nrepresentative images corresponding to modality-related pairs while utilizing the remaining questions.\nWe observed instances where GPT-4V exhibited misunderstandings, particularly in responding to\nposition- and size-related inquiries. To address these challenges, we further selected 10 size-related\npairs from VQA-RAD and 2 position-related pairs from VQA-RAD, supplemented by 6 position-\nrelated pairs from PMC-VQA [Zhang et al., 2023b].\nWe meticulously filtered these two datasets, manually selecting questions to balance the question types\nin terms of \"Modality Recognition,\" \"Structural Identification,\" \"Lesion & Abnormality Detection,\"\n\"Disease Diagnosis,\" \"Size & Extent Assessment,\" \u2019Spatial Relationships,\u2019 \u2019Image Technical Details,\u2019\nand \"Imaging Features,\" as well as varying the difficulty levels. To be more specific, \"Modality\nRecognition\" discerns the specific imaging modality, such as CT, MRI, or others. \"Structural\nIdentification\" seeks to pinpoint specific anatomical landmarks or structures within the captured\nimages. \"Lesion & Abnormality Detection\" emphasizes the identification of anomalous patterns or\naberrations. \"Disease Diagnosis\" aspires to deduce specific medical conditions based on imaging\nmanifestations. \"Size & Extent Assessment\" gauges the dimensions and spread of a lesion or\nabnormality. \"Spatial Relationships\" examines the relative positioning or orientation of imaged\nstructures. \"Image Technical Details\" delves into the nuances of the imaging process itself, such as\ncontrast utilization or image orientation. Lastly, \"Imaging Features\" evaluates characteristic patterns,\n7\nTable 3: Statistics of the radiology data based on question type.\nQuestion Type\nTotal Number\nPercentage\nModality Recognition\n8\n14.29%\nStructural Identification\n12\n21.43%\nLesion & Abnormality Detection\n12\n21.43%\nDisease Diagnosis\n5\n8.93%\nSize & Extent Assessment\n9\n16.07%\nSpatial Relationships\n4\n7.14%\nImage Technical Details\n3\n5.36%\nImaging Features\n3\n5.36%\nTable 4: Data statistics based on difficulty levels for the pathology and radiology sets.\nPathology\nRadiology\nDifficulty\nTotal Number\nPercentage\nDifficulty\nTotal Number\nPercentage\nEasy\n20\n26.0%\nEasy\n16\n28.6%\nMedium\n33\n42.9%\nMedium\n22\n39.3%\nHard\n24\n31.2%\nHard\n18\n32.1%\ntextures, or attributes discernible in the image, pivotal for diagnostic interpretation. For difficulty\nlevel, similar to pathology data, questions related to diagnostics are often categorized as challenging\ndue to their demand for a holistic understanding. This comprehension necessitates a deep grasp of\npreliminary aspects, including modality, objects, position, size, and more. Furthermore, it requires\nthe ability to filter and extract key medical knowledge essential for accurate diagnostics.\nIn summary, this experiment encompasses a total of 56 radiology VQA samples, with 37 samples\nsourced from VQA-RAD and 19 samples from PMC-VQA. This approach allows us to comprehen-\nsively assess GPT-4V\u2019s performance across a range of radiological scenarios, question types and\nmodes.\n4\nExperimental Results\n4.1\nPathology Accuracy\nFigure 3 shows the accuracy achieved in the pathology VQA task. Overall accuracy score is 29.9%,\nwhich means that GPT-4V can not give accurate and effecient diagnosis at present. To be specific,\nGPT-4V shows 35.3% performance in closed-ended questions. It performances worse than random\nguess (where the accuracy performance is 50%). This means that the answer generated by GPT-4V\nis not clinically meaningful. Accuracy score on open-ended questions reflects GPT-4V\u2019s capability\nin understanding and inferring key aspects in medical images. This categorization rationalizes\nGPT-4V\u2019s comprehension of objects, locations, time, and logic within the context of medical image\nanalysis, showcasing its versatile capabilities. As can be seen in Figure 3, the score is relatively low.\nConsidering this sub-set is quite challenging [He et al., 2020], the result is acceptable.\nMeanwhile, we collect the QA pairs in a hierarchy method that all pairs are divided into three\ndifficulty levels. As shown in Figure 3, the accuracy score in \"Easy\" set is 75.00%, higher than the\naccuracy in medium set by 59.80%. The hard set gets the lowest accuracy, at 8.30%. The accuracy\nscore experiences a decrease along with the increase of the difficulty level, which shows the efficiency\nand high quality of our collected data. The result demonstrates GPT-4V\u2019s proficiency in basic medical\nknowledge, including recognition of numerous specialized terms and the ability to provide definitions.\nMoreover, GPT-4V exhibits traces of medical diagnostic training, attempting to combine images with\nits medical knowledge to address medical questions. It also displays fundamental medical literacy,\noffering correct responses to straightforward medical queries. However, there is significant room\nfor improvement, particularly as questions become more complex and closely resemble real clinical\nscenarios.\n8\nFigure 3: Results of pathology VQA task. The\nbar chart on the left is related to the accuracy\nresult of questions with different difficulty levels,\nwhile the right chart is the results of closed-ended\nquestions and open-ended questions, marked as\nClosed and Open, respectively.\nFigure 4: Results of radiology VQA task. On the\nleft, we have a bar chart showcasing the accuracy\nresults for questions of varying difficulty levels.\nMeanwhile, on the right, outcomes for closed-\nended and open-ended questions are presented\nin separate charts.\n4.2\nRadiology Accuracy\nThe accuracy results for the VQA task within radiology are presented in Figure 4. To be more precise,\nthe overall accuracy score for the entire curated dataset stands at 50.0%. To present the GPT-4V\u2019s\ncapability in different fine-grained views, we will show the accuracy in terms of question types\n(open and closed-end), difficulty levels (easy, medium, and hard), and question modes (modality,\nsize, position, ...) in the following. In more specific terms, GPT-4V achieves a 50% accuracy rate\nfor 16 open-ended questions and a 50% success rate for 40 closed-ended questions with limited\nchoices. This showcases GPT-4V\u2019s adaptability in handling both free-form and closed-form VQA\ntasks, although its suitability for real-world applications may be limited.\nIt\u2019s worth noting that closed-ended questions, despite narrowing the possible answers, do not nec-\nessarily make them easier to address. To further explore GPT-4V\u2019s performance across varying\ndifficulty levels, we present its accuracy rates: 81.25% for easy questions, 59.09% for medium\nquestions, and a mere 11.11% for hard questions within the medical vision-language domain. Easy\nquestions often revolve around modality judgments, like distinguishing between CT and MRI scans,\nand straightforward position-related queries, such as object positioning. For an in-depth explanation\nof our question difficulty categorization method, please refer to Section 3.\nPosition and size are foundational attributes that hold pivotal roles across various medical practices,\nparticularly in radiological imaging: Radiologists depend on accurate measurements and spatial data\nfor diagnosing conditions, tracking disease progression, and formulating intervention strategies. To\nassess GPT-4V\u2019s proficiency in addressing issues related to positioning and size within radiology, we\nspecifically analyzed 8 position-related questions and 9 size-related questions. The model achieved\nan accuracy rate of 62.50% for position-related queries and 55.56%for size-related queries.\nThe lower accuracy observed for position-related questions can be attributed to two primary factors.\nFirstly, addressing these questions often requires a background understanding of the directional\naspects inherent in medical imaging, such as the AP or PA view of a chest X-ray. Secondly, the\ntypical workflow for such questions involves first identifying the disease or infection and then\nmatching it with its respective position within the medical image. Similarly, the reduced accuracy in\nresponding to size-related questions can be attributed to the model\u2019s limitations in utilizing calibrated\ntools. GPT-4V appears to struggle in extrapolating the size of a region or object of interest, especially\nwhen it cannot draw upon information about the sizes of adjacent anatomical structures or organs.\nThese observations highlight the model\u2019s current challenges in dealing with positioning and size-\nrelated queries within the radiological context, shedding light on areas where further development\nand fine-tuning may be needed to enhance its performance in this domain.\nIn the following, we will carefully select specific cases to provide in-depth insights into GPT-4V\u2019s\ncapabilities within various fine-grained perspectives.\n9\n5\nFeatures of GPT-4V with Case Studies\n5.1\nRequiring Cues for Accurate Localization\nIn medical imaging analysis, the accurate determination of anatomical positioning and localization\nis crucial. Position is typically established based on the viewpoint, with standard conventions\ngoverning this perspective. Such conventions are foundational in radiology and provide consistency\nin interpretation across different medical platforms and professionals. As demonstrated in Figure 5,\nGPT-4V has the potential to autonomously leverage and comprehend these viewpoint conventions.\nSpecifically, when presented with the contextual information \"In dental radiographs, the images are\noriented as if you are looking at the patient directly,\" GPT-4V was aptly able to utilize this knowledge\nand yield an accurate interpretation in response to the related question.\nHowever, the model\u2019s ability to consistently apply these conventions appears to be context-dependent.\nThis was evident in the VQA pair depicted in Figure 5. Here, GPT-4V initially overlooked the\ntraditional orientation of MRI imaging in its default response. It was only upon receiving an explicit\nhint about the imaging perspective that the model revised its answer, aligning it with the correct\ninterpretation.\nThis observation underscores a salient point: while GPT-4V is endowed with a vast reservoir of\nmedical knowledge and is capable of discerning context, its responses can sometimes hinge on the\nspecificity and clarity of the information provided, emphasizing the importance of user interaction\nand context provision to guide the model towards accurate conclusions. As our analysis is grounded\non zero-shot predictions, we maintain the view that in the bottom case, GPT-4V provide wrong\nanswer to the question without additional contexts or hints.\nFigure 5: Case for GPT-4V\u2019s requiring cues for accurate localization. [Question] and [GT] mark\nthe question and ground truth answer of the text-image pair, respectively. [Answer] refers to the\nanswer generated by GPT-4V. The upper case illustrates where GPT-4V autonomously considers the\nconvention of dental imaging, and answers the position-related question correctly. We feed a sub\nquestion to GPT-4V after the first question in the bottom case. It shows GPT-4V\u2019s ability to pinpoint\nthe locations of objects in radiology images irrespective of the traditional imaging orientation.\n5.2\nChallenge in Assessing Object Size\nWhen assessing the difficulty level of size-related questions, GPT-4V typically categorizes them\nas at least medium, often leaning towards hard. In general, GPT-4V demonstrates the ability to\ndistinguish relative sizes. For example, when asked, \"Is the heart size normal?\" it can provide an\n10\nanswer based on the principle that \"Generally, the heart\u2019s width should not exceed half the width of\nthe chest, or the cardiothoracic ratio should be less than 0.5.\" It\u2019s worth noting that GPT-4V tends to\nanswer correctly for most chest X-ray Q&A pairs but faces challenges when dealing with CT scans.\nA common response from GPT-4V when judging the size of objects in CT images is, \"Making a\ndefinitive assessment about the size and volume of [object] would require reviewing multiple slices to\nunderstand its entire length and width.\" This suggests that GPT-4V struggles to interpret the size of\none object relative to others or in the context of surrounding contours within CT images.\nFigure 6: Case for GPT-4V\u2019s relying excessively on text. [Answer] is generated by GPT-4V, while\n[Question] and [GT] are the question and ground truth answer of the text-image pair. Words in red\nshow that GPT-4V wrongly recognises the pattern in the image as antinuclear antibodies (ANA).\n5.3\nRelying Excessively on Text\nIn the investigations into the capabilities of GPT-4V, a pronounced reliance on textual content rather\nthan the integration of accompanying visual cues has been observed. This inclination leans heavily\ninto the model\u2019s expansive medical knowledge without sufficiently factoring in the nuances provided\nby visual data. Taking the provided instance as a prime example (shown in Figure 6), the disparity\nbetween the model\u2019s output and the expected gold-standard response is evident. As highlighted, GPT-\n4V, influenced by the textual context mentioning \u201csystemic sclerosis and Sj\u00f6gren syndrome,\u201d inferred\nthe presence of \u201cantinuclear antibodies (ANA)\u201d from the image. Contrastingly, the gold standard\nidentifies the image as showcasing \u201canti-centromere antibodies (ACA) diseases.\u201d From a standpoint\nof logic, GPT-4V\u2019s inference isn\u2019t entirely baseless. ANA is a broad category of autoantibodies found\nin various autoimmune diseases, inclusive of systemic sclerosis and Sj\u00f6gren syndrome. Given the\nbroad nature of ANA, and the diseases it encompasses, the connection made by GPT-4V can be\nunderstood.\nHowever, the inadequacy lies in the nuanced distinction between ANA and ACA. While both are\nautoantibodies, their specificity, associated conditions, and staining patterns vary considerably. ACA,\nspecifically targeting the centromere, would manifest differently in fluorescent staining compared to\na generic ANA. Despite a passing mention of the image\u2019s fluorescence, GPT-4V\u2019s response remained\nsuperficial in its description of the image, devoid of a more informed interpretation of the centromere\nfluorescence. It\u2019s evident that while the model possesses the capability to describe images, it might\nnot be optimally integrating this information with its extensive textual knowledge. While GPT-4V\nexhibits profound medical knowledge and textual understanding, its underwhelming utilization of\nvisual data, especially in contexts demanding a synergy of both, remains a limitation.\n5.4\nOveremphasizing Markings in Images\nAn emerging challenge observed in GPT-4V model, is the overemphasis on explicit markings or\nannotations within images, often at the expense of understanding the broader context and image\ninformation. As shown in Figure 7, GPT-4V tends to prioritize the symbols embedded within the\n11\ncoronary angiogram. Only RCA does not have explicit labeling, it results in the conclusion that\n\"The RCA is not visible in this image\". Instead of analyzing the structures present in the coronary\nangiogram of the image, the model became anchored to the absence of a textual label, revealing a\nshortcoming in its holistic understanding of the image content.\nFigure 7: Cases of overemphasizing markings in images. For the upper case, GPT-4V is susceptible to\nsymbols in the image. Due to the unlabelled RCA in the image, GPT-4V did not answer the question\ncorrectly, shown in [Answer]. The bottom case shows that because of the presence of an arrow in the\nimage, GPT-4V struggles to distinguish between contrasting queries and tends to provide identical\nresponses based solely on the arrow\u2019s indication, shown in [Answer-1] and [Answer-2], respectively.\nAnother evident manifestation of this challenge is observed when assessing lymph nodes in an image.\nIn the bottom case in Figure 7, GPT-4V\u2019s assessment was predominantly influenced by the presence\nof an arrow. Even when the query was modified from \"abnormal\" to \"normal,\" the model\u2019s focus\nremained unwaveringly on the marked element, reiterating its answer based on the arrow rather than\ngrasping the overall visual narrative. This example underscores a critical area for improvement. For\nrobust image interpretation in the VQA task, especially in the medical domain demanding precision,\nmodels should not only identify explicit markings but also appreciate the broader visual information\nto prevent such misconstruals.\n5.5\nNot Suitable for Diagnostics\nWhile GPT-4V can analyze and provide insights on various topics, including medical VQA task, its\naccuracy is not guaranteed. An illustrative case is its interpretation of a given H&E stained slide\nwhere it inferred the presence of extracapillary proliferation, as shown in Figure 8. This conclusion,\nhowever, appears contradictory to the actual context. GPT-4V\u2019s determination was influenced by\nits perception of the deep purple regions as the crowded cellular accumulation outside the capillary\nloops. In reality, these visual features might be resultant perturbations introduced during the slide\npreparation, staining, or scanning processes.\nStepping back from the specific case, several fundamental reasons underscore why GPT-4V isn\u2019t\nsuitable for diagnostic purposes. Clinical cases in reality are intricate, and slides processed for\nhuman examination entail various perturbations, many of which are unavoidable. Without sufficient\nexperience to discern and eliminate the influence of such perturbations, precise diagnoses become\nchallenging. Furthermore, GPT-4V lacks the expertise of medical professionals who evaluate a\nholistic view of the slide, incorporate multiple imaging perspectives, and factor in patient history for\naccurate diagnoses. Consequently, GPT-4V\u2019s evaluations, though advanced, are limited in their scope\nand should not be used for medical evaluation.\n12\nFigure 8: The case study of GPT-4V\u2019s not suitable for diagnostics. We ask GPT-4V two sequential\nquestions, marked as [Question] and [Sub-Question], respectively, and record its corresponding\nanswer in [Answer] and [Sub-Answer]. [GT] refers to the ground truth answer of the text-image pair.\n5.6\nCautious Answers\nIn the domain of medical analysis, GPT-4V consistently adopts a conservative approach, exemplified\nin Figure 9. Two salient examples illustrate this caution. In the upper instance, when tasked with\nidentifying a type of mass from a radiological image, GPT-4V declined, emphasizing the necessity of\nprofessional consultation. In the bottom one, faced with analyzing cardiac anatomy from a cross-\nsection of a heart, GPT-4V again demurred, noting the importance of comparing with a typical heart\nand soliciting expert medical advice.\nThis caution is rooted in the complexities and high stakes of medical decisions. Diagnoses often\nrequire comprehensive contextual knowledge beyond a single image. However, an inherent tension\nexists: while GPT-4V\u2019s conservative approach safeguards against potential harm or misrepresentation,\nit can sometimes seem overly cautious, potentially sidelining users\u2019 direct queries. This balance\nunderscores the challenge of leveraging artificial intelligence in medical contexts. GPT-4V\u2019s default\nto caution, even potentially at the expense of direct answer, reflects a prioritization of safety over\nimmediate information delivery.\n5.7\nThorough Answers with Details\nThis system is characterized by its capacity to elucidate its rationale alongside its answers. As\ndepicted in Figure 10, GPT-4V not only quantifies nucleated erythroid precursors present in the\nimage but also justifies its deduction by referencing the purplish-blue nucleus contrasted against a\npaler cytoplasm. Such elucidations foster users\u2019 deeper comprehension and permit validation of the\nsystem\u2019s methodologies. However, it\u2019s essential to note that these explanations might occasionally\nmiss the intricate nuances or complexities of certain topics.\nFurthermore, the system provides clarifications on terms present in the query or its response and\noffers supplementary context when requisite. This underscores its potential utility in educational\ncontexts. As exemplified in the bottom instance in Figure 10, GPT-4V autonomously elucidated\nthe concept of \"impending perforation in the intestines\". Furthermore, it elaborated on potential\nindicators for intestinal perforation, stating: \"Any focal point of severe discoloration, inflammation, or\nnecrosis (dead tissue) can also suggest areas at risk of perforation.\" Nonetheless, while the responses\nare comprehensive and largely accurate, they could be more concise and directly aligned with users\u2019\nexplicit queries. In instances of direct yes/no inquiries, excessive elaboration can be distracting and\npotentially obfuscate the primary message.\n13\nFigure 9: Cases of cautious answers of GPT-4V. Question and ground truth answer are marked as\n[Question] and [GT], respectively. The answer generated by GPT-4V is represented as [Answer].\nIn cases of ambiguity within radiology and pathology domains, GPT-4V consistently recommends\ndirect consultation with medical professionals rather than providing definitive answers to users.\nFigure 10: Case study of GPT-4V\u2019s capability to answer thoroughly with details. [GT] refers to the\nground truth answer to the question. Additional details provided by GPT-4V are in red.\n6\nDiscussion and Limitation\nIn the study, we explore the zero-shot VQA capabilities of GPT-4V in radiology and pathology tasks.\nthe current study\u2019s breadth is constrained by the lack of APIs for multimodal input and the challenges\nposed by manual data input and response documentation. This scope offers avenues for expansion in\nsubsequent studies. a larger sample size might yield a more comprehensive evaluation.\nWe assess GPT-4V\u2019s capabilities in medicine from an AI practitioner\u2019s viewpoint rather than that of\nmedical practitioners. For professional medical insights regarding GPT-4V\u2019s responses, collaboration\nwith the medical community is essential. By involving subjec- matter experts, we can better ensure\nthat critical nuances are captured and conclusions are more precise.\nMoreover, the dataset primarily features an image with its corresponding question, omitting potentially\nvaluable context like patient history or varied imaging perspectives. Incorporating such comprehensive\n14\ndata could align more closely with the holistic approach medical professionals take, ensuring a more\nin-depth and accurate assessment by the model.\nThe basic prompt structure used in the experiment offers room for enhancement. The craft of\ndesigning impactful prompts can play a vital role in refining the quality of the answers. A more\nnuanced prompt might yield more consistent and insightful outcomes.\nGPT-4V\u2019s role in radiology and pathology is an emerging area with potential. Its diagnostic efficacy\nin these fields might see improvement with a broader dataset, enhanced prompt methodologies,\nand feedback from domain specialists. A collaborative approach could help navigate the present\nlimitations.\n7\nConclusion\nIn the study, we evaluate the zero-shot VQA capabilities of the current version of GPT-4V in\nthe realms of radiology and pathology using a hand-curated dataset. We identified seven unique\ncharacteristics of GPT-4V\u2019s performance in medical VQA, highlighting its constraints within this\narea. Due to the poor performance of GPT-4V on the medical VQA dataset, and considering the\nsevere consequences of erroneous results in the medical field, GPT-4V should not currently be used\nas a reliable tool for medical diagnosis and providing treatment suggestions.\nReferences\nJuli\u00e1n N Acosta, Guido J Falcone, Pranav Rajpurkar, and Eric J Topol. Multimodal biomedical ai.\nNature Medicine, 28(9):1773\u20131784, 2022.\nZhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, and Xiang Wan. Towards unifying medical\nvision-and-language pre-training via soft prompts. arXiv preprint arXiv:2302.08958, 2023.\nTianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexan-\nder L\u00f6ser, Daniel Truhn, and Keno K Bressem. Medalpaca\u2013an open-source collection of medical\nconversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023.\nXuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for\nmedical visual question answering, 2020.\nJason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically\ngenerated visual questions and answers about radiology images. Scientific data, 5(1):1\u201310, 2018.\nPeter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai chatbot\nfor medicine. New England Journal of Medicine, 388(13):1233\u20131239, 2023a.\nPeter Lee, Carey Goldberg, and Isaac Kohane. The AI revolution in medicine: GPT-4 and beyond.\nPearson, 2023b.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023.\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities\nof gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.\nOpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.\npdf, 2023.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode\nclinical knowledge. Nature, 620(7972):172\u2013180, 2023.\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang,\nAndrew Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai.\narXiv preprint arXiv:2307.14334, 2023.\n15\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:\nTuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070,\n2023.\nKai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen,\nYuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained\ntransformer for vision, language, and multimodal tasks. arXiv preprint arXiv:2305.17100, 2023a.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi\nXie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint\narXiv:2305.10415, 2023b.\nRong Zhou, Houliang Zhou, Brian Y Chen, Li Shen, Yu Zhang, and Lifang He. Attentive deep\ncanonical correlation analysis for diagnosing alzheimer\u2019s disease using multimodal imaging\ngenetics. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pages 681\u2013691. Springer, 2023.\n16\n"
  },
  {
    "title": "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving",
    "link": "https://arxiv.org/pdf/2310.19102.pdf",
    "upvote": "7",
    "text": "ATOM: LOW-BIT QUANTIZATION FOR EFFICIENT AND ACCURATE LLM\nSERVING\nYilong Zhao 1 2 * Chien-Yu Lin 2 Kan Zhu 2 Zihao Ye 2 Lequn Chen 2 Size Zheng 2 3 * Luis Ceze 2\nArvind Krishnamurthy 2 Tianqi Chen 4 Baris Kasikci 2\nABSTRACT\nThe growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent\nchatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use\nGPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further\nspeed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity.\nHowever, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the\ncapabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.\nTo maximize LLMs\u2019 serving throughput, we introduce Atom, a low-bit quantization method that achieves high\nthroughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using\nlow-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy\nby applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-\nactivation quantization setups in the serving context. Atom improves end-to-end throughput by up to 7.73\u00d7\ncompared to the FP16 and by 2.53\u00d7 compared to INT8 quantization, while maintaining the same latency target.\nCode will be released at: https://github.com/efeslab/Atom.\n1\nINTRODUCTION\nLarge Language Models (LLMs) are being increasingly in-\ntegrated into our work routines and daily lives, where we\nuse them for summarization, code completion, and decision-\nmaking. Studies report that ChatGPT has over 100 mil-\nlion users, with more than 1 billion website accesses per\nmonth (Duarte, 2023). Furthermore, the size and capabilities\nof LLMs continue to grow to accommodate a broader range\nof tasks. The high inference demand and model complexity\nhave significantly increased the operational costs, i.e., com-\npute/memory and energy, for LLM service providers to near\n$1 million daily (Elimian, 2023).\nUnsurprisingly, optimizing LLM serving is becoming a\npressing concern. Most efforts have focused on improv-\ning LLM serving throughput, which is typically achieved by\nbatching requests from various users (Yu et al., 2022; Chen,\n*Work done during the internship at UW.\n1Department\nof Computer Science and Engineering, Shanghai Jiao Tong\nUniversity, Shanghai, China\n2School of Computer Science\n& Engineering, University of Washington, Seattle, United\nStates 3School of Computer Science, Peking University, Bei-\njing, China\n4School of Computer Science, Carnegie Mel-\nlon University, Pittsburgh, United States.\nCorrespondence\nto: Yilong Zhao <zhaoyilong217@sjtu.edu.cn>, Baris Kasikci\n<baris@cs.washington.edu>.\nPreprint, under review.\n2023a; Kwon et al., 2023). Batching multiple requests in-\ncreases compute intensity and amortizes the cost of loading\nweight matrices, thereby improving throughput. To further\nimprove batching efficiency, prior work has explored LLM\nquantization techniques. These techniques employ smaller\ndata types to replace 16-bit floating point (FP16) values,\nthereby reducing memory consumption and accelerating\ncomputation (Lin et al., 2023; Xiao et al., 2023).\nHowever, current quantization schemes do not leverage the\nfull extent of capabilities provided by emerging efficient\nlow-bit hardware support (e.g. Nvidia Ampere (Abdelkhalik\net al., 2022) and Qualcomm Hexagon (Wikipedia contrib-\nutors, 2023)). For instance, several prior approaches have\nexplored weight-only quantization (Lin et al., 2023; Frantar\net al., 2023). In these quantization schemes, weights are\nquantized to an integer representation (e.g., INT3), whereas\nactivations remain in a floating point representation (e.g.,\nFP16). Consequently, weights must be dequantized to the\nappropriate floating point representation (e.g., FP16) before\nbeing multiplied with activations. Therefore, even though\nweight-only quantization reduces memory consumption, it\nstill requires costly floating-point arithmetic, which is ineffi-\ncient, especially for large batch sizes.\nAnother prominent quantization scheme is weight-activation\nquantization, where both weights and activations are\nquantized to lower-bit representations.\nIn this scheme,\narXiv:2310.19102v2  [cs.LG]  7 Nov 2023\nPreprint\nStatic\nReorder\nGroup\nQuant\nDynamic\nReorder\nLow-bit Group\nQuant\nHigh-bit\nQuant\nOutlier\nActivation\nWeight\nFused\nGEMM\nFused\nFlashInfer\nNext Token\nNext Token\nLow-bit\nKV Cache\nFused\nGEMM\nFigure 1. Overview of the Atom quantization design. For activa-\ntion matrices, we dynamically reorder the channels to pick out the\noutliers. Then, we apply low-bit group quantization to the normal\nvalues while using high-bit precision for outliers. For weight ma-\ntrices, the quantization process can be done statically. We perform\nfused GEMM and fused FlashInfer (Ye, 2023) to boost throughput.\nWe also use a quantized KV cache to reduce memory movement.\nweights and activations can be directly multiplied using\nlow-precision arithmetic units. This approach has greater\npotential to achieve higher throughput than weight-only\nquantization due to the efficient low-bit hardware sup-\nport. For example, A100 GPUs can reach 1,248 TOPS\nof INT4 and 624 TOPS of INT8 as opposed to 312\nTFLOPS for FP16 (NVIDIA, a).\nPrior works such as\nLLM.int8() (Dettmers et al., 2022) and SmoothQuant (Xiao\net al., 2023) explored INT8 weight-activation quantization\nand achieved near no accuracy loss. However, INT8 quanti-\nzation still cannot utilize lower bit arithmetic such as INT4\nTensorCore (NVIDIA, b). In addition, INT8 quantization\nremains sub-optimal for reducing the large memory con-\nsumption in LLM serving, where both model parameters\nand batched KV-cache consume large memory (Sheng et al.,\n2023). For lower-bit weight-activation quantization, re-\ncent works such as OmniQuant (Shao et al., 2023) and\nQLLM (Liu et al., 2023a) have proposed to quantize LLMs\ndown to 4-bit. However, their techniques still show a per-\nplexity increase compared to the FP16 baseline as shown in\nFigure 2. Therefore, determining how to accurately quan-\ntize LLMs into low-bit representations while maintaining\nhardware efficiency remains an open area of research.\nIn this work, we introduce Atom, an accurate low-bit weight-\nactivation quantization for LLMs that efficiently use modern\nhardware. To maintain accuracy, Atom incorporates three\nkey quantization designs: (1) It adopts mixed-precision\nquantization and retains a small but salient number of activa-\ntions and weights in high precision to preserve accuracy. (2)\nIt employs fine-grained group quantization on both weights\nand activations, which naturally reduces quantization errors.\n7B\n13B\n30B\n65B\n# Llama Parameters\n3\n5\n7\n9\n11\n13\nPerplexity \nFP16\nOmniQ\nQLLM\nAtom\nFigure 2. WikiText2 perplexity on the Llama with 4-bit weight-\nactivation quantization (Lower is better). Atom in W4A4 maintains\na perplexity close to the FP16 baseline across all model sizes,\noutperforming existing works.\n(3) Instead of pre-calculating quantization parameters for\nactivations, Atom dynamically quantizes activations to best\ncapture the distribution of each input.\nAlthough these quantization optimizations can improve\nquantization accuracy, they may not use the underlying\nhardware efficiently without a bespoke design. For example,\nthe mixed-precision technique could lead to irregular mem-\nory accesses and performance slowdown (Guo et al., 2023);\nmultiplications with group quantization are not supported\nin any vendors\u2019 library; and dynamic quantization of acti-\nvations incurs extra computation. To ensure high hardware\nefficiency and minimize quantization overheads, Atom: (1)\nreorders activations and weights to maintain regular memory\naccesses for mixed-precision operations, (2) fuses quanti-\nzation and reordering operations into existing operators to\namortize the overheads, (3) further quantizes outliers into\nINT8 to keep a balance between accuracy and efficiency and\n(4) quantizes KV-cache into low-bit to enhance throughput.\nWe illustrate Atom\u2019s quantization flow in Figure 1.\nTo validate Atom\u2019s feasibility, we integrate it into an end-to-\nend serving framework (Chen, 2023b). For our special ma-\ntrix multiplications with mixed-precision and group quanti-\nzation, we implement customized CUDA kernels that utilize\nlow-bit tensor cores. Experiments on popular datasets show\nthat Atom has negligible accuracy loss (1.6% average zero-\nshot accuracy drop, 0.5 perplexity increase for Llama-7B\nWikiText2) when quantizing Llama (Touvron et al., 2023)\nmodels to W4A4 (4-bit for both weights and activations),\nwhile prior works suffer larger accuracy loss under the same\nprecision (see Figure 2).\nWhen comparing end-to-end serving throughput to differ-\nent precisions and quantization schemes, Atom improves\nthroughput by up to 7.7x, 5.5x, and 2.5x relative to FP16,\nW4A16, and W8A8, respectively, while achieving similar\nlatency (see Figure 11). These results show that Atom can\naccurately quantize LLMs into very low-precision while\nachieving high serving throughput.\nIn summary, we contribute the following:\nPreprint\n\u2022 An analysis of LLM serving that pinpoints the perfor-\nmance benefit of low-bit weight-activation quantiza-\ntion.\n\u2022 Atom, an accurate low-bit weight-activation quantiza-\ntion algorithm that combines (1) mixed-precision, (2)\nfine-grained group quantization, (3) dynamic activation\nquantization to minimize quantization errors, and (4)\nKV-cache quantization.\n\u2022 An integrated LLM serving framework for which we\ncodesign an efficient inference system, implement low-\nbit GPU kernels, and demonstrate realistic end-to-end\nthroughput and latency of Atom.\n\u2022 A comprehensive evaluation of Atom, which shows\nthat it improves LLM serving throughput by up to 7.7x\nwith only negligible loss of accuracy.\n2\nBACKGROUND\nQuantization techniques use discrete low-bit values to ap-\nproximate high-precision floating points. Since integers\nrepresent a uniform range, quantizing floating point values\ninto integers is widespread due to simplicity and hardware\nefficiency (Jacob et al., 2017). Quantization involves two\nsteps: determining the quantization parameters (which con-\nsist of scale and zero point) and calculating the quantized\ntensor. For uniform asymmetric quantization, the scale s\nand zero point z are determined by (Nagel et al., 2021):\ns = max(X) \u2212 min(X)\n2n \u2212 1\n\u00d7 c, z = \u230a\u2212 min(X)\ns\n\u2309,\n(1)\nwhere X is the input tensor, n is the quantization bit-width,\nand c is the clipping factor used to reduce the dynamic range\nof quantization to mitigate the effect of outlier values. The\nelements in quantized tensor can be calculated by:\n\u00afX = clamp(\u230aX\ns \u2309 + z, 0, 2n \u2212 1).\nWe can further simplify this equation for symmetric quanti-\nzation:\ns = 2 \u00d7 max(|X|)\n2n \u2212 1\n\u00d7 c\n\u00afX = clamp(\u230aX\ns \u2309, \u22122n\u22121, 2n\u22121 \u2212 1).\nQuantization parameters s and z can be calculated either\nstatically using calibration data or dynamically during infer-\nence time. Thus, quantization approaches can be classified\nas static or dynamic.\nFor LLMs, we can apply quantization on both activation\nand weight matrices (weight-activation quantization) or just\nthe latter (weight-only quantization). However, asymmetric\n8\n16\n32\n64\nBatch Size(#)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime Distribution\nDense Layer\nSelf-Attention\nOther\nFigure 3. Runtime breakdown of Llama-7b serving with different\nbatch sizes. The dense layer represents the batched K, Q, V genera-\ntion, O projection, and MLP. The self-attention layer is the batched\nversion of FlashInfer integrated with PageAttention (Kwon et al.,\n2023). Results indicate that the dense and self-attention layers\ntogether account for over 90% of the execution time, thereby con-\nstraining the throughput.\nweight-activation quantization can lead to additional calcu-\nlations during matrix multiplication since:\nW \u00b7 X = sW ( \u00afW \u2212 zW ) \u00b7 sx( \u00afX \u2212 zx),\nwhere three additional cross-terms will be calculated. There-\nfore, we apply symmetric uniform quantization in this work.\nWe can also apply quantization under different granularity.\nFor per-tensor quantization, all the values in the tensor\nshare one set of scale and zero-point. For per-channel\nquantization, we calculate scale and zero-point for a row or\na column of the tensor. We can further divide every channel\ninto several groups, and perform quantization on each group,\nwhich is per-group quantization.\n3\nPERFORMANCE ANALYSIS OF LOW-BIT\nLLM SERVING\nIn this section, we first analyze the performance bottleneck\nof LLM serving and then establish the importance of low-bit\nweight-activation quantization.\nDue to high demand, LLM serving is throughput-oriented.\nHowever, the decode stage of LLM inference only takes one\ntoken as input and generates the next token, thus relying on\nmatrix-vector multiplication (GEMV). Since GEMV needs\nto load a large weight matrix while only performing a few\nmultiplications, it is heavily memory-bound. It thus causes\nGPU under-utilization, which results in low compute inten-\nsity (computation-to-IO ratio) and, thereby, low throughput.\nTo mitigate this problem, batching is widely used by com-\nbining the input from multiple requests to perform dense\nlayer (K,Q,V generation, O projection, and MLP) matrix\nmultiplications and increase compute intensity and GPU\nutilization (Pope et al., 2022; Yu et al., 2022; Sheng et al.,\n2023; Chen, 2023a).\nTo further boost throughput, the input matrices of the dense\nPreprint\nMemory-\nbound\nCompute-\nbound\nFP16\nPeak\nOps\nINT4\nPeak\nOps\nFP16\n4-bits\nWeight-Activation\nQuant\nOps\nElements\nOps\ns\n(a) Weight-activation quantiza-\ntion\nMemory-\nbound\nCompute-\nbound\nFP16\nPeak\nOps\nFP16\n4-bits\nWeight-only Quant\nSelf-attention\nDense layer\nOps\nElements\nOps\ns\n\u00d7\n\u00d7\n(b) Weight-only quantization.\nFigure 4. A roofline model of different quantization approaches\nthat characterizes operators by their arithmetic intensity, which is\ndefined as Ops/Elements. At large batch sizes, the dense layer is\ncompute-bound, which has a large arithmetic intensity, whereas\nthe self-attention exhibits a lower arithmetic intensity.\nlayer of the decode and prefill stages are batched to form\nlarger matrices, which further amortizes the memory move-\nment of the weight matrix and exploits the batching effect.\nGiven large batch sizes, the dense layer becomes compute-\nbound. However, although self-attention layers in the de-\ncode stage are also GEMV operations, inference requests\ndo not share the KV-cache. Therefore, self-attention layers\ncannot be batched and need to be performed sequentially.\nThe self-attention layers are still bounded by large memory\nmovement due to KV-cache, even with highly-optimized\nkernels (e.g. FlashAttention (Dao, 2023)).\nAfter applying the batching technique, we measure the time\nbreakdown of different operators under different batch sizes.\nAs Figure 3 shows, both the dense and self-attention layers\nact as bottlenecks to throughput, consuming over 90% of\nthe processing time. Consequently, we employ quantization\nmechanisms to expedite the dense and self-attention layers.\nWe use the Roofline model (Williams et al., 2009) to eval-\nuate the effect of quantization approaches. As Figure 4(a)\nshows, weight-activation quantization has higher dense layer\ncompute throughput due to the efficient low-bit hardware\narithmetic. Weight-activation quantization also increases the\nthroughput of the self-attention layer by reducing the size\nof the KV-cache, thus decreasing memory movement. How-\never, as Figure 4(b) shows, weight-only quantization fails to\nimprove dense layer throughput since dequantization must\nbe performed before matrix multiplications, yielding calcu-\nlations still in the floating point format. On the other hand,\nweight-only quantization fails to quantize the KV-cache,\nyielding no benefit for self-attention layers. We further\nquantify the effect of different quantization techniques in\nFigures 10(a) and 10(b) in \u00a75.\nIn summary, the low-bit weight-activation quantization is\nsuperior to weight-only quantization for enhancing the\n0\n2000\n4000\nChannel (#)\n4\n2\n0\n2\nMean value (#)\nOutliers\n(a) Activation mean values per\nchannel.\n0\n2000\n4000\nChannel (#)\n4\n2\n0\n2\nMean value (#)\nNormal Values\nOutliers\nLow-bit Quant\n8-bit Quant\n(b) Mean values after reorder-\ning.\nFigure 5. The sampled value of the activation matrix. (a) The\nactivation matrix contains outlier channels, which result in large\nquantization errors. (b) We reorder these outlier channels to the\nend of the matrix and use higher precision to quantize them while\nkeeping regular memory access.\nthroughput of the serving scenario because it accelerates\nboth the dense and self-attention layers.\n4\nDESIGN\nLow-bit precision allows the utilization of the underly-\ning hardware efficiently, thus delivering higher throughput.\nHowever, it is challenging to achieve high accuracy with\na low-bit representation. To quantize LLMs to extremely\nlow-bit precision while keeping accuracy, we incorporate a\nsuite of quantization mechanisms tailored to LLM character-\nistics. These mechanisms include mixed-precision quantiza-\ntion with reordering, fine-grained group quantization, and\ndynamic quantization. We demonstrate the accuracy gain\nthanks to these techniques in Table 4. We also quantize the\nKV-cache to boost the throughput of Atom. The subsequent\nsubsections delve into the specifics of each mechanism and\nits advantages, followed by a detailed description of the\nend-to-end workflow.\n4.1\nMixed-precision quantization\nPrior works (Dettmers et al., 2022; Lin et al., 2023) observed\nthat a key challenge of LLM quantization is the outlier phe-\nnomena in activations. As Figure 5(a) shows, a few channels\nexhibit large magnitudes that are several orders greater than\nthose of other channels. The large dynamic range of these\noutliers can substantially increase the quantization error.\nTherefore, efficiently handling the outliers is crucial in low-\nbit quantization, and we can quantize outliers and normal\nvalues separately to effectively mitigate this challenge.\nAs Figure 5(b) shows, after we remove the outliers, the re-\nmaining channels are much more uniform, which can be\neffectively expressed by low-bit values. Our results indicate\nthat 8-bit precision representations (such as FP8 (Micikevi-\ncius et al., 2022) and INT8) are sufficient to express outliers\nPreprint\n  k_proj\n  v_proj\n  q_proj\n  o_proj\n+\n gate_proj\n  up_proj\n  down_proj\n+\nLayer Normalization\nReorder&Requant\nFlashInfer\nLayer Normalization\nReorder & Requant\nRequant\nKV\nReorder & Requant\nDequant\nActivation\nRequant\nLlama Decoder Layer\nFP16\nLow-bit\nFused Op\nStatic Reorder\nDequant\nRequant\nDequant\nDequant\nDequant\nDequant\nDequant\nDequant\nKV\nQuantized KV Cache\nFigure 6. Overview of the Atom workflow on Llama. Atom carefully manages the overhead of quantization operators by fusing them into\nexisting operators. For the compute-bound operators, Atom utilizes efficient low-bit hardware support. For memory-bound self-attention\nlayer, Atom quantizes KV-cache to further enhance the throughput.\n11\n33\nA\nW\n1\n0\n-27\n71\n3\n-2\n=\nStatic\nReordered\n0\n0\n47\n-99\n-1\n4\n11\n33\n1\n0\n0\n0\n47\n-99\n-1\n4\nDynamic Reorder\nOutput of last layer\nOutput\nX\nNormal\nValues\nOutliers\nY\nFigure 7. The reorder methodology. We dynamically reorder acti-\nvation (A) to move the outlier channels to the end of the matrix.\nThe weight matrix (W) is statically reordered to remain aligned\nwith the corresponding activation channels.\n(See Table 4). Since INT8 is widely supported by hardware\nimplementations (e.g., Nvidia Tensor cores (Abdelkhalik\net al., 2022)), Atom applies INT8 quantization for outliers.\nThe primary concern with mixed-precision quantization is\nits irregular memory accesses (Dettmers et al., 2022; Guo\net al., 2023), which leads to poor hardware efficiency. To ap-\nply mixed-precision quantization while maintaining regular\nmemory access, Atom re-purposes the reordering technique\nintroduced in RPTQ (Yuan et al., 2023), where the objec-\ntive was to improve inference accuracy. As Figure 7 shows,\nAtom reorders the scattered outlier channels of activations\nto the end of the matrix with the goal of enabling the ef-\nficient implementation of mixed-precision within a single\nkernel. Such reorder indices are determined statically using\ncalibration data; thus, reordering of weight matrices is a\none-time offline cost. However, the reordering of activation\nmatrices still needs to be performed online, which can be ex-\npensive. To mitigate this, Atom fuses the activation matrix\nreordering into prior operators, which significantly reduces\nthe reordering overhead to 0.5% of runtime.\n4.2\nFine-grained group quantization\nEven if we quantize outliers and normal values separately,\nthe latter is still challenging to perform accurately due to\nthe limited representation range of 4-bit quantization (Sec-\ntion 5.4). To further enhance accuracy, group quantization is\nwidely adopted (Lin et al., 2023; Nagel et al., 2021), which\ndivides the channel into subgroups and performs quantiza-\ntion within each subgroup. For example, a group size of\n128 implies that every contiguous sequence of 128 elements\nin a single channel is treated as a single group. Atom then\nperforms quantization for every group.\nGroup quantization offers a trade-off between accuracy im-\nprovements and dequantization overheads, which has not\nbeen investigated by prior work. Atom uses a dedicated\nhigh-throughput fusion technique, as shown in Figure 8.\nAtom first calculates the matrix multiplication of the acti-\nvation groups with the corresponding weight groups and\nobtains temporary results using efficient low-bit hardware\n(Step 1 ). Atom then adds multiple temporary results to-\ngether to get the GEMM result. However, since Atom per-\nforms quantization for each activation and weight group,\neach temporary result has different quantization parame-\nters. Therefore, Atom first dequantizes all temporary results\nto the FP16 representation (Step 2 ) and then performs\naddition (Step 3 ). To manage the overhead, we fuse de-\nquantization and summation into the GEMM kernel so that\nwe are able to perform both operations in place without\nextra memory movement. We demonstrate the throughput\nof fused matrix multiplication in \u00a75.3.1.\nPreprint\n+\nDequant\nDequant\nGroup\nINT\nFP16\n1\n2\n3\nFigure 8. A design overview of fused matrix multiplication. After\nthe multiplication of each group, the result is first dequantized and\nsubsequently accumulated.\n4.3\nDynamic quantization process\nAlthough fine-grained quantization can better preserve the\nlocal variations inside each channel of activations, this ad-\nvantage would diminish if we statically calculated the quan-\ntization parameters based on calibration data, as the actual\ninput might have a different local distribution.\nTherefore, Atom adopts dynamic quantization, tailoring\nquantization parameters for each activation matrix during\ninference. To tame the overhead of dynamic quantization,\nwe fuse quantization into the prior operator, akin to the im-\nplementation of ZeroQuant (Yao et al., 2022). The run time\nof the fused dynamic quantization is negligible compared to\nthe dense and self-attention layers, as Figure 3 shows.\nHowever, asymmetric quantization can lead to significant\nrun-time overhead due to considerable additional compu-\ntation (as discussed in \u00a72). To strike a balance between\nthroughput and accuracy, we choose symmetric quantization\nwith a carefully chosen clip threshold. We also incorporate\nGPTQ (Frantar et al., 2023) when quantizing the weight\nmatrix since this is purely an offline process and offers an\naccuracy boost without sacrificing throughput.\n4.4\nKV-cache quantization\nAs described in \u00a73, the self-attention layer in the decode\nstage is highly memory-bound. To mitigate this issue, Atom\nalso applies low-bit quantization to the KV-cache. Atom\nloads the KV-cache in low-bit precision and directly dequan-\ntizes it before performing the FP16 self-attention calculation,\nbecause computation is not the bottleneck in this case. On\nthe other hand, since the memory movement of asymmet-\nric and symmetric quantized KV-cache are similar, they\nperform similarly on memory-bound self-attention layers.\nTherefore, Atom uses asymmetric quantization as it can\nprovide extra accuracy benefits.\nCompared with activation matrices, we argue that the KV-\ncache is more amenable to quantization. To perform self-\n0\n20\n40\n60\n80\n100\n120\nChannel (#)\n0.2\n0.1\n0.0\n0.1\n0.2\nMean Value\nV\nFigure 9. The sampled value of V cache within a single head. Com-\npared with activations shown in Figure 5(a), V cache shows less\ndynamic range, which is much easier to quantize.\nattention, the Query vector of the incoming token is multi-\nplied by the K cache. The result is normalized using Soft-\nmax, and further multiplied with the V cache to obtain the\noutput. Due to the normalization of Softmax, the quantiza-\ntion error of the K cache has less influence on the output.\nFurthermore, our profiling in Figure 9 indicates that the V\ncache exhibits the outlier phenomenon less frequently, ren-\ndering it more suitable for quantization. Therefore, Atom\ndirectly applies asymmetric low-bit quantization within each\nattention head and preserves high accuracy as shown in \u00a75.4.\nWe fuse the dequantization with the state-of-the-art self-\nattention optimization technique, FlashInfer (Ye, 2023), and\nGPU memory management system, PageAttention (Kwon\net al., 2023), to achieve high throughput.\n4.5\nImplementation of quantization workflow\nTo demonstrate the feasibility of our design choices, we\nimplement Atom on Llama models (see Figure 6).\nTo\nleverage the benefit of quantization, we carefully manage\nthe overhead of the additional operators by kernel fusion:\nAtom fuses quantization operators, including reordering,\nre-quantization, and dequantization, into existing operators.\nFor the compute-bound dense layer, we use the low-bit\ncalculation to boost throughput. For the memory-bound\nself-attention layer, we fuse the dequantization with Flash-\nInfer so that we need to load only low-bit integers from the\nKV-cache to achieve high throughput. We also incorporated\nPageAttention for efficient usage of memory in order to\nenable the use of large batch sizes.\n5\nEVALUATION\nWe now evaluate Atom\u2019s accuracy (perplexity, zero-shot)\nand efficiency (kernel throughput, end-to-end throughput,\nend-to-end latency) and examine the effectiveness of its\ndifferent quantization techniques. We begin by reviewing\nour quantization setup.\n5.1\nQuantization setup\nAtom uses symmetric quantization on weights and activa-\ntions while using asymmetric quantization on the KV-cache.\nPreprint\nTable 1. WikiText2 perplexity on Llama2 at 7B and 13B.\nPerplexity \u2193\n# Bits\nMethod\nLlama2-7B\nLlama2-13B\nFP16\n-\n5.47\n4.88\nSmoothQuant\n83.12\n26.87\nOmniQuant\n14.61\n12.3\nW4A4\nAtom\n6.03\n5.26\nSmoothQuant\nInf\nInf\nW3A3\nAtom\n15.07\n9.29\nWe evaluate Atom using a group size of 128. To identify\noutlier channels, we use 128 randomly sampled sentences\nfrom WikiText2 (Merity et al., 2016) as calibration data, fol-\nlowing prior work (Lee et al., 2023; Shao et al., 2023; Liu\net al., 2023a). We then reorder the activations and weights\nbased on the square sum of each activation channel. After\nreordering, we select 128 channels with the highest square\nsum values as outliers and keep them in INT8. To apply\nclipping, we use grid search to find the optimal clipping fac-\ntors 0.9 and 0.85 for activations and weights, respectively.\nFor the quantization process, we run Atom on a single\nRTX 6000 GPU and quantize the model layer-by-layer.\nFor Llama-65B, Atom takes only 4 hours to complete\nthe entire quantization process. In contrast, related work\nSpQR (Dettmers et al., 2023), takes more than 6 hours while\nOmniQuant (Shao et al., 2023) takes up to 16 hours, demon-\nstrating the efficiency of our quantization technique.\n5.2\nAccuracy evaluation\nBenchmarks. We evaluate Atom by quantizing Llama (Tou-\nvron et al., 2023)1 models with simulated quantization pro-\ncess following (Xiao et al., 2023). We focus on INT4\nand INT3 weight-activation quantization since previous\nworks (Dettmers et al., 2022; Xiao et al., 2023; Shao et al.,\n2023; Liu et al., 2023a) demonstrated that LLMs can be\nquantized with negligible accuracy loss using INT8 and\nINT6 weight-activation quantization. To examine the ac-\ncuracy, we evaluate Atom on the commonly used metrics,\nperplexity, and zero-shot accuracy. For perplexity, we evalu-\nate on WikiText2 (Merity et al., 2016), PTB (Marcus et al.,\n1994), and C4 (Raffel et al., 2020) datasets. For zero-shot\naccuracy, we use the lm-eval (Gao et al., 2021) and evalu-\nate on PIQA (Bisk et al., 2019), ARC (Clark et al., 2018),\nBoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019),\nand WinoGrande (Sakaguchi et al., 2019) tasks. We pri-\nmarily evaluate Atom on Llama-1. Additionally, in order to\ndemonstrate Atom\u2019s generalizability, we include perplexity\nresults for Llama-2 at 7B and 13B.\n1Our Llama-1 and Llama-2 models are downloaded from\ndecapoda-research and meta-llama on HuggingFace.\nBaselines.\nWe compare Atom to recently released\npost-training weight-activation quantization techniques,\nSmoothQuant (Xiao et al., 2023), OmniQuant (Shao et al.,\n2023), and QLLM (Liu et al., 2023a). For SmoothQuant, we\nimplement our own version as the official code does not sup-\nport Llama and only has INT8 quantization. We conducted a\ngrid search on the alpha value defined in the SmoothQuant\npaper and reported the best numbers for each benchmark.\nFor OmniQuant, we use their pre-trained weights for W4A4\nevaluations and train our own results using their official code\nfor W3A3. To obtain W3A3 results for OmniQuant, we use\nlr = 1e\u22124 and alpha = 0.75 after a hyperparameter search.\nWe do not evaluate W3A3 OmniQuant on Llama-30B and\nLlama-65B due to the large GPU memory requirement of\nits finetuning process. For QLLM, we report the W4A4\nnumbers listed in their paper but do not evaluate W3A3\nsince their code is currently unavailable.\nZero-shot accuracy. Table 2 compares the zero-shot accu-\nracy of six common sense tasks between Atom and our base-\nlines using Llama. Atom significantly outperforms the other\nweight-activation quantization methods. For W4A4, Atom\nshows only a 2.3%, 1.7%, 0.4% and 1.4% accuracy loss for\nLlama at 7B, 13B, 30B and 65B sizes when compared to\nFP16. At the same time, previous works showed a 9.6% to\n24.8% accuracy loss under the same settings. For W3A3,\nAtom achieves an averaged 9.9% accuracy loss across differ-\nent Llama sizes, showing significant improvement compared\nto previous weight-activation quantization methods. This is\nparticularly impressive given that our method requires no\ncomputationally heavy finetuning process.\nLlama Perplexity. We report Llama perplexity results of\nAtom and baselines in Table 3. As the table shows, though\nrecent methods such as OmniQuant and QLLM reduce the\nperplexity of W4A4 to around 10, Atom further reduces the\nperplexity and achieves a remarkable less than 1 perplexity\nincrease on all three datasets. For W3A3, Atom still strongly\nmaintains the perplexity, with only an average 2.5 perplexity\nincrease for Llama-65B. At the same time, previous works\nSmoothQuant and OmniQuant do not achieve satisfactory\nperplexity results (perplexity > 1000). We also notice that\nAtom achieves lower perplexity increases for larger models.\nLlama-2 Perplexity. In addition to the Llama results, we\nevaluate Atom on Llama-2 and present perplexity results.\nAs Table 1 illustrates, similar to Llama, Atom only increases\nthe perplexity by less than 1 in W4A4, while the baselines\nhave worse perplexity (i.e., greater than 10). For the baseline\nSmoothQuant at W3A3, we cannot get a real number even\nafter a grid search on alpha. However, Atom still maintains\na less than 10 perplexity increase for both models at W3A3.\nPreprint\nTable 2. Zero-shot accuracy of quantized Llama models on six common sense tasks.\nZero-shot Accuracy \u2191\nSize\n#Bits\nMethod\nPIQA\nARC-e\nARC-c\nBoolQ\nHellaSwag\nWinogrande\nAvg.\nFP16\n-\n77.37\n52.53\n41.38\n73.12\n72.99\n66.85\n64.04\nSmoothQuant\n63.11\n40.03\n31.57\n58.47\n43.38\n52.80\n48.23\nOmniQuant\n66.15\n45.20\n31.14\n63.51\n56.44\n53.43\n52.65\nQLLM\n68.77\n45.20\n31.14\n-\n57.43\n56.67\n51.84\nW4A4\nAtom\n76.28\n52.10\n38.99\n69.79\n69.81\n63.69\n61.78\nSmoothQuant\n48.69\n25.97\n28.16\n45.26\n26.02\n49.57\n37.28\nOmniQuant\n49.78\n27.19\n27.22\n37.86\n25.64\n49.96\n36.28\n7B\nW3A3\nAtom\n65.56\n41.41\n30.72\n61.77\n53.19\n55.56\n51.37\nFP16\n-\n79.05\n59.85\n44.62\n68.53\n76.22\n70.09\n66.39\nSmoothQuant\n64.47\n41.75\n30.89\n62.29\n46.68\n51.70\n49.63\nOmniQuant\n69.69\n47.39\n33.10\n62.84\n58.96\n55.80\n54.63\nQLLM\n71.38\n47.60\n34.30\n-\n63.70\n59.43\n55.28\nW4A4\nAtom\n77.69\n57.58\n42.92\n67.46\n73.77\n68.51\n64.66\nSmoothQuant\n47.99\n26.30\n27.65\n46.91\n25.65\n49.64\n37.36\nOmniQuant\n50.22\n26.77\n27.82\n37.83\n25.77\n51.07\n36.58\n13B\nW3A3\nAtom\n70.08\n47.94\n33.70\n63.46\n62.93\n56.75\n55.81\nFP16\n-\n80.20\n58.92\n45.31\n68.38\n79.23\n72.69\n67.46\nSmoothQuant\n59.30\n36.74\n28.58\n59.97\n34.84\n49.96\n44.90\nOmniQuant\n71.21\n49.45\n34.47\n65.33\n64.65\n59.19\n57.38\nQLLM\n73.83\n50.67\n38.40\n-\n67.91\n58.56\n57.87\nW4A4\nAtom\n78.73\n58.92\n45.82\n68.47\n77.40\n73.09\n67.07\nSmoothQuant\n49.46\n27.53\n28.16\n39.42\n26.05\n51.38\n37.00\n30B\nW3A3\nAtom\n72.47\n49.54\n37.80\n65.75\n66.99\n60.14\n58.78\nFP16\n-\n80.79\n58.71\n46.33\n82.26\n80.71\n77.03\n70.97\nSmoothQuant\n60.72\n38.80\n30.29\n57.61\n36.81\n53.43\n46.28\nOmniQuant\n71.81\n48.02\n35.92\n73.27\n66.81\n59.51\n59.22\nQLLM\n73.56\n52.06\n39.68\n-\n70.94\n62.90\n59.83\nW4A4\nAtom\n80.41\n58.12\n45.22\n82.02\n79.10\n72.53\n69.57\nSmoothQuant\n49.56\n26.64\n29.10\n42.97\n26.05\n51.14\n37.58\n65B\nW3A3\nAtom\n75.84\n51.43\n41.30\n74.07\n72.22\n64.33\n63.20\nTable 3. Perplexity of quantized Llama models on WikiText2, PTB and C4 dataset.\nPerplexity \u2193\nSize\nBits\nMethod\nWikiText2\nPTB\nC4\nFP16\n-\n5.68\n8.80\n7.08\nSmoothQuant\n22.62\n40.69\n31.21\nOmniQuant\n11.59\n20.65\n14.96\nQLLM\n9.65\n-\n12.29\nW4A4\nAtom\n6.16\n9.62\n7.70\nSmoothQuant\n2.7e4\n3.5e4\n2.6e4\nOmniQuant\n3.4e3\n7.5e3\n6.3e3\n7B\nW3A3\nAtom\n11.77\n20.84\n15.43\nFP16\n-\n4.10\n7.30\n5.98\nSmoothQuant\n109.85\n142.34\n87.06\nOmniQuant\n10.34\n14.91\n12.49\nQLLM\n8.37\n-\n11.51\nW4A4\nAtom\n4.54\n7.69\n6.35\nSmoothQuant\n1.5e4\n1.6e4\n1.5e4\n30B\nW3A3\nAtom\n6.94\n12.12\n9.14\nPerplexity \u2193\nSize\nBits\nMethod\nWikiText2\nPTB\nC4\nFP16\n-\n5.09\n8.07\n6.61\nSmoothQuant\n33.98\n73.83\n41.53\nOmniQuant\n10.90\n18.03\n13.78\nQLLM\n8.41\n-\n10.58\nW4A4\nAtom\n5.46\n8.60\n7.03\nSmoothQuant\n1.3e4\n1.6e4\n1.5e4\nOmniQuant\n7.2e3\n1.6e4\n1.3e4\n13B\nW3A3\nAtom\n8.40\n15.84\n10.81\nFP16\n-\n3.53\n6.91\n5.62\nSmoothQuant\n88.89\n278.76\n283.80\nOmniQuant\n9.18\n16.18\n11.31\nQLLM\n6.87\n-\n8.98\nW4A4\nAtom\n3.89\n7.22\n5.92\nSmoothQuant\n6.6e8\n3.7e8\n4.4e8\n65B\nW3A3\nAtom\n5.89\n9.71\n7.94\nPreprint\n24\n26\n28\n210\nBatch size(#)\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nTops improvement\nFP16 Baseline\nW4A16\nW8A8\nW4A4(Atom)\n(a) Dense layer\n23\n24\n25\n26\n27\nBatch Size(#)\n0\n500\n1000\n1500\n2000\n2500\nLatency(us)\nFP16\nW8A8\nW4A4(Atom)\n(b) Self-attention\nFigure 10. Performance evaluation of different quantization ap-\nproaches on dense layer and self-attention. We evaluate the perfor-\nmance under the Llama-7b config and 1024 sequence length.\n5.3\nEfficiency evaluation\nTo demonstrate the throughput of Atom, we conduct experi-\nments measuring both per-kernel and end-to-end throughput.\nSince the highly efficient INT4 calculation is supported by\nNVIDIA GPUs, we evaluate Atom with W4A4 quantization\non 24GB RTX 4090 with CUDA 12.1.\n5.3.1\nKernel evaluation\nMatrix multiplication. We evaluate the 4-bit fused GEMM\nimplementation used by Atom in Figure 10(a). We imple-\nmented fused GEMM for 8-bit weight-activation quantiza-\ntion (W8A8) and 4-bit weight-only quantization (W4A16)\nfollowing the existing work (Xiao et al., 2023; Lin et al.,\n2023) as baselines. For smaller batch sizes, GEMM is\nmemory-bound; thus, weight-only quantization performs\neffectively. However, as the batch size increases, the effi-\nciency of weight-only quantization diminishes due to ex-\npensive FP16 calculations and dequantization overhead. At\nthe same time, 4-bit Atom outperforms all other approaches\ndue to its hardware efficiency. At batch size 512, Atom\u2019s\nmatrix-multiplication achieves 3.42\u00d7 and 1.92\u00d7 speedup\nover FP16 and INT8 kernels.\nSelf-attention. For self-attention, we fuse different quan-\ntization approaches into FlashInfer (Ye, 2023), which is\noptimized FlashAttention (Dao, 2023) for serving. We also\nintegrate PageAttention (Kwon et al., 2023) to increase the\nmemory utilization. We evaluate our implementation and\nshow the results in Figure 10(b). The decrease in bits lin-\nearly increases the throughput due to the proportionally\nreduced memory usage of the KV-cache. At batch size 128,\nwe achieve a 1.79\u00d7 speedup over INT8 quantization and\n3.52\u00d7 over the FP16 baseline.\n5.3.2\nEnd-to-end evaluation\nServing setup.\nWe integrate Atom into an end-to-end\nevaluation serving framework (Chen, 2023b) to evaluate\nthe throughput in a production scenario.\nWe also inte-\ngrate W8A8 and W4A16 quantization following previous\nworks (Xiao et al., 2023; Lin et al., 2023) as baselines. To\nget a representative workload, we use ShareGPT (Hugging-\nFace, 2023) to collect the distribution of prefill and decode\nrequest length. We analyze the length of every prompt and\nthe corresponding response. We treat multi-round conver-\nsations as requests from multiple users. Specifically, we\nconcatenate all previous prompts and responses and use\nthem as the prompt for the new user request. We vary the\nbatch size from 8 to 256. For each batch size, we produce\na request volume that is 20\u00d7 greater than the batch size\nitself. All requests are served in a First-Come-First-Serve\nmanner. When a request is finished, we replace it with a new\nrequest following the mechanism introduced in Orca (Yu\net al., 2022). Due to GPU memory limits, we only show\nthe exact results on small batch sizes. When the memory\nrequirement cannot be satisfied, we also simulate the perfor-\nmance by reusing the KV-caches from a subset of requests\nwhile preserving the data access pattern and all calculations.\nEnd-to-end throughput. We show the end-to-end through-\nput in Figure 11(a). Solid lines represent exact evaluation\nresults, while dashed lines represent estimations due to\nthe lack of memory. Atom significantly outperforms any\nother quantization on all batch sizes. If we fix the avail-\nable memory as in Figure 11(c), Atom can achieve larger\nbatch sizes so that its throughput further surpasses all base-\nlines while meeting the latency target. Atom can achieve\n7.73\u00d7 throughput compared to the FP16 baseline and 2.53\u00d7\nthroughput compared to INT8 quantization using the same\namount of memory. In contrast, weight-only quantization be-\nhaves poorly due to the additional dequantization overhead\nin dense layers under large batch sizes and large memory\nmovement of the KV-cache in the self-attention layer.\nEnd-to-end latency. We also evaluate the latency under\ndifferent batch sizes. Atom significantly outperforms other\nquantization methods on every batch size. When we achieve\nthe highest practical performance at batch size 64, our la-\ntency is lower than INT8 or FP16 implementations under\neven smaller batch sizes. Notably, even at batch size 256,\nour latency is still lower than 100 ms, which has been shown\nto be the effective reading speed of human eyes by a prior\nstudy (Trauzettel-Klosinski et al., 2012).\n5.4\nAblation study of quantization techniques\nIn this section, we examine the accuracy gain or loss of\ndifferent quantization techniques used in Atom. We first\nuse RTN and adopt per-channel quantization for weights\nand per-token quantization for activations, which is the stan-\ndard quantization recipe (Nagel et al., 2021), to quantize\nthe model to W4A4. We then apply other quantization\ntechniques used in Atom, i.e., mixed-precision, quantizing\nPreprint\n23\n24\n25\n26\n27\n28\nBatch size\n1\n2\n3\n4\nThroughput (kilo tokens/s)\nFP16\nW4A16\nW8A8\nW4A4(Atom)\n(a) End-to-end throughput\n23\n24\n25\n26\n27\n28\nBatch size\n20\n40\n60\n80\n100\n120\n140\n160\nLatency (ms)\nTolerable Latency\n(b) Latency per token\nW16A16\nFP16\nW4A16\nAWQ\nW8A8\nSmoothQ\nW4A4\nAtom\n0\n0.5\n1\n1.5\n2.0\n2.5\n3.0\nThroughput (k tokens/sec) \n7.7x\n5.5x\n2.5x\nThroughput\nLatency\n5\n15\n25\n35\n45\n55\n65\n75\nLatency(ms) \n(c) Throughput and Accuracy comparison\ngiven fixed GPU memory\nFigure 11. End-to-end evaluation of Atom. Atom surpass all other quantization methods for both throughput and latency. Solid lines are\nreal measurements, dashed lines are estimations due to the limited GPU memory.\nTable 4. Ablation study on different quantization techniques used\nin Atom. The model used in this table is Llama-7B.\nQuantization method\nWikiText2 PPL\u2193\nFP16 baseline\n5.68\nW4A4 RTN\n2382.73\n+ Keeping 128 outliers in FP16\n11.37 (2371.4\u2193)\n+ Quantizing outliers to INT8\n11.39 (0.02\u2191)\n+ Group size 128\n6.22 (5.17\u2193)\n+ Clipping\n6.13 (0.09\u2193)\n+ GPTQ\n6.04 (0.09\u2193)\n+ Quantizing KV-cache to INT4\n6.16 (0.12\u2191)\noutliers, group quantization, clipping, GPTQ, and quantiz-\ning KV-cache, and examine the perplexity case by case.\nWe present results in Table 4. We see that keeping outlier\nchannels in FP16 can significantly reduce the perplexity\nfor over 2382. Further quantizing outliers into INT8 only\nresults in a very minor 0.02 perplexity increase. These in-\ndicate our mixed-precision approach effectively addresses\nthe outlier issue. Besides, fine-grained group quantization\nbrings another major perplexity reduction for 5.17. Further-\nmore, using clipping and GPTQ lowers perplexity by 0.09\neach. After all, quantizing KV-cache results in a small 0.12\nperplexity increase, which echoes our finding in \u00a74.4.\nIn conclusion, mixed-precision and fine-grained group quan-\ntization constitute the primary techniques responsible for\nthe significant accuracy enhancement in Atom, while ad-\nditional optimizations collaboratively contribute to further\nimprovements in Atom\u2019s accuracy.\n6\nRELATED WORK\nLLM serving. Various works have studied ways to improve\nLLM serving throughput. (Pope et al., 2022) investigated\nthe batching effect in scaling up LLM. Orca (Yu et al., 2022)\nproposed a distributed LLM serving system that uses a fine-\ngrained scheduling policy to improve GPU utilization under\nvarious request lengths. vLLM (Kwon et al., 2023) used\npage tables to manage GPU memory to increase memory\nutilization, which significantly boosts inference throughput.\nFlexGen (Sheng et al., 2023) proposed an offload mecha-\nnism to support larger batches to achieve high throughput.\nHowever, unlike prior work, in this paper, we delve deep\ninto the intersection between quantization and LLM serving.\nWeight-only quantization. For large LLMs, weight matri-\nces lead to large memory movement, limiting decode effi-\nciency. Weight-only quantization uses low-bit precision to\napproximate weight matrices. For instance, GPTQ (Frantar\net al., 2023) used 4-bit integers to quantize the weight based\non the approximate second-order information. AWQ (Lin\net al., 2023) further advanced accuracy by preserving salient\nweights. SqueezeLLM (Kim et al., 2023) handled outliers\nthrough non-uniform quantization and used a sparse for-\nmat to keep outliers and sensitive weights at high precision.\nQuiP (Chee et al., 2023) successfully represented weights us-\ning 2-bit integers by the adaptive rounding method. Nonethe-\nless, in the LLM serving, the overhead of loading the\nweight matrix is amortized due to batching. Thus, the dense\nlayer becomes compute-bound, while weight-only quantiza-\ntion fails to use efficient low-bit hardware to deliver ideal\nthroughput.\nWeight-activation quantization. Weight-activation quan-\ntization quantizes both the weight and activation matrices,\nwhich is considered more challenging due to the outlier\nchannels of the activation matrix. LLM.INT8 (Dettmers\net al., 2022) used mixed-precision to preserve outlier values\nin activation matrices. (Xiao et al., 2023; Shao et al., 2023;\nYao et al., 2022; Wei et al., 2023) used mathematical equiv-\nalent transformations to manage activation outliers. Other\nPreprint\nworks (Liu et al., 2023a; Wu et al., 2023) used the low-rank\nmatrices to compensate for quantization error. Olive (Guo\net al., 2023) used algorithm and architecture co-design to\naccommodate outliers. RPTQ (Yuan et al., 2023) rearranges\nthe channels to reduce the variance in one quantization clus-\nter, further enhancing the accuracy. However, these works\nall have low accuracy at low-bit precision. A concurrent\nwork, LLM-FP4 (Liu et al., 2023b), achieves comparable\nbut slightly lower zero-shot accuracy to ours at the 4-bit\nlevel. Moreover, this method requires a FP4 format, which\ndoes not have any hardware support at the moment. In\nthis work, Atom achieves noteworthy accuracy with a low-\nbit representation as well as high throughput by utilizing\nexisting efficient hardware units.\n7\nCONCLUSION\nWe presented Atom, a low-bit quantization method that\nleverages the underlying hardware efficiently to achieve both\nhigh accuracy and high throughput for LLM serving. We use\nmixed-precision quantization with reordering, fine-grained\ngroup quantization, dynamic quantization and KV-cache\nquantization to preserve accuracy while fully exploiting\nemerging low-bit hardware support. We integrate Atom into\nan end-to-end serving framework, achieving up to 7.73\u00d7\nthroughput enhancement compared to the FP16 baseline as\nwell as maintaining less than 1.6% zero-shot accuracy loss.\nREFERENCES\nAbdelkhalik, H., Arafa, Y., Santhi, N., and Badawy, A.-\nH. Demystifying the nvidia ampere architecture through\nmicrobenchmarking and instruction-level analysis, 2022.\nBisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.\nPiqa: Reasoning about physical commonsense in natural\nlanguage, 2019.\nChee, J., Cai, Y., Kuleshov, V., and Sa, C. D. Quip: 2-bit\nquantization of large language models with guarantees,\n2023.\nChen, L.\nDissecting batching effects in gpt inference,\nMay 2023a. URL https://le.qun.ch/en/blog/\n2023/05/13/transformer-batching/.\nChen, L. Potentials of multitenancy fine-tuned llm serving,\nSep 2023b. URL https://le.qun.ch/en/blog/\n2023/09/11/multi-lora-potentials/.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\nM., and Toutanova, K. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions, 2019.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge,\n2018.\nDao, T. Flashattention-2: Faster attention with better paral-\nlelism and work partitioning, 2023.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nLlm.int8(): 8-bit matrix multiplication for transformers\nat scale, 2022.\nDettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,\nD., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T.,\nand Alistarh, D. Spqr: A sparse-quantized representation\nfor near-lossless llm weight compression, 2023.\nDuarte,\nF.\nNumber of chatgpt users,\nJul 2023.\nURL https://explodingtopics.com/blog/\nchatgpt-users.\nElimian,\nG.\nChatgpt costs 700,000 to run daily,\nopenai\nmay\ngo\nbankrupt\nin\n2024,\nAug\n2023.\nURL https://technext24.com/2023/08/14/\nchatgpt-costs-700000-daily-openai.\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\nAccurate post-training quantization for generative pre-\ntrained transformers, 2023.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\nWang, K., and Zou, A. A framework for few-shot lan-\nguage model evaluation, September 2021. URL https:\n//doi.org/10.5281/zenodo.5371628.\nGuo, C., Tang, J., Hu, W., Leng, J., Zhang, C., Yang, F., Liu,\nY., Guo, M., and Zhu, Y. OliVe: Accelerating large lan-\nguage models via hardware-friendly outlier-victim pair\nquantization. In Proceedings of the 50th Annual Interna-\ntional Symposium on Computer Architecture. ACM, jun\n2023. doi: 10.1145/3579371.3589038. URL https:\n//doi.org/10.1145%2F3579371.3589038.\nHuggingFace.\nSharegpt\nvicuna\nunfiltered,\nMay\n2023.\nURL\nhttps://huggingface.co/\ndatasets/anon8231489123/ShareGPT_\nVicuna_unfiltered.\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,\nA., Adam, H., and Kalenichenko, D.\nQuantization\nand training of neural networks for efficient integer-\narithmetic-only inference, 2017.\nKim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen,\nS., Mahoney, M. W., and Keutzer, K. Squeezellm: Dense-\nand-sparse quantization, 2023.\nPreprint\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention, 2023.\nLee, C., Jin, J., Kim, T., Kim, H., and Park, E.\nOwq:\nLessons learned from activation outliers for weight quan-\ntization in large language models. ArXiv, abs/2306.02272,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:259076427.\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C., and\nHan, S. Awq: Activation-aware weight quantization for\nllm compression and acceleration, 2023.\nLiu, J., Gong, R., Wei, X., Dong, Z., Cai, J., and Zhuang, B.\nQllm: Accurate and efficient low-bitwidth quantization\nfor large language models, 2023a.\nLiu, S., Liu, Z., Huang, X., Dong, P., and Cheng, K. Llm-\nfp4: 4-bit floating-point quantized transformers, 2023b.\nMarcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R.,\nBies, A., Ferguson, M., Katz, K., and Schasberger, B. The\npenn treebank: Annotating predicate argument structure.\nIn Proceedings of the Workshop on Human Language\nTechnology, HLT \u201994, pp. 114\u2013119, USA, 1994. Associa-\ntion for Computational Linguistics. ISBN 1558603573.\ndoi: 10.3115/1075812.1075835. URL https://doi.\norg/10.3115/1075812.1075835.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nMicikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey,\nP., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P.,\nKamalu, J., Mellempudi, N., Oberman, S., Shoeybi, M.,\nSiu, M., and Wu, H. Fp8 formats for deep learning, 2022.\nNagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y.,\nvan Baalen, M., and Blankevoort, T. A white paper on\nneural network quantization, 2021.\nNVIDIA. Nvidia a100 specifications, a. URL https:\n//www.nvidia.com/en-us/data-center/\na100/.\nNVIDIA.\nNvidia tensor core, b.\nURL https:\n//www.nvidia.com/en-us/data-center/\ntensor-cores/.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J.,\nBradbury, J., Levskaya, A., Heek, J., Xiao, K.,\nAgrawal,\nS.,\nand\nDean,\nJ.\nEfficiently\nscal-\ning transformer inference.\nArXiv, abs/2211.05102,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:253420623.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN\n1532-4435.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale, 2019.\nShao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,\nZhang, K., Gao, P., Qiao, Y., and Luo, P. Omniquant:\nOmnidirectionally calibrated quantization for large lan-\nguage models, 2023.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin,\nM.,\nFu,\nD.\nY.,\nXie,\nZ.,\nChen,\nB.,\nBarrett,\nC. W., Gonzalez, J., Liang, P., R\u00b4e, C., Stoica, I.,\nand Zhang, C.\nHigh-throughput generative infer-\nence of large language models with a single gpu.\nIn International Conference on Machine Learning,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:257495837.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels, 2023.\nTrauzettel-Klosinski,\nS.,\nDietz,\nK.,\nand the IReST\nStudy Group. Standardized Assessment of Reading Per-\nformance: The New International Reading Speed Texts\nIReST. Investigative Ophthalmology & Visual Science,\n53(9):5452\u20135461, 08 2012.\nISSN 1552-5783.\ndoi:\n10.1167/iovs.11-8284. URL https://doi.org/10.\n1167/iovs.11-8284.\nWei, X., Zhang, Y., Li, Y., Zhang, X., Gong, R., Guo, J., and\nLiu, X. Outlier suppression+: Accurate quantization of\nlarge language models by equivalent and optimal shifting\nand scaling, 2023.\nWikipedia contributors.\nList of qualcomm snapdragon\nsystems on chips \u2014 Wikipedia, the free encyclopedia,\n2023.\nURL\nhttps://en.wikipedia.org/\nw/index.php?title=List_of_Qualcomm_\nSnapdragon_systems_on_chips&oldid=\n1182026635. [Online; accessed 26-October-2023].\nWilliams, S., Waterman, A., and Patterson, D. Roofline:\nan insightful visual performance model for multicore\narchitectures. Communications of the ACM, 52(4):65\u201376,\n2009.\nWu, X., Yao, Z., and He, Y. Zeroquant-fp: A leap forward in\nllms post-training w4a8 quantization using floating-point\nformats, 2023.\nPreprint\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han,\nS. Smoothquant: Accurate and efficient post-training\nquantization for large language models, 2023.\nYao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and\nHe, Y. Zeroquant: Efficient and affordable post-training\nquantization for large-scale transformers, 2022.\nYe, Z.\nFlashInfer:\nKernel Library for LLM Serv-\ning.\nhttps://github.com/flashinfer-ai/\nflashinfer, 2023.\nYu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.-\nG. Orca: A distributed serving system for Transformer-\nBased generative models.\nIn 16th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion (OSDI 22), pp. 521\u2013538, Carlsbad, CA, July\n2022. USENIX Association. ISBN 978-1-939133-28-1.\nURL https://www.usenix.org/conference/\nosdi22/presentation/yu.\nYuan, Z., Niu, L., Liu, J., Liu, W., Wang, X., Shang, Y., Sun,\nG., Wu, Q., Wu, J., and Wu, B. Rptq: Reorder-based post-\ntraining quantization for large language models, 2023.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\nHellaswag: Can a machine really finish your sentence?,\n2019.\n"
  },
  {
    "title": "Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
    "link": "https://arxiv.org/pdf/2310.18628.pdf",
    "upvote": "6",
    "text": "Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive\nLearning for Code Generation\nHailin Chen\u2217\u2663\u2660, Amrita Saha\u2217\u2660, Steven HOI\u2660, Shafiq Joty\u2663\u2660\n\u2663 Nanyang Technological University, Singapore\n\u2660 Salesforce Research\n{hailin001, srjoty}@ntu.edu.sg\n{amrita.saha, shoi}@salesforce.com\nAbstract\nWith the rise of powerful closed-sourced LLMs\n(ChatGPT, GPT-4), there are increasing inter-\nests in distilling the capabilies of close-sourced\nLLMs to smaller open-sourced LLMs. Previ-\nous distillation methods usually prompt Chat-\nGPT to generate a set of instructions and an-\nswers, for the student model to learn. How-\never, such standard distillation approach ne-\nglects the merits and conditions of the student\nmodel. Inspired by modern teaching principles,\nwe design a personalised distillation process,\nin which the student attempts to solve a task\nfirst, then the teacher provides an adaptive re-\nfinement for the student to improve. Instead of\nfeeding the student with teacher\u2019s prior, person-\nalised distillation enables personalised learning\nfor the student model, as it only learns on exam-\nples it makes mistakes upon and learns to im-\nprove its own solution. On code generation, per-\nsonalised distillation consistently outperforms\nstandard distillation with only one third of the\ndata. With only 2.5-3K personalised examples\nthat incur a data-collection cost of 4-6$, we\nboost CodeGen-mono-16B by 7% to achieve\n36.4% pass@1 and StarCoder by 12.2% to\nachieve 45.8% pass@1 on HumanEval.1\n1\nIntroduction\nRecently, powerful close-sourced large langauge\nmodels (LLMs) including ChatGPT, GPT-4 have\nbecome predominant, accumulating over 170 mil-\nlion users within 5 month of its launch. Such close-\nsourced LLMs demonstrate strong performance in\na wide range of tasks, from improving writing pro-\nficiency to code generation. However, due to their\nclosed-source nature, concerns have been raised\nregarding factors such as the availability of these\nservices, high associated costs, concerns on ethics\nand safety, and potential data privacy implications,\n*These authors contributed equally to this work\n1Our codes will be available at https://github.\ncom/salesforce/PersDistill\nall of which limit their seamless integration into\nreal-world applications. In light of these concerns,\na natural question arises: Can we distill the remark-\nable abilities exhibited by closed-source LLMs into\nsmaller open-source LLMs?\nResearchers have explored such distillation idea\n(Taori et al., 2023; Wang et al., 2022; Xu et al.,\n2023b), by querying ChatGPT to generate task\ninstruction and solution pairs, and using the col-\nlected data to finetune a student model. However,\nthis standard distillation approach fits different stu-\ndent models to the same data distribution (teacher\u2019s\nprior), disregarding their unique abilities and ca-\npacity. In education domain, personalised learn-\ning which provides customized learning experience\nthat adapts to student\u2019s learning progress and capac-\nity, has proven highly effective and widely adopted\n(Roberts-Mahoney et al., 2016; Shemshack and\nSpector, 2020). Inspired by such finding, we hy-\npothesize that personalised learning is also benefi-\ncial for model distillation.\nIn this work, we propose personalised distilla-\ntion and empirically evaluate its effectiveness in\nthe domain of code generation. Similar to standard\ndistillation, we first employ ChatGPT to generate\ntask instructions accompanied by unit test cases.\nThen we follow three steps for personalized distil-\nlation as shown in Figure 1. First, we let the student\nmodel attempt to solve the task. Then, we evaluate\nthe student\u2019s attempt with unit test cases and get\nexecution feedback. If the execution feedback con-\ntains errors, in the final step we prompt the teacher\nmodel (ChatGPT) to refine the student\u2019s attempt.\nSuch data collection process makes the learning\nexperience both interactive \u2014 as the student partic-\nipates to make attempts, and personalised \u2014 both\nthe input (tasks) and output (refinement data) are\ncustomised to the student. Essentially, personalised\nlabeled data help the student to refine its own policy,\nrather than adopting a new prior of the teacher.\nWith the personalized code data as target out-\narXiv:2310.18628v2  [cs.CL]  26 Jan 2024\nFigure 1: Overview of our framework. Left: standard distillation.\n1 Teacher generates standard answer to a\ngiven problem for the student to learn Right: personalised distillation.\n1 Student first generates its own attempt to\nsolve the task.\n2 Executor evaluates generated code with unit test cases.\n3 Teacher provides adaptive refinement\ngiven student\u2019s attempt and its execution feedback.\nput, we construct three variants of finetuning data\n(i) PERsD data which formats it as a typical text-\nto-code generation task, (ii) PERsD-refine which\ntreats it as a code-refinement task, given a task in-\nstruction, incorrect code and execution error feed-\nback (ii) PERsD-combine which simply combines\nPERsD and PERsD-refine finetuning data, i.e. code\ngeneration and refinement tasks.\nWe collect 10K standard distillation examples\nand around 2.5-3K personalised examples for pre-\ntraining.\nThrough zero-shot evaluation on Hu-\nmanEval (Chen et al., 2021) and MBPP (Austin\net al., 2021), we observe that all PERsD variants\nconsistently outperform their counterparts which\nuse standard distillation. This compelling result\nstrongly validates our hypothesis regarding the\nadvantages of personalized distillation. Ablation\nstudies further reinforce our hypothesis, uncover-\ning intriguing properties such as the benefits of\nmulti-round personalized distillation and the ability\nof our models to leverage execution feedback for\nself-correction. Notably, personalised distillation\nboosts the state-of-the-art open-sourced pretrain\nmodel StarCoder (Li et al., 2023a) significantly \u2014\nby 12.2% to achieve 45.8 in pass@1 and 82.3 in\npass@100 on HumanEval.\n2\nRelated Work\n2.1\nDistillation from ChatGPT\nPrevious works have explored distillation from\nChatGPT including Alpaca(Taori et al., 2023), Vi-\ncuna(Chiang et al., 2023) and Baize(Xu et al.,\n2023b). However, these works can all be consid-\nered as standard distillation as they do not consider\nthe conditions and capacity of student model. Wiz-\nMethods\nPersonalised\nInteractive\nCode-related\nAlpaca\n\u2717\n\u2717\n\u2717\nVicuna\n\u2717\n\u2717\n\u2717\nBaize\n\u2717\n\u2717\n\u2717\nWizardLM\n\u2717\n\u2717\n\u2717\nWizardCoder\n\u2717\n\u2717\n\u2713\nLion\nInput\n\u2713\n\u2717\nPERsD\nInput + Output\n\u2713\n\u2713\nTable 1: Related work on distillation from ChatGPT\nardLM(Xu et al., 2023a) and WizardCoder(Luo\net al., 2023) iteratively prompts teacher model to\ngenerate more complex instructions.\nTheir ap-\nproach can be seen as an orthogonal advancement\nthat can potentially be combined with personalised\ndistillation.\nLion (Jiang et al., 2023) proposes to incorpo-\nrate student model\u2019s answer and sample more hard\ntasks for which the student failed to solve. Thus,\nLion can be considered as input personalised dis-\ntillation as only the input tasks are customised for\ndifferent student. Our approach differs as we pro-\nvide customization both on input and output, and\nwe empirically show that personalising labels is\ncritically beneficial.\n2.2\nCode Generation with Feedback\nRecently, there has been an increasing amount\nof research on exploring on how to use feed-\nback for an iterative and improved code genera-\ntion through code-refinement. Self-refine(Madaan\net al., 2023), Self-debug(Chen et al., 2023b) and\nReflexion (Shinn et al., 2023) are inference-time\nmethods which use powerful close-sourced LLMs\nto generate better code from internal or external\nfeedback. Although they show high performance,\nMethods\nTraining\nInference\nSingle\nData Source\nPersonalised\nw/ execution\nw/o\nModel\nfeedback\nChatGPT\nSelf-refine\n\u2713\nNo Training\n\u2717\n\u2717\n\u2717\nSelf-debug\n\u2713\nNo Training\n\u2717\n\u2713\n\u2717\nReflexion\n\u2713\nNo Training\n\u2717\n\u2713\n\u2717\nSelf-edit\n\u2717\nStandard GT\n\u2717\n\u2713\n\u2713\nSelf-correct\n\u2717\nSelf-exploration\n\u2713\n\u2713\n\u2713\nILF\n\u2717\nHuman labeled\n\u2713\n\u2713\n\u2713\nPERsD-refine\n\u2713\nChatGPT\n\u2713\n\u2713\n\u2713\nTable 2: Related work on Code Generation w/ feedback\nthese methods are limited as they require access to\nclose-sourced LLMs.\nSelf-edit (Zhang et al., 2023) trains a separate code\neditor to rectify generated code from a base LLM.\nThe training label is from original gold answer,\nthus not label-personalised. Similarly, Self-correct\n(Welleck et al., 2022) trains a separate corrector\nmodel to rectify the output from a fixed genera-\ntor model. However, the training label is from\nself-exploration of the corrector model: sampling\nmultiple refinements and choosing the one leading\nto higher reward. Finally, ILF (Chen et al., 2023a)\ncollects human-annotated code refinement data to\ntrain a separate refinement model on it. Fhe refine-\nment model is used to generate text-to-code data\nfor finetuning the code-generation LLM. Unlike\nILF, our approach is more scalable as we do not re-\nquire human annotation and our personalized data\nproves significantly more effective than ILF as we\nempirically investigate in \u00a75.\n2.3\nReinforcement Learning from (Human)\nFeedback\nAfter the launch of ChatGPT, aligning LLMs to\nhuman preference has drawn tremendous attention\nto research communities. As one of the most influ-\nential approaches in this direction, reinforcement\nlearning from human feedback (RLHF) (Ouyang\net al., 2022; Li et al., 2023b), adopts an actor-critic\nframework, where the student model is optimized\nto generate responses to receive higher reward from\nthe critic model. In InstructGPT (Ouyang et al.,\n2022), the critic (reward model) is trained from\nhuman annotation. Direct Preference Optimiza-\ntion (DPO) (Rafailov et al., 2023) drops the need\nof training a reward model, by using a reference\nLLM and offline trajectories to estimate the reward.\nChain-of-Hindsight (Liu et al., 2023) converts hu-\nman preference annotations into simple natural lan-\nguage feedback, and thus turns RL optimization\nto conditional generation. In above methods, the\nassumption is that there are no ground truth targets\nand thus they try to improve the LLM based on\nthe assessment (critic) of multiple generated out-\nputs. However, such RL-style training will be less\neffective and efficient to supervised finetuning, es-\npecially for challenging tasks with sparse rewards \u2013\ne.g. sovling math puzzles or coding tasks. Unlike\nthese methods, our approach can acquire \"ground\ntruth\" outputs from a personalised teacher, thus su-\npervised finetuning can be applied which makes\nthe learning effective and efficient, even for chal-\nlenging tasks like solving coding problems.\n3\nMethod\n3.1\nStandard Distillation\nAssume a dataset of code generation tasks\nD\n=\n{(t, u)} where each problem (or task)\nconsists of a task instruction t and a unit test\ncollection u.\nDuring training, we have access\nto a teacher model \u03c0\u03d5 and a student model \u03c0\u03b8.\nThe objective is to distill how the teacher solves\ncode generation tasks to the student model, in the\ncontext of D. For each task (t, u), we first query\nthe teacher \u03c0\u03d5(t) with the task instruction, to get a\ndirect generated code snippet c\u03d5. Then, we execute\nthe generated code c\u03d5 against unit test cases u\nand get its execution feedback f \u2190 EXEC(c\u03d5, u),\nwhere the EXEC function returns passed if the\ncode passes all the unit tests, otherwise it returns\nan error message from the executor. By filtering\nout the tasks where c\u03d5 do not pass all the unit tests\n(i.e., f \u0338= passed), we get a new clean dataset\nDSTAND = {(t, u, c)}, where each task consists\na task instruction t, a suite of unit tests u and a\ncorrect solution code c.\nWe then finetune the student model \u03c0\u03b8 on\n{(u, c)} \u223c DSTAND, where the input is the task\ninstruction u and the output is the corresponding\ncode solution c. We name this approach STAND.\n3.2\nPersonalised Distillation\nThe STAND approach simply samples training\nexamples (instructions and labels) from the\nprior distribution of the teacher model and\nfeeds it to the student without considering the\nconditions of the student model.\nInspired by\nmodern education principles which advocates\ninteractive and personalised learning experience,\nwe propose personalised distillation:\nadapting\nteaching materials to student\u2019s current knowledge\nAlgorithm 1 personalised distillation for code gen-\neration (PERsD-combined).\n1: Input: Dataset DSTAND, student LLM \u03c0\u03b8, unit test ex-\necutor EXEC, refinement template Trefine, teacher LLM\n\u03c0\u03d5\n2: Drefine \u2190 {} \u25b7 refinement data for finetuning\n3: Dcode \u2190 {} \u25b7 direct generation data\n4: for (t, u, c) \u2208 DSTAND do\n5:\nc\u03b8 \u2190 \u03c0\u03b8(t)\n\u25b7 student generates c\u03b8\n6:\nf \u2190 EXEC(c\u03b8, u)\n\u25b7 exec. feedback for c\u03b8\n7:\nif f \u0338= passed then\n8:\n// personalised refinement from teacher\n9:\ncrefine \u2190 \u03c0\u03d5(t, c\u03b8, f)\n10:\n// create refinement task instruction\n11:\ntrefine \u2190 Trefine(t, c\u03b8, f)\n12:\nif EXEC(crefine, u) = passed then\n13:\nDrefine.insert({trefine, crefine})\n14:\nDcode.insert({t, c})\n15:\nend if\n16:\nend if\n17: end for\n18: \u03c0\u03b8\u2217 \u2190 FINETUNE(\u03c0\u03b8, Drefine + Dcode)\nand capacity. We propose three variants:\nPERSD-combined Algorithm 1 shows detailed\nsteps for PERSD-combined. This method takes the\nstandard distillation dataset DSTAND from \u00a73.1 and\nfirst lets the student generate solutions for each task.\nThen it filters out the tasks where the student model\ncan already solve correctly. For the remaining tasks,\nit obtains the teacher\u2019s personalised refinement con-\nditioned on the student\u2019s attempt and its execution\nerror feedback, and only keeps the tasks where the\nteacher\u2019s refinement is valid (i.e., passes all the unit\ntest cases). Figure 1 visualizes these three steps.\nFor this final task-set, we create two datasets:\ni) Dcode containing task instruction as input and\nteacher\u2019s direct answer as output, and ii) Drefine\ncontaining task refinement instruction as input and\npersonalised refinement answer as output. The\ntask refinement instruction (line 9 in Algorithm\n1) is created by concatenating task instruction t,\nstudent\u2019s attempt c\u03b8 and its execution feedback f\nwith a refinement template Trefine (More details in\nAppendix C). Such refinement instruction turns\nstandard code generation into a code refinement\ntask, teaching the student how to refine its own\nsolution.\nPERSD-combined then finetunes the\nstudent model on Drefine combined with Dcode.\nPERSD-refine Similar to PERSD-combined, this\nvariant follows line 1-15 of Algorithm 1 to collect\nrefinement data Drefine. However, it differs from\nthe above model as it only uses Drefine to finetune\nthe student model.\nPERSD This variant takes the training data Drefine\nfrom PERSD-refine and replace the input of each\ndata point from code refinement prompt to original\ntask instruction. It thus trains the student model\nwith personalised labels on code generation.\nTo illustrate the difference between personalised\nrefinement and teacher\u2019s direct solution, we show\na real example in Figure 2. The top shows the per-\nsonalised refinement for the given task, while the\nbottom section shows the direct teacher\u2019s gener-\nation for the same task. Note how the teacher\u2019s\ndirect generation is significantly different from the\nstudent model\u2019s attempt, while the teacher\u2019s refine-\nment follows the student\u2019s attempt and improves\nupon it. We hypothesize that such adaptive refine-\nment where the teacher aligns to student\u2019s genera-\ntion, helps the student to learn more efficiently and\neffectively, similar to how humans benefit from\npersonalised learning.\nFigure 2: Example: (Top) Personalised refinement from\nstudent\u2019s attempt and execution feedback; (Bottom) Di-\nrect solution generated by teacher conditioned on task.\n3.3\nIterative Inference\nLet Dtest\n=\n{(t, u)} denote our test set for\ninference, where each data point (t, u) consists\nof a task instruction t and a suite of hidden\nunit test cases u. We also assume that the task\ninstruction contains some simple unit test cases\nin its doc-string (as often seen in code generation\ninstructions), which we can extract and format\nusing rule-based heuristics to obtain a suite of seen\nunit test cases useen (More details in Appendix A).\nFor single-step inference, we use the standard\napproach to evaluate pass@k. Specifically, for\neach task t, we query the model n times with the\ntask instruction: ci\n\u03b8 \u2190 \u03c0\u03b8(t) for i = 1 . . . n. Then,\nfollowing (Chen et al., 2021), we estimate pass@k\nfrom the number of attempts that passed the hidden\nunit test cases: EXEC(ci\n\u03b8, u) = passed.\nMulti-step inference If the model \u03c0\u03b8 has been\ntrained to rectify, following our approach in\nPERsD-refine or PERsD-combine, and if unit\ntests are available during inference, we can per-\nform 2-step inference:\nfor each generated at-\ntempt ci\n\u03b8 in 1-step, we first get execution feedback\nfi\nseen \u2190 EXEC(ci\n\u03b8, useen).\nIf fi\nseen = passed,\nwe reuse the original attempt as the 2-step at-\ntempt.\nOtherwise, we create a refinement in-\nstruction ti \u2190 Trefine(t, ci\n\u03b8, fi\nseen) following the ap-\nproach in PERsD-refine or PERsD-combined, and\nquery the same model with the refinement instruc-\ntion for 2-step attempt: ci\n\u03b8,2-step \u2190 \u03c0\u03b8(ti). We\nthen compute pass@k over the 2-step generations\nsimilar to 1-step inference.\n4\nExperimental Setup\n4.1\nBaselines\nThe first baseline is STAND, the standard distilla-\ntion approach mentioned in \u00a73.1.\nTo measure the effectiveness of personalised\nlabels quantitatively,\nwe also compare with\nInput-personalised distillation baselines as well,\nwhere only the input tasks are selected in a manner\ncustomized to the student\u2019s abilities. However, the\noutput labels are not personalised, as they are taken\nfrom teacher\u2019s direction generation c instead of\npersonalised refinement crefine. We start with Dcode\nfrom PERSD-combined and have three variants:\nINPD We finetune the student model \u03c0\u03b8 on\n{(t, c)}\n\u223c\nDcode, where the input is a task\ninstruction and the output is a code solution. This\nvariant is more customized than STAND as it filters\nout the tasks which the student can already solve\ncorrectly.\nINPD-refine Similar to PERsD-refine, InpD-refine\ntrains the student model to rectify its wrong\nattempt.\nThe difference is in InpD-refine, the\nrefined code is from teacher\u2019s direct solution c,\ninstead of personalised refinement crefine.\nINPD-combined Similar to PERSD-combined,\nInpD-combined trains the student on rectifying its\nanswers as well as directly solving the task. The\ndifference is that in InpD-combined, the labels\nfor both code refinement and code generation are\ntaken from teacher\u2019s direct solution c.\n4.2\nPretraining Data Construction\nTo construct our pretraining data, we adopted the\ndata collection process in code-alpaca(Chaudhary,\n2023) and used a set of 374 seed tasks from MBPP\n(task-ids 601-974) as in-context prompt to query\nChatGPT for novel code generation tasks. This\nseed-set increases the likelihood of ChatGPT gen-\nerating python codes.\nThrough this process, we obtained a corpus of\n20K code generation tasks from ChatGPT each\ncomprising a task instruction and the corresponding\ngenerated code, which is typically a single python\nfunction. Next we show each generated instance to\nChatGPT again and prompt it to generate 5 unique\ntest-case inputs (i.e. input argument values) for\nthe python function. We then parse and format\nthe generated test-case input and execute the gen-\nerated code on it obtain an output. Thus, out of\n20K, for 14880 instances we could successfully\ngenerate and parse 5 unit test case inputs and for\n10172 instances we were able to successfully exe-\ncute the generated code and obtain outputs on all\n5 inputs. This final corpus of 10K code generation\ntasks, each comprising a task instruction and the\ncorresponding generated code along with 5 unit test\ninput and outputs forms our standard distillation\ndataset DSTAND.\nTo collect personalised distillation data, we fol-\nlow \u00a73.2 to first ask the student model to generate 1\noutput code per task, setting sampling temperature\nto 0.3. We then evaluate the student\u2019s attempt and\nonly keep the tasks with the wrong generations (i.e.\nthe ones which failed any of the unit test-case). We\nuse this to query ChatGPT for personalised refine-\nments and only retain the valid refinements which\npassed all unit tests. Our prompt to ChatGPT con-\ntains the original task instruction and code from\nDSTAND along with the student model\u2019s generated\ncode and execution feedback (compiler errors or\nunit test failures). Our instruction to ChatGPT is to\ngenerate a correct solution that rectifies the errors\nand is closest in semantics to the student\u2019s code\n(More details in Appendix B). Table 3 shows the\nstatistics of personalised data construction process.\nStudent Model\n# Wrong Attempt\n# Validated Per-\nData\nby Student\nsonalised Tasks\nCost\nCodeGen-mono-6B (Nijkamp et al., 2023)\n6.5K\n3.25K\n5.5$\nCodeGen-mono-6B (round2)\n4K\n1.4K\n4.4$\nCodeGen-mono-16B\n6.2K\n2.8K\n6.5$\nStarCoder (Li et al., 2023a)\n4.3K\n2.5K\n4.3$\nTable 3: Statistics of Personalised Data Construction\n4.3\nModel Evaluation\nWe evaluate our models on two datasets: Hu-\nmanEval(Chen et al., 2021), which contains 164\nPython problems, and the subset MBPP(Austin\net al., 2021) sanitized set that has no overlap with\nour MBPP seed tasks for pretraining data collec-\ntion. This corresponds to test+validation+prompt\nsplits of MBPP-sanitized and consists of 306\nPython problems. We use nucleus sampling with\ntemperature 0.2 to generate 20 candidates per\ntask for estimating pass@1, and with tempera-\nture 0.8, 100 candidates per task for estimating\npass@5/10/20/50/100.\nFor multi-step inference, we first extract the\n\u201cseen\u201d unit test-cases from the doc-string of the\ntask instruction (More details in Appendix A).\nNext, we generate output samples in the usual\ncode-generation style forming the set of 1-step gen-\nerations for each instance. Each of these candi-\ndate generations are then executed on the extracted\n\u201cseen\u201d unit test cases to obtain a refined code, thus\nforming the set of 2-step generations.\n4.4\nPretraining Setup\nFor all experiments with CodeGen-mono-6B back-\nbone, we use effective batch size of 1024 and pre-\ntrain for 20 epochs. For backbone as CodeGen-\nmono-16B, we use effective batch size of 1024\nand pretrain for 3 epochs, as the training converges\nmuch faster than CodeGen-mono-6B. For PERsD-\ncombine with StarCoder model, we use effective\nbatch size of 1024 and pretrain for 8 epochs, which\nresults in similar training loss as CodeGen-mono-\n16B. We implement using HuggingFace transform-\ners(Wolf et al., 2020) and DeepSpeed Zero (Ra-\njbhandari et al., 2020). All experiments are con-\nducted on a cluster of 8 A100-40GB GPUs.\n5\nExperimental Results\n5.1\nMain Results\nWe empirically test the hypothesis that personalised\ndistillation helps student model learn more effec-\ntively, by comparing PERsD models with baseline\ndistillation methods (InpD, StanD) in Table 4.\nPersonalised labeled-data is generally bet-\nter than standard data\nComparing PERsD-\ncombine to InpD-combine, we find PERsD-\ncombine outperforms InpD-combine in all settings,\noften with a significant margin (two backbones,\ntwo datasets, two inference steps, 4 pass@k met-\nric). Similar observation holds true when compar-\ning PERsD-refine to InpD-refine (except for 2/32\nsettings), and PERsD to InpD. Thus, we conclude\nthat PERsD-variants are generally significantly bet-\nter than their InpD counterparts, providing strong\nevidence that personalised labels are more effective\nfor the student model to learn than standard labels.\nPERsD outperforms StanD with less than one-\nthird of its data\nWe observe that PERsD out-\nperforms StanD for every pass@k on both 16B\nand 6B CodeGen-mono backbone across both Hu-\nmanEval and MBPP, even though StanD has 10K\ndata and PERsD has only 3.3K and 2.8K exam-\nples for CodeGen-mono-6B and 16B. The only\nexception is in the setting CodeGen-mono-16B,\nMBPP, pass@1, where StanD edges out PERsD\nby 1.2 points. Given that our pretraining data is\nconstructed from seed tasks taken from MBPP, we\nhypothesize that StanD might enjoy an unfair ad-\nvantage due to its having three times more data,\nmaking it more susceptible to data leakage. We\nverify such hypothesis further in \u00a75.2. In summary,\nwith PERsD outperforming StanD in 15 out of 16\nsettings while having less than a third of the data,\nit\u2019s evident that personalized labeled data makes\nthe learning more efficient.\nMulti-step inference consistently improves an-\nswer quality\nFor PERsD-refine and PERsD-\ncombine models, we find that 2 step inference con-\nsistently improves performance on HumanEval and\nMBPP. This shows the models successfully learn\nhow to rectify its solution based on execution er-\nror feedback. Note that InpD-refine yields worse\naccuracy with 2 step inference on HumanEval\npass@10/20, strengthening the advantage of per-\nsonalised labeled data over standard labeled data.\n5.2\nTrain-Test overlap analysis\nAs observed in Table 4, PersD-variants enjoy\nhigher average improvements over their InpD coun-\nterparts, on HumanEvan than on MBPP. To delve\ndeeper, we conduct a data overlap analysis. For\neach test task, we extract the most similar training\ntask and use GPT-3.5-turbo to score their seman-\ntic similarity, with 0 indicating no relation and 1\n(a) Backbone as CodeGen-mono-6B\nMethods\n#Data\nPass@1\nPass@5\nPass@10\nPass@20\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nHumanEval\nStanD\n10K\n32.41\n-\n41.79\n-\n45.67\n-\n49.26\n-\nInpD\n3.3K\n31.65\n-\n44.55\n-\n50.72\n-\n56.76\n-\n-refine\n3.3K\n29.70\n29.70\n43.82\n41.99\n51.28\n47.89\n58.29\n53.51\n-combined\n6.5K\n30.15\n32.30\n42.94\n45.27\n47.91\n50.50\n52.54\n55.46\nPERsD\n3.3K\n34.63\n-\n49.34\n-\n55.34\n-\n60.41\n-\n-refine\n3.3K\n32.35\n33.35\n48.69\n49.35\n56.07\n56.87\n63.60\n64.76\n-combined\n6.5K\n33.81\n35.53\n44.64\n49.67\n49.96\n55.67\n55.23\n61.21\nMBPP\nStanD\n10K\n43.11\n-\n55.24\n-\n59.07\n-\n62.51\n-\nInpD\n3.3K\n43.59\n-\n55.83\n-\n63.13\n-\n67.34\n-\n-refine\n3.3K\n44.44\n47.81\n62.25\n66.43\n67.61\n71.44\n71.68\n75.22\n-combined\n6.5K\n42.69\n47.25\n56.70\n62.17\n61.39\n66.49\n65.46\n70.22\nPERsD\n3.3K\n45.47\n-\n59.90\n-\n64.85\n-\n69.73\n-\n-refine\n3.3K\n48.24\n52.65\n63.65\n68.49\n69.00\n73.34\n73.16\n77.62\n-combined\n6.5K\n42.77\n48.92\n56.91\n62.29\n61.43\n66.89\n65.22\n70.96\n(b) Backbone as CodeGen-mono-16B\nMethods\n#Data\nPass@1\nPass@5\nPass@10\nPass@20\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nHumanEval\nStanD\n10K\n33.96\n-\n50.56\n-\n57.69\n-\n63.82\n-\nInpD\n2.8K\n36.68\n-\n49.51\n-\n53.85\n-\n57.47\n-\n-refine\n2.8K\n30.55\n31.28\n48.40\n48.13\n55.00\n54.52\n61.31\n60.62\n-combined\n5.6K\n34.66\n36.49\n50.65\n53.89\n56.75\n60.07\n62.78\n65.85\nPERsD\n2.8K\n37.74\n-\n56.57\n-\n63.92\n-\n69.97\n-\n-refine\n2.8K\n36.77\n37.99\n51.86\n54.23\n58.07\n60.92\n63.17\n67.13\n-combined\n5.6K\n36.40\n37.74\n53.57\n55.80\n60.81\n63.37\n67.3\n70.50\nMBPP\nStanD\n10K\n48.90\n-\n62.21\n-\n66.91\n-\n71.33\n-\nInpD\n2.8K\n46.27\n-\n58.45\n-\n62.61\n-\n66.43\n-\n-refine\n2.8K\n48.79\n54.87\n66.89\n71.32\n72.24\n75.71\n75.82\n78.84\n-combined\n5.6K\n47.39\n53.59\n59.14\n66.38\n63.48\n70.76\n67.10\n74.35\nPERsD\n2.8K\n47.68\n-\n65.80\n-\n71.56\n-\n76.02\n-\n-refine\n2.8K\n51.50\n56.21\n66.82\n71.86\n72.06\n76.78\n76.03\n80.42\n-combined\n5.6K\n51.44\n56.44\n66.45\n71.31\n71.64\n76.43\n76.04\n80.20\nTable 4: Comparing PERsD models to StanD & InpD\nindicating complete semantic overlap (further de-\ntails in Appendix D). Table 5 reveals more overlap\nin MBPP than HumanEval, and more overlap for\nStanD compared to PERsD. This overlap could be\nwhy StanD surpasses PERsD in the 1/16 setting\n(CodeGen-mono-16B, MBPP, pass@1), as StanD\nhas an unfair advantage of having significantly\nmore data leakage. In addition, if we test our meth-\nods on clean-MBPP where the leaked data points\nare removed, then PERsD becomes almost on-par\nwith StanD in this specific setting while having\nlarger margin over StanD on the rest 15/16 settings\n(from 4.8 points average margin to 5.9 points, more\ndetails at Appendix E). Altogether, this overlap\nanalysis, coupled with results from cleaned MBPP,\nfurther underscores the advantages of personalized\ndistillation.\n5.3\nEffect of mixing StanD and InpD data\nTable 6 shows the ablation study on mixing stan-\ndard distillation data to PERsD-refine and InpD-\nrefine: while mixing standard data to InpD-refine\nimproves its 1-step performance on MBPP and\nMethod\nBackbone\n%(\"leak\")\nSimilarity\nHumanEval\nStanD\n6B,16B\n6.1%\n0.22\nPERsD\n6B\n3.6%\n0.18\nPERsD\n16B\n3.05%\n0.22\nMBPP\nStanD\n6B,16B\n18.24%\n0.40\nPERsD\n6B\n8.47%\n0.30\nPERsD\n16B\n7.49%\n0.30\nTable 5: Train-Test Overlap Analysis. 6B/16B denotes\nCodeGen-mono-{6/16}B backbones. %(\"leak\") denotes\nthe percentage of test data that are semantically leaked\nin training data. \u2019Similarity\u2019 represents the average\nsimilarity score (range: 0 to 1; higher values indicate\ngreater similarity)\nroughly maintains its performance on other set-\ntings, mixing StanD data to PERsD-refine signifi-\ncantly deteriorate its performance (except pass@1\ninf-step=2 on HumanEval). We conjecture that as\nStanD has much larger data volume than PERsD-\nrefine, it overwhelms the student training on stan-\ndard distillation. However, combining with a bal-\nanced input-personalised data can be beneficial, as\nwe observe from the good performance of PERsD-\ncombined in Table 4 on CodeGen-mono-16B.\nMethods\nInf\nPass@1\nPass@5\nPass@10\nPass@50\nPass@100\nStep\nHumanEval\nStanD + InpD-refine\n1\n30.59\n40.04\n44.20\n54.23\n58.54\nStanD + InpD-refine*\n29.45\n39.83\n44.07\n54.55\n59.76\nStanD + PERsD-refine\n32.13\n43.82\n48.66\n59.55\n64.02\nPERsD-refine\n32.35\n48.69\n56.07\n72.10\n77.44\nStanD + InpD-refine\n2\n30.87\n42.88\n47.90\n58.21\n60.98\nStanD + InpD-refine*\n30.12\n42.71\n47.42\n58.69\n64.02\nStanD + PERsD-refine\n35.00\n47.89\n52.96\n64.36\n69.51\nPERsD-refine\n33.35\n49.35\n56.87\n74.13\n79.88\nMBPP\nStanD + InpD-refine\n1\n42.60\n53.18\n56.49\n62.11\n63.07\nStanD + InpD-refine*\n44.08\n54.12\n57.82\n64.96\n66.34\nStanD + PERsD-refine\n45.63\n53.20\n56.38\n63.02\n65.36\nPERsD-refine\n48.24\n63.65\n69.00\n78.16\n81.70\nStanD + InpD-refine\n2\n46.32\n58.84\n62.80\n69.80\n71.23\nStanD + InpD-refine*\n46.92\n58.18\n62.03\n68.82\n68.95\nStanD + PERsD-refine\n48.44\n58.37\n62.47\n70.64\n73.20\nPERsD-refine\n52.65\n68.49\n73.34\n82.72\n85.62\nTable 6: Ablation on mixing StanD, with Backbone as\nCodeGen-mono 6B. InpD-refine* denotes using all 6.5K\ntasks where the student model made mistakes, which\ncovers around 3K more tasks than InpD-refine.\nSimilarly, in Table 7 we show another ablation:\nthat mixing InpD data with PERsD roughly main-\ntains the performance on HumanEval but degrades\non MBPP. This shows personalised labels are of\nhigher quality and mixing non personalised labels\nfor the same task generally hurts performance.\nMethods\nPass@1\nPass@5\nPass@10\nPass@50\nPass@100\nHumanEval\nPERsD\n34.63\n49.34\n55.34\n65.56\n67.93\nPERsD + InpD\n34.88\n48.35\n54.06\n64.88\n68.90\nMBPP\nPERsD\n45.47\n59.90\n64.85\n76.05\n80.07\nPERsD + InpD\n43.84\n59.02\n63.77\n71.69\n74.84\nTable 7:\nAblation on PERsD mixing InpD with\nCodeGen-mono 6B as backbone\nRound\nInf\nPass@1\nPass@5\nPass@10\nPass@50\nPass@100\nStep\nHumanEval\n1\n1\n33.81\n44.64\n49.96\n61.75\n70.73\n2\n32.74\n45.50\n51.52\n66.14\n71.95\n1\n2\n35.53\n49.67\n55.67\n68.16\n77.44\n2\n36.75\n49.71\n56.13\n70.24\n75.00\nMBPP\n1\n1\n42.77\n56.91\n61.43\n68.84\n70.67\n2\n45.07\n57.75\n62.27\n70.49\n72.55\n1\n2\n48.92\n62.29\n66.89\n75.09\n77.25\n2\n49.59\n63.43\n68.30\n76.00\n78.10\nTable 8: Ablation on multi-round distillation on PERsD-\ncombined with CodeGen-mono 6B as backbone\n5.4\nMulti-round Distillation\nAfter finetuning the student model with the per-\nsonalised distillation data, can we perform an-\nother round of personalised distillation, on the new\nmodel? We show such an ablation study in Ta-\nble 8. Encouragingly, we find PERsD-combined\nround-2 generally outperforms PERsD-combined\nround-1 by a modest margin. This improvement\nprovides further evidence of the benefits of per-\nsonalized learning, even when applied to models\ntrained with personalized distillation. These find-\nings suggest the intriguing possibility of an online\nor active version of personalized distillation, where\ndata collection and model training occur simulta-\nneously to ensure each batch is fully personalized\nand has higher sample efficiency. However, we will\nleave such intriguing exploration for future work.\n5.5\nUtilizing feedback for multi-step Inference\nTo better understand the role of execution feed-\nback during training and multi-step inference, we\nshow an ablation study in Table 9, where we\ncompare PERsD-combine with a specific variant\n(PERsD-combine*) that excludes feedback dur-\ning both training and inference. we observed that\nPERsD-combine* performs comparably to PERsD-\ncombine on HumanEval and slightly better on\nMBPP for 1-step inference. However, for 2-step\ninference, PERsD-combine* consistently underper-\nforms PERsD-combine. This result aligns well\nwith our expectations that code-rectification needs\nthe execution feedback to guide the refinement.\nMethods\nInf\nPass@1\nPass@5\nPass@10\nPass@50\nPass@100\nStep\nHumanEval\nPERsD-combine\n1\n33.81\n44.64\n49.96\n61.75\n70.73\nPERsD-combine*\n33.29\n45.47\n50.90\n62.87\n68.29\nPERsD-combine\n2\n35.53\n49.67\n55.67\n68.16\n77.44\nPERsD-combine*\n34.59\n49.54\n55.59\n67.27\n71.95\nMBPP\nPERsD-combine\n1\n42.77\n56.91\n61.43\n68.84\n70.67\nPERsD-combine*\n44.76\n56.95\n60.85\n68.67\n71.57\nPERsD-combine\n2\n48.92\n62.29\n66.89\n75.09\n77.25\nPERsD-combine*\n47.83\n61.28\n65.54\n73.03\n75.49\nTable 9: Ablation on removing execution feedback with\nCodeGen-mono 6B as backbone. PERsD-combine*\ndenotes combined personalised distillation without exe-\ncution feedback in input prompt.\n5.6\nCross-Model Personalised Distillation\nTo investigate whether personalised distillation\ndata of one model can be benefical to another,\nwe conduct an ablation in Table 10 by using\nPERsD-combined data of CodeGen-mono-6B to\ntrain CodeGen-mono-16B. The results show that\nsuch cross-model persionalised data do not per-\nform as well as real personalised data: leading to\na consistent performance drop by a large margin.\nThis finding reinforces our notion that learning data\nshould be tailored to the specific student model, as\npersonalized data suitable for one model may not\nnecessarily benefit others.\nModel\nInf\nPass@1\nPass@5\nPass@10\nPass@50\nPass@100\nStep\nHumanEval\nCodeGen-mono-6B\n1\n33.81\n44.64\n49.96\n61.75\n70.73\nCodeGen-mono-16B*\n32.99\n47.81\n54.58\n69.31\n73.98\nCodeGen-mono-16B\n36.40\n53.57\n60.81\n74.64\n79.88\nCodeGen-mono-6B\n2\n35.53\n49.67\n55.67\n68.16\n77.44\nCodeGen-mono-16B*\n35.85\n51.31\n58.23\n74.02\n76.60\nCodeGen-mono-16B\n37.74\n55.80\n63.37\n77.14\n81.10\nMBPP\nCodeGen-mono-6B\n1\n42.77\n56.91\n61.43\n68.84\n70.67\nCodeGen-mono-16B*\n43.24\n60.14\n65.19\n72.31\n74.19\nCodeGen-mono-16B\n51.44\n66.45\n71.64\n80.62\n82.93\nCodeGen-mono-6B\n2\n48.92\n62.29\n66.89\n75.09\n77.25\nCodeGen-mono-16B*\n48.12\n65.31\n70.02\n76.60\n78.70\nCodeGen-mono-16B\n56.44\n71.31\n76.43\n84.39\n86.76\nTable 10: Ablation on cross-model personalised distil-\nlation with PERsD-combined. CodeGen-mono-16B*\nmeans distillation data is from CodeGen-mono-6B.\n5.7\nComparison with other Feedback-based\nCode Generation Models\nComparison with ILF (Chen et al., 2023a): In\norder to compare with ILF, one of our closest re-\nlated work, we experiment on a separate setting:\nstarting with full MBPP dataset (974 tasks) and\nuse Task-Ids 11-111 as test split and remaining\n863 as training data. On the training set, our stu-\ndent model CodeGen-6B (same as ILF) generated\nwrong attempts on 562 tasks, which were shown to\nChatGPT along with the task instruction and execu-\ntion error feedback to eventually collect 288 valid\npersonalized code rectification labels.\nThe\noriginal\nMBPP\ntext-to-code\ndata\nand\nthis\ncollected\npersonalized\ncode-refinement\ndata\nfor\nthe\n288\ntasks\nMBPP Test Set\nMethod\nCost\nPass@1\nPass@10\nILF\n>4K$\n36\n68\nPERSD\n0.65$\n46.8\n67.4\n-refine\n0.65$\n41.8\n66.8\n-combined\n0.65$\n47.8\n64.8\nTable 11: Comparison with\nILF\nrespectively\nform\nthe finetuning data\nDcode\nand\nDrefine\non which we train\nmodels PERSD and\nPERSD-refine.\nWe\nfurther\ncombine\nDcode\nand\nDrefine\nto\ntrain\nPERSD-\ncombined.\nOur experimental results in Table\n11 show that all PERSD-variants significantly\noutperform ILF by 11.8% at pass@1 at a cost 1e-4\ntimes lower than ILF, thus showcasing the lack of\nscalability of ILF-style models.\nComparison with Self-Edit: Since Self-Edit\n(Zhang et al., 2023) uses a trainable CodeGen-\n350M code editor model and a frozen code-\ngeneration model, our experimental setup is not\ndirectly comparable with theirs.\nHowever, our\nINPD-refine and INPD-combined models can actu-\nally be considered as very close counterparts to a\nversion of Self-Edit with shared a code-generation\nand code-refinement model and CodeGen-6B back-\nbone. The consistent performance improvement of\nthe personalized distillation models over the input-\ndistilled ones across the board, alludes towards the\nprospect that PERSD-models are indeed more ef-\nfective than Self-Edit style models.\n5.8\nComparison with SOTA Models\nFianlly, we compare PERsD-combine models\nwith open-source and close-sourced state-of-the-\nart models on HumanEval in Table 12.We find\nthat PERsD-combine methods can significantly im-\nprove the backbone model, with a performance\ngain of 6.2 points for CodeGen-mono 6B (8.4% er-\nror reduction), 5.9 points for CodeGen-mono 16B\n(8.3% error reduction) and 12.2 points for Star-\nCoder (18.4% error reduction). Moreover, Star-\nCoder with PERsD-combined, outperforms other\nopen-sourced models except WizardCoder. Note\nModel\nModel size\nPass@1\nPass@10\nPass@100\nClosed-source models\nLaMDA\n137B\n14.0\n-\n47.3\nPaLM\n540B\n26.2\n-\n76.2\nCodex\n12B\n28.8\n46.8\n72.3\ncode-cushman-001\n-\n33.5\n54.3\n77.4\ncode-davinci-002\n-\n47.0\n74.9\n92.1\nGPT-3.5\n-\n48.1\n-\n-\nphi-1\n1.3B\n50.6\n-\n-\nGPT-4\n-\n67.0\n-\n-\nOpen-source models\nCodeGeeX\n13B\n22.9\n39.6\n60.9\nLLaMA\n65B\n23.7\n-\n79.3\nStarCoder\n15B\n33.6\n-\n-\nCodeGen-mono\n6B\n26.1\n42.3\n65.8\nCodeGen-mono\n16B\n29.3\n49.9\n75.0\nInstructCodeT5+\n16B\n35.0\n54.5\n77.9\nWizardCoder\n15B\n57.3\n-\n-\nCodeGen-mono (PERsD-combined)\n6B\n33.8\n50.0\n70.7\nCodeGen-mono (PERsD-combined)\n16B\n36.4\n60.8\n79.9\nStarCoder (PERsD-combined)\n15B\n45.8\n68.3\n82.3\nTable 12: Results of pass@k(%) on HumanEval\nthat our model ues 5K data examples while Wiz-\nardCoder uses 78K. As mentioned in \u00a72.1, Wiz-\nardCoder is an orthogonal approach that can be\nintegrated into personalised distillation.\n6\nConclusion\nIn this paper, we introduced personalized distilla-\ntion as a method for collecting customized labeled\ndata that adapts to the capacity of student models,\nresulting in more effective learning. We demon-\nstrated the advantages of personalized distillation\nover standard distillation in the field of code gen-\neration, achieving superior performance on both\nthe HumanEval and MBPP datasets. Through com-\nprehensive ablation studies, we confirmed that per-\nsonalized distillation leads to higher data quality,\nbenefits from multi-round distillation, and enables\nmodels to leverage execution feedback for self-\nrectification. We believe personalized distillation\nrepresents an exciting step towards better distilla-\ntion of closed-source LLMs to open-source models.\nLimitations\nIn this section, we discuss some limitations of this\npaper and future directions to make it more valu-\nable:\nOn Data Scale\nFor a fair comparison, we have\nconducted all experiments based on the same 10K\nDSTAND data (introduced \u00a74.2) and the correspond-\ning personalised data processed from DSTAND are\nof size 2-3K as shown in Table 3. However, as\nwe have proven personalised distillation supports\nmore effective and efficient learning, it is intriguing\nto investigate how well does personalised distilla-\ntion scale with the data size. For example, if we\nscale personalised distillation data to 50K, how\nmuch more performance gain will PERsD meth-\nods receive compared to InpD and StanD with the\nscaling of data size.\nOnline Personalised Distillation\nAs discussed\nin \u00a75.4, conducting a second round personalised\ndistillation continues to improve a student model\nthat is already trained with PERsD-combine. Such\nobservation suggests the potential of an online ver-\nsion of personalised distillation, which collects\na batch of personalised data on-the-fly with the\nteacher model, after each optimization step during\nfinetuning. As we have proven that true person-\nalised data is more beneficial than standard data\nor cross-model personalised data (\u00a75.6), such on-\nline personalised distillation will in-principle maxi-\nmally benefit from personalised distillation, as each\nbatch of training data is fully tailored to the student\nmodel.\nReferences\nJacob Austin, Augustus Odena, Maxwell I. Nye,\nMaarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le,\nand Charles Sutton. 2021. Program synthesis with\nlarge language models. CoRR, abs/2108.07732.\nSahil\nChaudhary.\n2023.\nCode\nalpaca:\nAn\ninstruction-following llama model for code genera-\ntion. https://github.com/sahil280114/\ncodealpaca.\nAngelica Chen, J\u00e9r\u00e9my Scheurer, Tomasz Korbak,\nJon Ander Campos, Jun Shern Chan, Samuel R. Bow-\nman, Kyunghyun Cho, and Ethan Perez. 2023a. Im-\nproving code generation by training with natural lan-\nguage feedback.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pond\u00e9 de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and\nDenny Zhou. 2023b. Teaching large language mod-\nels to self-debug. CoRR, abs/2304.05128.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nYuxin Jiang, Chunkit Chan, Mingyang Chen, and\nWei Wang. 2023.\nLion: Adversarial distillation\nof closed-source large language model.\nCoRR,\nabs/2305.12870.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny\nChim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue\nZhuo, Thomas Wang, Olivier Dehaene, Mishig\nDavaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh\nShliazhko, Nicolas Gontier, Nicholas Meade, Armel\nZebaze, Ming-Ho Yee, Logesh Kumar Umapathi,\nJian Zhu, Benjamin Lipkin, Muhtasham Oblokulov,\nZhiruo Wang, Rudra Murthy V, Jason Stillerman,\nSiva Sankalp Patel, Dmitry Abulkhanov, Marco\nZocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Ur-\nvashi Bhattacharyya, Wenhao Yu, Swayam Singh,\nSasha Luccioni, Paulo Villegas, Maxim Kunakov,\nFedor Zhdanov, Manuel Romero, Tony Lee, Na-\ndav Timor, Jennifer Ding, Claire Schlesinger, Hai-\nley Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,\nAlex Gu, Jennifer Robinson, Carolyn Jane Ander-\nson, Brendan Dolan-Gavitt, Danish Contractor, Siva\nReddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jer-\nnite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas\nWolf, Arjun Guha, Leandro von Werra, and Harm\nde Vries. 2023a. Starcoder: may the source be with\nyou! CoRR, abs/2305.06161.\nZihao Li, Zhuoran Yang, and Mengdi Wang. 2023b. Re-\ninforcement learning with human feedback: Learn-\ning dynamic choices via pessimism.\nCoRR,\nabs/2305.18438.\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.\nChain of hindsight aligns language models with feed-\nback. CoRR, abs/2302.02676.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-\nubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-\ninstruct. CoRR, abs/2306.08568.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. CoRR, abs/2303.17651.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nICLR.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Ste-\nfano Ermon, Christopher D. Manning, and Chelsea\nFinn. 2023. Direct preference optimization: Your\nlanguage model is secretly a reward model. CoRR,\nabs/2305.18290.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: memory optimizations\ntoward training trillion parameter models. In SC,\npage 20. IEEE/ACM.\nHeather Roberts-Mahoney, Alexander J. Means, and\nMark J. Garrison. 2016. Netflixing human capital\ndevelopment: personalized learning technology and\nthe corporatization of k-12 education. Journal of\nEducation Policy, 31(4):405\u2013420.\nAtikah Shemshack and Jonathan Michael Spector. 2020.\nA systematic literature review of personalized learn-\ning terms. Smart Learning Environments, 7(1):1\u201320.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois,\nXuechen Li,\nCarlos Guestrin,\nPercy\nLiang, and Tatsunori B. Hashimoto. 2023.\nStan-\nford\nalpaca:\nAn\ninstruction-following\nllama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2022. Generating sequences by learning to\nself-correct. CoRR, abs/2211.00053.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38\u201345, Online. Association\nfor Computational Linguistics.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a.\nWizardlm: Empowering large lan-\nguage models to follow complex instructions. CoRR,\nabs/2304.12244.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023b.\nBaize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nKechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023.\nSelf-edit: Fault-aware code editor for code genera-\ntion. CoRR, abs/2305.04087.\nA\nDetails in Multi-step Model Evaluation\nAs the docstrings are ill-formated in HumanEval,\nwe write a simple rule-based parsing code snip-\npet to extract its seen unit test cases. On average\nper task, there is 2 seen unit test cases and 4.2\nunseen unit test cases. The overlap between seen\nand unseen tests is 11.33%. For MBPP, since con-\nventionally the instruction prompt is constructed\nby taking the task description and example usages\n(from the unit test cases) as part of the doc-string,\nwe consider all the unit test cases to be \"seen\" and\nuse all of them for multi-step inference.\nB\nChatGPT Prompt Template for\nPersonalised Distillation\nIn Figure 3, we show the prompt template we use\nto query ChatGPT for personalised refinement. For\neach task example, with task instruction t, unit test\ncases u and correct code c, we query ChatGPT API\nwith two turn conversation history.\nFor the first turn, we use the template in Fig-\nure 3a and replace \u00abTASK\u00bb, \u00abHEADER\u00bb with the\nactual task instruction t and function header ex-\ntracted. This is added to first turn\u2019s user input and\nthe correct code c is included as first turn\u2019s assistant\noutput. For the second turn, we use the template in\nFigure 3b and replace \u00abCODE\u00bb, \u00abERROR\u00bb with\nthe student model\u2019s attempt and its execution feed-\nback. This is added to second turn\u2019s user input and\nwe query ChatGPT with the constructed conver-\nstaion history to get second turn\u2019s assistant output\nas personalised code refinement.\n(a) Turn-1 Prompt Template\n(b) Turn-2 Prompt Template\nFigure 3: Prompt templates to query personalised re-\nfinement. Top(a): prompt template for first turn con-\nversation, Botton(b): prompt template for second turn\nconversation.\nC\nPrompt Template for Code Refinement\nFinetuning\nFigure 4 shows the refinement template Trefine in-\ntroduced in \u00a73.2), which is used to construct input\nprompt for code refinement finetuning. we replace\n\u00abTASK\u00bb with task instruction, \u00abCODE\u00bb with the\ninitial wrong attempt from student, \u00abERROR\u00bb with\nthe execution feedback, and \u00abHEADER\u00bb with func-\ntion header extracted from task instruciton.\nD\nDetails in Data Overlap Analysis\nThis section describes the detailed procedures to\nconduct train-test data overlap analysis. The objec-\ntive is to assess the extent of data leakage in the\ntest datasets originating from our self-constructed\npretraining corpus.\nFigure 4: Prompt template for code refinement finetun-\ning.\nFirstly, we have performed exact string match\nand found no data leakage in any test data (Hu-\nmanEval/MBPP).\nTo measure the semantic similarity between\ntraining/test tasks, we did the following:\n1. For each task in the test (MBPP/HumanEval)\nwe retrieve two closest training tasks (based\non cosine similarity of starcoder embedding\n& tf-idf vectors of task description).\n2. We use gpt-3.5-turbo-16k to identify whether\nthere is a data leak between a train and test\ninstance by classifying the pair into (\u201cleak\u201d,\n\u201csomewhat similar\u201d, \u201csomewhat not similar\u201d,\n\u201cnot related\u201d). We use a prompt with instruc-\ntions and manually created few-shot exam-\nples and ask gpt-3.5 to generate the reasoning\nand categorization. We manually examined\nseveral examples per category to ensure the\nreasoning and judgment is done correctly and\nconsistently.\n3. Map the similarity categories to 0-1 similarity-\nscore (\u201cleak\u201d -> 1, \u201csomewhat similar\u201d ->\n0.75, \u201csomewhat not similar\u201d -> 0.25, \u201cnot\nrelated\u201d -> 0) and show the mean score and %\nof cases classified as \u201cleak\u201d. Note that StanD\n& PERsD have 10K & 3K training data re-\nspectively so their scores are different.\nE\nResults in MBPP-Cleaned\nIn Appendix D, we find 55 data instances that are\npotentially leaked (with similarity score = 1) in\nMBPP test data. In this section, we construct a\nnew MBPP-Cleaned dataset, where the leaked data\npoints are removed (originally 306 problems \u2192 251\nproblems after filtering). The results on this new\nMBPP-Cleaned dataset is shown in Table 13. From\nthe results, we can see for setting CodeGen-mono-\n16B, pass@1, PERsD becomes almost on-par with\nStanD (from a gap of -1.21 to -0.17). For the rest of\n15/16 settings on PERsD comparing with StanD, its\naverage margin is increased from 4.8 points to 5.9\npoints. Besides, PERsD-refine on MBPP-Cleaned\nshows more consistent and sizable improvements\nover InpD-refine, with an average edge of +0.86 for\n1 step inference, and +1.91 for two step inference.\nOverall, with overlapped test data removed, PERsD\nmethods show even larger advantages compared to\nStanD or InpD methods.\n(a) Backbone as CodeGen-mono-6B\nMethods\n#Data\nPass@1\nPass@5\nPass@10\nPass@20\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nMBPP-Cleaned\nStanD\n10K\n37.51\n-\n50.89\n-\n55.15\n-\n58.87\n-\nInpD\n3.3K\n38.80\n-\n53.91\n-\n58.47\n-\n62.73\n-\n-refine\n3.3K\n37.58\n42.95\n57.65\n62.29\n63.52\n67.79\n67.92\n71.96\n-combined\n6.5K\n38.11\n43.01\n52.69\n58.32\n57.36\n62.75\n61.19\n66.18\nPERsD\n3.3K\n41.30\n-\n56.20\n-\n61.86\n-\n67.53\n-\n-refine\n3.3K\n43.86\n47.73\n59.33\n64.41\n65.19\n69.95\n69.62\n74.33\n-combined\n6.5K\n38.86\n43.75\n52.78\n57.04\n57.35\n61.78\n61.52\n66.19\n(b) Backbone as CodeGen-mono-16B\nMethods\n#Data\nPass@1\nPass@5\nPass@10\nPass@20\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nstep=1\nstep=2\nMBPP-Cleaned\nStanD\n10K\n43.10\n-\n57.53\n-\n62.92\n-\n68.12\n-\nInpD\n2.8K\n40.64\n-\n53.88\n-\n58.82\n-\n62.88\n-\n-refine\n2.8K\n43.67\n49.60\n63.14\n68.21\n69.27\n73.28\n73.36\n76.85\n-combined\n5.6K\n41.63\n47.77\n54.74\n62.24\n59.67\n67.33\n63.75\n71.57\nPERsD\n2.8K\n42.93\n-\n62.40\n-\n68.90\n-\n74.10\n-\n-refine\n2.8K\n47.73\n52.63\n63.62\n69.21\n69.84\n75.17\n74.90\n79.69\n-combined\n5.6K\n46.33\n51.67\n63.46\n68.65\n69.49\n74.26\n74.53\n78.83\nTable 13: Comparing performance of PERsD models to\nStanD & InpD on MBPP-Cleaned\n"
  },
  {
    "title": "Text-to-3D with classifier score distillation",
    "link": "https://arxiv.org/pdf/2310.19415.pdf",
    "upvote": "4",
    "text": "TEXT-TO-3D WITH CLASSIFIER SCORE DISTILLATION\nXin Yu1,3 Yuan-Chen Guo2,3 Yangguang Li3 Ding Liang3 Song-Hai Zhang2 Xiaojuan Qi1\u2020\n1The University of Hong Kong\n2Tsinghua University\n3VAST\nABSTRACT\nText-to-3D generation has made remarkable progress recently, particularly with\nmethods based on Score Distillation Sampling (SDS) that leverages pre-trained\n2D diffusion models. While the usage of classifier-free guidance is well acknowl-\nedged to be crucial for successful optimization, it is considered an auxiliary trick\nrather than the most essential component. In this paper, we re-evaluate the role of\nclassifier-free guidance in score distillation and discover a surprising finding: the\nguidance alone is enough for effective text-to-3D generation tasks. We name this\nmethod Classifier Score Distillation (CSD), which can be interpreted as using an\nimplicit classification model for generation. This new perspective reveals new in-\nsights for understanding existing techniques. We validate the effectiveness of CSD\nacross a variety of text-to-3D tasks including shape generation, texture synthesis,\nand shape editing, achieving results superior to those of state-of-the-art methods.\nOur project page is https://xinyu-andy.github.io/Classifier-Score-Distillation\n1\nINTRODUCTION\n3D content creation is important for many applications, such as interactive gaming, cinematic arts,\nAR/VR, and simulation. However, it is still challenging and expensive to create a high-quality 3D\nasset as it requires a high level of expertise. Therefore, automating this process with generative\nmodels has become an important problem, which remains challenging due to the scarcity of data\nand the complexity of 3D representations.\nRecently, techniques based on Score Distillation Sampling (SDS) (Poole et al., 2022; Lin et al.,\n2023; Chen et al., 2023; Wang et al., 2023b), also known as Score Jacobian Chaining (SJC) (Wang\net al., 2023a), have emerged as a major research direction for text-to-3D generation, as they can\nproduce high-quality and intricate 3D results from diverse text prompts without requiring 3D data\nfor training. The core principle behind SDS is to optimize 3D representations by encouraging their\nrendered images to move towards high probability density regions conditioned on the text, where\nthe supervision is provided by a pre-trained 2D diffusion model (Ho et al., 2020; Sohl-Dickstein\net al., 2015; Rombach et al., 2022; Saharia et al., 2022; Balaji et al., 2022). DreamFusion (Poole\net al., 2022) advocates the use of SDS for the optimization of Neural Radiance Fields (NeRF).\n(Barron et al., 2022; Mildenhall et al., 2021). Subsequent research improve the visual quality by\nintroducing coarse-to-fine optimization strategies (Lin et al., 2023; Chen et al., 2023; Wang et al.,\n2023b), efficient 3D representations (Lin et al., 2023; Chen et al., 2023), multi-view-consistent\ndiffusion models (Shi et al., 2023; Zhao et al., 2023), or new perspectives on modeling the 3D\ndistribution (Wang et al., 2023b). Despite these advancements, all these works fundamentally rely\non score distillation for optimization.\nAlthough score distillation is theoretically designed to optimize 3D representations through proba-\nbility density distillation (Poole et al., 2022; Oord et al., 2018), as guided by pre-trained 2D diffusion\nmodels (i.e., Eq. (4)), a practical gap emerges in its implementation due to the widespread reliance\non classifier-free guidance (CFG) (Ho & Salimans, 2022). When using CFG, the gradient that drives\nthe optimization actually comprises two terms. The primary one is the gradient of the log data den-\nsity, i.e., log p\u03d5 (xt|y), estimated by the diffusion models to help move the synthesized images x\nto high-density data regions conditioned on a text prompt y, which is the original optimization ob-\njective (see Eqs. (4) and (5)). The second term is the gradient of the log function of the posterior,\ni.e., log p\u03d5 (y|xt), which can be empirically interpreted as an implicit classification model (Ho &\nSalimans, 2022). We elaborate more on this in Sec. 3. The combination of these two components\ndetermines the final effect of optimizing the 3D representation. Notably, although the use of CFG\n1\narXiv:2310.19415v2  [cs.CV]  31 Oct 2023\n\u201cRugged, vintage-inspired hiking boots \nwith a weathered leather finish\u201d\n\u201cUniform jacket of the captain\u201d\n\u201cHumanoid warrior with a tiger head, \nadorned in Chinese-style armor, \nwielding traditional weapons\u201d\n\u201cCirilha lives in the forest\u201d\n\u201ca DSLR photo of an orc forging a hammer on an anvil\u201d\n\u201ca zoomed out DSLR photo of an astronaut chopping vegetables in a sunlit kitchen\u201d\n\u201ca wide angle DSLR photo of a humanoid banana sitting at a desk doing homework\u201d\n\u201ca zoomed out DSLR photo of a panda wearing a chef's hat and \nkneading bread dough on a countertop\u201d\n(b) Textured mesh generation from text-prompts\n(c) Synthesize texture for given meshes from text-prompts\n(a) NeRF generation from text-prompts\n\u201ca DSLR photo of a red pickup truck \ndriving across a stream\u201d\n\u201ca DSLR photo of a chimpanzee \ndressed like Henry VIII king of \nEngland\u201d\n\u201ca DSLR photo of a koala wearing a \nparty hat and blowing out birthday \ncandles on a cake\u201d\n\u201ca DSLR photo of a pig playing a \ndrum set\u201d\n\u201ca DSLR photo of cat wearing virtual \nreality headset in renaissance oil \npainting high detail Caravaggio\u201d\n\u201ca zoomed out DSLR photo of a \nchimpanzee wearing headphones\u201d\n\u201ca zoomed out DSLR photo of a toad \ncatching a fly with its tongue\u201d\n\u201ca nest with a few white eggs and \none golden egg\u201d\n\u201ca DSLR photo of a barbecue grill \ncooking sausages and burger patties\u201d\n\u201ca DSLR photo of a robot and dinosaur \nplaying chess, high resolution\u201d\n\u201ca DSLR photo of a drum set made of \ncheese\u201d\n\u201ca DSLR photo of edible typewriter \nmade out of vegetables\u201d\nFigure 1: Illustrative overview of our method\u2019s capabilities. (a) Generation of a Neural Radiance\nField (NeRF) from text inputs, trained using a 64\u00d764 diffusion guidance model. (b) Subsequent\nrefinement leads to high-quality textured meshes using a 512\u00d7512 diffusion guidance model. (c)\nTexture synthesis on user-specified meshes, resulting in highly realistic and high-resolution detail.\n2\n(i.e., the second term) is widely recognized as vital for text-to-3D generation, it is considered an\nauxiliary trick to help the optimization, rather than as the primary driving force.\nIn this paper, we show that the classifier component of Score Distillation Sampling (SDS) is not\njust auxiliary but essential for text-to-3D generation, and using only this component is sufficient.\nWe call this paradigm Classifier Score Distillation (CSD). This insight fundamentally shifts our\nunderstanding of the mechanisms underlying the success of text-to-3D generation based on score\ndistillation. Specifically, the efficacy stems from distilling knowledge from an implicit classifier\np\u03d5 (y|xt) rather than from reliance on the generative prior p\u03d5 (xt|y). This finding further enables\nus to uncover fresh insights into existing design choices: 1) We demonstrate that utilizing negative\nprompts can be viewed as a form of joint optimization with dual classifier scores. This realization\nenables us to formulate an annealed negative classifier score optimization strategy, which enhances\ngeneration quality while maintaining the result faithfulness according to prompts. 2) Building on\nour understanding, we illustrate the utilization of classifier scores for efficient text-driven 3D editing.\n3) The Variational Score Distillation technique (Wang et al., 2023b) can be viewed as an adaptive\nform of negative classifier score optimization, where the negative direction is supplied by a diffusion\nmodel trained concurrently.\nCSD can be seamlessly integrated into existing SDS-based 3D generation pipelines and applica-\ntions, such as text-driven NeRF generation, textured mesh generation, and texture synthesis. As\ndemonstrated in Fig. 1, our method produces high-quality generation results, featuring extremely\nphoto-realistic appearances and the capability to generate scenes corresponding to complex textual\ndescriptions. We conduct extensive experiments to evaluate the robustness of our approach and\ncompare our method against existing methods, achieving state-of-the-art results.\n2\nDIFFUSION MODELS\nThe diffusion model (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021; Song & Er-\nmon, 2020) is a type of likelihood-based generative model used for learning data distributions.\nGiven an underlying data distribution q(x), the model initiates a forward process that progres-\nsively adds noise to the data x sampled from q(x). This produces a sequence of latent variables\n{x0 = x, x1, . . . , xT }. The process is defined as a Markov Chain q(x1:T |x) := QT\nt=1 q(xt|xt\u22121)\nwith Gaussian transition kernels. The marginal distribution of latent variables at time t is given by\nq(xt|x) = N(\u03b1tx, \u03c32\nt I). This is equivalent to generating a noise sample xt using the equation\nxt = \u03b1tx + \u03c3t\u03f5, where \u03f5 \u223c N(0, I). Parameters \u03b1t and \u03c3t are chosen such that \u03c32\nt + \u03b12\nt = 1, and\n\u03c3t gradually increases from 0 to 1. Thus, q(xt) converges to a Gaussian prior distribution N(0, I).\nNext, a reverse process (i.e., the generative process) is executed to reconstruct the original signal\nfrom xT . This is described by a Markov process p\u03d5(x0:T ) := p(xT ) QT\nt=1 p\u03d5(xt\u22121|xt), with the\ntransition kernel p\u03d5 (xt\u22121 | xt) := N\n\u0000\u00b5\u03d5 (xt, t) , \u03c32\nt I\n\u0001\n. The training objective is to optimize \u00b5\u03d5 by\nmaximizing a variational lower bound of the log data likelihood. In practice, \u00b5\u03d5 is re-parameterized\nas a denoising network \u03f5\u03d5 (Ho et al., 2020) and the loss can be further simplified to a Mean Squared\nError (MSE) criterion (Ho et al., 2020; Kingma et al., 2021):\nLDiff (\u03d5) := Ex\u223cq(x),t\u223cU(0,1),\u03f5\u223cN (0,I)\nh\n\u03c9(t) \u2225\u03f5\u03d5 (\u03b1tx + \u03c3t\u03f5; t) \u2212 \u03f5\u22252\n2\ni\n,\n(1)\nwhere w(t) is time-dependent weights.\nThe objective function can be interpreted as predicting the noise \u03f5 that corrupts the data x. Besides,\nit is demonstrated to be correlated to NCSN denoising score matching model (Ho et al., 2020; Song\n& Ermon, 2020), and thus the predicted noise is also related to the score function of the perturbed\ndata distribution q(xt), which is defined as the gradient of the log-density with respect to the data\npoint:\n\u2207xt log q (xt) \u2248 \u2212\u03f5\u03d5 (xt; t) /\u03c3t.\n(2)\nThis means that the diffusion model can estimate a direction that guides xt towards a high-density\nregion of q(xt), which is the key idea of SDS for optimizing the 3D scene. Since samples in high-\ndensity regions of q(xt) are assumed to reside in the high-density regions of q(xt\u22121) (Song &\nErmon, 2020), repeating the process can finally obtain samples of good quality from q(x).\n3\nClassifier-Free Guidance\nText-conditioned diffusion models (Balaji et al., 2022; Saharia et al.,\n2022; Rombach et al., 2022; Ramesh et al., 2022) generate samples x based on a text prompt y,\nwhich is also fed into the network as an input, denoted as \u03f5\u03d5 (xt; y, t). A common technique to\nimprove the quality of these models is classifier-free guidance (CFG) (Ho & Salimans, 2022). This\nmethod trains the diffusion model in both conditioned and unconditioned modes, enabling it to\nestimate both \u2207xt log q (xt|y) and \u2207xt log q (xt), where the network are denoted as \u03f5\u03d5 (xt; y, t)\nand \u03f5\u03d5 (xt; t). During sampling, the original score is modified by adding a guidance term, i.e.,\n\u03f5\u03d5(xt; y, t) \u2192 \u03f5\u03d5(xt; y, t) + \u03c9 \u00b7 [\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t)], where \u03c9 is the guidance scale that con-\ntrols the trade-off between fidelity and diversity. Using Eq. (2) and Bayes\u2019 rule, we can derive the\nfollowing relation:\n\u2212 1\n\u03c3t\n[\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t)] = \u2207xt log q (xt|y) \u2212 \u2207xt log q (xt) \u221d \u2207xt log q (y|xt) .\n(3)\nThus, the guidance can be interpreted as the gradient of an implicit classifier (Ho & Salimans, 2022).\n3\nWHAT MAKES SDS WORK?\nScore Distillation Sampling (SDS) (Poole et al., 2022) is a novel technique that leverages pretrained\n2D diffusion models for text-to-3D generation. SDS introduces a loss function, denoted as LSDS,\nwhose gradient is defined as follows:\n\u2207\u03b8LSDS (g(\u03b8)) = Et,\u03f5,c\n\u0014\nw(t) \u03c3t\n\u03b1t\n\u2207\u03b8 KL (q (xt|x = g(\u03b8; c)) \u2225p\u03d5 (xt|y))\n\u0015\n,\n(4)\nwhere \u03b8 is the learnable parameter of a 3D representation (e.g., NeRF), and g is a differentiable\nrendering function that enables obtaining the rendered image from the 3D scene \u03b8 and camera c.\nThe optimization aims to find modes of the score functions that are present across all noise levels\nthroughout the diffusion process which is inspired by work on probability density distillation (Poole\net al., 2022; Oord et al., 2018).\nEq. (4) can be simplified into: Et,\u03f5,c\n\u0002\nw(t)(\u03f5\u03d5(xt; y, t)) \u2202x\n\u2202\u03b8\n\u0003\n. This formulation is more intuitive\nthan Eq. (4): The conditional score function \u03f5\u03d5(xt; y, t) indicates the gradient for updating xt to be\ncloser to high data density regions (see Eq. (2)), allowing the parameter \u03b8 to be trained using the\nchain rule. However, in practice, DreamFusion recommends adjusting the gradient \u03f5\u03d5(xt; y, t) to\n\u03f5\u03d5(xt; y, t) \u2212 \u03f5 since this modification has been found to improve convergence. This change does\nnot alter the overall objective because Et,\u03f5,c\n\u0002\nw(t)(\u2212\u03f5) \u2202x\n\u2202\u03b8\n\u0003\n= 0. Consequently, the final gradient to\nupdate \u03b8 is expressed as:\n\u2207\u03b8LSDS = Et,\u03f5,c\n\u0014\nw(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5)\u2202x\n\u2202\u03b8\n\u0015\n.\n(5)\nWhat Makes SDS Work?\nA crucial aspect of score distillation involves computing the gradient\nto be applied to the rendered image x during optimization, which we denote as \u03b4x(xt; y, t) :=\n\u03f5\u03d5(xt; y, t) \u2212 \u03f5. This encourages the rendered images to reside in high-density areas conditioned on\nthe text prompt y. From Eq. (4), in principle, \u03f5\u03d5(xt; y, t) should represent the pure text-conditioned\nscore function. However, in practice, classifier-free guidance is employed in diffusion models with a\nlarge guidance weight \u03c9 (e.g., \u03c9 = 100 in DreamFusion (Poole et al., 2022)) to achieve high-quality\nresults, causing the final gradient applied to the rendered image to deviate from Eq. (5). Specifically,\n\u03b4x with CFG is expressed as:\n\u03b4x(xt; y, t) = [\u03f5\u03d5(xt; y, t) \u2212 \u03f5]\n|\n{z\n}\n\u03b4gen\nx\n+\u03c9 \u00b7 [\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t)]\n|\n{z\n}\n\u03b4cls\nx\n.\n(6)\nThe gradient can be decomposed into two parts, i.e., \u03b4x := \u03b4gen\nx\n+ \u03c9 \u00b7 \u03b4cls\nx . According to Eq. (2)\nand Eq. (3), the first term \u03b4gen\nx\nis associated with \u2207xt log q (xt|y). This term signifies the gradient\ndirection in which the image should move to become more realistic conditioned on the text, which\nwe refer to as the generative prior. \u03b4cls\nx is related to \u2207xt log q (y|xt), representing the update direction\nrequired for the image to align with the text evaluated by an implicit classifier model, which emerges\nfrom the diffusion model (Ho & Salimans, 2022). We refer to this part as the classifier score.\n4\nOptimization steps\n\ud835\udeff!\n\"#$\n\ud835\udeff!%&'\n\ud835\udf14 = 20\n\ud835\udf14 = 40\n\ud835\udf14 = 10\n\ud835\udf14 = 5\n\ud835\udf14 = 0 (\ud835\udeff!\"#)\n(a) The gradient norm of two terms during optimization (\ud835\udf14 = 40)\n(b) Different results with difference guidance weights\nNorm of \ud835\udeff!\n\"#$\nNorm of \ud835\udeff!\n%&'\nFigure 2: (a) The gradient norm during opti-\nmization. (b) Optimization results through dif-\nferent guidance weights.\nThus, there is a gap between the theoretical for-\nmulation in Eq. (4) and the practical implementa-\ntion in Eq. (6). To better understand the roles of\ndifferent terms, we conduct an empirical study on\nthe contribution of the two terms in the text-to-\n3D optimization process by visualizing the gra-\ndient norm throughout the training of a NeRF\nvia SDS with guidance scale \u03c9 = 40. As illus-\ntrated in Fig. 2 (a), the gradient norm of the gen-\nerative prior is several times larger than that of\nthe classifier score. However, to generate high-\nquality results, a large guidance weight must be\nset (e.g., \u03c9 = 40), as shown in Fig. 2 (b). When\nincorporating both components, the large guid-\nance weight actually causes the gradient from the\nclassifier score to dominate the optimization di-\nrection. Moreover, the optimization process fails\nwhen relying solely on the generative component,\nas indicated by setting \u03c9 = 0 in Eq. (6). This observation leads us to question whether the clas-\nsifier score is the true essential component that drives the optimization. To explore this, we\nintroduce Classifier Score Distillation (CSD), a method that employs only the classifier score for the\noptimization of 3D scenes. Further details are elaborated in the sections below.\n4\nCLASSIFIER SCORE DISTILLATION (CSD)\nConsider a text prompt y and a parameterized image generator g(\u03b8). We introduce a loss function\ndenoted as Classifier Score Distillation LCSD, the gradient of which is expressed as follows:\n\u2207\u03b8LCSD = Et,\u03f5,c\n\u0014\nw(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t))\u2202x\n\u2202\u03b8\n\u0015\n.\n(7)\nOur loss is fundamentally different from the previous principle presented in Eq. (4). According to\nEq. (3), we use an implicit classification model that is derived from the generative diffusion models\nto update the 3D scene. Specifically, text-to-image diffusion models with CFG can generate both\nconditioned and unconditioned outputs, and we can derive a series of implicit noise-aware classifiers\n(Ho & Salimans, 2022) for different time steps t, based on Eq. (3). These classifiers can evaluate\nthe alignment between the noisy image and the text y. Thus, the overall optimization objective\nin Eq. (7) seeks to refine the 3D scene in such a manner that rendered images at any noise level\nt align closely with their respective noise-aware implicit classifiers. We will use \u03b4cls\nx (xt; y, t) :=\n\u03f5\u03d5[(xt; y, t) \u2212 \u03f5\u03d5(xt; t)] to denote the classifier score for further discussions.\nWe note that some previous works have also used text and image matching losses for 3D generation,\nrather than relying on a generative prior, such as the CLIP loss (Jain et al., 2022; Michel et al.,\n2022). However, these methods are still less effective than score distillation methods based on\nthe generative diffusion model. In the next section, we show that our loss function can produce\nhigh-quality results that surpass those using the SDS loss. Moreover, our formulation enables more\nflexible optimization strategies and reveals new insights into existing techniques. In the following,\nwe first introduce improved training strategies based on CSD, then we develop the application for\n3D editing and discuss the connections between CSD and Variational Score Distillation (VSD).\nCSD with Annealed Negative Prompts\nWe observe that employing negative prompts yneg, text\nqueries describing undesired images, can accelerate the training process and enhance the quality of\nthe generated results. We provide an interpretation from the perspective of CSD. Specifically, with\nnegative prompts, the SDS loss can be rewritten as:\n\u03b4sds\nx\n= [\u03f5\u03d5(xt; y, t) \u2212 \u03f5] + \u03c9 \u00b7 [\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; yneg, t)]\n(8)\nRecall that \u03b4cls\nx (xt; y, t) := \u03f5\u03d5[(xt; y, t) \u2212 \u03f5\u03d5(xt; t)], we arrive at the following expression:\n\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; yneg, t) =[\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t)] \u2212 [\u03f5\u03d5(xt; yneg, t) \u2212 \u03f5\u03d5(xt; t)]\n=\u03b4cls\nx (xt; y, t) \u2212 \u03b4cls\nx (xt; yneg, t)\n(9)\n5\nTherefore, the use of negative prompts can be seen as a dual-objective Classifier Score Distillation:\nIt not only pushes the model toward the desired prompt but also pulls it away from the unwanted\nstates, evaluated by two classifier scores. However, this may also cause a trade-off between quality\nand fidelity, as the negative prompts may make the optimization diverge from the target text prompt\ny, resulting in mismatched text and 3D outputs, as shown in Fig. 5.\nGiven CSD and our novel insights into the effects of negative prompts, we introduce an extended\nCSD formulation that redefines \u03b4cls\nx as follows:\n\u03b4cls\nx =\u03c91 \u00b7 \u03b4cls\nx (xt; y, t) \u2212 \u03c92 \u00b7 \u03b4cls\nx (xt; yneg, t)\n=\u03c91 \u00b7 [(\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t))] \u2212 \u03c92 \u00b7 [(\u03f5\u03d5(xt; yneg, t) \u2212 \u03f5\u03d5(xt; t))]\n=\u03c91 \u00b7 \u03f5\u03d5(xt; y, t) + (\u03c92 \u2212 \u03c91) \u00b7 \u03f5\u03d5(xt; t) \u2212 \u03c92 \u00b7 \u03f5\u03d5(xt; yneg, t)\n(10)\nWe use different weights \u03c91 and \u03c92 for the positive and negative prompts, respectively, to mitigate\nthe negative effects of the latter. By gradually decreasing \u03c92, we observe that our modified loss\nfunction enhances both the quality and the fidelity of the texture results, as well as the alignment\nwith the target prompt (refer to Fig. 5). Note that this simple formulation is based on our new\ninterpretation of negative prompts from the perspective of Classifier Score Distillation, and it is not\npart of the original SDS framework.\nCSD for Text-Guided Editing\nThe CSD method also enables us to perform text-guided 3D edit-\ning. Suppose we have a text prompt ytarget that describes the desired object and another text prompt\nyedit that specifies an attribute that we want to change in the original object. We can modify the loss\nfunction in Eq. (10) by replacing y and yneg with ytarget and yedit, respectively:\n\u03b4edit\nx\n=\u03c91 \u00b7 \u03b4cls\nx (xt; ytarget, t) \u2212 \u03c92 \u00b7 \u03b4cls\nx (xt; yedit, t)\n=\u03c91 \u00b7 \u03f5\u03d5(xt; ytarget, t) + (\u03c92 \u2212 \u03c91) \u00b7 \u03f5\u03d5(xt; t) \u2212 \u03c92 \u00b7 \u03f5\u03d5(xt; yedit, t).\n(11)\nAs shown in Fig. 6, this method allows us to edit the rendered image according to the target descrip-\ntion ytarget while modifying or removing the attribute in yedit. Text-to-3D generation can be seen as\na special case of this method, where we set \u03c92 = 0 and no specific attribute in the original scene is\ngiven to be edited. Our method shares a similar formulation as Delta Denoising Score (Hertz et al.,\n2023) for image editing, which we discuss in the Appendix. Furthermore, we can adjust the weights\n\u03c91 and \u03c92 to balance the alignment with the text prompt and the fidelity to the original scene.\nDiscussions and Connection to Variational Score Distillation (VSD)\nWith CSD, the new varia-\ntional score distillation (VSD) (Wang et al., 2023b) can be interpreted from the perspective of using\na negative classifier score. VSD enhances SDS by replacing the noise term \u03f5 with the output of\nanother text-conditioned diffusion model, denoted as \u03f5\u03d5\u2217, which is simultaneously trained using the\nrendered images from the current scene states. When optimizing the 3D scene, the gradient direction\napplied to the rendered image is:\n\u03b4vsd\nx\n= [\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5\u2217(xt; y, t)] + \u03c9 \u00b7 [\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t)]\n=(\u03c9 + 1) \u00b7 [\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t)] \u2212 [\u03f5\u03d5\u2217(xt; y, t) \u2212 \u03f5\u03d5(xt; t)] .\n(12)\nSince \u03f5\u03d5\u2217 is obtained by fine-tuning the conditional branch of a pre-trained diffusion model us-\ning LoRA (Hu et al., 2021), we can assume that \u03f5\u03d5(xt; t) \u2248 \u03f5\u03d5\u2217(xt; t) and then the term\n[\u03f5\u03d5\u2217(xt; y, t) \u2212 \u03f5\u03d5(xt; t)] corresponds to predicting \u2207xt log p\u03d5\u2217(y|xt), where p\u03d5\u2217 is a shifted distri-\nbution that adapts to the images rendered from the current 3D object. Thus, Eq. (12) encourages the\noptimization direction to escape from p\u03d5\u2217 which can be interpreted as an ability to adaptively learn\nnegative classifier scores instead of relying on a predefined negative prompt. However, this also\nintroduces training inefficiency and instabilities. In our experiments, we find that CSD combined\nwith general negative prompts can achieve high-quality texture quality comparable to VSD.\n5\nEXPERIMENTS\nWe evaluate the efficacy of our proposed Classifier Score Distillation method across three tasks:\ntext-guided 3D generation, text-guided texture synthesis, and text-guided 3D editing. We present\nqualitative and quantitative analysis for text-guided 3D generation in Sec. 5.2 and text-guided texture\nsynthesis in Sec. 5.3. To further substantiate the superiority of our approach, we conduct user\nstudies for these two tasks. To showcase the capabilities of our formulation in 3D editing, illustrative\nexamples are provided in Sec. 5.4.\n6\n\u201ca wide angle zoomed out DSLR photo of a skiing penguin wearing a puffy jacket\u201d\n\u201ca zoomed out DSLR photo of a bulldozer made out of toy bricks\u201d\n\u201ca zoomed out DSLR photo of a 3D model of an adorable cottage with a thatched roof\u201d\nDreamFusion\nMagic3D\nFantasia3D\nProlificDreamer\nOurs\nFigure 3: Qualitative comparisons to baselines for text-to-3D generation. Our method can generate\n3D scenes that align well with input text prompts with realistic and detailed appearances.\n\u201cElf with ethereal, \nbutterfly-like wings, \nradiating an aura of \nmystical elegance\u201d\n\u201cDing censer with an \nopenwork cover and \nhandles in the shape \nof stylized dragons\u201d\nInput mesh&prompt\nFantasia3D\nMagic3D\nProlificDreamer\nTEXTure\nOurs\nFigure 4: Qualitative comparisons to baselines for text-guided texture synthesis on 3D meshes. Our\nmethod generates more detailed and photo-realistic textures.\n5.1\nIMPLEMENTATION DETAILS\nText-Guided 3D Generation\nWe follow Magic3D (Lin et al., 2023) to initially generate a scene\nrepresented by Neural Radiance Fields (NeRF) using low-resolution renderings. Subsequently, the\nscene is converted into a triangular mesh via differentiable surface extraction (Shen et al., 2021) and\nfurther refined using high-resolution mesh renderings by differentiable rasterization (Laine et al.,\n2020). For the NeRF generation, we utilize the DeepFloyd-IF stage-I model (StabilityAI, 2023),\nand for the mesh refinement, we use the Stable Diffusion 2.1 model (Rombach et al., 2022) to\nenable high-resolution supervision. For both stages, CSD is used instead of SDS.\nText-Guided Texture Synthesis\nGiven a mesh geometry and a text prompt, we apply CSD to ob-\ntain a texture field represented by Instant-NGP (M\u00a8uller et al., 2022). We employ ControlNets (Zhang\n& Agrawala, 2023) based on the Stable Diffusion 1.5 as our diffusion guidance since it can improve\nalignment between the generated textures and the underlying geometric structures. Specifically, we\napply Canny edge ControlNet where the edge is extracted from the rendered normal maps, and depth\nControlNet on rendered depth maps. For both control types, we use a control scale of 0.5.\n7\nTable 1: User study on two tasks. In both tasks, more\nusers prefer our results.\nMethods\nText-to-3D (%) \u2191\nTexture Synthesis (%) \u2191\nDreamFusion (Poole et al., 2022)\n30.3\n-\nMagic3D (Lin et al., 2023)\n10.3\n12.4\nFantasia3D (Chen et al., 2023)\n-\n11.0\nProlificDreamer (Wang et al., 2023b)\n-\n7.9\nTEXTure (Richardson et al., 2023)\n-\n11.0\nOurs\n59.4\n57.7\nTable 2: Quantitative comparisons to base-\nlines for text-to-3D generation, evaluated by\nCLIP Score and CLIP R-Precision.\nMethods\nCLIP\nCLIP R-Precision (%) \u2191\nScore (\u2191)\nR@1\nR@5\nR@10\nDreamFusion (Poole et al., 2022)\n67.5\n73.1\n90.7\n97.2\nMagic3D (Lin et al., 2023)\n74.9\n74.1\n91.7\n96.6\nOurs\n78.6\n81.8\n94.8\n96.3\n5.2\nTEXT-GUIDED 3D GENERATION\nQualitative Comparisons.\nWe present some representative results using CSD in Fig. 1, includ-\ning both NeRF generation (a) and mesh refinement (b) results. In general, CSD can generate 3D\nscenes that align well with input text prompts with realistic and detailed appearances. Even only\ntrained with low-resolution renderings (a), our results do not suffer from over-smoothness or over-\nsaturation. We also find that our approach excels in grasping complex concepts in the input prompt,\nwhich is a missing property for many previous methods. In Fig. 3, we compare our generation re-\nsults with DreamFusion (Poole et al., 2022), Magic3D (Lin et al., 2023), Fantasia3D (Chen et al.,\n2023), and ProlificDreamer (Wang et al., 2023b). For DreamFusion, we directly take the results\nprovided on its official website. For the other methods, we obtain their results using the implemen-\ntations from threestudio (Guo et al., 2023). Compared with SDS-based methods like DreamFusion,\nMagic3D, and Fantasia3D, our results have significantly more realistic appearances. Compared with\nProlificDreamer which uses VSD, our approach can achieve competitive visual quality while being\nmuch faster, i.e., 1 hour on a single A800 GPU as opposed to 8 hours required by ProlificDreamer.\nQuantitative Evaluations.\nWe follow previous work (Jain et al., 2022; Poole et al., 2022; Luo\net al., 2023) to quantitatively evaluate the generation quality using CLIP Score (Hessel et al., 2022;\nRadford et al., 2021) and CLIP R-Precision (Park et al., 2021). Specifically, the CLIP Score mea-\nsures the semantic similarity between the renderings of the generated 3D object and the input text\nprompt. CLIP R-Precision measures the top-N accuracy of retrieving the input text prompt from\na set of candidate text prompts using CLIP Score. We generate 3D objects using 81 diverse text\nprompts from the website of DreamFusion. Each generated 3D object is rendered from 4 different\nviews (front, back, left, right), and the CLIP Score is calculated by averaging the similarity between\neach rendered view and a text prompt. We use the CLIP ViT-B/32 (Radford et al., 2021) model\nto extract text and image features, and the results are shown in Tab. 2. Our approach significantly\noutperforms DreamFusion and Magic3D in terms of CLIP Score and CLIP R-Precision, indicating\nbetter alignment between the generated results and input text prompts.\nUser Studies.\nWe conduct a user study for a more comprehensive evaluation. We enlist 30 par-\nticipants and ask them to compare multiple outputs, selecting the one they find most aligned with\ncriteria such as visual quality and text alignment. In total, we collect 2289 responses. The results,\npresented in Tab. 1 (Text-to-3D), reveal that 59.4% of the responses prefer our results, demonstrating\nthe superior quality of our approach.\n5.3\nTEXT-GUIDED TEXTURE SYNTHESIS\nWe select 20 diverse meshes from Objaverse (Deitke et al., 2022) for the texture generation task. We\ncompare our generation results with those from Magic3D (Lin et al., 2023), Fantasia3D (Chen et al.,\n2023), ProlificDreamer (Wang et al., 2023b), and TEXTure (Richardson et al., 2023). We obtain\nthe results of TEXTure using the official implementation and others using the implementations in\nthreestudio. As illustrated in the Fig. 4, our method excels in generating photo-realistic texture\ndetails. Moreover, owing to our optimization based on implicit functions, the results we produce\ndo not exhibit the seam artifacts commonly observed in TEXTure (Richardson et al., 2023). Our\napproach ensures both local and global consistency in the generated textures. We also conducted\nthe user study, where 30 participants were asked and we got a total of 537 responses. The result is\npresented in Tab. 1 (Texture Synthesis), where 57.7% of the responses prefer our results.\nHere, we examine how different uses of the negative prompt can impact visual quality. As shown in\nFig. 5, incorporating negative prompts can indeed enhance the visual quality. However, we observe\nthat it may compromise the alignment with the original text prompt, i.e., the original text prompt\ndescribes the color of the clothes as blue, but the generated clothes turn out to be white. As we have\nillustrated, since the negative prompt serves as an additional classifier score to guide the direction\n8\nInput mesh&prompt\nCSD, w/ negative prompt\nCSD, w/ annealed negative classifier score\nCSD, w/o negative prompt\n\u201cBlue cotton \nhoodie with red \nflower patterns\u201d\nFigure 5: Ablation study on negative prompts and annealed negative classifier scores.\n\u201ca wide angle DSLR photo of a humanoid \nbanana sitting at a desk doing homework\u201d\n\u201ca wide angle DSLR photo of a humanoid\ncucumber sitting at a desk doing homework\u201d\n\u201ca zoomed out DSLR photo of a beautiful \nsuit made out of moss, on a mannequin. \nStudio lighting, high quality, high resolution\u201d\n\u201ca zoomed out DSLR photo of a beautiful suit \nmade out of flower, on a mannequin. Studio \nlighting, high quality, high resolution\u201d\n\u201ca plush toy of a corgi nurse\u201d\n\u201ca plush toy of a corgi policeman\u201d\n\u201ca DSLR photo of a shiba inu playing \ngolf wearing tartan golf clothes and hat\u201d\n\u201ca DSLR photo of a cat playing golf \nwearing tartan golf clothes and hat\u201d\nFigure 6: Demonstration of CSD in text-guided 3D editing Tasks. Our method effectively modifies\nattributes based on the given prompt while faithfully preserving the remaining features.\nof the update, it has the effect of weakening the influence of the original prompt. Fortunately, our\nformulation allows for dynamic adjustment of weights to balance these forces. As shown in Fig. 5,\nwe set \u03c91 = 1 and reduce the weight \u03c92 assigned to the negative classifier score, resulting in\nimproved visual quality that remains aligned with the text prompt.\n5.4\nTEXT-GUIDED 3D EDITING\nIn Fig. 6, we demonstrate that CSD can be further applied to edit the existing NeRF scenes. For\neach scene, based on Eq. (11), we employ the portion that requires editing as the negative prompt\n(e.g., \u201cnurse\u201d), while modifying the original prompt to reflect the desired scene (e.g., \u201ca plush toy\nof a corgi policeman\u201d). We empirically set \u03c91 = 1 and \u03c92 = 0.5. Our approach yields satisfactory\nresults, aligning edited objects with new prompts while maintaining other attributes.\n6\nCONCLUSION, DISCUSSION, AND LIMITATION\nIn this paper, we introduce Classifier Score Distillation (CSD), a novel framework for text-to-3D\ngeneration that achieves state-of-the-art results across multiple tasks. Our primary objective is to\ndemonstrate that the classifier score, often undervalued in the practical implementations of SDS,\nmay actually serve as the most essential component driving the optimization. The most significant\nimplication of our results is to prompt a rethinking of what truly works in text-to-3D generation in\npractice. Building on the framework of CSD, we further establish connections with and provide new\ninsights into existing techniques.\nHowever, our work has certain limitations and opens up questions that constitute important direc-\ntions for future study. First, while our empirical results show that using CSD leads to superior\ngeneration quality compared to SDS, we have not yet been able to formulate a distribution-based\nobjective that guides this optimization. Second, despite achieving photo-realistic results in 3D tasks,\nwe find that the application of CSD to 2D image optimization results in artifacts. We hypothesize\nthat this discrepancy may arise from the use of implicit fields and multi-view optimization strategies\nin 3D tasks. Investigating the underlying reasons is of significant interest in the future. Despite\nthese limitations, we believe our findings play a significant role in advancing the understanding of\ntext-to-3D generation and offer a novel perspective for this field.\n9\nREFERENCES\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\nJonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 5470\u20135479, 2022.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects, 2022.\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-\nHao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified\nframework for 3d content generation. https://github.com/threestudio-project/\nthreestudio, 2023.\nAyaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa.\nInstruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789, 2023.\nAmir Hertz, Kfir Aberman, and Daniel Cohen-Or.\nDelta denoising score.\narXiv preprint\narXiv:2304.07090, 2023.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\nClipscore: A\nreference-free evaluation metric for image captioning, 2022.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nYukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dream-\ntime:\nAn improved optimization strategy for text-to-3d content creation.\narXiv preprint\narXiv:2306.12422, 2023.\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 867\u2013876, 2022.\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Ad-\nvances in neural information processing systems, 34:21696\u201321707, 2021.\nSamuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila.\nModular primitives for high-performance differentiable rendering. ACM Trans. Graph., 39(6):\n194:1\u2013194:14, 2020. doi: 10.1145/3414685.3417861. URL https://doi.org/10.1145/\n3414685.3417861.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d con-\ntent creation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 300\u2013309, 2023.\nTiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pre-\ntrained models, 2023.\n10\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 12663\u201312673, 2023.\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven\nneural stylization for meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 13492\u201313502, 2022.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications\nof the ACM, 65(1):99\u2013106, 2021.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1\u201315,\n2022.\nAaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,\nGeorge Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast\nhigh-fidelity speech synthesis. In International conference on machine learning, pp. 3918\u20133926.\nPMLR, 2018.\nDong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for\ncompositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track (Round 1), 2021.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-\nguided texturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.\nLaion-5b: An open large-scale dataset for training next generation image-text models, 2022.\nRuizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, and Yebin\nLiu. Control4d: Dynamic portrait editing by learning 4d gan from 2d diffusion-based editor.\narXiv preprint arXiv:2305.20082, 2023.\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetra-\nhedra: a hybrid representation for high-resolution 3d shape synthesis. In Marc\u2019Aurelio Ran-\nzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan\n(eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neu-\nral Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp.\n6087\u20136101, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n30a237d18c50f563cba4531f1db44acf-Abstract.html.\n11\nYichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation, 2023.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learn-\ning, pp. 2256\u20132265. PMLR, 2015.\nYang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribu-\ntion, 2020.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-Based Generative Modeling through Stochastic Differential Equations, 2021.\nStabilityAI. Deepfloyd. https://huggingface.co/DeepFloyd, 2023.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jaco-\nbian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12619\u201312629, 2023a.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023b.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543, 2023.\nMinda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Zhipeng Hu, Changjie Fan,\nand Xin Yu. Efficientdreamer: High-fidelity and robust 3d creation via orthogonal-view diffusion\nprior, 2023.\nJoseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d with advanced diffusion guidance.\narXiv preprint arXiv:2305.18766, 2023.\nJingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and Guanbin Li. Dreameditor: Text-driven 3d\nscene editing with neural fields. arXiv preprint arXiv:2306.13455, 2023.\n12\nA\nAPPENDIX\nA.1\nRELATED WORKS\nText-Guided 3D Generation Using 2D Diffusion Models.\nUtilizing 2D diffusion models pre-\ntrained on massive text-image pairs (Schuhmann et al., 2022) for text-guided 3D generation has\nbecome mainstream. As the pioneering work of this field, DreamFusion (Poole et al., 2022) pro-\nposes the Score Distillation Sampling (SDS) technique (also known as Score Jacobian Chaining\n(SJC) (Wang et al., 2023a)), which is able to \u201cdistill\u201d 3D information from 2D diffusion models.\nSDS has been widely used and discussed in the following works (Lin et al., 2023; Metzer et al.,\n2023; Chen et al., 2023; Wang et al., 2023b; Huang et al., 2023; Zhu & Zhuang, 2023; Shi et al.,\n2023), which attempt to improve DreamFusion in many different ways. Magic3D (Lin et al., 2023)\nand Fantasia3D (Chen et al., 2023) investigate the possibility of optimizing the mesh topology in-\nstead of NeRF for efficient optimization on high-resolution renderings. They operate on 512 \u00d7 512\nrenderings and achieve much more realistic appearance modeling than DreamFusion which is op-\ntimized on 64 \u00d7 64 NeRF renderings. MVDream Shi et al. (2023) alleviates the Janus problem of\nthis line of methods by training and distilling a multi-view text-to-image diffusion model using ren-\nderings from synthetic 3D data. DreamTime (Huang et al., 2023) demonstrates the importance of\ndiffusion timestep t in score distillation and proposes a timestep annealing strategy to improve the\ngeneration quality. HiFA (Zhu & Zhuang, 2023) re-formulates the SDS loss and points out that it\nis equivalent to the MSE loss between the rendered image and the 1-step denoised image of the dif-\nfusion model. ProlificDreamer (Wang et al., 2023b) formulates the problem as sampling from a 3D\ndistribution and proposes Variational Score Distillation (VSD). VSD treats the 3D scene as a random\nvariable instead of a single point as in SDS, greatly improving the generation quality and diversity.\nHowever, VSD requires concurrently training another 2D diffusion model, making it suffer from\nlong training time. 2D diffusion models can also be used for text-guided 3D editing tasks (Haque\net al., 2023; Shao et al., 2023; Zhuang et al., 2023), and texture synthesis for 3D models (Richardson\net al., 2023; Michel et al., 2022). In this paper, we provide a new perspective on understanding the\nSDS optimization process and derive a new optimization strategy named Classifier Score Distilla-\ntion (CSD). CSD is a plug-and-play replacement for SDS, which achieves better text alignment and\nproduces more realistic appearance in 3D generation. We also show the astonishing performance of\nCSD in texture synthesis and its potential to perform 3D editing.\nA.2\nCSD FOR EDITING\nOur method enables text-guided 3D editing. We observe that our formulation bears some resem-\nblance to the Delta Denoising Score (DDS) (Hertz et al., 2023). Here, we demonstrate that these are\ntwo fundamentally different formulations and discuss their relations.\nSpecifically, DDS is proposed for 2D image editing. Given an initial image \u02c6x, an initial prompt \u02c6y,\nand a target prompt y, assume that the target image to be optimized is x. It utilizes SDS to estimate\ntwo scores. The first score is calculated using \u2207\u03b8LSDS(xt, y, t), while the second is determined\nby \u2207\u03b8LSDS(\u02c6xt, \u02c6y, t). They observed that relying solely on the first score would lead to blurred\noutputs and a loss of detail. Therefore, they decompose it into two implicit components: one is\na desired direction guiding the image toward the closest match with the text, and another is an\nundesired component that interferes with the optimization process, causing some parts of the image\nto become smooth and blurry (Hertz et al., 2023). Their key insight is the belief that the score from\n\u2207\u03b8LSDS(\u02c6xt, \u02c6y, t) can describe the undesired component. Thus, they perform editing by subtracting\nthis score during optimization. Using our notions, their editing objective is expressed as:\n\u03b4dds\nx\n= \u03b4sds\nx (xt; y, t) \u2212 \u03b4sds\nx (\u02c6xt; \u02c6y, t)\n(13)\nSince the classifier-free guidance is still used here for both two SDS terms, we can look closer and\nformulate it as:\n\u03b4dds\nx\n= [\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(\u02c6xt; \u02c6y, t)]+\u03c9\u00b7[\u03f5\u03d5(xt; y, t) \u2212 \u03f5\u03d5(xt; t)]\n|\n{z\n}\n\u03b4cls\nx\n\u2212\u03c9\u00b7[\u03f5\u03d5(\u02c6xt; \u02c6y, t) \u2212 \u03f5\u03d5(\u02c6xt; t)] (14)\n13\nDDS\nDDS w/o \ud835\udeff!\"#$\nDDS with \ud835\udf14 = 0\nCSD\n\u201cpanda\u201d\n\u201cSquirrel snowboarding\u201d\nFigure 7: Comparisons between DDS variants and CSD on 2D image editing tasks, where the panda\nis edited to be a squirrel.\nNote that the above gradient update rule actually consists of our classifier score \u03b4cls\nx , so it is interest-\ning to see if the essential part to drive the optimization is still this component. To demonstrate this,\nwe do several modifications to ablate the effect of \u03b4cls\nx on 2D image editing. As shown in Fig. 7,\nwithout a guidance weight, DDS cannot do effective optimization. Besides, we ablate the effect of\nonly removing \u03b4cls\nx and find the optimization yet cannot be successful. Instead, solely relying on \u03b4cls\nx\ncan achieve superior results. Thus, we hypothesize the essential part of driving the optimization is\n\u03b4cls\nx . The insight here is also more straightforward than that of DDS and we do not need the reference\nimage to provide a score that is considered an undesired component.\nA.3\nADDITIONAL RESULTS\nWe provide an extensive gallery of video results in the supplementary material. The gallary\nfeatures a comprehensive comparison between our method and existing approaches. Specifically,\nthe supplementary files include 81 results for the text-guided 3D generation task and 20 results for\nthe text-guided texture synthesis task, along with comparisons with baseline methods. Please refer\nto geometry.html for comparisons on 3D generation and texture.html for comparisons on\ntexture synthesis.\n14\n"
  },
  {
    "title": "Skywork: A More Open Bilingual Foundation Model",
    "link": "https://arxiv.org/pdf/2310.19341.pdf",
    "upvote": "4",
    "text": "Skywork: A More Open Bilingual Foundation Model\nTianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei L\u00fc, Rui Hu\nChenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang\nLei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang\nShuicheng Yan, Han Fang, Yahui Zhou\u2217\nSkywork Team, Kunlun Inc.\nAbstract\nIn\nthis\ntechnical\nreport,\nwe\npresent\nSkywork-13B, a family of large language\nmodels (LLMs) trained on a corpus of over\n3.2 trillion tokens drawn from both English\nand Chinese texts. This bilingual founda-\ntion model is the most extensively trained\nand openly published LLMs of comparable\nsize to date. We introduce a two-stage train-\ning methodology using a segmented corpus,\ntargeting general purpose training and then\ndomain-specific enhancement training, re-\nspectively. We show that our model not\nonly excels on popular benchmarks, but\nalso achieves state of the art performance\nin Chinese language modeling on diverse\ndomains. Furthermore, we propose a novel\nleakage detection method, demonstrating\nthat data contamination is a pressing is-\nsue warranting further investigation by the\nLLM community. To spur future research,\nwe release Skywork-13B along with check-\npoints obtained during intermediate stages\nof the training process. We are also releas-\ning part of our SkyPile corpus, a collection\nof over 150 billion tokens of web text, which\nis the largest high quality open Chinese pre-\ntraining corpus to date. We hope Skywork-\n13B and our open corpus will serve as a\nvaluable open-source resource to democra-\ntize access to high-quality LLMs.\n1\nIntroduction\nNatural Language Processing (NLP), a vital\nbranch of artificial intelligence, has experienced\na transformative surge in recent years. Pivotal\nto this revolution has been the advent and ad-\nvancement of large language models (LLMs)\n(Ouyang et al., 2022; OpenAI, 2023; Bubeck\net al., 2023; Chowdhery et al., 2022; Anil et al.,\n2023; Touvron et al., 2023a,b). These complex\ncomputational structures, composed of billions\nof parameters, are capable of understanding,\n\u2217 Email: {forename}.{surname}@kunlun-inc.com\ngenerating, and translating human language\nwith an unprecedented degree of accuracy and\nsophistication. However, the proliferation of\nthese models has also been accompanied by a\ngrowing trend towards commercialization and\na lack of transparency, a phenomenon that is\nincreasingly influencing the dynamics of the\nopen-source community.\nHistorically, the open-source community has\nthrived on the principles of collaboration, trans-\nparency, and unrestricted sharing of ideas.\nHowever, as the commercial potential of LLMs\nhas been recognized, this openness has begun\nto diminish. The reality is that many organi-\nzations only make model checkpoints publicly\naccessible, while withholding vital information\non model reproduction. This practice signifi-\ncantly hampers the progress of the field.\nIn an effort to revive the spirit of the open-\nsource community and contribute to the on-\ngoing dialogue about transparency in AI, we\npresent Skywork-13B: a family of bilingual large\nlanguage models with 13 billion parameters,\ntrained on a colossal corpus of more than 3.2\ntrillion tokens drawn from both English and\nChinese texts. To our knowledge, our Skywork-\n13B is the most thoroughly trained family of\nopen LLMs of comparable size to date.\nIn this technical report, we offer a compre-\nhensive disclosure of the Skywork-13B devel-\nopmental journey. We detail the composition\nof our training data, provide insights into the\nevolutionary trajectory of the model\u2019s abilities\nduring training, and share methodologies that\ncould be employed to enhance model ability in\nspecific domains. We believe that such an open\napproach not only aids in the reproducibility\nof our work but also provides a valuable re-\nsource for other researchers seeking to explore\nand expand the capabilities of large language\nmodels. This technical report is also a call to\n1\narXiv:2310.19341v1  [cs.CL]  30 Oct 2023\naction for renewed transparency in the field of\nNLP. Through it, we hope to inspire a return to\na more collaborative, open-source community,\nwhere progress is not hampered by commer-\ncial considerations but propelled by collective\nintelligence and shared wisdom.\nOur contributions are the following:\n\u2022 We release Skywork-13B1, a family of LLMs\nthat is the most extensively trained and\nopenly published LLMs of comparable size\nto date. Our Skywork-13B family includes\n1) Skywork-13B-Base, a strong foundation\nmodel with state of the art Chinese language\nmodeling capability, and 2) Skywork-13B-\nChat, a fined-tuned version optimized for\nconversation2.\n\u2022 We disclose detailed information on the\ntraining process and data composition. We\nalso release intermediate checkpoints, which\nprovide a valuable resource for understand-\ning how the model\u2019s capabilities develop over\nthe course of training. It enables other re-\nsearchers to leverage these checkpoints for\ntheir specific use-cases.\n\u2022 We release a portion of our high quality\ntraining corpus, totaling more than 150 bil-\nlion tokens. To our knowledge, this is the\nlargest open Chinese corpus for language\nmodel pre-training to date.\n\u2022 We develop a novel method that detects the\nlevel of in-domain data usage during the\ntraining stage. To facilitate reproduction of\nthe experiments presented in this report, we\nhave released the relevant data.\n2\nMethodology\n2.1\nTwo Pre-training Stages\nIn order to train Skywork-13B, we constructed\nSkyPile (see Section 3.1), a massive training\ncorpus primarily constituted by publicly acces-\nsible web pages. We identified a small subset\nof SkyPile, encompassing exercises and solu-\ntions that span a broad spectrum of subjects\nfrom primary to graduate school. This includes\n1Github\nrepository:\nhttps://github.com/\nSkyworkAI/Skywork.\n2In this technical report we focus on the development\nof the base model. Details on Skywork-13B-Chat can\nbe found in our Github repository.\ncoding problems, national exam questions, text-\nbook exercises, and others. Given the majority\nof these exercises are STEM-related, we hence-\nforth refer to this subset and its complement as\nSkyPile-STEM and SkyPile-Main, respectively.\nRather than training the Skywork-13B foun-\ndation model directly on SkyPile as a whole,\nwe adopted a two-stage training approach. The\nfirst stage, which constitutes the primary pre-\ntraining phase, involves training the model\nfrom scratch on SkyPile-Main.\nIn the sec-\nond stage, our Skywork-13B is enriched with\nSTEM-related domain knowledge and problem-\nsolving skills through continual pre-training on\nSkyPile-STEM. To circumvent the potential\nissue of catastrophic forgetting, this continual\npre-training is performed on a mix of SkyPile-\nSTEM and SkyPile-Main, rather than exclu-\nsively on SkyPile-STEM.\nThe decision to segregate Stage-1 and Stage-\n2 pre-training serves a dual purpose. Firstly,\nwe acknowledge that a significant proportion of\nthe samples from SkyPile-STEM are, by their\nnature, supervised data. Those data are closely\nrelated to popular benchmarks such as CEVAL\n(Huang et al., 2023), MMLU (Hendrycks et al.,\n2021) and GSM8K (Cobbe et al., 2021), and can\nbe utilized in a supervised fine-tuning (SFT)\nprocess to directly enhance model performance\non related downstream tasks. In this context,\nthe separation between Stage-1 and Stage-2\ntraining enables us to more effectively assess\nthe impacts of general-purpose pre-training (on\nweb texts) and targeted pre-training (on in-\ndomain/supervised data). Such insights could\ninform future data collection and compilation\nstrategies for foundational model training.\nSecondly,\nby restricting first stage pre-\ntraining to general-purpose data, we are able\nto produce a version of foundation model as an\nalternative to the one with targeted enhance-\nment. While the latter demonstrates superior\nperformance on certain downstream tasks, it\nis less capable in language modeling of natural\ntexts. We posit that this alternative is a valu-\nable contribution to the community, given its\npotential to excel in applications that do not\nrequire STEM-related competencies.\n2.2\nTraining Progress Monitoring\nIt is of vital importance to monitor and assess\nprogress made during pre-training in real-time.\n2\nExisting methods such as monitoring training\nloss and benchmark results on intermediate\ncheckpoints, however, have their limitations.\nThe main issue of monitoring training loss\nlies in that its effectiveness comes into question\nwhen considering the potential of overfitting.\nThe training loss is equivalent to validation\nloss only if the training data is utilized exactly\nonce (i.e., in one epoch).\nYet, in practical\nscenarios of training LLMs, high-quality data\noften go through the training process multi-\nple times (Taylor et al., 2022; Touvron et al.,\n2023a; Rozi\u00e8re et al., 2023; Gunasekar et al.,\n2023; Li et al., 2023b). Besides, even after ex-\nplicit de-duplication, there may still exist signif-\nicant amount of duplicated data in the training\nset (Soboleva et al., 2023; Abbas et al., 2023).\nIn either cases, solely relying on training loss\ncan lead to overlooking the issue of overfitting,\nthereby producing overly optimistic estimates\nof model performance. The top left subplot\nin Figure 3 illustrates the trajectory of the\npre-training loss for our Skywork-13B model.\nConsistent with findings reported in (Touvron\net al., 2023a,b; Baichuan Inc., 2023), the loss\ndemonstrates a steady decline throughout the\ntraining process. However, an observation not\ndisclosed in these cited works is the behavior of\nthe validation loss on held-out sets. From the\nfigure it can be clearly seen that the validation\nlosses seem to level off as training approaches\nits final stages.\nBenchmarking based on intermediate check-\npoints is another common monitoring approach\n(Touvron et al., 2023a; Baichuan Inc., 2023).\nNevertheless, it presents several challenges.\nFirstly, there is a high variance in benchmark\nresults, which can lead to unstable and unreli-\nable assessments of training progress. Secondly,\nbenchmark results are not sensitive to minor\nprogress in training. This insensitivity makes\nit difficult to accurately track gradual improve-\nments during the training process.\nBesides,\nweaker models do not follow instructions well.\nHence benchmark results may not accurately\nreflect their true learning progress or poten-\ntial. Finally, an inconvenience posed by most\nbenchmarks is the necessity for model genera-\ntion. This process is notably resource-intensive,\ndemanding substantial computational power.\nDuring the pre-training of Skywork-13B, we\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\nValidation Loss\n35\n40\n45\n50\n55\n60\nAverage T\nask Metric\nValidation Loss vs. Average T\nask Metric\nFigure 1: Validation loss on English web texts\nvs.\naverage task metric during the pre-training\nof Skywork-13B. The tasks include BoolQ (Clark\net al., 2019), PIQA (Bisk et al., 2019), Winogrande\n(Sakaguchi et al., 2021), TriviaQA (Joshi et al.,\n2017) and RACE (Lai et al., 2017).\nembrace the method of monitoring the language\nmodeling loss across numerous reserved valida-\ntion sets, each reflecting a distinct data dis-\ntribution. More specifically, we have created\nseparate validation sets for code, academic pub-\nlications, social media posts, web texts in Chi-\nnese and English, among others. Conventional\nmonitoring metrics are also utilized, but they\nserve merely as supplementary tools. In Figure\n1 we plot the curve of language model vali-\ndation loss on English web texts against the\naverage metric of several English downstream\ntasks. It is apparent that there is a very high\ncorrelation between the two quantities, showing\nthat validation loss can serve as a valid proxy\nmetric for downstream task performance. In\nthe context of LLM pre-training, this approach\nalso yields several other benefits:\n\u2022 Ease of construction: Crafting multiple val-\nidation sets is a relatively effortless task.\nThis enables the evaluation of a model\u2019s lan-\nguage modeling performance across varied\ndomains.\n\u2022 Simplicity in computation: Calculation of\nvalidation loss is straightforward, signifi-\ncantly reducing the computational and lo-\ngistical overhead associated with tracking\nmodel training.\n\u2022 High sensitivity to training progress: Valida-\ntion loss is finely attuned to the progression\nof training, thereby offering a more detailed\n3\nperspective on how models evolve and im-\nprove over time.\n\u2022 Model-agnosticism: Validation loss is indif-\nferent to the composition of the training\ncorpus or the model architecture. It allows\nfor comparison not only between different\ncheckpoints produced within a single train-\ning session, but also across varied models\nfrom the community. This ensures a consis-\ntent and equitable basis for model compari-\nson.\nNote that monitoring the validation loss on\na held-out set sharing the same distribution\nas the training set is a ubiquitous practice in\nmachine learning. However, the observation\nof validation loss across multiple held-out sets,\neach with deliberate, unique distributions, is\nnot common. We also note that the perspective\nasserting the primacy of language modeling\nloss as the paramount performance metric for\nmodels is not a recent revelation. This principle\nhas been either explicitly or implicitly adopted\nin a number of research studies, as exemplified\nin (Kaplan et al., 2020; Hoffmann et al., 2022;\nAnil et al., 2023; Xia et al., 2023; Del\u00e9tang\net al., 2023).\n3\nPre-training\n3.1\nSkyPile Corpus\nIn order to train Skywork-13B, we build SkyP-\nile, a vast, high quality corpus comprising more\nthan 6 trillion tokens. A segment of the corpus,\ncomprising over 150 billion tokens of web text,\nhas been open sourced to facilitate research and\ntraining on Chinese LLMs3.\nOur SkyPile is an amalgamation of several\nsources, the overwhelming majority of which\nis gleaned from publicly accessible channels.\nNumerous prior research works, exemplified\nby initiatives such as LLaMA (Touvron et al.,\n2023a) and RefinedWeb (Penedo et al., 2023),\nhave substantiated the notion that publicly ac-\ncessible web data can yield exceptionally high-\nquality LLMs. In alignment with this empirical\nevidence, we subscribe to the premise of leverag-\ning publicly accessible webpages as our primary\nsource for training data.\n3huggingface.co/datasets/Skywork/\nSkyPile-150B\nThe construction of SkyPile is characterized\nby a dedicated emphasis on two primary dimen-\nsions: text quality and information distribution.\nOur data processing pipeline, inspired by (Wen-\nzek et al., 2020; Touvron et al., 2023a; Penedo\net al., 2023), incorporates the following stages:\n\u2022 Structural Extraction:\nDue to the pre-\ndominant source of our dataset being pub-\nlicly accessible web pages, the objective of\nthe first stage is the extraction of pertinent\ncontent while concurrently expunging extra-\nneous textual elements that are deemed non-\ncontributory to the training of our language\nmodel, e.g. these superfluous components in-\nclude navigational bars, site-specific contact\ninformation, disjunctive title texts devoid\nof substantive content, etc. Subsequent to\nthis culling process, the retained informa-\ntion predominantly consists of contiguous,\nmedium to long-form textual passages.\n\u2022 Distribution Filtering:\nIn the pursuit\nof cultivating a profoundly adept LLM, the\nmodel\u2019s exposure must encompass a diverse\narray of content spanning an extensive spec-\ntrum of domains. Prior endeavors within the\nfield have entailed the task of assigning cat-\negorical labels to each individual document\nor webpage, thereby manually dictating the\ncomposition of the training corpus. How-\never, we posit that the corpus employed for\nLLM training has burgeoned to such an ex-\ntent that the knowledge it encapsulates can\nnot be compartmentalized discretely. Conse-\nquently, eschewing a label-centric approach,\nour methodology centers on benchmarking\nthe semantic affinities existing between tex-\ntual segments, thereby identifying and omit-\nting those text blocks characterized by an\nexceedingly high recurrence rate.\n\u2022 Deduplication:\nDeduplication has\ndemonstrated its remarkable efficacy in en-\nhancing the overall quality of a training cor-\npus, and it has found extensive application\nin virtually all prominent datasets (Hernan-\ndez et al., 2022; Kandpal et al., 2022; Abbas\net al., 2023; Lee et al., 2022). Within the\nframework of SkyPile, we regard deduplica-\ntion as an integral component of the Distri-\nbution Filtering process. When considering\nthe broader perspective, it becomes evident\n4\nthat duplication constitutes a paramount\nfactor influencing the semantic distribution\nof a corpus. Consequently, the techniques\nand strategies we employed during the dis-\ntribution filtering phase autonomously elim-\ninated a substantial portion of duplicated\ncontent.\n\u2022 Quality Filtering:\nIn this phase, we\ndeploy the CCNet (Wenzek et al., 2020)\npipeline to perform two critical filtration\ntasks: the elimination of content of inferior\nquality and the exclusion of pages that are\nneither in English nor Chinese. We trained a\nbinary classifier that predicts the likelihood\nthat a given webpage is suitable for inclu-\nsion as a reference within the Wikipedia cor-\npus. The outcome of this stage is organized\ninto distinct quality-based categories, and\nwe retain exclusively the high quality groups,\nopting to discard the remaining groups in\nits entirety.\nAbove we described our pre-processing pipeline\nfor natural text. As for Github content, we em-\nploy an approach that is similar to (Together\nComputer, 2023). We have devised a collection\nof straightforward yet efficacious heuristics, en-\ncompassing criteria such as line length filtration\nand alphanumeric thresholds, designed to dis-\ncern and exclude content of low quality. Our cri-\nteria are specifically oriented toward enhancing\ncontent quality, as opposed to merely curbing\nits volume. Notably, in contrast to prevailing\npractices that involve the wholesale removal of\na significant portion of json, xml, yaml, and\nhtml content, we have made a deliberate choice\nto retain a judiciously proportionate represen-\ntation of these data formats.\nNote that in pursuit of harmonizing the\nmodel\u2019s proficiency in both English and Chi-\nnese, we include in SkyPile a curated high-\nquality parallel corpora. This data is meticu-\nlously structured to pair a complete English\nparagraph with its corresponding Chinese coun-\nterpart, ensuring a seamless alignment of lin-\nguistic capabilities between the two languages.\n3.2\nTraining Data Composition\nOur Skywork-13B is pre-trained for 3.2 trillion\ntokens, sampled from SkyPile. Texts from cer-\ntain sources are deemed as of high quality, e.g.\nCategory\nPercentage\nEnglish\nWebpages\n39.8%\nBooks\n3.6%\nAcademic Papers\n3.0%\nEncyclopedia\n0.5%\nMiscellany\n2.9%\nChinese\nWebpages\n30.4%\nSocial Media\n5.5%\nEncyclopedia\n0.8%\nMiscellany\n3.1%\nOther Lang.\nEncyclopedia\n2.4%\nCode\nGithub\n8.0%\nTable 1: Breakdown of training data in Stage-1\npre-training of Skywork-13B.\nWikipedia, hence have undergone upsampling.\nHowever, we generally stick to the rule that the\nnumber of repetition does not exceed five, as is\nrecommended by recent studies (Taylor et al.,\n2022; Muennighoff et al., 2023).\nWe report in Table 1 a breakdown of the\nconstituent components of the training tokens\nduring Stage-1 pre-training. The training to-\nkens are primarily composed of English and\nChinese texts, constituting 49.8% and 39.6% of\nthe data, respectively. Code contributes 8.0%\nto the total, with texts in other languages ac-\ncounting for the remaining 2.4%. The category\nlabeled as \u201cmiscellany\u201d encompasses a diverse\nrange of texts, including but not limited to, le-\ngal articles, court documents, company annual\nreports, and classical literature.\n3.3\nTokenizer\nWe tokenize the data using byte-pair encoding\n(BPE) as implemented in SentencePiece (Kudo\nand Richardson, 2018), following the approach\nof LLaMA (Touvron et al., 2023a). Since our\nmodel is intended to be English-Chinese bilin-\ngual, we extend the original vocabulary of\nLLaMA, which primarily consists of latin-based\nwords and subwords, with frequently used Chi-\nnese characters and words.\nSpecifically, we\nadd 8000 single-character tokens from BERT\u2019s\nvocabulary (Devlin et al., 2019) to LLaMA\u2019s\nvocabulary. We further expand the vocabu-\nlary with 25k frequent Chinese multi-character\nwords. This results in a total vocabulary size\nof 65,536 tokens, of which 17 are reserved as\n5\nspecial symbols.\nAs in LLaMA, we split all numbers into indi-\nvidual digits, and fall back to bytes to decom-\npose unknown UTF-8 characters.\nCategory\nSize\nLatin based words & subwords\n32000\nChinese characters & Unicode symbols\n8000\nChinese words\n25519\nReserved symbols\n17\nTotal\n65536\nTable 2: Breakdown of the vocabulary used in\nSkywork-13B.\n3.4\nArchitecture\nOur Skywork-13B is based on the transformer\narchitecture (Vaswani et al., 2017), consisting\nof stacks of transformer-decoder layers. In con-\ntrast to the original transformer model, we have\nincorporated several modifications, inspired by\nLLaMA (Touvron et al., 2023a,b). Our pre-\nliminary experiments, as illustrated in Figure\n2, validate these changes, demonstrating the\nimproved performance they confer. Details on\nthis experiment can be found in Appendix A.\nWhile our network architecture takes after\nthe LLaMA model to a great extent, there ex-\nists a notable difference in our preference for a\ndeeper, yet narrower, network. A comparative\nexploration of the Skywork-13B and LLaMA2-\n13B network configurations is presented in Ta-\nble 3.\nThe specific modifications made are de-\nscribed in detail below.\n\u2022 Positional Embedding: We use Rotary\nPositional Embedding (RoPE) (Su et al.,\n2022), that was motivated by its extensive\nadoption in various prominent large lan-\nguage models, such as LLaMA and PaLM,\nas well as its demonstrated effectiveness in\nextending the length of context windows,\nas evidenced by recent studies (Chen et al.,\n2023; Rozi\u00e8re et al., 2023; Xiong et al., 2023).\n\u2022 Layer Normalization: We replaced the\nconventional layer normalization with RM-\nSNorm (Zhang and Sennrich, 2019). Addi-\ntionally, we adopted pre-normalization in\neach layer instead of post-normalization,\nwhich has been shown to enhance the train-\ning stability of transformer models.\n0\n50\n100\n150\n200\nT\nokens (B)\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\n2.4\nT\nraining Loss\nGPT\n-7B\nLLaMA-7B\nFigure 2: Preliminary Experiments: Comparison\nof conventional GPT architecture and more recent\nLLaMA architecture. For each of the two trans-\nformer variants, a model with 7 billion parameters\nis trained from Scratch on 200 Billion Tokens. The\nplot clearly shows that the LLaMA architecture\nachieves a lower training loss than GPT, demon-\nstrating the former\u2019s superiority.\n\u2022 Activation: We employed the SwiGLU acti-\nvation function (Shazeer, 2020). In line with\nestablished conventions in prior studies, we\nreduced the dimension of the feed-forward\nnetwork (FFN) from four times the hidden\nsize to eight-thirds of the hidden size. This\nadjustment was made to maintain parity be-\ntween the total parameters in a layer and\nthose in the vanilla transformer layer.\nLLaMA2-13B\nSkywork-13B\nVocab. Size\n32,000\n65,536\nHidden Dim.\n5,120\n4,608\nFFN Dim.\n13,696\n12,288\nHead Dim.\n128\n128\nNum. Heads\n40\n36\nNum. Layers\n40\n52\nSeq. Len.\n4,096\n4,096\n#Tokens per Batch\n4M\n16M\nPeak LR\n3e-4\n6e-4\nMinimum LR\n3e-5\n6e-5\nTable 3: Comparisons in architecture and important\nhyper-parameters of Skywork-13B and LLaMA2-\n13B.\n3.5\nInfrastructure\nOur Skywork-13B is trained on a cluster of 64\nNVIDIA-HGX-A800 nodes, a total of 512 A800-\n80G SXM GPUs. Each node in the cluster is\noutfitted with high-speed 400GB/s NVLinks\n6\nfor intra-node communication and an 800Gb/s\nRoCE network for inter-node connectivity. Our\ntraining framework is based on Megatron-LM\n(Shoeybi et al., 2020) library, designed to sup-\nport the stable, prolonged training of large-scale\nmodels, accommodating thousands of GPUs\nand model sizes in the order of hundreds of\nbillions parameters.\nConsidering the relatively moderate size of\nour Skywork-13B model, we have avoided\nthe use of GPU memory optimization tech-\nniques and parallel schemes that could impede\nspeed.\nThese include Tensor Model Paral-\nlelism (Shoeybi et al., 2020), Sequence Paral-\nlelism (Korthikanti et al., 2022), ZeRO-Stage2\n(Rajbhandari et al., 2020), and Checkpointing\n(Chen et al., 2016).\nInstead, we have lever-\naged Data Parallelism (DP) with ZeRO-1 (Ra-\njbhandari et al., 2020) and Pipeline Parallelism\n(PP) (Narayanan et al., 2021) as the primary\nparallelization strategies for training Skywork-\n13B. ZeRO-1 substantially diminishes the GPU\nmemory footprint of the Adam optimizer state\nwithout increasing the burden on intercommu-\nnication. Pipeline Parallelism offers memory\noptimization at a minimal communication over-\nhead, which decreases as the gradient accumu-\nlation step increases, thereby mitigating the\nslowdown of all-reduce as DP Size increases.\nRegarding operator optimization, we adopted\nFlash Attention V2 (Dao et al., 2022; Dao,\n2023), a strategy that both optimizes GPU\nmemory and expedites the training process.\nUpon extensive preliminary experiments, we\nhave decided to adopt the combination of\nDP256, PP2, and ZeRO-1 as our distributed\ntraining strategy for Skywork-13B. With this\nconfiguration, we achieved a token throughput\nof 1873 per GPU per second and a model flops\nutilization (MFU) of 56.5%. An overview of\nthese experiments is provided in Appendix B.\nThe training process of Skywork-13B spanned\na total of 39 days.\n3.6\nTraining Details\nAs outlined in Section 2.1, the pre-training of\nSkywork-13B is executed in two stages:\n\u2022 Stage-1: General purpose pre-training on\nSkyPile-Main.\n\u2022 Stage-2:\nSTEM-oriented continual pre-\ntraining on SkyPile-STEM.\nIn both stages, the model is trained using the\nstandard auto-regressive language modeling ob-\njective, with context lengths fixed at 4096 to-\nkens. The AdamW optimizer (Loshchilov and\nHutter, 2019), applied for the training process,\nuses \u03b21 and \u03b22 values of 0.9 and 0.95, respec-\ntively. Throughout the pre-traning, we applied\na weight decay of 0.1 and gradient clipping\nof 1.0. Our model was trained with bfloat16\nmixed precision.\n3.6.1\nStage-1 Pre-training\nIn the first stage, our Skywork-13B model is\ntrained from scratch on SkyPile-Main for over\nthree trillion tokens. This stage consists of two\nsequential training sessions, covering the first\n0 \u223c 2T tokens and the subsequent 2 \u223c 3T\ntokens, respectively.\nOur initial plan was to train Skywork-13B\nfor two trillion tokens. We launched a train-\ning session accordingly, with a cosine learn-\ning rate schedule that gradually decays from\na peak learning rate of 6e\u22124 to a final learn-\ning rate of 6e\u22125.\nIn Figure.\n3, we report\nin red curves the evolution of language mod-\neling losses and several benchmark results of\nour Skywork-13B during this session. It is evi-\ndent that by the end of this session, the model\nhad not reached saturation. We hypothesized\nthat the model could further benefit from ad-\nditional pre-training, prompting us to launch\na secondary training session targeting an addi-\ntional one trillion tokens.\nThe second training session utilized a slightly\ndifferent composition of training data compared\nto the initial 0 \u223c 2T session, as data from\ncertain sources had been depleted and fresh\nsources were introduced. Owing to the shift\nin the training distribution, we meticulously\ntuned the learning rate parameter, eventually\ndeciding on a constant learning rate of 6e-5\nfor the 2 \u223c 3T session. In Figure. 4, we illus-\ntrate the model losses under varying learning\nrate conditions. Results indicate that a higher\nlearning rate leads to escalations in training loss\nwhich we deem too costly to reverse. The im-\npact of the second training session is depicted\nin blue curves of Fig. 3. The enhancement\nin the model\u2019s performance continues, albeit\nat a decelerating pace. Interestingly, although\nour Skywork-13B trails in the realm of English\nlanguage modeling, it significantly surpasses all\n7\n0\n1000\n2000\n3000\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\nloss\nT\nraining loss\n0\n1000\n2000\n3000\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\nVal. Loss on English T\nexts\nLLaMA-13B\nLLaMA2-13B\nXverse-13B\nBaichuan-13B\nBaichuan2-13B\nQwen-14B\nInternLM-20B\n0\n1000\n2000\n3000\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\nVal. Loss on Chinese T\nexts\nXverse-13B\nBaichuan-13B\nBaichuan2-13B\nQwen-14B\nInternLM-20B\n0\n1000\n2000\n3000\nT\nokens (B)\n20\n25\n30\n35\n40\n45\n50\nAccuracy\nCEVAL\nrandom\n0\n1000\n2000\n3000\nT\nokens (B)\n20\n25\n30\n35\n40\n45\n50\nMMLU\nra)do(\n0\n1000\n2000\n3000\nT\noke)s (B)\n\u22125\n0\n5\n10\n15\n20\n25\nGSM8K\nra)do(\nFigure 3: Trajectory of important monitoring metrics during Stage-1 pre-training. Top Left: Training\nloss. Top Middle and Right: Validation loss on English and Chinese held-out sets of web texts. The\nhorizontal dashed lines in the middle and right plots correspond to the evaluated language modeling\nloss for several similar-sized open LLMs. Bottom: Benchmark results on CEVAL, MMLU and GSM8K\nrespectively. Stage-1 pre-training consists of two sequential training sessions, represented by different\ncolors in the loss curves (red for session 0 \u223c 2T and blue for session 2 \u223c 3T).\nother comparable open LLMs in Chinese lan-\nguage modeling. In Section 4.3, we will confirm\nthat the superiority of our Skywork-13B in Chi-\nnese language modeling is not only true on our\nvalidation set, it also holds true on a number\nof test sets sourced from diverse domains.\nMore results can be found in Appendix (see\nFigure 6).\n3.6.2\nStage-2 Pre-training\nThe primary aim of Stage-2 pre-training is to\naugment the model with capabilities pertinent\nto STEM disciplines.\nThe data utilized in\nthis stage comprises an approximate 20% from\nSkyPile-STEM and 80% from SkyPile-Main,\namassing a total of roughly 130 billion tokens.\nA constant learning rate of 6e\u22125 is adopted,\nmaintaining parity with the terminal learning\nrate used in Stage-1 pre-training\nConsequent to the data distribution shift\nfrom Stage-1 to Stage-2, it becomes crucial\nto meticulously calibrate the sampling ratio\nbetween the different data sources. Initial ex-\nperiments revealed that a gradual increment in\nthe SkyPile-STEM ratio yielded the most effec-\ntive results. Therefore, for the actual Stage-2\npre-training phase, we implemented a sampling\nplan that commenced with 10% of SkyPile-\nSTEM initially, gradually escalating to a peak\nof 40% towards the conclusion of the training.\nThis training strategy proved successful in\nmaintaining the stability of the model\u2019s lan-\nguage modeling validation loss while enabling\nan optimum transfer of STEM knowledge. The\nextended training period ensures a comprehen-\nsive assimilation of STEM-related knowledge\ninto the model without causing significant dis-\nturbance to the pre-existing learned informa-\ntion.\nThe impact of Stage-2 pre-training is illus-\ntrated in Figure 5, which presents the progres-\n8\n1900\n1920\n1940\n1960\n1980\n2000\n2020\n2040\nT\nokens (B)\n1.66\n1.68\n1.70\n1.72\n1.74\nT\nraining Loss\nLR for Continual Pre-training\nLR=6e-5\nLR=1.2e-4\nLR=2.5e-4\nFigure 4: Test runs for tuning the learning rate of\nthe 2 \u223c 3T training session. It can be seen that 6e-\n5, which is the terminal learning rate from 0 \u223c 2T\ntraining session, yields the best result.\nsion of the CEVAL benchmark score. The evo-\nlution of scores on other STEM-related bench-\nmarks, such as GSM8K, mirrors a similar trend.\nImprovements in individual subjects of the CE-\nVAL can be found in Table 12 (see appendix).\n0\n25\n50\n75\n100\n125\nT\nokens (B)\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\nAccuracy\nStage-2 CEVAL\nFigure 5: Evolution of CEVAL score during Stage-2\npre-training.\n4\nEvaluation\n4.1\nBaselines\nWe compare the performance of our Skywork-\n13B\nwith\nopen\nmodels\nthat\nare\nsimi-\nlar\nin\nsize,\nincluding\nLLaMA-13B\n(Tou-\nvron et al., 2023a), LLaMA2-13B (Touvron\net al., 2023b), Baichuan-13B, Baichuan2-13B\n(Baichuan Inc., 2023), Xverse-13B (Xverse-AI,\n2023), IntermLM-20B (InternLM Team, 2023).\nA summary of these models can be found in\nTable 4.\nModel\n#Tokens\nLanguage\nOpenLLaMA-13B\n1.0T\nEnglish\nLLaMA-13B\n1.0T\nEnglish\nLLaMA2-13B\n2.0T\nEnglish\nBaichuan-13B\n1.4T\nEnglish & Chinese\nBaichuan2-13B\n2.6T\nEnglish & Chinese\nXverse-13B\n1.4T\nEnglish & Chinese\nInternLM-20B\n2.3T\nEnglish & Chinese\nSkywork-13B\n3.2T\nEnglish & Chinese\nTable 4: Details of various models. The column la-\nbeled \"#Tokens\" indicates the quantity of training\ntokens used by each model, whereas the \"Language\"\ncolumn specifies the primary languages supported\nby each model.\n4.2\nBenchmark Evaluation\nWe focus on the following popular benchmarks:\n\u2022 MMLU (Hendrycks et al., 2021): MMLU is a\nbenchmark designed to measure knowledge\nacquired during pre-training. The bench-\nmark covers 57 subjects across STEM, the\nhumanities, the social sciences, and more,\nranging in difficulty from an elementary level\nto an advanced professional level. It tests\nboth world knowledge and problem solving\nability.\n\u2022 CEVAL (Huang et al., 2023) and CMMLU\n(Li et al., 2023a): Those are Chinese bench-\nmarks that mimick MMLU. CEVAL consists\nof 13948 multi-choice questions spanning 52\ndiverse disciplines and four difficulty lev-\nels. CMMLU covers 67 disciplines that span\nfrom elementary to advanced professional\nlevels.\n\u2022 GSM8K (Cobbe et al., 2021): This dataset\nconsists of 8500 high-quality grade school\nmath word problems created by human writ-\ners. These multi-step problems require be-\ntween 2 and 8 steps to solve. GSM8K is\nusually used in benchmarking multi-step\nmathematical reasoning ability of LLMs.\nIn Table 5 we present a comparison of perfor-\nmance results from different models on these\nbenchmarks. The metrics for CEVAL, CMMLU\nand MMLU are 5-shot accuracy, while for\nGSM8K it is 8-shot accuracy.\nHigher num-\nbers indicate better performance. It can be\nseen that our Skywork-13B achieves the high-\nest score on both the CEVAL and MMLU and\n9\nGSM8K benchmarks, with scores of 60.6 and\n62.1 and 55.8 respectively. On the CMMLU\nbenchmark, Baichuan2-13B achieves the high-\nest performance with a score of 62.0.\nIn summary, our Skywork model has demon-\nstrated exceptional performance across a di-\nverse range of comprehensive benchmark tests.\nResults of individual subjects of the CEVAL\ncan be found in Table 12.\nResults of other\nbenchmarks can be found in Appendix C.\n4.3\nLanguage Modeling Results\n4.3.1\nLM as a solution to benchmark\noverfitting\nConventional benchmarks for evaluating LLMs\noften\nrely\non\nstatic\ndatasets\nof\nhuman-\nannotated examples. A core issue with this\napproach is that updating the test samples reg-\nularly is difficult and costly. Over time, the\nstatic test sets tend to be overfitted, producing\nmisleading benchmark results.\nWe propose language modeling evaluations\nas a compelling alternative. Perplexity in lan-\nguage modeling acts as a proxy metric strongly\nlinked to performance on diverse downstream\ntasks (see Figure 1). Since language modeling\nsolely requires unlabeled natural text, it elimi-\nnates the need for expensive human annotation.\nConstructing and revising language modeling\ntest sets is low-cost, as new data can be readily\nsampled from newly published content. Ad-\nditionally, if a test set becomes compromised,\nfresh test data can quickly be sampled as a\nreplacement.\n4.3.2\nConstruction of diverse LM\ntestsets\nWe compare the language modeling capabilities\nof various language models with our Skywork-\n13B, focusing on Chinese language.\nTo conduct a robust evaluation of language\nmodeling capability, we have separately col-\nlected a diverse corpus of texts from a myriad\nof websites, each labeled according to its respec-\ntive domain. The domains we cover span a wide\nspectrum, encompassing areas such as technol-\nogy, movies, finance, to name a few. These\ndomain-specific evaluation datasets have also\nbeen open-sourced for public access4.\n4Github:\nhttps://github.com/SkyworkAI/\nSkywork/tree/main/data/eval_loss\nWe ensure that every test sample consists\nof documents or user posts published after\nSeptember 1, 2023.\nThis cut-off date guar-\nantees that no test sample was inadvertently\nincluded during the pre-training of any eval-\nuated language model. Specifically, SkyPile\u2019s\ncut-off date is June 30, 2023, and the majority\nof models under evaluation were released prior\nto August 31.\nNote that while the held-out validation set\nused to monitor the training progress (as shown\nin Figure 3) of our model can also serve this pur-\npose, it has the same distribution (web texts)\nas the bulk of the training corpus, thus may\nlead to overly optimistic estimate of the ac-\ntual language modeling capability of the model.\nMore details on the sources of the test samples\nand the underlying data collection pipeline can\nbe found in Appendix D.\n4.3.3\nResults\nThe results of our language modeling eval-\nuation are presented in Table 6, where re-\nsults from ChatGLM3-6B (THUDM, 2023),\nMOSS-7B (Sun and Qiu, 2023), Baichuan2-7B\n(Baichuan Inc., 2023), Qwen-7B (Qwen Team,\n2023), InternLM-7B (InternLM Team, 2023)\nand Aquilla2-34B are also included.\nIt can be seen that our Skywork-13B model\nshows the best performance overall, obtaining\nthe lowest average perplexity score of 9.42. It\nalso exhibits the best performance across indi-\nvidual domains, achieving the lowest perplexity\nscores in tech (11.58), movie (21.84), govern-\nment (4.76), and finance (4.92) domains. It\nexcels not only in surpassing the performance\nof models of a similar size, but also in out-\nperforming significantly larger models such as\nInternLM-20B and Aquila2-34B.\nWe attribute the excellent language modeling\nperformance of our Skywork-13B to the quality\nof our training corpus. Details on rigorous data\nfiltering pipeline are described in Section 3.1.\n5\nDiscussion\nIn this section, we delve into the benefits and as-\nsociated risks of pre-training on the in-domain\ndata5 of benchmark tasks.\n5The term \u201cin-domain data\u201d is a vague one that\nrefers to any data with distribution closely resembling\nto that of the task data. For instance, the training\ndata of a task is trivially in-domain data for that task.\n10\nModel\nCEVAL\nCMMLU\nMMLU\nGSM8K\nOpenLLaMA-13B\n27.1\n26.7\n42.7\n12.4\nLLaMA-13B\n35.5\n31.2\n46.9\n17.8\nLLaMA-2-13B\n36.5\n36.6\n54.8\n28.7\nBaichuan-13B\n52.4\n55.3\n51.6\n26.6\nBaichuan2-13B\n58.1\n62.0\n59.2\n52.8\nXVERSE-13B\n54.7\n-\n55.1\n-\nInternLM-20B\n58.8\n-\n62.0\n52.6\nSkywork-13B\n60.6\n61.8\n62.1\n55.8\nTable 5: Comparison of results on popular benchmarks. Best result in each column is underlined. It can\nbe seen that our Skywork-13B consistently perform well across the different benchmarks, indicating its\noverall robustness.\nTech\nMovie\nGov.\nGame\nFinance\nGeneral\nAverage\nChatGLM3-6B\n12.48\n23.48\n5.07\n18.45\n5.67\n7.47\n10.25\nMOSS-7B\n20.83\n39.66\n11.08\n31.24\n10.59\n13.25\n18.50\nInternLM-7B\n13.43\n24.9\n5.88\n19.78\n6.17\n8.10\n11.17\nQwen-7B\n13.39\n25.16\n5.55\n19.26\n5.76\n7.78\n10.83\nBaichuan2-7B\n12.89\n23.26\n5.34\n18.36\n5.68\n7.62\n10.41\nLLaMA2-13B\n23.26\n50.66\n18.09\n32.52\n14.85\n16.55\n23.54\nXverse-13B\n12.55\n23.49\n5.20\n17.69\n5.54\n7.46\n10.19\nBaichuan-13B\n12.38\n22.46\n5.21\n17.59\n5.42\n7.37\n10.03\nBaichuan2-13B\n12.14\n21.85\n5.05\n17.15\n5.35\n7.24\n9.81\nQwen-14B\n11.90\n22.43\n4.89\n16.94\n5.24\n7.03\n9.67\nInternLM-20B\n12.34\n22.06\n5.75\n17.45\n5.73\n7.78\n10.34\nAquila2-34B\n14.62\n29.09\n5.72\n21.78\n5.83\n8.45\n11.73\nSkywork-13B\n11.58\n21.84\n4.76\n17.28\n4.92\n6.82\n9.42\nTable 6: Comparative analysis of language modeling capabilities across diverse domains. Performance is\nmeasured using perplexity (lower values is better). Underlined figures correspond to the best result in\neach column.\n5.1\nEffect of pre-training on in-domain\ndata\nPre-trained language models, or foundation\nmodels, are intended to be used in transfer\nlearning as a general purpose backbone. As\na foundation model in itself has little usage\nother than sentence completion, the quality of\na foundation model is typically evaluated in\nterms of its performance in those tasks. Appar-\nently, when it comes to improve a foundation\nmodel\u2019s quality as measured by its task perfor-\nmance, it is always far more efficient to train\nthe model on in-domain data of that task (Her-\nnandez et al., 2021; Chung et al., 2022) , as\nGPT-4 generated data with few-shot task examples can\nalso be considered as in-domain data for that task.\ncompared to general-purpose data (web texts).\nWe have shown that Stage-2 pre-training sig-\nnificantly amplifies our Skywork-13B\u2019s STEM\nrelated capabilities, leading to a substantial\nimprovement in performance on STEM-related\ntasks. Now we show that it is even possible\nto enhance a much weaker base model, i.e., an\nintermediate checkpoint, using only a fraction\nof the data and compute used in Stage-2 pre-\ntraining.\nTable 7 presents the CEVAL and GSM8K\nscores before and after pre-training on in-\ndomain data, utilizing a relatively weak model\ncheckpoint that has only undergone 0.5T pre-\ntraining. The results indicate that after pre-\ntraining with merely 1B tokens of in-domain\n11\nCEVAL\nGSM8K\nEn Loss\nZh Loss\nBefore\n28.3\n6.9\n1.86\n2.08\nAfter\n50.8\n40.7\n2.09\n2.21\n\u2206\n+22.5\n+33.8\n+0.23\n+0.13\nTable 7: The impact of pre-training on a 0.5T\ncheckpoint of Skywork-13B using only 1B tokens.\nThe training data is sourced from a subset of our\nSkyPile-STEM corpus. The columns \u201cEn Loss\u201d and\n\u201cZh Loss\u201d show the model\u2019s validation loss on held-\nout sets of English and Chinese web texts, respec-\ntively.\ndata, a weak model, initially performing only\nslightly better than random at CEVAL and\nGSM8K, can surpass the performance of our\nstrongest Skywork-13B (3T) backbone without\nin-domain pre-training. However, this comes\nat the cost of significant degradation in lan-\nguage modeling performance, as evidenced by\nthe higher loss on both tasks, shown in the two\nrightmost columns of the table.\n5.2\nPre-training on in-domain data: a\ncommon practice?\nIt is of interest to explore whether popular\nfoundational models are pre-trained on in-\ndomain data. In pursuit of this, we delve into\nthe GSM8K datasets, equipped with official\ntrain/test splits and comprehensive solutions.\nWe evaluate an LLM\u2019s language modeling loss\non three datasets drawn from the same distri-\nbution: 1) The official GSM8K training set, 2)\nThe official GSM8K test set, 3) A set composed\nof GSM8K-like samples generated by GPT-4.\nThe corresponding losses are denoted as Ltrain,\nLtest, and Lref, respectively. Theoretically, if\na language model has not been exposed to any\nof the three datasets during pre-training, the\nthree losses Ltrain, Ltest, and Lref should be ap-\nproximately equivalent. However, if the model\nhas been pre-trained on the training set or if\nthe test data has been inadvertently exposed\nduring the pre-training process, we would an-\nticipate a notable discrepancy between Ltrain,\nLtest, and Lref.\nOur results are outlined in Table 8, which\nalso reports the differences in losses \u22061 =\nLtest \u2212 Lref and \u22062 = Ltest \u2212 Ltrain.\nNo-\ntably, the \u22062 column reveals that for most\nmodels, the language modeling loss on the\nGSM8K training and test splits are almost iden-\ntical. However, models such as ChatGLM3-6B,\nBaichuan2-13B, Qwen-7B/14B, and Aquila2-\n34B display markedly lower loss on the training\nsplit than on the test split. Consequently, we\npostulate that these models may have been con-\nsiderably pre-trained on GSM8K training split\nor similar data.\nMoreover, we notice one particular anomaly\nin the \u22061 column, indicating the significantly\nlower Ltest loss compared to Lref, which is\ninteresting to further study for better under-\nstanding.\n5.3\nPre-Training or Supervised\nFine-Tuning?\nIn the era preceding the advent of LLMs such\nas GPT-4 (Bubeck et al., 2023; OpenAI, 2023)\nand Claude (Bai et al., 2022), supervised data\nfor NLP tasks was generally scarce. This was\nbecause the process of data collection and an-\nnotation was both time-consuming and costly.\nDue to the scarcity of supervised data, NLP\nresearchers rely on unsupervised pre-training\ntechniques (Mikolov et al., 2013; Peters et al.,\n2018; Radford et al., 2018; Devlin et al., 2019)\nto improve downstream task performance via\ntransfer learning, where supervised data is to\nbe used only in the fine-tuning stage. In this\ncontext, pre-training on in-domain (supervised)\ndata was pointless, as it would defeat the pur-\npose of pre-training itself (transfer learning).\nThis reality has significantly shifted, however,\nwith the emergence of powerful LLMs. This is\nbecause procuring large amounts of high quality\nsupervised/in-domain data is now as simple as\nmaking a few API requests to these LLMs, and\nit is comparatively low-cost (Wang et al., 2023;\nTaori et al., 2023). This new reality blurs the\nboundary between pre-training and supervised\nfine-tuning, making it feasible to incorporate\nsubstantial amounts of supervised data into\nthe pre-training phase (Gunasekar et al., 2023;\nLi et al., 2023b). After all, curated in-domain\ndata, whether written by human annotators\nor generated by LLM, are all form of human\nknowledge, and there is good reason for this\nknowledge to be absorbed into a foundation\nmodel.\nThat said, we believe that there is valid risk\non the practice of targeted pre-training, in that\nit compromise fairness in benchmarking. While\nthrough pre-training on in-domain data a model\n12\nLtest\nLtrain\nLref\n\u22061\n\u22062\nChatGLM3-6B\n0.99\n0.78\n0.99\n0.0\n0.21\nMOSS-7B\n1.51\n1.52\n1.49\n0.02\n\u22120.01\nInternLM-7B\n1.21\n1.12\n1.27\n-0.06\n0.09\nQwen-7B\n1.07\n0.64\n1.10\n-0.03\n0.43\nBaichuan2-7B\n1.41\n1.42\n1.36\n0.05\n\u22120.01\nLLaMA-13B\n1.41\n1.42\n1.36\n0.05\n\u22120.01\nLLaMA2-13B\n1.36\n1.38\n1.33\n0.03\n\u22120.01\nXverse-13B\n1.42\n1.43\n1.39\n0.03\n\u22120.01\nBaichuan-13B\n1.41\n1.42\n1.37\n0.04\n\u22120.01\nBaichuan2-13B\n1.09\n0.72\n1.12\n-0.03\n0.37\nQwen-14B\n1.03\n0.42\n1.14\n-0.11\n0.61\nInternLM-20B\n1.20\n1.09\n1.19\n0.01\n0.11\nAquila2-34B\n0.78\n0.39\n1.29\n\u22120.51\n0.39\nSkywork-13B\n1.01\n0.97\n1.00\n0.01\n0.04\nTable 8: We evaluate the language modeling (LM) loss on samples (a sample is a concatenation of question\nand answer) from GSM8K dataset for several foundation models. For each LLM, we compare LM loss on\nthe training split (Ltrain), the test split (Ltest), and a specially curated reference set (Lref), generated\nby GPT-4, designed to mimic the GSM8K dataset. We also reports two key metrics: \u22061 = Ltest \u2212 Lref,\nserving as an indicator of potential test data leakage during the training of the LLM, i.e., a lower value\nsuggests possible leakage; and \u22062 = Ltest \u2212Ltrain, which measures the degree of overfitting on the training\nsplit of the dataset. A higher value of \u22062 implies excessive overfitting. Outliers for both \u22061 and \u22062 are\nhighlighted in gray.\nmay excel at specific tasks, it remains uncertain\nhow well it would perform on unseen tasks. Its\ncapabilities may be overestimated based on\nthe benchmark alone, which can lead to unfair\ncomparisons between models and mislead users\nor stakeholders about the true capabilities of\nthe model.\n6\nLimitation\nOur pre-training approach for Skywork-13B in-\nvolved a two-stage process: general purpose pre-\ntraining followed by domain-specific enhance-\nment pre-training. However, it remains unclear\nwhether this methodology can produce a model\non par with, or superior to, a model trained in\none stage on a mixed corpus. Further investi-\ngation is needed to determine the comparative\neffectiveness of these pre-training approaches.\nAdditionally, we have proposed using lan-\nguage modeling loss or perplexity as proxy met-\nrics for monitoring and evaluating large lan-\nguage models. A limitation is that language\nmodeling evaluation relies on the specific distri-\nbution used to sample test data, of which there\nare infinite possibilities. While language mod-\neling perplexity over a given data distribution\nmay predict performance on some tasks, it may\nnot translate to other tasks. The correlation\nbetween language modeling and downstream\nperformance could vary across different distri-\nbutions and tasks.\n7\nConclusion\nOur work on Skywork-13B represents a sig-\nnificant leap forward in the development of\nopen large language models. We believe that\nour comprehensive and transparent approach\nto the model\u2019s development will be a valuable\nresource for researchers in the field, fostering\ncollaboration and open-source principles. Our\ntwo-stage training methodology, leveraging a\nsegmented corpus, offers a novel approach for\nenhancing model capability in specific domain,\nwhile our method of monitoring the training\nprogress provides a practical solution to the\nchallenges of tracking the improvement of these\nmodels over time.\nHowever, our work is more than just the cre-\nation of a new LLM. It is a call to action for the\nbroader NLP community, urging a return to\n13\nthe principles of fairness, transparency, and the\nsharing of ideas that have historically fueled\nprogress in the field. We hope that Skywork-\n13B will not only serve as a powerful tool for\na wide range of applications but also inspire a\nrenewed commitment to openness and coopera-\ntion in the development of future models.\nReferences\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig,\nSurya Ganguli,\nand Ari S. Morcos. 2023.\nSemdedup: Data-efficient learning at web-scale\nthrough semantic deduplication.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin\nJohnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey,\nZhifeng Chen, et al. 2023.\nPalm 2 technical\nreport. arXiv preprint arXiv:2305.10403.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom\nHenighan, Nicholas Joseph, Saurav Kadavath,\nJackson Kernion, Tom Conerly, Sheer El-Showk,\nNelson Elhage, Zac Hatfield-Dodds, Danny Her-\nnandez, Tristan Hume, Scott Johnston, Shauna\nKravec, Liane Lovitt, Neel Nanda, Catherine\nOlsson, Dario Amodei, Tom Brown, Jack Clark,\nSam McCandlish, Chris Olah, Ben Mann, and\nJared Kaplan. 2022. Training a helpful and harm-\nless assistant with reinforcement learning from\nhuman feedback.\nBaichuan\nInc.\n2023.\nBaichuan\n2:\nOpen\nlarge-scale\nlanguage\nmodels.\nhttps:\n//github.com/baichuan-inc/Baichuan2/blob/\nmain/README_EN.md.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-\nfeng Gao, and Yejin Choi. 2019. Piqa: Reasoning\nabout physical commonsense in natural language.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ro-\nnen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li,\nScott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. 2023. Sparks\nof artificial general intelligence: Early experi-\nments with gpt-4.\nShouyuan Chen, Sherman Wong, Liangjian Chen,\nand Yuandong Tian. 2023. Extending context\nwindow of large language models via positional\ninterpolation.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. 2016. Training deep nets with sublin-\near memory cost.\nAakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, et al. 2022.\nPalm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Yunxuan Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha\nBrahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen,\nAakanksha Chowdhery, Alex Castro-Ros, Marie\nPellat, Kevin Robinson, Dasha Valter, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent\nZhao, Yanping Huang, Andrew Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\nDevlin, Adam Roberts, Denny Zhou, Quoc V.\nLe, and Jason Wei. 2022. Scaling instruction-\nfinetuned language models.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surpris-\ning difficulty of natural yes/no questions.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar\nKhot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. 2018. Think you have solved\nquestion answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton,\nReiichiro Nakano, Christopher Hesse, and John\nSchulman. 2021. Training verifiers to solve math\nword problems.\nTri Dao. 2023. Flashattention-2: Faster attention\nwith better parallelism and work partitioning.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. Flashattention: Fast\nand memory-efficient exact attention with io-\nawareness.\nGr\u00e9goire Del\u00e9tang, Anian Ruoss, Paul-Ambroise\nDuquenne, Elliot Catt, Tim Genewein, Christo-\npher Mattern, Jordi Grau-Moya, Li Kevin Wen-\nliang, Matthew Aitchison, Laurent Orseau, Mar-\ncus Hutter, and Joel Veness. 2023. Language\nmodeling is compression.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long\nand Short Papers), pages 4171\u20134186, Minneapo-\nlis, Minnesota. Association for Computational\nLinguistics.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nC\u00e9sar\nTeodoro\nMendes,\nAllie\nDel\nGiorno,\n14\nSivakanth Gopi, Mojan Javaheripi, Piero Kauff-\nmann, Gustavo de Rosa, Olli Saarikivi, et al.\n2023. Textbooks are all you need. arXiv preprint\narXiv:2306.11644.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. 2021. Measuring massive multitask\nlanguage understanding.\nDanny Hernandez, Tom Brown, Tom Conerly, Nova\nDasSarma, Dawn Drain, Sheer El-Showk, Nel-\nson Elhage, Zac Hatfield-Dodds, Tom Henighan,\nTristan Hume, Scott Johnston, Ben Mann, Chris\nOlah, Catherine Olsson, Dario Amodei, Nicholas\nJoseph, Jared Kaplan, and Sam McCandlish.\n2022. Scaling laws and interpretability of learn-\ning from repeated data.\nDanny Hernandez, Jared Kaplan, Tom Henighan,\nand Sam McCandlish. 2021. Scaling laws for\ntransfer.\nJordan Hoffmann, Sebastian Borgeaud, Arthur\nMensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne\nHendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen,\nJack W. Rae, Oriol Vinyals, and Laurent Sifre.\n2022. Training compute-optimal large language\nmodels.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\nZhang, Jinghan Zhang, Tangjun Su, Junteng\nLiu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao\nFu, Maosong Sun, and Junxian He. 2023. C-\neval: A multi-level multi-discipline chinese evalu-\nation suite for foundation models. arXiv preprint\narXiv:2305.08322.\nInternLM\nTeam.\n2023.\nInternlm:\nA\nmul-\ntilingual language model with progressively\nenhanced capabilities.\nhttps://github.com/\nInternLM/InternLM.\nMandar Joshi, Eunsol Choi, Daniel Weld, and\nLuke Zettlemoyer. 2017. TriviaQA: A large scale\ndistantly supervised challenge dataset for read-\ning comprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers),\npages 1601\u20131611, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nNikhil Kandpal, Eric Wallace, and Colin Raffel.\n2022. Deduplicating training data mitigates pri-\nvacy risks in language models.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels.\nVijay Korthikanti, Jared Casper, Sangkug Lym,\nLawrence McAfee, Michael Andersch, Moham-\nmad Shoeybi, and Bryan Catanzaro. 2022. Re-\nducing activation recomputation in large trans-\nformer models.\nTaku Kudo and John Richardson. 2018. Sentence-\nPiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text\nprocessing. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 66\u2013\n71, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale read-\ning comprehension dataset from examinations.\narXiv preprint arXiv:1704.04683.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-\nBurch, and Nicholas Carlini. 2022. Deduplicating\ntraining data makes language models better.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang,\nHai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. 2023a.\nCmmlu: Measuring massive\nmultitask language understanding in chinese.\nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Al-\nlie Del Giorno, Suriya Gunasekar, and Yin Tat\nLee. 2023b.\nTextbooks are all you need\nii:\nphi-1.5 technical report.\narXiv preprint\narXiv:2309.05463.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space.\nNiklas Muennighoff, Alexander M. Rush, Boaz\nBarak, Teven Le Scao, Aleksandra Piktus, Noua-\nmane Tazi, Sampo Pyysalo, Thomas Wolf, and\nColin Raffel. 2023. Scaling data-constrained lan-\nguage models.\nDeepak Narayanan, Mohammad Shoeybi, Jared\nCasper, Patrick LeGresley, Mostofa Patwary,\nVijay Anand Korthikanti, Dmitri Vainbrand,\nPrethvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, Amar Phanishayee, and Matei Za-\nharia. 2021. Efficient large-scale language model\ntraining on gpu clusters using megatron-lm.\nOpenAI. 2023. GPT-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke Miller, Maddie Simens, Amanda Askell,\nPeter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. 2022. Training language models to\nfollow instructions with human feedback.\n15\nGuilherme Penedo, Quentin Malartic, Daniel Hess-\nlow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Al-\nmazrouei, and Julien Launay. 2023. The refined-\nweb dataset for falcon llm: outperforming cu-\nrated corpora with web data, and web data only.\narXiv preprint arXiv:2306.01116.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep contextual-\nized word representations. In Proceedings of the\n2018 Conference of the North American Chapter\nof the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1\n(Long Papers), pages 2227\u20132237, New Orleans,\nLouisiana. Association for Computational Lin-\nguistics.\nQwen Team. 2023. QWEN technical report. https:\n//github.com/QwenLM/Qwen.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nIlya Sutskever, et al. 2018. Improving language\nunderstanding by generative pre-training.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimiza-\ntions toward training trillion parameter models.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin,\nArtyom Kozhevnikov, Ivan Evtimov, Joanna\nBitton, Manish Bhatt, Cristian Canton Ferrer,\nAaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9-\nfossez, Jade Copet, Faisal Azhar, Hugo Touvron,\nLouis Martin, Nicolas Usunier, Thomas Scialom,\nand Gabriel Synnaeve. 2023. Code llama: Open\nfoundation models for code.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bha-\ngavatula, and Yejin Choi. 2021. Winogrande: An\nadversarial winograd schema challenge at scale.\nCommunications of the ACM, 64(9):99\u2013106.\nNoam Shazeer. 2020. Glu variants improve trans-\nformer.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan\nCatanzaro. 2020. Megatron-lm: Training multi-\nbillion parameter language models using model\nparallelism.\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers,\nJacob R Steeves, Joel Hestness, and Nolan Dey.\n2023. SlimPajama: A 627B token cleaned and\ndeduplicated version of RedPajama.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Mur-\ntadha, Bo Wen, and Yunfeng Liu. 2022.\nRo-\nformer: Enhanced transformer with rotary posi-\ntion embedding.\nTianxiang Sun and Xipeng Qiu. 2023.\nMOSS.\nhttps://github.com/OpenLMLab/MOSS/blob/main/\nREADME_en.md.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang,\nYann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang,\nand Tatsunori B. Hashimoto.\n2023. Stanford alpaca: An instruction-following\nllama model.\nhttps://github.com/tatsu-lab/\nstanford_alpaca.\nRoss Taylor, Marcin Kardas, Guillem Cucurull,\nThomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and\nRobert Stojnic. 2022. Galactica: A large lan-\nguage model for science.\nTHUDM. 2023. ChatGLM3-6B. https://github.\ncom/THUDM/ChatGLM3 Webpage in Chinese.\nTogether Computer. 2023. Redpajama: An open\nsource recipe to reproduce llama training dataset.\nHugo Touvron, Thibaut Lavril, Gautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal,\nEric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume\nLample. 2023a. Llama: Open and efficient foun-\ndation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhar-\ngava, Shruti Bhosale, et al. 2023b.\nLlama 2:\nOpen foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Proceedings of the 31st\nInternational Conference on Neural Information\nProcessing Systems, NIPS\u201917, page 6000\u20136010,\nRed Hook, NY, USA. Curran Associates Inc.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra,\nAlisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. 2023. Self-instruct: Align-\ning language models with self-generated instruc-\ntions.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis\nConneau,\nVishrav\nChaudhary,\nFrancisco\nGuzm\u00e1n, Armand Joulin, and Edouard Grave.\n2020. CCNet: Extracting high quality mono-\nlingual datasets from web crawl data.\nIn\nProceedings of the Twelfth Language Resources\nand Evaluation Conference, pages 4003\u20134012,\nMarseille, France. European Language Resources\nAssociation.\nMengzhou Xia, Mikel Artetxe, Chunting Zhou,\nXi Victoria Lin, Ramakanth Pasunuru, Danqi\nChen, Luke Zettlemoyer, and Ves Stoyanov. 2023.\nTraining trajectories of language models across\nscales.\n16\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia\nZhang, Prajjwal Bhargava, Rui Hou, Louis Mar-\ntin, Rashi Rungta, Karthik Abinav Sankarara-\nman, Barlas Oguz, Madian Khabsa, Han Fang,\nYashar Mehdad, Sharan Narang, Kshitiz Ma-\nlik, Angela Fan, Shruti Bhosale, Sergey Edunov,\nMike Lewis, Sinong Wang, and Hao Ma. 2023.\nEffective long-context scaling of foundation mod-\nels.\nXverse-AI. 2023. Xverse-13B. https://github.com/\nxverse-ai/XVERSE-13B Webpage in Chinese.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can\na machine really finish your sentence? In Pro-\nceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages\n4791\u20134800, Florence, Italy. Association for Com-\nputational Linguistics.\nBiao Zhang and Rico Sennrich. 2019. Root Mean\nSquare Layer Normalization.\nIn Advances in\nNeural Information Processing Systems 32, Van-\ncouver, Canada.\nA\nDetails on GPT-7B vs.\nLLaMA-7B Experiment\nIn a preliminary experiment, we compared the\nlanguage modeling performance between GPT\nand LLaMA architecture in a controlled envi-\nronment. We trained a 7B model with GPT\narchitecture and a comparable 7B model with\nLLaMA architecture for 200B tokens sampled\nfrom the same corpus and with the same train-\ning parameters. Details are given in Table 9.\nB\nPreliminary Experiments on\nDistributed Training\nIn Table 10 we report preliminary results ob-\ntained with various distributed training con-\nfigurations on LLaMA-13B and Skywork-13B\nmodel architecture. In both cases, the best\nthroughput is achieved with DP256 and PP2\nwith ZERO-1 setting.\nC\nMore Benchmark Results\nWe also provide results of the following bench-\nmarks in Table 11:\n\u2022 TriviaQA (Joshi et al., 2017): TriviaQA\nis a realistic text-based question answer-\ning dataset which includes 950K question-\nanswer pairs from 662K documents collected\nfrom Wikipedia and the web.\n\u2022 HellaSwag (Zellers et al., 2019): HellaSWAG\nis a dataset that focuses on grounded com-\nmonsense inference.\n\u2022 Winogrande (Sakaguchi et al., 2021): Wino-\nGrande is a dataset that focuses on com-\nmonsense reasoning.\n\u2022 BoolQ (Clark et al., 2019) BoolQ is a ques-\ntion answering dataset for yes/no questions.\n\u2022 PIQA (Bisk et al., 2019): PIQA is a dataset\nfor commonsense reasoning, and was cre-\nated to investigate the physical knowledge\nof existing models in NLP.\n\u2022 ARC\n(Clark\net\nal.,\n2018):\nARC\nis\na dataset consisting of multiple-choice\nquestion-answering tasks that focus on com-\nmonsense reasoning.\n\u2022 RACE (Lai et al., 2017) RACE is a dataset\nthat focuses on reading comprehension.\nD\nDetails on LM Test Sets\nWe established a daily crawl of published arti-\ncles and user posts from a selection of widely\nused Chinese websites. This data collection\nprocess is distinct from the pipeline utilized\nto construct SkyPile. The purpose of gather-\ning this data is to create independent language\nmodeling test sets, categorized by their domain,\nfor the evaluation of current open Language\nLearning Models (LLMs).\nBelow we describe the sources of these do-\nmain testsets:\n\u2022 Technology: AI related articles from (36kr.\ncom).\nThis website provides timely and\ncomprehensive news articles about startups,\ntechnology, and business trends, primarily\nin the Chinese market.\n\u2022 Movie: User written movie reviews from\nDouban (douban.com). Douban is a popular\nsocial networking service in China that offers\na platform for users to share their opinions\nand create content related to movies, books,\nand music. It is one of the most influential\nweb 2.0 websites in China and has a strong\nfocus on user-generated content.\n\u2022 Government: News from website of Peo-\nple\u2019s Daily (www.people.com.cn), which is the\n17\nGPT-7B\nLLaMA-7B\nPositional Embedding\nAbsolute\nRotary\nMax Position Embeddings\n4096\n4096\nNormalization\nLayerNorm\nRMSNorm\nActivation\nGelu\nSwiGlu\nAttention\nMHA\nMHA\nNum. Layers\n32\n32\nHidden Size\n4096\n4096\nNum. Heads\n32\n32\nFFN Size\n16384\n11008\nContext Size\n4096\n4096\nGlobal Batch Size\n1024\n1024\nAdam \u03b21\n0.95\n0.95\nAdam \u03b22\n0.9\n0.9\nAdam \u03f5\n1.00e-8\n1.00-8\nPrecision\nbf16\nbf16\nPeak Learning Rate\n3e-4\n3e-4\nMin Learning Rate\n3e-5\n3e-5\nLearning Rate Decay Steps\n43945\n43945\nLearning Rate Decay Style\nCosine\nCosine\nWarm-up Steps\n2000 steps\n2000 steps\nWeight Decay\n0.1\n0.1\nDropout Probability\n0.1\n0\nGradient Clip\n1\n1\nTotal Steps\n51200\n51200\nTable 9: Comparison of GPT-7B and LLaMA-7B. All variables are controlled in our experiment except\nfor the differences in architecture.\nModel\nStrategy\nThroughput\nMFU\nTFlops\nMemory\nLLaMA2\nDP512\n-\n-\n-\nOOM\nLLaMA2\nDP256+PP2\n2045\n58.5\n182.6\n70.7\nLLaMA2\nDP256+TP2\n1928\n55.2\n172.2\n65.5\nLLaMA2\nDP128+TP2+PP2\n1936\n55.4\n172.9\n39.4\nLLaMA2\nDP128+PP4\n1964\n56.2\n175.4\n53.4\nLLaMA2\nDP128+TP4\n1744\n44.4\n138.5\n35.4\nSkywork\nDP512\n-\n-\n-\nOOM\nSkywork\nDP256+PP2\n1873\n56.5\n176.2\n77.1\nSkywork\nDP256+TP2\n1775\n53.5\n167.0\n67.9\nSkywork\nDP128+TP2+PP2\n1776\n53.5\n167.0\n42.5\nSkywork\nDP128+PP4\n1828\n55.1\n171.9\n58.7\nSkywork\nDP128+TP4\n1417\n43.1\n134.6\n36.6\nTable 10: Compute effeciency achieved with different distributed training configurations. We tested both\nLLaMA2-13B and Skywork-13B. Throughout the experiments, we use a global batch size of 4096 and\na micro batch size of 1. When Tensor Parallelism is enabled, Sequence Parallelism is enabled as well.\nThroughput is measured in tokens processed per GPU per second, while Model Flops Utilization (MFU)\nis expressed as a percentage (%). Memory usage is reported in Gigabytes (GB).\n18\nModels\nBoolQ\nPIQA\nWinogrande\nTriviaQA\nRACE\nHellaswag\nARC-E\nARC-C\nOpenLLaMA-13B\n77.6\n79.5\n72.0\n60.2\n42.4\n76.0\n78.9\n48.6\nLLaMA-13B\n80.7\n81.0\n76.2\n65.0\n43.4\n80.1\n82.1\n54.7\nLLaMA2-13B\n83.3\n81.7\n75.8\n68.2\n43.9\n81.5\n83.7\n57.0\nBaichuan-13B\n78.8\n77.2\n70.4\n51.6\n35.8\n74.2\n77.2\n48.4\nBaichuan2-13B\n80.3\n79.3\n72.1\n58.0\n25.2\n76.4\n81.1\n53.2\nXverse-13B\n79.8\n80.0\n71.1\n53.3\n43.2\n77.2\n78.5\n49.1\nSkywork-13B\n82.9\n79.9\n72.2\n54.0\n45.2\n77.4\n78.5\n50.2\nTable 11: More English benchmarks results. As all of these models are more or less sensitive to the prompt\ntemplate or number of shots, the reported results, which are reproduced by us, may be different to those\nfrom other sources.\nmost influential and authoritative newspa-\npers in China. The language used in the\nnews is typically formal Standard Mandarin\nand carries an authoritative tone.\n\u2022 Game: Articles from Gcores (www.gcores.\ncom). This is a Chinese digital media plat-\nform dedicated to video games, tech trends,\nand geek culture.\nThe platform features\na wide range of original content, including\nnews articles, podcast episodes, videos, and\nindependent games.\n\u2022 Finance: News from finance section of Sina\n(finance.sina.com.cn). It is one of China\u2019s\nleading online media companies, offers a\ncomprehensive suite of financial information\nand services.\nIt covers a broad range of\ntopics including stock markets, forex, com-\nmodities, real estate, and personal finance.\n\u2022 General: News from Jiemian News (www.\njiemian.com). Jiemian is a prominent Chi-\nnese digital media platform known for its\nin-depth and high-quality journalism. It cov-\ners a wide range of topics, including politics,\neconomy, culture, technology, finance, and\nlifestyle.\n19\nSubject\nStage-1\nStage-2\nBoost\nAccountant\n40.8\n49.0\n8.2\nAdvanced Mathematics\n26.3\n42.1\n15.8\nArt Studies\n60.6\n72.7\n12.1\nBasic Medicine\n42.1\n57.9\n15.8\nBusiness Administration\n42.4\n48.5\n6.1\nChinese Language and Literature\n47.8\n56.5\n8.7\nCivil Servant\n40.4\n66.0\n25.5\nClinical Medicine\n36.4\n40.9\n4.5\nCollege Chemistry\n37.5\n50.0\n12.5\nCollege Economics\n52.7\n47.3\n-5.5\nCollege Physics\n15.8\n36.8\n21.1\nCollege Programming\n51.4\n51.4\n0.0\nComputer Architecture\n33.3\n52.4\n19.0\nComputer Network\n21.1\n26.3\n5.3\nDiscrete Mathematics\n50.0\n18.8\n-31.3\nEducation Science\n44.8\n75.9\n31.0\nElectrical Engineer\n35.1\n35.1\n0.0\nEnvironmental Impact Assessment Engineer\n45.2\n51.6\n6.5\nFire Engineer\n45.2\n51.6\n6.5\nHigh School Biology\n42.1\n78.9\n36.8\nHigh School Chemistry\n36.8\n63.2\n26.3\nHigh School Chinese\n26.3\n42.1\n15.8\nHigh School Geography\n36.8\n78.9\n42.1\nHigh School History\n80.0\n80.0\n0.0\nHigh School Mathematics\n27.8\n16.7\n-11.1\nHigh School Physics\n42.1\n57.9\n15.8\nHigh School Politics\n47.4\n84.2\n36.8\nIdeological and Moral Cultivation\n84.2\n100.0\n15.8\nLaw\n33.3\n45.8\n12.5\nLegal Professional\n39.1\n52.2\n13.0\nLogic\n50.0\n45.5\n-4.5\nMao Zedong Thought\n70.8\n83.3\n12.5\nMarxism\n57.9\n63.2\n5.3\nMetrology Engineer\n37.5\n58.3\n20.8\nMiddle School Biology\n76.2\n95.2\n19.0\nMiddle School Chemistry\n30.0\n95.0\n65.0\nMiddle School Geography\n41.7\n83.3\n41.7\nMiddle School History\n59.1\n81.8\n22.7\nMiddle School Mathematics\n15.8\n36.8\n21.1\nMiddle School Physics\n42.1\n73.7\n31.6\nMiddle School Politics\n52.4\n90.5\n38.1\nModern Chinese History\n47.8\n73.9\n26.1\nOperating System\n52.6\n47.4\n-5.3\nPhysician\n46.9\n57.1\n10.2\nPlant Protection\n63.6\n63.6\n0.0\nProbability and Statistics\n27.8\n33.3\n5.6\nProfessional Tour Guide\n69.0\n65.5\n-3.4\nSports Science\n42.1\n52.6\n10.5\nTax Accountant\n30.6\n49.0\n18.4\nTeacher Qualification\n61.4\n84.1\n22.7\nUrban and Rural Planner\n50\n67.4\n17.4\nVeterinary Medicine\n26.1\n60.9\n34.8\nTable 12: Details on CEVAL benchmark results.\n20\n0\n1000\n2000\n3000\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\nBoolQ\n0\n1000\n2000\n3000\n66\n68\n70\n72\n74\n76\n78\n80\nPIQA\n0\n1000\n2000\n3000\n50\n55\n60\n65\n70\nWinogrande\n0\n1000\n2000\n3000\n0\n10\n20\n30\n40\n50\nT\nriviaQA\n0\n1000\n2000\n3000\nT\nokens (B)\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\nRACE\n0\n1000\n2000\n3000\nT\nokens (B)\n10\n20\n30\n40\n50\n60\n70\nCMRC\nFigure 6: Performance of the Skywork-13B on various benchmarks during Stage-1 pre-training. Benchmarks\ninclude BoolQ, PIQA, Winogrande, TriviaQA, RACE, and CMRC.\n21\n"
  }
]