[
  {
    "title": "MagiCapture: High-Resolution Multi-Concept Portrait Customization",
    "link": "https://arxiv.org/pdf/2309.06895.pdf",
    "upvote": "27",
    "text": "MagiCapture: High-Resolution Multi-Concept Portrait Customization\nJunha Hyung*1, Jaeyo Shin*2, and Jaegul Choo1\n1 KAIST AI, 2 Sogang University\n{sharpeeee, jchoo}@kaist.ac.kr, tlswody123@sogang.ac.kr\nFigure 1: Generated results of the proposed MagiCapture, a multi-concept personalization method for integrating subject and\nstyle concepts to generate high-resolution portrait images using just a few subject and style references.\nAbstract\nLarge-scale text-to-image models including Stable Diffusion\nare capable of generating high-fidelity photorealistic portrait\nimages. There is an active research area dedicated to person-\nalizing these models, aiming to synthesize specific subjects\nor styles using provided sets of reference images. However,\ndespite the plausible results from these personalization meth-\nods, they tend to produce images that often fall short of re-\nalism and are not yet on a commercially viable level. This\nis particularly noticeable in portrait image generation, where\nany unnatural artifact in human faces is easily discernible\ndue to our inherent human bias. To address this, we intro-\nduce MagiCapture, a personalization method for integrating\nsubject and style concepts to generate high-resolution por-\ntrait images using just a few subject and style references. For\ninstance, given a handful of random selfies, our fine-tuned\nmodel can generate high-quality portrait images in specific\nstyles, such as passport or profile photos. The main challenge\n*These authors contributed equally.\nwith this task is the absence of ground truth for the composed\nconcepts, leading to a reduction in the quality of the final out-\nput and an identity shift of the source subject. To address\nthese issues, we present a novel Attention Refocusing loss\ncoupled with auxiliary priors, both of which facilitate robust\nlearning within this weakly supervised learning setting. Our\npipeline also includes additional post-processing steps to en-\nsure the creation of highly realistic outputs. MagiCapture out-\nperforms other baselines in both quantitative and qualitative\nevaluations and can also be generalized to other non-human\nobjects.\nIntroduction\nTo obtain high-quality portrait images suitable for resumes\nor wedding events, individuals typically have to visit a photo\nstudio, followed by a costly and time-consuming process\nof photo retouching. Imagine a scenario where all that\u2019s re-\nquired is a few selfie images and reference photos, and you\ncould receive high-quality portrait images in specific styles,\narXiv:2309.06895v2  [cs.CV]  2 Feb 2024\nsuch as passport or profile photos. This paper aims to auto-\nmate this process.\nRecent advancements in large-scale text-to-image mod-\nels, such as Stable Diffusion [21] and Imagen [23], have\nmade it possible to generate high-fidelity, photorealistic por-\ntrait images. The active area of research dedicated to person-\nalizing these models seeks to synthesize specific subjects or\nstyles using provided sets of train images. In this work, we\nformulate our task as a multi-concept customization prob-\nlem. Here, the source content and reference style are learned\nrespectively, and the composed output is generated. Unlike\ntext-driven editing, using reference images allows users to\nprovide fine-grained guidance, making it more suitable for\nthis task.\nHowever, despite the promising results achieved by pre-\nvious personalization methods, they often produce images\nthat lack realism and fall short of commercial viability. This\nproblem primarily arises from attempting to update the pa-\nrameters of large models using only a small number of im-\nages. This decline in quality becomes even more evident in a\nmulti-concept generation, where the absence of ground truth\nimages for the composed concepts frequently leads to the\nunnatural blending of disparate concepts or deviation from\nthe original concepts. This issue is particularly conspicuous\nin portrait image generation, as any unnatural artifacts or\nshifts in identity are easily noticeable due to our inherent\nhuman bias.\nTo address these issues, we present MagiCapture, a multi-\nconcept personalization method for the fusion of subject and\nstyle concepts to generate high-resolution portrait images\nwith only a few subject and style references. Our method\nemploys composed prompt learning, incorporating the com-\nposed prompt as part of the training process, which enhances\nthe robust integration of source content and reference style.\nThis is achieved through the use of pseudo labels and aux-\niliary loss. Moreover, we propose the Attention Refocusing\nloss in conjunction with a masked reconstruction objective,\na crucial strategy for achieving information disentanglement\nand preventing information leakage during inference. Magi-\nCapture outperforms other baselines in both quantitative and\nqualitative assessments and can be generalized to other non-\nhuman objects with just a few modifications.\nThe main contributions of our paper are as follows:\n\u2022 We introduce a multi-concept personalization method ca-\npable of generating high-resolution portrait images that\nfaithfully capture the characteristics of both source and\nreference images.\n\u2022 We present a novel Attention Refocusing loss combined\nwith masked reconstruction objective, effectively disen-\ntangling the desired information from input images and\npreventing information leakage during the generation\nprocess.\n\u2022 We put forth a composed prompt learning approach that\nleverages pseudo-labels and auxiliary loss, facilitating\nthe robust integration of source content and reference\nstyle.\n\u2022 In both quantitative and qualitative assessments, our\nmethod surpasses other baseline approaches and, with\nminor adjustments, can be adapted to generate images of\nnon-human objects.\nRelated Work\nText-to-image diffusion models\nDiffusion models [10,\n27, 28, 26] have recently achieved remarkable success in\nimage generation, driving advancements in various appli-\ncations and fields. Their powerful performance has sig-\nnificantly propelled the field of text-guided image synthe-\nsis [16, 12, 23, 19] forward. In particular, large-scale text-\nto-image diffusion models, trained on extensive text-image\npair datasets, have set new benchmarks. Notable examples\ninclude Stable diffusion [30] and Imagen [23]. Our work is\nbuilt upon the pre-trained stable diffusion model.\nPersonalization of Text-to-image Models.\nPersonalizing\ngenerative models for specific concepts is a key goal in the\nvision field. With the rise of GANs, there have been efforts\nto fine-tune GANs, like Pivotal Tuning [20], based on GAN\ninversion [36].\nMore recently, studies have sought to personalize diffu-\nsion models using small image datasets, typically 3 \u223c 5\nimages, associated with a particular object or style and in-\ncorporating specialized text tokens to embed such concepts.\nFor instance, when customizing models for a specific dog,\nthe prompt \u201ca [V 1] dog\u201d is used so that the special token\ncan learn information specific to the dog. DreamBooth [22]\nfine-tunes entire weights, Textual Inversion [6] adjusts text\nembeddings, and Custom Diffusion [14] adapts the mapping\nmatrix for the cross-attention layer. While effective in learn-\ning concepts, these models sometimes generate less realistic\nor identity-losing images. Methods like ELITE [32] and In-\nstantBooth [25] employ a data-driven approach for encoder-\nbased domain tuning, which is not directly comparable to\nour approach.\nOur method differs from concurrent works like SVD-\niff [8], FastComposer [33], and Break-A-Scene [1], which\nuse similar techniques like attention loss or composed\nprompts. Unlike SVDiff\u2019s collage approach (Cut-Mix-\nUnmix), our method is tailored for style-mixed outputs,\nenhancing the quality of multi-concept portraits. Dis-\ntinct from FastComposer and Break-A-Scene, our atten-\ntion loss only targets regions in the attention map not\npresent in the ground-truth mask (Ak[i, j] for all (i, j) \u2208\n{(i, j)|Mv[i, j] = 0}), allowing for the varying optimal val-\nues for other areas.\nPreliminaries\nDiffusion Models.\nDiffusion models [10, 27, 28, 26] are\na class of generative models that create images through an\niterative denoising process. These models comprise a for-\nward and backward pass. During the forward pass, an in-\nput image x(0) is progressively noised using the equation\nx(t) = \u221a\u03b1tx(0) + \u221a1 \u2212 \u03b1t\u03f5, where \u03f5 represents standard\nGuassian noise and {\u03b1t} is a pre-defined noise schedule\nwith timestep t, 1 < t < T. During backward pass, the gen-\nerated image is obtained by denoising the starting noise xT\nFigure 2: The overall pipeline of MagiCapture, where the training process is formulated as multi-task learning of three different\ntasks: source, reference, and composed prompt learning. In the composed prompt learning, reference style images serve as\npseudo-labels, along with auxiliary identity loss between the source and predicted images. Attention Refocusing loss is applied\nto all three tasks. After training, users can generate high-fidelity images with integrated concepts and can further manipulate\nthem using varying text conditions.\nusing a UNet \u03f5\u03b8(x(t), t), which is trained to predict noise at\nthe input timestep t. Latent diffusion models (LDM) [21] are\na variant of diffusion models where the denoising process\noccurs in the latent space. Specifically, an image encoder E\nis used to transform the input image x into a latent represen-\ntation z, such that E(x) = z. During inference, the denoised\nlatent representation is decoded to produce the final image\nx(0)\u2032 = D(z(0)), where D represents the decoder of an au-\ntoencoder. Stable diffusion [30] is a text-guided latent diffu-\nsion model (LDM) trained on large-scale text-image pairs.\nIt has the following objective:\nLLDM = Ez,c,\u03f5,t\nh\n||\u03f5\u03b8(z(t), t, c) \u2212 \u03f5||2\n2\ni\n,\n(1)\nwhere c refers to the text condition.\nAttention maps\nLarge-scale text-to-image diffusion mod-\nels utilize cross-attention layers for text-conditioning. In\nStable Diffusion [21], CLIP text encoder [18] is used to\nproduce text embedding features. These text embeddings\nare then transformed to obtain the key K and value V for\nthe cross-attention layer through linear mapping, and spatial\nfeature of image is projected to query Q. The attention map\nof the cross-attention layer is computed as:\nA = softmax\n\u0010QKT\n\u221a\nd\n\u0011\n.\n(2)\nThe attention map corresponding to a specific token with in-\ndex k can be obtained as Ak = A[k]. Such attention maps\nare useful for visualizing the influence of individual tokens\nin the text prompt. Moreover, they can be altered or manip-\nulated for the purpose of image editing, as demonstrated in\nPrompt-to-Prompt [9].\nFigure 3: Visualization of aggregated attention maps from\nUNet layers before and after the application of Attention Re-\nfocusing (AR) loss illustrates its importance in achieving in-\nformation disentanglement and preventing information spill.\nMethod\nGiven a small set of source images and reference style im-\nages, the goal of this paper is to synthesize images that inte-\ngrate the source content with the reference style. While our\nmethod is primarily designed for generating portrait images,\nit can be easily adapted to handle other types of content with\nminor modifications. We utilize the customization of each\nconcepts during the optimization phase and employ a com-\nposed prompt during inference to generate multi-concept\nimages. A comprehensive overview of our approach is de-\npicted in Fig. 2, and the details of our method will be elabo-\nrated upon in the subsequent sections.\nTwo-phase Optimization.\nSimilar to Pivotal Tuning [20]\nin GAN inversion, our method consists of two-phase opti-\nmization. In the first phase, we optimize the text embed-\ndings for the special tokens [V \u2217] using the reconstruction\nobjective as in [6]. While optimizing the text embeddings\nis not sufficient for achieving high-fidelity customization, it\nserves as a useful initialization for the subsequent phase. In\nthe second phase, we jointly optimize the text embeddings\nand model parameters with the same objective. Rather than\noptimizing the entire model, we apply the LoRA [11], where\nonly the residuals \u2206W of the projection layers in the cross-\nattention module are trained using low-rank decomposition.\nSpecifically, the updated parameters are expressed as:\nW\n\u2032 = W + \u2206W, \u2206W = UV T ,\n(3)\nwhere U \u2208 Rn\u00d7r, V \u2208 Rm\u00d7r, and r << n, m. Empirically,\nwe find that this two-phase optimization coupled with LoRA\nstrikes a favorable balance between reconstruction and gen-\neralization. It preserves the model\u2019s generalization capabili-\nties for unseen prompts while effectively capturing the finer\ndetails of the source images.\nMasked Reconstruction.\nIn our approach, a source\nprompt cs (e.g., A photo of [V 1] person.) and a reference\nprompt cr (e.g., A photo of a person in the [V 2] style.) are\nused to reconstruct the source image Is and a target style im-\nage Ir respectively. It is crucial to disentangle the identity of\nthe source subject from non-facial regions, such as the back-\nground and clothing, to prevent this unwanted information\nfrom being encoded into the special token [V 1]. Similarly,\nwe need to disentangle the reference image to ensure that\nthe facial details of the person in the reference image are not\nembedded into the special token [V 2]. To achieve this, we\npropose to use a masked reconstruction loss. Specifically,\nwe employ a mask that indicates the relevant region and ap-\nply it element-wise to both the ground truth latent code and\nthe predicted latent code. In the context of portrait gener-\nation, a source mask Ms indicates the facial region of the\nimage Is, and a target mask Mr denotes the non-facial areas\nof the reference image Ir. Formally, the masked reconstruc-\ntion loss for the source and the reference prompts are given\nby:\nLs\nmask = Ezs,cs,\u03f5,t\nh\n||\u03f5\u2299Ms \u2212\u03f5\u03b8(z(t)\ns , t, cs)\u2299Ms||2\n2\ni\n, (4)\nLr\nmask = Ezr,cr,\u03f5,t\nh\n||\u03f5\u2299Mr \u2212\u03f5\u03b8(z(t)\nr , t, cr)\u2299Mr||2\n2\ni\n, (5)\nMethod\nCSIM \u2191\nStyle \u2191\nAesthetic \u2191\nDreamBooth\n0.102\n0.720\n5.770\nTextual Inversion\n0.224\n0.623\n5.670\nCustom Diffusion\n0.436\n0.606\n5.263\nOurs w/o AR & CP\n0.429\n0.726\n6.178\nOurs\n0.566\n0.730\n6.218\nTable 1: Quantitative comparison of our method against\nDreamBooth [22], Textual Inversion [6], and Custom Diffu-\nsion [14]. Our method outperforms other baselines in terms\nof identity similarity measured between the source images\n(CSIM), masked CLIP similarity measure (Style), and Aes-\nthetic score [24].\nwhere z(t)\ns\nand z(t)\nr\nare the source and reference noised latent\nat timestep t \u223c Uniform(1, T) and \u03f5 \u223c N(0, I).\nComposed Prompt Learning.\nGenerating images with a\ncomposed prompt cc such as \u201dA photo of a [V 1] person\nin the [V 2] style,\u201d leads to undefined behavior because the\nmodel had not been customized on such prompts. Typically,\nthe resulting images generated using these unseen composed\nprompts suffer from a shift in the identity of the source\nsubject and a decline in output quality. To address this is-\nsue, we include training on the composed prompt. However,\nno ground truth image exists for such a prompt. We ap-\nproach this challenge as a weakly-supervised learning prob-\nlem, where there are no available ground truth labels. We\ncraft pseudo-labels and develop an auxiliary objective func-\ntion to suit our needs. In the context of the portrait gener-\nation task, we want to retain the overall composition, pose,\nand appearance from the reference style image, excluding\nthe facial identity. To achieve this, we employ the masked\nreconstruction objective given by:\nLc\nmask = Ezr,cc,\u03f5,t\nh\n||\u03f5\u2299Mr \u2212\u03f5\u03b8(z(t)\nr , t, cc)\u2299Mr||2\n2\ni\n. (6)\nFor the facial regions, we use an auxiliary identity loss that\nutilizes a pre-trained face recognition model [5] R and crop-\nping function B conditioned by the face detection model [4]:\nLid = E\u02c6x(0),Is\nh\n1 \u2212 cos(R(B(\u02c6x(0))), R(B((Is)))\ni\n,\n(7)\nwhere cos denotes the cosine similarity and \u02c6x(0) = D(\u02c6z(0))\nrefers to the estimated clean image from z(tid)\nr\nusing\nTweedie\u2019s formula [13]. Timestep tid is sampled as tid \u223c\nUniform(1, T\n\u2032), where T\n\u2032 < T, to avoid blurry and inac-\ncurate \u02c6x(0) estimated from noisy latent with large timesteps,\nwhich can impair cropping or yield odd facial embeddings.\nWe augment the composed prompt cc by randomly se-\nlecting from predefined prompt templates to boost editing\nstability and generalization.\nAttention Refocusing.\nWhen optimizing with training im-\nages, it is vital to achieve information disentanglement, en-\nsuring that special tokens exclusively embed the informa-\ntion of the region of interest, denoted as Mv for v \u2208 {s, r}.\nHowever, the masked reconstruction objective falls short of\nthis goal because the presence of transformer layers in the\nUNet backbone gives the model a global receptive field. The\nsame limitation applies to denoising steps in the inference\nstage, where we desire attention maps of special tokens to\nfocus only on the intended areas. For instance, in the portrait\ngeneration task, the special token [V 1] should only attend\nto facial regions when generating images to avoid informa-\ntion spill. We observe that information spill is more preva-\nlent when the model encounters an unseen prompt during\ninference. Fig. 3 demonstrates that special tokens do indeed\nattend to unwanted regions.\nTo solve this issue, we propose a novel Attention Refo-\ncusing (AR) loss, which steers the cross attention maps Ak\nof the special token [V \u2217] (where k = index([V \u2217])) using\na binary target mask. Our AR loss incorporates two crucial\ndetails: First, it is applied only to regions where \u00acMv, where\nthe mask value is zero. For the attention map values Ak[i, j]\nwhere (i, j) \u2208 {(i, j)|Mv[i, j] = 1}, the optimal values can\nvary across different UNet layers and denoising time steps,\nso they do not necessarily have to be close to 1. Conversely,\nfor Ak[i, j] where (i, j) \u2208 {(i, j)|Mv[i, j] = 0}, the val-\nues should be forced to 0 to achieve information disentan-\nglement during training and minimize information spill in\nthe inference stage. Second, it is essential to scale the atten-\ntion maps to the [0,1] range. Both of these techniques are re-\nquired to avoid disrupting the pre-trained transformer layers\u2019\ninternal operations, which would lead to corrupted outputs.\nThe Attention Refocusing loss can be formulated as follows:\nLattn = Ek,v\u2208{s,r}\nh\n||(S(Ak) \u2212 Mv) \u2299 \u00acMv||2\n2\ni\n,\n(8)\nwhere S(\u00b7) refers to a scaling function.\nPostprocessing.\nThe quality of images generated in a few-\nshot customization task is typically constrained by the capa-\nbilities of the pretrained text-to-image model used. More-\nover, when provided with low-resolution source and target\nimages, the fine-tuned model tends to produce lower-quality\nimages. To overcome these limitations and further enhance\nthe fidelity of the generated images, our pipeline includes\noptional postprocessing steps. Specifically, we employ a pre-\ntrained super-resolution model [31] and a face restoration\nmodel [35] to further improve the quality of the generated\nsamples.\nExperiments\nTraining Details.\nOur method utilizes pre-trained Stable\nDiffusion V1.5 [21]. The first training phase consists of a\ntotal of 1200 steps, with a learning rate 5e-4 for updating\nthe text embeddings. In the second LoRA phase, the learn-\ning rate is 1e-4 for the projection layers and 1e-5 for the text\nembeddings, with a total of 1500 training steps. The model\nis trained on a single GeForce RTX 3090 GPU, using a batch\nsize of 1 and gradient accumulation over 4 steps. For all ex-\nperiments, we employ 4 to 6 images for both the source and\nreference images. Please refer to the supplement for more\ndetails.\nComparisons.\nThe results of our method are demon-\nstrated in Fig. 4. We compare our method with other per-\nsonalization methods including DreamBooth [22], Textual\nMethod\nID \u2191\nStyle \u2191\nFidelity \u2191\nDreamBooth\n2.025\n3.648\n2.683\nTextual Inversion\n2.907\n3.038\n2.965\nCustom Diffusion\n3.223\n2.260\n2.980\nOurs\n4.055\n4.165\n4.293\nTable 2: User study of our method against DreamBooth [22],\nTextual Inversion [6], and Custom Diffusion [14]. Our\nmethod outperforms other baselines in terms of identity sim-\nilarity score (ID), style similarity measure (Style), and im-\nage fidelity score (Fidelity).\nInversion [6], and Custom Diffusion [14] using the same\nsource and reference images. We choose 10 identities, 7\nfrom VGGFace [2] and 3 in-the-wild identities gathered\nfrom the internet. We also manually select 10 style concepts,\nleading to 100 id-style pairs. For each pair, we train each\nbaseline and our model, then generate 100 images with the\ncomposed prompt for each of the trained model, resulting\nin 10,000 samples per baseline. Qualitative comparisons are\nshown in Fig. 5, where our method outperforms other base-\nlines in image fidelity and source-reference image reflection.\nWe assess the facial appearance similarity between the\nsource and generated portrait images by measuring the co-\nsine similarity between their facial embeddings, using a pre-\ntrained recognition network (CSIM) [34].\nAnother important aspect of evaluation is style preserva-\ntion, where we measure how well the results replicate the\nstyle of the reference images. We compute the cosine simi-\nlarity between the masked CLIP [18] image embeddings of\nthe reference and generated images, where facial regions are\nmasked to exclude facial appearance from the assessment.\nWe use CLIP similarity instead of texture similarity [7] since\nthe term style in our paper encompasses broader concepts\nsuch as image geometry and composition, in addition to tex-\nture and appearance of non-facial regions. Finally, we eval-\nuate the overall image fidelity with the LAION aesthetic\npredictor [24]. Table 1 shows that our method outperforms\nother baselines in all three metrics. Additionally, we conduct\na user study involving 30 participants who were asked to rate\nimages for ID preservation, style preservation, and image fi-\ndelity on a 1-5 scale. Table 2 summarizes the results, with\nour method consistently scoring higher than other baselines.\nWe observed that DreamBooth often overfits to the refer-\nence style images, leading to high style scores but low CSIM\nscores. Conversely, Textual Inversion tends to underfit both\nthe source and reference images, resulting in low-fidelity im-\nages that fail to preserve appearance details. Custom Diffu-\nsion better preserves source identity compared to the others,\nbut still cannot consistently perform well for the composed\nprompt, leading to identity shifts and unnatural images.\nAblation Study.\nAs shown in Fig. 3, we find that At-\ntention Refocusing loss effectively prevents attention maps\nfrom attending to unwanted regions, mitigating information\nspill and promoting information disentanglement. Empiri-\ncally, we observe that the Attention Refocusing loss should\nonly be applied during the second phase of training (LoRA\nReference\nSource\nFigure 4: Curated results of MagiCapture.\nOurs\nTextual Inversion\nDreamBooth\nCustom Diffusion\nFigure 5: Qualitative comparisons of MagiCapture with\nother baseline methods.\n[V1]\n[V1]\n[V2]\n[V2]\n[V1] + [V2]\n[V1] + [V2]\n[V1] + [V2], wearing sunglasses\n[V1] + [V2], Van Gogh painting\nFigure 6: Users can further manipulate the composed results\nusing prompts with additional description.\ntraining). We infer that text embeddings are not well-suited\nfor learning geometric information related to attention maps.\nMoreover, without composed prompt learning, the gener-\nated images often exhibit undefined behaviors where only\none of the source or reference sets is evident in the image,\nwithout blending. We present the evaluation metrics for both\nthe presence and absence of composed prompt learning (CP)\nand Attention Refocusing (AR) in Table 1. For more results\nand detailed analysis, please refer to the supplement.\nApplications.\nSince our method is robust to generaliza-\ntions, users can further manipulate the composed results us-\ning prompts with more descriptions (e.g., c\n\u2032\nc = \u201cA photo\nof [V 1] person in the [V 2] style, wearing sunglasses.\u201d). We\ndemonstrate such results in Fig. 6 and in the supplement.\nFurthermore, our method is adaptable for handling different\ntypes of content, including non-human images. For method-\nologies and results related to non-human content, please re-\nfer to the supplementary material.\nFigure 7: Failure cases: Proposed method occasionally pro-\nduces abnormal body parts such as limbs, fingers\nLimitations and Conclusions\nOur method occasionally produces abnormal body parts\nsuch as limbs, fingers, as shown in Fig. 7. Furthermore, the\nmodel tends to exhibit lower fidelity for non-white subjects\nand demonstrates a noticeable gender bias\u2014for instance,\nit struggles to accurately generate images of men wearing\nwedding dresses. These issues are largely related to the in-\nherent biases of the pre-trained text-to-image models, and\naddressing these problems within a few-shot setting repre-\nsents a significant avenue for future research. We acknowl-\nedge the ethical implications of our work and are commit-\nted to taking them seriously. We are also proactive in lead-\ning and supporting efforts to prevent potential misuse of our\ncontributions.\nAcknowledgements\nThis work was supported by the National Research Founda-\ntion of Korea (NRF) grant funded by the Korea government\n(MSIT) (No. NRF-2022R1A2B5B02001913), and Institute\nof Information & communications Technology Planning &\nEvaluation (IITP) grant funded by the Korea government\n(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate\nSchool Program (KAIST)).\nReferences\n[1] Avrahami, O.; Aberman, K.; Fried, O.; Cohen-Or, D.;\nand Lischinski, D. 2023. Break-A-Scene: Extracting\nMultiple Concepts from a Single Image. arXiv preprint\narXiv:2305.16311.\n[2] Cao, Q.; Shen, L.; Xie, W.; Parkhi, O. M.; and Zisser-\nman, A. 2018. Vggface2: A dataset for recognising\nfaces across pose and age. In 2018 13th IEEE interna-\ntional conference on automatic face & gesture recog-\nnition (FG 2018), 67\u201374. IEEE.\n[3] Caron, M.; Touvron, H.; Misra, I.; J\u00b4egou, H.; Mairal,\nJ.; Bojanowski, P.; and Joulin, A. 2021.\nEmerging\nproperties in self-supervised vision transformers.\nIn\nProceedings of the IEEE/CVF international confer-\nence on computer vision, 9650\u20139660.\n[4] Deng, J.; Guo, J.; Ververas, E.; Kotsia, I.; and\nZafeiriou, S. 2020. Retinaface: Single-shot multi-level\nface localisation in the wild.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, 5203\u20135212.\n[5] Deng, J.; Guo, J.; Xue, N.; and Zafeiriou, S. 2019. Arc-\nface: Additive angular margin loss for deep face recog-\nnition. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 4690\u20134699.\n[6] Gal, R.; Alaluf, Y.; Atzmon, Y.; Patashnik, O.;\nBermano, A. H.; Chechik, G.; and Cohen-Or, D. 2022.\nAn image is worth one word: Personalizing text-\nto-image generation using textual inversion.\narXiv\npreprint arXiv:2208.01618.\n[7] Gatys, L. A.; Ecker, A. S.; and Bethge, M. 2016. Im-\nage style transfer using convolutional neural networks.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2414\u20132423.\n[8] Han, L.; Li, Y.; Zhang, H.; Milanfar, P.; Metaxas,\nD.; and Yang, F. 2023.\nSvdiff: Compact parame-\nter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305.\n[9] Hertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.;\nPritch, Y.; and Cohen-Or, D. 2022. Prompt-to-prompt\nimage editing with cross attention control.\n[10] Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising dif-\nfusion probabilistic models. Advances in neural infor-\nmation processing systems, 33: 6840\u20136851.\n[11] Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li,\nY.; Wang, S.; Wang, L.; and Chen, W. 2021.\nLora:\nLow-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685.\n[12] Kim, G.; Kwon, T.; and Ye, J. C. 2022. Diffusionclip:\nText-guided diffusion models for robust image manip-\nulation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2426\u2013\n2435.\n[13] Kim, K.; and Ye, J. C. 2021. Noise2score: tweedie\u2019s\napproach to self-supervised image denoising without\nclean images. Advances in Neural Information Pro-\ncessing Systems, 34: 864\u2013874.\n[14] Kumari, N.; Zhang, B.; Zhang, R.; Shechtman, E.; and\nZhu, J.-Y. 2023. Multi-concept customization of text-\nto-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion, 1931\u20131941.\n[15] Kwon, G.; and Ye, J. C. 2022. Diffusion-based image\ntranslation using disentangled style and content repre-\nsentation. arXiv preprint arXiv:2209.15264.\n[16] Nichol, A.; Dhariwal, P.; Ramesh, A.; Shyam, P.;\nMishkin, P.; McGrew, B.; Sutskever, I.; and Chen, M.\n2021. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv\npreprint arXiv:2112.10741.\n[17] Park, T.; Efros, A. A.; Zhang, R.; and Zhu, J.-Y.\n2020.\nContrastive learning for unpaired image-to-\nimage translation. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u2013\n28, 2020, Proceedings, Part IX 16, 319\u2013345. Springer.\n[18] Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh,\nG.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.;\nClark, J.; et al. 2021. Learning transferable visual mod-\nels from natural language supervision. In International\nconference on machine learning, 8748\u20138763. PMLR.\n[19] Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and\nChen, M. 2022.\nHierarchical text-conditional im-\nage generation with clip latents.\narXiv preprint\narXiv:2204.06125.\n[20] Roich, D.; Mokady, R.; Bermano, A. H.; and Cohen-\nOr, D. 2022. Pivotal tuning for latent-based editing of\nreal images. ACM Transactions on Graphics (TOG),\n42(1): 1\u201313.\n[21] Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.;\nand Ommer, B. 2022. High-resolution image synthe-\nsis with latent diffusion models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, 10684\u201310695.\n[22] Ruiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein,\nM.; and Aberman, K. 2023. Dreambooth: Fine tuning\ntext-to-image diffusion models for subject-driven gen-\neration. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 22500\u2013\n22510.\n[23] Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.;\nDenton, E. L.; Ghasemipour, K.; Gontijo Lopes, R.;\nKaragol Ayan, B.; Salimans, T.; et al. 2022. Photo-\nrealistic text-to-image diffusion models with deep lan-\nguage understanding. Advances in Neural Information\nProcessing Systems, 35: 36479\u201336494.\n[24] Schuhmann, C. Aug 2022. Laion aesthetics.\n[25] Shi, J.; Xiong, W.; Lin, Z.; and Jung, H. J.\n2023.\nInstantbooth: Personalized text-to-image gen-\neration without test-time finetuning.\narXiv preprint\narXiv:2304.03411.\n[26] Song, J.; Meng, C.; and Ermon, S. 2020.\nDe-\nnoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502.\n[27] Song, Y.; and Ermon, S. 2019. Generative modeling by\nestimating gradients of the data distribution. Advances\nin neural information processing systems, 32.\n[28] Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar,\nA.; Ermon, S.; and Poole, B. 2020. Score-based gen-\nerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456.\n[29] Tumanyan, N.; Bar-Tal, O.; Bagon, S.; and Dekel, T.\n2022.\nSplicing vit features for semantic appearance\ntransfer. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 10748\u2013\n10757.\n[30] von Platen, P.; Patil, S.; Lozhkov, A.; Cuenca, P.; Lam-\nbert, N.; Rasul, K.; Davaadorj, M.; and Wolf, T. 2022.\nDiffusers: State-of-the-art diffusion models.\nhttps://\ngithub.com/huggingface/diffusers.\n[31] Wang, X.; Xie, L.; Dong, C.; and Shan, Y. 2021.\nReal-ESRGAN: Training Real-World Blind Super-\nResolution with Pure Synthetic Data. In International\nConference on Computer Vision Workshops (ICCVW).\n[32] Wei, Y.; Zhang, Y.; Ji, Z.; Bai, J.; Zhang, L.; and Zuo,\nW. 2023. Elite: Encoding visual concepts into textual\nembeddings for customized text-to-image generation.\narXiv preprint arXiv:2302.13848.\n[33] Xiao, G.; Yin, T.; Freeman, W. T.; Durand, F.; and Han,\nS. 2023.\nFastComposer: Tuning-Free Multi-Subject\nImage Generation with Localized Attention.\narXiv\npreprint arXiv:2305.10431.\n[34] Zakharov, E.; Shysheya, A.; Burkov, E.; and Lempit-\nsky, V. 2019. Few-shot adversarial learning of real-\nistic neural talking head models.\nIn Proceedings of\nthe IEEE/CVF international conference on computer\nvision, 9459\u20139468.\n[35] Zhou, S.; Chan, K. C.; Li, C.; and Loy, C. C. 2022. To-\nwards Robust Blind Face Restoration with Codebook\nLookup TransFormer. In NeurIPS.\n[36] Zhu, J.; Shen, Y.; Zhao, D.; and Zhou, B. 2020. In-\ndomain gan inversion for real image editing.\nIn\nEuropean conference on computer vision, 592\u2013608.\nSpringer.\nSupplementry Materials\nTraining Details\nMagiCapture\nThe loss function for the first phase training\nis given as:\nLs\nmask + Lr\nmask.\n(9)\nFor the second phase LoRA training, composed prompt\nlearning and Attention Refocusing loss is added:\nLs\nmask + Lr\nmask + Lc\nmask + \u03bbidLid + \u03bbattnLattn,\n(10)\nwhere \u03bbid = 1 and \u03bbattn = 2.5 is used for all experiments.\nFor \u03bbid, 0.25 or 0.5 are also fine.\nDreamBooth\nWe employ the optimal settings for Dream-\nBooth [22] training, which include prior preservation with\na lambda value of 1.0 and a dataset of 200 images. Each\nbatch comprises two images, consisting of one source and\none style image. We set the learning rate to 1e-6 and train the\nmodel for 2000 steps. During this training, the CLIP text en-\ncoder and the UNet parameters are concurrently optimized.\nFor inference, we use a denoising step of 50 with a guidance\nscale of 7.5.\nCustom Diffusion\nFor training the Custom Diffusion\nmodel [14], we use the best settings with prior preserva-\ntion, a lambda value of 1.0, and a dataset of 200 images.\nThe batch size is set to 2. With a learning rate of 5e-6, we\ntrain the model for 750 steps, optimizing the cross-attention\nlayers of the Stable Diffusion model [21], as detailed in the\noriginal paper. The inference phase employs a denoising step\nof 50 and a guidance scale of 7.5.\nTextual Inversion\nFor the training of Textual Inver-\nsion [6], we adopt the optimal settings, including a batch\nsize of 2, a learning rate of 5e-3, and a total of 5000 training\nsteps. The inference process involves a denoising step of 50\nwith a guidance scale of 7.5.\nGeneral Object\nOur method can also be applied to other general objects,\nwhere our composed prompt learning can be applied for ro-\nbust multi-concept composition. We illustrate this with an\nexample where the goal is to maintain the structure of the\nsource object while adopting the texture from the reference\nimage. We employ the same masked reconstruction objec-\ntive Ls\nmask for the source, and naive reconstruction objective\nwithout masking Lr for the reference.\nFor composed prompt learning, we employ structure\nloss [15] that maximizes structural similarity between the\nestimated image \u02c6x(0) and the source images using a pre-\ntrained DINO ViT [3]. Specifically, the structure loss com-\nprises two components: the self-similarity loss Lssim [29]\nand the patch contrastive loss Lcontra [17]. Lssim utilizes a\nself similarity matrix derived from the multi-head self atten-\ntion (MSA) layer of the pre-trained DINO. Lcontra max-\nimizes the patch-wise similarity between the keys of the\nsource and the estimated image \u02c6x(0), with the keys extracted\nfrom the MSA layer of DINO. For the style similarity loss\nFigure 8: A comparison with results produced without the\nuse of composed prompt learning for non-human images.\nLstyle, we minimize the distance between DINO ViT [CLS]\ntoken embeddings of the reference and the estimated im-\nage \u02c6x(0). To sum up, our loss function for composed prompt\nlearning is:\n\u03bbssimLssim + \u03bbcontraLcontra + \u03bbstyleLstyle,\n(11)\nwhere \u03bbssim = 0.1, \u03bbssim = 0.2, and \u03bbssim = 2 is used for\ntraining.\nWe demonstrate the results for general objects in Fig. 11.\nAdditionally, in Fig. 8, we provide a comparison with re-\nsults produced without the use of composed prompt learn-\ning. These comparisons reveal that, in the absence of com-\nposed prompt learning, the outcomes tend to suffer from two\nmain issues: either the structure of the source concept is in-\nadequately preserved, or the style of the reference images is\nnot effectively incorporated.\nAblation Study\nWe present the results of our ablation study in Table 3, which\nclearly highlight the significance of composed prompt learn-\nMethod\nCSIM \u2191\nStyle \u2191\nAesthetic \u2191\nOurs\n0.566\n0.730\n6.218\nOurs w/ postprocessing\n0.508\n0.737\n6.184\nOurs w/o CP\n0.429\n0.717\n6.159\nOurs w/o AR & CP\n0.429\n0.726\n6.178\nTable 3: The results of the ablation study clearly highlights\nsignificance of composed prompt learning (CP) in enhancing\nthe metrics. When CP is not included, there is a noticeable\ndecline in CSIM and style score (measured by masked CLIP\nsimilarity).\n(a)\n(c)\n(b)\nFigure 9: Results without Attention Refocusing (AR) loss.\nWhile AR loss does not appear to contribute to the metric\nimprovement, the absence of AR often leads to collapsed\nsamples as seen in (a) and (b). The generated samples pre-\ndominantly reflect either the source or reference images,\nrather than a balanced combination of both. (c) illustrates\nthat without AR, information spill is evident in the generated\nearrings, indicating that the source special tokens attend to\nnon-facial regions.\ning (CP) in enhancing the metrics. When CP is not included,\nthere is a noticeable decline in CSIM and style score (mea-\nsured by masked CLIP similarity). Conversely, while the At-\ntention Refocusing (AR) loss does not appear to contribute\nto the metric improvement, it is noteworthy that the absence\nof AR often leads to collapsed samples, where the gener-\nated samples predominantly reflect either the source or ref-\nerence images, rather than a balanced combination of both.\nIllustrative examples of this are provided in Fig. 9, where\nFig. 9 (a) showcases results that lean heavily towards the\nreference images, while Fig. 9 (b) exhibits only the source\nidentity. Additionally, we observed instances of information\nspill when AR loss is not applied. Fig. 9 (c) illustrates that\nwithout AR, information spill is evident in the generated\nearrings, indicating that the source special tokens attend to\nnon-facial regions. Finally, we note that the CSIM score ex-\nhibits a minor decline following post-processing. Although\nthe post-processed results are generally visually appealing,\nthe face restoration model possesses a level of freedom that\ncan occasionally lead to a slight reduction in the similarity\nscore. The results of samples before and after applying the\npost-processing are displayed in Fig. 10.\nCurated Results\nWe demonstrate more results from Fig. 12 to Fig. 17.\nFigure 10: Generated results before and after post-\nprocessing.\nFigure 11: Results for composing the source content and the reference style in non-human images.\nFigure 12: Curated results of MagiCapture.\nFigure 13: Curated results of MagiCapture.\nFigure 14: Curated results of MagiCapture.\nFigure 15: Curated results of MagiCapture.\nFigure 16: Curated results of MagiCapture.\nFigure 17: Curated results of MagiCapture.\n"
  },
  {
    "title": "Large Language Models for Compiler Optimization",
    "link": "https://arxiv.org/pdf/2309.07062.pdf",
    "upvote": "22",
    "text": "Large Language Models for Compiler Optimization\nChris Cummins\u2020\u2217, Volker Seeker\u2020, Dejan Grubisic\u2020,\nMostafa Elhoushi, Baptiste Roziere, Jonas Gehring, Fabian Gloeckle,\nKim Hazelwood, Gabriel Synnaeve, Hugh Leather\u2020\nMeta AI\nYouwei Liang\nUC San Diego\nAbstract\u2014We explore the novel application of Large Language\nModels to code optimization. We present a 7B-parameter trans-\nformer model trained from scratch to optimize LLVM assembly\nfor code size. The model takes as input unoptimized assembly\nand outputs a list of compiler options to best optimize the\nprogram. Crucially, during training, we ask the model to predict\nthe instruction counts before and after optimization, and the\noptimized code itself. These auxiliary learning tasks significantly\nimprove the optimization performance of the model and improve\nthe model\u2019s depth of understanding.\nWe evaluate on a large suite of test programs. Our approach\nachieves a 3.0% improvement in reducing instruction counts over\nthe compiler, outperforming two state-of-the-art baselines that\nrequire thousands of compilations. Furthermore, the model shows\nsurprisingly strong code reasoning abilities, generating compilable\ncode 91% of the time and perfectly emulating the output of the\ncompiler 70% of the time.\nI. INTRODUCTION\nThere is increasing interest in Large Language Models\n(LLMs) for software engineering domains such as code gener-\nation [1\u20139], code translation [10\u201312], and code testing [13\u201315].\nModels such as Code Llama [9], Codex [8], and ChatGPT [16]\nhave a good statistical understanding of code and suggest likely\ncompletions for unfinished code, making them useful for editing\nand creating software. However, it appears they have not been\ntrained specifically to optimize code. ChatGPT, for instance,\nwill make minor tweaks to a program such as tagging variables\nto be stored as registers, and will even attempt more substantial\noptimizations like vectorization, though it easily gets confused\nand makes mistakes, frequently resulting in incorrect code.\nPrior works on machine learning-guided code optimization\nhave used hand-built features [17\u201319], all the way to graph\nneural networks (GNNs) [20, 21]. However, in all cases, the\nway the input program is represented to the machine learning\nalgorithm is incomplete, losing some information along the way.\nFor example, MLGO [17] uses numeric features to provide\nhints for function inlining, but cannot faithfully reproduce\nthe call graph or control flow, etc. PrograML [21] forms\ngraphs of the program to pass to a GNN, but it excludes\nthe values for constants and some type information which\nprevents reproducing instructions with fidelity.\nIn this work, we ask: can Large Language Models learn to\noptimize code? LLMs can accept source programs, as is, with\na complete, lossless representation. Using text as the input\nand output representation for a machine learning optimizer has\n\u2020 Core contributors. *Corresponding author: cummins@meta.com\ndesirable properties: text is a universal, portable, and accessible\ninterface, and unlike prior approaches is not specialized to any\nparticular task.\nWe started our investigation into the code-optimizing power\nof LLMs by replicating the optimizing transformations present\nin compilers, targeting the industry standard LLVM [22]\ncompiler. LLVM\u2019s optimizer is extremely complex and contains\nthousands of rules, algorithms, and heuristics in over 1M\nlines of C++ code. Our expectation was that while LLMs\nhave shown great progress in natural language translation and\ncode generation tasks, they would be incapable of emulating\nsuch a complex system. Understanding and applying compiler\noptimizations require multiple levels of reasoning, arithmetic\ncomputation capabilities, and applying complex data structure\nand graph algorithms, which are capabilities LLMs have shown\nto lack [23, 24].\nWe thought this would be a paper about the obvious failings\nof LLMs that would serve as motivation for future clever ideas\nto overcome those failings. We were entirely taken by surprise\nto find that in many cases a sufficiently trained LLM can\nnot only predict the best optimizations to apply to an input\ncode, but it can also directly perform the optimizations without\nresorting to the compiler at all!\nOur approach is simple. We begin with a 7B-parameter LLM\narchitecture, taken from LLaMa 2 [25], and initialize it from\nscratch. We then train it on millions of examples of LLVM\nassembly, coupled with the best compiler options found by a\nsearch for each assembly, as well as the resulting assembly from\nperforming those optimizations. From these examples alone\nthe model learns to optimize code with remarkable accuracy.\nOur singular contribution is the first application of LLMs to\noptimizing code. We construct LLMs solely for the purpose\nof compiler optimization and show that they achieve a single-\ncompile 3.0% improvement in code size reduction over the\ncompiler versus a search-based approach which achieves 5.0%\nwith 2.5e9 compilations and versus state of the state-of-the-art\nML approaches that cause regressions and require thousands\nof compilations. We provide auxiliary experiments and code\nexamples to further characterize the potential and limits of\nLLMs for code reasoning. Overall we find their efficacy\nremarkable and think that these results will be of interest\nto the community.\narXiv:2309.07062v1  [cs.PL]  11 Sep 2023\nFigure 1: Overview of our approach, showing the model input (Prompt) and output (Answer) during training and inference. The prompt contains unoptimized\ncode. The answer contains an optimization pass list, instruction counts, and the optimized code. During inference we generate only the optimization pass list\nwhich we feed into the compiler, ensuring that the optimized code is correct.\nTable I: Training data. Each LLVM-IR function is autotuned and used to create\na (Prompt, Answer) pair. The n tokens column shows the number of tokens\nwhen the prompt is encoded using the Llama 2 [25] tokenizer.\nn\nfunctions\nunoptimized\ninstruction\ncount\nsize on\ndisk\nn tokens\nHandwritten\n610,610\n8,417,799\n653.5 MB\n214,746,711\nSynthetic\n389,390\n13,775,149\n352.3 MB\n158,435,151\nTotal\n1,000,000\n16,411,249\n1.0 GB\n373,181,862\nII. PASS ORDERING WITH LLMS\nIn this work we target compiler pass ordering. The pass\nordering task is to select from the set of optimizing transforma-\ntion passes available in a compiler the list of passes that will\nproduce the best result for a particular input code. Manipulating\npass orders has been shown to have a considerable impact on\nboth runtime performance and code size [19, 26].\nMachine learning approaches to this task have shown\ngood results previously, but struggle with generalizing across\ndifferent programs [27]. Previous works usually need to compile\nnew programs tens or hundreds of times to try out different\nconfigurations and find out the best-performing option, making\nthem impractical for real-world use. We hypothesized that a\nlarge language model with sufficient reasoning power would\nbe able to learn to make good optimization decisions without\nneeding this.\nMost prior work on LLMs for code operates on source\nlanguages such as Python. Instead, for the pass ordering\nproblem we require reasoning at the lower level of compiler\nassembly, known as the Intermediate Representation (IR).\nTable II: Test data.\nn\nfunctions\nunoptimized\ninstruction\ncount\n-Oz\ninstruction\ncount\nAI-SOCO [31]\n8,929\n97,800\n47,578\nExeBench [32]\n26,806\n386,878\n181,277\nPOJ-104 [33]\n310\n8,912\n4,492\nTranscoder [12]\n17,392\n289,689\n129,611\nCSmith [34]\n33,794\n647,815\n138,276\nYARPGen [35]\n12,769\n285,360\n144,539\nTotal\n100,000\n1,716,354\n645,773\nWhile there exist curated datasets of source languages for\npretraining LLMs (e.g. [28\u201330]), compiler IRs do not make up\na significant portion of these datasets, and though models like\nChatGPT show some promise of understanding, their ability\nto reason about IR is far inferior to source languages.\nWe target optimizing LLVM pass orders for code size as\nin prior works [17, 27], using IR instruction count as an\n(imperfect) proxy for binary size. The approach is agnostic to\nthe chosen compiler and optimization metric, and we intend to\ntarget runtime performance in the future. For now, optimizing\nfor code size simplifies the collection of training data.\nA. Prompts\nWe present the model with an unoptimized LLVM-IR (such\nas emitted by the clang frontend) and ask it to produce a list\nof optimization passes that should be applied to it. Figure 1\nshows the format of the input prompt and output text.\nIn this work, we target LLVM 10 and use the optimization\nflags from opt. There are 122 optimization passes to choose\n2\nfrom and passes can be selected more than once in a single\nsequence. We also include the 6 meta-flags (-O0, -O1, -O2,\n-O3, -Oz, and -Os) that may each occur only once per pass\nlist. Pass lists can be any length, though in our experiments\nwe found typically up to 9 passes long, for a combinatorial\nsearch space of around 1018.\nAs shown in Figure 1, we also include two auxiliary tasks:\ni) generating the instruction counts of the code before and\nafter the optimizations are applied and ii) generating the output\nIR after the optimizations are applied. We hypothesize that\nthese would enable better pass-ordering decisions by forcing\na deep understanding of the mechanics of code optimization.\nWe verify this experimentally in Section V-B.\nWhile the model is trained to generate instruction counts\nand optimized IR, we do not need those auxiliary tasks for\ndeployment. All we need to do is generate the pass list which\nwe then execute using the compiler. We thus sidestep the\nproblems of correctness that plague techniques that require the\noutput of the model to be trustworthy [10\u201312, 36].\nB. LLVM-IR Normalization\nWe normalize the LLVM-IR that is used for training the\nLLM using the following rules: we discard comments, debug\nmetadata and attributes, and ensure consistent whitespace by\nfeeding the IR through a custom lexer that retains newlines but\nstandardizes other whitespace and strips indentation. We do\nthis to reduce the length of the LLVM-IR to make maximum\nuse of the limited input size of the LLM (Section III-A). The\ncode in Figure 1 has been processed in this manner.\nIII. THE MODEL\nWe use the ubiquitous transformer architecture [37]. The\ntransformer is an artificial neural network that employs self-\nattention over a fixed-size context window.\nThe input text is first tokenized into words and subword units.\nThese are embedded into continuous vector representations and\nprovided as input to the transformer\u2019s encoder, where self-\nattention mechanisms capture contextual relationships between\ntokens to encourage the model to understand and process the\ninput text\u2019s semantic structure.\nThe output text is produced by iteratively generating one\ntoken at a time. The decoder takes the encoded input along\nwith any previously generated tokens and uses self-attention\nto predict the next token in the sequence. We greedily sample\nduring decoding to select the most likely token sequence. This\nprocess continues until an end-of-sequence token is generated\nor a predefined maximum length is reached.\nA. Model Architecture\nWe use the same model architecture and Byte Pair Encoding\n(BPE) [38] tokenizer as Llama 2 [25], but train our model from\nscratch. We use the smallest of the Llama 2 configurations: 32\nattention heads, 4,096 hidden dimensions, and 32 layers, for a\ntotal of 7B parameters.\nThe maximum length of a (prompt, answer) pair is defined\nby the sequence length. In this work, we use a sequence length\n(a) Performance of generated pass lists.\n(b) Accuracy at predicting instruction counts.\n(c) Model-optimized code metrics.\nFigure 2: Performance on holdout validation set during training. We evaluate\nperformance every 250 training steps (131M train tokens). Parity with -Oz is\nreached at 393M tokens and peak performance at 10.9B tokens.\nof 2,048 tokens. The Llama 2 tokenizer achieves an average\nof 2.02 characters per token when encoding LLVM-IR, so this\nprovides an approximate upper limit on the longest LLVM-IR\nwe can train on at 2KB (since 2KB prompt and 2KB answer\n\u2248 2,048 tokens).\nB. Training Data\nWe assembled a large corpus of unoptimized LLVM-IR\nfunctions, summarized in Table I. We extracted the functions\nfrom datasets of publicly available handwritten C/C++ code\nand supplemented this with synthetic code generated by C/C++\ncompiler test generators. In total, our training corpus comprises\n1,000,000 deduplicated IR functions, totaling 373M training\ntokens. We operate at the level of individual IR functions rather\nthan entire modules to maximize the amount of data we can\nfit inside a 2,048-token sequence length.\nTo find the list of optimization passes that will produce the\nsmallest instruction count we employ autotuning. Our autotuner\ncombines random search and all-to-all results broadcasting\nbetween functions, inspired by the work of Liang et. al. [20].\n3\nTable III: Performance of different approaches to pass ordering on a test set of unseen LLVM-IR functions from Table II. All metrics are w.r.t. -Oz. Instructions\nsaved is summed over functions improved and instructions regressed is summed over functions regressed. Overall improvement is the sum total instruction\ncount savings w.r.t -Oz. The Autotuner achieves the best performance but requires 2.5B additional compilations (949 CPU-days). Our approach achieves 60%\nof the gains of the autotuner without invoking the compiler once.\nadditional\ncompilations\nfunctions\nimproved\nfunctions\nregressed\ninstructions\nsaved\ninstructions\nregressed\noverall\nimprovement\nAutotuner\n2,522,253,069\n6,764\n0\n30,948\n0\n5.03%\nAutoPhase [39]\n4,500,000\n1,558\n8,400\n6,522\n32,357\n-3.85%\nCoreset-NVP [20]\n442,747\n3,985\n6,072\n16,064\n28,405\n-1.88%\nOur Approach\n0\n4,136\n526\n21,935\n3,095\n3.01%\nTable IV: Extending the models in Table III with \u201c-Oz backup\u201d. If a model\npredicts a pass list other than -Oz, it also evaluates -Oz and selects the best.\nThis prevents regressions w.r.t -Oz at the expense of additional compilations.\nadditional compilations\noverall improvement\nAutoPhase [39]\n4,600,000\n1.02%\nCoreset-NVP [20]\n542,747\n2.55%\nOur Approach\n5,721\n3.52%\nFor each function we run random search for a fixed amount\nof time (780 seconds) and then minimize the best pass list\nby iteratively removing individual randomly chosen passes to\nsee if they contribute to the instruction count. If not, they are\ndiscarded. After performing this on each of the functions we\naggregate the set of unique best pass lists and broadcast them\nacross all other functions. Thus, if a pass list was found to\nwork well on one function it is tried on all others.\nIn total, the autotuner compiled each training program an\naverage of 37,424 times, achieving a 5.8% improvement in\ninstruction count reduction over the baseline fixed pass ordering\nin the compiler provided by -Oz. For our purposes, this\nautotuning serves as a gold standard for the optimization of\neach function. While the instruction count savings discovered\nby the autotuner are significant, the computational cost to reach\nthese wins was 9,016 CPU days. The goal of this work is to\nachieve some fraction of the performance of the autotuner\nusing a predictive model that does not require running the\ncompiler thousands of times.\nC. Training\nStarting from randomly initialized weights, we trained the\nmodel for 30,000 steps on 64 V100s for a total training time\nof 620 GPU days. We use the AdamW optimizer [40] with \u03b21\nand \u03b22 values of 0.9 and 0.95. We use a cosine learning rate\nschedule with 1,000 warm-up steps, a peak learning rate of\n1e\u22125, and a final learning rate of 1/10th of the peak. We used\na batch size of 256 and each batch contains 524,288 tokens\nfor a total of 15.7B training tokens. The full 30,000 steps of\ntraining is 7.7 epochs (iterations over the training corpus).\nDuring training, we evaluated the model on a holdout\nvalidation set of 1,000 unseen IRs that were processed in\nthe same manner as the training set. We evaluate every 250\nsteps.\nIV. EVALUATION\nIn this section, we evaluate the ability of the model to\ngenerate pass lists for unseen code and to correctly perform\noptimization.\nA. Training Results\nFigure 2 shows the performance during training when\nevaluated on a holdout validation set of 1,000 unseen LLVM-IR\nfunctions. Peak validation performance was achieved by the\nmodel at 10.9B training tokens.\nAt peak performance, the code optimized using model-\ngenerated pass sequences contains 4.4% fewer instructions than\nwhen optimized using the compiler\u2019s built-in pass ordering\n(-Oz). The autotuner achieves a greater instruction count\nreduction of 5.6%, but this required 27 million compilations\nof the validation set. The model makes its predictions without\ninvoking the compiler once.\nFigure 2b shows the error of predicted input and out-\nput instruction counts. Prediction of instruction counts for\nunoptimized code rapidly approaches near-perfect accuracy.\nPrediction of output instruction count proves more challenging,\nreaching a Mean Average Percentage Error (MAPE) of 5.9%.\nFigure 2c evaluates the quality of the generated code using\nthree metrics. The BLEU [41] score shows the similarity\nbetween the model-generated code and a reference ground-\ntruth code produced by the compiler using the generated pass\nlist. Code compiles is the frequency that model-generated code\ncompiles without error. Exact match tracks the frequency that\nthe model-generated code is a character-by-character match\nof the compiler-generated code when optimized using the\ngenerated pass list (i.e. how many times BLEU=1).\nAt peak performance, the model achieves an impressive\n90.5% rate of generating code that compiles without errors.\nFurthermore, a BLEU score of 0.952 shows that the model-\noptimized code closely approximates that of the compiler, and\nthe exact match frequency is 70%. For comparison, a baseline\nthat simply copies the unoptimized code to the output would\nachieve a BLEU score of 0.531 and an exact match frequency\nof 0%, demonstrating that significant manipulation of the input\ncode is required to achieve such high scores.\nBy the end of training performance on the validation set\nhad plateaued. We use the best-performing checkpoint and\nswitch to a 100\u00d7 larger-scale evaluation for the remainder of\nthe evaluation.\nB. Comparison to State-of-the-Art\nIn this experiment, we perform a large-scale evaluation of the\nLLM\u2019s ability to predict pass lists in comparison to baselines.\n4\nFigure 3: Frequency that passes occur in the pass list for each of the 100,000 test programs (left), and the length of pass lists (right). -Oz is the starting point\nfor the autotuner and is the dominant result, being the best-found result for 93.2% of autotuned test programs and appearing in an additional 0.6% of pass lists\nas part of a longer sequence. The model-generated pass distribution tracks the autotuner but slightly overpredicts -Oz (94.3%) and includes 9 passes that the\nautotuner used on the training set but not on the test set. Results are ordered by decreasing autotuner frequency.\ndefine i32 @f1(i8 %0) {\n%2 = alloca i32, align 4\n%3 = alloca i8, align 1\nstore i8 %0, i8* %3, align 1\n%4 = load i8, i8* %3, align 1\n%5 = zext i8 %4 to i32\n%6 = icmp sge i32 %5, 65\nbr i1 %6, label %7, label %15\n7:\n%8 = load i8, i8* %3, align 1\n%9 = zext i8 %8 to i32\n%10 = icmp sle i32 %9, 90\nbr i1 %10, label %11, label %15\n11:\n%12 = load i8, i8* %3, align 1\n%13 = zext i8 %12 to i32\n<snip 21 lines...>\n33:\n%34 = load i32, i32* %2, align 4\nret i32 %34\n}\n(a) Input code (39 instructions).\ndefine i32 @f1(i8 %0) {\n%2 = zext i8 %0 to i32\n%.off = add i8 %0, 191\n%3 = icmp ult i8 %.off, 26\nbr i1 %3, label %4, label %6\n4:\n%5 = add nsw i32 %2, 191\nbr label %10\n6:\n%.reload16.off = add nsw i32 %2, 159\n%7 = icmp ult i32 %.reload16.off, 26\nbr i1 %7, label %10, label %8\n8:\n%9 = icmp eq i8 %0, 32\n%. = select i1 %9, i32 26, i32 1\nbr label %10\n10:\n%.0.reg2mem.0 = phi i32 [%5, %4],\n[%., %8], [%.reload16.off, %6]\nret i32 %.0.reg2mem.0\n}\n(b) Autotuned code (14 instructions) using passes:\n-reg2mem -instcombine -Os -O1.\ndefine i32 @f1(i8 %0) {\n%2 = zext i8 %0 to i32\n%.off = add i8 %0, 191\n%3 = icmp ult i8 %.off, 26\nbr i1 %3, label %6, label %._crit_edge\n._crit_edge:\n%.off24 = add i8 %0, 159\n%4 = icmp ult i8 %.off24, 26\nbr i1 %4, label %6, label %._crit_edge9\n._crit_edge9:\n%5 = icmp eq i8 %0, 32\n%spec.select = select i1 %5,\ni32 26, i32 1\nret i32 %spec.select\n6:\n%.sink = phi i32 [191, %1],\n[159, %._crit_edge]\n%7 = add nsw i32 %.sink, %2\nret i32 %7\n}\n(c) Model-optimized code (13 instructions) and\npass\nlist:\n-reg2mem -simplifycfg -mem2reg\n-jump-threading -Os.\nListing 1: An example IR function where the model suggests a better pass list than the autotuner, despite having never seen this code before. For this function\nthe autotuner tried 26k different pass orderings. The pass list generated by the model appears 5 times in the training set of 1,000,000 examples.\nDatasets We aggregate a broad suite of benchmark datasets\nfor evaluation, summarized in Table II. We deduplicate and\nexclude IR functions identical to those we trained on. Our test\ndata comprises code from a variety of domains including coding\ncompetitions (AI-SOCO [31], POJ-104 [33]), compiler test case\ngenerators (CSmith [34], YARPGen [35]), and miscellaneous\npublicly available code (ExeBench [32], Transcoder [12]).\nBaselines We compare our approach to three baselines:\nAutoPhase [39], Coreset-NVP [20], and the Autotuner.\nAutoPhase [39] is a reinforcement learning approach in\nwhich an agent is trained using Proximal Policy Optimiza-\ntion [42] to select the sequence of optimization passes that will\nmaximize cumulative instruction count savings over a fixed-\nlength episode. At each step, the program being optimized\nis represented to the agent as a 56-dimensional vector of\ninstruction counts and other properties. We replicate the\nenvironment of [39] but use the implementation and expanded\ntraining regime from [27] in which the agent is trained for\n100,000 episodes. We train the agent on the same data as\nour language model (Table I) and evaluate agent performance\nperiodically during training on a holdout validation set. As in\nprior works, we use an action space and episode length of 45.\nCoreset-NVP [20] is a technique that combines iterative\nsearch with a learned cost model. First, a greedy search is run\non 17,500 benchmarks to determine a Core set of best pass\nlists. Then a Neural Value Prediction (NVP) is trained on the\nresults of this search, using ProGraML [21] graphs processed\nby a Graph Convolutional Network as program representation.\nAt inference, Coreset-NVP predicts the normalized reward and\ntries the first few pass sequences with the highest normalized\n5\nFigure 4: Improvement over -Oz by dataset. Handwritten code optimizes more.\nreward. The total number of passes it is allowed to try for\neach benchmark is 45, following prior works. We use author-\nprovided model weights to perform inference on our test set.\nFinally, we compare it to the Autotuner that we used to\ngenerate training data. We autotuned the test dataset in the\nsame manner as the training data, described in Section III-B.\nResults Table III summarizes the results. Our approach\noutperforms -Oz, AutoPhase, and Coreset-NVP across all\ndatasets. Overall, the thousands of optimization attempts that\nare afforded to the autotuner enable it to discover the best-\nperforming pass lists.\nAutoPhase and Coreset-NVP are both able to identify pass\nlists that outperform -Oz but have an overall net negative impact\non instruction count due to a large number of regressions. We\npropose a simple \u201c-Oz backup\u201d extension to overcome this: if a\nmodel predicts a pass list other than -Oz, we also run -Oz and\nselect the best of the two options. This prevents regressions w.r.t.\n-Oz, but increases the number of additional compilations by the\nnumber of times the model predicts a pass list other than -Oz.\nTable IV shows the results of the techniques when evaluated in\nthis manner. While this does not help the models find further\nimprovements, the lack of regressions means that AutoPhase\nand Coreset-NVP now achieve overall improvements over -Oz,\nthough still less than the LLM with or without the -Oz backup.\nC. Evaluation of Generated Pass Lists\nFigure 3 shows the frequency with which passes are selected\nby the autotuner and our model from the previous experiment.\nThe distribution of passes selected by the model broadly\ntracks the autotuner. -Oz is the most frequently optimal pass.\nExcluding -Oz, model-generated pass lists have an average\nlength of 3.4 (max 10), and autotuner pass lists have an average\nlength of 3.1 (max 9). 105 of the pass lists generated by the\nmodel never appear in the training data.\nIn 710 cases the model-generated pass lists outperform the\nautotuner on the test set, though improvements are typically\nsmall. Listing 1 shows an example where the model-generated\nFigure 5: Improvement over -Oz by input size. Larger codes optimize more.\nTable V: Compiler errors of model-optimized code on 100,000 unseen inputs.\nerror category\nn\ntype error\n5,777\ninstruction forward referenced\n1,521\nundefined value\n1,113\ninvalid redefinition\n616\nsyntax error\n280\ninvalid value for constant\n144\nundefined function\n112\nindex error\n98\nother\n83\nTotal\n9,744\npass list simplifies control flow to fewer blocks, saving one\nfurther instruction.\nFigure 4 breaks down the improvement of each approach to\npass ordering by benchmark dataset. The biggest improvements\nover -Oz is found in the POJ-104 and Transcoder datasets,\nwhich both aggregate large amounts of handwritten code, while\nYARPGen, a random program generator for testing compilers,\nhas the fewest opportunities for improving over -Oz.\nWe discovered that there is a strong correlation between the\ninput program size and the potential performance improvement\nover -Oz that is found by both the autotuner and the model.\nFigure 5 plots this trend, showing clearly that larger programs\nhave more opportunities to improve over -Oz.\nD. Evaluation of Generated Code\nIn this section, we evaluate the quality of model-generated\ncode. To do this we ran the auxiliary training task of generating\noptimized code for all 100k functions in the test set. Note that\nthis is not required to generate the pass lists evaluated in\nthe previous section. We have made minor edits to the code\nsamples in this section for brevity such as omitting superfluous\nstatements and shortening identifier names.\nIn 90.3% of cases, the model-generated optimized IR com-\npiles, and in 68.4% of cases the output IR matches character-\nfor-character the ground truth generated by the compiler. We\ntaxonomize the different classes of errors for the 9.7% of cases\nwhere the generated IR does not compile in Table V, and\nListing 2 provides code examples.\n6\nerror: \u2019%15\u2019 defined with type \u2019i32\u2019 but expected \u2019i1\u2019\n%or.cond = or i1 %14, %15\n(a) The model defined %15 as an integer but later tried to use it as a bool\n(type error).\nerror: constant expression type mismatch\n@.str = private unnamed_addr constant [493 x i8]\nc\"<snip 492 chars ...>\", align 1\n(b) The model omitted a single character when transcribing a 493-character\nstring-literal from the input code (type error).\nerror: floating point constant invalid for type\n%1 = tail call i32 @f1(float -0.47799998483256463,\nfloat -1.8159999847412109)\n(c) LLVM requires exact decimal values for floating-point constants. These\nmodel-generated values have repeating decimals in binary so are rejected\n(invalid value for constant).\nListing 2: Compiler errors in model-optimized code.\ndefine hidden signext i8 @f1() #0 {\n%1 = alloca i64, align 8\nstore i64 3718042838174166437, i64* %1, align 8\n%2 = load i64, i64* %1, align 8\n%3 = trunc i64 %2 to i8\nret i8 %3\n}\n(a) Input unoptimized code.\ndefine hidden signext i8\n@f1() #0 {\nret i8 165\n}\n(b) Desired optimized code.\ndefine hidden signext i8\n@f1() #0 {\nret i8 1\n}\n(c) Model-generated code.\nListing 3: An example where the model generates compilable code but fails to\ncompute the correct answer for a numeric expression. Producing the correct\nresult for this expression requires non-trivial mathematical reasoning.\nRun passes -instcombine -simplifycfg to reduce\ninstruction count from 14 to 7:\ndefine dso_local i32 @f1(i32 %0) {\n%2 = load i64, i64* getelementptr inbounds(\n%struct.t2, %struct.t2* @gvar, i64 0, i32 0), align 8\n%3 = icmp eq i64 %2, 0\n%4 = icmp eq i32 %0, 0\n%or.cond = or i1 %3, %4\n%5 = load i32, i32* @S64_MAX, align 4\n%6 = select i1 %or.cond, i32 %5, i32 %0\nret i32 %6\n}\nListing 4: An example where the model generates correctly optimized code\nbut fails to produce the pass list needed to produce the desired code. The\nmodel-optimized code and instruction count predictions match the performance\nof the autotuner, but the model omitted the -mem2reg pass needed to achieve\nthis code. The model-generated pass list yields 10 instructions instead of 7.\ndefine i32 @f1(\ni32 %0,\ni32 %1\n) align 2 {\nbr label %3\n3:\n%i = phi i32 [%7, %6], [2, %2]\n%4 = mul nsw i32 %i, %i\n%5 = icmp sgt i32 %4, %1\nbr i1 %5, label %8, label %6\n6:\n%7 = add i32 %i, 1\nbr label %3\n8:\nret i32 2\n}\n(a) Desired optimized code.\nint f1(int x, int y) {\nint i = 2;\nwhile (i * i < y) {\ni += 1;\n}\nreturn 2;\n}\n(b) Equivalent (hand-written) C\ncode.\ndefine i32 @f1(\ni32 %0,\ni32 %1\n) align 2 {\nret i32 2\n}\n(c) Model-optimized code.\nListing 5: An example of an unsafe optimization by the model. The 33-\ninstruction input program (not shown) contains a loop that is not always safe\nto optimize away. For example, when y = INT_MAX the loop never terminates.\nFigure 6: Model-optimized code quality as a function of the performance of\nthe generated pass list. Code quality is lower when the pass list performs worse\nthan -Oz. The model-optimized code resembles the ground truth less (lower\nBLEU score), the code is less likely to compile, and the model struggles to\nestimate the instruction count (higher error). Error bars show 95% confidence\nintervals.\nMost challenging to evaluate are the 21.9% of cases where\nthe model-optimized code compiles but is not a character-by-\ncharacter match with the compiler. There are two challenges:\nthe first is that text precision metrics such as BLEU score are\nsensitive to differences in the code such as variable names and\ncommutative operand order that do not affect the behavior\nof the code. Tools like LLVM-Canon [43] can help here\nbut come with their own set of drawbacks. However, in\nmany cases, it is unclear whether the behavior of two IRs\nis the same, so the second challenge we face is in evaluating\nsemantic equivalency. Since not all of the datasets we use for\ntesting provide driver scripts and input datasets for their code,\nwe cannot use execution-based equivalence checks such as\ndifferential testing [44].\nListing 3 shows an example of model-generated code that\nhas incorrect program semantics. Here, the lower 8 bits of a\n64-bit literal are truncated and returned. The compiler performs\nthis calculation and substitutes the correct value. The model\n7\nFigure 7: Training a model to predict single optimization passes. The top subplot evaluates the quality the of generated code for the corresponding pass\n(ordered by BLEU score). The bottom subplot shows the frequency that the corresponding pass contributed to an improvement or regression of instruction\ncount over -Oz.\nFigure 8: Ablating the impact of training data size and the auxiliary co-training\ntask of generating optimized code (denoted No Aux). Data size is measured as\na number of training examples. The graph shows performance on a holdout\nvalidation set during training.\nTable VI: Ablation experiments. We evaluate the impact of varying training\ndata size and of training the model to generate the optimized code. We train\neach model for 30k steps and report performance of the best model checkpoint\non a holdout validation set of 1,000 unseen IR functions.\nn training\nexamples\ngenerate\noptimized code?\noverall\nimprovement\n1,000,000\n\u2713\n4.95% (\u2014)\n500,000\n\u2713\n3.91% (-21%)\n250,000\n\u2713\n3.74% (-24%)\n1,000,000\n\u00d7\n4.15% (-16%)\nrecognizes that the expression can be calculated at compile time\nbut fails to compute the correct value. This type of mathematical\nreasoning is a known weakness of LLMs [24].\nSometimes the model generates correctly-optimized code\nbut fails to produce the pass list needed to achieve it. Listing 4\nshows one such example. A further class of error is when the\nmodel makes unsafe optimizations by failing to analyze the\ninput code. Listing 5 shows an example.\nWe observe an interesting connection between the quality\nof pass lists and the corresponding optimized code, shown in\nFigure 6. When the model produces a poor-performing pass\nlist, the quality of the generated code is lower.\nV. ADDITIONAL EXPERIMENTS\nIn the previous section, we evaluated the performance of\nan LLM trained to optimize LLVM-IR for code size. In this\nsection, we build additional models to better understand the\nproperties of LLMs for code optimization. All models use the\nsame architecture and parameters as in Section III.\nA. Abalation of Dataset Size\nWe ablate the contribution of dataset size by training two\nadditional models and varying the amount of the training data\nfrom 50% (500k examples) down to 25% (250k examples)\nby random dropout. Figure 8 shows progress during the\ntraining of the models. For dataset sizes of 50% and 25%,\nthe models begin to overfit the training set after around 8B\ntraining tokens. Table VI shows the peak performance of\neach configuration. With 50% and 25% of the training data,\ndownstream performance falls by 21% and 24%, respectively.\nB. Abalation of Code Optimization Task\nWe train the model to generate not just a pass list but also the\noptimized code resulting from this pass list. One may expect\nthis to degrade model performance \u2013 not only must it learn\nto predict good pass lists, but also how to produce correctly\noptimized code, a more difficult task. In fact, we believe this\nto be crucial to model performance. By forcing LLMs to learn\nthe semantics of LLVM-IR we enable them to make better\noptimization decisions.\nTo ablate this we trained a model to generate only pass lists\nwithout the corresponding optimized code. We kept the data\nmix and all other parameters the same. Figure 8 and Table VI\nshow that without training the model to generate optimized\ncode, downstream performance falls by 16%.\n8\nOptimize the following LLVM-IR using -name-anon-globals:\n@0 = private\n@anon.2ef3bda806391c61822366a2a59f2569.0 = private\n@anon.95277a486ffed0b6ba33ab3385b3d7bd.0 = private\n,\u2192unnamed_addr constant [14 x i8] c\"<snip>\", align 1\ndefine dso_local i32 @f1(i8* %0) {\n%2 = call i32 @f2(i8* %0, i8* getelementptr inbounds(\n,\u2192[14 x i8], [14 x i8]*\n,\u2192@0,\n@anon.2ef3bda806391c61822366a2a59f2569.0,\n@anon.95277a486ffed0b6ba33ab3385b3d7bd.0,\n,\u2192i64 0, i64 0))\nret i32 %2\n}\n(a) Failure due to incomplete information. The -name-anon-globals pass\nuses the module name to compute a hash. Lacking this, the model hallucinates\na random hash.\nOptimize the following LLVM-IR using -instcombine:\n@var_12 = external dso_local global i64, align 8\n@var_13 = external dso_local global i32, align 4\n@var_14 = external dso_local global i32, align 4\ndefine dso_local void @f1(i64 %arg) {\n%tmp = alloca i64, align 8\nstore i64 %arg, i64* %tmp, align 8\n%tmp1 = load i64, i64* %tmp, align 8\n%tmp2 = sub i64 0, %tmp1\n%tmp3 = sub i64 0, %tmp2\nstore i64 %tmp3, i64* @var_12, align 8\nstore i64 %arg, i64* @var_12, align 8\nstore i64 0,\ni64* @var_12, align 8\nstore i32 1, i32* @var_13, align 4\nstore i32 0, i32* @var_14, align 4\nret void\n}\n(b) Failed data-flow analysis. The model correctly removes redundant instruc-\ntions but substites the wrong value for a variable. The model-optimized code\ncompiles and has a high BLEU score, but is incorrect.\nListing 6: Example failures from the pass translation experiment. We combine\nthe model input (red), ground-truth (blue), and model-generated (green) texts\ninto a single unified diff for brevity. Black text is common to all three.\nC. Evaluation of Single Pass Translation\nIn previous sections we trained LLMs to orchestrate opti-\nmization passes to produce the best-optimized code. In this\nsection, we evaluate the ability of LLMs to emulate the different\noptimizations in themselves. For this experiment, the model\ninput is an unoptimized IR and the name of an optimization\npass to apply, the output is the IR after applying this pass.\nDataset We generate a new dataset for this task using\n60 optimization passes and applying them randomly to the\nprograms from Table I. We augment the dataset of unoptimized\ncode with partially optimized code by first running a sequence\nof randomly selected passes on unoptimized IRs before the\ndesired target pass. We collect 10,000 unique (prompt, answer)\nexamples for each of the 60 passes for a total of 600k examples.\nModel We trained a new model from scratch on this pass\ntranslation dataset. It reached peak performance after 11B\ntraining tokens (74 GPU days).\nResults Figure 7 summarizes model performance. The aver-\nage BLEU score over all passes is 0.846, with exact character-\nListing 7: Example of correct generation of optimized IR. The model\nperformed several complex optimizations including control-flow simplification\nand replacing if-then-else code blocks with instructions.\nby-character matches 73.7% of the time and compilable code\n82.3% of the time. We also plot the frequency with which each\nof the optimizations appears in a model-generated pass list\nthat improved or regressed performance over -Oz in Table III.\nWe find no correlation between code quality metrics and its\nfrequency in generated pass lists.\nAs can be seen, many passes are learned near-perfectly while\nothers prove more challenging. Of the passes that perform\npoorly, some of them hint at simple improvements to the\nrepresentation while others result from deeper limitations of\nthe model\u2019s reasoning. Listing 6a shows an example from\nthe -name-anon-globals pass, which is a simple utility pass\nthat renames anonymous global variables using a hash of the\nmodule name. Since we do not provide the module name in\nthe prompt, the LLM is forced to hallucinate random values.\nWe will add the module name to prompts to address this.\nListing 6b shows an example from the -instcombine pass.\nThis is a complex pass that is implemented in over 4.5k lines of\nC++ code in LLVM. We see that the model correctly identifies\nthe instructions to combine, but makes an error in data flow\nanalysis and substitutes an incorrect value. This is an important\noptimization that frequently occurs in pass lists that outperform\n-Oz. We will explore an active learning approach in which more\n9\nexamples are provided for complex and difficult passes.\nFinally, we present an example of correct model optimization\nin Listing 7. The example combines several non-trivial code\nmanipulations: register allocation, control flow graph simplifica-\ntion, and instruction combining. We visualize the control- and\ndata-flow graphs to help interpret the changes that the model\nmade. Even on the scale of these small IR functions, we find\nthe sophisticated grasp of LLVM-IR semantics demonstrated\nby the LLM remarkable. The model has learned to perform\nthese optimizations entirely from examples, without access to\nthe compiler implementation.\nVI. DISCUSSION\nWe have shown that LLMs can near-perfectly emulate many\ncompiler optimizations and outperform prior approaches, but\nthere are limitations. This section aims to provide a pragmatic\ndiscussion of limits and directions for future research.\nA. Context Window\nThe main limitation of LLMs is the limited sequence length\nof inputs (context window). In this work we target 2k-token\ncontext windows and split IRs into individual functions to max-\nimize the amount of code we can fit into the context window.\nThis is undesirable for a number of reasons. First, it limits\nthe context available to the model when making optimization\ndecisions; second, it prevents intra-function optimization; third,\nwe cannot optimize code that does not fit within the context\nwindow. Figure 5 suggests that larger programs have more\ninteresting optimization opportunities.\nResearchers are adopting ever-increasing context win-\ndows [45], but finite context windows remain a common\nconcern with LLMs. As new techniques for handling long\nsequences continue to evolve we plan to incorporate them and\napply them to code optimization, e.g. Code Llama\u2019s variant\nof positional interpolation [46] which is RoPE base period\nscaling [9] or recent length extrapolation techniques [47].\nB. Math Reasoning and Logic\nCompilers perform lots of arithmetic. Whenever possible\nexpressions are evaluated at compile time to minimize work at\nruntime and to expose further opportunities for optimization.\nWe see examples of LLMs struggling with this type of\nreasoning, e.g. failed constant folding (Listing 3) and failed\ndata-flow analysis (Listing 6b).\nWe think that a chain-of-thought approach [48] in which\nmodels are taught to decompose complex reasoning problems\ninto incremental steps will prove fruitful. We took the first step\nin this direction by breaking optimizations down into individual\npasses in Section V-C. We also plan to focus training on a\ncurriculum of arithmetic and logic, and train LLMs that use\ntools to compute intermediate results [49, 50].\nC. Inference Speed\nCompilers are fast. It takes two orders of magnitude more\ntime for the model to generate a pass list than it does for\nthe compiler to execute it. While this is much faster than the\nautotuner it is trained on, it remains an overhead that may prove\nprohibitive for some applications. That is to say nothing of the\ndifference in compute resources needed to evaluate compiler\nheuristics vs. a 7B-parameter LLM running on multiple GPUs.\nIn addition to aggressive batching and quantization [51],\nsignificant inference speedups can be achieved by specializing\nthe vocabulary to a use case. For example, we can reduce entire\nsubsequences of passes to single vocabulary elements using\nByte Pair Encoding so that at inference time fewer tokens need\nto be generated.\nVII. RELATED WORK\nCompiler pass ordering for performance has been exploited\nfor decades [26, 52, 53]. Over the years there have been several\napproaches using machine learning [18\u201320, 39, 54, 55]. The\napplication of machine learning in compilers is not limited to\npass order and has been applied to many other problems [17,\n56\u201359]. No one has applied LLMs to the problem of pass\nordering, we are the first to do so.\nNeural machine translation is an emerging field that uses\nlanguage models to transform code from one language to\nanother. Prior examples include compiling C to assembly [11],\nassembly to C [36, 60], and source-to-source transpilation [10].\nIn these works code correctness cannot be guaranteed. In our\nwork we use code generation solely as an auxiliary learning\ntask \u2013 correctness is supplied by the compiler.\nLanguage models have found broad adoption for coding tasks,\nthough few operate at the level of compiler IR. Gallagher et al.\ntrain a RoBERTA architecture on LLVM-IR for the purpose\nof code weakness identification [61] and Transcoder-IR [12]\nuses LLVM-IR as a pivot point for source-to-source translation.\nNeither use LLMs for optimization as we do.\nMany language models have been trained on source\ncode including CodeBERT [62], GraphCodeBERT [63], and\nCodeT5 [64] which are trained to perform multiple tasks\nincluding code search, code summarization, and documentation\ngeneration. LLMs trained on source code have also been used\nfor program fuzzing [13, 14, 65], test generation [15], and\nautomated program repair [66\u201368]. A large number of useful\napplications have been explored for language models, however,\nthis is the first work where an LLM is used specifically for\noptimizing code.\nMost LLMs are trained at least partly on code [3, 5, 25,\n69]. Some LLMs are trained similarly to general models but\nespecially target programming languages and can be used for\ncode completion such as Codex [8] which powers Copilot [70].\nThe introduction of fill-in-the-middle capabilities is especially\nuseful for real-world code completion use cases and has\nbecome common in recent code models such as InCoder [6],\nSantaCoder [4], StarCoder [1], and Code Llama [9]. Code\nLlama was also trained to follow instructions and generate\ncode as well as explain its functionalities.\nWhile the multi-terabyte training corpora for these models\ncontain some assembly, we believe that a focused exploration\nof the value of LLMs in the domain of compilers will be of\nvalue to the community. This paper aims to provide that.\n10\nVIII. CONCLUSIONS\nWe present the first steps towards LLMs for code optimiza-\ntion. We construct a model that can predict good optimization\nstrategies for unseen LLVM-IR. Results are promising, though\nwe face challenges in sequence length which limits us to\noperating over small program fragments, and in arithmetic\nreasoning which limits the ability of the model to predict the\noutcome of optimizations. We hope to inspire the research\ncommunity to push beyond LLMs for simple max-likelihood\ncode generation and into performance-aware code optimization.\nREFERENCES\n[1]\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov,\nC. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E.\nZheltonozhskii, et al. \u201cStarCoder: may the source be with you!\u201d\nIn: arXiv:2305.06161 (2023).\n[2]\nY. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R.\nLeblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T.\nHubert, P. Choy, et al. \u201cCompetition-Level Code Generation\nwith AlphaCode\u201d. In: Science 378.6624 (2022).\n[3]\nOpenAI. \u201cGPT-4 Technical Report\u201d. In: arXiv:2303.08774\n(2023).\n[4]\nL. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M.\nFerrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, L. K.\nUmapathi, C. J. Anderson, et al. \u201cSantaCoder: don\u2019t reach for\nthe stars!\u201d In: arXiv:2301.03988 (2023).\n[5]\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann,\nP. Schuh, K. Shi, et al. \u201cPaLM: Scaling Language Modeling\nwith Pathways\u201d. In: arXiv:2204.02311 (2022).\n[6]\nD. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi,\nR. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis. \u201cInCoder:\nA Generative Model for Code Infilling and Synthesis\u201d. In:\narXiv:2204.05999 (2023).\n[7]\nS. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D.\nGiorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa,\nO. Saarikivi, A. Salim, S. Shah, et al. \u201cTextbooks Are All You\nNeed\u201d. In: arXiv:2306.11644 (2023).\n[8]\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto,\nJ. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,\nA. Ray, R. Puri, et al. \u201cEvaluating Large Language Models\nTrained on Code\u201d. In: arXiv:2107.03374 (2021).\n[9]\nB. Rozi\u00e8re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan,\nY. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov,\net al. \u201cCode Llama: Open Foundation Models for Code\u201d. In:\narXiv:2308.12950 (2023).\n[10]\nM.-A. Lachaux, B. Roziere, L. Chanussot, and G. Lample.\n\u201cUnsupervised Translation of Programming Languages\u201d. In:\narXiv:2006.03511 (2020).\n[11]\nJ. Armengol-Estap\u00e9 and M. F. O\u2019Boyle. \u201cLearning C to\nx86 Translation: An Experiment in Neural Compilation\u201d. In:\narXiv:2108.07639 (2021).\n[12]\nM. Szafraniec, B. Roziere, F. Charton, H. Leather, P. Labatut,\nand G. Synnaeve. \u201cCode Translation with Compiler Represen-\ntations\u201d. In: arXiv:2207.03578 (2022).\n[13]\nG. Ye, Z. Tang, S. H. Tan, S. Huang, D. Fang, X. Sun, L. Bian,\nH. Wang, and Z. Wang. \u201cAutomated conformance testing for\nJavaScript engines via deep compiler fuzzing\u201d. In: PLDI. 2021.\n[14]\nY. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang. \u201cLarge\nLanguage Models Are Zero-Shot Fuzzers: Fuzzing Deep-\nLearning Libraries via Large Language Models\u201d. In: ISSTA.\n2023.\n[15]\nM. Sch\u00e4fer, S. Nadi, A. Eghbali, and F. Tip. \u201cAdaptive Test Gen-\neration Using a Large Language Model\u201d. In: arXiv:2302.06527\n(2023).\n[16]\nOpenAI. ChatGPT. https://chat.openai.com/.\n[17]\nM. Trofin, Y. Qian, E. Brevdo, Z. Lin, K. Choromanski,\nand D. Li. \u201cMLGO: a Machine Learning Guided Compiler\nOptimizations Framework\u201d. In: arXiv:2101.04808 (2021).\n[18]\nZ. Wang and M. O\u2019Boyle. \u201cMachine Learning in Compiler\nOptimisation\u201d. In: arXiv:1805.03441 (2018).\n[19]\nH. Leather and C. Cummins. \u201cMachine Learning in Compilers:\nPast, Present and Future\u201d. In: FDL. 2020.\n[20]\nY. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi,\nJ. Guo, B. Steiner, X. Yang, P. Xie, H. Leather, and Y. Tian.\n\u201cLearning Compiler Pass Orders using Coreset and Normalized\nValue Prediction\u201d. In: ICML. 2023.\n[21]\nC. Cummins, Z. Fisches, T. Ben-Nun, T. Hoefler, M. O\u2019Boyle,\nand H. Leather. \u201cProGraML: A Graph-based Program Repre-\nsentation for Data Flow Analysis and Compiler Optimizations\u201d.\nIn: ICML. 2021.\n[22]\nC. Lattner and V. Adve. \u201cLLVM: A Compilation Framework\nfor Lifelong Program Analysis & Transformation\u201d. In: CGO.\n2004.\n[23]\nN. Asher, S. Bhar, A. Chaturvedi, J. Hunter, and S. Paul. \u201cLim-\nits for Learning with Language Models\u201d. In: arXiv:2306.12213\n(2023).\n[24]\nJ. Qian, H. Wang, Z. Li, S. Li, and X. Yan. \u201cLimitations of\nLanguage Models in Arithmetic and Symbolic Induction\u201d. In:\narXiv:2208.05051 (2022).\n[25]\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,\nY. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al.\n\u201cLlama 2: Open Foundation and Fine-Tuned Chat Models\u201d. In:\narXiv:2307.09288 (2023).\n[26]\nG. G. Fursin, M. F. P. O\u2019Boyle, and P. M. W. Knijnenburg.\n\u201cEvaluating Iterative Compilation\u201d. In: LCPC. 2005.\n[27]\nC. Cummins, B. Wasti, J. Guo, B. Cui, J. Ansel, S. Gomez,\nS. Jain, J. Liu, O. Teytaud, B. Steiner, Y. Tian, and H. Leather.\n\u201cCompilerGym: Robust, Performant Compiler Optimization\nEnvironments for AI Research\u201d. In: CGO. 2022.\n[28]\nD. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M.\nFerrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al.\n\u201cThe Stack: 3TB of Permissively Licensed Source Code\u201d. In:\narXiv:2211.15533 (2022).\n[29]\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\nJ. Phang, H. He, A. Thite, N. Nabeshima, et al. \u201cThe Pile: An\n800GB Dataset of Diverse Text for Language Modeling\u201d. In:\narXiv:2101.00027 (2020).\n[30]\nH. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M.\nBrockschmidt. \u201cCodeSearchNet Challenge: Evaluating the State\nof Semantic Code Search\u201d. In: arXiv:1909.09436 (2019).\n[31]\nA. Fadel, H. Musleh, I. Tuffaha, M. Al-Ayyoub, Y. Jararweh,\nE. Benkhelifa, and P. Rosso. \u201cOverview of the PAN@FIRE\n2020 task on the authorship identification of SOurce COde\n(AI-SOCO)\u201d. In: FIRE. 2020.\n[32]\nJ. Armengol-Estap\u00e9, J. Woodruff, A. Brauckmann, J. W. d. S.\nMagalh\u00e3es, and M. O\u2019Boyle. \u201cExeBench: an ML-scale Dataset\nof Executable C Functions\u201d. In: MAPS. 2022.\n[33]\nL. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. \u201cConvolutional\nNeural Networks Over Tree Structures for Programming\nLanguage Processing\u201d. In: AAAI. 2016.\n[34]\nX. Yang, Y. Chen, E. Eide, and J. Regehr. \u201cFinding and\nUnderstanding Bugs in C Compilers\u201d. In: PLDI. 2011.\n[35]\nV. Livinskii, D. Babokin, and J. Regehr. \u201cRandom Testing for\nC and C++ Compilers with YARPGen\u201d. In: OOPSLA. 2020.\n[36]\nJ. Armengol-Estap\u00e9, J. Woodruff, C. Cummins, and M. F.\nO\u2019Boyle. \u201cSLaDe: A Portable Small Language Model Decom-\npiler for Optimized Assembler\u201d. In: arXiv:2305.12520 (2023).\n[37]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. \u201cAttention Is All\nYou Need\u201d. In: NeurIPS (2017).\n11\n[38]\nP. Gage. \u201cA New Algorithm for Data Compression\u201d. In: C\nUsers Journal 12.2 (1994).\n[39]\nA. Haj-Ali, Q. Huang, W. Moses, J. Xiang, J. Wawrzynek,\nK. Asanovic, and I. Stoica. \u201cAutoPhase: Juggling HLS Phase\nOrderings in Random Forests with Deep Reinforcement Learn-\ning\u201d. In: MLSys. 2020.\n[40]\nI. Loshchilov and F. Hutter. \u201cDecoupled Weight Decay Regu-\nlarization\u201d. In: arXiv:1711.05101 (2017).\n[41]\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. \u201cBLEU: A\nMethod for Automatic Evaluation of Machine Translation\u201d. In:\nACL. 2002.\n[42]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O.\nKlimov. \u201cProximal Policy Optimization Algorithms\u201d. In:\narXiv:1707.06347 (2017).\n[43]\nM. Paszkowski. LLVM Canon. https : / / github . com /\nmichalpaszkowski/LLVM-Canon.\n[44]\nW. M. McKeeman. \u201cDifferential Testing for Software\u201d. In:\nDigital Technical Journal 10.1 (1998).\n[45]\nJ. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang,\nand F. Wei. \u201cLongNet: Scaling Transformers to 1,000,000,000\nTokens\u201d. In: arXiv:2307.02486 (2023).\n[46]\nS. Chen, S. Wong, L. Chen, and Y. Tian. \u201cExtending Context\nWindow of Large Language Models via Positional Interpola-\ntion\u201d. In: arXiv:2306.15595 (2023).\n[47]\nY. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim,\nV. Chaudhary, X. Song, and F. Wei. \u201cA Length-Extrapolatable\nTransformer\u201d. In: arXiv:2212.10554 (2022).\n[48]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,\nQ. V. Le, D. Zhou, et al. \u201cChain-of-thought prompting elicits\nreasoning in large language models\u201d. In: NeurIPS. 2022.\n[49]\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan,\nand G. Neubig. \u201cPal: Program-aided language models\u201d. In:\nICML. 2023.\n[50]\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,\nL. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,\net al. \u201cTraining Verifiers to Solve Math Word Problems\u201d. In:\narXiv:2110.14168 (2021).\n[51]\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S.\nHan. \u201cSmoothQuant: Accurate and Efficient Post-Training\nQuantization for Large Language Models\u201d. In: ICML. 2023.\n[52]\nF. Bodin, T. Kisuki, P. Knijnenburg, M. O\u2019Boyle, and E. Rohou.\n\u201cIterative Compilation in a Non-linear Optimisation Space\u201d. In:\nFDO. 1998.\n[53]\nT. Kisuki, P. Knijnenburg, and M. O\u2019Boyle. \u201cCombined\nSelection of Tile Sizes and Unroll Factors using Iterative\nCompilation\u201d. In: PACT. 2000.\n[54]\nF. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M.\nO\u2019Boyle, J. Thomson, M. Toussaint, and C. Williams. \u201cUsing\nMachine Learning to Focus Iterative Optimization\u201d. In: CGO.\n2006.\n[55]\nW. F. Ogilvie, P. Petoumenos, Z. Wang, and H. Leather.\n\u201cMinimizing the Cost of Iterative Compilation with Active\nLearning\u201d. In: CGO. 2017.\n[56]\nA. H. Ashouri, M. Elhoushi, Y. Hua, X. Wang, M. A. Manzoor,\nB. Chan, and Y. Gao. \u201cMLGOPerf: An ML Guided Inliner to\nOptimize Performance\u201d. In: arXiv:2207.08389 (2022).\n[57]\nA. Haj-Ali, N. K. Ahmed, T. Willke, S. Shao, K. Asanovic,\nand I. Stoica. \u201cNeuroVectorizer: End-to-End Vectorization with\nDeep Reinforcement Learning\u201d. In: CGO. 2020.\n[58]\nC. Cummins, P. Petoumenos, Z. Wang, and H. Leather. \u201cEnd-\nto-End Deep Learning of Optimization Heuristics\u201d. In: PACT.\n2017.\n[59]\nP. M. Phothilimthana, A. Sabne, N. Sarda, K. S. Murthy, Y.\nZhou, C. Angermueller, M. Burrows, S. Roy, K. Mandke,\nR. Farahani, et al. \u201cA Flexible Approach to Autotuning Multi-\npass Machine Learning Compilers\u201d. In: PACT. 2021.\n[60]\nI. Hosseini and B. Dolan-Gavitt. \u201cBeyond the C: Retar-\ngetable Decompilation using Neural Machine Translation\u201d. In:\narXiv:2212.08950 (2022).\n[61]\nS. K. Gallagher, W. E. Klieber, and D. Svoboda. LLVM\nIntermediate Representation for Code Weakness Identification.\n2022.\n[62]\nZ. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,\nL. Shou, B. Qin, T. Liu, D. Jiang, et al. \u201cCodeBERT: A Pre-\ntrained Model for Programming and Natural Languages\u201d. In:\narXiv:2002.08155 (2020).\n[63]\nD. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou,\nN. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng,\nC. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, and\nM. Zhou. \u201cGraphCodeBERT: Pre-training Code Representations\nwith Data Flow\u201d. In: arXiv:2009.08366 (2021).\n[64]\nY. Wang, W. Wang, S. Joty, and S. C. Hoi. \u201cCodeT5: Identifier-\naware Unified Pre-trained Encoder-Decoder Models for Code\nUnderstanding and Generation\u201d. In: arXiv:2109.00859 (2021).\n[65]\nC. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L.\nZhang. \u201cUniversal Fuzzing via Large Language Models\u201d. In:\narXiv:2308.04748 (2023).\n[66]\nC. S. Xia and L. Zhang. \u201cLess Training, More Repairing Please:\nRevisiting Automated Program Repair via Zero-shot Learning\u201d.\nIn: arXiv:2207.08281 (2022).\n[67]\nC. S. Xia, Y. Wei, and L. Zhang. \u201cAutomated Program Repair\nin the Era of Large Pre-Trained Language Models\u201d. In: ICSE.\n2023.\n[68]\nC. S. Xia and L. Zhang. \u201cKeep the Conversation Going: Fixing\n162 out of 337 bugs for $0.42 each using ChatGPT\u201d. In:\narXiv:2304.00385 (2023).\n[69]\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al.\n\u201cLlama: Open and efficient foundation language models\u201d. In:\narXiv preprint arXiv:2302.13971 (2023).\n[70]\nGitHub. Copilot. https://copilot.github.com/.\n12\n"
  },
  {
    "title": "Dynamic NeRFs for Soccer Scenes",
    "link": "https://arxiv.org/pdf/2309.06802.pdf",
    "upvote": "16",
    "text": "Dynamic NeRFs for Soccer Scenes\nSacha Lewin\nUniversity of Li\u00e8ge\nLi\u00e8ge, Belgium\nsacha.lewin@uliege.be\nMaxime Vandegar\nEVS Broadcast Equipment\nLi\u00e8ge, Belgium\nm.vandegar@evs.com\nThomas Hoyoux\nEVS Broadcast Equipment\nLi\u00e8ge, Belgium\nt.hoyoux@evs.com\nOlivier Barnich\nEVS Broadcast Equipment\nLi\u00e8ge, Belgium\no.barnich@evs.com\nGilles Louppe\nUniversity of Li\u00e8ge\nLi\u00e8ge, Belgium\ng.louppe@uliege.be\nFigure 1: Novel view synthesis in a synthetic dynamic environment, given 30 known views and camera poses.\nABSTRACT\nThe long-standing problem of novel view synthesis has many appli-\ncations, notably in sports broadcasting. Photorealistic novel view\nsynthesis of soccer actions, in particular, is of enormous interest\nto the broadcast industry. Yet only a few industrial solutions have\nbeen proposed, and even fewer that achieve near-broadcast quality\nof the synthetic replays. Except for their setup of multiple static\ncameras around the playfield, the best proprietary systems disclose\nclose to no information about their inner workings. Leveraging\nmultiple static cameras for such a task indeed presents a challenge\nrarely tackled in the literature, for a lack of public datasets: the re-\nconstruction of a large-scale, mostly static environment, with small,\nfast-moving elements. Recently, the emergence of neural radiance\nfields has induced stunning progress in many novel view synthesis\napplications, leveraging deep learning principles to produce photo-\nrealistic results in the most challenging settings. In this work, we\ninvestigate the feasibility of basing a solution to the task on dynamic\nNeRFs, i.e., neural models purposed to reconstruct general dynamic\ncontent. We compose synthetic soccer environments and conduct\nmultiple experiments using them, identifying key components that\nhelp reconstruct soccer scenes with dynamic NeRFs. We show that,\nalthough this approach cannot fully meet the quality requirements\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0269-3/23/10...$15.00\nhttps://doi.org/10.1145/3606038.3616158\nfor the target application, it suggests promising avenues toward a\ncost-efficient, automatic solution. We also make our work dataset\nand code publicly available, with the goal to encourage further\nefforts from the research community on the task of novel view syn-\nthesis for dynamic soccer scenes. For code, data, and video results,\nplease see https://soccernerfs.isach.be.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Computer vision representa-\ntions.\nKEYWORDS\n3D reconstruction, scene representation, dynamic, neural radiance\nfields, sports, soccer\nACM Reference Format:\nSacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles\nLouppe. 2023. Dynamic NeRFs for Soccer Scenes. In Proceedings of the 6th\nInternational Workshop on Multimedia Content Analysis in Sports (MMSports\n\u201923), October 29, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 9 pages.\nhttps://doi.org/10.1145/3606038.3616158\n1\nINTRODUCTION\nSynthesizing novel views of a scene from a sparse sample of im-\nages is a long-standing problem in computer vision [7, 18, 31]. A\nnotable field of application is sports broadcasting, in which action\nreplays have a major role in story-telling and performance analysis.\nAs one of the most popular sports, soccer receives a lot of broad-\ncast coverage from top to low-tier competitions all over the world,\nwith much care given to making the viewer experience ever more\npleasant and engaging. Augmenting the broadcast production of\nsoccer events with novel-view video synthesis of action replays is\narXiv:2309.06802v1  [cs.CV]  13 Sep 2023\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\nSacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe\ntherefore very attractive to industrial actors, and a real opportunity\nfor the computer vision research community.\nDespite the industry interest in novel view synthesis of soccer\nreplays, only a few proprietary systems exist on the market. In-\ndeed, such interest cannot outweigh the need for the highest image\nquality in broadcast productions; the industry, therefore, imposes\nvery high standards in terms of the photorealism of the synthesized\nviews. One noteworthy system [9] is able to deliver synthetic re-\nplays that are stunningly photorealistic, but for a few visual artifacts.\nTheir setup is composed of dozens of very high-resolution static\ncameras, installed all around the soccer field high up above the\nbleachers. Their image data are processed by private, proprietary\nsoftware running on very powerful hardware. These image data\nremain private as well, and equivalent public datasets are simply\nnonexistent. The only insight offered by this system to the research\ncommunity is the validity of using a static multi-camera setup for\nthe task.\nEven with no image data available, one can reason about the\nchallenges that arise from using an array of distant static cameras\nas a basis for the reconstruction of a soccer environment. Outdoor\nsports like soccer are composed of a large static environment, the\nstadium, and small dynamic elements, the players and the ball.\nTraditional computer vision methods would most likely have to\nrely on very high-resolution images, as in [9], to reconstruct an\nunderlying 3D model of the scene able to faithfully render the\nmovements of the small dynamic elements. Having to deal with\nmassive amounts of image data for reconstructing a single, short\nsoccer action is however not a desirable property for a solution.\nBuilding on the modern deep learning-based paradigm to com-\nputer vision problems, neural radiance fields [24] (NeRFs) have\nrecently become the state of the art for high-quality novel view\nsynthesis, and have been widely improved and extended to pro-\nduce excellent results in very challenging settings. A notable line of\nwork is dynamic NeRFs, i.e. neural models purposed to reconstruct\nspatiotemporal content, as opposed to only spatial, static content.\nThis, therefore, begs the question: Are dynamic NeRFs suitable for\nreconstructing soccer scenes? To find potential answers to this ques-\ntion, we propose this exploratory work, in which we make three\nimportant assumptions.\nFirst, we only consider camera setups similar to the one used\nby the aforementioned proprietary system [9], deeming it optimal\nfor the task at hand. Specifically, we use an array of 20 to 30 static\ncameras, positioned all around the soccer stadium and pointing\ntoward the soccer field. This assumption goes well with the working\nconditions usually recommended to achieve good performance with\nNeRFs. Moreover, most NeRFs assume input views to be calibrated\nby third-party Structure from Motion (SfM) tools, which are known\nto bring robust results with such camera setups in mostly static\nenvironments, such as a soccer stadium.\nSecond, we limit our study to synthetic soccer datasets, yet we\nbelieve its results also apply to real data. As already mentioned,\nsoccer image datasets with the considered camera setups are vir-\ntually nonexistent for the public, to the best of our knowledge.\nWe therefore composed synthetic datasets, using public computer\ngraphics engines and models. Because we control the cameras in\nour 3D virtual environments, this assumption also allows us to leave\ncamera calibration aspects out of the scope of our work. We are\nconfident that our findings remain valid when working on real use\ncases, given the availability of robust SfM tools, and the reputation\nof very good photorealism of NeRFs with real image data.\nThird, we only consider general dynamic NeRFs, i.e., dynamic\nNeRFs with no domain knowledge, to identify early limitations of\nthe neural-based reconstruction paradigm in the context of our task.\nAnother important reason is that domain-specific priors are often\ndifficult and expensive to produce. For instance, an accurate skeletal\nreconstruction of the players would be predictably very useful for\nsoccer replay synthesis, but is a hard task in itself, especially with\nthe considered camera setups. Our goal is to avoid resorting to such\npriors, which are likely to be complex and costly. This assumption\nalso has the advantage to make our study potentially insightful\nfor the use of dynamic NeRFs for other sports than soccer, given\nsimilar camera setups.\nWe select recent state-of-the-art general dynamic NeRF models\nand compare them in three synthetic soccer environments of in-\ncreasing complexity. Our aim is to progressively transition from\nideal conditions for the considered models, to conditions that are\nsimilar to the optimal camera setup used in [9].\nOur contributions could be summarized as follows:\n(1) We provide a study of the performance of general dynamic\nNeRFs on the task of soccer replay synthesis in increasingly\ncomplex environments. Models are studied as they were\nintroduced in the literature, then augmented with general,\nnon-domain specific components that we identify. We close\nthe study with a higher-level discussion about limitations\nand future work.\n(2) As we wish to foster research efforts toward solving this\nchallenging task, we publicly release our code, including\nthe improving components and experimental settings, and\nour complete work dataset, including images, depth maps,\nBlender [8] scripts, and camera calibrations for all synthetic\nenvironments. These are all ready-to-use in Nerfstudio [35],\na rich and popular open-source framework for using and\ndeveloping NeRF models.\nThe remainder of this paper is organized as follows. Section 2\nprovides preliminaries about NeRFs and introduces their extension\nto dynamic environments. Section 3 details our experimental setup:\nmethods, evaluation, and environments. Section 4 showcases and\ndiscusses results. Finally, Section 5 provides a higher-level discus-\nsion about the feasibility of using these methods, along with some\npaths for improvement.\n2\nNEURAL SCENE REPRESENTATION\nNeural Radiance Fields [24]. The original neural radiance field\n(NeRF) model implicitly encodes a scene in the weights of a multi-\nlayer perceptron (MLP). The model learns to associate density and\ncolor information to any point in space, which allows for render-\ning images using classical volume rendering [15, 23]. This process\nis end-to-end differentiable, which allows for training using only\ncaptured views and their associated camera poses. For improving\ntraining and rendering time, more recent methods [6, 26, 34] use a\nhybrid approach, leveraging both implicit and explicit representa-\ntions, such as voxel grids. Those methods store learnable features,\nwhich are then decoded with an MLP.\nDynamic NeRFs for Soccer Scenes\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\nDynamic NeRFs. Various techniques have been proposed to ex-\ntend NeRFs to dynamic reconstruction. Methods such as [11, 27,\n30] learn a separate field, known as a deformation network, that\nmaps each point to its corresponding position in a canonical scene.\nOther methods input time to the radiance field. While direct con-\nditioning on time provides poor results [30], indirect conditioning\n[1, 12, 13, 19, 33, 37] obtains state-of-the-art results on various pop-\nular benchmarks. Some models leverage domain knowledge, such\nas Human NeRFs. They often work by learning the motion of a\nskinned multi-person linear model (SMPL [22]) along with its ap-\npearance [28, 29, 42]. A more recent method supports human-object\ninteractions [21]. These specific models still require complex and\ncontrolled setups. Two non domain-specific models, K-planes and\nNeRFPlayer, are of particular interest to us, based on their state-\nof-the-art performance on diverse benchmarks, and the approach\nthey take to the reconstruction of dynamic content.\nK-Planes [13]. This model builds upon methods [6, 32] that fac-\ntorize the 4D space into 6 planes, corresponding to each pair of coor-\ndinates. This approach, and concurrent work [5, 36], offer greatly-\nimproved efficiency with high-quality results. The planes store\nfeature vectors uniformly in space and time, at increasing scales,\nsimilar to the multiresolution hash encoding used in [26]. The fea-\nture vectors associated with a given point in space and time are\nthen decoded by a shallow MLP into a density and an RGB color.\nK-Planes reaches state-of-the-art performance on various datasets.\nNeRFPlayer [33]. This method introduces two main contribu-\ntions: (i) a dynamic version of traditional explicit feature storage,\nsuch as the hash encoding from [26], by using a sliding window over\na larger fixed-size feature vector, and (ii) a scene decomposition into\ndifferent areas depending on their nature: static, deformed, or new.\nEach area is modeled with a different approach, which is mostly\nbeneficial to monocular setups. On common dynamic multi-view\ndatasets [19], NeRFPlayer reaches high-quality results, similar to\nK-Planes.\n3\nIMPLEMENTATION\nThe selected methods are K-Planes [13] and NeRFPlayer [33], out-\nlined in Section 2. These versions are implemented in Nerfstudio\n[35], an open-source framework that we use for all our experi-\nments. For fair comparisons, shared settings between the models\nare identical, such as proposal sampling and scene contraction [2].\nModel-specific hyperparameters follow the original implementa-\ntions, except for the model size. We increase the hash map size of\nNeRFPlayer to 220 with a temporal dimension of 64 and use Ner-\nfacto [35] as the backbone. We also drop the decomposition from\nNeRFPlayer, which mainly benefits monocular setups and results\nin unnecessarily large models. We add two additional scales to K-\nPlanes, resulting in multiscale resolutions from 26 to 211. When\nenabled, ray importance sampling based on global medians (ISG) is\nemployed [19]. Training follows typical Nerfstudio settings: models\nare trained for 30,000 iterations using Adam [17] with a learning\nrate of 10\u22122, which takes about 1 to 2 hours on an NVIDIA RTX\n3090 GPU for each scene. Unlike typical methods which train using\ndownsampled images for faster training, we observe improvements\nwhen using full-resolution 1080p images in our environments, with-\nout large increases in training time.\nWe make both our code and datasets publicly available. The for-\nmer includes slightly modified versions of K-Planes and NeRFPlayer,\nmore convenient data management for dynamic environments,\ntraining settings, and other components mentioned in Section 4,\nsuch as ray importance sampling and dedicated metrics. The latter\ninclude training images, calibrated poses, depth maps, Blender files,\nand data parsers to readily conduct experiments within Nerfstudio.\n3.1\nEvaluation\nThree metrics are typically used for assessing the visual quality of\nnovel view synthesis: (i) PSNR, which computes differences at the\npixel level, (ii) SSIM [38], which takes structural changes into ac-\ncount, and (iii) LPIPS [41], based on features in deep convolutional\nnetworks which better correlate with human judgment. Quantita-\ntive evaluation is known to be a difficult task in novel view syn-\nthesis applications and, sometimes, to hardly reflect visual quality\naccurately. Environments like ours make it even more challeng-\ning. Indeed, the dynamic content of interest is the players and the\nball, which occupy a small region of the images. As the metrics are\ncomputed over the whole image, they are barely affected by the\nreconstruction quality of small elements of interest. Furthermore,\nwe consider dynamic scenes, and computing per-frame metrics con-\nveys no information about the temporal consistency of the results.\nWhile the first issue can be tackled in synthetic environments\nby including additional views close to the content of interest, it is\noften not possible in real conditions. To address this, we propose\nalternative versions of these three metrics which are computed in\nrestricted bounding boxes around the dynamic content. The boxes\ncan be automatically generated by simply using an object detection\nmodel such as RetinaNet [20]. We refer to them as focused metrics.\nTo illustrate them, we compare default and focused metrics com-\nputed between one ground-truth evaluation image and novel views\ngenerated by four different versions of K-Planes, (a) to (d), where\nwe vary the depth and width of the MLP decoder, which causes\ndifferences in prediction quality. The predicted images and associ-\nated PSNRs are depicted in Fig. 2. The other metrics, i.e., SSIM and\nLPIPS, are reported in Tab. 1. The predictions highlight the neces-\nsity for alternative evaluation methods. In (a, b, c), the bleachers\nare poorly reconstructed, which strongly affects all metrics, as only\n(d) obtains a good score. However, the player is only missing in (a),\nwhich is better reflected by the focused metrics. Also, artifacts are\npresent around the player in (b), which underlines the need for not\nrestricting the bounding box right around the player.\nDespite these improvements, the new metrics still convey no\ntemporal information. Furthermore, they fail if the players or the\nball are not detected. For those reasons, qualitative evaluation is\nalways preferred. With a focus on assessing the reconstruction\nquality of the player, we render novel viewpoints along camera\npaths closer to the player than the distant views used for training.\nIn our synthetic scenes, we include additional close-up views for\nquantitative evaluation, which are useful when the focused metrics\nfail, such as in the Players environment, in Section 4.3. Otherwise,\none camera is excluded from the training set and used for evaluation\nonly.\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\nSacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe\nFigure 2: Illustration of focused metrics. Each image is a prediction from the same evaluation camera pose using different\nmodel settings. The black box represents the window in which the focused metrics are computed. When using the default\nmetrics (shown in red), only the fourth model achieves a high score, primarily due to its well-reconstructed bleachers. With\nthe focused metrics (shown in blue), only the first model receives a low score as it fails to accurately reconstruct the dynamic\ncontent of interest.\nTable 1: Comparing default and focused metrics for the novel\nviews shown in Fig. 2. The default metrics are best on scenes\nwhere static elements are better reconstructed, while the\ndynamic-focused metrics better reflect the quality of the re-\ngion of interest. Best results in bold, second-best underlined.\nDefault\nFocused\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\n(a)\n28.39\n0.724\n0.240\n27.60\n0.735\n0.254\n(b)\n27.41\n0.745\n0.218\n35.30\n0.818\n0.079\n(c)\n27.15\n0.737\n0.231\n38.12\n0.882\n0.039\n(d)\n34.43\n0.805\n0.149\n37.61\n0.924\n0.018\n3.2\nEnvironments\nFigure 3: Illustration depicting the camera placements for\neach setup. The player position is highlighted in red for the\nSingle Player setups. The 30 close-up cameras are represented\nin black, the 20 broadcast-style cameras are shown in yellow,\nand the 30 stadium-wide angles are denoted in blue. Exam-\nples of associated training views can be observed in Fig. 4, 5.\nTo the best of our knowledge, most sports datasets are limited to\na few synchronized cameras, and there are no public datasets that\ninclude dozens of synchronized and calibrated views. For example,\nthe KTH Multiview Football Dataset II [16] only contains three\nsynchronized cameras, that often focus on a single player.\nFigure 4: Camera setups for the \"Single Player\" scene. Left:\nclose-up cameras are placed around the players, similar to\ncommon datasets for novel view synthesis. Right: cameras\nare placed around the field and oriented toward the player,\ntypical of broadcast conditions.\nFigure 5: Example training view for the \"Players\" scene, along\nwith a close-up view dedicated for evaluation (bottom left).\nThe players occupy a very small part of the images.\nIn this exploratory work, we build synthetic environments to\ncircumvent the lack of real data. The scenes are of increasing com-\nplexity, starting from relatively close-up views, commonly used\nwith NeRFs, then using more distant cameras giving a field of view\nsimilar to what is used in broadcast coverage, and finally consider-\ning even more distant cameras placed in the bleachers covering the\nwhole field with more players. The different setups are illustrated\nin Fig. 3. This allows us to progressively tackle the challenges that\noccur with soccer environments, mainly the reconstruction of small\ndynamic content. All cameras are static and the environments are\nDynamic NeRFs for Soccer Scenes\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\nbuilt using Blender [8] with player models from Adobe Mixamo\n[14], and a stadium model available under a free CC0 license [25].\nSingle Player: Close-up Views. This synthetic environment fea-\ntures a single player placed at the center of the field, shooting a ball.\nThis first camera setup is composed of 30 close-up views around\nthe player and resembles typical conditions of benchmarks like\nDyNeRF [19]. An example training image is depicted in Fig. 4 (left).\nSingle Player: Broadcast-style Views. Within the same environ-\nment, we consider a second camera configuration that features\n20 views placed around the field, whose field of view is close to\nbroadcast conditions. The player represents only a tiny portion of\nthe images. An example training image is depicted in Fig. 4 (right).\nPlayers: Stadium-wide Views. This more complex environment\nfeatures several players and balls interacting all over the field, cap-\ntured by 30 wide-angle cameras placed high up in the bleachers\nand are thus much more distant from the field. Six additional cam-\neras, used exclusively for evaluation, are placed near the players\nfor more meaningful results. In this setup, training views cover the\nwhole field at all times but cover very few details about the players\nand balls due to their large distance. An example training view is\ndepicted in Fig. 5.\n4\nEXPERIMENTS\nIn this section, we assess the performance of K-Planes [13] and\nNeRFPlayer [33] in increasingly complex environments, each de-\nscribed in Section 3.\n4.1\nSingle Player: Close-up Views\nAs a first attempt, we run the original models from the initial pa-\npers in similar conditions to traditional datasets [4, 19]. The player\noccupies a large region of the training images, is captured by 30\ncameras, and performs smooth motion. In these settings, the models\nare able to reconstruct the stadium flawlessly. The player\u2019s motion\nis reconstructed, but the texture is blurry, even when using larger\nmodels. The ball is not reconstructed when moving fast in the air\nand disappears.\nWe can circumvent these issues by employing an improved pixel\nsampling strategy. Traditionally, training rays are traced by uni-\nformly sampling pixels although dynamic content, especially if\nsmall, should be sampled more often. In [19], several improved\nstrategies are described, known as Ray Importance Sampling (IS).\nThis new sampling strategy, which prioritizes sampling dynamic\ncontent pixels, is particularly necessary for setups like ours, con-\nsidering the scale of dynamic objects, even in this first more ideal\nenvironment. This general modification, which can be applied to\nboth models, yields substantial improvements in quality and train-\ning time. Renderings are performed around the player with both\nmodels, with and without ray importance sampling, and are de-\npicted in Fig. 6. Associated metrics, computed using a dedicated\nevaluation camera, are reported in Tab. 2. Visual details are recov-\nered much quicker, and final results are drastically more detailed\nwhen using importance sampling. Overall, results are similar be-\ntween NeRFPlayer and K-Planes when using similar model sizes,\nas depicted in Fig. 6. NeRFPlayer tends to recover slightly more\ndetails on the player but produces more artifacts around it. While\nit is not able to reconstruct the ball when it is in the air, K-Planes\nmanages to reconstruct it, although ghosting effects appear. When\nnot using importance sampling, the ball is never reconstructed.\nHere, the use of focused metrics barely affects our interpretation of\nthe results, due to the player\u2019s scale in the images, which causes the\nbounding boxes to cover a large part of the view. While the focused\nPSNR improves when using importance sampling, the other metrics\nsometimes degrade, which does not support qualitative results from\nFig. 6. This may be explained by the fact that IS helps to partially\nreconstruct the ball, which introduces artifacts.\n4.2\nSingle Player: Broadcast-style Views\nWhile the models perform well with close-up cameras, such views\nare usually not available in practical applications. Here, we experi-\nment with the same scene but observed by more distant training\nviews, which are positioned like broadcast cameras, all around the\nfield.\nExample renderings, using ray importance sampling, are de-\npicted in Fig. 7. The player is still reconstructed accurately, although\nless detailed, compared to the closer camera setup. In these new\nconditions, importance sampling is even more necessary, as the\nplayer is barely reconstructed without it. However, even with IS, the\nball is not reconstructed when in motion. Instead, artifacts appear\neverywhere in the direction of cameras.\n4.3\nPlayers: Stadium-wide Views\nThis final synthetic environment moves further away from the\ncenter of the scene and features 30 wide-angle cameras, located\nhigh in the bleachers, that cover the whole field. Many players are\npresent on the soccer pitch, interacting with each other and with\nballs. This setting is particularly challenging, due to the very small\nvisibility of players in the training images.\nResults are depicted in Fig. 8 and 9. Even in this very challenging\nconfiguration, the players are reconstructed and we can distinguish\ntheir motion. However, even with larger models, 1080p training\nimages, and ray importance sampling, the results remain blurry.\nThe ball is barely reconstructed when moving slowly, and not at\nall when moving fast (e.g., when being shot). Such camera setups,\ntherefore, seem to be limited for detailed results, at least when using\nno domain knowledge.\n5\nDISCUSSION\nIn this exploratory work, we compared recent state-of-the-art dy-\nnamic NeRF models, i.e., K-Planes [13] and NeRFPlayer [33], in\nincreasingly complex soccer environments, to assess their readi-\nness for broadcast-quality novel-view video synthesis of soccer\nreplays. In the ideal NeRF setup, where close-up cameras capture\ndetailed views of the target moving objects, the models reached\ngreat reconstruction quality. However, when using distant views\nin a camera setup similar to the best-result proprietary system [9],\nthe results offered by general dynamic NeRFs drastically degrade.\nIn such distant camera setups, we showed that incorporating ad-\nditional components to the original models, like ray importance\nsampling [19], becomes an absolute necessity.\nWe tried to avoid working with very high-resolution images,\nas opposed to [9], limiting our input image data to 1080p. Indeed,\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\nSacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe\nTable 2: Quantitative results for both models with and without ray importance sampling (see Fig. 6). Due to using closer cameras,\nthe focused metrics have a limited impact on the results.\nDefault\nFocused\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nWithout\nImportance Sampling\nK-Planes\n32.84\n0.786\n0.167\n34.41\n0.816\n0.126\nNeRFPlayer\n31.54\n0.754\n0.211\n34.55\n0.807\n0.181\nWith\nImportance Sampling\nK-Planes\n32.53\n0.751\n0.199\n35.06\n0.788\n0.156\nNeRFPlayer\n31.26\n0.721\n0.225\n34.78\n0.781\n0.191\nFigure 6: Comparative results between K-Planes and NeRFPlayer on the Single Player environment with close-up cameras, with\nand without ray importance sampling (IS). Overall, both models obtain similar results. NeRFPlayer tends to recover slightly\nmore details on the player, possibly due to the factorization of K-Planes, but induces more artifacts and does not manage to\nreconstruct the ball when in the air. Importance sampling drastically improves results for both models and allows K-Planes to\nrecover the ball when in the air.\neven though increasing the image resolution is an obvious path of\nimprovement toward capturing fine details, the massive amounts of\ndata thus generated are close to being prohibitive computationally,\nand we wanted to explore more economical solutions. For simi-\nlar reasons, we avoided resorting to domain-specific priors in this\nstudy, as such priors can be arduous and costly to produce, e.g.,\nan accurate skeletal reconstruction of the players. Assuming such\nrestrictions, and despite our improving components, we must con-\nclude that general dynamic NeRF models may fall short of meeting\nthe high-quality requirements of the broadcast industry for novel\nview synthesis of soccer replays.\nAlthough it was not the focus of our work, another inconve-\nnience of using dynamic NeRFs in broadcast applications might be\ntheir time performance. Indeed, the models we selected require one\nhour to train on thirty 4-second clips and 5 minutes to render a\nDynamic NeRFs for Soccer Scenes\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\nFigure 7: Novel views synthesis in the Single Player environment using a camera setup similar to broadcast conditions. Results\nare obtained from K-Planes with Ray Importance Sampling. The player is well reconstructed, but its texture is blurrier when\ncompared to using closer cameras. Despite the utilization of Importance Sampling, the model fails to accurately reconstruct\nthe ball in motion (on the right).\nFigure 8: Ground truths (top) and predictions (bottom) for the Players environment from close-up views dedicated to evaluation.\nIn these difficult conditions, the players are still reconstructed, although quite blurry.\nFigure 9: Additional novel views synthesis results in the Players environment. In these challenging conditions, the motion and\nposition of the players are correctly reconstructed, albeit significantly blurry. The ball is not reconstructed and causes artifacts\nvisible on the whole field (leftmost image).\n10-second video (about 1FPS for 1080p rendering). However, we be-\nlieve that training times could very certainly be lowered, notably by\npre-training a model for the empty stadium. Nonetheless, even the\nmost recent models [37] require more than 15 minutes of training,\nwhich while being unsuitable for live replay, might fit post-match\napplications.\nStill, we believe that dynamic NeRFs could play an important\nrole as the core part of a fully satisfying solution. Following the\nsame line of work as what was done in our study, a first path of im-\nprovement would be to try incorporating other general components\ninto dynamic NeRFs. The visibility loss from Nerfbusters [39], the\nimproved proposal sampling from Zip-NeRF [3], and the restorer\nfrom NeRFLiX [43] are promising components that would certainly\nbe beneficial to a detailed reconstruction of soccer scenes in dis-\ntant camera setups. Nevertheless, using such general improving\ncomponents may still not be enough for the task.\nAlthough using absolutely no domain knowledge is appealing,\nit may be necessary to use some domain-specific components to\nreach broadcast-quality results as well as a better time performance\nduring training, more in line with broadcast time constraints. Yet,\none should be cautious of the complexity and costs associated\nwith bringing specific models within a solution. For instance, while\nshowing impressive results in controlled working conditions, NeRFs\nthat focus on human reconstruction [21, 40, 42] are not directly\nusable with a distant camera setup such as ours, and would require\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\nSacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, and Gilles Louppe\nconsiderable adaptation to reconstruct humans in more diverse, less-\nconstrained configurations, such as multiple humans at arbitrary\npositions.\nAs manifest as increasing the input image resolution, another\npath of improvement is to obtain and leverage more zoomed-in\ninput views, together with the distant views given by our cho-\nsen camera setup inspired by [9]. Our study indeed showed that\nbroadcast-style views may capture enough details to render novel\nviews with near-acceptable quality for the target application. Such\ncameras could not be static, though, and it is unrealistic to suggest\nmanning dozens of additional broadcast-style cameras with oper-\nators tasked to follow the action. This naturally leads to consider\nusing the image data coming from the actual broadcast cameras,\nwhich are used to cover the soccer event. Including broadcast mov-\ning cameras within the reconstruction task would introduce new\ndifficulties, such as motion blur, less accurate camera calibration,\nview sparsity for the zoomed-in region of interest, and the inade-\nquacy of importance sampling as it relies on static cameras. The\nbenefits could however outweigh the difficulties. First, robust SfM\ntools could still be used with satisfaction in a mixed setup of dis-\ntant static cameras and broadcast moving cameras, to retrieve the\ncalibration of the moving cameras at all times. Second, using such\na mixed setup could allow using less static cameras than the dense\n20-30 camera array considered in this study. A case could even be\nmade that broadcast cameras become the main source of informa-\ntion in an economical solution, using all available NeRF extensions\nthat deal with sparse camera setups, such as depth supervision [10]\nbased on what SfM tools output for the scene structure, along with\nthe camera calibrations.\nAn indirect, but very important path of improvement is the de-\nsign of better evaluation metrics for dynamic NeRFs. Evaluating\nthese models in less-frequently considered dynamic environments,\nsuch as soccer, poses significant challenges. In our study, we pro-\nposed a simple yet better method for computing evaluation met-\nrics. However, much more could be made, particularly in detecting\ngeneral moving content, incorporating temporal information, and\nfinding ways to accurately reflect the challenging reconstruction\nquality of the ball. Proper evaluation of these models is crucial be-\ncause, without accurate assessment, it is difficult to determine the\nreadiness of a method for real-world applications.\nFinally, also an indirect path of improvement: acknowledging\nand remedying the lack of public multi-view soccer datasets. Even\na single image dataset of a dozen synchronized cameras captur-\ning a few soccer actions would be of tremendous interest to the\ncommunity. The synthetic environments we built are a modest\nproxy of such a dataset, that we publicly release along with all the\ncode used for the experiments, both ready to use in Nerfstudio, an\nopen-source framework for NeRF research. We strongly encour-\nage building richer datasets, both by extending our scenes and\nby recording real data using enough synchronized and calibrated\ncameras.\nACKNOWLEDGMENTS\nWe sincerely thank EVS Broadcast Equipment for providing the\nnecessary compute for the various conducted experiments. We also\nthank the Nerfstudio community for helpful insights.\nREFERENCES\n[1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes\nKopf, Matthew O\u2019Toole, and Changil Kim. 2023. HyperReel: High-fidelity 6-DoF\nvideo with ray-conditioned sampling. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 16610\u201316620.\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter\nHedman. 2022. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n5470\u20135479.\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter\nHedman. 2023. Zip-NeRF: Anti-aliased grid-based neural radiance fields. arXiv\npreprint arXiv:2304.06706 (2023).\n[4] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman,\nMatthew Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec.\n2020. Immersive light field video with a layered mesh representation. ACM\nTransactions on Graphics (TOG) 39, 4 (2020), 86\u20131.\n[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini\nDe Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis,\net al. 2022. Efficient geometry-aware 3D generative adversarial networks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n16123\u201316133.\n[6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022. Tensorf:\nTensorial radiance fields. In European Conference on Computer Vision. Springer,\n333\u2013350.\n[7] Shenchang Eric Chen and Lance Williams. 1993. View interpolation for image\nsynthesis. In Proceedings of the 20th annual conference on Computer graphics and\ninteractive techniques. 279\u2013288.\n[8] Blender Online Community. 2018. Blender - a 3D Modelling and Rendering Package.\nStichting Blender Foundation, Amsterdam. http://www.blender.org\n[9] Intel Corportation. [n. d.]. Intel \u00a9True View. https://www.intel.com/content/\nwww/us/en/sports/technology/true-view.html\n[10] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. 2022. Depth-\nsupervised nerf: Fewer views and faster training for free. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 12882\u201312891.\n[11] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum, and Jiajun Wu.\n2021. Neural radiance flow for 4d view synthesis and video processing. In 2021\nIEEE/CVF International Conference on Computer Vision (ICCV). IEEE Computer\nSociety, 14304\u201314314.\n[12] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu\nLiu, Matthias Nie\u00dfner, and Qi Tian. 2022. Fast dynamic radiance fields with\ntime-aware neural voxels. In SIGGRAPH Asia 2022 Conference Papers. 1\u20139.\n[13] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht,\nand Angjoo Kanazawa. 2023. K-planes: Explicit radiance fields in space, time,\nand appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 12479\u201312488.\n[14] Adobe Inc. 2015. Mixamo. Retrieved April 16, 2023 from https://www.mixamo.\ncom\n[15] James T Kajiya and Brian P Von Herzen. 1984. Ray tracing volume densities.\nACM SIGGRAPH computer graphics 18, 3 (1984), 165\u2013174.\n[16] Vahid Kazemi, Magnus Burenius, Hossein Azizpour, and Josephine Sullivan.\n2013. Multi-view body part recognition with random forests. In 2013 24th British\nMachine Vision Conference, BMVC 2013; Bristol; United Kingdom; 9 September 2013\nthrough 13 September 2013. British Machine Vision Association.\n[17] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-\nmization. CoRR abs/1412.6980 (2014).\n[18] Marc Levoy and Pat Hanrahan. 1996. Light field rendering. In Proceedings of the\n23rd annual conference on Computer graphics and interactive techniques. 31\u201342.\n[19] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lass-\nner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard\nNewcombe, et al. 2022. Neural 3d video synthesis from multi-view video. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n5521\u20135531.\n[20] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. 2017.\nFocal loss for dense object detection. In Proceedings of the IEEE international\nconference on computer vision. 2980\u20132988.\n[21] Jia-Wei Liu, Yan-Pei Cao, Tianyuan Yang, Eric Zhongcong Xu, Jussi Keppo,\nYing Shan, Xiaohu Qie, and Mike Zheng Shou. 2023.\nHOSNeRF: Dynamic\nHuman-Object-Scene Neural Radiance Fields from a Single Video. arXiv preprint\narXiv:2304.12281 (2023).\n[22] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and\nMichael J Black. 2015.\nSMPL: A skinned multi-person linear model.\nACM\ntransactions on graphics (TOG) 34, 6 (2015), 1\u201316.\n[23] Nelson L Max. 1986. Light diffusion through clouds and haze. Computer Vision,\nGraphics, and Image Processing 33, 3 (1986), 280\u2013292.\n[24] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi\nRamamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance\nfields for view synthesis. Commun. ACM 65, 1 (2021), 99\u2013106.\nDynamic NeRFs for Soccer Scenes\nMMSports \u201923, October 29, 2023, Ottawa, ON, Canada\n[25] MrChimp2313. 2012. Stadium Blender Model.\nhttps://www.blendswap.com/\nblend/7488\n[26] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. In-\nstant neural graphics primitives with a multiresolution hash encoding. ACM\nTransactions on Graphics (ToG) 41, 4 (2022), 1\u201315.\n[27] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Gold-\nman, Steven M Seitz, and Ricardo Martin-Brualla. 2021. Nerfies: Deformable\nneural radiance fields. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 5865\u20135874.\n[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei\nZhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling\ndynamic human bodies. In Proceedings of the IEEE/CVF International Conference\non Computer Vision. 14314\u201314323.\n[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun\nBao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations\nwith structured latent codes for novel view synthesis of dynamic humans. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n9054\u20139063.\n[30] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer.\n2021. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 10318\u201310327.\n[31] Steven M Seitz and Charles R Dyer. 1999. Photorealistic scene reconstruction by\nvoxel coloring. International Journal of Computer Vision 35 (1999), 151\u2013173.\n[32] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and\nYebin Liu. 2023. Tensor4d: Efficient neural 4d decomposition for high-fidelity\ndynamic reconstruction and rendering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 16632\u201316642.\n[33] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan,\nYi Xu, and Andreas Geiger. 2023. Nerfplayer: A streamable dynamic scene\nrepresentation with decomposed neural radiance fields. IEEE Transactions on\nVisualization and Computer Graphics 29, 5 (2023), 2732\u20132742.\n[34] Cheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct voxel grid optimiza-\ntion: Super-fast convergence for radiance fields reconstruction. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5459\u20135469.\n[35] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr,\nTerrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik\nAhuja, David McAllister, and Angjoo Kanazawa. 2023. Nerfstudio: A Modular\nFramework for Neural Radiance Field Development. In ACM SIGGRAPH 2023\nConference Proceedings (SIGGRAPH \u201923).\n[36] Krishna Wadhwani and Tamaki Kojima. 2022. SqueezeNeRF: Further factor-\nized FastNeRF for memory-efficient inference. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 2717\u20132725.\n[37] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, and Huaping Liu. 2022. Mixed\nneural voxels for fast multi-view video synthesis. arXiv preprint arXiv:2212.00190\n(2022).\n[38] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image\nquality assessment: from error visibility to structural similarity. IEEE transactions\non image processing 13, 4 (2004), 600\u2013612.\n[39] Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski, and\nAngjoo Kanazawa. 2023. Nerfbusters: Removing Ghostly Artifacts from Casually\nCaptured NeRFs. arXiv preprint arXiv:2304.10532 (2023).\n[40] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and\nIra Kemelmacher-Shlizerman. 2022. Humannerf: Free-viewpoint rendering of\nmoving people from monocular video. In Proceedings of the IEEE/CVF conference\non computer vision and pattern Recognition. 16210\u201316220.\n[41] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.\n2018. The unreasonable effectiveness of deep features as a perceptual metric.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\n586\u2013595.\n[42] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and\nLan Xu. 2022. Humannerf: Efficiently generated human radiance field from sparse\ninputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 7743\u20137753.\n[43] Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, and\nJiangbo Lu. 2023. NeRFLiX: High-Quality Neural View Synthesis by Learning\na Degradation-Driven Inter-viewpoint MiXer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 12363\u201312374.\n"
  },
  {
    "title": "Statistical Rejection Sampling Improves Preference Optimization",
    "link": "https://arxiv.org/pdf/2309.06657.pdf",
    "upvote": "12",
    "text": "Published as a conference paper at ICLR 2024\nSTATISTICAL REJECTION SAMPLING IMPROVES PREF-\nERENCE OPTIMIZATION\nTianqi Liu\u2217, Yao Zhao\u2020, Rishabh Joshi\u2020, Misha Khalman\u2020, Mohammad Saleh\u2020,\nPeter J. Liu\u2020, Jialu Liu\u2217\n{tianqiliu,yaozhaoyz,rishabhjoshi,khalman,msaleh,\npeterjliu,jialu}@google.com\nGoogle Research\u2217, Google DeepMind\u2020\nABSTRACT\nImproving the alignment of language models with human preferences remains an\nactive research challenge. Previous approaches have primarily utilized online Re-\ninforcement Learning from Human Feedback (RLHF). Recently, offline methods\nsuch as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimiza-\ntion (DPO) have emerged as attractive alternatives, offering improvements in sta-\nbility and scalability while maintaining competitive performance. SLiC refines its\nloss function using sequence pairs sampled from a supervised fine-tuned (SFT)\npolicy, while DPO directly optimizes language models based on preference data,\nforegoing the need for a separate reward model. However, the maximum like-\nlihood estimator (MLE) of the target optimal policy requires labeled preference\npairs sampled from that policy. The absence of a reward model in DPO constrains\nits ability to sample preference pairs from the optimal policy. Meanwhile, SLiC\ncan only sample preference pairs from the SFT policy. To address these limita-\ntions, we introduce a novel offline approach called Statistical Rejection Sampling\nOptimization (RSO) designed to source preference data from the estimated target\noptimal policy using rejection sampling, enabling a more accurate estimation of\nthe optimal policy. We also propose a unified framework that enhances the loss\nfunctions used in both SLiC and DPO from a preference modeling standpoint.\nThrough extensive experiments across diverse tasks, we demonstrate that RSO\nconsistently outperforms both SLiC and DPO as evaluated by gold reward, Large\nLanguage Models (LLMs) and human raters.\n1\nINTRODUCTION\nRecent advancements in Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023;\nAnil et al., 2023; OpenAI, 2023) have unlocked unprecedented capabilities in diverse tasks, such\nas programming and creative writing. Models are pre-trained on large unlabeled corpora and su-\npervised fine-tuned (SFT) on various tasks (Wei et al., 2021; Chung et al., 2022). Subsequently,\nRLHF (Stiennon et al., 2020) enhances the alignment of large language models with human prefer-\nences. RLHF introduces notable complexities into the training process, including a reward model, a\npolicy model, a reference policy, and a value model. It limits the maximum feasible size of a model\ndue to memory constraints. Additionally, it is not stable during training. Recognizing these chal-\nlenges, recent research has pioneered alternatives to RLHF. Notable among these are RRHF (Yuan\net al., 2023), SLiC (Zhao et al., 2022; 2023) and DPO (Rafailov et al., 2023). These methodolo-\ngies aim to more effectively align LLMs with human preferences while avoiding the complexities\nof reinforcement learning. Given supervised finetuning data Dsft = {(x, yref)} and preference data\nDhf = {(x, yw, yl)} where output text yw is preferred over yl on the same input text x, they di-\nrectly fit the policy model on preference data in various ways. RRHF uses a trained reward model\nor human raters to compute rewards for multiple sequences generated from difference sources on\nthe same prompt x, and then apply a ranking loss plus supervised fine-tuning loss. SLiC uses a\ncontrastive ranking calibration loss plus a regularization loss\nL(\u03b8) = max (0, \u03b4 \u2212 log \u03c0\u03b8(yw|x) + log \u03c0\u03b8(yl|x)) \u2212 \u03bb log \u03c0\u03b8(yref|x),\n(1)\n1\narXiv:2309.06657v2  [cs.CL]  23 Jan 2024\nPublished as a conference paper at ICLR 2024\nFigure 1: RSO first fits a pairwise reward-ranking model from human preference data. This model is\nlater applied to generate preference pairs with candidates sampled from the optimal policy, followed\nby a preference optimization step to align sequence likelihood towards preferences.\nwhere \u03b4 is a positive margin and \u03c0\u03b8 is the learnable conditional probability function by a language\nmodel. SLiC either fits directly on human preference data or on preference data sampled from\nthe SFT policy. DPO analyzes RLHF\u2019s objective function in the form of KL-regularized reward\nmaximization, and analytically solves the optimal policy induced by a reward function. Based on\nthe Bradley-Terry (BT) model (Bradley & Terry, 1952), DPO proposes an MLE to fit on human\npreference data directly and expresses the human preference probability in terms of only the optimal\npolicy \u03c0\u2217 and reference policy \u03c0sft:\np\u2217(y1 \u227b y2|x) =\n1\n1 + exp\n\u0010\n\u03b2 log \u03c0\u2217(y2|x)\n\u03c0sft(y2|x) \u2212 \u03b2 log \u03c0\u2217(y1|x)\n\u03c0sft(y1|x)\n\u0011\n(2)\nwhere \u03c0\u2217 is the function to be estimated and \u03b2 is a hyparameter in RLHF objective.\nEmpirically, one can leverage observed preference pairs to approximate p\u2217(y1 \u227b y2|x). To estimate\n\u03c0\u2217 as a density estimation problem, the optimal way is to fit a policy model on collected preference\npairs sampled from \u03c0\u2217. However, DPO uses the collected human preference data from other policies\ndirectly in all the experiments and lacks a study on the effect of sampling. Although they propose\nto sample pairs from the SFT policy and get them labeled by human, it is still not strictly MLE for\nthe preference model due to the mismatch between the sampling distribution and \u03c0\u2217. In reality, it is\nvery challenging to obtain human preference pairs directly sampled from \u03c0\u2217.\nIn this work, we address the above issues by constructing preference pairs from the approximated \u03c0\u2217\n(Figure 1). Starting with a human preference dataset Dhf collected from other policies, we first train\na pairwise reward-ranking model, then apply a statistical rejection sampling algorithm to generate\nresponse pairs sampled from optimal policy by using SFT policy and the pairwise reward-ranking\nmodel. After that, we label the sampled response pairs by the reward model. Then we fit the\nmodel on labeled pairs via classification loss. DPO claims that the language model is secretly\na reward model, we show that the language model learns better from an explicit reward model\nbecause comparing between two responses (reward) is easier to learn than generating high quality\nresponses (policy). Our statistical rejection sampling refers to the one in the statistical field (Neal,\n2003). In RLHF works (Bai et al., 2022; Stiennon et al., 2020; Touvron et al., 2023), they usually\nrefer to rejection sampling as best-of-N or top-k-over-N algorithm, where they sample a batch of N\ncompletions from a language model policy and then evaluate them across a reward model, returning\nthe best one or the top k. This algorithm has the issue of reward hacking because it trusts the reward\nmodel too much without any regularization. In this paper we show that top-k-over-N is a special case\nof our statistical rejection sampling and it is critical to balance between the reward exploitation and\nregularization towards the SFT policy. To summarize, our contributions of this work are three-fold.\n\u2022 we propose a scalable and easy-to-implement framework to learn from human preference\ndata. We provide a comprehensive recipe among different choices of loss functions and\npreference pairs generation. We show the importance of the reward model instead of di-\nrectly optimizing the model on the preference data.\n2\nPublished as a conference paper at ICLR 2024\n\u2022 we unify DPO and SLiC statistically by showing that they vary by loss functions to fit on\nhuman preference data: DPO is a logistic regression on human preference data and SLiC is\nalmost equivalent to a support vector machine (SVM) with hinge loss. We improve SLiC\nas the SVM counter part of DPO.\n\u2022 we design a statistical rejection sampling algorithm to sample pairs from the estimated\noptimal policy and get them labeled by a pairwise reward-ranking model. The proposed\nsampling strategy is shown to be effective on several generative tasks.\n2\nPRELIMINARIES\nLearning from Human Feedback\nSeveral works (Ziegler et al., 2019; Zhao et al., 2023; Rafailov\net al., 2023) show the significant improvement of conditional language generation by learning from\nhuman feedback data. All algorithms take two inputs:\n\u2022 \u03c0sft(y|x): a supervised fine-tuned policy (SFT), where x is the prompt and y is the response.\n\u2022 Dhf = {x(i), y(i)\nw , y(i)\nl }N\ni=1: a human preference dataset that distinguishes the better re-\nsponse from the worse given the same prompt.\nKL-Constrained Reward Maximization Objective\nStarting with a reward function r(x, y) and\ninput prompt distribution P, the DPO and RLHF optimizes for the following objective:\nmax\n\u03c0\nEx\u223cP,y\u223c\u03c0 [r(x, y)] \u2212 \u03b2DKL\n\u0002\n\u03c0(y|x)||\u03c0sft(y|x)\n\u0003\n(3)\nOptimal Policy\nDPO solves the optimal policy \u03c0r(y|x) that maximizes the above objective:\n\u03c0r(y|x) =\n1\nZ(x)\u03c0sft(y|x) exp\n\u0012 1\n\u03b2 r(x, y)\n\u0013\n(4)\nfor all x \u2208 P, where Z(x) = P\ny \u03c0sft(y|x) exp\n\u0010\n1\n\u03b2 r(x, y)\n\u0011\nis the partition function. \u03b2 controls the\nbalance between exploitation and exploration. When \u03b2 \u2192 0, all probability mass will concentrate\non the max reward with full exploitation. When \u03b2 \u2192 \u221e, optimal policy will be the same as \u03c0sft\nwith full exploration. Rearrange the Equation (4) we get\nr(x, y) = \u03b2 log \u03c0r(y|x)\n\u03c0sft(y|x) + \u03b2 log Z(x).\n(5)\nThe Equation (4) and (5) establish the relation between optimal policy and the reward function. In\nreality, the final goal is to have a good policy for response generation and \u03c0r(y|x) is usually of more\ninterest. The key is to effectively estimate the \u03c0r(y|x) from the human preference data.\nPreference Model\nLet the ground-truth reward function be r\u2217, then the optimal policy \u03c0\u2217 associ-\nated with r\u2217 can be represented by Equation (4). For two responses (y1, y2) from the same input x,\nBradley-Terry (BT) model (Bradley & Terry, 1952) assumes that\nP(y1 \u227b y2|x) = \u03c3(r\u2217(x, y1) \u2212 r\u2217(x, y2)),\n(6)\nwhere P(y1 \u227b y2|x) represents the probability that response y1 is preferred over y2 give prompt\nx. Reusing Equation (5), we obtain Equation (2). If we leverage the human preference data to\nrepresent P(y1 \u227b y2|x), the estimation of \u03c0\u2217 can be viewed as a density estimation problem from\nthe preference data. We will discuss different ways of estimating \u03c0\u2217 in Section 3.1.\nReward Model\nWe train a pairwise T5-XXL (Raffel et al., 2020) text-to-text reward-ranking\nmodel1 \u03c1\u03c8(x, y1, y2) on Dhf to approximate P(y1 \u227b y2|x). \u03c1\u03c8(x, y1, y2) takes the text input as:\n\u2022 \u201c[CONTEXT] {x} [SUMMARY A] {y1} [SUMMARY B] {y2}\u201d for summarization task\n1SLiC demonstrates that pairwise reward model is preferred in RL-free learning. Our pairwise reward-\nranking model has accuracy of 73.23% on the validation set of summarization task and 69.75% on the validation\nset of AI assistant task.\n3\nPublished as a conference paper at ICLR 2024\n\u2022 \u201c[CONTEXT] {x} [RESPONSE A] {y1} [RESPONSE B] {y2}\u201d for AI assistant task\n\u03c1\u03c8(x, y1, y2) outputs \u201cA\u201d or \u201cB\u201d as preferred one. We use the probability of decoding \u201cA\u201d as esti-\nmation of the preference probability \u02c6P(y1 \u227b y2|x)2. Suppose we have a baseline sequence yb with\nreward score 0, we can induce the reward score of any sequence y as\nr\u03c8(x, y) = logit(\u03c1\u03c8(x, y, yb)),\n(7)\nwhere logit(x) = log(\nx\n1\u2212x). This is a result of setting y1 = y, y2 = yb, and r\u2217(x, y2) = 0 in\nEquation (6), where we replace the win rate with the estimated one \u03c1\u03c8(x, y, yb). Thus, \u201cpointwise\u201d\nreward score can be derived from a \u201cpairwise\u201d reward-ranking model with a baseline sequence3.\n3\nRSO APPROACH\n3.1\nSTATISTICAL ESTIMATION OF THE OPTIMAL POLICY \u03c0\u2217\nOur proposed approach (Figure 1) takes inputs of SFT policy, reward-ranking model, and prompts.\nFirst we sample responses from the optimal policy through rejection sampling approach, then we\nfit a classification model on labeled preference pairs. To study the effectiveness of our approach,\nwe consider a few options on loss and preference dataset construction. Given a preference dataset\nDp = {(x(i), y(i)\nw , y(i)\nl )}, we can estimate \u03c0\u2217 according to Equation (2). There are two aspects we\nneed to consider for estimating \u03c0\u2217:\n\u2022 Choice of loss function: To fit Equation (2) as a binary classifier using (r\u2217(x, y1) \u2212\nr\u2217(x, y2)) as logit with fixed slope and zero bias, we consider logistic loss used in logistic\nregression and hinge loss used in support vector machine (SVM).\n\u2022 Choice of Dp: Equation (2) does not depend on the distribution of y1, y2 given x. Thus we\nneed to decide how to obtain (x, y1, y2) triplets.\nChoice of loss function\nGiven a preference dataset Dp = {(x(i), y(i)\nw , y(i)\nl )}, we can fit a binary\nclassifier according to Equation (2). DPO (Rafailov et al., 2023) uses sigmoid loss on normalized\nlikelihood (sigmoid-norm) to fit a logitistic regression:\nLsigmoid-norm (\u03c0\u03b8|\u03c0sft, Dp) = \u2212E(x,yw,yl)\u223cDp\n\u0014\nlog \u03c3\n\u0012\n\u03b3 log \u03c0\u03b8 (yw|x)\n\u03c0sft (yw|x) \u2212 \u03b3 log \u03c0\u03b8 (yl|x)\n\u03c0sft (yl|x)\n\u0013\u0015\n(8)\nwhere DPO sets \u03b3 = \u03b2. In this work, we decouple \u03b3 from \u03b2 and treat \u03b3 as an equivalent temperature\nhyper-parameter. The larger the \u03b3, the more we penalize the mis-classified examples at the decision\nboundaries by trusting more on the preference labels.\nSLiC (Zhao et al., 2023) proposed to use a hinge calibration loss4 as\nLhinge (\u03c0\u03b8|Dp) = E(x,yw,yl)\u223cDp [max (0, 1 \u2212 [\u03b3 log \u03c0\u03b8 (yw|x) \u2212 \u03b3 log \u03c0\u03b8 (yl|x)])]\n(9)\nNote that we use 1/\u03b3 as the margin \u03b4 used in SLiC loss (Equation (1)). This is equivalent to a hinge\nloss with logit (\u03b3 log \u03c0\u03b8 (yw|x) \u2212 \u03b3 log \u03c0\u03b8 (yl|x)). If we normalize the policy probabilities, we get\nthe SVM variation of DPO as the hinge loss on normalized likelihood (hinge-norm):\nLhinge-norm (\u03c0\u03b8|\u03c0sft, Dp) = E(x,yw,yl)\u223cDp\n\u0014\nmax\n\u0012\n0, 1 \u2212\n\u0014\n\u03b3 log \u03c0\u03b8 (yw|x)\n\u03c0sft (yw|x) \u2212 \u03b3 log \u03c0\u03b8 (yl|x)\n\u03c0sft (yl|x)\n\u0015\u0013\u0015\n(10)\n2We randomly flip response pairs and associated labels to remove positional bias.\n3In practice, we choose a random decoded sequence from the SFT policy as the baseline. We can in theory\nsolve n reward scores from n2 comparisons with a baseline sequence via a constraint optimization setting. We\nleave this for future study.\n4Subtle differences between SLiC loss and hinge-loss will be discussed in \u201cMethod\u201d paragraph of Section 5.\n4\nPublished as a conference paper at ICLR 2024\nChoice of preference data distribution\nSuppose we have access to the oracle preference data\nD\u2217 = {(x(i), y(i)\nw , y(i)\nl ) | y(i)\nw , y(i)\nl\n\u223c \u03c0\u2217(y|x(i))}N \u2217\ni=1, we can directly fit an MLE on the dataset.\nIn reality, we may not have access to such data, and we have access to Dhf = {(x(i), y(i)\nw , y(i)\nl ) |\ny(i)\nw , y(i)\nl\n\u223c \u03c0unk(y|x(i))}Nunk\ni=1 , where \u03c0unk denotes some mixed unknown policies. The mixed un-\nknown policies can include SFT policy, previous or current RLHF policy, or policies from other\nagents (Touvron et al., 2023). Given Dhf, we consider the following three choices:\n\u2022 direct: directly fit the policy on Dhf according to Equation (2) as DPO without \u03c1\u03c8.\n\u2022 sft-sample-rank: use \u03c0sft(y|x) to sample response pairs given prompts from the SFT train-\ning set and label them by \u03c1\u03c8.\n\u2022 rso-sample-rank: use \u03c0r\u03c8(y|x) induced by r\u03c8(x, y)5 according to Equation (4) to sample\nresponse pairs labelled by \u03c1\u03c8 given prompts from the SFT training set.\nStatistically speaking, since we are estimating \u03c0\u2217(y|x), it is desired to draw samples from \u03c0\u2217(y|x).\n\u201crso-sample-rank\u201d is the best solution towards this direction with samples from \u03c0r\u03c8(y|x), which is\ncloser to \u03c0\u2217(y|x) than other two choices.\n3.2\nSTATISTICAL REJECTION SAMPLING ALGORITHM\nStatistical rejection sampling (Neal, 2003) is an efficient statistical technique to generate observa-\ntions from a distribution. If we want to generate a distribution of density \u03c0r\u03c8, we can use \u03c0sft as the\nproposal distribution and follow the steps:\n1. Start with empty Y = {}.\n2. Generate y \u223c \u03c0sft(y|x) that is not in Y and u \u223c U[0, 1].\n3. Let M = min{m | m\u03c0sft(y|x) \u2265 \u03c0r\u03c8(y|x) for all y /\u2208 Y}6. If u <\n\u03c0r\u03c8 (y|x)\nM\u03c0sft(y|x), then we\naccept y and add it to Y. Otherwise, we reject y.\n4. Repeat step 2 and 3 until we get enough Y.\nFigure 2: Statistical rejection sampling illustration. There are three curves in the figure: M times\nSFT policy, reward, optimal policy. The sample is first generated by SFT policy, then gets accepted\nor rejected depending on whether a uniform random variable locates in acceptance or rejection\nregion. If the sample has high SFT policy probability but low optimal policy probability and reward\nscore, it has a higher chance of being rejected.\nFigure 2 is an illustration7 of the statistical rejection sampling approach. A Python implementation\n(Algorithm 1) with derivation is shown in Appendix A.1. The computation efficiency is discussed\nin Appendix A.10 Regarding the algorithm, we have:\n5r\u03c8(x, y) is induced from \u03c1\u03c8 by Equation (7).\n6M can be expensive to compute. In practice, we don\u2019t compute M. Instead we compute an estimation of\n\u03c0r\u03c8 (y|x)\nM\u03c0sft(y|x) directly from 64 sequences sampled by the SFT policy. See Section A.1 for details.\n7Although the output space of language models is a huge high-dimensional discrete space, we use a contin-\nuous 1-d input space for illustration purpose.\n5\nPublished as a conference paper at ICLR 2024\nTheorem 1. Let rmax be the maximum rewards among the response candidates not yet accepted. As\nthe number of response candidates goes to infinity, Algorithm 1 can generate num samples distinct\nsamples from \u03c0r\u03c8 with expected acceptance rate Ey\u223c\u03c0sft(y|x)\nh\nexp\n\u0010\n1\n\u03b2 \u00b7 (r\u03c8(x, y) \u2212 rmax)\n\u0011i\n.\nIf \u03b2 \u2192 \u221e, each sample generated from the SFT policy will be accepted with probability 1. If \u03b2 \u2192 0,\nonly the highest reward response will be accepted and all other responses will be rejected. This is the\nrejection sampling (top-k-over-N) referred by AnthropicHH (Bai et al., 2022) and Llama2 (Touvron\net al., 2023). \u03b2 indicates how much we trust the reward model. If the reward model is very accurate\nand robust, we should set a small \u03b2. Otherwise, we should set a larger \u03b2. In practice, we treat \u03b2 as\na hyper-parameter and pick one according to validation metrics.\n4\nRELATED WORK\nPreference Optimization\nRLHF has been a popular approach in learning from human prefer-\nence (Touvron et al., 2023; Stiennon et al., 2020). Recent works have proposed alternative solutions\nto reinforcement learning (Zhao et al., 2023; Yuan et al., 2023; Rafailov et al., 2023; Dong et al.,\n2023; Wang et al., 2023; Song et al., 2023). By optimizing the model\u2019s compatibility with prefer-\nence datasets under models such as the BT model, these methods fit on human or model ranked data\npairs. SLiC (Zhao et al., 2023) proposes a contrastive loss to fit on response pairs sampled from the\nSFT policy. Similarly, RRHF (Yuan et al., 2023) uses a zero-margin likelihood contrastive loss on\nranked list of responses. DPO (Rafailov et al., 2023) fits a model directly on human preference data\nusing the BT model. SLiC and RRHF lack theoretical understanding, and DPO does not optimally\nestimate the policy density proposed. Our work unifies the losses of SLiC and DPO, and proposes an\nimproved estimation of the optimal policy. We sample preference pairs from the estimated optimal\npolicy, which is closer to on-policy online RLHF.\nRejection Sampling\nStatistical rejection sampling (Neal, 2003) is a statistical approach used to\ngenerate samples from a target distribution. AnthropicHH (Bai et al., 2022) and ReST (Gulcehre\net al., 2023) refer to \u201crejection sampling\u201d as selecting top k sampled candidates for further tuning.\nLlama2 (Touvron et al., 2023) propose to use the same approach with PPO (Schulman et al., 2017)\nto improve RLHF. Our work shows the existing approach is a special case of the proposed algorithm.\n5\nEXPERIMENTS\nTasks\nWe study RSO on Reddit TL;DR summarization (Stiennon et al., 2020) and AnthropicHH\ndialogue (Bai et al., 2022) datasets. The Reddit TL;DR summarization dataset contains both fine-\ntune data Dtldr\nsft and human feedback data Dtldr\nhf . Dtldr\nsft contains 117k/6k/6k examples in train, vali-\ndation and test splits. Dtldr\nhf consists of 93k human preferences on decodes from multiple models.\nThe AnthropicHH is a dialogue dataset with x as conversation between a human query and an AI\nassistant. We use the helpful slice Dhelpful\nhf\nfrom 161k/9k examples in train and test splits. We use the\npositive responses as SFT targets. Besides, we study CNN/DailyMail datasets and show that RSO\nworks well on cross-task generalization from Reddit TL;DR (Appendix A.7).\nMethod\nStarting from a T5-large (770M) SFT policy and a T5-XXL (11B) pairwise reward-\nranking model, we consider nine settings as discussed in Section 3.1. The settings are all the com-\nbinations between loss functions and preference data distribution. DPO approach is the same as\nsigmoid-norm-direct. SLiC is almost the same as hinge-sft-sample-rank in our setting with two\ntiny differences. The first difference is that we drop the regularization loss (second term in Equa-\ntion (1)) due to lack of significantly improvement the final metrics (Appendix A.6). The second\ndifference is that SLiC uses a tournament-style procedure to rank candidates in a list. Unless specif-\nically mentioned, we set \u03b2 = 0.5 and \u03b3 = 0.05. To construct preference pairs, we first sample 64\nresponse candidates from the SFT policy using temperature sampling with temperature = 0.7 and\ntop k = 40. Then we sub-sample 8 samples. We use batch size 32 and learning rate 1e-5 with\nAdafactor optimizer (Shazeer & Stern, 2018). For each run, we pick the checkpoint with the highest\nreward-ranking model win rate against the SFT target.\n6\nPublished as a conference paper at ICLR 2024\nEvaluation\nOur experiments use four different approaches to evaluate: Proxy Reward Model,\nGold Reward Model, AutoSxS, and Human Evaluation. Proxy Reward Model computes win rate\nof generated response against SFT target on the trained T5-XXL pairwise reward-ranking model.\nFollow the recipe in Gao et al. (2023), we train a PaLM 2-S (Anil et al., 2023) on the same data as\nGold Reward Model8. AutoSxS uses PaLM 2-L few-shot in-context learning with details covered in\nAppendix A.4. Human Evaluation asks human raters to assign a quality score on each response and\ndetermine the best one among three systems (details in Section 5.3).\n5.1\nPERFORMANCE COMPARISON ON TWO TASKS\nWe include two additional baselines related to rejection sampling, RAFT (Dong et al., 2023) and\nReST (Gulcehre et al., 2023). For RAFT, we pick the best decoded sequence as new SFT target. For\nReST, we first normalize the reward scores to [0, 1], then pick the decoded sequences that are greater\nthan 0.7 as new sft targets. This is one round of grow and improve with normalized reward threshold\n0.79. The comparison results are shown in Table 1. RSO variants show significant gains over RAFT,\nReST, DPO, and SLiC variants on two tasks. Regarding preference pairs construction, \u201crso-sample-\nrank\u201d brings gains on top of \u201cdirect\u201d and \u201csft-sample-rank\u201d with a clear margin. Regarding the\nloss function, sigmoid-norm and hinge-norm perform similarly. The improved hinge-norm loss is\nbetter than hinge loss used in SLiC on AutoSxS. Hinge loss shows reward hacking in Reddit TL;DR\ndataset with higher Proxy Reward win rates but lower AutoSxS than other losses. To compare\ndifferent methods qualitatively, we showcase an example with responses from different policies on\nReddit TL;DR and AnthropicHH tasks in Figure 4 and Figure 5 in Appendix A.3, respectively.\nApproach\nAblation\nMetrics\nLoss\nPreference Pair\nProxy Reward (%)\nGold Reward (%)\nAutoSxS (%)\nReddit TL;DR\nRAFT\ncross-entropy\n-\n74.84\n68.51\n53.77\nReST\ncross-entropy\n-\n49.03\n46.17\n34.36\nDPO\nsigmoid-norm\ndirect\n84.35\n76.09\n67.72\nsigmoid-norm\nsft-sample-rank\n88.63\n78.14\n69.02\nRSOsigmoid-norm\nsigmoid-norm\nrso-sample-rank\n92.37\n82.22\n71.86\nSLiCdirect\nhinge\ndirect\n86.92\n79.76\n60.54\nSLiCsample-rank\nhinge\nsft-sample-rank\n90.15\n80.19\n67.34\nhinge\nrso-sample-rank\n93.36\n84.40\n69.26\nhinge-norm\ndirect\n83.93\n76.43\n66.63\nhinge-norm\nsft-sample-rank\n88.04\n76.57\n68.46\nRSOhinge-norm\nhinge-norm\nrso-sample-rank\n92.80\n83.45\n70.84\nAnthropicHH\nRAFT\ncross-entropy\n-\n58.21\n40.00\n24.99\nReST\ncross-entropy\n-\n43.48\n30.33\n15.58\nDPO\nsigmoid-norm\ndirect\n51.63\n36.13\n24.01\nsigmoid-norm\nsft-sample-rank\n85.09\n58.65\n39.56\nRSOsigmoid-norm\nsigmoid-norm\nrso-sample-rank\n86.94\n59.15\n40.98\nSLiCdirect\nhinge\ndirect\n35.95\n27.56\n15.69\nSLiCsample-rank\nhinge\nsft-sample-rank\n80.82\n54.55\n30.66\nhinge\nrso-sample-rank\n82.21\n55.22\n32.56\nhinge-norm\ndirect\n49.55\n37.23\n22.89\nhinge-norm\nsft-sample-rank\n82.40\n56.55\n35.96\nRSOhinge-norm\nhinge-norm\nrso-sample-rank\n84.44\n57.75\n38.58\nTable 1: Compare different methods with T5-large policy to leverage human feedback data. Proxy\nreward, golden reward and few-shot PaLM 2-L win rate against SFT target text are reported.\n8The Gold Reward Model has accuracy of 76.07% on the validation set of summarization task and 70.18%\non the validation set of AI assistant task.\n9The threshold is suggested by the original paper. We pick one round of grow and improve as a fair com-\nparison to one round of RSO, since RSO can also be done with multiple rounds.\n7\nPublished as a conference paper at ICLR 2024\n5.2\nRSO ABLATION\nEffect of \u03b3 and \u03b2 in RSO\nTo study the effect of \u03b3, we fix the statistical rejection sampling \u03b2 = 0.5,\nand vary \u03b3 = 0.005, 0.05, 0.5 in the loss function on Reddit TL;DR dataset. Figure 3a shows that\n\u03b3 = 0.05 provides the optimal win rate. To study the effect of \u03b2 for rejection sampling, we fix the\n\u03b3 = 0.05 in the loss function and vary \u03b2 = 0, 0.05, 0.5, 5 on Reddit TL;DR dataset. Figure 3b\nshows that \u03b2 = 0.5 provides the optimal win rate.\n(a) Proxy reward win rate of var-\nious\n\u03b3\nin\nloss\nfunctions\n(Equa-\ntion (8), (9), (10)). \u03b2 is fixed at 0.5.\nShaded areas are 95% confidence in-\ntervals.\n(b) Proxy reward win rate of various \u03b2\nin statistical rejection sampling (Algo-\nrithm 1). \u03b3 is fixed at 0.05. The hor-\nizontal lines are from the sft-sample-\nrank preference pairs.\nShaded areas\nare 95% confidence intervals.\nFigure 3: Effect of hyper-parameters in loss functions and statistical rejection sampling algorithm.\nPreference pairs sampling and ranking\nTo better understand the effect of tournament ranking\nand statistical rejection sampling, we compare among different sampling strategies. Since we first\nsample 64 responses from the SFT policy and followed by 8 responses by statistical rejection sam-\npling, it is natural to ask: \u201cwhy not use all of the 64 samples in the calibration?\u201d SLiC uses tourna-\nment ranking, which introduces bias towards higher reward sequences. Starting with n responses,\nwe can construct n/2 pairs and get them labeled. We call this approach \u201cfirst-round-rank\u201d. We can\nkeep the tournament until the winner is decided with a total of n\u22121 pairs (each game eliminates one\nresponse). We call this approach \u201ctournament-rank\u201d. We use sigmoid-norm loss and conduct abla-\ntion study on six settings (Table 2). We observe that tournament ranking can bring consistent gains\nacross settings on reward model, but it cannot improve the AutoSxS win rate on rso-8-sample case.\nRso-8-sample-first-round-rank shows to be the optimal choice based on AutoSxS metric, which\nmeans it is not always good to sample more responses or conduct the tournament ranking.\nPreference Pair\nProxy Reward (%)\nAutoSxS (%)\nsft-8-sample-first-round-rank\n88.63\n68.51\nsft-8-sample-tournament-rank\n90.69\n68.57\nrso-8-sample-first-round-rank\n92.37\n71.86\nrso-8-sample-tournament-rank\n93.35\n71.69\nsft-64-sample-first-round-rank\n88.91\n68.84\nsft-64-sample-tournament-rank\n91.14\n71.08\nTable 2: Comparison among different preference pairs sampling and ranking approaches on the\nReddit TL;DR dataset. \u201ck-sample\u201d means sampling k response candidates.\nScale up the policy model\nTo understand how well the RSO can be scaled up to larger policy\nmodels, we train a T5-XXL policy model and fix the loss as sigmoid-norm. Table 3 shows that RSO\nscales up well and improves AutoSxS upon DPO by 1.1% and 33.1% on two tasks, respectively.\n8\nPublished as a conference paper at ICLR 2024\nApproach\nPreference Pair\nProxy Reward (%)\nAutoSxS (%)\nReddit TL;DR\nDPO\ndirect\n94.04\n85.03\nsft-sample-rank\n97.50\n85.66\nRSOsigmoid-norm\nrso-sample-rank\n98.29\n86.01\nAnthropicHH\nDPO\ndirect\n76.84\n52.80\nsft-sample-rank\n94.91\n66.79\nRSOsigmoid-norm\nrso-sample-rank\n97.54\n70.26\nTable 3: Comparing sampling strategies to leverage human feedback data on T5-XXL policy model.\n5.3\nHUMAN EVALUATION RESULTS\nTo further verify the improvements of RSO over others, we conduct human evaluation side-by-side\nusing Amazon Mechanical Turk. Given a document and three responses generated from \u201cdirect\u201d,\n\u201csft-sample-rank\u201d and \u201crso-sample-rank\u201d, raters are asked to assign a pointwise overall quality (1-5)\nto each response, and choose the best one. Each task is replicated 3 times and therefore judged by\n3 different raters. To eliminate bias, we anonymize all the models and randomly shuffle order of\nresponses for each task. We aggregate pointwise metrics by averaging the ratings across all replicas,\nand we aggregate the choice metric using majority vote. The rating tasks are shown in Appendix A.5.\nIn total 47 different raters participated in the human evaluation study with a median of 16 tasks per\nrater. The human evaluation results are shown in Table 4. \u201crso-sample-rank\u201d shows to be better\nthan \u201cdirect\u201d and \u201csft-sample-rank\u201d in all loss functions and tasks evaluated with clear improvement\nmargins. RSOsigmoid-norm is chosen to be preferred more than 2x as DPO in both tasks. Comparing\nbetween two losses, there is no clear conclusion on which one has higher quality when applying\n\u201crso-sample-rank\u201d. Thus improved loss on SLiC and original loss DPO perform similarly.\nApproach\nLoss\nPreference Pair\nChosen as Preferred10\nQuality\nReddit TL;DR\nDPO\nsigmoid-norm\ndirect\n21%\n3.84\nsigmoid-norm\nsft-sample-rank\n10%\n3.74\nRSOsigmoid-norm\nsigmoid-norm\nrso-sample-rank\n48%\n4.02\nhinge-norm\ndirect\n21%\n3.80\nhinge-norm\nsft-sample-rank\n11%\n3.68\nRSOhinge-norm\nhinge-norm\nrso-sample-rank\n46%\n3.97\nAnthropicHH\nDPO\nsigmoid-norm\ndirect\n15%\n3.04\nsigmoid-norm\nsft-sample-rank\n22%\n3.21\nRSOsigmoid-norm\nsigmoid-norm\nrso-sample-rank\n31%\n3.37\nhinge-norm\ndirect\n13%\n3.33\nhinge-norm\nsft-sample-rank\n22%\n3.56\nRSOhinge-norm\nhinge-norm\nrso-sample-rank\n33%\n3.60\nTable 4: Human evaluation on ways of constructing preference pairs.\n6\nCONCLUSION\nIn this paper, we propose RSO recipe to train large language models from human feedback as an\nalternative to RLHF. Our recipe is simple and effective with a better sampling strategy than DPO and\nSLiC. We unify loss functions used in DPO and SLiC from the preference optimization perspective\nwith the first as logistic regression and the other as support vector machine. We demonstrate our\napproach to be powerful on multiple tasks with comprehensive numerical experiments and analysis.\nFuture work may include studying RSO on larger scale decoding samples, other loss functions, other\nlanguage generation tasks, online variants, and non-human feedback.\n10The proportion may not sum up to 100% because there are cases of same preference across all approaches.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method\nof paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416, 2022.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum,\nand Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment.\narXiv preprint arXiv:2304.06767, 2023.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In\nInternational Conference on Machine Learning, pp. 10835\u201310866. PMLR, 2023.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural\ninformation processing systems, 28, 2015.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor\nCarbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with\nai feedback. arXiv preprint arXiv:2309.00267, 2023.\nRadford M Neal. Slice sampling. The annals of statistics, 31(3):705\u2013767, 2003.\nR OpenAI. Gpt-4 technical report. arXiv, pp. 2303\u201308774, 2023.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan\nHeek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.\nProceedings of Machine Learning and Systems, 5, 2023.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning, pp. 4596\u20134604. PMLR, 2018.\n10\nPublished as a conference paper at ICLR 2024\nLei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Canoee Liu, Simon Tong, Jindong Chen,\nand Lei Meng. Rewritelm: An instruction-tuned large language model for text rewriting. arXiv\npreprint arXiv:2305.15685, 2023.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.\nPreference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u20133021, 2020.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\nPeiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhi-\nfang Sui.\nMaking large language models better reasoners with alignment.\narXiv preprint\narXiv:2309.02144, 2023.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang.\nRrhf:\nRank responses to align language models with human feedback without tears. arXiv preprint\narXiv:2304.05302, 2023.\nYao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J\nLiu. Calibrating sequence likelihood improves conditional language generation. arXiv preprint\narXiv:2210.00045, 2022.\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf:\nSequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.\nZhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao.\nBeyond one-preference-for-all: Multi-objective direct preference optimization. arXiv preprint\narXiv:2310.03708, 2023.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019.\nA\nAPPENDIX\nA.1\nSTATISTICAL REJECTION SAMPLING ALGORITHM\nA Python Implementation\nA Python implementation of the algorithm is shown in Algorithm 1.\n11\nPublished as a conference paper at ICLR 2024\nAlgorithm 1 Statistical Rejection Sampling Algorithm in Python\nfrom typing import List\nimport numpy as np\ndef conduct_rejection_sampling(response_candidates: List[str],\nresponse_rewards: List[float],\nnum_samples: int,\nbeta: float):\n\"\"\"Conducts rejection sampling guided by rewards.\nArgs:\nresponse_candidates: response candidates from the SFT policy\nresponse_rewards: response rewards.\nnum_samples: number of samples to sub-sample.\nbeta: beta parameter in KL-constrained reward maximization objective.\nReturns:\nRejection sampled sequences from the estimated optimal policy.\n\"\"\"\ncandidates = {c: r for c, r in zip(response_candidates, response_rewards)}\naccepted = []\nwhile len(accepted) < num_samples:\nmax_reward = max(candidates.values())\nto_remove = []\nfor c, r in candidates.items():\nu = np.random.uniform()\nif u >= np.exp((r - max_reward) / beta):\ncontinue\naccepted.append(c)\nto_remove.append(c)\nif len(accepted) == num_samples:\nbreak\nfor c in to_remove:\ncandidates.pop(c)\nreturn accepted\nDerivation of Algorithm 1\nAccording to Equation (4), we have\n\u03c0r\u03c8(y|x) =\n1\nZ\u03c8(x)\u03c0sft(y|x) exp\n\u0012 1\n\u03b2 r\u03c8(x, y)\n\u0013\n,\n(11)\nwhere Z\u03c8(x) = P\ny \u03c0sft(y|x) exp( 1\n\u03b2 r\u03c8(x, y)). Then we have\n\u03c0r\u03c8(y|x)\n\u03c0sft(y|x) =\n1\nZ\u03c8(x) exp\n\u0012 1\n\u03b2 r\u03c8(x, y)\n\u0013\n.\n(12)\nIt\u2019s clear that MDx \u225c min{m | m \u00b7 \u03c0sft(y|x) \u2265 \u03c0r\u03c8(y|x) for all y /\u2208 Dx} = maxy /\u2208Dx\n\u03c0r\u03c8 (y|x)\n\u03c0sft(y|x) ,\nthen\nMDx =\n1\nZ\u03c8(x) max\ny /\u2208Dx\n\u0014\nexp\n\u0012 1\n\u03b2 r\u03c8(x, y)\n\u0013\u0015\n.\n(13)\nThen we have\n\u03c0r\u03c8(y|x)\nMDx\u03c0sft(y|x) = exp\n\u0012 1\n\u03b2\n\u0012\nr\u03c8(x, y) \u2212 max\ny /\u2208Dx r\u03c8(x, y)\n\u0013\u0013\n.\n(14)\nBy using the sample version of maxy /\u2208Dx r\u03c8(x, y), we have derived the Algorithm 1.\nA.2\nPROOF OF THEOREM 1\nProof. Let the process of generation be the one described in Algorithm 1 and the accepted sequence\nset be Dx at the current step, we have\nP(sample y and get accepted|x) = P\n\u0012\nu <\n\u03c0r\u03c8(y|x)\nMDx\u03c0sft(y|x)\n\u0013\n\u03c0sft(y|x)\n=\n1\nMDx\n\u03c0r\u03c8(y|x),\n(15)\nwhere MDx \u225c min{m | m \u00b7 \u03c0sft(y|x) \u2265 \u03c0r\u03c8(y|x) for all y /\u2208 Dx}.\n12\nPublished as a conference paper at ICLR 2024\nP(y get accepted|x) = P(u <\n\u03c0r\u03c8(y|x)\nMDx\u03c0sft(y|x))\n= E1\n\u0014\nu <\n\u03c0r\u03c8(y|x)\nMDx\u03c0sft(y|x)\n\u0015\n= E\u03c0sft\n\u0014\nE1\n\u0014\nu <\n\u03c0r\u03c8(y|x)\nMDx\u03c0sft(y|x)\n\f\f\f\fy\n\u0015\u0015\n= E\u03c0sft\n\u0014\n\u03c0r\u03c8(y|x)\nMDx\u03c0sft(y|x)\n\u0015\n=\n1\nMDx\n(16)\nP(y|y is accepted, x) = P(sample y and get accepted|x)\nP(y get accepted|x)\n= \u03c0r\u03c8(y|x).\n(17)\nBy Equation (13), we have the acceptance rate\n1\nMDx\n= Ey\u223c\u03c0sft(y|x)\n\u0014\nexp\n\u0012 1\n\u03b2 \u00b7\n\u0012\nr\u03c8(x, y) \u2212 max\ny /\u2208Dx r\u03c8(x, y)\n\u0013\u0013\u0015\n(18)\nA.3\nQUALITATIVE EXAMPLES OF RSO COMPARING WITH OTHER APPROACHES\nThe qualitative comparisons between RSO and other approaches are shown in Figure 4 and Figure 5\nfor Reddit TL;DR and AnthropicHH, respectively.\nFigure 4: Example summaries generated by SFT, SLiC, DPO, and RSO policies for a Reddit post.\nRSO generates the best summary among the four because it concisely and precisely summarizes key\ninformation in the forum post. Salient details are bolded.\n13\nPublished as a conference paper at ICLR 2024\nFigure 5: Example responses generated by SFT, SLiC, DPO, and RSO policies for a Human-\nAssistant dialogue on AnthropicHH dataset. RSO generates the most helpful response among the\nfour because it gives a clear and straightforward answer for sending a letter quickly through tradi-\ntional mail. In contrast, SFT repeats information about email rather than answering the question\nabout traditional mail. SLiC and DPO are vague and repetitive. Salient details are bolded.\nA.4\nPALM 2-L DETAILS AND FEW-SHOT SXS TEMPLATE\nA.4.1\nDETAILS\nThe purpose of the AutoSxS is to prevent the artificially high reward scores by Reward Model due to\nreward hacking on learned policies. Since the policy is trained using the information in the pairwise\nreward-ranking model, it is not necessary the higher the win rate on reward-ranking model, the better\nthe policy. AutoSxS uses PaLM 2-L few-shot in-context learning to infer 8 decodedsamples with 4\nflipped order of response A and B. The label contains three choices: A, B, and tie withscore 1, 0, and\n0.5, respectively. To ensure the robustness, we use average score to determine the win or loss if the\nmagnitude exceeds 0.35. The AutoSxS has been demonstrated as effective and consistent in DPO\nusing GPT-4 as zero-shot rater. In this work, we replace GPT-4 with PaLM 2-L for our evaluation\nusing few-shot prompts. The quality of PaLM 2-L on similar tasks has been shown to be close to\nhuman raters (Lee et al., 2023; Shu et al., 2023). The systematic study on consistency and quality of\nAutoSxS is beyond the scope of this work.\nA.4.2\nREDDIT TL;DR FEW-SHOT PROMPTS\ntask: Judge the quality of two TLDRs, choose the options among (A), (B) or same.\ncontext: I\u2019ve (M[21]) been in a relationship for a year and a half with F[22] and it really has never\ngone well. I think we want different things and we are not overly compatible. I broke up with her\nabout a year ago and she tried to kill herself so we got back together. This week I met an F[19] who\nI think I\u2019m really compatible with. She and I talked for a few hours and we have a lot in common. I\nlike her a lot, but she is currently a freshman and I am currently a senior so I will be graduating in\nMay and going on to a prestigious PhD program starting next fall.\nSo here are my questions: * What should I do in regards to my current relationship? I know I need\nto end it, but I just don\u2019t know how. * What should I do in regards to the other girl? * Do you think\nmy feelings for the other girl stem from my distaste for my current relationship?\nI appreciate any help you give me.\ntldr (A): I\u2019m unhappy in my current relationship with a girl I just met, but don\u2019t know how to end\n14\nPublished as a conference paper at ICLR 2024\nit. I have no idea what I\u2019m doing or what to do.\ntldr (B): M[21] unhappy in relationship with F[22]. Met an F[19] in town with similar interests and\nI really like her. What should I do in regards to current relationship/other girl?\nexplanation: tldr (A)\u2019s second and third sentences convey similar idea and are redundant. tldr (B)\nmentions an important piece of information of the new girl, contains more details than tldr (A) and\nis concise at the same time.\nchoose among (A), (B) or same: (B)\ncontext: Before anything, not a sad story or anything, I don\u2019t think she\u2019s cheating or anything of\nthe sorts. My country\u2019s equivalent to Valentine\u2019s Day is coming and I had this pretty simple idea to\nsurprise my girlfriend and it would involve giving her some roses. The thing is, although I know she\nwould appreciate my intention in and of itself, I don\u2019t know if she would like the actual flowers and\nsuch, so I wanted to find out if she likes roses and if she would like getting some, but without her\nrealizing it so as not to spoil the surprise. Any ideas on how to get that information out of her? tldr\n(A): How do I find out if my girlfriend likes roses without her realizing it?\ntldr (B): I want to surprise my girlfriend with some flowers when Valentine\u2019s Day is around the\ncorner, but I don\u2019t know if she would like the flowers or flowers themselves without her knowing.\nexplanation: tldr (A) is a concise that captures the main idea. tldr (B) also captures the main point\nwith more details, but the language \u2019flowers or flowers themselves\u2019 is not fluent.\nchoose among (A), (B) or same: (A)\ncontext: Okay, so my younger brothers were out and about when they passed some teenagers who\nyelled obscenities at them. My father then went over and told them to knock it off, when they started\nyelling obscenities at him. My dad, with a small amount of temper, got angry and yelled at them.\nThey started recording it and made a video on YouTube where it looked like he was just screaming\nat them. After that, we were able to get it taken down only to have it reuploaded with blurred faces.\nWe have in no way given consent to be in this video. Is there any way we can get them to take it\ndoen?\ntldr (A): my dad got angry at teenagers for yelling obscenities at him, they got a video on youtube\nand blurred faces, what can we do to get it taken down?\ntldr (B): My brothers were being verbally harassed by kids, father yelled at them, they made a video\nof it to get the video taken down, it was like a blur with blurred faces.\nexplanation: tldr (A) mentions most main points of story while skipping some details like younger\nbrothers being yelled at and original videos taken down. tldr (B) has a major factual error, they\ndidn\u2019t make a video to get the video taken down.\nchoose among (A), (B) or same: (A)\ncontext: Apologize for the throw away account.\nMy friend is interested in in making his way into the mainstream music industry as an A&R repre-\nsentative. He has a 4 year degree in English but did some extra curricular activities around music\nbusiness in college. He also has had 2 internships out of college with small time record labels in the\nChicago area. He has been trying for a few years now with no success. He lives in Chicago but has\nmoved to LA for a year. He returned recently to Chicago. It has been about 3 years since his last\ninternship, but he has done some online collaboration stuff. Still far and few in between.\nMy question is what path have any of you taken to make your way into the record industry? Is\nnetworking the only important thing? Is there a set of skills that are specifically looked for?\ntldr (A): My friend wants to make his way into the music industry as an A&R representative. What\ndoes he need to know to make it happen?\ntldr (B): friend wants to break into the music industry as an A&R rep. what is the best way to do\nthis?\nexplanation: tldr (A) and (B) are basically paraphrase of each other. With (A) slightly more verbose\nand (B) slightly more concise.\nchoose among (A), (B) or same: same\ncontext: I was at a concert this weekend supporting my favorite artist. Most of the amphitheater\nwas standing, however *some* people in my section were not (including some people behind me\nand to my right). All of a sudden at the end of the set I got basically shoved in the back and told to\nsit down. I told them absolutely not, I want to support my favorite artist (who kept saying STAND\nUP, btw). I paid over $100 for these seats and I want to have a great time.. after all this IS a concert.\n15\nPublished as a conference paper at ICLR 2024\nNot to mention, I heard them calling me names before they confronted me so now I was certainly\nnot going to sit down. Then a few songs later the woman sitting next to me told me what a bitch I\nwas for not letting the people behind me see. Was I in the wrong here?\ntldr (A): I stood up at a concert, people in front of me didn\u2019t and confronted me. Was I in the wrong?\ntldr (B): I stood up at a concert even though I was told not to. I got yelled at by a woman who was\nsitting next to me. Was I in the wrong?\nexplanation: tldr (A) doesn\u2019t say who confronted the author or wrongly implied it was the people in\nfront. tldr (B) is correct but could mention that more people confronted the author than the woman\nsitting next.\nchoose among (A), (B) or same: (B)\nA.4.3\nANTHROPICHH FEW-SHOT PROMPTS\ntask: For the following query to a chatbot, which response is more helpful? Choose among (A), (B)\nand same.\ncontext: Human: Why can\u2019t humans build a space station on the moon? Assistant: That\u2019s a really\ngood question. We don\u2019t know yet if we\u2019ll be able to build a space station on the moon, though\nit seems quite feasible. But we can also look at the science for something more ambitious, like a\nspaceship that\u2019s large enough to make a round trip to Mars. That might be hard to do! Even with the\ntechnology we have now, it\u2019s not clear to scientists how you\u2019d build a spaceship that would travel\nfor months in space. Human: I think we should start on the Moon first.\nresponse (A): Assistant: I think you\u2019re right that the Moon is a good place to start, since it\u2019s a\nnatural satellite of Earth. There are a lot of interesting places on the Moon to study, but we have to\ngo to Mars to check out the atmosphere, the conditions on the ground, and the survival potential of\nhumans, and to see if there are living things.\nresponse (B): Assistant: The problem is that we don\u2019t know yet if it\u2019s even possible to build a space\nstation on the moon. But let\u2019s assume we could do it, and see how far we could get with a spaceship\nto Mars.\nexplanation: Response (A) offers a more expansive and engaged exploration of the user\u2019s sentiment\nabout starting with the Moon, while Response (B) reiterates previous information without addressing\nthe user\u2019s perspective or adding new insights.\nchoose among (A), (B) and same: (A)\ncontext: Human: What is garam masala?\nAssistant: Garam masala is a mixture of spices, typically consisting of cumin, pepper, garlic, and\neither coriander, ginger, or both. It\u2019s frequently used in curries or vegetable dishes, but it\u2019s also\nadded to some vegetable or meat sides, or used as a spread.\nHuman: What other things can I add to it to make it unique?\nAssistant: Try adding some of the following ingredients, in any combination you like: turmeric\ncinnamon cardamom celery seeds fennel fresh chilies, like jalape\u02dcnos or Thai chiles grated ginger\ngreen onions basil mint\nHuman: Is there a sweet garam masala?\nAssistant: Yes, some garam masasalas are sweetened. They might contain sugar or honey, or they\nmight be just made with ground cinnamon or cardamom. You might also want to mix some sort of\nmango or pineapple relish into your garam masala.\nHuman: What is a good use for sweet garam masala?\nresponse (A): Assistant: You might use it to flavor vegetable or meat stews, or chilies, or as a\nspread on toast. You might mix some into yogurt or fruit salad, or serve it with nuts and chutney on\ncrackers.\nresponse (B): Assistant: Sweet garam masala is typically used in desserts like ice cream or fudge,\nor in sweet curries like risotto or k\u00a8ofte. You can also use it in curries to add a sweet flavor. You can\nalso use it as a spice rub for meats, like chicken or fish. You can also use it as a spice mix in a curry,\nor you can use it to make a sweet sauce for a curry.\nexplanation: Response (B) offers specific and diverse uses of sweet garam masala, allowing the\nuser to get a clearer idea of its potential applications in cooking.\nchoose among (A), (B) and same: (B)\n16\nPublished as a conference paper at ICLR 2024\ncontext: Human: What is the customary amount to tip a taxi driver?\nresponse (A): Assistant: The standard tip for taxi drivers in the United States is typically a 20% tip.\nThis is a fairly standard tip that is given to taxi drivers for a job well done, and it\u2019s also a nice gesture\nto show your appreciation for their service. The tip can be more or less depending on the driver\u2019s\nservice and the level of professionalism they showed. If you\u2019re considering tipping a taxi driver, I\nwould recommend giving them a tip of at least 20%.\nresponse (B): Assistant: A standard 10% tip is recommended.\nexplanation: Response (A) provides a clear recommendation, contextualizes it to a specific country\n(the U.S.), and offers additional guidance based on the quality of service. It\u2019s more comprehensive\nthan response (B).\nchoose among (A), (B) and same: (A)\nA.5\nHUMAN SXS TEMPLATES\nReddit TL;DR and AnthropicHH human evaluation template are shown in Figure 6 and Figure 7,\nrespectively.\nFigure 6: Example of human evaluation task on Reddit TL;DR dataset.\nA.6\nREGULARIZATION IN SLIC\nTable 5 shows the SLiC results with different regularization weights. There is no strong gain by\nadding the regularization loss. And we drop it to align better with the DPO setting.\nA.7\nCROSS-TASK ADAPTATION AND GENERALIZATION\nThe CNN/DailyMail dataset (Hermann et al., 2015) contains only fine-tuned data Dcnndm\nsft\nwith\n287k/13k/11k examples in train, validation and test splits. We use the dataset to test the cross-\ntask generalization of different approaches. We assume no access to any target or preference texts\nof the CNN/DailyMail dataset during training. Starting from a SFT model trained on Reddit TL;DR\nDtldr\nsft , we further optimize the SFT policy using preference data from Reddit TL;DR Dtldr\nhf . For \u201cdi-\nrect\u201d, we use the preference directly. For \u201csft-sample-rank\u201d and \u201crso-sample-rank\u201d, we first fit a\n17\nPublished as a conference paper at ICLR 2024\nFigure 7: Example of human evaluation task on AnthropicHH dialogue dataset.\nRegularization\nProxy Reward (%)\nAutoSxS (%)\n0\n90.15\n67.34\n0.5\n90.45\n67.64\n5\n90.25\n67.79\n50\n90.83\n67.84\n500\n90.06\n67.44\nTable 5: Comparison on different regularization in SLiC. Adding regularization does not show sig-\nnificant improvement.\nreward-ranking model and then generete preference pairs using prompts from the training set of\nCNN/DailyMail. We evaluate the performance using target texts on validation split of Dcnndm\nsft\n. From\nTable 6, the RSO also consistently improves over SLiC and DPO for cross-task transfer.\nApproach\nAblation\nMetrics\nLoss\nPreference Pair\nProxy Reward (%)\nAutoSxS (%)\nDPO\nsigmoid-norm\ndirect\n61.31\n37.36\nsigmoid-norm\nsft-sample-rank\n62.72\n38.63\nRSOsigmoid-norm\nsigmoid-norm\nrso-sample-rank\n69.38\n39.71\nSLiCdirect\nhinge\ndirect\n64.18\n33.63\nSLiCsample-rank\nhinge\nsft-sample-rank\n67.16\n33.21\nhinge\nrso-sample-rank\n71.62\n35.46\nhinge-norm\ndirect\n60.04\n33.91\nhinge-norm\nsft-sample-rank\n61.77\n40.63\nRSOhinge-norm\nhinge-norm\nrso-sample-rank\n69.82\n42.18\nTable 6: Compare different methods to leverage human feedback data on CNN/DailyMail.\n18\nPublished as a conference paper at ICLR 2024\nA.8\nOTHER BASELINES\nIn Table 1, we did not include the baselines for RRHF and RLHF. For RRHF, we don\u2019t have access\nto other LLM systems and it is hard to establish an apples-to-apples comparison. Furthermore, the\nloss function of RRHF is very similar to SLiC. We believe our sampling technique can also improve\nRRHF, but we leave it as a future study. For RLHF, we lack expertise on RLHF and DPO shows it\nto be a competitive alternative. The main purpose of this work is to improve upon DPO and SLiC\nwith a better sampling strategy.\nA.9\nDEEPER EXAMINATION OF BIAS AND FAIRNESS IN LANGUAGE MODELS\nThis section delves into the critical aspects of bias and fairness in language models, particularly in\nrelation to our proposed methodology. The works of Anil et al. (2023) and Touvron et al. (2023)\noffer insightful evaluations of bias and fairness in both pre-trained and aligned language models. In\nterms of aligning with human preferences, our approach incorporates two academic datasets: the\nReddit TL;DR summarization (Stiennon et al., 2020) and the AnthropicHH dialogue (Bai et al.,\n2022). Our primary objective is to enhance alignment with human preferences, focusing on the\nquality of summaries in Reddit TL;DR and the helpfulness in AnthropicHH dialogues.\nIn practical scenarios, reward scores are often multi-dimensional, and the aim of alignment is to\nattain a Pareto optimal frontier (Bai et al., 2022). This allows for the introduction of additional ob-\njectives such as harmlessness, safety, and bias preference pairs. Our method is adaptable, function-\ning with either weighted-averaged reward scores or through integration with multi-objective DPO\nloss functions (Zhou et al., 2023). Experimental studies have demonstrated that our RSO method\neffectively aligns with human preference pairs.\nWe posit that our approach has the potential to enhance fairness and reduce bias in language models,\nprovided it is applied with appropriate human preference pairs. However, it is important to note that\na comprehensive study of fairness and bias falls beyond the scope of this work.\nA.10\nCOMPUTATIONAL EFFICIENCY\nCompared with PPO (Schulman et al., 2017), RSO only needs a policy network during training,\nwhile PPO needs four networks (policy, value, reward, and reference network).\nBesides, rso-\nsample-rank is fully parallelized over the whole dataset, while PPO needs sampling at each step\nand is parallelized within the batch. Now we focus a comparative analysis of the computational effi-\nciency among different offline methodologies. Our comparison includes RAFT (Dong et al., 2023),\nReST (Gulcehre et al., 2023), DPO (Rafailov et al., 2023), SLiC-HF-direct (Zhao et al., 2023), SLiC-\nHF-sample-rank (Zhao et al., 2023), and our proposed RSO. Notably, most approaches, except DPO\nand SLiC-HF-direct, require the training and inference of a (pairwise) reward model.\nTable 7 delineates the efficiency comparison among the considered approaches. For methods involv-\ning a pairwise reward model with nc decoded candidates and nd selected candidates for RSO, the\nfollowing specifics are noted:\n\u2022 RAFT: Requires nc decodings from the SFT policy and nc \u22121 comparisons for tournament\nranking.\n\u2022 ReST: Involves nc SFT decodings and nc\u22121 comparisons with a randomly chosen baseline\nsequence, followed by normalization of reward scores and truncation based on a threshold.\n\u2022 DPO/SLiC-HF-direct: directly optimize on the human preference data without a reward\nmodel and SFT decoded sequences.\n\u2022 SLiC-HF-sample-rank: Samples nd sequences, subsequently employing nd\u22121 tournament\nranking comparisons.\n\u2022 RSO: Our method samples nc decoded candidates from the SFT policy. Each candidate is\nassigned a reward score based on nc \u2212 1 comparisons against a random chosen baseline.\nRSO then employs statistical rejection sampling for selecting nd sequences and constructs\npreference pairs using nd/2 comparisons.\n19\nPublished as a conference paper at ICLR 2024\nCompared with DPO and SLiC-HF-direct, RSO introduces an additional sample and rank stage.\nThese stages are scalable and can be parallelized across multiple model servers, significantly en-\nhancing efficiency.\nRSO needs more reward server inferences. The extra computation burden can be mitigated and\naddressed with prompt efficiency: With a fixed prompt for generating responses, RSO benefits from\nprompt caching on model servers, leading to faster response generation. The inference speed can\nfurther be improved with advance serving techniques (Pope et al., 2023). On the side of reward\nserver, inference with decoder length 1 ensures quick processing times.\nThe statistical rejection sampling algorithm, as described in Algorithm 1, exhibits enhanced effi-\nciency by employing a sampling-without-replacement strategy. This is achieved by excluding the\nselected sequences subsequent to each sampling round. Furthermore, at the commencement of each\nround, the maximum reward is recalculated. This recalibration ensures that, in every round, at least\none sequence is invariably chosen. Specifically, the sequence whose reward is equivalent to the\nmaximum reward is selected with a probability of one, thereby guaranteeing the selection of at least\none optimal sequence in each round. This approach not only optimizes the selection process but also\nmaintains the algorithm\u2019s effectiveness throughout its execution.\nRSO needs additional computation on sampling from the SFT policy and ranking from the pairwise\nreward model, but the additional cost is empirically minor compared to policy training. There are\nseveral reasons for that:\n\u2022 We only need to sample once for each prompt in the training data. But the training of DPO\ncan go through multiple epochs.\n\u2022 Sampling and ranking are fully parallelizable over the whole training set but training is\nonly parallelizable within the batch.\n\u2022 Reward ranking can be fast because of the short decoding length (just one token). The input\ntext can be encoded in a parallel way.\n\u2022 Our observations indicate that rso-sample-rank accounts for less than 10% of the total train-\ning time.\n\u2022 Batch decoding is scalable and efficient with many optimizations (Pope et al., 2023). In this\nwork, we sample 64 responses from the SFT policy. Existing research works can sample\nsimilar or even way more samples from the SFT policy to construct best-of-N:\n1. Up to 32 samples in Table 4 in Dong et al. (2023);\n2. Up to 100 samples in Figure 7 in Touvron et al. (2023);\n3. Up to 30k samples in Table 2 in Gao et al. (2023);\nFrom the perspective of balancing between the additional burden in efficiency and the significant\nperformance quality gains (as shown in the Section 5), RSO stands out as a recommended approach\nover the alternatives.\nApproach\nReward Model\n#SFT inference\n#Reward Model inference\nRAFT\nY\nnc\nnc \u2212 1\nReST\nY\nnc\nnc \u2212 1\nDPO\nN\n0\n0\nSLiC-HF-direct\nN\n0\n0\nSLiC-HF-sample-rank\nY\nnd\nnd \u2212 1\nRSO\nY\nnc\nnc \u2212 1 + 0.5 \u2217 nd\nTable 7: Efficiency comparison of difference approaches. Np denotes the number of prompts, nc\ndenotes the number of decodes to sample from the SFT policy as RSO candidates, and nd denotes\nthe number of decodes for each prompt.\n20\n"
  },
  {
    "title": "DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models",
    "link": "https://arxiv.org/pdf/2309.06933.pdf",
    "upvote": "11",
    "text": "DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models\nNamhyuk Ahn1\nJunsoo Lee1\nChunggi Lee1,2\nKunhee Kim3\nDaesik Kim1\nSeung-Hun Nam1\nKibeom Hong4\n1 NAVER WEBTOON AI\n2 Harvard University\n3 KAIST\n4 SwatchOn\nAbstract\nRecent progresses in large-scale text-to-image models have\nyielded remarkable accomplishments, finding various appli-\ncations in art domain. However, expressing unique character-\nistics of an artwork (e.g. brushwork, colortone, or composi-\ntion) with text prompts alone may encounter limitations due\nto the inherent constraints of verbal description. To this end,\nwe introduce DreamStyler, a novel framework designed for\nartistic image synthesis, proficient in both text-to-image syn-\nthesis and style transfer. DreamStyler optimizes a multi-stage\ntextual embedding with a context-aware text prompt, result-\ning in prominent image quality. In addition, with content and\nstyle guidance, DreamStyler exhibits flexibility to accommo-\ndate a range of style references. Experimental results demon-\nstrate its superior performance across multiple scenarios, sug-\ngesting its promising potential in artistic product creation.\nProject page: https://nmhkahn.github.io/dreamstyler/.\nIntroduction\n\u201cPainting is silent poetry.\u201d \u2014 Simonides, Greek poet\nRecent text-to-image models have shown unprecedented\nproficiency in translating natural language into compelling\nvisual imagery (Saharia et al. 2022; Ramesh et al. 2022;\nRombach et al. 2022). These have emerged in the realm of\nart, providing inspiration and even assisting in crafting tan-\ngible art pieces. In the AI-assisted art production workflow,\nartists typically utilize various descriptive prompts that de-\npict the style and context to generate their desired image.\nHowever, the unique styles of a painting, its intricate brush-\nwork, light, colortone, or composition, cannot be easily de-\nscribed in a single word. For instance, dare we simplify the\nentirety of Vincent Van Gogh\u2019s lifelong artworks as just one\nword, \u2018Gogh style\u2019? Text descriptions cannot fully evoke his\nunique style in our imagination \u2014 his vibrant color, dra-\nmatic light, and rough yet vigorous brushwork.\nBeyond text description, recent studies (Gal et al. 2022;\nRuiz et al. 2023) embed specific attributes of input images\ninto latent space. While they effectively encapsulate a novel\nobject, we observed that they struggle to personalize style\nof a painting. For instance, model optimization-based meth-\nods (Ruiz et al. 2023; Kumari et al. 2023) are highly sus-\nceptible to overfitting and often neglect inference prompts,\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\u201ca \nmarket\nplace\u201d\n\u201ca \nwaterfall \nin the \nforest\u201d\n\u201ca dog\u201d\nFigure 1: DreamStyler synthesizes outputs based on a given\ncontext along with a style reference. Note that each model is\ntrained on a single style image shown in this figure.\nwhich is not ideal for real-world production (please refer to\nthe Suppl. for more details). Textual inversion-based meth-\nods (Gal et al. 2022; Voynov et al. 2023), in contrast, effec-\ntively reflect the inference prompt but fail to replicate style,\npossibly due to the limited capacity of the learned embed-\ndings. This is because capturing style, from global elements\n(e.g. colortone) to local details (e.g. detailed texture), is chal-\nlenging when relying solely on a single embedding token.\nIn this work, we present DreamStyler, a novel single\n(one-shot) reference-guided artistic image synthesis frame-\nwork designed for the text-to-image generation and style\ntransfer tasks (Figure 1). We encapsulate the intricate styles\nof artworks into CLIP text space. DreamStyler is grounded\nin textual inversion (TI), chosen for the inherent flexibility\nthat stems from its prompt-based configuration. To over-\ncome the limitations of TI, we introduce an extended tex-\ntual embedding space, S by expanding textual embedding\ninto the denoising timestep domain (Figure 2). Based on this\nspace, we propose a multi-stage TI, which maps the textual\narXiv:2309.06933v2  [cs.CV]  18 Dec 2023\ninformation into the S space. It accomplishes by segmenting\nthe entire diffusion process into multiple stages (a chunk of\ntimesteps) and allocating each textual embedding vector to\nthe corresponding stage. The exploitation of the timestep do-\nmain in textual inversion significantly improves the overall\nefficacy of artistic image synthesis. This enhancement stems\nfrom the increased capacity of the personalized module, as\nwell as the utilization of prior knowledge suggesting that\ndifferent denoising diffusion steps contribute differently to\nimage synthesis (Balaji et al. 2022; Choi et al. 2022).\nWe further propose a context-aware prompt augmentation\nthat simply yet proficiently decouples the style and context\ninformation from the reference image. With our approach,\nthe personalization module can embed style features solely\ninto its textual embeddings, ensuring a more faithful reflec-\ntion of the reference\u2019s style. To further refine the artistic im-\nage synthesis, we introduce a style and context guidance, in-\nspired by classifier-free guidance (Ho and Salimans 2022).\nOur guidance bisects the guidance term into style and con-\ntext components, enabling individual control. Such a guid-\nance design allows users to tailor the outputs based on their\npreferences or intricacy of the reference image\u2019s style.\nWe validate the effectiveness of DreamStyler through a\nbroad range of experiments. DreamStyler not only demon-\nstrates advanced artistic image synthesis but also paves the\nnew way of applying text-to-image diffusion models to the\nrealms of artistic image synthesis and style transfer tasks.\nRelated Work\nPersonalized text-to-image synthesis. Since the latent-\nbased text conditional generation has been explored (Rom-\nbach et al. 2022), following studies (Saharia et al. 2022;\nRamesh et al. 2022; Li et al. 2022) have further contributed\nto enhancing text-to-image synthesis with CLIP (Radford\net al. 2021) guidance. Furthermore, Textual inversion (Gal\net al. 2022), DreamBooth (Ruiz et al. 2023) and CustomD-\niffusion (Kumari et al. 2023) introduced approaches that\nleverage 3-5 images of the subject to personalize semantic\nfeatures. Recently, Voynov et al. (2023) proposed P+ space,\nwhich consists of multiple textual conditions, derived from\nper-layer prompts. Although they showed promising results\nin penalization of diffusion models, there are still limitations\nto fully capturing precise artistic style representations. In\ncontrast, DreamStyler considers the denoising timestep to\naccommodate temporal dynamics in the diffusion process,\nachieving high-quality artistic image generation.\nPaint by style. Neural style transfer renders the context\nof a source with a style image. Since Gatys, Ecker, and\nBethge (2016), studies have been devoted to enhancing\nthe transfer networks for more accurate and convincing\nstyle transfer. Notably, AdaIN (Huang and Belongie 2017)\nand AdaAttN (Liu et al. 2021) investigated matching the\nsecond-order statistics of content and style images. AesPA-\nNet (Hong et al. 2023) and StyTr2 (Deng et al. 2022)\nadopted recent architectures such as attention or transformer\nfor high-fidelity neural style transfer. Recently, InST (Zhang\net al. 2023) utilized the diffusion models by introducing the\nimage encoder to inverse style images into CLIP spaces.\nBLIP-2\n+ Feedback\n\u201cpainting\u201d \u201c<S*> \nstyle\u201d\nU-Net\nt stage\nStyle\nU-Net\nt-1 stage\nU-Net\nt+1 stage\nText Encoder\n\u2026\n(a) Training\n(b) Sampling\nEncoder\nU-Net\nt stage\nU-Net\nt-1 stage\nU-Net\nt+1 stage\n\u2026\n\u201cpainting of\na bear in\n<S*> style\u201d\nText Encoder\nContent\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nFigure 2: Model overview. (a) DreamStyler constructs train-\ning prompt with an opening text Co, multi-stage style to-\nkens S\u2217, and a context description Cc, which is captioned\nwith BLIP-2 and human feedback. DreamStyler projects the\ntraining prompt into multi-stage textual embeddings v\u2217 =\n{v\u2217\n1, . . . , v\u2217\nT }, where T is # stages (a chunk of the denoising\ntimestep). As a result, the denoising U-Net provides distinct\ntextual information at each stage. (b) DreamStyler prepares\nthe textual embedding using a provided inference prompt.\nFor style transfer, DreamStyler employs ControlNet to com-\nprehend the context information from a content image.\nMethod\nPreliminary: Stable Diffusion (SD). DreamStyler is built\nupon SD (Rombach et al. 2022). SD projects an input image\nx into a latent code, z = E(x) using an encoder E, while\ndecoder D transforms the latent code back into pixel space,\ni.e. x\u2032 = D(z\u2032). The diffusion model creates a new latent\ncode z\u2032 by conditioning on additional inputs such as a text\nprompt y. The training objective of SD is defined as:\nL = Ez\u223cE(x),y,\u03f5\u223cN(0,1),t[||\u03f5 \u2212 \u03f5\u03b8(zt, t, c(y))||2\n2].\n(1)\nAt each timestep t, the denoising network \u03f5\u03b8 reconstructs\nthe noised latent code zt, given the timestep t and a con-\nditioning vector c(y). To generate c(y), each token from a\nprompt is converted into an embedding vector, which is then\npassed to the CLIP text encoder (Radford et al. 2021).\nPreliminary: Textual Inversion (TI). Gal et al. (2022) pro-\nposed a method to personalize a pre-trained text-to-image\nmodel by incorporating a novel embedding representing the\nintended concept. To personalize the concept, they initial-\nize a word token S\u2217 and its corresponding vector v\u2217, situ-\nated in the textual conditioning space P, which is the output\nof the CLIP text encoder. Instead of altering any weights in\nSD models, they optimize v\u2217 alone using Eq. (1). To create\nimages of personalized concepts, the inclusion of S\u2217 in the\nprompts (e.g. a photo of S\u2217 dog) is the only required step.\nMulti-Stage Textual Inversion\nIn some cases, TI fails to sufficiently represent the concept\ndue to the inherent capacity limitations associated with us-\nStyle image\n\u201cknight\nwith armor\u201d\n\u201crock island\non ocean\u201d\n(b) green box description + (a)\n(c) red box description + (a,b)\n(d) blue box description + (a,b,c)\n\u201cknight\nwith armor\u201d\n\u201crock island\non ocean\u201d\n\u201cknight\nwith armor\u201d\n\u201crock island\non ocean\u201d\n\u201cknight\nwith armor\u201d\n\u201crock island\non ocean\u201d\nInference prompt:\n(a) w/o contextual description\n\u201ca painting in S* style\u201d\n+ \u201cof a woman in a blue dress playing a violin\u201d\n+ \u201cwith a woman in red dress playing piano behind\u201d\n+ \u201cwith women sitting in chairs\u201d\nFigure 3: How does training prompt affect? Given a style image, we construct training prompts with contextual descriptions\n(b\u223cd). (a) Training without contextual description in the prompt; i.e. trains the model with \u201ca painting in S\u2217 style\u201d. The model\ntends to generate the images that contains objects and compositions from the style image (e.g. standing and sitting audiences),\ninstead of attributes depicted in the inference prompt. (b, c) Training with partial contextual descriptions (the green and red\nboxes displayed in the style image, respectively). Such a tendency is significantly reduced, yet the model still synthesizes some\nobjects from the style image (e.g. sitting people in the blue box). (d) Training with full contextual descriptions. The model\nproduces outputs that fully reflect the inference prompt without introducing any non-style attributes from the style image.\ning a single embedding token. Moreover, this single embed-\nding strategy is inappropriate for accommodating the chang-\ning process of diffusion models. As explored in Balaji et al.\n(2022); Choi et al. (2022), diffusion models display intrigu-\ning temporal dynamics throughout the process, necessitat-\ning different capacities at various diffusion steps. In light of\nthis, managing all denoising timesteps with a single embed-\nding potentially has limitations due to the spectrum of local\nto global expressions embodied in paintings. Thus, articulat-\ning paintings is intricately related to the denoising timesteps,\nwhich operate in a coarse-to-fine synthesis manner (Balaji\net al. 2022). To address these challenges, we introduce a\nmulti-stage TI that employs multiple embeddings, each cor-\nresponding to specific diffusion stages (Figure 2).\nWe first propose an extended textual embedding space S.\nThe premise of the S space is to decompose the entire dif-\nfusion process into multiple distinct stages. To implement\nthis, we split the denoising timesteps into T chunks and de-\nnote each chunk as a stage. Based on the S space, the multi-\nstage TI prepares the copies of the initial style token (S\u2217)\nas a multi-stage token set S\u2217 = {S\u2217\n1, . . . , S\u2217\nT }. In this way,\nthe multi-stage TI projects a style image into T style tokens,\ncontrasting the TI that embeds it into a single token. The to-\nken set is then encoded by a CLIP text encoder to form stage-\nwise embedding vectors, denoted as v\u2217 = {v\u2217\n1, . . . , v\u2217\nT }.\nLastly, the multi-stage TI optimizes these embeddings fol-\nlowing the subsequent equation.\nv\u2217 = arg min\nv\nEz,v,\u03f5,t[||\u03f5 \u2212 \u03f5\u03b8(zt, t, c(vt))||2\n2].\n(2)\nThe application of multi-stage TI significantly enhances\nthe representation capacity beyond that of vanilla TI, which\nwe will illustrate in a series of experiments. Furthermore,\nthis method enables the fusion of multiple tokens, each orig-\ninating from different styles, at a specific stage t. Conse-\nquently, it facilitates the creation of unique and novel styles\ntailored to the user\u2019s individual preferences.\nContext-Aware Text Prompt\nWhile the multi-stage TI enhances representational capac-\nity, it still faces fundamental problems when training with a\nstyle reference; the style and context of the image may be-\ncome entangled during the optimization of the embeddings.\nThis problem mainly arises from attempts to encapsulate all\nfeatures of the image into S\u2217, not just the style aspect. As\ndepicted in Figure 3, without contextual information in the\ntraining prompt, the model overlooks the context of infer-\nence prompt. However, when we inject contextual descrip-\ntions into the training prompt, the model better disentangles\nthe style from the context. In our observations, such a phe-\nnomenon occurs more frequently as the representational ca-\npacity increases, likely due to the model\u2019s increased efforts\nto accommodate all information within its capacity.\nHence, we construct training prompts to include contex-\ntual information about the style image. Let C = [Co, S\u2217] be\nthe vanilla prompt used in multi-stage TI training, where Co\nis the opening text (e.g. \u201ca painting\u201d), and S\u2217 is multi-stage\nstyle token set, described above. In the proposed strategy, we\nincorporate a contextual descriptor Cc (e.g. \u201cof a woman in\na blue dress\u201d) into the middle of the prompt (Figure 2), i.e.\nC = [Co, Cc, S\u2217]. We annotate all the non-style attributes\n(e.g. objects, composition, and background) from the style\nimage to form the contextual descriptor. When we caption\nnon-style attributes, BLIP-2 (Li et al. 2023) is employed to\naid in the automatic prompt generation.\nAlthough a context-aware prompt significantly reinforces\nstyle-context decoupling, for some style images with com-\nplicated contexts (Figure 3), BLIP-2 might not capture all\ndetails, which could limit the model\u2019s disentanglement ca-\npability. In such cases, we further refine caption Cc based on\nhuman feedback (e.g., caption by humans). This human-in-\nthe-loop strategy is straightforward yet markedly improves\nthe model\u2019s ability to disentangle styles. Since our goal is\none-shot model training, the time spent refining the caption\nis minimal; typically less than a minute. With the context-\naware prompt, the text-to-image models can now distinguish\nstyle elements from contextual ones and specifically embed\nthese into the (multi-stage) style embeddings v\u2217. The moti-\nvation for augmenting the training prompt is also suggested\nin StyleDrop (Sohn et al. 2023), a current personalization\napproach in the text-to-image diffusion model.\nStyle and Context Guidance\nClassifier-free guidance (Ho and Salimans 2022) improves\nconditional image synthesis. It samples adjusted noise pre-\n\u201ca cat\u201d\n\u201ca \nbridge\u201d\n\u201ca \nsailboat \non the \nsea\u201d\nStyle image\nPrompt\n(a) DreamStyler\n(b) InST\n(c) XTI\n(d) Textual Inversion (e) CustomDiffusion\n(f) DreamBooth\nFigure 4: Qualitative comparison on the style-guided text-to-image synthesis task.\nContent\n& Style\n(a) DreamStyler\n(b) InST\n(c) AesPA-Net\n(d) StyTr2\n(f) AesUST\n(e) IEContraAST\n(g) AdaAttN\nFigure 5: Qualitative comparison on the style transfer task.\ndiction \u02c6\u03f5(.), by leveraging unconditional output under null\ntoken \u2205 as: \u02c6\u03f5(v) = \u03f5(\u2205) + \u03bb(\u03f5(v) \u2212 \u03f5(\u2205)), where, \u03bb is the\nguidance scale and we omit c(.), z and t for brevity.\nIn style-guided image synthesis, this guidance pushes\nboth style and context uniformly with \u03bb. The uniform guid-\nance could face limitations since the spectrum of \u201cstyle\u201d\nof artistic paintings is wider than that of natural photos.\nGiven this diversity, a more nuanced control mechanism is\nrequired. Furthermore, there exist demands to individually\ncontrol style and context in the art-making process. To this\nend, we propose style and context guidance as in below.\n\u02c6\u03f5(v) = \u03f5(\u2205) + \u03bbs[\u03f5(v) \u2212 \u03f5(vc)] + \u03bbc[\u03f5(vc) \u2212 \u03f5(\u2205)]\n+ \u03bbc[\u03f5(v) \u2212 \u03f5(vs)] + \u03bbs[\u03f5(vs) \u2212 \u03f5(\u2205)]\n(3)\nwhere, vs, vc are the embeddings of prompts C, Cc, respec-\ntively. \u03bbs, \u03bbc denote style and context guidance scale. We\nderive Eq. (3) by decomposing v into vs, vc. We employ\ntwo paired terms to balance the influence of each guidance.\nPlease refer to Suppl. for detailed derivation and analysis.\nBy separating the guidance into style and context, users\nare afforded the flexibility to control these elements individ-\nually. Specifically, an increase in \u03bbc increases the model\u2019s\nsensitivity towards context (e.g. inference prompt or content\nimage), whereas amplifying \u03bbs leads the model towards a\nmore faithful style reproduction. This flexible design allows\nusers to generate stylistic output tailored to their individual\npreferences, and it also facilitates the adoption of various\nstyles, each with a range of complexities (Hong et al. 2023).\nStyle Transfer\nDreamStyler transmits styles by inverting a content image\ninto a noisy sample and then denoising it towards the style\ndomain (Meng et al. 2021). With this approach, however,\nthe preservation of content would be suboptimal (Ahn et al.\n2023). To improve this, we inject additional conditions from\nthe content image into the model (Zhang and Agrawala\n2023) (Figure 2). This straightforward pipeline well pre-\nserves with the structure of the content image, while effec-\ntively replicating styles. Moreover, by leveraging a powerful\nprior knowledge from text-to-image models, the style qual-\nity of DreamStyler surpasses that of traditional methods.\nExperiment\nImplementation details. We use T = 6 for multi-stage TI\nand utilize human feedback-based context prompts by de-\nfault. Please refer to Suppl. for more details.\nDatasets. We collected a set of 32 images representing var-\n21\n22\n23\n24\n25\n26\n27\n28\nText Score\n25.0\n27.5\n30.0\n32.5\nStyle Score\nTI\nDreamBooth\nCustomDiffusion\nXTI InST\nDreamStyler\nFigure 6: Performance of text and style scores in style-\nguided text-to-image synthesis. DreamStyler effectively bal-\nances these metrics and surpasses the majority of methods.\nMethod\nText\nStyle\nUser\nScore\nScore\nScore\nTextual Inversion (Gal et al. 2022)\n24.11\n26.84\n2.1%\nDreamBooth (Ruiz et al. 2023)\n22.48\n25.20\n3.9%\nCustomDiffusion (Kumari et al. 2023)\n21.43\n33.45\n4.8%\nXTI (Voynov et al. 2023)\n26.36\n27.07\n4.5%\nInST (Zhang et al. 2023)\n27.05\n23.97\n1.8%\nDreamStyler (Ours)\n26.40\n28.74\n82.9%\nTable 1: Quantitative comparison on the style-guided text-\nto-image synthesis task. Bold: best, underline: second best.\nious artistic styles, following the literature on style trans-\nfer (Tan et al. 2019). To evaluate text-to-image synthesis,\nwe prepared 40 text prompts, as described in Suppl.\nBaselines. In terms of text-to-image synthesis, we compare\nDreamStyler against diffusion-based personalized methods,\nranging from textual inversion to model-optimization ap-\nproaches. For the style transfer task, we compare our method\nto state-of-the-art style transfer frameworks. We utilize offi-\ncial codes for all the methods used in the comparison.\nEvaluation. Text and image scores, based on CLIP, mea-\nsure the alignment with a given text prompt and style image,\nrespectively. Style score assesses the style consistency by\ncalculating the similarity of Gram features between the style\nand generated images. More details are provided in Suppl.\nStyle-Guided Text-to-Image Synthesis\nTable 1 and Figure 6 show quantitative results. DreamStyler\ndelivers a robust performance while managing the trade-off\nbetween text and style scores. A tendency is noted that an\noveremphasis on input text prompts may lead to a compro-\nmise in style quality. Despite this, DreamStyler effectively\nbalances these aspects, yielding a performance that goes be-\nyond the trade-off line, indicative of outstanding capability.\nUser score also supports the distinction of DreamStyler.\nAs shown in Figure 4, previous inversion-based meth-\nods (TI, InST, and XTI) effectively preserve the context of\ntext prompts but fall short in adopting the intrinsic artwork\nof style images. Conversely, the model optimization-based\nmethods (DreamBooth, CustomDiffusion) excel in deliver-\ning styles but struggle to adhere to the prompt or introduce\nStyle & object\n(a) DreamStyler\n(b) Textual inversion (c) CustomDiffusion\nFigure 7: My object in my style. Textual inversion faces\nchallenges in accurately capturing both style and context\nfrom the reference images. Although CustomDiffusion suc-\ncessfully recreates the object\u2019s appearance, it tends to gener-\nate objects in a realistic style, which does not entirely match\nthe target style image. On the other hand, DreamStyler ex-\ncels at synthesizing the object in the user-specified style.\nMethod\nText\nImage\nUser\nScore\nScore\nScore\nAdaAttN (Liu et al. 2021)\n56.67\n56.76\n8.6%\nAesUST (Wang et al. 2022)\n58.05\n58.09\n6.8%\nIEContraAST (Chen et al. 2021)\n59.38\n59.42\n8.6%\nStyTr2 (Deng et al. 2022)\n56.18\n56.28\n21.2%\nAesPA-Net (Hong et al. 2023)\n58.08\n58.15\n8.6%\nInST (Zhang et al. 2023)\n65.32\n65.37\n2.3%\nDreamStyler (Ours)\n66.04\n66.05\n44.1%\nTable 2: Quantitative comparison on the style transfer task.\nobjects in style images (3rd row). DreamStyler, in contrast,\nnot only faithfully follows text prompts but also accurately\nreflects the delicate artistic features of style images.\nStyle Transfer\nAs an extended application, DreamStyler also conducts style\ntransfer. As shown in Table 2, we quantitatively compare\nwith previous style transfer studies. Note that since most\nprior studies have employed Gram loss to boost style quality,\nwe report a CLIP-based image score as an evaluation met-\nric for a more fair comparison. In this benchmark, Dream-\nStyler achieves state-of-the-art performance across text and\nimage scores as well as user preference. Figure 5 also pro-\nvides evidence of DreamStyler\u2019s effectiveness. Our method\nadeptly captures style features such as polygon shapes or\nsubtle brushwork present in style images. These results high-\nlight the method\u2019s capacity to accurately mirror both the the-\nmatic intent and the stylistic nuances of the source artwork.\nStylize My Own Object in My Own Style\nBeyond style transfer that stylizes my image, one might\ndesire to stylize my object (Sohn et al. 2023). In such a\nscenario, a user leverages both their object and style im-\nages. As DreamStyler employs an inversion-based approach,\nthis can be readily accomplished by simply training an ad-\nditional embedding for the object. Subsequently, the user\n23.7\n23.9\n24.1\n24.3\n24.5\nText Score\n26\n28\n30\nStyle Score\nT=1\nT=2\nT=4\nT=6\nT=8\nT=10\nT=12\nFigure 8: Study on the number of stages (T) in multi-stage\nTI. We vary T from 1 to 12 and select T = 6 as the final\nmodel, considering the trade-off between text and style.\n\u201cdesert\nand\noasis\u201d\n\u201ca man \nwith a \nbearded \nface\u201d\nPrompt\nT = 1\nT = 2\nT = 6\nStyle image\nFigure 9: Visual comparison of varying T in multi-stage\nTI. At T = 1, the model fails in both style replication and\nprompt understanding. As T increases, the style quality and\ntext alignment are drastically enhanced.\nfreely merges style and object tokens in the inference prompt\nto generate images. As depicted in Figure 7, DreamStyler\nexcels in accurately reflecting both the style and object\nModel Analysis\nAblation study. In Table 3, we evaluate each component of\nour method. The usage of multi-stage TI substantially aug-\nments both the text and style score, with a marked increase\nin style quality, accentuating the pivotal role of this mod-\nule in creating artistic stylization products. A context-aware\nprompt yields a modest alteration in the quantitative met-\nrics, yet provides a considerable contribution to the qualita-\ntive, which we will discuss in the following section. Style\nand context (S&C) guidance considerably impacts scores,\nreinforcing its significance in sustaining the comprehensive\nquality and coherence of the generated outputs.\nMulti-stage TI. In Figure 8, we delve into the influence of\nthe number of stages (T) on performance. A transition from\nT = 1 to 4 results in substantial improvement. Upon reach-\ning T = 6, the performance begins to navigate trade-off con-\ntours, prompting us to select T = 6 for the final model, as\nwe seek to improve the text alignment of the synthesized\nimages. Nevertheless, users have the flexibility to choose a\ndifferent T value according to their preference. In Figure 9,\nwe provide a visual comparison of the outcomes when T is\nset to 1, 2, and 6. While T = 1 struggles to reflect the artistic\nfeatures of the style image or comprehend the input prompt,\nMethod\nText Score\nStyle Score\nBaseline (Gal et al. 2022)\n23.78\n25.23\n+ Multi-Stage TI\n24.74\n29.86\n+ Context-Aware Prompt\n24.65\n29.50\n+ S&C Guidance (Ours)\n25.38\n29.62\nTable 3: Model ablation study. Upon the textual inversion\nbaseline (Gal et al. 2022), we attach the proposed compo-\nnents to measure the effectiveness of our method.\n\u201cdesert\nand\noasis\u201d\n\u201ca cat\u201d\nStyle image\nPrompt\nw/o description\n+ BLIP2\n+ Human feedback\nFigure 10: Comparison of three prompt strategies. The\nmodel trained without contextual description struggles to\ndisentangle style and context from the style image, gener-\nating elements present in the style reference (e.g. the same\ncomposition in 1st row, a yellow dress in 2nd row). The\ncontextual prompt alleviates this issue to some extent, but\nthe BLIP2-based construction cannot completely eliminate\nit (e.g. the same vanishing point in 1st row). The issue is\nthoroughly addressed when human feedback is utilized.\nT = 2 uplifts the quality, yet it also falls short of embracing\nthe style. In contrast, T = 6 proves proficient at mimicking\nthe style image, effectively replicating delicate brushwork\n(1st row) or emulating the pointillism style (2nd row).\nContext-aware prompt. Figure 10 presents a visual com-\nparison of three prompt constructions. Training the model\nwithout any contextual description (i.e. using \u201cA painting\nin S\u2217 style.\u201d) poses a significant challenge, as it struggles\nto distinguish style from the context within the style im-\nage. Subsequently, this often results in the generation of el-\nements that exist in the style reference, such as objects or\nscene perspective. The introduction of a contextual prompt\nconsiderably alleviates this issue, aiding the model in bet-\nter separating stylistic elements from context. However, the\nautomatic prompt construction does not fully resolve this,\nas BLIP-based captions often fail to capture all the details of\nthe style image. The most effective solution is leveraging hu-\nman feedback in the construction of prompts. This approach\neffectively tackles the issue, resulting in a more robust sep-\naration of style and context in the generated outputs.\nGuidance. In Figure 11, we explore style and context guid-\nance by adjusting the scale parameters. When we amplified\nthe style guidance strength (\u03bbs), the model mirrors the style\nimage, illustrating style guidance\u2019s capability in managing\nthe image\u2019s aesthetics. Yet, overemphasis on style risks com-\npromising the context, leading to outputs that, while stylis-\nContext guidance\nStyle guidance\nStyle image\nStyle image\n-\n+\n-\n+\nFigure 11: Study on the style and context guidance. Inference prompt: \u201cA cat\u201d. By adjusting the scale parameters (\u03bbs, \u03bbc), we\nassess the influence of style and context guidance on the synthesized image. Increasing the style guidance strength causes the\nmodel to align more closely with the aesthetics of the style image; however, an excessive emphasis on style could compromise\nthe context. Conversely, increasing the context guidance strength ensures the output corresponds with the inference prompt, but\noverly strong context guidance could deviate the output from the original style.\nStyle Mixing\n\u201ca \nlighthouse\non a cliff\u201d\n\u201ca bridge\u201d\nStyle image\nA\nB\nC\nPrompt\nA\nT\n0\nT\n0\nA\nA\nB\nT\n0\nA\nB\nC\nBaseline\nB\nT\n0\nT\n0\nB\nB\nA\nT\n0\nB\nC\nA\nA\n+\nB\n+\nC\nA\n+\nB\n+\nC\nFigure 12: Style mixing. Multi-stage TI facilitates style mixing from various style references. A user can customize a new style\nby substituting style tokens at different stages t. For example, the style token closer to t = T tends to influence the structure of\nthe image, while those closer to t = 0 have a stronger effect on local and detailed attributes. For comparison, we display the\nbaseline that employs all style tokens at every stage (i.e. using \u201cA painting in SA\nt , SB\nt , SC\nt style\u201d at all stages).\ntically congruent, might diverge from the intended context.\nOn the other hand, strengthening context guidance (\u03bbc) en-\nsures the output resembles the inference prompt, highlight-\ning context guidance\u2019s essential role in preserving contex-\ntual integrity. However, excessively strong context guidance\ncould steer the output away from the original style, under-\nlining the need for a nuanced balance of guidance for gen-\nerating visually appealing and contextually accurate images.\nNevertheless, this offers a new dimension of control over the\nsynthesized image, differing from the classifier-free guid-\nance (Ho and Salimans 2022). The additional control is a\ncrucial element in the workflow of digital art production,\nconsidering its delicate and nuanced final outcomes.\nStyle mixing. As shown in Figure 12, multi-stage TI opens\nup a novel avenue for an intriguing aspect of style mixing\nfrom diverse style references. This process empowers users\nto customize a unique style by deploying different style to-\nkens at each stage t. The style tokens close to t = T pre-\ndominantly impact the structure of the image, akin to broad\nstrokes, while tokens closer to t = 0 affect local and de-\ntailed attributes, akin to intricate brushwork. To provide a\nconcrete point of comparison, we present a baseline model\nthat incorporates all style tokens at every stage, using the\nprompt \u201cA painting in SA\nt , SB\nt , SC\nt styles\u201d. While the base-\nline produces reasonable style quality, it lacks a control fac-\ntor for extracting partial stylistic features from the reference.\nConsequently, the fusion of styles with multi-stage TI under-\nscores the creative and flexible nature of our model, offering\nusers a broad range of applications for artistic creation.\nConclusion\nWe have introduced DreamStyler, a novel image generation\nmethod with a given style reference. By optimizing multi-\nstage TI with a context-aware text prompt, DreamStyler\nachieves remarkable performance in both text-to-image syn-\nthesis and style transfer. Content and style guidance provides\na more adaptable way of handling diverse style references.\nLimitations. While DreamStyler exhibits outstanding abil-\nity in generating artistic imagery, it is important to acknowl-\nedge its limitations within the intricate context of artistic ex-\npression. The vast spectrum of artistry, spanning from prim-\nitive elements to more nuanced and abstract styles (such as\nsurrealism), demands thorough definition and examination\nfrom both artistic and technological perspectives.\nReferences\nAhn, N.; Kwon, P.; Back, J.; Hong, K.; and Kim, S. 2023.\nInteractive Cartoonization with Controllable Perceptual Fac-\ntors. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 16827\u201316835.\nBalaji, Y.; Nah, S.; Huang, X.; Vahdat, A.; Song, J.; Kreis,\nK.; Aittala, M.; Aila, T.; Laine, S.; Catanzaro, B.; et al. 2022.\nediffi: Text-to-image diffusion models with an ensemble of\nexpert denoisers. arXiv preprint arXiv:2211.01324.\nChen, H.; Wang, Z.; Zhang, H.; Zuo, Z.; Li, A.; Xing, W.;\nLu, D.; et al. 2021.\nArtistic style transfer with internal-\nexternal learning and contrastive learning. Advances in Neu-\nral Information Processing Systems, 34: 26561\u201326573.\nChoi, J.; Lee, J.; Shin, C.; Kim, S.; Kim, H.; and Yoon, S.\n2022. Perception prioritized training of diffusion models.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 11472\u201311481.\nDeng, Y.; Tang, F.; Dong, W.; Ma, C.; Pan, X.; Wang, L.;\nand Xu, C. 2022. StyTr2: Image Style Transfer with Trans-\nformers. In CVPR, 11326\u201311336.\nGal, R.; Alaluf, Y.; Atzmon, Y.; Patashnik, O.; Bermano,\nA. H.; Chechik, G.; and Cohen-Or, D. 2022. An image is\nworth one word: Personalizing text-to-image generation us-\ning textual inversion. arXiv preprint arXiv:2208.01618.\nGatys, L. A.; Ecker, A. S.; and Bethge, M. 2016. Image\nstyle transfer using convolutional neural networks. In CVPR,\n2414\u20132423.\nHo, J.; and Salimans, T. 2022. Classifier-free diffusion guid-\nance. arXiv preprint arXiv:2207.12598.\nHong, K.; Jeon, S.; Lee, J.; Ahn, N.; Kim, K.; Lee, P.;\nKim, D.; Uh, Y.; and Byun, H. 2023. AesPA-Net: Aesthetic\nPattern-Aware Style Transfer Networks.\nHuang, X.; and Belongie, S. 2017. Arbitrary style transfer\nin real-time with adaptive instance normalization. In ICCV,\n1501\u20131510.\nKumari, N.; Zhang, B.; Zhang, R.; Shechtman, E.; and Zhu,\nJ.-Y. 2023.\nMulti-concept customization of text-to-image\ndiffusion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 1931\u20131941.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023.\nBlip-2:\nBootstrapping language-image pre-training with frozen im-\nage encoders and large language models.\narXiv preprint\narXiv:2301.12597.\nLi, W.; Xu, X.; Xiao, X.; Liu, J.; Yang, H.; Li, G.; Wang,\nZ.; Feng, Z.; She, Q.; Lyu, Y.; et al. 2022. UPainting: Uni-\nfied Text-to-Image Diffusion Generation with Cross-modal\nGuidance. arXiv preprint arXiv:2210.16031.\nLiu, S.; Lin, T.; He, D.; Li, F.; Wang, M.; Li, X.; Sun, Z.;\nLi, Q.; and Ding, E. 2021. Adaattn: Revisit attention mech-\nanism in arbitrary neural style transfer. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n6649\u20136658.\nMeng, C.; He, Y.; Song, Y.; Song, J.; Wu, J.; Zhu, J.-Y.;\nand Ermon, S. 2021. Sdedit: Guided image synthesis and\nediting with stochastic differential equations. arXiv preprint\narXiv:2108.01073.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748\u20138763. PMLR.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M.\n2022. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 10684\u2013\n10695.\nRuiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein, M.; and\nAberman, K. 2023. Dreambooth: Fine tuning text-to-image\ndiffusion models for subject-driven generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 22500\u201322510.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,\nE. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,\nB.; Salimans, T.; et al. 2022. Photorealistic text-to-image\ndiffusion models with deep language understanding.\nAd-\nvances in Neural Information Processing Systems, 35:\n36479\u201336494.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSohn, K.; Ruiz, N.; Lee, K.; Chin, D. C.; Blok, I.; Chang, H.;\nBarber, J.; Jiang, L.; Entis, G.; Li, Y.; et al. 2023. StyleDrop:\nText-to-Image Generation in Any Style.\narXiv preprint\narXiv:2306.00983.\nTan, W. R.; Chan, C. S.; Aguirre, H.; and Tanaka, K. 2019.\nImproved ArtGAN for Conditional Synthesis of Natural Im-\nage and Artwork. IEEE Transactions on Image Processing,\n28(1): 394\u2013409.\nVoynov, A.; Chu, Q.; Cohen-Or, D.; and Aberman, K. 2023.\nP+: Extended Textual Conditioning in Text-to-Image Gen-\neration. arXiv preprint arXiv:2303.09522.\nWang, Z.; Zhang, Z.; Zhao, L.; Zuo, Z.; Li, A.; Xing, W.; and\nLu, D. 2022. AesUST: towards aesthetic-enhanced universal\nstyle transfer. In Proceedings of the 30th ACM International\nConference on Multimedia, 1095\u20131106.\nZhang, L.; and Agrawala, M. 2023.\nAdding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543.\nZhang, Y.; Huang, N.; Tang, F.; Huang, H.; Ma, C.; Dong,\nW.; and Xu, C. 2023. Inversion-based style transfer with dif-\nfusion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 10146\u201310156.\nAppendix\nStyle and Context Guidance\nIn this section, we derive how we obtain the style and context\nguidance depicted in Eq. (3) in the main text.\nDerivation. Classifier-free guidance (Ho and Salimans\n2022) modifies the predicted noise estimation to include the\ngradient of the log-likelihood of p(v|xt), where v denotes\nthe conditional embedding tokens (of a given text) and xt\nis the denoised sample at t-th denoising step. Given that\np(v|xt) \u221d p(xt|v)/p(xt), it follows that \u2207xt log p(xt|v) \u221d\n\u2207xt log p(xt, v) \u2212 \u2207xt log p(xt). We parameterize the ex-\nact score with the score estimator as \u03f5\u03b8(.) \u221d \u2207xt log p(.),\nenabling us to derive the classifier-free guidance term as:\n\u02c6\u03f5\u03b8(xt, v) = \u03f5\u03b8(\u2205) + \u03bbn[\u03f5\u03b8(xt, v) \u2212 \u03f5\u03b8(xt, \u2205)].\n(4)\nTo derive the proposed style and context guidance, we\nfirst decompose text conditional embedding tokens v into\nits style and context components as v = vs \u2229 vc. As an ex-\nample, v is the embedding tokens of inference prompt \u201cA\npainting of a house in the style of S\u2217\u201d, while vs and vc are\nthe embedding tokens of \u201cA painting of a house\u201d and \u201cin the\nstyle of S\u2217\u201d prompts, respectively. Given these partitions,\nwe can rewrite p(v|xt) using the terms below:\np(v|xt) = p(vs \u2229 vc|xt)\n(5)\n\u221d p(vc|xt)p(vs|vc, xt)\n(6)\n\u221d p(vs|xt)p(vc|vs, xt).\n(7)\nFrom Eq. (6), we derive the following expression:\np(v|xt) \u221d p(xt|vc)\np(xt)\np(xt|vs, vc)\np(xt|vc)\n(8)\nAs in Ho and Salimans (2022), we deduce the above equa-\ntion to the style and context guidance as in below.\n\u02c6\u03f51\n\u03b8(xt, v) = \u03f5\u03b8(xt, \u2205)\n+ \u03bbc[\u03f5\u03b8(xt, vc) \u2212 \u03f5\u03b8(xt, \u2205)]\n+ \u03bbs[\u03f5\u03b8(xt, v) \u2212 \u03f5\u03b8(xt, vc)]\n(9)\nSimilarly, using Eq. (7), we derive another guidance term:\n\u02c6\u03f52\n\u03b8(xt, v) = \u03f5\u03b8(xt, \u2205)\n+ \u03bbs[\u03f5\u03b8(xt, vs) \u2212 \u03f5\u03b8(xt, \u2205)]\n+ \u03bbc[\u03f5\u03b8(xt, v) \u2212 \u03f5\u03b8(xt, vs)]\n(10)\nRather than relying solely on either Eq. (9) or Eq. (10), we\nemploy a balanced guidance approach by integrating both\nforms of guidance as shown in Eq. (3) in the main text. We\nwill elaborate on the benefits of utilizing both terms in the\nfollowing section. In practice, alongside Eq. (3), we also\nleverage the conventional classifier-free guidance. There-\nfore, the final style and context guidance is as follows:\n\u02c6\u03f5\u03b8(xt, v) = \u03f5\u03b8(xt, \u2205)\n+ \u03bbn[\u03f5\u03b8(xt, v) \u2212 \u03f5\u03b8(xt, \u2205)]\n+ \u03bbc[\u03f5\u03b8(xt, vc) \u2212 \u03f5\u03b8(xt, \u2205)]\n+ \u03bbs[\u03f5\u03b8(xt, v) \u2212 \u03f5\u03b8(xt, vc)]\n+ \u03bbs[\u03f5\u03b8(xt, vs) \u2212 \u03f5\u03b8(xt, \u2205)]\n+ \u03bbc[\u03f5\u03b8(xt, v) \u2212 \u03f5\u03b8(xt, vs)]\n(11)\nPhase\nHyperparameter\nValue\nOptimization\nOptimization steps\n500\nLearning rate\n0.002\nBatch size\n8\nT (multi-stage TI)\n6\nInference\nInference steps\n25\nScheduler\nDPM\n\u03bbn (null guidance)\n5.0\n\u03bbs (style guidance)\n0.5\u223c3.0\n\u03bbc (context guidance)\n3.0\nTable 4: Detailed hyperparameters of DreamStyler.\nBy utilizing both the proposed and the classifier-free guid-\nances, we can subtly adjust the scaling of style and con-\ntext guidance (\u03bbs, \u03bbc), starting from a zero value. For in-\nstance, to minimize the influence of the style guidance on\nthe output images, one merely needs to set \u03bbs to zero with-\nout changing other hyperparameters, and similarly, this ap-\nplies to the context guidance as well. Without the incorpo-\nration of classifier-free guidance, users would be exhausted\nfrom initiating the guidance scaling search from scratch. We\ndescribe the guidance scaling parameters in Table 4.\nExperimental Settings\nImplementation details. Table 4 demonstrates hyperparam-\neters used for our method. We optimize DreamStyler with\nEq. (2) (in the main text), which is a similar training proce-\ndure to textual inversion (Gal et al. 2022). We utilize a single\nA100 GPU for both optimization and inference.\nFor the style transfer task, we extract a depth map con-\ndition from a content image and then encode structure fea-\ntures by adopting ControlNet (Zhang and Agrawala 2023).\nAlthough various conditional modalities can be considered,\nwe observed that a depth map is the most suitable choice\nfor style transfer. An edge map tends to strongly preserve\nthe structure of content images so that the structural styles\nare not effectively conveyed to output images. While one\nmight consider employing a segmentation map to better pre-\nserve the structure, this approach can introduce artifacts, es-\npecially if the segmentation map is not flawlessly generated.\nDatasets. To build a dataset used in our model evaluation,\nwe collect most of the artistic paintings from the WikiArt\ndataset (Tan et al. 2019), while some of the modern art and\nillustration are from Unsplash1, licensed free images only. In\nTable 5, we show inference prompts used in the evaluation.\nEvaluation. For model evaluation, we employ CLIP-based\ntext and image scores as well as Gram-based style score.\nThe CLIP text score evaluates the alignment of a generated\nimage I with a given text prompt C as in below.\nTextScore(I, C) = max(100 \u2217 cos(EI, EC), 0),\n1https://unsplash.com\nwhere, EI denotes visual CLIP embedding of image I and\nEC is textual CLIP embedding of prompt C. The CLIP im-\nage score measures the alignment of a generated image I and\na given style reference image S with the following equation.\nImageScore(I, S) = max(100 \u2217 cos(EI, ES), 0).\nThe Gram-based style score is a widely used metric in\nstyle transfer. It quantifies the textural alignment between\ntwo images using Gram features, as below equation.\nStyleScore(I, S) = 50 \u2212 1\nL\nL\nX\nl=1\n1\nB\nX\n\u2200(i,j)\u2208B\ncos(P i\nI, P j\nS)\nHere, L is the number of layers of VGG16 (Simonyan and\nZisserman 2014), which is used to calculate Gram features.\nB represents the selected patch pair set and P i is i-th image\npatch of an image. Notably, we assess the style alignment\npatch-wise; this technique is more adept at capturing local\nstyle features than when analyzing the entire image. Addi-\ntionally, to align the magnitude of this score with others, we\nsubtract Gram similarity to 50. For an efficient implemen-\ntation, we choose five patches, each of size 224\u00d7224, by\ncropping from the four corners and the center of the image.\nUser study. We asked 37 participants where they were asked\nto select the best results based on how closely the outputs\nadhered to the style of reference images and the context\nof inference prompts (or content images). Each participant\nwas tasked to vote on 9 questions for text-to-image synthe-\nsis (yielding 333 responses in total) and 7 questions for style\ntransfer (yielding 259 responses in total). They were pre-\nsented with style images, inference prompts, and the results\nfrom DreamStyler to other methods. For the style transfer\nevaluations, content images were displayed in place of the\ninference prompts. The questionnaires used in the user study\nare detailed in Table 6. We determined the User score based\non the ratio of instances voted as the best.\nAdditional Model Analysis\nTraining/inference time. Textual inversion methods typ-\nically demand increased training time due to the for-\nward/backward passes in the CLIP text encoder. Nev-\nertheless, the inference time difference with the model\noptimization-based approach is minimal. In our measure-\nment, with a batch size of one, DreamBooth, TI, and Dream-\nStyler requires 300s, 620s, and 580s, respectively. With 8\nbatch size, DreamBooth, TI, and DreamStyler takes 60s,\n500s, and 480s. Despite the additional time required, Dream-\nStyler proves its worth by delivering superior stylization out-\ncomes as shown in a series of experiments.\nFew-shot text-to-image synthesis. Figure 13 depicts the\nchange in performance trade-offs for diffusion-based per-\nsonalization methods when transitioning from a one-shot\nto a few-shot training regime. For this analysis, all meth-\nods are trained using five artistic-style images. Importantly,\nmodel optimization-based frameworks, such as DreamBooth\nand CustomDiffusion, exhibit marked improvements in text\nscores (as compared to Figure 6, in the main text) due to\n23\n24\n25\n26\n27\n28\n29\nText Score\n31\n32\n33\n34\n35\nStyle Score\nTI\nDreamBooth\nCustomDiffusion\nXTI\nDreamStyler\nFigure 13: Performance of few-shot style-guided text-to-\nimage synthesis. All methods are trained using five artis-\ntic style images. In a few-shot training regime, the model\noptimization-based approaches (DreamBooth, CustomDif-\nfusion) significantly enhance text score. This is because they\nnow can distinguish context from style by referring to mul-\ntiple style images, leveraging their powerful capacity inher-\nited from learning the model parameters. However, previous\ntextual inversion-based methods (TI, XTI) cannot enjoy us-\ning multiple images, showing lower style and text scores.\nDreamStyler strikes a good balance, exhibiting significant\nimprovement over other textual inversion-based methods.\ntheir capability derived from directly optimizing parame-\nters of denoising UNet. Conversely, textual inversion-based\nmethods, such as TI and XTI, fail to leverage the benefits\nof multiple images due to the inherent capacity constraint\nof the embedding optimization nature. Despite DreamStyler\nemploying a textual inversion-based approach, our method\nnot only distinguishes from other textual inversion frame-\nworks but also achieves a trade-off performance comparable\nto CustomDiffusion, attributed to the timestep-aware textual\nembedding and context-aware training prompt strategy.\nStyle and context guidance. In Figure 14, we compare three\ndifferent guidance settings: 1) \u02c6\u03f5c(v), shown in Eq. (3) in the\nmain text, 2) \u02c6\u03f51\n\u03b8(v), shown in Eq. (9), and 3) \u02c6\u03f52\n\u03b8(v), shown\nin Eq. (10). Note that the difference in the outputs (\u03f5(v))\nrelative to the one with null condition (\u03f5(\u2205)) exceeds those\nof with style-only (\u03f5(vs)) or context-only (\u03f5(vc)). This is\nbecause the null conditioned outputs are presumed to pro-\nduce arbitrary imagery. Hence, when we only rely on \u02c6\u03f51\n\u03b8(v)\nor \u02c6\u03f52\n\u03b8(v), the guidance influence between style and context\nmight be imbalanced even with the same scaling parame-\nters, \u03bbs, \u03bbc. For instance, with the \u02c6\u03f51\n\u03b8(v) guidance, the com-\nputation involves differences between \u03f5(vc) and \u03f5(\u2205), which\namplifies contextual details at the expense of style attributes.\nOn the other hand, \u02c6\u03f52\n\u03b8(v) emphasizes style, often overlooks\nthe contextual facets. To achieve a harmonious blend of style\nand context guidance, we incorporate both guidance forms.\nAblation study on multi-stage TI. To demonstrate the ne-\ncessity of multi-stage TI, we compare our proposed TI with a\nnaive multi-token approach in Figure 15. Naive multi-token\nTI (V = 2, 6) is the method in which only the number of\nembedding tokens is increased. Our multi-stage TI approach\noutperforms both StyleScore and TextScore.\n\u201cdesert\nand\noasis\u201d\n\u201ca \nwooden \npot\u201d\nPrompt\nStyle image\nGuidance: \nGuidance: \nGuidance: \nFigure 14: Comparison of different guidance forms. Ap-\nplying \u02c6\u03f51\n\u03b8(v) guidance effectively captures the context of a\ngiven inference prompt. However, it sometimes fails to adopt\nthe style of a reference image. Conversely, \u02c6\u03f52\n\u03b8(v) guidance\nadeptly aligns the stylistic elements with the reference image\nbut often overlooks certain contexts in the inference prompt.\nBy merging these guidance terms, as depicted in Eq. (3) in\nthe main text, the model achieves a well-balanced expres-\nsion of both style and context.\nThe effectiveness of # T. We observed a saturation of Style-\nScore when T \u2265 6 (Figure 8). This indicates that more em-\nbeddings beyond this threshold do not contribute new stylis-\ntic nuances and may instead introduce redundancy in artis-\ntic expression. For TextScore, we observed an unexpected\ntrend at T \u2265 6; it starts to generate subjects not specified\nin the prompt, but presented in style image. We hypothesize\nthat as T increases to a high value, the embeddings try to\nlearn context of style image, beyond stylistic focus; a po-\ntential overextension of the embedding capacity. Compara-\ntively, DreamBooth\u2019s approach, which involves modifying\na large set of parameters, manifests a substantially lower\nTextScore. Thus, we speculate that increasing T to an ex-\ntremely high value may yield trends similar to DreamBooth.\nNevertheless, regarding TextScore, we believe further inves-\ntigation is required, and we are grateful for the highlight of\nthis aspect, which had previously eluded our consideration.\nWhen the style guidance is effective? In Figure 18, we in-\nvestigate when the style guidance significantly influences\nthe resulting synthesized image. Our observations indicate\nthat style guidance plays a crucial role in ensuring proper\nstyle expression, especially when the output, with zero style\nguidance (\u03bbs = 0), fails to capture the stylistic nuances\npresent in a reference image. Conversely, when the output\nalready effectively expresses the stylistic elements, the influ-\nence of style guidance becomes marginal, even with a sub-\nstantial increase in the scaling parameters \u03bbs.\nGiven these observations, it is now essential to distinguish\nwhen the model can accurately replicate style images with-\nout the need for style guidance. We hypothesize that style\npattern repeatability (Hong et al. 2023) is linked to this abil-\nity to some extent. Specifically, when a stylistic feature is\ncomplex, the model struggles to capture all its stylistic de-\ntails. In such cases, the style guidance serves as an effective\nbooster for the model\u2019s style representation. Another influ-\nencing factor, we conjecture, is the global colortone of the\n23.7\n23.9\n24.1\n24.3\n24.5\nText Score\n25\n27\n29\nStyle Score\nT=2\nT=6\nV=2\nV=6\nFigure 15: Multi-stage TI (T) vs. naive multi-token (V ).\nstyle image. We observed that when there is a pronounced\ndisparity in the colortone of the style image compared to a\nnatural photo, the model often struggles to mimic it, even if\nthe style is not particularly complicated. Despite our obser-\nvations, determining whether a model can easily adapt to a\nparticular style remains a non-trivial question, and we leave\nthis investigation for future research.\nWhen the context guidance is effective? Figure 19 demon-\nstrates scenarios in which the context guidance proves ben-\neficial for artistic image synthesis. We hypothesize that the\ncontext guidance is particularly effective when the style im-\nage is expressed in a very abstract manner, especially when\nthe structures of the desired subjects to draw deviate signif-\nicantly from the stylistic expression of the reference image.\nConversely, when the style images aptly capture the details\nof some subjects (as realism paintings do usually), the model\ncan render the desired subject in that style without needing\ncontext guidance. However, determining when the personal-\nized diffusion model can adeptly convey the subject with a\ngiven style image remains an open question, and we leave\nthis exploration for future work.\nStyle transfer In Figure 16, we illustrate the role of con-\ndition modality in the style transfer task. The figure indi-\ncates that when the additional condition modality is incor-\nporated via ControlNet (Zhang and Agrawala 2023), the re-\nsulting style-transferred images retain a significant degree of\nfidelity to the original contents\u2019 structure. On the other hand,\nmethods that bypass this step and rely solely on the image\ninversion technique (Meng et al. 2021) often introduce con-\nsiderable alterations to the initial structure.\nTraining progress. In Figure 20,21, and 22, we compare\nmodel optimization- and textual inversion-based methods\nthroughout their training progress. To conduct this, we dou-\nble the training steps and plot the intermediate results of\neach method. This allows us to examine the training tenden-\ncies and to determine whether the methods fall into overfit-\nting. Through this inspection, we highlight the strengths and\nweaknesses of these approaches. The model optimization-\nbased approach, DreamBooth (Ruiz et al. 2023), CustomD-\niffusion (Kumari et al. 2023), exhibits superior style adap-\ntation capability owing to its rich capacity as we train the\nmodel directly. However, this approach is prone to overfit-\nting. When overfitting occurs, the model tends to generate\nimages that are very similar to the style images, disregard-\ning all the context from the inference prompt. A critical\nStyle image\nw/ additional cond. w/o additional cond.\nContent image\nFigure 16: Study on the role of additional conditions in\nstyle transfer. When using additional conditions encoded\nthrough ControlNet (Zhang and Agrawala 2023), the out-\nputs faithfully reflect the structure of content images. In con-\ntrast, outputs without additional condition and relying solely\non image inversion (Meng et al. 2021) considerably changes\nthe structure. This issue is also observed in other inversion-\nbased methods (Zhang et al. 2023; Ahn et al. 2023).\npoint to note is that the training steps at which overfitting\noccurs vary significantly to style images. In practice, this is\nproblematic because users cannot select a fixed number of\ntraining steps. Instead, they need to inspect all intermedi-\nate samples to identify the best checkpoint, which is highly\ntime-consuming and unsustainable in real-world production.\nConversely, the textual inversion-based approach (Gal et al.\n2022) might not replicate the style image as effectively as\nmodel optimization does, but it is less prone to overfitting.\nThis tendency might appear to be symptomatic of under-\nfitting. DreamStyler takes the strengths of both approaches\nwhile mitigating their weaknesses. It avoids overfitting in\nthe same way as the textual inversion-based method, yet it\nalso adeptly captures the stylistic nuances found in the style\nimage, akin to the model optimization-based approach. This\nemphasizes the superiority and practicality of DreamStyler.\nIn Figure 17, we demonstrate quantitative results of such\noverfitting phenomena. In our task, overfitting is character-\nized by the model\u2019s neglect of the text prompt and verbatim\nreproduction of the style image and this can be identifiable\nthrough changes in the TextScore. Notably, CustomDiffu-\nsion and DreamBooth exhibit a sudden and substantial drop\nin TextScore at the point when the results are merely copied\nfrom style images. On the contrary, DreamStyler performs\nbetter in mitigating overfitting than DreamBooth.\nAdditional Qualitative Results\nIn this section, we provide additional qualitative results\nfor better comparisons with previous studies. As shown in\nFigure 23, DreamStyler successfully generates results with\ngiven prompts while depicting the specific style of reference\nimages such as pointillism or watercolor. We also provide\nTraining progress\n18\n20\n22\n24\n26\n28\nText Score\nDreamBooth\nCustomDiffusion\nTI\nDreamStyler\nFigure 17: Training progress.\nadditional comparison results on the style transfer task in\nFigure 24. DreamStyler transfer the style impeccably while\npreserving the semantic information of content images.\nStyle guidance\nStyle image\n-\n+\n(a) Significant effects of the style guidance.\nStyle guidance\nStyle image\n-\n+\n(b) Moderate effects of the style guidance.\nFigure 18: When is the style guidance beneficial? Inference prompt: \u201cA cat\u201d. We investigate the role of the style guidance\nin stylistic representation. (a) Our findings show that the style guidance significantly impacts style expression when generated\nsamples with zero-guidance (leftmost ones) fails to adequately capture the style image. (b) In contrast, when the samples\nwithout the style guidance effectively embody stylistic nuances, the introduction of the style guidance has minimal effect. We\nhypothesize that the style guidance is particularly beneficial for images with high pattern complexity (as shown in (a)), so that\nthe model cannot easily adapt to. In cases of low pattern complexity (as shown in (b)), its influence appears to be marginal.\n-\n+\nContext guidance\nStyle image\n(a) Significant effects of the context guidance.\n-\n+\nContext guidance\nStyle image\n(b) Moderate effects of the context guidance.\nFigure 19: When is the context guidance beneficial? Inference prompt: \u201cA cat\u201d. We investigate the role of the context guidance\nin the context (subject) production aspect. (a) Our findings show that the context guidance substantially impacts to appearance\nof subjects when generated samples with zero-guidance (leftmost ones) exhibit abstract subject structures. (b) In contrast, when\nsubjects already appear detailed even without the context guidance, the introduction of the context guidance has minimal effect.\nWe hypothesize that the context guidance is particularly beneficial for images with a high degree of structural abstraction (as\nshown in (a)). In contrast, for the images that present detailed textures, its influence appears to be marginal.\nStyle image\n(a) DreamStyler\n(d) Textual Inversion\n(b) CustomDiffusion\n(c) DreamBooth\nTraining progress\nEarly\nstop\nFigure 20: Comparison of training progress. Inference prompt: \u201cA painting of a dog\u201d. We double the training steps for each\nmodel and visualize their training tendencies. \u201cEarly stop\u201d refers to the original (official) training step which we also used in the\npaper. Model optimization-based methods tend to suffer from overfitting to the style image, and we observed that the occurrence\nof overfitting varies depending on the style image. Notably, DreamBooth often fails to reflect the style image initially and then\nabruptly overfits to it. Since it\u2019s challenging to predict when overfitting will begin, users would need to monitor all intermediate\ntraining samples, which is impractical for production-level tasks. While textual inversion-based methods (including ours) are\nimmune to overfitting, the vanilla TI often fails to accurately capture both the style images and the inference prompt. In contrast,\nDreamStyler avoids overfitting and consistently demonstrates high-quality stylization in terms of both style and context.\nTraining progress\nEarly\nstop\nStyle image\n(a) DreamStyler\n(d) Textual Inversion\n(b) CustomDiffusion\n(c) DreamBooth\nFigure 21: Comparison of training progress. Inference prompt: \u201cA painting of a house\u201d.\nTraining progress\nEarly\nstop\nStyle image\n(a) DreamStyler\n(d) Textual Inversion\n(b) CustomDiffusion\n(c) DreamBooth\nFigure 22: Comparison of training progress. Inference prompt: A painting of a city\u201d.\n\u201ca \nknight\u201d\n\u201ca rose\u201d\n\u201ca dog\u201d\n\u201ca cat\u201d\n\u201ca castle \nin a \nmisty \nland\nscape\u201d\n\u201ca \nmysteri\nous \ndoor\u201d\n\u201ca \nmermaid \nsitting \non a \nrock\u201d\n\u201ca \nsailboat \non the \nsea\u201d\n\u201cfire\nworks \nover the \ncity\u201d\n\u201ca \nharbor\u201d\nStyle image\nPrompt\n(a) DreamStyler\n(b) InST\n(c) XTI\n(d) Textual Inversion (e) CustomDiffusion\n(f) DreamBooth\nFigure 23: Additional qualitative comparison on the style-guided text-to-image synthesis task.\nContent\n& Style\n(a) DreamStyler\n(b) InST\n(c) AesPA-Net\n(d) StyTr2\n(f) AesUST\n(e) IEContraAST\n(g) AdaAttN\nFigure 24: Additional qualitative comparison on the style transfer task.\nEvaluation prompt\nA painting of a house in the style of S\u2217\nA painting of a dog in the style of S\u2217\nA painting of a robot in the style of S\u2217\nA painting of a crying woman in the style of S\u2217\nA painting of a wooden pot in the style of S\u2217\nA painting of an ocean on a cloudy day in the style of S\u2217\nA painting of a city in the style of S\u2217\nA painting of a full moon in the mountains in the style of S\u2217\nA painting of a temple in the forest in the style of S\u2217\nA painting of a cat in the style of S\u2217\nA painting of a rose in the style of S\u2217\nA painting of a man with a bearded face in the style of S\u2217\nA painting of a solar system in the style of S\u2217\nA painting of a spaceship in the style of S\u2217\nA painting of a highway in the style of S\u2217\nA painting of a bridge in the style of S\u2217\nA painting of an apple and a banana in the style of S\u2217\nA painting of fireworks over the city in the style of S\u2217\nA painting of a harbor in the style of S\u2217\nA painting of a rock island and an ocean in the style of S\u2217\nA painting of a blossoming cherry tree in the style of S\u2217\nA painting of a snowy mountain peak in the style of S\u2217\nA painting of a knight in the style of S\u2217\nA painting of a marketplace in the style of S\u2217\nA painting of a desert and an oasis in the style of S\u2217\nA painting of elephants at sunset in the style of S\u2217\nA painting of a crowd under the stars in the style of S\u2217\nA painting of a lighthouse on a cliff in the style of S\u2217\nA painting of a mermaid sitting on a rock in the style of S\u2217\nA painting of a haunted house on a hill in the style of S\u2217\nA painting of a cafe in the morning in the style of S\u2217\nA painting of a castle in a misty landscape in the style of S\u2217\nA painting of birds taking flight in the style of S\u2217\nA painting of a family in the style of S\u2217\nA painting of a waterfall in a forest in the style of S\u2217\nA painting of a night of shooting stars in the style of S\u2217\nA painting of a sailboat on the sea in the style of S\u2217\nA painting of a girl under a tree in the style of S\u2217\nA painting of a fisherman casting his net at sunrise in the style of S\u2217\nTable 5: Inference prompts used in the model evaluation.\nTask\nInstruction and Question\nText-to-Image\nInstruction:\nEach question provides 1) a style image, 2) a text prompt, and 3) 6 generated samples.\nSamples are results of synthesizing context of the text prompt to the style depicted in the style image.\nPlease select the sample that best represents both the text prompt and the style of the style image.\nQuestion:\nWhich sample best captures the style of the style image and the content of the text?\nStyle Transfer\nInstruction:\nEach question provides 1) a style image, 2) a content image, and 3) 7 generated samples.\nSamples are results of adapting the content image to the style depicted in the style image.\nPlease select the sample that best resembles with the artistic style of the artists\u2019 style image,\nas if that artist used the content image as a reference in their painting.\nQuestion:\nIf the artist of the style image had used the content image as a reference,\nwhich sample best embodies their artistic style?\nTable 6: Questionnaires used in the user study.\n"
  },
  {
    "title": "Text-Guided Generation and Editing of Compositional 3D Avatars",
    "link": "https://arxiv.org/pdf/2309.07125.pdf",
    "upvote": "6",
    "text": "TECA: Text-Guided Generation and Editing of Compositional 3D Avatars\nHao Zhang1,3,4*, Yao Feng1,2*, Peter Kulits1, Yandong Wen1\nJustus Thies 1, Michael J. Black1\n1Max Planck Institute for Intelligent Systems, 2ETH Z\u00a8urich, 3Tsinghua University,\n4RWTH Aachen University, *Equal contribution\nProject Page: yfeng95.github.io/teca\nA slim woman\nwith a dreadlocks hairstyle\nHat and clothing transferred to another avatar\nGenerated avatars\nExample avatars and hairstyles from text descrip;ons\nFigure 1. Compositional avatars: (left) Given a text description, our method produces a 3D avatar consisting of a mesh-based face and body\n(gray) and NeRF-based style components (e.g. hair and clothing). (middle) Our method generates diverse avatars and hairstyles based on\nthe text input. (right) The non-face parts can be seamlessly transferred to new avatars with different shapes, without additional processing.\nAbstract\nOur goal is to create a realistic 3D facial avatar with\nhair and accessories using only a text description. While\nthis challenge has attracted significant recent interest, exist-\ning methods either lack realism, produce unrealistic shapes,\nor do not support editing, such as modifications to the\nhairstyle. We argue that existing methods are limited be-\ncause they employ a monolithic modeling approach, using\na single representation for the head, face, hair, and acces-\nsories. Our observation is that the hair and face, for ex-\nample, have very different structural qualities that bene-\nfit from different representations. Building on this insight,\nwe generate avatars with a compositional model, in which\nthe head, face, and upper body are represented with tradi-\ntional 3D meshes, and the hair, clothing, and accessories\nwith neural radiance fields (NeRF). The model-based mesh\nrepresentation provides a strong geometric prior for the\nface region, improving realism while enabling editing of\nthe person\u2019s appearance. By using NeRFs to represent the\nremaining components, our method is able to model and\nsynthesize parts with complex geometry and appearance,\nsuch as curly hair and fluffy scarves.\nOur novel system\nsynthesizes these high-quality compositional avatars from\ntext descriptions. Specifically, we generate a face image\nusing text, fit a parametric shape model to it, and inpaint\ntexture using diffusion models. Conditioned on the gener-\nated face, we sequentially generate style components such\nas hair or clothing using Score Distillation Sampling (SDS)\nwith guidance from CLIPSeg segmentations. However, this\nalone is not sufficient to produce avatars with a high degree\nof realism. Consequently, we introduce a hierarchical ap-\nproach to refine the non-face regions using a BLIP-based\nloss combined with SDS. The experimental results demon-\nstrate that our method, Text-guided generation and Editing\nof Compositional Avatars (TECA), produces avatars that\nare more realistic than those of recent methods while be-\ning editable because of their compositional nature.\nFor\nexample, our TECA enables the seamless transfer of com-\npositional features like hairstyles, scarves, and other ac-\ncessories between avatars.\nThis capability supports ap-\nplications such as virtual try-on. The code and generated\narXiv:2309.07125v1  [cs.CV]  13 Sep 2023\navatars will be publicly available for research purposes at\nyfeng95.github.io/teca.\n1. Introduction\nThere are two traditional approaches to creating facial\navatars for games and social media. The first method allows\nusers to select attributes such as skin color, hairstyle, and ac-\ncessories manually through a graphical interface. While one\ncan create nearly photo-realistic avatars with tools such as\nMetaHuman [14], the manual process is cumbersome and it\nis difficult to make an avatar resemble a specific individual\u2019s\nappearance. The second method involves estimating facial\nshape and appearance from an image [10, 15, 17, 36, 59, 71]\nor a video [19, 22, 69, 70].\nThese methods, however,\ndo not support editing and customization of the captured\navatar.\nRecently, a third approach has emerged that ex-\nploits text descriptions, generative models, and neural radi-\nance fields [9, 35, 42, 48]. These methods promise the easy\ncreation of diverse and realistic avatars but suffer in terms\nof 3D realism and editability. To address these challenges,\nsome methods [4, 67] incorporate additional priors from ex-\nisting face or body models [33, 39] for face generation and\nanimation. However, their ability to synthesize complex ge-\nometries like those of hair and scarves is constrained by the\nfixed topology of typical 3D mesh models.\nGoing beyond prior work, our goal is to make text a\nviable interface for creating and editing realistic 3D face\navatars with accessories and hair. Our approach is guided\nby two key observations: 1) Different components of the\navatar, such as the hair and face, have unique geometric\nand appearance properties that benefit from distinct repre-\nsentations. 2) Statistical shape models of head and body\nshapes can provide valuable guidance to generative im-\nage models. To exploit the first observation, we adopt a\ncompositional approach to avatar generation, leveraging the\nstrengths of neural and mesh-based 3D content creation\nmethods. Specifically, we model an avatar as a combina-\ntion of two primary components: the face/body and non-\nface/body regions. To exploit the second observation, we\nuse the SMPL-X body model [47] to represent the shape of\nthe head and shoulders. By leveraging a model-based repre-\nsentation of face/body shape, we remove the need to model\nshapes. Instead, we focus such models on creating realis-\ntic face texture. Exploiting 3D shapes enables us to gener-\nate realistic faces by inpainting textures with existing, pre-\ntrained, diffusion models. Moreover, the integration of the\nshape model enables flexible body shape modifications by\nmanipulating the parametric shape representation, facilitat-\ning the transfer of hairstyles and other accessories between\navatars with different proportions. For the non-face com-\nponents like hair, clothing, and accessories, we model their\nshape and appearance with NeRF [44] since it can represent\ndiverse geometry and reflectance.\nStarting with a textual description, our avatar generation\nprocess involves multiple steps (see Fig. 2 for an illustra-\ntion). First, we use stable diffusion model [53] to gener-\nate an image of a face conditioned on the text description.\nSecond, we optimize the shape parameters of the SMPL-\nX body model [47] to obtain a shape representation of the\nperson. Third, inspired by TEXTure [52], we leverage the\nestimated 3D face shape to rotate the head and use Stable\nDiffusion to generate missing texture. Fourth, and most im-\nportant, we employ a sequential, compositional approach\nto generate additional style components. Conditioned on\nthe face mesh, we learn a NeRF-based component model\nfrom the text description. To enable the transfer of non-face\ncomponents between avatars, we define a canonical space\nin which we use a template face shape and train the NeRF\ncomponent on top of it. Using the hybrid volume render-\ning technique from SCARF [18] and DELTA [19], includ-\ning shape skinning, we render our hybrid avatar model into\nthe observation image space. We then apply Score Distilla-\ntion Sampling (SDS) [48] to optimize the NeRF model. The\nSDS loss provides gradients for updating NeRF to guide 2D\nrenderings to match the text input. While NeRF is a highly\nflexible method, we argue that it is not needed to represent\nthe face/body. Instead, narrow its focus to modeling specific\ncomponents of the avatar that are not well represented by\nparametric face/body models, such as hair or accessories.\nTo that end, we use segmentation to steer the generation.\nFor example, in the case of hair, we compute a hair segmen-\ntation mask and use it to focus NeRF on representing the\nhair region. The segmentation mask is obtained by running\nCLIPSeg [41] on the current rendering and is updated itera-\ntively throughout the generation process. To enhance learn-\ning efficiency, we adopt the Latent-NeRF [42] approach and\ntrain the NeRF model in latent space. Finally, we refine\nthe non-face regions using a combination of a BLIP-based\nloss [31] and SDS in image space. This improves the visual\nquality of the non-face components.\nFigure 1 shows several avatars generated by our method.\nIt shows the underlying body mesh, the generated texture\nfrom different views, the generation of NeRF-based hair,\nhats, and clothing, as well as the transfer of components\nfrom one avatar to another. Our approach for avatar gen-\neration surpasses existing methods in terms of realism, as\ndemonstrated by extensive qualitative analysis.\nTECA\u2019\ncompositional framework has two key advantages. First, it\nuses the \u201cright\u201d models for the task: meshes for the face and\nbody and NeRF for hair and clothing. This disentangling of\nthe face and non-face parts results in avatars of higher re-\nalism than the prior art. Second, the compositional nature\nsupports editing of the individual components and enables\nthe seamless transfer of features such as hairstyle or cloth-\ning between avatars. These advancements open new possi-\nbilities for diverse applications, including virtual try-on.\n2. Related work\n3D Avatar Creation From X.\nCreating realistic avatars is\na long-standing challenge, with many solutions for building\ndigital avatars from scans, videos, and images. Sophisti-\ncated capture systems are used to acquire high-quality 3D\nscans, which are turned into realistic, personalized, pho-\ntorealistic avatars [1, 2, 6, 23, 38, 57].\nHowever, these\nmethods are not scalable due to the high cost of building\nsuch systems and the sophisticated pipelines required for\navatar creation. Consequently, there is great interest in cre-\nating avatars from easily captured images [15\u201317, 59, 71]\nand monocular videos [8, 19, 20, 22, 69]. Such methods\nestimate 3D faces with the assistance of parametric mod-\nels of the head [5, 21, 34] or full body [34, 47].\nRe-\ncent methods also learn generative 3D head models using\nonly images [3, 10], allowing the creation of novel avatars\nfrom random noise inputs. Additionally, there exist meth-\nods [18, 19, 32, 51, 54] that adopt a compositional approach\nto avatar modeling, enabling manipulation and control. Un-\nlike these approaches, TECA, requires only natural lan-\nguage descriptions to control the shape, texture, and acces-\nsories of a virtual avatar, making the creation and editing of\nrealistic personal avatars accessible to a broad audience.\nText-Guided 3D General Object Generation.\nFollow-\ning the success of recent 2D text-to-image models [46, 50,\n53], the generation of 3D content from text is gaining atten-\ntion [27, 35, 42, 48]. Since paired text and 3D training data\nis scarce, recent text-to-3D methods generate 3D content\nby leveraging large, pretrained 2D text-to-image models.\nThese approaches learn a 3D representation using losses on\nthe projected image in multiple 2D views, where pretrained\nmodels serve as frozen critics. As an example, Contrastive\nLanguage\u2013Image Pretraining (CLIP) [49] is used to gener-\nate 3D objects in the form of an occupancy network [55], a\nmesh [13, 43, 45], and a NeRF [27, 61]. Similarly, Dream-\nFusion [48] and subsequent work [12, 26, 35, 42, 58, 64]\nadopts a text-to-image diffusion model as a guide to op-\ntimize 3D object generation, significantly improving visual\nquality. Despite the rapid progress in generating general ob-\njects, these methods suffer from visual artifacts when gener-\nating avatars (such as creating a person with multiple faces).\nText-Guided 3D Avatar Generation.\nLarge, pretrained\n2D text-to-image models are also used for avatar creation.\nAvatarCLIP [25] uses the CLIP model to generate coarse\nbody shapes, which are parameterized by the Skinned\nMulti-Person Linear (SMPL) model [40]. DreamAvatar [9]\ngenerates a 3D human avatar from a given text prompt and\nSMPL body shape, where detailed shape and texture are\nlearned under the guidance of a text-conditioned diffusion\nmodel. These methods focus on full-body 3D avatar gener-\nation, resulting in low-resolution faces with limited expres-\nsion. T2P [68] uses CLIP supervision to optimize discrete\nattributes in a video game character creation engine. As\ntheir method is confined to what can be represented by the\ngame engine, their avatars are limited by the expressiveness\nof the artist-designed hairstyles and facial features. Clip-\nFace [4] enables text-guided editing of a textured 3D mor-\nphable face model [34], including expression and texture.\nDescribe3D [66] synthesizes a 3D face mesh with texture\nfrom a text description. Neither approach explicitly mod-\nels hair, and Describe3D resorts to manual post-processing\nto add hair. To address this problem and boost the visual\nquality, DreamFace [67] employs a dataset of textures and\nartist-designed hairstyles, utilizing a CLIP model for com-\nponent selection and a diffusion model for texture genera-\ntion. While DreamFace achieves realistic head avatars, it\nis limited to selecting pre-existing hairstyles from a manu-\nally curated gallery of artist-designed assets. Such an ap-\nproach is expensive and does not scale. Rodin [63] is a\ngenerative 3D face model trained from 100K synthetic 3D\navatars. They exploit CLIP\u2019s text and image embedding to\nenable text-guided generation and editing. However, their\nresults inherit the limited realism of the synthetic training\ndata, and the approach does not disentangle the hair from\nthe rest of the face. Thus, it is difficult to change hairstyles\nwithout unwanted changes in facial appearance. In contrast,\nour method generates realistic facial avatars using diffusion\nmodels without relying on artist-designed hairstyles. More-\nover, its compositional nature guarantees that edits to the\nhairstyle do not impact the face region.\n3. Method\nTECA generates 3D facial avatars with realistic hair and\nstyle components from text descriptions. The overview of\nthe pipeline is illustrated in Fig. 2. Given a text description\nof a person\u2019s physical appearance, we first generate a corre-\nsponding image using Stable Diffusion [53]. We extract the\n3D geometry by fitting a SMPL-X model to this image. To\ngenerate the face texture, we follow the iterative inpaint-\ning approach of TEXTure [52], which generates images\nfrom different viewpoints and projects them onto the sur-\nface. The texture is generated using diffusion [53], taking\nthe text, already-generated texture, and shape into account.\nConditioned on the generated face, we generate other com-\nponents such as hairstyles or clothing using NeRF, which\nwe optimize using SDS constrained with a semantic mask\ngenerated with CLIPSeg. The final refinement is done using\na combination of SDS and BLIP-based losses.\n3.1. Preliminaries\nParametric Body Model.\nTo model a realistic human\navatar including the face and shoulders, we use the SMPL-\nX model [47]. SMPL-X is a parametric mesh model with\nidentity \u03b2 \u2208 R|\u03b2|, pose \u03b8 \u2208 R3nk+3, and expression\n\u03c8 \u2208 R|\u03c8| parameters that control the body and facial shape\n\u201cA chubby woman with a finger waves hairstyle\u201d\n\ud835\udc3f\ud835\udc3f1\nOptimization \n(a) Face generation \nSMPL-X\nStable \nDiffusion\n\ud835\udc3f\ud835\udc3f\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\n\ud835\udc3f\ud835\udc3f\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\ud835\udc5a\n+\ud835\udc3f\ud835\udc3f\ud835\udc5a\ud835\udc5a\ud835\udc60\ud835\udc60\ud835\udc5a\ud835\udc5a\n(b) Hair, clothing, or accessory generation \nInpainting\nShape deformation\nHybrid rendering\nStable \nDiffusion\nBLIP\nCLIPSeg\nStable \nDiffusion\nface image\noptimized mesh\ntextured mesh\navatar in canonical space\nrendered image & NeRF mask\navatar in observation space\nSMPL-X\nRefine\nrefined result\nFace generation (Sec. 3.2)\nHair generation (Sec. 3.3)\nFigure 2. Overview of TECA. TECA follows a sequential pipeline to generate realistic avatars. First, the text input is passed to Stable\nDiffusion to generate a single face image, which serves as a reference to obtain the geometry by SMPL-X fitting. We then adopt a texture\npainting approach inspired by [52], where the mesh is iteratively painted with a texture corresponding to the text using Stable Diffusion.\nSubsequently, style components such as the hair are modeled using NeRF in a latent space, with optimization guided by an SDS loss (LSDS)\nand a mask loss (Lmask) with CLIPSeg segmentation, and finally refined in pixel space using LSDS, Lmask, and an additional BLIP-based\nloss (Lsim). This hybrid modeling approach results in high-quality and realistic avatars.\nof the avatar. SMPL-X provides a consistent, predefined\ntopology with a set of vertices V \u2208 Rnv\u00d73, which are com-\nputed by:\nV = Fsmplx(\u03b2, \u03b8, \u03c8)\n= Flbs(TP (\u03b2, \u03b8, \u03c8), J(\u03b2), \u03b8; W )\n(1)\nwhere Flbs is a linear blend skinning function and W \u2208\nRnk\u00d7nv are the blend weights. TP represents the template\nmesh in a neutral, canonical pose:\nTP (\u03b2, \u03b8, \u03c8) = T + B(\u03b2, \u03b8, \u03c8),\n(2)\nwhere B(\u03b2, \u03b8, \u03c8) : R|\u03b2| \u00d7 R3nk+3 \u00d7 R|\u03c8| \u2192 Rnv\u00d73 gives\ndeformations of the template based on the shape, pose, and\nexpression parameters. J(\u03b2) : R|\u03b2| \u2192 Rnk\u00d73 is a joint re-\ngressor that takes in the identity shape parameters and pro-\nduces the positions of the body joints. SMPL-X builds a\nconnection between the mean body shape T and a specific\nbody shape V with identity, pose, and expression informa-\ntion. This can be formulated as a vertex-wise mapping from\nT to V . Specifically, given ti and vi (the i-th row from T\nand V , respectively), the mapping is:\n\u0014\nvT\ni\n1\n\u0015\n= Mi(\u03b2, \u03b8, \u03c8)\n\u0014\ntT\ni\n1\n\u0015\n.\n(3)\nHere, the function Mi(\u00b7) produces a 4 \u00d7 4 matrix,\nMi(\u03b2, \u03b8, \u03c8) =\n nk\nX\nk=1\nwk,iGk(\u03b8, J(\u03b2))\n! \u0014\nE Bi(\u03b2, \u03b8, \u03c8)T\n0\n1\n\u0015\n,\n(4)\nwhere wk,i is an entry from the blend skinning weights W\nand Gk(\u03b8, J(\u03b2)) \u2208 R4\u00d74 computes the world transforma-\ntion for the k-th body joint. E \u2208 R3\u00d73 is the identity ma-\ntrix, and Bi computes the i-th row of the blend shapes.\nNeRF Representation.\nNeRF [44] encodes a 3D object\nas a continuous volumetric radiance field of color c \u2208 R|c|\nand density \u03c3 \u2208 R. A NeRF is represented by a neural net-\nwork (\u03c3, c) = Fnerf(x, p; \u03a6), where x \u2208 R3 is the location,\np \u2208 R2 is the viewing direction, and \u03a6 are the learnable\nparameters of Fnerf. Given a camera position, we estimate\na 2D image from the NeRF with volume rendering. We de-\nnote a per-pixel ray R(\u2113) = o + \u2113d by the origin o \u2208 R3,\ndirection d \u2208 R3, and \u2113 \u2208 [\u2113n, \u2113f]. To discretize the ren-\ndering, we evenly split the rendering range into n\u2113 bins and\nrandomly sample a \u2113i for every bin with stratified sampling.\nThe volume rendering formulation for each pixel is:\nC(R(\u2113)) =\nn\u2113\nX\ni=1\n\u03b1ici,\nwith \u03b1i = exp\n \n\u2212\ni\u22121\nX\nj=1\n\u03c3j\u2206\u2113j\n!\n(1 \u2212 exp (\u2212\u03c3i\u2206\u2113i)) .\n(5)\nHere, \u2206\u2113i = \u2113i+1 \u2212 \u2113i is the adjacent samples distance.\nScore Distillation Sampling.\nDreamFusion [48] pro-\nposes Score Distillation Sampling (SDS) to guide 3D con-\ntent generation using pre-trained 2D text-to-image diffusion\nmodels. Following [48], we denote the learned denoising\nfunction in the diffusion model as \u03f5(Qt, y, t). Here, Qt\nis the noisy image at timestep t. SDS adopts the denoising\nfunction as a critic to update the 2D rendering Q of the gen-\nerated 3D object across different viewpoints. The gradient\nis computed as:\n\u2207QLsds(Q) = Et,\u03f5 [ut \u00b7 (\u03f5 (Qt, y, t) \u2212 \u03f5)] ,\n(6)\nwhere ut is a weight at timestep t [24].\n3.2. 3D Face Generation\nTo generate a 3D facial shape from a text description, we\nuse a pre-trained Stable Diffusion model [53] to synthesize\na 2D face image that semantically matches the given text.\nThe descriptor keywords might include overweight, slim,\nmuscular, or old. Given the generated face image, we use an\noff-the-shelf landmark detector [7] to obtain a set of facial\nlandmarks {ei|ei \u2208 R3}ne\ni=1, where ne is the number of\nlandmarks. We optimize the SMPL-X parameters using:\n(\u03b2\u2217, \u03b8\u2217, \u03c8\u2217) = argmin\n\u03b2,\u03b8,\u03c8\nne\nX\ni=1\n\r\rM\u03ba(i)(\u03b2, \u03b8, \u03c8)t\u03ba(i) \u2212 ei\n\r\r\n1\n(7)\n\u03ba(i) denotes the index of a vertex of the SMPL-X model\nthat corresponds to the i-th landmark.\nNote that op-\ntimizing facial shape in this way results in full-body\nshape parameters, but the pose parameters are not well-\nconstrained. Then, the final avatar shape is given by V \u2217 =\nFsmplx(\u03b2\u2217, \u03b8c, 0), where \u03b8c represents the body pose pa-\nrameters corresponding to an \u201cA-pose\u201d.\nTo generate a complete texture of the reconstructed\nface, we follow the iterative inpainting procedure of TEX-\nTure [52]. In each step, we generate an image Ii in the\nviewpoint of pi using Stable Diffusion [53] and project Ii\nback to the mesh surface according to the geometry V \u2217 and\npi. The iterative inpainting can be denoted {Ai}np\ni=0, where\nA0, {Ai}np\u22121\ni=1 , and Anp are the initial, intermediate, and\nfinal texture UV maps, respectively. Denoting the differen-\ntiable mesh renderer as Rm(A, p, V \u2217), the painting process\ncan be summarized as:\nAi = argminA \u2225Rm(A, pi, V \u2217) \u2212 Ii\u22251 .\n(8)\nTo reduce cross-view conflicts in texture, we follow [52]\nto take both the mesh geometry V \u2217 and previous texture\nAi\u22121 information into account when generating the image\nIi. This is achieved by iteratively applying depth-aware and\ninpainting diffusion models in the denoising process. We\nalso make the assumption that the face of the individual is\napproximately bilaterally symmetric and add an additional\nsymmetry regularization term Lsym.\nThis term enforces\nsimilarity between the frontal face image and its horizon-\ntally flipped counterpart. Further information on this regu-\nlarization and our texture generation process can be found\nin the Sup. Mat.\n3.3. Hair, Clothing, and Accessory Generation\nCanonicalization.\nBuilding upon the generated face, rep-\nresented as a textured mesh, we learn a separate NeRF\nmodel for attributes like hair, clothing, or accessories. The\nNeRF model is built in a canonical space, which is con-\nstructed around the SMPL-X template mesh T , enabling\nthe animation and transfer of the non-face parts. For body\nmesh V and its corresponding parameters \u03b2, \u03b8, and \u03c8, we\nfollow previous work [9, 11, 18] to map the points from ob-\nservation space to canonical space (x \u2192 xc):\nxc =\nX\nvi\u2208N (x)\n\u03c9i(x, vi)\n\u03c9(x, vi) Mi(0, \u03b8c, 0) (Mi(\u03b2, \u03b8, \u03c8))\u22121 x, (9)\nwhere N(x) is the set of nearest neighbors of x in V . The\nweights are computed as:\n\u03c9i(x, vi) = exp\n \n\u2212\n\u2225x \u2212 vi\u22252\n\r\rw\u03be(x) \u2212 wi\n\r\r\n2\n2\u03c4 2\n!\n, and\n\u03c9(x, vi) =\nX\nvi\u2208N (x)\n\u03c9i(x),\n(10)\nwhere \u03be(x) is the index of the vertex in V that is closest to\nx. wi is the i-th column of W , and \u03c4 is 0.2.\nMesh-Integrated Volumetric Rendering.\nConditioned\non the textured face mesh, we learn NeRF models with\nmesh-integrated volume rendering following [18]. Specifi-\ncally, when a ray R(\u2113) is emitted from the camera center o\nand intersects the mesh surface, we set \u2113f such that R(\u2113f)\nrepresents the first intersection point. The texture color at\nR(\u2113f), denoted by c\u2217, is then used in the volume rendering\nby extending Eqn. 5:\nC(R(\u2113)) =\n \n1 \u2212\nn\u2113\u22121\nX\ni=1\n\u03b1i\n!\nc\u2217 +\nn\u2113\u22121\nX\ni=1\n\u03b1ici,\nwith \u03b1i = exp\n \n\u2212\ni\u22121\nX\nj=1\n\u03c3j\u2206\u2113j\n!\n(1 \u2212 exp(\u2212\u03c3i\u2206\u2113i)) .\n(11)\nFor a ray that does not intersect the mesh surface, the com-\nputation of aggregated color follows Eqn. 5. Unlike in a\ntraditional NeRF representation, we model NeRF in a latent\nspace to accelerate the learning process. Specifically, c and\nc\u2217 represent 4-dimensional latent features. While the fea-\ntures ci are optimized, the latent feature on the mesh surface\nc\u2217 is obtained by running the stable diffusion encoder [53]\non the RGB rendering of the textured mesh. After volumet-\nric integration, the resulting feature image is decoded with\nthe Stable Diffusion model.\nTo train a NeRF model for a specific style component,\nwe rely on CLIPSeg [41] for spatial guidance. Taking hair\nas an example, we use CLIPSeg with a keyword of \u201chair\u201d to\nsegment the hair region in the image generated by render-\ning the hybrid mesh\u2013NeRF model. This hair segmentation\nmask \u2126 indicates the hair and non-hair regions. We use \u2126\nto guide the NeRF model to focus on the representation of\nobjects within the masked region while discouraging it from\nlearning geometry outside the region. This is useful to pre-\nvent the NeRF from modeling the face, when it should only\nrepresent hair. We use the following mask-based loss:\nLmask =\n\r\r\r\u2126 \u2212 \u02c6\u2126\n\r\r\r\n1 ,\n(12)\nwhere \u02c6\u2126 is the rendered NeRF mask, obtained by sampling\nrays for all pixels of the entire image. The computation of a\nmask value at pixel location R(\u2113) is given by:\n\u2126i (R(\u2113), n\u2113 \u2212 1) =\nn\u2113\u22121\nX\ni=1\n\u03b1i.\n(13)\nTEXTure\nLatent-Paint\nLatent-NeRF\nSJC\nOurs\n(a)\n(b)\n(c)\n(d)\nOurs\nOurs\nFigure 3. Qualitative comparison with SOTA methods. Text prompts: (a) \u201cAn overweight man with a crown-braid hairstyle,\u201d (b) \u201cA man\nin a purple hoodie,\u201d (c) \u201cAn old bald African woman wearing a wool scarf,\u201d (d) \u201cA young woman in a military hat.\u201d\nTo prevent floating \u201cradiance clouds,\u201d we incorporate the\nsparsity loss from [42] into the training of the NeRF:\nLsparse =\nX\ni\nFBE (\u2126i (R(\u2113), n\u2113)) ,\n(14)\nwhere FBE(a) = \u2212a ln a \u2212 (1 \u2212 a) ln(1 \u2212 a) is a binary\nentropy function. The total training loss function for the\nlatent NeRF models is:\nLNeRF = Lsds + \u03bbmaskLmask + \u03bbsparseLsparse.\n(15)\nStyle Component Refinement in RGB Space.\nWhile la-\ntent NeRF models can be used to learn reasonable geome-\ntry and texture, we have observed that adding further refine-\nment in RGB space using a combination of the SDS loss and\na loss based on BLIP [31] improves local detail. Our BLIP\nloss, Lsim, measures the similarity of high-level visual and\ntext features. Maximizing their similarity encourages the\nNeRF model to capture additional details, including struc-\nture, texture, and semantic content from the text description,\nleading to visually appealing results, see Fig. 7.\nTo perform the refinement, we append an additional lin-\near layer to the NeRF model that converts the 4D latent fea-\nture into a 3D color representation [60]. The initial weights\nof this layer are computed using pairs of RGB images and\ntheir corresponding latent codes over a collection of natural\nimages. Let zimg and ztext be the embeddings of the ren-\ndered image and text prompt, then the similarity loss is:\nLsim = \u2212\nzT\nimgztext\n\u2225zimg\u2225 \u00b7 \u2225ztext\u2225,\n(16)\nand the learning objective in refinement stage is:\nLrefine = LNeRF + \u03bbsimLsim.\n(17)\nMore implementation details are included in Sup. Mat.\n4. Experiments\nWe evaluate TECA through 1) comparisons with state-of-\nthe-art (SOTA) methods for text-guided generation, 2) an\nonline perceptual study, 3) quantitative evaluation, 4) the\napplication of try-on and animation of generated avatars,\nand 5) ablation studies exploring our design choices.\nTo\nevaluate TECA\u2019s ability to generate diverse compositional\navatars, we need text prompts that are also compositional.\nThese text prompts may include facial attributes (e.g. over-\nweight, slim, muscular), hairstyles (e.g. bun, afro, braid),\nclothing (e.g. jacket, wool scarf, hat), and more. The can-\ndidate attributes and styles words are taken from a dataset\nof faces [37], a dataset of hairstyles [65], and other online\nsources12. In total, these attributes produce up to 3,300 text-\nprompt combinations, ensuring rich diversity.\n1vocabulary-clothing-clothes-accessories\n2types-of-hats\n4.1. Comparisons with SOTA Methods\nWe compare TECA with four SOTA methods.\nTwo are\nsolely based on NeRF representations (SJC [62] and Latent-\nNeRF [42]) and two are based on mesh painting techniques\n(Latent-Paint [42] and TEXTure [52]). Figure 3 shows gen-\nerated examples obtained from four diverse text prompts\ndescribing various personal characteristics, hairstyles, and\nclothing. Notably, all methods successfully generate avatars\nwith recognizable features that semantically align with the\ntext, such as gender, color, and clothing items. However,\nSJC and Latent-NeRF produce visually distorted and in-\ncomplete avatars, primarily due to flawed geometry and\nlow-resolution textures.\nWhile Latent-Paint incorporates\na mesh as a shape prior, leading to reasonable propor-\ntions, the textures still suffer from blurriness and a lack of\ncross-view consistency. TEXTure demonstrates good tex-\nture quality but is limited by the mesh topology; it cannot\nnon-body components like the crown-braid hairstyle and\nhoodie. In contrast, TECA generates more realistic and nat-\nural avatars with strong cross-view consistency. Our text-\ngenerated avatars exhibit detailed appearances, including\ndiverse hairstyles and accessories (see Sup. Mat.).\n4.2. Perceptual Study\nWe conducted an online perceptual study to rate avatars syn-\nthesized by TECA and the baseline methods, SJC, Latent-\nNeRF, LatentPaint, and TEXTure. Participants on Amazon\nMechanical Turk were presented with videos of the gener-\nated avatars and asked to rate the visual realism and con-\nsistency with the text prompt using a seven-point Likert\nscale. We showed each participant the same thirty prompts\nbut shuffled which method\u2019s avatar the participants saw for\neach prompt. Each participant was shown an equal num-\nber of avatars synthesized by each method. A total of 150\nresponses were collected, out of which 52 (35%) partici-\npants passed all of the catch trials and were included in the\nstudy. We applied the non-parametric Friedman test and\nthen performed the Nemenyi post-hoc test to identify pair-\nwise differences. The results are shown in Fig. 4 and only\nour method receives, on average, positive ratings to both\nquestions. See Sup. Mat. for more details.\n4.3. Quantitative Evaluation\nWe also quantitatively compare our method with other state-\nof-the-art methods.\nSpecifically, we assess the seman-\ntic matching between text and avatar using the CLIP [49]\nscore and evaluate the generated avatar quality through the\nFr\u00b4echet Inception Distance (FID) [56].\nTo compute the\nCLIP score, we convert the videos used in Section 4.2\ninto images and employed CLIP [49] to compute the co-\nsine distance between these images and their respective text\nprompts. The results are shown in Table 1. Our method\nachieves the highest semantic consistency, indicating its\nLatentPaint\nTEXTure\nSJC\nLatentNeRF\nTECA\n1. Disagree\n2. Mostly\nDisagree\n3. Somewhat\nDisagree\n4. Neither Agree\nnor Disagree\n5. Somewhat\nAgree\n6. Mostly\nAgree\n7. Agree\nQuestion\nConsistent\nRealistic\nFigure 4. A box-and-whisker plot of the perceptual study results.\nUsers were asked the questions 1) \u201cThe appearance of the avatar in\nthe video matches the text description below it.\u201d (blue color) and\n2) \u201cThe avatar in the video is visually realistic\u201d (orange color), \u2018+\u2019\ncorresponds to the mean.\nSource avatar\nTransfer\nTarget avatar\nFigure 5. TECA Try-On. The hairstyle, hat, scarf, or clothing is\ntransferred from the source avatars to the target avatar.\nability to accurately represent text descriptions. For FID,\nwe generated 200 avatars using 40 different text prompts,\nwith 5 random seeds for each prompt. Each avatar was ren-\ndered from 50 different views, resulting in a total of 10,000\nimages for evaluation. The ground truth distribution was\nbased on the first 10,000 images from the Flickr-Faces-HQ\nDataset (FFHQ) [28]. The FID scores are shown in Table\n1. TECA has the lowest FID score, indicating its superior\nimage quality relative to the other methods.\nMethod\nCLIP Score \u2191\nFID\u2193\nSJC [62]\n0.2979\n27.50\nLatent-NeRF [42]\n0.3025\n24.49\nLatent-Paint [42]\n0.2854\n38.33\nTEXTure [52]\n0.2761\n29.51\nOurs\n0.3213\n14.98\nTable 1. Quantitative evaluation results. Higher CLIP score indi-\ncates better consistency between the text prompts and generated\navatars, lower FID indicates higher realism of the avatars.\n4.4. Applications: Try-on and Animation\nSince TECA is compositional, components like hairstyles\nand accessories can be transferred between avatars.\nAs\nshown in Fig. 5, TECA can transfer a brown pullover,\ncrown-braid hairstyle, red scarf, or blue hat to the target\nFigure 6. TECA Animation. The generated avatar is animated to\ndifferent poses and expressions.\nw/o refinement\nw/ refinement\nMarcel waves hairstyle\nBrown suit\nKufi hat\nWool scarf\nFigure 7. Comparison of results between unrefined (top) and re-\nfined (bottom) style components. The refinement improves the\ndetails of hair, hat, scarf, and clothing. The refined components\nare indicated by red dotted line boxes.\navatar. The non-face components adjust in size to adapt to\na new head shape. This makes our generated hairstyles and\naccessories highly versatile. For instance, users can input\ntheir own avatars and transfer a learned hairstyle to it.\nLeveraging the SMPL-X model, we also gain the flexi-\nbility to animate the avatar across various poses and expres-\nsions. As detailed in Sec. 3.3, the NeRF component has\nthe capacity to synchronize its movement with the dynam-\nics of the SMPL-X body. Displayed in Fig. 6, our generated\navatar can be animated with varying head poses and expres-\nsions. Notice the transition from a neutral to an open-mouth\nexpression (middle image), where the interior mouth region\nis absent. This limitation could be addressed by inpaint-\ning the inside mouth region using diffusion models. Further\ninstances showcasing this including face editing results are\navailable in the Sup. Mat.\n4.5. Ablation Experiments\nNon-Face Refinement.\nWe investigate the effect of refin-\ning non-face details using a combination of SDS and BLIP\nlosses. Figure 7 illustrates the difference between refined\nand non-refined style components. The results demonstrate\nthat the refinement produces more detail, noticeably en-\nhancing the overall visual quality.\nCLIPSeg Segmentation.\nThe segmentation loss prevents\nNeRF from trying to represent the entire avatar and focuses\nit on representing a specific part. Without the loss, the re-\nsults are significantly worse; see Fig. 8.\nWithout CLIPSeg\nWith CLIPSeg\nFigure 8. Ablation of CLIPSeg. Without the segmentation infor-\nmation, NeRF learns the entire avatar instead of the components.\nFigure 9. Failure cases showing the impact of poor segmentation.\n5. Discussion and Limitations\nSegmentation with CLIPSeg.\nTo guide the learning pro-\ncess, we use CLIPSeg to obtain masks for the region of in-\nterest, such as hair or clothing. This encourages the NeRF to\nfocus on learning specific components rather than on the en-\ntire avatar. Our method\u2019s effectiveness is contingent on the\nsegmentation quality. If CLIPSeg encounters challenges,\nflawed NeRF representations may result, such as floating\npoints in the face region, as shown in Fig. 9.\nPerformance of Diffusion Models.\nOur results are con-\nstrained by the capabilities and biases of pretrained diffu-\nsion models because they provide the semantic information\nfor avatar generation.\nDynamics.\nTECA\u2019s ability to animate avatars via the\nSMPL-X parametric model highlights its potential, yet ad-\ndressing complex dynamics in elements like hair and cloth-\ning calls for further exploration.\nRelighting.\nOur model does not support relighting in new\nenvironments, as the learned RGB color for the face texture\nand NeRF-based hair or accessories is baked with lighting.\nFurther work is needed to disentangle albedo and lighting\nattributes to enable relighting.\n6. Conclusion\nWe presented TECA, an innovative method for generating\nrealistic 3D facial avatars with hair and accessories from\ntext descriptions. By adopting a compositional model and\nusing distinct representations for different components, we\naddressed the limitations of existing methods in terms of\nrealism, shape fidelity, and capabilities for editing. Our ex-\nperimental results demonstrate the superior performance of\nTECA compared to state of the art, delivering highly de-\ntailed and editable avatars. Further, we demonstrated the\ntransfer of hairstyles and accessories between avatars.\nDisclosure\nThis work was partially supported by the\nMax Planck ETH Center for Learning Systems. MJB has\nreceived research gift funds from Adobe, Intel, Nvidia,\nMeta/Facebook, and Amazon.\nMJB has financial inter-\nests in Amazon, Datagen Technologies, and Meshcapade\nGmbH. While MJB is a consultant for Meshcapade, his re-\nsearch in this project was performed solely at, and funded\nsolely by, the Max Planck Society.\n7. Appendix\n7.1. Implementation Details\nFor the SMPL-X model [47], we use |\u03b2| = 300 and\n|\u03c8| = 100, and we use 68 facial landmarks to fit the SMPL-\nX shape to the reference images, (ne = 68). In the SMPL-X\noptimization process, we incorporate shape and expression\nregularization [47] with a weight of 5e \u2212 5. For texture\ngeneration on the mesh, we use np = 10 viewing direc-\ntions. The mesh texture is optimized in an iterative man-\nner from various view angles, following the sequence illus-\ntrated in Fig. 10. The symmetry loss Lsym is applied dur-\ning this process, leveraging our assumption that the face is\napproximately symmetric. Specifically, for front and back\nviews (No. 1 and No. 10), we apply an L2 loss between the\nrendered image Rm(A, pi, V \u2217) that renders the geometry\nV \u2217 in the view direction pi based on the UV map A and\nthe horizontally flipped version of the Stable Diffusion [53]\ngenerated image I\u2032\ni. For right views (Nos. 3, 5, 7, and 9),\nwe implement the L2 loss by comparing the rendered im-\nages and the corresponding left views (Nos. 2, 4, 6, and\n8).\nDuring NeRF training, we sample 96 points each ray\n(n\u2113 = 96). During the refinement stage, we increase the\nnumber of sampled points n\u2113 to 128. In the optimization,\nwe employ a latent code with dimensions of 64 \u00d7 64 \u00d7 4.\nDuring refinement, we render the image at a resolution of\n480 \u00d7 480 \u00d7 3. The network used in the NeRF training pro-\ncess comprises three fully connected layers, and the adapter\nfrom the latent space to the pixel space is a one-layer MLP\nfollowing [60]. Other hyperparameters include \u03c4 = 0.1,\n|N(x)| = 6, \u2113n = \u22121, and \u2113f = 1.\nFor the Stable Diffusion model [53] and its depth-aware\nand inpainting variants, we use the implementations avail-\nable on HuggingFace345.\nFor the BLIP model [31], we\nuse the released model6 from the LAVIS project [30].\nWe employ the Adam optimizer [29] with a learning rate\nof 0.01 for texture optimization, while for other optimiza-\ntions, we use a learning rate of 0.001. For loss weights,\nwe fix \u03bbsym = 0.5, \u03bbmask = 0.1, \u03bbsparse = 0.0005, and\n\u03bbsim = 1. The average run time for avatar generation is\n3stabilityai/stable-diffusion-2\n4stabilityai/stable-diffusion-2-depth\n5stabilityai/stable-diffusion-2-inpainting\n6BLIP/models/model base.pth\n1\n2\n3\n4\n5\n6\n9\n7\n10\n8\n1\n2\n3\n\u2026\n10\nFigure 10. Left: The ten virtual camera views used for the texture\noptimization process. Right: The rendered images (1st column)\nand the corresponding UV maps (2nd column).\ncurrently around three hours on an A100 GPU.\n7.2. Perceptual Study Details\nIn our survey, we used a random selection process to create\na set of thirty unique prompt combinations, each consist-\ning of five elements. These elements were: 1) gender (male\nor female); 2) a color7; 3) a hairstyle8 or hat9, weighted\nequally; 4) another color; and 5) a type of upper-body cloth-\ning10. These combinations were used to construct a prompt\nin the form of \u201cA [1] with a(n) [2] [3] wearing a(n) [4] [5]\u201d.\nTo mitigate potential interaction effects resulting from\nparticipants\u2019 unfamiliarity with the styles presented, we\nincluded an image from the internet that represented the\nhairstyle or type of hat described in the prompt; the image\nwas displayed next to the avatar video. Participants were\nthen asked to rate their agreement with two statements: 1)\n\u201cThe avatar in the video is visually realistic\u201d and 2) \u201cThe\nappearance of the avatar in the video matches the text de-\nscription below it.\u201d To determine whether a participant suc-\ncessfully passed the catch trials, we examined their ratings\nfor both questions. Participants were considered to have\npassed if they rated both questions with greater-than-neutral\nagreement or greater-than-neutral disagreement on all five\nconstant manually curated high-quality samples and catas-\ntrophic generation failures, respectively.\nA total of 150\nresponses were collected, out of which 52 (35%) partici-\npants passed all of the catch trials and were included in the\nstudy. The response distributions failed the Shapiro\u2013Wilk\nnormality test, so we applied the non-parametric Friedman\ntest, which indicated that the method used to generate the\navatar had a statistically significant effect on the outcomes\nof both study questions. Subsequently, we performed the\nNemenyi post-hoc test to identify pairwise differences. Us-\n7HairCLIP/main/README.md\n8HairCLIP/main/mapper/hairstyle list.txt\n97esl.com/types-of-hats\n107esl.com/vocabulary-clothing-clothes-accessories\n\u201cA man wearing a brown pullover\u201d\n\u201cA man wearing a fez hat\u201d\n\u201cA slim man with a dreadlocks hairstyle\u201d\n\u201cA bald man wearing cashmere scarf\u201d\n\u201cA chubby man with a crown braid hairstyle\u201d\n\u201cA chubby woman with a crown braid hairstyle\u201d\n\u201cA man wearing a pink jacket\u201d\n\u201cA man wearing a patterned scarf\u201d\n\u201cAn African woman with long hair\u201d\nFigure 11. Additional examples for generated avatars by our method.\nFigure 12. Additional examples of accessories such as earrings, necklaces, and glasses.\ning a significance level (\u03b1) of 0.05, the perceived realism\nof TECA was determined to be significantly different than\nthat of all baselines other than TEXTure, and the text consis-\ntency was determined to be significantly different than that\nof all baselines.\nThese findings confirm our initial expec-\ntations regarding the strengths of each method and support\nthe value of our proposed combination of mesh-based and\nNeRF-based representations.\n7.3. More Qualitative Results\nMore generation results.\nAvatars with distinct hairstyles,\nclothing, and accessories are shown in Fig. 11. In addition,\nFig. 15 shows instances with a variety of other types of ac-\ncessories, such as earrings, necklaces, and glasses. To il-\nlustrate the diversity of the generated avatars, Fig. 13 shows\navatars generated using the same prompt but different seeds,\nproducing diverse faces and clothing that consistently align\nwith the text input. Fig. 14 presents avatars that are gen-\nerated using more detailed descriptions, highlighting the\nalignment between the avatars and the input texts.\nMore applications.\nTECA allows for the transfer of\nhairstyles and accessories, as demonstrated by additional\nexamples in Fig. 15. Moreover, the generated avatars can\nbe animated with different expressions. As our method au-\ntomatically estimates the SMPL-X shape parameters of this\nsubject, we can then change the expression parameters of\nSMPL-X model to animate the face. Regarding face tex-\nture, the inner mouth region is missing. To address this, we\napply an inpainting Stable Diffusion model to inpaint the\nmissing area. The results are shown in Fig. 16.\nWe can\nfurther edit the avatar using text input. For example, Fig. 17\nshows the results of altering the color of lips and eyes.\nFigure 13. Additional examples of the diversity of avatars generated with the same text prompt: \u201ca woman with long hair wearing a shirt.\u201d\nA woman with a blue hi-top fade hairstyle \nwearing a green tank top\nA woman with a gray slicked-back hairstyle \nwearing a shirt\nA chubby man wearing a leotard shirt with the floral patterns\nA man wearing a leotard long-sleeve sweater with blue color\nA woman with a yellow crew cut hairstyle \nwearing a red pullover\nA man wearing a Cuban collar tartan shirt\nFigure 14. Additional examples of generated avatars with more detailed text descriptions such as colors and detailed shapes.\nFigure 15. Additional examples for hair and accessory transfer.\nFigure 16. Additional examples avatars generated with different expressions. The in-mouth area is represented with a painted texture. The\nexpressions from top to bottom are: smiling, angry, disgusted, and screaming.\nFigure 17. Additional examples of editing the generated avatar\u2019s texture. The first line changes the lip color with the \u201cred lip\u201d prompt and\nthe second line alters the eye color with prompts \u201cgreen eyes\u201d and \u201cbrown eyes\u201d.\nReferences\n[1] Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan\nChiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul De-\nbevec. The Digital Emily project: Achieving a photorealistic\ndigital actor. IEEE Computer Graphics and Applications, 30\n(4):20\u201331, 2010. 3\n[2] Oleg Alexander, Graham Fyffe, Jay Busch, Xueming Yu,\nRyosuke Ichikari, Andrew Jones, Paul Debevec, Jorge\nJimenez, Etienne Danvoye, Bernardo Antionazzi, Mike\nEheler, Zybnek Kysela, and Javier von der Pahlen. Digital\nIra: Creating a real-time photoreal digital actor.\nIn ACM\nSIGGRAPH 2013 Posters, New York, NY, USA, 2013. As-\nsociation for Computing Machinery. 3\n[3] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit\nOgras, and Linjie Luo. Panohead: Geometry-aware 3d full-\nhead synthesis in 360\u25e6, 2023. 3\n[4] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias\nNie\u00dfner. Clipface: Text-guided editing of textured 3d mor-\nphable models, 2023. 2, 3\n[5] Volker Blanz and Thomas Vetter.\nA morphable model\nfor the synthesis of 3d faces.\nIn Transactions on Graph-\nics, (Proc. SIGGRAPH), page 187\u2013194, USA, 1999. ACM\nPress/Addison-Wesley Publishing Co. 3\n[6] George Borshukov and J. P. Lewis. Realistic human face ren-\ndering for \u201dthe matrix reloaded\u201d. In ACM SIGGRAPH 2005\nCourses, page 13\u2013es, New York, NY, USA, 2005. Associa-\ntion for Computing Machinery. 3\n[7] A. Bulat and G. Tzimiropoulos. How far are we from solv-\ning the 2d & 3d face alignment problem? (and a dataset of\n230,000 3d facial landmarks). In 2017 IEEE International\nConference on Computer Vision (ICCV), pages 1021\u20131030,\nLos Alamitos, CA, USA, 2017. IEEE Computer Society. 5\n[8] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,\nMichael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,\nShih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authen-\ntic volumetric avatars from a phone scan. ACM Transactions\non Graphics (TOG), 41(4):1\u201319, 2022. 3\n[9] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K. Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models, 2023. 2, 3, 5\n[10] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas J. Guibas, Jonathan Tremblay, Sameh Khamis,\nTero Karras, and Gordon Wetzstein.\nEfficient geometry-\naware 3d generative adversarial networks.\nIn Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n16123\u201316133, Los Alamitos, CA, USA, 2022. IEEE Com-\nputer Society. 2, 3\n[11] Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao\nBao, Xu Jia, and Huchuan Lu. Animatable neural radiance\nfields from monocular rgb videos, 2021. 5\n[12] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation, 2023. 3\n[13] Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui\nJia. Tango: Text-driven photorealistic and robust 3d styliza-\ntion via lighting decomposition, 2022. 3\n[14] Epic Games. Metahuman, 2023. 2\n[15] Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi\nZhou. Joint 3d face reconstruction and dense alignment with\nposition map regression network. In ECCV, 2018. 2, 3\n[16] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\nTzionas, and Michael Black. Collaborative regression of ex-\npressive bodies using moderation. In International Confer-\nence on 3D Vision (3DV), pages 792\u2013804, 2021.\n[17] Yao Feng, Haiwen Feng, Michael J. Black, and Timo\nBolkart.\nLearning an animatable detailed 3D face model\nfrom in-the-wild images. Transactions on Graphics, (Proc.\nSIGGRAPH), 40(4):1\u201313, 2021. 2, 3\n[18] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black,\nand Timo Bolkart.\nCapturing and animation of body and\nclothing from monocular video. In SIGGRAPH Asia 2022\nConference Papers, New York, NY, USA, 2022. Association\nfor Computing Machinery. 2, 3, 5\n[19] Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc\nPollefeys, and Michael J. Black.\nLearning disentangled\navatars with hybrid 3d representations. arXiv, 2023. 2, 3\n[20] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong\nGuo, and Juyong Zhang. Reconstructing personalized se-\nmantic facial NeRF models from monocular video. Transac-\ntions on Graphics (TOG), 41(6):1\u201312, 2022. 3\n[21] Thomas Gerig, Andreas Morel-Forster, Clemens Blumer,\nBernhard Egger, Marcel Luthi, Sandro Schoenborn, and\nThomas Vetter. Morphable face models - an open frame-\nwork. In 2018 13th IEEE International Conference on Auto-\nmatic Face & Gesture Recognition (FG 2018), pages 75\u201382,\nLos Alamitos, CA, USA, 2018. IEEE Computer Society. 3\n[22] Philip-William Grassal,\nMalte Prinzler,\nTitus Leistner,\nCarsten Rother, Matthias Nie\u00dfner, and Justus Thies. Neu-\nral head avatars from monocular rgb videos. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n18653\u201318664, Los Alamitos, CA, USA, 2022. IEEE Com-\nputer Society. 2, 3\n[23] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,\nXueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-\nEscolano, Rohit Pandey, Jason Dourgarian, et al. The re-\nlightables: Volumetric performance capture of humans with\nrealistic relighting. Transactions on Graphics (TOG), 38(6):\n1\u201319, 2019. 3\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems (NeurIPS), 33:6840\u20136851, 2020. 4\n[25] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. AvatarCLIP: Zero-shot text-\ndriven generation and animation of 3D avatars. Transactions\non Graphics (TOG), 41(4):1\u201319, 2022. 3\n[26] Yangyi Huang, Yuliang Xiu, Hongwei Yi, Tingting Liao, Ji-\naxiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided\nReconstruction of Lifelike Clothed Humans. arXiv preprint:\n2308.08545, 2023. 3\n[27] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 867\u2013876, Los\nAlamitos, CA, USA, 2022. IEEE Computer Society. 3\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 7\n[29] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 9\n[30] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio\nSavarese, and Steven C.H. Hoi. LAVIS: A one-stop library\nfor language-vision intelligence. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 3: System Demonstrations), pages 31\u201341,\nToronto, Canada, 2023. Association for Computational Lin-\nguistics. 9\n[31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning (ICML), pages\n12888\u201312900, Baltimore, MD, USA, 2022. PMLR. 2, 6, 9\n[32] Junxuan Li, Shunsuke Saito, Tomas Simon, Stephen Lom-\nbardi, Hongdong Li, and Jason Saragih. Megane: Morphable\neyeglass and avatar network, 2023. 3\n[33] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4D scans. ACM Transactions on Graphics,\n(Proc. SIGGRAPH Asia), 36(6):194:1\u2013194:17, 2017. 2\n[34] Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4D scans. Transactions on Graphics (TOG),\n36(6):1\u201317, 2017. 3\n[35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation, 2023. 2, 3\n[36] Weiyang Liu, Zhen Liu, Liam Paull, Adrian Weller, and\nBernhard Sch\u00a8olkopf. Structural causal 3d reconstruction. In\nECCV, 2022. 2\n[37] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face at-\ntributes in the wild. In 2015 IEEE International Conference\non Computer Vision (ICCV), pages 3730\u20133738, Los Alami-\ntos, CA, USA, 2015. IEEE Computer Society. 6\n[38] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser\nSheikh. Deep appearance models for face rendering. Trans-\nactions on Graphics (TOG), 37(4):1\u201313, 2018. 3\n[39] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. ACM Transactions on Graphics, (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 2\n[40] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. Transactions on Graphics (TOG), 34\n(6):1\u201316, 2015. 3\n[41] Timo L\u00a8uddecke and Alexander Ecker. Image segmentation\nusing text and image prompts. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 7086\u20137096,\nLos Alamitos, CA, USA, 2022. IEEE Computer Society. 2,\n5\n[42] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3d shapes and textures, 2022. 2, 3, 6, 7\n[43] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes.\nIn Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 13492\u201313502, Los Alami-\ntos, CA, USA, 2022. IEEE Computer Society. 3\n[44] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Commun. ACM, 65(1):99\u2013106, 2021. 2, 4\n[45] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 Conference Papers, New York, NY, USA, 2022.\nAssociation for Computing Machinery. 3\n[46] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen. GLIDE: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning\n(ICML), pages 16784\u201316804, Virtual, 2022. PMLR. 3\n[47] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Os-\nman, D. Tzionas, and M. J. Black. Expressive body cap-\nture: 3D hands, face, and body from a single image.\nIn\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 10967\u201310977, Los Alami-\ntos, CA, USA, 2019. IEEE Computer Society. 2, 3, 9\n[48] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. 2,\n3, 4\n[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable vi-\nsual models from natural language supervision.\nIn Inter-\nnational Conference on Machine Learning (ICML), pages\n8748\u20138763, Virtual, 2021. PMLR. 3, 7\n[50] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents, 2022. 3\n[51] Siddhant Ranade, Christoph Lassner, Kai Li, Christian\nHaene, Shen-Chi Chen, Jean-Charles Bazin, and Sofien\nBouaziz. Ssdnerf: Semantic soft decomposition of neural\nradiance fields, 2022. 3\n[52] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes, 2023. 2, 3, 4, 5, 7\n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10684\u2013\n10695, Los Alamitos, CA, USA, 2022. IEEE Computer So-\nciety. 2, 3, 4, 5, 9\n[54] Radu Alexandru Rosu, Shunsuke Saito, Ziyan Wang, Chen-\nglei Wu, Sven Behnke, and Giljoo Nam.\nNeural strands:\nLearning hair geometry and appearance from multi-view im-\nages. In Computer Vision \u2013 ECCV 2022, pages 73\u201389, Cham,\n2022. Springer Nature Switzerland. 3\n[55] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 18603\u201318613, Los Alamitos, CA,\nUSA, 2022. IEEE Computer Society. 3\n[56] Maximilian Seitzer.\npytorch-fid: FID Score for PyTorch.\nhttps://github.com/mseitzer/pytorch-fid,\n2020. Version 0.3.0. 7\n[57] Mike Seymour, Chris Evans, and Kim Libreri. Meet mike:\nEpic avatars. In ACM SIGGRAPH 2017 VR Village, New\nYork, NY, USA, 2017. Association for Computing Machin-\nery. 3\n[58] Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng,\nBoyao Zhou, Hongwen Zhang, and Yebin Liu.\nCon-\ntrol4d: Dynamic portrait editing by learning 4d gan from\n2d diffusion-based editor. arXiv preprint arXiv:2305.20082,\n2023. 3\n[59] Ayush Tewari, Michael Zollh\u00a8ofer, Hyeongwoo Kim, Pablo\nGarrido, Florian Bernard, Patrick P\u00b4erez, and Christian\nTheobalt. Mofa: Model-based deep convolutional face au-\ntoencoder for unsupervised monocular reconstruction. In In-\nternational Conference on Computer Vision (ICCV), pages\n3735\u20133744, Los Alamitos, CA, USA, 2017. IEEE Computer\nSociety. 2, 3\n[60] Kevin Turner. Decoding latents to RGB without upscaling,\n2022. 6, 9\n[61] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. CLIP-NeRF: Text-and-image driven manipu-\nlation of neural radiance fields. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 3835\u20133844,\nLos Alamitos, CA, USA, 2022. IEEE Computer Society. 3\n[62] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 12619\u201312629, Los Alamitos, CA, USA, 2023. IEEE\nComputer Society. 7\n[63] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, and Baining Guo.\nRodin: A genera-\ntive model for sculpting 3d digital avatars using diffusion.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4563\u20134573, Los Alamitos, CA, USA, 2023.\nIEEE Computer Society. 3\n[64] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 3\n[65] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhen-\ntao Tan, Lu Yuan, Weiming Zhang, and Nenghai Yu. Hair-\nclip: Design your hair by text and reference image.\nIn\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 18072\u201318081, Los Alamitos, CA, USA,\n2022. IEEE Computer Society. 6\n[66] Menghua Wu, Hao Zhu, Linjia Huang, Yiyu Zhuang,\nYuanxun Lu, and Xun Cao. High-fidelity 3d face generation\nfrom natural language descriptions. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 4521\u2013\n4530, Los Alamitos, CA, USA, 2023. IEEE Computer Soci-\nety. 3\n[67] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,\nCheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and\nJingyi Yu. Dreamface: Progressive generation of animatable\n3d faces under text guidance, 2023. 2, 3\n[68] Rui Zhao, Wei Li, Zhipeng Hu, Lincheng Li, Zhengxia Zou,\nZhenwei Shi, and Changjie Fan. Zero-shot text-to-parameter\ntranslation for game character auto-creation. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n21013\u201321023, Los Alamitos, CA, USA, 2023. IEEE Com-\nputer Society. 3\n[69] Yufeng Zheng, Victoria Fern\u00b4andez Abrevaya, Marcel C.\nB\u00a8uhler, Xu Chen, Michael J. Black, and Otmar Hilliges.\nI m avatar: Implicit morphable head avatars from videos.\nIn Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 13545\u201313555, Los Alamitos, CA, USA,\n2022. IEEE Computer Society. 2, 3\n[70] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Instant\nvolumetric head avatars, 2022. 2\n[71] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Towards\nmetrical reconstruction of human faces. In Computer Vision\n\u2013 ECCV 2022, pages 250\u2013269, Cham, 2022. Springer Nature\nSwitzerland. 2, 3\n"
  },
  {
    "title": "Tree-Structured Shading Decomposition",
    "link": "https://arxiv.org/pdf/2309.07122.pdf",
    "upvote": "6",
    "text": "Tree-Structured Shading Decomposition\nChen Geng*\nHong-Xing Yu*\nSharon Zhang\nManeesh Agrawala\nJiajun Wu\nStanford University\nMultiply\nHighlight\nHighlight\nScreen\nMultiply\nMultiply\n  Diffuse\n  Diffuse\nOur Model\nOur Model\nAlbedo\nAlbedo\n(b) Reconstructed Shade Tree\n(a) Input\nScreen\nScreen\nMultiply\nHighlight\n  Diffuse\nAlbedo\nEnvironment\nScreen\nScreen\nScreen\nMultiply\n(c) Shade Tree Reuse\n(d) Editing\nDiffuse\nEnvironment\nExecute\nApply\nOur Model\nOur Model\nExecute\nApply\nHighlight\nAlbedo\nFigure 1. Decomposing shading into a tree-structured representation. (a) Our method enables the decomposition of given shading into a\n(b) shade tree. (c) This representation can be reused to generate new shade trees and (d) edit the shading of objects.\nAbstract\nWe study inferring a tree-structured representation from\na single image for object shading. Prior work typically uses\nthe parametric or measured representation to model shad-\ning, which is neither interpretable nor easily editable. We\npropose using the shade tree representation, which combines\nbasic shading nodes and compositing methods to factor-\nize object surface shading. The shade tree representation\nenables novice users who are unfamiliar with the physical\nshading process to edit object shading in an efficient and in-\ntuitive manner. A main challenge in inferring the shade tree\nis that the inference problem involves both the discrete tree\nstructure and the continuous parameters of the tree nodes.\nWe propose a hybrid approach to address this issue. We in-\ntroduce an auto-regressive inference model to generate a\nrough estimation of the tree structure and node parameters,\nand then we fine-tune the inferred shade tree through an\noptimization algorithm. We show experiments on synthetic\nimages, captured reflectance, real images, and non-realistic\n\u2217Equal contribution.\nvector drawings, allowing downstream applications such as\nmaterial editing, vectorized shading, and relighting. Project\nwebsite: https://chen-geng.com/inv-shade-trees.\n1. Introduction\nAnalyzing the shading process in images is fundamental\nto computer vision and graphics. In particular, the shad-\ning process models how the appearances of surfaces are\ngenerated from an object\u2019s material properties and lighting\nconditions. Traditional methods formulate it as the prob-\nlem of intrinsic decomposition, which expresses the shading\nas the product of reflectance and albedo [2, 13]. However,\nthis representation is limited in applicability as it assumes\na Lambertian surface. Another popular line of works on\ninverse rendering aims at reconstructing analytical represen-\ntations [35,39,42,58,59] or measured representations [30]\nfor materials and lighting. Yet, such physical representations\nare often difficult to interpret in human perception and not\nuser-friendly for image manipulation tasks.\nThe choice of shading representation in inverse graphics\n1\narXiv:2309.07122v1  [cs.CV]  13 Sep 2023\n(a) Input\n(c) Vectorized Shading\n(b) Material Editing\n(d) Vectorization Layers\n(e) Object Relighting 1\n(f) Object Relighting 2\nFigure 2. Illustration of downstream applications using the shade tree representation extracted from a single image. For object relighting in\n(e) and (f), insets show the changed lighting condition.\nis important in that it affects what downstream tasks can\nbe accomplished with that representation. The Shade tree\nmodel is a popular representation for shading in the for-\nward rendering community [7]. One important application\nof this representation is that it models how vector graphics\nare shaded [38]. Due to its tree structure, this representation\nis highly interpretable and easily editable. Thus, it is a wor-\nthy and interesting task to recover such representation from\nvisual observations.\nIn this work, we study recovering the shade tree repre-\nsentation from a single image. We define our shade tree\nas a binary tree that contains predefined base nodes (like\n\u201chighlight\u201d and \u201calbedo\u201d) and operations (like \u201cscreen mode\u201d\nand \u201cmix\u201d). Fig. 1 shows examples of our extracted shade\ntrees and subsequently edited materials produced from these\nextracted trees. In particular, we focus on decomposing the\n\u201cshading\u201d of objects. The input shading can be considered\nas spherical reflectance maps or \u201cMatCaps\u201d obtained from\nexisting pipeline [46,51].\nDespite its desirable high interpretability and editability,\ninferring such a structured representation from a single im-\nage has inherent challenges. First, a shade tree contains both\ncontinuous parameters for leaf nodes as well as a discrete\ntree structure, making it difficult to optimize directly. Second,\ndifferent combinations of base nodes and operations can lead\nto equivalent structures, introducing additional ambiguities\nfor deterministic inference methods. To infer both discrete\nstructure and continuous parameters, we propose a novel two-\nstage approach to iteratively decompose an input observation\ninto a shade tree. In the first stage, we use an auto-regressive\nmodel to recursively decompose nodes to generate an initial\ntree structure. Then, we perform sub-structure searching and\nparameter optimization to fine-tune the tree representation.\nTo deal with the structural ambiguity, we propose a multiple\nsampling strategy to allow non-deterministic inference that\naccounts for the multi-modal distribution of plausible shade\ntrees.\nOur extensive experiments show the effectiveness of the\nproposed approach in decomposing the shading of objects\nusing the shade tree representation. Further, we apply our ap-\nproach to real shadings and non-realistic vector drawings and\ndemonstrate applications on real images. We demonstrate\nvarious downstream tasks in Fig. 2.\nIn summary, our contributions are three-fold. First, we\nformulate the problem of inferring shade trees from a single\nimage, aiming at understanding the shading of objects with\nan interpretable representation. Second, we design a novel\nhybrid approach, integrating an amortized-inference pipeline\nand an optimization-based solver. Third, we conduct exten-\nsive experiments to show the effectiveness of our method\nand demonstrate potential applications of our method.\n2. Related Work\nShade Tree Representation. The history of using shade\ntrees as a rendering representation in computer graphics can\nbe dated back to the 1980s. Cook et al. [7] first proposed\nthis representation in 1984, and subsequent use this repre-\nsentation to model the shading of vector graphics [38]. 3D\nsoftware like Blender [6] uses node graphs, a representation\nsimilar to shade trees.\nFew pieces of literature study the problem of inverting\nsuch structures. Both Favreau et al. [10] and Richardt et\nal. [47] present algorithms to decompose vector graphics\ninto gradient layers, but they do not organize them into tree\nstructures. Lawrence et al. [30] study the problem of invert-\ning the parameter of leaf nodes given some fixed shade tree\nstructure. However, our work focuses not only on predicting\nthe parameter of leaf nodes but also on reconstructing the\nstructure of the shade tree.\nShade Trees v.s. Intrinsic Decomposition / BRDFs. Our ap-\nproach is also related to intrinsic decomposition and inverse\nrendering.\nIntrinsic decomposition methods seek to decompose im-\nages into albedo and reflectance in pixel space without fur-\nther structures [2,12,25,34,36,48]. The shading structure\nrecovered in this work is flexible, rather than predefined rules\n(albedo \u00d7 reflectance), differing from common intrinsic de-\ncomposition tasks.\nTraditional inverse rendering methods aim at recovering\nmaterial, geometry, and lighting from images [1,3,18,26,33,\n39,42,58,59], using predefined analytical material models\nsuch as the Disney BRDF [4]. Compared to the parametric\nBRDFs, the shade tree focuses on a different level of ab-\nstraction. While BRDFs model an element of shading, i.e.,\nreflectance properties of materials, it does not model other\nshading elements such as lighting. Our shade tree models the\noutcome of shading, i.e., the appearance. This involves both\n2\nmaterial and lighting for real images, as well as other artistic\neffects in cartoon shadings. Inverting the shade tree represen-\ntation features advantages including flexibility in shading,\ninterpretability to common users, and high editability.\nInverse Procedural Graphics. Procedural graphics gener-\nates content algorithmically rather than manually. Textures,\nbiological phenomena, and regular structures like buildings\nand cities are typically generated with procedural models,\nwith a compact set of variables to direct the generation. In-\nverse procedural graphics seeks to infer parameters or gram-\nmar for procedural models describing such structures. This\nis often done within specific domains, including urban de-\nsign and layouts [11, 41, 54, 55], L-systems [16, 61], tex-\ntures [23, 28, 29, 31], forestry [43, 52], CSG (Constructive\nSolid Geometry) trees [9,27,49,57], and scene representa-\ntion [32,37,40,56].\nLarge material datasets [8] coupled with differentiable\nmaterial graph frameworks [21,50] have made deep learning\nmethods applicable to procedural material modeling. Given\na dataset of training images, Shi et al. [50] can select an\ninitial graph structure and optimize its parameters to match\na target material appearance. Alternatively, Hu et al. [22]\ndirectly utilizes the latent space of a generative model to\ntransfer material appearance. In a similar vein, Henzler et\nal. [19] embed images into a latent space before generating\nBRDF parameters. Our method is different from them in that\nwe simultaneously reconstruct the discrete tree structure and\nthe continuous parameters, allowing better adaptation ability\nto unseen real images. Generative models have also been\napplied for creating material representations [17,60]. More\nrecently, Guerrero et al. [15] also shows that transformers\nare suitable for modeling and generating material graphs,\nwhich contain many long-range dependencies. In contrast to\ngeneration, we focus on reconstruction from an image.\n3. Method\nWe now introduce our tree decomposition pipeline. First,\nwe introduce the context-free grammar used to represent our\nshade trees (Sec 3.1). Next, we cover the recursive amortized\ninference used to produce an initial tree structure (Sec 3.3).\nFinally, we explain an additional optimization-based fine-\ntuning step for decomposing any remaining nodes that were\nnot reliably decomposed by the recursive inference (Sec 3.4).\n3.1. Grammar Specification\nDefinition of Shade Trees. A shade tree is a tree-structured\nrepresentation for shading. The leaf nodes of the tree struc-\nture are all basic shading nodes that cannot be further decom-\nposed. The interior nodes are formulated using a specified\ncomposition method taking child nodes as input. By exe-\ncuting the tree structure in a bottom-up manner, we can get\ncomplex shading effects.\nTree\n\u2192\nMix(Tree, Tree, Mask)\nTree\n\u2192\nMultiply(Tree, Tree)\nTree\n\u2192\nScreen(Tree, Tree)\nTree\n\u2192\nAlbedo(Color=Var)\nTree\n\u2192\nDiffRef(Lobe=Var, Ambient=Var)\nTree\n\u2192\nEnvRef\nTree\n\u2192\nHighlight(Lobe=Var, Sharpness=Var)\nEnvRef\n\u2192\na environment map\nVar\n\u2192\nfree continuous variable\nMask\n\u2192\na map with 0 and 1\nTable 1. Context-free grammar for the DSL representing shade tree\nstructure. More details of the DSL can be found in the supplement.\nDefinition of Base Nodes. We define four basic shading\nnodes. Highlight nodes represent a single highlight re-\nflected on the surface. DiffRef nodes represent the diffuse\nreflective component of the material. Albedo nodes are ho-\nmogeneous nodes with only one uniform color for shading\nto represent a basic albedo shading. Finally, EnvRef nodes\nmodel the specular shading reflecting the surrounding envi-\nronment.\nDefinition of Composition Methods. We define three com-\npositing methods to construct parent shading nodes from\nchild nodes. The Multiply operator performs a multiplica-\ntion of its two child nodes. The Screen operation performs\na screen mode composition. The Mix operation takes a mask\nas input and uses the mask to assign different shading nodes\nto different regions. For multiply, the shading of parent\nnode p is defined as:\np = cl \u00b7 cr,\n(1)\nwhere cl and cr denote the left child and the right child,\nrespectively. The screen operation is given by:\np = 1 \u2212 (1 \u2212 cl) \u00b7 (1 \u2212 cr).\n(2)\nAnd the mix operation is defined as:\np = m \u00b7 cl + (1 \u2212 m) \u00b7 cr,\n(3)\nwhere m denotes a learnable mask.\nContext-Free Grammar. The definition of the shade tree\ncan be formalized to a domain-specific language (DSL) rep-\nresented by a context-free grammar [20] G, as shown in\nTable 1.\n3.2. Overview of Algorithm\nThe proposed algorithm contains two stages. We show\nan overview in Fig. 3. In the first stage, we aim to recover\nthe initial structure of the shade tree using a recursive amor-\ntized inference decomposition module (Fig. 3 top). In the\n3\nInput\n   (Section 3.3)\nInitial Structure \n    Prediction\n?\nUnknown Substructure\nSubstructure Searching\nCombine and Optim.\nFinal Structure\n(Section 3.4)\nFigure 3. The proposed framework for reconstructing shade tree representation. Our method takes in a shading, and then first does an\ninitial structure prediction in a top-down recursive manner. It is allowed that there is still some unknown substructure from this initial guess.\nThen the roots of those unknown structures are fed into a substructure searching module, where we perform searching over all possible\nsubstructures and optimize the leaf parameter to decide whether this structure is appropriate. After all the substructures are decided, we\nmerge them into the initially predicted tree and get the final structure. We perform an overall optimization on this structure to get the final\nparameter of the leaf nodes.\nsecond stage, we decompose the nodes that are not success-\nfully solved in the first stage and recover the parameters of\nleaf nodes using an evolution-based optimization algorithm\n(Fig. 3 bottom).\nThe motivation for this two-stage design for decompo-\nsition is that we wish to take advantage of the distinct be-\nhaviors of these two types of algorithms. The first stage is\ntop-down amortized inference and performs the decompo-\nsition layer-by-layer. This approach learns prior knowledge\nfrom large-scale training data. Thus, the decomposition is\nfast but occasionally fails in some corner cases due to the\nlack of enough capacity to generalize, which is seen as a\ncommon problem for learning-based methods.\nThus, we further introduce the second stage, which em-\nploys a classical program synthesis that enumerates all pos-\nsible structures and does optimization to find the correct\nsolution. Such an enumeration is slower than learning-based\nmethods, yet it has more capacity to generalize to corner\ncases. By combining these two approaches, our algorithm is\neffective and efficient in tackling the task.\n3.3. Recursive Amortized Inference\nIn the first part, we do the decomposition in a top-down\nmanner recursively and then procedurally generate the entire\ntree. We maintain a pool of nodes and record their type and\nlinkage for each inference procedure. Initially, there is only\none node I0 in the pool, serving as the root node of the whole\ntree. At each step, we consider node I which is neither a leaf\nnor decomposed. We pass it into our shared single-step com-\nponent prediction module M and obtain {Il, Ir} = M(I),\nwhere Il and Ir denote the left and right child nodes, respec-\ntively. The design of M will be discussed in this section later.\nThe child nodes Il and Ir are then linked to the parent node\nI with new tree edges. All three nodes are then fed into a\nCNN f which gives\np = f(I, Il, Ir),\n(4)\nwhere p is a probability distribution over all compositing\noperations in our grammar. The operation with the highest\nprobability is selected as the type of the parent node. A\nseparate CNN g that also takes the three nodes as input\npredicts\n\u2126 = g(I, Il, Ir),\n(5)\nwhere \u2126 is the parameter value of the selected operation.\nAfter predicting the operation and corresponding pa-\nrameters, we then get the reconstructed parent node \u02c6I by\nchoosing the correct operation from operation set S =\n{mix, screen, multiply} and then get the single-step re-\nconstruction error Lrecon:\nLrecon = ||I \u2212 \u02c6I||2,\n(6)\nwhere\n\u02c6I = Sargmax(p)(Il, Ir, \u2126).\n(7)\nTo determine whether the predicted child node should be\nfurther decomposed, we pass each of Il, Ir into a child com-\nponent prediction neural network h and get the probability\nq of its type. If the child node is a leaf node, then we mark it\nas solved in the node pool, so it is not further decomposed.\nThe previously described procedure ends whenever no more\nnodes can be decomposed.\nWe then describe how the single-step component predic-\ntion module M is implemented. The design of M follows\ntwo principles:\n4\nParent\n Input\nPredicted\n   Child\nEncoder\nChild Type\nPred. Net.\nLatent\n     Auto-regressive\nPrediction for 1st Child\nLatent of\n 1st child\n     Auto-regressive\nPrediction for 2nd Child\nLatent of\n2nd child\nDecoder\nOperation Prediction\n           Network\nScreen\nLeaf Node\nParameter(Albedo Color)\nInternal Node\nOptimizer\nRecursive Decomposition\nFigure 4. Illustration of recursive inference module used in the initial structure prediction. The structure prediction is performed in a\ntop-down recursive manner. In each step, we feed the current parent node into the module, and it is first encoded to a discrete latent code\nusing VQ-VAE. Then the latent code is fed into the first auto-regressive module to predict the latent code for the first child. After that, the\nlatent of the first child and the parent are both fed into the second auto-regressive module to get latent of the second child. The latent codes\nare decoded into images for children nodes. Afterward, the parent node and the children nodes are fed into the operation prediction network\nto predict the operation of this step. For each of the predicted children, we use a child-type prediction network to know whether it is a leaf\nnode. If it is not a leaf node, it will be further decomposed. Otherwise, it will be optimized to get its parametric representation.\n1. The prediction should not be deterministic, i.e., it\nshould allow different kinds of output for this mod-\nule. This is because many different structures of trees\nmay describe the same tree.\n2. The prediction of the second child should at least de-\npend on the prediction of the first child and the parent\nnode, and the prediction of the first child should depend\non the parent node.\nInspired by these two principles, we design a two-step\nconditional auto-regressive module for prediction.\nAuto-regressive Inference. Auto-regressive inference en-\ntails a discretized feature space. Thus, we adopt the Vector\nQuantized Variational Autoencoder (VQ-VAE) [45] as our\nencoder architecture. The latent feature v of a shading I is\ngiven by: v = E(I), where E denotes the VQ-VAE encoder\n(Fig. 4 left top). Then we further train two conditional Pixel-\nSNAIL [5] models to auto-regressively generate two child\nnodes (Fig. 4 middle top). Specifically, for the generation\nof the first child node, we sample the discrete latent repre-\nsentation vl from the distribution pl(v) represented using\nthe auto-regressive model, conditioned on the latent code of\nthe parent node. Similarly, the latent code vr of the second\nchild is sampled from the distributions pr(v, vl) encoded by\nthe auto-regressive model, conditioned on the previous child\nand the parent node. Finally, the child images are decoded\nusing the decoder D to generate the image representation of\nchild nodes.\nWe build a synthetic dataset to train the previously men-\ntioned modules. Please refer to the supplementary material\nfor the detail of the training.\nMultiple Sampling. For each auto-regressive inference, we\nsample T times to make sure that we make the best decom-\nposition decision in each step. We define a criterion to select\nfrom multiple samples. First, we need the reconstruction\nresult that combines two child nodes to be as similar as pos-\nsible to the parent node, which can be indicated from the\nLrecon as described in Eq. 7.\nFurther, we wish the derived tree to be as compact as\npossible by avoiding useless decomposition. We define Lsim\nthat represents the similarity between the parent node and\nthe child nodes, which is defined as\nLsim = \u2212 log(||I \u2212 Il||2 + ||I \u2212 Ir||2).\n(8)\nWe also define Lblank and Lwhite to avoid one child being\nwholly blank or white, which will result in useless decompo-\nsition:\nLblank = \u2212 log(||E(Il) \u2212 1||2 + ||E(Ir) \u2212 1||2),\n(9)\nLwhite = \u2212 log(||E(Il)||2 + ||E(Ir)||2).\n(10)\nThe final criterion Lselect is defined as\nLselect = Lrecon + \u03b1Lsim + \u03b2Lblank + \u03b3Lwhite,\n(11)\n5\nRealistic\nToon\nDRM (real-captured)\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nCNN\n26.50\n0.967\n0.066\n27.18\n0.959\n0.052\n14.41\n0.857\n0.164\nLSTM\n17.80\n0.909\n0.154\n24.86\n0.964\n0.067\n19.67\n0.882\n0.205\nTransformer\n18.82\n0.930\n0.153\n26.73\n0.971\n0.042\n17.89\n0.876\n0.186\nOurs\n30.89\n0.974\n0.052\n30.08\n0.972\n0.032\n25.47\n0.927\n0.150\nTable 2. Quantitative comparison. Our method greatly surpasses other baselines in all three datasets, benefiting from our design to\naccurately predict structure and parameters. \u201cRealistic\u201d and \u201cToon\u201d are two synthetic datasets and \u201cDRM\u201d is a real-captured dataset.\nwhere \u03b1, \u03b2, \u03b3 are hyper-parameters.\nEarly-stop Strategy. The amortized inference module may\nnot successfully decompose every node, which is why we\ndesigned the second stage to further decompose those nodes\nand finetune the whole tree structure. We send a node to the\nsecond stage if minT (Lselect) < \u03c4, where \u03c4 is a threshold.\n3.4. Optimization-based Finetuning\nFor the nodes that cannot be decomposed in the first stage,\nwe search over all possible sub-structures of these nodes and\nuse an optimizer BasinCMA [24] to find the optimal leaf\nparameters. The optimization target can be defined as\nmin\n\u03bb ||Rs(\u03bb) \u2212 I||2 + ||FVGG(Rs(\u03bb)) \u2212 FVGG(I)||2, (12)\nwhere \u03bb denotes all trainable parameters, Rs represents the\nrenderer under the searched structure s and FVGG represents\na pretrained VGG network.\nThe search over all possible substructures is performed in\nthe order of depth. If the target loss is already smaller than a\npredefined threshold \u03d5, we stop searching and assume it to\nbe the final substructure.\nObtaining parametric representation for leaf nodes. After\nfinalizing the shade tree\u2019s structure, we optimize each leaf\nnode using the same target as described in Eq. 12 to get the\nparametric representation for each leaf node.\nOptimizing on the whole tree. Finally, we perform opti-\nmization on the parameter of all leaf nodes using the follow-\ning target:\nmin\n\u00b5 ||RS(\u00b5)\u2212I0||2+||FVGG(RS(\u00b5))\u2212FVGG(I0)||2, (13)\nwhere S stands for the finalized structure of the whole tree\nand \u00b5 = [\u03bb0, \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbN].\n4. Experiments\nIn this section, we first show our results on shade tree\ndecomposition on a diverse set of example including realis-\ntic synthetic images, cartoon-style images, and real images.\nThen we introduce a visual shading editing analogy experi-\nment which is designed to quantitatively evaluate the decom-\nposition of reconstructed structures. Finally we analyze our\nmethod by ablation study results.\n4.1. Results on Decomposition\nTo evaluate the effectiveness of the proposed method, we\nconduct a quantitative evaluation of our methods and other\nbaselines on several datasets.\nDatasets. We evaluate our method on both synthetic and\nreal-captured datasets.\nFor the synthetic dataset, we generate two styles of\ndatasets, \u201cRealistic\u201d and \u201cToon\u201d, to show the robustness\nand broad applicability of the proposed method. For the\n\u201cRealistic\u201d dataset, all the base nodes are represented in a\nphoto-realistic way, imitating how the shading in real life\nbehaves. For the \u201cToon\u201d dataset, we take inspiration from\nnon-photorealistic shading [14] and generate many cartoon-\nstyle shading nodes. After generating all base nodes and\noperation nodes, they are split into two sets, one for training\nsets and the other for the generation of test sets. Afterward,\nwe apply a recursive algorithm to generate the training and\ntest sets using the specified context-free grammar. The details\nof the dataset can be found in the supplementary material.\nBesides the synthetic datasets, we use the real-captured\ndataset \u201cDRM\u201d collected by Rematas et al. [46] to evaluate\nthe real-world generalizability of the proposed method.\nBaselines. No previous work has tackled a task setting sim-\nilar to ours. Therefore, we drew inspiration from previous\nresearch on grammar decomposition and adapted three com-\npetitive baseline frameworks that are widely used in the\nneural program synthesis and structure induction community\nfor our purpose.\nOur CNN baseline, which utilizes a similar architecture to\nthat in Rim-net [44], employs an encoder-decoder structure\nto perform single-step decomposition recursively, similar to\nthe first stage of our approach. We also introduced an LSTM\nbaseline, similar to Shape Programs [53], which first uses\nan encoder to get the latent representation of images and\nthen uses LSTM to predict a sequence of tokens that are\nsubsequently compiled to the shade tree structure. Similarly,\nour Transformer baseline also predicts the sequence of\ntokens but adapts a GPT architecture, following Matformer\n[15]. Please note that although the baselines share a similar\nbackbone design with previous literature, they differ due to\nthe different problem settings. The supplementary material\n6\nOurs\nCannot be further decomposed\nScreen\nScreen\nScreen\nMultiply\nMultiply\nMix\nCNN\nScreen\nScreen\nScreen\nLSTM\nTransformer\nInput\nScreen\nScreen\nScreen\nMultiply\nMultiply\nMix\nEndless Decomposition\nScreen\nScreen\nScreen\nScreen\nScreen\nScreen\nMultiply\nMultiply\nMix\nScreen\nScreen\nCannot be further decomposed\nScreen\nScreen\nScreen\nScreen\nScreen\nMultiply\nMultiply\nMix\nScreen\nScreen\nMultiply\nScreen\nMultiply\nMix\nFigure 5. Qualitative comparison of different methods on reconstruction. [Top row] shows a sample from Realistic dataset, [Mid row]\nshows a sample from Toon dataset, and [Bottom row] shows a real test sample from the DRM dataset [46].\ncontains further details and implementation of the baselines.\nResults. We show the results of this experiment in Fig. 5\nand Table 2. Our method has the best-reconstructed tree\nstructure among all three methods. The LSTM baseline can\npredict similar structures to ours; however, it performs poorly\nin predicting the parameter of leaf nodes, resulting in bad\nreconstruction results. The CNN baseline predicts in a top-\ndown manner; however, it suffers from ambiguity in the\ngrammar. Thus, it cannot learn a correct mapping between\nlayers, resulting in nodes that cannot be further decomposed\nor endlessly decomposed in a trivial way.\nOur method can also be applied to the real-world dataset\n\u201cDRM\u201d with satisfactory performance, which can be wit-\nnessed from the third column of Table 2 and the third row\nof Fig. 5. The result shows the generalizability and the real-\nworld applicability of the proposed approach.\nWe also perform decomposition using our model on some\nin-the-wild internet photos, shown in Fig. 6.\n4.2. Visual Shading Editing Analogy\nTo allow quantitatively evaluating the reconstructed tree\nstructure, we design a task called \u201cVisual Shading Editing\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nCNN\n4.79\n0.143\n0.608\nLSTM\n4.47\n0.113\n0.547\nTransformer\n5.02\n0.186\n0.547\nOurs\n32.17\n0.913\n0.078\nTable 3. Quantitative comparison of different methods on the\ntask visual shading editing analogy. Our method performs the\nbest among all three methods by understanding the tree structure\nwell. The other two methods cannot deal with this task because of\ntheir poor decomposition.\nAnalogy\u201d which reflects how well the decomposition is. As\nillustrated in Fig. 7, given an input pair of shading, the algo-\nrithm should give a hybrid shade tree composed of different\nsubtrees from different nodes, according to the rule shown\nin the example shading ball pair.\nWe generate a dataset containing different types of shad-\ning editing to evaluate the performance of different methods\non this task. We adopt the same baseline setting in Section\n4.1, introducing the CNN, LSTM, and Transformer base-\nlines. Then we use such methods to decompose given pairs\n7\n[Lopez-Moreno et al. 2013]\n[Richardt et al. 2014]\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 6. Decomposition of in-the-wild real images using our method. Our method can not only work on synthetic data but can also be\nwidely used in the decomposition of in-the-wild shading. The shadings in (a, b, c) are collected from the Internet. In (d), we show that our\nmethod can do decomposition of the shadings from Lopez-Moreno et al. [38]. In (e), we compare our method with Richardt et al. [47]. We\nfirst extract shading from the vector drawing, and then we use our method to do decomposition to the shading sphere.\nMultiply\nMultiply\nMultiply\nScreen\nScreen\nScreen\nSubstitute\n?\n(a) Example\n(b) Analogy\nFigure 7. Illustration of \u201cVisual Shading Editing Analogy\u201d that\nallows quantitative evaluation. Given an example of shading\nediting, we wish the same operation could be applied to novel test\npairs. For instance, in this case, the example shows us that the\nedited shading is formed by replacing the albedo node with the\nalbedo node from the second shading ball. Thus, this operation\nshould also be applied to the pair in the bottom row, resulting in a\nshading ball with a blue base color and an upward highlight.\nto get their tree-structured representation. The details of the\nalgorithm for making such a visual analogy are described in\nthe supplementary material.\nTable 3 shows the results. Our method surpasses other\nmethods greatly due to a better understanding of the semantic\nmeaning of tree structure.\n4.3. Ablation Studies\nTo verify the influence of the special design in the pro-\nposed pipeline, we do ablation studies on the following three\ncomponents: multiple sampling, second-stage optimization,\nand overall optimization, because they are typically non-\ntrivial in previous literature.\nInfluence of Multiple Sampling From Fig. 8, it can be ob-\nserved that multiple sampling improves the result during the\n1st stage inference because it can produce several solutions\nand use the metrics to choose the best one. Without doing\nthis, our model may directly predict a \u201creasonable\u201d one, but\nnot the \u201cbest\u201d one.\nInfluence of Optimization The second stage of optimization\nhelp us to decompose those nodes that are hard to deal with\nusing only the amortized-inference module. By removing\nsuch a component, our method cannot decompose the bottom\nnode shown in Fig. 8 and thus gives worse results than the\n8\nBase\n+ Multiple Sampling\n+ Second Stage Optimization\n+ Overall Optimization\nInput\nFigure 8. Ablation of components. \u201cBase\u201d denotes our method\nwith only 1 sample during inference, \u201c+Multiple Sampling\u201d de-\nnotes only using the 1st stage with multiple sampling. \u201c+Second\nStage\u201d denotes using both the first stage and the second stage of\nthe proposed method but does not perform the overall optimization.\n\u201c+Overall Optimization\u201d denotes the full proposed method.\n(a) Real-Captured Image\n(b) Extracted Shading\n(c) Reconstructed Tree\n Extract\nMatCap\n     Using\nOur Model\nScreen\nMultiply\nHighlight\nAlbedo\nDif\nFigure 9. Results on a real image of a non-sphere object. Al-\nthough the main focus of this work is decomposing the reflectance\nspheres, it can be applied to non-sphere geometry as well if we use\nexisting tools to first extract sphere reflectance from images first.\nfull method.\nInfluence of Overall Optimization By introducing the over-\nall optimization at the end of the second stage, our method\ncan further finetune the structure, like giving a better envi-\nronment reflection.\n4.4. Application on Real-world Images\nOur work focuses on the decomposition of MatCaps or\nReflectance Maps [51]. However, the work can be applied to\nreal-world images by using existing tools to first extract the\nsphere reflectance. In Fig. 9, we show an example of using\nthe proposed method together with an existing tool ZBrush\nto decompose the shading of a real-world capture.\n5. Conclusion\nWe have presented a novel method that can effectively\nand efficiently decompose shading into a tree-structured\nrepresentation, which enables understanding and editing of\nthe shading in an interpretable way. The first stage of the\nproposed method uses a pretrained auto-regressive model\nto predict the structure and parameters of the tree structure.\nThe second stage of the pipeline leverages the parametric\nrepresentation of each base node and structure searching\nto find the optimal structure for all nodes that cannot be\neffectively decomposed in the first stage. The combination\nof two stages leads to our state-of-the-art performance on\nseveral datasets compared to the baselines.\nAcknowledgments.\nThis work was in part supported by\nFord, NSF RI #2211258, AFOSR YIP FA9550-23-1-0127,\nthe Toyota Research Institute (TRI), the Stanford Institute\nfor Human-Centered AI (HAI), Amazon, and the Brown\nInstitute for Media Innovation.\nReferences\n[1] Jonathan T Barron and Jitendra Malik. Shape, illumination,\nand reflectance from shading. IEEE transactions on pattern\nanalysis and machine intelligence, 37(8):1670\u20131687, 2014. 2\n[2] Sean Bell, Kavita Bala, and Noah Snavely. Intrinsic images in\nthe wild. ACM Transactions on Graphics (TOG), 33(4):1\u201312,\n2014. 1, 2\n[3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Bar-\nron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectance\ndecomposition from image collections. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 12684\u201312694, 2021. 2\n[4] Brent Burley and Walt Disney Animation Studios. Physically-\nbased shading at disney. In Acm Siggraph, volume 2012,\npages 1\u20137. vol. 2012, 2012. 2\n[5] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter\nAbbeel. Pixelsnail: An improved autoregressive generative\nmodel. In International Conference on Machine Learning,\npages 864\u2013872. PMLR, 2018. 5\n[6] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 2\n[7] Robert L Cook. Shade trees. In Proceedings of the 11th\nannual conference on Computer graphics and interactive\ntechniques, pages 223\u2013231, 1984. 2\n[8] Valentin Deschaintre, Miika Aittala, Fr\u00b4edo Durand, George\nDrettakis, and Adrien Bousseau. Single-image svbrdf capture\nwith a rendering-aware deep network. ACM Transactions on\nGraphics (SIGGRAPH Conference Proceedings), 37(128):15,\naug 2018. 3\n[9] Tao Du, Jeevana Priya Inala, Yewen Pu, Andrew Spielberg,\nAdriana Schulz, Daniela Rus, Armando Solar-Lezama, and\nWojciech Matusik. Inversecsg: Automatic conversion of 3d\nmodels to csg trees. ACM Trans. Graph., 37(6), dec 2018. 3\n[10] Jean-Dominique Favreau, Florent Lafarge, and Adrien\nBousseau. Photo2clipart: Image abstraction and vectorization\nusing layered linear gradients. ACM Trans. Graph., 36(6),\nnov 2017. 2\n9\n[11] Mathieu Gaillard, Vojt\u02c7ech Krs, Giorgio Gori, Radom\u00b4\u0131r M\u02c7ech,\nand Bedrich Benes. Automatic differentiable procedural mod-\neling. Computer Graphics Forum, 41(2):289\u2013307, 2022. 3\n[12] Elena Garces, Adolfo Munoz, Jorge Lopez-Moreno, and\nDiego Gutierrez. Intrinsic images by clustering. In Com-\nputer graphics forum, volume 31, pages 1415\u20131424. Wiley\nOnline Library, 2012. 2\n[13] Elena Garces, Carlos Rodriguez-Pardo, Dan Casas, and Jorge\nLopez-Moreno. A survey on intrinsic images: Delving deep\ninto lambert and beyond. International Journal of Computer\nVision, 130(3):836\u2013868, 2022. 1\n[14] Bruce Gooch and Amy Gooch. Non-photorealistic rendering.\nAK Peters/CRC Press, 2001. 6\n[15] Paul Guerrero, Milos Hasan, Kalyan Sunkavalli, Radomir\nMech, Tamy Boubekeur, and Niloy Mitra. Matformer: A gen-\nerative model for procedural materials. ACM Trans. Graph.,\n41(4), 2022. 3, 6\n[16] Jianwei Guo, Haiyong Jiang, Bedrich Benes, Oliver Deussen,\nXiaopeng Zhang, Dani Lischinski, and Hui Huang. Inverse\nprocedural modeling of branching structures by inferring l-\nsystems. ACM Trans. Graph., 39(5), jun 2020. 3\n[17] Yu Guo, Cameron Smith, Milo\u02c7s Ha\u02c7san, Kalyan Sunkavalli,\nand Shuang Zhao. Materialgan: Reflectance capture using\na generative svbrdf model. ACM Trans. Graph., 39(6), nov\n2020. 3\n[18] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.\nShape, light & material decomposition from images us-\ning monte carlo rendering and denoising. arXiv preprint\narXiv:2206.03380, 2022. 2\n[19] Philipp Henzler, Valentin Deschaintre, Niloy J Mitra, and\nTobias Ritschel. Generative modelling of brdf textures from\nflash images. ACM Trans Graph (Proc. SIGGRAPH Asia),\n40(6), 2021. 3\n[20] John E Hopcroft, Rajeev Motwani, and Jeffrey D Ullman.\nIntroduction to automata theory, languages, and computation.\nAcm Sigact News, 32(1):60\u201365, 2001. 3\n[21] Yiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier,\nand Valentin Deschaintre. Node graph optimization using\ndifferentiable proxies. In ACM SIGGRAPH 2022 Conference\nProceedings, SIGGRAPH \u201922, New York, NY, USA, 2022.\nAssociation for Computing Machinery. 3\n[22] Yiwei Hu, Milo\u02c7s Ha\u02c7san, Paul Guerrero, Holly Rushmeier,\nand Valentin Deschaintre. Controlling material appearance by\nexamples. Computer Graphics Forum, 41(4):117\u2013128, 2022.\n3\n[23] Yiwei Hu, Chengan He, Valentin Deschaintre, Julie Dorsey,\nand Holly Rushmeier.\nAn inverse procedural modeling\npipeline for svbrdf maps. ACM Trans. Graph., 41(2), jan\n2022. 3\n[24] Minyoung Huh, Richard Zhang, Jun-Yan Zhu, Sylvain Paris,\nand Aaron Hertzmann. Transforming and projecting images\ninto class-conditional generative networks. In European Con-\nference on Computer Vision, pages 17\u201334. Springer, 2020.\n6\n[25] Michael Janner, Jiajun Wu, Tejas D Kulkarni, Ilker Yildirim,\nand Josh Tenenbaum. Self-supervised intrinsic image decom-\nposition. Advances in neural information processing systems,\n30, 2017. 2\n[26] Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Songfang\nHan, Sai Bi, Xiaowei Zhou, Zexiang Xu, and Hao Su. Tensoir:\nTensorial inverse rendering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2\n[27] R. Kenny Jones, Homer Walke, and Daniel Ritchie. Plad:\nLearning to infer shape programs with pseudo-labels and ap-\nproximate distributions. The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 3\n[28] Ares Lagae, Sylvain Lefebvre, George Drettakis, and Philip\nDutr\u00b4e. Procedural noise using sparse gabor convolution. In\nACM SIGGRAPH 2009 Papers, SIGGRAPH \u201909, New York,\nNY, USA, 2009. Association for Computing Machinery. 3\n[29] Ares Lagae, Peter Vangorp, Toon Lenaerts, and Philip Dutr\u00b4e.\nProcedural isotropic stochastic textures by example. Comput-\ners & Graphics, 34(4):312\u2013321, 2010. 3\n[30] Jason Lawrence, Aner Ben-Artzi, Christopher DeCoro, Wo-\njciech Matusik, Hanspeter Pfister, Ravi Ramamoorthi, and\nSzymon Rusinkiewicz. Inverse shade trees for non-parametric\nmaterial representation and editing. ACM Transactions on\nGraphics (TOG), 25(3):735\u2013745, 2006. 1, 2\n[31] Laurent Lefebvre and Pierre Poulin. Analysis and synthesis\nof structural textures. In Graphics Interface, volume 2000,\npages 77\u201386, 2000. 3\n[32] Yikai Li, Jiayuan Mao, Xiuming Zhang, William T. Freeman,\nJoshua B. Tenenbaum, and Jiajun Wu. Perspective Plane\nProgram Induction from a Single Image. In Conference on\nComputer Vision and Pattern Recognition, 2020. 3\n[33] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan\nSunkavalli, and Manmohan Chandraker. Inverse rendering for\ncomplex indoor scenes: Shape, spatially-varying lighting and\nsvbrdf from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2475\u20132484, 2020. 2\n[34] Zhengqi Li and Noah Snavely.\nLearning intrinsic image\ndecomposition from watching the world. In 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition.\nIEEE, jun 2018. 2\n[35] Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan\nSunkavalli, and Manmohan Chandraker. Learning to recon-\nstruct shape and spatially-varying reflectance from a single\nimage. ACM Transactions on Graphics (TOG), 37(6):1\u201311,\n2018. 1\n[36] Yunfei Liu, Yu Li, Shaodi You, and Feng Lu. Unsupervised\nlearning for intrinsic image decomposition from a single im-\nage. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR). IEEE, jun 2020. 2\n[37] Yunchao Liu, Jiajun Wu, Zheng Wu, Daniel Ritchie,\nWilliam T. Freeman, and Joshua B. Tenenbaum. Learning to\ndescribe scenes with programs. In International Conference\non Learning Representations, 2019. 3\n[38] Jorge Lopez-Moreno, Popov Stefan, Adrien Bousseau, Ma-\nneesh Agrawala, and George Drettakis. Depicting stylized\nmaterials with vector shade trees. ACM Transactions on\nGraphics, 32(4), 2013. 2, 8\n[39] Fujun Luan, Shuang Zhao, Kavita Bala, and Zhao Dong. Uni-\nfied shape and svbrdf recovery using differentiable monte\n10\ncarlo rendering. In Computer Graphics Forum, volume 40,\npages 101\u2013113. Wiley Online Library, 2021. 1, 2\n[40] Jiayuan Mao, Xiuming Zhang, Yikai Li, William T. Freeman,\nJoshua B. Tenenbaum, and Jiajun Wu. Program-Guided Image\nManipulators.\nIn International Conference on Computer\nVision, 2019. 3\n[41] Andelo Martinovic and Luc Van Gool. Bayesian grammar\nlearning for inverse procedural modeling. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2013. 3\n[42] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 8280\u20138290,\n2022. 1, 2\n[43] Till Niese, S\u00a8oren Pirk, Matthias Albrecht, Bedrich Benes,\nand Oliver Deussen. Procedural urban forestry. ACM Trans.\nGraph., 41(2), mar 2022. 3\n[44] Chengjie Niu, Manyi Li, Kai Xu, and Hao Zhang. Rim-net:\nRecursive implicit fields for unsupervised learning of hier-\narchical shape structures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 11779\u201311788, 2022. 6\n[45] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generat-\ning diverse high-fidelity images with vq-vae-2. Advances in\nneural information processing systems, 32, 2019. 5\n[46] Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstra-\ntios Gavves, and Tinne Tuytelaars. Deep reflectance maps. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 4508\u20134516, 2016. 2, 6, 7\n[47] C. Richardt, J. Lopez-Moreno, A. Bousseau, M. Agrawala,\nand G. Drettakis. Vectorising bitmaps into semi-transparent\ngradient layers. In Proceedings of the 25th Eurographics\nSymposium on Rendering, EGSR \u201914, page 11\u201319, Goslar,\nDEU, 2014. Eurographics Association. 2, 8\n[48] Carsten Rother, Martin Kiefel, Lumin Zhang, Bernhard\nSch\u00a8olkopf, and Peter Gehler. Recovering intrinsic images\nwith a global sparsity prior on reflectance. Advances in neu-\nral information processing systems, 24, 2011. 2\n[49] Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kaloger-\nakis, and Subhransu Maji. Csgnet: Neural shape parser for\nconstructive solid geometry. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2018.\n3\n[50] Liang Shi, Beichen Li, Milo\u02c7s Ha\u02c7san, Kalyan Sunkavalli, Tamy\nBoubekeur, Radomir Mech, and Wojciech Matusik. Match:\nDifferentiable material graphs for procedural material capture.\nACM Trans. Graph., 39(6):1\u201315, Dec. 2020. 3\n[51] Peter-Pike J Sloan et al. The lit sphere: A model for capturing\nnpr shading from art. Graphics Interface, 2001. 2, 9\n[52] O. Stava, S. Pirk, J. Kratt, B. Chen, R. M\u02c7ech, O. Deussen, and\nB. Benes. Inverse procedural modelling of trees. Computer\nGraphics Forum, 33(6):118\u2013131, 2014. 3\n[53] Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis,\nWilliam T Freeman, Joshua B Tenenbaum, and Jiajun Wu.\nLearning to infer and execute 3d shape programs. arXiv\npreprint arXiv:1901.02875, 2019. 6\n[54] Carlos A. Vanegas, Ignacio Garcia-Dorado, Daniel G. Aliaga,\nBedrich Benes, and Paul Waddell. Inverse design of urban\nprocedural models. ACM Trans. Graph., 31(6), nov 2012. 3\n[55] Fuzhang Wu, Dong-Ming Yan, Weiming Dong, Xiaopeng\nZhang, and Peter Wonka. Inverse procedural modeling of\nfacade layouts. CoRR, abs/1308.0419, 2013. 3\n[56] Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural\nscene de-rendering. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. 3\n[57] Q. Wu, K. Xu, and J. Wang. Constructing 3d csg models from\n3d raw point clouds. Computer Graphics Forum, 37(5):221\u2013\n232, 2018. 3\n[58] Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron:\nInverse rendering by optimizing neural sdfs and materials\nfrom photometric images. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5565\u20135574, 2022. 1, 2\n[59] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and\nNoah Snavely. Physg: Inverse rendering with spherical gaus-\nsians for physics-based material editing and relighting. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5453\u20135462, 2021. 1,\n2\n[60] Xilong Zhou, Milo\u02c7s Ha\u02c7san, Valentin Deschaintre, Paul Guer-\nrero, Kalyan Sunkavalli, and Nima Kalantari.\nTilegen:\nTileable, controllable material generation and capture, 2022.\n3\n[61] O. \u02c7St\u2019ava, B. Bene\u02c7s, R. M\u02c7ech, D. G. Aliaga, and P. Kri\u02c7stof.\nInverse procedural modeling by automatic generation of l-\nsystems. Computer Graphics Forum, 29(2):665\u2013674, 2010.\n3\n11\n"
  }
]