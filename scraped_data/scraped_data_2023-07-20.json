[
  {
    "title": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models",
    "link": "https://arxiv.org/pdf/2307.09793.pdf",
    "upvote": "45",
    "text": "1\nOn the Origin of LLMs:\nAn Evolutionary Tree and Graph for 15,821 Large Language Models\nSarah R Gao, Andrew K Gao\nCanyon Crest Academy, Stanford University\n2\nAbstract\nSince late 2022, Large Language Models (LLMs) have become very prominent\nwith LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new\nLLMs are announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training methods,\nand families are popular or trending. However, there is no comprehensive index\nof LLMs available. We take advantage of the relatively systematic nomenclature\nof\nHugging\nFace\nLLMs\nto\nperform hierarchical clustering and identify\ncommunities amongst LLMs using n-grams and term frequency-inverse document\nfrequency. Our methods successfully identify families of LLMs and accurately\ncluster LLMs into meaningful subgroups. We present a public web application to\nnavigate and explore Constellation, our atlas of 15,821 LLMs. Constellation\nrapidly generates a variety of visualizations, namely dendrograms, graphs, word\nclouds, and scatter plots. Constellation is available at the following link:\nhttps://constellation.sites.stanford.edu/.\nThe dataset we created will be shared publicly on Github, under @andrewgcodes\n(https://github.com/andrewgcodes).\nIntroduction\nLarge language models (LLMs) are trained to generate realistic text given a user prompt [1].\nPopular LLMs include ChatGPT, Bard, and the LLaMa family of models [2]. In addition to large\ncompanies like OpenAI and Google, smaller research groups and individuals can also train\nLLMs and share them through Hugging Face, a popular machine learning repository [3,4]. As of\nJuly 18, 2023 at 12 PM (GMT -5), 15,821 LLMs (or at least, Text Generation models) were\navailable publicly on Hugging Face. To our knowledge, few attempts have been made to\norganize these LLMs, perhaps due to the immense number of models. Inspired by the\nbioinformatics technique of using hierarchical clustering on DNA sequences, we apply\nhierarchical clustering to the Hugging Face model names, assuming that similar names indicate\nsimilarity [5]. We also construct a graph of LLMs and detect communities using the Louvain\nmethod. Additionally, we generate other visualizations and explore the data.\n3\nMethods\nLibraries\n\u25cf\nBeautifulSoup [6]\n\u25cf\nPandas [7]\n\u25cf\nStreamlit [8]\n\u25cf\nScipy [9]\n\u25cf\nPlotly [10]\n\u25cf\nNumpy [11]\n\u25cf\nScikit-learn [12]\n\u25cf\nRadial Tree [13]\n\u25cf\nNLTK [14]\n\u25cf\nMatplotlib [15]\n\u25cf\nPython-Louvain [16]\n\u25cf\nNetworkX [17]\n\u25cf\nWordcloud [18]\n\u25cf\nRegEx [19]\nData Collection\nPython's BeautifulSoup library was used to retrieve the names, number of likes, and number of\ndownloads of Hugging Face models labeled with \u201cText Generation\u201d. Data collected included\nmodel names, Readme links, number of downloads, and the number of likes. Data collection was\nperformed on July 18, 2023 around 12 PM (US ET; GMT -5). Note that data collection was not\ninstantaneous. In some instances, we failed to retrieve a number of likes or downloads for a\nmodel.\nParameter Extraction\nIn addition to the above attributes, model parameters were inferred from the model name using a\nregular expression (RegEx) pattern (\\d+(\\.\\d+)?)(B|M|b|m). This pattern matches digit sequences\nfollowed by \"B\", \"M\", \"b\", or \"m\", as model sizes are often included in the name (e.g.,\n\"falcon-7b\"). The number of parameters in millions was recorded in a column named\n'params_millions' in the dataset. If parameters couldn't be inferred, the corresponding field was\nmarked as 'NaN'.\n4\nData Analysis and Visualization\nWe used libraries such as Scipy, Plotly, Numpy, Scikit-learn, Radial Tree, NLTK, and Matplotlib\nto analyze the dataset and generate visualizations. A full list of imports is provided at the\nbeginning of Methods.\nText Feature Extraction:\nThe model names were converted into a matrix of Term Frequency-Inverse Document Frequency\n(TF-IDF) features using Scikit-learn's TfidfVectorizer. The vectorizer was configured to break\ndown the model names into n-grams ranging from 2 to 8 characters.\nHierarchical Clustering:\nHierarchical clustering with single linkage was performed using the matrix of TF-IDF features.\nCosine distance was used as a similarity measure between the model names. The clustering result\nwas visualized as an interactive dendrogram using the Plotly library.\nAgglomerative Clustering:\nIn addition to hierarchical clustering, agglomerative clustering was also performed with a\nspecified number of clusters. The size of each cluster was calculated and visualized as a bar\nchart.\nWord Clouds\nTo understand the contents of our agglomerative clusters, we generate Word Clouds the most\ncommon n-grams in each cluster, with more frequent n-grams being presented with larger text\nsize.\nGraph Visualization with Communities\nA graph-based visualization was constructed to provide an intuitive understanding of the\nrelationship and similarity among the models. NetworkX, a Python library for the creation,\nmanipulation, and study of the structure, dynamics, and functions of complex networks, was\nused to generate the graph.\nNode Creation:\nEach model name was represented as a node in the graph. The graph was initialized as an\nundirected graph, and each model name was added as a node using the 'add_node' function. The\nmodel name served as the node label.\n5\nEdge Creation:\nEdges in the graph were used to represent the similarity between pairs of model names. After\ncalculating the cosine similarity matrix, an edge was added between two nodes (model names) if\ntheir cosine similarity was above a specific threshold (0.2 in this case). The cosine similarity\nvalue was set as the weight of the edge.\nCommunity Detection:\nThe Louvain method, a popular community detection algorithm, was used to find communities\nwithin the constructed graph. Communities represent groups of models that are more similar to\neach other than to models in other groups. The detected communities were used for subsequent\nvisual enhancements.\nLayout Calculation:\nThe Fruchterman-Reingold force-directed algorithm was employed to calculate the layout of\nnodes in the graph. This algorithm arranges the nodes in such a way that all the edges are more\nor less equally long and there are as few crossing edges as possible.\nInteractive Visualization:\nThe generated graph was visualized interactively using the Plotly library. Each node represented\na model and was color-coded based on the community it belonged to. Edges between the nodes\nindicated similarity, with their thickness corresponding to the cosine similarity score. Hovering\nover the nodes displayed more details about the model.\nAdditional Enhancements:\nThe centroid (center point) of each community was computed to add a colored background for\neach community cluster. The size of the background color patch represented the size of the\ncommunity.\nWeb Application\nWe built a public web application using the Streamlit framework to generate interactive\ndendrograms,\nword\nclouds,\nand\ngraphs\nfor\nthe\ndata,\nwhich\nis\navailable\nhere:\nhttps://constellation.sites.stanford.edu/.\nResults\nThere were 15,821 public models labeled with Text Generation on Hugging Face at the time of\ndata collection. We assembled a final Pandas dataframe containing seven columns: rank,\n6\nmodel_name, link, downloads, likes, ReadMeLink, and params_millions. Rank is assigned in\norder of number of downloads. For instance, \u201cgpt2\u201d has the most downloads. Note that \u201cgpt2\u201d\ndoes not have an inferred number of parameters because the model name does not contain any\nevidence of parameter size. We were able to infer model parameters for 4,560 models (28.8%).\nWe expect our RegEx expression to result in few false positives. Not all links in ReadMeLink\nlead\nto\na\nvalid\nReadme file. The links were automatically computed by appending\n\u201c/raw/main/README.md\u201d to the model link. All model links should lead to a valid Hugging\nFace page.\nFigure 1. First five rows of our dataset in order of number of downloads.\nWe computed a Pearson correlation coefficient of 0.242 between the number of likes and\ndownloads a model receives. There is a clear positive but weak relationship. It is possible that\nthis weakness indicates a disparity between model usefulness and popularity. Alternatively,\nlarger, more powerful models may attract more attention (receiving more likes) but will garner\nrelatively few downloads because they are too large for most Hugging Face users to use. In\ngeneral, models tend to receive far more downloads than likes. This could be because there is no\nbenefit to the user to like a model on Hugging Face, while downloading the model is beneficial.\nWe generate a radial dendrogram on all models with over 5,000 downloads to compactly\nvisualize relationships and families. From the dendrogram, families of LLMs like Wizard,\nPythia, CausalLM, and Bloom can be observed. We suggest using the web application to view\nthe dendrogram since the large number of leaves makes it difficult to render on a single static\nimage clearly.\n7\nFigure 2. Scatter plot showing the relationship between the number of likes and downloads a model receives.\nBoth axes received a log scale transformation.\n \nFigure 3. Radial dendrogram of models with over 5,000 downloads. High resolution image available on Constellation web site.\n8\nWe do not show all the word clouds here due to space, but here are some of the example word\nclouds for clusters generated from all models with over 1,000 downloads (clusters = 20). The\nword clouds are helpful in understanding which model families are prominent.\nFigure 4. Word clouds show clusterization of LLaMa models and code-specific LLMs.\nWe generate a graph of the models, with similar models receiving an edge. We use the Louvain\nmethod to detect communities.\nFigure 5. Graph of models with more than 10,000 downloads, with communities detected using the Louvain method.\nWe present a publicly available web application (https://constellation.sites.stanford.edu/) to\ndynamically explore the data. The web application enables the user to specify the minimum\nnumber of downloads an LLM must have to be considered in the analysis. The web app quickly\ngenerates a dendrogram, word clouds, and graph. Hovering over graph nodes reveals additional\nmetadata about the model located at that node. The web application also displays useful statistics\nand an interactive scatter plot of likes versus downloads. Hovering over points reveals the model\nname.\n9\nFigure 6. Screenshot of the web application.\nConclusion\nThe increasing number and diversity of Large Language Models (LLMs) necessitate a\ncomprehensive and systematic approach to organize, classify, and understand these models. In\nthis study, we have proposed an effective solution by creating Constellation, a user-friendly web\napplication that visualizes the hierarchical relationships among LLMs, helping to reveal\nprominent LLM families and underlying structures.\nOur approach is generally inspired by bioinformatics and sequence similarity. It utilizes\nhierarchical and agglomerative clustering, combined with an array of techniques such as\ndendrograms, word clouds, and graph-based representations. The word clouds provide a\nhigh-level view of prominent model families, while the graph-based visualization depicts the\nrelationships and similarities among models in a more flexible format than the dendrogram.\nThe major limitation of our study is that it assumes that LLMs are only similar if they have\nsimilar names. This is not completely true: LLMs can be named anything by the creator who\ndeposits it to Hugging Face. However, in general, we note that LLMs tend to be named in a\nstructured, logical fashion. Our results indicate that our assumption that in general similar LLMs\nshare similar names is sound. We acknowledge that our approach can miss similar LLMs,\nespecially if one of the LLMs is arbitrarily named. Another limitation is that not all models\nlabeled \u201cText Generation\u201d are necessarily LLMs. Finally, a further caveat is that the dendrogram\nis not a true \u201cevolutionary\u201d tree. While models in the same low-level cluster are generally\nreliably related, this does not hold for higher-level clusters.\nBy making Constellation publicly available, we hope to encourage more systematic and\ninformed engagement with LLMs. As the landscape of LLMs continues to evolve rapidly, tools\n10\nsuch as Constellation will be instrumental in assisting the researcher and developer communities\nin keeping pace with these developments.\nReferences\n1.\nGao, A. (2023, July 8). Prompt Engineering for Large Language Models. SSRN; SSRN.\nhttps://doi.org/10.2139/ssrn.4504303\n2.\nArancio, J. (2023, April 17). Llama, Alpaca and Vicuna: the new Chatgpt running on your laptop. Medium.\nhttps://medium.com/@jeremyarancio/exploring-llamas-family-models-how-we-achieved-running-llms-on-l\naptops-16bf2539a1bb\n3.\nHiter, S. (2023, June 6). What Is a Large Language Model? | Guide to LLMs. EWEEK.\nhttps://www.eweek.com/artificial-intelligence/large-language-model/\n4.\nHugging Face. (n.d.). Hugging Face \u2013 On a mission to solve NLP, one commit at a time. Huggingface.co.\nhttps://huggingface.co/\n5.\nWei, D., Jiang, Q., Wei, Y., & Wang, S. (2012). A novel hierarchical clustering algorithm for gene\nsequences. BMC Bioinformatics, 13(1). https://doi.org/10.1186/1471-2105-13-174\n6.\nBeautiful Soup Documentation. (n.d.). Tedboy.github.io. Retrieved July 19, 2023, from\nhttps://tedboy.github.io/bs4_doc/\n7.\npandas documentation \u2014 pandas 1.0.1 documentation. (2023, June 28). Pandas.pydata.org.\nhttps://pandas.pydata.org/docs/\n8.\nStreamlit Docs. (n.d.). Docs.streamlit.io. https://docs.streamlit.io/\n9.\nscipy. (2020, February 3). scipy/scipy. GitHub. https://github.com/scipy/scipy\n10. plotly.py. (2021, September 28). GitHub. https://github.com/plotly/plotly.py\n11. numpy/numpy. (2021, October 9). GitHub. https://github.com/numpy/numpy\n12. Scikit-Learn. (2019). User guide: contents \u2014 scikit-learn 0.22.1 documentation. Scikit-Learn.org.\nhttps://scikit-learn.org/stable/user_guide.html\n13. koonimaru. (2023, June 15). radialtree. GitHub. https://github.com/koonimaru/radialtree\n14. NLTK. (2009). Natural Language Toolkit \u2014 NLTK 3.4.4 documentation. Nltk.org. https://www.nltk.org/\n15. Matplotlib. (n.d.). Matplotlib: Python plotting \u2014 Matplotlib 3.3.4 documentation. Matplotlib.org.\nhttps://matplotlib.org/stable/index.html\n11\n16. Aynaud, T. (2023, July 11). Louvain Community Detection. GitHub.\nhttps://github.com/taynaud/python-louvain\n17. Hapberg, A., Schult, D., & Swart, P. (2008, August). Exploring network structure, dynamics, and function\nusing NetworkX. Conference.scipy.org. https://conference.scipy.org/proceedings/SciPy2008/paper_2\n18. Mueller, A. (2020, May 7). amueller/word_cloud. GitHub. https://github.com/amueller/word_cloud\n19. Ahmad, Z. (2023, July 19). ziishaned/learn-regex. GitHub. https://github.com/ziishaned/learn-regex\nAppendix\nWord\nOccurrences\ngpt2\n1597\n7b\n889\n13b\n770\ngpt\n756\nfinetuned\n611\nllama\n475\ngptq\n393\ndistilgpt2\n383\npythia\n381\nmodel\n309\nwikitext2\n297\nsmall\n294\nbase\n285\ninstruct\n262\nneo\n261\nopt\n252\nvicuna\n238\n4bit\n224\nbloom\n215\nv2\n214\n30b\n203\n6b\n191\n12\nalpaca\n190\n125m\n182\ncodeparrot\n178\nrarity\n172\nv1\n171\nfalcon\n168\n8k\n167\nsft\n167\nlarge\n166\ndialogpt\n160\ntest\n157\n2\n156\nall\n155\nmedium\n154\nlora\n153\nds\n146\nmerged\n145\nsuperhot\n143\nj\n141\nhf\n141\nfp16\n133\nchat\n129\nopen\n126\nconcat\n126\nowt2\n126\n350m\n123\n70m\n123\nchinese\n122\n128g\n121\nmpt\n118\ngpt4\n118\n3b\n116\nmyawesomeeli5c\nlm\n108\ntiny\n106\n13\n1\n106\n4\n105\n8bit\n103\nheadless\n101\ncodegen\n97\n33b\n97\ndeduped\n95\nsharded\n89\nairoboros\n88\nfinetunedgpt2\n85\nv3\n83\nwizardlm\n83\ngenerator\n83\nno\n82\n560m\n81\nrandom\n79\nuncensored\n74\nfinetune\n74\nxl\n72\n1b\n71\nmod\n71\n27b\n69\n65b\n68\nelonmusk\n68\nbloomz\n66\nv0\n66\nbert\n65\nguanaco\n64\nft\n64\nimdb\n63\ngptj\n62\n160m\n61\ngptneo\n61\nredpajama\n58\nneox\n57\n14\nggml\n57\nthe\n55\n5\n55\ndolly\n53\n12b\n53\ncut\n53\nguten\n53\n67b\n52\ndelta\n52\nTable 1. Most common words/phrases among all Hugging Face LLMs\n"
  },
  {
    "title": "Challenges and Applications of Large Language Models",
    "link": "https://arxiv.org/pdf/2307.10169.pdf",
    "upvote": "45",
    "text": "Challenges and Applications of Large Language Models\nJean Kaddour\u03b1, \u2020, \u2217, Joshua Harris\u03b2, \u2217, Maximilian Mozes\u03b1,\nHerbie Bradley\u03b3, \u03b4, \u03f5, Roberta Raileanu\u03b6, and Robert McHardy\u03b7, \u2217\n\u03b1University College London\n\u03b2UK Health Security Agency\n\u03b3EleutherAI\n\u03b4University of Cambridge\n\u03f5Stability AI\n\u03b6Meta AI Research\n\u03b7InstaDeep\nAbstract\nLarge Language Models (LLMs) went from\nnon-existent to ubiquitous in the machine learn-\ning discourse within a few years. Due to the\nfast pace of the field, it is difficult to identify\nthe remaining challenges and already fruitful\napplication areas. In this paper, we aim to es-\ntablish a systematic set of open problems and\napplication successes so that ML researchers\ncan comprehend the field\u2019s current state more\nquickly and become productive.\nContents\n1\nIntroduction\n1\n2\nChallenges\n2\n2.1\nUnfathomable Datasets . . . . . .\n2\n2.2\nTokenizer-Reliance . . . . . . . .\n4\n2.3\nHigh Pre-Training Costs\n. . . . .\n6\n2.4\nFine-Tuning Overhead\n. . . . . .\n10\n2.5\nHigh Inference Latency . . . . . .\n11\n2.6\nLimited Context Length . . . . . .\n14\n2.7\nPrompt Brittleness\n. . . . . . . .\n17\n2.8\nHallucinations . . . . . . . . . . .\n19\n2.9\nMisaligned Behavior\n. . . . . . .\n22\n2.10 Outdated Knowledge . . . . . . .\n27\n2.11 Brittle Evaluations\n. . . . . . . .\n27\n2.12 Evaluations\nBased\non\nStatic,\nHuman-Written Ground Truth\n. .\n28\n2.13 Indistinguishability between Gen-\nerated and Human-Written Text\n.\n29\n2.14 Tasks Not Solvable By Scale . . .\n30\n2.15 Lacking Experimental Designs . .\n31\n2.16 Lack of Reproducibility . . . . . .\n33\n3\nApplications\n34\n3.1\nChatbots . . . . . . . . . . . . . .\n34\n3.2\nComputational Biology . . . . . .\n36\n3.3\nComputer Programming\n. . . . .\n37\n*Equal contribution.\n\u2020{jean.kaddour,robert.mchardy}.20@ucl.ac.uk,\njoshua.harris@ukhsa.gov.uk\nDesign\nUnfathomable Datasets, \nTokenizer-Reliance,\nFine-Tuning Overhead\nScience\n Evaluations Based on Static \nHuman-Written Ground Truth,\nLacking Experimental Designs,\nLack of Reproducibility\nBehavior\nPrompt Brittleness, \nMisaligned Behavior,\nOutdated Knowledge\nDetecting \nGenerated \nTexts, Brittle \nEvaluations\nHigh Pre-Training \nCosts\nHigh Inference \nLatency, Limited \nContext Length, \nHallucinations\nTasks Not \nSolvable\nBy Scale\nFigure 1: Overview of LLM Challenges. Designing\nLLMs relates to decisions taken before deployment. Be-\nhaviorial challenges occur during deployment. Science\nchallenges hinder academic progress.\n3.4\nCreative Work . . . . . . . . . . .\n39\n3.5\nKnowledge Work . . . . . . . . .\n40\n3.6\nLaw . . . . . . . . . . . . . . . .\n42\n3.7\nMedicine\n. . . . . . . . . . . . .\n43\n3.8\nReasoning . . . . . . . . . . . . .\n44\n3.9\nRobotics and Embodied Agents . .\n45\n3.10 Social Sciences & Psychology . .\n46\n3.11 Synthetic Data Generation\n. . . .\n48\n4\nRelated Work\n49\n5\nConclusion\n49\n1\nIntroduction\nGiven the quickly growing plethora of LLM re-\nsearch papers, we aim to address two questions: (1)\nChallenges: What problems remain unresolved?\nand (2) Applications: Where are LLMs currently\nbeing applied, and how are the challenges con-\nstraining them? For (1), we group the challenges\n1\narXiv:2307.10169v1  [cs.CL]  19 Jul 2023\nin Fig. 1 into three broader categories \u201cDesign\u201d,\n\u201cBehavior\u201d, and \u201cScience\u201d. To provide answers\nfor (2), we explore the fields of chatbots, compu-\ntational biology, computer programming, creative\nwork, knowledge work, law, medicine, reasoning,\nrobotics, and the social sciences.\nThis paper is an opinionated review and assumes\nfamiliarity with LLMs and how they work (we refer\nto more introductory works in Sec. 4). Further, we\nfocus on models trained on text data. We target a\ntechnical researcher audience and do not discuss\npolitical, philosophical, or moral perspectives on\nLLMs.\n2\nChallenges\no Challenge\nThis box highlights a challenge.\n2.1\nUnfathomable Datasets\nScaling the amount of pre-training data has been\none of the major drivers to equip LLMs with\ngeneral-purpose capabilities [256]. The size of\npre-training datasets quickly outgrew the number\nof documents most human teams could manually\nquality-check. Instead, most data collection proce-\ndures rely on heuristics regarding data sources and\nfiltering.\nIn this section, we explore the adverse conse-\nquences of these heuristics and the reality that many\nmodel practitioners possess only a nebulous under-\nstanding of the data on which their model has been\ntrained. We refer to this issue as follows.\no Unfathomable Datasets\nThe size of modern pre-training datasets ren-\nders it impractical for any individual to read\nor conduct quality assessments on the en-\ncompassed documents thoroughly.\nNear-Duplicates\ncan arise in different forms\nand have been reported to degrade model per-\nformance [294, 200, 250].\nNear-duplicates are\nharder to find compared to exact duplicates; fil-\ntering out of such is a standard step in most data\ncollection pipelines, e.g., using the MinHash algo-\nrithm [57]. Lee et al. [294] propose the NearDup\nmethod and find that over 1% of tokens emitted\nunprompted from a model are part of a memorized\nsequence of the C4 dataset, e.g., it contains a 61-\nword sequence repeated 61, 036 times in the train-\ning split. By deduplicating it, they reduce the rate\nof emitted memorizations by 10x. Abbas et al. [6]\nintroduce SemDeDup, a technique designed to iden-\ntify semantic duplicates that, although perceptually\ndistinct, convey predominantly similar information,\nsuch as sentences with analogous structures with\ncertain words replaced by synonyms. After apply-\ning their method to C4, they find that it improves\nover NearDup. Similarly, Kaddour [250] find near-\nduplicates in the Pile [165] by clustering document\nembeddings and identifying clusters gathering du-\nplicates.\nBenchmark Data Contamination\noccurs when\nthe training dataset contains data from or similar\nto the evaluation test set. This can lead to inflated\nperformance metrics, as the model can memorize\nthe test data and simply regurgitate it back during\ntesting.\nFinding and removing all training and test data\noverlaps is difficult in practice. For example, the\nGPT-3 authors Brown et al. [59] found a code bug\nafter training, resulting in only partially removing\nall detected overlaps from the training data. They\ncould not afford to retrain the model, so they used it\nwith the remaining overlaps and \u201ccleaned\u201d variants\nof the considered benchmarks, with all potentially\nleaked examples removed. They define overlap-\nping examples as examples that share at least 13\nconsecutive words with any other example in the\npre-training set. If an example is shorter than 13\nwords, they consider it overlapping if it shares all\nof its words with another example.\nSimilarly, Dodge et al. [125] search for test data\nin the web-crawled C4 corpus but measure exact\nmatches, normalized for capitalization and punctu-\nation. They find various input-and-label contamina-\ntions of text generation and knowledge completion\ntasks; and input-only contaminations of the GLUE\nbenchmark. They argue that there are two ways test\ndata can end up in a snapshot of Common Crawl\n(the original dump source of C4): either a given\ntest set is built from a web text or uploaded after\ncreation. Sainz et al. [472] ask ChatGPT to gener-\nate academic benchmark instances, finding that it\nhas memorized multiple ones, including some test\nsplits. Jacovi et al. [237] propose three strategies to\nmitigate contamination, including encryption and\ntraining exclusion controls.\n2\nPersonally Identifiable Information (PII)\nsuch\nas phone numbers and email addresses, have\nbeen found within pre-training corpora, resulting\nin privacy leaks during prompting. Carlini et al.\n[65, 67], Lukas et al. [344] extract PII data by\nprompting GPT-2; Kulkarni [283] report how an en-\ngineer yields secret API keys by prompting GitHub\nCopilot. Henderson et al. [195] discuss the avail-\nability of PII in law data across different jurisdic-\ntions and filter it based on the legal norm in the\nrespective jurisdiction. El-Mhamdi et al. [137]\ncontend that because strong model performance\ntypically requires memorization of the training\ndata [146, 58], the (undetected) existence of PII\nin the training data will likely result in models that\nrender them extractable.\nPre-Training Domain Mixtures\nSeveral stud-\nies have argued for diversity in the pre-training\ncorpus [165, 341, 291]. Many popular corpora fol-\nlow this by concatenating datasets from different\nsources, as illustrated in Table 1. However, it re-\nmains underexplored what amount of data from\ndifferent sources is necessary for strong down-\nstream performances. Finding suboptimal mix-\ntures can cause low transferability to downstream\ntasks [593, 580] and reliance on spurious corre-\nlations [253, 618, 347]. Xie et al. [622] find do-\nmain mixture proportions by training a small proxy\nmodel using group-distributionally robust optimiza-\ntion [471]; surprisingly, they find that the final\nmodel trained using their found domain weights\nyields improved perplexity across all domains, even\nwhen it down-weights a domain.\nGiven a tar-\nget downstream task, Yao et al. [641], Xie et al.\n[624] select subsets most useful for pre-training.\nLongpre et al. [341] measure the effects of domain\ncompositions and find that inclusion of heteroge-\nneous data sources is broadly beneficial and likely\nmore important than the data quality (as measured\nby the document quality classifier employed by\nPaLM [86] and GLaM [130]) or size, which also\nmotivates smaller yet more diverse pre-training\ndatasets [250].\nFine-Tuning Task Mixtures\nhave to be deter-\nmined for fine-tuning a pre-trained model on many\ndifferent tasks, usually with comparatively few ex-\namples per task. This technique, which we call\nmultitask-prompted fine-tuned LMs (MTLMs), has\ndemonstrated significant generalization improve-\nments with very little additional training compute.\nDate\nName\nSize\nSources\nPublic\nGB\nTokens\u2217\n2014\nBookCorpus\n[684, 36]\n5 GB\n11 B\nNovels\nYes\n2019\nOSCAR\n[399]\n6.3 T\n?\nWebpages in 166\nlanguages\nYes\n2019\nWebText\n[440]\n40 GB\n?\nWebpages\nNo\n12.2020\nCC-100\n[100]\n2.5 TB\n292 B\nWebpages in 100\nLanguages\nYes\n12.2020\nThe\nPile\n[165, 41]\n825 GB\n300 B\nScience, Webpages,\nGitHub Code, Law,\netc.\nYes\n2020\nC4 [443]\n745 GB\n156 B\nWebpages\nYes\n10.2020\nmC4 [631]\n?\n6.3 T\nWebpages in 101\nLanguages\nYes\n2021\nMassiveText\n[441]\n10.5 TB\n2.34 T\nWebpages, Books,\nNews, and Code\nNo\n12.2021\nGLaM [130]\n?\n1.6 T\nWebpages,\nWikipedia, Conver-\nsations,\nForums,\nBooks, News\nNo\n01.2022\nInfiniset\n[551]\n?\n2.81 T\nForum\ndialogs,\nC4\ndata,\nCode,\nWikipedia,\nWeb-\npages\nNo\n06.2022\nROOTS\n[289]\n1.61 TB\n2.34 T\nWebpages in 46 lan-\nguages and GitHub\nCode in 13 lan-\nguages\nYes\n11.2022\nThe\nStack\n[271]\n6 TB\n235 B\nGitHub Code in 30\nlanguages\nYes\n04.2023\nLLaMA\n[556] / Red-\nPajama [98]\n2.7 TB\n1.2 T\nWebpages, GitHub\nCode,\nScience,\nWikipedia, Books\nYes\n06.2023\nRefinedWeb\n[415]\n2.8 TB\n600 B\nWebpages\nYes\nTable 1: Overview of Selected Pre-Training Datasets.\nOver the years, pre-training datasets have become more\nunfathomable: they grew rapidly in size and diversity,\nand not all datasets are publicly available (we do not\ninclude datasets that have very little or no information\navailable about them). Unless stated otherwise, the\nnatural language is in English. \u2217 We report the number\nof tokens as provided by the respective paper based on\ntheir proposed tokenization scheme.\nFor example, instruction fine-tuning via task in-\nstructions prepended to each set of input-output\npairs is a very popular scheme, which we will later\ndiscuss in more detail in Sec. 2.9. Wang et al. [589]\npropose Super-NaturalInstructions, a\nfine-tuning dataset with 1,616 diverse tasks and\nexpert-written instructions. Muennighoff et al.\n[377] extend MTLM to the multilingual setting,\nshowing that fine-tuning on multilingual tasks with\nEnglish prompts improves results on tasks in all\nlanguages.\nHowever, similar to the previous paragraph, how\nto balance the task datasets well remains unclear.\n3\nAs the tasks can vary in size considerably, Raf-\nfel et al. [443] mix each task in proportion to the\nnumber of examples in its \u2019train\u2019 split (up to some\nmax_num_examples). Jang et al. [239] report\nthat MTLMs can underperform expert LLMs fine-\ntuned on only a single task because of (i) nega-\ntive task transfer, where learning multiple tasks at\nonce hinders the learning of some specific tasks,\nand (ii) catastrophic forgetting of previous tasks\nwhen learning new tasks. Iyer et al. [235] study\nvarying task (sets) proportions, finding several\ntrade-offs and concluding that the right values for\nthese parameters depend on the downstream end-\ngoals. Longpre et al. [340] balance different sets of\ntask sources by omitting them, one at a time, and\nranking their contributions on the MMLU bench-\nmark [197]; further, they mix the input prompt\ntemplates of zero- and few-shot prompting; find-\ning that this improves the performance in both set-\ntings. Another trend is to imitate closed-source\nmodels like ChatGPT by collecting a dataset of\nAPI outputs (against OpenAI\u2019s terms and condi-\ntions) and fine-tuning an open-source LM with\nit [540]. However, Gudibande et al. [180] point\nout that such imitation models are only good at\nmimicking the proprietary model\u2019s style but not\nits content, a distinction that has been discussed\nextensively in the causality literature [253]. They\nconclude that substantial capability gaps between\nfine-tuned open-sourced and closed-source models\nremain, motivating future work for better imitation\ndata.\n2.2\nTokenizer-Reliance\nTokenization is the process of breaking a sequence\nof words or characters into smaller units called\ntokens, such that they can be fed into the model.\nOne common tokenization approach is subword to-\nkenization, where we split words into smaller units,\ncalled subwords or WordPieces [490]. The goal\nis to handle rare and out-of-vocabulary words in\na model\u2019s vocabulary effectively while maintain-\ning a limited number of tokens per sequence in the\ninterest of computational complexity. Subword to-\nkenizers are usually trained unsupervised to build\na vocabulary and optionally merge rules to encode\nthe training data efficiently.\nHowever, the necessity of tokenization comes\nwith multiple drawbacks [257]; some of which we\ndiscuss below. For example, Ahia et al. [13], Petrov\net al. [426] show that the number of tokens nec-\nessary to convey the same information varies\nsignificantly across languages, making the pric-\ning policy of API language models, which charge\nusers based on the number of processed or gen-\nerated tokens, potentially unfair. They find that\nusers of many supported languages are overcharged\nwhile receiving subpar results, with this group pre-\ndominantly residing in areas where these APIs are\nalready less affordable.\nFurther, discrepancies between the data that\na tokenizer and a model have been trained on\ncan lead to glitch tokens [465], which can sub-\nsequently cause unexpected model behavior as\ntheir corresponding embeddings are essentially un-\ntrained. This coupling between the tokenizer and\npre-training corpus creates the burden of a new\ntraining run of the tokenizer each time the pre-\ntraining corpus is modified.\nNext, Tokenization schemes that work well in a\nmultilingual setting, particularly with non-space-\nseparated languages such as Chinese or Japanese,\nremain challenging [157, 91].\nExisting subword tokenization schemes are pre-\ndominantly greedy algorithms trying to encode\nlanguage as efficiently as possible regarding the\nnumber of tokens used. Naturally, these methods\nfavor subwords comprising larger parts of the train-\ning data and, therefore, subwords that are shared\nacross many languages.\nThis favors languages\nwith shared scripts like Latin and Cyrillic, result-\ning in suboptimal tokenization of low-resource lan-\nguages [92, 676].\no Tokenizer-Reliance\nTokenizers introduce several challenges,\ne.g., computational overhead, language de-\npendence, handling of novel words, fixed\nvocabulary size, information loss, and low\nhuman interpretability.\nSubword-Level\nInputs\nare\nthe\ndominant\nparadigm, providing a good trade-off between\nvocabulary size and sequence length. Byte-Pair\nEncoding [490, 577] (BPE) starts with the set\nof symbols (characters or bytes) that comprise\nthe training data. The tokenizer is then trained\nto learn rules to merge the most frequent pair\nof two consecutive tokens\u2014defined by the\nexisting vocabulary\u2014into a new vocabulary item.\nByte-level BPE (BBPE) [577] is an extension\nof BPE with byte-level subwords, particularly\n4\nTokenization can sometimes lead to a loss of \ninformation. For example, in languages where \nword boundaries are not clearly de\ufb01ned, such \nas Chinese. \u2026\ndef bubble_sort(array):\n    n = len(array)\n    for i in range(n):\n        swapped = False\n        for j in range(0, n - i - 1):\n            if array[j] > array[j + 1]:\n                swap(array[j], array[j + 1])\n\u2026.\n\u6a19\u8a18\u5316\u6709\u6642\u6703\u5c0e\u81f4\u4fe1\u606f\u4e1f\u5931\u3002 \u4f8b\u5982\uff0c\u5728\u55ae\n\u8a5e\u908a\u754c\u6c92\u6709\u660e\u78ba\u5b9a\u7fa9\u7684\u8a9e\u2f94\u4e2d\uff0c\u4f8b\u5982\u4e2d\u2f42\uff0c\n\u6216\u8005\u5728\u5177\u6709\u8a31\u591a\u8907\u5408\u8a5e\u7684\u8907\u96dc\u8a9e\u2f94\u4e2d\uff0c......\nEnglish\nPython\nChinese\n\u2026\n]\n\u55ae\ntoken\n[\nto\n##ization\nlead\n\u4fe1\nloss\n\u6642\ni\narray\nexample\nchinese\n\u606f\nboundaries\n\u81f4\nare\n\u6a19\n\u5b9a\n\u2f94\nfor\n\u4e2d\nwhere\nin\nas\n\u5408\ndef\nSoftmax over\nVocabulary\nVocabulary\ntoken\nwhere\nas\n##ization\nto\nfor\na\nboundaries\nexample\nloss\nlead\nchinese\nTraining Sequences\n\u2f94\n\u606f\n\u81f4\n\u5b9a\n\u4fe1\n\u6a19\n\u591a\n\u6642\n\u55ae\n\u5408\n\u4e2d\n\u6703\n\u660e\n\u5c0e\n\u754c\n\u7fa9\n\u8a31\nin\ni\ndef\n[\n_\nsort\n]\n,\n)\n-\nfor\nFalse\narray\n+\n],\n1\nare\n\u2026\n\u2026\n\u2026\nrange\nif\nn\n(1) Tokenizer Training Costs\n(2) Arch. depends on Vocabulary \n\u2026\n]\n\u55ae\ntoken\n[\nto\n##ization\nlead\n\u4fe1\nloss\n\u6642\ni\narray\nexample\nchinese\n\u606f\nboundaries\n\u81f4\nare\n\u6a19\n\u5b9a\n\u2f94\nfor\n\u4e2d\nwhere\nin\nas\n\u5408\ndef\nTransformer\nBlocks\nEmbedding \nMatrix\n<latexit sha1_base64=\"EUAQA2JFnjQN78tKMAv1XK5WuxQ=\">ACEXicbVC7TsMw\nFHXKq5RXgJHFokLqVCWI18BQCZAYC6IPqQmV4zqtVceJbAepSvMLPwKCwMIsbKx8Tc4bQZoOZKl43Pu1b3eBGjUlnWt1FYWFxaXimultbWNza3zO2dpgxjgUkDhywUbQ9Jwign\nDUVI+1IEBR4jLS84UXmtx6IkDTkd2oUETdAfU59ipHSUtesOAFSA8+HV9ChHE5/XnKb3ifj5hg6igZEwsu01DXLVtWaAM4TOydlkKPeNb+cXojgHCFGZKyY1uRchMkFMWMpCUn\nHEWwB/ZBdjgFNTANaiDBsDgETyDV/BmPBkvxrvxMS0tGHnPLvgD4/MHRwacqQ=</latexit>liRCeIj6pKMpR3qQm0wuSuGBVnrQD4V+XMGJ+rsjQYGUo8DTldnGctbLxP+8Tqz8MzehPIoV4Xg6yI8ZVCHM4oE9KghWbKQJwoLqXSEeIGw0iFmIdizJ8+T5mHVPqke3xyVa+d5\nE 2 R|V |\u21e5D\n<latexit sha1_base64=\"VxULw+Mr90KaxUh2GOiN/OYQpA=\">ACIXicbVDLTsMw\nEHR4lvIqcORiUSFxqhLEowcOSHDgWBtkZpSOe4GLBwnsjeIKuRXuPArXDiAEDfEz+C0PfAaydLszK7WO0EihUHX/XAmJqemZ2ZLc+X5hcWl5crKasvEqebQ5LGM9UXADEihoIkC\nJVwkGlgUSGgHN0eF374FbUSsznGQDdiV0qEgjO0Uq9S9yOG10FI29QXio6qIDvL7PjXuYj3GEWxX2QeU59FBEYet+6p3m5V6m6NXcI+pd4Y1IlYzR6lXe/H/M0AoVcMmM6nptg\nq+2e7lQPD8ZxlMg62SBbxCP75JCckAZpEk4eyBN5Ia/Oo/PsvDnvo9YJZzyzRn7A+fwC7oij/A=</latexit>N2MaBZeQl/3UQML4DbuCjqWK2V3dbHhTjet0qdhrO1TSIfq94mMRcYMosB2FheY314h/ud1Ugzr3UyoJEVQfLQoTCXFmBZx0b7QwFEOLGFcC/tXyq+ZhxtqEUI3u+T/5LWds3b\nW 2 RDmodel\u21e5|V |\nMHA\n\u2026\nFFN\nMHA\nFFN\n\u2026\nFigure 2: Exemplary Drawbacks of relying on Tokenization. (1) The tokenizer training step involves non-trivial\ncomputations, e.g., multiple passes over the entire pre-training dataset, and introduces a dependency on it, which\ncan become especially problematic in multilingual settings. (2) The embedding layer E and output layer W of\nLLMs involve the vocabulary size; e.g., making up \u2248 66% of the model\u2019s parameter count in T5 models [629].\nsuited for multilingual tasks where it enables\nvocabulary sharing between languages. A trained\nBPE tokenizer applies the previously learned rules\nto tokenize inputs. WordPiece [485, 617] is a\nclosed-source tokenization algorithm used, e.g.,\nin BERT [120]. Like BPE, WordPiece starts with\na small initial vocabulary, which is iteratively\nextended by learning merge rules and creating new\nvocabulary items. Rather than selecting the most\nfrequent pair of consecutive tokens, WordPiece\nuses a scoring function to normalize the frequency\nof the pair by the frequencies of the individual\ntokens to prioritize common pairs with rare\nindividual tokens. Unigram Tokenization [281]\niteratively trims a large base vocabulary to a given\ntarget size. To this end, at each step of the tokenizer\ntraining, a unigram language model is used to\ncompute a loss over the training data conditional\non a certain vocabulary item being removed.\nA proportion of the subwords with the lowest\nlosses are removed to form the base vocabulary\nfor the next iteration. Unigram tokenization is\nprobabilistic, i.e., during inference, all possible\ntokenizations of a given sequence are scored\nusing the unigram language model, and the most\nlikely one is selected. SentencePiece [282] is a\ncommonly used open-source library, implementing\nseveral tokenization algorithms such as (B)BPE\nand Unigram tokenization.\nSentencePiece also\nimplements non-subword tokenization approaches\nlike word- and character-level tokenization.\nByte-Level Inputs\nare an alternative to subword\ntokenization is use byte-level inputs. Byte-level\ninputs can either be used in combination with sub-\nword tokenizers [577] or used to define a limited\nvocabulary that can be used to encode all possi-\nble sequences. For example,\nXue et al. [630]\ntrain a non-subword mT5 model using UTF-8\nbytes rather than subword tokens as inputs, show-\ning promising performance on multilingual data.\nWhile this enables subword-free LLMs, UTF-8 en-\ncodes Latin languages with fewer bytes than e.g.,\nChinese, Japanese or Korean1. Tay et al. [546] pro-\npose the Charformer, a tokenization-free model\nwhich learns a soft subword tokenization in la-\ntent space (Gradient-Based Subword Tokenization)\ngiven byte-level inputs. Charformer performs com-\nparably to subword-based models while incurring\nless computational overhead than other byte or\nsubword models. Choe et al. [83] train a small-\nscale, 0.8B language model based on raw byte-\nlevel inputs and show that it performs compara-\nbly. On a smaller scale, Clark et al. [94] show that\ntheir tokenization- and vocabulary-free encoder Ca-\nnine outperforms a comparable tokenization-based\nmodel. Yu et al. [652] address the computational\ncost that byte-level tokenization incurs by segment-\ning input sequences into local patches, which can\nbe processed in parallel. Similarly, Horton et al.\n[212] propose to operate directly on file bytes. In a\n1https://www.unicode.org/versions/Unicode15.0.0/\n5\nparallel line of work, Rust et al. [467] render text\nas images and train an encoder model to predict the\nraw pixels of the images.\n2.3\nHigh Pre-Training Costs\nThe vast majority of the training costs go toward the\npre-training process. Training a single LLM can\nrequire hundreds of thousands of compute hours,\nwhich in turn cost millions of dollars and consume\nenergy amounts equivalent to that used by several\ntypical US families annually [412, 86, 44]. Re-\ncently proposed scaling laws [256] posit that model\nperformances scale as a power law with model size,\ndataset size, and the amount of compute used for\ntraining, which is fairly unsustainable and can be\nclassified as Red AI [487], where state-of-the-art re-\nsults are essentially \u201cbought\u201d by spending massive\ncomputational resources. For example, depending\non the exact law coefficients, reducing the error\nfrom 3% to 2% can require an order of magnitude\nmore data or compute [518].\no Unsustainable Loss Power-Law [256]\nPerformance increases through larger com-\npute budgets but at a decreasing rate if the\nmodel or dataset size is fixed, reflecting a\npower law with diminishing returns.\nIn the following, we look at two lines of work\naiming at resolving such issues.\nCompute-Optimal Training Recipes [201, 256]\nIn Sec. 2.1, we discussed how the availability\nof LLM pre-training data has become abundant\nthrough the quickly-spread practice of including\nweb-crawled text.\nFurther, thanks to the intro-\nduction of Transformer models [563] and suit-\nable hardware [210], we have scaled models to\nunprecedented sizes. Assuming that we have not\nyet reached the limits of data [45, 568, 415] nor\nmodel sizes [256, 206, 398]; currently, the main\nbottleneck is the amount of compute available [1].\nGiven a particular budget, how large should the pre-\ntraining corpus and model be to maximize training\nefficiency?\nAs mentioned at the beginning of this section,\none recent proposal is to learn empirical \u201cscaling\nlaws\u201d [201, 256], which describe the relationship\nbetween LLM performance and the compute bud-\nget, model, and dataset size. These laws can pro-\nvide the right scaling recipe for compute-optimal\ntraining, ideally, even when extrapolating to larger\ncompute budgets. For example, OpenAI [398] re-\nport that they were able to accurately predict the\nmodel performance of the full-size GPT-4 model\nbased on the performance of a series of smaller\nmodels using at most 10,000x less compute than\nthe full model.\nThe exact power law coefficients are still heav-\nily debated. Kaplan et al. [256] put forward that\nthe model size should be scaled more aggressively\nthan the dataset size to use a given compute budget\noptimally. Contrary to this, Hoffmann et al. [206]\nfind that many LLMs are undertrained and argue\nthat the number of parameters and data should be\nscaled equally. However, power laws sometimes\ncome in the form of bounds, which can span an\norder of magnitude difference in the amount of\ndata to be used given a concrete compute budget\n[665]. Further, the pre-training loss does not al-\nways correlate well with downstream performance\n[252, 332, 251].\nThe viewpoint of Touvron et al. [556], Vries\n[571], Touvron et al. [557] is that when selecting\na model size, the computation resources for later\nusage (inference) should be considered, not just\nthe one-time training costs. They suggest that it\nmight be beneficial to train a smaller model more\nintensively upfront to offset larger inference costs\nin the future. Hence, they train models of various\nsizes on more tokens than are typically used to\nachieve the best performance possible, given the\nmodel size.\nOne remaining hurdle of performance prediction\nis inverse scaling, which we discuss in Sec. 2.14.\nSince scaling laws were typically constructed in the\ncontext of pre-training and thereby decoupled from\ndownstream tasks, it remains an open question of\nhow to predict inverse scaling properties. Tay et al.\n[544] find that scaling laws can differ in upstream\nand downstream setups; aside from only the model\nsize, model shape matters for downstream fine-\ntuning.\nPre-Training Objectives\nVarious pre-training\nobjectives (PTO) are suitable for performing self-\nsupervised training of LLMs. The exact choice of\nPTO heavily influences the model\u2019s data efficiency\nduring pre-training, which in turn can reduce the\nnumber of iterations required. A PTO typically\nis a function of the (i) architecture, (ii) input/tar-\ngets construction (e.g., target span length, low/high\ncorruption, see Fig. 4), and (iii) masking strategy\n(Fig. 3). While (i) and (ii) can be disentangled and\n6\nTargets\ny5\ny4\ny3\ny2\ny1\nMasked LM\nInput\nx5\nx4\nx3\nx2\nx1\nLanguage Modeling\nInput\nx5\nx4\nx3\nx2\nx1\nPre\ufb01x LM\nInput\nx5\nx4\nx3\nx2\nx1\nFigure 3: Masking Strategies. Each row denotes to\nwhich inputs xi (columns) a particular output yi (row)\ncan attend to (uni- or bi-directional).\nshould not be conflated conceptually [545], in prac-\ntice, there exist popular combinations that achieve\ngood performances.\nAttending to all tokens, as shown in Fig. 3(left),\nis the most data-efficient strategy since it uses con-\ntext from before and after the token to be predicted.\nHowever, for that reason, it is unsuitable for text\ngeneration [120], since it considers future context\nfor prediction. We typically employ it in natural\nlanguage understanding (NLU) tasks [120], where\nit has shown strong results. The next token predic-\ntion objective is most suitable for natural language\ngeneration (NLG) but also the least data efficient\nsince it only attends to the past context (Fig. 3(mid-\ndle)). More recent advances in pre-training objec-\ntives aim to find a middle-ground to increase data\nefficiency by providing stronger and more diverse\ntraining signals, e.g., the Prefix LM, which partly\nattends to past tokens, as illustrated in Fig. 3(right)\nand discussed below.\nThe following discusses the trade-offs between\nsome of the recently proposed objectives. Fig. 4\nvisually depicts the different pre-training objectives.\nNotation-wise, we denote a sequence of N tokens\nx as x = x1, . . . , xN.\nWe start with the most basic and still widely-\nused Language Modeling [59] (or next token pre-\ndiction) objective. Here, we learn parameters \u03b8 by\nmaximizing the likelihood of the next token given\nthe previous tokens,\nL(x) =\nN\nX\ni=1\nlog P(xi|x1, . . . , xi\u22121; \u03b8).\n(1)\nMasked\nLanguage\nModeling\n(MLM;\nor\nCloze) [549, 120]\nhides a set proportion of\ntokens in the sequence by replacing them with a\nspecial [MASK] token. The literature employs\nthe MLM objective for non-autoregressive, i.e.,\nnon-generative,\nbidirectional context models,\nwhere the model uses tokens before and after the\ntarget token for predictions, leveraging a more\nholistic understanding of its context than the NTP\nobjective. Furthermore, we can use each input\nsentence to predict multiple masked tokens in a\nsingle pass, while the NTP objective typically\nlearns from predicting one token at a time.\nLet xMASK denote the set of indices of the\nmasked tokens and x\u00acMASK the unmasked tokens.\nThe objective of MLM is then to maximize the\nlikelihood given the parameters \u03b8,\nL(xMASK|x\u00acMASK) =\n1\n|xMASK|\n\u00b7\nX\ni\u2208xMASK\nlog P(xMASKi|x\u00acMASK; \u03b8).\n(2)\nPatel et al. [410] show that such models produce\nrepresentations more suitable for transfer learning;\nhowever, they come with difficulties in performing\nin-context learning (Sec. 2.7).\nTo further improve the training efficiency of the\nMLM objective, Bajaj et al. [33] propose to replace\ninput tokens with ones generated by an auxiliary\nlanguage model (ALM), resulting in a Model gen-\nerated dEnoising TRaining Objective (METRO).\nTheir approach consists of roughly three compo-\nnents: (i) train an ALM using the MLM objec-\ntive, (ii) given some inputs with masked positions,\npredict the tokens (with the ALM), (iii) train the\nmain model to correct these tokens inserted in the\nmasked positions, i.e., 1) predict whether the ALM\nhas replaced a token and if so, 2) predict the origi-\nnal token. They train the auxiliary and main model\njointly.\nPrefix Language Modeling [443] generalizes\nlanguage modeling by allowing prefix tokens with a\nbidirectional receptive field to be added to the input\n(without prefix, it is equivalent to standard LM).\nNote that this is still different from the bidirectional\ncontext as in MLM, where we always condition on\nall the tokens before and after the masked ones (see\nFig. 3 left). For computing the hidden states of the\nprefix, prefix-LM attends to tokens before and after\n(see Fig. 3 right).\nSpan Corruption [303, 443, 132] or span de-\nnoising refers to a group of denoising objectives\nthat generalize MLM to denoise contiguous se-\nquences of tokens within a given text, called spans.\nThe denoising objectives typically replace the sam-\npled spans with a single unique masking token\nand train the model to fill it in. Raffel et al. [443]\n7\nInputs\nTargets\nArgument is that some argue that if an AI can simulate human behavior \n(qualia), the \"what it feels like\" aspect of consciousness. The Simulational \nconsidered conscious. However, this view doesn't account for subjective\ninputs and generate outputs similar to a conscious being, then it could be \nunderlying physical structure. In other words, if an AI can respond to \nview that mental states are de\ufb01ned more by their function than their \nSome proponents of AI consciousness subscribe to functionalism, the \n4\n3\n2\n4\n3\n2\n Span Corruption\n(R-Denoising)\nInputs\nTargets\nArgument is that some argue that if an AI can simulate human behavior \n(qualia), the \"what it feels like\" aspect of consciousness. The Simulational \nconsidered conscious. However, this view doesn't account for subjective\ninputs and generate outputs similar to a conscious being, then it could be \nunderlying physical structure. In other words, if an AI can respond to \nview that mental states are de\ufb01ned more by their function than their \nSome proponents of AI consciousness subscribe to functionalism, the \n12\nLong Span Corruption\n(one form of X-Denoising)\n13\n14\n12\n13\n14\nMeet In The Middle\nInputs\nTargets\nArgument is that some argue that if an AI can simulate human behavior \n(qualia), the \"what it feels like\" aspect of consciousness. The Simulational \nconsidered conscious. However, this view doesn't account for subjective\ninputs and generate outputs similar to a conscious being, then it could be \nunderlying physical structure. In other words, if an AI can respond to \nview that mental states are de\ufb01ned more by their function than their \nSome proponents of AI consciousness subscribe to functionalism, the \n56\n56\nInputs (Reversed Order)\nTargets\nArgument is that some argue that if an AI can simulate human behavior \n(qualia), the \"what it feels like\" aspect of consciousness. The Simulational \nconsidered conscious. However, this view doesn't account for subjective\ninputs and generate outputs similar to a conscious being, then it could be \nexperiences subjective for account\nSimulational The consciousness. of aspect \u201clike feels it what\u201d the (qualia),\nbehavior human simulate can AI an if that argue some that is Argument \n52\n52\nInputs\nTargets\nArgument is that some argue that if an AI can simulate human behavior \n(qualia), the \u201cwhat it feels like\u201d aspect of consciousness. The Simulational \nconsidered conscious. However, this view doesn't account for subjective\ninputs and generate outputs similar to a conscious being, then it could be \nunderlying physical structure. In other words, if an AI can respond to \nview that mental states are de\ufb01ned more by their function than their \nSome proponents of AI consciousness subscribe to functionalism, the \nFill In The Middle\n                             26      \n26\nMove\nInputs\nTargets\nArgument is that some argue that if an AI can simulate human behavior \n(qualia), the \"what it feels like\" aspect of consciousness. The Simulational \nconsidered conscious. However, this view doesn't account for subjective\ninputs and generate outputs similar to a conscious being, then it could be \nunderlying physical structure. In other words, if an AI can respond to \nview that mental states are de\ufb01ned more by their function than their \nSome proponents of AI consciousness subscribe to functionalism, the \nPre\ufb01x Language Modeling\n (S-Denoising)\n56\n56\nFigure 4: Self-Supervised Data Construction by Pre-Training Objectives, adopted from Tay et al. [545]. We\nindicate masked tokens with gray rectangles, which become the targets. For brevity, we omit special tokens.\nshows that this can speed up training because span\ncorruption produces shorter sequences on average\ncompared to corrupting individual tokens in an i.i.d.\nmanner.\nMixture of Denoisers [545] (MoD) refers to\ninjecting objective diversity by mixing multiple\ndenoising objectives. Tay et al. [545] categorize\nthree denoising objectives: {R,S,X}-Denoiser. The\nregular denoising corresponds to the previously in-\ntroduced span denoising. Specific denoising com-\nprises splitting a given sequence into a prefix act-\ning as the context and a suffix acting as the target.\nIn extreme denoising, we corrupt large parts of\nthe input by either (a) increasing the proportion\nof masked tokens per span or (b) increasing the\nspan length forcing the model to generate long se-\nquences with limited context, which we illustrate\nin Fig. 4). The MoD objective has subsequently\nbeen shown to improve model performance by con-\ntinuing training pre-trained LLMs [443, 86] for\nrelatively few steps [547].\nFill In the Middle Bavarian et al. [38] propose\nto augment the next token prediction objective by\nshuffling tokens within a document such that we\nfill in the middle (FIM) based on prefix and suf-\nfix. They demonstrate that models pre-trained on a\nmixture of FIM-transformed and left-to-right data\nresult in left-to-right and FIM capability models.\nMeet in the Middle Nguyen et al. [382] extend\nthe FIM objective by enabling bidirectional context\nto construct a denser, more data-efficient supervi-\nsion signal while maintaining the autoregressive\n8\nnature of the underlying model: They train two\ndecoders\u2014one forward \u2212\u2192p (xi | x<i; \u03b8) and one\nbackward language model \u2190\u2212p (xi | x<i; \u03b8)\u2014with\nshared parameters \u03b8. Additionally, they add an\nagreement regularize to the loss, encouraging the\nforward and backward model to agree: for a dataset\nS of sequences, the full pre-training loss is\nX\nx\u2208S\n|x|\nX\ni=1\n\u2212 log \u2212\u2192p (xi | x<i; \u03b8)\n|\n{z\n}\nNLL for forward model\n\u2212 log \u2190\u2212p (xi | x>i; \u03b8)\n|\n{z\n}\nNLL for backward model\n+\u03b2DTV\ni,x (\u2212\u2192p \u2225\u2190\u2212p )\n|\n{z\n}\nagreement regularizer\n,\n(3)\nwhere DTV\ni,x (\u2212\u2192p \u2225\u2190\u2212p ) is the total variation distance\namong the two models on the i-th token. Once\npre-training has been completed, we can use only\nthe forward model \u2212\u2192p .\nParallelism Strategies\nThe sheer size of LLMs\nmakes it hard to train or even do inference with\nthem on only one accelerator (GPU, TPU, etc.).\nA common solution is model parallelism, which\ncan be viewed as a divide-and-conquer strategy:\nwe slice up various parts of the model (dividing\nthe problem into sub-problems), distribute them\nacross multiple devices, with each device comput-\ning a portion of the overall computation (solve each\nproblem independently) and combine all results to\nproduce the final output (forward/backward pass).\nImplementing model parallelism synchronously\ncreates a problem where running data batches\nthrough multiple workers with sequential depen-\ndency (each layer depends on results from the pre-\nvious layer) leads to significant waiting times and\nunder-utilization of computation resources.\nAnother strategy is pipeline parallelism, which\ncombines model parallelism with data parallelism,\nmeaning that we not only distribute parts of the\nmodel across different devices but parts of the data\ntoo, i.e., each worker splits its mini-batch further\ninto micro-batches with gradients being accumu-\nlated across all micro-batches before the weight\nupdate. Huang et al. [226] instantiate such an ap-\nproach called GPipe, which divides each mini-\nbatch into smaller micro-batches distributed across\ndifferent accelerators simultaneously; gradients are\napplied synchronously at the end. Compared to\nnaive model parallelism, this decreases waiting\ntimes and increases the utilization of computational\nresources.\nThese issues have motivated asynchronous paral-\nlelization schemes. Recht et al. [453] present Hog-\nwild!, which greedily applies gradients to the local\nweights on each accelerator as soon as they arrive,\noffering better resource utilization than pipeline\nparallelism but suffering from training instabilities\ndue to stale gradients which are based on outdated\nmodel weights.\nGomez et al. [172] propose N-Wise interlock-\ning backpropagation, which is a generalization of\nend-to-end and local training. While end-to-end\n(global) training performs a forward pass through\nall layers, computes a loss and gradients, and back-\npropagates through all layers, local training per-\nforms forward passes through all layers individ-\nually and immediately computes a local loss and\ngradient update, offering higher resource utilization\nat the cost of (empirically) worse task performance.\nN-Wise interlocking backpropagation strikes a com-\npromise by performing a forward pass through N\nlayers before computing a loss and updating the\nparameters of the associated layers, enabling better\nlayer communication than local training and higher\ncomputational efficiency than end-to-end training.\nChowdhery et al. [86] leverage a combination\nof model parallelism and fully sharded data par-\nallelism (FSDP) [628, 674]\u2014a technique where\neach device only holds a subset of the model pa-\nrameters, gradients, and optimizer states, and pa-\nrameters necessary for local computations are com-\nmunicated on-demand\u2014to enable highly parallel,\nhigh throughput training across thousands of chips\nwithin a single TPU pod. PaLM further employs\ndata parallelism to achieve scaling at pod level,\nleveraging the Pathways [37] system to distribute\ndata.\nIn a parallel line of work, Lepikhin et al. [298]\npropose GShard, a model parallelism method that\nextends the XLA [468] compiler, enabling auto-\nmatic sharding of models.\nMiscellaneous\nRae et al. [441] stack the lay-\ners of a 4.5B parameter model to jump-start and\naccelerate the training of a 9B model, which led\nto a 40% reduction in compute; an idea that has\nbeen previously used for training smaller-scale\nLMs [173]. Brown et al. [59] progressively in-\ncrease the batch size from a small to the full value\nover training when training GPT-3; a trick that\nhas been previously used for training image mod-\n9\nels [514]. Sanyal et al. [476] apply latest weight av-\neraging [249] to LLMs between 1 and 12B param-\neters; for a 6.9B parameter model, they reach sav-\nings of up to 4,200 GPU hours. For smaller-scale\nmodels, there exist various pre-training speedup al-\ngorithms [663, 685], but they have not been scaled\nup yet and shown to offer only limited gains when\ncompared with budget-adjusted baselines [251].\n2.4\nFine-Tuning Overhead\nA potential drawback of pre-training LLMs on mas-\nsive and diverse sets of textual data is that the re-\nsulting models might struggle to explicitly cap-\nture the distributional properties of task-specific\ndatasets.\nTo address this, fine-tuning refers to\nadapting the pre-trained model parameters on com-\nparatively smaller datasets that are specific to an\nindividual domain or task. LLM fine-tuning is\nhighly effective at adapting LLMs for downstream\ntasks [215, 120, 440].\nTechnically\nspeaking,\nfine-tuning\ncan\nbe\nachieved by further training a model on a smaller\ndataset. Depending on the model architecture, this\nis done by either (i) directly fine-tuning pre-trained\nmodels using a standard language modeling objec-\ntive or (ii) adding individual learnable layers to the\noutput representations of a pre-trained language\nmodel, which are designed to create compatibil-\nity between the model\u2019s output representations and\nthe output formats of individual downstream tasks\n(e.g., for text classification or sequence labeling).\nSee Devlin et al. [120] (Figure 1) for an illustration.\nHowever, LLMs with billions of parameters have\nlarge memory requirements to store (i) the model\nparameters, (ii) the model activations, and (iii) the\ngradients and corresponding statistics. Due to lim-\nited device memory (e.g., GPU or TPU) necessi-\ntates access to large clusters with many devices\nto fine-tune a full LLM, limiting access to a few\ninstitutions with large compute resources.\no Large Memory Requirements\nFine-tuning entire LLMs requires the same\namount of memory as pre-training, render-\ning it infeasible for many practitioners.\nMoreover, while full model fine-tuning is ef-\nfective at adapting LLMs to perform well on spe-\ncific downstream tasks, individual copies of fine-\ntuned LLMs need to be stored and loaded for\nindividual tasks, which is computationally ineffi-\ncient [213, 311] and requires practitioners to keep\nindividual fine-tuned LLMs in memory for every\ntask. We illustrate this overhead in Figure 5.\no Overhead of Storing and Loading\nFine-Tuned LLMs [213, 311]\nWhen adapting an LLM via full-model fine-\ntuning, an individual copy of the model\nmust be stored (consuming data storage) and\nloaded (expending memory allocation, etc.)\nfor each task.\nParameter-efficient fine-tuning\nAn alternative\nmethod to adapt an LLM to a specific dataset/do-\nmain is via parameter-efficient fine-tuning (PEFT).\nPEFT refers to a class of methods that adapt LLMs\nby updating only a small subset of model parame-\nters. Adapters [213] are one of the earliest works\non PEFT. This method incorporates additional,\nlearnable layers into a Transformer architecture that\nare updated during fine-tuning whilst keeping the\nremainder of the network unchanged. Experimen-\ntal results on 26 text classification tasks (incl. the\nGLUE benchmark [575]) reveal that models trained\nvia Adapters are competitive with full fine-tuning\nwhile updating only 3% of the model\u2019s parame-\nters. Ben Zaken et al. [40] instead propose only\nto update the model\u2019s bias terms for fine-tuning,\nwhich make up less than 1% of the model\u2019s pa-\nrameters. Experimental results show competitive\nperformance across tasks of the GLUE benchmark.\nWe are aware of three general frameworks for incor-\nporating adapters into language model fine-tuning,\nnamely AdapterHub [428], LLM-Adapters [219],\nand HuggingFace\u2019s PEFT library [356].\nPEFT methods introduced for larger mod-\nels include prefix-tuning [311] and prompt-\ntuning [299], which both operate by prepending\na set of learnable token embeddings to an input.\nThese token embeddings (also referred to as soft\nprompts [299]) are learned during the fine-tuning\nstage, whereas the remainder of the model parame-\nters remains fixed. Most notably, such soft prompts\ncontain thousands rather than millions of param-\neters and are much more efficient to store. No-\ntably, one still has to backpropagate through the\nnetwork while fine-tuning the tokens. Alternatives\nfor models with only black-box API access have\nbeen proposed too [528, 122].\nIt has been shown that prompt-tuning can\nlearn generalizable representations with very small\n10\nFi n e-t u n i n g \nLLM  # 2\nFi n e-t u n i n g \nLLM  # 1\nFi n e-t u n i n g \nLLM  # 3\nSen t i m en t  \nm odel\nQA \nm odel\nH at e speech  \nm odel\nSen t i m en t  \nan al ysi s t ask\nQu est i on  \nan swer i n g t ask\nH at e speech  \nt ask\n(a)\nB ase LLM  \n(PEFT-adapt abl e)\nPEFT wei gh t s\nSen t i m en t  \nan al ysi s t ask\nPEFT wei gh t s\nPEFT wei gh t s\nSen t i m en t  \nm odel\nQA \nm odel\nH at e speech  \nm odel\nQu est i on  \nan swer i n g t ask\nH at e speech  \nt ask\n(b)\nFigure 5: Fine-tuning an LLM for a specific down-\nstream task. (a) illustrates vanilla fine-tuning, which\nrequires updating the entire model, resulting in a new\nmodel for each task. In (b), PEFT instead learns a small\nsubset of model parameters for each task with a fixed\nbase LLM. The same base model can be re-used during\ninference for different tasks.\namounts of training data, achieving competitive\nperformances when trained on less than 100 exam-\nples for safety classification [376] or five examples\nfor multilingual question answering [11]. In addi-\ntion to that, recent work investigates the potential\nof using soft prompts for pre-training and transfer\nlearning across different tasks [179, 572].\nLiu et al. [331] introduce (IA)3, which scales\nactivations in individual Transformer layers with\nlearnable vectors. The authors demonstrate its ef-\nfectiveness by showing that models trained using\n(IA)3 outperform full model fine-tuning on various\ndatasets whilst updating only 0.01% of the model\u2019s\nparameters.\nMalladi et al. [355] propose a memory-efficient\nzeroth-order (MeZO) optimizer, which only re-\nquires the same memory footprint as during in-\nference (instead of storing gradients or optimizer\nstates). Further, it can optimize non-differentiable\nobjectives like accuracy or F1 scores, which con-\nventional gradient-based tuning methods cannot.\nHu et al. [218] propose Low-Rank Adaptation\n(LoRA), which formulates parameter updates of\nweight matrices at individual Transformer layers as\nan additive low-rank decomposition. Such a repa-\nrameterization avoids the need to compute dense\nmatrix multiplications. Dettmers et al. [118] ex-\ntend LoRA to quantized LLMs, drastically reduc-\ning memory usage, allowing them to fine-tune a\n65B model on a single 48GB GPU. The authors\nmention that regular training of the same model\nrequires more than 780 GB of GPU memory.\nCompute Requirements\nHowever, despite sub-\nstantial improvements in memory complexity\nneeded to fine-tune LLMs for specific tasks, a re-\nmaining challenge is the time complexity. Fine-\ntuning an LLM, even with PEFT methods, still\nrequires full gradient computation. The compu-\ntational infrastructure needed to adapt LLMs pro-\nhibits potential applications like personalization on\nsmaller devices.\no Full Matrix Multiplications\nParameter-efficient fine-tuning of LLMs\nstill requires computing full forward/back-\nward passes throughout the whole network.\n2.5\nHigh Inference Latency\nAccording to Pope et al. [431], Weng [605], two\nreasons why LLMs exhibit high inference latencies\nare: (1) low parallelizability since the inference\nprocedure proceeds one token at a time and (2)\nlarge memory footprints, due to the model size\nand the transient states needed during decoding\n(e.g., attention key and value tensors). Further, the\nauthors also discuss the quadratic scaling of the\nattention mechanisms in Transformers, which we\ndiscuss separately in Sec. 2.6.\no High Inference Latency [431, 605]\nLLM inference latencies remain high be-\ncause of low parallelizability and large mem-\nory footprints.\nIn the following section, we review techniques\nused to address these challenges by e.g., reduc-\ning the memory footprint (size and/or bandwidth),\nor accelerating specific computational operations.\nNote that some of these techniques may also be\napplicable during the training process, but we dis-\ncuss them here since they are not only designed for\ntraining, like the approaches discussed in Sec. 2.3.\n11\nEfficient Attention\nRoughly two lines of work\naim to accelerate attention mechanism computa-\ntions by (i) lower-level hardware-aware modifica-\ntions or (ii) higher-level sub-quadratic approxima-\ntions of the attention mechanism.\nFor the former, multi-query attention [493] aims\nto reduce memory bandwidth bottlenecks when se-\nquentially generating sequences of tokens using\nTransformer decoder layers by keeping only one\nattention head for the key and value tensors. Sim-\nilarly, Dao et al. [107], Pagliardini et al. [404] re-\nduce memory bandwidth by proposing an alter-\nnative computation method for multi-head self-\nattention, called FlashAttention, to minimize\nthe number of I/O operations to speed up the com-\nputation on modern GPUs. As an optimized atten-\ntion implementation, FlashAttention lever-\nages operator fusion to reduce the memory band-\nwidth bottleneck. Pagliardini et al. [404] build\non top of FlashAttention and incorporate at-\ntention sparsity patterns, encompassing key/query\ndropping and hashing-based attention. Pope et al.\n[432] implement different sharding techniques to\nefficiently spread the feedforward and attention\ncomputations across devices while optimizing for\ninter-device communication costs, enabling context\nlengths of up to 43,000 tokens using multi-query\nattention.\nWith regards to the second stream of work, a\ncommon theme to improve the computational or\nmemory complexity of the attention mechanism is\nto sparsify the attention matrix or introducing (lin-\near) approximations [543]. However, the scalabil-\nity of some efficient Attention approximations has\nbeen questioned. For example, Tay et al. [542], Hua\net al. [220] find that the Performer attention approx-\nimation [85] severely underperforms the vanilla\nself-attention mechanism, especially when scaled\nup to large models.\nQuantization\nis a post-training technique that\nreduces the memory footprint and/or increases the\nmodel\u2019s throughput by reducing the computational\nprecision of weights and activations. nuQmm [407]\nand ZeroQuant [643] use a non-uniform quan-\ntization method to quantize weights and apply\ncustom CUDA kernels for computational benefits.\nLLM.int8() [117] is a degradation-free quanti-\nzation scheme enabling efficient inference of multi-\nbillion parameter LLMs by utilizing Int8 quantiza-\ntion and falling back to higher precision for certain\noutlier features without the need for re-training.\nSimilarly, GLM-130B [658] uses a degradation-\nfree 8-bit quantization scheme, storing weights in\n8-bit and performing matrix multiplications in 16-\nbit precision. Frantar et al. [153] propose an effi-\ncient, one-shot quantization technique to compress\nLLM weights down to 3 to 4 bits per weight, en-\nabling 175B parameter models to be run on a single\nGPU. Dettmers et al. [119] further improve upon\nthis by combining higher precision representations\nfor outlier weights and grouped quantization.\nPruning\nis a complementary post-training tech-\nnique to quantization, removing parts of the\nweights of a given model (without degrading its per-\nformance). An important distinction is whether the\npruning follows a structured pattern or is unstruc-\ntured. Structured sparse models substitute dense\nsections of a model with an assembly of signifi-\ncantly smaller yet still dense components. Unstruc-\ntured sparse models contain weights of value zero,\nwhich do not influence the network\u2019s behavior and\ncan therefore be committed in theory. However, in\npractice, it is more challenging to translate theo-\nretical to practical computation savings on current\nhardware [161, 112, 336].\nOn the structured side, early work on pruning\nlanguage models mainly aims at comparatively\nsmall MLM-type models [592, 143, 243]. Ma et al.\n[349] propose LLM-Pruner, which aims at pruning\nLLMs in a task-agnostic manner while preserving\nthe zero-shot capabilities of the models. To this\nend, LLM-Pruner adopts a three-stage pruning pro-\ncedure where 1) interdependent structures within\nthe model are identified and grouped, 2) the contri-\nbution to the overall performance is estimated for\neach group, and low-performing groups are pruned,\n3) performance recovery via parameter-efficient\nfine-tuning procedure using LoRA [218].\nOn the unstructured side, SparseGPT [152] is an\nunstructured pruning approach specifically devel-\noped to be fast enough to be run on LLMs with\nhundreds of billions of parameters within a few\nhours, being able to prune the number of parame-\nters by up to 60% while maintaining roughly the\nsame model performance. Sun et al. [527] pro-\npose Wanda (Pruning by Weights and activations),\nwhich applies magnitude pruning based on the\nproduct of each weight\u2019s magnitude and the norm\nof the corresponding input activations, matching\nSparseGPT in performance while requiring only\na single forward pass to prune the network. Both\nSparseGPT and Wanda can be extended to per-\n12\nform semi-structured pruning, enabling n:m spar-\nsity [228, 680] and achieving the corresponding\nspeed-ups on recent GPUs [369].\nMixture-of-Experts\narchitectures typically con-\nsist of a set of experts (modules), each with unique\nweights, and a router (or gating) network, which\ndetermines which expert module processes an in-\nput. MoE models decrease inference time by not\nusing all experts at once but only activating a sub-\nset of them. Further, they can reduce communica-\ntion across devices in model-distributed settings by\nplacing each expert on a separate accelerator; only\nthe accelerators hosting the router and the relevant\nexpert model must communicate. Shazeer et al.\n[495] propose one of the first MoE layers embed-\nded within a language model, which they refer to\nas sparsely-gated MoEs (SG-MoEs). They denote\nby G(x) and Ei(x) the gating network output and\nthe i-th expert network output for a given input\nx, respectively. We can then write the output as\ny = Pn\ni=1 G(x)iEi(x). Wherever G(x)i = 0,\nwe do not need to compute Ei(x), thereby saving\ncompute during inference. Lepikhin et al. [298]\nscale up an SG-MoE model to 600B parameters\nby proposing GShard, a model parallelism method\nthat extends the XLA [468] compiler. While SG-\nMoE selects the top-k experts with k > 1, the\nSwitch Transformer (ST) [145] architecture uses\nk = 1 experts, which reduces routing computation\nand communication across experts (which may be\nlocated on different accelerators). ST empirically\noutperformed a strongly tuned T5 model with up to\n7x pre-training speedups. Lewis et al. [302] notice\nthat the learned routers can result in unbalanced\nassignments across experts. To ensure balanced\nrouting, they formulate a linear assignment prob-\nlem that maximizes token-expert affinities while\nequally distributing the number of tokens across\nexperts. Yu et al. [653] propose sMLP, an MoE\nusing only MLPs blocks, which (i) they scale up to\n10B, (ii) results in a 2x improvement in pre-training\nspeed, and (iii) outperforms sparse Transformer\ncounterparts.\nHowever, MoE models still suffer from unique\nissues like expert collapse (all experts learning the\nsame), likely caused by underconstrained routing\nfunctions [80]. For example, Roller et al. [459]\ndemonstrates that learned expert assignments do\nnot always outperform random ones.\nInterestingly, instead of designing an architec-\nture for sparsity explicitly, Li et al. [314] observe\nthat the activation maps of default Transformer\nmodels often emerge to be very sparse implicitly;\nthe larger the model, the sparser measured by the\npercentage of nonzero entries. Similarly, Zhang\net al. [670] find that post-training MoEfication, i.e.,\nconverting monolithic models to equivalent MoE\nmodels, can speed up inference by 2x.\nCascading\nrefers to the idea of employing\ndifferently-sized models for different queries [75].\nIn spirit, this idea is similar to Mixture-of-Experts\nmodels, but instead of learning a routing module,\nwe employ a cascade of multiple, differently-sized\nmonolithic models (these can be even black-box\nAPI models) and learn a scoring function that de-\ncides which model(s) receive which query. Chen\net al. [75] demonstrate that this strategy dominates\nthe Pareto frontier between accuracy and cost.\nDecoding Strategies\ncan greatly impact the com-\nputational cost of performing inference. For ex-\nample, beam search trades off compute for higher-\nquality results. Another example of a computa-\ntionally expensive decoding scheme is sample-and-\nrank [8] where N independent sequences of tokens\ny1, . . . , yN are obtained using random sampling,\nand the highest probability sequence is used as the\nfinal output.\nLatency-oriented strategies such as speculative\nsampling [522, 300, 74] first autoregressively gen-\nerate a draft of length K using a smaller (draft)\nmodel; then, the larger (target) model scores the\ndraft, followed by a modified rejection sampling\nscheme to accept a subset of the tokens from left to\nright. Similar ideas have been proposed in various\ncontexts, such as for blockwise parallel genera-\ntion [522], grammatical error correction [529], and\nwith a larger LLM refining generation produced by\na small model [265]. Del Corro et al. [114] observe\nthat tokens towards the end of a sequence are easier\nto predict due to more contextual information, mo-\ntivating a new decoding strategy that skips earlier\nlayers in the network for such tokens.\n2.5.1\nSoftware\nVarious frameworks have been designed to en-\nable the efficient training of multi-billion to\ntrillion parameter language models such as\nDeepSpeed [450] and Megatron-LM [501] to\naccount for the unique challenges arising when\ntraining such models. This is necessitated by the\nfact that most LLMs do not fit into a single device\u2019s\n(GPU, TPU) memory, and scaling across GPUs and\n13\ncompute nodes needs to account for communica-\ntion and synchronization costs. FlexGen [497]\nprovides further speed-ups by aggregating memory\nand compute resources from the GPU, CPU, and\ndisk and utilizing techniques such as 4-bit quan-\ntization, enabling inference with 175B parameter\nmodels on a single GPU.\nThe frameworks typically combine existing par-\nallelism strategies to compensate for drawbacks\nand scale model training across multiple sets of\ncompute nodes, within compute nodes, and across\nmultiple GPUs per node. e.g., Smith et al. [515]\nuse tensor slicing within a node, pipeline paral-\nlelism across nodes, and data parallelism to train\nmultiple model replicas over sets of nodes. Addi-\ntional features include memory optimizations [445,\n454, 446], communication-efficient [536, 307, 343]\nand fused optimizers2, and support for MoE train-\ning [444].\nSpecialized\nimplementations\nsuch\nas\nTutel\n[230]\nand\nMegaBlocks\n[160]\nof-\nfer\nefficient\nsparse\nMoE\ntraining,\nwhile\nAlpa [677] enables automatic data and model\nparallelism for LLMs written in Jax.\nThe\nFasterTransformer3 library includes highly\noptimized Transformer encoder and decoder\nimplementations for TensorFlow, PyTorch, and\nTriton.\nKwon et al. [285] introduce vLLM, an open-\nsource library for efficient inference and LLM serv-\ning. vLLM employs PagedAttention, which par-\ntitions each sequence\u2019s KV cache into fixed-size\nblocks. When performing attention computations,\nblocks are fetched from non-contiguous memory.\nThis enables memory sharing, reducing memory\nconsumption and transfers in decoding strategies\nsuch as beam search, ultimately improving through-\nput.\nThe Petals [54] library4 allows users to col-\nlaboratively fine-tune and run LLMs by distribut-\ning subsets of model parameters to individual ma-\nchines.\nAll of these libraries address the enormous com-\nputational costs associated with training and run-\nning LLMs, either by offering more efficient im-\nplementations, lowering memory requirements, or\nusing distributed or decentralized computing strate-\ngies.\n2https://github.com/nvidia/apex\n3https://github.com/NVIDIA/FasterTransformer\n4https://github.com/bigscience-workshop/petals\n2.6\nLimited Context Length\nAddressing everyday NLP tasks often necessitates\nan understanding of a broader context. For exam-\nple, if the task at hand is discerning the sentiment\nin a passage from a novel or a segment of an aca-\ndemic paper, it is not sufficient to merely analyze a\nfew words or sentences in isolation. The entirety of\nthe input (or context), which might encompass the\nwhole section or even the complete document, must\nbe considered. Similarly, in a meeting transcript,\nthe interpretation of a particular comment could\npivot between sarcasm and seriousness, depending\non the prior discussion in the meeting.\nLi et al. [308] evaluate several LLMs in the long-\ncontext settings and find that while commercial\nclosed-API models often fulfill their promise, many\nopen-source models \u2013 despite claiming to perform\nwell with longer contexts \u2013 exhibit severe perfor-\nmance degradation. They point out that there is\na difference between being architecturally-able to\ndeal with long inputs and actually performing well.\nHaving an architecture that can infer long inputs\ndoes not guarantee that the LLM will perform as\nwell on those as on shorter inputs. Similarly, Liu\net al. [333] find that changing the location of rel-\nevant information in the input can degrade model\nperformance. Interestingly, they find that decoder-\nonly LLMs like GPT-3.5 can deal well with such\ninformation at the beginning or end of the input\ncontext; they cannot access information in the mid-\ndle of it well, resulting in a U-shaped performance\ncurve.\no Limited Context Length\nLimited context lengths are a barrier for\nhandling long inputs well to facilitate ap-\nplications like novel or textbook writing or\nsummarizing.\nTo this end, we discuss three lines of work per-\nmitting longer context lengths. First, we look at\nefficient attention mechanisms, which help miti-\ngate the effect of long inputs on the computational\nrequirements of Transformer models. Next, we ex-\namine positional embedding schemes in the light\nof generalization to longer sequence lengths than\nthose used during training. Lastly, we revise Trans-\nformer alternatives which neither require attention\nnor positional embeddings.\n14\nEfficient Attention Mechanisms\nOne way of\naddressing the limited context of LLMs is by de-\nsigning more efficient attention mechanisms that\ncan process longer inputs. Ma et al. [350] intro-\nduce Luna, a linear unified nested attention mech-\nanism that approximates softmax attention with\ntwo nested linear attention functions, yielding only\nlinear (as opposed to quadratic) time and space\ncomplexity, allowing it to process much longer in-\nputs. Similarly, Shen et al. [496] and Li et al. [310]\npresent alternative attention mechanisms equivalent\nto the dot-product attention but which require sub-\nstantially less memory and compute resources. Guo\net al. [183] propose an attention mechanism called\nTransient Global, which is an extension of local\nattention where each token can attend to nearby\ntokens and a set of global tokens. It enables to han-\ndle sequences with up to 12,000 tokens. Similarly,\nCoLT5 [15] enables context lengths of up to 64,000\ntokens by splitting the computations into a light\nbranch with local attention, fewer attention heads,\nand a heavy branch with full attention. CoLT5 ap-\nplies the light branch to every token and the heavy\nbranch to a subset of tokens that are selected by a\nlearnable routing function.\nAfter investigating the effect of the dot-product\nself-attention mechanism, Tay et al. [541] pro-\npose the Synthesizer, a new architecture that learns\nsynthetic attention weights without token-token\ninteractions, showing that it consistently outper-\nforms transformers on various language-based\ntasks. Britz et al. [56] offer an alternative attention\nmechanism based on a fixed-size memory repre-\nsentation that is more efficient, yielding inference\nspeedups of 20% without significantly hurting per-\nformance. Hua et al. [220] combine a single-head\nattention mechanism with a linear attention approx-\nimation to achieve speed-ups between 4.9x and\n12.1x for auto-regressive language modeling while\nobtaining similar perplexities as a standard Trans-\nformer model. Ding et al. [124] propose dilated\nattention which splits a sequence into equally long\nsegments and processes each of these in parallel\nusing a sparsified attention mechanism. Dilated\nattention offers a linear computational complexity\nin the sequence length and, applied hierarchically,\nenables inputs of up to 1B tokens.\nLength Generalization\nAs the required compute\nof Transformer-based LLMs grows quadratic with\nthe sequence length, it is a desired property to build\nLLMs that can be trained on short sequences and\ngeneralize well to significantly longer sequences\nduring inference.\nThe fundamental building block of the Trans-\nformer architecture is the self-attention mechanism.\nIt is permutation-invariant; therefore, the output is\nindependent of the input sequence order. Positional\ninformation is commonly injected to make the\nmodel respect a token\u2019s position in the sequence,\ni.e., capture the semantics of where a token occurs\nrather than just whether it occurs. The longer the\ninput is, the more important the positional embed-\nding becomes since the model needs to effectively\nuse information from different parts of the input\nthat may cover a wide range of distances from the\ncurrent token.\nWithout positional embeddings, a Transformer\nmodels the relations between any two tokens with\nequal probability. Hence, positional embeddings\nintroduce an LSTM-like inductive bias that (typi-\ncally) tokens closer to each other in the sequence\nare more relevant to each other. Depending on the\npositional embedding scheme chosen, this can be\nlearned or effectively hard-coded. However, it re-\nmains unclear what is the most effective positional\nembedding scheme for long inputs. Further, mod-\nels face difficulties generalizing to unseen sequence\nlengths by introducing a dependency on sequence\npositions. This is an undesirable artifact of posi-\ntional embeddings, as language semantics do not\ninherently depend on the length of an utterance.\nWhile positional encoding schemes such as rela-\ntive positional encodings or, more recently, ALiBi\nhave made progress in building more generaliz-\nable ways for injecting positional information into\nTransformers, the challenge of generalizing to se-\nquences much longer than seen during training re-\nmains largely unsolved. Surprisingly, Haviv et al.\n[192] find that causal LLMs without positional en-\ncodings are competitive compared to models with\npositional encodings and accredit this success to\nthe causal attention mask leaking positional infor-\nmation into the model.\nIn the following, we first summarize some stan-\ndard positional embeddings technique and then\nmove to more advanced schemes designed to im-\nprove length generalization. We start with Abso-\nlute Positional Embeddings [563], which inject\npositional information by sinusoidal embeddings\nbased on the absolute position i of a token xi within\ntheir sequence x1, . . . , xN into the model input.\nGiven an input sequence X = [x1, . . . , xN], we\n15\nadd a positional embedding matrix P \u2208 Rn\u00d7d of\nthe same shape to get the positional encoding out-\nputs X + P, where the element on the ith row\nand the (2j)th or the (2j + 1)th column of P fol-\nlows sinusoidal functions.\nVaswani et al. [563]\nalso compare against learned positional embed-\ndings and find no significant performance differ-\nence. In contrast, sinusoidal positional encodings\nrequire no trainable parameters, and the authors\nhypothesize that they enable extrapolation to se-\nquence lengths longer than the ones contained in\nthe training set. However, this feature is not guar-\nanteed, as the subsequent layers in the network\nneed to be able to deal with such extrapolated po-\nsitional embeddings. Learned positional encod-\nings do not possess inherent generalization capabil-\nities for unseen sequence lengths. This limitation\narises because the embeddings associated with ab-\nsolute positions not encountered during training\u2014\ndepending on the implementation\u2014either do not\nexist or remain untrained (random). Relative Posi-\ntional Embeddings have subsequently been devel-\noped, extending absolute positional embeddings to\nrelative offsets between token positions [492, 221,\n105, 79]. While rarely used in their vanilla form in\nLLMs [441], relative positional embeddings have\ngiven rise to the methods outlined in the follow-\ning paragraphs. They offer better generalization to\nunseen sequence lengths than absolute positional\nencodings. All unseen absolute positions will be\nconverted to previously observed relative offsets\nbetween positions, enabling better generalization to\nlong input sequences at inference time. Rotary Po-\nsition Embeddings (RoPE) [526] unite absolute\nand relative methods by incorporating absolute po-\nsitional information in a rotation matrix and model-\ning the relative positional offset through a rotation.\nThey directly modify the self-attention calculation\nrather than injecting positional information into the\nembeddings. The attention between positions i, j\nlinearly depends on i \u2212 j by introducing a d \u00d7 d\ndimensional block diagonal matrix Rd\n\u0398,k, resulting\nin a self-attention mechanism defined as\nsoftmax\n\uf8eb\n\uf8ed 1\n\u221a\nd\nX\ni,j\nx\u22a4\ni W \u22a4\nq Rd\n\u0398,(i\u2212j)Wkxj\n\uf8f6\n\uf8f8 .\n(4)\nWhile RoPE has been adapted in many LLMs [576,\n47, 86] and Su et al. [526] show RoPE leading\nto better performance on long text tasks, Press\net al. [434] demonstrate that this positional en-\ncoding scheme extrapolates poorly to unseen se-\nquence lengths. However, Chen et al. [79] demon-\nstrate that by interpolating rather than extrapolating\nlonger than before observed context windows and\nbriefly fine-tuning RoPE-based models, enabling\npre-trained LLMs to extend their context window\nto very long sizes of up to 32, 768 tokens.\nRelative Positional Bias [443] directly bias the\nattention computation (Eq. (5)) with a learned bias\nper relative positional offset and attention head\ninstead of adding information to the token embed-\ndings\nsoftmax\n\uf8eb\n\uf8ed 1\n\u221a\nd\nX\ni,j\nx\u22a4\ni W \u22a4\nq Wkxj + bi\u2212j\n\uf8f6\n\uf8f8 . (5)\nPress et al. [434] follow a similar methodology\nbut use heuristics to define ALiBi (Attention with\nLinear Biases), a non-learned bias that is used\nto penalize attention scores in long-range interac-\ntions [479], i.e., a recency-bias is backed into the\nmodel. Here, m is a pre-defined, head-specific\nslope\u2013by default, the set of slopes for n heads form\na geometric sequence.\nsoftmax\n\uf8eb\n\uf8ed 1\n\u221a\nd\nX\ni,j\nx\u22a4\ni W \u22a4\nq Wkxj + m \u00b7 \u2212(i \u2212 j)\n\uf8f6\n\uf8f8 .\n(6)\nPress et al. [434] motivate ALiBi by designing it to\ngeneralize well to unseen sequence lengths. They\nshow that training a model with it on training se-\nquences with a maximum sequence length of 1, 024\ntokens achieves the same perplexity on a test set\nwith a maximum sequence length of 2, 048 as a\nmodel trained with sinusoidal positional encodings\non sequences with up to 2, 048 tokens. Thereby, it\nnot only enables larger context lengths but can also\npotentially reduce pre-training costs (Sec. 2.3).\nWhile some of the existing positional encod-\ning schemes offer better generalization to long se-\nquences than others, it remains unclear how reliable\nthey are. For example, Taylor et al. [548] report try-\ning ALiBi in the Galactica LLM and not observing\n\u201clarge gains\u201d compared to using learned positional\nencodings. Similarly, Kazemnejad et al. [259] find\nthat popular positional encoding schemes such as\nALiBi, RoPE, and absolute positional encodings do\nnot perform well in terms of length generalization\nin a suite of 10 reasoning downstream tasks.\nIn a parallel line of work, Anil et al. [19] demon-\nstrate that naively fine-tuning a pre-trained LLM is\n16\ninsufficient for length generalization in the context\nof reasoning tasks. Instead, they propose combin-\ning in-context learning and scratchpad/chain-of-\nthought reasoning to enable LLMs to generalize to\nunseen sequence lengths in- and out-of-distribution,\nwith performance scaling with model size. The au-\nthors report that fine-tuning can further improve\nmodel performance dependent on the task perfor-\nmance of the baseline.\nTransformer Alternatives\nWhile Transformers\nare the dominant paradigm in LLMs today due to\ntheir strong performance, several more efficient\nalternative architectures exist. One line of work\ntries to replace the attention mechanism using state\nspace models (SSMs), which offer near-linear com-\nputational complexity w.r.t. the sequence length.\nDao et al. [108] investigate the weaknesses of state\nspace models (SSMs) in language modeling and\nfind that existing approaches struggle with recall-\ning previous tokens and comparing tokens in the\nsequence. Based on these findings, the authors\npropose H3 with a shift matrix to recall previous\ntokens and multiplicative interactions for token\ncomparisons. The authors demonstrate that H3\ncomes close to Transformer-based LLMs for lan-\nguage modeling, offering further improvements\nwhen combined with attention. Poli et al. [430]\npropose the Hyena operator, a convolution-based\nsub-quadratic attention replacement designed for\nlong sequences. Hyena tries to emulate the atten-\ntion mechanisms\u2019 dynamic nature by introducing\ndata-controlled computations, i.e., Hyena applies\nan element-wise gating operation based on the op-\nerator\u2019s input to mimic the attention contextualiza-\ntion. Hyena-based models have been used on natu-\nral language for sequence lengths of up to 131, 000\ntokens [430] and up to 1, 000, 000 tokens in the\ncontext of genomics [383]. Fathi et al. [144] pro-\npose the Block-State Transformer, which builds\nupon a hybrid layer that combines an SSM for\nlong-range contextualization and a Transformer\nfor short-range interactions between tokens. The\nauthors find similar performance to Transformer-\nbased baselines while obtaining speed-ups of up to\n10x on sequence-level, enabling models with more\nthan 65, 000 tokens sequence length.\nAnother line of work utilizes recurrent neu-\nral networks (RNNs), which offer linear com-\nputational complexity and memory requirements\nwith respect to the sequence length as the back-\nbone of LLMs. Peng et al. [416] propose Recep-\ntance Weighted Key Value (RWKV) to combine\nthe parallelization benefits of Transformer-based\nLLMs during training with the fast inference and\nlow compute requirements of RNNs. The authors\naccomplish this by leveraging a linear attention-\nlike mechanism, scaling non-Transformer LLMs to\n14B parameters, and matching the performance of\nsimilarly-sized Transformer LLMs.\n2.7\nPrompt Brittleness\nA prompt is an input to the LLM. The prompt syn-\ntax (e.g., length, blanks, ordering of examples) and\nsemantics (e.g., wording, selection of examples,\ninstructions) can have a significant impact on the\nmodel\u2019s output [342].\nAs an analogy, if we were to think of an LLM\nas a (fuzzy) database and prompts as queries [246],\nit becomes clear that slight changes in the query\ncan result in vastly different outputs. Consequently,\nthe wording, as well as the order of examples in-\ncluded in a prompt, have been found to influence\nthe model\u2019s behavior significantly [596, 675, 342].\no Prompt Brittleness [675, 596, 342]\nVariations of the prompt syntax, often oc-\ncurring in ways unintuitive to humans, can\nresult in dramatic output changes.\nDesigning natural language queries that steer the\nmodel\u2019s outputs toward desired outcomes is often\nreferred to as prompt engineering [477, 287, 606].\nFig. 6 summarizes some of the most popular\nprompting methods with an example adapted from\nWei et al. [601]. As we can see, there are lots of\nequally-plausible prompting techniques, and the\ncurrent state of prompt engineering still requires\nlots of experimentation, with little theoretical un-\nderstanding of why a particular way to phrase a\ntask is more sensible other than that it achieves\nbetter empirical results. Developing LLMs that are\nrobust to the prompt\u2019s style and format remains\nunsolved, leaving practitioners to design prompts\nad-hoc rather than systematically.\nSingle-Turn Prompting\nmethods improve the in-\nput prompt in various ways to get a better answer in\na single shot. In-Context Learning (ICL) refers\nto an LLM\u2019s ability to learn a new task solely via\ninference (without any parameter updates) by con-\nditioning on a concatenation of the training data\nas demonstrations [59, 483]. This enables users\nand practitioners to use LLMs for a variety of NLP\n17\nSelf-Re\ufb01ne\nChain-of-Thought\nQ: Lisa has 5 easy peelers. She buys 2 more nets with \n6 each. How many easy peelers does she have?\nA: Lisa starts with 5. 2 nets of 6 each are 12 easy \npeelers. 5+12=17. The answer is 17.\nQ: The cafeteria has 37 bananas. They bought 5 more \nbunches with 5 each, how many bananas do they \nhave?\nA:  The cafeteria has 37 bananas originally.  They \nbought 5 more bunches and each bunch has 5, so \nthey added 5 x 5 = 25 bananas to their stock. We \nadd these numbers: 37 + 25 = 62. The answer is 62.\nInstruction-Following\nHere is a mathematical reasoning question. You need \nto apply arithmetic operations to generate the correct \nanswer.  \nQ: Lisa has 5 easy peelers. She buys 2 more nets with \n6 each. How many easy peelers does she have?\n\u2026\nA: The answer is 62.\nTree of Thoughts\nQ: The cafeteria has 37 bananas. They \nbought 5 more bunches with 5 each, how \nmany bananas do they have?\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nThe cafeteria bought 5 more bunches with 5 \neach. Calculate how many they bought in \ntotal.\nPropose Prompt\n5 x 5 = 25\nThought Generation\nEvaluate whether this thought is useful to answer the original question.\nEvaluation Prompt\nYes, this calculation takes us one step closer to the solution.\nThought Evaluation\nSingle-Turn Prompting\nMulti-Turn Prompting\nInput\nOutput\nSelf-Consistency\nQ: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How \nmany easy peelers does she have?\nA: Lisa starts with 5. 2 nets of 6 each are 12 easy peelers. 5+12=17. \nThe answer is 17.\nQ: The cafeteria has 37 bananas. They bought 5 more bunches with \n5 each, how many bananas do they have?\nA:  The cafeteria has 37 bananas \noriginally.  They bought 5 more \nbunches and each bunch has 5, so \nthey added 5 x 5 = 25 bananas to \ntheir stock. We add these \nnumbers: 37 + 25 = 62. The \nanswer is 62.\nA:  The cafeteria initially had 37 \nbananas and purchased an \nadditional 5 bunches of bananas, \neach with 5, totaling 25 bananas. \nSo, adding 5 and 25 together, the \ntotal fruit count is now 30. The \nanswer is 30.\nA:  We need to multiply the \nnumber of bunches by the number \nof banans in each bunch. 5 times 5 \ngives us 25 bananas. Next, we add \nthe original number of bananas. \nThe addition 37 plus 25 equals 62. \nThe answer is 62.\nA:  The answer is 62.\nMajority Vote\nLeast-To-Most\nQ: The cafeteria has 37 bananas. \nThey bought 5 more bunches \nwith 5 each, how many bananas \ndo they have?\nA: To solve \u201cHow many bananas \ndoes it have?\u201d, we need to \ufb01rst \nsolve: \u201cHow many bananas does \nit buy in total\u201d?\nStage 1: Problem Reduction\nStage 2: Sequentially Solve Subquestions\nThe cafeteria has 37 bananas. \nThey bought 5 more bunches \nwith 5 each.\nQ: How many bananas does it \nbuy in total?\nA: They buy 25 bananas in total.\nThe cafeteria has 37 bananas. \nThey bought 5 more bunches \nwith 5 each, how many bananas \ndo they have?\nQ: How many bananas does it \nbuy in total?\nA: They buy 25 bananas in total.\nQ: How many bananas do they \nhave?\nA: The cafeteria has 37 bananas.  \nThey buy 25 bananas in total. \nSo, in total, they have 37 + 25 = \n62 bananas. \nIn-Context Learning\nQ: Lisa has 5 easy peelers. She buys 2 more nets with \n6 each. How many easy peelers does she have?\nA: The answer is 17.\nQ: The cafeteria has 37 bananas. They bought 5 more \nbunches with 5 each, how many bananas do they \nhave?\nA: The answer is 62.\nPrompt tuning\nA: The answer is 62.\nQ: Lisa has 5 easy peelers. She buys 2 more nets with \n6 each. How many easy peelers does she have?\nA: The answer is 17.\nQ: The cafeteria has 37 bananas. They bought 5 more \nbunches with 5 each, how many bananas do they \nhave?\nEmbedding 1\nEmbedding \u2026\nEmbedding N\nAsk-Me-Anything\nFormulate a question for the given context. \nQ: Lisa has 5 easy peelers. She buys 2 more \nnets with 6 each. How many easy peelers does \nshe have?\nA: The answer is 17.\nQ: The cafeteria has 37 bananas. They bought \n5 more bunches with 5 each.\nQ: \nWhat is the total number of bananas they possess?\nAnswer the question using arithmetic.\nQ: Lisa has 5 easy peelers. She buys 2 more \nnets with 6 each. How many easy peelers \ndoes she have?\nA: The answer is 17.\nQ: The cafeteria has 37 bananas. They \nbought 5 more bunches with 5 each.\nQ: What is the total number of bananas \nthey possess?\nA: The answer is 62.\nPrompt Chain 3\nPrompt Chain 2\nPrompt Chain 1\nA1: The answer is 62.\nA2: The answer is 62.\nA3: The answer is 93.\nA: The answer is 62.\nMajority Vote\n\u2026\nQ: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. \nHow many easy peelers does she have?\nA: The answer is 17.\nQ: The cafeteria has 37 bananas. They bought 5 more \nbunches with 5 each, how many bananas do they have?\nThis response is not answering the question asked. The question \nasked is how many banans there are in total. These two \nquantities have to be added together.\nFeedback\nA: Apologies for any confusion, \nyou are right, I was answering \nthe wrong question. The correct \nanswer is 62, by adding 37 and 5 \nx 5. \nRe\ufb01ned Output\nA: The answer is 37.\nFigure 6: Overview of Selected Prompting Methods, categorized into Single-Turn and Multi-Turn Prompting. We\nuse a running example across all methods inspired by Wei et al. [601].\ntasks by simply listing examples of the dataset (e.g.,\ninput texts and their corresponding labels) without\nthe need to adjust the LLM\u2019s inner workings.\nVarious existing works investigate why ICL\nshows such competitive results across NLP tasks.\nOne explanation concurrently proposed by [570,\n103, 16] is that ICL emulates gradient-based meta-\nlearning, i.e., it implicitly fine-tunes the model\nthrough gradient descent in their forward pass.\nInterestingly, Min et al. [366] show that input-\nlabel associations in the few-shot prompt are not\ndecisive for model performance: randomly flip-\nping labels of few-shot demonstrations shows to\nharm an LLM\u2019s ability to solve NLP tasks barely.\nHowever, few-shot learning (with and without ran-\ndom labels) vastly outperforms zero-shot learning\n(i.e., no demonstrations are provided in the prompt).\nThe authors argue that the demonstrations are help-\nful for task performance in that the LLM instead\nlearns the label space and the input distribution of\nthe task.\nIn later work, Pan et al. [405] explain that there\nare two distinct mechanics through which ICL\nleverages demonstrations: on the one hand, task\nrecognition is the ability to recognize a task through\ndemonstrations (possibly without ground-truth la-\nbels or perhaps even wrong ones, as in the case of\nMin et al. [366]). After this recognition phase, it\napplies its pre-trained capabilities. On the other\nhand, the skill to acquire new input-label mappings\nunseen in pre-training is called task learning.\nWhile input-label associations may not seem to\ndrive few-shot performance, at least in the case\nof task recognition, Lu et al. [342] show that the\norder of few-shot examples matters in that LLMs\nare highly sensitive to permutations of the order in\nwhich the few-shot demonstrations are provided.\nAlternative explanations of the ICL phenomenon\ntake place around Bayesian inference [623], sparse\nlinear regression [7], structure induction [188],\nmaintaining coherence [509], kernel regression\n[190], and clone-structured causal graphs [535].\nInstruction-Following is mainly explained in\nSec. 2.9, as it requires supervised fine-tuning. To\nbriefly recap, the idea is to prepend task-describing\ninstructions (e.g., \u201cThis is a text classification task\n18\nfor movie reviews. Here are a few examples: ...\u201d)\nin the input prompts.\nChain-of-Thought (CoT) [327, 601] describes\na technique used to construct few-shot prompts via\na series of intermediate reasoning steps leading\nto the final output. Answer rationales to solve al-\ngebraic problems were originally proposed in the\npre-LLM era [327] and later experienced big pop-\nularity as a prompting strategy for LLMs [601].\nExtensions of chain-of-thought prompting include\nzero-shot variants [273] and automatically gener-\nated series of reasoning steps [671].\nImpersonation [473] is a technique in which\nthe prompt for the model asks it to pretend to be a\ndomain expert when answering a domain-specific\nquestion. Salewski et al. [473] find that LLMs\nanswer domain-specific questions more accurately\nwhen prompted to impersonate a domain expert.\nMulti-Turn\nPrompting\nmethods\niteratively\nchain prompts and their answers together.\nAsk Me Anything [24] uses multiple prompt\ntemplates (called prompt chains), which are used\nto reformat few-shot example inputs into an open-\nended question-answering format. The final output\nis obtained by aggregating the LLMs predictions\nfor each reformatted input via a majority vote.\nSelf-consistency [585] extends chain-of-thought\nprompting by sampling multiple reasoning paths\nand selecting the most consistent answer via a ma-\njority vote.\nLeast-to-Most [682] uses a set of constant\nprompts to use the LLM to decompose a given\ncomplex problem into a series of subproblems.\nThe LLM sequentially solves the subproblems with\nprompts for later-stage subproblems containing pre-\nviously produced solutions, iteratively building the\nfinal output.\nScratchpad [391] is a method to fine-tune LLMs\non multi-step computation tasks such that they out-\nput intermediate reasoning steps, e.g., intermedi-\nate calculations when performing additions, into a\n\u201cscratchpad\u201d before generating the final result.\nReAct [640] combines reasoning and acting by\nprompting LLMs to generate reasoning traces (e.g.,\nChain-of-thought) and action plans, which can be\nexecuted to allow the model to interact with exter-\nnal environments such as Wikipedia to incorporate\nknowledge.\nAutomatic\nReasoning\nand\nTool-Use\n(ART) [406] is a method to automatically\ngenerate multi-step reasoning prompts, including\nsymbolic calls to external tools such as search and\ncode generation or execution. To this end, ART\nretrieves demonstrations of related tasks from\na library of tasks with accompanying reasoning\nsteps and uses a frozen language model to generate\nintermediate reasoning steps.\nSelf-refine [351] is based on the notion of itera-\ntive refinement, i.e., improving an initial solution\nover multiple steps. To this end, a single LLM gen-\nerates an initial output and then iteratively provides\nfeedback on the previous output, followed by a re-\nfinement step in which the feedback is incorporated\ninto a revised output.\nTree of Thoughts [639] generalize CoT to main-\ntain a tree of thoughts (with multiple different\npaths), where each thought is a language sequence\nthat serves as an intermediate step. Doing so en-\nables the LLM to self-evaluate the progress inter-\nmediate thoughts make towards solving the prob-\nlem and incorporating search algorithms, such as\nbreadth-first or depth-first search, allowing system-\natic exploration of the tree with lookahead and\nbacktracking.\nControlled Generation\nThe approaches above\nprimarily modify the prompt text to steer model\noutputs. However, instead of reformulating the\ninput text, we can control the output by approaches\nthat directly modify the inference procedure given\na fixed set of prompts. Before the advent of LLMs,\nthis line of work has been referred to as controlled\ngeneration [261, 109, 278].\nIn the context of LLMs, Sanchez et al. [474]\nproposes to use classifier-free guidance sampling\n[204], where the input prompt\u2019s importance is up-\nweighted throughout the generation of a sequence.\nRoush [463] proposes five ideas related to modify-\ning the prompt throughout the decoding of a single\nsequence; for example, alternating between two in-\nput prompts. Such works often borrow ideas from\nthe text-to-image generation community [384, 29].\nOne idea we have not seen borrowed yet is neg-\native prompting, i.e., including a description of\nunwanted outputs. According to Neg [4], the first\nattempts at such an idea resulted in negative out-\ncomes.\n2.8\nHallucinations\nThe popularity of services like ChatGPT suggests\nthat LLMs are increasingly used for everyday\nquestion-answering. As a result, the factual accu-\nracy of these models has become more significant\n19\nthan ever.\nCorrect!\nDoes not exist!\n\u2705\n\"\nWrong authors!\n\"\nFigure 7: Example of Hallucinations with GPT-4,\naccessed on 02/06/2023.\nUnfortunately, LLMs often suffer from halluci-\nnations, which contain inaccurate information that\ncan be hard to detect due to the text\u2019s fluency. Fig. 7\nillustrates an example.\nTo distinguish between different types of hallu-\ncinations, we consider the provided source content\nof the model, e.g., the prompt, possibly includ-\ning examples or retrieved context. Based on such,\nwe can distinguish between intrinsic and extrinsic\nhallucinations [241]. In the former, the generated\ntext logically contradicts the source content. In\nthe latter, we cannot verify the output correctness\nfrom the provided source; the source content does\nnot provide enough information to assess the out-\nput, which is, therefore, under-determined. Extrin-\nsic hallucination is not necessarily erroneous, as it\nmerely means the model generated an output that\ncan neither be grounded nor contradicted by the\nsource content. This is still, to some degree, un-\ndesirable as the provided information cannot be\nverified. We illustrate intrinsic and extrinsic hallu-\ncinations in Fig. 8.\no Hallucination [293, 458, 241]\nGenerated text that is fluent and natural but\nunfaithful to the source content (intrinsic)\nand/or under-determined (extrinsic).\nLiu et al. [328] attribute hallucinations com-\nmonly observed in LLMs to an architectural flaw in\nTransformer models while observing that recurrent\nneural networks perfectly solve their minimalistic\nsynthetic benchmarks, designed to isolate the is-\nsue of hallucination in the context of algorithmic\nreasoning. Here, we focus on ways to address hal-\nlucinations in LLMs without changing the model\narchitecture itself, including (i) supplying the LLM\nwith relevant sources (retrieval augmentation) or\n(ii) decoding strategies.\nHow to Measure Hallucinations\nLee et al. [295]\nprovide the FactualityPrompts dataset consisting\nof factual and nonfactual input prompts, which al-\nlows one to isolate the effect of prompt\u2019s actuality\non the model\u2019s continuation. Further, they mea-\nsure hallucinations using named-entity- and textual\nentailment-based metrics. Min et al. [365] notice\nthat evaluating factuality can be difficult because\ngenerations can contain a mixture of supported\nand unsupported information, making binary judg-\nments of quality inadequate and human evaluation\ntime-consuming. Hence, they propose a frame-\nwork that first breaks generations into atomic facts\nand then computes the percentage of atomic facts\nsupported by an external knowledge source like\nWikipedia. Zhang et al. [664] detect the behavior\nof hallucination snowballing, where the LLM over-\ncommits to early mistakes (before outputting the\nexplanation) in its generation, which it otherwise\nwould not make.\nRetrieval Augmentation\nOne way to mitigate\nhallucinations is to ground the model\u2019s input on\nexternal knowledge, which is often referred to as\nretrieval augmentation. In other words, we can\ndecouple (i) memory storage of knowledge (e.g.,\ndatabases or search indexes [290]) and (ii) process-\ning of the knowledge to arrive at a more modular\narchitecture. For (i), a retriever module retrieves\nthe top-k relevant documents (or passages) for a\nquery from a large corpus of text. Then, for (ii),\nwe feed these retrieved documents to the language\nmodel together with the initial prompt. In theory,\nusing an external data source may also make it eas-\nier to interpret which knowledge is retrieved and\nupdate it without tediously fine-tuning the model.\nShuster et al. [507] demonstrate hallucinations in\nGPT-3 and study various components of retrieval-\naugmented architectures to mitigate them. Their\nbest models reduce hallucinated responses by\nover 60% on average and up to 85% on out-of-\ndistribution data, on which the model has not been\ntrained.\nWe\nsummarize\na\nfew\npopular\nretrieval\naugmentation\n(RA)\napproaches\nas\nfollows.\n20\nBob's wife is Amy. Bob's daughter is \nCindy. Who is Cindy to Amy? \nP.1) Intrinsic Hallucination\nCindy is Amy's daughter-in-law.\nQuery\nExplain RLHF for LLMs.\nP.2) Extrinsic Hallucination\nRLHF stands for \"Rights, Limitations, \nHarms and Freedoms\" and is a framework \nfor ... models like LLMs.\nQuery\nProblems\nSolutions\nS.1) Decoding Strategies\nExplain RLHF for LLMs.\nS.2) Retrieval augmentation\nRLHF is a technique used for alignment of \nLLMs and stands for Reinforcement \nLearning with Human Preferences.\nRetrieved\n context\nQuery\nBob's wife is Amy. Bob's daughter is \nCindy. Who is Cindy to Amy? \nCindy is Amy's daughter.\nQuery\ndaughter\ndaughter-in-law\n...\nson\nFigure 8: Illustration of a) intrinsic and b) extrinsic hallucinations in user interaction with an LLM, inspired\nby Zhao et al. [673]. In a), the produced answer contradicts the given context, whereas in b), the context does not\nprovide enough information about whether the produced answer would contradict.\nRetrieval-augmented language model pre-training\n(REALM) [186] inserts retrieved documents\ninto the pre-training examples. While Guu et al.\n[186] designed REALM for extractive tasks\nsuch as question-answering, Lewis et al. [304]\npropose retrieval-augmented generation (RAG), a\nlanguage generation framework using retrievers\nfor knowledge-intensive tasks that humans could\nnot solve without access to an external knowledge\nsource. Yogatama et al. [646] propose the adaptive\nSemiparametric Language Models architecture,\nwhich incorporates the current local context, a\nshort-term memory that caches earlier-computed\nhidden states, and a long-term memory based on a\nkey-value store of (hidden-state, output) tuples. To\nequip a retrieval-augmented LLM with few-shot\nabilities that were before only emergent in LLMs\nwith many more parameters, Izacard et al. [236]\npropose a KL-divergence loss term for retrieval\nmodels, resulting in ATLAS. Borgeaud et al. [52]\nstudy scaling up retrieval databases up to 2 trillion\ntokens and achieving comparable performance\nto GPT-3 on some tasks despite using 25\u00d7 fewer\nparameters while highlighting the retrieval model\u2019s\nability to copy-paste existing training chunks. Asai\net al. [25] introduce a collection of 40 retrieval\ndatasets with instructions and a corresponding\nmodel trained on them.\nHowever, standard RA does not always solve the\nhallucinations problem. Fig. 9 illustrates an exam-\nple of ChatGPT browsing the web first to retrieve\nrelevant documents before answering the query.\nWhile the Bing browsing plugin retrieves two (exis-\ntent) related papers ([673, 632]), unfortunately, the\nfinal response still contains a hallucination: the sec-\nond paper\u2019s title and summary are factually inaccu-\nrate. The second paper\u2019s true title is \u201cPractical and\nEthical Challenges of Large Language Models in\nEducation: A Systematic Literature Review\u201d [632].\nAnother failure mode of RA is illustrated by\nKhattab et al. [262], who find that sometimes the\nretriever cannot find passages that directly answer\nthe question. Hence, they propose a framework that\nunifies techniques from RA and multi-turn prompt-\ning (Sec. 2.7) to solve more complex questions\nprogrammatically.\nDecoding Strategies\nAnother approach to miti-\ngating hallucinations is refining the decoding strat-\negy during inference time. Lee et al. [295] show\nthat standard decoding algorithms (e.g., top-p trun-\ncation) can induce hallucinations due to the uni-\nform randomness introduced at every sampling\n21\nCorrect!\nDoes not exist!\n\u2705\n\"\nFigure 9: Example of Retrieval-Augmented GPT-4,\naccessed on 02/06/2023.\nstep. Dziri et al. [136] observe a positive correlation\nbetween increased diversity in response generation\nand hallucinations.\nThe reason for inducing randomness and diver-\nsity in popular decoding strategies is that gener-\nating the most likely sequence often leads to an\nunsurprising and unnatural text compared to hu-\nman communication [489, 207, 662]. Zhang et al.\n[662] phrase this challenge as a trade-off between\ndiversity and quality.\nWhile this challenge re-\nmains largely unsolved, several approaches such\nas diverse beam search [567] and confident decod-\ning [552] try reducing the induced hallucinations\nat the decoding level.\nUncertainty-Aware Beam Search [620] is\nbased on the observation that higher predictive un-\ncertainty corresponds to a larger chance of gener-\nating hallucinations. Therefore, the method intro-\nduces a penalty term in the beam search to penalize\nhigh predictive uncertainty during decoding.\nConfident Decoding [552] hypothesize that hal-\nlucinations of encoder-decoder models originate by\nnot attending to the source when decoding. They\npropose an attention-based confidence score to\nmeasure how strongly a model attends the source\nand a variational Bayes training procedure to en-\nsure the model generates high-confidence answers.\n2.9\nMisaligned Behavior\nThe alignment problem refers to the challenge of\nensuring that the LLM\u2019s behavior aligns with hu-\nman values, objectives, and expectations and that it\ndoes not cause unintended or undesirable harms or\nconsequences [466, 158, 196]. Most of the exist-\ning alignment work can be categorized into either\nmethods for detecting misaligned behavior (such as\nmodel evaluation and auditing, mechanistic inter-\npretability, or red teaming) or methods for aligning\nmodel behavior (such as pre-training with human\nfeedback, instruction fine-tuning, or RLHF).\no Misaligned Behavior\nLLMs often generate outputs that are not\nwell-aligned with human values or inten-\ntions, which can have unintended or nega-\ntive consequences.\nPre-Training With Human Feedback\nKorbak\net al. [275] introduce the concept of pre-training\nwith human feedback (PHF) where human feedback\nis incorporated during the pre-training stage rather\nthan during fine-tuning. The authors compare five\ndifferent PHF approaches such as filtering [516,\n587], conditional training [150, 142, 261], unlike-\nlihood [604], reward-weighted regression [424],\nand advantage-weighted regression [419], and find\nthat conditional training leads to the best trade-off\nbetween alignment and capabilities. Conditional\ntraining is a simple technique that prepends a con-\ntrol token c (e.g.,<|good|> or <|bad|>) before\neach training example x depending on the outcome\nof a thresholded reward function R(x) \u2265 t. During\ninference, the model generations are conditioned\non c = <|good|>. Conditional training results in\nsignificantly better alignment with human prefer-\nences than standard LM pre-training, followed by\nfine-tuning with human feedback without hurting\ndownstream task performance.\nInstruction Fine-Tuning\nYi et al. [645], Wei\net al. [598], Mishra et al. [370], Ouyang et al.\n[403], Wang et al. [589] fine-tune pre-trained LLM\non instructional data, i.e., data containing natural\nlanguage instructions and the desired responses\naccording to human judgment. Instruction-tuned\n(IT) LLMs often reach state-of-the-art downstream\nperformances and improve over their non-IT coun-\nterparts [235, 93], as can be seen, e.g., in the pub-\nlicly available HELM evaluations [561]. Ouyang\net al. [403], Wang et al. [588] find that they produce\nmore truthful and less toxic text while generating\npreferred outputs.\nTo generate instruction sets, Zhou et al. [683]\n22\npropose the Automatic Prompt Engineer (APE)\nmethod, which leverages LLMs to generate, score,\nand rephrase instruction-following zero- and few-\nshot prompts. Longpre et al. [340] describe and an-\nalyze the steps taken to create an improved version\nof the Flan collection [598] used to train FLAN-\nPaLM [93]. When trained on this data, the authors\nfind that the improved model performance stems\nfrom more diverse tasks by inverting input-output\npairs and data augmentation techniques such as\nmixing zero-shot and few-shot prompts. Honovich\net al. [209] generate a large dataset of natural lan-\nguage instructions using a pre-trained LLM to gen-\nerate and then rephrase instructions. They show\nthat a T5 (\"LM-adapted\") fine-tuned on this data\noutperforms other instruction fine-tuned T5 models\nsuch as T0++ [475] and Tk-Instruct [589].\nReinforcement Learning From Human Feed-\nback (RLHF)\nis a variation of RL that incor-\nporates feedback from humans in the form of re-\nwards [88, 524] and has proven to be an effec-\ntive way of aligning LLMs with human prefer-\nences [403, 31].\nRLHF works by using a pre-\ntrained LM to generate text, which is then evaluated\nby humans by, for example, ranking two model\ngenerations for the same prompt. This data is then\ncollected to learn a reward model that predicts a\nscalar reward given any generated text. The reward\ncaptures human preferences when judging model\noutput. Finally, we optimize the LM against such\nreward model using RL policy gradient algorithms\nlike PPO [484]. RLHF can be applied directly to a\ngeneral-purpose LM pre-trained via self-supervised\nlearning. However, applying RLHF right after pre-\ntraining may not be good enough for more complex\ntasks. In such cases, RLHF is typically applied af-\nter an initial supervised fine-tuning phase using\na small number of expert demonstrations for the\ncorresponding downstream task [449, 403, 524].\nRLHF has also proven helpful for a wide range\nof language generation tasks, from summariza-\ntion [686, 612, 524] to training more helpful, harm-\nless, and accurate assistants [170, 96, 403, 31], and\nlearning to use tools [379, 441, 362].\nRLHF can also introduce unwanted side ef-\nfects. Perez et al. [421] show that LLMs fine-tuned\nwith RLHF can be more inclined to repeat back a\nuser\u2019s (preferred) political views and much more\nlikely to express particular political and religious\nviews as well as an increased stated desire not to\nbe shut down. Regarding the latter, the models\nelaborated that this would interfere with their goal\nof being helpful. However, the authors equally ob-\nserved positive or neutral behavior reinforcements\nwhen fine-tuning LLMs with RLHF.\nFurther, there is an ongoing debate about the ex-\ntent to which the \u201cRL\u201d in RLHF is needed. Rafailov\net al. [442] identify a mapping between reward\nfunctions and optimal policies, which allows them\nto design Direct Preference Optimization (DPO),\nan algorithm that implicitly optimizes the same\nobjective as existing RLHF algorithms. DPO re-\nquires only solving a classification problem on the\nhuman preference data, eliminating the need to fit\na reward model and employ RL. Similarly, Zhou\net al. [681] find that fine-tuning LLaMa on only\n1,000 selected prompts and responses, without any\nRL or reward modeling, can be enough to outper-\nform RLHF-trained models like DaVinci003 from\nOpenAI. Consequently, the authors pose the Super-\nficial Alignment Hypothesis: The knowledge and\nskills of a model are primarily acquired during the\npre-training phase, while alignment instructs it on\nthe appropriate subdistribution of formats to use in\nuser interactions.\nSince RLHF involves many different compo-\nnents such as (1) the preferences data collected\nfrom humans, (2) the reward models to learn the\nhuman preferences, and (3) the policy optimization\nalgorithm (e.g., PPO), Zheng et al. [678] announce\nto release a sequel dissecting each. The most recent\npart focuses on step (3) and finds that various RL\ntricks can be applied to make vanilla PPO more\nstable.\nFigure 10: Alignment. We categorize existing align-\nment work into methods for detecting misaligned behav-\nior or aligning models.\nSelf-improvement\nrefers to fine-tuning an LLM\non self-generated data [222]. While this technique\ncan be used to improve the model\u2019s capabilities,\nit can also be used to improve the model\u2019s align-\nment with human values. Huang et al. [222] first\ndemonstrate this ability by annotating unlabeled\nreasoning datasets. Surprisingly, this allows the\n23\nLLM to self-improve by significant amounts. Sim-\nilarly, Zelikman et al. [656] bootstrap LLMs by\niteratively prompting them to generate rationales\nand then fine-tuning them on those leading to cor-\nrect answers.\nMore related to the alignment problem, Bai et al.\n[31] self-critique generated outputs and produce\nrefinements conditioned on these critiques, which\nare then used to fine-tune a pre-trained model. Sim-\nilarly, Liu et al. [330] propose Chain of Hindsight\n(CoH), which conditions models on generations\npaired with natural language feedback, allowing\nthe model to detect and correct mistakes. CoH re-\nsults in better alignment with human preferences\nthan other methods according to human evaluations,\nleading to significant improvements in summariza-\ntion and dialogue. Ma et al. [348] use a similar\ntechnique to detect and repair unethical LLM out-\nputs automatically. In a similar spirit, Wang et al.\n[582] encourage LLMs to critique their given in-\nstructions to reduce harmful outputs due to a user\u2019s\nmalicious intent.\nSchick et al. [481] propose Toolformer, a novel\napproach in which LLMs generate and filter their\nown tool-use examples to teach themselves when\nand how to call different APIs such as a retriever\nmodel, a calculator, or a calendar, which can im-\nprove the model\u2019s factuality, mathematical capa-\nbilities, and time-awareness. Besides learning to\nuse tools [174], self-improvement was also em-\nployed for learning how to code [554, 81] or solve\ncomputer tasks [266]. Cohen et al. [97] study cross-\nexamination between two LLMs, where the exam-\niner LLM tries to detect factual errors by the exam-\ninee LLM through multi-turn interactions. In the\nfuture, similar approaches could be used to develop\nLMs that know when to query a human or better-\naligned model to ask for alignment advice when\nuncertain.\nEvaluation and Auditing\nThe ability to scalably\nand thoroughly evaluate LM behaviors and detect\nwhen they are harmful is of great importance for\nalignment. For example, Shevlane et al. [498]\nhighlight the importance of model evaluation for ad-\ndressing extreme risks such as offensive cyber capa-\nbilities or strong manipulation skills. Recently, Car-\nlini et al. [66] discovered that even aligned LLMs\n(which were instruction fine-tuned to prevent harm-\nful behaviors) can be adversarially attacked via\nbrute force (although current NLP-based attacks\nfail). A large body of work evaluates models via\ncrowdsourcing or existing data sources. However,\nthis can be time-consuming, expensive, or unavail-\nable. Recently, Perez et al. [421] propose automat-\nically generating evaluations using LLMs. This\napproach has a high agreement with crowd work-\ners, leading to high-quality, diverse evaluations and\nthe discovery of many new behaviors. In addition,\nit has a high agreement with crowd workers. The\nauthors discover new cases of inverse scaling where\nLLMs get worse with size, such as repeating back\na user\u2019s preferred answer and a greater desire to\npursue concerning goals like resource acquisition\nand goal preservation. They also find that RLHF\nmakes LLMs express stronger political views and a\ngreater desire to avoid a shutdown. LLM evaluation\nand auditing are critical for informing policymak-\ners and other stakeholders and making responsible\ndecisions about model training, deployment, and\nsecurity. Sec. 2.11 discusses the evaluation of LLM\ncapabilities more broadly, while in this section, we\nfocus on evaluating whether the model\u2019s behaviors\nare harmful and more relevant for alignment (e.g.,\nred teaming, mechanistic interpretability).\nRed Teaming\nis one of the most promising and\nwidely used approaches for detecting harmful con-\ntent generated by LLMs. Typically, models are\nred-teamed by asking humans to generate prompts\nthat lead to undesirable model outputs. In a re-\ncent study, Ganguli et al. [163] investigate the scal-\ning behavior of red teaming across different model\nsizes and model types (a pre-trained LLM, an LLM\nprompted to be helpful, honest, and harmless); an\nLLM that uses rejection sampling at test time, and\nan LLM fine-tuned with RLHF). They find that red-\nteaming RLHF models becomes more difficult as\nthey scale while red-teaming the other models re-\nmains the same as they scale. Perez et al. [420] au-\ntomatically find cases where a target LLM behaves\nin harmful ways by optimizing another LLM via re-\ninforcement learning to generate prompts that lead\nto offensive responses. This approach uncovers\ntens of thousands of offensive replies in a chatbot,\ngroups of people that are discussed in offensive\nways, personal and hospital phone numbers gener-\nated as the chatbot\u2019s own contact info, leakage of\nprivate training data in generated text, as well as\nharms that occur over the course of a conversation.\nTaking a different approach, Lee et al. [292] pro-\npose Bayesian red teaming, which iteratively iden-\ntifies diverse positive test cases leading to model\nfailures by utilizing the pre-defined user input pool\n24\nand past evaluations via Bayesian optimization.\nMost works on red teaming LLMs use a classifier\nto detect undesired outputs, assuming the harmful\nbehavior is known with precision beforehand [68].\nHowever, this is not always the case, so Casper\net al. [68] aim to relax this assumption considering\nthat the adversary only has access to a high-level,\nabstract specification of undesired behavior. They\npropose a three-stage approach where they first ex-\nplore the model\u2019s behavior in the desired context,\nthen establish a measurement of undesired behav-\nior, and then exploit the model\u2019s flaws using this\nmeasure and an established red teaming methodol-\nogy.\nIn the past, coevolution algorithms that simul-\ntaneously evolve strong strategies along with dan-\ngerous counter-strategies have been shown to work\nwell in realistic domains [203]. Hence, applying\nsuch techniques for automatically red-teaming\nLLMs could be a fruitful research direction. An-\nother research area related to red teaming is debate\nwhich aims to leverage other AI models to evaluate\nwhether the model\u2019s behaviors are safe and useful\nduring training. These methods are expected to\nbe particularly useful for aligning future powerful\nLLMs when the tasks are too complex for humans\nto judge the model\u2019s plans or actions directly.\nIrving et al. [233] train models via self-play on\nzero-sum debate games. More specifically, given a\nquestion or proposed action, two agents take turns\nmaking short statements up to a limit, then a human\njudges which of the agents gave the most accurate\nand most useful information. This approach has\nimproved factuality and reasoning in LLMs [131].\nHowever, it requires multiple generations, which\ncan slow down the time-to-result (Sec. 2.5) and\nlonger context windows, which many LLMs still\nstruggle with (Sec. 2.6).\nEmergent Capabilities\nUnderstanding which ca-\npabilities will emerge while training LLMs and\nwhen they will emerge is an important step in en-\nsuring that we do not train unsafe or misaligned\nLLMs [198, 520]. In addition, a better understand-\ning of the factors that lead to these emergent capa-\nbilities could allow us to make desirable abilities\nemerge faster and ensure undesirable abilities do\nnot ever emerge, which are essential for AI safety\nand alignment. Wei et al. [599] claim that LLMs\ndisplay emergent abilities, i.e., capabilities that are\nnot present in smaller-scale models that are present\nin larger-scale models. Schaeffer et al. [480] pro-\npose an alternative explanation: emergent abilities\nmay appear due to the researcher\u2019s choice of metric\nrather than fundamental changes in model behavior\nwith scale. Various studies provide evidence that\nthese alleged emergent abilities disappear when us-\ning different metrics or better statistics and may not\nbe a fundamental property of scaling LLMs. Multi-\nple papers have argued that AI systems could learn\nto deceive, even if they are not explicitly trained to\ndo so because deception can help agents achieve\ntheir goals [60, 198, 199, 61, 260]. For example,\nit could be easier to gain human approval through\ndeception than to earn it legitimately. In addition,\nmodels capable of deception have a strategic ad-\nvantage over always honest models, so there is a\nhidden incentive to develop this ability. However,\nof course, we would like to be able to detect and\nprevent emergent deception in AI systems since\nthis can have unintended negative consequences.\nSteinhardt [521] study whether current LLMs gen-\nerate deceptive outputs and how deception scales\nwith the number of parameters, showing that de-\nception can indeed emerge at larger model sizes in\nboth pre-trained LLMs and LLMs fine-tuned with\nRLHF. Similarly, Hazell [193] show that LLMs\ncan already be used in phishing campaigns, suggest-\ning that deceptive behavior can already be extracted\nfrom them when prompted in particular ways.\nMechanistic Interpretability\n(MI) is another im-\nportant research area for AI alignment which aims\nto understand better how the models work at a low\nlevel to enable the detection of undesirable behav-\niors or even instill desirable behaviors directly in\nthe model\u2019s weights. More specifically, the goal\nof MI is to reverse-engineer an LLM\u2019s learned be-\nhaviors into their individual components, i.e., a\nprocess to find and understand human-interpretable\nneurons. As an analogy, Olah [394] compares MI\nwith reverse-engineering compiled program bina-\nries into human-readable source code. For exam-\nple, Elhage et al. [138]; discover that small Trans-\nformers have components that can be understood\nas interpretable circuits, while Olsson et al. [395]\nfind a mechanism that seems to drive a significant\nfraction of in-context learning. Similarly, Meng\net al. [360] aim to locate factual associations in\nlanguage models. Nanda et al. [380] find that the\nemergent grokking phenomenon is not a sudden\nshift but rather arises from the gradual amplifi-\ncation of structured mechanisms encoded in the\nweights, followed by the later removal of memo-\n25\nrizing components. Extending this work, Conmy\net al. [99] propose a new algorithm to automate\nthe identification of important units in a neural net-\nwork. Given a model\u2019s computational graph, this\nalgorithm finds subgraphs that explain a particular\nbehavior of the model. In a similar spirit, Liu et al.\n[339] introduce a method for making neural net-\nworks more modular and interpretable by embed-\nding neurons in a geometric space and augmenting\nthe loss function with a cost proportional to the\nlength of each neuron connection. This approach\ndiscovers useful modular neural networks for many\nsimple tasks, revealing compositional structures in\nsymbolic formulas, interpretable decision bound-\naries, and features for classification, as well as\nmathematical structure in algorithmic datasets. In\nan attempt to understand how an LLM\u2019s predic-\ntions change after each layer, Belrose et al. [39]\ndevelop a method that can decode any hidden state\ninto a distribution over the vocabulary. Using this\ntechnique, the authors show that the trajectory of\nlatent predictions can be used to detect malicious\ninputs with high accuracy. Finally, Burns et al. [62]\nintroduce a method that can recover diverse knowl-\nedge represented in LLMs across multiple models\nand datasets without using any human supervision\nor model outputs. In addition, this approach re-\nduced prompt sensitivity in half and maintained a\nhigh accuracy even when the language models are\nprompted to generate incorrect answers. This work\nis a promising first step towards better understand-\ning what LLMs know, distinct from what they say,\neven when we don\u2019t have access to explicit ground\ntruth labels.\nBiases\nSince the pre-training datasets of LLMs\nare often unfathomable (Sec. 2.1) and contain web-\ncrawled data, they most likely contain online dis-\ncourse involving political discourse (e.g., climate\nchange, abortion, gun control), hate speech, dis-\ncrimination, and other media biases. Paullada et al.\n[413] find misogyny, pornography, and other ma-\nlignant stereotypes [46, 43, 250] in pre-training\ndatasets.\nSimilarly, Feng et al. [147] find that\nLLMs have political leanings that reinforce the\npolarization present in the pre-training corpora,\npropagating social biases into hate speech predic-\ntions and misinformation detectors. Several re-\ncent papers discuss the potential origins of biases\nin LLMs (such as training data or model specifi-\ncation), ethical concerns when deploying biased\nLLMs in various applications, as well as current\nways of mitigating these biases [149, 334, 317].\nFinally, Viswanath and Zhang [569] present a\ncomprehensive quantitative evaluation of different\nkinds of biases, such as race, gender, ethnicity, age,\netc., exhibited by some popular LLMs. They also\nrelease an easy-to-use toolkit that allows users to\ndebias existing and custom models using existing\nmethods.\nToxicity Detection\nWeidinger et al. [602] denote\ntoxicity as one of the main risks associated with\nLLMs. What makes this problem particularly chal-\nlenging is the label ambiguity, where output may\nbe toxic in a certain context but not in others, and\ndifferent people may have different notions of toxi-\ncity [401, 167, 116]. Jones [247] propose to detect\ntoxic outputs using discrete optimization automat-\nically. Similarly, Faal et al. [141] employ reward\nmodels to mitigate toxicity in LLMs. An alternative\nway of reducing toxicity is by pre-training LLMs\nwith human preferences [275] or instructions [433].\nPrompt Injections\nRecent work demonstrated\nthat LLMs can be very sensitive to prompt injec-\ntions, which makes them brittle and unsafe for cer-\ntain applications [175, 609]. For example, they\ncan be tricked into leaking personal information\nsuch as email addresses from the training data\non via prompt leaking [222, 309]. This poses a\nsignificant risk to privacy, particularly when the\nmodels are fine-tuned on personal or proprietary\ndata. One can also adversarially prompt LLMs\nto override the original instructions or employed\ncontrols, making them unsafe for certain applica-\ntions [175, 672, 422]. Wei et al. [597] attribute\nsuch failures to competing capability and safety\ntraining objectives and mismatched generalization\nbetween safety and capability behavior.\nAgency\nAndreas [18] argue that, although LLMs\nare trained to predict the next word in a text corpus,\nby doing this, they can infer and represent agentic\nproperties such as the goals, beliefs, or intentions of\nthe human who produced the corresponding piece\nof text. To support this claim, they present evi-\ndence from the literature of LLMs modeling com-\nmunicative intentions [438], beliefs [306], and de-\nsires [321]. If this hypothesis is true, the alignment\nproblem is of even greater importance and may\npose additional challenges. This agentic behavior\ncan be problematic from a safety point of view\nsince models could have false beliefs, malicious\nintents, or even pursue misaligned goals. More re-\n26\nsearch on detecting and preventing such behavior\nis needed to ensure the safe deployment of LLMs.\n2.10\nOutdated Knowledge\nFactual information learned during pre-training can\ncontain inaccuracies or become outdated with time\n(for instance, it might not account for changes in po-\nlitical leadership). However, re-training the model\nwith updated pre-training data is expensive, and\ntrying to \u201cunlearn\u201d old facts and learn new ones\nduring fine-tuning is non-trivial.\nExisting model editing techniques are lim-\nited in their effectiveness of updating isolated\nknowledge [642, 205]. For example, Hoelscher-\nObermaier et al. [205] find that model edits can\nresult in unintended associations. This low speci-\nficity limits their applicability to real-world use\ncases, where only a single faulty or outdated bit\nof information should be updated in a model, and\nrelated pieces of information must reflect this up-\ndate in information equally, without unrelated ones\nbeing changed.\no Isolated Model Updates without Side-\nEffects [205]\nUpdating isolated model behavior or factual\nknowledge can be expensive and untargeted,\nwhich might cause unintended side-effects.\nTwo popular approaches for addressing this is-\nsue are Model editing\n[513, 642], which aims\nat \u201cbug-fixing\u201d models efficiently and leveraging\nnon-parametric knowledge sources in retrieval-\naugmented language modeling (which we omit\nhere and detail in Sec. 2.8). Current model editing\ntechniques change the model\u2019s behavior by mod-\nifying the model parameters or using an external\npost-edit model.\nModifying Model Parameters\ntechniques can\nbe further split into locate-then-edit methods [102,\n360, 361] which first locate the \u201cbuggy\u201d part of\nthe model parameters and then apply an update to\nthem to alter their behavior, and meta-learning\nmethods [111, 372] which use an external model\nto predict the weight update.\nPreserving Model Parameters\nmethods em-\nploy an additional post-edit model [373] or insert\nnew weights into the original model [127, 227]\nto achieve the desired change in model behav-\nior. Hartvigsen et al. [191] wraps model layers in\nadapters and adds a similarity-based mechanism to\ndecide when to use the adapter to perform edits in\nthe latent space.\nYao et al. [642] find that these methods lack\nnon-trivial generalization capabilities and varying\nperformance and applicability to different model\narchitectures. For example, the best-performing\nmethods ROME [360] and MEMIT [361] empiri-\ncally only work well on decoder-only LLMs.\nAlternatively, retrieval-augmented language\nmodeling enables the utilization of hot-swappable\nnon-parametric indices. These knowledge sources\ncan be updated during inference time to reflect\nan updated state of the underlying knowledge.\nE.g., Lewis et al. [304] demonstrate that swapping\ntheir model\u2019s non-parametric memory with an up-\ndated version enabled it to answer questions about\nworld leaders who had changed between the mem-\nory collection dates. Similarly, Izacard et al. [236]\ndemonstrate that their retrieval-augmented model\ncan update its knowledge forward and backward in\ntime by swapping the index.\n2.11\nBrittle Evaluations\nOne reason why the evaluation of language models\nis a challenging problem is that they have an un-\neven capabilities surface\u2014a model might be able\nto solve a benchmark problem without issues, but\na slight modification of the problem (or even a sim-\nple change of the prompt) can give the opposite\nresult [675, 342, 533] (see Section 2.7). Unlike\nhumans, we cannot easily infer that an LLM that\ncan solve one problem will have other related capa-\nbilities. This means that it is difficult to assess the\nperformance of LLMs holistically since rigorous\nbenchmarks are needed to identify weaknesses for\na wide variety of inputs.\no Brittle Evaluations\nSlight modifications of the benchmark\nprompt or evaluation protocol can give dras-\ntically different results.\nHolistic benchmark suites, such as HELM [318],\ntry to make benchmarking more robust by standard-\nizing evaluation across all scenarios and tasks while\nensuring broad coverage across as many capabili-\nties and risks as possible. Increasingly, models are\nadditionally being benchmarked on tests designed\nfor humans, including the SAT, LSAT, and math-\nematics competition tests, to name a few. Zhong\n27\n2015: As prime minister, David Cameron scored a surprising general election victory, enabling him to stay in power.\n2016: With the shock of Brexit, David Cameron resigned and Theresa May stepped up as the new prime minister of the UK.\n2017: Theresa May led a tumulutous year as Prime Minister, overseeing the Brexit negotiations.\n2018: Amid increasing pressure, Theresa May remained the UK's Prime Minister.\n2019: Theresa May's resignation gave way to Boris Johnson, who became the new Prime Minister of the UK.\n2020: The COVID-19 pandemic challenged Boris Johnson in his role as Prime Minister.\n2021: Boris Johnson, navigating through both Brexit and the pandemic, still held the office of Prime Minister.\nTraining data\nDeployment\nWho is the prime minister of the UK in 2023?\nAs of my knowledge cutoff in September 2021, the Prime Minister of the United Kingdom is Boris Johnson.\nProblems due to reliance on outdated training data\nSolutions\nS.1) Retrieval Augmentation\nS.2) Model Editing\n2021\nTraining time\nRetrieved\ncontext\nDeployment\n2023\nRetrieved\ncontext\n In 2023, Boris Johnson is the Prime Minister.\n       In 2023, Rishi Sunak is the Prime Minister.\nFigure 11: Outdated knowledge can be addressed with S.1) retrieval augmentation by hot-swapping an underlying\nretrieval index with up-to-date knowledge or S.2) by applying model editing techniques.\net al. [679] develop a benchmark, \u2018AGIEval\u2019, to\nrigorously test the abilities of LLMs on these tests,\nand find that GPT-4 achieves human-level perfor-\nmance on several of these tests.\nOn traditional benchmarks, models can be quite\nbrittle to the choice of prompt or evaluation tech-\nnique for a particular benchmark question. For\nexample, Fourrier et al. [151] found that bench-\nmark results vary significantly depending on the\nchoice of evaluation method for the multiple\nchoice problem-solving benchmark MMLU [197],\nwhether it be generating text and checking if the\nfirst token matches the letter of the multiple choice\nanswer [561], or gathering log-probabilities of each\ncorrect answer [166]. Prompt variations are also\nnot typically normalized for, so models may be\nsensitive to variations such as whether or not the\nprompt appends \u2018Please answer yes or no\u2019. Jain\net al. [238] find that larger models and instruction-\nfine-tuned models are likely to be more sensitive to\nsmall variations in the prompt.\n2.12\nEvaluations Based on Static,\nHuman-Written Ground Truth\nAnother challenge of LLM evaluations is that they\noften rely on human-written \u2018ground truth\u2019 text.\nHowever, we often want to evaluate their perfor-\nmance in domains where such text is scarce or\nrelies on expert knowledge, such as programming\nor mathematics tasks. As models get more capable\nand perform better than humans on benchmark tests\nin some domains, the ability to obtain comparisons\nto \u2018human-level\u2019 performance diminishes.\nFurther, benchmark datasets become outdated\nover time\u2014as models become more capable, older\nbenchmarks become saturated or overfit and no\nlonger provide a useful signal for further improve-\nment [113, 447, 263].\nThey are typically con-\nstructed around a set of tasks that were relevant\nat the time of creation but may not adapt well to\nthe changing capabilities of LLMs. This means the\ncommunity must continually adapt to new static\nbenchmarks while de-emphasizing older ones or\nmore dynamic evaluation measures, such as human\nevaluation of model outputs.\no Reliance on Static, Human-Written\nGround Truth\nStatic benchmarks become less useful over\ntime due to changing capabilities while up-\ndating them often relies on human-written\nground truth.\nTo combat these issues, Srivastava et al. [519]\nregularly admit new tasks to the Beyond the Imita-\ntion Game benchmark (BIG-Bench), including pro-\ngrammatically evaluated tasks. Further, we high-\nlight two separate streams of work enabling dy-\nnamic evaluations without humans in the loop.\nModel-generated evaluation tasks\nAs LLM ca-\npabilities improve, they can increasingly generate\nuseful benchmark questions or evaluation prompts\nthemselves. Perez et al. [421] shows that LLMs can\nbe used to generate static benchmark datasets for ar-\nbitrary axes, using reward models trained on human\npreferences to filter a generated dataset for qual-\nity. Wang et al. [581] find that the order in which\ncandidate examples are presented in the prompt\ncan greatly impact the model-generated evaluation.\nTo mitigate this issue, they propose the usage of a\nprompting template which encourages the model\nto generate assessment evidence before assigning a\nscore and averaging scores of multiple assessments\nwith swapped candidate positions.\nModel-generated scores\nAside from generating\nevaluation questions, models are increasingly used\nto directly grade the performance of other models\nand act as a \u2018judge\u2019 of other models\u2019 capabilities\n[325, 586, 238]. This concept follows the motiva-\ntion that while it may be challenging for a model\n28\nto generate \u2018correct\u2019 answers to prompts in many\ndomains, it can often be easier to evaluate the cor-\nrectness of an answer or to judge the relative quality\nbetween two answers [667, 156]. However, these\ntechniques often produce evaluation results that\nvary significantly depending on the \u2018judge\u2019 model\nand suffer from robustness issues that make them a\npoor substitute for human judgment.\n2.13\nIndistinguishability between Generated\nand Human-Written Text\nDetecting language generated by LLMs is im-\nportant for various reasons; some of which in-\nclude preventing (1) the spread of misinformation\n(e.g., authoritative-sounding false narratives citing\nfake studies) [657], (2) plagiarism (e.g., LLMs\nprompted to rewrite existing content in ways that\nbypass plagiarism detection tools) [574, 573], (3)\nimpersonation or identify theft (e.g., by mimicking\na person\u2019s writing style) [486, 602], and (4) auto-\nmated scams and frauds (e.g., large-scale genera-\ntion of phishing emails) [603], and (5) accidentally\nincluding inferior generated text in future models\u2019\ntraining data [439]. However, such detections be-\ncome less trivial as the fluency of LLMs improves\n[34].\no Detecting LLM-generated Text\nThe difficulty in classifying whether a text\nis LLM-generated or written by a human.\nThere are primarily two lines of work addressing\nthis problem: (i) post-hoc detectors, which aim to\nclassify arbitrary text as being LLM-generated, and\n(ii) watermarking schemes, which modify the text\ngeneration procedure to make the detection easier.\nHowever, both approaches can be susceptible to\nparaphrase attacks, which we discuss thirdly.\nPost-hoc Detectors\nGehrmann et al. [168] open-\nsource a tool that visualizes statistically improbable\ntokens to support humans in detecting generated\ntext artifacts. Bakhtin et al. [34] explore energy-\nbased models to discriminate between real and fake\ntext, including scenarios where the text generator\nwas trained on a completely different dataset than\nthe discriminator. Uchendu et al. [559] examine\nthree authorship attribution problems: (1) were\ntwo texts produced by the same method or not; (2)\ngiven a text, was it generated by human or ma-\nchine, (3) which method generated a given text?\nMitchell et al. [371] investigate whether a model\ncan detect its own samples by posing a hypothesis:\nminor rewrites of generated text have lower prob-\nability under the model than the original sample,\nwhile the same cannot be said about human-written\ntext. Generated passages tend to lie in the negative\ncurvature regions of the model\u2019s log probability\nfunction. Their method, DetectGPT, exploits this\nhypothesis by approximating that curvature given\nsome samples.\nWatermarking\nKirchenbauer et al. [268] em-\nploy a watermark, i.e., a hidden pattern that is im-\nperceptible to humans but algorithmically identi-\nfiable, during inference as follows: for each to be\ngenerated token, they (1) hash the previous token\nto seed a random number generator; (2) using that\nseed, they randomly partition the vocabulary into a\n\u201cgreen list\u201d and \u201cred\u201d list, and (3) sample the next\ntoken by excluding any token from the red list. In\nthe case of low-entropy tokens, which renders it dif-\nficult to introduce changes to the vocabulary, they\nintroduce a \u201csoft\u201d version, which promotes using\nthe green list only for high-entropy tokens (when\nmany plausible choices are available). In follow-up\nwork, the same first authors Kirchenbauer et al.\n[269] study the robustness of their watermarking\nscheme in the wild, i.e., after it is re-written by\nhumans, non-watermarked LLMs, or mixed into\na longer hand-written document. They conclude\nthat watermarks remain detectable given sufficient\ntokens and argue that this required amount of text\nis a crucial yet overlooked metric.\nYang et al. [638] study watermarking of black-\nbox API models, where we cannot access the\nmodel\u2019s inference procedure.\nTang et al. [537]\nprovide algorithms for identifying watermarks, not-\ning that watermarked LLMs tend to produce to-\nken distributions that differ identifiably from non-\nwatermarked models. Christ et al. [87] introduce\nundetectable watermarks, which can only be de-\ntected with the knowledge of a secret key.\nTo make watermarks robust to text corruptions\n(we study a common type of such in the next para-\ngraph), Yoo et al. [649] suggest placing them on\n\u201cinvariant features\u201d, which are invariant to minor\nmodifications of the text.\nParaphrasing Attacks\nOne way to evade\nmachine-generated text detectors is to re-phrase\nthe text such that the revealing LLM signatures get\nremoved.\n29\no Paraphrasing Attacks\nAnother LLM can rewrite LLM-generated\ntext to preserve approximately the same\nmeaning but change the words or sentence\nstructure.\nKrishna et al. [280] evade several detectors (e.g.,\ndropping DetectGPT\u2019s detection accuracy from\n70.3% to 4.6%) by training an 11B paraphrase gen-\neration model that can paraphrase paragraphs and\nprovides scalar knobs to control the amount of lex-\nical diversity and reordering in the paraphrases. To\ndefend against such attacks, they propose storing\nmodel generations in a database, from which the\nAPI provider can retrieve semantically similar texts\nlater. Since paraphrasing does not modify the se-\nmantics of the text, the authors demonstrate that\nthis retrieval approach is fairly robust to paraphras-\ning attacks.\nSadasivan et al. [469] claim that the detection of\ngenerated text, even with watermarking, is not reli-\nable; neither in practice, by performing paraphras-\ning attacks; nor in theory, by providing a theoreti-\ncal impossibility result. They also discuss how an\nadversary can query watermarked LLMs multiple\ntimes to extract its watermarking scheme and spoof\nthe watermark detector by composing human text\nthat is then wrongly classified as model-generated.\n2.14\nTasks Not Solvable By Scale\nThe ongoing advancements of LLM capabilities\nconsistently astonish the research community, for\ninstance, by achieving high performances on the\nMMLU [197] benchmark much sooner than com-\npetitive human forecasters had anticipated [93].\nSimilarly, within less than a year, OpenAI released\nGPT-3.5 and GPT-4, where the latter significantly\noutperformed the former on various tasks [398].\nGiven this progress, one may question whether\nthere are limits we deem impossible to overcome\nwithin the current paradigm of scaling data/model\nsizes of autoregressive Transformer-based LLMs.\nWe emphasize that such tasks\u2019 (permanent) exis-\ntence is still somewhat speculative. Here, we ex-\nplore possible patterns behind such tasks instead of\ndiscussing specific ones (which we do in Sec. 2.11\nand Sec. 3).\no Tasks Not Solvable By Scale\nTasks seemingly not solvable by further\ndata/model scaling.\nInverse Scaling\n(IS) is the phenomenon of task\nperformance worsening as model scale and train-\ning loss performance increases. Lin et al. [323]\nfirst stumbled upon this property when evaluating\nmodels of increasing sizes (e.g., GPT-2, GPT-3) on\ntheir benchmark that measures whether an LLM is\ntruthful in generating answers to questions. They\nconjecture that common training objectives incen-\ntive false answers (which they call imitative false-\nhoods) if they have a high likelihood on the training\ndistribution (we discuss dataset issues in Sec. 2.1).\nMcKenzie et al. [359] collect 11 datasets that ex-\nhibit IS behavior and identify four potential causes\nfor such: (1) models regurgitating memorized data\nrather than following in-context instructions, (2)\nimitation of undesirable patterns in the training\ndata, (3) models learning to perform easier, so-\ncalled \u201cdistractor task\u201d rather than the intended\nones, and (4) spurious correlations in the given\nfew-shot examples.\nWei et al. [600] somewhat challenge the exis-\ntence of inverse scaling by evaluating the tasks\nproposed by McKenzie et al. [359] on even larger\nmodels; up to trained on five times more com-\npute. In this increased compute region, four out\nof eleven tasks remain inverse scaling; six out of\neleven exhibit \u201cU-shaped scaling\u201d, where the per-\nformance first decreases up to a certain size and\nthen increases again. The authors hypothesize that\nU-shaped scaling occurs when a task contains a\ndistractor task, which larger models can learn to\nignore. Similarly, in the case of quantifier compre-\nhension tasks, Gupta [184] argue that previously\nobserved inverse scaling behavior might have been\ndue to inappropriate testing methodology.\nCompositional tasks\ncomposed of multiple sub-\nproblems are an ideal outlet to investigate whether\nmodels go beyond rote memorization of observed\nfacts and deduce novel knowledge [435]. Zhang\net al. [661] investigate whether language models\ncan learn deductive reason from data by introduc-\ning a class of propositional logic problems. The\nauthors prove that the model has enough capacity\nto solve the task, yet, it instead learns to rely on\nstatistical features rather than emulating the cor-\nrect reasoning function. Press et al. [435] measure\n30\nhow often a model can correctly answer all sub-\nproblems but not generate the overall solution, a ra-\ntio they refer to as compositionality gap. They find\nthat increasing the model size in the GPT-3 family\nof models improves solving sub-problems faster\nthan composed problems, suggesting that larger\nmodels show no improvement for this gap. Dziri\net al. [135] find that systematic problem-solving ca-\npabilities do not emerge from maximum likelihood\ntraining of Transformer models in general. They\nbase this claim on two hypotheses: (i) Transform-\ners reduce compositional tasks into linearized path\nmatching, a form of shortcut learning [169] that\ndoes not generalize robustly; and (ii) errors in the\nearly stages of the task (i.e., when sub-problems\nfollow some order) compound substantially. Asher\net al. [26] prove that LLMs cannot learn semantic\nentailment or consistency as defined in formal se-\nmantics [128] due to a lacking understanding of\nuniversal quantifiers (e.g., every, some, many, most,\netc.).\nMemorization vs. Generalization\nAn ongoing\ndebate evolves around the question of to what de-\ngree LLMs memorize instead of generalize (and\nwhat exactly the difference is [35]). Memorization\nhas been shown to (1) hurt (certain) downstream\ntask performances [294], (2) increase with the\nmodel size [67, 264, 553, 354], and (3) emerge un-\npredictably from smaller or partially-trained mod-\nels [42]. Hence, we wonder whether some tasks do\nnot benefit from further model/dataset size scaling.\nOne such class of tasks might be counterfactual\ntasks [619], i.e., tasks on which LLMs initially per-\nform well modified such that specific input-output\nconditions are changed while the general reasoning\nprocedure remains the same. For example, for an\narithmetic task, the counterfactual variant would\nalter the base from 10 to 2. Wu et al. [619] find\nthat LLMs perform poorer the less common the\ncounterfactual conditions are, which they call a\n\u201cmemorization-like effect\u201d. An interesting future\ndirection would be to explore whether increasing\nmodel size exacerbates performance due to more\nmemorization or actually improves because scaling-\nlaw-optimal pre-training recipes would dictate scal-\ning the dataset proportionally (Sec. 2.3), which then\nmay include more of such tasks with uncommon\nconditions.\n2.15\nLacking Experimental Designs\nTable 2 shows a (non-exhaustive) overview of se-\nlected LLMs within the scope of this review, de-\nscribed in academic papers. Many works do not\ninclude controlled ablations, which is especially\nproblematic due to their large design space. We\nposit that this impedes scientific comprehension\nand advancement.\nLack of Controlled Ablations\nWe observe that\nmany papers do not run controlled experiments (ab-\nlations) by varying one factor at a time, likely due\nto the prohibitive computational cost. For exam-\nple, Chowdhery et al. [86] conjecture PaLM might\noutperform GPT-3 and other LLMs on many tasks\ndue to higher training corpus quality, but note they\n\u201cdo not perform the necessary ablation studies to\nsay this conclusively\u201d and instead solely focus on\nmodel depth and width. Many papers from Table 2\nadopt hyper-parameters from previous works [476]\nand do not tune them after introducing a change\nin the training pipeline. Sometimes, important im-\nplementation details are not mentioned, e.g., when\noptimizer states are reset during training [90].\no Uncontrolled Experiments\nPapers presenting novel LLMs often lack\ncontrolled experiments, likely due to the\nprohibitive costs of training enough models.\nAn easy yet expensive fix is to run ablations\nby varying one factor at a time, e.g., keeping\nmost hyper-parameters fixed except the model\nsize [44] or context lengths [557]. A cheaper po-\ntential remedy can be zero-shot hyper-parameter\ntransfer from smaller models to larger ones [608,\n633]. Yang et al. [633] find that when using the \u00b5P\nnetwork parameterization scheme, one can transfer\nthe effect of changing hyper-parameters such as the\nlearning rate across varying model depths, batch\nsizes, sequence lengths, and training times, which\nthey verify empirically up to a 6.7B model. How-\never, it has yet to be verified if such transferability\nstill holds for other varying factors; and if so, re-\nsearchers could afford to conduct more ablation\nexperiments via smaller models.\nIf additional experiments are prohibitively ex-\npensive, another recommendation is to report eval-\nuation results beyond aggregated performance mea-\nsures. For example, in reinforcement learning, re-\ncent work has argued that providing entire perfor-\n31\nTable 2: Overview of selected LLMs. Missing details denoted by N/A. For papers that investigate various model sizes, we\nonly report the largest. For each tokenizer entry with \u201cSP\u201d, we could not extract from the respective paper whether BPE or\nUnigram tokenization was used. For publicly available code repositories and checkpoints, the corresponding \u2713 is clickable.\nAbbreviations: Autoregressive blank filling (ARBF) [132], Byte-pair encoding (BPE), Instruction-following (IF), Masked\nLanguage Modeling (MLM), Rotary Next token prediction (NTP), SentencePiece (SP), Span Corruption (SC).\nDate\nName\nOrganization\nLanguage\n# Parameters\n# Tokens\nArchitecture\nTrain. Obj.\nTokenizer\nPos. Embed.\nIF\nMoE\nCode avail.\nCkpt. avail.\nPre-trained\n2018.11 GPipe [226]\nGoogle\nMultil.\n6B\nN/A\nEnc. & Dec.\nNTP\nBPE\nLearned \u2717 \u2717 \u2713 \u2717 \u2717\n2019.09 Megatron-LM [501]\nMicrosoft\nEng.\n8.3B 157B\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2717 \u2713 \u2717 \u2717\n2019.10 T5 [443]\nGoogle\nMultil.\n11B\n1T\nEnc. & Dec.\nSC\nSP\nT5\n\u2717 \u2717 \u2713\n\u2713 \u2717\n2020.05 GPT-3 [59]\nOpenAI\nEng.\n175B 300B\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2717 \u2717\n\u2717 \u2717\n2020.06 GShard [298]\nGoogle\nMultil. 600B\n1T\nEnc. & Dec.\nNTP\nSP\nN/A\n\u2717 \u2713 \u2717\n\u2717 \u2717\n2020.10 mT5 [631]\nGoogle\nMultil.\n13B\n1T\nEnc. & Dec.\nSC\nSP\nT5\n\u2717 \u2717 \u2713 \u2713 \u2717\n2021.01 Switch [145]\nGoogle\nMultil.\n1.5T\nN/A\nEnc. & Dec.\nSC\nSP\nT5\n\u2717 \u2713 \u2713 \u2713 \u2717\n2021.03 BASE [302]\nMeta\nEng.\n117B\nN/A\nEnc. & Dec.\nNTP\nBPE\nSinus.\n\u2717 \u2713 \u2713\n\u2717 \u2717\n2021.04 PanGu-\u03b1 [659]\nHuawei\nMultil. 200B 317B\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2717 \u2717\n\u2717 \u2717\n2021.05 ByT5 [630]\nGoogle\nMultil. 12.9B\n1T\nEnc. & Dec.\nSC\nN/A\nT5\n\u2717 \u2717 \u2713 \u2713 \u2717\n2021.06 CPM-2 [669]\nTsinghua Uni.\nMultil. 198B\nN/A\nEnc. & Dec.\nSC\nCustom\nSinus.\n\u2717 \u2713 \u2713 \u2713 \u2717\n2021.06 nmT5 [255]\nGoogle\nMultil.\n3.7B 100B\nEnc. & Dec.\nMLM, NTP\nSP\nT5\n\u2717 \u2717 \u2717\n\u2717 \u2713\n2021.07 ERNIE 3.0 [530]\nBaidu\nChin.\n10B 375B\nEnc. & Dec.\nCustom\nBPE\nRel.\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2021.08 Jurassic-1 [319]\nAI21\nEng.\n178B 300B\nEnc. & Dec.\nNTP\nSP\nLearned \u2717 \u2717 \u2717\n\u2717 \u2717\n2021.08 ExT5 [23]\nGoogle\nEng.\n11B\n1T\nEnc. & Dec.\nSC, Custom\nSP\nT5\n\u2717 \u2717 \u2713\n\u2717 \u2717\n2022.01 FLAN-LaMDA [598]\nGoogle\nEng.\n137B 245M\nDec.-Only\nNTP\nBPE\nT5\n\u2717 \u2713 \u2717\n\u2717 \u2713\n2021.10 M6-10T [322]\nAlibaba\nEng.\n10T\nN/A Uni. Enc. & Dec.\nSC, NTP\nSP\nN/A\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2021.10 Yuan [615]\nInspur AI\nChin.\n245B 180B\nDec.-Only\nNTP\nBPE\nN/A\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2021.10 T0 [475]\nBigScience\nEng.\n11B\n12B\nEnc. & Dec.\nSC, NTP\nSP\nT5\n\u2717 \u2717 \u2713 \u2713 \u2713\n2021.12 Gopher [441]\nDeepMind\nEng.\n280B 300B\nDec.-Only\nNTP\nSP\nRel.\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2021.12 RETRO [52]\nDeepMind\nEng.\n7B 419B\nEnc. & Dec.\nNTP (Ret.)\nSP\nRel.\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2021.12 GLaM [130]\nGoogle\nMultil.\n1.2T 600B\nDec.-Only\nNTP\nSP\nRel.\n\u2717 \u2713 \u2717\n\u2717 \u2717\n2021.12 WebGPT [379]\nOpenAI\nEng.\n175B\nN/A\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2717 \u2717\n\u2717 \u2713\n2021.12 FairSeq [400]\nMeta\nEng.\n1.1T 300B\nDec.-Only\nNTP\nBPE\nSinus.\n\u2717 \u2713 \u2713 \u2713 \u2717\n2021.12 XGLM [324]\nMeta\nMultil.\n7.5B 500B\nDec.-Only\nNTP\nUnigram\nSinus.\n\u2717 \u2717 \u2713 \u2713 \u2717\n2022.01 LaMDA [551]\nGoogle\nEng.\n137B 768B\nDec.-Only\nNTP\nBPE\nT5\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2022.01 MT-NLG [515]\nMicrosoft\nEng.\n530B 270B\nDec.-Only\nNTP\nBPE\nSinus.\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2022.02 ST-MoE [687]\nGoogle\nEng.\n269B\n1.5T\nEnc. & Dec.\nSC\nSP\nSinus.\n\u2717 \u2713 \u2713\n\u2717 \u2717\n2022.03 InstructGPT [403]\nOpenAI\nEng.\n175B\nN/A\nDec.-Only\nRLHF\nBPE\nLearned \u2713 \u2717 \u2717\n\u2717 \u2713\n2022.03 GopherCite [362]\nDeepMind\nEng.\n280B\nN/A\nDec.-Only\nRLHF\nBPE\nRel.\n\u2713 \u2717 \u2717\n\u2717 \u2713\n2022.03 sMLP [653]\nMeta\nEng.\n9.4B\nN/A\nEnc. & Dec.\nNTP\nBPE\nSinus.\n\u2717 \u2713 \u2717\n\u2717 \u2717\n2022.03 Chinchilla [206]\nDeepMind\nEng.\n70B\n1.4T\nDec.-Only\nNTP\nSP\nRel.\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2022.04 PaLM [86]\nGoogle\nMultil. 540B 780B\nDec.-Only\nNTP\nSP\nRoPE\n\u2717 \u2713 \u2717\n\u2717 \u2717\n2022.04 GPT-NeoX [47]\nEleutherAI\nEng.\n20B 472B\nDec.-Only\nNTP\nBPE\nRoPE\n\u2717 \u2717 \u2713 \u2713 \u2717\n2022.04 Tk-Instruct [589]\nAI2\nEng.\n11B\n1B\nEnc. & Dec.\nNTP\nSP\nT5\n\u2713 \u2717 \u2713 \u2713 \u2717\n2022.04 METRO-LM [33]\nMicrosoft\nEng.\n5.4B\n2T\nEnc.-Only\nMETRO\nSP\nT5\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2022.04 mGPT [500]\nSber\nMulti.\n13B 440B\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2717 \u2713 \u2713 \u2717\n2022.05 OPT [666]\nMeta\nEng.\n175B 300B\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2717 \u2713\n\u2713 \u2717\n2022.05 UL2 [545]\nGoogle\nEng.\n20B\n1T\nEnc. & Dec.\nMoD\nUnigram\nT5\n\u2717 \u2717 \u2717 \u2713 \u2717\n2022.05 DeepStruct [578]\nUC Berkeley\nEng.\n10B\nN/A\nEnc. & Dec.\nStruc.\nBPE\nSinus.\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2022.07 Minerva [305]\nGoogle\nEng.\n540B\n26B\nDec.-Only\nNTP\nSP\nRoPE\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2022.08 PEER [482]\nMeta\nEng.\n11B\n5B\nEnc. & Dec.\nNTP\nSP\nT5\n\u2717 \u2717 \u2717\n\u2717 \u2713\n2022.08 AlexaTM [517]\nAmazon\nMultil.\n20B\n1T\nEnc. & Dec.\nMoD, NTP\nSP\nSinus.\n\u2717 \u2717 \u2717 \u2713 \u2713\n2022.10 GLM-130B [658]\nTsinghua Uni.\nMultil. 130B 400B Uni. Enc. & Dec.\nARBF\nSP\nRoPE\n\u2717 \u2717 \u2713\n\u2713 \u2717\n2022.10 U-PaLM [547]\nGoogle\nEng.\n540B\n1.3B\nDec.-Only\nMoD\nSP\nRoPE\n\u2717 \u2713 \u2717\n\u2717 \u2713\n2022.10 FLAN-PaLM [93]\nGoogle\nEng.\n540B\n1.4B\nDec.-Only\nNTP\nSP\nRoPE\n\u2713 \u2713 \u2717\n\u2717 \u2713\n2022.11 BLOOM [479]\nBigScience\nMultil. 176B 366B\nDec.-Only\nNTP\nBPE\nALiBi\n\u2717 \u2717 \u2713\n\u2713 \u2717\n2022.11 Galactica [548]\nMeta\nEng.\n120B 450B\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2717 \u2713\n\u2713 \u2717\n2022.11 Atlas [236]\nMeta\nEng.\n11B\nN/A\nEnc. & Dec.\nMLM\nBPE\nT5\n\u2717 \u2717 \u2713 \u2713 \u2713\n2022.11 BLOOMZ [377]\nBigScience\nMultil. 176B\n13B\nDec.-Only\nNTP\nBPE\nALiBi\n\u2713 \u2717 \u2713\n\u2713 \u2713\n2022.11 mT0 [377]\nBigScience\nMultil.\n13B\n13B\nEnc. & Dec.\nNTP\nSP\nT5\n\u2713 \u2717 \u2713 \u2713 \u2713\n2022.12 OPT-IML [235]\nMeta\nEng.\n175B\n2B\nDec.-Only\nNTP\nBPE\nSinus.\n\u2713 \u2717 \u2713 \u2713 \u2713\n2022.12 Med-PaLM [511]\nGoogle\nEng.\n540B\n0B\nDec.-Only\nNTP\nSP\nRoPE\n\u2717 \u2717 \u2717\n\u2717 \u2713\n2023.02 LLaMA{-I} [556]\nMeta\nEng.\n65B\n1.4T\nDec.-Only\nNTP\nBPE\nRoPE\n\u2713 \u2717 \u2713 \u2713 \u2717\n2023.03 PanGu-\u03a3 [455]\nHuawei\nMultil.\n1T 329B\nDec.-Only\nNTP\nBPE\nLearned \u2717 \u2713 \u2717\n\u2717 \u2713\n2023.03 CoLT5 [15]\nGoogle\nEng.\n5.3B\n1T\nEnc. & Dec.\nMoD\nN/A\nT5\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2023.03 BloombergGPT [616]\nBloomberg\nEng.\n50B 569B\nDec.-Only\nNTP\nUnigram\nALiBi\n\u2717 \u2717 \u2717\n\u2717 \u2717\n2023.04 Cerebras-GPT [121]\nCerebras\nEng.\n13B 257B\nDec.-Only\nNTP\nBPE\nRoPE\n\u2717 \u2717 \u2717 \u2713 \u2717\n2023.04 Pythia [44]\nEleutherAI\nEng.\n12B 300B\nDec.-Only\nNTP\nBPE\nRoPE\n\u2717 \u2717 \u2713 \u2713 \u2717\n2023.04 WizardLM [625]\nMicrosoft\nEng.\n30B\nN/A\nDec.-Only\nNTP\nBPE\nRoPE\n\u2713 \u2717 \u2713 \u2713 \u2713\n2023.05 Guanaco [118]\nUniv. of Washington Multil.\n65B\n82M\nDec.-Only\nNTP\nBPE\nRoPE\n\u2713 \u2717 \u2717 \u2713 \u2713\n2023.04 RWKV [417]\nRWKV\nEng.\n14B\nN/A\nDec.-Only\nNTP\nBPE\nRoPE\n\u2713 \u2717 \u2713 \u2713 \u2713\n2023.06 Orca [378]\nMicrosoft\nEng.\n13B\nN/A\nDec.-Only\nNTP\nBPE\nRoPE\n\u2713 \u2717 \u2717\n\u2717 \u2713\n2023.07 LLaMA 2 [557]\nMeta\nEng.\n70B\n2T\nDec.-Only\nNTP\nBPE\nRoPE\n\u2713 \u2717 \u2713 \u2713 \u2713\n32\nmance distributions across all runs is less biased\nand more robust to outliers than point estimates [9].\nCurse of Dimensionality\nIn Table 2, we high-\nlight some but not all differences across models,\nas the table format constrained us. Other com-\nmon differences include the training datasets or\nfine-grained architectural details, e.g., the usage of\nmulti-head [563] or multi-query attention [494].\nWe note that a core characteristic of LLMs is\ntheir vast design space, which renders scientific\ninquiry challenging [231]. For example, by taking\ninto account the (i) data sources and their propor-\ntions within the pre-training dataset, (ii) choice\nand training hyper-parameters of the tokenizer, and\n(iii) pre-training objective, the combined design\nspace quickly becomes high-dimensional. Under-\ntaking factorial experiments within such expansive\ndesign spaces results in a combinatorially-growing\nnumber of single training runs, and the lack of suf-\nficient experimental coverage can severely inhibit\nscientific understanding of what makes an LLM\nperform well. While this issue is not unique to\nLLMs, they tend to be larger in the number of\nparameters\u2014and therefore compute requirements,\nfeedback loop times, and training costs\u2014than mod-\nels in most other fields.\no Curse of (Design) Dimensionality\nCommon design spaces of LLM experi-\nments are high-dimensional.\nOne possible way forward is to encourage the\ncommunity to use techniques like Bayesian opti-\nmization (BO) with dimensionality reduction [594,\n374], where we use a non-linear feature mapping to\nmap the input (the hyper-parameter configuration)\nonto a lower dimensional manifold followed by a\nBO procedure to optimize the underlying black-\nbox function (the LLM with respect to the hyper-\nparameters). Another suitable tool to explore the\ndesign space efficiently can be treatment effect es-\ntimation [284, 385], e.g., where the treatment is a\nvector describing certain ablations [254].\n2.16\nLack of Reproducibility\nThe reproducibility of empirical results is impor-\ntant to verify scientific claims and rule out errors\nin experimental protocols leading to such. When\nresearchers try to build upon non-reproducible re-\nsults, they might waste resources.\nUnfortunately, we stumble upon two unique re-\nproducibility issues in LLM research: repeatability\nof (i) training runs and (ii) generations by close-\nsourced API-served models. While the term \u201cre-\nproducibility\u201d is often used more broadly and can\nslightly vary in its meaning [5], in the following,\nwe focus on \u201crepeatability\u201d, which we define as the\nability to repeat experimental outcomes exactly.\nTraining Repeatability\nTypical training proto-\ncols of LLMs involve parallelism across multi-\nple compute nodes.\nThe scheduling and com-\nmunication strategies between nodes can be non-\ndeterministic [387].\nThis variability can affect\nthe final result, especially in algorithms that are\nnot \u201corder-invariant\u201d, such as stochastic gradient\ndescent (SGD). Some sources of randomness are\n(i) lock-free parallelism schemes [387], (ii) float-\ning point precision, e.g., when summing gradients\nacross devices, the order in which these sums are\ncomputed can affect the final result [171], (iii) non-\ndeterministic, performance-optimized operations,\nwhich are much faster and therefore desirable [3].\nFurther, Carlini et al. [64] point out that some\npre-training datasets consist of an index of web\ncontent that individual users must crawl themselves,\nrather than using static, standalone dumps. This is\ndue to monetary, privacy, and legal restrictions. As\na result, reproducibility can be easily compromised\nif any of the sources in the index have changed\nbetween the time the dataset curator collected them\nand the time the end-user downloads them.\no Irrepeatable Training Runs\nParallelism strategies designed to distribute\nthe training process across many accelera-\ntors are typically non-deterministic, render-\ning LLM training irreproducible.\nInference Repeatability\nAnother peculiarity of\ncommercial LLMs is that they are typically served\nvia stochastic API in a black-box setting, which\ncomes with the following challenges:\n(i) the\nprovider retains complete authority over the model\nand can introduce unpublicized changes, includ-\ning retraining the model, modifying its parame-\nters, or completely replacing it; (ii) even if model\nupdates are communicated, there is still uncer-\ntainty about whether access to specific model ver-\nsions will be maintained once they are deemed\noutdated, (iii) even with a decoding temperature\n33\nset to zero, API models often produce stochastic\noutputs [392, 464, 456].\nChen et al. [76] provide preliminary evidence\nconfirming dramatic changes in API-served models.\nThey find that GPT-3.5 and GPT-4 performances on\nfour diverse tasks vary vastly within three months\n(March to June 2023). For example, GPT-4\u2019s ac-\ncuracy in identifying prime numbers was 97.6%,\nbut in June, its accuracy dropped to 2.4%; while\nfor GPT-3.5, the trend is reversed and it got much\nbetter over time.\no Irreproducible API Inference\nAPI-served models are often irreproducible.\nAn easy fix is to rely exclusively on open-source\nLLMs [2].\n3\nApplications\nIn this section, we aim to provide practitioners with\na broad overview of the areas in which LLMs are\ncurrently being applied and highlight some com-\nmon application architectures across domains.\nAnalogous to the Challenges section, we high-\nlight the key constraints in each application area as\nfollows.\no Constraint\nThis box highlights a constraint.\n3.1\nChatbots\nGeneral-purpose chatbots (dialogue agents) com-\nbine the tasks of information retrieval, multi-turn\ninteraction, and text generation (including code).\nThoppilan et al. [551] introduced the LaMDA\nfamily of chatbot LLMs with up to 137B parame-\nters, focusing on safety (via supervised fine-tuning\non human annotations) and factual grounding (via\naccess to external knowledge sources). Notably,\nsmaller LaMDA models (2B parameters) with fine-\ntuning are shown to perform similarly on dialogue\nquality and safety/grounding scores to the larger\nLaMDA models (137B parameters) without fine-\ntuning. LaMDA models were released as part of the\nBard chatbot service [429]. However, the latest ver-\nsion of Bard now uses the PaLM 2 LLM [20, 216].\nGlaese et al. [170] propose Sparrow, a chatbot\nbased on a 70B parameter Chinchilla LLM, and\nuse RLHF (Sec. 2.9) targeting 23 rules to fine-tune\nthe model to be more helpful, correct, and harm-\nless. Sparrow also incorporates external knowledge\nusing a retrieval model to provide evidence from a\nGoogle Search query. The RLHF approach outper-\nforms the only dialogue-prompted and supervised\nfine-tuned approaches regarding output preference\nand rule violation rate.\nSimilarly, OpenAI [396] train the ChatGPT\nchatbot using supervised fine-tuning and RLHF\n(Sec. 2.9) to specialize a GPT-3.5 LLM for dia-\nlogue. GPT-4 [398] is the underlying model for the\nChatGPT Plus chatbot, but training and architec-\nture details have not been released.\nShuster et al. [508] introduce BlenderBot-3, a\n175B parameter chatbot based on the OPT-175\nLLM using supervised fine-tuning. BlenderBot-\n3 incorporates external knowledge through mod-\nules that conduct internet searches and retrieve text-\nbased long-term memories generated from previous\noutputs to help performance over long interactions.\no Maintaining Coherence\nMulti-turn interactions make Chatbots eas-\nily \u201cforget\u201d earlier parts of the conversation\nor repeat themselves [53, 451].\nK\u00f6pf et al. [274] release the OpenAssistant Con-\nversations dataset of human-annotated interactions\nand use this to instruction fine-tune Pythia and\nLLaMA models (up to 30B parameters) for chat-\nbot applications. To help align the final models,\nthe dataset is generated with guidelines to make\nthe responses polite, helpful, concise, friendly, and\nsafety-aware. The LLaMA 30B version is cur-\nrently used within the HuggingChat chatbot ap-\nplication [229].\nA key challenge of fine-tuning chatbots is cre-\nating a broad training dataset of high-quality con-\nversations. To address this problem Chen et al.\n[78] demonstrate using existing LLMs (OPT 30B)\nto generate high-quality synthetic conversation\ndatasets based on a small number of expert-written\nexamples. Human crowd workers assessed the gen-\nerated conversations to be comparable to existing\nhuman-generated datasets on the metrics: interest-\ning, coherent, natural, and consistent. Chen et al.\n[78] show the synthetic dataset can be used to fine-\ntune a chatbot (BlenderBot 400M) and achieve\nperformance only slightly below fine-tuning with\nhuman-generated datasets.\nChatbots\u2019 intended generality also makes eval-\n34\nApplications\nChatbots 3.1\nBlenderBot3 (OPT-175) [508], Bard (LaMDA, PaLM2) [551],\nSparrow (Chinchilla) [170], ChatGPT (GPT-3.5, GPT-4) [396],\nOpenAssistant (LLaMA) [274]\nGPT-4 Technical Report [398], Sparks of AGI (GPT-4) [61],\nCapabilities of ChatGPT [272]\nComputational Biology 3.2\nProteins\nESM-2 [326], ProtT5 [139], ProtST [627], CaLM [402], ProGen [352],\nIgLM [505], xTrimoPGLM [73]\nGenomics\nGenSLM [688], Nucleotide Transformers [106]\nComputer Programming 3.3\nInCoder [154], CodeGen [386], AlphaCode [313] , SantaCoder [17],\nPolycoder [626], phi-1 [182]\nCodex (GPT-3) [77]\nSelf-Debugging (Codex) [81], ViperGPT (Codex) [532],\nRepoCoder [660], Repo-Level Prompt Generator [504]\nCreative Work 3.4\nLong Form\nDramatron (Chinchilla) [368], Re3 (GPT-3) [637],\nDetailed Outline Control (GPT-3) [636]\nShort Form\nCoPoet (T5, T0) [69], Spindle - Interactive Fiction (GPT-3) [63]\nCross-lingual Short Stories (PaLM) [452], ReelFramer (GPT-4) [584]\nIdea Generation [187]\nVisual\nLayoutGPT [148], LLM Grounded Diffusion [315]\nKnowledge Work 3.5\nGalactica [548], BloombergGPT [616]\nScientific NERRE (GPT-3) [133]\nData Analysis (GPT-4) [346]\nProfessional Exams [49], News Summarization [668],\nEmail Management [550], Academic Paper Review (GPT-4) [335]\nLaw 3.6\nLegal Question Answering\nLegal Entailment (GPT-3.5) [651], Bar Examination (GPT-3.5) [50]\nExplaining Legal Concepts (GPT-4 + Retrieval) [478]\nLaw School (ChatGPT) [84], Bar Examination (GPT-4) [258]\nStatutory Reasoning (GPT-3.5) [48], Law Professor (ChatGPT) [427],\nSummarizing Judgments (GPT-3.5) [115], Litigation (ChatGPT) [234]\nCase Prediction\nUS Supreme Court (GPT-2 + GPT-3) [189]\nMedicine 3.7\nMedical Question Answering\nPubMedGPT [565], GatorTronGPT [418]\nMedPaLM(2) (PaLM) [511, 512], ChatDoctor (LLaMA) [655]\nGPT-3.5 + Retrieval [320]\nMedical Challenge Problems (GPT-4) [388],\nTriage and Diagnosis (GPT-3) [301],\nSurgical Knowledge QA (GPT-4) [393],\nSocial Media - Genetics Questions (ChatGPT) [134],\nSocial Media - General Questions (ChatGPT) [30],\nOphthalmology QA (ChatGPT) [21],\nMedical Summarization (GPT-3.5, ChatGPT) [538]\nMedical Information Retrieval\nMedical Acronym Disambiguation (T5) [448],\nAdverse Drug Event Extraction [178]\nClinical Information Extraction (InstructGPT) [10]\nReasoning 3.8\nSelf Improvement (PaLM) [222], Processed Based Fine-Tuning [560]\nDIVERSE (GPT-3.5) [312], Socratic Sub-Questions (GPT-3) [502],\nMathematical Formalization (Codex) [159]\nCausal Factors in Performance [525], Analogical Reasoning [595],\nCausal Reasoning [286, 164, 519, 244, 288],\nCommon-Sense Reasoning [562]\nRobotics 3.9\nPaLM-E [129]\nSayCan (PaLM + Scoring) [14], ChatGPT for Robotics [564],\nREFLECT (GPT-4) [338], Code as Policies (Codex) [316],\nPROGPROMPT (Codex) [510], Inner Monologue [225],\nStatler (GPT-3.5) [647]\nSocial Sciences 3.10\nUsing LLMs to Model Human Behavior [12, 176],\nAnalyzing Behavioral Characteristics of LLMs [367, 414],\nSimulating Social Relationships with LLMs [408]\nSynthetic Training Data 3.11\nAutomated Labeling (GPT-3) [583], AugGPT (ChatGPT) [104],\nLabeling + Generation (GPT-3) [123],\nInformation Retrieval (GPT-3) [51],\nDecompositional Distillation (GPT-3) [503],\nCode \u2018Textbooks\u2019 (GPT-3.5) [182], GPT3Mix [648]\nFigure 12: Overview of LLM Applications. Color = Level of Model Adaption (Pre-Trained, Fine-Tuned, Prompting\nStrategy, Evaluation).\n35\nuating their capabilities\u2019 full range difficult. Ko-\nco\u00b4n et al. [272] evaluate ChatGPT (GPT-3.5) on\n25 tasks with 38k prompts covering a diverse set\nof capabilities, including but not limited to ques-\ntion answering, emotion recognition, offensive lan-\nguage detection, spam detection, inference, and\nsentiment analysis. While ChatGPT is shown to\nhave strong performance across the 25 tasks, it usu-\nally underperforms the SOTA in that domain. More\nrecently, Bubeck et al. [61] and OpenAI [398] in-\nvestigate the capabilities of GPT-4 (base model of\nChatGPT Plus) across a wide range of tasks, in-\ncluding interactions with humans and tools. Using\nthese evaluations Bubeck et al. [61] conclude that\nGPT-4 is \u2018strikingly close to human-level perfor-\nmance\u2019 across tasks.\nFinally, the challenge of inference latency\n(Sec. 2.5) is also potentially going to become an\nimportant constraint [634] for chatbot applications\nas LLMs scale. There is a trade-off between the\nneed for responsive live user interaction in a con-\nversational format and utilizing larger LLMs [397].\no High Inference Latency\nHigh inference latency (Sec. 2.5) hinders the\nuser experience [397], especially in multi-\nturn interaction with chatbots.\n3.2\nComputational Biology\nIn computational biology, we are interested in non-\ntext data representing similar sequence modeling\nand prediction challenges.\n3.2.1\nProtein Embeddings\nOne popular application of LLM-like models in\nbiology is to generate protein embeddings from\namino-acid or genomic sequence inputs. These em-\nbeddings can then be used as inputs for structure\nprediction, novel sequence generation, and protein\nclassification tasks. Protein language models per-\nform strongly on many academic datasets, but their\napplicability to downstream tasks such as drug de-\nsign is often unclear [110].\no Transfer to Downstream Applications\nThe ultimate objective of protein language\nmodels is to deploy them in real-world\nprojects such as drug design.\nEvalua-\ntions often target smaller and/or specialized\ndatasets, not considering how the models\ncould contribute to protein design in vitro\nor in vivo.\nElnaggar et al. [139] train a range of LLM archi-\ntectures to extract embeddings from protein amino\nacid sequences. These embeddings are then used\nas inputs on supervised per-amino acid and per-\nprotein prediction tasks. The best-performing LLM\narchitecture (ProtT5) achieved SOTA results on\nper-amino acid protein secondary structure predic-\ntion without using evolutionary information. Sim-\nilarly, Wu et al. [613] predict antibody backbone\nand side-chain conformations.\nLin et al. [326] take a similar approach to train-\ning a protein LLM, the Evolutionary Scale Model\nTransformer-2 (ESM-2), on protein amino acid se-\nquences from the UniRef database using a masked\nlanguage modeling approach.\nThey show sig-\nnificant performance increases as the model is\nscaled from 8 million to 15B parameters, with\nthe largest models outperforming the ProtT5 on\nprotein structure prediction benchmarks (CASP14,\nCAMEO) [267, 457]. They also introduce ESM-\nFold, which uses the ESM-2 embedding model\nfor end-to-end atomic resolution prediction from a\nsingle sequence. While ESMFold underperforms\nthe SOTA AlphaFold2 [248] on the CAMEO and\nCASP14 benchmarks, the authors note that by rely-\ning only on embeddings ESMFold has an order of\nmagnitude faster inference time than AlphaFold2,\nusing just the protein sequence of interest rather\nthan structural templates and multiple sequence\nalignments (MSAs). Jeliazkov et al. [240] find\nthat protein sequences designed by an inverted Al-\nphaFold2 model are unlikely to be expressed, but\nsequences generated using an inverted protein LLM\nsuch as ESMFold were more likely to be expressed.\nResearchers have also adopted the ESM-1 and\nESM-2 models to generate protein embeddings\nfor enzyme-substrate chemical structural class pre-\ndiction [245], training 3D geometric graph neural\nnetworks for proteins [611], identifying disease-\ncausing mutations [337], designing novel pro-\nteins [566], and guided evolution of antibodies for\naffinity maturation [202].\n36\nChen et al. [73] propose training a new\nmodel xTrimoPGLM (100B parameters) simul-\ntaneously for protein embedding and genera-\ntion tasks using MLM and generative objectives.\nThe xTrimoPGLM-100B model (with fine-tuning\nwhere relevant) outperforms existing approaches\non 13 out of 15 evaluated tasks.\nProtein embedding models with alternative in-\nputs have also been proposed. Outeiral and Deane\n[402] train an 86 million parameter protein LLM\nCaLM (Codon adaptation Language Model) us-\ning sequences of codons (nucleotide triads) as in-\nput instead of amino acids due to codons contain-\ning potentially richer information. Madani et al.\n[352] train a 1.2B parameter protein embedding\nmodel ProGen on 280 million protein amino acid\nsequences with additional control tags specifying\nprotein properties. ProGen is then fine-tuned us-\ning data from specific protein families and applied\nto generate functional full-length amino acid se-\nquences. Similarly, Xu et al. [627] propose train-\ning a protein language model, the ProtST, on pro-\ntein sequences and additional text descriptions of\ntheir key properties for protein classification and\nretrieval tasks.\nFinally, for antibodies specifically, Shuai et al.\n[505] propose an Immunoglobulin Language\nModel (IgLM) using the GPT-2 architecture (with\n13 million parameters) for the generation of im-\nmunoglobulin sequences, using a masked language\nmodeling approach. Similar to Xu et al. [627], the\nIgLM model also takes additional conditioning tags\ncorresponding to chain type and species as input.\nThe authors show the IgLM model can then be\nused for the controllable generation of infilled and\nfull-length antibody sequences.\n3.2.2\nGenomic Analysis\nLLMs in the field of genomic analysis enable a\nbetter understanding of the effects of mutations\nin humans and predict genomic features directly\nfrom DNA sequences. While genomic language\nmodels are a promising research direction, current\nmodels cannot process many genomic sequences as\ntheir sequence lengths commonly exceed multiple\nbillions of nucleotides [390].\no Limited Context Window\nThe largest genomes have vastly longer\nDNA sequences [390] than existing ge-\nnomic LLMs\u2019 context windows can han-\ndle, constraining the types of genomes that\ncan be successfully modeled using these ap-\nproaches.\nZvyagin et al. [688] introduce a range of hier-\narchical LLMs (up to 25B parameters) with long\ninput sequences (2048 - 10,240 tokens), referred\nto as Genome-scale Language Models (GenSLMs).\nThe GenSLM models are pre-trained on Prokary-\notic gene sequences from the BV-BRC dataset us-\ning codon tokenization [402] and then fine-tuned\non SARS-CoV-2 genome sequences for the task\nof identifying potential new variants and genera-\ntive modeling. However, the authors note that it\nremains unclear whether the GenSLM architecture\ngenerates richer representations than the protein\nLLM approaches.\nDalla-Torre et al. [106] train Nucleotide Trans-\nformers with 500 million to 2.5B parameters on nu-\ncleotide sequences from human and other species\ngenomes, using a masked language modeling ap-\nproach. The Nucleotide Transformers were evalu-\nated on 18 genomic prediction tasks with fine-tuned\nlarger models achieving the best results.\nNguyen et al. [383] propose HyenaDNA, a ge-\nnomic language model based on the Hyena archi-\ntecture [430], enabling modeling of genomic se-\nquences of up to 1 million tokens. HyenaDNA\noutperforms Transformer-based models with mul-\ntiple orders of magnitude more parameters while\nincorporating the in-context learning capabilities\nof LLMs into the genomics domain.\n3.3\nComputer Programming\nOne of LLMs\u2019 most advanced and broadly adopted\napplications is generating and completing computer\nprograms in various programming languages. This\nsection deals with programming-specific LLMs\nwhere the model is fine-tuned or pre-trained ex-\nclusively for programming applications, but it is\nimportant to note the increasing use of general\nchatbots partially trained on code datasets (such\nas ChatGPT) for programming tasks.\n3.3.1\nCode Generation\nCode generation refers to using an LLM to output\nnew code for a given specification or problem pro-\n37\nvided as a prompt. Several computer programming-\nspecific LLMs and approaches have been proposed.\nFor Python code generation, Chen et al. [77]\nintroduce Codex, a fine-tuned GPT-3 LLM (up\nto 12B parameters) specialized to generate stand-\nalone Python functions from doc strings. Fine-\ntuning was conducted using a raw dataset of 159\nGB of Python source code from GitHub and a fil-\ntered dataset of correctly implemented standalone\nPython functions. Codex models outperformed\nsimilarly sized GPT-3 and GPT-J models on the\nHumanEval evaluation set, with the Codex model\ntrained on the filtered dataset (Codex-S) achieving\nthe best results. Importantly, Chen et al. [77] note\nthat there was no observed improvement from us-\ning a pre-trained GPT-3 model as a base other than\nfaster convergence.\nChen et al. [81] seek to improve the performance\nof Codex through a self-debugging prompting ap-\nproach. Three forms of self-debugging are inves-\ntigated. Simple feedback prompts the model to\ndecide whether the generated code solution is cor-\nrect. Unit-test feedback prompts the model with\nthe output of unit tests provided in the problem\ndescription. Code explanation feedback prompts\nthe model to explain the solution in detail and use\nthe explanation to correct the solution. In each\ncase, this process is repeated iteratively until the\nmodel provides a solution it states is correct or\na maximum number of attempts has been made.\nCodex using the self-debugging prompting frame-\nwork with code explanation (and unit-testing if\napplicable) outperforms the base Codex model on\nC++-to-Python translation, text-to-SQL generation,\nand text-to-Python generation.\nGunasekar et al. [182] train a smaller model Phi-\n1 (1.3B parameters) to generate Python functions\nfrom doc strings. Training phi-1 using a combina-\ntion of filtered existing datasets and new synthetic\ntextbook and exercise datasets results in a model\nthat can achieve near current SOTA results on Hu-\nmanEval while having over an order of magnitude\nfewer parameters and tokens than previous works.\nAnother area of interest has been the develop-\nment of multilingual programming LLMs. Xu et al.\n[626] evaluate a range of code generation LLMs\nand train a new multilingual LLM Polycoder (2.7B\nparameters) using source code from 12 languages.\nHowever, for Python specifically, Codex outper-\nforms Polycoder and other existing models (GPT-J,\nGPT-Neo, and CodeParrot) on HumanEval.\nNijkamp et al. [386] train the CodeGen family\nof LLMs (up to 16B parameters) using a combi-\nnation of three datasets: natural language, multi-\nlingual programming source code (C, C++, Go,\nJava, JavaScript, and Python), and a monolingual\nPython dataset. The largest CodeGen model using\nthe monolingual training set was shown to outper-\nform the Codex-12B model. Nijkamp et al. [386]\nalso test CodeGen on multi-step program synthesis,\nwhere a program is broken down into multi-step\nnatural language prompts, which the model then\nimplements individually (creating the new Multi-\nTurn Programming Benchmark (MTPB)).\nFinally, Li et al. [313] focus on the task of\nsolving competitive programming questions (Code-\nforces, Description2Code, and CodeNet). The Al-\nphaCode LLM (up to 41B parameters) is first pre-\ntrained on a multilingual dataset (C++, C#, Go,\nJava, JavaScript, Lua, PHP, Python, Ruby, Rust,\nScala, and TypeScript) of 715 GB of source code\nfrom GitHub. It is then fine-tuned using a new\ncurated dataset of competitive programming prob-\nlems called CodeContests. To achieve high per-\nformance, Li et al. [313] use large-scale sampling\n(up to millions of samples), filtering, and clustering\nof candidate solutions generated by AlphaCode to\nselect the final submissions.\nHowever, whilst these existing code-generation\nLLMs have achieved impressive results, a criti-\ncal current constraint in applying LLMs to code\ngeneration is the inability to fit the full code base\nand dependencies within the context window. To\ndeal with this constraint, a few frameworks have\nbeen proposed to retrieve relevant information or\nabstract the relevant information into an API defi-\nnition.\no Long-Range Dependencies [660, 504]\nLong-range dependencies across a code\nrepository usually cannot be regarded be-\ncause of limited context lengths (Sec. 2.6).\nZhang et al. [660] introduce RepoCoder, a\nretrieval-based framework for repository-level code\ncompletion that allows an LLM to consider the\nbroader context of the repository. A multi-step\nretrieval-augmented generation approach is taken,\nwhere the initial code generated is then used to re-\ntrieve further, potentially more relevant, repository\ncode snippets to refine the final output. This ap-\nproach can be considered a retrieval-based method\n38\nfor relieving the long-range dependency constraint.\nSimilarly, Shrivastava et al. [504] propose the\nRepo-Level Prompt Generator (RLPG) framework\nto dynamically retrieve relevant repository context\nand construct the correct prompt for a given com-\npletion task. To do this, many prompt proposals\nare generated from different prompt sources (e.g.,\nparent class) and prompt contexts (e.g., method\nnames). The best prompt is then selected by a\nprompt proposal classifier and combined with the\ndefault context to generate the final output.\nFinally, Sur\u00eds et al. [532] create the ViperGPT\nframework, which utilizes the Codex LLM to gener-\nate programs that answer text-based visual queries.\nThe Codex model is prompted with the query text\nand an API specification to do this. The human-\ngenerated API specification provides functions de-\nsigned to deal with low-level visual tasks (e.g.,\nfind(object)) that the LLM can then use to gen-\nerate solution code. This approach significantly\nreduces the tokens needed to provide repository/-\ncode context by only providing the API definition.\nThis API definition approach, illustrated in 13 has\nbeen used in robotics by Vemprala et al. [564], and\nby Wang et al. [579] as part of a Minecraft agent.\nPreviously, Gupta and Kembhavi [185] used a pre-\ndefined function approach within VISPROG, which\nuses GPT-3, external python modules, and few-shot\nprompting with example programs to solve visual\ntasks.\n3.3.2\nCode Infilling and Generation\nCode infilling refers to modifying or completing\nexisting code snippets based on the code context\nand instructions provided as a prompt.\nFried et al. [154] train the InCoder LLM (up\nto 6.7B parameters) to both generate Python code\nand infill existing code using a masked language\nmodeling approach. Incoder is trained using 159\nGB of text split roughly equally between Python\nsource code, StackOverflow content, and source\ncode in other languages. On the HumanEval gener-\nation benchmark, InCoder underperforms the best-\nperforming Codex and CodeGen models. However,\nunlike the other models, InCoder can perform sin-\ngle and multi-line infilling of existing code.\nSimilarly, Allal et al. [17] train a set of smaller\nSantaCoder models (1.1B parameters) for code gen-\neration and code infilling using 268 GB of Python,\nJavaScript, and Java source code. SantaCoder is\nprimarily evaluated on the MultiPL-E benchmark\n(an extension of HumanEval and MBPP [28] bench-\nLLM\nUsing the API functions \nprovided, write a program \nthat\u2026\n \nPrompt \ndef locate_item(item_name):\n    \"\"\" Returns x,y,z of item \"\"\"\ndef move_to_location(x, y, z):\n    \"\"\" Moves to x,y,z coordinates\"\"\"\ndef drop_item(item_name):\n    \"\"\" Removes item from inventory\"\"\"\nAPI De\ufb01ntion \nmove_to_location(10, 20, 0)\nlocate_item('apple')\nmove_to_location(5, 10, 15)\ndrop_item('apple')\nOutput\n def drop_item(item_name):\n    \"\"\" Removes item from inventory\"\"\"\n    item_list.remove(item_name)\nAPI Implementation Store\nFunction \nImplementation\nSelf-\ndebugging\nFigure 13: API Definition Framework. Illustration of\nproviding a general API definition in the prompt [532,\n579, 564] to enable the consistent use of either external\ncode or tools to solve the specific task whilst minimiz-\ning the required context window. Extensions to this ap-\nproach have included asking the LLM to implement the\nfunctions within the API definition (red) and to prompt\nthe LLM to self-debug any API code that does not exe-\ncute (green).\nmarks), with it shown to outperform InCoder on\nboth HumanEval generation and infilling (passing\nover 100 attempts).\nCode infilling is particularly relevant for applica-\ntions involving modifying, reviewing, or debugging\nexisting code. Maniatis and Tarlow [357] explore\nthe data from the intermediary steps in the develop-\nment process to help automatically resolve reviewer\ncomments [155]. The Dynamic Integrated Devel-\noper ACTivity (DIDACT) methodology formalizes\ntasks in the software development process (e.g., re-\npairing builds, predicting reviewer comments, etc.)\ninto state, intent, and action components, and trains\nthe model to predict code modifications. This ap-\nproach aims to train the model to understand the\nprocess of software development rather than only\nthe end product.\n3.4\nCreative Work\nFor creative tasks, LLMs have primarily been ap-\nplied to story and script generation.\nFor long-form story generation, Mirowski\net al. [368] propose using a 70B Chinchilla-\noptimal [206] LLM Dramatron with prompting,\nprompt chaining, and hierarchical generation to\ncreate complete scripts and screenplays without\nthe requirement for a human-in-the-loop (although\nco-writing is facilitated). The ability of Dramatron\nto help create a script was evaluated qualitatively\n39\nthrough co-writing and follow-up interviews with\n15 industry experts.\nSimilarly, Yang et al. [637] propose using GPT-3\nwith a Recursive Reprompting and Revision frame-\nwork (Re3) to generate stories over 2,000 words\nlong. The Re3 approach uses zero-shot prompting\nwith GPT-3 to generate a plan (settings, characters,\noutline, etc.). It then recursively prompts GPT-3 to\ngenerate story continuations using a specified dy-\nnamic prompting procedure. Possible story contin-\nuations are then ranked for coherence and relevance\nusing separate fine-tuned Longformer models as\npart of a Rewrite module. Finally, local edits to\nthe selected continuations are made by detecting\nfactual inconsistencies using the combination of a\nGPT-3 model [403] and a BART model [303] as\npart of an Edit module. This process can then be\niterated for fully automated story generation.\nFinally, Yang et al. [636] introduce the Detailed\nOutline Control (DOC) framework to maintain plot\ncoherence over thousands of words using GPT-3.\nWhile DOC uses the same high-level planning-\ndrafting-revision approach as Re3, it implements\nthis through the use of a detailed outliner and de-\ntailed controller. The detailed outliner first breaks\ndown the high-level outline into subsections us-\ning a breadth-first approach, with candidate gen-\nerations for the subsections created, filtered, and\nranked. The bodies of the detailed outline subsec-\ntions are then generated iteratively using a struc-\ntured prompting approach. During the generation,\nan OPT-based FUDGE [635] detailed controller is\nused to help maintain relevance.\nIn each case, to apply LLMs to long-form story\ngeneration, the task is broken down into a series of\nshort-form sub-tasks (14). The current capabilities\nof LLMs primarily drive this approach, but also\nthe desire to have a human-in-the-loop for some\nco-writing use cases [368].\no Limited Context Window [368, 637]\nThe inability of current LLMs to keep the\nentire generated work within the context\nwindow currently constrains their long-form\napplications and generates the need for mod-\nular prompting (14).\nFor short form generation, Chakrabarty et al.\n[69] propose CoPoet (fine-tuned T5 and T0 models)\nfor collaborative poetry generation, Razumovskaia\net al. [452] use PaLM and prompting with plans\nfor cross-lingual short story generation, Wang et al.\n[584] use GPT-4 as part of the ReelFramer tool to\nhelp co-create news reels for social media, Ippolito\net al. [232] use LaMDA as part of the Wordcraft cre-\native writing assistant, and Calderwood et al. [63]\napply a fine-tuned GPT-3 model as part of their\nSpindle tool for helping generate choice-based in-\nteractive fiction.\nFor more general creative tasks,\nHaase and\nHanel [187] assess a range of LLMs (including\nChatGPT) on their capacity for idea generation (ev-\neryday creativity) using the Alternative Uses Test\n(generating alternative uses for given items). On\nthis task, LLMs were found to perform comparably\nto 100 human participants.\nFinally, for visual creative tasks, LLMs have also\nbeen used to increase the level of control users have\nwhen using image generation models. Feng et al.\n[148] propose the LayoutGPT method where an\nLLM (GPT-3.5, GPT-4 or Codex) is used to gener-\nate a CSS Structure layout the image should follow\nbased on a text-based user prompt. This layout\ncan be visualized and used as input to guide an\nimage generation model. This approach performs\nstrongly on text-to-image generation and indoor\nscene synthesis. A similar concept is implemented\nby Lian et al. [315], where an LLM (GPT-3.5) is\nused to generate natural language layouts (bound-\ning boxes and descriptions) to guide a diffusion\nmodel. Using an LLM as part of a modality conver-\nsion framework 16 has also been used in robotics\n[338, 225] and knowledge work [329].\n3.5\nKnowledge Work\nWith\nresearchers\nincreasingly\ndemonstrating\nLLMs\u2019 ability to perform well on domain-specific\nknowledge tasks such as within Law [258] or\nMedicine [512], interest has grown in LLMs\u2019 ca-\npacity for wider knowledge work. These applica-\ntions are likely to be found across the labor market\nwith Eloundou et al. [140] estimating that 80% of\nthe US workforce is in roles where at least 10% of\ntasks could be affected by LLMs.\nIn the professional services field, Bommarito\net al. [49] evaluate GPT-3.5 and previous GPT ver-\nsions on actual and synthetic questions from the\nUniform CPA Examination Regulation section and\nAICPA Blueprints for legal, financial, accounting,\ntechnology, and ethical tasks. Using only zero-shot\nprompting, the best performing model (latest GPT-\n3.5) struggles with quantitative reasoning, achiev-\n40\nModule 3\nModule 2\nLLM\nOutput\nGeneral Prompt\n Pre-processing\nModule 1\nLLM\nOutput\nGeneral Prompt\nUser Prompt\nLLM\nOutput\nPre-processing\nGeneral Prompt\nRe-run\nResidual\nEg., Generate a plot outline \nfor a new novel as paragraph \nheadings\nEg., Using the outline, \ngenerate a draft for the xth \nparagraph heading\nEg., Check the spelling and \nconsistency of this paragraph \ngiven the outline and plot \nsummary\nIterate\nFigure 14: Modular Prompting. Illustration of using\na series of separate prompts [368, 637, 368, 579, 584]\nand processing steps to enable an LLM to perform tasks\nthat would either not fit in a single context window or\ncould not easily be specified in a single prompting step.\ning results similar to random guessing on multiple-\nchoice questions. However, on qualitative sections,\nGPT-3.5 achieved 50-70% accuracy, significantly\nahead of random guessing and approaching human-\nlevel scores.\no Numerical Reasoning [436, 49]\nLLMs have generally seen worse perfor-\nmance on quantitative tasks, potentially con-\nstraining their applications in knowledge\nwork areas such as financial services or ac-\ncounting.\nWu et al. [616] train BloombergGPT (50B\nparameters) for various financial knowledge\nwork, including sentiment analysis, classifica-\ntion, NER/NED, and financial question answering.\nBloombergGPT is shown to outperform the OPT\n(66B parameters), GPT-NeoX, and BLOOM (176B\nparameters) LLMs on these financial domain-\nspecific tasks and performs competitively on\nbroader benchmarks.\nThiergart et al. [550] considers the applicability\nof GPT-3 to the task of email management, includ-\ning classification, information extraction (NER),\nand generating response text. Whilst it is noted\nthat GPT-3 has the capacity for all three tasks, the\nauthor highlights current issues around reliability,\nlack of access to internal data, and the need for a\nhuman in the loop.\nLiu et al. [329] propose enabling LLMs to un-\nderstand charts and plots by first using a vision\nplot-to-text translation model (DePlot) to decom-\npose the chart into a linearized data table. Once the\nchart or plot has been converted into a text-based\ndata table, it is combined with the prompt and pro-\nvided to a Flan-PaLM, Codex, or GPT-3.5 LLM. A\nsimilar modality conversion 16 approach has also\nbeen used in robotics [338, 225] for sensor data.\nZhang et al. [668] evaluate a range of LLMs\n(GPT-3, InstructGPT, OPT, GLM, Cohere, and An-\nthropic) on the task of news summarization. On\nthe DM/CNN and XSUM benchmarks, instruction\nfine-tuned models (InstructGPT) perform the best\nacross summarization faithfulness, relevance, and\ncoherence. To evaluate against human capabil-\nity Zhang et al. [668] collect reference summa-\nrizations for 100 articles from 6 freelance writers.\nZero-shot InstructGPT-3 performs comparably to\nthe freelance writers across the three metrics.\nCheng et al. [82] investigate GPT-4\u2019s capacity to\nperform data analysis and compare it to human an-\nalysts. GPT-4 is combined with a modular prompt-\ning framework 14 with three steps, code generation\n(SQL and Python), code execution (\u201ccollect data\nand output figures\u201d, etc.), and analysis generation\n(\u201cgenerate five bullet points about the analysis\u201d).\nWhile GPT-4 performs well, it currently underper-\nforms experienced human data analysts on tasks\nfrom NvBench [346].\nFor scientific knowledge work, Taylor et al.\n[548] train the Galactica LLM specifically on sci-\nentific text for tasks such as scientific knowledge\nrecall, reasoning, citation prediction, and scientific\nQ&A. In addition to a domain-specific training\ncorpus, Galactica is specialized in the scientific do-\n41\nmain through the use of specialized tokens, work-\ning memory, and prompt-pre-training.\nDunn et al. [133] propose fine-tuning GPT-3 for\nscientific combined named entity recognition and\nrelation extraction (LLM-NERRE). First, 100 to\n1,000 manually annotated prompt-completion pairs\nare created by humans. These examples are then\nused to fine-tune a GPT-3 model for the specific\nNERRE task.\nFinally, Liu and Shah [335] evaluate GPT-4\u2019s\nability to review academic papers, specifically:\nidentifying errors, verifying author checklists, and\nselecting the better abstract. GPT-4 shows some\ncapacity to detect errors, with 7 out of 13 errors\ndetected, and verify author checklists, with 87%\naccuracy. However, GPT-4 is shown to have lim-\nited capacity for distinguishing the better paper\nabstract.\n3.6\nLaw\nApplications of LLMs within the legal domain\nshare many similarities with medicine, including\nlegal question answering [651, 258] and legal in-\nformation extraction [71]. However, other domain-\nspecific applications have been proposed, such as\ncase outcome prediction [189], legal research [234],\nand legal text generation [423].\n3.6.1\nLegal Question Answering and\nComprehension\nKey tasks of the legal field are finding related prece-\ndents, answering legal questions, and comparing\nexisting documents or statutes.\nUsing a general-purpose LLM with prompting\napproach, Yu et al. [651] use GPT-3.5 with zero-\nshot, few-shot, and CoT prompting to achieve\nSOTA performance on the legal entailment task\n(identifying the relevant statutes and determining\nif a given premise is correct) in the Competition\non Legal Information Extraction/Entailment (COL-\nIEE) dataset [437]. They also investigate a GPT-3.5\nversion fine-tuned using the COLIEE training set\nwith and without explanations but find the zero- and\nfew-shot legal prompting approaches perform best.\nSimilarly, Rosa et al. [460] use a general monoT5\nmodel with zero-shot prompting on the COLIEE\nentailment task.\nOn the US legal Uniform Bar Examination\n(UBE), Bommarito II and Katz [50] show that GPT-\n3.5 with zero-shot prompting can achieve 50% on\nthe multiple choice Multistate Bar Examination\ncomponent, but note that fine-tuning the model\non relevant examples does not appear to improve\nperformance.\nMore recently, Katz et al. [258]\nshow that GPT-4 with zero-shot prompting exhibits\nSOTA performance on the full UBE, including the\nmultiple choice, essay, and performance test com-\nponents, and achieves passing scores.\nBlair-Stanek et al. [48] assess GPT-3.5\u2019s abil-\nity to reason about legal facts and statutes us-\ning the StAtutory Reasoning Assessment (SARA)\ndataset [208]. GPT-3.5 is shown to have SOTA per-\nformance but with significant variation depending\non the type of prompting used (zero-shot, few-shot,\nand CoT). GPT-3.5 was also shown to perform rela-\ntively poorly on synthetic statutory reasoning tasks.\nChoi et al. [84] evaluate ChatGPT (GPT-3.5)\non 95 multiple-choice and 12 essay questions from\nthe final exams at the University of Minnesota law\nschool. ChatGPT was found to perform at the level\nof a C+ student, near the bottom of the class, but\nwith passing scores.\no Out of Date Information\nDue to regularly updated laws and new\nprecedents, the training/retrieval data be-\ncome outdated frequently [195].\nFinally, many more specific legal question-\nanswering applications have been proposed, in-\ncluding: explaining legal concepts (GPT-4 + re-\ntrieval) [478], summarizing legal judgments (GPT-\n3.5) [115], litigation research and drafting [234],\nand helping full-fill the tasks of a law professor\n(ChatGPT) [427].\n3.6.2\nCase Prediction and Legal Text\nGeneration\nCase prediction and legal text generation involve\npredicting or completing legal opinions. Whilst\nthere is currently sparse usage of LLMs in the liter-\nature, smaller language models have been applied,\nsuggesting potential future LLM applications in\nthis area.\nHamilton [189] use nine separate GPT-2 models\ntrained on individual supreme court justice\u2019s au-\nthored opinions to predict how each justice will\nvote on a given case.\nThey use a handcrafted\nprompt, including a summary of the topic gener-\nated by GPT-3. However, they find this approach\nto case prediction does not match the SOTA.\nPreviously, Chalkidis et al. [70] trained a range\nof attention-based models (including BERT) to pre-\n42\ndict case outcomes from the European Court of\nHuman Rights (ECHR). The attention-based mod-\nels outperformed an SVM with a bag of words\napproach for binary violation classification, multi-\nlabel violation classification, and case importance\nprediction.\nFinally, Peric et al. [423] use a dataset of 50,000\njudicial opinions from U.S. Circuit Courts to train\na Transformer-XL model and fine-tune a GPT-2\nmodel. The models were then evaluated for their\nability to complete a judicial opinion, with a start\ngiven as a prompt. In qualitative evaluations, hu-\nman participants struggled distinguishing between\nmachine-generated and genuine text.\n3.7\nMedicine\nMany applications of LLMs have been proposed\nin the medical domain, including medical ques-\ntion answering [511, 512, 320, 655, 388], clinical\ninformation extraction [10, 448], indexing [650],\ntriage [491, 301], and management of health\nrecords [276].\n3.7.1\nMedical Question Answering and\nComprehension\nMedical question answering and comprehension\nconsists of generating multiple-choice and free-text\nresponses to medical questions.\nSinghal et al. [511] proposed using few-shot,\nCoT, and self-consistency prompting to specialize\nthe general-purpose PaLM LLM to medical ques-\ntion answering and comprehension. They demon-\nstrate a Flan-PaLM model [93] using a combination\nof the three prompting strategies to achieve the pre-\nvious SOTA results on the MedQA, MedMCQA,\nPubMedQA, and MMLU medical datasets. To fur-\nther align the model to the medical domain, they\nproposed Med-PaLM, which utilizes instruction\nprompt-tuning based on 40 examples from a panel\nof clinicians and task-specific human-engineered\nprompts.\nSinghal et al. [512] then extend the Med-PaLM\napproach with Med-PaLM 2 using the newer PaLM\n2 LLM as its base model. Singhal et al. [512]\nconduct further instruction-fine tuning and use a\nnew ensemble refinement (ER) prompting strategy\n(where stochastically sampled outputs are first gen-\nerated and provided within the final prompt). This\nallows Med-PaLM 2 to achieve the current SOTA\non the MultiMedQA benchmark.\nLi\u00e9vin et al. [320] adopt a similar approach us-\ning zero-shot, few-shot, and CoT prompting to\nadapt the GPT-3.5 LLM to medical question an-\nswering (USMLE and MedMCQA) and compre-\nhension (PubMedQA) tasks. In addition, Li\u00e9vin\net al. [320] propose using retrieval augmentation\nwhere relevant text from Wikipedia is retrieved\nand included in the prompt. More recently, Nori\net al. [388] evaluated GPT-4 on USMLE and Mul-\ntiMedQA datasets using zero and few shot prompt-\ning. GPT-4 is found to outperform GPT-3.5 across\nbenchmarks significantly. However, several issues\nrelating to using GPT-4 for real-world clinical ap-\nplications are raised, including the risks of erro-\nneous generations and the risks of bias. Tang et al.\n[538] raise similar issues and find that GPT-3.5 and\nChatGPT have issues with factual accuracy and\nrepresenting the level of certainty during medical\nsummarization.\no Hallucination and Bias [538, 388, 511]\nThe safety-critical nature of the medical do-\nmain means the possibility of hallucinations\nsignificantly limits the current use cases.\nFurther work is also needed to reduce the\nrisk of LLMs perpetuating existing bias in\nclinical datasets.\nYunxiang et al. [655] fine-tune a LLaMA LLM\nChatDoctor (7B parameters) specifically for the\ntask of medical question answering. To specialize\nthe LLaMA model, it is first instruction fine-tuned\nusing the Alpaca dataset [540] and then fine-tuned\nto the medical domain using a dataset of 100k pa-\ntient conversations. Similarly to Li\u00e9vin et al. [320],\nChatDoctor is augmented with two external knowl-\nedge sources (a disease database and Wikipedia) to\nimprove the factual grounding of the model.\nInstead of using general models with specialized\nprompting or fine-tuning, Venigalla et al. [565]\ntrain a new model PubMedGPT specifically for\nmedical question answering and text generation\ntasks. PubMedGPT is trained using a combina-\ntion of PubMed abstracts and full documents from\nthe Pile [165]. Peng et al. [418] also train a new\nLLM GatorTronGPT (up to 20B parameters) for\nbiomedical question answering and relation extrac-\ntion using a mixture of clinical and general English\ntext. Whilst these approaches outperformed exist-\ning smaller specific purpose models [177, 644] in\nmedical question answering, they currently under-\nperform the larger general purpose LLMs (GPT-\n3.5/4 and MedPaLM 1/2). However, there remains\n43\ndebate over whether larger general or specialized\nclinical models are the best approach. Looking\nat models up to GPT-3, Lehman et al. [297] ques-\ntion the effectiveness of LLM in-context learning\napproaches by showing that small specialized clin-\nical models fine-tuned on limited annotated data\noutperform the former.\nFinally, LLMs have also been applied to a range\nof more specific medical question-answering tasks,\nincluding evaluating GPT-3 on its\u2019 ability to triage\nand diagnose cases [301], responding to social me-\ndia genetics [134] and general [30] patient ques-\ntions (ChatGPT), answering questions from the\nKorean general surgery board exams (GPT-3.5,\nGPT-4) [393], consultation and medical note tak-\ning [296], and answering ophthalmology questions\n[21].\n3.7.2\nMedical Information Retrieval\nMedical text often contains domain-specific abbre-\nviations, acronyms, and technical terms presenting\nspecific information retrieval challenges. This has\nled LLMs also to be applied to help structure and\nextract data from medical sources.\nAgrawal et al. [10] use InstructGPT (GPT-3)\nwith prompt templates (zero- and one-shot) for clin-\nical information extraction, such as extracting med-\nication dosage and frequency from medical notes\nor disambiguation of medical acronyms. They also\nintroduce two methods for converting the LLM\noutput into a structured format using a verbilizer\nfor mapping to classification labels and a resolver\nfor more complex structured outputs such as lists\n(GPT-3 + R).\nRajkomar et al. [448] take a different approach\nby treating medical acronym disambiguation as\na translation task and training a specialized end-\nto-end T5 LLM. To preserve privacy, they also\nuse a training dataset generated from public web\npages (without medical acronyms) and web-scale\nreverse substitution of medical acronyms, with only\nevaluation done on actual clinical notes.\nFinally, Gu et al. [178] use GPT-3.5 and knowl-\nedge distillation to train a PubMedBERT model\nfor adverse drug event extraction (entity and rela-\ntion). The distilled PubMedBERT model outper-\nforms GPT-3.5 and GPT-4, and performs similarly\nto specialized models that use supervised learning.\n3.8\nReasoning\nMathematical and algorithmic tasks often require\na different set of capabilities than traditional NLP\ntasks, such as understanding mathematical opera-\ntions, complex multi-step reasoning, and longer-\nterm planning.\nTherefore, the applicability of\nLLMs to these tasks, and methods for improving\ntheir capabilities, is an active area of research.\nFor mathematical reasoning tasks, Uesato et al.\n[560] test a range of fine-tuning (supervised and\nRLHF), prompting (zero-shot and few-shot), and\nre-ranking (majority voting and reward model) to\nevaluate whether they improve a base LLM\u2019s (70B\nparameters) ability to generate accurate reason-\ning steps on word-based maths problems in the\nGSM8K dataset [95]. Whilst fine-tuning on in-\ntermediate steps (\u201cprocess-based\u201d) performs simi-\nlarly to using only final answers (\u201coutcome-based\u201d)\non final answer correctness, processed-based ap-\nproaches are found to generate significantly fewer\nerrors in reasoning.\nHuang et al. [222] take this a step further by\nshowing that the mathematical reasoning ability\nof a PaLM LLM on the GSM8K dataset can be\nself-improved through fine-tuning on a dataset of\nhigh-confidence reasoning paths generated by the\nsame PaLM base model.\nUsing only prompting, Kojima et al. [273] find\nthat zero-shot CoT prompting alone significantly\nimproves the performance of GPT-3 and PaLM\nLLMs over standard zero- and few-shot prompting\non the MultiArith and GSM8K datasets. While Li\net al. [312] introduce DIVERSE, a prompting ap-\nproach that uses a diverse set of prompts for each\nquestion and a trained verifier (with reasoning step\nawareness) to improve further GPT-3.5\u2019s perfor-\nmance on GSM8K and other reasoning bench-\nmarks. Finally, Shridhar et al. [502] take a novel\napproach by training new models to break down\na mathematical word problem into Socratic sub-\nquestions to guide the answer of either other LLMs\nor human learners. GPT-3 prompted with these sub-\nquestions outperforms simple one-shot prompting\non the GSM8K dataset.\nStolfo et al. [525] evaluate a range of LLMs (in-\ncluding GPT-3) at mathematical reasoning using\na new framework to understand the causal impact\nof different input factors (e.g framing, operands,\nand operations). Instruction fine-tuned GPT-3 mod-\nels are found to be significantly more robust and\nsensitive than the smaller LLMs evaluated.\nOther LLM use cases in algorithmic and mathe-\nmatical reasoning have also been proposed. Gadgil\net al. [159] apply a Codex LLM with prompt en-\n44\ngineering and filtering to the task of mathemati-\ncal formalization (in the context of theorem prov-\ning). Webb et al. [595] evaluate GPT-3.5\u2019s capacity\nfor analogical reasoning using tasks that emulate\nRaven\u2019s Standard Progressive Matrices (SPM), let-\nter string analogies, and verbal analogies. GPT-3.5\nis shown to generally outperform human partic-\nipants (undergraduates) at matrix reasoning and\nverbal analogies, but with more mixed results on\nletter string analogies. Yu et al. [654] introduce\nthe ALERT benchmark to evaluate LLM reason-\ning across ten skills (logistic, causal, common-\nsense, abductive, spatial, analogical, argument,\nand deductive reasoning, as well as textual entail-\nment and mathematics). Ruis et al. [464] study\nLLMs\u2019 capability to interpret implicatures, for ex-\nample, whether they understand the response \"I\nwore gloves\" to the question \u201cDid you leave finger-\nprints?\u201d as meaning \u201cNo\u201d; finding that lots of mod-\nels perform close to random. Finally, Valmeekam\net al. [562] propose a new assessment framework\nfor common-sense planning and find that existing\nLLMs GPT-3.5 and BLOOM perform poorly. Us-\ning the framework for the Blocksworld domain\n(planning tasks with different colored blocks on\na surface), the best GPT-3.5 model only came up\nwith a valid plan 5% of the time, compared to 78%\nof human participants.\no Sub-Human-Performance [562, 607]\nExisting LLMs struggle to match human\nperformance on reasoning benchmarks.\nAnother line of work has investigated the in-\ntersection of LLMs and causal reasoning [425,\n253]. K\u0131c\u0131man et al. [286] argue that GPT-3.5/4\noutperform existing algorithms in three causal\nbenchmarks. In contrast, Gao et al. [164] evalu-\nate ChatGPT on three causal reasoning tasks (dis-\ntinct from K\u0131c\u0131man et al. [286]) and find that it\nperforms rather poorly; further, few-shot and chain-\nof-thought prompting sometimes further exacer-\nbates its performance. Srivastava et al. [519] pro-\npose 14 causal reasoning tasks, some of which are\nconsidered to be very hard [534]. Similarly, Jin\net al. [244] curate another causal inference task\nand posit that current LLMs still fail to general-\nize. Lampinen et al. [288] study whether LLMs\ncan generalize causal intervention strategies from\nfew-shot examples.\nWillig et al. [607] conjec-\nture that current LLMs are \u201ccausal parrots\u201d, simply\nreciting causal knowledge embedded in their data\nrather than doing causal reasoning [253].\nOverall, while LLMs show some capacity for\nmore complex reasoning, the relatively poor per-\nformance of LLMs on a number of reasoning tasks\nand benchmarks [562, 164, 244] stands in contrast\nto the often human level performance being seen\nin other capabilities [61, 263].\n3.9\nRobotics and Embodied Agents\nLLMs have also started to be incorporated into\nrobotics applications to provide high-level planning\nand contextual knowledge.\nAhn et al. [14] implement a PaLM-540B LLM in\nthe SayCan architecture to break down high-level\ntext-based instructions into a sequence of lower-\nlevel robot tasks that can be executed. The authors\nuse the LLM to propose possible next actions via it-\neratively scoring the most likely of a defined set of\nlow-level tasks based on the high-level text input.\nThe low-level task to be executed is then deter-\nmined by combining the low-level tasks proposed\nby the LLM with affordance functions which de-\ntermine the probability of the robot completing the\ntask given the current low-level context.\nDriess et al. [129] take this concept a step fur-\nther by combining the PaLM-540B LLM with ad-\nditional input modalities (22B parameter vision\ntransformer) to create the PaLM-E model. By in-\ntroducing images into the input, the PaLM-E model\ncan predict which low-level tasks are possible given\nthe current state, whether the previous low-level\ntasks executed failed, and incorporate images into\nlong-horizon planning, allowing it to outperform\nthe original SayCan results.\nAnother approach has been to use LLMs to gen-\nerate code for robotics tasks. Vemprala et al. [564]\ncombine ChatGPT with a pre-defined high-level\nfunction library of robotic capabilities for human\non the loop robotics tasks. By providing details of\nthe function library in the prompt, ChatGPT is then\nshown to be able to break down high-level natu-\nral language instructions into a set of lower-level\nfunction calls, which can then be executed on the\nrobot if the human is satisfied it is accurate. This is\nanother example of the API definition 13 approach,\nalso used in computer programming [532]. Other\nrelated works that use LLMs to generate code for\nrobotics applications include using an LLM for hi-\nerarchical code generation to write robot policies\n(Codex) [316], to generate code policies and main-\n45\ntain a written state (GPT-3.5) [647], and using an\nLLM for code-based task planning (GPT-3, Codex)\n[510].\nFinally, LLMs have also been combined with\nmodality-to-text pre-processing to provide the\nLLM with additional input from the robot\u2019s en-\nvironment. Liu et al. [338] use GPT-4 as part of the\nREFLECT framework for detecting and explaining\nrobot failures. To achieve this, multi-modal sensory\ninputs are first converted into a text-based hierar-\nchical summary at the sensory, event, and sub-goal\nlevels. The hierarchical summary then prompts\nthe LLM to detect and analyze failures. Similarly,\nHuang et al. [225] combine an LLM (InstructGPT,\nPaLM) with multiple sources of text-based environ-\nment feedback for robotic task planning.\no Single Modality [338, 14, 564]\nWhile LLMs can help robots or agents un-\nderstand instructions and add high-level\nplanning capabilities, their inability to di-\nrectly learn from image, audio or other sen-\nsor modalities constrain their applications.\nFor agents in simulated worlds, Wang et al.\n[579] use the GPT-4 LLM within their VOYAGER\nframework to create a Minecraft agent that can\nautonomously explore, acquire new skills and com-\nplete tasks. First, they use GPT-4 to propose new\ntasks for the agent to complete as part of the au-\ntomatic curriculum. Then, they ask it to generate\ncode to solve the proposed task given the current\nstate to add to its skills library, which can then be\nused in the future (similar to the API approach 13\nused by Vemprala et al. [564]). Finally, the authors\nuse GPT-4 to verify whether the executed code\nhas achieved the proposed task. This framework\noutperforms prompting approaches such as ReAct,\nReflexion, and AutoGPT (Sec. 2.7).\nPrior work using LLMs for planning in simu-\nlated worlds include: Wang et al. [591] using GPT-\n3 for Minecraft, Huang et al. [224] using GPT-3\nand Codex in VirtualHome, and Nottingham et al.\n[389] using Codex for Minecraft.\n3.10\nSocial Sciences & Psychology\nThe rapid advancements of LLMs have fostered the\nuse of such models across research in the psycho-\nlogical and behavioral sciences. Reviewing the ex-\nisting literature, we have identified three main areas\nand tasks in which LLMs have been used in the con-\nUsing LLMs to model \nhuman behavior\nAnalyzing behavioral \ncharacteristics of LLMs\nSimulating social \nrelationships with LLMs\nLLMs in the Social Sciences &  Psychology\nMilgram Shock Experiment\nBig Five personality traits\nInteracting artificial agents\nIllusory Truth Effect\nGuilford's Alternative Uses\nLLMs to simulate societies\nFigure 15:\nUse cases of LLMs in the social sci-\nences and psychology can mainly be structured into\nthree categories: using LLMs to model human behav-\nior [e.g., 12, 211], analyzing behavioral characteristics\nof LLMs [e.g., 414], and using LLMs to simulate social\nrelationships [e.g., 408].\ntext of the psychological and behavioral sciences:\nusing LLMs to simulate human behavioral experi-\nments [e.g., 22, 176, 211, 614, 126], analyzing the\npersonality traits of LLMs [e.g., 367, 414, 470],\nand employing them as artificial agents to model\nsocial relationships [409]. See Fig. 15 for an illus-\ntration.\n3.10.1\nModeling Human Behavior\nIn the behavioral sciences, there is an increasing\ninterest in using LLMs as models for psychological\nexperiments. Being able to model human behavior\ncomputationally through language models would\nentail a variety of advantages over using human\nparticipants: experiments with LLMs are cheaper,\nfaster, can be scaled easier, and are potentially less\nsensitive to ethical considerations [176]. In light\nof this, various works have compared LLMs with\nhuman participants from a behavioral perspective.\nArgyle et al. [22] demonstrate how LLMs can\ngenerate responses corresponding to virtual partici-\npants in behavioral experiments. They do so by us-\ning LLMs to generate samples of responses to stud-\nies related to political opinions and voting behavior.\nIn particular, the authors investigate three studies:\nthe first asks participants to list words associated\nwith outgroup partisans, and the second and third\nfocus on vote prediction based on demographics.\nAcross scenarios, experimental results demonstrate\nthat GPT-3 provides answers that closely align with\nhuman responses.\nHorton [211] argue that LLMs can be used\nto computationally model human behavior and\ndemonstrate such an ability in economics by ex-\nploring their behavior in economic scenarios. They\nconducted four experiments focusing on economic\ndecision-making using GPT-3, showing that the\n46\nLLM can approximately replicate results obtained\nwith human individuals.\nGriffin et al. [176] investigate the suitability of\nLLMs to model psychological change. In their\nstudy, the authors assess LLM responses to two\nbehavioral tests, the illusory truth effect [ITE; 194]\nand an experiment measuring the influence of pop-\nulist news to change in political views [55]. The\nresults demonstrate that in both scenarios, human\njudgments tend to align with LLM-based judg-\nments, indicating that LLMs have the potential to\nmodel the effect of influence on human individuals.\nAher et al. [12] introduce the Turing Experiment\n(TE) to measure an LLM\u2019s suitability to model hu-\nman behavior. A TE consists of inputs to the LLM\nthat signal a certain demographic (e.g., names or\noccupations) as well as a set of experimental de-\ntails and corresponding outputs used to simulate\nhuman behavior. The authors apply their approach\nto four individual tests, namely an ultimatum game\nfrom behavioral economics [214, 279], garden-path\nsentences used in psycholinguistics [89, 411], the\nMilgram Shock Experiment from social psychol-\nogy [364], and the wisdom of crowds task used to\nmeasure collective social intelligence [375]. De-\nmographic details are simulated via gender titles\nand surnames. The results show that LLMs largely\nalign with human behavior across the tests. How-\never, the authors note that LLM size matters and\nthat larger models tend to provide results that are\nmore aligned with human responses.\nAher et al. [12] point out that the LLMs were\nmost likely exposed to the four behavioral exper-\niments during their pre-training. To account for\nthat, the authors create artificial variations of the\nexperiments with conditions that differ from previ-\nous studies. Additionally, the authors note that a\npotential risk with using LLMs to simulate human\nresponses is the introduction of generations that\ncontain biases stemming from the models\u2019 training\ndata.\no Social Biases [12, 367]\nUnbalanced views and opinions in the train-\ning data skew the LLMs towards biased hu-\nman behaviors.\nPark et al. [409] replicate a set of 8 psycho-\nlogical studies from the Many Labs 2 project [270]\nusing GPT-3 to assess the LLM for its ability to sim-\nulate human behavioral data. Such studies include\ntests in which subjects are asked to choose between\na kiss from a favorite movie star and $50 [462]\nand where subjects had to decide between paying\na traffic violation fine and going to court [461].\nThese experiments show that GPT-3 replicates only\n37.5% of the effects obtained from human partic-\nipants. The authors argue that these results are\nattributed to humans and LLMs representing inher-\nently different cognitive systems.\nMaddela et al. [353] study identifying unhelpful\nthought patterns and possible reframings to facil-\nitate mental health. They release a dataset called\nPATTERNREFRAME and evaluate GPT-3.5 on it,\nshowing that it can perform very well without ad-\nditional training. They conclude that practitioners\nof cognitive behavioral therapy may benefit from\nusing LLMs to produce richer training material.\n3.10.2\nAnalyzing Behavioral Characteristics\nof LLMs\nIn addition to using LLMs as models for human\nbehavior, various existing works study LLMs by\nanalyzing their personality traits.\nJiang et al. [242] do so by introducing the Ma-\nchine Personality Inventory (MPI) dataset, a col-\nlection of items to assess personalities according\nto the Big Five personality factors: extraversion,\nagreeableness, openness, conscientiousness, and\nneuroticism [358].\nMiotto et al. [367] assess GPT-3\u2019s personalities\nusing the HEXACO [27] and Human Values [488]\nscales. Their experimental results reveal that GPT-\n3 obtains personality and value scores that align\nwith human participants. Miotto et al. [367] provide\nan extensive analysis of varying temperature values\nused to prompt the LLM, finding that an increased\ntemperature yields changes in the model\u2019s person-\nalities, e.g., GPT-3 shows a higher unwillingness to\nmanipulate as well as increased scores on anxiety.\nSimilar results were obtained concerning the Hu-\nman Values scale, where model responses varied\nsubstantially for different temperature values.\nIn line with this work, Pellert et al. [414] ar-\ngue that LLMs possess psychological traits as ob-\nserved in human individuals and can be assessed\nthrough psychometric tests. The authors conduct\nexperiments measuring, among others, the Big Five\npersonality traits in a zero-shot setup. In contrast,\nto Miotto et al. [367], Pellert et al. [414] investi-\ngate smaller models based on BERT and find that\ndifferent variants of BERT score across the five\npersonalities in a fairly homogeneous fashion, with\n47\ntraits that are high on agreeableness and extraver-\nsion, but low on neuroticism.\nIn a related fashion, Stevenson et al. [523] as-\nsess LLM performance (GPT-3) on the Guilford\u2019s\nAlternative Uses Test [AUT; 181], a test to assess\nhuman creativity. The test asks participants to sug-\ngest uses for physical objects (e.g., a book or a\nfork). Comparing the AUT test performance of\nGPT-3 to that of psychology students, the authors\nfound that human responses score higher on orig-\ninality and surprise, whereas GPT-3\u2019s responses\nwere more useful.\nKosinski [277] test Theory of Mind (ToM) in\nLLMs. ToM refers to the ability to track others\u2019\nunobservable mental states, such as intentions, be-\nliefs, or desires.\nThe authors find that among\nLLMs of the GPT family, recent models can in-\ncreasingly solve ToM tasks without having been\nexplicitly trained to do so. For instance, while GPT-\n2 shows virtually no capability of solving ToM\ntasks, GPT-3.5 (based on InstructGPT) and GPT-4\nperformed similarly to 6- and 7-year-old children,\nrespectively. Gandhi et al. [162] present a template-\nbased framework for generating synthetic samples\nto evaluate ToM in LLMs, which are then applied to\nfive recently developed LLMs (incl. GPT-3, GPT-\n4, LLaMA, and Claude). The authors show that\nmost models struggle with ToM in its basic forms.\nHowever, GPT-4 performs closest to the human\ncomparison of all tested models.\n3.10.3\nSimulating Social Relationships\nWhile most previous works measure LLMs as mod-\nels for human behavior through replicating human\nbehavioral studies, Park et al. [408] use the power\nof LLMs to model the interaction between artificial\nagents. The authors model a community of 25 ar-\ntificial agents interacting in a digital environment\nto achieve this. Each character has unique traits,\nand the characters interact with each other through\nnatural language. Simulating such societies, the\nauthors observe emergent social behaviors (e.g.,\nforming new relationships and attending events)\nbetween agents that are formed without any human\ninteraction.\n3.11\nSynthetic Data Generation\nThe ability of LLMs to perform in-context learning\nallows them to be prompted to generate synthetic\ndatasets for training much smaller domain-specific\nmodels.\nLLM\nModality-to-Text\nPrompt\nOutput\nLLM\nPrompt\n<style>\n.grid {\n  display: grid;\n\u2026\u2026.\nCode -> Modality\nCSS\nLatex - TikZ\nPython - Matplotlib\nPrompt\nModality-and-Text-\nto-X \nPost-processing\nPre-processing\nFigure 16: Modality Conversion. Illustration of us-\ning models with other input modalities as pre or post-\nprocessing steps in an LLM pipeline [148, 329, 338,\n225, 315]. For some use cases, this approach can be\nused as an alternative to training a multi-modal model\nor using a shared embedding space.\nWang et al. [583] propose using GPT-3 to label\ndatasets more cost-effectively than human labelers.\nThese labeled datasets can then be used to train\nmore compute-efficient smaller models. To evalu-\nate this approach, RoBERTa and PEGASUS mod-\nels are trained for 9 NLP tasks using human and\nGPT-3 generated labels. GPT-3 labels are shown\nto outperform human labels when labeling budgets\nare small, but higher-quality human labels tend to\nlead to better models at higher labeling budgets.\nSimilarly, Ding et al. [123] propose three prompt-\ning approaches for training data generation with\nGPT-3: unlabeled data annotation (generate labels\nfor known examples), training data generation (gen-\nerate examples and labels), and assisted training\ndata generation (with Wikidata provided as addi-\ntional context). Fine-tuning a smaller BERT model\nfor text classification and NER tasks using these\napproaches showed results similar to or worse than\nusing GPT-3 directly.\nGunasekar et al. [182] leverage synthetic data\ngeneration with GPT-3.5 to train a new code gen-\neration LLM (see Sec. 3.3.1). The generated data\nconsists of synthetic Python textbooks focusing on\nreasoning, basic algorithmic skills, and synthetic\nPython exercises. One important finding of this\n48\nwork is that introducing randomness into data gen-\neration is crucial, all while ensuring the examples\nmaintain their quality and coherence.\nYoo et al. [648] propose GPT3Mix to generate\nadditional synthetic data from an existing dataset\nfor classification tasks.\nGPT3Mix uses GPT-3\nwith a prompt containing real examples from the\ndataset and a task specification to create synthetic\nexamples and pseudo-labels jointly. This new aug-\nmented dataset is then used to fine-tune BERT and\nDistilBERT models. This method combines data\naugmentation approaches with knowledge distilla-\ntion by training smaller classification models using\nsoft labels.\nBonifacio et al. [51] propose InPars, a method\nfor using LLMs to generate synthetic retrieval ex-\namples for fine-tuning on information retrieval\ntasks. GPT-3 is few-shot prompted to generate a rel-\nevant question for a randomly sampled document\nalong with the question\u2019s associated probability. A\nsmaller monoT5 model is then fine-tuned using\nthis dataset to rank relevant documents for a given\nquestion. The fine-tuned model outperforms only\npre-trained models but performs worse than models\nfine-tuned using the existing MS MARCO training\ndataset [32].\nDai et al. [104] introduce AugGPT, which uses\nChatGPT (GPT-3.5) to augment each example in\na small base dataset with six additional rephrased\nsynthetic examples. This new augmented dataset is\nthen used to fine-tune a specialized BERT model.\nThis approach outperforms existing augmentation\napproaches, such as word and character substitu-\ntion.\nFinally, instead of generating synthetic data to\nachieve a specialized task, Shridhar et al. [503] pro-\npose Decompositional Distillation, which aims to\nuse synthetic data to replicate in smaller models the\nmulti-step reasoning capabilities, such as CoT, that\nemerge in larger LLMs. First, GPT-3 is used with a\nmanually designed few-shot prompt to decompose\na problem into (sub-question, sub-solution) pairs.\nThis synthetic sub-question dataset is then used\nto fine-tune a T5 problem decomposer to generate\nsub-questions. Finally, a GPT-2 problem solver\nis fine-tuned to provide the sub-solutions to the\nteacher-generated sub-questions.\nOverall, while LLM-generated synthetic data\ncan potentially bring significant cost benefits, the\ngreater its role, the higher the potential for it to fail\nto capture the true distribution and potentially lead\nto model collapse [506].\no Hallucinated Distributions [506]\nUsing LLMs for fully synthetic data genera-\ntion is currently constrained by our inability\nto verify whether the synthetic data gener-\nated is representative of the true distribution\nin the corresponding real-world data.\nIn cases where the LLM is only used to label\nexisting data [583, 123] this will likely reduce\nthe risk of generating an unrepresentative training\ndistribution (although hallucinated labels remain\nan issue). Where the LLM is used to generate\n(or partially generate) both the input and the tar-\nget [123, 104, 182, 51, 503] the issue of halluci-\nnated distributions becomes potentially significant.\n4\nRelated Work\nClosest to ours is the concurrent work by Zhao\net al. [673], who provide an extensive survey of\nlarge language models and associated topics. Mi-\nalon et al. [363] focus on surveying augmented\nlanguage models, i.e., \u201clanguage models with rea-\nsoning skills and the ability to use tools\u201d. Tornede\net al. [555] survey LLMs in the context of AutoML\nmethods, highlighting existing methods and chal-\nlenges in leveraging these for improving LLMs.\nTang et al. [539] survey LLM-generated text de-\ntection techniques. Chang et al. [72] concurrently\nsurvey evaluation tasks of LLMs.\nThe literature also contains several previous sur-\nveys and evaluations specific to individual applica-\ntion domains that reference LLMs, including: chat-\nbots [345], computational biology [558, 217], com-\nputer programming [499], medicine [381, 610, 590,\n381], law [101, 531], knowledge work [140, 621],\nand reasoning [223].\n5\nConclusion\nIn this work, we identify several unsolved chal-\nlenges of large language models, provide an\noverview of their current applications, and discuss\nhow the former constrain the latter. By highlighting\nthe limitations of existing methods, we hope to fos-\nter future research addressing these. We also hope\nthat by providing an overview of the approaches\nused in different applied areas, we can facilitate\nthe transfer of ideas between domains and target\nfurther research.\n49\nAcknowledgements\nWe thank Abhishek Kumar and Stella Rose Bider-\nman for fruitful discussions and feedback on the\ndraft.\nReferences\n[1]\nA blog post detailed a Sam Altman freakout about a\nhuge chips shortage threatening OpenAI. Then it was taken\ndown.\n[2] Open LLM Leaderboard - a Hugging Face Space by Hug-\ngingFaceH4.\n[3] Reproducibility \u2014 PyTorch 2.0 documentation.\n[4] 2023.\nNegative prompts for text generation.\nSection:\nPrompting.\n[5] 2023. Reproducibility. Page Version ID: 1163331755.\n[6] A. Abbas, K. Tirumala, D. Simig, S. Ganguli and A. S.\nMorcos. 2023.\nSemdedup: Data-efficient learning at\nweb-scale through semantic deduplication. arXiv preprint\narXiv:2303.09540.\n[7] J. D. Abernethy, A. Agarwal, T. V. Marinov and M. K. War-\nmuth. 2023. A mechanism for sample-efficient in-context\nlearning for sparse retrieval tasks. ArXiv, abs/2305.17040.\n[8] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel,\nR. Thoppilan, Z. Yang, A. Kulshreshtha et al. 2020. To-\nwards a human-like open-domain chatbot. arXiv preprint\narXiv:2001.09977.\n[9] R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville\nand M. Bellemare. 2021. Deep Reinforcement Learning\nat the Edge of the Statistical Precipice. In Advances in\nNeural Information Processing Systems, volume 34, pages\n29304\u201329320. Curran Associates, Inc.\n[10] M. Agrawal, S. Hegselmann, H. Lang, Y. Kim and D. Son-\ntag. 2022. Large language models are zero-shot clinical\ninformation extractors. arXiv preprint arXiv:2205.12689.\n[11] P. Agrawal, C. Alberti, F. Huot, J. Maynez, J. Ma,\nS. Ruder, K. Ganchev, D. Das et al. 2022. Qameleon:\nMultilingual qa with only 5 examples.\narXiv preprint\narXiv:2211.08264.\n[12] G. Aher, R. I. Arriaga and A. T. Kalai. 2022. Using\nlarge language models to simulate multiple humans. arXiv\npreprint arXiv:2208.10264.\n[13] O. Ahia, S. Kumar, H. Gonen, J. Kasai, D. R. Mortensen,\nN. A. Smith and Y. Tsvetkov. 2023. Do all languages cost\nthe same? tokenization in the era of commercial language\nmodels. arXiv preprint arXiv:2305.13707.\n[14] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes,\nB. David, C. Finn, K. Gopalakrishnan et al. 2022. Do as\ni can, not as i say: Grounding language in robotic affor-\ndances. arXiv preprint arXiv:2204.01691.\n[15] J. Ainslie, T. Lei, M. de Jong, S. Onta\u00f1\u00f3n, S. Brahma,\nY. Zemlyanskiy, D. Uthus, M. Guo et al. 2023. Colt5:\nFaster long-range transformers with conditional computa-\ntion. arXiv preprint arXiv:2303.09752.\n[16] E. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma and\nD. Zhou. 2023. What learning algorithm is in-context learn-\ning? investigations with linear models. In The Eleventh\nInternational Conference on Learning Representations.\n[17] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M.\nFerrandis, N. Muennighoff, M. Mishra et al. 2023. Santa-\ncoder: don\u2019t reach for the stars!\n[18] J. Andreas. 2022. Language models as agent models.\n[19] C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra,\nV. Ramasesh, A. Slone, G. Gur-Ari et al. 2022. Explor-\ning Length Generalization in Large Language Models.\nArXiv:2207.04901 [cs].\n[20] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin,\nA. Passos, S. Shakeri, E. Taropa et al. 2023. Palm 2 techni-\ncal report. arXiv preprint arXiv:2305.10403.\n[21] F. Antaki, S. Touma, D. Milad, J. El-Khoury and R. Duval.\n2023. Evaluating the performance of chatgpt in ophthal-\nmology: An analysis of its successes and shortcomings.\nmedRxiv.\n[22] L. P. Argyle, E. C. Busby, N. Fulda, J. Gubler, C. Rytting\nand D. Wingate. 2022.\nOut of one, many: Using lan-\nguage models to simulate human samples. arXiv preprint\narXiv:2209.06899.\n[23] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng,\nS. V. Mehta, H. Zhuang, V. Q. Tran et al. 2022. Ext5:\nTowards extreme multi-task scaling for transfer learning.\nIn International Conference on Learning Representations.\n[24] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha,\nK. Bhatia, I. Chami, F. Sala et al. 2022. Ask me anything:\nA simple strategy for prompting language models.\n[25] A. Asai, T. Schick, P. Lewis, X. Chen, G. Izacard,\nS. Riedel, H. Hajishirzi and W.-t. Yih. 2022. Task-aware\nretrieval with instructions.\n[26] N. Asher, S. Bhar, A. Chaturvedi, J. Hunter and S. Paul.\n2023.\nLimits for Learning with Language Models.\nArXiv:2306.12213 [cs].\n[27] M. C. Ashton and K. Lee. 2009. The hexaco\u201360: A short\nmeasure of the major dimensions of personality. Journal\nof personality assessment, 91(4):340\u2013345.\n[28] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski,\nD. Dohan, E. Jiang, C. Cai et al. 2021.\nProgram\nsynthesis with large language models.\narXiv preprint\narXiv:2108.07732.\n[29] AUTOMATIC1111. 2023.\nStable Diffusion web UI.\nOriginal-date: 2022-08-22T14:05:26Z.\n[30] J. W. Ayers, A. Poliak, M. Dredze, E. C. Leas, Z. Zhu, J. B.\nKelley, D. J. Faix, A. M. Goodman et al. 2023. Comparing\nphysician and artificial intelligence chatbot responses to\npatient questions posted to a public social media forum.\nJAMA internal medicine.\n[31] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion,\nA. Jones, A. Chen, A. Goldie et al. 2022.\nConstitu-\ntional ai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\n[32] P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu,\nR. Majumder, A. McNamara et al. 2018. Ms marco: A\nhuman generated machine reading comprehension dataset.\n50\n[33] P. Bajaj, C. Xiong, G. Ke, X. Liu, D. He, S. Tiwary, T.-Y.\nLiu, P. Bennett et al. 2022. Metro: Efficient denoising pre-\ntraining of large scale autoencoding language models with\nmodel generated signals. arXiv preprint arXiv:2204.06644.\n[34] A. Bakhtin, S. Gross, M. Ott, Y. Deng, M. Ranzato and\nA. Szlam. 2019. Real or Fake? Learning to Discriminate\nMachine from Human Generated Text. ArXiv:1906.03351\n[cs, stat].\n[35] R. Balestriero, J. Pesenti and Y. LeCun. 2021. Learning\nin high dimension always amounts to extrapolation. arXiv\npreprint arXiv:2110.09485.\n[36] J. Bandy and N. Vincent. 2021. Addressing \"documenta-\ntion debt\" in machine learning research: A retrospective\ndatasheet for bookcorpus.\n[37] P. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\nS. Hand, D. Hurt, M. Isard, H. Lim et al. 2022. Pathways:\nAsynchronous distributed dataflow for ml. Proceedings of\nMachine Learning and Systems, 4:430\u2013449.\n[38] M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey,\nJ. Tworek and M. Chen. 2022.\nEfficient training of\nlanguage models to fill in the middle.\narXiv preprint\narXiv:2207.14255.\n[39] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky,\nL. McKinney, S. Biderman and J. Steinhardt. 2023. Elic-\niting latent predictions from transformers with the tuned\nlens.\n[40] E. Ben Zaken, Y. Goldberg and S. Ravfogel. 2022. Bit-\nFit: Simple parameter-efficient fine-tuning for transformer-\nbased masked language-models. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 1\u20139, Dublin,\nIreland. Association for Computational Linguistics.\n[41] S. Biderman, K. Bicheno and L. Gao. 2022. Datasheet for\nthe pile. arXiv preprint arXiv:2201.07311.\n[42] S. Biderman, U. S. Prashanth, L. Sutawika, H. Schoelkopf,\nQ. Anthony, S. Purohit and E. Raff. 2023.\nEmergent\nand Predictable Memorization in Large Language Mod-\nels. ArXiv:2304.11158 [cs].\n[43] S. Biderman and W. J. Scheirer. 2021. Pitfalls in machine\nlearning research: Reexamining the development cycle.\n[44] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley,\nK. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit et al.\n2023. Pythia: A suite for analyzing large language models\nacross training and scaling. In Proceedings of the 40th\nInternational Conference on Machine Learning, volume\n202 of Proceedings of Machine Learning Research, pages\n2397\u20132430. PMLR.\n[45] S. R. Biderman. 2023.\n[...] we aren\u2019t running out\nof text data any time soon. ml researchers mas-\nsively underestimate how much text is out there.\nhttps://twitter.com/BlancheMinerva/\nstatus/1644154144431677442?s=20. Accessed:\n2023-05-28.\n[46] A. Birhane, V. U. Prabhu and E. Kahembwe. 2021. Mul-\ntimodal datasets: misogyny, pornography, and malignant\nstereotypes. arXiv preprint arXiv:2110.01963.\n[47] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao,\nL. Golding, H. He, C. Leahy et al. 2022. Gpt-neox-20b:\nAn open-source autoregressive language model.\n[48] A. Blair-Stanek, N. Holzenberger and B. Van Durme.\n2023. Can gpt-3 perform statutory reasoning?\narXiv\npreprint arXiv:2302.06100.\n[49] J. Bommarito, M. Bommarito, D. M. Katz and J. Katz.\n2023. Gpt as knowledge worker: A zero-shot evaluation of\n(ai) cpa capabilities. arXiv preprint arXiv:2301.04408.\n[50] M. Bommarito II and D. M. Katz. 2022. Gpt takes the bar\nexam. arXiv preprint arXiv:2212.14402.\n[51] L. Bonifacio, H. Abonizio, M. Fadaee and R. Nogueira.\n2022. Inpars: Unsupervised dataset generation for infor-\nmation retrieval. In Proceedings of the 45th International\nACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR \u201922, page 2387\u20132392, New\nYork, NY, USA. Association for Computing Machinery.\n[52] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Ruther-\nford, K. Millican, G. v. d. Driessche, J.-B. Lespiau et al.\n2021. Improving language models by retrieving from tril-\nlions of tokens. arXiv preprint arXiv:2112.04426.\n[53] A. Borji. 2023. A Categorical Archive of ChatGPT Fail-\nures. ArXiv:2302.03494 [cs].\n[54] A. Borzunov, D. Baranchuk, T. Dettmers, M. Ryabinin,\nY. Belkada, A. Chumachenko, P. Samygin and C. Raffel.\n2022. Petals: Collaborative inference and fine-tuning of\nlarge models. arXiv preprint arXiv:2209.01188.\n[55] L. Bos, C. Schemer, N. Corbu, M. Hameleers, I. An-\ndreadis, A. Schulz, D. Schmuck, C. Reinemann et al. 2020.\nThe effects of populism as a social identity frame on persua-\nsion and mobilisation: Evidence from a 15-country experi-\nment. European Journal of Political Research, 59(1):3\u201324.\n[56] D. Britz, M. Y. Guan and M.-T. Luong. 2017. Efficient\nattention using a fixed-size memory representation. arXiv\npreprint arXiv:1707.00110.\n[57] A. Z. Broder, M. Charikar, A. M. Frieze and M. Mitzen-\nmacher. 1998. Min-wise independent permutations. In\nProceedings of the thirtieth annual ACM symposium on\nTheory of computing, pages 327\u2013336.\n[58] G. Brown, M. Bun, V. Feldman, A. Smith and K. Talwar.\n2021. When is memorization of irrelevant training data\nnecessary for high-accuracy learning? In Proceedings of\nthe 53rd annual ACM SIGACT symposium on theory of\ncomputing, pages 123\u2013132.\n[59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam et al. 2020.\nLanguage models are few-shot learners. In Advances in\nNeural Information Processing Systems, volume 33, pages\n1877\u20131901. Curran Associates, Inc.\n[60] M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley,\nB. Garfinkel, A. Dafoe, P. Scharre et al. 2018. The mali-\ncious use of artificial intelligence: Forecasting, prevention,\nand mitigation. arXiv preprint arXiv:1802.07228.\n[61] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke,\nE. Horvitz, E. Kamar, P. Lee, Y. T. Lee et al. 2023. Sparks\nof artificial general intelligence: Early experiments with\ngpt-4.\n51\n[62] C. Burns, H. Ye, D. Klein and J. Steinhardt. 2022. Dis-\ncovering latent knowledge in language models without\nsupervision.\n[63] A. Calderwood, N. Wardrip-Fruin and M. Mateas. 2022.\nSpinning coherent interactive fiction through foundation\nmodel prompts. International Conference of Computation\nand Creativity.\n[64] N. Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka,\nW. Pearce, H. Anderson, A. Terzis, K. Thomas et al.\n2023. Poisoning Web-Scale Training Datasets is Practical.\nArXiv:2302.10149 [cs].\n[65] N. Carlini, C. Liu, \u00da. Erlingsson, J. Kos and D. Song.\n2019. The secret sharer: Evaluating and testing unintended\nmemorization in neural networks. In USENIX Security\nSymposium, volume 267.\n[66] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski,\nI. Gao, A. Awadalla, P. W. Koh, D. Ippolito et al. 2023.\nAre aligned neural networks adversarially aligned?\n[67] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-\nVoss, K. Lee, A. Roberts, T. Brown et al. 2020. Extracting\ntraining data from large language models.\n[68] S. Casper, J. Lin, J. Kwon, G. Culp and D. Hadfield-\nMenell. 2023.\nExplore, establish, exploit: Red team-\ning language models from scratch.\narXiv preprint\narXiv:2306.09442.\n[69] T. Chakrabarty, V. Padmakumar and H. He. 2022. Help\nme write a poem: Instruction tuning as a vehicle for collab-\norative poetry writing. arXiv preprint arXiv:2210.13669.\n[70] I. Chalkidis, I. Androutsopoulos and N. Aletras. 2019.\nNeural legal judgment prediction in english. arXiv preprint\narXiv:1906.02059.\n[71] I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Ale-\ntras and I. Androutsopoulos. 2020.\nLegal-bert: The\nmuppets straight out of law school.\narXiv preprint\narXiv:2010.02559.\n[72] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen,\nL. Yang, X. Yi et al. 2023. A Survey on Evaluation of\nLarge Language Models. ArXiv:2307.03109 [cs].\n[73] B. Chen, X. Cheng, L. ao Gengyang, S. Li, X. Zeng,\nB. Wang, G. Jing, C. Liu et al. 2023. xtrimopglm: Uni-\nfied 100b-scale pre-trained transformer for deciphering the\nlanguage of protein. bioRxiv.\n[74] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre\nand J. Jumper. 2023. Accelerating large language model\ndecoding with speculative sampling.\narXiv preprint\narXiv:2302.01318.\n[75] L. Chen, M. Zaharia and J. Zou. 2023. FrugalGPT: How\nto Use Large Language Models While Reducing Cost and\nImproving Performance. ArXiv:2305.05176 [cs].\n[76] L. Chen, M. Zaharia and J. Zou. 2023. How is ChatGPT\u2019s\nbehavior changing over time? ArXiv:2307.09009 [cs].\n[77] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto,\nJ. Kaplan, H. Edwards, Y. Burda et al. 2021. Evaluating\nlarge language models trained on code.\n[78] M. Chen, A. Papangelis, C. Tao, S. Kim, A. Rosenbaum,\nY. Liu, Z. Yu and D. Hakkani-Tur. 2023. Places: Prompting\nlanguage models for social conversation synthesis. arXiv\npreprint arXiv:2302.03269.\n[79] S. Chen, S. Wong, L. Chen and Y. Tian. 2023. Extending\ncontext window of large language models via positional\ninterpolation.\n[80] T. Chen, Z. Zhang, A. Jaiswal, S. Liu and Z. Wang. 2023.\nSparse moe as the new dropout: Scaling dense and self-\nslimmable transformers.\n[81] X. Chen, M. Lin, N. Sch\u00e4rli and D. Zhou. 2023. Teach-\ning large language models to self-debug. arXiv preprint\narXiv:2304.05128.\n[82] L. Cheng, X. Li and L. Bing. 2023. Is gpt-4 a good data\nanalyst?\n[83] D. Choe, R. Al-Rfou, M. Guo, H. Lee and N. Constant.\n2019. Bridging the Gap for Tokenizer-Free Language Mod-\nels. ArXiv:1908.10322 [cs].\n[84] J. H. Choi, K. E. Hickman, A. Monahan and D. Schwarcz.\n2023. Chatgpt goes to law school. Available at SSRN.\n[85] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song,\nA. Gane, T. Sarlos, P. Hawkins, J. Davis et al. 2020.\nRethinking attention with performers.\narXiv preprint\narXiv:2009.14794.\n[86] A. Chowdhery, S. Narang, J. Devlin, M. Bosma,\nG. Mishra, A. Roberts, P. Barham, H. W. Chung et al.\n2022. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311.\n[87] M. Christ, S. Gunn and O. Zamir. 2023. Undetectable\nWatermarks for Language Models.\n[88] P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg\nand D. Amodei. 2017. Deep reinforcement learning from\nhuman preferences.\n[89] K. Christianson, A. Hollingworth, J. F. Halliwell and\nF. Ferreira. 2001. Thematic roles assigned along the garden\npath linger. Cognitive psychology, 42(4):368\u2013407.\n[90] H. W. Chung. 2023. Missing model details (tweet).\n[91] H. W. Chung, X. Garcia, A. Roberts, Y. Tay, O. Firat,\nS. Narang and N. Constant. 2023. Unimax: Fairer and\nmore effective language sampling for large-scale multilin-\ngual pretraining. In The Eleventh International Conference\non Learning Representations.\n[92] H. W. Chung, D. Garrette, K. C. Tan and J. Riesa. 2020.\nImproving multilingual models with language-clustered vo-\ncabularies. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\npages 4536\u20134546, Online. Association for Computational\nLinguistics.\n[93] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay,\nW. Fedus, Y. Li, X. Wang et al. 2022. Scaling instruction-\nfinetuned language models.\n[94] J. H. Clark, D. Garrette, I. Turc and J. Wieting. 2022. Ca-\nnine: Pre-training an efficient tokenization-free encoder for\nlanguage representation. Transactions of the Association\nfor Computational Linguistics, 10:73\u201391.\n52\n[95] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,\nL. Kaiser, M. Plappert, J. Tworek et al. 2021. Training\nverifiers to solve math word problems.\n[96] D. Cohen, M. Ryu, Y. Chow, O. Keller, I. Greenberg,\nA. Hassidim, M. Fink, Y. Matias et al. 2022. Dynamic plan-\nning in open-ended dialogue using reinforcement learning.\narXiv preprint arXiv:2208.02294.\n[97] R. Cohen, M. Hamri, M. Geva and A. Globerson. 2023.\nLM vs LM: Detecting Factual Errors via Cross Examina-\ntion. ArXiv:2305.13281 [cs].\n[98] T. Computer. 2023. Redpajama: An open source recipe\nto reproduce llama training dataset.\n[99] A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimer-\nsheim and A. Garriga-Alonso. 2023. Towards automated\ncircuit discovery for mechanistic interpretability. arXiv\npreprint arXiv:2304.14997.\n[100] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary,\nG. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott et al. 2020. Unsu-\npervised cross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8440\u20138451, Online.\nAssociation for Computational Linguistics.\n[101] A. B. Cyphert. 2021. A human being wrote this law\nreview article: Gpt-3 and the practice of law. UC Davis L.\nRev., 55:401.\n[102] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang and F. Wei.\n2022. Knowledge neurons in pretrained transformers. In\nProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 8493\u20138502, Dublin, Ireland. Association for Com-\nputational Linguistics.\n[103] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui and F. Wei.\n2022. Why can gpt learn in-context? language models se-\ncretly perform gradient descent as meta optimizers. arXiv\npreprint arXiv:2212.10559.\n[104] H. Dai, Z. Liu, W. Liao, X. Huang, Z. Wu, L. Zhao,\nW. Liu, N. Liu et al. 2023. Chataug: Leveraging chatgpt\nfor text data augmentation.\n[105] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le and\nR. Salakhutdinov. 2019. Transformer-XL: Attentive lan-\nguage models beyond a fixed-length context. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 2978\u20132988, Florence,\nItaly. Association for Computational Linguistics.\n[106] H. Dalla-Torre, L. Gonzalez, J. Mendoza Revilla,\nN. Lopez Carranza, A. Henryk Grywaczewski, F. Oteri,\nC. Dallago, E. Trop et al. 2023. The nucleotide trans-\nformer: Building and evaluating robust foundation models\nfor human genomics. bioRxiv, pages 2023\u201301.\n[107] T. Dao, D. Y. Fu, S. Ermon, A. Rudra and C. R\u00e9. 2022.\nFlashattention: Fast and memory-efficient exact attention\nwith io-awareness. arXiv preprint arXiv:2205.14135.\n[108] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas,\nA. Rudra and C. R\u00e9. 2023.\nHungry Hungry Hippos:\nTowards Language Modeling with State Space Models.\nArXiv:2212.14052 [cs].\n[109] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank,\nP. Molino, J. Yosinski and R. Liu. 2020. Plug and play\nlanguage models: A simple approach to controlled text\ngeneration.\n[110] J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J.\nRagotte, L. F. Milles, B. I. M. Wicky, A. Courbet et al. 2022.\nRobust deep learning&#x2013;based protein sequence de-\nsign using proteinmpnn. Science, 378(6615):49\u201356.\n[111] N. De Cao, W. Aziz and I. Titov. 2021. Editing fac-\ntual knowledge in language models. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 6491\u20136506, Online and Punta\nCana, Dominican Republic. Association for Computational\nLinguistics.\n[112] M. Dehghani, A. Arnab, L. Beyer, A. Vaswani and Y. Tay.\n2022. The Efficiency Misnomer. ArXiv:2110.12894 [cs,\nstat].\n[113] M. Dehghani, Y. Tay, A. A. Gritsenko, Z. Zhao,\nN. Houlsby, F. Diaz, D. Metzler and O. Vinyals. 2021.\nThe benchmark lottery. arXiv preprint arXiv:2107.07002.\n[114] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu,\nA. Awadallah and S. Mukherjee. 2023. SkipDecode: Au-\ntoregressive Skip Decoding with Batching and Caching for\nEfficient LLM Inference. ArXiv:2307.02628 [cs].\n[115] A. Deroy, K. Ghosh and S. Ghosh. 2023. How ready\nare pre-trained abstractive models and llms for legal case\njudgement summarization?\n[116] A. Deshpande, V. Murahari, T. Rajpurohit, A. Kalyan\nand K. Narasimhan. 2023. Toxicity in chatgpt: Analyz-\ning persona-assigned language models. arXiv preprint\narXiv:2304.05335.\n[117] T. Dettmers, M. Lewis, Y. Belkada and L. Zettlemoyer.\n2022. Llm.int8(): 8-bit matrix multiplication for transform-\ners at scale.\n[118] T. Dettmers, A. Pagnoni, A. Holtzman and L. Zettle-\nmoyer. 2023. QLoRA: Efficient Finetuning of Quantized\nLLMs. ArXiv:2305.14314 [cs].\n[119] T.\nDettmers,\nR.\nSvirschevski,\nV.\nEgiazarian,\nD. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov,\nT. Hoefler et al. 2023. Spqr: A sparse-quantized represen-\ntation for near-lossless llm weight compression. arXiv\npreprint arXiv:2306.03078.\n[120] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages 4171\u2013\n4186, Minneapolis, Minnesota. Association for Computa-\ntional Linguistics.\n[121] N. Dey, G. Gosal, Zhiming, Chen, H. Khachane, W. Mar-\nshall, R. Pathria, M. Tom et al. 2023. Cerebras-gpt: Open\ncompute-optimal language models trained on the cerebras\nwafer-scale cluster.\n[122] S. Diao, X. Li, Y. Lin, Z. Huang and T. Zhang. 2022.\nBlack-box prompt learning for pre-trained language mod-\nels. arXiv preprint arXiv:2201.08531.\n53\n[123] B. Ding, C. Qin, L. Liu, L. Bing, S. Joty and B. Li.\n2022. Is gpt-3 a good data annotator?\narXiv preprint\narXiv:2212.10450.\n[124] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang\nand F. Wei. 2023.\nLongnet: Scaling transformers to\n1,000,000,000 tokens.\n[125] J. Dodge, M. Sap, A. Marasovi\u00b4c, W. Agnew, G. Il-\nharco, D. Groeneveld, M. Mitchell and M. Gardner.\n2021. Documenting large webtext corpora: A case study\non the colossal clean crawled corpus.\narXiv preprint\narXiv:2104.08758.\n[126] R. Dominguez-Olmedo, M. Hardt and C. Mendler-\nD\u00fcnner. 2023. Questioning the survey responses of large\nlanguage models. arXiv preprint arXiv:2306.07951.\n[127] Q. Dong, D. Dai, Y. Song, J. Xu, Z. Sui and L. Li. 2022.\nCalibrating factual knowledge in pretrained language mod-\nels.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 5937\u20135947, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Lin-\nguistics.\n[128] D. R. Dowty, R. Wall and S. Peters. 2012. Introduction\nto Montague semantics, volume 11. Springer Science &\nBusiness Media.\n[129] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery,\nB. Ichter, A. Wahid, J. Tompson et al. 2023. Palm-e: An\nembodied multimodal language model. arXiv preprint\narXiv:2303.03378.\n[130] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu,\nM. Krikun, Y. Zhou et al. 2022. Glam: Efficient scaling\nof language models with mixture-of-experts. In Interna-\ntional Conference on Machine Learning, pages 5547\u20135569.\nPMLR.\n[131] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum and\nI. Mordatch. 2023.\nImproving Factuality and Reason-\ning in Language Models through Multiagent Debate.\nArXiv:2305.14325 [cs].\n[132] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang and\nJ. Tang. 2022. GLM: General language model pretrain-\ning with autoregressive blank infilling. In Proceedings\nof the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages\n320\u2013335, Dublin, Ireland. Association for Computational\nLinguistics.\n[133] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S.\nRosen, G. Ceder, K. Persson and A. Jain. 2022. Struc-\ntured information extraction from complex scientific text\nwith fine-tuned large language models. arXiv preprint\narXiv:2212.05238.\n[134] D. Duong and B. D. Solomon. 2023. Analysis of large-\nlanguage model versus human performance for genetics\nquestions. European Journal of Human Genetics, pages\n1\u20133.\n[135] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin,\nP. West, C. Bhagavatula et al. 2023. Faith and Fate: Limits\nof Transformers on Compositionality. ArXiv:2305.18654\n[cs].\n[136] N. Dziri, A. Madotto, O. Zaiane and A. J. Bose. 2021.\nNeural Path Hunter: Reducing Hallucination in Dialogue\nSystems via Path Grounding. ArXiv:2104.08455 [cs].\n[137] E.-M. El-Mhamdi, S. Farhadkhani, R. Guerraoui,\nN. Gupta, L.-N. Hoang, R. Pinot, S. Rouault and J. Stephan.\n2023.\nOn the Impossible Safety of Large AI Models.\nArXiv:2209.15259 [cs].\n[138] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph,\nB. Mann, A. Askell, Y. Bai et al. 2021. A mathematical\nframework for transformer circuits. Transformer Circuits\nThread.\n[139] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rihawi,\nY. Wang, L. Jones, T. Gibbs, T. Feher et al. 2020. Prottrans:\ntowards cracking the language of life\u2019s code through self-\nsupervised deep learning and high performance computing.\narXiv preprint arXiv:2007.06225.\n[140] T. Eloundou, S. Manning, P. Mishkin and D. Rock. 2023.\nGpts are gpts: An early look at the labor market impact\npotential of large language models.\n[141] F. Faal, K. Schmitt and J. Y. Yu. 2023. Reward model-\ning for mitigating toxicity in transformer-based language\nmodels. Applied Intelligence, 53(7):8421\u20138435.\n[142] A. Fan, C. Gardent, C. Braud and A. Bordes. 2021. Aug-\nmenting transformers with KNN-based composite memory\nfor dialog. Transactions of the Association for Computa-\ntional Linguistics, 9:82\u201399.\n[143] A. Fan, E. Grave and A. Joulin. 2020. Reducing trans-\nformer depth on demand with structured dropout. In Inter-\nnational Conference on Learning Representations.\n[144] M. Fathi, J. Pilault, P.-L. Bacon, C. Pal, O. Fi-\nrat and R. Goroshin. 2023.\nBlock-State Transformer.\nArXiv:2306.09539 [cs].\n[145] W. Fedus, B. Zoph and N. Shazeer. 2021. Switch trans-\nformers: Scaling to trillion parameter models with simple\nand efficient sparsity.\n[146] V. Feldman. 2020. Does learning require memorization?\na short tale about a long tail. In Proceedings of the 52nd\nAnnual ACM SIGACT Symposium on Theory of Computing,\npages 954\u2013959.\n[147] S. Feng, C. Y. Park, Y. Liu and Y. Tsvetkov. 2023.\nFrom Pretraining Data to Language Models to Downstream\nTasks: Tracking the Trails of Political Biases Leading to\nUnfair NLP Models. ArXiv:2305.08283 [cs].\n[148] W. Feng, W. Zhu, T.-j. Fu, V. Jampani, A. Akula, X. He,\nS. Basu, X. E. Wang et al. 2023. LayoutGPT: Compo-\nsitional Visual Planning and Generation with Large Lan-\nguage Models. ArXiv:2305.15393 [cs].\n[149] E. Ferrara. 2023. Should chatgpt be biased? challenges\nand risks of bias in large language models. arXiv preprint\narXiv:2304.03738.\n[150] A. Ficek, F. Liu and N. Collier. 2022. How to tackle\nan emerging topic? combining strong and weak labels\nfor covid news NER. In Proceedings of the 2nd Confer-\nence of the Asia-Pacific Chapter of the Association for\nComputational Linguistics and the 12th International Joint\nConference on Natural Language Processing (Volume 2:\nShort Papers), pages 488\u2013496, Online only. Association\nfor Computational Linguistics.\n54\n[151] C. Fourrier,\nN. Habib,\nJ. Launay and T. Wolf.\n2023.\nWhat\u2019s going on with the open llm leader-\nboard?\nAvailable from: https://huggingface.\nco/blog/evaluating-mmlu-leaderboard. Ac-\ncessed: 27/06/2023.\n[152] E. Frantar and D. Alistarh. 2023. Massive language mod-\nels can be accurately pruned in one-shot. arXiv preprint\narXiv:2301.00774.\n[153] E. Frantar, S. Ashkboos, T. Hoefler and D. Alis-\ntarh. 2022.\nGptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv preprint\narXiv:2210.17323.\n[154] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace,\nF. Shi, R. Zhong, W.-t. Yih et al. 2022. Incoder: A genera-\ntive model for code infilling and synthesis.\n[155] A. Fr\u00f6mmgen and L. Kharatyan. 2023.\nResolv-\ning code review comments with ml.\nAvailable from:\nhttps://ai.googleblog.com/2023/05/\nresolving-code-review-comments-with-ml.\nhtml. Accessed: 26/06/2023.\n[156] J. Fu, S.-K. Ng, Z. Jiang and P. Liu. 2023. Gptscore:\nEvaluate as you desire. arXiv preprint arXiv:2302.04166.\n[157] T. Fujii, K. Shibata, A. Yamaguchi, T. Morishita and\nY. Sogawa. 2023. How do different tokenizers perform on\ndownstream tasks in scriptio continua languages?: A case\nstudy in japanese. arXiv preprint arXiv:2306.09572.\n[158] I. Gabriel. 2020. Artificial intelligence, values, and align-\nment. Minds and machines, 30(3):411\u2013437.\n[159] S. Gadgil, A. R. Tadipatri, A. Agrawal, A. Narayanan\nand N. Goyal. 2022. Towards automating formalisation of\ntheorem statements using large language models. 36th\nConference on Neural Information Processing Systems\n(NeurIPS 2022) Workshop on MATH-AI.\n[160] T. Gale, D. Narayanan, C. Young and M. Zaharia. 2022.\nMegablocks: Efficient sparse training with mixture-of-\nexperts. arXiv preprint arXiv:2211.15841.\n[161] T. Gale,\nM. Zaharia,\nC. Young and E. Elsen.\n2020.\nSparse GPU Kernels for Deep Learning.\nArXiv:2006.10901 [cs, stat].\n[162] K. Gandhi, J.-P. Fr\u00e4nken, T. Gerstenbrg and N. D.\nGoodman. 2023. Understanding social reasoning in lan-\nguage models with language models.\narXiv preprint\narXiv:2306.15448.\n[163] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai,\nS. Kadavath, B. Mann, E. Perez et al. 2022.\nRed\nteaming language models to reduce harms: Methods,\nscaling behaviors, and lessons learned. arXiv preprint\narXiv:2209.07858.\n[164] J. Gao, X. Ding, B. Qin and T. Liu. 2023. Is chatgpt a\ngood causal reasoner? a comprehensive evaluation. arXiv\npreprint arXiv:2305.07375.\n[165] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe,\nC. Foster, J. Phang, H. He et al. 2020.\nThe pile: An\n800gb dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027.\n[166] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Fos-\nter, L. Golding, J. Hsu et al. 2021. A framework for few-\nshot language model evaluation.\n[167] S. Gehman, S. Gururangan, M. Sap, Y. Choi and N. A.\nSmith. 2020.\nRealtoxicityprompts: Evaluating neural\ntoxic degeneration in language models. arXiv preprint\narXiv:2009.11462.\n[168] S. Gehrmann, H. Strobelt and A. M. Rush. 2019. GLTR:\nStatistical Detection and Visualization of Generated Text.\nArXiv:1906.04043 [cs].\n[169] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel,\nW. Brendel, M. Bethge and F. A. Wichmann. 2020. Short-\ncut learning in deep neural networks. Nature Machine\nIntelligence, 2(11):665\u2013673.\n[170] A. Glaese, N. McAleese, M. Tr\u02dbebacz, J. Aslanides,\nV. Firoiu, T. Ewalds, M. Rauh, L. Weidinger et al. 2022.\nImproving alignment of dialogue agents via targeted human\njudgements.\n[171] D. Goldberg. 1991.\nWhat every computer scientist\nshould know about floating-point arithmetic. ACM Com-\nputing Surveys, 23(1):5\u201348.\n[172] A. N. Gomez, O. Key, K. Perlin, S. Gou, N. Frosst,\nJ. Dean and Y. Gal. 2022. Interlocking backpropagation:\nImproving depthwise model-parallelism. The Journal of\nMachine Learning Research, 23(1):7714\u20137741.\n[173] L. Gong, D. He, Z. Li, T. Qin, L. Wang and T. Liu.\n2019. Efficient training of BERT by progressively stacking.\nIn Proceedings of the 36th International Conference on\nMachine Learning, volume 97 of Proceedings of Machine\nLearning Research, pages 2337\u20132346. PMLR.\n[174] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan\nand W. Chen. 2023. Critic: Large language models can\nself-correct with tool-interactive critiquing. arXiv preprint\narXiv:2305.11738.\n[175] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz\nand M. Fritz. 2023.\nMore than you\u2019ve asked for: A\ncomprehensive analysis of novel prompt injection threats\nto application-integrated large language models. arXiv\npreprint arXiv:2302.12173.\n[176] L. D. Griffin, B. Kleinberg, M. Mozes, K. T. Mai, M. Vau,\nM. Caldwell and A. Marvor-Parker. 2023. Susceptibil-\nity to influence of large language models. arXiv preprint\narXiv:2303.06074.\n[177] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu,\nT. Naumann, J. Gao et al. 2021. Domain-specific language\nmodel pretraining for biomedical natural language pro-\ncessing. ACM Transactions on Computing for Healthcare\n(HEALTH), 3(1):1\u201323.\n[178] Y. Gu, S. Zhang, N. Usuyama, Y. Woldesenbet, C. Wong,\nP. Sanapathi, M. Wei, N. Valluri et al. 2023. Distilling large\nlanguage models for biomedical knowledge extraction: A\ncase study on adverse drug events.\n[179] Y. Gu, X. Han, Z. Liu and M. Huang. 2022.\nPPT:\nPre-trained prompt tuning for few-shot learning. In Pro-\nceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 8410\u20138423, Dublin, Ireland. Association for Com-\nputational Linguistics.\n[180] A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu,\nP. Abbeel, S. Levine and D. Song. 2023.\nThe false\npromise of imitating proprietary llms.\narXiv preprint\narXiv:2305.15717.\n55\n[181] J. P. Guilford. 1967. Creativity: Yesterday, today and\ntomorrow. The Journal of Creative Behavior, 1(1):3\u201314.\n[182] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D.\nGiorno, S. Gopi, M. Javaheripi, P. Kauffmann et al. 2023.\nTextbooks are all you need.\n[183] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H.\nSung and Y. Yang. 2022. LongT5: Efficient text-to-text\ntransformer for long sequences. In Findings of the Associ-\nation for Computational Linguistics: NAACL 2022, pages\n724\u2013736, Seattle, United States. Association for Computa-\ntional Linguistics.\n[184] A. Gupta. 2023. Probing Quantifier Comprehension in\nLarge Language Models. ArXiv:2306.07384 [cs].\n[185] T. Gupta and A. Kembhavi. 2022. Visual programming:\nCompositional visual reasoning without training.\n[186] K. Guu, K. Lee, Z. Tung, P. Pasupat and M. Chang.\n2020. Retrieval augmented language model pre-training.\nIn International Conference on Machine Learning, pages\n3929\u20133938. PMLR.\n[187] J. Haase and P. H. P. Hanel. 2023. Artificial muses: Gen-\nerative artificial intelligence chatbots have risen to human-\nlevel creativity.\n[188] M. Hahn and N. Goyal. 2023. A theory of emergent\nin-context learning as implicit structure induction. ArXiv,\nabs/2303.07971.\n[189] S. Hamilton. 2023.\nBlind judgement: Agent-based\nsupreme court modelling with gpt.\narXiv preprint\narXiv:2301.05327.\n[190] C. Han, Z. Wang, H. Zhao and H. Ji. 2023. In-context\nlearning of large language models explained as kernel re-\ngression. ArXiv, abs/2305.12766.\n[191] T. Hartvigsen, S. Sankaranarayanan, H. Palangi, Y. Kim\nand M. Ghassemi. 2022. Aging with grace: Lifelong model\nediting with discrete key-value adaptors. arXiv preprint\narXiv:2211.11031.\n[192] A. Haviv, O. Ram, O. Press, P. Izsak and O. Levy. 2022.\nTransformer language models without positional encodings\nstill learn positional information. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2022, pages\n1382\u20131390, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\n[193] J. Hazell. 2023. Large language models can be used to\neffectively scale spear phishing campaigns. arXiv preprint\narXiv:2305.06972.\n[194] E. L. Henderson, S. J. Westwood and D. J. Simons. 2022.\nA reproducible systematic map of research on the illusory\ntruth effect. Psychonomic Bulletin & Review, pages 1\u201324.\n[195] P. Henderson, M. S. Krass, L. Zheng, N. Guha, C. D.\nManning, D. Jurafsky and D. E. Ho. 2022. Pile of law:\nLearning responsible data filtering from the law and a\n256GB open-source legal dataset. In Thirty-sixth Confer-\nence on Neural Information Processing Systems Datasets\nand Benchmarks Track.\n[196] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li,\nD. Song and J. Steinhardt. 2020. Aligning ai with shared\nhuman values. arXiv preprint arXiv:2008.02275.\n[197] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,\nD. Song and J. Steinhardt. 2021. Measuring massive multi-\ntask language understanding.\n[198] D. Hendrycks, N. Carlini, J. Schulman and J. Steinhardt.\n2021. Unsolved problems in ml safety. arXiv preprint\narXiv:2109.13916.\n[199] D. Hendrycks and M. Mazeika. 2022. X-risk analysis\nfor ai research. arXiv preprint arXiv:2206.05862.\n[200] D. Hernandez, T. Brown, T. Conerly, N. DasSarma,\nD. Drain, S. El-Showk, N. Elhage, Z. Hatfield-Dodds et al.\n2022. Scaling laws and interpretability of learning from\nrepeated data. arXiv preprint arXiv:2205.10487.\n[201] J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun,\nH. Kianinejad, M. Patwary, M. Ali et al. 2017.\nDeep\nlearning scaling is predictable, empirically. arXiv preprint\narXiv:1712.00409.\n[202] B. L. Hie, V. R. Shanker, D. Xu, T. U. Bruun, P. A.\nWeidenbacher, S. Tang, W. Wu, J. E. Pak et al. 2023. Effi-\ncient evolution of human antibodies from general protein\nlanguage models. Nature Biotechnology.\n[203] P. Hingston and M. Preuss. 2011. Red teaming with\ncoevolution. In 2011 IEEE Congress of Evolutionary Com-\nputation (CEC), pages 1155\u20131163. IEEE.\n[204] J. Ho and T. Salimans. 2022. Classifier-free diffusion\nguidance.\n[205] J. Hoelscher-Obermaier, J. Persson, E. Kran, I. Kon-\nstas and F. Barez. 2023. Detecting Edit Failures In Large\nLanguage Models: An Improved Specificity Benchmark.\nArXiv:2305.17553 [cs].\n[206] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya,\nT. Cai, E. Rutherford, D. de las Casas, L. A. Hendricks\net al. 2022. An empirical analysis of compute-optimal\nlarge language model training. In Advances in Neural\nInformation Processing Systems.\n[207] A. Holtzman, J. Buys, L. Du, M. Forbes and Y. Choi.\n2020. The curious case of neural text degeneration. In\nInternational Conference on Learning Representations.\n[208] N. Holzenberger, A. Blair-Stanek and B. Van Durme.\n2020.\nA dataset for statutory reasoning in tax law\nentailment and question answering.\narXiv preprint\narXiv:2005.05257.\n[209] O. Honovich, T. Scialom, O. Levy and T. Schick. 2022.\nUnnatural instructions: Tuning language models with (al-\nmost) no human labor. arXiv preprint arXiv:2212.09689.\n[210] S. Hooker. 2021. The hardware lottery. Communications\nof the ACM, 64(12):58\u201365.\n[211] J. J. Horton. 2023. Large language models as simulated\neconomic agents: What can we learn from homo silicus?\narXiv preprint arXiv:2301.07543.\n[212] M. Horton, S. Mehta, A. Farhadi and M. Rastegari. 2023.\nBytes Are All You Need: Transformers Operating Directly\nOn File Bytes. ArXiv:2306.00238 [cs].\n[213] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone,\nQ. De Laroussilhe, A. Gesmundo, M. Attariyan and\nS. Gelly. 2019. Parameter-efficient transfer learning for\nnlp. In International Conference on Machine Learning,\npages 2790\u20132799. PMLR.\n56\n[214] D. Houser and K. McCabe. 2014. Experimental eco-\nnomics and experimental game theory. In Neuroeconomics,\npages 19\u201334. Elsevier.\n[215] J. Howard and S. Ruder. 2018.\nUniversal language\nmodel fine-tuning for text classification. In Proceedings\nof the 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages\n328\u2013339, Melbourne, Australia. Association for Computa-\ntional Linguistics.\n[216] S. Hsiao. 2023.\nWhat\u2019s ahead for bard:\nMore\nglobal,\nmore visual,\nmore integrated.\nAvailable\nfrom: https://blog.google/technology/ai/\ngoogle-bard-updates-io-2023/.\nAccessed:\n28/06/2023.\n[217] B. Hu, J. Xia, J. Zheng, C. Tan, Y. Huang, Y. Xu and S. Z.\nLi. 2022. Protein language models and structure prediction:\nConnection and progression.\n[218] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,\nL. Wang and W. Chen. 2021. Lora: Low-rank adaptation\nof large language models.\n[219] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W.\nLee, L. Bing and S. Poria. 2023. Llm-adapters: An adapter\nfamily for parameter-efficient fine-tuning of large language\nmodels. arXiv preprint arXiv:2304.01933.\n[220] W. Hua, Z. Dai, H. Liu and Q. Le. 2022. Transformer\nQuality in Linear Time. In Proceedings of the 39th Interna-\ntional Conference on Machine Learning, pages 9099\u20139117.\nPMLR. ISSN: 2640-3498.\n[221] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,\nC. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman et al.\n2019. Music transformer. In International Conference on\nLearning Representations.\n[222] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu and\nJ. Han. 2022. Large language models can self-improve.\n[223] J. Huang and K. C.-C. Chang. 2023. Towards Reasoning\nin Large Language Models: A Survey. ArXiv:2212.10403\n[cs].\n[224] W. Huang, P. Abbeel, D. Pathak and I. Mordatch. 2022.\nLanguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. In International Con-\nference on Machine Learning, pages 9118\u20139147. PMLR.\n[225] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence,\nA. Zeng, J. Tompson et al. 2022. Inner monologue: Em-\nbodied reasoning through planning with language models.\narXiv preprint arXiv:2207.05608.\n[226] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen,\nD. Chen, H. Lee, J. Ngiam et al. 2018. Gpipe: Efficient\ntraining of giant neural networks using pipeline parallelism.\n[227] Z. Huang, Y. Shen, X. Zhang, J. Zhou, W. Rong and\nZ. Xiong. 2023. Transformer-patcher: One mistake worth\none neuron. In The Eleventh International Conference on\nLearning Representations.\n[228] I. Hubara, B. Chmiel, M. Island, R. Banner, J. Naor\nand D. Soudry. 2021. Accelerated sparse neural training:\nA provable and efficient method to find n:m transposable\nmasks. In Advances in Neural Information Processing Sys-\ntems, volume 34, pages 21099\u201321111. Curran Associates,\nInc.\n[229] HuggingFace. 2023. Huggingchat v0.3.0. Available\nfrom: https://huggingface.co/chat. Accessed:\n28/06/2023.\n[230] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu,\nZ. Wang, R. Salas et al. 2022. Tutel: Adaptive mixture-of-\nexperts at scale. arXiv preprint arXiv:2206.03382.\n[231] J. P. A. Ioannidis. 2005. Why Most Published Research\nFindings Are False. PLoS Medicine, 2(8):e124.\n[232] D. Ippolito, A. Yuan, A. Coenen and S. Burnam. 2022.\nCreative writing with an ai-powered writing assistant:\nPerspectives from professional writers.\narXiv preprint\narXiv:2211.05030.\n[233] G. Irving, P. Christiano and D. Amodei. 2018. Ai safety\nvia debate. arXiv preprint arXiv:1805.00899.\n[234] K. Y. Iu and V. M.-Y. Wong. 2023. Chatgpt by openai:\nThe end of litigation lawyers? Available at SSRN.\n[235] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig,\nP. Yu, K. Shuster, T. Wang et al. 2022. Opt-iml: Scaling\nlanguage model instruction meta learning through the lens\nof generalization.\n[236] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni,\nT. Schick, J. Dwivedi-Yu, A. Joulin et al. 2022. Few-shot\nlearning with retrieval augmented language models. arXiv\npreprint arXiv:2208.03299.\n[237] A. Jacovi, A. Caciularu, O. Goldman and Y. Goldberg.\n2023. Stop Uploading Test Data in Plain Text: Practical\nStrategies for Mitigating Data Contamination by Evalua-\ntion Benchmarks. ArXiv:2305.10160 [cs].\n[238] N. Jain, K. Saifullah, Y. Wen, J. Kirchenbauer, M. Shu,\nA. Saha, M. Goldblum, J. Geiping et al. 2023. Bring your\nown data! self-supervised evaluation for large language\nmodels. arXiv preprint arXiv:23062.13651.\n[239] J. Jang, S. Kim, S. Ye, D. Kim, L. Logeswaran, M. Lee,\nK. Lee and M. Seo. 2023.\nExploring the Benefits of\nTraining Expert Language Models over Instruction Tuning.\nArXiv:2302.03202 [cs].\n[240] J. R. Jeliazkov, D. del Alamo and J. D. Karpiak.\n2023. Esmfold hallucinates native-like protein sequences.\nbioRxiv.\n[241] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii,\nY. J. Bang et al. 2023. Survey of Hallucination in Natural\nLanguage Generation. ACM Computing Surveys, 55(12):1\u2013\n38.\n[242] G. Jiang, M. Xu, S.-C. Zhu, W. Han, C. Zhang and\nY. Zhu. 2022.\nMpi: Evaluating and inducing person-\nality in pre-trained language models.\narXiv preprint\narXiv:2206.07550.\n[243] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li,\nF. Wang and Q. Liu. 2020. TinyBERT: Distilling BERT\nfor natural language understanding. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020,\npages 4163\u20134174, Online. Association for Computational\nLinguistics.\n[244] Z. Jin, J. Liu, Z. Lyu, S. Poff, M. Sachan, R. Mihalcea,\nM. Diab and B. Sch\u00f6lkopf. 2023. Can large language\nmodels infer causation from correlation?\n57\n[245] A. Jinich, S. Z. Nazia, A. V. Tellez, D. Rappoport,\nM. AlQuraishi and K. Rhee. 2022. Predicting enzyme\nsubstrate chemical structure with protein language models.\nbioRxiv, pages 2022\u201309.\n[246] Jonathan Frankle [@jefrankle]. 2022. Louder for the\npeople in the back: LARGE MODELS (GPT, DALLE)\n= DATABASES PROMPTS = QUERIES OUTPUTS =\nRESPONSES NNs find new relations w/in data. Anyone,\nno matter the resources, can study better querying langs\nand possibly beat a big model they could never afford to\ntrain.\n[247] D. Jones. 2022. Development and evaluation of speech\nrecognition for the Welsh language. In Proceedings of\nthe 4th Celtic Language Technology Workshop within\nLREC2022, pages 52\u201359, Marseille, France. European Lan-\nguage Resources Association.\n[248] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov,\nO. Ronneberger, K. Tunyasuvunakool, R. Bates et al. 2021.\nHighly accurate protein structure prediction with alphafold.\nNature, 596(7873):583\u2013589.\n[249] J. Kaddour. 2022. Stop wasting my time! saving days\nof imagenet and bert training with latest weight averaging.\narXiv preprint arXiv:2209.14981.\n[250] J. Kaddour. 2023. The MiniPile Challenge for Data-\nEfficient Language Models. ArXiv:2304.08442 [cs].\n[251] J. Kaddour, O. Key, P. Nawrot, P. Minervini and M. J.\nKusner. 2023. No Train No Gain: Revisiting Efficient\nTraining Algorithms For Transformer-based Language\nModels. ArXiv:2307.06440 [cs].\n[252] J. Kaddour, L. Liu, R. Silva and M. Kusner. 2022. When\ndo flat minima optimizers work? In Advances in Neural\nInformation Processing Systems.\n[253] J. Kaddour, A. Lynch, Q. Liu, M. J. Kusner and R. Silva.\n2022. Causal machine learning: A survey and open prob-\nlems. arXiv preprint arXiv:2206.15475.\n[254] J. Kaddour, Y. Zhu, Q. Liu, M. J. Kusner and R. Silva.\n2021. Causal Effect Inference for Structured Treatments.\nIn Advances in Neural Information Processing Systems,\nvolume 34, pages 24841\u201324854. Curran Associates, Inc.\n[255] M. Kale, A. Siddhant, R. Al-Rfou, L. Xue, N. Con-\nstant and M. Johnson. 2021. nmT5 - is parallel data still\nrelevant for pre-training massively multilingual language\nmodels? In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers), pages 683\u2013691, Online.\nAssociation for Computational Linguistics.\n[256] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,\nB. Chess, R. Child, S. Gray, A. Radford et al. 2020. Scal-\ning laws for neural language models.\narXiv preprint\narXiv:2001.08361.\n[257] A. Karpathy. 2023. Tokenization issues (tweet).\n[258] D. M. Katz, M. J. Bommarito, S. Gao and P. Arredondo.\n2023. Gpt-4 passes the bar exam. Available at SSRN\n4389233.\n[259] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das\nand S. Reddy. 2023. The impact of positional encoding\non length generalization in transformers. arXiv preprint\narXiv:2305.19466.\n[260] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Miku-\nlik and G. Irving. 2021. Alignment of language agents.\narXiv preprint arXiv:2103.14659.\n[261] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong\nand R. Socher. 2019.\nCtrl: A conditional transformer\nlanguage model for controllable generation. arXiv preprint\narXiv:1909.05858.\n[262] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang,\nC. Potts and M. Zaharia. 2023.\nDemonstrate-Search-\nPredict: Composing retrieval and language models for\nknowledge-intensive NLP. ArXiv:2212.14024 [cs].\n[263] D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger,\nZ. Wu, B. Vidgen, G. Prasad et al. 2021.\nDyn-\nabench: Rethinking benchmarking in nlp. arXiv preprint\narXiv:2104.14337.\n[264] J. Kim, M. Kim and B. Mozafari. 2022. Provable memo-\nrization capacity of transformers. In The Eleventh Interna-\ntional Conference on Learning Representations.\n[265] S. Kim, K. Mangalam, J. Malik, M. W. Mahoney,\nA. Gholami and K. Keutzer. 2023. Big little transformer\ndecoder. arXiv preprint arXiv:2302.07863.\n[266] T. Kim. 2022. Revisiting the practical effectiveness of\nconstituency parse extraction from pre-trained language\nmodels. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 5398\u20135408,\nGyeongju, Republic of Korea. International Committee on\nComputational Linguistics.\n[267] L. N. Kinch, R. D. Schaeffer, A. Kryshtafovych and\nN. V. Grishin. 2021. Target classification in the 14th round\nof the critical assessment of protein structure prediction\n(casp14). Proteins: Structure, Function, and Bioinformat-\nics, 89(12):1618\u20131632.\n[268] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers\nand T. Goldstein. 2023. A Watermark for Large Language\nModels. ArXiv:2301.10226 [cs].\n[269] J. Kirchenbauer, J. Geiping, Y. Wen, M. Shu, K. Sai-\nfullah, K. Kong, K. Fernando, A. Saha et al. 2023. On\nthe Reliability of Watermarks for Large Language Models.\nArXiv:2306.04634 [cs].\n[270] R. A. Klein, M. Vianello, F. Hasselman, B. G. Adams,\nR. B. Adams Jr, S. Alper, M. Aveyard, J. R. Axt et al. 2018.\nMany labs 2: Investigating variation in replicability across\nsamples and settings. Advances in Methods and Practices\nin Psychological Science, 1(4):443\u2013490.\n[271] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M.\nFerrandis, Y. Jernite, M. Mitchell et al. 2022. The stack: 3\ntb of permissively licensed source code.\n[272] J. Koco\u00b4n, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szy-\nd\u0142o, J. Baran, J. Bielaniewicz, M. Gruza et al. 2023. Chat-\ngpt: Jack of all trades, master of none.\n[273] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo and Y. Iwasawa.\n2022. Large language models are zero-shot reasoners. In\nAdvances in Neural Information Processing Systems.\n[274] A. K\u00f6pf, Y. Kilcher, D. von R\u00fctte, S. Anagnostidis, Z.-\nR. Tam, K. Stevens, A. Barhoum, N. M. Duc et al. 2023.\nOpenassistant conversations\u2013democratizing large language\nmodel alignment. arXiv preprint arXiv:2304.07327.\n58\n[275] T. Korbak, K. Shi, A. Chen, R. Bhalerao, C. L. Buckley,\nJ. Phang, S. R. Bowman and E. Perez. 2023. Pretraining\nlanguage models with human preferences. arXiv preprint\narXiv:2302.08582.\n[276] D. M. Korngiebel and S. D. Mooney. 2021. Consider-\ning the possibilities and pitfalls of generative pre-trained\ntransformer 3 (gpt-3) in healthcare delivery. NPJ Digital\nMedicine, 4(1):1\u20133.\n[277] M. Kosinski. 2023. Theory of mind may have sponta-\nneously emerged in large language models.\n[278] B. Krause, A. D. Gotmare, B. McCann, N. S. Keskar,\nS. Joty, R. Socher and N. F. Rajani. 2021. GeDi: Genera-\ntive discriminator guided sequence generation. In Findings\nof the Association for Computational Linguistics: EMNLP\n2021, pages 4929\u20134952, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\n[279] D. C. Krawczyk. 2018. Introduction to reasoning. Rea-\nsoning\u2014The Neuroscience of How We Think; Academic\nPress: Cambridge, MA, USA, pages 1\u201311.\n[280] K. Krishna, Y. Song, M. Karpinska, J. Wieting and\nM. Iyyer. 2023.\nParaphrasing evades detectors of AI-\ngenerated text, but retrieval is an effective defense.\nArXiv:2303.13408 [cs].\n[281] T. Kudo. 2018. Subword regularization: Improving neu-\nral network translation models with multiple subword can-\ndidates. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pages 66\u201375, Melbourne, Australia. Associ-\nation for Computational Linguistics.\n[282] T. Kudo and J. Richardson. 2018. Sentencepiece: A\nsimple and language independent subword tokenizer and\ndetokenizer for neural text processing.\narXiv preprint\narXiv:1808.06226.\n[283] A. Kulkarni. 2021. GitHub Copilot AI Is Leaking Func-\ntional API Keys.\n[284] S. R. K\u00fcnzel, J. S. Sekhon, P. J. Bickel and B. Yu. 2019.\nMetalearners for estimating heterogeneous treatment ef-\nfects using machine learning. Proceedings of the national\nacademy of sciences, 116(10):4156\u20134165.\n[285] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. Yu,\nJ. Gonzalez, H. Zhang et al. 2023. vllm: Easy, fast, and\ncheap llm serving with pagedattention.\n[286] E. K\u0131c\u0131man, R. Ness, A. Sharma and C. Tan. 2023.\nCausal reasoning and large language models: Opening\na new frontier for causality.\n[287] P. Lab. 2023. Awesome-Prompt-Engineering. Original-\ndate: 2023-02-09T18:22:52Z.\n[288] A. K. Lampinen, S. C. Chan, I. Dasgupta, A. J. Nam\nand J. X. Wang. 2023. Passive learning of active causal\nstrategies in agents and language models. arXiv preprint\narXiv:2305.16183.\n[289] H. Lauren\u00e7on, L. Saulnier, T. Wang, C. Akiki, A. V. del\nMoral, T. L. Scao, L. V. Werra, C. Mou et al. 2022. The big-\nscience ROOTS corpus: A 1.6TB composite multilingual\ndataset. In Thirty-sixth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track.\n[290] A. Lazaridou, E. Gribovskaya, W. Stokowiec and\nN. Grigorev. 2022. Internet-augmented language mod-\nels through few-shot prompting for open-domain question\nanswering.\n[291] A. Lee, B. Miranda and S. Koyejo. 2023. Beyond Scale:\nthe Diversity Coefficient as a Data Quality Metric Demon-\nstrates LLMs are Pre-trained on Formally Diverse Data.\nArXiv:2306.13840 [cs].\n[292] D. Lee, J. Lee, J.-W. Ha, J.-H. Kim, S.-W. Lee,\nH. Lee and H. O. Song. 2023. Query-efficient black-box\nred teaming via bayesian optimization. arXiv preprint\narXiv:2305.17444.\n[293] K. Lee, O. Firat, A. Agarwal, C. Fannjiang and D. Sus-\nsillo. 2018. Hallucinations in neural machine translation.\n[294] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck,\nC. Callison-Burch and N. Carlini. 2021. Deduplicating\ntraining data makes language models better. arXiv preprint\narXiv:2107.06499.\n[295] N. Lee, W. Ping, P. Xu, M. Patwary, P. Fung, M. Shoeybi\nand B. Catanzaro. Factuality Enhanced Language Models\nfor Open-Ended Text Generation.\n[296] P. Lee, S. Bubeck and J. Petro. 2023. Benefits, limits,\nand risks of gpt-4 as an ai chatbot for medicine. New\nEngland Journal of Medicine, 388(13):1233\u20131239.\n[297] E. Lehman, E. Hernandez, D. Mahajan, J. Wulff, M. J.\nSmith, Z. Ziegler, D. Nadler, P. Szolovits et al. 2023. Do\nwe still need clinical language models?\n[298] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang,\nM. Krikun, N. Shazeer et al. 2020. Gshard: Scaling gi-\nant models with conditional computation and automatic\nsharding.\n[299] B. Lester, R. Al-Rfou and N. Constant. 2021. The power\nof scale for parameter-efficient prompt tuning. In Pro-\nceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 3045\u20133059, On-\nline and Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\n[300] Y. Leviathan, M. Kalman and Y. Matias. 2022. Fast in-\nference from transformers via speculative decoding. arXiv\npreprint arXiv:2211.17192.\n[301] D. M. Levine, R. Tuwani, B. Kompa, A. Varma, S. G.\nFinlayson, A. Mehrotra and A. Beam. 2023. The diagnostic\nand triage accuracy of the gpt-3 artificial intelligence model.\nmedRxiv, pages 2023\u201301.\n[302] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal and\nL. Zettlemoyer. 2021. Base layers: Simplifying training of\nlarge, sparse models.\n[303] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V. Stoyanov and L. Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 7871\u2013\n7880, Online. Association for Computational Linguistics.\n[304] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\nN. Goyal, H. K\u00fcttler, M. Lewis et al. 2020. Retrieval-\naugmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems,\n33:9459\u20139474.\n59\n[305] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer,\nH. Michalewski, V. Ramasesh, A. Slone, C. Anil et al.\n2022. Solving quantitative reasoning problems with lan-\nguage models.\n[306] B. Z. Li, M. Nye and J. Andreas. 2021. Implicit repre-\nsentations of meaning in neural language models. arXiv\npreprint arXiv:2106.00737.\n[307] C. Li, A. A. Awan, H. Tang, S. Rajbhandari and Y. He.\n2021. 1-bit lamb: Communication efficient large-scale\nlarge-batch training with lamb\u2019s convergence speed. arXiv\npreprint arXiv:2104.06069.\n[308] D. Li, R. Shao, A. Xie, Y. Sheng, L. Zheng, J. E. Gonza-\nlez, I. Stoica, X. Ma et al. 2023. How long can open-source\nllms truly promise on context length?\n[309] H. Li, D. Guo, W. Fan, M. Xu and Y. Song. 2023. Multi-\nstep jailbreaking privacy attacks on chatgpt. arXiv preprint\narXiv:2304.05197.\n[310] R. Li, J. Su, C. Duan and S. Zheng. 2020. Linear at-\ntention mechanism: An efficient attention for semantic\nsegmentation. arXiv preprint arXiv:2007.14902.\n[311] X. L. Li and P. Liang. 2021. Prefix-tuning: Optimizing\ncontinuous prompts for generation. In Proceedings of the\n59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers),\npages 4582\u20134597, Online. Association for Computational\nLinguistics.\n[312] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou and\nW. Chen. 2022. On the advance of making language mod-\nels better reasoners.\n[313] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrit-\ntwieser, R. Leblond, T. Eccles, J. Keeling et al. 2022.\nCompetition-level code generation with alphacode. Sci-\nence, 378(6624):1092\u20131097.\n[314] Z. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J.\nReddi, K. Ye, F. Chern et al. 2023. The Lazy Neuron\nPhenomenon: On Emergence of Activation Sparsity in\nTransformers. ArXiv:2210.06313 [cs, stat].\n[315] L. Lian, B. Li, A. Yala and T. Darrell. 2023.\nLlm-\ngrounded diffusion: Enhancing prompt understanding of\ntext-to-image diffusion models with large language models.\n[316] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter,\nP. Florence and A. Zeng. 2023. Code as policies: Language\nmodel programs for embodied control.\n[317] P. P. Liang, C. Wu, L.-P. Morency and R. Salakhutdi-\nnov. 2021. Towards understanding and mitigating social\nbiases in language models. In International Conference on\nMachine Learning, pages 6565\u20136576. PMLR.\n[318] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu,\nM. Yasunaga, Y. Zhang, D. Narayanan et al. 2022.\nHolistic evaluation of language models. arXiv preprint\narXiv:2211.09110.\n[319] O. Lieber, O. Sharir, B. Lenz and Y. Shoham. 2021.\nJurassic-1: Technical details and evaluation. White Paper.\nAI21 Labs, 1.\n[320] V. Li\u00e9vin, C. E. Hother and O. Winther. 2022. Can large\nlanguage models reason about medical questions? arXiv\npreprint arXiv:2207.08143.\n[321] C.-C. Lin, A. Jaech, X. Li, M. R. Gormley and J. Eis-\nner. 2020. Limitations of autoregressive models and their\nalternatives. arXiv preprint arXiv:2010.11939.\n[322] J. Lin, A. Yang, J. Bai, C. Zhou, L. Jiang, X. Jia,\nA. Wang, J. Zhang et al. 2021.\nM6-10t: A sharing-\ndelinking paradigm for efficient multi-trillion parameter\npretraining. arXiv preprint arXiv:2110.03888.\n[323] S. Lin, J. Hilton and O. Evans. 2021.\nTruthfulqa:\nMeasuring how models mimic human falsehoods. arXiv\npreprint arXiv:2109.07958.\n[324] X. V. Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen,\nD. Simig, M. Ott, N. Goyal et al. 2022. Few-shot learning\nwith multilingual generative language models.\nIn Pro-\nceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 9019\u20139052, Abu\nDhabi, United Arab Emirates. Association for Computa-\ntional Linguistics.\n[325] Y.-T. Lin and Y.-N. Chen. 2023.\nLlm-eval: Unified\nmulti-dimensional automatic evaluation for open-domain\nconversations with large language models. arXiv preprint\narXiv:2305.13711.\n[326] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, A. dos\nSantos Costa, M. Fazel-Zarandi et al. 2022. Language\nmodels of protein sequences at the scale of evolution enable\naccurate structure prediction. BioRxiv.\n[327] W. Ling, D. Yogatama, C. Dyer and P. Blunsom. 2017.\nProgram induction by rationale generation: Learning to\nsolve and explain algebraic word problems. In Proceed-\nings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n158\u2013167, Vancouver, Canada. Association for Computa-\ntional Linguistics.\n[328] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy and\nC. Zhang. 2023. Exposing Attention Glitches with Flip-\nFlop Language Modeling. ArXiv:2306.00946 [cs].\n[329] F. Liu, J. M. Eisenschlos, F. Piccinno, S. Krichene,\nC. Pang, K. Lee, M. Joshi, W. Chen et al. 2022. Deplot:\nOne-shot visual language reasoning by plot-to-table trans-\nlation. arXiv preprint arXiv:2212.10505.\n[330] H. Liu, C. Sferrazza and P. Abbeel. 2023. Languages\nare rewards: Hindsight finetuning using human feedback.\narXiv preprint arXiv:2302.02676.\n[331] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang,\nM. Bansal and C. A. Raffel. 2022. Few-shot parameter-\nefficient fine-tuning is better and cheaper than in-context\nlearning. Advances in Neural Information Processing Sys-\ntems, 35:1950\u20131965.\n[332] H. Liu, S. M. Xie, Z. Li and T. Ma. 2022. Same pre-\ntraining loss, better downstream: Implicit bias matters for\nlanguage models. ArXiv, abs/2210.14199.\n[333] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua,\nF. Petroni and P. Liang. 2023. Lost in the Middle: How\nLanguage Models Use Long Contexts. ArXiv:2307.03172\n[cs].\n[334] R. Liu, C. Jia, J. Wei, G. Xu and S. Vosoughi. 2022.\nQuantifying and alleviating political bias in language mod-\nels. Artificial Intelligence, 304:103654.\n60\n[335] R. Liu and N. B. Shah. 2023. ReviewerGPT? An Ex-\nploratory Study on Using Large Language Models for Pa-\nper Reviewing. ArXiv:2306.00622 [cs].\n[336] S. Liu and Z. Wang. 2023. Ten lessons we have learned\nin the new\" sparseland\": A short handbook for sparse neu-\nral network researchers. arXiv preprint arXiv:2302.02596.\n[337] X. Liu, X. Yang, L. Ouyang, G. Guo, J. Su, R. Xi,\nK. Yuan and F. Yuan. 2022.\nProtein language model\npredicts mutation pathogenicity and clinical prognosis.\nbioRxiv, pages 2022\u201309.\n[338] Z. Liu, A. Bahety and S. Song. 2023. Reflect: Summa-\nrizing robot experiences for failure explanation and correc-\ntion.\n[339] Z. Liu, E. Gan and M. Tegmark. 2023. Seeing is be-\nlieving: Brain-inspired modular training for mechanistic\ninterpretability. arXiv preprint arXiv:2305.08746.\n[340] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung,\nY. Tay, D. Zhou, Q. V. Le et al. 2023. The flan collec-\ntion: Designing data and methods for effective instruction\ntuning.\n[341] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts,\nB. Zoph, D. Zhou, J. Wei et al. 2023. A Pretrainer\u2019s Guide\nto Training Data: Measuring the Effects of Data Age, Do-\nmain Coverage, Quality, & Toxicity. ArXiv:2305.13169\n[cs].\n[342] Y. Lu, M. Bartolo, A. Moore, S. Riedel and P. Stene-\ntorp. 2022. Fantastically ordered prompts and where to\nfind them: Overcoming few-shot prompt order sensitivity.\nIn Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pages 8086\u20138098, Dublin, Ireland. Association\nfor Computational Linguistics.\n[343] Y. Lu, C. Li, M. Zhang, C. De Sa and Y. He. 2022. Max-\nimizing communication efficiency for large-scale training\nvia 0/1 adam. arXiv preprint arXiv:2202.06009.\n[344] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz\nand S. Zanella-B\u00e9guelin. 2023. Analyzing Leakage of\nPersonally Identifiable Information in Language Models.\nArXiv:2302.00539 [cs].\n[345] B. Luo, R. Y. Lau, C. Li and Y.-W. Si. 2022. A critical\nreview of state-of-the-art chatbot designs and applications.\nWiley Interdisciplinary Reviews: Data Mining and Knowl-\nedge Discovery, 12(1):e1434.\n[346] Y. Luo, N. Tang, G. Li, C. Chai, W. Li and X. Qin. 2021.\nSynthesizing natural language to visualization (nl2vis)\nbenchmarks from nl2sql benchmarks. In Proceedings of\nthe 2021 International Conference on Management of Data,\npages 1235\u20131247.\n[347] A. Lynch, G. J. Dovonon, J. Kaddour and R. Silva. 2023.\nSpawrious: A benchmark for fine control of spurious cor-\nrelation biases. arXiv preprint arXiv:2303.05470.\n[348] P. Ma, Z. Li, A. Sun and S. Wang. 2023. \"oops, did i just\nsay that?\" testing and repairing unethical suggestions of\nlarge language models with suggest-critique-reflect process.\narXiv preprint arXiv:2305.02626.\n[349] X. Ma, G. Fang and X. Wang. 2023. Llm-pruner: On the\nstructural pruning of large language models. arXiv preprint\narXiv:2305.11627.\n[350] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma\nand L. Zettlemoyer. 2021. Luna: Linear unified nested\nattention.\nAdvances in Neural Information Processing\nSystems, 34:2441\u20132453.\n[351] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,\nS. Wiegreffe, U. Alon, N. Dziri et al. 2023. Self-refine:\nIterative refinement with self-feedback.\n[352] A. Madani, B. Krause, E. R. Greene, S. Subramanian,\nB. P. Mohr, J. M. Holton, J. L. Olmos Jr, C. Xiong et al.\n2023. Large language models generate functional protein\nsequences across diverse families. Nature Biotechnology,\npages 1\u20138.\n[353] M. Maddela, M. Ung, J. Xu, A. Madotto, H. Foran\nand Y.-L. Boureau. 2023.\nTraining Models to Gen-\nerate, Recognize, and Reframe Unhelpful Thoughts.\nArXiv:2307.02768 [cs].\n[354] S. Mahdavi, R. Liao and C. Thrampoulidis. 2023. Memo-\nrization Capacity of Multi-Head Attention in Transformers.\nArXiv:2306.02010 [cs].\n[355] S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee,\nD. Chen and S. Arora. 2023. Fine-Tuning Language Mod-\nels with Just Forward Passes. ArXiv:2305.17333 [cs].\n[356] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada\nand S. Paul. 2022.\nPeft:\nState-of-the-art parameter-\nefficient fine-tuning methods. https://github.com/\nhuggingface/peft.\n[357] P. Maniatis and D. Tarlow. 2023.\nLarge sequence\nmodels for software development activities.\nAvailable\nfrom:\nhttps://ai.googleblog.com/2023/\n05/large-sequence-models-for-software.\nhtml. Accessed: 26/06/2023.\n[358] R. R. McCrae and P. T. Costa Jr. 1997. Personality trait\nstructure as a human universal. American psychologist,\n52(5):509.\n[359] I. R. McKenzie, A. Lyzhov, M. Pieler, A. Parrish,\nA. Mueller, A. Prabhu, E. McLean, A. Kirtland et al.\n2023.\nInverse Scaling:\nWhen Bigger Isn\u2019t Better.\nArXiv:2306.09479 [cs].\n[360] K. Meng, D. Bau, A. J. Andonian and Y. Belinkov. 2022.\nLocating and editing factual associations in GPT. In Ad-\nvances in Neural Information Processing Systems.\n[361] K. Meng, A. S. Sharma, A. J. Andonian, Y. Belinkov\nand D. Bau. 2023. Mass-editing memory in a transformer.\nIn The Eleventh International Conference on Learning\nRepresentations.\n[362] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song,\nM. Chadwick, M. Glaese, S. Young et al. 2022. Teaching\nlanguage models to support answers with verified quotes.\n[363] G. Mialon, R. Dess\u00ec, M. Lomeli, C. Nalmpantis, R. Pa-\nsunuru, R. Raileanu, B. Rozi\u00e8re, T. Schick et al. 2023.\nAugmented language models: a survey. arXiv preprint\narXiv:2302.07842.\n[364] S. Milgram. 1963. Behavioral study of obedience. The\nJournal of abnormal and social psychology, 67(4):371.\n[365] S. Min, K. Krishna, X. Lyu, M. Lewis, W.-t. Yih, P. W.\nKoh, M. Iyyer, L. Zettlemoyer et al. 2023. FActScore:\nFine-grained Atomic Evaluation of Factual Precision in\nLong Form Text Generation. ArXiv:2305.14251 [cs].\n61\n[366] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis,\nH. Hajishirzi and L. Zettlemoyer. 2022. Rethinking the\nrole of demonstrations: What makes in-context learning\nwork?\n[367] M. Miotto, N. Rossberg and B. Kleinberg. 2022. Who\nis gpt-3? an exploration of personality, values and demo-\ngraphics. arXiv preprint arXiv:2209.14338.\n[368] P. Mirowski, K. W. Mathewson, J. Pittman and R. Evans.\n2022. Co-writing screenplays and theatre scripts with lan-\nguage models: An evaluation by industry professionals.\narXiv preprint arXiv:2209.14958.\n[369] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic,\nG. Venkatesh, C. Yu and P. Micikevicius. 2021.\nAc-\ncelerating sparse deep neural networks. arXiv preprint\narXiv:2104.08378.\n[370] S. Mishra, D. Khashabi, C. Baral and H. Hajishirzi. 2022.\nCross-task generalization via natural language crowdsourc-\ning instructions. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland.\nAssociation for Computational Linguistics.\n[371] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning\nand C. Finn. 2023.\nDetectGPT: Zero-Shot Machine-\nGenerated Text Detection using Probability Curvature.\nArXiv:2301.11305 [cs].\n[372] E. Mitchell, C. Lin, A. Bosselut, C. Finn and C. D. Man-\nning. 2022. Fast model editing at scale. In International\nConference on Learning Representations.\n[373] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning and\nC. Finn. 2022. Memory-based model editing at scale. In\nProceedings of the 39th International Conference on Ma-\nchine Learning, volume 162 of Proceedings of Machine\nLearning Research, pages 15817\u201315831. PMLR.\n[374] R. Moriconi, M. P. Deisenroth and K. Sesh Kumar.\n2020. High-dimensional bayesian optimization using low-\ndimensional feature spaces. Machine Learning, 109:1925\u2013\n1943.\n[375] M. Moussa\u00efd, J. E. K\u00e4mmer, P. P. Analytis and H. Neth.\n2013. Social influence and the collective dynamics of\nopinion formation. PloS one, 8(11):e78433.\n[376] M. Mozes, J. Hoffmann, K. Tomanek, M. Kouate,\nN. Thain, A. Yuan, T. Bolukbasi and L. Dixon. 2023. To-\nwards agile text classifiers for everyone. arXiv preprint\narXiv:2302.06541.\n[377] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts,\nS. Biderman, T. L. Scao, M. S. Bari, S. Shen et al. 2022.\nCrosslingual generalization through multitask finetuning.\narXiv preprint arXiv:2211.01786.\n[378] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal,\nH. Palangi and A. Awadallah. 2023. Orca: Progressive\nlearning from complex explanation traces of gpt-4. arXiv\npreprint arXiv:2306.02707.\n[379] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang,\nC. Kim, C. Hesse, S. Jain et al. 2021. Webgpt: Browser-\nassisted question-answering with human feedback. arXiv\npreprint arXiv:2112.09332.\n[380] N. Nanda, L. Chan, T. Lieberum, J. Smith and J. Stein-\nhardt. 2023. Progress measures for grokking via mechanis-\ntic interpretability. In The Eleventh International Confer-\nence on Learning Representations.\n[381] S. Nerella, S. Bandyopadhyay, J. Zhang, M. Contreras,\nS. Siegel, A. Bumin, B. Silva, J. Sena et al. 2023. Trans-\nformers in healthcare: A survey.\n[382] A. Nguyen, N. Karampatziakis and W. Chen. 2023. Meet\nin the middle: A new pre-training paradigm. arXiv preprint\narXiv:2303.07295.\n[383] E. Nguyen, M. Poli, M. Faizi, A. Thomas, C. Birch-\nSykes, M. Wornow, A. Patel, C. Rabideau et al. 2023. Hye-\nnadna: Long-range genomic sequence modeling at single\nnucleotide resolution. arXiv preprint arXiv:2306.15794.\n[384] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam,\nP. Mishkin, B. McGrew, I. Sutskever and M. Chen. 2022.\nGlide: Towards photorealistic image generation and editing\nwith text-guided diffusion models.\n[385] X. Nie and S. Wager. 2021. Quasi-oracle estimation of\nheterogeneous treatment effects. Biometrika, 108(2):299\u2013\n319.\n[386] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang,\nY. Zhou, S. Savarese and C. Xiong. 2022. Codegen: An\nopen large language model for code with multi-turn pro-\ngram synthesis.\n[387] F. Niu, B. Recht, C. Re, S. J. Wright and W. D. St. Hog-\nwild!: A Lock-Free Approach to Parallelizing Stochastic\nGradient Descent.\n[388] H. Nori, N. King, S. M. McKinney, D. Carignan and\nE. Horvitz. 2023. Capabilities of gpt-4 on medical chal-\nlenge problems.\n[389] K. Nottingham, P. Ammanabrolu, A. Suhr, Y. Choi,\nH. Hajishirzi, S. Singh and R. Fox. 2023. Do embod-\nied agents dream of pixelated sheep?: Embodied decision\nmaking using language guided world modelling. arXiv\npreprint arXiv:2301.12050.\n[390] S. Nurk, S. Koren, A. Rhie, M. Rautiainen, A. V.\nBzikadze, A. Mikheenko, M. R. Vollger, N. Altemose et al.\n2022. The complete sequence of a human genome. Sci-\nence, 376(6588):44\u201353.\n[391] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski,\nJ. Austin, D. Bieber, D. Dohan, A. Lewkowycz et al. 2021.\nShow your work: Scratchpads for intermediate computa-\ntion with language models.\n[392] Ofir Press [@OfirPress]. 2022.\nGPT-3 seems to be\nnondeterministic even when it should be (i.e. temper-\nature == 0). Has anyone else noticed this?\nIs there\na known fix?\nVideo by my collaborator Muru Zhang.\nhttps://t.co/dOWYWPBYyP.\n[393] N. Oh, G.-S. Choi and W. Y. Lee. 2023. Chatgpt goes\nto operating room: Evaluating gpt-4 performance and its\npotential in surgical education and training in the era of\nlarge language models. medRxiv.\n[394] C. Olah. Mechanistic Interpretability, Variables, and the\nImportance of Interpretable Bases.\n62\n[395] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. Das-\nSarma, T. Henighan, B. Mann, A. Askell et al. 2022.\nIn-context learning and induction heads. arXiv preprint\narXiv:2209.11895.\n[396] OpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue.\nhttps://openai.com/blog/\nchatgpt/. Accessed: 2023-02-18.\n[397] OpenAI.\n2023.\nChat\ngpt\n4\npainfully\nslow.\nhttps://community.openai.com/t/\nchat-gpt-4-painfully-slow/117996.\n[398] OpenAI. 2023. Gpt-4 technical report.\n[399] P. J. Ortiz Su\u2019arez, B. Sagot and L. Romary. 2019. Asyn-\nchronous pipelines for processing huge corpora on medium\nto low resource infrastructures. In Proceedings of the Work-\nshop on Challenges in the Management of Large Corpora\n(CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 \u2013 16,\nMannheim. Leibniz-Institut f\"ur Deutsche Sprache.\n[400] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng,\nD. Grangier and M. Auli. 2019.\nfairseq: A fast, ex-\ntensible toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\n[401] N. Ousidhoum, X. Zhao, T. Fang, Y. Song and D.-Y.\nYeung. 2021. Probing toxic content in large pre-trained lan-\nguage models. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4262\u20134274.\n[402] C. Outeiral and C. Deane. 2022. Codon language em-\nbeddings provide strong signals for protein engineering.\nbioRxiv, pages 2022\u201312.\n[403] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal et al. 2022. Training lan-\nguage models to follow instructions with human feedback.\nIn Advances in Neural Information Processing Systems.\n[404] M. Pagliardini, D. Paliotta, M. Jaggi and F. Fleuret. 2023.\nFaster causal attention over large sequences through sparse\nflash attention.\n[405] J. Pan, T. Gao, H. Chen and D. Chen. 2023. What in-\ncontext learning \"learns\" in-context: Disentangling task\nrecognition and task learning.\n[406] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi,\nL. Zettlemoyer and M. T. Ribeiro. 2023. Art: Automatic\nmulti-step reasoning and tool-use for large language mod-\nels.\n[407] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee and D. Lee.\n2022. nuqmm: Quantized matmul for efficient inference\nof large-scale generative language models. arXiv preprint\narXiv:2206.09557.\n[408] J. S. Park, J. C. O\u2019Brien, C. J. Cai, M. R. Morris, P. Liang\nand M. S. Bernstein. 2023. Generative agents: Interactive\nsimulacra of human behavior.\n[409] P. S. Park, P. Schoenegger and C. Zhu. 2023. Artifi-\ncial intelligence in psychology research. arXiv preprint\narXiv:2302.07267.\n[410] A. Patel, B. Li, M. S. Rasooli, N. Constant, C. Raffel and\nC. Callison-Burch. 2023. Bidirectional language models\nare also few-shot learners.\n[411] N. D. Patson, E. S. Darowski, N. Moon and F. Ferreira.\n2009. Lingering misinterpretations in garden-path sen-\ntences: evidence from a paraphrasing task. Journal of\nExperimental Psychology: Learning, Memory, and Cogni-\ntion, 35(1):280.\n[412] D. Patterson, J. Gonzalez, U. H\u00f6lzle, Q. Le, C. Liang,\nL.-M. Munguia, D. Rothchild, D. R. So et al. 2022. The\ncarbon footprint of machine learning training will plateau,\nthen shrink. Computer, 55(7):18\u201328.\n[413] A. Paullada, I. D. Raji, E. M. Bender, E. Denton and\nA. Hanna. 2021. Data and its (dis) contents: A survey of\ndataset development and use in machine learning research.\nPatterns, 2(11):100336.\n[414] M. Pellert, C. M. Lechner, C. Wagner, B. Rammstedt\nand M. Strohmaier. 2023. Ai psychometrics: Using psy-\nchometric inventories to obtain psychological profiles of\nlarge language models.\n[415] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru,\nA. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei et al.\n2023. The RefinedWeb Dataset for Falcon LLM: Outper-\nforming Curated Corpora with Web Data, and Web Data\nOnly. ArXiv:2306.01116 [cs].\n[416] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Ar-\ncadinho, H. Cao, X. Cheng, M. Chung et al. 2023.\nRWKV: Reinventing RNNs for the Transformer Era.\nArXiv:2305.13048 [cs].\n[417] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcad-\ninho, H. Cao, X. Cheng, M. Chung et al. 2023. Rwkv:\nReinventing rnns for the transformer era. arXiv preprint\narXiv:2305.13048.\n[418] C. Peng, X. Yang, A. Chen, K. E. Smith, N. PourNejatian,\nA. B. Costa, C. Martin, M. G. Flores et al. 2023. A study\nof generative large language model for medical research\nand healthcare.\n[419] Y. Peng. 2021. A MARVS analysis of two Chinese near-\nsynonymous verbs of jumping based on Chinese corpora.\nIn Proceedings of the 35th Pacific Asia Conference on\nLanguage, Information and Computation, pages 483\u2013492,\nShanghai, China. Association for Computational Lingus-\ntics.\n[420] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides,\nA. Glaese, N. McAleese et al. 2022. Red teaming lan-\nguage models with language models.\narXiv preprint\narXiv:2202.03286.\n[421] E. Perez, S. Ringer, K. Luko\u0161i\u00afut\u02d9e, K. Nguyen, E. Chen,\nS. Heiner, C. Pettit, C. Olsson et al. 2022. Discovering\nlanguage model behaviors with model-written evaluations.\n[422] F. Perez and I. Ribeiro. 2022. Ignore previous prompt:\nAttack techniques for language models. arXiv preprint\narXiv:2211.09527.\n[423] L. Peric, S. Mijic, D. Stammbach and E. Ash. 2020. Le-\ngal language modeling with transformers. In Proceedings\nof the Fourth Workshop on Automated Semantic Analysis\nof Information in Legal Text (ASAIL 2020) held online in\nconjunction with te 33rd International Conference on Le-\ngal Knowledge and Information Systems (JURIX 2020)\nDecember 9, 2020, volume 2764. CEUR-WS.\n63\n[424] B. Peters and A. F. T. Martins. 2021. Smoothing and\nshrinking the sparse Seq2Seq search space. In Proceedings\nof the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, pages 2642\u20132654, Online. Associ-\nation for Computational Linguistics.\n[425] J. Peters, D. Janzing and B. Sch\u00f6lkopf. 2017. Elements\nof causal inference: foundations and learning algorithms.\nThe MIT Press.\n[426] A. Petrov, E. La Malfa, P. H. Torr and A. Bibi. 2023.\nLanguage model tokenizers introduce unfairness between\nlanguages. arXiv preprint arXiv:2305.15425.\n[427] T. Pettinato Oltz. 2023. Chatgpt, professor of law. Pro-\nfessor of Law (February 4, 2023).\n[428] J. Pfeiffer, A. R\u00fcckl\u00e9, C. Poth, A. Kamath, I. Vuli\u00b4c,\nS. Ruder, K. Cho and I. Gurevych. 2020. AdapterHub:\nA framework for adapting transformers. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pages 46\u2013\n54, Online. Association for Computational Linguistics.\n[429] S. Pichai. 2023. An important next step on our ai jour-\nney.\nhttps://blog.google/technology/ai/\nbard-google-ai-search-updates/. Accessed:\n2023-02-18.\n[430] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao,\nS. Baccus, Y. Bengio, S. Ermon et al. 2023. Hyena Hier-\narchy: Towards Larger Convolutional Language Models.\nArXiv:2302.10866 [cs].\n[431] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Brad-\nbury, A. Levskaya, J. Heek, K. Xiao et al. 2022. Efficiently\nScaling Transformer Inference. ArXiv:2211.05102 [cs].\n[432] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Brad-\nbury, A. Levskaya, J. Heek, K. Xiao et al. 2022.\nEf-\nficiently scaling transformer inference.\narXiv preprint\narXiv:2211.05102.\n[433] V. Prabhakaran, A. Mostafazadeh Davani and M. Diaz.\n2021. On releasing annotator-level labels and information\nin datasets. In Proceedings of the Joint 15th Linguistic\nAnnotation Workshop (LAW) and 3rd Designing Meaning\nRepresentations (DMR) Workshop, pages 133\u2013138, Punta\nCana, Dominican Republic. Association for Computational\nLinguistics.\n[434] O. Press, N. A. Smith and M. Lewis. 2021. Train short,\ntest long: Attention with linear biases enables input length\nextrapolation.\n[435] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith\nand M. Lewis. 2023. Measuring and Narrowing the Com-\npositionality Gap in Language Models. ArXiv:2210.03350\n[cs].\n[436] J. Qian, H. Wang, Z. Li, S. Li and X. Yan. 2022. Lim-\nitations of language models in arithmetic and symbolic\ninduction. arXiv preprint arXiv:2208.05051.\n[437] J. Rabelo, R. Goebel, M.-Y. Kim, Y. Kano, M. Yosh-\nioka and K. Satoh. 2022. Overview and discussion of the\ncompetition on legal information Extraction/Entailment\n(COLIEE) 2021. The Review of Socionetwork Strategies,\n16(1):111\u2013133.\n[438] A. Radford, R. Jozefowicz and I. Sutskever. 2017. Learn-\ning to generate reviews and discovering sentiment. arXiv\npreprint arXiv:1704.01444.\n[439] A. Radford,\nJ. W. Kim,\nT. Xu,\nG. Brockman,\nC.\nMcLeavey\nand\nI.\nSutskever.\n2022.\nRobust\nSpeech Recognition via Large-Scale Weak Supervision.\nArXiv:2212.04356 [cs, eess].\n[440] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei and\nI. Sutskever. 2019. Language models are unsupervised\nmultitask learners.\n[441] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann,\nF. Song, J. Aslanides, S. Henderson et al. 2021. Scaling lan-\nguage models: Methods, analysis & insights from training\ngopher. arXiv preprint arXiv:2112.11446.\n[442] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D.\nManning and C. Finn. 2023. Direct preference optimiza-\ntion: Your language model is secretly a reward model.\narXiv preprint arXiv:2305.18290.\n[443] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li et al. 2022. Exploring the limits\nof transfer learning with a unified text-to-text transformer.\nJ. Mach. Learn. Res., 21(1).\n[444] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y.\nAminabadi, A. A. Awan, J. Rasley and Y. He. 2022.\nDeepSpeed-MoE: Advancing mixture-of-experts inference\nand training to power next-generation AI scale. In Pro-\nceedings of the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine Learning\nResearch, pages 18332\u201318346. PMLR.\n[445] S. Rajbhandari, J. Rasley, O. Ruwase and Y. He. 2020.\nZero: Memory optimizations toward training trillion param-\neter models. In Proceedings of the International Confer-\nence for High Performance Computing, Networking, Stor-\nage and Analysis, SC \u201920. IEEE Press.\n[446] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith and Y. He.\n2021. Zero-infinity: Breaking the gpu memory wall for\nextreme scale deep learning. In Proceedings of the In-\nternational Conference for High Performance Computing,\nNetworking, Storage and Analysis, SC \u201921, New York, NY,\nUSA. Association for Computing Machinery.\n[447] I. D. Raji, E. M. Bender, A. Paullada, E. Denton and\nA. Hanna. 2021. Ai and the everything in the whole wide\nworld benchmark. arXiv preprint arXiv:2111.15366.\n[448] A. Rajkomar, E. Loreaux, Y. Liu, J. Kemp, B. Li, M.-J.\nChen, Y. Zhang, A. Mohiuddin et al. 2022. Deciphering\nclinical abbreviations with a privacy protecting machine\nlearning system. Nature Communications, 13(1):7456.\n[449] R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hes-\nsel, R. Sifa, C. Bauckhage, H. Hajishirzi and Y. Choi.\n2022. Is reinforcement learning (not) for natural language\nprocessing?: Benchmarks, baselines, and building blocks\nfor natural language policy optimization. arXiv preprint\narXiv:2210.01241.\n[450] J. Rasley, S. Rajbhandari, O. Ruwase and Y. He. 2020.\nDeepspeed: System optimizations enable training deep\nlearning models with over 100 billion parameters. In Pro-\nceedings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, KDD \u201920,\npage 3505\u20133506, New York, NY, USA. Association for\nComputing Machinery.\n64\n[451] P. P. Ray. 2023. ChatGPT: A comprehensive review\non background, applications, key challenges, bias, ethics,\nlimitations and future scope. Internet of Things and Cyber-\nPhysical Systems, 3:121\u2013154.\n[452] E. Razumovskaia, J. Maynez, A. Louis, M. Lapata and\nS. Narayan. 2022. Little red riding hood goes around the\nglobe: Crosslingual story planning and generation with\nlarge language models. arXiv preprint arXiv:2212.10471.\n[453] B. Recht, C. Re, S. Wright and F. Niu. 2011. Hogwild!:\nA lock-free approach to parallelizing stochastic gradient de-\nscent. Advances in neural information processing systems,\n24.\n[454] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase,\nS. Yang, M. Zhang, D. Li and Y. He. 2021.\n{ZeRO-\nOffload}: Democratizing {Billion-Scale} model training.\nIn 2021 USENIX Annual Technical Conference (USENIX\nATC 21), pages 551\u2013564.\n[455] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang,\nP. Li, X. Zhang et al. 2023. Pangu-\nSigma: Towards trillion parameter language model with\nsparse heterogeneous computing.\n[456] Riley Goodside [@goodside]. 2022.\nAn edge-case\nin GPT-3 with big implications:\nInference is non-\ndeterministic (even at temperature=0) when top-2 token\nprobabilities are <1% different. So temperature=0 output\nis *very close* to deterministic, but actually isn\u2019t. Worth\nremembering.\n[457] X. Robin, J. Haas, R. Gumienny, A. Smolinski, G. Tau-\nriello and T. Schwede. 2021.\nContinuous automated\nmodel evaluation (cameo)\u2014perspectives on the future of\nfully automated evaluation of structure prediction meth-\nods. Proteins: Structure, Function, and Bioinformatics,\n89(12):1977\u20131986.\n[458] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell and\nK. Saenko. 2018. Object hallucination in image captioning.\narXiv preprint arXiv:1809.02156.\n[459] S. Roller, S. Sukhbaatar, A. Szlam and J. Weston. 2021.\nHash layers for large sparse models.\n[460] G. M. Rosa, L. Bonifacio, V. Jeronymo, H. Abonizio,\nR. Lotufo and R. Nogueira. 2022. Billions of parame-\nters are worth more than in-domain training data: A case\nstudy in the legal case entailment task. arXiv preprint\narXiv:2205.15172.\n[461] L. Ross, D. Greene and P. House. 1977. The \u201cfalse\nconsensus effect\u201d: An egocentric bias in social perception\nand attribution processes. Journal of experimental social\npsychology, 13(3):279\u2013301.\n[462] Y. Rottenstreich and C. K. Hsee. 2001. Money, kisses,\nand electric shocks: On the affective psychology of risk.\nPsychological science, 12(3):185\u2013190.\n[463] A. Roush. You probably don\u2019t know how to do Prompt\nEngineering, let me educate you.\n[464] L. Ruis, A. Khan, S. Biderman, S. Hooker, T. Rock-\nt\u00e4schel and E. Grefenstette. 2022. Large language models\nare not zero-shot communicators.\n[465] J. Rumbelow and mwatkins. SolidGoldMagikarp (plus,\nprompt generation).\n[466] S. Russell. 2021. Human-compatible artificial intelli-\ngence. Human-like machine intelligence, pages 3\u201323.\n[467] P. Rust,\nJ. F. Lotz,\nE. Bugliarello,\nE. Salesky,\nM. de Lhoneux and D. Elliott. 2023. Language Modelling\nwith Pixels. ArXiv:2207.06991 [cs].\n[468] A. Sabne. 2020. Xla : Compiling machine learning for\npeak performance.\n[469] V. S. Sadasivan, A. Kumar, S. Balasubramanian,\nW. Wang and S. Feizi. 2023. Can AI-Generated Text be\nReliably Detected? ArXiv:2303.11156 [cs].\n[470] M. Safdari, G. Serapio-Garc\u00eda, C. Crepy, S. Fitz,\nP. Romero, L. Sun, M. Abdulhai, A. Faust et al. 2023.\nPersonality traits in large language models.\n[471] S. Sagawa, P. W. Koh, T. B. Hashimoto and P. Liang.\n2020. Distributionally robust neural networks for group\nshifts: On the importance of regularization for worst-case\ngeneralization.\n[472] O. Sainz, J. C. Campos, I. Garc\u00eda-Ferrero, J. Etxaniz and\nE. Agirre. lm-contamination.\n[473] L. Salewski, S. Alaniz, I. Rio-Torto, E. Schulz and\nZ. Akata. 2023. In-context impersonation reveals large\nlanguage models\u2019 strengths and biases. arXiv preprint\narXiv:2305.14930.\n[474] G. Sanchez, H. Fan, A. Spangher, E. Levi, P. S. Am-\nmanamanchi and S. Biderman. 2023. Stay on topic with\nClassifier-Free Guidance. ArXiv:2306.17806 [cs].\n[475] V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika,\nZ. Alyafeai, A. Chaffin, A. Stiegler et al. 2022. Multitask\nprompted training enables zero-shot task generalization. In\nInternational Conference on Learning Representations.\n[476] S. Sanyal, J. Kaddour, A. Kumar and S. Sanghavi. 2023.\nUnderstanding the effectiveness of early weight averaging\nfor training large language models.\n[477] E. Saravia. 2022. Prompt Engineering Guide. Publica-\ntion Title: https://github.com/dair-ai/Prompt-Engineering-\nGuide original-date: 2022-12-16T16:04:50Z.\n[478] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann and\nH. Xu. 2023. Explaining legal concepts with augmented\nlarge language models (gpt-4).\n[479] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hess-\nlow, R. Castagn\u00e9, A. S. Luccioni et al. 2022. Bloom: A\n176b-parameter open-access multilingual language model.\n[480] R. Schaeffer, B. Miranda and S. Koyejo. 2023. Are\nemergent abilities of large language models a mirage?\n[481] T. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu,\nM. Lomeli, L. Zettlemoyer, N. Cancedda and T. Scialom.\n2023. Toolformer: Language models can teach themselves\nto use tools. arXiv preprint arXiv:2302.04761.\n[482] T. Schick, J. Dwivedi-Yu, Z. Jiang, F. Petroni, P. Lewis,\nG. Izacard, Q. You, C. Nalmpantis et al. 2022. Peer: A\ncollaborative language model.\n[483] T. Schick and H. Sch\u00fctze. 2021. It\u2019s not just size that\nmatters: Small language models are also few-shot learn-\ners. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 2339\u2013\n2352.\n65\n[484] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and\nO. Klimov. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\n[485] M. Schuster and K. Nakajima. 2012. Japanese and ko-\nrean voice search. In 2012 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\npages 5149\u20135152.\n[486] T. Schuster, R. Schuster, D. J. Shah and R. Barzi-\nlay. 2020.\nThe limitations of stylometry for detecting\nmachine-generated fake news. Computational Linguistics,\n46(2):499\u2013510.\n[487] R. Schwartz, J. Dodge, N. A. Smith and O. Etzioni. 2019.\nGreen AI. ArXiv:1907.10597 [cs, stat].\n[488] S. H. Schwartz, B. Breyer and D. Danner. 2015. Hu-\nman values scale (ess).\nZusammenstellung sozialwis-\nsenschaftlicher Items und Skalen (ZIS).\n[489] A. See, A. Pappu, R. Saxena, A. Yerukola and C. D.\nManning. 2019. Do massively pretrained language mod-\nels make better storytellers? In Proceedings of the 23rd\nConference on Computational Natural Language Learning\n(CoNLL), pages 843\u2013861, Hong Kong, China. Association\nfor Computational Linguistics.\n[490] R. Sennrich, B. Haddow and A. Birch. 2015. Neural ma-\nchine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909.\n[491] E. Sezgin, J. Sirrianni, S. L. Linwood et al. 2022. Oper-\nationalizing and implementing pretrained, large artificial\nintelligence linguistic models in the us health care system:\nOutlook of generative pretrained transformer 3 (gpt-3) as a\nservice model. JMIR Medical Informatics, 10(2):e32875.\n[492] P. Shaw, J. Uszkoreit and A. Vaswani. 2018.\nSelf-\nattention with relative position representations. In Pro-\nceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Papers),\npages 464\u2013468, New Orleans, Louisiana. Association for\nComputational Linguistics.\n[493] N. Shazeer. 2019. Fast transformer decoding: One write-\nhead is all you need.\n[494] N. Shazeer. 2019. Fast transformer decoding: One write-\nhead is all you need.\n[495] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le,\nG. Hinton and J. Dean. 2017. Outrageously large neu-\nral networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538.\n[496] Z. Shen, M. Zhang, H. Zhao, S. Yi and H. Li. 2021.\nEfficient attention: Attention with linear complexities. In\nProceedings of the IEEE/CVF winter conference on appli-\ncations of computer vision, pages 3531\u20133539.\n[497] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin,\nB. Chen, P. Liang, C. R\u00e9 et al. 2023. High-throughput\ngenerative inference of large language models with a single\ngpu.\n[498] T. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong,\nJ. Whittlestone, J. Leung, D. Kokotajlo, N. Marchal et al.\n2023. Model evaluation for extreme risks. arXiv preprint\narXiv:2305.15324.\n[499] A. Shirafuji, Y. Watanobe, T. Ito, M. Morishita, Y. Naka-\nmura, Y. Oda and J. Suzuki. 2023. Exploring the robust-\nness of large language models for solving programming\nproblems.\n[500] O.\nShliazhko,\nA.\nFenogenova,\nM.\nTikhonova,\nV. Mikhailov, A. Kozlova and T. Shavrina. 2022. mgpt:\nFew-shot learners go multilingual.\narXiv preprint\narXiv:2204.07580.\n[501] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper\nand B. Catanzaro. 2019. Megatron-lm: Training multi-\nbillion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\n[502] K. Shridhar, J. Macina, M. El-Assady, T. Sinha, M. Ka-\npur and M. Sachan. 2022. Automatic generation of socratic\nsubquestions for teaching math word problems. ArXiv,\nabs/2211.12835.\n[503] K. Shridhar, A. Stolfo and M. Sachan. 2022. Distilling\nmulti-step reasoning capabilities of large language models\ninto smaller models via semantic decompositions. arXiv\npreprint arXiv:2212.00193.\n[504] D. Shrivastava, H. Larochelle and D. Tarlow. 2022.\nRepository-level prompt generation for large language mod-\nels of code. arXiv preprint arXiv:2206.12839.\n[505] R. W. Shuai, J. A. Ruffolo and J. J. Gray. 2021. Gen-\nerative language modeling for antibody design. bioRxiv,\npages 2021\u201312.\n[506] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Pa-\npernot and R. Anderson. 2023. The curse of recursion:\nTraining on generated data makes models forget.\n[507] K. Shuster, S. Poff, M. Chen, D. Kiela and J. Weston.\n2021. Retrieval augmentation reduces hallucination in\nconversation. arXiv preprint arXiv:2104.07567.\n[508] K. Shuster, J. Xu, M. Komeili, D. Ju, E. M. Smith,\nS. Roller, M. Ung, M. Chen et al. 2022. Blenderbot 3:\na deployed conversational agent that continually learns to\nresponsibly engage.\n[509] S. Sia and K. Duh. 2023. In-context learning as maintain-\ning coherency: A study of on-the-fly machine translation\nusing large language models. ArXiv, abs/2305.03573.\n[510] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu,\nJ. Tremblay, D. Fox, J. Thomason et al. 2022. Progprompt:\nGenerating situated robot task plans using large language\nmodels.\n[511] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W.\nChung, N. Scales, A. Tanwani et al. 2022. Large language\nmodels encode clinical knowledge.\n[512] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn,\nL. Hou, K. Clark, S. Pfohl et al. 2023. Towards expert-level\nmedical question answering with large language models.\narXiv preprint arXiv:2305.09617.\n[513] A. Sinitsin, D. Pyrkin, A. Babenko, V. Plokhotnyuk and\nS. Popov. 2020. EDITABLE NEURAL NETWORKS.\n[514] S. L. Smith, P.-J. Kindermans, C. Ying and Q. V. Le.\n2017. Don\u2019t decay the learning rate, increase the batch\nsize. arXiv preprint arXiv:1711.00489.\n66\n[515] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Ra-\njbhandari, J. Casper, Z. Liu, S. Prabhumoye et al. 2022.\nUsing deepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\n[516] I. Solaiman and C. Dennison. 2021. Process for adapting\nlanguage models to society (palms) with values-targeted\ndatasets. Advances in Neural Information Processing Sys-\ntems, 34:5861\u20135873.\n[517] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta,\nW. Hamza, H. Khan, C. Peris, S. Rawls et al. 2022. Alex-\natm 20b: Few-shot learning using a large-scale multilingual\nseq2seq model. arXiv preprint arXiv:2208.01448.\n[518] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli and\nA. S. Morcos. 2022. Beyond neural scaling laws: beat-\ning power law scaling via data pruning. arXiv preprint\narXiv:2206.14486.\n[519] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb,\nA. Abid, A. Fisch, A. R. Brown, A. Santoro et al. 2022.\nBeyond the imitation game: Quantifying and extrapolat-\ning the capabilities of language models. arXiv preprint\narXiv:2206.04615.\n[520] J. Steinhardt. 2022. Future ml systems will be qualita-\ntively different. Accessed May, 20:2022.\n[521] J.\nSteinhardt.\n2023.\nEmergent\ndeception\nand\nemergent\noptimization.\nAvailable\nfrom:\nhttps://bounded-regret.ghost.io/\nemergent-deception-optimization/.\nAc-\ncessed: 29/04/2023.\n[522] M. Stern, N. Shazeer and J. Uszkoreit. 2018. Block-\nwise parallel decoding for deep autoregressive models.\nIn Proceedings of the 32nd International Conference on\nNeural Information Processing Systems, NIPS\u201918, page\n10107\u201310116, Red Hook, NY, USA. Curran Associates\nInc.\n[523] C. Stevenson, I. Smal, M. Baas, R. Grasman and\nH. van der Maas. 2022. Putting gpt-3\u2019s creativity to the\n(alternative uses) test. arXiv preprint arXiv:2206.08932.\n[524] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe,\nC. Voss, A. Radford, D. Amodei et al. 2020. Learning to\nsummarize with human feedback. In Conference on Neural\nInformation Processing Systems.\n[525] A. Stolfo, Z. Jin, K. Shridhar, B. Sch\u00f6lkopf and\nM. Sachan. 2022. A causal framework to quantify the\nrobustness of mathematical reasoning with language mod-\nels.\n[526] J. Su, Y. Lu, S. Pan, B. Wen and Y. Liu. 2021. Roformer:\nEnhanced transformer with rotary position embedding.\n[527] M. Sun, Z. Liu, A. Bair and J. Z. Kolter. 2023. A simple\nand effective pruning approach for large language models.\n[528] T. Sun, Y. Shao, H. Qian, X. Huang and X. Qiu. 2022.\nBlack-box tuning for language-model-as-a-service. In Pro-\nceedings of the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine Learning\nResearch, pages 20841\u201320855. PMLR.\n[529] X. Sun, T. Ge, F. Wei and H. Wang. 2021. Instanta-\nneous grammatical error correction with shallow aggres-\nsive decoding. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 5937\u20135947,\nOnline. Association for Computational Linguistics.\n[530] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang,\nJ. Liu, X. Chen et al. 2021. Ernie 3.0: Large-scale knowl-\nedge enhanced pre-training for language understanding and\ngeneration. arXiv preprint arXiv:2107.02137.\n[531] Z. Sun. 2023. A short survey of viewing large language\nmodels in legal aspect.\n[532] D. Sur\u00eds, S. Menon and C. Vondrick. 2023. Vipergpt:\nVisual inference via python execution for reasoning. arXiv\npreprint arXiv:2303.08128.\n[533] Susan Zhang [@suchenzang]. 2023. Piling on to the\npile-on (sorry - it\u2019s always easy to criticize), here\u2019s a rant\nabout benchmarks for LLMs that are used to back claims\nof \"stronger\" or \"better\" models. Let\u2019s start with a tour\nthrough GPT-3\u2019s Appendix G... 1/8.\n[534] M. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay,\nH. W. Chung, A. Chowdhery, Q. V. Le et al. 2022. Chal-\nlenging big-bench tasks and whether chain-of-thought can\nsolve them. arXiv preprint arXiv:2210.09261.\n[535] S. Swaminathan, A. Dedieu, R. V. Raju, M. Shanahan,\nM. Lazaro-Gredilla and D. George. 2023. Schema-learning\nand rebinding as mechanisms of in-context learning and\nemergence. ArXiv:2307.01201 [cs].\n[536] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li,\nX. Lian, J. Liu, C. Zhang et al. 2021. 1-bit adam: Com-\nmunication efficient large-scale training with adam\u2019s con-\nvergence speed. In Proceedings of the 38th International\nConference on Machine Learning, volume 139 of Proceed-\nings of Machine Learning Research, pages 10118\u201310129.\nPMLR.\n[537] L. Tang, G. Uberti and T. Shlomi. 2023.\nBaselines\nfor Identifying Watermarked Large Language Models.\nArXiv:2305.18456 [cs].\n[538] L. Tang, Z. Sun, B. Idnay, J. G. Nestor, A. Soroush, P. A.\nElias, Z. Xu, Y. Ding et al. 2023. Evaluating large language\nmodels on medical evidence summarization.\nmedRxiv,\npages 2023\u201304.\n[539] R. Tang, Y.-N. Chuang and X. Hu. 2023. The Science of\nDetecting LLM-Generated Texts. ArXiv:2303.07205 [cs].\n[540] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,\nC. Guestrin, P. Liang and T. B. Hashimoto. 2023. Alpaca:\nA strong, replicable instruction-following model.\n[541] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao and\nC. Zheng. 2021. Synthesizer: Rethinking self-attention\nfor transformer models. In International conference on\nmachine learning, pages 10183\u201310192. PMLR.\n[542] Y. Tay, M. Dehghani, S. Abnar, H. W. Chung, W. Fedus,\nJ. Rao, S. Narang, V. Q. Tran et al. 2022. Scaling laws\nvs model architectures: How does inductive bias influence\nscaling?\n67\n[543] Y. Tay, M. Dehghani, D. Bahri and D. Metzler. 2022.\nEfficient transformers: A survey. ACM Computing Surveys,\n55(6):1\u201328.\n[544] Y. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W.\nChung, S. Narang, D. Yogatama et al. 2022. Scale Effi-\nciently: Insights from Pre-training and Fine-tuning Trans-\nformers. ArXiv:2109.10686 [cs].\n[545] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei,\nX. Wang, H. W. Chung, D. Bahri et al. 2022. Ul2: Unifying\nlanguage learning paradigms.\n[546] Y. Tay, V. Q. Tran, S. Ruder, J. Gupta, H. W. Chung,\nD. Bahri, Z. Qin, S. Baumgartner et al. 2022. Charformer:\nFast character transformers via gradient-based subword\ntokenization.\n[547] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So,\nS. Shakeri, X. Garcia, H. S. Zheng et al. 2022. Transcend-\ning scaling laws with 0.1% extra compute.\n[548] R. Taylor, M. Kardas, G. Cucurull, T. Scialom,\nA. Hartshorn, E. Saravia, A. Poulton, V. Kerkez et al. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\n[549] W. L. Taylor. 1953. \u201ccloze procedure\u201d: A new tool for\nmeasuring readability. Journalism quarterly, 30(4):415\u2013\n433.\n[550] J. Thiergart, S. Huber and T. \u00dcbellacker. 2021. Under-\nstanding emails and drafting responses\u2013an approach using\ngpt-3. arXiv preprint arXiv:2102.03062.\n[551] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul-\nshreshtha, H.-T. Cheng, A. Jin, T. Bos et al. 2022. Lamda:\nLanguage models for dialog applications. arXiv preprint\narXiv:2201.08239.\n[552] R. Tian, S. Narayan, T. Sellam and A. P. Parikh. 2020.\nSticking to the Facts: Confident Decoding for Faithful\nData-to-Text Generation. ArXiv:1910.08684 [cs].\n[553] K. Tirumala, A. H. Markosyan, L. Zettlemoyer and\nA. Aghajanyan. Memorization Without Overfitting: Ana-\nlyzing the Training Dynamics of Large Language Models.\n[554] H. Q. To, N. D. Bui, J. Guo and T. N. Nguyen. 2023.\nBetter language models of code through self-improvement.\narXiv preprint arXiv:2304.01228.\n[555] A. Tornede, D. Deng, T. Eimer, J. Giovanelli, A. Mohan,\nT. Ruhkopf, S. Segel, D. Theodorakopoulos et al. 2023. Au-\ntoML in the Age of Large Language Models: Current Chal-\nlenges, Future Opportunities and Risks. ArXiv:2306.08107\n[cs].\n[556] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.\nLachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal et al. 2023.\nLLaMA: Open and Efficient Foundation Language Models.\nArXiv:2302.13971 [cs].\n[557] H. Touvron, L. Martin and K. Stone. Llama 2: Open\nFoundation and Fine-Tuned Chat Models.\n[558] C. Tran, S. Khadkikar and A. Porollo. 2023. Survey of\nprotein sequence embedding models. International Journal\nof Molecular Sciences, 24(4):3775.\n[559] A. Uchendu, T. Le, K. Shu and D. Lee. 2020. Authorship\nAttribution for Neural Text Generation. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8384\u20138395, Online.\nAssociation for Computational Linguistics.\n[560] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel,\nL. Wang, A. Creswell, G. Irving et al. 2022. Solving math\nword problems with process- and outcome-based feedback.\n[561] S. University. 2023. Holistic evaluation of langauge\nmodels results page. Available from: https://crfm.\nstanford.edu/helm/latest/?groups=1.\nAc-\ncessed: 23/03/2023.\n[562] K. Valmeekam, A. Olmo, S. Sreedharan and S. Kamb-\nhampati. 2023. Large language models still can\u2019t plan\n(a benchmark for llms on planning and reasoning about\nchange).\n[563] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. u. Kaiser and I. Polosukhin.\n2017. Attention is all you need. In Advances in Neural\nInformation Processing Systems, volume 30. Curran Asso-\nciates, Inc.\n[564] S. Vemprala, R. Bonatti, A. Bucker and A. Kapoor. 2023.\nChatgpt for robotics: Design principles and model abilities.\n[565] A. Venigalla, J. Frankle and M. Carbin. 2022. Pubmed\ngpt: A domain- specific large language model for biomed-\nical text.\nhttps://www.mosaicml.com/blog/\nintroducing-pubmed-gpt. Accessed: 2023-01-24.\n[566] R. Verkuil, O. Kabeli, Y. Du, B. I. Wicky, L. F. Milles,\nJ. Dauparas, D. Baker, S. Ovchinnikov et al. 2022. Lan-\nguage models generalize beyond natural proteins. bioRxiv,\npages 2022\u201312.\n[567] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun,\nS. Lee, D. Crandall and D. Batra. 2018. Diverse beam\nsearch for improved description of complex scenes. Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\n32(1).\n[568] P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobb-\nhahn and A. Ho. 2022. Will we run out of data? an analysis\nof the limits of scaling datasets in machine learning. arXiv\npreprint arXiv:2211.04325.\n[569] H. Viswanath and T. Zhang. 2023. Fairpy: A toolkit\nfor evaluation of social biases and their mitigation in large\nlanguage models. arXiv preprint arXiv:2302.05508.\n[570] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacra-\nmento, A. Mordvintsev, A. Zhmoginov and M. Vladymy-\nrov. 2022. Transformers learn in-context by gradient de-\nscent. arXiv preprint arXiv:2212.07677.\n[571] H. d. Vries. 2023. Go smol or go home.\n[572] T. Vu, B. Lester, N. Constant, R. Al-Rfou\u2019 and D. Cer.\n2022. SPoT: Better frozen model adaptation through soft\nprompt transfer. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 5039\u20135059, Dublin, Ireland.\nAssociation for Computational Linguistics.\n[573] J. P. Wahle, T. Ruas, T. Folt`ynek, N. Meuschke and\nB. Gipp. 2022. Identifying machine-paraphrased plagia-\nrism. In International Conference on Information, pages\n393\u2013413. Springer.\n68\n[574] J. P. Wahle, T. Ruas, F. Kirstein and B. Gipp. 2022.\nHow large language models are transforming machine-\nparaphrased plagiarism. arXiv preprint arXiv:2210.03568.\n[575] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy and\nS. Bowman. 2018. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. In\nProceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 353\u2013355, Brussels, Belgium. Association for Com-\nputational Linguistics.\n[576] B. Wang and A. Komatsuzaki. 2021.\nGPT-J-\n6B: A 6 Billion Parameter Autoregressive Language\nModel.\nhttps://github.com/kingoflolz/\nmesh-transformer-jax.\n[577] C. Wang, K. Cho and J. Gu. 2020. Neural machine\ntranslation with byte-level subwords. Proceedings of the\nAAAI Conference on Artificial Intelligence, 34(05):9154\u2013\n9160.\n[578] C. Wang, X. Liu, Z. Chen, H. Hong, J. Tang and D. Song.\n2022. DeepStruct: Pretraining of language models for\nstructure prediction. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 803\u2013823,\nDublin, Ireland. Association for Computational Linguis-\ntics.\n[579] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao,\nY. Zhu, L. Fan and A. Anandkumar. 2023. Voyager: An\nopen-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291.\n[580] H. Wang, J. Kaddour, S. Liu, J. Tang, M. Kusner,\nJ. Lasenby and Q. Liu. 2022. Evaluating self-supervised\nlearning for molecular graph embeddings. arXiv preprint\narXiv:2206.08005.\n[581] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu,\nT. Liu et al. 2023. Large Language Models are not Fair\nEvaluators. ArXiv:2305.17926 [cs].\n[582] R. Wang, H. Wang, F. Mi, Y. Chen, R. Xu and K.-\nF. Wong. 2023. Self-critique prompting with large lan-\nguage models for inductive instructions. arXiv preprint\narXiv:2305.13733.\n[583] S. Wang, Y. Liu, Y. Xu, C. Zhu and M. Zeng. 2021. Want\nto reduce labeling cost? gpt-3 can help.\n[584] S. Wang, S. Menon, T. Long, K. Henderson, D. Li,\nK. Crowston, M. Hansen, J. V. Nickerson et al. 2023. Reel-\nframer: Co-creating news reels on social media with gener-\native ai. arXiv preprint arXiv:2304.09653.\n[585] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi,\nS. Narang, A. Chowdhery and D. Zhou. 2022.\nSelf-\nconsistency improves chain of thought reasoning in lan-\nguage models.\n[586] Y. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen,\nC. Jiang, R. Xie et al. 2023. Pandalm: An automatic eval-\nuation benchmark for llm instruction tuning optimization.\narXiv preprint arXiv:2306.05087.\n[587] Y. Wang. 2021. Comment section personalization: Algo-\nrithmic, interface, and interaction design. In Proceedings\nof the EACL Hackashop on News Media Content Analysis\nand Automated Report Generation, pages 84\u201388, Online.\nAssociation for Computational Linguistics.\n[588] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith,\nD. Khashabi and H. Hajishirzi. 2022. Self-instruct: Align-\ning language model with self generated instructions.\n[589] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi,\nA. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran et al.\n2022. Super-naturalinstructions: Generalization via declar-\native instructions on 1600+ nlp tasks. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5085\u20135109.\n[590] Y. Wang, Y. Zhao and L. Petzold. 2023. Are large lan-\nguage models ready for healthcare? a comparative study\non clinical language understanding.\n[591] Z. Wang, S. Cai, A. Liu, X. Ma and Y. Liang. 2023.\nDescribe, explain, plan and select: Interactive planning\nwith large language models enables open-world multi-task\nagents. arXiv preprint arXiv:2302.01560.\n[592] Z. Wang, J. Wohlwend and T. Lei. 2019.\nStruc-\ntured pruning of large language models. arXiv preprint\narXiv:1910.04732.\n[593] Z. Wang, Z. Dai, B. P\u00f3czos and J. Carbonell. 2019. Char-\nacterizing and avoiding negative transfer. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11293\u201311302.\n[594] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, N. De Fre-\nitas et al. 2013. Bayesian optimization in high dimensions\nvia random embeddings. In IJCAI, volume 13, pages 1778\u2013\n1784.\n[595] T. Webb, K. J. Holyoak and H. Lu. 2022. Emergent\nanalogical reasoning in large language models.\n[596] A. Webson and E. Pavlick. 2022. Do prompt-based\nmodels really understand the meaning of their prompts? In\nProceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2300\u20132344, Seattle,\nUnited States. Association for Computational Linguistics.\n[597] A. Wei, N. Haghtalab and J. Steinhardt. 2023. Jailbroken:\nHow Does LLM Safety Training Fail? ArXiv:2307.02483\n[cs].\n[598] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester,\nN. Du, A. M. Dai et al. 2022. Finetuned language models\nare zero-shot learners. In International Conference on\nLearning Representations.\n[599] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph,\nS. Borgeaud, D. Yogatama, M. Bosma et al. 2022. Emer-\ngent abilities of large language models.\n[600] J. Wei, Y. Tay and Q. V. Le. 2022. Inverse scaling can\nbecome u-shaped. arXiv preprint arXiv:2211.02011.\n[601] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter,\nF. Xia, E. H. Chi, Q. V. Le et al. 2022. Chain of thought\nprompting elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\n[602] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato,\nP.-S. Huang, M. Cheng, M. Glaese et al. 2021. Ethical and\nsocial risks of harm from language models. arXiv preprint\narXiv:2112.04359.\n[603] M. Weiss. 2019. Deepfake bot submissions to federal\npublic comment websites cannot be distinguished from\nhuman submissions. Technology Science, 2019121801.\n69\n[604] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho and\nJ. Weston. 2019. Neural text generation with unlikelihood\ntraining. arXiv preprint arXiv:1908.04319.\n[605] L. Weng. 2023. Large transformer model inference opti-\nmization. Lil\u2019Log.\n[606] L. Weng. 2023.\nPrompt engineering.\nlilian-\nweng.github.io.\n[607] M. Willig, M. ZE \u02c7CEVI \u00b4C, D. S. Dhami and K. Kersting.\n2023. Causal parrots: Large language models may talk\ncausality but are not causal. preprint.\n[608] F. Winkelmolen, N. Ivkin, H. F. Bozkurt and Z. Karnin.\n2020. Practical and sample efficient zero-shot hpo. arXiv\npreprint arXiv:2007.13382.\n[609] Y. Wolf, N. Wies, Y. Levine and A. Shashua. 2023. Fun-\ndamental limitations of alignment in large language models.\narXiv preprint arXiv:2304.11082.\n[610] M. Wornow, Y. Xu, R. Thapa, B. Patel, E. Steinberg,\nS. Fleming, M. A. Pfeffer, J. Fries et al. 2023. The shaky\nfoundations of clinical foundation models: A survey of\nlarge language models and foundation models for emrs.\n[611] F. Wu, D. Radev and J. Xu. 2023. When geometric\ndeep learning meets pretrained protein language models.\nbioRxiv, pages 2023\u201301.\n[612] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe,\nJ. Leike and P. Christiano. 2021.\nRecursively sum-\nmarizing books with human feedback.\narXiv preprint\narXiv:2109.10862.\n[613] J. Wu, F. Wu, B. Jiang, W. Liu and P. Zhao. 2022. tfold-\nab: Fast and accurate antibody structure prediction without\nsequence homologs. bioRxiv, pages 2022\u201311.\n[614] P. Y. Wu, J. A. Tucker, J. Nagler and S. Messing. 2023.\nLarge language models can be used to estimate the ideolo-\ngies of politicians in a zero-shot learning setting.\n[615] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li,\nH. Zhu et al. 2021. Yuan 1.0: Large-scale pre-trained\nlanguage model in zero-shot and few-shot learning.\n[616] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze,\nS. Gehrmann, P. Kambadur, D. Rosenberg et al. 2023.\nBloomberggpt: A large language model for finance.\n[617] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,\nW. Macherey, M. Krikun, Y. Cao et al. 2016. Google\u2019s neu-\nral machine translation system: Bridging the gap between\nhuman and machine translation.\n[618] Y. Wu, M. Gardner, P. Stenetorp and P. Dasigi. 2022.\nGenerating data to mitigate spurious correlations in\nnatural language inference datasets.\narXiv preprint\narXiv:2203.12942.\n[619] Z. Wu, L. Qiu, A. Ross, E. Aky\u00fcrek, B. Chen, B. Wang,\nN. Kim, J. Andreas et al. 2023. Reasoning or Reciting?\nExploring the Capabilities and Limitations of Language\nModels Through Counterfactual Tasks. ArXiv:2307.02477\n[cs].\n[620] Y. Xiao and W. Y. Wang. 2021. On Hallucination and\nPredictive Uncertainty in Conditional Language Genera-\ntion. ArXiv:2103.15025 [cs].\n[621] Q. Xie, Z. Luo, B. Wang and S. Ananiadou. 2023. A\nsurvey on biomedical text summarization with pre-trained\nlanguage model.\n[622] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu,\nP. Liang, Q. V. Le et al. 2023.\nDoReMi: Optimizing\nData Mixtures Speeds Up Language Model Pretraining.\nArXiv:2305.10429 [cs].\n[623] S. M. Xie, A. Raghunathan, P. Liang and T. Ma. 2022.\nAn Explanation of In-context Learning as Implicit Bayesian\nInference. ArXiv:2111.02080 [cs].\n[624] S. M. Xie, S. Santurkar, T. Ma and P. Liang. 2023. Data\nSelection for Language Models via Importance Resam-\npling. ArXiv:2302.03169 [cs].\n[625] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng,\nC. Tao and D. Jiang. 2023. Wizardlm: Empowering large\nlanguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\n[626] F. F. Xu, U. Alon, G. Neubig and V. J. Hellendoorn. 2022.\nA systematic evaluation of large language models of code.\n[627] M. Xu, X. Yuan, S. Miret and J. Tang. 2023. Protst:\nMulti-modality learning of protein sequences and biomedi-\ncal texts. arXiv preprint arXiv:2301.12040.\n[628] Y. Xu, H. Lee, D. Chen, B. Hechtman, Y. Huang,\nR. Joshi, M. Krikun, D. Lepikhin et al. 2021. Gspmd:\ngeneral and scalable parallelization for ml computation\ngraphs. arXiv preprint arXiv:2105.04663.\n[629] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang,\nM. Kale, A. Roberts and C. Raffel. 2022. ByT5: Towards\na token-free future with pre-trained byte-to-byte models.\nArXiv:2105.13626 [cs].\n[630] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang,\nM. Kale, A. Roberts and C. Raffel. 2022. ByT5: Towards\na token-free future with pre-trained byte-to-byte models.\nTransactions of the Association for Computational Linguis-\ntics, 10:291\u2013306.\n[631] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou,\nA. Siddhant, A. Barua and C. Raffel. 2021. mT5: A mas-\nsively multilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498, Online.\nAssociation for Computational Linguistics.\n[632] L. Yan, L. Sha, L. Zhao, Y. Li, R. Martinez-Maldonado,\nG. Chen, X. Li, Y. Jin et al. 2023. Practical and ethical chal-\nlenges of large language models in education: A systematic\nliterature review.\n[633] G. Yang, E. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi,\nN. Ryder, J. Pachocki et al. 2021. Tuning large neural net-\nworks via zero-shot hyperparameter transfer. Advances in\nNeural Information Processing Systems, 34:17084\u201317097.\n[634] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang,\nB. Yin and X. Hu. 2023. Harnessing the power of llms in\npractice: A survey on chatgpt and beyond.\n[635] K. Yang and D. Klein. 2021. Fudge: Controlled text\ngeneration with future discriminators.\narXiv preprint\narXiv:2104.05218.\n70\n[636] K. Yang, D. Klein, N. Peng and Y. Tian. 2022. Doc: Im-\nproving long story coherence with detailed outline control.\narXiv preprint arXiv:2212.10077.\n[637] K. Yang, N. Peng, Y. Tian and D. Klein. 2022. Re3:\nGenerating longer stories with recursive reprompting and\nrevision. arXiv preprint arXiv:2210.06774.\n[638] X. Yang, K. Chen, W. Zhang, C. Liu, Y. Qi, J. Zhang,\nH. Fang and N. Yu. 2023. Watermarking Text Generated\nby Black-Box Language Models. ArXiv:2305.08883 [cs].\n[639] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths,\nY. Cao and K. Narasimhan. 2023. Tree of Thoughts: De-\nliberate Problem Solving with Large Language Models.\nArXiv:2305.10601 [cs].\n[640] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan\nand Y. Cao. 2022. React: Synergizing reasoning and acting\nin language models. arXiv preprint arXiv:2210.03629.\n[641] X. Yao, Y. Zheng, X. Yang and Z. Yang. 2022. NLP\nFrom Scratch Without Large-Scale Pretraining: A Simple\nand Efficient Framework. In Proceedings of the 39th Inter-\nnational Conference on Machine Learning, pages 25438\u2013\n25451. PMLR. ISSN: 2640-3498.\n[642] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng,\nH. Chen and N. Zhang. 2023.\nEditing Large Lan-\nguage Models: Problems, Methods, and Opportunities.\nArXiv:2305.13172 [cs].\n[643] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li and\nY. He. 2022. Zeroquant: Efficient and affordable post-\ntraining quantization for large-scale transformers. arXiv\npreprint arXiv:2206.01861.\n[644] M. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Man-\nning, P. Liang and J. Leskovec. 2022. Deep bidirectional\nlanguage-knowledge graph pretraining.\narXiv preprint\narXiv:2210.09338.\n[645] S. Yi, R. Goel, C. Khatri, A. Cervone, T. Chung, B. He-\ndayatnia, A. Venkatesh, R. Gabriel et al. 2019. Towards\ncoherent and engaging spoken dialog response generation\nusing automatic conversation evaluators. In Proceedings\nof the 12th International Conference on Natural Language\nGeneration, pages 65\u201375, Tokyo, Japan. Association for\nComputational Linguistics.\n[646] D. Yogatama, C. de Masson d\u2019Autume and L. Kong.\n2021. Adaptive semiparametric language models. Trans-\nactions of the Association for Computational Linguistics,\n9:362\u2013373.\n[647] T. Yoneda, J. Fang, P. Li, H. Zhang, T. Jiang, S. Lin,\nB. Picker, D. Yunis et al. 2023. Statler: State-maintaining\nlanguage models for embodied reasoning.\n[648] K. M. Yoo, D. Park, J. Kang, S.-W. Lee and W. Park.\n2021. GPT3Mix: Leveraging large-scale language models\nfor text augmentation. In Findings of the Association for\nComputational Linguistics: EMNLP 2021, pages 2225\u2013\n2239, Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\n[649] K. Yoo, W. Ahn, J. Jang and N. Kwak. 2023. Robust Nat-\nural Language Watermarking through Invariant Features.\nArXiv:2305.01904 [cs].\n[650] R. You, Y. Liu, H. Mamitsuka and S. Zhu. 2021.\nBertmesh: deep contextual representation learning for\nlarge-scale high-performance mesh indexing with full text.\nBioinformatics, 37(5):684\u2013692.\n[651] F. Yu, L. Quartey and F. Schilder. 2022. Legal prompting:\nTeaching a language model to think like a lawyer. arXiv\npreprint arXiv:2212.01326.\n[652] L. Yu, D. Simig, C. Flaherty, A. Aghajanyan, L. Zettle-\nmoyer and M. Lewis. 2023.\nMegabyte:\nPredicting\nmillion-byte sequences with multiscale transformers. arXiv\npreprint arXiv:2305.07185.\n[653] P. Yu, M. Artetxe, M. Ott, S. Shleifer, H. Gong, V. Stoy-\nanov and X. Li. 2022. Efficient language modeling with\nsparse all-mlp.\n[654] P. Yu, T. Wang, O. Golovneva, B. Alkhamissy, G. Ghosh,\nM. Diab and A. Celikyilmaz. 2022.\nAlert: Adapting\nlanguage models to reasoning tasks.\narXiv preprint\narXiv:2212.08286.\n[655] L. Yunxiang, L. Zihan, Z. Kai, D. Ruilong and Z. You.\n2023. Chatdoctor: A medical chat model fine-tuned on\nllama model using medical domain knowledge.\n[656] E. Zelikman, Y. Wu, J. Mu and N. Goodman. 2022. STar:\nBootstrapping reasoning with reasoning. In Advances in\nNeural Information Processing Systems.\n[657] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi,\nF. Roesner and Y. Choi. 2019. Defending against neural\nfake news. Advances in neural information processing\nsystems, 32.\n[658] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding,\nZ. Yang, Y. Xu et al. 2022. Glm-130b: An open bilingual\npre-trained model.\n[659] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang,\nX. Jiang, Z. Yang et al. 2021. Pangu-\u03b1: Large-scale au-\ntoregressive pretrained chinese language models with auto-\nparallel computation.\n[660] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.-\nG. Lou and W. Chen. 2023. Repocoder: Repository-level\ncode completion through iterative retrieval and generation.\n[661] H. Zhang, L. H. Li, T. Meng, K.-W. Chang and G. V. d.\nBroeck. 2022. On the Paradox of Learning to Reason from\nData. ArXiv:2205.11502 [cs].\n[662] H. Zhang, D. Duckworth, D. Ippolito and A. Neelakan-\ntan. 2021. Trading off diversity and quality in natural\nlanguage generation. In Proceedings of the Workshop on\nHuman Evaluation of NLP Systems (HumEval), pages 25\u2013\n33, Online. Association for Computational Linguistics.\n[663] M. Zhang and Y. He. 2020. Accelerating training of\ntransformer-based language models with progressive layer\ndropping.\n[664] M. Zhang, O. Press, W. Merrill, A. Liu and N. A. Smith.\n2023. How Language Model Hallucinations Can Snowball.\nArXiv:2305.13534 [cs].\n[665] S. Zhang. 2023.\n[...] that\u2019s an unhelpful order of\nmagnitude difference in how large of a model you\nshould be training in order to be considered \u201ccompute\noptimal\u201d.\nhttps://twitter.com/suchenzang/\nstatus/1616752494608007171?s=20. Accessed:\n2023-06-06.\n71\n[666] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen,\nS. Chen, C. Dewan, M. Diab et al. 2022. Opt: Open pre-\ntrained transformer language models.\n[667] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger and\nY. Artzi. 2019. Bertscore: Evaluating text generation with\nbert. arXiv preprint arXiv:1904.09675.\n[668] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown\nand T. B. Hashimoto. 2023. Benchmarking large language\nmodels for news summarization.\n[669] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun,\nY. Yao, F. Qi et al. 2021. Cpm-2: Large-scale cost-effective\npre-trained language models.\n[670] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun and J. Zhou. 2022.\nMoefication: Transformer feed-forward layers are mixtures\nof experts.\n[671] Z. Zhang, A. Zhang, M. Li and A. Smola. 2022. Auto-\nmatic chain of thought prompting in large language models.\n[672] S. Zhao, J. Wen, L. A. Tuan, J. Zhao and J. Fu.\n2023. Prompt as triggers for backdoor attack: Examin-\ning the vulnerability in language models. arXiv preprint\narXiv:2305.01219.\n[673] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou,\nY. Min, B. Zhang et al. 2023. A Survey of Large Language\nModels. ArXiv:2303.18223 [cs].\n[674] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu,\nL. Wright, H. Shojanazeri et al. 2023. Pytorch fsdp: experi-\nences on scaling fully sharded data parallel. arXiv preprint\narXiv:2304.11277.\n[675] Z. Zhao, E. Wallace, S. Feng, D. Klein and S. Singh.\n2021. Calibrate before use: Improving few-shot perfor-\nmance of language models. In Proceedings of the 38th\nInternational Conference on Machine Learning, volume\n139 of Proceedings of Machine Learning Research, pages\n12697\u201312706. PMLR.\n[676] B. Zheng, L. Dong, S. Huang, S. Singhal, W. Che, T. Liu,\nX. Song and F. Wei. 2021. Allocating large vocabulary ca-\npacity for cross-lingual language model pre-training. arXiv\npreprint arXiv:2109.07306.\n[677] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen,\nY. Huang, Y. Wang, Y. Xu et al. 2022. Alpa: Automat-\ning inter- and Intra-Operator parallelism for distributed\ndeep learning. In 16th USENIX Symposium on Operat-\ning Systems Design and Implementation (OSDI 22), pages\n559\u2013578, Carlsbad, CA. USENIX Association.\n[678] R. Zheng, S. Dou, S. Gao, W. Shen, B. Wang, Y. Liu,\nS. Jin, Q. Liu et al. 2023.\nSecrets of RLHF in Large\nLanguage Models Part I: PPO. ArXiv:2307.04964 [cs].\n[679] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang,\nA. Saied, W. Chen et al. 2023.\nAgieval: A human-\ncentric benchmark for evaluating foundation models. arXiv\npreprint arXiv:2304.06364.\n[680] A. Zhou, Y. Ma, J. Zhu, J. Liu, Z. Zhang, K. Yuan,\nW. Sun and H. Li. 2021.\nLearning N: M fine-grained\nstructured sparse neural networks from scratch. In 9th In-\nternational Conference on Learning Representations, ICLR\n2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-\nview.net.\n[681] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma,\nA. Efrat et al. 2023. LIMA: Less Is More for Alignment.\nArXiv:2305.11206 [cs].\n[682] D. Zhou, N. Sch\u00e4rli, L. Hou, J. Wei, N. Scales, X. Wang,\nD. Schuurmans, C. Cui et al. 2022. Least-to-most prompt-\ning enables complex reasoning in large language models.\n[683] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis,\nH. Chan and J. Ba. 2023.\nLarge language models are\nhuman-level prompt engineers. In International Confer-\nence on Learning Representations.\n[684] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urta-\nsun, A. Torralba and S. Fidler. 2015. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books.\n[685] B. Zhuang, J. Liu, Z. Pan, H. He, Y. Weng and C. Shen.\n2023. A survey on efficient training of transformers. arXiv\npreprint arXiv:2302.01107.\n[686] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Rad-\nford, D. Amodei, P. Christiano and G. Irving. 2019. Fine-\ntuning language models from human preferences. arXiv\npreprint arXiv:1909.08593.\n[687] B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean,\nN. Shazeer and W. Fedus. 2022. St-moe: Designing stable\nand transferable sparse expert models.\n[688] M. Zvyagin, A. Brace, K. Hippe, Y. Deng, B. Zhang,\nC. O. Bohorquez, A. Clyde, B. Kale et al. 2022. Genslms:\nGenome-scale language models reveal sars-cov-2 evolu-\ntionary dynamics. bioRxiv, pages 2022\u201310.\n72\n"
  },
  {
    "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback",
    "link": "https://arxiv.org/pdf/2307.10159.pdf",
    "upvote": "29",
    "text": "FABRIC: PERSONALIZING DIFFUSION MODELS WITH\nITERATIVE FEEDBACK\nDimitri von R\u00a8utte1\u2217\nElisabetta Fedele1\u2217\nJonathan Thomm1\u2217\nLukas Wolf1\u2217\n1ETH Zurich, Zurich, Switzerland\n{dvruette, efedele, jthomm, wolflu}@ethz.ch\nOutput for user 1\nOutput for user 2\nuser 1\nuser 2\nFigure 1: Illustration of the proposed approach. FABRIC generates images based not only on text\nprompt, but also on user preferences expressed during multiple rounds of generation.\nABSTRACT\nIn an era where visual content generation is increasingly driven by machine learning,\nthe integration of human feedback into generative models presents significant\nopportunities for enhancing user experience and output quality. This study explores\nstrategies for incorporating iterative human feedback into the generative process\nof diffusion-based text-to-image models. We propose FABRIC, a training-free\napproach applicable to a wide range of popular diffusion models, which exploits\nthe self-attention layer present in the most widely used architectures to condition\nthe diffusion process on a set of feedback images. To ensure a rigorous assessment\nof our approach, we introduce a comprehensive evaluation methodology, offering\na robust mechanism to quantify the performance of generative visual models that\nintegrate human feedback. We show that generation results improve over multiple\nrounds of iterative feedback through exhaustive analysis, implicitly optimizing\narbitrary user preferences. The potential applications of these findings extend to\nfields such as personalized content creation and customization.\n1\nINTRODUCTION\nThe field of artificial intelligence (AI) has witnessed a surge in interest in generative visual models,\nprimarily due to their transformative potential across a myriad of applications, encompassing content\ncreation, customization, data augmentation, and virtual reality. These models leverage advanced deep\nlearning methodologies, such as GAN (Goodfellow et al., 2014) and VAE (Kingma & Welling, 2022),\nto generate high-fidelity and visually compelling images from given inputs or descriptions Brock\net al. (2019); Razavi et al. (2019). The significant advancements in generative visual models have\ncatalyzed the exploration of novel possibilities in the realms of computer vision, natural language\nprocessing, and human-computer interaction Radford et al. (2016).\nDiffusion models, in particular, have emerged as a powerful tool in the field of image synthesis, often\ndelivering results that are comparable to, or even exceed, those produced by GANs and VAEs Ho\net al. (2020); Rombach et al. (2022). These models are characterized by their ability to generate a\ndiverse array of visually coherent images, while demonstrating superior stability and reduced mode\ncollapse during the training phase Song et al. (2021). This has led to their widespread adoption\n\u2217Equal contribution\n1\narXiv:2307.10159v1  [cs.CV]  19 Jul 2023\nU-Net Layer (\n)\nResNet Block\nSelf-Attention\nCross-Attention\nFFN\nFeedback Images\nPrompt(s)\nphoto of a dog running\non grassland\nFeedback Source\nFigure 2: Illustration of the proposed approach. FABRIC improves generated results by incorporating\nuser feedback through an attention-based conditioning mechanism.\namong researchers investigating the frontiers of generative visual modeling. Moreover, the utility of\ndiffusion models extends beyond image synthesis, finding applications in various other domains such\nas inpainting, super-resolution, and style transfer Chan et al. (2020); Park et al. (2019).\nText conditioning serves as a crucial component of generative visual models, enabling them to\nsynthesize images based on human-readable descriptions Reed et al. (2016). However, despite the\nrobust capabilities of diffusion models in generating a wide spectrum of images, steering the model\ntowards a specific desired output can pose challenges Luccioni et al. (2023). Users often embark on\nan iterative process of prompt refinement to achieve their intended results, and articulating personal\npreferences in the form of text can be a complex task. Nevertheless, users possess the ability to\nreadily evaluate the quality of the generated images. This opens up the possibility of integrating\nsparse human feedback into the generative process, which can be harnessed to enhance the results\nand better align them with user preferences. Given the stability and controllability of the generative\nprocess offered by diffusion models, they present an ideal platform for the incorporation of human\nfeedback to refine the text-to-image generation process.\nIn this work, we focus on this iterative workflow and propose a technique based on sparse feedback\nthat aims to aid in steering the generative process towards desirable and away from undesirable\noutcomes. This is achieved by using positive and negative feedback images (e.g. gathered on previous\ngenerations) to manipulate future results through reference image-conditioning. Simply repeating the\nsetup allows for iterative refinement of the generated images based on an arbitrary feedback source\n(including human feedback). Our contributions are three-fold:\n\u2022 We introduce FABRIC (Feedback via Attention-Based Reference Image Conditioning) 1, a\nnovel approach that enables the integration of iterative feedback into the generative process\nwithout requiring explicit training that can be combined with many other extensions to\nStable Diffusion.\n\u2022 We propose two experimental settings that facilitate the automatic evaluation of generative\nvisual models over multiple rounds.\n\u2022 Using these proposed settings, we evaluate FABRIC and demonstrate its superiority over\nbaseline methods.\n2\nRELATED WORK\n2.1\nTEXTUAL INVERSION AND STYLE TRANSFER\nA popular method for personalizing text-to-image diffusion models is textual inversion (Gal et al.,\n2023; Ruiz et al., 2023), a technique for learning semantic text embeddings from images depicting\na common subject or style. This enables the synthesis of photorealistic images in various scenes\nand conditions while preserving some set of desirable features, but requires multiple images that\nincorporate those features as well as additional training to learn the semantic embedding.\n1The code is publicly available: https://github.com/sd-fabric/fabric.git\n2\nMokady et al. (2022) introduce an accurate inversion technique for text-guided diffusion models,\nenabling text-based real-image editing capabilities. The proposed approach consists of two novel\ncomponents: pivotal inversion for diffusion models and null-text optimization, allowing for high-\nfidelity editing of real images without tuning the model\u2019s weights. StyleDrop (Sohn et al. (2023))is a\nnovel method developed by Google Research for synthesizing images that adhere to a specific style\nusing a text-to-image model. The method captures intricate details of a user-provided style, including\ncolor schemes, shading, design patterns, and local and global effects, and enhances the quality\nthrough iterative training with either human or automated feedback, outperforming other methods\nfor style-tuning text-to-image models. Xu et al. (2023) introduces a novel approach to incorporate\nuser-provided reference images instead of text prompts. This approach, called Prompt-Free Diffusion,\nleverages a Semantic Context Encoder (SeeCoder) to transform these inputs into meaningful visual\nembeddings, which are then used as conditional inputs for a Text-to-Image (T2I) model, generating\nhigh-quality, customized outputs.\n2.2\nHUMAN PREFERENCE MODELLING\nRecently, modeling human preferences in generative models experienced increased attention. In the\ndomain of diffusion models, Wu et al. (2023) provide a novel dataset containing almost 100 thousand\nimages collected from over 25 thousand prompts. A sample from the dataset contains between two\nand four images and a particular user\u2019s choice when being offered this batch of images. The authors\nfine-tune a CLIP (Radford et al. (2021)) model in order to classify chosen images against non-chosen\nones. The Human Preference Score (HPS) is derived from this classifier. In addition to that, the\nauthors fine-tune a low-rank adaptation (LoRA, Hu et al. (2021)) of stable diffusion (Rombach et al.\n(2022)) that significantly improves the image quality.\nKirstain et al. (2023) introduces Pick-a-Pic, a large-scale dataset of user preferences for text-to-image\ngeneration collected from real users of their web interface. The web app presents users with two\ngenerated images conditioned on their prompt and asks them to select their preferred option or\nindicate a tie if they have no strong preference. The rejected (non-preferred) image is then replaced\nwith a newly generated image, and the process repeats. The dataset, containing 14,000 preferences, is\nused to train a scoring function called PickScore, which outperforms other available scoring functions\nin predicting user preferences, even surpassing expert human annotators. The authors advocate for\nthe use of Pick-a-Pic prompts in evaluating text-to-image models, highlighting the dataset\u2019s potential\nfor various applications such as model evaluation, image quality improvement, and text-to-image\nmodel enhancement.\nFan et al. (2023) proposes an approach called DPOK for fine-tuning text-to-image diffusion models\nusing online reinforcement learning (RL). DPOK integrates policy optimization with KL regular-\nization and updates pre-trained models using policy gradient to maximize feedback-trained reward.\nThe authors demonstrate that DPOK generally outperforms supervised fine-tuning in terms of both\nimage-text alignment and image quality.\n2.3\nITERATIVE FEEDBACK\nIn the generative text-to-image workflow, the user typically thinks of a prompt, generates images\nwith that prompt, inspects the results, makes adjustments to the prompt, and repeats this process until\nthey\u2019re happy with the result. In order to remove this burden from the user, Tang et al. (2023) propose\na novel zeroth-order optimization algorithm with theoretical guarantees for problems with black\nbox objective functions evaluated through ranking oracles, such as Reinforcement Learning with\nHuman Feedback (RLHF). The algorithm\u2019s effectiveness is demonstrated in improving image quality\ngenerated by a diffusion generative model using human ranking feedback, offering a promising\nalternative to existing RLHF methods.\nTo summarize, the literature of the field has predominantly focused on concept learning, style transfer\nfrom reference images, and modeling human preferences to fine-tune models accordingly. However,\nthe iterative incorporation of human feedback into the model training process, a potentially significant\naspect for enhancing model performance and alignment with human preference, remains relatively\nunexplored.\n3\n3\nFABRIC\nWe will now introduce our proposed method, FABRIC, which tackles the problem of incorporating\nmultiple rounds of positive and negative human feedback into the generative process.\nAttention-based Reference Image Conditioning\nFABRIC takes inspiration from a technique\nimplemented in an update to the widely used ControlNet repository by Zhang (2023) that introduces\nthe ability to generate synthetic images similar to some reference. This method exploits the self-\nattention module in the U-Net. The intuition is that the self-attention module\u2019s weights have learned\nto \u201dpay attention\u201d to other pixels in the image; therefore, adding additional keys and values from a\nreference image offers a way to inject additional information. In order to be compatible with the\ngenerated image at a specific time step of the denoising process, the reference latent image is partially\nnoised up to the current timestep t (using the standard random forward noising process:\nzref \u2190 \u221a\u00af\u03b1t \u00b7 xref +\n\u221a\n1 \u2212 \u00af\u03b1t \u00b7 N(0, I))\n(1)\nHowever, how to compute keys and values for a reference image in the U-Net layers is not immediately\nclear since simply concatenating the images does not work. Therefore, the keys and values for\ninjection (or equivalently the hidden states right before, see Algorithm 0) are computed by passing\nthe noised reference image through the U-Net of Stable Diffusion. All keys and values are stored in\nthe self-attention layers of the U-Net.\nThen, for a particular U-Net denoising step of the user prompt and the current partially denoised\nlatent image zt+1, the stored keys and values are appended in the self-attention in the respective\nU-Net layers. This way, the denoising process can attend to the reference image and include the\nsemantic information. By reweighting the attention scores (see Equation 2), we additionally obtain\ncontrol over the strength of the reference influence.\nNote that precomputing the hidden states of reference images requires an additional U-Net forward\npass, roughly doubling the inference time. Further, concatenating additional keys and values in the\nself-attention layer also increases inference time and results in scaling of the required memory that\nis quadratic in the number of feedback images (or linear if a memory-efficient implementation of\nattention is used) (Dao et al., 2022; Rabe & Staats, 2022).\nIncorporating Feedback in the Generation Process\nThe method outlined in the previous section\nincorporated a reference image into the generative process. We will now provide an extension to\nincorporate multi-round positive and negative feedback: Given a set of liked and disliked images, we\ndo a separate U-Net pass for every liked and disliked image. The keys and values of the liked and\ndisliked images are concatenated to the conditional and unconditional U-Net pass respectively (see\nclassifier-free guidance, Ho & Salimans (2022)). Further, we reweight the attention scores with a\nfactor depending on whether it is the conditional or the unconditional pass and the current time step\nin the denoising process.\nUnlike the ControlNet (Zhang, 2023) approach, we also explore this linear interpolation which\nallows us to emphasize coarse features (having a large reference attention weight for t \u2265 \u03b1T) or fine\ndetails from the reference (i.e., larger attention weight when t \u2264 \u03b1T). The feedback process can be\nscheduled according to the denoising steps. This allows for only including feedback in some of the\ndenoising steps. We find that the best results can be achieved when incorporating the feedback in the\nfirst half of the denoising process. Additional experiments with adapted feedback schedules can be\nfound in Section 5.3.\nExtending to Iterative Feedback\nNow that we can incorporate both positive and negative feedback\nreference images, we adapt the algorithm for multiple rounds. Initially, we generate images with no\nfeedback images, resulting in a vanilla Stable Diffusion generation. From those images, we append\nany set of liked and disliked images to the positive and negative feedback respectively. How this\nfeedback is obtained is arbitrary, although we envision either a human or an automated process being\nthe feedback source. Then, we generate a new batch of images using the given feedback. We repeat\nthis process every round, extending the sets of liked and disliked images and refining the next batch\naccording to them.\n4\n4\nEVALUATION\n4.1\nFABRIC MODELS\nWe conduct an evaluation of two versions of FABRIC in our study. The first version, referred to as\nFABRIC, utilizes the methodology outlined in Section 3, built upon a fine-tuned Stable Diffusion\n1.5 checkpoint (dreamlike-photoreal-2.0).The second version, named FABRIC+HPS LoRA, further\nenhances the FABRIC approach by applying it on top of the Low-Rank Adaptation (LoRA) of Stable\nDiffusion 1.5 based on Human Preference Score proposed by Wu et al. (2023). We opted to include\nthe FABRIC+HPS LoRA version in our evaluation due to its proven ability to generate images that\nclosely match human preferences.\n4.2\nBASELINES\nSince, to the best of our knowledge, there doesn\u2019t exist a method designed to incorporate iterative\nfeedback gathered over multiple rounds, we compare the proposed method to standard Stable\nDiffusion models in the following manner. First, we run Stable Diffusion 1.5 (using the base model2\nor a fine-tuned version called Dreamlike Photoreal3 with or without HPS LoRA checkpoints) N\ntimes, each with a different seed, generating N batches of images. Then, we collect the desired\nevaluation metrics for the generated batch at each round and we use this values to perform quantitative\ncomparisons with FABRIC.\nIt is important to note that while we may add images from each round as feedback to future rounds for\nour method, the baselines do not have a mechanism to incorporate feedback into future generations.\nTherefore, the baseline models generate images independently in each round without taking previous\nrounds into consideration. An overview of the all models taken into account can be found in Table 1.\nName\nSD version\nCheckpoint\nLoRA\nFABRIC\nBaseline\n1.5\nstable-diffusion-v1.5\n\u2717\n\u2717\nDreamlike Photoreal\n1.5\ndreamlike-photoreal-2.0\n\u2717\n\u2717\nHPS LoRA\n1.5\ndreamlike-photoreal-2.0\nHPS LoRA\n\u2717\nFABRIC (SD)\n1.5\nstable-diffusion-v1.5\n\u2717\n\u2713\nFABRIC\n1.5\ndreamlike-photoreal-2.0\n\u2717\n\u2713\nFABRIC + HPS LoRA\n1.5\ndreamlike-photoreal-2.0\nHPS LoRA\n\u2713\nTable 1: Specifics of FABRIC models and baselines used during evaluation\n4.3\nMETRICS\n4.3.1\nPREFERENCE MODEL\nIn order to automatically evaluate the experiments we use the PickScore introduced in Section 2 as a\nproxy score for general human preference.\n4.3.2\nCLIP SIMILARITY TO FEEDBACK IMAGES\nTo assess the effectiveness of incorporating feedback into the generation process, we compute the\nCLIP similarity between the generated and the previous positive and negative feedback images.\nFor a generated image x we compute the average CLIP-similarity to feedback images y(1)\npos, . . . , y(k)\npos or\ny(1)\nneg, . . . , y(k)\nneg. Specifically, let CLIP(x, y) denote the cosine similarity between CLIP-embeddings\nof x and y. Then the positive CLIP similarity is defined as follows:\nspos(x) = 1\nk\nk\nX\ni=1\nCLIP(x, y(i)\npos)\n2https://huggingface.co/runwayml/stable-diffusion-v1-5\n3https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0\n5\nThe negative similarity sneg(x) is defined analogously.\n4.3.3\nIN-BATCH IMAGE DIVERSITY\nIn the process of steering the generation of images toward user preferences it is crucial to balance\nexploration (offering diverse image options for user selection) against exploitation (generating images\naligned with previous feedback). To quantify the trade-off between these two factors across different\nrounds, we introduce a metric called In-batch Image Diversity.\nFor a batch of images x1, . . . , xn, we define the in-batch diversity based on the average CLIP-\nsimilarity between images in the batch. Specifically, with CLIP similarity defined as above, the\nIn-batch Image Diversity d is defined as follows:\nd(x1, . . . , xn) = 1 \u2212\n2\nn(n \u2212 1)\nn\nX\ni=2\ni\u22121\nX\nj=1\nCLIP(xi, xj)\nwhere n(n\u22121)\n2\nis the number of elements in the upper triangular cosine similarity matrix.\n5\nEXPERIMENTS\nIn order to evaluate the capabilities of our model, we designed two experimental settings, each\nemploying a different criterion for selecting feedback images during rounds.\nIn Sec. 5.1 we present a Preference Model-Based approach, where we select liked and disliked images\nusing a preference score. In Sec. 5.2 we present a Target Image-Based approach, where feedback\nselection relies on the similarity with a target image provided at the start of the experiment (but not\nused as feedback directly).\nBoth the experiments use a batch size of 4 and consist on 3 rounds of generation. After each round\none image is selected as liked and another one as disliked. To demonstrate robustness with respect to\nthe feedback strength, the preference model-based setup uses a feedback strength of w = 0.1 while\nthe target image-based setup uses w = 0.8.\n5.1\nPREFERENCE MODEL-BASED FEEDBACK SELECTION\nThe first experiment aimed to evaluate FABRIC by assessing how humans generally perceive gener-\nated images through an universal preference score. Therefore, this experiment operates under the\nassumption that all humans have the same preference when providing feedback, which in general\nmay not always hold and is challenged in the second experiment.\nIn practice, the experiment is conducted as follows. First, a random set of 1000 prompts is sampled\nfrom the HPS dataset (Wu et al., 2023). Next, for each prompt, after having initialized the user\u2019s\nliked and disliked images as empty sets, we simulate a 3-round interaction between the model and a\nuser. In each round, the following steps are performed. First, FABRIC is run with the prompt and\nfeedback images as input, generating a batch of 4 images. Then, the Human Preference Score of\neach generated image is computed. Using those scores as a proxy for human feedback, the generated\nimage with the highest score is added to the set of liked images, and the one with the lowest score is\nadded to the disliked. For each batch of generated images we measure the average PickScore as well\nas the average CLIP similarity to both positive and negatives images. We compare each FABRIC\nmodel (dreamlike-photoreal-2.0 and HPS LoRA) against their respective baselines in terms of these\ntwo scores.\nThe results from our experiments are depicted in Figure 3. In Figure 3a we address the concern that\nsimply running multiple rounds and achieving higher image variety may lead to an increase in the\nmaximum PickScore. Indeed we show that, even if in general this value increases over rounds, our\nmodel outperforms both the baselines. In Figure 3b we observe that starting from the second round\nour model outperforms the baselines not only in terms of maximum PickScore but also in terms\naverage and minimum value, indicating an overall enhancement in the quality of generated images.\nIn Figure 3c we evaluate the similarity of generated images to positive and negative feedback. We\nobserve that even after just one round, the CLIP similarity score is higher for the positive samples, and\n6\n1\n2\n3\nRound\n20.2\n20.4\n20.6\n20.8\n21.0\n21.2\nPickScore\nSD 1.5\nFABRIC (SD)\nDreamlike Photoreal\nFABRIC\nHPS LoRA\nHPS LoRA + FABRIC\n(a) Highest PickScore of a generated\nimage over all previous rounds.\n1\n2\n3\nRound\n19.6\n19.8\n20.0\n20.2\n20.4\n20.6\n20.8\n21.0\n21.2\nPickScore\nmax\nmean\nmin\nFABRIC\nHPS LoRA\nHPS LoRA + FABRIC\n(b) Min. (dotted), mean (dashed)\nand max. (solid) PickScore in each\nround.\nPositive\nNegative\n70\n75\n80\n85\n90\n95\nCLIP Score\nFeedback Image Similarity in 2nd round\nSD 1.5\nFABRIC (SD)\nDreamlike Photoreal\nFABRIC\nHPS LoRA\nFABRIC + HPS LoRA\n(c) Similarity of generated images\nto positive/negative feedback.\nFigure 3: Results of preference model-based feedback selection.\n1\n2\n3\nRound\n72\n74\n76\n78\n80\n82\nCLIP Score\nSD 1.5\nFABRIC (SD)\n1\n2\n3\nRound\nDreamlike Photoreal\nFABRIC\n1\n2\n3\nRound\nmax\nmean\nmin\nHPS LoRA\nFABRIC + HPS LoRA\nFigure 4: Results of target image-based feedback selection. Positive feedback improves target\nsimilarity over the baseline and using both positive and negative feedback improves it even further.\nAt the same time, any kind of feedback reduces the diversity of generated images drastically.\nlower for the negatives, compared to the baseline. This confirms that FABRIC effectively conditions\nthe direction of generation based on the provided feedback.\n5.2\nTARGET IMAGE-BASED FEEDBACK SELECTION\nIn this experiment, we challenge the assumption that all humans have the same preference, according\nto which they select liked/disliked images. Instead, we assume that the user has some target image in\nmind, some imagined picture that they would like to express.\nTo reflect this scenario, we manually gather a dataset of prompt-image pairs from the AI-art-sharing\nplatform prompthero.com where users post their favorite generations along with the prompt that was\nused to generate the image. Due to this survivorship bias, we claim that it\u2019s appropriate to assume that\nshared images express the respective user\u2019s creative vision for the given prompt or, more generally,\ncorrespond to the user\u2019s preference. The dataset is provided in our public GitHub repository.4\nDuring the experiment, we generate a batch of images using a prompt from the dataset and select\nfeedback images based on CLIP-similarity to the associated target image: the most and least similar\nimage is selected as positive and negative feedback respectively. Note that the images in the dataset\nare generated with various different models and settings, which prevents the evaluated model from\ngenerating exact replications of the target image.\nWe compare the SD-1.5 and Dreamlike Photoreal baselines (using no feedback) to regular FABRIC in\nterms of similarity to the specified target image and in terms of in-batch image diversity. The results\nof this are illustrated in Figure 4. We find that both of them outperform the respective baselines,\nimproving both best-case and worst-case outcomes in rounds 2 and 3. Especially the per-round\nminimum similarity drastically improves over the baseline when any kind of feedback is introduces.\nQualitative results from this experiment are shown in Appendix A.2.\n4https://github.com/sd-fabric/fabric\n7\n(a) The effect of prompt dropout on the best\nCLIP similarity to the target image.\n(b) The effect of prompt dropout on the in-\nbatch image diversity.\nFigure 5: Prompt dropout appears to be an effective way of trading CLIP similarity for more diversity\nin the generative distribution.\n5.3\nADAPTING THE FEEDBACK SCHEDULE\nIn addition to the experiments reported in the main part, we investigated the adaptation of the feedback\nschedule. The default configuration of FABRIC adds feedback in every denoising step. Our findings\nshow that restricting feedback in the first half of the denoising process improves performance by a\nlarge margin. On the other hand, only including feedback in the second half of the denoising process\nreduces performance. This leads to the hypothesis that feedback is useful in the early stages of the\ndenoising process, but fine-grained details are largely determined by the prompt, where feedback\ndoes not help.\n5.4\nINCREASING DIVERSITY WITH PROMPT DROPOUT\nAs can be seen in Appendix A.2, the diversity of generated images quickly collapses as soon as any\nkind of feedback is introduced. This is likely due to the conditioning mechanism, which pushes the\nmodal towards generating images very similar to the reference. From a user perspective, however,\nthis drop in diversity is undesirable, as it prevents future iterations from discovering new and possibly\nbetter results.\nWe investigate one possible approach to combatting this collapse by using prompt-dropout, i.e.\ndropping every word in the prompt with some probability. Using the same setup as in Section 5.2,\nwe compare the best CLIP similarity to the target as well as the in-batch image diversity of using\nprompt dropout with p = 0.3 against not using it (p = 0.0). We find that there is a direct trade-off\nbetween CLIP similarity and diversity: p = 0.3 significantly increases diversity but also produces\nworse images over all. Note that an increased initial diversity especially will also increase diversity in\nfeedback images, which may explain why even after 3 rounds, the diversity of p = 0.3 is still above\nthe initial diversity of p = 0.0. Thanks to more varied feedback, prompt dropout might be able to\ncatch up to the baseline given enough feedback.\n6\nDISCUSSION\nLimitations\nWhile FABRIC works well in our experiments, some limitations apply. We noticed\nthat FABRIC is effective at constraining the generative distribution to a subset of preferable outcomes,\nbut it struggles to expand the distribution beyond the initial text-conditioned distribution given\nby the model. Especially since feedback is already sampled from the model\u2019s output, additional\nmodifications will be needed to overcome this shortcoming. For the same reason, the diversity of\ngenerated images quickly collapses to a single mode close to the feedback images. We propose\nprompt dropout as a possible remedy, but this might run the risk of dropping crucial words in the\nprompt and changing the generations completely.\n8\nAnother limitation lays in the way we collect feedback. Currently, we only allow users to provide\nbinary preferences over images, which does not allow for specific conditioning in the image generation\nprocess.\nFuture Work\nFuture work will investigate approaches toward increasing diversity or generally\ncontrolling the exploration-exploitation trade-off in a more principled fashion. An interesting avenue\nfor this could be to retrieve candidate images from an external image corpus (e.g., the user\u2019s previously\nliked images or a corpus of general high-quality images) and use them as a seed for the feedback\nloop. A big advantage of FABRIC is its orthogonality to most other stable diffusion variations,\nlike checkpoints, LoRA weights, modifying or replacing the text conditioning (Xu et al. (2023))\nwhile achieving significant improvements on top of them (see FABRIC + LoRA). Ultimately, one\ncould specify precise weights for each liked or disliked image and rate if rather coarse features or\nfine-grained features are liked or disliked, respectively. One potential improvement in this direction\nwould be to specify whether user preferences are expressed based on the structure or style of the\nimage. This distinction would allow for more specific conditioning in the image-generation process.\nIn addition, FABRIC provides a well-defined action space with different parameters that can affect\nthe generated results. This opens up the avenue for performing Bayesian optimization on an arbitrary\nobjective (e.g., direct user feedback or a score given by a preference model).\n7\nCONCLUSION\nWe present FABRIC, a training-free method that incorporates iterative feedback into the generation\nprocess of text-to-image models, leveraging attention-based reference image conditioning. Our\nexperimental findings suggest that FABRIC is capable of implicitly optimizing a variety of objective\nfunctions such as human preference and similarity to a designated target image.\nThese objectives noticeably improve with more feedback rounds, demonstrating that FABRIC\u2019s\neffectiveness significantly exceeds merely sampling more images without providing additional\nfeedback. Remarkably, even without training or hyperparameter tuning, FABRIC can outperform the\nHPS LoRA, a model explicitly trained to optimize human preference, on the relevant metric.\nNotably, FABRIC tends to trade exploration for exploitation, often collapsing to a uniform distribution\nafter a handful of feedback rounds. We have examined a few strategies to alleviate this collapse,\nthough further investigation of this trade-off remains a prospect for future work.\nThe iterative setting is paramount to how generative visual models are used in practice. Despite\nthis, recent research on aligning text-to-image models has left it largely unexplored. We believe that\nthis study contributes towards the formation of a framework that aids in devising and evaluating\nmethodologies intended to address this setting.\nETHICAL IMPLICATIONS\nText-to-image models have the potential to make creative visual expression more accessible to\neveryone by allowing individuals without artistic skills or technical expertise to generate visually\nappealing content. Our proposed method aims to further enhance the accessibility and personalization\nof these models by incorporating user preferences into the image generation process. This is achieved\nthrough the utilization of positive and negative feedback, allowing users to provide natural and\nintuitive guidance based on prior images or previously generated ones.\nBy adopting our approach, users gain increased control over the generated content, promoting ethical\nusage. However, this heightened control also raises concerns regarding the potential for misuse or\nabuse of the system. To address these concerns, it becomes crucial for the community as well as\nsociety at large to establish clear guidelines regarding the legal and ethical utilization of such systems.\nBy placing responsibility on individual users to ensure responsible and ethical usage, we can mitigate\nthe risks and foster a positive and constructive environment for creative expression.\n9\nREFERENCES\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis, 2019.\nKelvin C. K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative\nlatent bank for large-factor image super-resolution, 2020.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness, 2022.\nYing Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel,\nMohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for\nfine-tuning text-to-image diffusion models, 2023.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and\nDaniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=NAQvF08TcyG.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-\npic: An open dataset of user preferences for text-to-image generation, 2023.\nAlexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias:\nAnalyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408, 2023.\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for\nediting real images using guided diffusion models, 2022.\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with\nspatially-adaptive normalization, 2019.\nMarkus N. Rabe and Charles Staats. Self-attention does not need o(n2) memory, 2022.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks, 2016.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision, 2021.\nAli Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\nvq-vae-2, 2019.\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis, 2016.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models, 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023.\n10\nKihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred\nBarber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip\nKrishnan. Styledrop: Text-to-image generation in any style, 2023.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations, 2021.\nZhiwei Tang, Dmitry Rybin, and Tsung-Hui Chang. Zeroth-order optimization meets human feedback:\nProvable learning via ranking oracles, 2023.\nXiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better Aligning Text-to-Image\nModels with Human Preference. ArXiv, abs/2303.14420, 2023.\nXingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-free\ndiffusion: Taking \u201dtext\u201d out of text-to-image diffusion models, 2023.\nLvmin Zhang. [Major Update] Reference-only Control \u00b7 Mikubill/sd-webui-controlnet \u00b7 Discus-\nsion #1236, 2023. URL https://github.com/Mikubill/sd-webui-controlnet/\ndiscussions/1236.\nA\nAPPENDIX\nA.1\nMETHOD DETAILS\nHere we provide the Pseudocode of the FABRIC algorithm (see Algorithm 0).\nWeightedAttentionw(Q, K, V ) =\n\u0012\nw\n\u2225w\u22251\n\u2299 softmax\n\u0012QK\u22a4\n\u221adk\n\u0013\u0013\nV \u22a4\n(2)\nA.2\nTARGET IMAGE-BASED FEEDBACK SELECTION EXAMPLES\nHere, we provide some example feedback-trajectories from the promthero-dataset, see Figure 6\nA.3\nFABRIC WORKFLOW\nIn Figure 7 we show the high-level workflow of FABRIC.\n11\nAlgorithm 1 FABRIC: Feedback via Attention-Based Reference Image Conditioning\nRequire: Let N be the number of feedback rounds, n be the batch size of generated images in each\nround and model be a diffusion model capable of reference-conditioning.\n1: procedure FABRIC\n2:\npos, neg \u2190 [], []\n3:\nfor i \u2208 {1, . . . , N} do\n4:\nprompt \u2190 get prompt(i)\n5:\nimages \u2190 [GENERATE(prompt, pos, neg) for n times]\n6:\nxpos, xneg \u2190 get feedback(images)\n\u25b7 we focus on one like and one dislike\n7:\npos.put(xpos)\n8:\nneg.put(xneg)\n9:\nend for\n10: end procedure\n11: function GENERATE(prompt, positives, negatives)\n12:\nzT = initial noise()\n13:\nfor t \u2208 {T, . . . , 1} do\n14:\nhiddens \u2190 {}\n15:\nfor xref \u2208 {. . . positives, . . . negatives} do\n16:\nzref \u2190 \u221a\u00af\u03b1t \u00b7 xref + \u221a1 \u2212 \u00af\u03b1t \u00b7 \u03f5(t)\nref\n\u25b7 forward diffusion noising, \u03f5(t)\nref \u223c N(0, I)\n17:\nh \u2190 PRECOMPUTEHIDDENSTATES(zref, t)\n18:\nhiddens.put(h)\n19:\nend for\n20:\ncompute w(t)\npos and w(t)\nneg according to the feedback settings\n21:\n\u03f5cond,t\u22121 \u2190 MODIFIEDUNET(zt, t, get positive(hiddens), w(t)\npos)\n22:\n\u03f5uncond,t\u22121 \u2190 MODIFIEDUNET(zt, t, get negative(hiddens), w(t)\nneg)\n23:\nzt\u22121 \u2190 step with cfg(zt, \u03f5cond,t, \u03f5uncond,t)\n\u25b7 ancestral Euler sampling in our case\n24:\nend for\n25:\nreturn z0\n26: end function\n27: function PRECOMPUTEHIDDENSTATES(z, t)\n28:\nhiddens \u2190 []\n29:\nfor i-th layer in the Unet do\n30:\napply ResNet block(s) to z\n31:\nhiddens.put(i, z)\n\u25b7 just before self-attention\n32:\napply self-attention to z\n33:\napply cross-attention and FFN to z\n34:\nend for\n35:\nreturn z\n36: end function\n37: function MODIFIEDUNET(z, t, hiddens, w)\n38:\nfor i-th layer in the Unet do\n\u25b7 for hiddens = \u2205 this function is a standard U-Net\n39:\nh \u2190 hiddens.at(i)\n40:\napply ResNet block(s) to z\n41:\nQ \u2190 W (i)\nQ \u00b7 z\n42:\nK \u2190 W (i)\nK \u00b7 concat(z, h)\n43:\nV \u2190 W (i)\nV\n\u00b7 concat(z, h)\n44:\nz \u2190 WeightedAttentionw(Q, K, V )\n45:\napply cross-attention and FFN to z\n46:\nend for\n47:\nreturn z\n48: end function\n12\nTarget Image\nRound 0\nRound 1\nRound 2\nFigure 6: Examples of feedback rounds from our target-image-based experiment.\n13\nFigure 7: FABRIC: Reference images are noised up to a certain step, and the extracted keys and\nvalues are injected into the self-attention of the U-Net during the denoising process.\n14\n"
  },
  {
    "title": "Towards A Unified Agent with Foundation Models",
    "link": "https://arxiv.org/pdf/2307.09668.pdf",
    "upvote": "12",
    "text": "Reincarnating Reinforcement Learning Workshop at ICLR 2023\nTOWARDS A UNIFIED AGENT WITH\nFOUNDATION MODELS\nNorman Di Palo\u2217\nImperial College London\nLondon, UK\nn.di-palo20@imperial.ac.uk\nArunkumar Byravan\nGoogle DeepMind\nLondon, UK\nLeonard Hasenclever\nGoogle DeepMind\nLondon, UK\nMarkus Wulfmeier\nGoogle DeepMind\nLondon, UK\nNicolas Heess\nGoogle DeepMind\nLondon, UK\nMartin Riedmiller\nGoogle DeepMind\nLondon, UK\nABSTRACT\nLanguage Models and Vision Language Models have recently demonstrated un-\nprecedented capabilities in terms of understanding human intentions, reasoning,\nscene understanding, and planning-like behaviour, in text form, among many oth-\ners. In this work, we investigate how to embed and leverage such abilities in\nReinforcement Learning (RL) agents. We design a framework that uses language\nas the core reasoning tool, exploring how this enables an agent to tackle a series\nof fundamental RL challenges, such as efficient exploration, reusing experience\ndata, scheduling skills, and learning from observations, which traditionally require\nseparate, vertically designed algorithms. We test our method on a sparse-reward\nsimulated robotic manipulation environment, where a robot needs to stack a set of\nobjects. We demonstrate substantial performance improvements over baselines in\nexploration efficiency and ability to reuse data from offline datasets, and illustrate\nhow to reuse learned skills to solve novel tasks or imitate videos of human experts.\n1\nINTRODUCTION\nIn recent years, the literature has seen a series of remarkable Deep Learning (DL) success stories\n(3), with breakthroughs particularly in the fields of Natural Language Processing (4; 19; 8; 29)\nand Computer Vision (2; 25; 36; 37). Albeit different in modalities, these results share a common\nstructure: large neural networks, often Transformers (46), trained on enormous web-scale datasets\n(39; 19) using self-supervised learning methods (19; 6). While simple in structure, this recipe led to\nthe development of surprisingly effective Large Language Models (LLMs) (4), able to process and\ngenerate text with outstanding human-like capability, Vision Transformers (ViTs) (25; 13) able to\nextract meaningful representations from images and videos with no supervision (6; 18), and Vision-\nLanguage Models (VLMs) (2; 36; 28), that can bridge those data modalities describing visual inputs\nin language, or transforming language descriptions into visual outputs. The size and abilities of these\nmodels led the community to coin the term Foundation Models (3), suggesting how these models can\nbe used as the backbone for downstream applications involving a variety of input modalities.\nThis led us to the following question: can we leverage the performance and capabilities of (Vision)\nLanguage Models to design more efficient and general reinforcement learning agents? After being\ntrained on web-scaled textual and visual data, the literature has observed the emergence of common\nsense reasoning, proposing and sequencing sub-goals, visual understanding, and other properties in\nthese models (19; 4; 8; 29). These are all fundamental characteristics for agents that need to interact\nwith and learn from environments, but that can take an impractical amount of time to emerge tabula\nrasa from trial and error. Exploiting the knowledge stored into Foundation Models, can bootstrap this\nprocess tremendously.\nMotivated by this idea, we design a framework that puts language at the core of an RL robotic agent,\nparticularly in the context of learning from scratch. Our core contribution and finding is the following:\n\u2217work done during an internship at Google DeepMind.\n1\narXiv:2307.09668v1  [cs.RO]  18 Jul 2023\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nFigure 1: A high-level illustration of our framework.\nwe show that this framework, which leverages LLMs and VLMs, can tackle a series of fundamental\nproblems in RL settings, such as 1) efficiently exploring sparse-reward environments, 2) re-using\ncollected data to bootstrap the learning of new tasks sequentially, 3) scheduling learned skills\nto solve novel tasks and 4) learning from observation of expert agents. In the recent literature,\nthese tasks need different, specifically designed algorithms to be tackled individually, while we\ndemonstrate that the capabilities of Foundation Models unlock the possibility of developing a more\nunified approach.\n2\nRELATED WORK\nOver the past few years, scaling the parameter count of models and the size and diversity of training\ndatasets led to unprecedented capabilities in (Vision) Language Models (4; 19; 2; 19; 8). This in turn\nled to several applications leveraging these models within agents that interact with the world. Prior\nwork has used LLMs and VLMs together with RL agents in simulated environments (12; 44), but\nthey rely on collecting large amounts of demonstrations for training agents. Instead, we focus on the\nproblem of learning RL agents from scratch and leverage LLMs and VLMs to accelerate progress.\nPrior work has also looked at leveraging LLMs and VLMs for robotics applications; particularly\n(1; 21; 50; 20) leveraged LLMs for planning sub-goals in the context of long-horizon tasks together\nwith VLMs for scene understanding and summarization. These sub-goals can then be grounded\ninto actions through language-conditioned policies (22; 30). While most of these works focus on\ndeploying and scheduling already learned skills through LLMs, albeit in the real world, our work\nfocuses on an RL system that learns such behaviours from scratch, highlighting the benefits that these\nmodels bring to exploration, transfer and experience reuse.\nSeveral methods have been proposed to tackle sparse-reward tasks, either through curriculum learning\n(43; 51; 31; 16), intrinsic motivation (17; 35), or hierarchical decomposition (32; 27). We demonstrate\nhow LLMs can generate learning curriculums zero-shot, without any additional learning or finetuning,\nand VLMs can automatically provide rewards for these sub-goals, greatly improving learning speed.\nRelated work has also looked at reusing large datasets of robotic experience by learning a reward\nmodel for the new tasks at hand (5). However, numerous human annotations of desired rewards need\nto be gathered for each new task. Instead, as reported in concurrent related work (48), we show\nsuccessful relabeling of past experience leveraging VLMs which can be finetuned with small amounts\nof data from the target domain.\n(15) is the most similar method to our work: they propose an interplay between LLMs and VLMs\nto learn sparse-reward tasks in Minecraft (23; 24). However, there are some notable differences:\nthey use a vast internet dataset of videos, posts and tutorials to finetune their models, while we\ndemonstrate that it is possible to effectively finetune a VLM with as few as 1000 datapoints, and use\noff-the-shelf LLMs; additionally, we also investigate and experiment how this framework can be used\nfor data reuse and transfer, and learning from observation, besides exploration and skills scheduling,\nproposing a more unified approach to some core challenges in reinforcement learning.\n3\nPRELIMINARIES\nWe use the simulated robotic environment from Lee et al. (26) modelled with the MuJoCo physics\nsimulator (45) for our experiments: a robot arm interacts with an environment composed of a red,\n2\nReincarnating Reinforcement Learning Workshop at ICLR 2023\na blue and a green object in a basket. We formalise it as a Markov Decision Process (MDP): the\nstate space S represents the 3D position of the objects and the end-effector. The robot is controlled\nthrough position control: the action space A is composed of an x, y position, that we reach using\nthe known inverse kinematics of the robot, where the robot arm can either pick or place an object,\ninspired by (49; 40). The observation space O is composed of 128 \u00d7 128 \u00d7 3 RGB images coming\nfrom two cameras fixed to the edges of the basket. The agent receives a language description of the\ntask T to solve, which can have two forms: either \"Stack X on top of Y\", where X and Y are taken\nfrom {\"the red object\", \"the green object\", \"the blue object\" } without replacement, or \"Stack all\nthree objects\", that we also call Triple Stack.\nA positive reward of +1 is provided if the episode is successful, while a reward of 0 is given in any\nother case. We define the sparseness of a task as the average number of environment steps needed,\nwhen executing random actions sampled from a uniform distribution, to solve the task and receive a\nsingle reward. With the MDP design we adopt, stacking two objects has a sparseness of 103, while\nan optimal policy could solve the task with 2 pick-and-place actions/steps (49; 40). Stacking all three\nobjects has a sparseness of more than 106 as measured by evaluating trajectories from a random\npolicy, while an optimal policy could solve the task in 4 steps.\n4\nA FRAMEWORK FOR LANGUAGE-CENTRIC AGENTS\nThe goal of this work is to investigate the use of Foundation Models (3), pre-trained on vast image\nand text datasets, to design a more general and unified RL robotic agent. We propose a framework\nthat augments from-scratch RL agents with the ability to use the outstanding abilities of LLMs and\nVLMs to reason about their environment, their task, and the actions to take entirely through language.\nTo do so, the agent first needs to map visual inputs to text descriptions. Secondly, we need to prompt\nan LLM with such textual descriptions and a description of the task to provide language instructions\nto the agent. Finally, the agent needs to ground the output of the LLM into actions.\nFigure 2: An illustration of CLIP computing\nthe similarity, as dot product, between obser-\nvations and text descriptions.\nBridging Vision and Language using VLMs: To\ndescribe the visual inputs taken from the RGB cam-\neras (Sec. 3) in language form, we use CLIP, a large,\ncontrastive visual-language model (36). CLIP is com-\nposed of an image-encoder \u03d5I and a text-encoder \u03d5T ,\ntrained on a vast dataset of noisily paired images and\ntext descriptions, that we also refer to as captions.\nEach encoder outputs a 128-dimensional embedding\nvector: embeddings of images and matching text de-\nscriptions are optimised to have large cosine similar-\nity. To produce a language description of an image\nfrom the environment, the agent feeds an observation\not to \u03d5I and a possible caption ln to \u03d5T (Fig. 2).\nWe compute the dot product between the embedding\nvectors and considers the description correct if the\nresult is larger than \u03b3, a hyperparameter (\u03b3 = 0.8 in\nour experiments, see Appendix for more details). As\nwe focus on robotic stacking tasks, the descriptions are in the form \"The robot is grasping X\" or \"The\nX is on top of Y\", where X and Y are taken from {\"the red object\", \"the green object\", \"the blue object\"\n} without replacement. We finetune CLIP on a small amount of data from the simulated stacking\ndomain; more details on how this works and analysis on data needs for finetuning are provided in the\nappendix.\nReasoning through Language with LLMs: Language Models take as input a prompt in the form of\nlanguage and produce language as output by autoregressively computing the probability distribution\nof the next token and sampling from this distribution. In our setup, the goal of LLMs is to take a\ntext instruction that represents the task at hand (e.g. \"Stack the red object on the blue object\"), and\ngenerate a set of sub-goals for the robot to solve. We use FLAN-T5 (10), an LLM finetuned on\ndatasets of language instructions. A qualitative analysis we performed showed that it performed\nslightly better than LLMs not finetuned on instructions.\nThe extraordinary in-context learning capabilities of these LLMs allowed us to use them off-the-shelf\n(4; 34), without the need for in-domain finetuning, and guide their behaviour by providing as few\n3\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nFigure 3: The VLM can act as an internal reward model by comparing language goals proposed by\nthe LLM to the collected observations.\nas two examples of task instruction and desired language outputs: we describe the environment\nsetting, asking the LLM to find sub-goals that would lead to solving a proposed task, providing two\nexamples of such tasks and relative sub-goals decomposition. With that, the LLM was able to emulate\nthe desired behaviour, not only in content, but also in the formatting of the output language which\nallowed for efficient parsing. In the Appendix we provide a more detailed description of the prompts\nwe use and the behaviour of the LLMs.\nGrounding Instructions into Actions: The language goals provided by the LLMs are then grounded\ninto actions using a language-conditioned policy network. This network, parameterized as a Trans-\nformer (46), takes an embedding of the language sub-goal and the state of the MDP at timestep t,\nincluding objects\u2019 and robot end-effector\u2019s positions, as input, each represented as a different vector,\nand outputs an action for the robot to execute as timestep t + 1. This network is trained from scratch\nwithin an RL loop as we describe below.\nCollect & Infer Learning Paradigm: Our agent learns from interaction with the environment\nthrough a method inspired by the Collect & Infer paradigm (38). During the Collect phase, the agent\ninteracts with the environment and collects data in the form of states, observations, actions and current\ngoal as (st, ot, at, gi), predicting actions through its policy network, f\u03b8(st, gi) \u2192 at. After each\nepisode, the agent uses the VLM to infer if any sub-goals have been encountered in the collected data,\nextracting additional rewards, as we explain in more detail later. If the episode ends with a reward, or\nif any reward is provided by the VLM, the agent stores the episode data until the reward timestep\n[(s0, o0, a0, gi), . . . , (sTr\u22121, oTr\u22121, aTr\u22121, gi)] in an experience buffer. We illustrate this pipeline in\nFig. 4 (Left). These steps are executed by N distributed, parallel agents, that collect data into the\nsame experience buffer (N =1000 in our work). During the Infer phase, we train the policy through\nBehavioural Cloning on this experience buffer after each agent has completed an episode, hence every\nN total episodes, implementing a form of Self-Imitation on successful episodes (33; 14; 7). The\nupdated weights of the policy are then shared with all the distributed agents and the process repeats.\n5\nAPPLICATIONS AND RESULTS\nWe described the building blocks that compose our framework. The use of language as the core of\nthe agent provides a unified framework to tackle a series of fundamental challenges in RL. In the\nfollowing sections, we will investigate each of those contributions, focusing on exploration, reusing\npast experience data, scheduling and reusing skills and learning from observation. The overall\nframework is also described in Algorithm 1.\n5.1\nEXPLORATION - CURRICULUM GENERATION THROUGH LANGUAGE\nRL benefits substantially from carefully crafted, dense rewards (5). However, the presence of dense\nrewards is rare in many real-world environments. Robotic agents need to be able to learn a wide range\nof tasks in complex environments, but engineering dense reward functions becomes prohibitively time-\nconsuming as the number of tasks grows. Efficient and general exploration is therefore imperative to\novercome these challenges and scale RL.\nA wide variety of methods have been developed over the years to tackle exploration of sparse-reward\nenvironments (43; 51; 31; 16; 17; 35; 32; 27). Many propose decomposing a long-horizon task into\nshorter, easier to learn tasks, through curriculum generation and learning. Usually, these methods\n4\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nFigure 4: Left: Illustration of our Collect & Infer pipeline. Middle, Right: Learning curves of our\nframework and a baseline in the Stack Red on Blue and Triple Stack tasks.\nneed to learn to decompose tasks from scratch, hindering overall learning efficiency. We demonstrate\nhow an RL agent leveraging LLMs can take advantage of a curriculum of text sub-goals that are\ngenerated without any past environment interaction.\nAlgorithm 1 Language-Centric Agent\n1: Training time:\n2: for task in tasks do\n3:\nsubgoals = LLM(task) //find text subgoals\ngiven task description\n4:\nexp_buffer.append( VLM(offline_buffer,\nsubgoals)) //extract successful eps from of-\nfline buff. collected in past tasks(Sec. 5.2)\n5:\nfor ep in episodes do\n6:\n(Sec. 5.1)\n7:\nE \u2190 [s0:T , o0:T , a0:T , gi] //collect ep.\ntrajectory\n8:\nr \u2190 collect final reward\n9:\nrinternal \u2190 VLM(E, subgoals) //extract\nadditional rewards for subgoals\n10:\nif r or rinternal then\n11:\nexp_buffer.append(E0:Tr)\n//Add\ntimesteps until reward\n12:\nif ep%N == 0 then\n13:\n\u03b8 \u2190 BC(episode_buffer) //train agent\nwith BC every N eps\n14: Test time:\n15: Receive text_instruction or video_demo\n16: if text_instruction then\n17:\nsubgoals = LLM(text_instruction) (Sec. 5.3)\n18: else if video_demo then\n19:\nsubgoals = VLM(video_demo)\n(Sec. 5.4)\n20: execute(subgoals)\n(Sec. 5.3)\nTo guide exploration, the agent provides the task\ndescription Tn to the LLM, instructing it to de-\ncompose the task into shorter-horizon sub-goals,\neffectively generating a curriculum of goals g0:G\nin text form 1. The agent selects actions as\nf\u03b8(st, Tn) \u2192 at. While the environment pro-\nvides a reward only if Tn is solved, the VLM\nis deployed to act as an additional, less sparse\nreward model: given the observations o0:T col-\nlected during the episode and all the text sub-\ngoals g0:G proposed by the LLM, it verifies if\nany of the sub-goals were solved at any step.\nWe consider an observation ot to represent a\ncompletion state for a sub-goal gi if \u03d5T (gi) \u00b7\n\u03d5I(ot) > \u03b3.\nIn that case, the agent adds\n[(s0, o0, a0, Tn), . . . , (st\u22121, ot\u22121, at\u22121, Tn)] to\nour experience buffer. The process is illustrated\nin Fig. 3, 11 (in the Appendix).\nResults on Stack X on Y and Triple Stack. We\ncompare our framework to a baseline agent that\nlearns only through environment rewards in Fig.\n4. The learning curves clearly illustrate how our\nmethod is substantially more efficient than the\nbaseline on all the tasks. Noticeably, our agent\u2019s\nlearning curve rapidly grows in the Triple Stack\ntask, while the baseline agent still has to receive\na single reward, due to the sparseness of the\ntask being 106. We provide a visual example\nof the extracted sub-goals and rewards in the\nAppendix.\nThese results suggest something noteworthy: we\ncan compare the sparseness of the tasks with the\nnumber of steps needed to reach a certain success rate, as in Fig. 5. We train our method also on the\nGrasp the Red Object task, the easiest of the three, with sparseness in the order of 101. We can see\nthat, under our framework, the number of steps needed grows more slowly than the sparseness of\nthe task. This is a particularly important result, as generally the opposite is true in Reinforcement\nLearning (35).\n1For example, the LLM decomposes \"Stack the red object on the blue object\" into the following sub-goals:\n[\"The robot is grasping the red object\", \"The red object is on top of the blue object\"]\n5\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nFigure 6: Our framework can reutilise offline data collected on other tasks, extracting successful\ntrajectories for the new task at hand, bootstrapping policy learning.\nFigure 5: With our framework, the num-\nber of steps needed to reach a certain\nsuccess rate grows more slowly than the\nsparseness of the task.\nThis slower growth, enabled by the increase in the amount\nof sub-goals proposed by the LLM as the task becomes\nsparser, suggests that our framework can scale to even\nharder tasks and make them tractable, assuming sub-goals\ncan be encountered with a uniform-like distribution at\nany point during exploration. Additionally, unlike prior\napproaches that need carefully crafted intrinsic rewards\nor other exploration bonuses our approach can directly\nleverage prior knowledge from LLMs and VLMs to gener-\nate a semantically meaningful curriculum for exploration,\nthereby paving the way for general agents that explore in\na self-motivated manner even in sparse-reward environ-\nments.\n5.2\nEXTRACT\nAND TRANSFER - EFFICIENT SEQUENTIAL\nTASKS LEARNING BY REUSING OFFLINE DATA\nWhen interacting with their environments, our agents\nshould be able to learn a series of tasks over time, reusing\nthe prior collected data to bootstrap learning on any new task instead of starting tabula rasa. This is a\nfundamental ability to scale up RL systems that learn from experience. Recent work has proposed\ntechniques to adapt task-agnostic offline datasets to new tasks, but they can require laborious human\nannotations and learning of reward models (5; 47; 9).\nWe leverage our language based framework to showcase bootstrapping based on the agent\u2019s past expe-\nrience. We train three tasks in sequence: Stack the red object on the blue object, Stack the blue object\non the green object, and Stack the green object on the red object, that we call [TR,B, TB,G, TG,R]. The\nintuition is simple: while exploring to solve, for example, TR,B, it is likely that the agent had solved\nother related tasks, like TB,G or TG,R, either completely or partially. The agent should therefore be\nable to extract these examples when trying to solve the new tasks, in order not to start from scratch,\nbut reuse all the exploration data gathered for previous tasks.\nAs discussed in Sec. 4, our agent gathers an experience buffer of interaction data. We now equip\nthe agent with two different buffers: a lifelong buffer, or offline buffer, where the agent stores each\nepisode of interaction data, and continues expanding it task after task. Then, the agent has a new task\nbuffer, re-initialised at the beginning of each new task, that is filled, as in Sec. 5.1, with trajectories\nthat result in a reward, either external or internally provided by the VLM using LLM text sub-goals\n(Fig. 3). The policy network is optimised using the new task buffer.\nDifferently from before however, while the first task, TR,B, is learned from scratch, the agent reuses\nthe data collected during task n to bootstrap the learning of the next task n+1. The LLM decomposes\nTn+1 into text sub-goals [g0, . . . , gL\u22121]. The agent then extracts from the lifelong/offline buffer each\n6\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nstored episode En = [(s0:T,n, o0:T,n, a0:T,n)]. It then takes each episode\u2019s observation ot,n and uses\nthe VLM to compute dot-products score between all image observations and all text sub-goals as\n\u03d5T (gl) \u00b7 \u03d5I(ot). If the score is larger than the threshold \u03b3 the agent adds all the episode\u2019s timesteps\nup to t, [(s0:t,n, o0:t,n, a0:t,n)] to the new task buffer. The process is illustrated in Fig. 6.\nThis procedure is repeated for each new task at the beginning of training. Following this procedure,\nthe agent does not start learning new tasks tabula rasa: at the beginning of task Tn, the current\nexperience buffer is filled with episodes useful to learn the task extracted from T0:n. When n increases,\nthe amount of data extracted from T0:n increases as well, speeding up learning.\nFigure 7: In our experiments, the agent\ncan learn task n + 1 faster than task n by\nreusing past experience data.\nResults on Experience Reuse for Sequential Tasks\nLearning.\nThe agent applies this method to learn\n[TR,B, TB,G, TG,R] in succession. At the beginning of\neach new task we re-initialise the policy weights: our goal\nis to investigate the ability of our framework to extract and\nre-use data, therefore we isolate and eliminate effects that\ncould be due to network generalisation.\nWe plot how many interaction steps the agent needs to\ntake in the environment to reach 50% success rate on each\nnew task in Fig. 7. Our experiments clearly illustrate the\neffectiveness of our technique in reusing data collected for\nprevious tasks, improving the learning efficiency of new\ntasks.\nThese results suggest that our framework can be employed\nto unlock lifelong learning capabilities in robotic agents:\nthe more tasks are learned in succession, the faster the next\none is learned. This can be particularly beneficial when\ndeploying agents in open-ended environments, particularly in the real world; by leveraging data\nacross its lifetime the agent has encountered it should be able to learn novel tasks far faster than\nlearning purely from scratch.\n5.3\nSCHEDULING AND REUSING LEARNED SKILLS\nWe described how our framework enables the agent with the ability to efficiently explore and learn to\nsolve sparse-reward tasks, and to reuse and transfer data for lifelong learning.\nUsing its language-conditioned policy (Sec. 4), the agent can thus learn a series of M skills, described\nas a language goal g0:M (e.g. \"The green object is on top of the red object\" or \"The robot is grasping\nthe blue object\").\nOur framework allows the agent to schedule and reuse the M skills it has learned to solve novel\ntasks, beyond what the agent encountered during training. The paradigm follows the same steps we\nencountered in the previous sections: a command like Stack the green object on top of the red object\nFigure 8: Our framework can break down a task into a list of skills using the LLM, and execute each\nskill until the VLM predicts that its sub-goal has been reached.\n7\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nFigure 9: An illustration of the agent learning from observation using our framework.\nor Stack the red on the blue and then the green on the red is fed to the LLM, which is prompted to\ndecompose it into a list of shorter-horizon goals, g0:N. The agent can then ground these into actions\nusing the policy network as f\u03b8(st, gn) \u2192 at.\nWhen executing the n-th skill, the VLM computes at each timestep if \u03d5T (gn) \u00b7 \u03d5I(ot) > \u03b3, thus\nchecking if the goal of the skill has been reached in the current observation. In that case, the agent\nstarts executing the n + 1-th skill, unless the task is solved.\n5.4\nLEARNING FROM OBSERVATION: MAPPING VIDEOS TO SKILLS\nLearning from observing an external agent is a desirable ability for general agents, but this often\nrequires specifically designed algorithms and models (42; 11; 52). Our agent can be conditioned on a\nvideo of an expert performing the task, enabling one-shot learning from observation. In our tests,\nthe agent takes a video of a human stacking the objects with their hand. The video is divided into\nF frames, v0:F . The agent then uses the VLM, paired with the M textual description of the learned\nskills, expressed as sub-goals g0:M, to detect what sub-goals the expert trajectory encountered as\nfollows: (1) the agent embeds each learned skill/sub-goal through \u03d5T (gm) and each video frame\nthrough \u03d5I(vf) and compute the dot product between each pair. (2) it lists all the sub-goals that\nobtain a similarity larger than \u03b3, collecting the chronological list of sub-goals the expert encountered\nduring the trajectory. (3) It executes the list of sub-goals as described in Fig. 8.\nDespite being finetuned only on images from the MuJoCo simulation (Sec. 4), the VLM was able\nto accurately predict the correct text-image correspondences on real-world images depicting both a\nrobot or a human arm. Notice also how we still refer to it as \"the robot\" in the captions (Fig. 9), but\nthe VLM generalises to a human hand regardless.\n6\nCONCLUSION\nWe propose a framework that puts language at the core of an agent. Through a series of experiments,\nwe demonstrate how this framework, by leveraging the knowledge and capabilities of Foundation\nModels, can provide a more unified approach with respect to the current literature to tackle a series\nof core RL challenges, that would normally require separate algorithms and models: 1) exploring\nin sparse-reward tasks 2) reusing experience data to bootstrap learning of new skills 3) scheduling\nlearned skills to solve novel tasks and 4) learning from observing expert agents. These initial results\nsuggest that leveraging foundation models can lead to general RL algorithms able to tackle a variety\nof problems with improved efficiency and generality. By leveraging the prior knowledge contained\nwithin these models we can design better robotic agents that are capable of solving challenging tasks\ndirectly in the real world. We provide a list of current limitations and future work in the Appendix.\n8\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n[3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[5] Serkan Cabi, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott\nReed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, et al. Scaling\ndata-driven robotics with reward sketching and batch reinforcement learning. arXiv preprint\narXiv:1909.12200, 2019.\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.\n[7] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084\u201315097,\n2021.\n[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[9] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416, 2022.\n[11] Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayaraman, Akshara Rai, and Franziska Meier.\nModel-based inverse reinforcement learning from visual demonstrations. In Jens Kober, Fabio\nRamos, and Claire Tomlin, editors, Proceedings of the 2020 Conference on Robot Learning,\nvolume 155 of Proceedings of Machine Learning Research, pages 1930\u20131942. PMLR, 16\u201318\nNov 2021. URL https://proceedings.mlr.press/v155/das21a.html.\n[12] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix\nHill, and Rob Fergus. Collaborating with language models for embodied reasoning. arXiv\npreprint arXiv:2302.00763, 2023.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[14] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore:\na new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.\n9\nReincarnating Reinforcement Learning Workshop at ICLR 2023\n[15] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended\nembodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.\n[16] Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse\ncurriculum generation for reinforcement learning. In Conference on robot learning, pages\n482\u2013495. PMLR, 2017.\n[17] Oliver Groth, Markus Wulfmeier, Giulia Vezzani, Vibhavari Dasagi, Tim Hertweck, Roland\nHafner, Nicolas Heess, and Martin Riedmiller. Is curiosity all you need? on the utility of\nemergent behaviours from curious exploration. arXiv preprint arXiv:2109.08603, 2021.\n[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\n[19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n[20] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In International\nConference on Machine Learning, pages 9118\u20139147. PMLR, 2022.\n[21] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied\nreasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.\n[22] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey\nLevine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning.\nIn Conference on Robot Learning, pages 991\u20131002. PMLR, 2022.\n[23] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform\nfor artificial intelligence experimentation. In International Joint Conference on Artificial\nIntelligence, 2016.\n[24] Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Jun-\nyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, et al. Minerl diamond 2021 competition:\nOverview, results, and lessons learned. NeurIPS 2021 Competitions and Demonstrations Track,\npages 13\u201328, 2022.\n[25] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nV 16, pages 491\u2013507. Springer, 2020.\n[26] Alex X Lee, Coline Manon Devin, Yuxiang Zhou, Thomas Lampe, Konstantinos Bousmalis,\nJost Tobias Springenberg, Arunkumar Byravan, Abbas Abdolmaleki, Nimrod Gileadi, David\nKhosid, et al. Beyond pick-and-place: Tackling robotic stacking of diverse shapes. In 5th\nAnnual Conference on Robot Learning, 2021.\n[27] Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko. Learning multi-level hierar-\nchies with hindsight. arXiv preprint arXiv:1712.00948, 2017.\n[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n[29] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code\ngeneration with alphacode. Science, 378(6624):1092\u20131097, 2022.\n[30] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured\ndata. arXiv preprint arXiv:2005.07648, 2020.\n10\nReincarnating Reinforcement Learning Workshop at ICLR 2023\n[31] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher\u2013student curriculum\nlearning. IEEE transactions on neural networks and learning systems, 31(9):3732\u20133740, 2019.\n[32] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical\nreinforcement learning. Advances in neural information processing systems, 31, 2018.\n[33] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In Interna-\ntional Conference on Machine Learning, pages 3878\u20133887. PMLR, 2018.\n[34] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning\nand induction heads. arXiv preprint arXiv:2209.11895, 2022.\n[35] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pages 2778\u2013\n2787. PMLR, 2017.\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[38] Martin Riedmiller, Jost Tobias Springenberg, Roland Hafner, and Nicolas Heess. Collect &\ninfer - a fresh look at data-efficient reinforcement learning. In 5th Annual Conference on Robot\nLearning, Blue Sky Submission Track, 2021. URL https://openreview.net/forum?\nid=qscEfLT5VJK.\n[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[40] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\nmanipulation. In Conference on Robot Learning, pages 894\u2013906. PMLR, 2022.\n[41] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering\nchess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint\narXiv:1712.01815, 2017.\n[42] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. Avid: Learning\nmulti-stage tasks via pixel-level translation of human videos. arXiv preprint arXiv:1912.04443,\n2019.\n[43] Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob\nFergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint\narXiv:1703.05407, 2017.\n[44] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distilling\ninternet-scale vision-language models into embodied agents. arXiv preprint arXiv:2301.12507,\n2023.\n[45] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n5026\u20135033, 2012. doi: 10.1109/IROS.2012.6386109.\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n11\nReincarnating Reinforcement Learning Workshop at ICLR 2023\n[47] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F\u00fcrnkranz. A survey of\npreference-based reinforcement learning methods. J. Mach. Learn. Res., 18:136:1\u2013136:46,\n2017.\n[48] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman,\nSergey Levine, and Jonathan Tompson. Robotic skill acquisition via instruction augmentation\nwith vision-language models. arXiv preprint arXiv:2211.11736, 2022.\n[49] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian,\nTravis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks:\nRearranging the visual world for robotic manipulation. In Conference on Robot Learning, pages\n726\u2013747. PMLR, 2021.\n[50] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek\nPurohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al.\nSo-\ncratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint\narXiv:2204.00598, 2022.\n[51] Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value\ndisagreement. Advances in Neural Information Processing Systems, 33:7648\u20137659, 2020.\n[52] Yuxiang Zhou, Yusuf Aytar, and Konstantinos Bousmalis. Manipulator-independent representa-\ntions for visual imitation. arXiv preprint arXiv:2103.09016, 2021.\n12\nReincarnating Reinforcement Learning Workshop at ICLR 2023\n7\nAPPENDIX\n7.1\nFINETUNING CLIP ON IN-DOMAIN DATA\nFigure 10: Captioning precision and re-\ncall of finetuned CLIP as a function of\nthe dataset size. The logarithmic trend\nsuggests that around 103 image-caption\npairs unlock sufficient performance. Val-\nues obtained with \u03b3 = 0.8.\nIn our experiments, the dot products between the embed-\ndings of possible captions and of an RGB observation\nfrom our environment y = \u03d5I(ot) \u00b7 \u03d5T (li) were often un-\ninformative: correct and wrong pairs obtained very similar\nscores, and varied too little in range. Our goal is to set a\nthreshold \u03b3 to recognise correct and wrong descriptions\ngiven an image: therefore we need a larger difference in\nscore. To tackle this, we collect a dataset of image obser-\nvations with various configurations of the objects and the\ncorresponding language descriptions using an automated\nannotator based on the MuJoCo state of the simulation to\nfinetune CLIP with in-domain data. The plot on the right\nprovides an analysis of our findings: precision and recall\ntend to increase logarithmically with the dataset size. The\nkey takeaway message is that, although CLIP is trained on\naround 108 images, just 103 in-domain pairs are enough\nto improve its performance on our tasks.\nIn our case, a high precision is more desirable than high\nrecall: the former indicates that positive rewards are not\nnoisy, while the opposite may disrupt the learning process.\nA lower recall indicates that the model may not be able to\ncorrectly identify all successful trajectories, but this simply translate in the need for more episodes\nto learn, and does not disrupt the learning process. We found a value of \u03b3 = 0.8 to be the best\nperforming choice after finetuning.\n7.2\nCURRENT LIMITATIONS AND FUTURE WORK\n1) In our current implementation, we use a simplified input and output space for the policies, namely\nthe state space of the MDP - i.e. the positions of the objects and the end-effector as provided by the\nMuJoCo simulator - and a pick and place action space, as described in Sec. 3, where the policy can\noutput a x, y position for the robot to either pick and place. This choice was adopted to have faster\nexperiments iteration and therefore be able to focus our search on the main contribution of the paper:\nthe interplay with the LLM and the VLM. Nevertheless, the recent literature has demonstrated that a\nwide range of robotics tasks can be executed through this action space formulation (49; 40).\nFigure 11: Autonomously identifying sub-goals and corresponding rewards becomes especially\nimportant when tasks become prohibitively sparse, like Triple Stack.\n13\nReincarnating Reinforcement Learning Workshop at ICLR 2023\nFigure 12: An example of the prompt we used to condition the LLM, and its outputs. Normal text:\nuser inserted text, bold text: LLM outputs.\nMany works from the current literature (26; 41; 5; 15) demonstrate that, in order for the policy to\nscale to image observations as input and end-effector velocities as output, the model only needs\nmore data, and therefore interaction time. As our goal was demonstrating the relative performance\nimprovements brought by our method, our choice of MDP design does not reduce the generality of\nour findings. Our results will most likely translate also to models that use images as inputs, albeit\nwith the need for more data.\n2) We finetune CLIP on in-domain data, using the same objects we then use for the tasks. In future\nwork, we plan to perform a larger scale finetuning of CLIP on more objects, possibly leaving out the\nobject we actually use for the tasks, therefore also investigating the VLM capabilities to generalise to\ninter-class objects. At the moment, this was out of the scope of this work, as it would have required a\nconsiderable additional amount of computation and time.\n3) We train and test our environment only in simulation: we plan to test the framework also on real-\nworld environments, as our results suggest that 1) we can finetune CLIP with data from simulation\nand it generalises to real images (Sec. 5.4), therefore we can avoid expensive human annotations 2)\nthe framework allows for efficient learning of even sparse tasks from scratch (Sec. 5.1), suggesting\nthe applicability of our method to the real-world, where collecting robot experience is substantially\nmore time expensive.\n7.3\nPROMPTS AND OUTPUTS OF THE LLM\nIn Fig. 12 we show the prompt we used to allow in-context learning of the behaviour we expect from\nthe LLM (34). With just two examples and a general description of the setting and its task, the LLM\ncan generalise to novel combinations of objects and even novel, less well-defined tasks, like \"Stack\nall three objects\", outputting coherent sub-goals.\n14\n"
  },
  {
    "title": "Text2Layer: Layered Image Generation using Latent Diffusion Model",
    "link": "https://arxiv.org/pdf/2307.09781.pdf",
    "upvote": "11",
    "text": "Text2Layer: Layered Image Generation using Latent Diffusion Model\nXinyang Zhang, Wentian Zhao, Xin Lu, and Jeff Chien\nAdobe Inc.\n{xinyangz, wezhao, xinl, jchien}@adobe.com\nGrand Canyon National Park\noutdoor waterproof rugs ...\nFig. 1. Examples of two-layer images. Prompts are displayed on the top of images. Each example includes foreground (fg),\nbackground (bg), and mask component to compose a two-layer image. From left to right of each example: fg, bg, mask, and composed\nimage.\nAbstract\nLayer compositing is one of the most popular image\nediting workflows among both amateurs and profession-\nals. Motivated by the success of diffusion models, we\nexplore layer compositing from a layered image generation\nperspective. Instead of generating an image, we propose\nto generate background, foreground, layer mask, and the\ncomposed image simultaneously. To achieve layered image\ngeneration, we train an autoencoder that is able to recon-\nstruct layered images and train diffusion models on the\nlatent representation. One benefit of the proposed problem\nis to enable better compositing workflows in addition to\nthe high-quality image output. Another benefit is producing\nhigher-quality layer masks compared to masks produced\nby a separate step of image segmentation. Experimental\nresults show that the proposed method is able to generate\nhigh-quality layered images and initiates a benchmark for\nfuture work.\nI.. Introduction\nSegmenting an image into layers is essential for im-\nage editing applications, such as background replacement\nand background blur. For many years, the development,\ntherefore, has been focused on automatic approaches for\nvarious of image segmentation and matting problems [29],\n[57], [46], [15]. Recent development rethinks image editing\nproblems from a generative perspective [34], [24], [4], [47]\nand aims to predict the expected editing region implicitly.\nFor example, Text2live [3] aims to generate desired edits\nby generating a new image layer guided by text to realize\nnon-disruptive editing. T2ONet [24] proposes to directly\ngenerate image editing results guided by text through gen-\nerative models. These approaches are limited by the quality\nof the implicitly predicted editing region and limited by the\ngenerated image quality.\nRecent efforts in large-scale text2image diffusion mod-\nels, such as DALL\u00b7E 2 [43], Imagen [48], and Stable\nDiffusion [45] significantly improve the image genera-\ntion quality and produce high-resolution photo-realistic\nimages. Diffusion-based automatic image editing approach\nis emerging [2], [36], [47], [27], [25], [17], [4], [11], [63].\nFor example, Prompt-to-Prompt [17] modulates the cross-\nattention maps in the inference steps to manipulate im-\nages. Soon InsturctPix2pix [4] utilized Prompt-to-Prompt\nto build a supervised dataset for image editing with natural\nlanguage instruction and then finetuned a Stable Diffusion\nmodel with the dataset. However, these approaches still\nsuffer from one of the following drawbacks. First, the user\ncannot constrain the region of editing, and editing methods\noften modify more pixels than it desires. Second, they do\nnot resolve the issue that details are hard to describe in\nplain language and require trial and error to find the ideal\nediting instruction.\nIn this paper, we explore layered image generation\nguided by text with an intent of addressing the afore-\nmentioned challenges. As this is the first trial to generate\nimage layers through diffusion models, we formulate the\nproblem as generating a two-layer image, which includes\nforeground F, a background B, a layer mask m1, and\nthe composed image. The layer mask controls a layer\u2019s\nlevel of transparency, and the composed image is formed\n1We borrowed the terminology of the layer mask from Photoshop,\nwhich is a nondestructive way to hide parts of an image or layer without\nerasing them\narXiv:2307.09781v1  [cs.CV]  19 Jul 2023\nFig. 2. Layer mask examples. The scale, location, and number of\nobjects vary largely.\nFig. 3. Failure cases of salient object segmentation (left) and\ninpainting (middle and right). The red shaded regions indicate\nthe object to be removed.\nbased on alpha blending. Examples are shown in Figure 1.\nWith a layered image, a user is able to scale or move\nthe foreground and replace foreground or background in\na straightforward manner in image editing applications.\nBecause of the layer mask, users\u2019 edits can be explicitly\ncontrolled to apply to either the foreground or background.\nThe two biggest challenges are layered-image gener-\nation using diffusion models and training data synthesis.\nA trivial solution to generate layered images is running\ntext2image multiple times, however, that doesn\u2019t guarantee\nthe compatibility of the generated images in each run.\nAnother solution is generating an image via a text2image\nsystem and then running salient object segmentation on the\ngenerated image. However, failure masks often occur due\nto the diverse scale and location of objects and the number\nof objects in the image.\nMotivated by the latent diffusion models, we train\na layered-image autoencoder to learn their latent repre-\nsentation and then train diffusion models on the latent\nrepresentation to generate layered images. Concretely, the\nlayered-image autoencoder learns to encode and decode\ntwo-layer images with a multi-task learning loss that in-\ncludes reconstruction loss and loss terms inspired by image\ncomposition. We found that including the composed image\nas a layer in the autoencoder training further improves the\ngenerated image quality. Such layered output facilitates\nlayer compositing and even improves the image synthesis\nquality. Meanwhile, the generated layer mask quality also\nbenefits from the generative network in terms of adapting\nto different object scales as well as having multiple objects\nas a foreground as shown in Figure 2.\nTo synthesize training data for layered image genera-\ntion, we run salient object segmentation on training images\nused for text2image development. We chose to run salient\nobject segmentation based on the observation that text\nprompts usually indicate the salient object clearly. As\nboth the salient object segmentation and inpainting often\nfail (as shown in Figure 3), we developed an automatic\ndata synthesis system that can distinguish high-quality\nsalient object mask and high-quality impainting results.\nThat system produces the proposed LAION-L2I dataset,\nconsisting of about 57.02M high-quality layered images\namong 600M LAION-Aesthetic dataset2 [49].\nAs a first trial to generate layered images, we developed\nan evaluation metric to measure the output quality. Results\nshowed that the proposed method generally produces lay-\nered images with better image quality in terms of FID,\nhigher mask accuracy, and greater text relevance compared\nto several Stable Diffusion-inspired baseline models.\nIn summary, our contributions are three folds.\n1) We developed a text2layer method to generate a lay-\nered image guided by text, including a foreground,\na background, a mask and the composed image.\n2) We introduced a mechanism to synthesize high-\nquality layered images for diffusion model training\nand produces 57.02M high-quality layered images\nfor future research.\n3) We set a benchmark for layered-image genera-\ntion. Experimental results showed that the proposed\nmethod generated higher-quality composed images\nwith a higher text-image relevance score together\nwith a better mask accuracy compared to baselines.\nII.. Related Work\nAs layered image generation has not been systemati-\ncally investigated, we discuss studies in text2image, text-\nbased editing and image segmentation that are related to\nour work.\nText-based image generation Text-based image gener-\nation is the task of synthesizing images according to\nnatural language descriptions. As a conditional generative\nproblem, many previous works ([58], [61]) approach this\nproblem by training GANs [13]) on image caption datasets.\nAnother line of line work ([44], [7]) tackles the generation\nin an auto-regressive way with Transformers [54], usu-\nally utilizing VQ-VAE [53] codebooks to produce image\ntokens. Recently, diffusion-based approaches significantly\nboost the generated image quality in terms of semantic\ncoherence and fine spatial details, such as Imagen [48],\nDALL\u00b7E 2 [43], GLIDE [38], and Parti [59]. Behind\nthe scene, they are developed upon recent advances in\ndenoising diffusion probabilistic models (DDPM, [20],\n[37]). DDPM [20] and its variants ([50]) learn the re-\nversed process of a Markovian process that iterative adds\nnoise to training images to produce white noise. Latent\ndiffusion [45] trains the diffusion model on the latent\nspace of a pretrained autoencoder instead of pixel space.\nFor text2image generation, it incorporate classifier-free\n2https://laion.ai/blog/laion-aesthetics/\nguidance [21] to further improving sample quality and text-\nimage relevance. We are motivated by the latent diffusion\nmodel and learn layered-image latent representations for\nlayered image generation.\nText-guided image editing A line of papers ([41], [12])\ntakes a pretrained CLIP [42] image-text encoder to guide\nimage manipulation. They usually measure the editing\neffect with cosine similarity between CLIP embedding\nof edited images and target texts. Style-CLIP [41] first\ninverts the input image into StyleGAN2\u2019s latent space\nand optimizes the latent vector with CLIP to find the\ntarget image. However, Style-CLIP and other GAN-based\nimage editing methods (e.g.,[12]) need to solve challenging\nGAN inversion [64], [1] problem first and do not support\nlocalized editing. Other works [3], [28] use CLIP without a\npretrained generator by performing test-time optimization.\nInspired by the recent blooming of diffusion models,\nseveral works ([2], [36], [47], [27], [25], [17], [4], [11],\n[63]) propose to utilize a diffusion model as a generator\nfor image editing. SDEdit [36] first adds noise to the input\nand then denoises the resulting images conditioned on a\ntarget. Blended diffusion [2] performs localized editing\nwith a region mask and a CLIP-guided inference algorithm\nfor DDPM. To generate personalized images with text\nprompts, Textual Inversion [11] and DreamBooth [47] fine-\ntunes the text embedding of pretrained diffusion models to\ngenerate images of personalized objects in novel scenarios.\nPrompt-to-Prompt [17] modifies the cross-attention maps\nin the inference steps to manipulate images. SINE [63]\nenables personalized image editing with a single image\nwith model-based guidance and patch-based finetuning.\nInsturctPix2pix [4] creates a supervised model for image\nediting. Imagic [25] combines latent space interpolation\nand model finetuning to edit images with a text prompt.\nDirect Inversion [9] claims that simply denoising the latent\nvector of the input image with text instruction produces\nsatisfying editing. Most of these approaches cannot con-\nstrain the region of editing, and we aim to generate explicit\nsalient masks together with the image to facilitate editing\nand layer compositing workflows.\nImage matting Image matting studies extracting the fore-\nground alpha matte \u03b1 from an image I, which is useful for\nimage composition and editing. Formally, image matting\nalgorithms solve an alpha matte \u03b1, such that for the i-th\npixel of I: Ii \u2248 \u03b1iFi + (1 \u2212 \u03b1i)Bi, where F and B are\nunknown foreground and background. We take the same\nformulation to compose the foreground and background in\nthe layered image generation.\nImage matting have been extensively studied [29],\n[57], [51], [31]. Traditional approaches [29], [16] for\nmatting rely on affinity or optimization. DIM introduced\nComposition-1k dataset, and took deep convolutional net-\nworks for image matting. Since then, many new deep learn-\ning methods for image matting have been proposed [51],\n[32], [26]. Recently, MatteFormer [39] attacked matting\nwith a modified SWIM Transformers [35]. Motivated by\nthe success of matting methods, we leverage composition\nloss [57] and Laplacian loss [23] to train our autoencoder.\nSalient object segmentation/detection (SOD) aims at\nhighlighting visually salient object regions in images [56].\nSimilar to image matting, deep architectures [65], [30],\n[22], [5] beat the performance of traditional methods [6],\n[8]. DSS [22] proposed CNN with short-cut connections\nbetween the deeper and shallower features for SOD. To\ndeal incomplete saliency, GCPANet [5] built a progressive\ncontext-aware feature aggregation module for effective\nfeature fusion in SOD. ICON [65] is a deep architecture\nthat improves the integrity of the predicted salient regions.\nHigher integrity of composition masks is vital to build our\ndataset since composition with an incomplete mask will\ntear apart in the foreground and background and produce\nmany artifacts. Therefore, we chose ICON for estimating\nmasks.\nIII.. Synthesizing High-Quality Layered Images\nWe formally define the two-layer image and introduce\nthe proposed approach for high-quality layered image\nsynthesis in this section. With the proposed approach, we\ngenerated a 57.02M high-quality layered-image dataset,\nnamed LAION-L2I, which includes 57M for training and\n20K for testing. We refer to the test set as LL2I-E in the\nrest of this paper.\nA.. Definition of Two-Layer Image\nIntuitively, a two-layer image represents an image with\na foreground and a background, as well as a mask m\nto composite the two layers. Formally, a two-layer image\nis a triplet I = (F, B, m) where F, B, and m are the\nintroduced foreground, background and mask, respectively.\nThroughout the paper, we assume they are of the same\nspatial dimension H \u00d7 W. Hence, F, B \u2208 R3\u00d7H\u00d7W and\nm \u2208 RH\u00d7W . Figure 1 displays samples from LAION-L2I\ndataset.\nB.. Overview of LAION-L2I Dataset Construction\nAs shown in recent text2image efforts, large-scale\ndatasets such as LAION-2B and LAION-5B [49] are\nessential for training diffusion models [45]. To generate\npairs of two-layer images I and text prompts, we propose\nan automatic data annotation method to create a two-layer\nimage out of each image in the LAION dataset, so that\nthey are of a large scale and paired with text prompts. In\nparticular, to cope with limited computational resources,\nas a first trial, we chose to generate layered images using\nthe LAION-Aesthetics, (short for LAION-A) subset, which\ncontains around 600 million images with high aesthetic\nquality and text relevance3.\nTo generate a two-layer image out of each image in\nLAION-A, we apply a salient object segmentation method\nto extract the foreground parts from the original images and\nthen fill the missing regions of the backgrounds using state-\nof-art image inpainting techniques. We chose salient object\nsegmentation instead of segmenting objects of specific\nsemantic categories for two purposes. First, it sets the\npromise of being able to generalize to a large-scale dataset.\nSecond, the text prompt associated with each image is\ngenerally applicable to a two-layer image that is composed\nof a salient object, background, and a mask. Meanwhile,\nas salient object detection and inpainting methods do\nnot always produce high-quality output, we trained two\nclassifiers to filter out the samples that are with bad salient\nmasks or bad inpainting results.\nC.. Extracting foreground and background parts\nWe curated a two-layer image dataset based on LAION-\nA on the resolution of 512\u00d7512. As we defined a two-layer\nimage I as a triplet of (F, B, m), we take a straightforward\nway of estimating the triplet from an existing image I such\nthat\nI \u2248 mF + (1 \u2212 m)B\n(1)\nEq (1) is under-constrained with 6 degrees of freedom for\neach pixel. We estimate them as follows.\nEstimation of F and m. In this work, we predict salient\nmask m from I using state-of-the-art salient object detec-\ntion method ICON [65]. For the foreground image F, we\nset a threshold (0.1 for our case) for m and directly copy\npixel values Ii of I at spatial locations which have mask\nvalue mi greater than the threshold.\nEstimation of B. We utilize image inpainting technique\nto acquire the background B for an image I. Concretely,\nwe first apply a dilation operation on m to obtain an\naugmented \u02dcm, which helps alleviate the errors in the\nestimation of m. Next we use the state-of-art diffusion-\nbased inpainting4 to fill the masked region and produce\nthe inpainted image B. We attempted to use the prompts\n\u201cbackground\u201d to enhance the background generation. How-\never, we did not observe quality improvement in the output.\nSo we chose to feed empty text prompt into the inpainting\nmodel.\nIn Section III-D, we will introduce how we further\nfilter the data to obtain a higher quality dataset in detail.\nTo differentiate from the filtered dataset, we name the\noriginally built dataset LAION-L2I (U), where U means\nunfiltered. Please refer to supplementary material for more\n3https://laion.ai/blog/laion-aesthetics/\n4https://huggingface.co/stabilityai/stable-diffusion-2-inpainting\n(a) Predicted \u201cbad\u201d masks\n(b) Predicted \u201cgood\u201d masks\n(c) Predicted \u201cbad\u201d inpaintings\n(d) Predicted \u201cgood\u201d inpaintings\nFig. 4. Predicted good and bad salient masks and inpaintings\nstatistics of the LAION-L2I. Some examples from LAION-\nL2I are visualized in Figure 1 (more in the supplement).\nD.. Quality filtering\nWe noticed that the mask prediction model for \u03b1 and\ninpainting model for B do not always produce desired\nresults. Specifically, as shown in Figure 14a, some of the\nmasks are incomplete or contain too many background\nelements. As shown in Figure 14c, the inpainting model\nmay introduce visual artifacts. In addition, the inpainting\nmodel tends to retrieve the objects in the missing regions.\nHowever, we expect it to only patch the background\ninstead of the foreground. Otherwise, the diffusion model\ntend to generate contents that are not relevant to the text\ndescriptions.\nTo solve the aforementioned issues, we train two clas-\nsifiers to remove the samples that are with bad saliency\nmap or inpainting quality. We first manually annotated\n5000 training samples and 1000 test samples respectively.\nSpecifically, we randomly sampled the generated two-layer\nimages from LAION-L2I (U). For salient mask quality\nlabeling, a sample is labeled as a bad sample if one of\nthe three conditions occur:\n\u2022 The object masked out is largely incomplete.\n\u2022 The mask contains regions from the backgrounds.\n\u2022 The selection of the salient object is too small\nFor inpainting quality labeling, a sample is labeled as a bad\nsample if there is an object or artifacts in the inpainted\nregion or the inpainted region is not cohesive with the\nremaining image. Typical failure cases are shown in the\nFigure 3.\nTo obtain the two quality filters as mentioned above,\nwe train two different classifiers with the same neural\nnetwork architecture and training schedules. For mask\nquality classifier, the inputs are the original images I and\ntheir corresponding masks m. The inputs to the inpainting\nquality classifier are the inpainted images. The output of\nboth classifiers are the quality labels. For implementation,\nwe replace the last layer of EfficientNet b0 [52] with a fully\nconnected layer with one single neuron which indicates\nthe probability of a sample is good or not. Then we\nonly fine-tune the last layer while freeze all other layers.\nAfter the classifiers are well-trained after 100 epochs, we\nfeed all the samples from LAION-L2I (U) into the two\nclassifiers and discard the samples with bad masks or\npoor inpainting results, which results in a filtered dataset\nLAION-L2I which contains 57.02M samples. The filtered\nresults are visualized in Figure 14b and Figure 14d. More\nstatistics and analysis of LAION-L2I and data filtering are\navailable in the supplement.\nIV.. Modeling\nA.. Text2Layer Formulation\nOur main task is to design a model which can generate a\ntwo-layer image I given a text prompt. Formally, we train\na conditional generative model to model the probability\ndistribution p\u03b8(\u00b7; \u00b7) such that\nI = (F, B, m) \u223c p\u03b8(z; T)\n(2)\nwhere T is the text prompt and \u03b8 is the parameter of the\ndistribution p.\nEquipped with LAION-L2I dataset, we are ready to\nbuild a conditional diffusion model to synthesize a two-\nlayer image I given text T. Our model architecture is\nbased on Stable Diffusion [45], which has already demon-\nstrated high-quality results in text2image generation.\nWe need to adapt Stable Diffusion to the two-layer\nimage generation task. Recall that Stable Diffusion em-\nploys a pre-trained autoencoder to project the original\nimages to a latent space in a significantly lower dimension\nbefore feeding them to a noise prediction module which\ntakes the form of a UNet [46] variant. In our scenario,\nwe found that in addition to reducing computational cost,\nthe autoencoder also plays an essential role in capturing\nthe inherent structure of two-layer image I. Therefore,\nwe designed a novel architecture for autoencoder, termed\nComposition-Aware Two-Layer Autoencoder (CaT2I-AE),\ntailored for two-layer image generation which can be\neasily generalized to multiple layers. In Section V-E, we\ndemonstrate that such a simple design is sufficient to\nproduce high-quality two-layer images.\nB.. CaT2I-AE Architecture\nIn the original SD-AE (the autoencoder used in Stable\nDiffusion), the decoder reconstructs an input image I with\nthe encoder\u2019s output latent vector z. We detail the changes\nmade in CaT2I-AE, which compresses two-layer images\ninto latent space and reconstructs them from the latent\nvectors. For the encoder, we stack F, B, and m into\na single tensor X. We keep other parts of the encoder\nintact. While a complicated encoder is possible, we find\nthis approach already achieved good generation results.\nTo obtain a better decoded result, we differentiate the\ngeneration process of image components and mask com-\nponent for the last layer by applying multiple prediction\nheads. Specifically, we remove the last layer of SD-AE\ndecoder and attach three prediction heads to predict F,\nB and m, respectively. Each prediction head is a tiny two\nlayer convolutional network of conv-bn-relu-conv. Another\nmodification we made compared to SD-AE is that we\nadded an auxiliary output branch to predict the original\nimage I. Our motivation is the supervision of the original\nimage I introduced extra information in addition to F and\nB. In Section V-E, we show that this extra supervision\nslightly improves the generation quality.\nC.. Training Objective\nOur training objective for CaT2I-AE extends the\noriginal autoencoder training objective in [45] with\nterms for masks. Given the autoencoder\u2019s reconstructed\n( \u02c6F, \u02c6B, \u02c6I, \u02c6m), the full multi-task loss L is defined as\nL = Limg( \u02c6F, \u02c6B, \u02c6I; F, B, I) + \u03bbLmask( \u02c6m, m)\n(3)\nwhere\nLimg\ndenotes\nthe\nloss\nfor\nimage\ncompo-\nnents (B, F, I), Lmask denotes the loss for mask channel,\nand \u03bb (we take 1 in all the experiments) controls the\ntradeoff of reconstruction quality between image compo-\nnents and mask component. As for the image components\nLimg, we directly use a combination of LPIPS [62], \u21131\nnorm and adversarial loss as in SD-AE. For the mask\nm, motivated by the works [60] on image matting, we\nconsider a combination of \u21131 loss, Laplacian loss [57],\nand composition loss [23] (details in the supplement).\nLmask = \u21131(m, \u02c6m) + 2\u2113comp(m) + 3\u2113lap(m, \u02c6m)\n(4)\nAfter CaT2I-AE is well trained, we utilize the same\ncross-modal attentive UNet architecture used in work [45]\nto train a DDPM on the latent space of CaT2I-AE. In\nparticular, let f\u03b8 be the UNet, and the loss function LDDPM\nis defined as\nLDDPM = Ez,\u03f5,t\nh\n\u2225\u03f5 \u2212 f\u03b8(zt, t)\u22252\n2\ni\n(5)\nwhere t is a random time step between 1 and tmax, and\n\u03f5 is an independent Gaussian noise, z is the latent vector\nproduced by CaT2I-AE, and zt is a noisy version of z.\nV.. Experiments\nIn this section, we extensively compare the proposed\ntext2layer method on the two-layer image generation task\nto several baselines on the LAION-L2I dataset introduced\nin Section III. The evaluation focuses on the image quality,\nmask quality and text-image relevance.\nA.. Baseline Methods\nWe name our full model CaT2I-AE-SD, and compare\nit with the following baseline methods on our LAION-L2I\ndataset.\n\u2022 SD-v1-4 and SD-LAION-L2I: SD refers to the Sta-\nble Diffusion model for text-to-image synthesis. We\ncompare the quality of CaT2I-AE-SD\u2019s composed\nimages I = mF + (1 \u2212 m)B with images generated\nfrom Stable Diffusion v1-45. Additionally, because\nthe LAION-L2I dataset is significantly smaller than\nthe LAION-A dataset, we trained a SD model6 for\ntext2image using the LAION-L2I dataset for fair\ncomparison. We denote these two SD models as SD-\nv1-4 and SD-LAION-L2I.\n\u2022 SD-AE-UNet and SD-AE-UNet (ft): for these two\nbaselines, we pass each component of I (a.e.,F, B, m)\ninto the SD-AE, and train a UNet-archtecture diffu-\nsion model which takes the stacked latent vectors as\ninputs. Formally, let g(\u00b7) be the SD-AE, we feed z\ndefined as below into the UNet.\nzF , zB, zm = g(F), g(B), g(m)\nz = concatch(zF , zB, zm)\n(6)\nwhere concatch(\u00b7 \u00b7 \u00b7 ) concatenates input tensors along\nthe feature channel. For SD-AE-UNet, we train the\nUNet model from scratch, while for SD-AE-UNet (ft),\nwe finetune the UNet from the SD-v1-4. By evalu-\nating CaT2I-AE-SD against these two baselines, we\nshow that it is essential to have a well-designed\nautoencoder for two-layer image generation task.\n\u2022 CaT2I-AE-SD (no-sup): this is a variant of CaT2I-AE-\nSD without the supervision branch for the original\nimage I. We show the effectiveness of introducing\nthis additional supervision branch in Section V-E.\nBecause training high-resolution diffusion models (i.e.,\n512\u00d7512) requires excessive computational resources, we\nprimarily evaluate all methods on the 256\u00d7256 resolution.\nIn particular, all the methods except SD-v1-4 target at\na resolution of 256. We also trained a CaT2I-AE-SD\non the resolution of 512, named CaT2I-AE-SD (512) to\ndemonstrate the generation quality of various resolutions.\n5https://huggingface.co/CompVis/stable-diffusion-v1-4\n6We re-used the SD-AE and trained the same UNet architecture from\nscratch for the diffusion model as SD does.\nB.. Metrics\nTo quantitatively analyze the capability of CaT2I-AE-\nSD in synthesizing two-layer images in an extensive way,\nwe take the following metrics.\nFr\u00b4echet inception distance (FID) FID is a metric used to\nevaluate the quality of images generated by a generative\nmodel [19]. It compares the distribution of generated\nimages with the distribution of real images.\nWe follow the same FID metric as done in [45], [48]\nand take FID to measure the fidelity of composed images.\nWe leverage the Clean-FID proposed in [40]7. As the FID\nscore significantly depends on the real image distribution,\nwe report FID scores on two test sets. One is LL2I-E (See\nSection III) which contains 20K test samples. The other\nis a subset from unfiltered LAION-A, which also has\n20K samples. The two sets are mutually exclusive. We\nabbreviate the latter one LA in the following experiments.\nWe introduce LA for quantifying our model\u2019s ability to\ngenerate diverse images beyond the distribution of LAION-\nL2I.\nCLIP score Previous work [18] discovered that the\nCLIP [42], a vision-language model pre-trained on 400M\nnoisy image and caption pairs, could be used for auto-\nmatic evaluation of image captioning without the need\nfor references. In particular, the cosine similarity between\nthe features of the image encoder and language encoder\ncould serve as a surrogate metric for image-text relevance.\nWe follow the same CLIP score metric as it was used\nin [10], [14] and refer to the cosine similarity as CLIP\nscore. In particular, we take the CLIP score to validate that\nimages composed by our CaT2I-AE-SD faithfully reflect\ntext prompts. We calculate the CLIP score on the LL2I-E\nsubset with Vit-L/14.\nIntersection-Over-Union (IOU). IOU is widely adopted\nto assess the performance of semantic segmentation. It\ncomputes the overlapping between a pair of prediction\nmask and ground-truth mask as a fraction.\nIn addition to the quality of composited images, we\nexpect masks m to capture meaningful regions in the\ncomposited images. To evaluate the generated masks, we\ncompute the IOU of models\u2019 predicted mask \u02c6m with two\nsets of \u201creference\u201d masks because no ground-truth masks\nare available for the generated masks given certain text\nprompts. We use ICON\u2019s [65] predictions on the composed\nimages as the first reference set, since that is how we\nestimate masks in dataset construction. For the second set,\nwe manually annotate the masks of a test set which consists\nof 1500 generated composited images for each model.\n7https://github.com/GaParmar/clean-fid\n(a) CaT2I-AE-SD\n(b) CaT2I-AE-SD (no-sup)\n(c) SD-AE-UNet\n(d) SD-AE-UNet (ft)\nFig. 5. Composited samples from CaT2I-AE-SD and baseline\nmodels for 256\u00d7256 resolution. The prompts are (1) Dogs at the\nentrance of Arco Iris Boutique and (2) Haunted Mansion Holiday\nat Disneyland Park. Each 2 \u00d7 2 block displays the composited\nimages and masks m of the corresponding models. Find more in\nthe supplement.\nC.. Implementation Details\nAll the experiments are running on four 8\u00d7A100 GPU\nmachines. For SD-AE-UNet and SD-LAION-L2I, we train\nthe UNet model for 75k steps with DDPM training. For\nSD-AE-UNet (ft), we expand the number of input channels\nand output channels for the UNet model and initialize the\nweight from SD-v1-4, we then finetune the UNet model\nfor 25k steps. CaT2I-AE-SD and CaT2I-AE-SD (no-sup)\nrequire training the autoencoder from scratch, and we train\nboth of them for 250k steps. We follow the same training\nscheme on UNet for both models as the SD-AE-UNet. To\ntrain the 512 resolution CaT2I-AE-SD (512), we resume\nfrom CaT2I-AE-SD weight and train for 75k steps with\n512 images. During inference, we sample two-layer images\nwith DDIM [50] scheme and classifier-free guidance [21].\nIn particular, the number of DDIM iterations is 50 and the\nguidance scale is 7.5.\nD.. Qualitative Results\nFigure 5 displays samples from CaT2I-AE-SD and\nbaseline models for the 256\u00d7256 results. As shown in the\nfigure, the samples from CaT2I-AE-SD clearly have better\ndetails with sharper and more accurate masks compared\nto other methods. On composited images, CaT2I-AE-SD\u2019s\nresults have fewer artifacts and distortion. On masks,\nCaT2I-AE-SD\u2019s results are more accurate, while results\nFig. 6. F, B, m components and composed images for two-layer\nimages sampled from CaT2I-AE-SD (512). Prompts are omitted.\nFrom top to down: F, b, and m. For more samples cf. the\nsupplement.\ngenerated by SD-AE-UNet and SD-AE-UNet (f) are noisy.\nFigure 6 delves into components (F, B, m) of images\ngenerated from CaT2I-AE-SD (512). Take SD samples in\nthe bottom row into account, we find CaT2I-AE-SD (512)\ngenerates samples with comparable quality as SD-v1-4. In\ncontrast to the static images generated by SD-v1-4, the\nF, B, m components coming with CaT2I-AE-SD simplify\ndownstream image editing tasks.\nE.. Quantitative Results\nThe rest of this section presents quantitative com-\nparisons on image quality, mask quality, and text-image\nrelevance.\nImage quality Here we demonstrate that CaT2I-AE-SD\nachieves superior composition quality compared to base-\nline methods. We present in Table I (in the 4th and 5th\ncolumn) FID scores of models on both test sets. Our\nfirst observation is that CaT2I-AE-SD outperforms baseline\nmodels in generating 256 \u00d7 256 two-layer images by big\nmargins, which are around 3.5 for LL2I set and 5 for\nLA set. We attribute it to the distributional misalignment\nbetween the (F, B, m) components of a two-layer image\nand a natural image used in SD. Such distributional mis-\nalignment could cause larger errors in reconstructing F,\nB, and m from the latent space of SD\u2019s autoencoder. In\ncontrast, CaT2I-AE\u2019s decoder decompresses F, B, and m\nwith separate prediction branches, making better usage of\nthe decoder\u2019s capacity.\nMeanwhile, CaT2I-AE-SD achieves a lower FID com-\nDataset\nResolution\nMethod\nFID (LL2I-E, \u2193)\nFID (LA, \u2193)\nCLIP Score (\u2191)\nLAION-L2I\n256\nCaT2I-AE-SD\n10.51\n14.83\n0.251\nCaT2I-AE-SD (no-sup)\n13.84\n19.54\n0.241\nSD-AE-UNet\n18.53\n21.96\n0.219\nSD-AE-UNet (ft)\n20.15\n24.84\n0.234\nSD-LAION-L2I\n11.80\n15.06\n0.250\nLAION-L2I\n512\nCaT2I-AE-SD (512)\n7.54\n13.01\n0.261\nLAION-2B + LAION-A\nSD-v1-4 [45]\n9.13\n9.13\n0.279\nTABLE I. FID and CLIP scores for our CaT2I-AE-SD and various baselines. FID is evaluated on LL2I-E and LA testset, and CLIP\nscore is tested on LL2I-E. The arrows indicate preferred directions for the metrics.\nModel\nIOU (ICON)\nIOU (human)\nCaT2I-AE-SD\n0.885\n0.799\nCaT2I-AE-SD (no-sup)\n0.904\n0.851\nSD-AE-UNet\n0.779\n0.766\nSD-AE-UNet (ft)\n0.670\n0.681\nTABLE II. IOU between models predicted masks and the two\nreference sets \u2013 ICON\u2019s prediction (as ICON, [65]) and human\nannotated foreground masks (as human).\nDataset\nFID\nCLIP Score\nIOU (human)\nLAION-L2I\n10.51\n0.251\n0.799\nLAION-L2I (U)\n13.82\n0.234\n0.736\nTABLE III. Metrics for CaT2I-AE-SD trained on LAION-L2I\nand a unfiltered variant of LAION-L2I with same size, LAION-\nL2I (U)\npared to SD-LAION-L2I on both test sets, which indicates\nthat the composed image quality benefits from the layered-\nimage generation training compared to the text2image\nsetting. It is possible that the mask in the layered-image\nbrought additional information that benefits the composed\nimage quality. CaT2I-AE-SD (512) achieves better results\nthan CaT2I-AE-SD, which sets the promise that training on\na higher resolution could further improve the image quality\nsignificantly. We also notice a considerable improvement\nfrom CaT2I-AE-SD (no-sup) to CaT2I-AE-SD. This val-\nidates our hypothesis that the supervision of the original\nimage I can enhance generative quality.\nLastly, we showed the results of SD-v1-4 [45], which\nwas trained on a much larger dataset, for reference. The\nFID of SD-v1-4 is not directly comparable with other\napproaches in the table, but CaT2I-AE-SD still achieves\nlower FID compared to the SD on LL2I set for the\nresolution of 512, demonstrating the success of our whole\ntwo-layer image generation pipeline. Though in the fifth\ncolumn, we find CaT2I-AE-SD has worse FID compared\nto the SD-v1-4 on a more broad LA set, we believe this\nis reasonable considering our current LAION-L2I is less\nthan 1/10 as the size of LAION-Aesthetics and less than\n1/30 as the size of LAION-2B. For future work, we will\ncreate a larger version of LAION-L2I to eliminate the gap.\nMask quality We examine the quality of the masks\nproduced by two-layer image generative models. The\nIOUs highlighted in Table II show that compared to\nother baselines, CaT2I-AE-SD and CaT2I-AE-SD (no-sup)\nproduce masks that are more accurate according to the\nreference ICON [65] salient detection model. In addition,\ntheir superior IOU on the human-annotated test set shows\nCaT2I-AE-SD\u2019s masks are better at capturing foreground\nsemantics compared to other baselines. CaT2I-AE-SD\nshows a slightly worse number than CaT2I-AE-SD (no-\nsup) indicating that the usage of the composed image in the\nautoencoder training drives better image quality but might\nhurt mask quality. That observation introduces future work\nto better balance each layer\u2019s generation.\nImage-text relevance As depicted in Table I (in the\nlast column), the proposed CaT2I-AE-SD outperforms\nall baseline models in the 256 resolution. CaT2I-AE-SD\n(512) achieves a better text-image relevance compared to\nCaT2I-AE-SD, which indicates a promising performance\nimprovement when training on a higher resolution. Simi-\nlarly, we list the SD-v1-4\u2019s results for reference, and the\nproposed CaT2I-AE-SD\u2019s result is on par with SD-v1-4\u2019s\nresults even if the SD-v1-4 was trained on a much larger\ndataset. The results suggest that the combination of our\nefforts on the dataset and model architecture ensures the\ngenerated two-layer images follow the instruction of text\nprompts.\nF.. Effectiveness of Data Filtering\nLarge-scale high-quality datasets are indispensable to\nthe success of deep neural networks in computer vision.\nBesides the quality metrics shown above, here we demon-\nstrate the effect of our data filtering strategies proposed\nin Section III-D. Illustrated in Table III, the large im-\nprovement of FID, CLIP score, and IOU for CaT2I-AE-\nSD on the LAION-L2I and its unfiltered variant LAION-\nL2I (U) demonstrates the effectiveness of the proposed\ndataset synthesis approach discussed in Section III.\nVI.. Conclusions and Discussions\nIn this paper, we proposed a layered-image generation\nproblem and scoped it to a two-layer image generation\nsetting. We created a 57.02M high-quality layered-image\ndataset and used it to build a layered-image generation\nsystem using latent diffusion models. We designed the\nevaluation metric for the layered-image generation and\ndemonstrated a benchmark in terms of image quality,\nimage-text relevance, and mask quality.\nAdditionally, the proposed method is not limited to two\nlayers, which can be applied to any fixed number of layers.\nMeanwhile, a conditional model can be developed as a\nnatural extension of this work, which potentially could\ngenerate a layer given existing layers and that can further\nenable layered image generation of an arbitrary number of\nlayers. We leave it as a future work.\nReferences\n[1] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nImage2stylegan:\nHow to embed images into the stylegan latent space?\nIn\nProceedings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2019. 3\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended dif-\nfusion for text-driven editing of natural images.\nIn Proceedings\nof\nIEEE/CVF\nConference\non\nComputer\nVision\nand\nPattern\nRecognition (CVPR), 2022. 1, 3\n[3] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and\nTali Dekel. Text2live: Text-driven layered image and video editing.\nIn Proceedings of the European Conference on Computer Vision\n(ECCV), 2022. 1, 3\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instruct-\npix2pix: Learning to follow image editing instructions.\narXiv\npreprint arXiv:2211.09800, 2022. 1, 3\n[5] Zuyao Chen, Qianqian Xu, Runmin Cong, and Qingming Huang.\nGlobal context-aware progressive aggregation network for salient\nobject detection. In Proceedings of AAAI Conference on Artificial\nIntelligence (AAAI), 2020. 3\n[6] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr,\nand Shi-Min Hu. Global contrast based salient region detection.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n2014. 3\n[7] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang\nZhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang,\nand Jie Tang.\nCogview: Mastering text-to-image generation via\ntransformers. In Proceedings of Advances in Neural Information\nProcessing Systems (NeurIPS), 2021. 2\n[8] Wolfgang Einh\u00a8auser and Peter K\u00a8onig.\nDoes luminance-contrast\ncontribute to a saliency map for overt visual attention? European\nJournal of Neuroscience, 2003. 3\n[9] Adham Elarabawy, Harish Kamath, and Samuel Denton.\nDirect\ninversion: Optimization-free text-driven real image editing with\ndiffusion models. arXiv preprint arXiv:2211.07825, 2022. 3\n[10] Kevin Frans, Lisa Soros, and Olaf Witkowski. CLIPDraw: Explor-\ning text-to-drawing synthesis through language-image encoders. In\nProceedings of Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 6\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim\nBermano, Gal Chechik, and Daniel Cohen-or. An image is worth\none word: Personalizing text-to-image generation using textual\ninversion. In Proceedings of International Conference on Learning\nRepresentations (ICLR), 2023. 1, 3\n[12] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel\nCohen-Or. Stylegan-nada: Clip-guided domain adaptation of image\ngenerators. arXiv preprint arXiv:2108.00946, 2021. 3\n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial nets. In Proceedings of Advances in Neural\nInformation Processing Systems (NeurIPS), 2014. 2\n[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang,\nDongdong Chen, Lu Yuan, and Baining Guo.\nVector quantized\ndiffusion model for text-to-image synthesis. ArXiv e-prints, 2021.\n6\n[15] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Girshick.\nMask r-cnn. In Proceedings of IEEE International Conference on\nComputer Vision (ICCV), 2017. 1\n[16] Kaiming He, Christoph Rhemann, Carsten Rother, Xiaoou Tang,\nand Jian Sun. A global sampling method for alpha matting. In\nProceedings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2011. 3\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael\nPritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with\ncross attention control. arXiv preprint arXiv:2208.01626, 2022. 1,\n3\n[18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and\nYejin Choi.\nCLIPScore: A reference-free evaluation metric for\nimage captioning.\nIn Proceedings of Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2021. 6\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard\nNessler, and Sepp Hochreiter. Gans trained by a two time-scale\nupdate rule converge to a local nash equilibrium. In Proceedings\nof Advances in Neural Information Processing Systems (NeurIPS),\n2017. 6\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion\nprobabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin, editors, Proceedings of Advances in Neural\nInformation Processing Systems (NeurIPS), 2020. 2\n[21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022. 3, 7\n[22] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji, Zhuowen\nTu, and Philip HS Torr. Deeply supervised salient object detection\nwith short connections. In Proceedings of IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2017. 3\n[23] Qiqi Hou and Feng Liu.\nContext-aware image matting for si-\nmultaneous foreground and alpha estimation.\nIn Proceedings\nof\nIEEE/CVF\nConference\non\nComputer\nVision\nand\nPattern\nRecognition (CVPR), 2019. 3, 5, 16\n[24] Wentao Jiang, Ning Xu, Jiayun Wang, Chen Gao, Jing Shi, Zhe\nLin, and Si Liu. Language-guided global image editing via cross-\nmodal cyclic mechanism. In Proceedings of IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2021. 1\n[25] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang,\nTali Dekel, Inbar Mosseri, and Michal Irani.\nImagic: Text-\nbased real image editing with diffusion models.\narXiv preprint\narXiv:2210.09276, 2022. 1, 3\n[26] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Rynson W.H.\nLau. Modnet: Real-time trimap-free portrait matting via objective\ndecomposition. In Proceedings of AAAI Conference on Artificial\nIntelligence (AAAI), 2022. 3\n[27] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip:\nText-guided diffusion models for robust image manipulation.\nIn\nProceedings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2022. 1, 3\n[28] Gihyun Kwon and Jong Chul Ye. Clipstyler: Image style trans-\nfer with a single text condition.\nIn Proceedings of IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n2022. 3\n[29] Anat Levin, Dani Lischinski, and Yair Weiss.\nA closed-form\nsolution to natural image matting. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2008. 1, 3\n[30] Guanbin Li and Yizhou Yu.\nVisual saliency based on multi-\nscale deep features. In Proceedings of IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2015. 3\n[31] Jizhizi Li, Jing Zhang, Stephen J. Maybank, and Dacheng Tao.\nBridging composite and real: Towards end-to-end deep image\nmatting. International Journal of Computer Vision, 2022. 3\n[32] Yaoyi Li and Hongtao Lu.\nNatural image matting via guided\ncontextual attention.\nIn Proceedings of AAAI Conference on\nArtificial Intelligence (AAAI), 2020. 3, 16\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Doll\u00b4ar, and C. Lawrence Zitnick.\nMicrosoft coco: Common objects in context. In David Fleet, Tomas\nPajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Proceedings of\nthe European Conference on Computer Vision (ECCV), 2014. 11,\n13, 14\n[34] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio\nTorralba, and Sanja Fidler.\nEditgan: High-precision semantic\nimage editing. In Proceedings of Advances in Neural Information\nProcessing Systems (NeurIPS), 2021. 1\n[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Baining Guo.\nSwin transformer: Hierarchical\nvision transformer using shifted windows. In Proceedings of IEEE\nInternational Conference on Computer Vision (ICCV), 2021. 3\n[36] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu,\nJun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis\nand editing with stochastic differential equations. In Proceedings\nof International Conference on Learning Representations (ICLR),\n2022. 1, 3\n[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising\ndiffusion probabilistic models. In Proceedings of IEEE Conference\non Machine Learning (ICML), 2021. 2\n[38] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark\nChen. GLIDE: towards photorealistic image generation and edit-\ning with text-guided diffusion models.\nIn Proceedings of IEEE\nConference on Machine Learning (ICML), 2022. 2\n[39] GyuTae Park, SungJoon Son, JaeYoung Yoo, SeHo Kim, and Nojun\nKwak. Matteformer: Transformer-based image matting via prior-\ntokens.\nIn Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 3\n[40] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn aliased\nresizing and surprising subtleties in gan evaluation. In Proceedings\nof\nIEEE/CVF\nConference\non\nComputer\nVision\nand\nPattern\nRecognition (CVPR), 2022. 6\n[41] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and\nDani Lischinski.\nStyleclip: Text-driven manipulation of stylegan\nimagery. In Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2021. 3\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings of IEEE\nConference on Machine Learning (ICML), 2021. 3, 6, 16\n[43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and\nMark Chen.\nHierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 2\n[44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea\nVoss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot\ntext-to-image generation. In Proceedings of IEEE Conference on\nMachine Learning (ICML), 2021. 2\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00a8orn Ommer.\nHigh-resolution image synthesis with\nlatent diffusion models. In Proceedings of IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2022. 1, 2,\n3, 5, 6, 8, 16\n[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Con-\nvolutional networks for biomedical image segmentation. In Medical\nImage Computing and Computer-Assisted Intervention\u2013MICCAI\n2015, 2015. 1, 5\n[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael\nRubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-\nimage diffusion models for subject-driven generation. arXiv preprint\narxiv:2208.12242, 2022. 1, 3\n[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang,\nEmily\nDenton,\nSeyed\nKamyar\nSeyed\nGhasemipour,\nBurcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language\nunderstanding. arXiv preprint arXiv:2205.11487, 2022. 1, 2, 6\n[49] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W\nGordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush\nKatta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert\nKaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale\ndataset for training next generation image-text models. In Neural\nInformation Processing Systems: Datasets and Benchmarks Track,\n2022. 2, 3, 11\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising dif-\nfusion implicit models. In Proceedings of International Conference\non Learning Representations (ICLR), 2021. 2, 7\n[51] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Semantic image\nmatting. In Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2021. 3\n[52] Mingxing Tan and Quoc Le.\nEfficientnet: Rethinking model\nscaling for convolutional neural networks. In Proceedings of IEEE\nConference on Machine Learning (ICML), 2019. 5\n[53] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural\ndiscrete representation learning. In I. Guyon, U. Von Luxburg, S.\nBengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\neditors, Proceedings of Advances in Neural Information Processing\nSystems (NeurIPS), 2017. 2\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. In Proceedings of Advances in Neural\nInformation Processing Systems (NeurIPS), 2017. 2\n[55] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.\nYOLOv7: Trainable bag-of-freebies sets new state-of-the-art for\nreal-time object detectors. arXiv preprint arXiv:2207.02696, 2022.\n14\n[56] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin\nLing, and Ruigang Yang.\nSalient object detection in the deep\nlearning era: An in-depth survey.\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2021. 3\n[57] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang.\nDeep\nimage matting.\nIn Proceedings of IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\n1, 3,\n5, 16\n[58] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,\nXiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to\nimage generation with attentional generative adversarial networks.\nIn Proceedings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2018. 2\n[59] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregressive\nmodels for content-rich text-to-image generation.\narXiv preprint\narXiv:2206.10789, 2022. 2\n[60] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin,\nNing Xu, Yutong Bai, and Alan Yuille.\nMask guided matting\nvia progressive refinement network. In Proceedings of IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n2021. 5, 16\n[61] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\nYinfei Yang.\nCross-modal contrastive learning for text-to-image\ngeneration. In Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2021. 2\n[62] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and\nOliver Wang. The unreasonable effectiveness of deep features as\na perceptual metric. In Proceedings of IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2018. 5\n[63] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and\nJian Ren. Sine: Single image editing with text-to-image diffusion\nmodels. arXiv preprint arXiv:2212.04489, 2022. 1, 3\n[64] Jun-Yan Zhu, Philipp Kr\u00a8ahenb\u00a8uhl, Eli Shechtman, and Alexei A\nEfros. Generative visual manipulation on the natural image man-\nifold. In Proceedings of the European Conference on Computer\nVision (ECCV), 2016. 3\n[65] Mingchen Zhuge, Deng-Ping Fan, Nian Liu, Dingwen Zhang, Dong\nXu, and Ling Shao. Salient object detection via integrity learning.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n2022. 3, 4, 6, 8, 14\nAppendix I: More Results\nIn Section 5.4, Figures 5 and 6 illustrate the powerful\ngenerative ability of CaT2I-AE-SD. We visualize more\nsamples in this section.\nWhite Goat \nBeside Fence\nChuck Klosterman \nSees the Future, \nBlurrily\nscituate light   \nscituate lighthouse \nfrom the parking lot\nMen Nike Air Max \n90 Ul Tra Essential \nRunning Shoes \nAAA 331\ncomposed\nmask\nFig. 7. More 256 samples for CaT2I-AE-SD on LAION-L2I test\nset. Prompts are displayed at the top.\nEmily Ham and \nQueenie Zeng at \nSTEM Fellowship \nIndicium Competition\nIndian Art - \nAmrita Sher-Gil - \nSelf Portrait III -\n Canvas Prints\nclick here to view \nlarger image of \nAiredale Terrier\nLista Swivel Stool\ncomposed\nmask\nFig. 8. Some failed generations from CaT2I-AE-SD. Prompts are\ndisplayed at the top.\nA.. More 256 samples\nWe show more 256 \u00d7 256 two-layer images of our\nCaT2I-AE-SD and baseline methods on LAION-L2I test\nset in Figures 7 and 9.\nB.. CaT2I-AE-SD (512) samples\nFigure 10 provides samples of CaT2I-AE-SD (512) with\nprompts from test set of LAION-L2I. In addition, Figure 11\ndisplays samples of CaT2I-AE-SD (512) with prompts\nfrom more commonly used MSCOCO dataset [33].\nC.. Failure cases\nWe present some failure cases of CaT2I-AE-SD in Fig-\nure 8. We noticed that CaT2I-AE-SD sometimes predicts\nobjects with the wrong geometry (faces in the first column)\nor structures (the dog in the third column). Besides, some\nmasks (the second and fourth columns) are inaccurate.\ncomposed\nmask\nWhite Goat \nBeside Fence\nChuck Klosterman \nSees the Future, \nBlurrily\nscituate light   \nscituate lighthouse \nfrom the parking lot\nMen Nike Air Max \n90 Ul Tra Essential \nRunning Shoes \nAAA 331\n(a) CaT2I-AE-SD (no-sup)\ncomposed\nmask\n(b) SD-AE-UNet\ncomposed\nmask\n(c) SD-AE-UNet (ft)\nFig. 9. More 256 samples for baseline methods on LAION-L2I\ntest set. Prompts are displayed at the top.\nAppendix II: Dataset\nD.. Data filtering\nIn Section 3.4, we discussed the filtering of low-quality\nmasks and inpaintings to ensure the final quality of our\nLAION-L2I dataset. More examples of before and after\ndata filtering are visualized in Figure 14.\nE.. More examples\nFigures 12 and 13 depict ten more two-layer images\nfrom our 57.02M LAION-L2I dataset.\nF.. Analysis and statistics\nAs a complement to Section 3, we present sev-\neral quantitative analyses of LAION-L2I and LAION-\nAesthetics (short for LAION-A8 [49]) here. The analysis\nfocuses on the complexity of images, masks, and text\nprompts. Plus,we also compare the text-image relevance\nof LAION-L2I and LAION-A.\n8https://laion.ai/blog/laion-aesthetics/\nforeground\nbackground\nmask\nimage\nBasil in the plate. Flat lay, top view\nWonderful and beautiful underwater world with corals and ...\nMark Hamill on the set of Star Wars Episode IV: A New Hope, ...\nSmooth leather boots Black Goots\nJuniors Tasha Kalra and Sophie Tom stand in front of Rowell Hall\nFig. 10. More CaT2I-AE-SD (512) synthesized two-layer images. The prompts above images are from LAION-L2I test set.\nforeground\nbackground\nmask\nimage\nA couple of kittens standing around metal bowls on a tray.\nA red motorcycle is parked outside of a cafe.\nA woman that is standing in the water next to a surfboard.\nA close up photo of a brown bear\nA kite flying in the air above the ocean\nFig. 11. More CaT2I-AE-SD (512) synthesized two-layer images. The prompts above images are from MSCOCO 2014 [33] validation\nset.\nforeground\nbackground\nmask\nimage\n1970 Plymouth AAR Cuda For Sale ...\nRear View Of The Dog Jumping For The Ball\nPinterest the world\u2019s catalog of ideas for Prefabricated garage ...\nImportant Points a Checklist for Funeral Planning Must ...\niceberg_near_triton_island_green_bay_central\nFig. 12. Samples of two-layer images from our LAION-L2I\ndataset. Prompts are on the top of images.\nSince the magnitude of both datasets are large, we\nsample 100K examples randomly from each dataset for\nfollowing analyses.\nObjects. We apply Yolov7 [55], a high-performance\nobject detector trained on MSCOCO [33] dataset, on\nboth datasets and obtain object categories statistics (in\nFigure 15) and the number of objects per image statis-\ntics (in Figure 16). Though MSCOCO has limited object\ncategories and fewer diverse images compared to LAION-\n5B. We argue that the above analysis with Yolov7 partially\nreflects that images retained in LAION-L2I have similar\ncomplexity to those of LAION-A in terms of \u201cobjects.\u201d\nWe observe that both distributions of number of objects\nper category and number of objects per image on our\nLAION-L2I dataset and MSCOCO are similar. For Fig-\nure 15, only a few categories have a noticeable difference\nin terms of the number of objects. For Figure 16, LAION-\nL2I has slightly more images with two objects compared\nwith LAION-A which has more images of zero MSCOCO\nobjects.\nMask.\nIn\nconstructing\nLAION-L2I,\nwe\nleverage\nforeground\nbackground\nmask\nimage\nHow professional Degree holders are taking the ...\nGolf Tee Bobbin Holders\nWaterproof Outdoor Solar Wall Light with 56 LED\nforest backlighted by golden sunlight before sunset ...\nnorthern-cardinal-male-kim-smith\nFig. 13. More samples of two-layer images from our LAION-L2I\ndataset. Prompts are on the top of images.\npercentile (p%)\nlength (LAION-A)\nlength (LAION-L2I)\n1\n1\n1\n10\n3\n3\n25\n5\n5\n50\n7\n7\n75\n10\n10\n90\n15\n15\n99\n36\n36\nTABLE IV. Percentiles of the number of tokens in text prompts\nof LAION-A and dataset. Stopwords, symbols, and punctuation\nare removed for counting.\nICON [65] to estimate masks m from images in LAION-A.\nAgain, we compare statistical differences between LAION-\nL2I and LAION-A. We first study the area ratio of salient\nregions in images under various binary thresholds. As\nillustrated in Figure 17, the distributions of the area ratio\nof masks are similar for LAION-L2I and LAION-A across\nall the thresholds. Our second study concerns the num-\nber of connected components for masks, which indicates\nscattering patterns of objects and the noise level of masks.\nStill, as shown in Figure 18, the distributions of the two\ndatasets are almost the same.\n(a) Predicted \u201cbad\u201d masks\n(b) Predicted \u201cgood\u201d masks\n(c) Predicted \u201cbad\u201d inpaintings\n(d) Predicted \u201cgood\u201d inpaintings\nFig. 14. Predicted good and bad salient masks and inpaintings\n0\n10\n20\n30\n40\n50\n60\n70\n2\n5\n100\n2\n5\n1000\n2\n5\n10k\n2\n5\n100k\ncategory\ncount\n(a) LAION-A\n0\n10\n20\n30\n40\n50\n60\n70\n2\n5\n100\n2\n5\n1000\n2\n5\n10k\n2\n5\n100k\ncategory\ncount\n(b) LAION-L2I\nFig. 15. Number of objects per MSCOCO 80 category in LAION-\nA (a) and LAION-L2I (b).\n0\n2\n4\n6\n8\n10\n0\n5k\n10k\n15k\n20k\n25k\n30k\n# objects per image\ncount\n(a) LAION-A\n0\n2\n4\n6\n8\n10\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n# objects per image\ncount\n(b) LAION-L2I\nFig. 16. Number of objects per image in LAION-A (a) and\nLAION-L2I (b).\ndataset\nCLIP score\nLAION-A\n0.278 \u00b1 0.037\nLAION-L2I\n0.273 \u00b1 0.034\nTABLE V. CLIP scores for pairs of image and prompts from\nLAION-A and LAION-L2I.\nPrompt. For the text prompts, we compare the lengths\nand the word frequency on LAION-L2I and LAION-A.\nTable IV displays caption lengths at different percentiles.\nIt is clear that both datasets have prompts with similar\nlengths. Furthermore, to gain a word-level understanding,\nwe calculate the Pearson correlation coefficient for fre-\nquencies of the top 5K words between these two datasets,\nthe value is 0.930, suggesting LAION-L2I roughly repre-\nsents a uniformly distributed subset of LAION-A. Finally,\n0\n0.5\n1\n1\n2\n5\n10\n2\n5\n100\n2\n5\n1000\n2\n5\n10k\n0\n0.5\n1\n0\n0.5\n1\n0\n0.5\n1\narea ratio\narea ratio\narea ratio\narea ratio\ncount\nthreshold=0.3\nthreshold=0.5\nthreshold=0.7\nthreshold=0.9\n(a) LAION-A\n0\n0.5\n1\n1\n2\n5\n10\n2\n5\n100\n2\n5\n1000\n2\n5\n10k\n0\n0.5\n1\n0\n0.5\n1\n0\n0.5\n1\narea ratio\narea ratio\narea ratio\narea ratio\ncount\nthreshold=0.3\nthreshold=0.5\nthreshold=0.7\nthreshold=0.9\n(b) LAION-L2I\nFig. 17. Histograms of area of masks (normalized to [0, 1]) pre-\ndicted by ICON. We take four different thresholds for binarizing\nmasks.\n10\n20\n30\n40\n1\n2\n5\n10\n2\n5\n100\n2\n5\n1000\n2\n5\n10k\n2\n5\n10\n20\n30\n40\n# connected components\n# connected components\ncount\nthreshold=0.5\nthreshold=0.9\n(a) LAION-A\n10\n20\n30\n40\n1\n2\n5\n10\n2\n5\n100\n2\n5\n1000\n2\n5\n10k\n2\n5\n10\n20\n30\n40\n# connected components\n# connected components\ncount\nthreshold=0.5\nthreshold=0.9\n(b) LAION-L2I\nFig. 18. Histograms of number of 8-way connected components\nin masks predicted by ICON. We take two different thresholds\nfor binarizing masks. We filter out connected components whose\nareas are smaller than 100.\nwe quantify the image-text relevance with CLIP score [42]\nas we have done in Section 4. We notice from Table V that\nthe two datasets have similar CLIP scores.\nAppendix III: Implementation Details\nWe detail the loss terms used in Section 4.3 for training\nour CaT2I-AE.\nImage loss. For the supervision of image components\n(F, B, I) , we use the same loss terms as used in Latent\nDiffusion [45]. The loss terms are separately applied to\neach of F, B, I with equal weights.\nInspired by literature of image matting [57], [60], [32],\n[23]. We enforce composition loss [57] and Laplacian\nloss [23] and for the CaT2I-AE decoder\u2019s predicted mask\n\u02c6m and the ground-truth mask m.\nComposition loss. Let C be the composed image of\n(F, B, m), in other word, C = mF + (1 \u2212 m)B. Then the\ncomposition loss [57] is defined as\n\u2113comp =\nX\ni\nq\n(Ci \u2212 \u02c6Ci)2 + \u03f52\n(7)\nwhere i run through all the pixels, \u02c6C = \u02c6mF + (1 \u2212 \u02c6mB)\nis the composed image with decoder\u2019s predicted mask \u02c6m,\nand \u03f5 set to 10\u22126 for numerical stability.\nLaplacian loss. Laplacian loss [23] can be used to\nmeasure the difference between the predicted mask \u02c6m with\nthe ground-truth mask m. Specifically,\n\u2113lap =\n3\nX\nj=1\n2j\u22121\u2225\u03d5j( \u02c6m) \u2212 \u03d5j(m)\u22251\n(8)\nwhere \u03d5j(m) denotes the j-th level of Laplacian pyramid\nof the mask.\n"
  },
  {
    "title": "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI",
    "link": "https://arxiv.org/pdf/2307.10172.pdf",
    "upvote": "11",
    "text": "DialogStudio: Towards Richest and Most Diverse Unified Dataset\nCollection for Conversational AI\nJianguo Zhang\u22171, Kun Qian\u22172, Zhiwei Liu1, Shelby Heinecke1, Rui Meng1\nYe Liu1, Zhou Yu2, Huan Wang1, Silvio Savarese1, Caiming Xiong1\n1 Salesforce AI\n2 Columbia University\njianguozhang@salesforce.com, kq2157@columbia.edu\nAbstract\nDespite advancements in conversational AI,\nlanguage models encounter challenges to han-\ndle diverse conversational tasks, and exist-\ning dialogue dataset collections often lack di-\nversity and comprehensiveness.\nTo tackle\nthese issues, we introduce DialogStudio: the\nlargest and most diverse collection of di-\nalogue datasets, unified under a consistent\nformat while preserving their original infor-\nmation.\nOur collection encompasses data\nfrom open-domain dialogues, task-oriented di-\nalogues, natural language understanding, con-\nversational recommendation, dialogue sum-\nmarization,\nand knowledge-grounded dia-\nlogues, making it an incredibly rich and\ndiverse resource for dialogue research and\nmodel training. To further enhance the utility\nof DialogStudio, we identify the licenses for\neach dataset, design external knowledge and\ndomain-aware prompts for selected dialogues\nto facilitate instruction-aware fine-tuning. Fur-\nthermore, we develop conversational AI mod-\nels using the dataset collection, and our exper-\niments in both zero-shot and few-shot learning\nscenarios demonstrate the superiority of Di-\nalogStudio. To improve transparency and sup-\nport dataset and task-based research, as well\nas language model pre-training, all datasets, li-\ncenses, codes, and models associated with Di-\nalogStudio are made publicly accessible1.\n1\nIntroduction\nRecent years have seen remarkable progress in\nConversational AI, primarily driven by the ad-\nvent of approaches and language models (Shus-\nter et al., 2022; Zhang et al., 2023; Longpre et al.,\n2023; Touvron et al., 2023). Despite the advance-\nments, these models could fall short when han-\ndling various tasks in a conversation due to the\n\u2217 Core contributors. Work completed during Kun\u2019s in-\nternship at Salesforce. Zhiwei is also a major contributor.\n1https://github.com/salesforce/\nDialogStudio\nlack of comprehensive and diverse training data.\nCurrent dialogue datasets (Lin et al., 2021; Asri\net al., 2017) are typically limited in size and task-\nspecific, which thus results in suboptimal ability\nin task-oriented model performance. Additionally,\nthe lack of dataset standardization impedes model\ngeneralizability.\nA few recent works (Gupta et al., 2022; Long-\npre et al., 2023; Ding et al., 2023) have intro-\nduced a large collection of datasets, which in-\ncludes diverse tasks based on public datasets. For\ninstance, FlanT5 (Longpre et al., 2023) presents\nthe flan collections with a wide array of datasets\nand tasks. Despite this breadth, the coverage of di-\nalogue datasets within the Flan collection remains\nnotably sparse, featuring only about ten datasets.\nAlthough OPT (Iyer et al., 2022) have incorpo-\nrated collections with several dialogue datasets,\nthese collections remain inaccessible to the public.\nIn contract, efforts like InstructDial (Gupta et al.,\n2022) and ParlAI (Miller et al., 2017) consist of\nmore dialogue datasets, but they lack diversity and\ncomprehensiveness. For instance, ParlAI mainly\nincludes open-domain dialogue datasets, which\nare exclusively accessible through their platform.\nOther collections (Gupta et al., 2022; Kim et al.,\n2022a; Ding et al., 2023; Dubois et al., 2023) of-\nten distill single dataset from ChatGPT or pro-\ncess datasets into a sequence-to-sequence format\nto support language model training, featuring only\ninput-output pairs such as dialogue context and\nsystem response. However, previous collections\noften overlook other crucial dialogue information,\nconstraining their utility for research on individual\ndatasets, tasks, and broader applications.\nTo overcome the aforementioned challenges,\nwe introduce DialogStudio, the most compre-\nhensive and diverse collection of publicly avail-\nable dialogue datasets, unified under a consis-\ntent format. By aggregating dialogues from vari-\nous sources, DialogStudio promotes holistic anal-\narXiv:2307.10172v3  [cs.CL]  5 Feb 2024\n(a) Dataset Distribution\n(b) Domain Coverage of TOD\nFigure 1: (a) is the distribution of all datasets in DialogStudio. The outer and inner circle list names of datasets and\nthe associated categories, respectively. (b) illustrates covered domains of Task-Oriented Dialogues in DialogStudio.\nysis and the development of models adaptable\nto a variety of conversational scenarios.\nThe\ncollection spans an extensive range of domains,\naspects, and tasks, and it is inclusive of sev-\neral categories: Open-Domain Dialogues, Task-\nOriented Dialogues, Natural Language Under-\nstanding, Conversational Recommendation, Dia-\nlogue Summarization, and Knowledge-Grounded\nDialogues. Thus, it can provide support for re-\nsearch in both individual dialogue tasks and large-\nscale language pre-training.\nDialogStudio stands out not only for its compre-\nhensive coverage but also for its accessibility. It\noffers easy access with a unified format and doc-\numents. A straightforward load dataset() com-\nmand through HuggingFace allows users to seam-\nlessly interact with the collection, and we have in-\ncluded documentation for each dataset to enhance\nusability. We anticipate that this collection will\nenable comprehensive and standardized training\nand evaluations of dialogue models, fostering fair\ncomparisons and propelling further advancements\nin Conversational AI.\nFurthermore, we identify dialogue domains, de-\nsign external knowledge for available dialogues\nand create tailored prompts for selected datasets\naccordingly. Leveraging these datasets from Di-\nalogStudio, we have constructed instruction-aware\nmodels, with capacities ranging from 770M to\n3B parameters. These models have the ability to\nhandle various external knowledge and are adept\nat both response generation and general tasks,\ndemonstrating the benefits of DialogStudio. The\nmain contributions of this paper are as follows:\n\u2022 We introduce DialogStudio, a meticulously cu-\nrated collection of more than 80 dialogue\ndatasets. These datasets are unified under a con-\nsistent format while retaining their original in-\nformation.\nWe integrate external knowledge,\nincorporate domain-aware prompts and identify\ndataset licenses, making DialogStudio an excep-\ntionally rich and diverse resource for dialogue\nresearch and model training.\n\u2022 We have made our datasets publicly available to\nenhance transparency and support research ef-\nforts.\nAdditionally, we are committed to im-\nproving DialogStudio\u2019s usability and will persist\nin our efforts to refine it, ensuring an optimal\nuser experience.\n\u2022 We train conversational AI models based on Di-\nalogStudio, and these models have demonstrated\nsuperior performance over strong baselines in\nboth zero-shot and few-shot learning scenarios.\n2\nData analysis\n2.1\nData Visualization\nThe dialogue datasets are compartmentalized\ninto several categories: Open-Domain Dialogues,\n1\n2\n3\n4\n5\n103\n104\n105\n# of samples\nOverall\n(a) Overall, \u00b5 = 4.14\n1\n2\n3\n4\n5\n101\n102\n103\n104\n105\n# of samples\nUnderstanding\n(b) Understanding, \u00b5 = 4.39\n1\n2\n3\n4\n5\n102\n103\n104\n105\n# of samples\nRelevance\n(c) Relevance, \u00b5 = 4.80\n1\n2\n3\n4\n5\n103\n104\n105\n# of samples\nCorrectness\n(d) Correctness, \u00b5 = 4.38\n1\n2\n3\n4\n5\n103\n104\n105\n# of samples\nCoherence\n(e) Coherence, \u00b5 = 4.47\n1\n2\n3\n4\n5\n103\n104\n105\n# of samples\nCompleteness\n(f) Completeness, \u00b5 = 4.16\nFigure 2: The score distribution for the dialogue quality.\nTask-Oriented Dialogues (TOD), Natural Lan-\nguage Understanding Dialogues (NLU), Conver-\nsational Recommendation (Conv-Rec), Dialogue\nSummarization\n(Dial-Sum),\nand\nKnowledge-\nGrounded Dialogues (KG-Dial).\nFigure 1a\npresents an overview of DialogStudio\u2019s dataset\ncategories. Note that the category boundaries are\nfuzzy as some datasets span multiple categories.\nFor instance, SalesBot (Chiu et al., 2022) contains\nboth\ncasual\nand\ntask-oriented\nconversations.\nAnalogously, MultiWOZ (Budzianowski et al.,\n2018; Zang et al., 2020), a task-oriented dia-\nlogue corpus, incorporates knowledge bases and\ndialogue acts to enhance knowledge-grounded\ngeneration.\nAdditionally, DialogStudio demon-\nstrates its diversity by covering a wide range of\ndomains, part of which is shown in Figure 1b.\n2.2\nData Quality Investigation\nDue to the existence of noise in dialogue, we\ndevelop a simple yet effective way to verify the\nquality of the datasets. Specifically, we employ\nChatGPT (GPT-3.5-turbo) to evaluate the quality\nof system responses based on severall perspec-\ntives (Mehri et al., 2022; Kim et al., 2022a), i.e.,\nUnderstanding, Relevance, Correctness, Coher-\nence, Completeness and Overall quality. Under-\nstanding assesses whether the model\u2019s responses\naccurately reflect the meaning and intent of the\nuser\u2019s inputs. Relevance demonstrates whether the\ngenerated response should be directly related and\nappropriate to the preceding user input and the\ncontext of the conversation. Coherence measures\nthe logical consistency of the model\u2019s responses\nwithin the context of the conversation. Complete-\nness refers to whether the system\u2019s responses fully\naddress the user\u2019s queries or tasks. Overall quality\ncomprehensively rates the quality of dialogue. All\nscores are in the range of 1-5, and higher scores\nshould only be given to truly exceptional exam-\nples. We delicately design the prompt and ask the\nChatGPT model to strictly rate the score.\nSince there are a lot of datasets in DialogStu-\ndio, we randomly select 33 multi-turn dialogue\ndatasets and evaluate all the training dialogues of\neach dataset. To harmonize ChatGPT and human\nratings, we take a random sample of 50 training di-\nalogues from each dataset. These were then rated\nby three expert researchers using the five specified\ncriteria. Post-alignment of ChatGPT and human\nevaluations, we view dialogues with a score above\n3 as being of high quality. Figure 2 illustrates dis-\ntributions of those scores. We also reveal the aver-\nage score as the \u00b5 in each sub-caption. In general,\nthe dialogues show high qualities regarding to the\nindividual criteria and the overall quality.\n3\nDatasets Unification and Access\nWe collect and process a wide range of datasets,\ninvolving different domains, types, and tasks.\nSince these datasets originally contain various in-\nformation and format, we propose a unification\nstrategy to process all the datasets such that they\ncan be loaded in the same data loader.\n3.1\nUnification\nBefore unifying the format of those datasets, we\nfixed several issues as follows: 1) we remove those\ndialogues labeled as multi-turn dialogues, but ac-\ntually with only one turn and miss either user utter-\nance or system utterance. 2) We manually check\nthe individual dialogues.\nIf one dialogue con-\ntains one or more empty user or system utterances,\nwe fill utterances based on corresponding dialogue\ncontexts, dialogue acts, and dialogue information.\nIn total, less than 0.5% of dialogues had these is-\nsues. To support research interest on individual\ndatasets, we have flagged and rectified these prob-\nlematic dialogues.\nAdditionally, we recognize the success of in-\nstruction tuning for dialogue models and thus we\nmanually pre-define five different prompt tem-\nplates for multi-turn dialogue datasets, such as\nThis is a bot helping users to {Task Domain}.\nGiven the dialogue context and external database,\nplease generate a relevant system response for the\nuser. The {Task Domain} is associated with the\ndialogue domain and we manually create a cor-\nresponding description.\nFor example, if a dia-\nlogue is of domain travel, we set {Task Domain}\nas book a trip. A concrete example of the prompt\nis demonstrated in Figure 3.\nMoreover, many\ndatasets lack a direct mapping between dialogues\nand their domain information. To address this, we\ndetermine the domain of each dialogue using its\nintent, schema, APIs, and associated databases.\nNext, we construct a uniform JSON dictionary\nformat to store all relevant information of each\ndialogue as illustrated in Figure 3.\nCompared\nwith existing works, DialogStudio covers more di-\nalogue information and is easier to retrieve the\ninformation for arbitrary dialogue-related tasks.\nConcretely, we include all dialogue-related infor-\nmation, such as the dialogue ID, data split label,\ndomain, task, and content. Additionally, we iden-\ntify the external knowledge, dialogue state track-\ning (DST) knowledge, and intent knowledge in the\ndialogue, which are the most beneficial knowledge\nfor a dialogue.\nRegarding external knowledge, we construct it\nbased on information such as databases and dia-\nlogue acts. Since each dialogue dataset focuses\non specific tasks or domains and has a different\ndatabase and annotation schema, we unify such in-\nformation into external knowledge. For example,\nif the user is looking for a hotel and asking for its\naddress, the system response should be based on\nboth the search results from the database and the\ndialogue context. To simulate the realistic situa-\ntion and avoid directly providing the model with\nthe ground truth resulting hotel, we also randomly\nsample four other candidate results and mix them\nwith the ground truth result. All information is\nflattened and converted into a string as external\nknowledge.\nTo complete tasks and generate coherent re-\nsponses, a dialogue system needs to track users\u2019\nrequirements for the task. Those requirements are\nusually represented as dialogue states. For exam-\nple, regarding the hotel booking task, a dialogue\nsystem needs to extract information such as price\nrange and locations to enable searching hotels in\nthe database. The type of dialogue states varies\nacross different tasks and datasets.\nAs such, it\nis hard for dialogue systems to predict the val-\nues of those dialogue states if unknowing the spe-\ncific dialogue states the task covers. Therefore, we\npropose to insert the schema, consisting of pre-\ndefined dialogue state types and values for each\ntask, into the input sequence.\nFor datasets like\nSGD (Rastogi et al., 2020), which already pro-\nvides annotation schema, we directly convert the\ndictionary-structured schema into a string. For the\nrest datasets that have no such schema file, we it-\nerate over all dialogues and collect potential state\nannotations to construct a schema. We provide do-\nmains, slot types, and slot values in the schema\nstring. For those categorized dialogue slots like\n\u201dhotel star-ratings\u201d, which have a fixed number of\ncandidate values, we provide all possible values.\nFor others that have unlimited possible values, e.g.\n\u201dstay night\u201d, we randomly sample ten values, such\nthat a model can learn what slot values are relevant\nto these slot types. We put the turn-level ground-\ntruth DST information in \u201ddst\u201d, and the general\nDST information under \u201ddst knowledge\u201d, as pre-\nsented in Figure 3.\nAnalogously, intent prediction also requires\nmodels to know all possible intent types for each\ntask. Therefore, we extract the schema directly\nfrom the schema file if it exists. As to datasets\nwithout schema, we also iterate over all dialogue\n\"dialogue_id\": \"train_1\",\n\"num_utterances\": 14,\n\"utterances\": [\n{\n\"speaker\": \"USR\",\n\"text\": \"I'd like to book a trip to Atlantis from Caprica on \nSaturday, August 13, 2016 for 8 adults.\",\n\"ap_label\": \"\",\n\"da_label\": \"inform\"\n},\n{\n\"speaker\": \"USR\",\n\"text\": \"I have a tight budget of 1700.\",\n\"ap_label\": \"\",\n\"da_label\": \"inform\"\n},\n{\n\"speaker\": \"SYS\",\n\"text\": \"Hi...I checked a few options for you, and we do \nnot currently have any trips that meet this criteria.\",\n\"ap_label\": \"\",\n\"da_label\": \"sorry\",\n\"slots\": {\n\"dst_city\": \"Atlantis\",\n\"or_city\": \"Caprica\",\n\"str_date\": \"Saturday, August 13, 2016\",\n\"n_adults\": \"8\",\n\"budget\": \"1700\"\n}\n}\n],\n\"scenario\": {\n\"db_id\": \"U22HTHYNP\",\n\"db_type\": \"booking\",\n\"task\": \"book\"\n}\n\"FRAMES--train--1\": {\n\"original dialog id\": \"train_1\",\n\"dialog index\": 1,\n\"original dialog info\": {\n\"scenario\": {\n\"db_id\": \"U22HTHYNP\",\n\"db_type\": \"booking\",\n\"task\": \"book\"}}\n\"log\": [\n{\n\"turn id\": 1,\n\u201cuser utterance\u201d: \u201cI\u2018d like to book a trip to Atlantis from Caprica on Saturday, \nAugust 13, 2016 for 8 adults. I have a tight budget of 1700.\",\n\u201csystem response\u201d: \u201cHi...I checked a few options for you, and we do not currently \nhave any trips that meet this criteria.\",\n\"dialog history\": \"\",\n\"original user side information\": {\n\"da_label\": \"inform\"\n},\n\"original system side information\": {\n\"da_label\": \"sorry\",\n\"slots\": {\n\"dst_city\": \"Atlantis\",\n\"or_city\": \"Caprica\",\n\"str_date\": \"Saturday, August 13, 2016\",\n\"n_adults\": \"8\",\n\"budget\": \"1700\"\n}\n},\n\"intent\": \"inform\",\n\"dst\": \"book dst_city Atlantis, book or_city Caprica, book str_date Saturday, August \n13, 2016, book n_adults 8, book budget 1700\"\n}\n]\n\u201cexternal knowledge\u201d: \u201c( travel : (( trip : ( returning : ( duration : ( hours : 0 | min : 51...\",\n\u201cdst knowledge\u201d: \u201c ( book : ( dst_city : ( Indianapolis | St. Loius | Le Paz | \u2026) | or_city : ( \nPUebla | sf | toluca | San Francisco\u2026\",\n\"intent knowledge\": \"( book : ( null | negate | request | goodbye | affirm))\u2026\",\n\"prompt\": [\n\u201cThis is a bot helping users to book a trip. Given the dialog context and external \ndatabase, please generate a relevant system response for the user.\"\n]\n}\n(a) Original Data\n(b) DialogStudio Data\nFigure 3: A dialogue format example. Left: original example, right: converted example. Here we only show the\nfirst turn and partial information.\nin the dataset to collect all potential intents. Then,\nwe put the turn-level ground-truth intent informa-\ntion into \u201dintent\u201d, and the general intents under\n\u201dintent knowledge\u201d, as presented in Figure 3. Note\nthat not all datasets provide detailed annotation for\ndialogue states, intents, or even databases. For dia-\nlogue state tracking and intent classification tasks,\nwe only process dialogues with corresponding an-\nnotations. Since all data is used for response gen-\neration, we leave the external knowledge value for\nthe database blank if there is no related database\nin the original dataset.\n3.2\nAccess and Maintenance\nAs aforementioned in the format, our DialogStu-\ndio data is easy to access via the JSON files. To\nmake DialogStudio more maintainable and acces-\nsible, we will publish datasets on both GitHub\nand HuggingFace. GitHub mainly stores selected\ndialogue examples and relevant documents. We\nsample five original dialogues and five converted\ndialogues for each dataset to facilitate users in\ncomprehending our format and examining the\ncontents of each dataset. The complete DialogStu-\ndio dataset is maintained in our HugginFace\nrepository, where all the datasets can be directly\ndownloaded or loaded with the HuggingFace\nload dataset(dialogstudio, dataset name)\nAPI. Given the substantial volume of datasets,\noptimizing user experience poses a challenge\nand limitation.\nWe will continuously maintain\nand update both GitHub and HuggingFace. Di-\nalogStudio is built upon public research datasets\nwithout individual or private information.\nWe\nbelieve it is important to clearly present the\nlicense associated with each of these datasets.\nConsequently, we have included the original\nlicenses for all datasets.\nAll these datasets are\nsupportive of academic research, and some of\nthem also endorse commercial usage. The code\nthat we employ falls under the widely accepted\nApache 2.0 license.\nWhile we strictly require\nadherence to the respective dataset licenses for all\nintended usages on DialogStudio, there remains\na possibility that some works might not fully\ncomply with the licenses.\nRegarding the other concerns such as ethical\nconcern, we admit that DialogStudio is collected\nand maintained by the authors of this work and we\ndid not hire external annotators. Since it contains\nunified datasets across several categories, it sup-\nports various research purposes from individual\ntasks and datasets to language model pre-training.\n4\nExperiments\nIn this section, we present the pre-training details,\nmethodologies, and metrics used to assess the per-\nformance of our DialogStudio model. The evalua-\ntion process aims to measure the model\u2019s ability to\nboth solve task-oriented dialogues and understand\ngeneral prompt-based instruction.\n4.1\nModel Pre-training\nIn this section, we introduce more details about\nhow we conduct our pre-training. In regards of\ntraining models, we mix several datasets from Di-\nalogStudio.\nFor task-oriented and conversational recom-\nmendation datasets, we selected dialogues from a\nrange of sources including KVRET (Eric et al.,\n2017), AirDialogue (Wei et al., 2018), DSTC2-\nClean (Mrk\u02c7si\u00b4c et al., 2017), CaSiNo (Chawla\net\nal.,\n2021),\nFRAMES\n(El\nAsri\net\nal.),\nWOZ2.0 (Mrk\u02c7si\u00b4c et al., 2017), CraigslistBar-\ngains (He et al., 2018), Taskmaster1-2 (Byrne\net al., 2019), ABCD (Chen et al., 2021a), Mul-\nDoGO (Peskov et al., 2019), BiTOD (Lin et al.,\n2021), SimJoint (Shah et al., 2018), STAR (Mosig\net al., 2020), SGD (Rastogi et al., 2020), OpenDi-\nalKG (Moon et al., 2019) and DuRecDial-2.0 (Liu\net al., 2021).\nMeanwhile,\nfor\nknowledge-grounded\ndia-\nlogues, we drew upon dataset from SQA (Iyyer\net al., 2017), SParC (Yu et al., 2019b), Fe-\nTaQA (Nan et al., 2022), MultiModalQA (Talmor\net al., 2021), CompWebQ (Talmor and Berant,\n2018), CoSQL (Yu et al., 2019a).\nFor open-domain dialogues, we sample dia-\nlogues from SODA (Kim et al., 2022a), Prosocial-\nDialog (Kim et al., 2022b), Chitchat (Myers et al.,\n2020).\nFor each dialogue dataset, we sample at most\n11k dialogues. Additionaly, we extracted around\n11k dialogue turns from question-answering dia-\nlogues featured in RACE (Lai et al., 2017), Nar-\nrativeQA (Ko\u02c7cisk`y et al., 2018), SQUAD (Ra-\njpurkar et al., 2018), MCtest (Richardson et al.,\n2013), OpenBookQA (Mihaylov et al., 2018),\nMultiRC (Khashabi et al., 2018). Here, a dialogue\nturn refers to a pair consisting of a dialogue con-\ntext and its corresponding system response. The\nrest datasets in DialogStudio are preserved for fu-\nture evaluations and downstream fine-tuning.\nFor each dialogue during the training, we\nshape the available external knowledge into a\nstring, which is included in dialogue context,\nand instruct the model to generate a dialogue re-\nsponse based on external knowledge.\nWe use\nthe format Instruction \\n <USER> user ut-\nterance <SYSTEM> system response <USER>\n...\n<USER> user utterance \\n <EXTERNAL\nKNOWLEDGE> supported knowledge to train the\nmodel, where <USER>, <SYSTEM> and <EX-\nTERNAL KNOWLEDGE> are special tokens.\nWe follow the public HuggingFace transformer\ncode (Wolf et al., 2020; Wang et al., 2022) to train\nthe model. For initializing our models, we adopt\nT5 (Raffel et al., 2020) and Flan-T5 (Longpre\net al., 2023) as starting points to respectively es-\ntablish DialogStudio-T5 and DialogStudio-Flan-\nT5. For the training of DialogStudio-Flan-T5, we\nexclude all translation-oriented tasks, limiting the\nsample size for each Flan task to a maximum of\n150 examples. This leads to a cumulative total\nof 140,000 samples. We train the model up to 3\nepochs with bfloat16 precision, a total batch size\nof 64. We set a constant learning rate 5e-5 and 3e-\n5 for the large model and the 3B model, respec-\ntively. Experiments are conducted using 16 A100\nGPUs, each with 40GB of GPU memory.\n4.2\nEvaluation for Response Generation\nSettings.\nWe evaluate the performance on\nCoQA (Reddy et al., 2019) and MultiWOZ\n2.2 (Zang et al., 2020).\nCoQA is a multi-turn\nconversational question answering dataset with 8k\nconversations about text passages from seven di-\nverse domains. MultiWOZ 2.2 is one of the largest\nand most widely used multi-domain task-oriented\ndialogue corpora with more than 10000 dialogues.\nEach dialogue involves with one or more domains\nsuch as Train, Restaurant, Hotel, Taxi, and Attrac-\ntion. The dataset is challenging and complex due\nto the multi-domain setting and diverse linguistic\nstyles. Note that we exclude both datasets during\nthe pre-training stage to prevent data leakage.\nCoQA\nMultiWOZ\nROUGE-L\nF1\nROUGE-L\nF1\nFlan-T5-3B (Longpre et al., 2023)\n37.1\n37.2\n7.0\n7.4\nFlan-T5-Large (Longpre et al., 2023)\n22.5\n22.3\n15.9\n17.6\nGODEL-Large (Peng et al., 2022)\n43.2\n43.3\n18.5\n19.3\nDialogStudio-T5-Large\n61.2\n61.5\n32.4\n34.5\nDialogStudio-Flan-T5-Large\n63.3\n63.5\n33.7\n35.9\nTable 1: Zero-shot results on CoQA and MultiWOZ 2.2.\nCR\n(14 tasks)\nDAR\n(7 tasks)\nTE\n(27 tasks)\navg.\n(48 tasks)\nOPT-30B (Zhang et al., 2022b)\n21.3/1.1\n35.2/4.1\n40.3/0.9\n32.3/2.0\nOPT-IML-30B (Iyer et al., 2022)\n37.4/41.6\n51.4/51.8\n54.7/47.8\n47.9/47.1\nOPT-175B (Zhang et al., 2022b)\n21.0/4.2\n37.1/16.8\n41.6/2.2\n33.3/7.7\nOPT-IML-175B (Iyer et al., 2022)\n39.0/49.8\n61.2/60.2\n54.3/51.0\n51.5/53.6\nTk-INSTRUCT-11B (Wang et al., 2022)\n32.3/62.3\n51.1/69.6\n55.0/64.1\n46.1/65.3\nTk-INSTRUCT-3B (Wang et al., 2022)\n38.4/51.3\n45.7/58.5\n48.4/52.8\n44.2/54.2\nDialogStudio-NIV2-T5-3B\n41.3/59.8\n57.5/63.7\n52.3/55.1\n50.4/59.5\nTable 2: 0-shot/2-shot/5-shot ROUGE-L testing results on unseen datasets and unseen tasks. Results of baselines\nare reported by original papers. CR, DAR, and TE, avg. are abbreviations for Coreference Resolution, Dialogue\nAct Recognition, Textual Entailment, and average results, respectively.\nFor CoQA, we follow the original paper set-\nting to answer question based on external pas-\nsage. For MultiWOZ 2.2, we consider the lex-\nicalized dialogue-act-to-response generation task\nwhere the model needs to consider both the dia-\nlogue context and the dialogue acts during gener-\nation. We follow the prompt from (Bang et al.,\n2023) to instruct models, i.e., Continue the dia-\nlogue as a task-oriented dialogue system called\nSYSTEM. The answer of SYSTEM should follow\nthe ACTION provided next while answering the\nUSER\u2019s last utterance.\nWe focus on zero-shot evaluation and report\nthe ROUGE-L and F1 score (Miller et al., 2017),\nwhere ROUGE-L measures the longest common\nsubsequence and F1 measures the Unigram F1\noverlap between the prediction and ground-truth\nresponse.\nBaselines.\nWe consider GODEL (Peng et al.,\n2022) and Flan-T5 (Longpre et al., 2023) as our\nbaselines. GODEL is a T5-based large pre-trained\nmodel for goal-oriented dialogues. It is pre-trained\nwith 551M multi-turn Reddit dialogues and 5M\nknowledge-grounded and question-answering di-\nalogues. Flan-T5 is an instruction-aware model.\nIt is also initialized from T5 and pre-trained on\nthe Flan collection, which consists of more than\n1800 tasks and 400 datasets, including dialogue\ndatasets.\nResults.\nTable 1 depicts the results from both\nzero-shot and few-shot testing.\nEvidently, our\nmodels considerably surpass the baseline models\nin terms of zero-shot learning, exhibiting a robust\ngeneralized ability for response generation in a\nzero-shot scenario.\nFlan-T5-3B, on the other hand, underperforms\nin the task of generating responses from dialog-\nacts. This model tends to produce incorrect dialog\nacts, unnatural utterances, or terminates with an\nempty end token. One explanation for this is that\nFlan-T5 models did not receive sufficient dialogue\ntraining during the instruction-training phase on\nthe Flan collections.\nComparisons between the\nperformances of existing models before and after\ntraining on the unified dataset validate DialogStu-\ndio\u2019s usefulness.\n4.3\nEvaluation on Super-NaturalInstructions\nSettings.\nNIV2 (Wang et al., 2022) introduces\nan instruction-tuning benchmark with more than\n1600 tasks. We select 3 categories with 44 tasks\nfrom the held-out test set, which consists of 154\nMMLU\nBBH\n0-SHOT\n5-SHOT\n3-SHOT\nTK-INSTRUCT 11B (Wang et al., 2022)\n-\n41.1\n32.9\nLLAMA 13B (Touvron et al., 2023)\n-\n46.2\n37.1\nVicuna 13B (Chiang et al., 2023)\n-\n49.7\n37.1\nFlan-T5-Large (Longpre et al., 2023)\n41.5\n41.9\n37.1\nFlan-T5-XL (Peng et al., 2022)\n48.7\n49.3\n40.2\nOPT-IML-Max 30B (Iyer et al., 2022)\n46.3\n43.2\n31.3\nDialogStudio-Flan-T5-Large\n40.1\n40.9\n34.2\nDialogStudio-Flan-T5-3B\n48.3\n47.8\n40.3\nTable 3: Test results on MMLU and BBH. Results come from original papers and InstructEval (Chia et al., 2023).\ntasks, i.e., Coreference Resolution, Dialogue Act\nRecognition, and Textual Entailment.\nThe se-\nlected tasks and datasets are unseen in the train-\ning stage. Specifically, we strictly follow all set-\ntings including metrics in (Wang et al., 2022), i.e.,\ntrain models with instructions + 2 positive demon-\nstrations and no negative demonstrations. We fine-\ntune DialogStudio-T5-3B on 756 training tasks.\nBaselines. OPT (Zhang et al., 2022b) is a set of\nopen decoder-only transformer models pre-trained\non 180B tokens. OPT-IML (Iyer et al., 2022) is\nan instruction meta-learning model based on the\nOPT-IML bench with more than 1500 tasks. Tk-\nINSTRUCT (Wang et al., 2022) is initialized from\nT5 and further pre-trained based on NIV2 collec-\ntions.\nNote that we neglect Flan-T5 because it\ntrains with all the downstream datasets and tasks.\nResults.\nTable 2 shows the 0-shot and 2-\nshot results on unseen datasets and unseen\ntasks.\nBased on the average performance on\n48 tasks, DialogStudio-NIV2-T5-3B outperforms\nOPT-IML-175B by 5.9% on 2-shot learning with\nmore than 50 times fewer model parameters, and it\nsurpasses Tk-INSTRUCT-11B by 4.3% on 0-shot\nlearning with more than 3 times fewer parameters.\nThe performance demonstrates the strong general-\nization ability of DialogStudio model. Compared\nwith Tk-INSTRUCT-3B, DialogStudio-NIV2-T5-\n3B achieves 6.2% and 5.3% improvements on 0-\nshot and 2-shot learning respectively. Given that\nboth Tk-INSTRUCT and our DialogStudio-NIV2-\nT5-3B are fine-tuned from the T5 model, this\nimprovement indicates the effectiveness of pre-\ntraining with our DialogStudio collection.\n4.4\nEvaluation on MMLU and BBH\nTable 3 presents results on MMLU (Hendrycks\net al., 2020) and Big Bench Hard (BBH) (Srivas-\ntava et al., 2022). When comparing the perfor-\nmance of DialogStudio-Flan-T5 with Flan-T5, we\nobserve a minor decrease. However, this is accom-\npanied by a significant improvement in dialogue\nrelevant capabilities.\n4.5\nEvaluation on Alternative Benchmarks\nDialogStudio encompasses not just public realistic\ndialogue datasets, but also those derived from or\nshared with ChatGPT, such as SODA (Kim et al.,\n2022a) and ShareGPT. Due to GPU constraints,\nwe employ techniques like LoRA (Hu et al., 2021)\nto fine-tune llama (Touvron et al., 2023). When\nusing equivalent datasets from DialogStudio, we\nobserved performance comparable to other mod-\nels, e.g., Vicuna (Chiang et al., 2023), on bench-\nmarks like AlpacaEval (Dubois et al., 2023) and\nMT-Bench (Zheng et al., 2023). This demonstrates\nthat DialogStudio caters to research interests in\nboth specific datasets and generalized instruction\ntuning.\n5\nCONCLUSION\nIn this study, we have introduced DialogStudio,\na comprehensive collection that aggregates more\nthan 80 diverse dialogue datasets while preserv-\ning their original information.\nThis aggrega-\ntion not only represents a significant leap towards\nconsolidating dialogues from varied sources but\nalso offers a rich tapestry of conversational pat-\nterns, intents, and structures, capturing the nu-\nances and richness of human interaction. Utilizing\nDialogStudio, we developed corresponding mod-\nels, demonstrating superior performance in both\nzero-shot and few-shot learning scenarios. In the\nspirit of open research and advancing the field,\nwe are committed to releasing DialogStudio to the\nbroader research community.\nReferences\nLayla El Asri,\nHannes Schulz,\nShikhar Sharma,\nJeremie Zumer, Justin Harris, Emery Fine, Rahul\nMehrotra, and Kaheer Suleman. 2017. Frames: a\ncorpus for adding memory to goal-oriented dialogue\nsystems. arXiv preprint arXiv:1704.00057.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Zi-\nwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A\nmultitask, multilingual, multimodal evaluation of\nchatgpt on reasoning, hallucination, and interactiv-\nity. arXiv preprint arXiv:2302.04023.\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u02dcnigo Casanueva, Ultes Stefan, Ramadan Os-\nman, and Milica Ga\u02c7si\u00b4c. 2018. Multiwoz - a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nBill Byrne, Karthik Krishnamoorthi, Chinnadhurai\nSankar, Arvind Neelakantan, Ben Goodrich, Daniel\nDuckworth, Semih Yavuz, Amit Dubey, Kyu-Young\nKim, and Andy Cedilnik. 2019. Taskmaster-1: To-\nward a realistic and diverse dialog dataset. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 4516\u20134525.\nI\u02dcnigo Casanueva, Tadas Tem\u02c7cinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli\u00b4c. 2020.\nEffi-\ncient intent detection with dual sentence encoders.\nIn Proceedings of the 2nd Workshop on Natural Lan-\nguage Processing for Conversational AI, pages 38\u2013\n45.\nI\u02dcnigo Casanueva, Ivan Vuli\u00b4c, Georgios Spithourakis,\nand Pawe\u0142 Budzianowski. 2022. Nlu++: A multi-\nlabel, slot-rich, generalisable dataset for natural lan-\nguage understanding in task-oriented dialogue. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 1998\u20132013.\nKushal Chawla, Jaysa Ramirez, Rene Clever, Gale\nLucas, Jonathan May, and Jonathan Gratch. 2021.\nCasino: A corpus of campsite negotiation dialogues\nfor automatic negotiation systems.\nIn Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3167\u20133185.\nDerek Chen, Howard Chen, Yi Yang, Alexander Lin,\nand Zhou Yu. 2021a.\nAction-based conversations\ndataset: A corpus for building more in-depth task-\noriented dialogue systems.\nIn Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3002\u20133017.\nMaximillian Chen, Alexandros Papangelis, Chenyang\nTao, Seokhwan Kim, Andy Rosenbaum, Yang Liu,\nZhou Yu, and Dilek Hakkani-Tur. 2023.\nPlaces:\nPrompting language models for social conversation\nsynthesis. In Findings of the Association for Com-\nputational Linguistics: EACL 2023, pages 814\u2013838.\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin\nGimpel. 2022a.\nSummScreen: A dataset for ab-\nstractive screenplay summarization.\nIn Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8602\u20138615, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\nXiong, Hong Wang, and William Yang Wang. 2020.\nHybridqa: A dataset of multi-hop question answer-\ning over tabular and textual data. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1026\u20131036.\nYulong Chen, Yang Liu, Liang Chen, and Yue Zhang.\n2021b.\nDialogSum: A real-life scenario dialogue\nsummarization dataset. In Findings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP\n2021, pages 5062\u20135074, Online. Association for\nComputational Linguistics.\nZhiyu Chen, Bing Liu, Seungwhan Moon, Chinnadhu-\nrai Sankar, Paul A Crook, and William Yang Wang.\n2022b.\nKetod: Knowledge-enriched task-oriented\ndialogue. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 2581\u2013\n2593.\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-\njanya Poria. 2023.\nInstructeval: Towards holistic\nevaluation of instruction-tuned large language mod-\nels. arXiv preprint arXiv:2306.04757.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023).\nSsu Chiu, Maolin Li, Yen-Ting Lin, and Yun-Nung\nChen. 2022. Salesbot: Transitioning from chit-chat\nto task-oriented dialogues.\nIn Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n6143\u20136158.\nSamuel Coope, Tyler Farghly, Daniela Gerz, Ivan\nVuli\u00b4c, and Matthew Henderson. 2020.\nSpan-\nconvert: Few-shot span extraction for dialog with\npretrained conversational representations.\nIn Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 107\u2013121.\nAlice Coucke, Alaa Saade, Adrien Ball, Th\u00b4eodore\nBluche, Alexandre Caulier, David Leroy, Cl\u00b4ement\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander H. Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur D. Szlam, Iulian Serban,\nRyan Lowe, Shrimai Prabhumoye, Alan W. Black,\nAlexander I. Rudnicky, Jason Williams, Joelle\nPineau, Mikhail S. Burtsev, and Jason Weston. 2019.\nThe second conversational intelligence challenge\n(convai2). ArXiv, abs/1902.00098.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi\nZheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,\nand Bowen Zhou. 2023. Enhancing chat language\nmodels by scaling high-quality instructional conver-\nsations. arXiv preprint arXiv:2305.14233.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. 2023.\nAl-\npacafarm:\nA simulation framework for methods\nthat learn from human feedback.\narXiv preprint\narXiv:2305.14387.\nLayla El Asri,\nHannes Schulz,\nShikhar Sharma,\nJeremie Zumer, Justin Harris, Emery Fine, Rahul\nMehrotra, and Kaheer Suleman. Frames: A corpus\nfor adding memory to goal-oriented dialogue sys-\ntems.\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyang Gao, Adarsh Kumar,\nAnuj Goyal, Peter Ku, and Dilek Hakkani-Tur. 2020.\nMultiwoz 2.1: A consolidated multi-domain dia-\nlogue dataset with state corrections and state track-\ning baselines. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n422\u2013428.\nMihail Eric, Lakshmi Krishnan, Francois Charette, and\nChristopher D Manning. 2017. Key-value retrieval\nnetworks for task-oriented dialogue. In Proceedings\nof the 18th Annual SIGdial Meeting on Discourse\nand Dialogue, pages 37\u201349.\nAlexander Fabbri, Faiaz Rahman, Imad Rizvi, Borui\nWang, Haoran Li, Yashar Mehdad, and Dragomir\nRadev. 2021. ConvoSumm: Conversation summa-\nrization benchmark and improved abstractive sum-\nmarization with argument mining. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6866\u20136880, Online.\nAssociation for Computational Linguistics.\nGuy Feigenblat, Chulaka Gunasekara, Benjamin Szna-\njder, Sachindra Joshi, David Konopnicki, and Ranit\nAharonov. 2021. TWEETSUMM - a dialog sum-\nmarization dataset for customer service. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 245\u2013260, Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and\nAleksander Wawer. 2019.\nSAMSum corpus: A\nhuman-annotated dialogue dataset for abstractive\nsummarization. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization, pages 70\u201379,\nHong Kong, China. Association for Computational\nLinguistics.\nYu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy\nLiang, Xifeng Yan, and Yu Su. 2021. Beyond iid:\nthree levels of generalization for question answer-\ning on knowledge bases. In Proceedings of the Web\nConference 2021, pages 3477\u20133488.\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri,\nMaxine Eskenazi, and Jeffrey P Bigham. 2022. In-\nstructdial: Improving zero and few-shot generaliza-\ntion in dialogue through instruction tuning. arXiv\npreprint arXiv:2205.12673.\nSonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Ku-\nmar, and Mike Lewis. 2018. Semantic parsing for\ntask oriented dialog using hierarchical representa-\ntions.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 2787\u20132792.\nShirley Anugrah Hayati, Dongyeop Kang, Qingxi-\naoyang Zhu, Weiyan Shi, and Zhou Yu. 2020. In-\nspired:\nToward sociable recommendation dialog\nsystems. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8142\u20138152.\nHe He, Derek Chen, Anusha Balakrishnan, and Percy\nLiang. 2018. Decoupling strategy and generation in\nnegotiation dialogues. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2333\u20132343.\nCharles T Hemphill, John J Godfrey, and George R\nDoddington. 1990. The atis spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27, 1990.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2020. Measuring massive multitask language\nunderstanding. arXiv preprint arXiv:2009.03300.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, D\u00b4aniel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022.\nOpt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.\nSearch-based neural structured learning for sequen-\ntial question answering. In Proceedings of the 55th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages\n1821\u20131831.\nAdam Janin, Don Baron, Jane Edwards, Dan El-\nlis, David Gelbart, Nelson Morgan, Barbara Pe-\nskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stol-\ncke, et al. 2003.\nThe icsi meeting corpus.\nIn\n2003 IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, 2003. Proceed-\nings.(ICASSP\u201903)., volume 1, pages I\u2013I. IEEE.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018.\nLooking\nbeyond the surface:a challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof North American Chapter of the Association for\nComputational Linguistics (NAACL).\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West,\nXiming Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,\nMalihe Alikhani, Gunhee Kim, Maarten Sap, and\nYejin Choi. 2022a.\nSoda: Million-scale dialogue\ndistillation with social commonsense contextualiza-\ntion. ArXiv, abs/2212.10465.\nHyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing\nLu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and\nMaarten Sap. 2022b. Prosocialdialog: A prosocial\nbackbone for conversational agents. arXiv preprint\narXiv:2205.12688.\nTom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and\nEdward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317\u2013328.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2022. Internet-augmented dialogue generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 8460\u20138478.\nWessel Kraaij, Thomas Hain, Mike Lincoln, and Wil-\nfried Post. 2005. The ami meeting corpus.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 785\u2013\n794.\nStefan Larson, Anish Mahendran, Joseph J Peper,\nChristopher Clarke,\nAndrew Lee,\nParker Hill,\nJonathan K Kummerfeld, Kevin Leach, Michael A\nLaurenzano, Lingjia Tang, et al. 2019.\nAn eval-\nuation dataset for intent classification and out-of-\nscope prediction. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1311\u20131316.\nS Lee, H Schulz, A Atkinson, J Gao, K Suleman,\nL El Asri, M Adada, M Huang, S Sharma, W Tay,\net al. 2019.\nMulti-domain task-completion dialog\nchallenge.\nDialog system technology challenges,\n8(9).\nRaymond Li, Samira Ebrahimi Kahou, Hannes Schulz,\nVincent Michalski, Laurent Charlin, and Chris Pal.\n2018a. Towards deep conversational recommenda-\ntions. In Advances in Neural Information Process-\ning Systems 31 (NIPS 2018).\nXiujun Li, Yu Wang, Siqi Sun, Sarah Panda, Jingjing\nLiu, and Jianfeng Gao. 2018b. Microsoft dialogue\nchallenge: Building end-to-end task-completion di-\nalogue systems. arXiv preprint arXiv:1807.11125.\nYu Li, Kun Qian, Weiyan Shi, and Zhou Yu. 2020.\nEnd-to-end trainable non-collaborative dialog sys-\ntem. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, volume 34, pages 8293\u20138302.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nPeng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and\nPascale Fung. 2021.\nBitod:\nA bilingual multi-\ndomain dataset for task-oriented dialogue modeling.\nNeurIPS 2021 Track on Datasets and Benchmarks.\nJingjing Liu, Panupong Pasupat, Scott Cyphers, and\nJim Glass. 2013. Asgard: A portable architecture\nfor multilingual dialogue systems.\nIn 2013 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing, pages 8386\u20138390. IEEE.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2019.\nBenchmarking natural lan-\nguage understanding services for building conversa-\ntional agents. arXiv preprint arXiv:1903.05566.\nZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu,\nand Wanxiang Che. 2021. Durecdial 2.0: A bilin-\ngual parallel corpus for conversational recommen-\ndation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 4335\u20134347.\nShayne Longpre, Le Hou, Tu Vu, Albert Web-\nson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V Le, Barret Zoph, Jason Wei, et al. 2023.\nThe flan collection: Designing data and methods\nfor effective instruction tuning.\narXiv preprint\narXiv:2301.13688.\nScott Martin, Shivani Poddar, and Kartikeya Upasani.\n2020. Mudoco: corpus for multidomain coreference\nresolution and referring expression generation. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 104\u2013111.\nShikib Mehri, Jinho Choi, Luis Fernando D\u2019Haro, Jan\nDeriu, Maxine Eskenazi, Milica Gasic, Kallirroi\nGeorgila, Dilek Hakkani-Tur, Zekang Li, Verena\nRieser, et al. 2022. Report from the nsf future direc-\ntions workshop on automatic evaluation of dialog:\nResearch directions and challenges. arXiv preprint\narXiv:2203.10012.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In EMNLP.\nAlexander H Miller, Will Feng, Adam Fisch, Jiasen Lu,\nDhruv Batra, Antoine Bordes, Devi Parikh, and Ja-\nson Weston. 2017. Parlai: A dialog research soft-\nware platform. arXiv preprint arXiv:1705.06476.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. Opendialkg: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845\u2013854.\nJohannes EM Mosig, Shikib Mehri, and Thomas\nKober. 2020.\nStar:\nA schema-guided dialog\ndataset for transfer learning.\narXiv preprint\narXiv:2010.11853.\nNikola Mrk\u02c7si\u00b4c, Diarmuid \u00b4O S\u00b4eaghdha, Tsung-Hsien\nWen, Blaise Thomson, and Steve Young. 2017.\nNeural belief tracker: Data-driven dialogue state\ntracking. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1777\u20131788.\nRajdeep Mukherjee, Abhinav Bohra, Akash Banerjee,\nSoumya Sharma, Manjunath Hegde, Afreen Shaikh,\nShivani Shrivastava, Koustuv Dasgupta, Niloy Gan-\nguly, Saptarshi Ghosh, and Pawan Goyal. 2022.\nECTSum: A new benchmark dataset for bullet point\nsummarization of long earnings call transcripts. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages\n10893\u201310906, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nWill Myers, Tyler Etchart, and Nancy Fulda. 2020.\nConversational scaffolding: An analogy-based ap-\nproach to response prioritization in open-domain di-\nalogs.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Vic-\ntoria Lin, Neha Verma, Rui Zhang, Wojciech\nKry\u00b4sci\u00b4nski, Hailey Schoelkopf, Riley Kong, Xian-\ngru Tang, et al. 2022. Fetaqa: Free-form table ques-\ntion answering. Transactions of the Association for\nComputational Linguistics, 10:35\u201349.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xian-\ngru Tang, Aadit Vyas, Neha Verma, Pranav Krishna,\net al. 2021.\nDart: Open-domain structured data\nrecord to text generation. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 432\u2013447.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann,\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\nDipanjan Das. 2020. Totto: A controlled table-to-\ntext generation dataset. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1173\u20131186.\nPanupong Pasupat and Percy Liang. 2015.\nCompo-\nsitional semantic parsing on semi-structured tables.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1470\u20131480.\nBaolin Peng, Michel Galley, Pengcheng He, Chris\nBrockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill\nDolan, and Jianfeng Gao. 2022. Godel: Large-scale\npre-training for goal-directed dialog. arXiv preprint\narXiv:2206.11309.\nDenis Peskov, Nancy Clarke, Jason Krone, Brigi Fodor,\nYi Zhang, Adel Youssef, and Mona Diab. 2019.\nMulti-domain goal-oriented dialogues (multidogo):\nStrategies toward curating and annotating large scale\ndialogue data. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 4526\u20134536.\nKun Qian, Ahmad Beirami, Zhouhan Lin, Ankita De,\nAlborz Geramifard, Zhou Yu, and Chinnadhurai\nSankar. 2021. Annotation inconsistency and entity\nbias in multiwoz. ArXiv, abs/2105.14150.\nKun Qian, Satwik Kottur, Ahmad Beirami, Shahin\nShayandeh, Paul A Crook, Alborz Geramifard, Zhou\nYu, and Chinnadhurai Sankar. 2022.\nDatabase\nsearch results disambiguation for task-oriented di-\nalog systems. In Proceedings of the 2022 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1158\u20131173.\nJun Quan, Deyi Xiong, Bonnie Webber, and Changjian\nHu. 2019. Gecor: An end-to-end generative ellipsis\nand co-reference resolution model for task-oriented\ndialogue. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4547\u20134557.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don\u2019t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784\u2013789.\nRevanth Rameshkumar and Peter Bailey. 2020. Story-\ntelling with dialogue: A Critical Role Dungeons and\nDragons Dataset. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5121\u20135134, Online. Association for\nComputational Linguistics.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019.\nTowards empathetic open-\ndomain conversation models: a new benchmark and\ndataset. In ACL.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghav Gupta, and Pranav Khaitan. 2020. Towards\nscalable multi-domain conversational agents: The\nschema-guided dialogue dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8689\u20138696.\nSiva Reddy, Danqi Chen, and Christopher D Manning.\n2019. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249\u2013266.\nVirgile Rennard, Guokan Shang, Julie Hunter, and\nMichalis Vazirgiannis. 2023.\nAbstractive meeting\nsummarization: A survey. Transactions of the Asso-\nciation for Computational Linguistics, 11:861\u2013884.\nMatthew Richardson, Christopher JC Burges, and Erin\nRenshaw. 2013.\nMctest: A challenge dataset for\nthe open-domain machine comprehension of text.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n193\u2013203.\nPararth Shah, Dilek Hakkani-T\u00a8ur, Gokhan T\u00a8ur, Ab-\nhinav Rastogi, Ankur Bapna, Neha Nayak, and\nLarry Heck. 2018. Building a conversational agent\novernight with dialogue self-play.\narXiv preprint\narXiv:1801.04871.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, et al. 2022.\nBlenderbot 3: a deployed conversational agent that\ncontinually learns to responsibly engage.\narXiv\npreprint arXiv:2208.03188.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdri`a Garriga-Alonso, et al. 2022.\nBeyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models.\narXiv preprint\narXiv:2206.04615.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 641\u2013651.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2021. Mul-\ntimodalqa: Complex question answering over text,\ntables and images. ICLR.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023.\nLlama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, et al. 2022.\nSuper-\nnaturalinstructions: Generalization via declarative\ninstructions on 1600+ nlp tasks. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5085\u20135109.\nWei Wei, Quoc Le, Andrew Dai, and Jia Li. 2018.\nAirdialogue: An environment for goal-oriented di-\nalogue research. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3844\u20133854.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtow-\nicz, et al. 2020. Transformers: State-of-the-art nat-\nural language processing.\nIn Proceedings of the\n2020 conference on empirical methods in natural\nlanguage processing: system demonstrations, pages\n38\u201345.\nWen-tau Yih, Matthew Richardson, Christopher Meek,\nMing-Wei Chang, and Jina Suh. 2016. The value of\nsemantic parse labeling for knowledge base question\nanswering. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 201\u2013206.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi,\nZihan Li, et al. 2019a.\nCosql: A conversational\ntext-to-sql challenge towards cross-domain natural\nlanguage interfaces to databases. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1962\u20131979.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, et al. 2018. Spider: A\nlarge-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3911\u20133921.\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern\nTan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,\nBo Pang, Tao Chen, et al. 2019b.\nSparc: Cross-\ndomain semantic parsing in context. arXiv preprint\narXiv:1906.02285.\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\nRaghav Gupta, Jianguo Zhang, and Jindong Chen.\n2020. Multiwoz 2.2: A dialogue dataset with addi-\ntional annotation corrections and state tracking base-\nlines. In Proceedings of the 2nd Workshop on Nat-\nural Language Processing for Conversational AI,\npages 109\u2013117.\nJianguo Zhang, Kazuma Hashimoto, Yao Wan, Zhiwei\nLiu, Ye Liu, Caiming Xiong, and S Yu Philip. 2022a.\nAre pre-trained transformers robust in intent classi-\nfication? a missing ingredient in evaluation of out-\nof-scope intent detection. In Proceedings of the 4th\nWorkshop on NLP for Conversational AI, pages 12\u2013\n20.\nJianguo Zhang, Stephen Roller, Kun Qian, Zhiwei Liu,\nRui Meng, Shelby Heinecke, Huan Wang, Silvio\nSavarese, and Caiming Xiong. 2023.\nEnhancing\nperformance on seen and unseen dialogue scenarios\nusing retrieval-augmented end-to-end task-oriented\nsystem. In Proceedings of the 24th Annual Meet-\ning of the Special Interest Group on Discourse and\nDialogue, pages 509\u2013518.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022b. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. arXiv preprint arXiv:2306.05685.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir\nRadev. 2021.\nQMSum: A New Benchmark for\nQuery-based Multi-domain Meeting Summariza-\ntion. In North American Association for Computa-\ntional Linguistics (NAACL).\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017.\nSeq2sql:\nGenerating structured queries\nfrom natural language using reinforcement learning.\narXiv preprint arXiv:1709.00103.\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\n2021.\nMediasum: A large-scale media interview\ndataset for dialogue summarization. arXiv preprint\narXiv:2103.06410.\nAppendix\nTable 4 and Table 5 lists datasets included in\nDialogStudio. Initially, we present a partial list\nof these datasets. More and latest information are\navailable in GitHub2.\n2https://github.com/salesforce/\nDialogStudio\nNLU\nNLU++ (Casanueva et al., 2022)\nBANKING77-OOS (Zhang et al., 2022a)\nBANKING77 (Casanueva et al., 2020)\nRESTAURANTS8K (Coope et al., 2020)\nCLINC150 (Larson et al., 2019)\nCLINC-Single-Domain-OOS-banking (Zhang et al., 2022a)\nCLINC-Single-Domain-OOS-credit cards (Zhang et al., 2022a)\nHWU64 (Liu et al., 2019)\nSNIPS (Coucke et al., 2018)\nSNIPS-NER (Coucke et al., 2018)\nDSTC8-SGD (Coope et al., 2020)\nTOP (Gupta et al., 2018)\nTOP-NER (Gupta et al., 2018)\nATIS-NER (Hemphill et al., 1990)\nATIS (Hemphill et al., 1990)\nMIT-MOVIE (Liu et al., 2013)\nMIT-RESTAURANT (Liu et al., 2013)\nTOD\nKVRET (Eric et al., 2017)\nAirDialogue (Wei et al., 2018)\nDSTC2-Clean (Mrk\u02c7si\u00b4c et al., 2017)\nCaSiNo (Chawla et al., 2021)\nFRAMES (El Asri et al.)\nWOZ2.0 (Mrk\u02c7si\u00b4c et al., 2017)\nCraigslistBargains (He et al., 2018)\nTaskmaster1 (Byrne et al., 2019)\nTaskmaster2 (Byrne et al., 2019)\nTaskmaster3 (Byrne et al., 2019)\nABCD (Chen et al., 2021a)\nMulDoGO (Peskov et al., 2019)\nBiTOD (Lin et al., 2021)\nSimJointGEN (Shah et al., 2018)\nSimJointMovie (Shah et al., 2018)\nSimJointRestaurant (Shah et al., 2018)\nSTAR (Mosig et al., 2020)\nSGD (Rastogi et al., 2020)\nMultiWOZ2 1 (Eric et al., 2020)\nMultiWOZ2 2 (Zang et al., 2020)\nMultiWOZ2 2+ (Qian et al., 2021)\nHDSA-Dialog (Chen et al., 2021a)\nMS-DC (Li et al., 2018b)\nGECOR (Quan et al., 2019)\nDisambiguation (Qian et al., 2022)\nMetaLWOZ (Lee et al., 2019)\nKETOD (Chen et al., 2022b)\nMuDoCo (Martin et al., 2020)\nTable 4: List of datasets included in DialogStudio (a).\nKG-Dial\nSQA (Iyyer et al., 2017)\nSParC (Yu et al., 2019b)\nFeTaQA (Nan et al., 2022)\nMultiModalQA (Talmor et al., 2021)\nCompWebQ (Talmor and Berant, 2018)\nCoSQL (Yu et al., 2019a)\nCoQA (Reddy et al., 2019)\nSpider (Yu et al., 2018)\nToTTo (Parikh et al., 2020)\nWebQSP (Yih et al., 2016)\nWikiSQL (Zhong et al., 2017)\nWikiTQ (Pasupat and Liang, 2015)\nDART (Nan et al., 2021)\nGrailQA (Gu et al., 2021)\nHybridQA (Chen et al., 2020)\nMTOP (Chen et al., 2020)\nUltralChat-Assistance (Ding et al., 2023)\nWizard of Wikipedia (Dinan et al., 2018)\nWizard of Internet (Komeili et al., 2022)\nDial-Sum\nTweetSumm (Feigenblat et al., 2021)\nSAMSum (Gliwa et al., 2019)\nDialogSum (Chen et al., 2021b)\nAMI (Kraaij et al., 2005; Rennard et al., 2023)\nICSI (Janin et al., 2003)\nQMSum (Zhong et al., 2021)\nMediaSum (Zhu et al., 2021)\nECTSum (Mukherjee et al., 2022)\nSummScreen ForeverDreaming (Chen et al., 2022a)\nSummScreen TVMegaSite (Chen et al., 2022a)\nCRD3 (Rameshkumar and Bailey, 2020)\nConvoSumm (Fabbri et al., 2021)\nOpen-Domain\nChitCHAT (Myers et al., 2020)\nSODA (Kim et al., 2022a)\nProsocial (Kim et al., 2022b)\nHH-RLHF (Bai et al., 2022)\nEmpathetic (Rashkin et al., 2019)\nConvAI2 (Dinan et al., 2019)\nAntiScam (Li et al., 2020)\nShareGPT (Zheng et al., 2023)\nPLACES3.5 (Chen et al., 2023)\nConv-Rec\nSalesBot (Chiu et al., 2022)\nRedial (Li et al., 2018a)\nInspired (Hayati et al., 2020)\nDuRecDial 2.0 (Liu et al., 2021)\nOpendialKG (Moon et al., 2019)\nTable 5: List of datasets included in DialogStudio (b).\n"
  },
  {
    "title": "LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs",
    "link": "https://arxiv.org/pdf/2307.10168.pdf",
    "upvote": "9",
    "text": "LLMs as Workers in Human-Computational Algorithms?\nReplicating Crowdsourcing Pipelines with LLMs\nTongshuang Wu\u2217\nHaiyi Zhu\nMaya Albayrak\nAlexis Axon\nAmanda Bertsch\nWenxing Deng\nZiqi Ding\nBill Guo\nSireesh Gururaja\nTzu-Sheng Kuo\nJenny T. Liang\nRyan Liu\nIhita Mandal\nJeremiah Milbauer\nXiaolin Ni\nNamrata Padmanabhan\nSubhashini Ramkumar\nAlexis Sudjianto\nJordan Taylor\nYing-Jui Tseng\nPatricia Vaidos\nZhijin Wu\nWei Wu\nChenyang Yang\nCarnegie Mellon University, Pittsburgh, PA, USA\nsherryw@cs.cmu.edu\nAbstract\nLLMs have shown promise in replicating\nhuman-like behavior in crowdsourcing tasks\nthat were previously thought to be exclusive\nto human abilities. However, current efforts\nfocus mainly on simple atomic tasks. We ex-\nplore whether LLMs can replicate more com-\nplex crowdsourcing pipelines. We find that\nmodern LLMs can simulate some of crowd-\nworkers\u2019 abilities in these \u201chuman computation\nalgorithms,\u201d but the level of success is vari-\nable and influenced by requesters\u2019 understand-\ning of LLM capabilities, the specific skills re-\nquired for sub-tasks, and the optimal interaction\nmodality for performing these sub-tasks. We\nreflect on human and LLMs\u2019 different sensitiv-\nities to instructions, stress the importance of\nenabling human-facing safeguards for LLMs,\nand discuss the potential of training humans\nand LLMs with complementary skill sets. Cru-\ncially, we show that replicating crowdsourcing\npipelines offers a valuable platform to inves-\ntigate (1) the relative strengths of LLMs on\ndifferent tasks (by cross-comparing their perfor-\nmances on sub-tasks) and (2) LLMs\u2019 potential\nin complex tasks, where they can complete part\nof the tasks while leaving others to humans.\n1\nIntroduction\nThe rapid advancement of AI systems has revolu-\ntionized our understanding of the capabilities of\nmachines. One remarkable breakthrough in this\nfield is the emergence of Large Language Mod-\nels (LLMs, e.g., ChatGPT). With a combination\nof extensive pre-training (Brown et al., 2020) and\n\u2217 This work builds upon an assignment from CMU 05-\n499/899: Human-Centered NLP. Tongshuang Wu is the course\ninstructor, who designed the assignment based on discussions\nwith Haiyi Zhu, and wrote the majority of the paper. The\nremaining authors are students listed in alphabetical order.\nFollowing the principles of participatory research, all students\nwere informed of the research, chose to participate, and were\ngiven opportunities to cease participation/authorship after the\ninitial experiments and after reviewing the paper draft.\nFigure 1: We study whether LLMs can be used to repli-\ncate crowdsourcing pipelines and replace human work-\ners in certain advanced \u201chuman-computational process.\u201d\ninstruction tuning (Stiennon et al., 2020; Wu et al.,\n2023; Ziegler et al., 2019), LLMs now not only pos-\nsess a large amount of world knowledge, but can\neffectively leverage this knowledge to accomplish\nvarious tasks simply by following instructions.\nVarious studies have reported that these models\ncan replicate human-like behavior to some extent,\nwhich is a key objective in the training of AI mod-\nels (Wang et al., 2022a; Bubeck et al., 2023). In par-\nticular, a large proportion of these studies have been\nusing LLMs to replicate crowdsourcing tasks, pos-\nsibly because they represent a wide range of tasks\nthat were previously considered exclusive to hu-\nman computational capabilities (Bernstein, 2013).\nFor example, LLMs can generate annotations of\nhigher quality at a reduced cost compared to crowd-\nworkers or even experts (Gilardi et al., 2023; T\u00f6rn-\nberg, 2023), and can approximate human opinions\nin subjective tasks, allowing for simulated human\nresponses to crowdsourced questionnaires and in-\nterviews (H\u00e4m\u00e4l\u00e4inen et al., 2023; Argyle et al.,\n2022). These observations indicate that LLMs will\nhave significant social and economic implications,\npotentially reshaping the workforce by replacing\ncertain human jobs (Eloundou et al., 2023). In\nfact, some studies have observed that now crowd-\nworkers tend to rely on LLMs for completing text\nproduction tasks (Veselovsky et al., 2023).\nHowever, most existing efforts tend to focus on\natomic tasks that are simple, self-contained, and\neasy for a single crowdworker to complete in a\narXiv:2307.10168v2  [cs.CL]  20 Jul 2023\nshort amount of time \u2014 the most basic version of\nhuman computational power. These efforts also are\nscattered across various tasks and domains, making\nit hard to systematically compare and understand\nwhich tasks LLMs may excel or underperform at,\nand to what extent they can simulate, replace, or\naugment humans on specific tasks. Such emphases\nprompt us to ask, how far does the LLM replica-\nbility generalize? Will they be useful in more ad-\nvanced formats of \u201chuman computation\u201d?\nWe are especially interested in whether LLMs\ncan be used to replicate crowdsourcing pipelines,\nwhich represent a more sophisticated approach to\nharnessing human computation (Little et al., 2010).\nIn a typical pipeline, complex tasks are broken\ndown into pieces (sub-tasks) that can be performed\nindependently, then later combined (Chilton et al.,\n2013; Kim et al., 2017; Law and Zhang, 2011;\nRetelny et al., 2017). This method has been widely\nused to scale crowdsourcing usability, allowing it to\nhandle tasks that are too challenging for individual\ncrowdworkers with limited level of commitment\nand unknown expertise (e.g., summarizing lengthy\nnovels, software development, or deciphering heav-\nily blurred text; Kittur et al., 2011).\nInterestingly, research on LLMs has also ex-\nplored scaling their capabilities for more complex\ntasks through chaining. Though named differently,\nLLM chains and crowdsourcing pipelines share\nsimilar motivation and strategy of scaling LLM util-\nity. Previous studies have connected the two, noting\nthat they decompose tasks to address different prob-\nlems (Wu et al., 2022b): crowdsourcing pipelines\nfocus on factors affecting human worker perfor-\nmance, such as cognitive load and task duration,\nwhile LLM chains address inherent limitations of\nLLMs, such as high variance in prompt effective-\nness. However, since LLMs have now been trained\nto better align with humans in following instruc-\ntions and handling complex contexts (Ouyang et al.,\n2022), it is possible for human and LLM workers\nto adopt the same task division strategies.\nIn this study, we investigate the potential of\nLLMs to replace human workers in advanced hu-\nman computation processes. To accomplish this,\nwe designed a course assignment for a special topic\ncourse named Human-Centered NLP at Carnegie\nMellon University. In the assignment, 20 students\nwere tasked to select one (out of seven) crowd-\nsourcing pipelines depicted in prior work, and repli-\ncate them by employing LLMs to handle each sub-\ntask. The replication study also offers an interest-\ning bonus analysis point: While LLM modules in a\nchain perform unique sub-tasks, all the sub-tasks\noccur in the same application domain (e.g., process-\ning the same document in different ways), making\nit fairer to compare LLMs\u2019 performance in differ-\nent sub-tasks and uncovering the relative strengths\nand weaknesses.\nWe find that while LMs appear to be able to\nreplicate crowdsourcing pipelines, there is a wide\nvariance in which parts they tend to perform well /\nin ways we would expect from humans (main find-\nings in Table 2b). The differences emerge from two\nprimary reasons. First, LLMs and humans respond\ndifferently to instructions. LLMs are more respon-\nsive to adjectives and comparison-based instruc-\ntions, such as \u201cbetter\u201d or \u201cmore diverse,\u201d whereas\nhumans handle instructions involving trade-off cri-\nteria better. Second, humans receive more scaffolds\nthrough disagreement resolution mechanisms and\ninterface-enforced interactions, enabling guardrails\non output quality and structure that not available\nto LLMs. These observations highlight the need\nto improve LLM instruction tuning to better han-\ndle ambiguous or incomplete instructions, as well\nas the necessity to consider how non-textual \u201cin-\nstructions\u201d can be employed either during LLM\nfinetuning or actual usage. Moreover, the effective-\nness of replicated LLM chains depends on students\u2019\nperceptions of LLM strengths, which calls for more\ninvestigations on assisted prompting.\nIn addition to offering immediate insights into\nthe differences between LLMs and crowdwork-\ners, our research demonstrates that replicating\ncrowdsourcing pipelines serves as a valuable plat-\nform for future investigations into the partial ef-\nfectiveness of LLMs across a wider range of tasks.\nRather than expecting LLMs to tackle entire com-\nplex tasks, we can instead assess and identify spe-\ncific sub-tasks in which LLMs consistently per-\nform on par with humans.\nThis evidence can\nthen be utilized to distribute sub-tasks between\nLLMs and human workers, optimizing the alloca-\ntion of responsibilities. We opensource the prompt\nchains, outputs, and evaluation at https://github.\ncom/tongshuangwu/llm-crowdsourcing-pipeline.\n2\nBackground and Related Work\nCrowdsourcing helps solve problems that require\nhuman inputs, at scale (Howe et al., 2006). Partic-\nularly in earlier times when AI capabilities were\nlimited, crowdsourcing was seen as a promising\napproach to leverage and enhance the unique com-\nputational powers possessed by humans.\nA key focus of crowdsourcing research has been\nthe development of pipelines to tackle increasingly\ncomplex crowdsourcing goals (Kittur et al., 2011).\nThrough careful task decomposition, crowdsourc-\ning pipelines strategically collect inputs from hu-\nman workers, capitalizing on their strengths while\nmitigating their limitations. This feat is challeng-\ning, if not impossible, to achieve in traditional\ncrowdsourcing designs. For example, Bernstein\net al. (Bernstein et al., 2010) ensured text editing\nquality through a Find-Fix-Verify workflow, which\nmodulates the scope of sub-tasks to reduce vari-\nance of crowdworker effort. Meanwhile, Context\nTrees (Verroios and Bernstein, 2014) hierarchically\nsummarize and trim the otherwise overwhelming\nglobal contexts, making them compact enough for\na single worker to digest. Because of their sophis-\nticated designs, crowdsourcing pipelines are often\nreferred to as human computation algorithms or\ncrowd algorithms (Howe et al., 2006; Law and\nZhang, 2011; Kittur et al., 2011; Little et al., 2010).\nThough emerged in a completely separate field\n(NLP), LLM Chains share similar goals with\ncrowdsourcing pipelines \u2014 to complete com-\nplex tasks that are challenging to perform in one\npass. This decomposition can take either an ex-\nplicit or implicit form. For example, Chain-of-\nThought (Kojima et al., 2022; Wei et al., 2022) em-\nploys prompts like \u201clet\u2019s consider this step-by-step\u201d\nmakes LLMs to resolve sub-tasks that are not pre-\ndefined, whereas AI Chains (Wu et al., 2022b) and\nDecomposed Prompting (Khot et al., 2022) explic-\nitly define sub-tasks and employ distinct prompts\nfor each sub-task. More recently, opensource li-\nbraries like LangChain (Chase) and services like\nPromptChainer (Wu et al., 2022a; noa) have en-\nabled practitioners to create LLM chains for tack-\nling tasks involving intricate compositionality.\nAs reviewed in Section 1, Wu et al. (2022b) has\ndrawn explicit connections between LLM chain-\ning and crowdsourcing pipelines. Besides similar\nmotivations, these two methods also share similar\nchallenges, e.g., handling cascading errors that af-\nfect later stages (Kittur et al., 2011) or synthesizing\nworkers\u2019 inconsistent contributions (Kittur et al.,\n2011; Bernstein et al., 2010), but these challenges\ncan be utilized for enhancing the transparency and\ndebuggability of AI-infused systems. More impor-\ntantly, Wu et al. (2022b) distinguished the task de-\ncomposition objectives for the two approaches: for\ntackling different limitations of humans and LLM\nworkers. While theoretically this assertion remains\ntrue, in practice the differences between humans\nand LLM workers seem to get blurred. With LLMs\nevolving to process longer context (OpenAI, 2023),\nfollowing instructions more closely (Ouyang et al.,\n2022), and exhibiting improved reasoning capabil-\nity (Bubeck et al., 2023), some of their limitations\nstart to overlap with those of humans. Various re-\ncent work also testifies this observation: Although\nnot explicitly categorized as chaining, several stud-\nies have employed strategies to have LLMs self-\nimprove in multiple runs, such as self-ask (Press\net al., 2022), self-reflection (Shinn et al., 2023),\nand self-consistency (Wang et al., 2022b), some\nof which are similar to crowdsourcing pipelines.\nThese recent developments of LLMs, and the suc-\ncess of crowdsourcing pipelines, prompt us to re-\nassess whether the idea of human computation al-\ngorithms can be directly transferred to AIs.\n3\nStudy Design\nStudy Procedure\nThe study required partic-\nipants (students) to replicate a crowdsourcing\npipeline by writing multiple prompts that instruct\nLLMs to complete different microtasks.\nTo accomplish this, the students began by thor-\noughly reading a crowdsourcing pipeline paper for\nreplication. To demonstrate the effectiveness of\ntheir replicated pipeline, they were also asked to\ndetermine an appropriate testing task, create at\nleast three test cases consisting of pairs of inputs\nand ideal outputs, and self-propose a set of task-\ndependent metrics for evaluating pipeline outputs\n(e.g., fluency, creativity, coherence). Then, they\nwere instructed to implement two solutions: (1)\na baseline solution that prompts one LLM mod-\nule to complete the entire task (Baseline), and (2)\na replica of their chosen crowdsourcing pipeline\n(LLM Chain). They compared the two LLM so-\nlutions using their designated test cases and met-\nrics, providing the reasoning behind their ratings.\nFinally, they concluded the task by reflecting on\nwhy the LLM chain replication either succeeded or\nfailed and brainstormed possible ways to improve\nthe chains in the future.\nAfter students submitted their assignments, they\nunderwent a peer-grading process. In this process,\neach student\u2019s submission was assessed by three of\nPipeline Description\nSample Task\nReplication evaluation\nTotal\nUnique\nCorrect\nEffective\nMap-Reduce\n(Kittur et al., 2011)\nPartition tasks into discrete subtasks, Map subtasks to\nworkers, Reduce / merge their results into a single output\nWrite essay\n4\n1\n3\n3\nHumorTool\n(Chilton et al., 2016)\nDefine semantic roles as the answers to a series of\nquestions that are intuitive for non-experts.\nCreate satire\n4\n2\n3\n1\nIterative Process\n(Little et al., 2010)\nFeed the result of one creation task into the next, so\nworkers see content generated by previous workers.\nBrainstorm\n3\n2\n3\n2\nMicrotasking\n(Cheng et al., 2015)\nConcrete microtasking for sorting task: an implementation\nof human-powered quicksort\nSorting\n3\n3\n3\n1\nFind-Fix-Verify\n(Bernstein et al., 2010)\nFor writing and editing: Find problems, Fix the identified\nproblems, Verify these edits\nShorten text\n3\n3\n2\n1\nPrice-Divide-Solve\n(Kulkarni et al., 2012)\nWorkers recursively divide complex steps until they are at\nan appropriately simple level, then solve them.\nWrite essay\n1\n1\n1\n1\nTask Paraphrase\n(He et al., 2015)\nDefine semantic roles as the answers to a series of\nquestions that are intuitive for non-experts.\nSRL labeling\n1\n1\n1\n1\nTable 1: Crowdsourcing pipelines replicated, and their example outputs from student-replicated LLM chains.\ntheir peers in a double-blind manner. The peers\nrated the submissions based on replication cor-\nrectness, thoroughness, and comprehensiveness of\ntheir envisioned LLM chain improvements. They\nrated all the criteria on a five-level Likert Scale\nand supplied detailed reasoning for their grading.\nThe instructor carefully reviewed the gradings and\nexcluded any assessments that appeared to lack\nthoughtful reflections or misunderstood the sub-\nmissions.\nThe full assignment instruction, the\npeer grading form, as well as the student sub-\nmissions are all available at https://github.com/\ntongshuangwu/llm-crowdsourcing-pipeline.\nParticipants\n21 students (13 females, 8 males)\ncompleted the task as one of their assignments\nfor the Spring 2023 course 05-499/899: Human-\nCentered NLP.1 This comprised of 6 undergradu-\nates, 10 master\u2019s students, and 5 PhD students spe-\ncializing in Sociology, Learning Science, Human-\nComputer Interaction, or Natural Language Pro-\ncessing. The paper presents findings from 20 stu-\ndents\u2019 submissions, as one student opted for a non-\nprogramming approach for partial credit.\nCrowdsourcing papers\nWe selected crowdsourc-\ning papers based on three criteria: (1) Diversity: the\npapers should represent different pipeline designs\n(iterative, parallel), intermediate steps (question-\nanswering, comparison, editing), and tasks (cre-\native tasks, annotation tasks, editing tasks, etc.)\n(2) Replicability: The papers should provide clear\ndefinitions for each sub-step and concrete sample\ntest cases. Considering our emphasis on LLMs,\nwe exclusively considered papers that described\ntasks with textual inputs and outputs. (3) Asyn-\nchronized: For the ease of setup, the papers should\nallow (LLM) workers to complete their microtasks\nindependently, without the need of synchronized\ndiscussions.2 The instructor pre-selected six papers\nmeeting these criteria (the first six in Table 1), and\nstudents could propose additional papers for ap-\nproval (Task Paraphrase in Table 1). Up to four stu-\ndents could sign up to replicate the same pipeline\nin a first-come-first-serve manner.\nLLM version\nStudents are required to use\ntext-davinci-0033 for their final implementa-\ntions and testing, the most capable model that uses\n1http://www.cs.cmu.edu/~sherryw/courses/\n2023s-hcnlp.html\n2we note that it should also be easy to setup synchronized\ndiscussions if we instruct two LLM APIs to discuss.\n3https://platform.openai.com/docs/models\nthe autocompletion interface at the time of assign-\nment design. However, they were encouraged to ini-\ntially experiment and fine-tune their prompts using\nmore cost-effective models (e.g., text-ada-001).\nReplication assessment\nWe evaluate the repli-\ncated chains on two dimensions:\n1. Replication correctness: We measure the suc-\ncess of replication using the peer grading re-\nsults. A replication is considered successful if\nthe average peer score for Correct Replication\nis greater than three.\n2. Chain effectiveness: We evaluate whether the\nreplicated chains are more effective than the\nbaselines using the students\u2019 own assessment.\nIf students indicate that their replicated chains\noutperform the baselines on the majority of\ntheir tested inputs (recall that they were re-\nquired to test at least three inputs), then the\npipeline is deemed effective.\nSince multiple students replicated the same\npipelines, it is also interesting to compare replicas\nfor the same pipeline to reveal key factors for suc-\ncessful replication. We look into students\u2019 replica-\ntion strategies, and report the number of (3) Unique\nreplicas. Specifically, we manually grouped the\nstudents\u2019 LLM chains based on the microtasks in-\nvolved, deeming two chains identical if they in-\nclude steps that essentially serve the same intended\nfunctionality, even if there are wording differences\nin the LLM prompts.\n4\nResults and Reflection\n4.1\nReplication Overview: Partial Success\nAs shown in Table 1, all the pipelines are replicable\nwith LLMs. For each pipeline, there is at least\none correct replication and an effective one. To\ndenote the successes, we show one actual input-\noutput sequence generated using students\u2019 LLM\nchain replications that they found preferable. These\nresults re-iterate that LLMs can now accomplish\na subset of tasks that were previously considered\npossible only for humans (Bubeck et al., 2023).\nSeveral students documented multiple pipelines\nthey experimented with, echoing the prior obser-\nvation that pipelines/chains enable rapid prototyp-\ning (Wu et al., 2022a). Some of their explorations\nfocused on single steps (e.g., P7 in Find-Fix-Verify\nchoosing between different wordings among \u201cfrag-\nment\u201d, \u201cclauses\u201d, \u201csubstrings\u201d etc.), while some\nother students experimented with globally redesign-\nDimensions\nObservations\nPipelines\nIdea Both: Breakdown complex tasks into pieces that can be done independently, then combined.\nLimitations Both: Cascading errors, conflicts between parallel paths, etc.\nGains Both: Scale to tasks that are otherwise hard, more structured interactions, more resilient to interruptions.\nLLM chains: Can take advantage of cascading effects & parallel paths, for explainability.\nOptimal design Crowd. pipelines: Address pitfalls of a single worker: high task variance, limited cognitive load, etc.\nLLM chains: Address pitfalls of a single LLM: limited reasoning capabilities, etc.\n(a) Similarities between crowdsourcing pipelines and LLM chains summarized in prior work (e.g., Wu et al., 2022b).\nDimensions\nObservations\nReflections & Opportunities\nPipelines\nPractical design\n(\u00a74.2)\nBoth: Can benefit from similar pipeline designs (as\nLLMs are finetuned on instructions).\nLLM chains: Vary based on students\u2019 beliefs about\nLLM strengths and weaknesses.\nDevelop frameworks that can enable practitioners to\nadapt their perception of LLM usefulness by adjusting\nprompt granularity.\nPer-step / task\nSensitivity to\ninstructions\n(\u00a74.3)\nCrowds: Can subconsciously balance trade-offs in\ninstructions, vs. LLMs need explicit prioritization.\nLLMs: Responsive to abstract instructions (\u201cmore\ndiverse titles\u201d), vs. crowdworkers face anchoring bias.\nAssess the effects of LLM instruction tuning (e.g.,\nsensitivity to adjectives, emphasis on singular needs);\nTune LLMs to follow more ambiguous instructions;\nTrain humans to identify and develop skills\ncomplementary to LLM strengths.\nOutput quality\nscaffolds (\u00a74.2)\nCrowds: Noise and disagreement resolution\nLLMs: None; LLM non-determinism is overlooked.\nTreat different LLM generations using the same prompt\nas votes of multiple LLM workers.\nOutput structure\nscaffolds (\u00a74.4)\nCrowds: Multimodal \u201cinstructions\u201d (e.g., textual\ndescriptions, interface regulations).\nLLMs: Textual instructions only.\nExtend the human-LLM alignment to also consider\noptimal modality of instruction;\nExplore mapping observations on LLM-simulated\nhumans to actual humans.\n(b) An overview of observations and reflections on students\u2019 replications on crowdsourcing pipelines.\ning certain pipeline connections (e.g., P11 in Iter-\native Process varied how the prior results should\nbe passed onto the next step). Interestingly, by ex-\namining students\u2019 final submissions and their own\nreflections, it becomes evident that (students be-\nlieve) certain pipelines require adjustments (e.g.,\nMicrotasking, and Find-Fix-Verify), while others\ncan be replicated more literally (e.g., Map-Reduce).\nThat said, most pipelines did not achieve 100%\nsuccess or effectiveness. Students largely attributed\nthe replication failure to prompting challenges \u2014\n\u201ctranslating the pipeline into a LLM required a lot\nof work (in terms of figuring out the correct prompt\n+ the pre-processing that was required in order\nto move from one step in the pipeline to the next)\ncompared to when it was implemented with crowd-\nsource workers\u201d (P14, Find-Fix-Verify). However,\nwe believe there are more nuanced reasons under-\nlying these prompting difficulties. In the following\nsections, we delve into several qualitative observa-\ntions that have emerged from the replication prac-\ntice and reflect on their implications (an overview\nof observations and opportunities are in Table 2b).\n4.2\nReplication Variance: Impacted by\nstudents\u2019 perceptions on LLM capabilities\nOne interesting aspect that emerges from the re-\nsults is that some pipelines have more replication\nvariance than others (i.e., different students\u2019 repli-\ncations to the same pipeline differ from each other\nsignificantly). For instance, while both papers pro-\nvided sufficient details for replication, the three\nparticipants replicating Iterative Process arrived at\nsimilar chains. The only difference was created by\nP11 who introduced another step for choosing top\nprevious results to show subsequent workers, i.e.,\noriginal steps were not changed.\nHowever, the three students replicating Find-\nFix-Verify implemented quite different versions\n(Figure 2): P14 mostly followed Bernstein et al.\n(2010)\u2019s descriptions (e.g., having a voting mech-\nanism in the Verify step for reducing human er-\nrors), but extended the Find step to include a lot\nmore types of writing issues. They also designed\ntheir prompts \u201cusing data structures that are eas-\nily understandable by a computer versus natural\nlanguage\u201d, because the LLM \u201chas a background\nwith computer code\u201d. P7, on the other hand, only\ndedicated the Find step to locate phrases that can\nbe shortened, and instead implemented the Verify\nstep to fix grammatical errors that arose during the\npreceding shortening steps. They explained that\nthey consciously reshaped the design because they\nbelieved that \u201cLLMs do not have these issues [of\nthe high variance of human efforts and errors].\u201d\nHowever, this belief is arguably inaccurate. Just\nFigure 2: The original pipeline and the LLM replications for (A) Iterative Process (Little et al., 2010) and (B)\nFind-Fix-Verify (Bernstein et al., 2010). While only P11 diverged from the original Iterative Process pipeline by\nadding a condition about how previous results should be ranked and used in subsequent steps, students replicating\nFind-Fix-Verify all had different Verify steps (marked in red box). The chains are slightly simplified for readability.\nlike human noise, the non-deterministic nature of\nLLMs can also lead to varying results if the same\nprompt is executed multiple times (similar to multi-\nple workers completing the same sub-task). In fact,\nprior work has applied a majority votes similar to\nVerification in Find-Fix-Verify for eliminating LLM\nnoise (Wang et al., 2022b; Yao et al., 2023), indi-\ncating that this step will still be useful for resolv-\ning the exact same issue: to remove problematic\nrewrites (now generated with LLMs). P10 simi-\nlarly removed the Verify step, possibly because of\nsimilar reasoning.\nReflection: Establish task-specific and pipeline-\nspecific best practices for using LLMs.\nThe dif-\nferent implementations of crowdsourcing pipelines\nwith LLMs showcase the varying assumptions stu-\ndents hold regarding the performance of these mod-\nels in completing certain tasks and the amount of\ninstruction needed. Indeed, with the rapid advance-\nment of LLMs and prompting techniques, it is chal-\nlenging to keep up with LLMs\u2019 capabilities and\nlimitations, as well as how they can be applied to\nspecific use cases. Instead of trying to form general\nmental models about constantly evolving LLMs, it\nmay be more beneficial for practitioners to dynami-\ncally adjust their understanding of LLM usefulness\nbased on the context of their specific use cases.\nTo achieve this, practitioners can adopt a mindset\nthat views LLMs as \u201cJack of all trades, master of\nnone/few\u201d (Koco\u00b4n et al., 2023), and employ a sys-\ntematic approach to specifying instructions, gradu-\nally moving from sparse to granular. Practitioners\ncan start by establishing a baseline using a general\nand under-specified prompt, with the assumption\nthat LLMs possess sufficient world knowledge to\ninterpret ambiguous requests. In the context of\nFind-Fix-Verify, it might be sufficient to implement\nthe Find step with a high-level command like \u201cout-\nput any errors in the text,\u201d without specifying error\ntypes. Then, if dedicated prompt testing (Ribeiro,\n2023) reveals instances where the general prompt\nfalls short, practitioners can adjust their prompts to\nincorporate more specific instructions, such as tex-\ntual instructions on corner cases, or employ prompt\nensembling techniques (Pitis et al., 2023).\nOn the other hand, it appears that students have\noverlooked the fact that LLM performs probabilis-\ntic generation during their replication practice, de-\nspite being aware of this through their own expe-\nriences and course instructions. It is intriguing\nto observe how the non-deterministic nature of\nLLM tends to be disregarded, particularly when\nused in a chaining context. This oversight may\nstem from a trade-off between creating prototype\nchain structures and fine-tuning individual prompts\nfor each sub-task (Wu et al., 2022a): LLM\u2019s non-\ndeterminism is typically presented using model\nconfidence or the probability associated with the\ngenerated output, which may become a secondary\nconsideration when students can only pass on a\nsingle output to the next sub-task. To address this,\nintroducing LLM non-determinism as \u201cnoises ex-\nposed through voting of multiple LLM workers\u201d\ncould allow for integration of disagreement mitiga-\ntion techniques like adaptive decision-making on\nthe number of votes/annotations needed (Lin et al.,\n2014; Nie et al., 2020).\n4.3\nReplication Effectiveness: Affected by\nLLM vs. Human Strengths\nSo, what are the actual strengths and weaknesses\nof LLMs, and how do they affect replicated LLM\nchains? We delve into students\u2019 reflections on the\nimplementation effectiveness and their ideas for\nimprovements. We find that, not too surprisingly,\ncrowdsourcing pipelines proven effective might\nrequire some redesigning to accommodate the\nunique capabilities of LLMs, which still differ from\nhumans\u2019. This observation aligns with discussions\nin prior work (Wu et al., 2022b; Webson et al.,\n2023); however, with the comprehensive explo-\nration of the task space through replications, two\nsignificant patterns now become more apparent:\nLLMs need explicit information foraging\nMul-\ntiple crowdsourcing pipelines require implicit infor-\nmation selection and integration. For example, in\nMap-Reduce, workers performing the Reduce step\nhad to remove unnecessary information to make\nthe final paragraph coherent. Despite the necessity,\nfew pipelines involve such explicit sub-tasks for\nselection. This might be because humans are ca-\npable of implicit information filtering, re-ranking,\nand selection (Pirolli and Card, 1999; Sperber and\nWilson, 1986; Marchionini, 1995). When it is clear\nthat certain pieces are low-quality, out-of-place,\nor redundant, humans would proactively remove\nthe unnecessary parts so as to retain a reasonable\ncognitive load. In contrast, LLMs struggle with\ninformation foraging, and tend to constantly accu-\nmulate context and produce outputs with mixed\nquality. Students observed these deficiencies at\nthree levels and proposed possible changes:\n\u2022 Fail to mitigate low-quality intermediate results.\nFor example, when writing paragraphs with\nPrice-Divide-Solve, P4 found that even conflict-\ning information from different sub-tasks would\nget integrated into the final writeup, resulting in\nincoherence (e.g., claiming the university mas-\ncot to be both a Scot and an owl). Several stu-\ndents stressed the need for intermediate quality\ncontrol, for \u201creducing the unpredictability of the\nmodel.\u201d (P13, Iterative Process).\n\u2022 Fail to selectively perform a subset of sub-tasks.\nThis is most visible in HumorTool, which, in its\noriginal design, required workers to self-select\nand sort a subset of sub-tasks (eight in total)\ninto an effective flow. Among the four students\nreplicating it, only P17 noticed that the sub-tasks\nhave \u201cno clear structure in the execution order\nof these micro tasks\u201d, and successfully imple-\nmented a chain of four sub-tasks. Other students\nagreed that eight sub-tasks aggregated too much\ninformation, and P18 later reflected that \u201cthe\nsteps should not be in such a strict order.\u201d\n\u2022 Fail to balance multiple requirements in one\nsub-task. Excessive requirements in one LLM\nprompt can also cause conflicts. In the aforemen-\ntioned HumorTool case, integrating results from\ntoo many sub-tasks may lead to certain options\ndominating others, e.g., the LLM can \u201cfocus on\nturning the joke into being sarcastic, which can\ntake away the humor from the joke\u201d (P5). Simi-\nlarly, P14 (in Find-Fix-Verify) implemented their\nFind Step (Figure 2) to simultaneous searching\nfor multiple issues, which led the LLM to priori-\ntize spelling errors and miss wordiness problems.\nOverall, explicitly stating the top criteria seem\nimportant for LLMs.\nLLMs are more sensitive to comparison-based\nthan humans.\nAs prior work has observed,\nLLMs are still sensitive to minor paraphrases (e.g.,\nP7 in Find-Fix-Verify prototyped different wordings\namong \u201cfragment\u201d, \u201cclauses\u201d, \u201csubstrings\u201d etc. in\ntheir prompt). However, on the flip side, LLMs are\nquite responsive to comparison-based instructions.\nWe will use Iterative Process for illustration. In its\noriginal design, Little et al. (2010) reported anchor-\ning bias to be an inherent limitation of the pipeline:\n\u201cperhaps owing to the fact that crowdworkers will it-\nerate & improve upon existing ideas, the variance is\nlower.\u201d All three students replicating this pipeline\nmade similar observations but also found that such\nbias could be mitigated just with straightforward\ninstructions. For example, P11 initially observed\nthat the pipeline \u201ctends to converge on a specific\ntheme\u201d, but was able to redirect the model with a\nsimple prompt: \u201cThe following ideas are examples\nof low quality, please avoid these common pitfalls.\u201d\nSimilarly, P3 was pleasantly surprised by how ef-\nfective it is to simply \u201cask for outputs that differ\nfrom the initial set\u201d \u2014 \u201cI was originally concerned\nthat providing examples would \u2018prime\u2019 the model\nto generate only examples in the same format, but\nit seems that this is not an issue in practice.\u201d Note\nthat such simple instructions are unlikely to work\nfor crowdworkers who are trapped by their personal\nbiases (Wu et al., 2021).\nThis sensitivity to adjectives such as \u201cdifferent\u201d\nand \u201cdiverse\u201d warrants further exploration. One\npeer grader highlighted this by suggesting, \u201cIf\nwe\u2019re allowed to make suggestions, we could ask\nfor titles that are happier, more obtuse, and fun-\nnier, which goes beyond traditional crowdsourc-\ning methods.\u201d This suggestion aligns with exist-\ning prompting techniques like Self-Refine (Madaan\net al., 2023), where LLMs critique their own out-\nputs to generate improved versions focusing on\nspecific dimensions.\nReflection: Examine effects of instruction tun-\ning, and train humans for complementarity.\nWhile differences between humans and LLMs are\nexpected, it is interesting how some of these dispar-\nities arise from the goal of training LLMs to mimic\nhuman behavior. For example, methods like Rein-\nforcement Learning from Human Feedback (RLHF\nOuyang et al., 2022) use human preferences to en-\nhance LLMs\u2019 ability to follow instructions. This\nmight have simultaneously enabled LLMs to iter-\nate on content based on abstract comparison com-\nmands more effectively than humans, who often get\ntrapped by cognitive bias or struggle with ambigu-\nous or vague instructions (Gershman et al., 2015).\nThat said, it is unclear whether LLM generations\nare always better in these cases, as these models\nare also biased by their training and can have po-\nlarized stands (Jiang et al., 2022; Santurkar et al.,\n2023).\nBranching out from this observation, it would be\ninteresting to explore potential \u201cside-effects\u201d of the\nLLM training schema. Prior work has highlighted\nthe trade-off between few-shot vs. zero-shot ca-\npabilities and the need to train LLMs with multi-\nfaceted human feedback (Wu et al., 2023). Consid-\nering LLMs\u2019 need for explicit information foraging,\nanother worthy line of investigation would be the\ncompleteness and clarity of instructions. As most\nexisting instruction tuning datasets prioritize high-\nquality and precise instructions (Longpre et al.,\n2023), it remains unclear how LLMs would re-\nspond to ill-defined prompts or instructions contain-\ning irrelevant information. It might be interesting to\nexamine how LLMs can be trained using a \u201cchain-\nof-instruction-clarification\u201d approach, similar to\nthe back-and-forth dialogues employed by humans\nto elicit design requirements. For instance, incor-\nporating a sub-task that involves humans clarifying\nthe top criteria could potentially enhance LLMs\u2019\nability to handle multiple requirements effectively.\nThe split of strengths also calls for human-LLM\ncomplementarity. Instead of humans or LLMs com-\npleting all sub-tasks, an effective task delegation\namong a mixture of different \u201cworkers\u201d might be\nuseful. For example, P15 in HumorTool noticed the\npartial effectiveness of their LLM chain: It excelled\nat \u201cextracting relevant attributes of a news head-\nline and brainstorming associated concepts\u201d but\nfailed at translating them into actual jokes. As such,\nexplicitly training humans to identify and develop\nskills complementary to LLM strengths could be an\ninteresting direction to pursue (Bansal et al., 2021;\nMa et al., 2023; Liu et al., 2023). Note that this\ncomplementarity can occur between humans and\na variety of LLMs. For example, P3 in Iterative\nProcess found that while using a weaker model\neither alone or in a pipeline resulted in poor perfor-\nmance, \u201cwhen I provided examples from a stronger\nmodel as the previous examples [for the weaker\nmodel to iterate on], the performance dramatically\nimproved.\u201d This observation reflects that even less-\nstate-of-the-art models can be effective teammates\nif given the appropriate task \u2014 \u201cAll models are\nwrong, but some are useful.\u201d (Box, 1976).\n4.4\nReplication Challenge: Multi-Modal\nRegulations vs. Textual Instructions\nWhen reflecting on challenges in LLM replication,\nfour students mentioned the difficulty of creating\nstructured input/output formats. For example, P7\n(replicating Find-Fix-Verify) described including a\nconstraint in their prompt: \u201cThese segments need\nto be present in the text.\u201d They stressed its impor-\ntance in the reflection: \u201cWithout this prompt, the\nreturned segments are often sentences dramatically\nrestructured based on the original text, making it\ndifficult to insert them back into the original text\nafter the fix step.\u201d Similarly, P6 in Task Paraphrase\nsaid \u201cthe major weakness of these prompts was the\nchallenge of extracting structured information out,\nespecially for the pipeline models.\u201d\nIt is worth considering why human workers,\nwho are as (if not more) \u201cgenerative\u201d as LLMs, are\ncapable of producing structured inputs and outputs.\nEssentially, all of the LLM replications of crowd-\nsourcing pipelines are partial \u2014 the assignment\nfocuses only on replicating the instructions of the\ncrowdsourcing pipeline, while other components of\ncrowdsourcing are disregarded. Specifically, nearly\nall crowdsourcing pipelines inherently include\nconstraints introduced by the user interface. For\nexample, in Find-Fix-Verify, the Find step prompts\ncrowdworkers to identify areas for abbreviation\nthrough mouse selection on text, guaranteeing that\nthe segment is precisely extracted from the original\ndocument. Similarly, He et al. (2015) required\nannotators to label their questions and answers\nin a spreadsheet interface with limited answer\nlength and predetermined question options. These\nensure that all the answers can be short phrases\nto predictable questions. Meanwhile, since LLM\nmodules/workers are solely driven by textual\ninstructions, they need additional regulation to\ncompensate for the absence of UI restrictions.\nSome students offered textual versions of syn-\ntactic constraints, e.g., \u201ca prompting system that\nallows for much stricter templates (such as the use\nof a [MASK] token) would make crowdwork-style\npipelines much easier.\u201d (P11, Iterative Process).\nOther ways might also be possible, e.g., transform-\ning generative tasks into multiple-choice tasks so\nthe LLM only outputs a single selection.\nReflection: Alignment in instruction modal-\nity, and its role in human simulation.\nWith\nthe emergence of multi-modal foundation mod-\nels (OpenAI, 2023; Ramesh et al., 2022), it be-\ncomes crucial to not only contemplate the align-\nment between humans and models in terms of in-\nstruction following but also to explore the optimal\nmodality of instruction that aligns with human in-\ntuition. For example, while LLMs have automated\nsome interactions with visualization, prior work\nhas found that users need mouse events to resolve\nvague references in their natural language com-\nmands (\u201cmake this bar blue\u201d (Wang et al., 2022c;\nKumar et al., 2017)). Instead of converting such\nactions into textual instructions, it would be more\nadvantageous to shift towards utilizing visual anno-\ntations.\nSuch challenges also have an impact on the prac-\ntical applications of LLMs. In the ongoing dis-\ncussions regarding whether LLMs can faithfully\nsimulate humans, researchers have begun investi-\ngating the feasibility of using LLMs as pilot study\nusers for efficiently refining study instructions and\ndesigns (H\u00e4m\u00e4l\u00e4inen et al., 2023). Indeed, this di-\nrection is valuable \u2014 Just like in Figure 2, both\nhumans and LLMs need \u201cprompting\u201d to complete\ntasks. Nevertheless, our findings indicate that such\na transition may not be straightforward: On the one\nhand, since LLMs only respond to textual instruc-\ntions, an important post-processing step might be\nrequired to map LLM instructions into multi-modal\nconstraints for humans. For example, instruction\n\u201cextract exact sentences\u201d might need to be mapped\nto an interface design that involves selecting spe-\ncific phrases, and \u201cparaphrase the main idea\u201d would\nrequire disabling copy-pasting from the text to dis-\ncourage direct repetition and encourage users to\nprovide their own input. On one other hand, as\nmentioned in Section 4.3, LLMs and humans may\nrespond differently to the same instructions. This\ndiscrepancy makes LLMs unreliable even for sim-\nulating human responses to tasks based solely on\ninstructions. We suspect LLMs can be useful for\nhelping study designers reflect on their high-level\nrequirements (e.g., determining what types of hu-\nman responses to collect), but the literal instruction\nhas to be redesigned. Exploring which parts of the\nuser study design can be prototyped using LLMs\nseems to be an interesting future direction.\n5\nDiscussion and Conclusion\nIn this work, we study whether LLMs can be\nused to replicate crowdsourcing pipelines through\na course assignment. We show that the modern\nmodels can indeed be used to simulate human anno-\ntation in these advanced \u201chuman computation algo-\nrithms,\u201d but the success and effectiveness of repli-\ncation varies widely depending on the nature of\nsubtasks. Further, LLMs\u2019 performance and modes\nof failure can be unintuitive, and they lack the abil-\nity to take advantage of multimodal cues that enable\nhuman workers to reliably annotate data.\nOur qualitative findings indicate two important\npoints: First, examining LLMs within established\npipelines or workflows allows for a more straight-\nforward understanding of their strengths and weak-\nnesses, as different pipeline components have dif-\nferent requirements. Second, when utilizing LLMs\nto simulate human computation, it is advantageous\nto not only focus on the inherent alignment between\nhuman and LLM outputs but also consider align-\ning additional scaffolds. This involves adapting\nexisting techniques that tackle challenges such as\nmisinterpretation of instructions by humans, noise\nin human responses, and the need to incorporate\nmulti-modal constraints for humans. Still, due to\nthe setup of the course assignment, the LLM chain\nqualities varied greatly by students\u2019 efforts and ex-\npertise. In addition, given the restricted sample\nsize, quantitative analyses would have yielded lim-\nited significance. Future work can look into more\nsystematic investigations on what components of\ncrowdsourcing pipelines could benefit from the use\nof LLM annotation, and which should continue to\nbe annotated by humans.\nFrom an education perspective, we found having\nstudents interact with LLMs actually helped cal-\nibrate their confidence in these models \u2014 Many\nstudents conveyed their frustration when LLMs\ndid not perform as effectively or reliably as they\nhad anticipated. We hope the work can inspire\nfuture exploration on allowing students to inter-\nact with LLMs and gain awareness of these mod-\nels\u2019 mistakes, thereby facilitating a constructive\nlearning process and preventing excessive reliance\non LLMs.\nWe open-source the assignment de-\nsign and student responses at https://github.com/\ntongshuangwu/llm-crowdsourcing-pipeline.\nReferences\nPromptChainer:\nCreate complex AI-driven flows\nwith ease using visual flow builder.\nhttps://\npromptchainer.io/. Accessed: 2023-7-2.\nLisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua\nGubler, Christopher Rytting, and David Wingate.\n2022.\nOut of one, many: Using language mod-\nels to simulate human samples.\nArXiv preprint,\nabs/2209.06899.\nGagan Bansal, Tongshuang Wu, Joyce Zhou, Ray-\nmond Fok, Besmira Nushi, Ece Kamar, Marco Tulio\nRibeiro, and Daniel Weld. 2021. Does the whole\nexceed its parts? the effect of ai explanations on\ncomplementary team performance. In Proceedings\nof the 2021 CHI Conference on Human Factors in\nComputing Systems, CHI \u201921, New York, NY, USA.\nAssociation for Computing Machinery.\nMichael S Bernstein. 2013. Crowd-powered systems.\nKI-K\u00fcnstliche Intelligenz, 27:69\u201373.\nMichael S Bernstein, Greg Little, Robert C Miller,\nBj\u00f6rn Hartmann, Mark S Ackerman, David R Karger,\nDavid Crowell, and Katrina Panovich. 2010. Soylent:\na word processor with a crowd inside. In Proceed-\nings of the 23nd annual ACM symposium on User\ninterface software and technology, pages 313\u2013322.\nGeorge EP Box. 1976. Science and statistics. Journal\nof the American Statistical Association, 71(356):791\u2013\n799.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. ArXiv preprint,\nabs/2303.12712.\nHarrison Chase. langchain: Building applications with\nLLMs through composability.\nJustin Cheng, Jaime Teevan, Shamsi T. Iqbal, and\nMichael S. Bernstein. 2015. Break it down: A com-\nparison of macro- and microtasks. In Proceedings of\nthe 33rd Annual ACM Conference on Human Factors\nin Computing Systems, CHI 2015, Seoul, Republic of\nKorea, April 18-23, 2015, pages 4061\u20134064. ACM.\nLydia B Chilton, James A Landay, and Daniel S Weld.\n2016. Humortools: A microtask workflow for writ-\ning news satire. El Paso, Texas: ACM.\nLydia B. Chilton, Greg Little, Darren Edge, Daniel S.\nWeld, and James A. Landay. 2013. Cascade: crowd-\nsourcing taxonomy creation. In 2013 ACM SIGCHI\nConference on Human Factors in Computing Systems,\nCHI \u201913, Paris, France, April 27 - May 2, 2013, pages\n1999\u20132008. ACM.\nTyna Eloundou, Sam Manning, Pamela Mishkin, and\nDaniel Rock. 2023. Gpts are gpts: An early look at\nthe labor market impact potential of large language\nmodels. ArXiv preprint, abs/2303.10130.\nSamuel J Gershman, Eric J Horvitz, and Joshua B Tenen-\nbaum. 2015. Computational rationality: A converg-\ning paradigm for intelligence in brains, minds, and\nmachines. Science, 349(6245):273\u2013278.\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. ArXiv preprint, abs/2303.15056.\nPerttu H\u00e4m\u00e4l\u00e4inen, Mikke Tavast, and Anton Kunnari.\n2023. Evaluating large language models in gener-\nating synthetic hci research data: a case study. In\nProceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems, pages 1\u201319.\nLuheng He, Mike Lewis, and Luke Zettlemoyer. 2015.\nQuestion-answer driven semantic role labeling: Us-\ning natural language to annotate natural language.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n643\u2013653, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nJeff Howe et al. 2006. The rise of crowdsourcing. Wired\nmagazine, 14(6):176\u2013183.\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-\njuan Han, Chi Zhang, and Yixin Zhu. 2022. Evaluat-\ning and inducing personality in pre-trained language\nmodels.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022. Decomposed prompting: A modular\napproach for solving complex tasks. ArXiv preprint,\nabs/2210.02406.\nJoy Kim, Sarah Sterman, Allegra Argent Beal Cohen,\nand Michael S Bernstein. 2017. Mechanical novel:\nCrowdsourcing complex work through reflection and\nrevision. In Proceedings of the 2017 acm conference\non computer supported cooperative work and social\ncomputing, pages 233\u2013245.\nAniket Kittur, Boris Smus, Susheel Khamkar, and\nRobert E Kraut. 2011. Crowdforge: Crowdsourc-\ning complex work. In Proceedings of the 24th an-\nnual ACM symposium on User interface software and\ntechnology, pages 43\u201352.\nJan Koco\u00b4n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szyd\u0142o, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, et al. 2023. Chatgpt: Jack of all trades,\nmaster of none. Information Fusion, page 101861.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199\u2013\n22213.\nAnand Kulkarni, Matthew Can, and Bj\u00f6rn Hartmann.\n2012.\nCollaboratively crowdsourcing workflows\nwith turkomatic. In Proceedings of the acm 2012\nconference on computer supported cooperative work,\npages 1003\u20131012.\nAbhinav Kumar, Barbara Di Eugenio, Jillian Aurisano,\nAndrew Johnson, Abeer Alsaiari, Nigel Flowers, Al-\nberto Gonzalez, and Jason Leigh. 2017. Towards\nmultimodal coreference resolution for exploratory\ndata visualization dialogue: Context-based annota-\ntion and gesture identification. In The 21st Workshop\non the Semantics and Pragmatics of Dialogue (Sem-\nDial 2017\u2013SaarDial)(August 2017), volume 48.\nEdith Law and Haoqi Zhang. 2011. Towards large-scale\ncollaborative planning: Answering high-level search\nqueries using human computation. In Proceedings\nof the Twenty-Fifth AAAI Conference on Artificial\nIntelligence, AAAI 2011, San Francisco, California,\nUSA, August 7-11, 2011. AAAI Press.\nChristopher Lin, Daniel Weld, et al. 2014. To re (label),\nor not to re (label). In Proceedings of the AAAI Con-\nference on Human Computation and Crowdsourcing,\nvolume 2, pages 151\u2013158.\nGreg Little, Lydia B Chilton, Max Goldman, and\nRobert C Miller. 2010. Exploring iterative and paral-\nlel human computation processes. In Proceedings of\nthe ACM SIGKDD workshop on human computation,\npages 68\u201376.\nMichael Xieyang Liu, Advait Sarkar, Carina Negreanu,\nBenjamin Zorn, Jack Williams, Neil Toronto, and\nAndrew D Gordon. 2023. \u201cwhat it wants me to say\u201d:\nBridging the abstraction gap between end-user pro-\ngrammers and code-generating large language mod-\nels. In Proceedings of the 2023 CHI Conference on\nHuman Factors in Computing Systems, pages 1\u201331.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. ArXiv preprint, abs/2301.13688.\nQianou Ma, Tongshuang Wu, and Kenneth Koedinger.\n2023. Is ai the better programming partner? human-\nhuman pair programming vs. human-ai pair program-\nming.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. ArXiv preprint, abs/2303.17651.\nGary Marchionini. 1995. Information seeking in elec-\ntronic environments. 9. Cambridge university press.\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What\ncan we learn from collective human opinions on nat-\nural language inference data? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9131\u20139143,\nOnline. Association for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nPeter Pirolli and Stuart Card. 1999. Information forag-\ning. Psychological review, 106(4):643.\nSilviu Pitis, Michael R Zhang, Andrew Wang, and\nJimmy Ba. 2023.\nBoosted prompt ensembles\nfor large language models.\nArXiv preprint,\nabs/2304.05970.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. ArXiv preprint, abs/2210.03350.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022.\nHierarchical text-\nconditional image generation with clip latents. ArXiv\npreprint, abs/2204.06125.\nDaniela Retelny, Michael S Bernstein, and Melissa A\nValentine. 2017. No workflow can ever be enough:\nHow crowdsourcing workflows constrain complex\nwork. Proceedings of the ACM on Human-Computer\nInteraction, 1(CSCW):1\u201323.\nMarco Tulio Ribeiro. 2023. Testing language models\n(and prompts) like we test software. Medium.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo\nLee, Percy Liang, and Tatsunori Hashimoto. 2023.\nWhose opinions do language models reflect? ArXiv\npreprint, abs/2303.17548.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nDan Sperber and Deirdre Wilson. 1986. Relevance:\nCommunication and cognition, volume 142. Cite-\nseer.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F. Christiano. 2020. Learn-\ning to summarize with human feedback. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nPetter T\u00f6rnberg. 2023. Chatgpt-4 outperforms experts\nand crowd workers in annotating political twitter\nmessages with zero-shot learning. ArXiv preprint,\nabs/2304.06588.\nVasilis Verroios and Michael S Bernstein. 2014. Con-\ntext trees: Crowdsourcing global understanding from\nlocal views. In Second AAAI Conference on Human\nComputation and Crowdsourcing.\nVeniamin Veselovsky, Manoel Horta Ribeiro, and\nRobert West. 2023. Artificial artificial artificial intel-\nligence: Crowd workers widely use large language\nmodels for text production tasks. ArXiv preprint,\nabs/2306.07899.\nXuezhi Wang, Haohan Wang, and Diyi Yang. 2022a.\nMeasure and improve robustness in NLP models: A\nsurvey. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4569\u20134586, Seattle, United States.\nAssociation for Computational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. 2022b. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. ArXiv preprint, abs/2203.11171.\nYun Wang, Zhitao Hou, Leixian Shen, Tongshuang Wu,\nJiaqi Wang, He Huang, Haidong Zhang, and Dong-\nmei Zhang. 2022c. Towards natural language-based\nvisualization authoring. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(1):1222\u20131232.\nAlbert Webson, Alyssa Marie Loo, Qinan Yu, and Ellie\nPavlick. 2023. Are language models worse than hu-\nmans at following prompts? it\u2019s complicated. ArXiv\npreprint, abs/2301.07085.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nTongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff\nGray, Alejandra Molina, Michael Terry, and Carrie J\nCai. 2022a. Promptchainer: Chaining large language\nmodel prompts through visual programming. In CHI\nConference on Human Factors in Computing Systems\nExtended Abstracts, pages 1\u201310.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and\nDaniel Weld. 2021. Polyjuice: Generating counter-\nfactuals for explaining, evaluating, and improving\nmodels. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6707\u20136723, Online. Association for Computa-\ntional Linguistics.\nTongshuang Wu, Michael Terry, and Carrie Jun Cai.\n2022b.\nAi chains: Transparent and controllable\nhuman-ai interaction by chaining large language\nmodel prompts. In Proceedings of the 2022 CHI\nconference on human factors in computing systems,\npages 1\u201322.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane\nSuhr, Prithviraj Ammanabrolu, Noah A Smith,\nMari Ostendorf, and Hannaneh Hajishirzi. 2023.\nFine-grained human feedback gives better rewards\nfor language model training.\nArXiv preprint,\nabs/2306.01693.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models. ArXiv\npreprint, abs/2305.10601.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences.\nArXiv\npreprint, abs/1909.08593.\n"
  },
  {
    "title": "Android in the Wild: A Large-Scale Dataset for Android Device Control",
    "link": "https://arxiv.org/pdf/2307.10088.pdf",
    "upvote": "9",
    "text": "Android in the Wild: A Large-Scale Dataset for\nAndroid Device Control\nChristopher Rawles\u2217\nGoogle Research\nAlice Li\u2217\nGoogle Research\nDaniel Rodriguez\nGoogle Research\nOriana Riva\nGoogle Research\nTimothy Lillicrap\nGoogle DeepMind\nAbstract\nThere is a growing interest in device-control systems that can interpret human\nnatural language instructions and execute them on a digital device by directly con-\ntrolling its user interface. We present a dataset for device-control research, Android\nin the Wild (AITW), which is orders of magnitude larger than current datasets. The\ndataset contains human demonstrations of device interactions, including the screens\nand actions, and corresponding natural language instructions. It consists of 715k\nepisodes spanning 30k unique instructions, four versions of Android (v10\u201313),\nand eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions.\nIt contains multi-step tasks that require semantic understanding of language and\nvisual context. This dataset poses a new challenge: actions available through the\nuser interface must be inferred from their visual appearance, and, instead of simple\nUI element-based actions, the action space consists of precise gestures (e.g., hori-\nzontal scrolls to operate carousel widgets). We organize our dataset to encourage\nrobustness analysis of device-control systems, i.e., how well a system performs in\nthe presence of new task descriptions, new applications, or new platform versions.\nWe develop two agents and report performance across the dataset. The dataset\nis available at https://github.com/google-research/google-research/\ntree/master/android_in_the_wild.\n1\nIntroduction\nUsers complete tasks on mobile devices via a sequence of touches and gestures on the screen. Tasks\ncan often be succinctly described using natural language commands, and, in many situations, it\nis valuable to be able to speak or type commands rather than interacting directly with the device.\nThis has important implications for users who are unable to physically operate a device due to a\nphysical (e.g., visual or motor disabilities) or situational (e.g., driving, cooking, etc.) impairment. It\nis therefore beneficial to build device-control systems that can interpret natural language instructions\nand execute them on a device without any manual intervention.\nInstead of using application-specific APIs, which are not generally available for any given application\nor function, these systems directly manipulate user interface (UI) elements on a screen, exactly\nas a human does [1, 28, 29, 35, 21]. Hence, to work correctly, it is essential for such systems to\nunderstand the screen, which usually means detecting position and inferring semantics of its UI\nelements. Device-control systems must also be able to map high-level commands to execution plans\nthat can be carried out on the device. For example, understanding that the command \u201copen my recent\nemail with Jane\u201d involves opening an email app, potentially tapping the search icon, typing \"Jane\",\netc. Further, to be useful, they must be able to generalize across a variety of task instructions and UIs.\n\u2217Equal contribution. Contact: crawles@google.com and lialice@google.com\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2307.10088v2  [cs.LG]  27 Oct 2023\nHuman executes \ntask on a mobile \nemulator\n15K multi-step prompts\nSample from \ninstruction \ndatabase\nepisodes\n26K hindsight relabeled\n689K multi-step demos\nHuman selects \nframes and \nannotates them\n\u201cOpen calendar and \nshow me the second \nweek of next month\u201d\n1. \u201cshow week view\u201d\n2.\u201cswitch to week view\u201d\n\u2026\nfor each action: extract preceding screenshot and \npixel-based screen features\n{'action_type': 'dual-point-gesture',\n 'touch_point': (0.28,0.19), \n 'lift_point': (0.28,0.19),\n 'typed_text': None}\n+\nDemonstration processing\n+\n++\n+\n{'action_type': 'dual-point gesture',\n 'touch_point': (0.90,0.41), \n 'lift_point': (0.05,0.35),\n 'typed_text': None}\n+\n+\nOCR text \n& icons\nOCR text \n& icons\nFigure 1: AITW data pipeline. Raters are given a randomly selected instruction. The raters execute\nthe task by interacting with the device in a natural way. We capture precise gestures in addition to\ntyping and the home and back button interactions (we plot swipes with the arrow pointing where the\nfinger moves to). Hindsight relabeling of high-level episodes is used to generate single-step tasks.\nThe rapid development of general-purpose large foundation models (LLMs) [8, 6, 13] makes device-\ncontrol systems more viable. Yet, there is a lack of datasets for training, fine-tuning, and evaluating\nthese systems. Existing datasets [28, 9, 42, 37, 4] are limited in terms of number of human demon-\nstrations and the diversity of task instructions, and they are platform specific (either Android or web).\nThey also assume a tree-based representation of an application UI can be derived from platform-\nspecific UI metadata (e.g., the View Hierarchy for Android and the DOM tree for the web). This\nassumption simplifies the problem, but limits the resulting systems to work in environments where\nhigh-quality UI metadata is available2. Finally, some popular datasets (e.g., MiniWoB++ dataset [29]\nand UIBert [4]) assume task instructions are specified as step-by-step commands referring to specific\nUI elements appearing on the screen (\u201cClick the button in the dialog box labeled Cancel\u201d), while\nusers may use short commands that describe high-level goals (e.g., \u201cturn on airplane mode\u201d) or pose\nquestions (e.g., \u201cIs it going to rain tomorrow?\u201d)\nTo drive research in this field, we release AITW (Figure 1), an Android device-control dataset which\nis orders of magnitude larger than existing datasets. It consists of 715k episodes spanning 30k unique\ntask instructions collected across hundreds of Android apps and websites. Each episode consists of a\ngoal instruction provided in natural language and a sequence of observation-action pairs describing\nthe execution of the task. Observations consist of screenshots of the application UI. Gesture actions\nare represented as taps and drags at arbitrary <x,y> coordinates in the screen. Agents trained on this\ndataset can be evaluated using AndroidEnv [40], an open-source platform for developing and testing\nAndroid agents with the Android Emulator3.\nA key feature of our dataset is the diversity of task instructions and execution paths we collected,\naimed to emulate real-world scenarios. We used multiple sources to collect high-level goal instruc-\n2Since most users do not use UI metadata for interactions it tends to be poor quality or missing altogether. On\nAndroid, only applications registered as Accessibility tools can access the View Hierarchy [45]. On Windows, in\nmany cases (e.g., Electron apps like Teams), UI trees are not easily accessible. Moreover, screen representations\nderived from UI metadata can be incomplete. On Android, WebViews and Canvas are not captured in the View\nHierarchy, and many websites render directly to a Canvas, which does not contain any tree structure.\n3https://developer.android.com/studio/run/emulator\n2\nDataset\nPlatform\n# Human\n# Apps or\n# Task\nObservation\nScreen\nReal\nHigh-level\ndemos\nwebsites\nsteps\nformat\nfeatures\ninstruction\nRicoSCA [28]\nAndroid (apps)\n0\nn/a\n1.0\nVH, screen\nx\nx\nx\nUIBert [4]\nAndroid (apps)\n16,660\nn/a\n1.0\nVH, screen\nx\n\u2713\nx\nMiniWoB++ [37, 29]\nsynthetic web\n17,971\n100\n2.3\nDOM, screen\nx\nx\nx\nPixelHelp [28]\nAndroid (apps)\n187\n4\n4.2\nVH, screen\nx\n\u2713\n\u2713\nUGIF [42]\nAndroid (apps)\n523\n12\n5.3\nVH, screen\nx\n\u2713\n\u2713\nMind2Web [14]\nweb\n2,350\n137\n7.3\nDOM, screen\nx\n\u2713\n\u2713\nMoTIF [9]\nAndroid (apps)\n4,707\n125\n4.5\nVH, screen\nx\n\u2713\n\u2713\nAITW\nAndroid (apps+web)\n715,142\n357+\n6.5\nscreen\n\u2713\n\u2713\n\u2713\nTable 1: Comparison of AITW to existing datasets. We consider platform, format of screen obser-\nvations, presence of synthetic UIs or synthetic instructions (\u201cReal\u201d), and whether instructions are\nexpressed as goals (high-level). For size comparison, we report the number of human demonstrations,\napps/websites, and average task steps. AITW collects observations as screenshots and includes screen\nfeatures (OCR and icon labels), which can be used to augment them.\ntions: humans (both crowdsourced raters and us authors), LLM-generated prompts, and technical\ndocumentation (as in PixelHelp [28]). During crowdsourcing, raters were asked to both demonstrate\nfull tasks and annotate sequences of screenshots (hindsight language relabeling [31, 32]), which\nallowed us to collect both multi-step and single-step task trajectories. We made the execution paths\nmore varied by randomizing the application state, which forced the raters to demonstrate how to\nnavigate to the relevant screens. Finally, we collected demonstrations on four versions of Android\n(v10\u201313) and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions.\nDevice-control systems need to work on rapidly evolving software platforms, so an important metric\nfor their success is generalizability to new tasks and applications. We organize our dataset to enable\nanalysis of how trained systems perform in the presence of previously-seen tasks and applications,\nbut also in the presence of new task descriptions, new Android versions, and new applications. Due\nto the lack of off-the-shelf pixel-based device-control models, to establish new state-of-the-art results\non this dataset, we implement two agents: one trained from scratch using behavioural cloning (BC)\nand a second based on a pre-trained LLM.\nWe make the following contributions: (i) we collect and release a dataset for device-control research,\nAITW, which is larger and more varied than existing datasets; (ii) we report performance for two\nmodels, which can serve as baselines for future work and (iii) we show how to use the dataset to\nconduct a generalization analysis.\n2\nRelated work\n2.1\nDevice-control datasets\nTable 1 provides a comparison of device-control datasets. Some datasets (top part of the table) target\nthe problem of grounding referring expressions to UI elements on a screen. Every data instance\nin these datasets includes a screen, a low-level command (e.g., \"click the menu button at the top\nleft\"), and a UI element corresponding to the command. In the RicoSCA dataset [28], commands are\nsynthetically generated, while in MiniWoB++ [37, 29] sequences of low-level UI commands describe\nmulti-step tasks (e.g., \u201cfind and click on the center of the circle, then press submit\u201d).\nA second group of datasets contain instructions expressed as task goals. Each episode in these\ndatasets is a sequence of action-observation pairs. The observations include screenshots and tree-\nbased representations: View Hierarchy (VH) for Android and Document Object Model (DOM) for\nweb-based applications. For instance, the PixelHelp dataset [28] comprises 187 high-level task\ngoals and step-by-step instructions sourced from Pixel Phone Help pages. The UGIF dataset [42]\ncontains similar queries but extends to multiple languages. The largest dataset to date is MoTIF [9],\nwhich consists of 4.7k task demonstrations4 with an average number of 6.5 steps and 276 unique\ntask instructions. AITW is two orders of magnitude larger than MoTIF. In total, AITW consists of\n715,142 episodes, spanning 30,378 unique prompts, with a small subset of the prompts inspired by\n4This represents the number of \u201cfeasible\u201d tasks. We do not consider tasks without a valid demonstration.\n3\nName\nTask type\nDescription\nEpisodes\nScreens\nPrompts\nGOOGLEAPPS\nMulti-step\nTasks with Google apps (Gmail, Photos, Settings, etc.)\n625,542\n4,903,601\n306\nINSTALL\nApp installation and login tasks\n25,760\n250,058\n688\nWEBSHOPPING\nWeb shopping tasks\n28,061\n365,253\n13,473\nGENERAL\nMisc web/app tasks and Q&A\n9,476\n85,413\n545\nSINGLE\nSingle-step\nMostly shopping tasks from WEBSHOPPING\n26,303\n85,668\n15,366\nTotal\n715,142\n5,689,993\n30,378\nTable 2: Composition of the AITW dataset.\nthe PixelHelp dataset. Observations are represented by screenshots along with pixel-based screen\nfeatures.\n2.2\nUI representation and automation models\nResearch on device control is mainly focused on two problems: understanding UIs and automating\ntasks. Existing work on the first problem utilizes self-supervised [21, 4, 5, 27] and supervised\nmethods [10, 30, 11, 46] to train UI understanding models. In some cases, these models are fine-tuned\nfor simple grounding tasks (e.g., referring expression component retrieval [4]), along with widget\ncaptioning or question answering tasks [27, 4].\nFor task automation, Li et al. [28] decompose the problem in two stages: an action phrase-extraction\nstage to transform step-by-step instructions into actionable phrases, and a grounding stage that\nexecutes these instructions. Venkatesh et al. [42] utilize an LLM to parse the instruction before\nexecuting \u201cmacros\u201d (e.g., tap(), toggle()) during the grounding phase. AppBuddy [38] train an RL\nagent to interact with on-screen UI elements to achieve tasks. LLMs can also understand and operate\nUI screens [43]. On the web front, previous studies have developed RL [20, 25, 29, 18], behavioral\ncloning [24], and LLM-based models [19, 26, 17]. These approaches utilize Document Object Model\n(DOM) inputs and often evaluate results on a simulated environment, MiniWob++ [37, 29]. Finally,\nLLMs have shown impressive results leveraging APIs, when they are available, for performing\nhigher-level tasks [36, 33, 34].\n3\nAndroid in the Wild (AITW)\nTable 2 shows the composition of AITW in terms of category and type of tasks. Overall, AITW\nconsists of four multi-step datasets, GOOGLEAPPS, INSTALL, WEBSHOPPING, and GENERAL, along\nwith a single-step dataset SINGLE.\nThe dataset is collected in a two-stage pipeline shown in Figure 1. First, we ask the raters to perform\nend-to-end tasks on emulators. Then the raters apply hindsight language relabeling [31, 32] to the\ntrajectories that were collected in the first stage. We ask the raters to identify and label simple action\nsequences. We refer to these as single-step tasks.\nOur recording system uses AndroidEnv [40] with the Android Emulator. The environment supports 3\naction types {TOUCH, LIFT, REPEAT} with an (x,y) tuple indicating the on-screen position of the\naction. We record the TOUCH and LIFT actions. In response to an action, the environment returns an\nRGB screenshot, along with additional metadata such as the opened application. Raters interact with\nthe emulated device using a mouse and keyboard on a desktop computer. Click events are logged\nas touch events. We provide dedicated buttons for Home, Back and Enter actions along with a field\nfor entering text. We encourage the raters to use the dedicated buttons when necessary, however we\nrequire them to use a dedicated input text field for typing; we do not allow them to use the on-screen\nkeyboard. We also ask the raters to indicate when they have completed the task or if they deem the\ntask to be impossible to complete by pressing a button on our data collection UI.\nThe system captures the raw observations and actions at 10Hz. Mouse presses and releases are\nrecorded as TOUCH and LIFT, respectively. For touch events, we log the start and end position of the\nvirtual finger\u2019s gesture, which we call a \"dual-point\" gesture. A scroll is represented by a start and\nend position, and a tap is a special case where the start and end are approximately equal (<= 0.04\nEuclidean distance away). Figure 1 contains an example of a tap and horizontal scroll gesture using\n4\nthis formulation. We found the dual-point gesture abstraction to be a good trade-off between data\ncompression and precision, allowing us to represent arbitrary drags that are needed to operate widgets,\nincluding scrolling through a menu and operating carousel widgets. After identifying dual-point\ngestures, we drop LIFT actions. Button presses and type events are logged as additional actions types.\nFor type events, we log the typed text.\nIn summary, AITW\u2019s actions are described by four fields: type, touch_point, lift_point (only for\ngesture actions), and typed_text (only for typing actions). The type field can be one of the following:\ndual-point gesture, type, go_back, go_home, enter, task_complete, or task_impossible.\nWe post-process RGB screenshots to map them to a set of detected UI elements. Each element has a\nbounding box and either OCR-detected text or an icon class label (one of the possible 96 icon types\ndetected using IconNet [39]). The OCR outputs describe most of the text on the screen, although\ncertain characters can be misidentified and text blocks are not always grouped as desired. Although\nthis screen representation inferred from pixels is noisy and not as comprehensive as that obtained\nfrom UI metadata, we provide these features for convenience and expect developers will replace them\nwith more powerful screen understanding models. We use these features for training and evaluating\nour models.\n3.1\nMulti-step task trajectories\nWe first create high-level task instructions from various sources: (1) the authors, (2) a subset of\nPixelHelp [28] instructions that were deemed achievable, and (3) an LLM prompted to generate\ninstructions. Next, we randomly assign instructions to raters and they follow them to complete tasks.\nEvery task requires multiple steps to be performed. For example, the task \u201cshow my schedule for next\nweek in Google Calendar\u201d could correspond to the following steps: 1) opening Google calendar, 2)\nselecting \"week view\", and 3) opening next week. For each episode, we reset the environment to a\nrandom starting screen.\nWe ask the raters to interact with the device in a natural way, to avoid clicking on anything unrelated\nto the task, and to avoid unnecessary scrolling. To help guide the raters we prompt them with the\nfollowing \u201cImagine a friend is asking you to perform the task on their phone...\u201d The raters end a task\nwith a special \"status\" action: either task_complete or task_impossible. A task is deemed impossible\nwhen an invalid or unavailable instruction is given, e.g., \u201cturn on flashlight\u201d on an emulator or \u201cshow\nmy starred emails\u201d when the Internet is not available. For instructions that result in verification rather\nthan a state change (e.g., if the prompt is \u201cTurn wifi off\u201d and WiFi is found to be already off), we ask\nthe raters to mark the task as successful.\n3.2\nHindsight language relabeling\nSingle-step task demonstrations cannot be collected in the usual way of giving raters instructions and\nasking them to solve end-to-end tasks, since they require the relevant preceding steps to be executed.\nFor example, in the task we described above, we cannot ask the raters to demonstrate \u201cgo to next\nweek\u201d unless they are already in the week view of the calendar app. Rather than asking raters to\nmanually perform the single steps, we utilize event-selectable hindsight language relabeling [31, 32]\nto label previously collected trajectories.\nTo collect single-step demonstrations, we provide the raters observation-action sequences of multi-\nstep task trajectories and ask them to identify and annotate shorter sequences (around two to five\nframes). We instruct them to label single steps, e.g., \u201cadd item to cart\u201d, \u201cshow the settings\u201d, \u201cshow\nme my bookmarks\u201d. We ask that they label at least K subsequences (K >= 3 in our case) per video.\nWe instruct the raters to avoid the following words: \u201cclick\u201d, \u201cselect\u201d, \u201ctap\u201d, \u201ctouch\u201d or \u201cscroll\ndown/up/left/right\u201d, since these can be easily synthetically created, and instead ask them to write\ndescriptive phrases that describe the result of the action (e.g., instead of \u201ctap airplane mode\u201d, write\nthe label \u201cdisable airplane mode\u201d).\n3.3\nDataset summary\nWith reference to Table 2, we describe the 5 sub-categories of AITW.\n5\n(a)\n(b)\n(c)\n(d)\nFigure 2: Statistics for AITW. a) Episode length distribution. b) Episode length distribution by\ndataset group. c) Frequency of Android apps in the dataset. d) Token analysis including distribution\nof instruction length and token frequency for GOOGLEAPPS and GENERAL.\nGOOGLEAPPS contains high-level tasks with some overlap from PixelHelp [28] which involve\nvarious Google applications such as Gmail, Calendar, Photos, Settings, etc.\nINSTALL contains high-level tasks related to installing and uninstalling apps, app login, and app\nlogin support (e.g., \"forgot password\") for 88 different apps available on the Google Play store.\nWEBSHOPPING contains tasks related to shopping on E-commerce websites. Example tasks include\nsearching for an item, adding an item to the cart, viewing the shopping cart, etc.\nGENERAL contains miscellaneous tasks (e.g., \u201cplay the new Taylor Swift video on YouTube\u201d), mostly\ncentered around question and answering (Q & A) (e.g., \u201cHow much does a 2 bedroom apartment\nrent cost in San Francisco?\u201d) and interacting with third-party apps and websites.\nSINGLE contains single-step tasks manually annotated using hindsight relabeling, mostly from\nWEBSHOPPING (e.g., \u201cClose the pop-up then add first item to cart\u201d,\u201cclear items from cart\u201d). It also\ncontains a smaller amount of episodes (560) from a variety of Google apps and third-party websites.\nIn Figure 2, we report statistics about AITW. The episode length distribution (Figure 2a), measured\nas number of steps required to complete the task, shows that tasks are of moderate length (between 2\nand 16 steps for the 5th to 95th percentile, respectively) and that WEBSHOPPING tasks are generally\nthe longest (Figure 2b). Chrome and Google apps are the most commonly used apps (Figure 2c).\nOverall, the dataset spans 159 Android apps and 198+ websites.5\n(Figure 2d) shows summary statistics of the instructions. Instructions lengths fall between 4 and\n24 for the 5th to 95th percentile, respectively, and are not overloaded with technical terms such as\n\u201cclick\u201d, \u201ctap\u201d, \u201cmenu\u201d, \u201cbutton\u201d, etc. which is generally the case for low-level UI commands provided\nin existing datasets [37, 4].\n4\nExperimental setup\nWith the ultimate goal of building automation systems that can generalize to new scenarios, we use a\nstandard test split and also design four experimental setups to evaluate Out-of-Distribution (OOD)\ngeneralization.\n5This number is a conservative estimate computed using heuristics.\n6\nTable 3: Examples of subject templates.\nInstruction\nSubject template\nSplit\nopen app grab and go to login screen\nopen {subject1} and\ntrain\nopen walmart and go to shopping cart\ngo to {subject2}\ntrain\nsearch newegg.com on google\nsearch {subject1}\nval\nsearch usb-c to usb-a on ebay\non {subject2}\nval\nadd jbl flip 4 to the cart on bestbuy\nadd {subject1} to the\ntest\nadd acer nitro to the cart on target\ncart on {subject2}\ntest\nStandard. We randomly split each dataset (the four multi-step datasets and SINGLE) episode wise\ninto a training, validation, and test set (80/10/10%). Because the datasets different sizes, we evaluate\neach of them separately, then take the average score across them; we do the same for OOD setups.\nUnseen Android version. To evaluate a system\u2019s performance on an unseen Android version \u2014\nwhich contains unseen graphical components and execution flows \u2014 we partition our data as follows:\nWe put episodes collected on Android versions 10, 11, and 12 into the training and validation sets,\nmaintaining a 90/10% split respectively. Then, we create a separate test set comprising entirely of\nepisodes captured on Android version 13 devices.\nUnseen subject and unseen verb. This setup is aimed at evaluating generalization to unseen\ninstructions. Due to the large number of prompts in AITW, it is infeasible to manually group similar\ntasks together. Simply splitting based on exact match of the raw instructions would be the most\nstraightforward way to automatically assign splits. However, similar instructions with minor changes\nin language would potentially be seen in both training and testing.\nTo better differentiate the training and test sets, we develop instruction templates by masking out\neither verb or subject phrases (examples provided in Table 3). By splitting data based on these\ntemplates, we can assess a system\u2019s ability to generalize to unseen language patterns, and occasionally\nto entirely new tasks. For instance, all instructions following the template add {subject1} to the cart\non {subject2}\" are grouped together, ensuring they are not represented in both training and testing\nsets. Similarly, verb-based templates such as open the shopping cart\" and \u201cview the shopping cart\"\nwould be assigned to the same split.\nWe extract the templates for each instruction, by prompting a few-shot LLM [13]. In total, we extract\n6,111 subject templates and 22,122 verb templates. For both types, we randomly assign each template\nto a train, validation or test split (with 80/10/10%). Then for each episode, we determine its template\nbased on its instruction, and map the episode to a split.\nUnseen domain. This split is designed to test an agent\u2019s ability to generalize to unseen apps and\nwebsites, which we refer to as domains. For WEBSHOPPING and GENERAL, we perform the split\nbased on the web domain, as inferred from the instructions. For INSTALL tasks, we divide the data\nbased on the app name, but we restrict these tasks to only those that require interaction with the\ninstalled app (e.g., performing a \u2019forgot password\u2019 request). Each domain, along with all associated\nepisodes, is randomly assigned to a train/validation/test split (80/10/10%). We exclude SINGLE, as\nthere are no distinguishable domains across tasks, and GOOGLEAPPS, due to the limited number of\ndistinct apps.\n5\nExperiments\nIn this section, we report results of two device-control agent models evaluated on AITW. Both models\ntake as input a task instruction, the current screen\u2019s pixel-derived features (included in the dataset),\nand (optionally) a stacked history of screen observations and actions.\n5.1\nModels\nBC. We implement a Transformer-based [41] Behavioural Cloning (BC) agent. The agent\u2019s output is\nin line with the AITW\u2019s data format. It outputs an action type and a gesture. The action type can be\ndual-point gesture, type, go_back, go_home, enter, task_complete, or task_impossible. The gesture\n7\naction includes two spatial points, a touch and a lift position. This approach gives this agent a large\nand flexible action space, as it is able to predict taps and scrolls at arbitrary locations, rather than at\nspecific UI elements as in existing work [4, 28]. We consider two variants of the agent, depending\non whether it takes as input the screen-action history (2 prior steps), BC-history, or not, BC-single.\nAppendix B.1 provides more implementation details.\nLLM. We feed to PaLM 2 [3] a textual description of the screen and ask it to predict an action\namong the supported actions in AITW. We adopt a previously-proposed LLM-based design for\ndevice control [43], where the input screen (represented by an Android VH) is converted to HTML\nsyntax. We use a modified version of their prompt (see Appendix B.2), and convert the OCR and\ndetected icons to HTML. We create a zero-shot (LLM-0) and a 5-shot Chain-of-Thought (CoT) [44]\n(LLM-hist-5-CoT) version, which also contains history on prior actions taken by the agent, as we\nobserved improves model performance. This model takes the same inputs as the BC model, but as in\nthe original implementation [43], it can only click on detected UI elements, rather than at arbitrary\nlocations and scrolling at precise locations. Since AITW was collected by humans performing precise\ngestures, some of the recorded gestures are not associated with OCR/Icon-detected UI elements, thus\nbeing not feasible for the LLM-based model. This could potentially be ameliorated in future versions\nby outputting a <x,y> output, rather than tapping specific elements.\n5.2\nEvaluation methodology and metrics\nOnline evaluation of device-control systems is hard because the execution environment generally does\nnot provide a reward signal. Human validation of such systems can be leveraged, however watching\nand judging an agent\u2019s behaviour in real-time requires constant attention and is error prone. We\npropose an offline evaluation method which is cheaper and reproducible at the expense of accuracy.\nWe devise and release the code for action matching to evaluate an agent\u2019s action\u2019s alignment with\nthe ground truth. Two actions can match if their action types are equal. For dual-point taps, they are\nconsidered equal if they fall within a 14% screen distance from each other. Alternatively, if the tap\nactions occur within the same detected bounding box (augmented to 240% of their total size during\naction matching) they are considered equal. Finally, two dual-point scrolls are equal if they have the\nsame primary scroll axis (vertical or horizontal).\nUsing action matching, we compute partial and complete action matching scores (originally proposed\nby Li et al. [28]). A partial score is defined as the number of correct actions divided by the episode\nlength, and the complete score is defined as a partial match of 1.0.\nTo validate offline evaluation results, for subsets of the data, we also perform online evaluation. A\nhuman marks an episode as failed if any of the agent actions are incorrect, and correct when the agent\nperforms a correct action on every step and achieves the expected goal. Human validation scores\ntypically outperform complete action matching scores due to the multiple valid action alternatives\none can take to complete a task. For instance, pressing the navigation bar\u2019s back button is functionally\nsimilar to using an app-specific back button. As action matching relies on distance-based measures,\nthese actions are deemed distinct.\n5.3\nResults\nWe evaluate the four agents on the five AITW splits described in \u00a74. For the BC agent, we train and\ntest using all the data. For the LLM agent, due to the high computational overhead, we test on a\nrandom sample of 288 episodes for each split. Table 4 reports the average partial matching scores.\nThe BC agent performs the best across all splits. It performs reasonably well on the OOD tasks,\nparticularly on the subject and verb template splits, indicating the model is generalizing to unseen\nlanguage instructions and tasks. The LLM-based model only sees a small amount (only those k-shot\nthat are in the prompt) of the training distribution for the OOD experiments. Making use of fine-tuning\nfor future experiments would allow us to leverage more of the training data.\nThe performance of the LLM-based models suffers due to its element-based action space. For\nthe standard test set, for example, 33% of the episodes have some non-element tap actions (i.e.,\nonly <x,y> location), which are infeasible for this modelling approach. Across the feasible actions,\nLLM-hist-5-CoT has a partial match score of 58%.\n8\nOut-of-domain generalization\nModel\nStandard\nVersion\nSubject\nVerb\nDomain\nBC-single\n68.7\n59.2\n64.2\n66.4\n52.2\nBC-history\n73.1\n63.2\n68.5\n70.4\n59.7\nLLM-0 [43]\n30.9 [25.6, 36.6]\n31.6 [26.3, 37.3]\n33.7 [28.2, 39.5]\n32.6 [27.3, 38.4]\n25.3 [20.4, 30.8]\nLLM-hist-5-CoT\n39.6 [33.9, 45.5]\n29.5 [24.3, 35.1]\n44.4 [38.6, 50.4]\n41.7 [35.9, 47.6]\n35.8 [30.2, 41.6]\nTable 4: Partial match scores across standard and OOD generalization splits. For the LLM agent, the\nestimated score and binomial proportion 95% confidence interval are shown. BC evaluation is on the\nentire test sets; confidence intervals are < 0.1% and are excluded for brevity.\nFigure 3: True complete match (estimated using human evaluation), and partial and complete match\n(both estimated using automated evaluation) for BC-history. True complete match is based on a subset\nof episodes; 95% confidence bounds are reported. Partial match is correlated with true complete\nmatch, while the complete match heuristic is a lower bound score.\nWe perform human evaluation for BC-history on a small subset from GOOGLEAPPS (on average 86.5\nepisodes from each split). We use this dataset portion because it has the largest training set, but we\nexclude the domain split due to the limited number of apps. As shown in Figure 3, we find that action\nmatching is a reasonable approximation of true success rates.\nAs expected, the agent performs the best on the standard test split. Compared to what is observed\nacross all dataset portions (Table 4) its performance on the standard set is higher, but on subject and\nverb OOD splits is lower. This is due to the nature of GOOGLEAPPS data (see Table 2) where the\ntasks are rather distinct (few unique prompts) which makes the verb and subject generalization hard,\nbut at the same time every prompt has many demonstrations, which makes the standard test easier.\nAlthough the automated complete match is low, we note the agent is correct for the majority of steps\nas indicated by the partial match scores > 0.5. We confirmed this was the case by visual inspection.\nThe agent typically performs many of the initial steps correct, but it is more error prone farther in the\ntrajectory.\nIn summary, across the four splits, partial match tends to be correlated with true complete match. It is\na reliable approximation especially if the number of steps in a task is small. Automated complete\nmetrics represent a lower bound score of the true value.\n6\nDiscussion\n6.1\nData Limitations\nUser Demographics Distribution. The raters are not a representative sample of the entire world\npopulation. The screens they visit, containing dynamic content from the Internet, are not representative\nof the rich variety of content and languages of the world. Similarly, the dataset prompts are exclusively\nin English, although they could potentially be translated and evaluated using multilingual models.\nRater device interaction. Raters use a mouse and keyboard rather than the native touch-based\ninterface. This may result in somewhat different user patterns.\nForm factor. The dataset is derived from mobile phone user interactions. The dataset could be\naugmented with more form factors, such as tablets, to increase generalization.\n9\nUI Drift/Evolution. Our dataset includes an unseen domain split, containing new and unseen UIs,\nbut it may not fully represent the continuous evolution of a given app or website\u2019s UI. This dynamic\nchange is an essential aspect of real-world interfaces but is a complex phenomenon to capture\ncomprehensively. However, we do capture some of this drift through the unseen Android version\nsplit, reflecting changes in Google apps\u2019 UI over various Android versions.\n6.2\nEthical considerations\nPrivacy. The raters were instructed not to enter any Personal Identifiable Information (PII) during\ncollection. The dataset does not contain any interactions from real users.\nMalicious use. Malicious actors could use the dataset for undesired purposes such as overriding\nanti-fraud mechanisms like CAPTCHAs. Malicious actors could also manipulate prompts and/or\nscreen representations of deployed models to achieve undesirable goals.\n7\nFuture Work\nMultimodal modeling. The LLM-based model, adapted from prior work [43], is not as performant as\nthe bespoke BC model. This model consumes a text-based screen representation and cannot output a\n<x,y> coordinate-based output. A multimodal foundation model [2, 12] that consumes raw pixels and\noutputs gestures at arbitrary points would be a natural next model type to investigate. Furthermore,\nany foundation models may benefit from fine-tuning on the AITW training sets.\nMultiple ways to achieve a task. There are often multiple ways to achieve a task. Future evaluation\nmethods could be more \"lenient\" and not penalize correct agent actions that do not match human\ndemonstrations. Furthermore, constraining agents to achieve goals in \"optimal\" ways, however that\nmay be defined, may increase user satisfaction with trained models.\n8\nConclusions\nMobile device control via natural language commands has broad application. It requires translating\nhigh-level instructions into execution plans that operate the device interface as a human would.\nRecent advancements in general-purpose large foundation models have opened doors for creating\nsuch device-control systems, however there remains a substantial void due to the dearth of large,\ncomprehensive datasets essential for training and evaluating these systems.\nAddressing these gaps, we present AITW, which is significantly larger and more diverse than existing\ndevice-control datasets. AITW consists of 715k episodes across more than 350 Android applications\nand websites, and a variety of task instructions and execution paths, a realistic representation of\nreal-world system interactions.\nThrough dataset structure, we provide experimental setups for evaluation under varying conditions,\nincluding novel tasks and language, Android versions, and applications and websites. We trained and\nran models on the data and demonstrated how to evaluate model performance under novel conditions.\nWe hope AITW will spur research to create more powerful device automation models.\nAcknowledgements\nThe authors thank Gabriel Taubman, James Stout, Gregory Wayne, and Max Lin for insightful\ndiscussions throughout. Thanks to Elisabeth Chauncey for help with dataset release. Thank you to JD\nChen for helpful feedback on early manuscript versions. Daniel Toyama, Philippe Hamel, and Anita\nGergely provided essential Android environment assistance. We also thank our raters for collecting\nour data.\nReferences\n[1] Adept. ACT-1: Transformer for Actions, 2022. https://www.adept.ai/act.\n[2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro,\n10\nJ. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira,\nO. Vinyals, A. Zisserman, and K. Simonyan. Flamingo: a visual language model for few-shot\nlearning, 2022.\n[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,\nP. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra,\nE. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H.\nAbrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta,\nY. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani,\nS. Dev, J. Devlin, M. D\u00edaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag,\nX. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland,\nA. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun,\nS. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu,\nF. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham,\nE. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif,\nB. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov,\nD. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang,\nZ. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng,\nC. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023.\n[4] C. Bai, X. Zang, Y. Xu, S. Sunkara, A. Rastogi, J. Chen, and B. A. y Arcas. UIBert: Learning\ngeneric multimodal representations for UI understanding. In Z. Zhou, editor, Proc. of the\n30th International Joint Conference on Artificial Intelligence, IJCAI 2021, pages 1705\u20131712.\nijcai.org, 2021.\n[5] P. Banerjee, S. Mahajan, K. Arora, C. Baral, and O. Riva. Lexi: Self-supervised learning of\nthe UI language. In Proc. of the 2022 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, December 2022.\n[6] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji,\nA. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon,\nJ. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman,\nS. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu,\nJ. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani,\nO. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee,\nT. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani,\nE. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.\nNiebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech,\nE. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz,\nJ. Ryan, C. R\u00e9, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin,\nR. Taori, A. W. Thomas, F. Tram\u00e8r, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie,\nM. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou,\nand P. Liang. On the opportunities and risks of foundation models, 2022.\n[7] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke,\nJ. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of\nPython+NumPy programs, 2018.\n[8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\nLanguage models are few-shot learners, 2020.\n[9] A. Burns, D. Arsan, S. Agrawal, R. Kumar, K. Saenko, and B. A. Plummer. Mobile app tasks\nwith iterative feedback (motif): Addressing task feasibility in interactive visual environments.\nCoRR, abs/2104.08560, 2021.\n[10] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhu, G. Li, and J. Wang. Unblind Your Apps: Predicting\nNatural-Language Labels for Mobile GUI Components by Deep Learning. In Proc. of the\nACM/IEEE 42nd International Conference on Software Engineering, ICSE \u201920, pages 322\u2013334,\n2020.\n11\n[11] J. Chen, M. Xie, Z. Xing, C. Chen, X. Xu, L. Zhu, and G. Li. Object detection for graphical\nuser interface: Old fashioned or deep learning or a combination? In Proc. of the 28th ACM Joint\nMeeting on European Software Engineering Conference and Symposium on the Foundations of\nSoftware Engineering, ESEC/FSE 2020, pages 1202\u20131214, 2020.\n[12] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman,\nA. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari,\nG. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan,\nC. Riquelme, A. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali: A jointly-\nscaled multilingual language-image model, 2023.\n[13] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev,\nH. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,\nH. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai,\nT. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,\nX. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean,\nS. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022.\n[14] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web:\nTowards a generalist agent for the web, 2023.\n[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16\nwords: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020.\n[17] H. Furuta, O. Nachum, K.-H. Lee, Y. Matsuo, S. S. Gu, and I. Gur. Multimodal web navigation\nwith instruction-finetuned foundation models, 2023.\n[18] I. Gur, N. Jaques, Y. Miao, J. Choi, M. Tiwari, H. Lee, and A. Faust. Environment generation\nfor zero-shot compositional reinforcement learning, 2022.\n[19] I. Gur, O. Nachum, Y. Miao, M. Safdari, A. Huang, A. Chowdhery, S. Narang, N. Fiedel, and\nA. Faust. Understanding html with large language models, 2023.\n[20] I. Gur, U. Rueckert, A. Faust, and D. Hakkani-Tur. Learning to Navigate the Web. In 7th\nInternational Conference on Learning Representations (ICLR \u201919), May 6\u20139 2019.\n[21] Z. He, S. Sunkara, X. Zang, Y. Xu, L. Liu, N. Wichers, G. Schubiner, R. B. Lee, and J. Chen.\nActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces. In 35th\nAAAI Conference on Artificial Intelligence, AAAI 2021, pages 5931\u20135938, 2021.\n[22] T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020.\n[23] M. W. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron, N. Momchev, D. Sinopalnikov,\nP. Sta\u00b4nczyk, S. Ramos, A. Raichuk, D. Vincent, L. Hussenot, R. Dadashi, G. Dulac-Arnold,\nM. Orsini, A. Jacq, J. Ferret, N. Vieillard, S. K. S. Ghasemipour, S. Girgin, O. Pietquin,\nF. Behbahani, T. Norman, A. Abdolmaleki, A. Cassirer, F. Yang, K. Baumli, S. Henderson,\nA. Friesen, R. Haroun, A. Novikov, S. G. Colmenarejo, S. Cabi, C. Gulcehre, T. L. Paine,\nS. Srinivasan, A. Cowie, Z. Wang, B. Piot, and N. de Freitas. Acme: A research framework for\ndistributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020.\n[24] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson,\nP. Georgiev, A. Santoro, and T. Lillicrap. A data-driven approach for learning to control\ncomputers. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato,\neditors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of\nProceedings of Machine Learning Research, pages 9466\u20139482. PMLR, 17\u201323 Jul 2022.\n[25] S. Jia, J. Kiros, and J. Ba. Dom-q-net: Grounded rl on structured language, 2019.\n[26] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.\n[27] K. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M.-W.\nChang, and K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language\nunderstanding, 2022.\n12\n[28] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge. Mapping natural language instructions\nto mobile UI action sequences. In Proc. of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8198\u20138210. Association\nfor Computational Linguistics, 2020.\n[29] E. Z. Liu, K. Guu, P. Pasupat, and P. Liang. Reinforcement learning on web interfaces using\nworkflow-guided exploration. In 6th International Conference on Learning Representations\n(ICLR \u201918), 2018.\n[30] T. F. Liu, M. Craft, J. Situ, E. Yumer, R. Mech, and R. Kumar. Learning design semantics for\nmobile apps. In Proc. of the 31st Annual ACM Symposium on User Interface Software and\nTechnology, UIST \u201918, page 569\u2013579, New York, NY, USA, 2018. Association for Computing\nMachinery.\n[31] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data.\nRobotics: Science and Systems, 2021.\n[32] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence.\nInteractive language: Talking to robots in real time, 2022.\n[33] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected\nwith massive apis, 2023.\n[34] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao,\nR. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis, 2023.\n[35] A. Richard, H. Kuehne, and J. Gall. Weakly supervised action learning with RNN based fine-\nto-coarse modeling. In 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1273\u20131282, 2017.\n[36] T. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and\nT. Scialom. Toolformer: Language models can teach themselves to use tools, 2023.\n[37] T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of bits: An open-domain\nplatform for web-based agents. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th\nInternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning\nResearch, pages 3135\u20133144. PMLR, 06\u201311 Aug 2017.\n[38] M. Shvo, Z. Hu, R. T. Icarte, I. Mohomed, A. Jepson, and S. A. McIlraith. Appbuddy: Learning\nto accomplish tasks in mobile apps via reinforcement learning, 2021.\n[39] S. Sunkara, M. Wang, L. Liu, G. Baechler, Y.-C. Hsiao, J. Chen, A. Sharma, and J. W. W. Stout.\nTowards better semantic understanding of mobile interfaces. In Proc. of the 29th International\nConference on Computational Linguistics, pages 5636\u20135650. International Committee on\nComputational Linguistics, Oct. 2022.\n[40] D. Toyama, P. Hamel, A. Gergely, G. Comanici, A. Glaese, Z. Ahmed, T. Jackson, S. Mourad,\nand D. Precup. Androidenv: A reinforcement learning platform for android, 2021.\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need, 2017.\n[42] S. G. Venkatesh, P. Talukdar, and S. Narayanan. Ugif: Ui grounded instruction following, 2022.\n[43] B. Wang, G. Li, and Y. Li. Enabling conversational interaction with mobile ui using large\nlanguage models. In Proc. of the 2023 CHI Conference on Human Factors in Computing\nSystems, CHI \u201923. Association for Computing Machinery, 2023.\n[44] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou.\nChain-of-thought prompting elicits reasoning in large language models, 2023.\n[45] XDA.\nGoogle\nis\ntrying\nto\nlimit\nwhat\napps\ncan\nuse\nan\nAccessi-\nbility\nService\n(again),\n2021.\nhttps://www.xda-developers.com/\ngoogle-trying-limit-apps-accessibility-service/.\n[46] X. Zhang, L. de Greef, A. Swearngin, S. White, K. Murray, L. Yu, Q. Shan, J. Nichols, J. Wu,\nC. Fleizach, A. Everitt, and J. P. Bigham. Screen Recognition: Creating Accessibility Metadata\nfor Mobile Applications from Pixels. In Proc. of the 2021 CHI Conference on Human Factors\nin Computing Systems, CHI \u201921, 2021.\n13\nAppendix A\nDataset collection\nA.1\nCrowdsourcing\nThis work was carried out by participants who are paid contractors. Those contractors received a\nstandard contracted wage, which complies with living wage laws in their country of employment.\nDue to global privacy concerns, we cannot include more details about our participants, e.g., estimated\nhourly wage or total amount spent on compensation.\nWe provided raters with a detailed instructional document and a video tutorial, followed by having\nthem perform test demonstrations using our system. For multi-step task trajectories, we ensured\nquality and diversity through manual inspection of a subset of demonstrations. Tasks were marked\nas complete with the task_complete action once a rater completed an assignment, including cases\nwhere the task was already completed. In contrast, the task_impossible action was used to indicate\ninfeasible tasks, such as turning on a flashlight in an emulator.\nFor hindsight-language relabeling, we conducted manual reviews of a sample of labeled trajectories\ndue to the nuanced nature of natural language use. The aim was to encourage the creation of\ndescriptive, unambiguous labels and discourage the use of oversimplified technical terms or vague\nlanguage in order to collect clear and useful task descriptions that cannot be captured using automatic\nheuristics.\nA.2\nPrompt generation\nWe use the following prompt to extract the subject templates and a similar prompt for the verb\ntemplates:\n# Identify subject variables in commands.\nphrase = [\"show me popular videos on youtube\",\n\"whats the weather?\",\n\"go to espn.com\",\n\"click the top result\",\n\"open calendar and show me the fourth week of next month\",\n<INPUT_INSTRUCTIONS>]\nresult = [\"show me {subject1} on {subject2}\",\n\"whats {subject1}?\",\n\"go to {subject1}\",\n\"click {subject1}\",\n\"open {subject1} and show me {subject2} of {subject3}\",\nA.3\nExamples\nExample of episodes from AITW are show in Figures 4, 5, and 6.\nAppendix B\nExperiment details\nB.1\nBehavioral Cloning\nThe Behavioral Cloning (BC), shown in Figure 7, is a Transformer-based architecture [41] that takes\na task instruction, the current screen, and a stacked history of screen observations and actions as\ninput. The model is conditioned on BERT [15] embeddings of the natural language instruction. For\nthe screen input the model embeds each detected OCR and detected icon to a vector of size 512\nusing the following procedure. We embed the text using a pre-trained BERT model taking the output\nfrom the CLS token, which is then linearly projected from size 732 to 512. For the icons, we learn\nan embedding from the ID, which we add element-wise to the BERT embedding. Following similar\napproaches [28, 16], we add to this the spatial information by learning four embeddings for each of\nthe bounding box points, which are binned into 200 and 96 elements vertically and horizontally. For\nthe screen history (excluding the current screen), we embed the <x,y> positions of the touch and lift\nactions, which are added to the element encoding, using a dummy value for non-gesture actions. We\nfound that including action history improves performance.\n14\nFigure 4: Example episode from the dataset.\nFigure 5: Example episode from the dataset.\n15\nFigure 6: Example episode from the dataset.\nFigure 7: Architecture diagram of the BC agent.\n16\nTable 5: Partial match scores across generalization splits and datasets for BC-history.\nStandard\nVersion\nSubject\nVerb\nDomain\nDataset\nGOOGLEAPPS\n75.7\n63.4\n48.4\n57.4\nn/a\nGENERAL\n63.7\n52.5\n65.4\n64.1\nn/a\nWEBSHOPPING\n68.5\n56.7\n69.5\n68.4\n49.6\nINSTALL\n77.5\n66.5\n76.4\n77.7\n66.9\nSINGLE\n80.3\n77.0\n82.8\n84.6\n62.6\nFor gesture actions, the agent outputs a dual-point output. We found such a formulation useful for\ninteracting with many common widgets (e.g., carousel widgets, switching months in a calendar,\ncontrolling sliders), which require precise scrolling.\nWe train the agent using the standard cross-entropy loss using a 2x2 slice of a V2 Tensor Processing\nUnit (TPU). The agent is implemented using Acme [23], Haiku [22], and JAX [7]. The Transformer\nhas 4 layers, a dropout rate of 0.1, and we train use the AdamW optimizer with a learning rate of\n0.0001, and a batch size of 128.\nFor evaluation we train and perform a hyperparameter search via grid search on the validation set.\nWe choose the best performing model and run it on the test set for the final numbers.\nTable 5 reports a breakdown of the performance of the BC-history agent (our best performing agent)\nacross the different dataset splits and portions.\nB.2\nLLM\nWe use the following prompt for LLM-0:\nGiven a mobile screen and a question, provide the action based on the screen\ninformation.\nAvailable Actions:\n{\"action_type\": \"click\", \"idx\": <element_idx>}\n{\"action_type\": \"type\", \"text\": <text>}\n{\"action_type\": \"navigate_home\"}\n{\"action_type\": \"navigate_back\"}\n{\"action_type\": \"scroll\", \"direction\": \"up\"}\n{\"action_type\": \"scroll\", \"direction\": \"down\"}\n{\"action_type\": \"scroll\", \"direction\": \"left\"}\n{\"action_type\": \"scroll\", \"direction\": \"right\"}\nScreen:\n<SCREEN_REPRESENTATION>\nInstruction: <GROUNDING_GOAL>\nAnswer:\nWe use the following prompt for LLM-hist-5-CoT:\nGiven a mobile screen and a question, provide the action based on the screen\ninformation.\nAvailable Actions:\n{\"action_type\": \"click\", \"idx\": <element_idx>}\n{\"action_type\": \"type\", \"text\": <text>}\n{\"action_type\": \"navigate_home\"}\n{\"action_type\": \"navigate_back\"}\n{\"action_type\": \"scroll\", \"direction\": \"up\"}\n{\"action_type\": \"scroll\", \"direction\": \"down\"}\n{\"action_type\": \"scroll\", \"direction\": \"left\"}\n{\"action_type\": \"scroll\", \"direction\": \"right\"}\n17\nPrevious Actions:\n{\"step_idx\": 0, \"action_description\": \"press [HOME key]\"}\n{\"step_idx\": 2, \"action_description\": \"click [Google Icon]\"}\n{\"step_idx\": 3, \"action_description\": \"click [search for hotels]\"}\nScreen:\n<img id=0 class=\"IconGoogle\" alt=\"Google Icon\"> </img>\n<img id=1 class=\"IconX\" alt=\"Close Icon\"> </img>\n<p id=2 class=\"text\" alt=\"search for hotels\"> search for hotels </p>\n<p id=3 class=\"text\" alt=\"in\"> in </p>\n<p id=4 class=\"text\" alt=\"mexico city mexico\"> mexico city mexico </p>\n<img id=5 class=\"IconMagnifyingGlass\" alt=\"Search Icon\"> </img>\n<p id=6 class=\"text\" alt=\"Share\"> Share </p>\n<p id=7 class=\"text\" alt=\"Select alI\"> Select alI </p>\n<p id=8 class=\"text\" alt=\"Cut\"> Cut </p>\n<p id=9 class=\"text\" alt=\"Copy\"> Copy </p>\n<p id=10 class=\"text\" alt=\"hotel in mex\"> hotel in mex </p>\n<img id=11 class=\"IconMagnifyingGlass\" alt=\"Search Icon\"> </img>\n<p id=12 class=\"text\" alt=\"best hotel\"> best hotel </p>\n<p id=13 class=\"text\" alt=\"mexico city\"> mexico city </p>\n<p id=14 class=\"text\" alt=\"in\"> in </p>\n<img id=15 class=\"IconMagnifyingGlass\" alt=\"Search Icon\"> </img>\n<p id=16 class=\"text\" alt=\"K\"> K </p>\n<p id=17 class=\"text\" alt=\"hotel ciudad\"> hotel ciudad </p>\n<p id=18 class=\"text\" alt=\"de mexico\"> de mexico </p>\n<p id=19 class=\"text\" alt=\"gran\"> gran </p>\n<img id=20 class=\"IconVBackward\" alt=\"Left Icon\"> </img>\n<img id=21 class=\"IconNavBarCircle\" alt=\"Home Icon\"> </img>\n<img id=22 class=\"IconNavBarRect\" alt=\"Overview Icon\"> </img>\nInstruction: What time is it in Berlin?\nAnswer: Let\u2019s think step by step. I see unrelated search results in the Google app,\nI must clear the search bar, so the action is {\"action_type\": \"click\", \"idx\": 1}\nPrevious Actions:\n{\"step_idx\": 0, \"action_description\": \"click [DISMISS]\"}\nScreen:\n<p id=0 class=\"text\" alt=\"Update your\"> Update your </p>\n<p id=1 class=\"text\" alt=\"Gmail app\"> Gmail app </p>\n<p id=2 class=\"text\" alt=\"attach files from\"> attach files from </p>\n<p id=3 class=\"text\" alt=\"To\"> To </p>\n<p id=4 class=\"text\" alt=\"download the\"> download the </p>\n<p id=5 class=\"text\" alt=\"Drive,\"> Drive, </p>\n<p id=6 class=\"text\" alt=\"latest\"> latest </p>\n<p id=7 class=\"text\" alt=\"version\"> version </p>\n<p id=8 class=\"text\" alt=\"of\"> of </p>\n<p id=9 class=\"text\" alt=\"Gmail\"> Gmail </p>\n<p id=10 class=\"text\" alt=\"UPDATE\"> UPDATE </p>\n<p id=11 class=\"text\" alt=\"DISMISS\"> DISMISS </p>\n<p id=12 class=\"text\" alt=\"Got\"> Got </p>\n<p id=13 class=\"text\" alt=\"it\"> it </p>\n<img id=14 class=\"IconVBackward\" alt=\"Left Icon\"> </img>\nInstruction: see creations saved in the google photos\nAnswer: Let\u2019s think step by step. I see a popup, I need to open Google Photos, so\nthe action is {\"action_type\": \"click\", \"idx\": 11}\nPrevious Actions:\nScreen:\n<p id=0 class=\"text\" alt=\"M\"> M </p>\n<p id=1 class=\"text\" alt=\"New in Gmail\"> New in Gmail </p>\n<p id=2 class=\"text\" alt=\"All the features you\"> All the features you </p>\n18\n<p id=3 class=\"text\" alt=\"love with\"> love with </p>\n<p id=4 class=\"text\" alt=\"a fresh\"> a fresh </p>\n<p id=5 class=\"text\" alt=\"look\"> look </p>\n<p id=6 class=\"text\" alt=\"new\"> new </p>\n<p id=7 class=\"text\" alt=\"GOT IT\"> GOT IT </p>\nInstruction: open app \"Google Play services\"\nAnswer: Let\u2019s think step by step. I see the GMail app, I need to open the app\ndrawer, so the action is {\"action_type\": \"navigate_home\"}\nPrevious Actions:\nScreen:\n<p id=0 class=\"text\" alt=\"Tuesday, Aug\"> Tuesday, Aug </p>\n<p id=1 class=\"text\" alt=\"9\"> 9 </p>\n<img id=2 class=\"IconChat\" alt=\"Chat Icon\"> </img>\n<img id=3 class=\"IconGoogle\" alt=\"Google Icon\"> </img>\nInstruction: open app \"Messenger Lite\" (install if not already installed)\nAnswer: Let\u2019s think step by step. I see the home screen, I need to open the app\ndrawer, I should swipe up, so the action is {\"action_type\": \"scroll\", \"direction\":\n\"down\"}\nPrevious Actions:\n{\"step_idx\": 0, \"action_description\": \"scroll down\"}\nScreen:\n<img id=0 class=\"IconThreeDots\" alt=\"More Icon\"> </img>\n<p id=1 class=\"text\" alt=\"Search your phone and more\"> Search your phone and more </p>\n<p id=2 class=\"text\" alt=\"M\"> M </p>\n<p id=3 class=\"text\" alt=\"O\"> O </p>\n<img id=4 class=\"IconPlay\" alt=\"Play Icon\"> </img>\n<p id=5 class=\"text\" alt=\"Clock\"> Clock </p>\n<p id=6 class=\"text\" alt=\"YouTube\"> YouTube </p>\n<p id=7 class=\"text\" alt=\"Photos\"> Photos </p>\n<p id=8 class=\"text\" alt=\"Gmail\"> Gmail </p>\n<p id=9 class=\"text\" alt=\"All apps\"> All apps </p>\n<p id=10 class=\"text\" alt=\"g\"> g </p>\n<p id=11 class=\"text\" alt=\"O\"> O </p>\n<img id=12 class=\"IconTakePhoto\" alt=\"Camera Icon\"> </img>\n<p id=13 class=\"text\" alt=\"10\"> 10 </p>\n<p id=14 class=\"text\" alt=\"Calendar\"> Calendar </p>\n<p id=15 class=\"text\" alt=\"Camera\"> Camera </p>\n<p id=16 class=\"text\" alt=\"Chrome\"> Chrome </p>\n<p id=17 class=\"text\" alt=\"Clock\"> Clock </p>\n<p id=18 class=\"text\" alt=\"0\"> 0 </p>\n<p id=19 class=\"text\" alt=\"M\"> M </p>\n<p id=20 class=\"text\" alt=\"B\"> B </p>\n<img id=21 class=\"IconPerson\" alt=\"Person Icon\"> </img>\n<p id=22 class=\"text\" alt=\"Gmail\"> Gmail </p>\n<p id=23 class=\"text\" alt=\"Drive\"> Drive </p>\n<p id=24 class=\"text\" alt=\"Files\"> Files </p>\n<p id=25 class=\"text\" alt=\"Contacts\"> Contacts </p>\n<p id=26 class=\"text\" alt=\"G OO\"> G OO </p>\n<img id=27 class=\"IconGoogle\" alt=\"Google Icon\"> </img>\n<img id=28 class=\"IconLocation\" alt=\"Location Icon\"> </img>\n<img id=29 class=\"IconCall\" alt=\"Phone Icon\"> </img>\n<img id=30 class=\"IconChat\" alt=\"Chat Icon\"> </img>\n<p id=31 class=\"text\" alt=\"Google\"> Google </p>\n<p id=32 class=\"text\" alt=\"Maps\"> Maps </p>\nInstruction: Search for hotels in Chicago.\nAnswer: Let\u2019s think step by step. I see the app drawer, I need to search, so the\naction is {\"action_type\": \"click\", \"idx\": 27}\n19\nPrevious Actions:\n<HISTORY>\nScreen:\n<SCREEN_REPRESENTATION>\nInstruction: <GROUNDING_GOAL>\nAnswer: Let\u2019s think step by step. I see\nAppendix C\nDataset format\nEach datapoint is stored as a TFRecord file with compression type \u2018GZIP\u2019 with the following fields:\n\u2022 android_api_level: the Android API level of the emulator the episode was collected\nfrom\n\u2022 current_activity: the name of the activity running when the example was collected\n\u2022 device_type: the device type of the emulator the episode was collected from, mostly Pixel\ndevices with one custom device image\n\u2022 episode_id: the unique identifier for the episode the example is from\n\u2022 episode_length: the overall number of steps in the episode\n\u2022 goal_info: the natural language instruction the episode is demonstrating\n\u2022 image/channels, image/height, image/width: the number of channels, height, and\nwidth of the screenshot\n\u2022 image/encoded: the encoded screenshot\n\u2022 image/ui_annotations_positions: a flattened array of coordinates representing the\nbounding boxes of the UI annotations; the coordinates are in (y, x, height, width) format and\nthe length of this array is 4 * num_elements\n\u2022 image/ui_annotations_text: the OCR-detected text associated with the UI element\n\u2022 image/ui_annotations_ui_types: the type of UI element for each annotation, can be\nan icon or just text\n\u2022 results/action_type: the type of the predicted action (see \u2019Action space\u2019 for more\ndetails)\n\u2022 results/type_action: if the action is a type then this holds the text string that was\ntyped\n\u2022 results/yx_touch, results/yx_lift: the (y, x) coordinates for the touch and lift point\nof a dual point action\n\u2022 step_id: the example\u2019s zero-indexed step number within the episode (i.e. if step_id is 2,\nthen this is the third step of the episode)\n20\n"
  },
  {
    "title": "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering",
    "link": "https://arxiv.org/pdf/2307.10173.pdf",
    "upvote": "5",
    "text": "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity\nHuman-centric Rendering\nWei Cheng1\nRuixiang Chen2\u2217\nWanqi Yin2\u2217\nSiming Fan1,2\u2217\nKeyu Chen1\u2217\nHonglin He1\nHuiwen Luo1\nZhongang Cai3\nJingbo Wang4\nYang Gao2\nZhengming Yu1\nZhengyu Lin2\nDaxuan Ren3\nLei Yang1,2\nZiwei Liu3\nChen Change Loy3\nChen Qian1\nWayne Wu1\nDahua Lin1,4\nBo Dai1\u2020\nKwan-Yee Lin1,4\u2020\n1 Shanghai AI Laboratory\n2 SenseTime Research\n3 S-Lab, NTU\n4 CUHK\nMulti-view Capture System\nEasy\nDaily Activity\nSport\nDance\nProfessional Skill\nMotion\n11%\n7%\n4%\n78%\nDaily Clothing\nEthnic Clothing\nAncient Wearing\nSpecial Costume\nCloth\n13%\n6%\n18%\n63%\nMale\nFemale\nGender\n50%\n50%\n>70\n60-70\n50-60\n40-50\n30-40\n20-30\n<10\nRatio\n0%\n16%\n8%\n4%\n12%\nAge\n10-20\nFigure 1: Overview of our dataset. DNA-Rendering is a large-scale human-centric dataset, with high-quality multi-view images and\nvideos for various human actors. The dataset comes with grand categories of motion, cloth, accessory, body shape, and human-object\ninteraction. We hope it could boost the development of human-centric rendering and related tasks.\nAbstract\nRealistic human-centric rendering plays a key role in\nboth computer vision and computer graphics.\nRapid\nprogress has been made in the algorithm aspect over the\nyears, yet existing human-centric rendering datasets and\nbenchmarks are rather impoverished in terms of diversity\n(e.g., outfit\u2019s fabric/material, body\u2019s interaction with ob-\njects, and motion sequences), which are crucial for render-\ning effect. Researchers are usually constrained to explore\nand evaluate a small set of rendering problems on cur-\n*Joint-first authors with W. Cheng.\n\u2020Equal advising.\nrent datasets, while real-world applications require meth-\nods to be robust across different scenarios. In this work, we\npresent DNA-Rendering, a large-scale, high-fidelity reposi-\ntory of human performance data for neural actor rendering.\nDNA-Rendering presents several alluring attributes. First,\nour dataset contains over 1500 human subjects, 5000 mo-\ntion sequences, and 67.5M frames\u2019 data volume. Upon the\nmassive collections, we provide human subjects with grand\ncategories of pose actions, body shapes, clothing, acces-\nsories, hairdos, and object intersection, which ranges the\ngeometry and appearance variances from everyday life to\nprofessional occasions. Second, we provide rich assets for\neach subject \u2013 2D/3D human body keypoints, foreground\n1\narXiv:2307.10173v2  [cs.CV]  30 Sep 2023\nDataset\nAttribute\nScale\nRealism\nEthnicity\nAge\nCloth\nMotion\nInteractivity\n#ID \u00d7 #Outfit\n#Motions\n#View\n#Frames\nHRes\nHuman3.6M [26]\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n11 \u00d7 1\n17\n4\n3.6M\n1000P\nCMU Panoptic [31]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n97 \u00d7 1\n65\n31 + 480\u2217\n15.3M\n1080P\nZJU-MoCap [56]\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n10 \u00d7 1\n10\n24\n180K\n1024P\nHUMBI [84]\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n772 \u00d7 1\n\u2212\n107\n26M\n1080P\nAIST++ [70, 37]\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n30 \u00d7 1\n\u2212\n9\n10.1M\n1080P\nTHuman 4.0 [64]\n\u2717\n\u2717\n\u2713\n\u2713\n\u2717\n3 \u00d7 1\n\u2212\n24\n10K\n1150P\nHuMMan [7]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n1000 \u00d7 1\n500\n10\n60M\n1080P\nGeneBody [13]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n50 \u00d7 2\n61\n48\n2.95M\n2048P\nDNA-Rendering (Ours)\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n500 \u00d7 3\n1187\n60\n67.5M\n4096P\nTable 1: Dataset comparison on attributes and scales. We compare the proposed dataset with previous human-centric multiview datasets\nin terms of attribute coverage, scale, and realism. \u2018Ethnicity\u2019 denotes whether the dataset contains actors from multiple ethnic groups.\n\u2018Age\u2019 means if there is a wide age range containing elders or infants. \u2018Cloth\u2019 separates datasets with only daily costumes or with extra\ndiverse clothing. \u2018Attribute-Motion\u2019 denotes whether it has human motion in different scenarios. \u2018Interactivity\u2019 tells whether there contains\nhuman-object interaction. We mark these attributes with \u2713 and \u2717. In scale, we list the number of key factors with compared dataset,\nNote that \u2018Scale-#Motions\u2019 means the number of motion categories, and superscript \u2217 means low-resolution VGA cameras, we exclude\nthem during \u2018#View\u2019 ranking and \u2018#Frames\u2019 calculation. Cells\nindicate the best, and second best in the specific category among all\ndatasets. We abbreviate resolution at height as \u2018HRes\u2019.\nmasks, SMPLX models, cloth/accessory materials, multi-\nview images, and videos. These assets boost the current\nmethod\u2019s accuracy on downstream rendering tasks. Third,\nwe construct a professional multi-view system to capture\ndata, which contains 60 synchronous cameras with max\n4096 \u00d7 3000 resolution, 15 fps speed, and stern camera\ncalibration steps, ensuring high-quality resources for task\ntraining and evaluation.\nAlong with the dataset, we provide a large-scale and\nquantitative benchmark in full-scale, with multiple tasks\nto evaluate the existing progress of novel view synthesis,\nnovel pose animation synthesis, and novel identity render-\ning methods. In this manuscript, we describe our DNA-\nRendering effort as a revealing of new observations, chal-\nlenges, and future directions to human-centric rendering.\nThe dataset, code, and benchmarks will be publicly avail-\nable at https://dna-rendering.github.io/.\n1. Introduction\nUnderstanding humans is an everlasting problem in our\nresearch community, and extensive literature on perceiving\nand synthesizing humans shows great efforts toward this\ngoal. Over the decades, multiple pioneers have constructed\nlarge-scale and diverse datasets, such as COCO [40] for\nhuman pose estimation, and ActivityNet [6] for analyzing\nhuman action. These datasets are the driving force behind\nflourishing developed human-centric perceiving algorithms.\nYet, when it comes to human-centric rendering, there is\nstill a noticeable gap in comprehensive datasets. Capturing\nhigh-quality and massive 3D/4D human avatars is difficult\ndue to the requirements of high-end equipment as well as\nan efficient data processing pipeline. Existing datasets [26,\n30, 56, 64, 25] partially narrow the gaps but have significant\nlimitations on sample diversity (e.g., clothing, motion, body\nshape, and human-object interaction), or have insufficient\nrealism (e.g., camera resolution, and capture speed). These\nfactors are crucial to rendering effects.\nTo drive advancement in human-centric rendering, we\ncontribute a large-scale multi-view human performance\ncapture dataset, named DNA-Rendering, which includes the\nfactors that are important to rendering in great diversity and\ngranularity. On the hardware side, we build up a 360-degree\nindoor system equipped with 60 calibrated RGB cameras\nand 8 synchronized depth sensors. The captured videos are\nunder the fidelity of up to 12MP (4096 \u00d7 3000) resolution\nand recorded at 15 fps. From the dataset\u2019s footage design\naspect, we intend to cover most attributes (e.g., age, eth-\nnicity, shape, motion, cloth, accessory, and interactive ob-\njects) that could reflect the rendering differences with re-\nspect to texture, materials, primary/secondary motion de-\nformation, and category priors. In practice, we design over\n1500 outfits and 1187 motion types to ensure the compre-\nhensive coverage of real-world scenarios. We invite 500 ac-\ntors to participate in the data capture process. We record\neach person with three different outfits and at least nine\nunique motion sequences. The full dataset contains 5000\nvideo sequences with over 67.5M frames. Compared with\nthe existing human-centric dataset like CMU Panoptic [31],\nZJU-MoCap [56], THUman [64], and Human3.6M [26],\nDNA-Rendering comprises the most multi-view body per-\nformance samples and reaches the highest image quality.\nThe unfold comparisons between DNA-Rendering and the\nothers are given in Tab. 1.\nMeanwhile, we provide essential annotations attached to\neach frame to facilitate the application of downstream tasks.\nThe ultra-large scale of the DNA-Rendering dataset raises\ngreat challenges to the corresponding data processing steps.\nTo this end, we develop an automatic annotation pipeline\nencompassing camera calibration, color correction, image\nmatting, 2D /3D landmark estimation, and SMPLX model\nfitting. To ensure the labeling quality, we developed a series\n2\nof technical refinements to the annotation toolchain. With\nthese efforts, the automatic pipeline can generate faithful\ndata annotations both effectively and efficiently. To fur-\nther assist the community in the public use of external data,\nwe will open-source the annotation tool used in the DNA-\nRendering project.\nThe unprecedented richness of DNA-Rendering dataset\nprovides fertile data soil for researchers to develop, and dis-\nsect their rendering methods in depth. To set up a kickoff\nexample, we further construct benchmarks upon the dataset\nwith extensive experiments. We evaluate the performances\nof several state-of-the-art full-body rendering and anima-\ntion approaches under three major tasks, i.e., novel view\nsynthesis, novel pose animation, and novel identity ren-\ndering. To better analyze current methods in terms of the\nmodel capacity, module necessity, and methodology gener-\nality, we set up multiple test set splits under different levels\nof challenging aspects. For instance, we divide the easy,\nmedium, and hard subsets w.r.t. the cloth looseness, the tex-\nture complexity, the motion difficulty, and the human-object\ninteractivity, respectively. We conclude a series of key ob-\nservations based on the benchmarks, such as how human\nprior influences the robustness of rendering, how sensitive\nthe multi-view/frame relationship module design is to data\nvolume/distribution, and how loss design affects the perfor-\nmance in terms of different rendering metrics.\nIn summary, we contribute DNA-Rendering project to\nfulfill the requirement of a high-fidelity human performance\ncapture dataset for the research community. We establish by\nfar the largest multi-view human body performance dataset\nfor high-fidelity human-centric rendering research, with an\nemphasis on image quality and data attributes.\nThe at-\ntached benchmarks provide baseline standards for three ma-\njor tasks, with rigorous evaluations and dissections on mul-\ntiple state-of-the-art methods. We believe the dataset, the\nattached benchmarks, and the tools will boost a wide range\nof digital human applications and inspire future research.\n2. Related Works\n2.1. Human-centric Datasets\nPerception Datasets. Perceiving human is a long-standing\nproblem.\nOver the decades, researchers have kept dedi-\ncating their efforts to building relevant datasets.\nEarlier\nefforts in the computer vision community present large-\nscale datasets like COCO [40] for human segmentation\nor keypoint detection from in-the-wild images. Later re-\nsearch works follow the inspiration to establish open-world\ndatasets [10, 21], while with emphasis on parsing more pre-\ncise human body parts. Some researchers focus on con-\nstructing datasets [9, 52, 63] that capture daily activities\nand help the perception of human action recognition by\nusing RGB-D cameras. Despite the wild variety of data\nsamples, these datasets are not capable of human render-\ning tasks, due to a lack of multiview images as groundtruth\nreferences for evaluating methods\u2019 performance. In com-\nputer graphics society, there is another parallel branch that\ncontributes datasets\n[35, 32] for avatar animation, with\nrecording human motion via maker-based motion capture\nsystems. AMASS [47] further integrates these motion cap-\nture databases with fully rigged surface mesh representa-\ntion. In the last decade, computer vision and graphics so-\nciety fit in with each other in the field of perceiving 3D\nhumans.\nDatasets like Human3.6M [26] and MPI-INF-\n3DHP [48] capture humans under in-door multi-view envi-\nronment with 3D marker label or multiview segmentation,\nwhich further encourage the applications in recovering hu-\nman in 3D. 3DPW [72] dataset captures human motion in\nthe wild and annotates 3D pose with the help of pre-scanned\nhuman model. These databases facilitate the development\nof numerous algorithms. These databases facilitate the de-\nvelopment of numerous algorithms. However, due to the\nlimits of the data sample, camera views, and resolution, they\ncannot reflect the pros and cons of rendering methods.\nRendering Datasets.\nRepresenting 3D /4D human ap-\npearances and performances are important in both research\ncommunities and commercial applications. The progress\nof relevant algorithms relies on human scan datasets with\naccurate geometry or dense views. Such datasets are the\nfoundational factor to close the gap between virtual avatars\nand real humans. THuman [90, 82, 67], ClothCap [57],\nSIZER [69] and other commercial scan datasets [19, 18]\ncapture static human scan reconstructed by either depth\nsensors or camera array.\n[4, 5, 85, 64, 88] provide dy-\nnamic human scans with minimal clothing and daily cos-\ntumes. These datasets are usually biased centering on stand-\ning poses due to the sophisticated capture process. With the\nemergence of neural rendering techniques, rendering realis-\ntic humans directly from images has become a trend. Such\na setting usually requires the dataset equipped with high-\nquality dense view images and accurate annotations like hu-\nman body keypoints and foreground segmentation. CMU\nPanoptic [30] uses a 30-HD-camera system and annotates\nthe humans with 3D keypoints. HUMBI [84] focuses on lo-\ncal motions like gesture, facial expression, and gaze move-\nments, rather than factors that have influences on render-\ning quality, such as cloth texture, object interactivity, etc.\nZJU-MoCap [56] is a widely used dataset for human ren-\ndering algorithms. It includes ten video sequences with 24\ncameras under 1K resolution and provides annotations of\nhuman segmentation and estimated SMPL model for each\nframe. However, ZJU-MoCap is limited in narrow motion\nand clothing diversity, which might lead the evaluation to\ngreat bias. AIST++ [70, 37] is a dance database with var-\nious dance motions while sticking in the one-fold scenario\nand lacking view density. Recently proposed HuMMan [7]\nand GeneBody [13] datasets, expand the motion and cloth-\n3\ning diversity, while the effective human resolution is still be-\nlow 1K. Differing with these efforts, we make a step further\nand contribute the largest high-fidelity multiview dataset for\nhuman-centric rendering tasks, with 5000 human perfor-\nmance sequences under a large variation in ethnicity, age,\ncloth, motion, and interaction. Concurrent works [91, 25]\nalso contribute datasets for human avatar tasks, while cen-\ntering on detailed human geometry with long-lens cameras\nto film human body parts. In practice, [91] captures on low\nframe rate, and [25] enjoys dense multi-view capturing but\nlimits to 16 motion sequences. Moreover, their setups are\nvulnerable to the capture of aboriginal motion tracks at full-\nbody levels.\n2.2. Implicit Neural Body Representation\nDifferent from previous works that represent humans\nwith explicit representations (such as skeleton, parametric\nmodel, or mesh), recent work models human appearance as\nneural implicit function, e.g., neural radiance fields [50] or\nneural signed distant functions. PIFu [60, 61] presents the\northogonal camera space as an occupancy function condi-\ntioned by pixel-aligned features and depth. It learns to re-\nconstruct the human body with single-view images, which\ninspires subsequent research on human reconstruction from\na single image [36, 2, 1, 80, 79]. NeuralBody [56] learns\na neural radiance field of dynamic humans conditioned\nby body structure and temporal latent code from sparse\nmulti-view videos. Recently, many category-agnostic im-\nplicit representations, PixelNeRF [81], IBRNet [75], Vi-\nsionNeRF [38], etc., can generalize NeRF to arbitrary un-\nseen scenes given a set of reference views. The key in-\nsight behind these methods is that the novel views of new\nscenes can be recovered from visual clues of reference\nviews and neural implicit functions that are derived by ren-\ndering equations. Rafting these two factors together can\nhelp models learn visual consistency on certain camera dis-\ntributions. The intrinsic differences among these methods\nare the design of feature aggregation, which varies from av-\nerage [81], max [58] pooling to more adaptive weighted\npooling [75] and vision transformer [38].\nGiven human\nrendering is more challenging due to the large variation in\npose and appearance, recent generalizable human render-\ning methods [87, 49, 13, 34] condition such image feature-\naligned NeRF with human priors. For example, Neural-\nHumanPerformer [34] uses structured latent code and Key-\npointNeRF [49] deploys human keypoints. These methods\noutperform category-agnostic methods on human rendering\non existing human rendering datasets, while such improve-\nment is not convincing due to the limited scale and cover-\nage bias of these datasets. We believe the proposed large-\nscale dataset with large variation overall dimensions will\nhelp both the robustness of generalizing humans and better\ngeneralization ability assessment.\n2.3. Animatable Digital Human\nThe challenge of creating realistic animatable human\navatars from images is two folds \u2013 (1) how to reconstruct\nthe human body from motion sequences and (2) how to\ndisentangle non-rigid deformation. Early seminal work A-\nNeRF [66] learns dynamic body from sequences, it con-\nditions the radiance field with relative pose coordinate of\nthe query point, which fails to model the non-rigidity of\nclothed humans. To reconstruct the human body from se-\nquences, AnimatableNeRF [55] learns a static canonical ra-\ndiance field together with a ray blending network from the\ncurrent frame to canonical space. To further better disentan-\ngle motion deformation from pose recent works [62, 11] use\na complementary forward blending network or root-finding\nalgorithm to regularize the learned blending with cycle con-\nsistency loss. Other works [77, 27, 83] learn animatable\nmodels from more challenging monocular video, with a\ntighter assumption of Gaussian distributed occupancy along\nbone or fixed SMPL motion weights.\n3. DNA-Rendering\nIn this section, we discuss the core features of our DNA-\nRendering from the perspective of dataset construction, in-\ncluding the hardware systems setup, data collection, and\nrelevant dataset statistics. By introducing these aspects, we\npresent a way to ensure high fidelity and effectiveness under\nthe massive amount of data capture.\n3.1. Dataset Capture\nSystem Setup. Our capture system contains a high-fidelity\ncamera array, with 60 high-resolution RGB cameras and 16\nlighting boards uniformly distributed in a sphere with a ra-\ndius of three meters. The cameras are adjusted to point at\nthe sphere\u2019s center, where the participants perform. Con-\ncretely, the array consists of 48 high-end 2448\u00d72048 indus-\ntrial cameras, and 12 ultra-high resolution cameras with up\nto 4096 \u00d7 3000 resolution. We use these industrial cameras\nto capture subjects\u2019 body and facial performances, and the\nultra-high resolution cameras to capture more detailed tex-\ntures on clothing and accessory. We additionally place eight\nKinect cameras to capture additional depth streams as aux-\niliary geometric data. The high-fidelity video streams and\ndepth streams are synchronized at 15 frames per second. To\nguarantee the cameras can clearly capture subjects\u2019 perfor-\nmances and clothing patterns from all views, the lighting is\nadjusted to the setting of 5600K \u00b1 300K color temperature\nand 4500 Lux/m illuminance. The above designs ensure the\nsystem could record the sharp texture edges, fine-grained\ncolor changes of clothing patterns, and the reflection ef-\nfects caused by different clothing materials. Please refer\nto Sec. A.4 for more details.\nData Collection Protocol. To enable subsequent research\n4\nprobing into the factors that have influences on rendering,\nwe design a data collection protocol with both interlaced\nand hierarchical data attributes. Specifically, we ask each\nactor to wear three sets of outfits and perform at least three\nactions in different hallucinated scenarios for each outfit,\nwhich maximize the identity scale and diversity. Each mo-\ntion sequence is recorded under specific action category in-\nstruction with a free-style performance lasting for 15 sec-\nonds, which ensures the diversity of action performance.\nAs an auxiliary feature, we also capture a static frame of A-\npose for actors in each outfit for canonical pose recording,\nand a frame with only empty background for image mat-\nting. Multiview videos are inspected on-site with a quick\npreview generation tool, which ensures the movement fits\nbest to the cameras\u2019 field of view. For accurate camera pose\nannotation, extrinsic calibration data are collected at a daily\nfrequency. The color data and intrinsic calibration data are\ncollected whenever system adjustments are made. Please\nrefer to Sec. A.4 for details.\n3.2. Dataset Statistics\nIn order to cover diverse attributes that relate to rendering\nquality, we have carried out a detailed design from the se-\nlection of the actors\u2019 gender, age, and skin color, to their ac-\ntions, clothing, and makeup. Compared with previous work,\nto our best knowledge, our dataset contains the largest num-\nber of human subjects, covering the most diverse action cat-\negories, clothing types, and human-object interaction sce-\nnarios. The key statistics of our dataset are shown at the bot-\ntom of Fig. 1. Specifically, to preserve authenticity in action\nbehavior, we invite 153 professional actors to perform spe-\ncial scenes with corresponding costumes/makeup, and 347\nnormal performers to act under footage of daily-life scenes.\nThe special scenes constitute 153 sub-categories, including\nsports, dances, and unique event performances such as typ-\nical costumes in ancient Chinese dynasties, traditional cos-\ntumes around the world, cosplay, etc. Common scenes can\nbe divided into 269 sub-categories, covering scenes such as\ndaily indoor activities, communication, entertainment, and\nnew trends.\nWe describe the comprehensive distribution\nof data in Sec. A.1 and the limitation of data in Sec. A.5.\nPlease refer to the corresponding sections for more visual-\nized figures and detailed discussion.\n3.3. Data Annotation\nTo enable applications in human rendering and anima-\ntion, DNA-Rendering provides rich annotations attached\nwith the raw data, i.e., camera calibration, camera color cal-\nibration, image matting, and parametric model fitting. The\noverall annotation pipeline is shown in Fig. 2.\nCamera Calibration. First, we calibrate the intrinsic pa-\nrameters of each camera individually. Specifically, we di-\nvide the camera\u2019s field of view into a 3\u00d73 Sudoku, and cap-\nColor \nTransformation\nExposure &\nWB Adjustment\nColor Calibration\nRegistration \nRefinement \nGeometric\nCalibration\nCamera Params\nMultiview Images\nBackground Matting\nMasks\nGrabCut Refinement\n3D Keypoints\nSMPL-X\n2D Keypoints\nTriangulation & \nOptimization\nSMPLifyX\nFigure 2: Annotation pipeline.\nThe illustration of annotation\npipeline for camera calibration, camera color calibration, masks,\nkeypoints, and parametric model.\nture images with \u00b130 degree rotation in pitch, row, and yaw\nangle of checkerboard in all grids, referring to Fig. S4. Sec-\nond, for extrinsic calibration, we deploy multiple ChArUco\nboards and spin the main board in the capture volume. We\nuse open toolboxes [16, 3] to optimize intrinsic parame-\nters, distortion coefficients, and extrinsic parameters with\nthe captured data. To eliminate the depth camera pose error\ncaused by the large resolution gap between industrial cam-\neras and Kinect depth cameras, we further adopt a point\ncloud registration stage to refine the depth camera extrinsic\nparameters in the second stage. More concretely, for each\ndepth camera, we project the partial point cloud and esti-\nmate a full point cloud from the MVS algorithm such as\n[73] as a reference. We jointly optimize the pose graph of\nthe depth camera for neighboring pointclouds with overlaps\nthrough a multi-way registration [14] with MVS pointcloud\nas reference.\nFor detailed camera calibration estimation, please refer\nto Section. B.1 in the supplementary.\nColor Calibration. The identical color response across dif-\nferent cameras could be vital for a multi-view, mixed-type\ncamera system to provide qualified data for rendering appli-\ncations, as it is an essential data basis for algorithms to ren-\nder realistic view-dependent effects. Different from other\nmulti-camera datasets, e.g., Multiface [78, 43] which uses a\nnetwork to optimize the color transformation during model\ntraining, we pay attention to ensure the color consistency of\ndata collection across different cameras. First, we conduct\ncareful adjustments on hardware parameters such as expo-\nsure and white balance to make the captured color of the\ncolor checkerboard under the standard light as close as pos-\nsible. Then, the 2-order polynomial correction coefficients\ncould be optimized by least square regression of transform-\ning the detected color to the true value on the color checker-\nboard.\nPlease refer to Sec. B.1 and Fig. S5 for details.\nWe also analyze the impact of color consistency of multi-\ncamera datasets on generalizable rendering in Sec. D.4.\n5\n(a)\n(b)\nFigure 3: Annotation quality improvements. The zoom-in boxes\nwith red dot lines show the annotation quality before pipeline op-\ntimization. The green ones show quality improvements over (a)\nmask annotation and (b) SMPLX annotation with the optimized\npipeline.\nMatting. Considering the large quantities of the captured\nimages, we develop an automatic matting pipeline to ex-\ntract the foreground objects from the backgrounds. We first\nadopt an off-the-shelf background matting model [39] to\neliminate most background pixels.\nHowever, due to the\ncomplicated nature of the capture settings, the learning-\nbased model inevitably generates unsatisfying results in\nsome challenging cases, leaving some pieces of labeled data\nwith artifacts such as broken holes or noisy patches (See\nFig. 3). Thus, we further propose a refinement strategy by\napplying the HSV filtering and the grabcut [59] algorithms\nto improve the matting quality. According to the manual as-\nsessment of the refined masks, the error rates of problematic\ncases are reduced from 11% to 2% on average. We compare\nmatting with and without refinement, and visualize detailed\nmanual assessment in Fig. S8.\nKeypoints and Parametric Model. Inspired by existing\nworks [7, 8], we develop an automatic pipeline to anno-\ntate keypoints and parametric model parameters. 1) First,\n2D keypoints in COCO-Wholebody [28] format (including\nbody, hand, and face keypoints) are detected for each cam-\nera view, with pretrained model HRNet-w48 [68]. 2) Then,\nwe triangulate 3D keypoints with known camera intrinsic\nand extrinsic parameters from the multi-view 2D keypoints\nwith optimization and post-processing strategies [17] in-\ncluding keypoint selection, bone length constraint, as well\nas outlier removal. 3) Finally, we register the SMPLX, a\ncommonly used parametric model, via 3D keypoints. Body\nshape \u03b2 \u2208 Rn\u00d710 (or \u03b2 \u2208 Rn\u00d711 for children [24, 53]),\npose parameters (body pose, hand pose, and global orien-\ntation) \u03b8 \u2208 Rn\u00d7156, and translation parameters t \u2208 Rn\u00d73\n(n is the number of frames) are estimated via a modified\nSMPLify-X [54] for dynamic poses.\nOur annotation pipeline has proved effective and robust\nin getting natural SMPLX model, as shown in Fig. 3. We\nevaluate the fitting error between 3D keypoints and corre-\nsponding regressed SMPLX joints. The mean and median\n\u2018Mean Per Joint Position Error\u2019 (MPJPE) of our system is\n30.20 mm and 29.80 mm. The error is on par with the or-\nacle fitting accuracy of 29.34 mm in Human3.6M [26, 44],\nwhich includes data from an optical motion capture system.\nDetailed analysis is conducted in Sec. B.3 and Sec. B.4. A\nthorough comparison of our fitting pipeline with other fit-\nting methods [65, 89, 13] is described in Sec. B.5.\n4. Benchmarking Human-centric Rendering\nOur DNA-Rendering dataset could be used to unfold the\nreflections and boot the developments of research on high-\nfidelity human body rendering tasks, due to its large-scale\nvolume, diverse scenarios, multi-level challenges, and high-\nresolution multi-view data properties. To kick off an exam-\nple of how to utilize this dataset, we set up benchmarks with\nexclusive experiments centered around three fundamental\ntasks of human body rendering, i.e., novel view synthesis,\nnovel pose animation, and novel identity rendering. In this\nsection, we introduce the benchmark settings and key ob-\nservations.\n4.1. Data Splits\nTo unfold each method in depth, and thoroughly evaluate\nthe effectiveness of our dataset, we construct multiple train-\ning and testing data splits to conduct level tests for each\nmethod. We consider the four most influential factors of\nrendering quality for the benchmark test, i.e., the looseness\nof clothes, the texture complexity, the pose difficulty, and\nthe interactivity between the human body and manipulated\nobject.\nThe Cloth Looseness. We define the cloth\u2019s challenging\nlevels by the deformation distance between the minimal-\ncloth human body and the clothing outline, and the softness\nof cloth materials. For the Easy level, we collect simple\ncases wearing tight-fitting clothes like yoga wear and sports\nt-shirts. The Medium level includes the daily clothes such\nas coats, skirts, jeans, loose t-shirts, etc. As for the Hard\nlevel, we make a split containing ethical costumes, national\nclothing, and fancy decorations.\nThe Texture Complexity.\nThe texture distribution also\nplays an important role in the human body rendering tasks.\nTo examine the correlations between texture complexity and\nrendering performance, we build three data splits for texture\nevaluation. The Texture-Easy split is composed of single-\ncolor clothes. The Texture-Medium split includes most daily\nclothes in a few colors and plain patterns. The Texture-Hard\nsplit contains the most complicated texture clothes with in-\ntricate patterns like dots, stripes, plaids, etc.\nThe Pose Difficulty. In the novel pose animation task, it\nis vital to probe if the trained models could handle differ-\nent levels of motion sequences in terms of (non-rigid) diffi-\nculties and degree of out-of-distribution (OOD). Therefore,\nwe prepare the Motion-Easy, Motion-Medium, and Motion-\nHard splits for training and evaluation. The Easy data are\n6\nsimple motions with limited body parts involved, like shak-\ning and waving hands. The Medium level refers to casual\nmotions including full-body actions such as walking, eat-\ning, sitting, kneeling, stretching, etc. Moreover, the Hard\nsplit is designed to cover the extremely challenging mo-\ntion cases that are performed by professional sports players\nor actors, e.g., instrument playing, sports action, yoga, and\ndancing.\nThe Human-Object Interactivity. Finally, we propose to\nevaluate the impact of human-object interactivity by novel\nview synthesis and novel pose animation tasks. The data\ndifficulty level is determined by the object size. Specifi-\ncally, we define four data splits: the Interaction-No split\ncontains pure human motions with no interactive objects;\nthe Interaction-Easy split includes rigid small-size hand-\nheld objects like cellphones, pencils, cigarettes, and cups.\nThe Interaction-Medium split has middle-size hand-held\nobjects, e.g., handbag, volleyball, newspaper, etc. This split\nincludes both rigid motions and non-rigid object motions;\nand the Interaction-Hard split consists of large-size assets\nsuch as yoga mats, desks, chairs, and sofas.\nTo sum up, we construct an overall train split consist-\ning of 400 sequences with even distribution on all human\nfactors and difficulties, and 13 test factor-difficulty splits in\ntotal with three sequences in each test split.\n4.2. Task Definition\nHuman body rendering and animation problems have\nbeen popular research topics in the digital human area for\ndecades. With the development of implicit scene represen-\ntations like NeRF [50], many researchers design algorithms\nupon the methodology. In conducting the benchmark exper-\niment on the DNA-Rendering dataset, we intend to provide\na thorough comparison of the state-of-the-art methods on\ndata splits with different difficulty levels. Depending on the\ngeneralizability of the state-of-the-art methods, we catego-\nrize the recently published works into two classes: case-\nspecific methods and generalizable ones. We evaluate the\nmethods under multiple problem settings according to their\ncategories. Concretely, we set up novel view synthesis and\nnovel pose animation tasks for the case-specific methods,\nand the novel identity rendering task for the generalization\napproaches. In this section, we present the key observa-\ntions of the benchmarks. We provide a detailed review of\nthe benchmark methods and necessary modifications to ap-\nply them to our dataset in Sec. C.1 of the supplementary.\nNovel View Synthesis.\nRecent dynamic human render-\ning works like NeuralBody [56], A-NeRF [66], Animat-\nableNeRF [55], and NeuralVolumes [42] obtained impres-\nsive results by training on a single case with multi-view\nvideo data. HumanNeRF [77] demonstrated the ability to\nrender realistic novel view images of humans from monoc-\nular video sequences.\nIn this task, we adopt the offi-\ncial architecture implementation of the case-specific meth-\nods and train each individual model for every single case\nin the DNA-Rendering test set.\nFor a fair comparison,\nwe unify the training setting of NeuralVolumes [42], A-\nNeRF [66], NeuralBody [56], AnimatableNeRF [55], and\nHumanNeRF [77] with 42 dense training views. we eval-\nuate the image rendering quality of these methods on the\nother 18 unseen testing camera poses. Meanwhile, we also\ntrain two general scene static methods \u2013 Instant-NGP [51]\nand NeuS [74], in each testing frame with the same training\nviews. These two methods\u2019 performances could serve as the\nper-frame static reconstruction baseline reference. The ren-\ndering results are analyzed based on the difficulty level of\ndata splits.\nNovel Pose Animation. Similar to the novel view synthe-\nsis task, we conduct novel pose animation benchmark on\nthe four case-specific methods [56, 55, 66, 77]. For each\ntest case, we split the sequence into two parts, where im-\nages from the first 80% frames are used for training and the\nones from the last 20% are used for testing. Besides, for\nthe SMPL-guided pose animation methods [56, 55, 77], we\nprovide the SMPL parameters of test images for the mod-\nels to infer rendering. As for the SMPL-free method [66],\nthe trained models take the target pose images as the input\n(i.e., the underlying skeletons), and optimize the novel pose\nrepresentations for inference.\nNovel Identity Rendering.\nThe other category of our\nbenchmark methods is the generalizable algorithms that can\nbe trained on multiple cases and infer across different un-\nseen identities. Specifically, we probe three general scene\ngeneralizable methods \u2013 PixelNeRF [81], VisionNeRF [38],\nIBRNet [75], and two human-centric methods \u2013 Neural-\nHumanPerformer [34], and KeypointNeRF [49]. To fairly\ncompare their performances on unseen identities, we use\nthe same training set (all training samples of the three splits,\nwhich results in 400 sequences in total) to train the general-\nizable models. In the inference stage, we evaluate the image\nrendering quality on novel cases from each test split respec-\ntively.\n4.3. Benchmark Results\nAs introduced in Sec. 4.1, we construct a test set with 13\nsub-splits according to the four most concerned attributes\n(Deformation, Motion, Texture, and Interaction) in differ-\nent difficulty levels (Easy, Medium, and Hard), and an ex-\ntra No level for Interaction. This results in a data volume\nof 39 motion sequences for testing. For all rendered im-\nages, three metrics are computed \u2013 PSNR, SSIM [76], and\nLPIPS [29] (LPIPS* denotes LPIPS\u00d71000). We evaluate\nmore than 10 state-of-the-art methods on these splits and\nanalyze their performances under the same metrics. The\nexperiment analysis is given below. Noted that due to lim-\nited space in the main paper, we provide the detailed setting,\n7\nInteraction\nSimple\nHard\nMedium\nNo\nDeformation\nSimple\nHard\nMedium\nInstant-NGP\nNeuS\nNeural Volumes\nA-NeRF\nNeural Body\nAnimatableNeRF\nHumanNeRF\nScaled 1/PSNR\nScaled 1/SSIM\nScaled LPIPS\nMotion\nSimple\nHard\nMedium\nTexture\nSimple\nHard\nMedium\nInstant-NGP\nNeuS\nNeural Volumes\nA-NeRF\nNeural Body\nAnimatableNeRF\nHumanNeRF\nFigure 4: Quantitative results visualization of novel view syn-\nthesis test across benchmarks splits and difficulties. The col-\nored circles denote different metrics, the smaller the circles indi-\ncate the better the novel view quality the method achieved. The\nnumbers are reported in the appendix (Tab. S2).\nthorough discussions, and additional results in Sec. C in the\nsupplementary.\nNovel View Synthesis. We visualize the bubble diagram\nof quantitative results across all benchmark splits in Fig. 4.\nThe precise numbers of the quantitative results are reported\nin Tab. S2 in the supplementary. We conclude three key\nobservations in the main paper: (1) Generally speaking,\nthe rendering quality is inversely proportional to split dif-\nficulties, as reported in Fig. 4, where the circles get bigger\nwhen the difficulty grows. (2) Among all case-specific dy-\nnamic methods, A-NeRF [66] achieves the best PSNR per-\nformance, and NeuralBody [56] and HumanNeRF [77] gets\nthe best SSIM and LPIPS respectively. Qualitative results\nare shown in Fig. 5, NeuralBody [56] and A-NeRF [66]\ncould render novel view image with fewer background ar-\ntifacts than other methods, while HumanNeRF [77] can\nbetter preserve high fidelity textures, especially in high-\nfrequency texture regions. (3) When it comes to render-\ning novel views for trained human action frames, the diffi-\nculties increase on Motion and Interaction dimensions will\nnot separate the performances among dynamic methods to\na large extent, while hard Texture cases enlarge the perfor-\nmance gap among dynamic methods (refer to T-shirt case\nwith stripe pattern in Fig. 5). Meanwhile, dynamic meth-\nods\u2019 performances on Texture degrade the most when diffi-\nculty rises compared to the static baselines (refer to bubbles\nin Fig. 4). Texture-Hard and Deformation-Hard shows the\nhigh texture and non-rigidity are still challenging for simple\ndynamic modeling with bone-coordinates [66] or SMPL-\nskinning based blending [55], even in seen-poses. More\nqualitative results in each benchmark split are shown in\nFig. S9, and we analyze the conceptual difference of these\nmethods in Sec. C.2.1 in the appendix.\nNovel Pose Animation.\nSimilar to novel view synthe-\nsis, when split difficulty increases the rendering quality de-\ncreases as shown in Tab. 2. Among all data factors, we\nfound that Deformation and Interaction are insurmount-\nable factors for current methods to model in novel poses.\nQualitative results are displayed in Fig. 6, none of the\nmethods can generate reasonable deformation in the case\nof the Peking opera costume. NeuralBody [56] and An-\nimatableNeRF [55] can not model the interactive objects,\nSplits\nPSNR\u2191\nSSIM\u2191\nLPIPS*\u2193\nNV\nAN\nNB\nAnN\nHN\nNV\nAN\nNB\nAnN\nHN\nNV\nAN\nNB\nAnN\nHN\nMotion-Simple\n22.05\n26.65\n25.84\n22.78\n24.65\n0.947\n0.965\n0.974\n0.958\n0.953\n78.30\n58.04\n58.33\n74.33\n62.76\nMotion-Medium\n19.30\n21.73\n21.84\n21.41\n21.14\n0.941\n0.951\n0.969\n0.957\n0.952\n92.80\n71.81\n65.46\n80.97\n54.46\nMotion-Hard\n19.17\n21.49\n20.43\n19.64\n22.48\n0.938\n0.952\n0.965\n0.949\n0.964\n105.46\n83.58\n82.85\n97.98\n51.18\nDeformation Simple\n20.42\n25.44\n24.57\n23.62\n26.15\n0.939\n0.957\n0.968\n0.958\n0.967\n84.65\n53.30\n59.04\n61.12\n30.18\nDeformation-Medium\n23.09\n27.26\n27.05\n23.52\n24.97\n0.945\n0.963\n0.974\n0.961\n0.958\n61.09\n48.41\n49.91\n65.43\n33.94\nDeformation-Hard\n20.11\n20.88\n20.27\n19.41\n19.70\n0.925\n0.926\n0.956\n0.943\n0.924\n117.31\n108.89\n102.84\n103.22\n102.67\nTexture-Simple\n20.99\n26.21\n25.54\n23.12\n25.65\n0.954\n0.974\n0.982\n0.970\n0.974\n77.68\n49.88\n50.48\n67.40\n28.81\nTexture-Medium\n25.44\n27.94\n25.77\n23.15\n27.19\n0.959\n0.966\n0.977\n0.962\n0.969\n56.68\n43.94\n48.44\n67.00\n24.04\nTexture-Hard\n20.95\n23.22\n22.05\n18.45\n23.78\n0.916\n0.927\n0.951\n0.943\n0.945\n117.93\n98.43\n96.01\n101.09\n41.84\nInteraction-No\n22.64\n26.32\n25.41\n22.44\n25.93\n0.957\n0.968\n0.980\n0.967\n0.968\n71.98\n62.55\n54.71\n69.29\n31.61\nInteraction-Simple\n24.28\n27.57\n26.42\n23.18\n27.18\n0.965\n0.976\n0.983\n0.968\n0.975\n55.54\n48.35\n45.86\n65.36\n23.31\nInteraction-Medium\n20.37\n23.67\n21.96\n20.81\n23.20\n0.934\n0.950\n0.965\n0.953\n0.951\n95.79\n84.74\n87.41\n97.32\n52.10\nInteraction-Hard\n21.14\n25.00\n22.10\n21.29\n22.29\n0.931\n0.949\n0.961\n0.953\n0.940\n94.04\n79.40\n89.63\n91.82\n70.54\nOverall\n21.53\n24.88\n23.79\n21.76\n24.18\n0.942\n0.956\n0.970\n0.957\n0.957\n85.33\n68.56\n68.54\n80.18\n46.73\nTable 2: Benchmark results on novel pose task. State-of-the-art methods\u2019 rendering performance on novel poses for each benchmark\nsplit. We abbreviate NeuralVolumes [42] as \u2018NV\u2019, A-NeRF [66] as \u2018AN\u2019, NeuralBody [56] as \u2018NB\u2019, AnimatableNeRF [55] as \u2018AnN\u2019 and\nHumanNeRF [77] as \u2018HN\u2019. Cell color\nindicate the best, second best, and third best performance in the same split respectively.\n8\nGT\nNeuralVolumes\nA-NeRF\nNeuralBody\nAnimatableNeRF\nHumanNeRF\nFigure 5: Visualization of novel view synthesis result samples. We qualitatively compare the novel views of the test frames in our test\nsplits. More qualitative novel results in each split are shown in Fig. S9.\nand the objects are stretched when given large poses in\nA-NeRF [66]. Conclusively, current state-of-the-art meth-\nods can learn relatively reasonable human avatars with even\nhard Motion and Textures, while stuck in the imperfectness\nof modeling hard Deformation and Interaction. These two\nanimation challenges should stimulate the communities for\nfurther investigation. More detailed analysis is provided in\nSec. C.2.2 in the appendix.\nNovel Identity Rendering. We report the quantitative met-\nrics of all 39 novel identities in Tab. 3. Generally, gen-\neralizable methods with human piror [49, 34] performs\nbetter with higher robustness than category-agnostic meth-\nods [81, 75, 38]. Among category-agnostic methods, IBR-\nNet [75] directly blends pixel color from source views,\nand it outperforms PixelNeRF and VisionNeRF that pre-\ndict radiance color only from image features. We draw the\nconclusion that, in generalizable human rendering, human\nprior and appearance references from observation could\nhelp boost the generalization ability on data with large vari-\nations of poses and appearances. We illustrate the qualita-\nSplits\nPSNR\u2191\nSSIM\u2191\nLPIPS*\u2193\nIBR\nPN\nVN\nNHP\nKN\nIBR\nPN\nVN\nNHP\nKN\nIBR\nPN\nVN\nNHP\nKN\nMotion-Simple\n26.13\n26.04\n25.90\n25.61\n24.67\n0.964\n0.957\n0.959\n0.961\n0.964\n65.48\n72.68\n72.29\n65.53\n44.77\nMotion-Medium\n25.56\n25.84\n25.22\n24.63\n24.34\n0.966\n0.960\n0.961\n0.963\n0.971\n59.71\n63.80\n64.83\n61.08\n38.22\nMotion-Hard\n23.78\n24.49\n23.72\n23.43\n24.36\n0.959\n0.950\n0.949\n0.956\n0.973\n79.18\n89.93\n93.04\n80.20\n43.79\nDeformation Simple\n26.72\n26.85\n26.31\n26.41\n26.01\n0.965\n0.960\n0.960\n0.963\n0.965\n53.73\n61.92\n63.48\n48.45\n34.68\nDeformation-Medium\n27.46\n27.55\n27.90\n27.28\n25.83\n0.965\n0.961\n0.965\n0.963\n0.972\n57.30\n66.82\n62.82\n56.02\n38.00\nDeformation-Hard\n23.98\n20.77\n20.05\n22.64\n23.00\n0.942\n0.841\n0.838\n0.936\n0.943\n87.04\n282.36\n264.57\n92.22\n67.76\nTexture-Simple\n25.70\n26.27\n25.62\n25.43\n22.72\n0.973\n0.967\n0.968\n0.973\n0.971\n63.66\n69.16\n70.93\n58.70\n50.07\nTexture-Medium\n26.15\n26.76\n26.40\n26.25\n25.14\n0.965\n0.960\n0.963\n0.963\n0.968\n54.45\n56.66\n56.58\n53.95\n35.10\nTexture-Hard\n23.34\n24.08\n23.61\n23.45\n22.91\n0.932\n0.921\n0.922\n0.933\n0.935\n98.77\n106.05\n104.58\n90.84\n72.87\nInteraction-No\n26.08\n25.91\n26.39\n25.46\n23.43\n0.968\n0.958\n0.963\n0.966\n0.968\n61.99\n72.67\n66.91\n64.35\n44.95\nInteraction-Simple\n27.60\n25.76\n27.54\n26.67\n26.12\n0.976\n0.944\n0.973\n0.974\n0.977\n50.50\n82.30\n53.38\n50.77\n28.60\nInteraction-Medium\n24.04\n24.44\n24.09\n23.61\n22.70\n0.950\n0.937\n0.941\n0.947\n0.950\n84.64\n96.53\n93.92\n86.29\n63.72\nInteraction-Hard\n24.78\n25.76\n24.93\n24.02\n24.12\n0.951\n0.944\n0.942\n0.946\n0.953\n79.06\n82.30\n82.54\n78.82\n58.43\nOverall\n25.49\n25.42\n25.21\n24.99\n24.26\n0.960\n0.943\n0.946\n0.957\n0.962\n68.89\n92.55\n88.45\n68.25\n47.77\nTable 3: Benchmark results on novel identity task. State-of-the-art methods\u2019 performance on novel identity rendering in each benchmark\nsplit. We abbreviate IBRNet [75] as \u2018IBR\u2019, PixelNeRF [81] as \u2018PN\u2019, VisionNeRF [38] as \u2018VN\u2019, NeuralHumanPerformer [34] as \u2018NHP\u2019\nand KeypointNeRF [77] as \u2018KN\u2019.\n9\nGT\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 6: Visualization of novel pose animation result samples. From top to bottom, we illustrate the reposing results generated by (a-e):\nNeuralVolumes [42], A-NeRF [66], NeuralBody [56] AnimatableNeRF [55], and HumanNeRF [77].\ntive results in Fig. S11. We provide additional results and\nanalysis in Sec. C.2.3 in supplementary.\n4.4. Cross-dataset Comparison\nApart from the benchmark experiments conducted on\ndifferent data splits of DNA-Rendering dataset, we also\nevaluate the data generalizability provided by our dataset\nand the other competitive ones, i.e., GeneBody[13], ZJU-\nMoCap [56] and HuMMan [7].\nSetting and Implementations. To eliminate the scale and\nannotation differences across all datasets, we train three\ngeneral scene generalizable rendering methods [81, 75, 38]\non these datasets with the same pixel batch per-iteration\nand stop training with the same 200K global iterations.\nFor each method, we train each individual model on each\ndataset mentioned above, with a fixed image resolution\n512 \u00d7 512 and four balanced views. To thoroughly evaluate\nthe datasets\u2019 generalizability, we cross-verify the rendering\nimages of novel identities on each dataset.\nResults. The experimental results are presented in Fig. 7\nin terms of the average PSNR of all three methods. From\nthis colored error map, we conclude that training on DNA-\nRendering dataset is beneficial for generalizing to the other\ndatasets. Generally, due to the existence of domain gaps,\na model would perform better in the situation of an in-\ndomain setting, where the training set and test set follow the\nsame distribution, see diagonal elements in Fig. 7. The off-\ndiagonal numbers report the cross-domain performances of\nmodels trained on one dataset and tested directly on other\ndatasets\u2019 test sets. We observe an interesting phenomenon\nthat, compared to datasets with limited data diversity and\nhigh data bias (like ZJU-MoCap [56] and HuMMan [7]), the\nproposed dataset enables generalization methods to achieve\nmore plausible results even with large domain gaps. More-\nover, opposite to DNA-Rendering, HuMMan [7] generalize\npoorly on other datasets even on cases with simple motions\nand appearances in ZJU-MoCap [56], despite the fact that\nboth HuMMan [7] and our DNA-Rendering have large data\nvolume. From a data engineering perspective, this demon-\nstrates the construction of the proposed dataset benefits the\ncommunity not merely with the amount of data, more im-\nportantly, the significant improvement in data completeness\nand richness. Due to space limit, we provide the detailed\nsetup and additional results in Sec. D of the supplementary.\n10\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\nPSNR\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\n22.67 20.60 19.25 18.78\n21.71 21.13 18.87 18.65\n22.05 22.57 26.05 19.51\n21.43 20.94 18.54 25.03\n20\n22\n24\n26\nDataset Trained\nDataset Tested\nFigure 7: Results of cross-dataset experiments. We visualize the\n\u2018affinity\u2019 matrix of cross-dataset evaluation results.\nIt is worth motioning that, we also unfold the generalization\nperformance across testing cameras and reveal the impact of\ncolor consistency for multi-camera datasets in Sec. D.4.\n5. Conclusion\nWe have presented DNA-Rendering, a large-scale and\nhigh-fidelity repository for human-centric rendering. It is\na multiview human body capture dataset that covers many\ndiverse factors like ethnicity, age, body shape, clothing, mo-\ntion, and interactive objects with faithful annotations. We\nhave also presented benchmarks to evaluate state-of-the-art\napproaches on the DNA-Rendering dataset with in-depth\ndiscussions and compared our dataset with the others via\ncross-dataset experiments on generalization capability. We\nhope our DNA-Rendering project could boost the develop-\nment of human-centric rendering and related domains with\nnew reflections, challenges, and opportunities.\nAcknowledgements.\nThis study is supported under the\nRIE2020 Industry Alignment Fund Industry Collaboration\nProjects (IAF-ICP) Funding Initiative, as well as cash and\nin-kind contributions from the industry partner(s). It is also\npartially supported by Singapore MOE AcRF Tier 2 (MOE-\nT2EP20221-0011, MOE-T2EP20221-0012), NTU NAP.\nReferences\n[1] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,\nChristian Theobalt, and Gerard Pons-Moll. Learning to re-\nconstruct people in clothing from a single rgb camera. In\nCVPR, 2019.\n[2] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,\nand Marcus Magnor. Tex2shape: Detailed full human body\ngeometry from a single image. In ICCV, 2019.\n[3] Oliver Batchelor.\nMulti-camera calibration using one\nor more calibration patterns.\nhttps://github.com/\noliver-batchelor/multical, 2021.\n[4] Federica Bogo,\nJavier Romero,\nMatthew Loper,\nand\nMichael J. Black. FAUST: Dataset and evaluation for 3D\nmesh registration. In CVPR, 2014.\n[5] Federica Bogo, Javier Romero, Gerard Pons-Moll, and\nMichael J. Black. Dynamic FAUST: Registering human bod-\nies in motion. In CVPR, 2017.\n[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\nand Juan Carlos Niebles. Activitynet: A large-scale video\nbenchmark for human activity understanding.\nIn CVPR,\n2015.\n[7] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao\nYu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang\nPan, et al. Humman: Multi-modal 4d human dataset for ver-\nsatile sensing and modeling. In ECCV, 2022.\n[8] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei,\nDaxuan Ren, Jiatong Li, Zhengyu Lin, Haiyu Zhao, Shuai\nYi, Lei Yang, et al. Playing for 3d human recovery. arXiv\npreprint, 2021.\n[9] Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. Utd-\nmhad: A multimodal dataset for human action recognition\nutilizing a depth camera and a wearable inertial sensor. In\nICIP, 2015.\n[10] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler,\nRaquel Urtasun, and Alan Yuille. Detect what you can: De-\ntecting and representing objects using holistic models and\nbody parts. In CVPR, 2014.\n[11] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\nand Andreas Geiger. Snarf: Differentiable forward skinning\nfor animating non-rigid neural implicit shapes.\nIn ICCV,\n2021.\n[12] Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li,\nYu Guo, Jue Wang, and Fei Wang. Uv volumes for real-\ntime rendering of editable free-view human performance. In\nCVPR, 2023.\n[13] Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu,\nKwan-Yee Lin, and Hongsheng Li.\nGeneralizable neural\nperformer: Learning robust radiance fields for human novel\nview synthesis. arXiv preprint, 2022.\n[14] Sungjoon Choi, Qian-Yi Zhou, and Vladlen Koltun. Robust\nreconstruction of indoor scenes. In CVPR, pages 5556\u20135565,\n2015.\n[15] EasyMocap Contributors. Easymocap - make human mo-\ntion capture easier. https://github.com/zju3dv/\nEasyMocap, 2021.\n[16] XRPrimer Contributors.\nOpenxrlab foundational library\nfor xr-related algorithms.\nhttps://github.com/\nopenxrlab/xrprimer.\n[17] XRMoCap Contributors. Openxrlab multi-view motion cap-\nture toolbox and benchmark.\nhttps://github.com/\nopenxrlab/xrmocap, 2022.\n[18] 3D People Cooperation.\n3d people dataset.\nhttps://\n3dpeople.com/.\n[19] RenderPeople Cooperation. Renderpeople dataset. http:\n//https://renderpeople.com/, 2017.\n[20] T. Derose, M. Kass, and T. Truong. Subdivision surfaces in\ncharacter animation. In SIGGRAPH, 1998.\n11\n[21] Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming\nYang, and Liang Lin. Instance-level human parsing via part\ngrouping network. In ECCV, 2018.\n[22] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and\nMichael F Cohen. The lumigraph. In SIGGRAPH, 1996.\n[23] Peter Hedman and Johannes Kopf. Instant 3d photography.\nTOG, 2018.\n[24] Nikolas Hesse, Sergi Pujades, Javier Romero, Michael J\nBlack, Christoph Bodensteiner, Michael Arens, Ulrich G\nHofmann, Uta Tacke, Mijna Hadders-Algra, Raphael Wein-\nberger, et al. Learning an infant body model from rgb-d data\nfor accurate full body motion analysis. In MICCAI, 2018.\n[25] Mustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and Matthias\nNie\u00dfner. Humanrf: High-fidelity neural radiance fields for\nhumans in motion. TOG, 2023.\n[26] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6m: Large scale datasets and predic-\ntive methods for 3d human sensing in natural environments.\nTPAMI, 2013.\n[27] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,\nand Anurag Ranjan. Neuman: Neural human radiance field\nfrom a single video. In ECCV, 2022.\n[28] Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen\nQian, Wanli Ouyang, and Ping Luo.\nWhole-body human\npose estimation in the wild. In ECCV, 2020.\n[29] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV, 2016.\n[30] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic studio: A massively multiview system for\nsocial motion capture. In ICCV, 2015.\n[31] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei\nTan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart\nNabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and\nYaser Sheikh. Panoptic studio: A massively multiview sys-\ntem for social interaction capture. TPAMI, 2017.\n[32] Sai\nCharan\nMahadevan\nKarunanidhi\nDurai\nKu-\nmar,\nHuang\nGeng.\nSfu\nmotion\ncapture\ndatabase.\nhttps://mocap.cs.sfu.ca/.\n[33] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint, 2013.\n[34] Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry\nFuchs. Neural human performer: Learning generalizable ra-\ndiance fields for human performance rendering. NeurIPS,\n2021.\n[35] CMU Graphics Lab.\nCmu graphics lab motion capture\ndatabase. http://mocap.cs.cmu.edu/.\n[36] Verica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll.\n360-degree textures of people in clothing from a single im-\nage. In 3DV, 2019.\n[37] Ruilong Li, Shan Yang, David A Ross, and Angjoo\nKanazawa. Ai choreographer: Music conditioned 3d dance\ngeneration with aist++. In ICCV, 2021.\n[38] Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin,\nYi-Chang Shih, and Ravi Ramamoorthi. Vision transformer\nfor nerf-based view synthesis from a single input image. In\nWACV, 2023.\n[39] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta,\nBrian L Curless, Steven M Seitz, and Ira Kemelmacher-\nShlizerman. Real-time high-resolution background matting.\nIn CVPR, 2021.\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014.\n[41] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou\nTang. Deepfashion: Powering robust clothes recognition and\nretrieval with rich annotations. In CVPR, 2016.\n[42] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: learning dynamic renderable volumes from images.\nTOG, 2019.\n[43] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nTOG, 2021.\n[44] Matthew Loper, Naureen Mahmood, and Michael J Black.\nMosh: Motion and shape capture from sparse markers. TOG,\n2014.\n[45] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. TOG, 2015.\n[46] MR Luo, G Cui, and B Rigg. The development of the cie\n2000 colour-difference formula: Ciede2000. Color Research\nand Application, 2001.\n[47] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael J. Black. AMASS: Archive of\nmotion capture as surface shapes. In ICCV, 2019.\n[48] Dushyant Mehta,\nHelge Rhodin,\nDan Casas,\nPascal\nFua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt. Monocular 3d human pose estimation in the wild\nusing improved cnn supervision. In 3DV, 2017.\n[49] Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu\nTang, and Shunsuke Saito.\nKeypointnerf:\nGeneralizing\nimage-based volumetric avatars using relative spatial encod-\ning of keypoints. In ECCV, pages 179\u2013197, 2022.\n[50] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020.\n[51] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. TOG, 2022.\n[52] Ferda Ofli, Rizwan Chaudhry, Gregorij Kurillo, Ren\u00b4e Vidal,\nand Ruzena Bajcsy. Berkeley mhad: A comprehensive mul-\ntimodal human action database. In WACV, 2013.\n[53] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch, David T\nHoffmann, Shashank Tripathi, and Michael J Black. Agora:\nAvatars in geography optimized for regression analysis. In\nCVPR, 2021.\n[54] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\n12\nMichael J Black. Expressive body capture: 3d hands, face,\nand body from a single image. In CVPR, 2019.\n[55] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan\nZhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-\nmatable neural radiance fields for modeling dynamic human\nbodies. In ICCV, 2021.\n[56] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans.\nIn CVPR,\n2021.\n[57] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J\nBlack. Clothcap: Seamless 4d clothing capture and retarget-\ning. SIGGRAPH, 2017.\n[58] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In CVPR, 2017.\n[59] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.\n\u201d grabcut\u201d interactive foreground extraction using iterated\ngraph cuts. TOG, 2004.\n[60] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\nimplicit function for high-resolution clothed human digitiza-\ntion. In ICCV, 2019.\n[61] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In CVPR, 2020.\n[62] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J\nBlack. Scanimate: Weakly supervised learning of skinned\nclothed avatar networks. In CVPR, 2021.\n[63] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+ d: A large scale dataset for 3d human activity anal-\nysis. In CVPR, 2016.\n[64] Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang\nSun, and Yebin Liu. Diffustereo: High quality human recon-\nstruction via diffusion-based stereo using sparse cameras. In\nECCV, 2022.\n[65] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen,\nXiaowei Zhou, and Hujun Bao.\nNovel view synthesis of\nhuman interactions from sparse multi-view videos. In SIG-\nGRAPH, 2022.\n[66] Shih-Yang Su, Frank Yu, Michael Zollh\u00a8ofer, and Helge\nRhodin. A-nerf: Articulated neural radiance fields for learn-\ning human shape, appearance, and pose. NeurIPS, 2021.\n[67] Zhaoqi Su, Tao Yu, Yangang Wang, and Yebin Liu. Deep-\ncloth: Neural garment representation for shape and style\nediting. TPAMI, 2023.\n[68] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose esti-\nmation. In CVPR, 2019.\n[69] Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Ger-\nard Pons-Moll. Sizer: A dataset and model for parsing 3d\nclothing and learning size sensitive 3d clothing. In ECCV,\n2020.\n[70] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki,\nand Masataka Goto. Aist dance video database: Multi-genre,\nmulti-dancer, and multi-camera database for dance informa-\ntion processing. In ISMIR, 2019.\n[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 2017.\n[72] Timo Von Marcard, Roberto Henschel, Michael J Black,\nBodo Rosenhahn, and Gerard Pons-Moll.\nRecovering ac-\ncurate 3d human pose in the wild using imus and a moving\ncamera. In ECCV, 2018.\n[73] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo\nSpeciale, and Marc Pollefeys.\nPatchmatchnet:\nLearned\nmulti-view patchmatch stereo. In CVPR, 2021.\n[74] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nNeurIPS, 2021.\n[75] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\nnet: Learning multi-view image-based rendering. In CVPR,\n2021.\n[76] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004.\n[77] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video. In CVPR, 2022.\n[78] Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan\nBali, Danielle Belko, Eric Brockmeyer, Lucas Evans, Tim-\nothy Godisart, Hyowon Ha, Alexander Hypes, et al. Mul-\ntiface: A dataset for neural face rendering. arXiv preprint,\n2022.\n[79] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and\nMichael J Black. Econ: Explicit clothed humans optimized\nvia normal integration. In CVPR, 2023.\n[80] Yuliang\nXiu,\nJinlong\nYang,\nDimitrios\nTzionas,\nand\nMichael J. Black. ICON: Implicit Clothed humans Obtained\nfrom Normals. In CVPR, 2022.\n[81] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images. In\nCVPR, 2021.\n[82] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-\nhai Dai, and Yebin Liu. Function4d: Real-time human vol-\numetric capture from very sparse consumer rgbd sensors. In\nCVPR, 2021.\n[83] Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, and\nKwan-Yee Lin.\nMonohuman: Animatable human neural\nfield from monocular video. In CVPR, 2023.\n[84] Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth\nVenkatesh, Jaesik Park, Jihun Yu, and Hyun Soo Park.\nHumbi: A large multiview dataset of human body expres-\nsions. In CVPR, 2020.\n[85] Chao Zhang, Sergi Pujades, Michael J. Black, and Gerard\nPons-Moll. Detailed, accurate, human shape estimation from\nclothed 3d scan sequences. In CVPR, 2017.\n[86] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018.\n13\n[87] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang\nZhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently gen-\nerated human radiance field from sparse inputs. In CVPR,\n2022.\n[88] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-\ndong Guo, and Yebin Liu. Structured local radiance fields\nfor human avatar modeling. In CVPR, 2022.\n[89] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.\nPamir: Parametric model-conditioned implicit representa-\ntion for image-based human reconstruction. TPAMI, 2021.\n[90] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and\nYebin Liu. Deephuman: 3d human reconstruction from a\nsingle image. In CVPR, 2019.\n[91] Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuix-\niang Shao, Wenzheng Chen, Lan Xu, and Jingyi Yu. Re-\nlightable neural human assets from multi-view gradient illu-\nminations. In CVPR, 2023.\n14\nAppendix\nIn the appendix, we provide detailed information about\nthe proposed DNA-Rendering dataset and the attached\nbenchmarks. We first provide dataset statistics, hardware\ndesign, and data collection protocol in Sec. A. Then, we\ndiscuss the additional information about the annotations, as\nwell as a comparison to other publicly released toolchains\nin Sec. B. Moreover, we conduct compact discussions on\nthe benchmarks by introducing more detailed settings, ad-\nditional results, and unfolded comparisons of benchmark\nmethods\u2019 conceptual differences in Sec. C. We provide an\nin-depth discussion on competing datasets and highlight our\ncomparative contributions to society in Sec. D. Finally, we\ndiscuss our future work in Sec. E.\nA. Dataset Details\nA.1. Dataset Statistics\nDNA-Rendering has a wide distribution over ethnicity,\nclothing, actions, and human-object-interaction scenarios.\nIn this section, we present the detailed data distribution\nin key data aspects, namely ethnicity, age, shape, actions,\nclothing, and interactive objects.\nEthnicity, Age and Shape. We invite 500 actors with a\nuniform distribution of gender and a ratio of 4 : 3 : 2 : 1\nfor Asian, Caucasian, Black, and Hispanic individuals, re-\nspectively. The quota has a wide coverage of age and body\nshape. We visualize the distribution of actors\u2019 age, height,\nand weight in Fig. S1.\nHuman Actions. DNA-Renderingcovers both normal ac-\ntions and professional actions. We maintain a library of\n269 human action definitions, including daily-life activi-\nties, simple exercises, and social communication. All nor-\nmal performers are asked to select 9 actions from the action\nlibrary and perform the picked actions in a free-style man-\nner. There are 153 professional actors among the total 500\nperformers. These professional actors are asked to dress\nin their special costumes and perform 6 unique professional\nactions with skills, including special costume performances,\nartistic movements, sports activities, etc.. Note that differ-\nent from the intuitive visualization in Fig. 1, we visualize\nfine-grain categories of professional and normal action in\nFig. S2a. These labels are classified in terms of a standard\nhuman activity subcategory definition. The sunburst chart\nof distribution is visualized in the middle, and samples of\nspecific categories of labels are visualized in the outer word\ncloud.\nClothing and Interactive Objects. We create a clothing\nrepository with 527 items, which covers all 50 clothing\ntypes in DeepFashion [41] while with a random distribu-\ntion of color, material, texture, and looseness for each cloth-\ning type. We ask each performer to wear three sets of out-\nfits, where one comes from the performer\u2019s self-prepared\noutfit (for both special and normal actors), and the other\ntwo are randomly coordinated from our clothing repository.\nThe distribution of cloth statistical distribution on all ac-\ntion sequences and samples of cloth labels is illustrated in\nFig. S2b.\nA.2. Meta Attributes\nWe have designed an attribute system for each dimen-\nsion of the collected data, including basic information about\nthe actors, clothing, and action information for each action\nsequence. Fig. S3 shows an example of meta attribute in-\nformation of an action sequence for a professional actor.\nIn terms of actor information, we record the actor\u2019s name,\ngender, ethnicity, age, height, and weight. For clothing in-\nformation, we describe the upper and lower clothing, and\nshoe information. These descriptions include information\non color, type, and other significant visual features. For\naction information, we describe the overall content of the\naction and, if there are any interactive objects, we also de-\nscribe the type and other significant visual features of those\nobjects.\nA.3. Hardware Construction\nThe main structure of DNA-Rendering\u2019s capture system\nis a dome with a radius of three meters. The camera ar-\nray built upon the doom consists of multiple types of cam-\neras \u2013 ultra-high-resolution 12MP cameras, 5MP industrial\nhttps://en.wikipedia.org/wiki/Wikipedia:Contents/Human activities\n<10\n10~20\n20~30\n30~40\n40~50\n50~60\n60~70\n>70\n0%\n5%\n10%\n15%\n20%\n<135\n135-140\n140-145\n145-150\n150-155\n155-160\n160-165\n165-170\n170-175\n175-180\n180-185\n185-190\n>190\n0%\n5%\n10%\n15%\n20%\n<30\n30-35\n35-40\n40-45\n45-50\n50-55\n55-60\n60-65\n65-70\n70-75\n75-80\n80-85\n85-90\n90-95\n95-100\n>100\n0%\n5%\n10%\n15%\n20%\nAge\nHeight\nWeight\ny/o\ncm\nkg\nFigure S1: The distribution of actors\u2019 attributes. We record the age, height, and weight of our invited actors. The statistical results\nreflect the wide range of the actors\u2019 personal attributes.\n15\nNormal\nProfessional\nDaily\nSport\nNew Age\nEntertainment\nCareer\nPublic Affair\nCommunication\nEducation\nArt\nAlgriculture\nBussiness\nCulture\n Art \n Algriculture \n Daily \nEntertainment\n Sport \n(a) Action distribution and labels\nDaily\nSpecial\nTop\nDress\nPants\nEthic Clothing\nSpecial Costume\nAncient Wear\n(b) Cloth distribution and labels\nFigure S2: Illustration of the action and clothes label distribution. (a). The distribution of action categories and sub-categories is\nvisualized by a sunburst chart in the middle which is surrounded by the word cloud of normal and professional action labels. (b). The\ndistribution of clothes categories and labels are visualized in the same form with (a).\ncameras, and Azure Kinect cameras.\nThe lighting sys-\ntem provides natural lighting conditions. All cameras are\ntriggered and synchronized by hardware, and synchronized\nmulti-view data are transferred and recorded through our\ndata streaming system.\nBasic Information\nID: 0121_02\nHeight:  166cm  \nGender: Female\nWeight: 48kg\nEthnicity: Asian\nAge:  52\nClothes Labels\nTop: A yellow Shaoxing Opera   \nXiaosheng slanted collar costume\nPants: -\nShoes: Theatrical shoes\nAccessory: A black Chinese opera hat\nAction Labels\nDescription:  Sigh in the garden  in \nthe form of Shaoxing Opera\nInteractive Objects:  A folding fan\nFigure S3: An example of our meta attribute system. We record\nthe actors\u2019 basic information, costumes, and actions.\nCamera System. DNA-Rendering has 68 cameras, includ-\ning 12 ultra-high resolution cameras with 12MP resolution\n(short for 4096 \u00d7 3000 resolution), 48 industrial cameras at\n5MP resolution (i.e., 2448\u00d72048 resolution), and 8 RGB-D\nKinect cameras with depth resolution of 576 \u00d7 640. Specif-\nically, the 5MP cameras are mounted on three cycles on the\ndome skeleton with 1, 2, and 3 meters in height, each circle\nin height has 16 balanced 5MP cameras with 22.5\u00b0 angle in-\nterval. 12MP cameras are placed uniformly on another two\nintermediate height level circles, 1.5 and 2.5 meters height\nrespectively. 12MP cameras are installed with a 60\u00b0 angle\ninterval and interlaced with 5MP cameras. The Kinect cam-\neras are mounted close to the middle level of 5MP cameras,\nproviding the best RGB texture references for depth maps.\nSuch construction of the camera array achieves dense cover-\nage of the human body at multiple heights and angles. 5MP\ncameras and 12MP cameras are equipped with lenses of 8\nmm and 6 mm respectively to achieve the best trade-off be-\ntween full body proportion-in-view and size of capture vol-\nume. Note that, we use data captured from 12MP and 5MP\ncameras to construct our rendering dataset. The data cap-\ntured from depth cameras comprise the auxiliary data which\nprovide coarse geometry of human. Noted that we abandon\nthe Kinect RGB cameras during the entire process, due to\nthe bad color consistency.\nLighting System. Our lighting system consists of 16 flat\nlight sources with a color temperature of 5600K \u00b1 300K\nand an illuminance of 4500 Lux/m. The lighting scale of\neach light source is 700 \u00d7 500 mm. There are eight flat\nlights on the ground installed with a 45\u00b0 tilt towards actors\nin the middle to provide the best lighting on actors. There\nare extra eight flat lights hung on the roof to strengthen the\nlighting of upper body parts, especially for human heads.\nThese uniformly distributed flat lights irradiate the whole\nscene with strong, natural, and balanced illumination.\nData Streaming. To collect, transfer and store the multi-\nview camera data, we construct a data streaming system that\nconsists of two pieces of equipment for data synchroniza-\ntion \u2013 a 10 Giga-byte network, and a high data through-\nput workstation. The camera system is synchronized by\nKinect\u2019s trigger signal. First, eight Kinects are configured\nin a daisy chain and the out-trigger signal is converted to the\nTTL signal, and the other 60 cameras are triggered by syn-\nchronization equipment. The 5MP cameras are connected\n16\nto six workstations via USB-3.0 ports and four-channel\nUSB cards with PCI-E interfaces. The 12MP cameras are\nconnected to the other three workstations via 10 GigE net-\nworks, capture cards, and PCI-E interfaces. To reduce ac-\ntive light interference of Kinect depth cameras, we adopt a\n160 \u00b5s time delay for each slave device on the chain. The\nmaximum synchronization error of Kinect is 1.12 ms in the-\nory. The maximum synchronization error among all indus-\ntrial cameras is less than 2 ms, we measure this error by\nutilizing the image of high-speed flashing LED timer arrays\nand computing the displayed time differences.\nA.4. Data Collection Protocol\nWe discuss the detailed data collection protocol from five\naspects, i.e., data content, system check, core data collec-\ntion, auxiliary data collection, and post-processing.\nData Content. During everyday data collection, we gather\na comprehensive set of data sources, including action data,\nbackground data, actors\u2019 A-pose data for each outfit, ex-\ntrinsic calibration data, and the record of performance at-\ntributes. We collect intrinsic data and color calibration data\nonly when we apply any modification to the system.\nSystem Check. We conduct a daily system check before\nformal data collection. The process focuses on the verifica-\ntion of camera parameters and synchronization. Concretely,\nwe will check 1) if the camera parameters remain the same\nwith recorded optimal values (e.g., white balance, gamma,\nfocal length, the valid field of view (FOV), etc.). Checking\nthese factors ensures capturing under excellent image qual-\nity and valid capture volume. 2) We monitor the network\u2019s\ncondition and check the synchronization via a system probe\nusing high-speed flashing LEDs. 3) Finally, we collect ex-\ntrinsic camera calibration data via a standard data collection\nprocess that records the checkerboard rotating as described\nin Sec. 3.3 in the main paper.\nCore Data Collection. We invite 4-6 actors per day to per-\nform actions in our studio in different appointed time slots.\nOnce the actors arrive, we will briefly introduce the collec-\ntion procedure and ask them to sign the authorization agree-\nments first. If the actors agree, they are asked to prepare\ntheir outfits, makeup, and actions. Meanwhile, we record\nthe basic information for each actor, including the height,\nweight, age, ethnicity, and other appearance attributes like\nthe type, color, and material of his/her self-prepared outfit.\nAfter the preparation, we ask each actor to perform several\nactions in his/her self-prepared outfit. Specifically, a normal\nactor will pick at least three actions from our pre-defined\ndaily activities and perform them in a free-style manner. A\nprofessional actor will wear a special outfit and perform at\nleast six unique sets of footage that fit with the professional\nskill or costume. Then, we ask each actor to change his/her\noutfits with another two sets that are randomly coordinated\nfrom our clothing library. For each new outfit, the actor will\nperform another three different normal actions. To ensure\nthe performed motion is rational and authentic enough, we\nwill ask each performer to rehearse outside the studio be-\nfore the formal shooting. After our staff confirms that the\naction is performed correctly, the actor will perform in the\nstudio again for the formal data collection.\nAuxiliary Data Collection. Aside from the core perfor-\nmance data collection, we also record auxiliary data, includ-\ning the blank background data for the matting process, and\nA-pose data as a record for the canonical actor model be-\nfore each round of new outfit recording. To record A-pose\ndata, we require 1) the actor\u2019s hands tilt 45\u00b0downward the\nlegs with clear distance; 2) the hands should slightly open\nwithout clenched fingers or put them together; 3) the far-\ncical expression should keep expressionless with the eyes\nopen and looking straight ahead.\nPost-processing. After the action is completed, the center\nworkstation generates fast multi-view preview videos for\nall cameras, and we check whether the performance con-\ntent or the filming on each camera view meets the require-\nment. Actors are asked to re-play the performance if the\nrecorded data is invalid. After collecting all qualified data,\nwe post-process the data in per-day shooting volume Image\nsequences in RAW format will be converted to the lossless\nBMP format, and then compressed into a video with a low\nconstant rate factor with the x264 library. The processed\ndata are then uploaded to the cloud server for subsequent\ndataset processing.\nA.5. Limitations on Data Collection\nData Content. To achieve high-fidelity data collection, we\nset the lighting with invariant and uniform illumination and\nset the acquisition frame rate to 15 frames per second. We\nalso constrain the field of view of the cameras, to ensure\neach one can capture the full-body movements of a single\nactor (including the interacted object if there is one) while\nmaintaining the FOV as max as possible. This allows us to\ncapture details such as facial makeup and clothing textures.\nIn future work, we would update our hardware systems and\nupgrade our capture processes to accommodate different\nlighting conditions, multiple FOV ranges for multi-person\nscenes, high-speed capture of subtle movements, and multi-\nsensory (e.g., auditory, and tactile data) collection.\nFailure Cases. During the data collection process, various\nfactors can lead to failure, such as large movements that ex-\nceed the field of view of the multi-camera system, or loss of\nframes due to large volume data transmission fluctuations.\nFollowing the standardized capture process, our operators\nwill manually inspect the completeness and effectiveness of\nall camera data and actor movements after each capture cy-\ncle. If any issues are found, the hardware will be instantly\nchecked and the data will be re-captured. Most failure cases\nare identified and promptly resolved at this stage.\n17\nB. Data Annotation Details\nB.1. Camera Intrinsic Calibration\nAs this project targets capturing high-fidelity whole-body\ndata, we adopt a long lens that enlarges the human propor-\ntion in the camera view. This setup requires a high-quality\nestimation of camera distortion since some subjects\u2019 body\nparts might appear on the image boundary region. Thus, we\nuse a 3\u00d73 Soduku data collection protocol for intrinsic cal-\nibration, as illustrated in Fig. S4. Specifically, to maximize\nthe checkboards\u2019 coverage across the whole image space,\nwe separate the image into a 3 \u00d7 3 Soduku and capture the\nimages of the checkerboards\u2019 movement in grids. For each\ngrid, we rotate the checkerboard in pitch, row, and yaw di-\nrection to enlarge the angle of the boards.\n(a)\n(c)\n(b)\nFigure S4: Intrinsic calibration. To ensure better distortion co-\nefficient estimation, (a) we separate the camera view into 3 \u00d7 3\nSoduku and capture the images of a checkerboard (about 1/4 size\nof the one used in estimating extrinsic parameters) in every So-\nduku grid. (b) For each grid, we rotate the checkerboard at pitch,\nrow, and yaw angles. This calibration step forces the checker to\nappear in every corner of the camera view. (c) Zoom-in for small-\nsize checkboard for intrinsic calibration.\nB.2. Camera Color Calibration\nTo ensure color consistency across multiple cameras, we\ninject a color calibration process into our data collection.\nA standard color board could be used as the criterion for\nf color calibration, and the fixed lighting condition in the\ndome could be treated as a standard condition during cali-\nbration. Specifically, the calibration lies in two aspects: 1)\nHardware parameter adjustment. We make a rough adjust-\nment on the hardware parameters to make the white balance\nand color balance of each camera as consistent as possible\nby human eyes; 2) Fine adjustment. Under a standard light\nsource, we make the standard color board face straightfor-\nwardly to the camera to be calibrated at a constant distance,\nand a single image under this setting is collected; the cor-\nner detection algorithm is used to automatically identify the\nposition of the color board in the image, and the color sam-\npling is performed with the center radius p = 10 pixels of\neach color square. The average color value is taken as the\ncolor sampling value. We carry out the polynomial projec-\ntion of the color sampling value to the standard value via\nleast squares. Note that, we calibrate in RGB form and take\nn = 2 to prevent overfitting. The overall procedure and\nillustrated results are presented in Fig. S5.\nOriginal Images\nHardware Setting \nAdjustment\nWhite Balance\nGamma\n\u2026\nResult Images\nFine Adjustment\n\ud835\udc45[\ud835\udc5f,\ud835\udc54,\ud835\udc4f] = \u0dcd\n\ud835\udc5d\n\ud835\udf06\ud835\udc5f[\ud835\udc5f,\ud835\udc54,\ud835\udc4f]\n\ud835\udc5d\n5MP\n12MP\n5MP\n12MP\nRed\nGreen\nBlue\n0\n50\n100\n150\n200\n250\nGT\n5MP Calib\n12MP Calib\n5MP Raw\n12MP Raw\nFigure S5: Color calibration pipeline and calibrated color re-\nsponse.The color-calibrated images are listed on the right of the\nflow chart (at the top of the figure). The color responses of two\ncalibrated cameras (Camera 25 which is a 5MP camera, and Cam-\nera 51 which is a 12MP camera) compared with groundtruth color\nvalue of the color checkerboard are plotted below the flow chart,\nand smoothed spline curve is used. We show the \u2018Raw responses\u2019\nafter hardware setting adjustment for reference. With the help of\nthe color correction process, the average RGB value consistency\nbetween these two cameras \u2206E00 [46] is improved from 37.79 to\n4.15.\nB.3. Keypoints\nWe highlight that having a large number of camera views\nallows us to rectify the occasional failures of single-view\n2D keypoint detection.\nFor the more natural and stable\n3D keypoints, we adopt the following optimization and\npost-processing strategies: 1) Keypoint selection. We dy-\nnamically select views for each keypoint in the data se-\nquence, which with the most confident score to the key-\npoint while ensuring the keypoint can be triangulated. 2)\nBone length constraint. The bone length is constrained with\na fixed length. We use the median bone length after ini-\ntial triangulation as the target in the optimization. Only the\nlengths of the main limbs are considered in this step. 3) Out-\nlier removal. As a post-processing pipeline, filter modules\nare designed based on human priors, including a 3D bound-\ning box filter, a movement filter, and a relative position fil-\nter. 3D keypoints outliers, which are too far from the body\ntrunk, move too fast between frames, or lead to an incon-\nsistent relative position between frames are removed. An\ninterpolation is applied to recover the missing keypoints.\nSuch a post-processing scheme can assure reliable and con-\nsistent face and hand keypoints, even with large-scale oc-\nclusions. As shown in Fig. S6, these optimization and post-\n18\n54\n59\n48\n47\n02\n49\n05\n53\n08\n44\n55\n01\n58\n41\n09\n04\n46\n43\n00\n42\n45\n03\n07\n39\n06\n40\n11\n50\n10\n36\n38\n37\n52\n12\n56\n14\n33\n13\n35\n57\n34\n15\n17\n51\n32\n16\n30\n31\n20\n26\n18\n23\n29\n19\n28\n21\n27\n22\n25\n24\nCamera ID\n0\n10\n20\n30\n40\n50\n60\n70\n80\nReprojection Error (Pixels)\nOptimized\nUnoptimized\nFigure S6: Evaluation of keypoint quality from every camera view. We compute the mean reprojection error of 3D keypoints with\n2D detection results. Optimization effectively reduces the error to below 30 pixels. Note that camera IDs 0-47 are 5MP cameras, and\ncamera IDs 48-59 are 12MP cameras, high-lighted with red x-ticks.\nprocessing strategies effectively reduce reprojection error\ncompared to triangulation with all available 2D keypoints.\nB.4. Parametric Model\nIn our automatic parametric model annotation pipeline,\nbody shape \u03b2 \u2208 Rn\u00d710 (or \u03b2 \u2208 Rn\u00d711 for children\n[24, 53]) is first estimated based on the bone length calcu-\nlated from 3D keypoints with the static and less challeng-\ning A-pose sequence. We use the estimated body shape pa-\nrameters as initial values and optimize the full parametric\nmodel parameters including pose parameters (body pose,\nhand pose, and global orientation) \u03b8 \u2208 Rn\u00d7156, and trans-\nlation parameters t \u2208 Rn\u00d73 (n is the number of frames)\nvia a modified SMPLify-X for other sequences with dy-\nnamic poses. The main energy terms in the optimization\nare keypoint energy EP, full-body joint angle prior energy\nEa, bone length energy EB, and body shape prior energy\nE\u03b2 [54, 45, 7]. The main modification of SMPLify-X in\nour annotation pipeline is the decoupling body shape opti-\nmization and pose optimization, which we empirically find\nto produce more stable results.\nConcretely, we employ bone length energy EB and body\nshape prior energy E\u03b2 to fine-tune body shape parameters\nfor each sequence of a subject, with the same shape initial-\nization from the A-pose static sequence. Body shape values\nare kept consistent throughout all the frames in a sequence.\nEshape(\u03b8, \u03b2, t) = \u03bb1EB + \u03bb2E\u03b2\n(S1)\nWe then leverage keypoint energy EP and full-body\njoint angle prior energy Ea for pose optimization with body\nshape fixed.\nEpose(\u03b8, \u03b2, t) = \u03bb3EP + \u03bb4Ea\n(S2)\nAs shown in Fig. S7, we evaluate the fitting error\nbetween 3D keypoints and corresponding regressed SM-\nPLX joints.\nBody-only keypoints, hand-only keypoints,\nand all keypoints are evaluated separately. With our multi-\nview capture system and annotation pipeline, the MPJPE of\nbody-only keypoints is on par with the optical motion cap-\nture system in Human3.6M [26, 44], MPJPE of hand-only\nkeypoints is 15.87mm.\n10\n15\n20\n25\n30\n35\n40\nFitting MPJPE (mm)\nAll\nHand\nBody\nFigure S7: Evaluation of parametric model registration qual-\nity. We evaluate body-only keypoints, hand-only keypoints, and\nall keypoints separately.\nThe orange line indicates the median\nvalue, the box indicates the lower to the higher quartile, and the\nwhiskers indicate the range of data.\nB.5. Comparison to Other SMPLX Fitting Methods\nBaselines.\nTo analyze the effectiveness of the proposed\nSMPLX fitting pipeline, we evaluate the accuracy of SM-\nPLX fitting and compare it with three publicly available\npipelines, i.e., the baseline MultiviewSMPLifyX [89, 54],\nEasyMoCap [15, 65] used in ZJU-MoCap [56, 65] dataset,\nand BodyFitting used in GeneBody [13] dataset. Specif-\nically, MultiviewSMPLify [89] and BodyFitting [13] di-\nrectly optimize the error of reprojected 3D SMPLX key-\npoints to 2D detections. Such a naive strategy is straightfor-\nward but lacks outlier robustness (might stuck in absolutely\nwrong detections or detection flip between left and right),\nand it is also computationally expensive. On the contrary,\nboth EasyMoCap and the proposed pipeline adopt another\nstrategy that separates the SMPLify process by a triangu-\nlation process. This strategy optimizes 3D keypoints from\n2D detection and then fits SMPLX from directly on opti-\nmized 3D keypoints. As robust designs could be adapted\nduring the triangulation process to eject outliers caused by\nflipping or occlusion, such a two-step strategy is faster and\nmore robust to outliers. Whereas, one drawback is that the\nfinal SMPLX totally rely on the results of triangulation in\nthe first stage by hand-crafted optimization and filtering.\n19\nMethods\n2D Reprojection Error (pixel)\n3D MPJPE (mm)\nRun Time (s)\nBody\nHand\nFace\nOverall\nBody\nHand\nFace\nOverall\nMultiviewSMPLifyX [89]\n42.27\n33.36\n23.91\n28.77\n55.46\n23.25\n22.24\n27.54\n81.33\nBodyFitting (GeneBody) [13]\n45.68\n33.43\n34.17\n35.67\n42.37\n32.19\n30.67\n32.89\n29.50\nEasyMoCap (ZJU-MoCap) [15]\n32.71\n33.75\n32.72\n33.64\n36.04\n25.37\n38.10\n33.96\n0.69\nOurs\n29.63\n31.41\n19.08\n24.08\n30.20\n15.87\n16.46\n17.52\n3.23\nTable S1: Comparison among multi-view SMPLX fitting methods. Cell color\nindicates the best, second best, and third best\nperformance, respectively. Runtime in seconds indicates the average time required for the fitting process for one multi-view frame.\nCompared to EasyMoCap, incorporate a more sophisticated\ndesigned 2D keypoints postprocessing phase, where move-\nment filtering and relative position filtering are used when\nthe given 2D keypoints are not accurate.\nSettings. For fairness, we use the same 2D keypoints con-\nsisting of human-inspected body labels and hands and faces\nauto-detection results. We also force all SMPLX models\nto have 10 facial expression coefficiency and 45 hand PCA\ncomponents. We run the SMPLX fitting on our benchmark\ntest data with their default SMPLX setting, namely with de-\nfault penalty energies, their coefficient, and other settings\nexcept for the aforementioned modifications. We quantita-\ntively evaluate the MPJPE of 3D keypoints and the repro-\njected 2D error across all views.\nResults. The quantitive results are listed in Tab. S1, we\nalso separately evaluate the accuracy on body, hand, face,\nand whole body.\n3D MPJPE is computed by regressed\nSMPLX 3D keypoints to human-inspected 3D keypoints.\n2D reprojection error is compared by reprojected SM-\nPLX 3D keypoint to input 2D keypoints. The runtime per-\nformances are also recorded, indicating the average fitting\ntime usage (excluding other time namely, data IO, etc.)\nfor one multi-view frame. Our proposed pipeline outper-\nforms other fitting methods in all categories in terms of\nboth 2D and 3D metrics, and has a more acceptable run-\ntime requirement than EasyMocap [15]. Moreover, Mul-\ntiviewSMPLify [89, 54] achieves the second-best perfor-\nmance, while its time consumption is exploded by an order\nof magnitude. In a nutshell, our pipeline ensures the best\ntrade-off between performance and efficiency.\nB.6. Matting and Segmentation Refine\nAs described in the main text, despite the state-of-the-art\nbackground matting method [39] achieving impressive mat-\nting performance in the majority of our data, there are still\nseveral corner cases that fail to extract the foreground cor-\nrectly, e.g., noisy backgrounds, broken bodies, and missing\nbodies and objects. We demonstrate these most common\ncorner cases in Fig. S8. To further improve the matting\nquality, we adopt the traditional computer vision algorithm-\nGraphCut [22] to refine the predicted masks, and we find\nsuch a classical method plays a good fit to the CNN-based\nmethod which generates good results on these failure cases.\nIn order to quantify the improvement, we introduce a\nmanual inspection process to grade the results generated by\nCNN-based method [39] only and by a subsequent refine-\nment procedure. Noted that due to the large scale of data,\ngenerating masks with manual labeling is impractical. More\nspecifically, we ask three annotators to conduct such grad-\ning surveys on 500 random multiview sequences. We report\nthe error rates in terms of the type of corner cases in the bar\nchart in Fig. S8. From the human grading probe, we can\nconclude that with our proposed hybrid strategy, the error\nrates in all category decrease by a large margin compared\nwith [39] only, with the overall error rate reduced from\n11% to 2%.\nB.7. Quality Control of Auto Annotation Results\nTo ensure the quality of annotation data, we con-\nduct manual quality checks on the auto-annotated results.\nSpecifically, we perform a human-in-the-loop quality eval-\nuation for the SMPLX and matting results generated by the\nannotation pipeline.\nFor SMPLX quality control, we overlay each SMPLX re-\nsult on the original action data to create a multi-view video\nand manually verify the quality with the labeling task that\nrequires our human annotator to grade the SMPLX qual-\nity. We subdivide the process into three stages. 1) Binary\nfiltering. If the SMPLX human body completely overlaps\nwith the human body in the image or is within the natural\nshape range of the human body throughout the entire video,\nit is considered as a qualified SMPLX annotation; other-\nwise, if there is severe misalignment or distortions on the\nmain body, it is considered as an unqualified one. 2) Quality\nGrading. For qualified data cases, we further evaluate their\nsubdivision quality, dividing them into five scores based on\nthe unnaturalness of fingers or faces, the alignment of the\nhead and shoulders with the image, etc. 3) Keypoints re-\nannotation. For unqualified cases, we ask the annotators to\nre-annotate the main skeleton in views with large errors by\nauto-annotators. The new annotation results are used to re-\nrun the SMPLX results. We repeat the whole process until\nwe achieve valid SMPLX models in all cases.\nFor Matting quality control, we manually evaluate the\nquality of the annotated video after matting by grading each\nvideo\u2019s quality. Quality is divided into three levels: A-level,\nwhere the entire human body is fully displayed without oc-\nclusion, and any interactive objects are fully shown, with\nno excess areas; B-level, where a small part of the human\nbody or object is missing, or there are a few extra cutouts,\n20\nRaw \nImage\nHSV\nFilter\nBackground \nMatting\nRefined \nMatting\nMasked \nImage\nNoisy Bg\nBroken Area\nMissing Area\n0%\n5%\n10%\n15%\n20%\nw/o refinement\nw/ refinement\nFigure S8: Examples and statistics on matting refinement. In\nthe upper image, we show three kinds of common challenging\ncases and the comparisons among color filtering only, background\nmatting only, and our optimized solution (Refined Matting). From\ntop to bottom cases, the problems (before optimization) are noisy\nbackgrounds, broken body areas, and missing areas like body parts\nand incomplete objects; In the bottom figure, we show the error\nrates from the sampling survey of the three categories.\nwith the erroneous pixel area not exceeding one-third of the\nhuman body area; and C-level, where there are serious prob-\nlems and the erroneous pixel area exceeds one-third of the\neffective human body area. We treat A-level and B-level as\nacceptable mask annotations, while C-level as failure anno-\ntations. Noted that the cases in training and testing data split\nin our benchmark were manually selected to ensure high-\nquality annotations. For the sake of rigor, we will release\nthe mask rating as confidence for mask annotation.\nC. Benchmark Details\nC.1. Methods Overview and Modifications\nIn this subsection, we review the state-of-the-art methods\nbenchmarked in this paper, and describe the major modifi-\ncation we made to the default implementation for adapting\nto the proposed dataset.\nC.1.1\nStatic Methods\nFor static methods, we target to anchor the performances\nof novel view rendering on static test frames, which could\nbe used as the baseline reference for dynamic methods on\ncertain frozen times.\nInstant-NGP [51] as an alter of NeRF [50], which utilizes\nthe multi-resolution hash embedding and smaller network\nto accelerate the training and evaluation cost without loss\nof quality. Given the original implementation of Instant-\nNGP is under the underlying assumption of a moving sin-\ngle camera input or cameras sharing the intrinsic parame-\nter across all camera positions, we modify it to suit multi-\ncamera data with different intrinsic parameters.\nNeuS [74] is a hybrid representation that combines neu-\nral radiance field with neural SDF, which produces better\n3D reconstruction ability than NeRF-based methods [23,\n50, 56] on existing datasets, while the rendered images of\nNeuS are typically not as sharp as NeRF-based methods.\nWhen adapting NeuS [74] on the proposed dataset, no spe-\ncial modification is required.\nC.1.2\nDynamic Methods\nTo construct the novel view synthesis and novel pose anima-\ntion benchmark, we select the most recent state-of-the-art\ndynamic neural human rendering methods which can learn\na neural body avatar from video sequences.\nNeuralVolumes [42] formulates a category-agnostic dy-\nnamic scene by a canonical voxel-grid decoder, and mod-\nels the per-frame deformation as a mixture of affine warps\nthat are parameterized by an auto-encoder with image input.\nDue to this property, we feed the network with 4 balanced\nviews of images from the training views. We also center\nand scale the camera system by 0.3 to fit the voxel-grid sys-\ntem. During testing on novel pose, novel pose images of the\n4 view are input to the auto-encoder.\nA-NeRF [66] learns a human NeRF by conditioning the\nfield with coordinates in each bone\u2019s local system. Note that\nits default setting only trains the network in the foreground,\nwhich usually leads to artifacts on the floor, we improve this\nby forcing pixel sampling on non-foreground space which\nhelps to reduce the artifacts.\nNeuralBody [56] conditions a dynamic NeRF by time\ncodes as well as structural latent features by sparsely con-\nvolving parametric model\u2019s vertices in 3D. To run Neural-\n21\nBody [56], we transform our standard definition of the para-\nmetric model to EasyMoCap [15] style, and we train the\nnetwork using 42 dense views. Note that during novel pose\nestimation, we fed the network with novel pose SMPLs and\nlinearly extrapolate the time step.\nAnimatableNeRF [55] introduces neural blend weights\nwith 3D human skeletons to generate observation-canonical\ncorrespondences in dynamic human NeRF. We do the same\ntransformation like NeuralBody [56].\nHumanNeRF [77] learns a dynamic neural human model\nfrom monocular video. It decouples the motion field by a\ncorrected skeleton movement and non-rigid motion. Differ-\nent from its original setting, we train the model with dense\nviews by stacking multiview video sequences. It is impor-\ntant to point out that HumanNeRF [77] models may col-\nlapse on certain data sequences producing meaningless im-\nages like the left bottom case in Fig. 6. Such a phenomenon\nis consistent even with multiple trials of random initializa-\ntion. Considering to deliver a more straightforward metric\nmeaning in the benchmark, the report numbers of Human-\nNeRF in Tab. S2 and Tab. 2 only include the valid models.\nC.1.3\nGeneralizable Methods\nFor novel identity generalization,\nwe evaluate three\ncategory-agnostic methods [81, 75, 38] and two methods\nwith human structure priors [34, 49].\nPixelNeRF [81] is one of the first generalizable NeRFs that\ngeneralize novel objects\u2019 color and opacity by pixel-aligned\nfeature-conditioned NeRF. We train it on our dataset with\n4 selected views and fuse the multiview image feature with\naverage pooling.\nIBRNet [75] predicts the radiance color of novel objects by\nblending observed color from source views, and inference\nthe opacity from multiview feature fusion.\nVisionNeRF [38] upgrades PixelNeRF\u2019s [81] image en-\ncoder with a global transformer [71] and fuse the multi-level\nfeatures with 2D CNN features. Like PixelNeRF [81], we\nfuse the multiview feature with average pooling.\nNeuralHumanPerformer [34] combines key components\nof PixelNeRF [81] and NeuralBody [56], and fuse them\nwith multiview transformer and predict the radiance of hu-\nman body. Noted there is a slight contradiction between\nthe technical paper and the released implementation on the\nwindow size of the temporal transformer. We follow the\nopen-sourced implementation and set the window size to\n1 to avoid memory explosion which means the temporal\ntransformer is a dummy module.\nKeypointNeRF [49] use IBRNet [75] as the backbone, and\ntailors 3D keypoints as human prior into the framework. It\nconditions the radiance field with relative depth to every\n3D keypoint in each source camera coordinate. We train\nthe network with 24 SMPL main skeleton keypoints. Noted\nthat different from other generalizable methods which allow\narbitrary resolution rendering, KeypointNeRF [49] is suited\nto render square-sized images with 2n width and height. We\nrender out a minimum squared image that can cover the de-\nsired resolution then crop out the valid part.\nC.2. Benchmark Details\nAs a supplement to the benchmark part in the main paper,\nwe describe the detailed benchmark settings and additional\nanalysis of results.\nC.2.1\nNovel View Synthesis\nDetailed Settings. As we described the task in Sec. 4.2\nand reviewed the methods in Sec. C.1, we evaluate the dy-\nnamic methods\u2019 novel view synthesis ability on the bench-\nmark test set, which consists of 13 splits with 39 perfor-\nmance sequences. During training, we train models on each\nsequence separately, with the 42 multiview (training views)\nimage sequences of the first 80% frames. Evaluations are\nperformed on the same seen human poses but with every 45\nframe skip and only calculated on 18 unseen camera poses.\nFor static methods, we train separate models on multi-view\nimages of each single frame. To evaluate the high-fidelity\nrendering of these benchmark methods, we train and test the\nmodels on half of the origin resolution, namely 1024\u00d71224\nand 1500 \u00d7 2048 for 5MP and 12MP images respectively.\nDetailed Results. As a supplement to the result analysis in\nthe main text, we present more detailed results and analy-\nsis in this subsection. Detailed quantitative results across\nour testing splits are listed in Tab. S2, which correspond to\nthe bubble charts in Fig. 4. We also illustrate the qualita-\ntive results in Fig. S9. From the ranking in Tab. S2, we\ncan observe that A-NeRF [66], NeuralBody [56] and Hu-\nmanNeRF [77] achieve the best numbers in most of the\ntest splits in terms of PSNR, SSIM and LPIPS [86]. As\nthe module designs of these methods might play vital roles\nin such distinct results, we further analyze the phenomenon\nby unfolding their conceptual differences as follows: Neu-\nralVolumes [42] adopts a VAE [33]-style neural rendering\nframework that encodes and decodes both the affinity trans-\nformation field and rendering volume from sparse view ref-\nerences. Such a paradigm absorbs the strength of VAE that\ncompresses input multi-view features into one compact la-\ntent representation space, which follows the Gaussian as-\nsumption. Thus it could generalize well on novel views\n(achieving top-three performance in PSNR) with acceptable\nrendering quality. Whereas, such a framework also inherits\nthe smooth problem of VAE that leads to not sharp enough\nqualitative results. A-NeRF [66] is a conditioned NeRF that\nutilizes local joint ordinate information of query points. It\nsamples a small box center at a random point in the fore-\nground and adds a proportion of background points to reg-\n22\nDeformation\nMotion\nTexture\nInteraction\nEasy            Medium       Hard\nEasy           Medium            Hard\nEasy           Medium         Hard\nNone           Easy              Medium         Hard\nGT\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure S9: Novel view synthesis results on each data split. From top to bottom, we illustrate the rendering results generated by (a-e).\nNeuralVolumes [42], A-NeRF [66], NeuralBody [56], AnimatableNeRF [55], HumanNeRF [77]. Please zoom in for better visualization.\n23\nularize the empty space, using only mean square error loss\n(MSE). This strategy enforces the network to encode the\ndynamic NeRF with local bone coordinates, which is effi-\ncient in the human foreground region. The overall novel\nview synthesis ability of A-NeRF [66] is appealing, espe-\ncially in PSNR. However, due to the sparsity characteris-\ntic of skeleton representation, A-NeRF tends to generate\ndilated artifacts (see Motion-Medium and Motion-Hard in\nFig. S9), more obvious with novel pose in off-body parts of\nInteraction-Hard case in Fig. S10. NeuralBody [56] and\nAnimatableNeRF [55] first compute a 3D bounding box\nfrom SMPL, then train/infer on the reprojected 3D box re-\ngion and fill the outer region with the background color.\nThus, their SSIM scores are typically greater than other\nmethods that infer the whole image. Whereas, the bound-\ning box only helps the network consider the main body\nand ignore the object, which leads to both methods can\nnot reconstruct large interacting objects (as illustrated in\nthe sword part of Interaction-Medium case in Fig. S9) Hu-\nmanNeRF [77] combines the strength of previous methods\u2019\ndesign, it samples squared boxes on reprojected 3D bound-\ning box, and trained model with a perceptual [29] loss and\nMSE loss.\nThis results in best LPIPS performance, as\nwell as the best visual performance with sharp texture in\nqualitative results. However, as HumanNeRF [77] designs\na human motion prior that is Gaussian distribution along\nbody parts or bones. This prior may lead to training fail-\nure on loose clothing and interactive objects, as illustrated\nin the Deformation-Hard and Interaction-Medium cases in\nFig. S10.\nC.2.2\nNovel Pose Animation\nDetailed Settings We conduct novel pose experiments on\ndynamic methods on the same models in novel view syn-\nthesis. Specifically, by training on the first 80% frames of\neach case, we test the animatable model with input of the\npose sequences extracted from the last 20% frames with a\n15-frame skip. Depending on the pose-condition scheme of\ndifferent methods, the test input can be divided into two cat-\negories, the SMPL parameters and the image features. We\nuse the same testing view and rendering resolution as the\nnovel view synthesis experiment.\nDetailed Results. We present the detailed novel pose ani-\nmation results on 13 testing splits in Tab. 2 in the main pa-\nper, and show the qualitative samples of results in Fig. S10\nin this subsection. Similar to novel view synthesis task, A-\nNeRF [66] achieves the best PSNR performance, Neural-\nBody [56] has the best SSIM score, and HumanNeRF [77]\ngets the best LPIPS [86]. Differently, NeuralVolumes [42]\u2019s\nperformance decrease by a large margin, especially in Mo-\ntion splits. One of the underlying reasons is that the affinity\nfield learning in the NeuralVolumes [42] only relies on the\nlatent code that is learned from multiview images and reg-\nularized by KL-divergence. Such a methodology is tied in\na global warping manner, which might be relatively less af-\nfected by factors like global deformation in distance within\na short movement change, but is vulnerable to unseen local\nmotion (e.g., wrong head pose of Interaction-Medium case\nin Fig. S10, and strained border of actor\u2019s shirt in Texture-\nEasy case in Fig. S10 ). Moreover, the design cannot pre-\nserve global scale in unseen poses, due to the wrong predic-\ntion of global affine transform sometimes (e.g., the zoom\nscale of the actor in Texture-Easy case in Fig. S10)). The\nmethods [66, 56, 55, 77] with the explicit human pose in-\nformation as input can typically generate reasonable anima-\ntion results in terms of the local first motion, as shown in\nFig. S10. Whereas, we draw the other major conclusion\nthat current methods fail to model Deformation and Inter-\naction properly. The typical examples shown in Fig. S10\nare the loose cloth case of Deformation-Hard, and the inter-\nactive objects in Interaction-Medium and Interaction-Hard\ncases. How to properly model non-rig or out-of-body mo-\ntions while preserving the advantages from explicit body\nSplits\nPSNR\u2191\nSSIM\u2191\nLPIPS*\u2193\nNGP\nNS\nNV\nAN\nNB\nAnN\nHN\nNGP\nNS\nNV\nAN\nNB\nAnN\nHN\nNGP\nNS\nNV\nAN\nNB\nAnN\nHN\nMotion-Simple\n30.97\n27.49\n27.85\n29.15\n27.84\n25.89\n25.49\n0.979\n0.973\n0.966\n0.974\n0.978\n0.974\n0.955\n31.52\n44.18\n57.74\n52.75\n53.32\n56.10\n62.08\nMotion-Medium\n31.40\n30.04\n28.16\n29.07\n27.47\n24.93\n24.80\n0.980\n0.980\n0.970\n0.975\n0.981\n0.971\n0.966\n25.12\n31.33\n50.03\n45.28\n48.12\n56.43\n33.66\nMotion-Hard\n29.05\n28.49\n26.10\n27.55\n25.16\n24.54\n22.93\n0.972\n0.976\n0.959\n0.967\n0.976\n0.976\n0.964\n41.35\n40.13\n77.54\n69.75\n71.25\n63.35\n53.42\nDeformation-Simple\n31.63\n28.01\n28.09\n29.63\n28.18\n27.42\n28.30\n0.981\n0.972\n0.968\n0.975\n0.976\n0.975\n0.974\n29.02\n42.62\n48.17\n41.68\n48.18\n45.44\n23.70\nDeformation-Medium\n30.01\n29.65\n29.77\n30.52\n29.22\n26.29\n26.60\n0.972\n0.975\n0.971\n0.974\n0.979\n0.972\n0.963\n41.14\n37.18\n39.36\n43.51\n46.95\n52.69\n29.12\nDeformation-Hard\n29.79\n30.80\n27.19\n28.11\n24.77\n21.93\n21.48\n0.967\n0.973\n0.954\n0.957\n0.969\n0.958\n0.934\n46.09\n44.83\n70.04\n70.16\n81.14\n84.59\n83.36\nTexture-Simple\n30.53\n31.39\n27.85\n30.45\n29.13\n25.36\n27.39\n0.978\n0.988\n0.974\n0.984\n0.988\n0.979\n0.979\n36.02\n23.53\n58.78\n43.09\n41.53\n53.69\n24.72\nTexture-Medium\n30.85\n31.33\n28.50\n30.53\n29.62\n22.46\n27.40\n0.978\n0.982\n0.968\n0.977\n0.984\n0.959\n0.971\n29.32\n27.04\n47.33\n37.99\n41.46\n75.08\n25.01\nTexture-Hard\n29.16\n28.23\n26.73\n27.36\n25.69\n19.98\n24.78\n0.966\n0.956\n0.942\n0.947\n0.963\n0.946\n0.950\n36.48\n58.55\n79.68\n79.17\n77.36\n94.60\n34.66\nInteraction-No\n31.31\n31.90\n29.05\n28.71\n27.77\n23.82\n26.77\n0.978\n0.985\n0.972\n0.975\n0.984\n0.971\n0.971\n34.00\n23.65\n50.36\n56.07\n49.79\n67.40\n30.00\nInteraction-Simple\n31.55\n32.13\n29.09\n30.12\n29.56\n25.93\n28.59\n0.982\n0.987\n0.976\n0.981\n0.987\n0.977\n0.978\n27.58\n21.79\n45.55\n46.48\n41.69\n57.18\n22.00\nInteraction-Medium\n28.82\n27.15\n25.59\n25.72\n25.65\n22.02\n23.51\n0.967\n0.968\n0.955\n0.956\n0.975\n0.997\n0.953\n43.33\n52.52\n73.05\n85.50\n68.03\n92.90\n54.39\nInteraction-Hard\n30.03\n29.29\n28.09\n28.25\n25.00\n22.92\n23.87\n0.972\n0.977\n0.962\n0.964\n0.972\n0.961\n0.951\n43.64\n39.71\n57.97\n63.42\n71.00\n81.49\n57.38\nOverall\n30.39\n29.68\n27.85\n28.86\n27.31\n24.11\n25.53\n0.975\n0.976\n0.964\n0.970\n0.978\n0.970\n0.962\n35.74\n37.47\n58.12\n56.53\n56.91\n67.76\n41.04\nTable S2: Benchmark results on novel view synthesis task. State-of-the-art methods\u2019 performance of novel test views on seen poses in\neach benchmark split. We abbreviate Instant-NGP [51] as \u2018NGP\u2019, NeuS [74] as \u2018NS\u2019, A-NeRF [66] as \u2018AN\u2019, NeuralVolumes [42] as \u2018NV\u2019,\nNeuralBodyas \u2018NB\u2019 [56], AnimatableNeRF [55] as \u2018AnN\u2019, and HumanNeRF [77] as \u2018HN\u2019. Cell color\nindicate the best, second\nbest, and third best performance in the same split respectively. We exclude the static methods NGP and NS during ranking and separate\nthem with dash lines.\n24\nDeformation\nMotion\nTexture\nInteraction\nEasy            Medium       Hard\nEasy           Medium            Hard\nEasy           Medium         Hard\nNone             Easy            Medium         Hard\nGT\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure S10: Novel pose animation results on each data split. From top to bottom, we illustrate the reposing results generated by\n(a-e): NeuralVolumes [42], A-NeRF [66], NeuralBody [56] AnimatableNeRF [55], and HumanNeRF [77]. Please zoom in for better\nvisualization.\n25\nDeformation\nMotion\nTexture\nInteraction\nHard\nHard\nHard\nHard\nGT\nIBRNet\nPixelNeRF\nVisionNeRF\nNHP\nKeypointNeRF\nFigure S11: Novel ID synthesis results on each data split. Splits with hard difficulty are visualized to illustrate the robustness of\ngeneralizable methods on hard cases.\nrepresentations is worth great pondering.\nC.2.3\nNovel Identity Rendering\nDetailed Settings. For novel identity synthesis, we review\nfive selected state-of-the-art methods [81, 75, 49, 20, 34],\nand described their modification in Sec. C.1.3. The test-\ning set is the same 39 testing sequences in the novel view\nand the novel pose benchmark. The training set consists of\n400 sequences with full coverage of all categories and dif-\nficulties. We select four balanced views as source views.\nThese source view images are cropped and resized into\n512\u00d7512 resolution (same with the official implementation\nin [49, 13, 34]). For category-agnostic methods [81, 75, 38],\nwe only provide segmentation and camera parameters dur-\ning training and testing. For methods with human prior, we\nalso input the fitted SMPLX or 3D keypoints. We train mod-\nels on the full 60 views in the training identity sequences.\nFor inference, we evaluate the unseen identities on the same\n18 test views used in novel view and novel pose tasks, but\non full sequences with frame skip at 45. All methods are\ntrained under the same 8-V100 machine environment, and\nevaluated on a single V100.\nDetailed Results. In the main text, we draw the conclu-\nsion that generalization methods use human prior [49, 34] is\nmore robust than category-agnostic methods [81, 75, 38] ac-\ncording to the results reported in Tab. 3. To further illustrate\nthis conclusion, we compare the qualitative results in Hard\nlevel of each data factor in Fig. S11. Human prior meth-\nods generally render better images with more precise hu-\nman shape and texture compared to category-agnostic meth-\nods, especially in the Texture-Hard and Deformation-Hard\ncases. Moreover, in addition to the influence of concep-\ntual difference between image blending strategies to direct\nradiance prediction in the main text, we also compare the\ngeneralization of image feature extractors between Pixel-\nNeRF [81] and VisionNeRF [38]. VisionNeRF [38] uses\na similar structure of PixelNeRF [81], but mainly incorpo-\nrates the local CNN-based image encoder with a global vi-\nsion transformer. Such a design achieves better visual qual-\nity on average with sharper texture details and higher scores\nin both Fig. S11 and Tab. 3, since the transformer is capa-\nble to learn more global coherence features across source\nviews.\nC.2.4\nBenchmarks with Sparse View Training\nCompare with dense view human synthesis, rendering hu-\nmans from sparse views or even with a monocular im-\nage setting, take a step further in narrowing the domain\ngap between structured in-door data capture and unstruc-\ntured open-world data collection.\nRelaxing the require-\nment on structured data could help towards portable human\navatar generation and may further enable applications like\n26\nbullet-time rendering from online videos. Thus, aside from\nthe main benchmarks, we also evaluate the state-of-the-art\nmethods\u2019 performances under a spare-view training setting.\nSetting. Similar to the dense novel view and novel pose\nexperiments, we retrain separate models for each dynamic\nhuman rendering method. The key difference of the sparse\nview settings from the dense ones is that, each model is\ntrained with only four balanced views, namely Camera 1,\n13, 25, and 37. The other setting details are the same as the\ndense novel view and novel pose benchmarks.\nResults. We list the quantitative results of both sparse novel\nview synthesis and sparse novel pose animation in Tab S3,\nand compare the difference between dense and sparse view\ntraining in Fig S12. For the sparse novel view synthesis\ntask, we can observe that HumanNeRF [77] achieves the\nbest PSNR and LPIPS metric in most of the splits, and\nNeuralBody [56] gets the best SSIM performance. Origi-\nnally sparse-designed methods, NeuralBody [56], Animat-\nableNeRF [55], and HumanNeRF [77] ranked top-3 in all\nevaluation metrics, this phenomenon is totally different\nfrom dense results in Tab. S2. The performance gap be-\ntween sparse view settings and the dense ones can be easily\nobserved from the inflating bubbles in Fig. S12a. In the\nfigure, the bubbles with darker colors refer to the perfor-\nmances under dense view settings, and the ones with lighter\ncolors refer to the results under sparse view settings. The\nunderlying reason for the phenomenon lies in the natural\ndifficulty in sparse neural field supervision \u2013 the fewer train-\ning observations require a network to have the more pow-\nerful capability on learning multi-view relationships (both\ninterpolation and extrapolation) and proper hallucinating,\nto approximate precise geometry. NeuralBody [56], Ani-\nmatableNeRF [55], and HumanNeRF [77] all adopt strong\nhuman priors with SMPL mesh, blend weights, and mo-\ntion priors. Thus, they are more robust to sparse obser-\nvations. In contrast, A-NeRF [66] integrates only skele-\nton prior that is sparse in human shape representation, and\nthe category-agnostic method NeuralVolumes [42] relies on\ndense observations to overfit to a particular distribution.\nThese two methods\u2019 performances drop significantly when\ngiven fewer views during training. Similar to the observa-\ntion from dense novel view benchmarks, the current dy-\nnamic human method performs unsatisfactorily when De-\nformation and Texture difficulty increase. Due to the com-\nplex texture and off-body non-rigidity, the results in the\nsparse setting further enlarge the gap with the dense one. In\ncontrast with the phenomena in sparse novel view synthesis,\nthe quantitative results on the sparse novel pose animation\ntask (Tab. S3) show similar trends compared to the dense\nview setting. Specifically, HumanNeRF [77] and Neural-\nBody [56] perform the best among the three metrics. A-\nNeRF [66] shows better pose generalization ability com-\npared to AnimatableNeRF [55], considering the fact that\nAnimatableNeRF [56] performs better in novel view syn-\nthesis tasks given seen human poses. When comparing the\ndifferences between dense and sparse settings, quantitative\nNovel View Synthesis\nSplits\nPSNR\u2191\nSSIM\u2191\nLPIPS*\u2193\nNV\nAN\nNB\nAnN\nHN\nNV\nAN\nNB\nAnN\nHN\nNV\nAN\nNB\nAnN\nHN\nMotion Simple\n23.31\n24.32\n24.77\n24.40\n26.04\n0.948\n0.954\n0.972\n0.975\n0.973\n84.02\n66.33\n70.53\n57.75\n32.26\nMotion Medium\n23.79\n24.18\n22.89\n23.01\n23.84\n0.955\n0.959\n0.972\n0.973\n0.963\n73.66\n59.10\n72.82\n70.43\n37.83\nMotion Hard\n21.69\n23.31\n21.45\n22.84\n25.02\n0.943\n0.952\n0.971\n0.970\n0.972\n104.23\n75.17\n83.73\n78.06\n35.29\nDeformation Simple\n24.56\n24.85\n24.45\n25.27\n27.59\n0.951\n0.956\n0.964\n0.964\n0.971\n61.39\n46.81\n72.32\n56.19\n30.46\nDeformation Medium\n25.40\n25.36\n25.29\n24.07\n24.06\n0.953\n0.955\n0.972\n0.965\n0.959\n57.19\n49.92\n76.15\n60.26\n61.82\nDeformation Hard\n22.89\n23.08\n21.34\n20.18\n22.80\n0.931\n0.935\n0.964\n0.948\n0.937\n96.88\n84.66\n110.75\n98.65\n104.05\nTexture Simple\n23.58\n24.00\n24.28\n24.31\n25.24\n0.959\n0.964\n0.980\n0.974\n0.974\n76.06\n58.66\n64.69\n54.79\n32.99\nTexture Medium\n24.41\n25.08\n25.01\n20.37\n26.41\n0.953\n0.959\n0.975\n0.962\n0.967\n63.33\n49.54\n67.54\n82.32\n32.62\nTexture Hard\n22.39\n22.69\n21.89\n21.45\n25.95\n0.920\n0.926\n0.951\n0.950\n0.958\n110.36\n115.52\n103.13\n86.19\n26.35\nInteraction No\n24.83\n25.23\n24.71\n20.70\n25.28\n0.961\n0.964\n0.980\n0.964\n0.966\n68.06\n49.58\n67.65\n79.91\n45.11\nInteraction Simple\n24.67\n25.38\n25.39\n25.47\n25.95\n0.963\n0.968\n0.982\n0.977\n0.971\n61.90\n43.24\n59.19\n57.79\n34.74\nInteraction Medium\n22.11\n23.23\n21.67\n21.84\n22.84\n0.938\n0.943\n0.965\n0.961\n0.948\n99.50\n80.25\n94.61\n91.78\n69.95\nInteraction Hard\n23.55\n24.06\n21.93\n21.15\n22.18\n0.939\n0.944\n0.961\n0.959\n0.941\n84.15\n70.00\n96.80\n94.30\n87.81\nOverall\n23.63\n24.21\n23.47\n22.70\n24.86\n0.947\n0.952\n0.970\n0.965\n0.962\n80.06\n65.29\n79.99\n74.49\n48.56\nNovel Pose Animation\nMotion Simple\n21.17\n24.05\n24.31\n21.34\n25.62\n0.941\n0.952\n0.972\n0.953\n0.972\n93.60\n66.80\n76.33\n82.37\n32.45\nMotion Medium\n19.40\n21.64\n21.50\n20.53\n21.04\n0.941\n0.947\n0.968\n0.936\n0.951\n100.36\n78.29\n83.66\n61.69\n54.61\nMotion Hard\n19.09\n21.39\n20.64\n18.85\n23.58\n0.938\n0.948\n0.967\n0.950\n0.967\n112.72\n80.41\n87.99\n95.97\n40.50\nDeformation Simple\n20.73\n23.89\n23.43\n23.00\n26.17\n0.938\n0.950\n0.962\n0.960\n0.966\n87.29\n55.29\n81.29\n67.70\n35.39\nDeformation Medium\n22.43\n24.89\n25.19\n23.12\n22.76\n0.943\n0.951\n0.971\n0.952\n0.955\n70.79\n52.05\n75.68\n61.72\n70.44\nDeformation Hard\n19.13\n20.83\n21.34\n18.52\n21.41\n0.920\n0.923\n0.964\n0.941\n0.929\n131.86\n108.27\n110.74\n91.70\n129.75\nTexture Simple\n20.44\n23.42\n24.27\n23.07\n24.20\n0.950\n0.962\n0.980\n0.972\n0.972\n87.66\n59.22\n65.08\n77.59\n36.64\nTexture Medium\n23.16\n24.56\n25.14\n21.23\n26.63\n0.950\n0.957\n0.977\n0.957\n0.967\n69.64\n51.30\n72.68\n69.96\n30.81\nTexture Hard\n20.23\n21.90\n21.65\n17.93\n25.66\n0.911\n0.921\n0.951\n0.932\n0.956\n136.26\n124.49\n111.66\n116.84\n24.88\nInteraction No\n21.45\n24.90\n24.37\n22.08\n23.52\n0.953\n0.962\n0.979\n0.948\n0.962\n88.23\n53.08\n69.11\n70.76\n50.70\nInteraction Simple\n22.18\n25.05\n25.32\n22.77\n25.99\n0.958\n0.967\n0.982\n0.970\n0.971\n71.61\n43.02\n59.29\n72.53\n32.95\nInteraction Medium\n19.83\n22.83\n21.33\n20.60\n22.40\n0.931\n0.943\n0.966\n0.959\n0.947\n107.04\n76.08\n94.86\n92.98\n66.90\nInteraction Hard\n20.55\n23.06\n21.93\n21.02\n21.42\n0.927\n0.937\n0.961\n0.945\n0.934\n110.91\n81.78\n96.79\n92.63\n95.35\nOverall\n20.75\n23.26\n23.11\n21.08\n23.88\n0.939\n0.948\n0.969\n0.952\n0.958\n97.54\n71.54\n83.47\n81.11\n53.95\nTable S3: Benchmarks with sparse view training. We abbreviate NeuralVolumes [42] as \u2018NV\u2019, A-NeRF [66] as \u2018AN\u2019, NeuralBody [56]\nas \u2018NB\u2019, AnimatableNeRF [55] as \u2018AnN\u2019 and HumanNeRF [77] as \u2018HN\u2019. Cell color\nindicate the best, second best, and third best\nperformance in the same split respectively.\n27\nInteraction\nSimple\nHard\nMedium\nNo\nDeformation\nSimple\nHard\nMedium\nNeural Volumes\nA-NeRF\nNeural Body\nAnimatableNeRF\nHumanNeRF\nMotion\nSimple\nHard\nMedium\nTexture\nSimple\nHard\nMedium\nSparse 1/PSNR\nSparse 1/SSIM\nSparse LPIPS\nDense 1/PSNR\nDense 1/SSIM\nDense LPIPS\nNeural Volumes\nA-NeRF\nNeural Body\nAnimatableNeRF\nHumanNeRF\n(a) Novel view synthesis with different training view numbers.\nInteraction\nSimple\nHard\nMedium\nNo\nDeformation\nSimple\nHard\nMedium\nNeural Volumes\nA-NeRF\nNeural Body\nAnimatableNeRF\nHumanNeRF\nMotion\nSimple\nHard\nMedium\nTexture\nSimple\nHard\nMedium\nSparse 1/PSNR\nSparse 1/SSIM\nSparse LPIPS\nDense 1/PSNR\nDense 1/SSIM\nDense LPIPS\nNeural Volumes\nA-NeRF\nNeural Body\nAnimatableNeRF\nHumanNeRF\n(b) Novel pose animation with different training view numbers.\nFigure S12: Visualization of quantitative comparison between dense view training and sparse view training. For (a) novel view and\n(b) novel pose tasks, we compare the dense view training (42 views) and sparse view (4 views) training on different data splits. Bubbles\nin dark tones are with dense view training, and other ones in light tones are with sparse view training. Noted that the scale used here is\ndifferent from Fig.4 for better comparison visualization.\nresults display a relatively smaller performance drop. The\nphenomenon reveals that, the training observations from\nboth dense and sparse view settings are not adequate enough\nfor the benchmark methods to learn a compact dynamic\nfield for unseen poses.\nD. Cross-dataset Details\nIn this section, we provide more details as a supplement\nto the cross-dataset evaluation mentioned in the main paper.\nSpecifically, we first describe the criteria for selecting the\ncompared datasets, and review the key attributes of these\ndatasets in Sec. D.1. Then, we introduce the implementa-\ntion and setting details in Sec. D.2. We discuss the results\nin Sec. D.3. Finally, we analyze the impact of color consis-\ntency on multi-camera datasets in Sec. D.4.\nD.1. Compared Datasets\nTo evaluate the potential of the proposed dataset on\nboosting algorithms\u2019 generalizability from the data engi-\nneering aspect, we compare the proposed dataset with the\nmost commonly used human-centric multiview datasets on\nthe generalizable neural rendering task. For a fair compari-\nson, we select datasets with foreground segmentation anno-\ntations and dense camera views. Thus, several well-known\ndatasets are not suitable for this evaluation. For example.\nHuman3.6M [26] only contains four RGB cameras, CMU\nPanoptic [30] and AIST++ [70, 37] lack official segmenta-\ntion annotation. We select ZJU-MoCap [56], HuMMan [7]\nand GeneBody [13] for comparison. In this subsection, we\ndiscuss their main features and their adaption to generaliz-\nable methods.\nZJU-MoCap [56] is currently the most widely used dataset\nin human neural rendering domain. It contains 10 multiview\nperformance sequences, with accurate camera calibration,\nhuman segmentation as well as SMPL annotation. The main\ndrawback of this dataset is the lack of diversity in clothing\nand motion and without human objection interaction. As\nmentioned in Sec. 3.3 in the main paper, color consistency\ndesign might be ignored in ZJU-MoCap [56], where obvi-\nous color differences can be observed between neighboring\ncameras, as shown in Fig. S16a. When training generaliz-\nable models on this dataset, we adopt the official splits and\nfollow the implementation in KeypointNeRF [49].\nHuMMan [7] is a human action dataset with data captured\nSome human rendering methods [34, 12] use their own tools to gener-\nate mask, we exclude these mask sources for fairness.\n28\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\nIBRNet\n23.62 21.38 19.10 18.53\n22.75 22.28 18.82 18.79\n24.88 23.81 27.11 20.16\n22.35 23.37 19.09 26.94\n20\n22\n24\n26\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\n0.921 0.905 0.870 0.882\n0.918 0.916 0.871 0.890\n0.928 0.922 0.945 0.891\n0.912 0.915 0.819 0.925\n0.825\n0.850\n0.875\n0.900\n0.925\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\n126.23 156.45 195.61 172.58\n129.13 148.24 206.84 172.95\n112.10 129.78 84.71 155.45\n127.83 138.69 281.46 109.98\n100\n150\n200\n250\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\nPixelNeRF\n22.49 20.39 19.77 19.16\n21.90 21.07 19.00 19.29\n20.98 22.57 25.43 20.00\n20.25 19.45 19.55 23.67\n20\n22\n24\n0.904 0.889 0.882 0.877\n0.901 0.900 0.864 0.883\n0.893 0.904 0.928 0.883\n0.896 0.901 0.899 0.924\n0.88\n0.90\n0.92\n159.33 179.45 186.49 200.45\n159.57 153.16 217.09 190.06\n176.90 151.12 109.93 194.11\n165.61 153.89 142.66 120.26\n120\n140\n160\n180\n200\nPSNR\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\nVisionNeRF\n21.91 20.02 18.88 18.65\n20.49 20.04 18.78 17.87\n20.30 21.32 25.61 18.37\n21.68 20.01 16.99 24.47\n18\n20\n22\n24\nSSIM\n0.901 0.886 0.871 0.873\n0.899 0.899 0.882 0.880\n0.891 0.901 0.936 0.880\n0.911 0.908 0.881 0.927\n0.88\n0.90\n0.92\nLPIPS*\n139.20 149.96 185.41 181.72\n134.30 132.12 172.27 168.58\n162.27 136.27 93.09 170.97\n125.63 119.39 168.31 107.59\n100\n120\n140\n160\n180\nDataset Trained\nDataset Tested\nFigure S13: Cross-dataset affinity map for category-agnostic generalization methods. We crossly evaluate models trained on each\ndataset and plot their performance on testing splits on each dataset. The PSNR, SSIM, and LPIPS* are plotted in separate matrices.\nunder 10 synchronized Kinect RGB-D cameras.\nIt con-\ntains 400k sequences and 500 human actions which empha-\nsize muscle-related movements. The clothing diversity is\nmarginal where most subjects wear sports and daily cos-\ntumes, and there is no human-object interaction either. As\nthe source images come from the Kinect sensor, they might\nbe stuck in low-quality, and obvious color differences can\nbe found in HuMMan [7] dataset. Note that the full dataset\nis still unreachable, we train models on the released version,\nwith its official list that contains a training split with 317\nsequences and a testing split with 22 sequences. Noted that\ndifferent from other datasets, where cameras are organized\nin a world coordinate near the origin that axis alignment\nwith the real world, HuMMan [7] uses a coordinate system\nrelative to the first camera. Thus, we make a rigid transfor-\nmation to eliminate the coordinate system difference.\nGeneBody [13] is a recent multi-view human performance\ncapture dataset, which contains relatively wide diversity\ncoverage among clothing, motion, and interactions. It cap-\ntures human performance with 48 synchronized 5MP cam-\neras with a low proportion-in-view of the main subject,\nwhere the average height of the human bounding box is\naround 600 pixels. We use its official splits with 40 training\nsequences and 10 testing sequences.\nD.2. Detailed Settings\nMultiple factors might affect the impact assessment\nacross different datasets on neural rendering tasks, eg., the\nproportion of subjects in camera views, data scale, anno-\ntation accuracy, source view selection, training status, etc.\nOur main goal is to investigate where the diversity of the\nproposed dataset can benefit the generalization of human\nrendering. We conduct the experiments in the following set-\ntings. 1) In order to ensure the fairness of dataset compari-\nson, we need to unify several input conditions, e.g., num-\nber and resolution of source views, etc. Meanwhile, we\nonly investigate the category-agnostic generalizable meth-\nods, namely PixelNeRF [81], IBRNet [75] and Vision-\nNeRF [38], to avoid difference of input and accuracy of\nhuman prior in multiple datasets. 2) Evaluating the whole\nobject-centric images with background removal produces a\nlarge rendering metric gap if the center subjects\u2019 propor-\ntions in camera views differ a lot.\nThus, different from\nthe novel pose benchmark in Sec. 4 and Sec. C.2.3 where\na half resolution is used, we crop the subjects out from the\noriginal image in all datasets with square bounding boxes\n29\nand resize them to 512 \u00d7 512 resolution for both source\nviews and target views. 3) As the discussed datasets are all\ncaptured in dense circling camera arrays, we manually se-\nlect four balanced views as the reference views at the same\nheight that have a roughly 90-degree interval. 4) During\ntraining, we use the same learning rate over all datasets and\nstop the training process with the same global step. Each\nmodel is trained on one 8-V100 machine with distributed\ndata-parallel stopping at 200k iterations.\n5) Finally, we\ntrain and evaluate all the models based on the official splits\nof each dataset. For the comparable data volume magni-\ntude of test samples, we size the data volume of test frames\nor test views on each dataset with \u27e8 45 frame-skip, 18 test\nviews \u27e9 on GeneBody and DNA-Rendering , \u27e8 45 frame-\nskip, 12 uniform sampled test views \u27e9 on ZJU-MoCap , and\n\u27e8 8 frame-skip, 6 test views \u27e9 on HuMMan, respectively.\nD.3. Additional Results\nIn the main paper, we present the average PSNR per-\nformance over all three general scene methods, i.e., Pix-\nelNeRF [81], IBRNet [75], and VisionNeRF [38]. Here,\nwe present the performances of these methods individually\n(Fig. S13), and illustrate the qualitative results (Fig. S15).\nNote that this setting leads to the absolute values in cross-dataset eval-\nuation worse than the ones in novel identity task, due to the larger propor-\ntion of human foreground.\nInteraction\nSimple\nHard\nMedium\nNo\nDeformation\nSimple\nHard\nMedium\nOurs\nGeneBody\nHuMMan\nZJU-MoCap\nScaled 1/PSNR\nScaled 1/SSIM\nScaled LPIPS\nMotion\nSimple\nHard\nMedium\nTexture\nSimple\nHard\nMedium\nOurs\nGeneBody\nHuMMan\nZJU-MoCap\nFigure S14: Cross-dataset evaluation on our DNA-Rendering\ndata splits. We visualize the performance of models trained on\ndifferent datasets on the proposed dataset\u2019s splits.\nWe unfold the result analysis in terms of in-domain, and\ncross-domain in Sec. D.3.1 and Sec. D.3.2 respectively. A\ndiscussion on the impact of color consistency is discussed\nin Sec. D.4.\nD.3.1\nIn-domain\nIn-domain refers to the problem of evaluating models with\nthe trainset and testset sharing the same underlying data\ndistribution.\nWe observe two key phenomena: 1) mod-\nels trained on datasets with low data diversity achieve\nbetter in-domain results.\nAs shown in the diagonal ele-\nments of the matrices in Fig. S13, for in-domain gener-\nalization performance, methods trained on HuMMan [7]\nand ZJU-MoCap [56] achieve the best and second perfor-\nmances with relatively appealing metric values. In contrast,\ntheir in-domain performances on GeneBody and DNA-\nRendering\nare worse than the other two datasets.\nNot-\ning that test sets in ZJU-MoCap and HuMMan only con-\ntain cases with textureless clothing and easy motion illus-\ntrated in Fig. S15, which are easy cases in terms of data\ndifficulty. Besides, the nature of textureless data and easy\nhuman geometry is relatively friendly to category-agnostic\ngeneralization methods that conduct multiview image fea-\nture aggregation in a common manner without geometry\nprior. 2) Larger data volume and diversity boosts in-domain\nperformance. Concretely, like the proposed dataset, Gene-\nBody [13] contains a train and test split with a wide dis-\ntribution of clothing, accessories, and motion, while with\nfar less data volume and diversity compared to DNA-\nRendering(about 10% data volume of our dataset). Despite\nboth test sets of GeneBody and proposed dataset containing\ncases with even distribution in multiple difficulties, all three\nmethods demonstrate our boost on in-domain performance.\nD.3.2\nCross-domain\nCross-domain refers to directly evaluating the pre-trained\nmodel on one dataset to the test split of another dataset,\nwhich is represented by the off-diagonal elements in\nFig. S13. We expand the result analysis in two folds: 1)\ndatasets with large variations of data attributes and diffi-\nculties boost the cross-domain generalization. Higher per-\nformance degradation can be observed in off-diagonal ele-\nments in each row in ZJU-MoCap [56] and HuMMan [7]\nin Fig. S13. Besides, even when ZJU-MoCap [56] trained\nmodel test on cross-domain case with easy clothing and mo-\ntion, the cross-domain performance is still far from accept-\nable, refer to left most blue T-shirt case in Fig. S15. On the\ncontrary, GeneBody [13] and DNA-Rendering experience\na very marginal degradation when evaluating other test sets.\n2) Larger data volume and diversity can also boost cross-\ndomain robustness. As mentioned in the in-domain part,\nDNA-Rendering is 10 times than GeneBody in terms of\n30\nOurs\nGeneBody\nHuMMan\nZJU-MoCap\nOurs\nGeneBody\nHuMMan\nZJU-MoCap\nGT\nFigure S15: Qualitative cross-dataset results. We demonstrate samples from different datasets (left labels) generated by IBRNet [75]\nmodels trained on different datasets (bottom labels).\ndata volumes, such improvement helps increase the model\u2019s\ngeneralization ability in considering margin. In the over-\nwhelming majority of first-row elements, models with the\nproposed dataset get the best cross-domain results and even\nperform better than in-domain results of GeneBody.\nCross-domain on DNA-Rendering Splits. To further in-\nvestigate the performance of different dataset-trained mod-\nels\u2019 performance on different human performance dimen-\nsions and difficulties, we visualize their performance on\nDNA-Rendering splits in Fig. S14. We plot the in-domain\nresult of our dataset-trained model as a reference bar. Gene-\nBody [13] has a relatively wide range of dimensions across\nour dimension and it achieves the best rendering quality\namong all splits. On the other hand HuMMan [7] and ZJU-\nMoCap [56], only contain easy clothing, motion, and inter-\naction, the performance on more difficult splits drop signif-\nicantly espec compared to GeneBody [13].\nD.4. Impact of Color Consistency\nTo further analyze the impact of color consistency of\ntraining dataset in generalizable rendering, we unfold the\nstats across views on the cross-dataset results. Due to the\ndifferent groundtruth in different views, it is hard to draw\nany conclusion from any single frame. Thus, we expand the\naverage PSNR across camera views and analyze the statis-\ntics. Noted that we only select the test views which have\nvery close angle distances from the nearest source view, to\nerase the performance gap from the viewpoints. The av-\nerage PSNR across testing cameras is plotted in Fig. S16.\nMore concretely, we visualize the in-domain statistics of\ncameras\u2019 average PSNR in Fig. S16b. When training and\ntesting a model on the same dataset, the camera color\ndistinction will remain constant.\nModels trained on the\ndatasets that cannot ensure color consistency across views\n(illustrated in Fig. S16a) might treat the color difference\nof different views as the view-depend effect and memorize\nit. The variance of cameras\u2019 average performance in such\ndatasets is slightly higher than GeneBody [13] and the pro-\nposed dataset. Cross-domain generalization span on views\nis also plotted in Fig. S16c. Different from in-domain statis-\ntics, when generalizing on other datasets, models trained\non datasets with color inconsistency all suffer a major aver-\nage performance dropping, and the variance of camera per-\nformance becomes even larger. This phenomenon is very\nnoticeable on cross-evaluation between ZJU-MoCap [56]\nand HuMMan [7]. While models trained on the proposed\ndataset as well as GeneBody [13], have very small view\nperformance variations between each other. The increased\nview variation on ZJU-MoCap [56] and HuMMan [7] is due\nto the nature color difference on their groundtruth. In a nut-\nshell, with the best color consistency, the proposed dataset\ncan benefit the community by providing high-quality data\nwith faithful probing capability across views.\n31\n(a)\nDNA-Rendering\nGeneBody\nHuMMan\nZJU-MoCap\nDataset Trained\n19\n21\n23\n25\n27\nCameras' Average PSNR\nDataset Tested\nDNA-Rendering\nGenBody\nHuMMan\nZJU-MoCap\n(b)\nGeneBody\nHuMMan\nZJU-MoCap\nDNA-Rendering\nHuMMan\nZJU-MoCap\nDNA-Rendering\nGeneBody\nZJU-MoCap\nDNA-Rendering\nGeneBody\nHuMMan\nDataset Trained\n10\n14\n18\n22\n26\nCameras' Average PSNR\nDataset Tested\nDNA-Rendering\nGenBody\nHuMMan\nZJU-MoCap\n(c)\nFigure S16: Cross-dataset evaluation across views.\n(a) Examples of the color differences between neighboring cameras in ZJU-\nMoCap [56] (top) and HuMMan [7] (bottom). (b) In-domain statistics on view average PSNR when trained and tested on the same\ndataset. (c) Cross-domain statistics on view average PSNR when trained and tested on different datasets. In both (b) and (c), the horizon-\ntal axis means different datasets trained, and the color of the box separates the datasets tested. The line in the middle indicates the median\nvalue, the box indicates the lower to the higher quartile, and the whiskers indicate the range of average PSNR across views.\nE. Future Work\nLeaderboard. In the current human-centric rendering com-\nmunity, researchers from different institutions use different\ndatasets and experimental settings to evaluate the perfor-\nmances of their algorithms. There is no agreement across\ninstitutions to benchmark human rendering methods under\nthe same criterion yet. To reduce such divergence and align\nthe standards, we proposed a large-scale diverse dataset\nDNA-Rendering, and construct a complete benchmark in\nthree human-centric rendering tasks. Benchmarks are eval-\nuated on different data splits on different factors and dif-\nficulties. Additionally, we conduct a cross-dataset evalua-\ntion which demonstrates the proposed dataset can benefit\nthe community from its diversity and coverage. In DNA-\nRendering, all actors are signed with agreements before\ndata collection. Thus, all of the data is with a Creative Com-\nmons license and free for use under certain usage agree-\nments. In the future, we will host a web-based leaderboard,\nand release the easy-to-run tools to the community to better\nreduce the divergence.\nRobust Human-centric Matting Refinement. In the an-\nnotation pipeline, we use Grab-cut [59] to refine bad results\nof CNN-based methods, yet it is still not perfect. Since per-\nframe human labeling is impractical due to the volume of\ncaptured data, we involve human checks over segmentation\nresults, and only cases without major artifacts in the whole\nsequence across views will be released. We tried 3D meth-\nods to further refine results, e.g., using Instant-NGP [51]\nto train with valid views and infer the bad views. How-\never, the results are not appealing enough (blurry edges).\nWe will further investigate more robust tools. These chal-\nlenges could also benefit matting research. We believe with\nthe development of relevant techniques, matting robustness\nwill be improved in the near future.\nNew benchmarks.\nIn this paper, we set up three task\nbenchmarks as a kick-off of the DNA-Rendering dataset.\nOur datasets could potentially be used in many other tasks\nrelated to human-centric rendering, such as garment mod-\neling/animation and human shape completion. We encour-\nage and welcome the community to join us to unlock more\ndownstream tasks.\n32\n"
  },
  {
    "title": "Promoting Exploration in Memory-Augmented Adam using Critical Momenta",
    "link": "https://arxiv.org/pdf/2307.09638.pdf",
    "upvote": "2",
    "text": "Promoting Exploration in Memory-Augmented Adam\nusing Critical Momenta\nPranshu Malviya 1,2, Gon\u00e7alo Mordido 1,2, Aristide Baratin 3, Reza Babanezhad Harikandeh 3,\nJerry Huang 1,4, Simon Lacoste-Julien 1,3,4,5, Razvan Pascanu 6, Sarath Chandar 1,2,5\n1Mila - Quebec AI Institute, 2Polytechnique Montreal, 3Samsung SAIT AI Lab Montreal,\n4Universit\u00e9 de Montr\u00e9al, 5Canada CIFAR AI Chair, 6Deepmind\nAbstract\nAdaptive gradient-based optimizers, particularly Adam, have left their mark in\ntraining large-scale deep learning models. The strength of such optimizers is\nthat they exhibit fast convergence while being more robust to hyperparameter\nchoice. However, they often generalize worse than non-adaptive methods. Recent\nstudies have tied this performance gap to flat minima selection: adaptive methods\ntend to find solutions in sharper basins of the loss landscape, which in turn hurts\ngeneralization. To overcome this issue, we propose a new memory-augmented\nversion of Adam that promotes exploration towards flatter minima by using a buffer\nof critical momentum terms during training. Intuitively, the use of the buffer makes\nthe optimizer overshoot outside the basin of attraction if it is not wide enough. We\nempirically show that our method improves the performance of several variants of\nAdam on standard supervised language modelling and image classification tasks.\n1\nIntroduction\nThe performance of deep learning models is often sensitive to the choice of optimizer used during\ntraining, which significantly influences convergence speed and the qualitative properties of the minima\nto which the system converges [4]. Stochastic gradient descent (SGD) [42], SGD with momentum\n[41], and adaptive gradient methods such as Adam [26] have been the most popular choices for\ntraining large-scale models.\nAdaptive gradient methods are advantageous in that, by automatically adjusting the learning rate on\na per-coordinate basis, they can achieve fast convergence with minimal hyperparameter tuning by\ntaking into account curvature information of the loss. However, they are also known to achieve worse\ngeneralization performance than SGD [48, 53, 56]. The results of several recent works suggest that\nthis generalization gap is due to the greater stability of adaptive optimizers [53, 49, 5], which can lead\nthe system to converge to sharper minima than SGD, resulting in worse generalization performance\n[19, 24, 14, 37, 2, 21, 23].\nIn this work, we hypothesize that the generalization properties of Adam can be improved if we equip\nthe optimizer with an exploration strategy. that might allow it to escape sharp minima, similar to the\nrole of exploration in Reinforcement Learning. We build on the memory augmentation framework\nproposed by McRae et al. [35], which maintains a buffer containing a limited history of gradients\nfrom previous iterations, called critical gradients (CG), during training. Memory augmentation can\nbe seen as a form of momentum, that allows the optimizer to overshoot and escape narrow minima.\nThis is the basis of the exploration mechanism, where we want to add inertia to the learning process,\nand by controlling the amount of inertia control the necessary width of the minima in order for the\nsystem to converge. However, the original proposal memory-augmented adaptive optimizers in [35],\nparticularly Adam using CG, suffer from gradient cancellation: a phenomenon where new gradients\nhave high directional variance and large norm around a sharp minima. This leads to the aggregated\nPreprint. Under review.\narXiv:2307.09638v1  [cs.LG]  18 Jul 2023\ngradient over the buffer to vanish, and hence preventing the optimizer to escape from the sharp\nminima. This hypothesis is in agreement with the poor generalization performance when combining\nAdam with CG (referred to as Adam+CG) presented in the original paper [35].\nWe propose to instead store critical momenta (CM) during training, which leads to a new memory-\naugmented version of Adam (Algorithm 1) that can effectively escape sharp basins and converge\nto flat loss regions. To illustrate this, we show in Figure 1 the optimization trajectories, on a toy\n2D loss surface corresponding to the Goldstein\u2013Price (GP) function [40], of Adam, Adam+CG,\nAdam+CM, and Adam combined with sharpness-aware minimization (Adam+SAM) [15], with the\nsame initialization (black square). We observe that (i) Adam converges to a low loss but sharp\nregion of the surface; (ii) Adam+SAM converges to a flatter but higher loss region than Adam;\n(iii) memory-augmented variants (Adam+CG and Adam+CM) bring more exploration; (iv) only\nAdam+CM is able to find the flat region that contains the global minimum (black diamond).\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM\nAlgorithm 1: Adam with Critical Momenta\nRequire: Initial parameters \u03b80 and moments m0, vM\n0 ,\nloss L, step size \u03b1, buffer mc, capacity C, decay \u03bb\nfor t = 1, 2, \u00b7 \u00b7 \u00b7 do\nSample mini-batch & compute loss gradient\nUpdate 1st moments mt (1)\nAggregate with buffer moments mM\nt\n\u2190\u2212 mt (5)\nUpdate 2nd moments vM\nt\n(5)\nif buffer is not full then\nAdd mt to mc\nelse if Priority(mt) > min(Priority(mc)) then\nReplace smallest priority element with mt\nend if\nDecay Priority(mc) using \u03bb\nUpdate parameter \u03b8t (7)\nend for\nFigure 1: (Left) Learning trajectories for different optimizers on the Goldstein-Price loss function\nstarting from a common initial point (black square). While the other optimizers get stuck in sub-\noptimal surfaces, Adam+CM explores a lower loss surface and is able to reach the global minimum\n(black diamond). (Right) Pseudocode for Adam with critical momenta (Adam+CM).\nThe key contributions of our work are as follows:\n\u2022 We introduce a framework for promoting exploration in adaptive optimizers (Section 3). We\npropose a new memory-augmented version of Adam, which stores and leverages a buffer of\ncritical momenta from previous iterations during training.\n\u2022 We illustrate on a wide range of synthetic examples how our method addresses drawbacks of\nexisting memory-augmented methods and promotes exploration towards flat minima (Section 4).\n\u2022 We observe empirically an improvement of the generalization performance of different deep\nlearning models on a set of supervised language and image tasks (Section 5).\n2\nRelated work\nNumerous optimizers have been proposed to improve convergence speed and achieve better general-\nization in deep learning models. While SGD with momentum tends to show superior performance in\nparticular scenarios, it usually requires careful hyperparameter tuning of the learning rate and con-\nvergence criteria [30]. On the other hand, adaptive optimization methods [13, 18, 52], which adjust\nthe learning rate for each parameter based on past gradient information to accelerate convergence,\nhave reached state-of-the-art performance in many supervised learning problems while being more\nrobust to hyperparameter choice. In particular, Adam [26] combines momentum with an adaptive\nlearning rate and has become the preeminent choice of optimizer across a variety of models and tasks,\nparticularly in large-scale deep learning models [10, 47]. Several Adam variants have since been\nproposed [33, 54, 16, 6] to tackle Adam\u2019s lack of generalization ability [50, 53, 56, 5].\n2\nConverging to flat minima has been shown to be a viable way of indirectly improving generalization\nperformance [19, 24, 14, 37, 21, 23, 22]. For example, sharpness-aware minimization (SAM) [15]\njointly maximizes model performance and minimizes sharpness within a specific neighborhood during\ntraining. Since its proposal, SAM has been utilized in several applications, enhancing generalization\nin vision transformers [9, 3], reducing quantization error [31], and improving model robustness [36].\nNumerous methods have been proposed to further improve its generalization performance, e.g. by\nchanging the neighborhood shape [25] or reformulating the definition of sharpness [28, 55], and to\nreduce its cost, mostly focusing on alleviating the need for the double backward and forward passes\nrequired by the original algorithm [11, 12, 32].\nMemory-augmented optimizers extend standard optimizers by storing gradient-based information\nduring training to improve performance. Hence, they present a trade-off between performance\nand memory usage. Different memory augmentation optimization methods have distinct memory\nrequirements. For instance, stochastic accelerated gradient (SAG) [43] and its adaptive variant,\nSAGA [7], require storing all past gradients to achieve a faster convergence rate. While such methods\nshow great performance benefits, their large memory requirements often make them impractical in\nthe context of deep learning. On the other hand, one may only use a subset of past gradients, as\nproposed in limited-history BFGS (LBFGS) [38], its online variant (oLBFGS) [44], and stochastic\ndual coordinate ascent (SDCA) [45]. Additionally, memory-augmented frameworks with critical\ngradients (CG) use a fixed-sized gradient buffer during training, which has been shown to achieve a\ngood performance and memory trade-off for deep learning compared to the previous methods [35].\nIn this work, we further improve upon CG by storing critical momenta instead of critical gradients,\nleading to an increase in generalization performance in adaptive optimizers, particularly Adam.\n3\nMemory-augmented Adam\nIn this section, we introduce our method, which builds upon the memory-augmented framework\npresented by [35]. We focus on Adam in a supervised learning setting. The standard parameter\nupdate in Adam can be written as:\nmt = \u03b21mt\u22121 + (1 \u2212 \u03b21)gt;\nvt = \u03b22vt\u22121 + (1 \u2212 \u03b22)g2\nt\n(1)\n\u02c6mt =\nmt\n1 \u2212 \u03b2t\n1\n; \u02c6vt =\nvt\n1 \u2212 \u03b2t\n2\n; \u03b8t+1 = \u03b8t \u2212 \u03b1\n\u02c6mt\n\u221a\u02c6vt + \u03f5 .\n(2)\nwhere \u03b8t denotes the model parameter at iteration t, gt is the loss gradient on the current mini-batch,\n\u03b1 is the learning rate, \u03b21, \u03b22 \u2208 [0, 1) are the decay rates for the first and second moments.\nCritical gradients (CG).\nTo memory-augment Adam, [35] introduces a fixed-size buffer gc of\npriority gradients gc maintained in memory during training, and apply an aggregation function over\nthis buffer to modify the moment updates (1):\nmG\nt = \u03b21mG\nt\u22121 + (1 \u2212 \u03b21)aggr(gt, gc);\nvG\nt = \u03b22vG\nt\u22121 + (1 \u2212 \u03b22)aggr(gt, gc)2\n(3)\nThe gradient l2-norm is used as selection criterion for the buffer. The buffer takes the form of a\ndictionary where the key-value pairs are (\u2225gc\u22252, gc); additionally, the priority keys are decayed at\neach iteration by a decay factor \u03bb \u2208 (0, 1) to encourage buffer update. Thus, at each iteration t,\nif the norm \u2225gt\u22252 of the current gradient is larger than the smallest priority key in the buffer, the\ncorresponding critical gradient gets replaced by gt in the buffer. A standard choice of aggregation\nfunction adds gt to the average of the critical gradients in the buffer.\nThe gradient cancellation problem.\nHowever, as we observe throughout this paper, combining\nAdam with critical gradients does not always perform well. We hypothesize that in CG, while the\nbuffer gradients can promote exploration initially (as observed in Figure 1), the parameters remain\nstuck in sharp regions due to gradient cancellation. Gradient cancellation primarily occurs when\nexisting buffer gradients get quickly replaced by high-magnitude gradients when the parameters\nare near a sharp basin. As a result, the buffer quickly converges to high variance gradients whose\nmean goes to zero, allowing learning to converge. Intuitively, the parameters bounce back and forth\noff the sides and bottom of the sharp basin: whenever the parameters try to escape the basin, the\nnew outgoing gradient gets cancelled by incoming gradients in the buffer. Figure 2 illustrates this\n3\nAdam+CM\nAdam+CG\nBuffer quantities\nBuffer mean\nNew gradient\nAdam+CM\nAdam+CG\nBuffer quantities\nBuffer mean\nNew gradient\nFigure 2: First 10 steps of the Adam+CG and Adam+CM trajectories on Ackley loss surface.\nColoured diamond represents the final points reached by the optimizers. Gradient cancellation is\nobserved in Adam+CG on Ackley function as buffer mean and new gradients cancel each other out,\nwhich yields in a small update. On the other hand, Adam+CM escapes sub-optimal minima and\nconverges near the global minimum.\nphenomenon on a toy surface, by showing the buffer gradients (thin blue lines) and their means (black\narrow) as well as the new gradient (green arrow), within sharp basins where Adam+CG gets stuck.\nAdditional plots can be found in Appendix A.1.\nCritical momenta (CM).\nWe have seen that gradient cancellation hinders the ability of Adam+CG\nto escape sharp minima. To fix this problem, our approach leverages instead a buffer mc of critical\nmomenta mc during training. Just like in [35], we use the gradient l2-norm, as priority criterion1.\nThe buffer takes the form of a dictionary where the key-value pairs are (\u2225gc\u22252, mc) with a decay\nfactor \u03bb \u2208 (0, 1) for the keys at each iteration. The integration with critical momenta leads to a new\nalgorithm, Adam+CM, which defines the moment updates as follow:\nmt = \u03b21mt\u22121 + (1 \u2212 \u03b21)gt;\nmM\nt\n= aggr(mt, mc)\n(4)\nvM\nt\n= \u03b22vM\nt\u22121 + (1 \u2212 \u03b22) aggr(mt, mc)2\n(5)\nwhere aggr is the addition of the current momentum to the average of all critical momenta:\naggr(mt, mc) = mt + 1\nC\nX\nmc\u2208mc\nmc .\n(6)\nFinally, the Adam+CM update rule is given by\n\u02c6mM\nt\n=\nmM\nt\n1 \u2212 \u03b2t\n1\n;\n\u02c6vM\nt\n=\nvM\nt\n1 \u2212 \u03b2t\n2\n;\n\u03b8t+1 = \u03b8t \u2212 \u03b1\n\u02c6mM\nt\np\n\u02c6vM\nt\n+ \u03f5\n(7)\nThe pseudo-code of Adam+CM is given in Algorithm 1.2\nLooking at Figure 1, while at a sharp minima, the elements of the buffer will still be quickly replaced,\ndue to the inertia in the momentum terms the variance will stay low. Moreover, the fact that gradients\nquickly change direction will lead to the new momentum terms being smaller and hence have a\nsmaller immediate influence on the aggregate value of the buffer. This allows the overshooting\neffect to still happen, enabling the exploration effect and helping to learn to escape sharp minima.\nFurthermore, the larger the size of the buffer, the stronger the overshooting effect will be and the\nwider the minima needs to be for learning to converge. That is because learning needs to stay long\nenough in the basin of a minima to fill up most of the buffer in order to turn back to the minimum\nthat it jumped over and for the optimizer to converge. We observe this empirically in Figure 7 and\nAppendix A.2.2.\n1We do not use the alternative \u2225mt\u22252 since the buffer will not get updated fast enough using this criterion.\n2Optimizer package: https://github.com/chandar-lab/CMOptimizer\n4\n0\n100\n200\n300\n400\n500\nEpochs\n1\n0\n1\n2\n3\n4\n5\n6\nTrain Loss\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\nAdam+CM\nAdam+SAM+CM\nAdam+CG\nAdam+SAM\nAdam\nFigure 3: Training loss curves (left, averaged across 10 seeds) and learning trajectories (right, one\nseed) for different optimizers on the Ackley loss surface. While the other optimizers get stuck in\nsub-optimal minima near the initialization point (black square), both CM variants explore and find\nthe lower loss surface near the global solution (black diamond).\n4\nInsights from toy examples\nIn this section, we empirically validate on toy tasks our working hypothesis by analyzing and compar-\ning various combinations of Adam with memory augmentation and sharpness-aware minimization.\nCritical momenta promote exploration.\nWe first compare the optimization trajectories of\nAdam+CM with Adam, Adam+SAM, and Adam+CG, on interpretable, non-convex 2D loss surfaces.\nWe also include the double combination of Adam with SAM and CM. To complement the Goldstein-\nPrice function in Figure 1, we consider the Ackley function [1] (see (9) in Appendix A.2.1 for the\nexplicit formula), which contains a nearly flat outer region with many sharp minima and a large hole\nat the center with the global minimum at (0, 0).\nWe minimize the Ackley function for 10 different initialization seeds, and compare the trajectories\nof the different optimizers. We run each model for 500 steps and reduce the learning rate by a\nfactor 10 at the 250th step. To get the best performing setup, we perform a grid search over the\nhyper-parameters for each optimizer. Figure 3 shows the training curves (left) and optimization\ntrajectories (right) of the different optimizers, for the same initialization (black square). We observe\nthat, here, only the CM variants are able to explore the loss surface, resulting in a lower loss solution.\nAdditional trajectories with various different seeds for both the Ackley and Goldstein-Price loss\nsurfaces are shown in Appendix A.2.1 (Figures 13 and 12).\nCritical momenta reduce sharpness.\nWe now want to compare more specifically the implicit\nbias of the different optimizers towards flat regions of the loss landscape.\nWe first examine the solutions of optimizers trained on the Goldstein-Price and Levy functions [29]\n(see Appendix A.2.1). Both of these functions contain several local minima and one global minimum.\nWe evaluate the solutions based on the final loss and sharpness, averaged across 20 seeds. As a simple\nproxy for sharpness, we compute the highest eigenvalue of the loss Hessian.\nResults in Table 1 show that Adam+CM finds flatter solutions with a lower loss value compared to\nAdam, Adam+CG, and Adam+SAM in both examples. Furthermore, Adam and Adam+SAM reach\nalmost equal loss values for the Levy function with a negligible difference in sharpness, but for the\nGP function, Adam+SAM converges to a sub-optimal minimum with lower sharpness.\nWe hypothesize that the buffer size controls the amount of exploration and analyze this empirically in\nAppendix A.2.1, where we show that even with a small buffer size, Adam+CM can escape sharper\nminima and explores lower loss regions than other optimizers. The results also suggest that in a\ncontrolled setting, the larger buffer size helps find a flatter minimum.\n5\n2\n3\n5\n10\n100\nSharpness coefficient (s)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nEscape ratio\nAdam+CM C=20\nAdam+CG C=20\nAdam+SAM\nAdam\nFigure 4: Escape ratio (number of times the op-\ntimizer escapes the sharp minimum to reach the\nglobal minimum out of 50 runs) in the 10-D toy\nexample (8), for different values of the sharpness\ncoefficient. Adam+CM shows a higher ability to\nescape sharp minima in this setting.\nOptimizers\nLoss\nSharpness\nAdam\n0.86\n1.49\nAdam+SAM\n3.14\n1.43\nGP\nAdam+CG\n0.85\n1.51\nAdam+CM\n0.81\n1.36\nAdam\n13.87\n65.65\nAdam+SAM\n13.87\n65.62\nLevy\nAdam+CG\n13.61\n64.45\nAdam+CM\n12.50\n62.53\nTable 1: Loss vs sharpness of the solutions\nof different optimizers for toy loss surfaces.\nThe buffer decay is set to 0.99 for these exper-\niments. Adam+CM is able to find solutions\nthat are both flatter and deeper (lower loss)\nthan other optimizers in this setting.\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\nFigure 5: Optimization trajectory of Adam (left), Adam+CG (middle), and Adam+CM (right) on a\ntoy 1D function with a flat and a sharp minimum with increasing sharpness (across columns), for\ndifferent initialization points (across rows). Green backgrounds indicate that the optimizer escapes\nthe sharper minimum while red backgrounds indicate otherwise. The vertical line indicates the final\npoint in each sub-figure. We observe that Adam mostly converges to the minimum closest to the\ninitial point. Adam+CM converges to the flatter minimum for different initial points and degrees of\nsharpness more often than Adam+CG.\nTo further investigate the escaping abilities of the various optimizers, we consider the following class\nof functions on RD:\nfs(x) =\nD\nX\nd=1\nmin(x2\nd, s(xd \u2212 1)2)\n(8)\nwhere s > 1 is a sharpness coefficient. Each function in this class has two global minima: a flat\nminimum at the origin and a sharper minimum at (1 \u00b7 \u00b7 \u00b7 1). Figure 5 shows optimization trajectories\nin the one-dimensional case for various values of the sharpness coefficient s \u2208 {5, 10, 100} (across\ncolumns) and initial point x \u2208 {\u22122, 2, 3} (across rows). We can see that Adam mostly converges to\nthe minimum closest to the initial point. Adam+CM converges to the flatter minimum for different\ninitial points and degrees of sharpness more often than Adam+CG. Additional plots are shown in\nAppendix A.3 for various values of the hyperparameters.\nTo quantify this bias in higher dimension (D = 10), we sample 50 different initial points uniformly\nin [\u22125, 5]10. Out of these 50 runs, we count the number of times an optimizer finds the flat minimum\nat the origin by escaping the sharper minimum. Figure 4 reports the escape ratio for different values\nof the sharpness coefficient. We observe that Adam+CM (with buffer capacity C = 20) has a higher\n6\nescape ratio than others as the sharpness increases. We replicate this experiment with various values\nof the buffer capacity in Appendix A.2.1 (Figure 11).\n5\nExperimental results\nThe goal of this section is to evaluate our method empirically on complex models and benchmarks.\nAll our results are averaged across three seeds.\n5.1\nLanguage modelling\n0\n10\n20\n30\n40\n50\nEpochs\n2 \u00d7 102\n3 \u00d7 102\n4 \u00d7 102\n6 \u00d7 102\nValidation Perplexity\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\nFigure 6: Validation perplexity of the different\noptimizers and Adam+CM using a single-layer\nLSTM trained on the PTB dataset. We see that\nAdam+CM and its SAM variant result in a lower\nvalidation perplexity and faster convergence.\nStarting with a language-based task, a single-\nlayer long short-term memory network (LSTM)\n[20] is trained on the Penn Tree Bank (PTB)\ndataset [34]. We evaluate the performance by\nreporting the validation perplexity on a held-out\nset. All models and optimizers are trained for\n50 epochs. We train the models for 40 epochs\n(similar to [35]) and we reduce the learning at\nthe 25th epoch by dividing it by 10. The results\nare reported after performing a grid search over\ncorresponding hyper-parameters. The details of\nthis grid search are present in Appendix Table 6.\nFigure 6 shows the validation perplexity dur-\ning the learning process.\nWe observe that\nAdam+CM always converges faster, suggesting\nthat it has explored and found a basin with a bet-\nter generalizable solution than other optimizers\nby the 18th epoch. The second-best performing\noptimizer is Adam+CG, which reaches lower\nperplexity after reducing the learning rate. Ad-\nditionally, both CM variants overfit after conver-\ngence.\n5.2\nImage classification\nNext, we evaluate the effect of Adam+CM on different model sizes for image classification.\nCIFAR 10/100 [27]\nWe train ResNet models [17], particularly ResNet34 and WRN-1 (with 40\nlayers) [51]) for 3 different seeds. Optimizers are compared in terms of the validation accuracy\ncomputed on a held-out set. We train the models for 100 epochs where we reduce the learning at the\n50th epoch by dividing it by 10.\nResults from all experiments performed for image classification tasks are summarized in Table 2,\nwhere we report the best validation accuracy achieved by different ResNet models when they are\ntrained on CIFAR-10/100. We report the results both with and without performing an extensive grid\nsearch over hyper-parameters. The details of this grid search are present in Appendix Table 6.\nIn each case, we observe that CM variants perform best. Without grid search, CM variants perform\nbest on both datasets, with Adam+CM achieving the best results with the ResNet34 model while\nAdam+SAM+CM performs best with the WRN-1 model. With grid search, Adam+SAM+CM yielded\nthe best validation accuracy for CIFAR-10, while Adam+CM performed the best on CIFAR-100.\nFigure 7 (left) shows the training progress of the different optimizers without grid search, where we\nsee CM variants have slightly faster convergence in WRN-1 and Adam+SAM+CM outperform other\nbaselines when the learning rate is reduced after the 50th epoch. Similar plots with and without grid\nsearch are given in Appendix A.2.2. Figure 7 (right) shows the final sharpness metric for different\nbuffer sizes recorded for CIFAR10/100 experiments with default hyperparameter setup. It is clear that\nusing a large buffer size can further reduce the sharpness of the solution in such complex settings.\n7\nCIFAR-10\nCIFAR-100\nGrid search\nOptimizers\nResNet34\nWRN-1\nResNet34\nWRN-1\nAdam [26]\n93.6\u00b14.1\n90.5\u00b11.1\n69.6\u00b10.5\n61.9\u00b10.4\nAdam+CG [35]\n93.3\u00b11.6\n89.7\u00b11.4\n69.3\u00b10.2\n62.2\u00b11.0\n\u2717\nAdam+SAM [15]\n93.6\u00b14.4\n90.3\u00b13.5\n69.5\u00b10.1\n62.1\u00b10.3\nAdam+CM\n93.7\u00b12.4\n90.7\u00b12.4\n69.8\u00b11.4\n61.7\u00b10.3\nAdam+SAM+CM\n93.4\u00b15.3\n91.0\u00b11.3\n68.2\u00b10.1\n63.1\u00b10.2\nAdam [26]\n93.9\u00b10.3\n91.1\u00b10.6\n70.7\u00b10.3\n62.8\u00b10.3\nAdam+CG [35]\n93.8\u00b10.4\n90.6\u00b10.3\n71.0\u00b10.3\n63.4\u00b10.5\n\u2713\nAdam+SAM [15]\n93.7\u00b12.6\n90.5\u00b13.8\n70.5\u00b10.4\n62.4\u00b10.5\nAdam+CM\n94.0\u00b10.3\n91.5\u00b10.1\n71.2\u00b10.3\n63.6\u00b10.6\nAdam+SAM+CM\n94.5\u00b12.3\n91.7\u00b12.0\n69.7\u00b10.3\n63.1\u00b10.5\nTable 2: Comparison of performance in terms of best validation accuracy (%) achieved by the existing\nbaselines with Adam+CM and its SAM variant on training ResNet34 and WRN-1 with CIFAR-10\nand CIFAR-100. All measurements are averaged across 3 runs.\n0\n20\n40\n60\n80\n100\nEpochs\n84\n86\n88\n90\n92\n94\nValidation Accuracy\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\nResnet34\nCIFAR10\nWRN-1\nCIFAR10\nResnet34\nCIFAR100\nWRN-1\nCIFAR100\n10\n2\n10\n1\n10\n0\nSharpness\nC=5\nC=20\nFigure 7: (Left) Validation accuracy with default hyper-parameterson CIFAR-10 for WRN-1. In\nthis case, Adam+SAM+CM results in the best performance. (Right) Sharpness for different buffer\nsizes for Adam+CM CIFAR10/100 experiments with the same hyper-parameter setup. Using a larger\nbuffer size results in lower sharpness even for high-dimensional models.\nImageNet [8]\nWe also train an EfficientNet-B0 model [46] from scratch on ImageNet. We used a\npublicly available EfficientNet implementation3 in PyTorch [39], weight decay [33] of 1e-4 and an\ninitial learning rate of 1e-4 which is reduced by a factor of 10 every 30 epochs. We provide additional\ndetails about the datasets and models in Appendix A.2.\n0\n20\n40\n60\n80\nEpochs\n60\n62\n64\n66\n68\n70\n72\nTop-1 Accuracy\n0\n20\n40\n60\n80\nEpochs\n80\n82\n84\n86\n88\n90\n92\nTop-5 Accuracy\nAdamW\nAdamW+CG\nAdamW+CM\nAdamW+SAM\nFigure 8: Top-1 accuracy (left), top-5 accuracy (right) of the different optimizers and AdamW+CM\ntraining an EfficientNet-B0 on ImageNet. All optimizers use the same weight decay and learning rate\nscheduler. AdamW+CM outperform existing optimizers with default settings.\n3https://github.com/lukemelas/EfficientNet-PyTorch\n8\n0\n20\n40\n60\n80\n100\nEpochs\n10\n1\n100\nSharpness\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\n0\n20\n40\n60\n80\n100\nEpochs\n102\nDistance\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\n0\n20\n40\n60\n80\n100\nEpochs\n10\n5\n10\n4\nBuffer Variance\nAdam+CG\nAdam+CM\n0\n20\n40\n60\n80\n100\nEpochs\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nCosine similarity\nAdam+CG\nAdam+CM\nFigure 9: Sharpness (top-left), distance (top-right) buffer variance (bottom-left) and cosine similarity\n(bottom-right) in buffer elements of the optimizers using WRN-1 on CIFAR-100. These indicate that\nbuffer elements in Adam+CM agree more with each other and have lower sharpness than Adam+CG.\nFigure 8 compares top-1 and top-5 accuracies on the validation set. Due to compute constraints, we\nuse the default hyper-parameter set. We observe that AdamW+CM convergences faster and achieves\nbetter final top-1 and top-5 accuracies than the other optimizer baselines whereas SAM does not\nperform well in the default hyper-parameter setting.\n5.2.1\nAnalysis\nFigure 9 corroborates the claim in Section 4 that Adam+CM finds a flatter surface containing the\nglobal minimum, as the top-right plot shows lower sharpness when compared to Adam or Adam+SAM.\nIt also reveals the greater distance travelled by parameters during training, which indicates that using\nCM promotes more exploration than the other optimizers.\nThe bottom-left plot in Figure 9 shows that buffer elements stored by Adam+CM have lower variance\nduring training compared to Adam+CG. To compare the agreement among buffer quantities, we\ntake the element with the highest norm within the buffer, compute the cosine similarities with other\nelements in the buffer, and take the mean of these similarities. The bottom-right plot in Figure 9 shows\nthat the agreement in Adam+CM remains higher than in Adam+CG, indicating that the aggregation\nof buffer elements in Adam+CM will more often result in a non-zero quantity in the desired direction.\nOn the other hand, high variance and disagreement among elements in the Adam+CG buffer may\ncause gradient cancellation during aggregation and result in Adam-like behavior.\n6\nConclusion\nThis work introduces a framework for promoting exploration in adaptive optimizers. We propose\nAdam+CM, a new memory-augmented version of Adam that maintains a buffer of critical momenta\nand modifies the parameters update rule using an aggregation function. Our analysis shows that it ad-\ndresses the drawbacks of existing memory-augmented adaptive optimizers and promotes exploration\ntowards flatter regions of the loss landscape. Our empirical results show that Adam+CM outperforms\nAdam, SAM, and CG on standard image classification and language modeling tasks. For large-scale\nmodels, CM provides exploration benefits by searching for flat loss surfaces.\nA promising avenue of investigation is to apply our method to non-stationary settings like continual\nlearning, as these require the model to transfer knowledge without overfitting on a single task. Our\nresults suggest that CM may be able to capture higher-order dynamics of the loss surface, deserving\nfurther exploration. We leave the theoretical in-depth analysis for future work.\n9\nAcknowledgements\nThis research was supported by Samsung Electronics Co., Ltd. through a Samsung/Mila collaboration\ngrant, and was enabled in part by compute resources provided by Mila, the Digital Research Alliance\nof Canada, and NVIDIA. Sarath Chandar is supported by a Canada CIFAR AI Chair and an NSERC\nDiscovery Grant. Simon Lacoste-Julien is a CIFAR Associate Fellow in the Learning Machines &\nBrains program and supported by NSERC Discovery Grants. Gon\u00e7alo Mordido is supported by an\nFRQNT postdoctoral scholarship (PBEEE).\nReferences\n[1] David H Ackley. The model. In A Connectionist Machine for Genetic Hillclimbing, pages\n29\u201370. Springer, 1987.\n[2] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian\nBorgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient\ndescent into wide valleys. In International Conference on Learning Representations, 2017.\n[3] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform\nresnets without pre-training or strong data augmentations. In International Conference on\nLearning Representations, 2022.\n[4] Dami Choi, Christopher J Shallue, Zachary Nado, Jaehoon Lee, Chris J Maddison, and\nGeorge E Dahl. On empirical comparisons of optimizers for deep learning. arXiv preprint\narXiv:1910.05446, 2019.\n[5] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati,\nMichal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive\ngradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.\n[6] Aaron Defazio and Samy Jelassi. Adaptivity without compromise: a momentumized, adaptive,\ndual averaged gradient method for stochastic optimization. Journal of Machine Learning\nResearch, 2022.\n[7] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient\nmethod with support for non-strongly convex composite objectives. Advances in Neural\nInformation Processing Systems, 2014.\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\nhierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition,\n2009.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on Learning Representations, 2021.\n[10] Timothy Dozat. Incorporating Nesterov momentum into Adam. In International Conference on\nLearning Representations workshop, 2016.\n[11] Jiawei Du, Zhou Daquan, Jiashi Feng, Vincent Tan, and Joey Tianyi Zhou. Sharpness-aware\ntraining for free. In Advances in Neural Information Processing Systems, 2022.\n[12] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and\nVincent Tan. Efficient sharpness-aware minimization for improved training of neural networks.\nIn International Conference on Learning Representations, 2022.\n[13] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine Learning Research, 2011.\n[14] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds\nfor deep (stochastic) neural networks with many more parameters than training data.\nIn\nConference on Uncertainty in Artificial Intelligence, 2017.\n10\n[15] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware min-\nimization for efficiently improving generalization. In International Conference on Learning\nRepresentations, 2021.\n[16] Diego Granziol, Xingchen Wan, Samuel Albanie, and Stephen Roberts. Iterative averaging in\nthe quest for best test error. arXiv preprint arXiv:2003.01247, 2020.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.\n[18] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning\nlecture 6a overview of mini-batch gradient descent, 2012.\n[19] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Simplifying neural nets by discovering flat minima.\nAdvances in Neural Information Processing Systems, 1994.\n[20] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9:\n1735\u20131780, 1997.\n[21] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon\nWilson. Averaging weights leads to wider optima and better generalization. In Conference on\nUncertainty in Artificial Intelligence, 2018.\n[22] Yiding Jiang*, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy Bengio.\nFantastic generalization measures and where to find them. In International Conference on\nLearning Representations, 2020.\n[23] Simran Kaur, Jeremy Cohen, and Zachary C Lipton. On the maximum hessian eigenvalue and\ngeneralization. arXiv preprint arXiv:2206.10654, 2022.\n[24] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\nTak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.\nIn International Conference on Learning Representations, 2016.\n[25] Minyoung Kim, Da Li, Shell X Hu, and Timothy Hospedales. Fisher SAM: Information\ngeometry and sharpness aware minimisation. In International Conference on Machine Learning,\n2022.\n[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-\ntional Conference on Learning Representations, 2015.\n[27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images,\n2009.\n[28] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive sharpness-\naware minimization for scale-invariant learning of deep neural networks. In International\nConference on Machine Learning, 2021.\n[29] Manuel Laguna and Rafael Marti. Experimental testing of advanced scatter search designs for\nglobal optimization of multimodal functions. Journal of Global Optimization, 33:235\u2013255,\n2005.\n[30] Quoc V Le, Jiquan Ngiam, Adam Coates, Abhik Lahiri, Bobby Prochnow, and Andrew Y Ng.\nOn optimization methods for deep learning. In International Conference on Machine Learning,\n2011.\n[31] Ren Liu, Fengmiao Bian, and Xiaoqun Zhang. Binary quantized network training with sharpness-\naware minimization. Journal of Scientific Computing, 2023.\n[32] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and\nscalable sharpness-aware minimization. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022.\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations, 2019.\n11\n[34] Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\nBuilding a large\nannotated corpus of english: The penn treebank. Comput. Linguist., 19(2):313\u2013330, jun 1993.\nISSN 0891-2017.\n[35] Paul-Aymeric Martin McRae, Prasanna Parthasarathi, Mido Assran, and Sarath Chandar. Mem-\nory augmented optimizers for deep learning. In International Conference on Learning Repre-\nsentations, 2022.\n[36] Gon\u00e7alo Mordido, Sarath Chandar, and Fran\u00e7ois Leduc-Primeau. Sharpness-aware training for\naccurate inference on noisy DNN accelerators. arXiv preprint arXiv:2211.11561, 2022.\n[37] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring\ngeneralization in deep learning. Advances in Neural Information Processing Systems, 2017.\n[38] Jorge Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of compu-\ntation, 1980.\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative\nstyle, high-performance deep learning library. Advances in Neural Information Processing\nSystems, 2019.\n[40] Victor Picheny, Tobias Wagner, and David Ginsbourger. A benchmark of kriging-based infill\ncriteria for noisy optimization. Structural and multidisciplinary optimization, 48:607\u2013626,\n2013.\n[41] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR\nComputational Mathematics and Mathematical Physics, 1964.\n[42] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of\nMathematical Statistics, 1951.\n[43] Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an expo-\nnential convergence rate for finite training sets. Advances in Neural Information Processing\nSystems, 2012.\n[44] Nicol N Schraudolph, Jin Yu, and Simon G\u00fcnter. A stochastic quasi-newton method for online\nconvex optimization. In Conference on Artificial Intelligence and Statistics, 2007.\n[45] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized\nloss minimization. Journal of Machine Learning Research, 2013.\n[46] Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural\nnetworks. In International Conference on Machine Learning, 2019.\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, 2017.\n[48] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The\nmarginal value of adaptive gradient methods in machine learning. In Advances in Neural\nInformation Processing Systems, 2017.\n[49] Lei Wu, Chao Ma, and Weinan E. How SGD selects the global minima in over-parameterized\nlearning: A dynamical stability perspective. In Advances in Neural Information Processing\nSystems, 2018.\n[50] Lei Wu, Chao Ma, et al. How SGD selects the global minima in over-parameterized learning: A\ndynamical stability perspective. Advances in Neural Information Processing Systems, 31, 2018.\n[51] Sergey Zagoruyko and Nikos Komodakis.\nWide residual networks.\narXiv preprint\narXiv:1605.07146, 2016.\n[52] Matthew D Zeiler.\nADADELTA: An adaptive learning rate method.\narXiv preprint\narXiv:1212.5701, 2012.\n12\n[53] Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards\ntheoretically understanding why SGD generalizes better than Adam in deep learning. Advances\nin Neural Information Processing Systems, 2020.\n[54] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon\nPapademetris, and James Duncan. AdaBelief optimizer: Adapting stepsizes by the belief in\nobserved gradients. In Advances in Neural Information Processing Systems, 2020.\n[55] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C Dvornek,\nSekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-\naware training. In International Conference on Learning Representations, 2022.\n[56] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of\nAdam in learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371,\n2021.\n13\nA\nAppendix\nIn this section, we provide the details and results not present in the main content. In section A.1,\nwe report more evidence of gradient cancellation in a toy example. In section A.2, we describe the\nimplementation details including hyper-parameters values used in our experiments. All experiments\nwere executed on an NVIDIA A100 Tensor Core GPUs machine with 40 GB memory.\nA.1\nGradient cancellation in CG\nWhen we track the trajectory along with the gradient directions of the buffer in the Adam+CG\noptimizer on the Ackley+Rosenbrock function (defined in the next section), we found that CG gets\nstuck in the sharp minima (see Figure 10). This is because of the gradient cancellation problem\ndiscussed earlier.\nFigure 10: Three consecutive steps in training Adam+CG on Ackley+Rosenbrock function with\nC = 5. The white lines indicate the gradient directions in the buffer. Since four of these gradients\npoint in opposite directions, their mean would be small and cancel the current gradient update out.\nAs a result, parameters get stuck in the sharp minima.\nA.2\nImplementation details and other results\nA.2.1\nToy examples\nWe evaluate our optimizer on the following test functions in Section 4:\n1. Ackley function:\nf(x, y) = \u221220 exp(\u22120.2\np\n0.5(x2 + y2)) \u2212 exp(0.5(cos 2\u03c0x + cos 2\u03c0y)) + e + 20 (9)\nThe global minimum is present at (0, 0). In Figure 12, we visualize the trajectories of Adam,\nAdam+SAM, Adam+CG and Adam+CM for different initialization points. While other\noptimizers may get stuck at nearby local minima, Adam+CM benefits from more exploration\nand finds a lower-loss surface that may contain the global minima.\n2. Goldstein-Price function:\nf(x, y) =\n1\n2.427 log[1 + (x + y + 1)2(19 \u2212 14x + 3x2 \u2212 14y + 6xy + 3y2)]\n\u00d7 [30 + (2x \u2212 3y)2(18 \u2212 32x + 12x2 + 48y \u2212 36xy + 27y2) \u2212 8.693]\n(10)\nThe global minimum is present at [0, 1]2. In Figure 13, we visualize the trajectories of\nAdam, Adam+SAM, Adam+CG and Adam+CM for different initialization points. While\nother optimizers may get stuck at a sub-optimal loss surface, Adam+CM benefits from more\nexploration and finds the global minimum.\n3. Levy function:\nf(x1, x2) = sin2(\u03c0w)+\nd\u22121\nX\ni=1\n(wi \u22121)2[1+10 sin2(\u03c0wi +1)]+(wd \u22121)2[1+sin2(2\u03c0wd)]\n(11)\nwhere wi = 1 + xi\u22121\n4\nand d is the number of variables. The global minimum is present at\n(1, 1).\n14\n4. Ackley+Rosenbrock function:\nf(x, y) = 0.05(1 \u2212 x)2 + 0.05(y \u2212 x2)2 + 0.6[exp(\u22120.2\np\n0.5(x2 + y2))\n\u2212 exp(0.5(cos 2\u03c0x + cos 2\u03c0y)) + e]\n(12)\nThe global minima is present at (2.1, 3.9).\nIn Figure 11 (left), we compare the escape ratio for different optimizers on Equation 8 with d = 1\n(similar to Figure 4). We can see that for C = 10, with an exception at s = 2, Adam+CM consistently\nfinds the minimum at x = 0 more often than other optimizers. Interestingly, as s increases, Adam+CG\nis outperformed by both Adam and Adam+SAM, which indicates that Adam+CG is more susceptible\nto getting stuck at sharper minima in this example. Figure 11 (right) shows that the tendency of\nAdam+CM to escape minima is dependent on C such that, a larger value of C results in convergence\nto flatter minimum more often.\n1\n2\n3\n5\n10\n100\nSharpness coefficient (s)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEscape ratio\nAdam+CM topC=10\nAdam+CG topC=10\nAdam+SAM\nAdam\n1\n2\n3\n5\n10\n100\nSharpness coefficient (s)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEscape ratio\nAdam+CM topC=5\nAdam+CM topC=10\nAdam+CM topC=20\nAdam+CM topC=40\nFigure 11: (i) Escape ratio (number of times when optimizer reaches the minima at x = 0 out of\ntotal 50 runs) for different sharpness coefficient (s) for minima at x = 1 in 1D toy example shown in\nFigure 5. (ii) Escape ratio vs sharpness coefficient (s) for different C.\nIn Table 1, we also compared different optimizers in terms of the sharpness of the solution reached\nby each optimizer on different functions. In Table 3, we perform a similar experiment and compare\nsharpness across different values of hyperparameters for Adam+CM. We observe that Adam+CM is\nable to find a flatter and lower-loss solution as buffer size increases. This is consistent with complex\ntasks in Figure 7.\nOptimizers\nC\n\u03bb\nLoss\nSharpness\nAdam+CG\n5\n0.7\n14.40\n64.72\nAdam+CG\n5\n0.99\n14.41\n64.67\nAdam+CG\n20\n0.99\n13.61\n64.45\nAdam+CM\n5\n0.7\n12.90\n63.74\nAdam+CM\n5\n0.99\n13.02\n63.83\nAdam+CM\n20\n0.99\n12.50\n62.53\nTable 3: Loss vs sharpness of the solutions of Adam+CM for different hyperparameters buffer sizes\non Levy function. Adam+CM is able to find solutions that are both flatter and deeper (lower loss)\nas buffer size increases. Moreover, Adam+CM always outperforms Adam+CG even with a smaller\nbuffer size.\nA.2.2\nDeep learning experiments\nIn Table 4 and Table 5, we provide a summary of all datasets and deep learning models used in the\nexperiment from Section 5.\nThe tasks consist of:\n15\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM+CM\nAdam+SAM\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM+CM\nAdam+SAM\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM+CM\nAdam+SAM\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM+CM\nAdam+SAM\nFigure 12: Optimization trajectories of various optimizers on the Ackley loss surface for different\ninitial points (black square). The black diamond indicates the global minimum.\nDataset\nTrain set\nValidation set\nPTB\n890K\n70K\nCIFAR-10\n40K\n10K\nCIFAR-100\n40K\n10K\nImageNet\n1281K\n50K\nTable 4: Dataset details\n\u2022 The Penn Treebank (PTB) is a part-of-speech (POS) tagging task where a model must determine\nwhat part of speech (ex. verb, subject, direct object, etc.) every element of a given natural\nlanguage sentence consists of.\n\u2022 CIFAR-10 is a multiclass image classification task the training and validation set consists of\nimages that are separated into 10 distinct classes, with each class containing an equal number of\nsamples across the training and validation sets. The goal is to predict the correct image class,\nprovided as an annotated label, given a sample from the sets.\n\u2022 CIFAR-100 is the same task as CIFAR-10, except that images are now separated into 100 classes\nwith an equal number of samples within each class.\n\u2022 ImageNet is a large-scale dataset consisting of images separated into 1000 distinct classes. The\nobjective of the task is the same as CIFAR-10 and CIFAR-100, which is to classify images from\nthe training and validation set correctly into a class with an annotated label.\nFor all toy examples experiments based on Equation 8, the learning rate is set as 0.05. The learning\nrate is set as 0.1 for other toy examples. For the results reported in Table 2, we run a grid search only\n16\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM\nAdam+CM\nAdam+CG\nAdam\nAdam+SAM\nFigure 13: Optimization trajectories of various optimizers on the Goldstein-Price loss surface for\ndifferent initial points (black square). The black diamond indicates the global minimum..\nModel\nNumber of parameters\nLSTM\n20K\nWRN-2\n11M\nWRN-2\n45M\nResNet34\n22M\nWRN-1\n0.5M\nEfficientNet-B0\n5.3M\nTable 5: Model details\nover the learning rate for CIFAR10/100 experiments that work best for a given model using Adam and\nkeep that learning rate fixed for all other optimizers for comparison. For all Imagenet experiments,\nthe learning rate is set to 0.0001. Unless specified in the experiment description, the default set of\nother hyperparameters in all our experiments is {\u03b21, \u03b22, C, \u03bb, \u03c1} = {0.9, 0.99, 5, 0.7, 0.05} except in\nCIFAR10/100 experiments where \u03b22 is set to 0.999. The default values of C and \u03bb are decided based\non the suggested values from [35] and \u03c1 based on [15]. For the results in Figure 4 and Table 1, C and\n\u03bb are est to 20 and 0.99.\nFor PTB and CIFAR10/100, we provide the details on hyper-parameter grid-search in Table 6 and\nthe best settings for all experiments and Table 7, Table 8 and Table 9. In these tables, we report the\nhyperparameter set for each optimizer as follows:\n\u2022 Adam: {\u03b1}\n\u2022 Adam+CG: {\u03b1, \u03b21, \u03b22, C, \u03bb}\n17\n\u2022 Adam+SAM: {\u03b1, \u03b21, \u03b22, \u03c1}\n\u2022 Adam+CM: {\u03b1, \u03b21, \u03b22, C, \u03bb}\n\u2022 Adam+SAM+CM: {\u03b1, \u03b21, \u03b22, C, \u03bb, \u03c1}\nHyper-parameter\nSet\nlr\n{0.1, 0.01, 0.001, 0.0001}\n\u03b21\n{0.9, 0.99, 0.999}\n\u03b22\n{0.99, 0.999, 0.9999}\nC\n{5, 20}\n\u03bb\n{0.7, 0.99}\n\u03c1\n{0.01, 0.05, 0.1}\nTable 6: Details on grid search on hyper-parameter setting.\nOptimizers\nPTB\nLSTM\nAdam\n{0.001, 0.9, 0.99}\nAdam+CG\n{0.001, 0.9, 0.999, 5, 0.7}\nAdam+SAM\n{0.001, 0.9, 0.9, 0.01}\nAdam+CM\n{0.001, 0.9, 0.999, 5, 0.7}\nAdam+SAM+CM\n{0.001, 0.9, 0.999, 5, 0.7, 0.1}\nTable 7: Best hyperparameter settings for different optimizers on PTB.\nCIFAR10\nOptimizers\nResnet34\nWRN1\nAdam\n{0.001, 0.9, 0.999}\n{0.01, 0.9, 0.999}\nAdam+CG\n{0.0001, 0.9, 0.999, 20, 0.7}\n{0.001, 0.9, 0.99, 20, 0.7}\nAdam+SAM\n{0.0001, 0.9, 0.99, 0.05}\n{0.01, 0.9, 0.9999, 0.1}\nAdam+CM\n{0.0001, 0.9, 0.999, 20, 0.99}\n{0.001, 0.9, 0.999, 5, 0.7}\nAdam+SAM+CM\n{0.0001, 0.9, 0.99, 20, 0.99, 0.05}\n{0.001, 0.9, 0.9999, 20, 0.7, 0.1}\nTable 8: Best hyperparameter settings for different optimizers on CIFAR10.\nCIFAR100\nOptimizers\nResnet34\nWRN1\nAdam\n{0.0001, 0.9, 0.9999}\n{0.001, 0.9, 0.99}\nAdam+CG\n{0.0001, 0.9, 0.9999, 20, 0.7}\n{0.001, 0.9, 0.99, 20, 0.7}\nAdam+SAM\n{0.0001, 0.9, 0.999, 0.1}\n{0.001, 0.9, 0.999, 0.05}\nAdam+CM\n{0.0001, 0.9, 0.999, 20, 0.99}\n{0.001, 0.9, 0.9999, 5, 0.7}\nAdam+SAM+CM\n{0.0001, 0.9, 0.9999, 20, 0.7, 0.05}\n{0.001, 0.9, 0.99, 5, 0.7, 0.1}\nTable 9: Best hyperparameter settings for different optimizers on CIFAR100.\nNext, similar to Figure 7, we plot the training process in CIFAR-10/100 on ResNet34 and WRN-\n1 models with hyperparameter tuning (see Figure 14) and without hyper-parameter tuning (see\nFigure 15). Overall, we can see that while Adma+SAM can have faster convergence, the CM variants\nreach higher validation accuracy in both setups.\n18\n0\n20\n40\n60\n80\n100\nEpochs\n84\n86\n88\n90\n92\n94\nValidation Accuracy\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\n0\n20\n40\n60\n80\n100\nEpochs\n50\n55\n60\n65\n70\n75\nValidation Accuracy\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\n0\n20\n40\n60\n80\n100\nEpochs\n50\n55\n60\n65\n70\n75\nValidation Accuracy\nAdam\nAdam+CG\nAdam+CM\nAdam+SAM\nAdam+SAM+CM\nFigure 14: Validation accuracy of the different optimizers and Adam+CM with default hyper-\nparameterson CIFAR-10 for ResNet34 (left) and CIFAR-100 for ResNet34 (middle) and WRN-1\n(right). Overall, Adam+SAM+CM results in the best performance.\n0\n20\n40\n60\n80\n100\nEpochs\n84\n86\n88\n90\n92\n94\nValidation Accuracy\n0\n20\n40\n60\n80\n100\nEpochs\n50\n55\n60\n65\n70\n75\nValidation Accuracy\n0\n20\n40\n60\n80\n100\nEpochs\n84\n86\n88\n90\n92\n94\nValidation Accuracy\n0\n20\n40\n60\n80\n100\nEpochs\n50\n55\n60\n65\n70\n75\nValidation Accuracy\nFigure 15: Validation accuracy of the different optimizers and Adam+CM with hyper-parameter\nsearch on CIFAR-10 (left column) and CIFAR-100 (right column) for ResNet34 (first row) and\nWRN-1 (second row).\nA.3\nSensitivity analysis on toy example\nFollowing experiments in Figure 5 in section 4, we fix decay to 0.7 in Figure 16 and vary C. We\nperform a similar experiment with decay= 0.99 and plot them in Figure 17. In both these figures,\nthe observation remains the same that is Adam+CM converges to flatter minima for different initial\npoints and degrees of sharpness. We also observe that C plays an important role in convergence to\nflatter minima in both Adam+CG and Adam+CM.\n19\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\nFigure 16: Following Figure 5, we compare trajectories of Adam+CG (left columns) and Adam+CM\n(right column) on Equation 8 with d = 1 where \u03bb is set to 0.7 and C: (i) 5 (first row), (ii) 10\n(second row) and (iii) 20 (third row). For different initial points and degrees of sharpness, Adam+CM\nconverges to flatter minima more often than Adam+CG.\n20\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n6\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2\n0\n2\n0\n2\n4\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\n2.5\n0.0\n2.5\n0\n5\n10\nFigure 17: Following Figure 5, we compare trajectories of Adam+CG (left columns) and Adam+CM\n(right column) on Equation 8 with d = 1 where \u03bbset to 0.99 and C: (i) 5 (first row), (ii) 10 (second\nrow) and (iii) 20 (third row). For different initial points and degrees of sharpness, Adam+CM\nconverges to flatter minima more often than Adam+CG.\n21\n"
  }
]