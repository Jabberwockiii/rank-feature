[
  {
    "title": "Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All",
    "link": "https://arxiv.org/pdf/2401.13795.pdf",
    "upvote": "61",
    "text": "Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion\nModels for Virtual Try-All\nMehmet Saygin Seyfioglu*\nKarim Bouyarmane\u2020\nSuren Kumar\nAmir Tavanaei\nIsmail B. Tutar\nAmazon\nhttps://diffuse2choose.github.io\nFigure 1. Diffuse to Choose (DTC) allows users to virtually place any e-commerce item in any setting, ensuring detailed, semantically\ncoherent blending with realistic lighting and shadows.\nAbstract\nAs online shopping is growing, the ability for buyers to\nvirtually visualize products in their settings\u2014a phenomenon\nwe define as \u201cVirtual Try-All\u201d\u2014has become crucial. Re-\ncent diffusion models inherently contain a world model,\nrendering them suitable for this task within an inpaint-\ning context. However, traditional image-conditioned diffu-\nsion models often fail to capture the fine-grained details of\nproducts. In contrast, personalization-driven models such\n*University of Washington, work done during an internship at Amazon.\n\u2020Correspondence at Amazon: bouykari@amazon.com\nas DreamPaint are good at preserving the item\u2019s details\nbut they are not optimized for real-time applications. We\npresent \u201dDiffuse to Choose,\u201d a novel diffusion-based image-\nconditioned inpainting model that efficiently balances fast\ninference with the retention of high-fidelity details in a given\nreference item while ensuring accurate semantic manipula-\ntions in the given scene content. Our approach is based\non incorporating fine-grained features from the reference\nimage directly into the latent feature maps of the main dif-\nfusion model, alongside with a perceptual loss to further\npreserve the reference item\u2019s details. We conduct extensive\ntesting on both in-house and publicly available datasets,\nand show that Diffuse to Choose is superior to existing zero-\n1\narXiv:2401.13795v1  [cs.CV]  24 Jan 2024\nshot diffusion inpainting methods as well as few-shot diffu-\nsion personalization algorithms like DreamPaint.\n1. Introduction\nThe ever-growing demand for online shopping underscores\nthe need for a more immersive shopping experience, allow-\ning shoppers to virtually \u2018try\u2019 any product from any cate-\ngory (clothes, shoes, furniture, decoration, etc.) within their\npersonal environments. The concept of a Virtual Try-All\n(Vit-All) model hinges on its functionality as an advanced\nsemantic image composition tool. In practice, this involves\ntaking an image from a user, selecting a region within that\nimage, and using a reference product image from an online\ncatalog to semantically insert the product into the selected\narea while preserving its details. For such a model to be\neffective, it must fulfill three primary conditions: 1) oper-\nate in any \u2019in-the-wild\u2019 user image (not only on staged stu-\ndios or professional human model images with predefined\nposes), and reference image, 2) integrate the reference prod-\nuct harmoniously with the surrounding context while main-\ntaining the product\u2019s identity (not replacing the product with\na generic image of a product from similar category), and 3)\nperform fast inference to facilitate real-time usage across\nbillions of products and millions of users.\nExisting solutions tend to be specialized. Instead of aim-\ning for a general purpose Vit-All approach, models are often\ndeveloped for specific tasks and domains (model for cloth-\ning, model for furniture, model for eyeglasses, etc.). For ex-\nample, early GAN-based works focused primarily on virtual\ntry-on of clothing on human models in limited contexts or\ncontrolled environment (such as only certain clothing seg-\nments and no in-the-wild user images, or product images)\n[3, 5, 12, 17, 18, 38]. Other approaches to the problem uti-\nlize somewhat expensive 3D AR/VR technologies for items\nlike furniture in rooms [1, 29], which is hard to scale to\nevery single item on catalogs of billions of products that\noften lack 3D models. Consequently, a unified model offer-\ning a comprehensive Vit-All experience \u2014 one that enables\nconsumers to digitally interact with any product from any\ncategory in any setting \u2014 is currently not available.\nThe emergence of diffusion models has marked a signifi-\ncant breakthrough in the generative capabilities of complex\nimage modeling [25, 27, 33]. Unlike GANs, Diffusion mod-\nels inherently grasp the nuances of the 3D world, exhibiting\na degree of geometry and physics awareness, as demon-\nstrated in inpainting tasks by [41], establishing their use-\nfulness for Vit-All applications. A DreamBooth-based [26]\ntechnique, called DreamPaint [28], showed that Stable Dif-\nfusion [25] can be few-shot fine-tuned for the Vit-All use\ncase. It can infer how to warp clothes to a person\u2019s body,\nor how to place a certain furniture on a certain spot in a se-\nmantically correct manner without being explicitly trained\nto do so. While DreamPaint meets the first two criteria to be\nan effective Vit-All model, it requires few-shot fine-tuning\nfor each product separately, compromising its suitability for\nreal-time applications thus failing to meet the third criterion.\nA\nrecently\nintroduced\nimage-referenced\ninpainting\nmodel, Paint By Example (PBE) [40], operates in a zero-\nshot setting and can handle in-the-wild images, meeting cri-\nteria one and three. However it encounters a limitation due\nto its reliance on an information bottleneck in its condition-\ning process, utilizing only the [CLS] token of the reference\nimage. This constraint leads to an over-generalization of\nthe reference image, degrading the model\u2019s ability to main-\ntain the fine-grained details essential for the Vit-All context,\nthus PBE fails to meet criterion two. Additionally, operating\nwithin a latent space, PBE struggles to retain fine-grained\ndetails of each item, underscoring the necessity for incor-\nporating some form of pixel-level guidance.\nIn this work, we introduce \u201cDiffuse to Choose\u201d (DTC),\na novel diffusion inpainting approach designed for the Vit-\nAll application. DTC, a latent diffusion model, effectively\nincorporates fine-grained cues from the reference image\ninto the main U-Net decoder using a secondary U-Net en-\ncoder. Inspired by ControlNet [42], we integrate a pixel-\nlevel \u201chint\u201d into the masked region of an empty image,\nwhich is then processed through a shallow convolutional\nnetwork, ensuring dimensional alignment with the masked\nimage processed by the Variational Autoencoder (VAE).\nDTC harmoniously blends the source and reference images,\nmaintaining the integrity and details of the reference im-\nage. To further enhance alignment of basic features such as\ncolor, we employ perceptual loss using a pre-trained VGG\nmodel [7]. The complete architecture is illustrated in Fig. 2,\nwith examples showcased in Fig. 1 and Fig. 5.\nDTC effectively fulfills all three criteria for the Vit-All\nuse case: 1) It efficiently handles in-the-wild images and\nreferences, 2) It adeptly preserves the fine-grained details\nof products while ensuring their seamless integration into\nthe scene, and 3) It facilitates rapid zero-shot inference. We\ntrained DTC on an in-house training dataset with sampled\n1.2M source-reference pairs and a smaller public dataset,\nVITON-HD-NoFace [5]1. Our quantitative evaluations and\nhuman studies demonstrate that DTC surpasses all PBE\nvariants \u2014 for which we implemented several enhance-\nments to facilitate a fair comparison against DTC \u2014 and\nmatches the performance of non-real-time, few-shot person-\nalization models like DreamPaint [28], within the Vit-All\ncontext.\n2. Related Work\nVirtual Try-On. The primary goal of virtual try-on ap-\nproaches is to create an image of a person wearing a tar-\n1The VITON-HD public dataset was modified to remove and crop all\nmodel faces from the images in the dataset.\n2\nU-Net\ndecoder\nFiLM\nVAE encoder\nU-Net \nencoder\nVAE decoder\nMLP\nU-Net \nencoder\nAdapter\nImage \nencoder\n+\nResize and \ninsert\n+\nSource\nMask\nReference\nHint\nHint Signal\nGenerated\nGaussian \nNoise\nImage Condition\nMasked\nFrozen \nmodules\nTrainable \nmodules\nDiffuse-To-Choose (DTC) \ntraining pipeline for \nVirtual Try-All (Vit-All)\nFigure 2. Pipeline of Diffuse to Choose. The process initiates with masking the source image, followed by inserting the reference image\nwithin the masked area. This pixel-level \u2018hint\u2019 is then adapted by a shallow CNN to align with the VAE output dimensions of the source\nimage. Subsequently, a U-Net Encoder processes the adapted hint. Then, at each U-Net scale, a FiLM module affinely aligns the skip-\nconnected features from the main U-Net Encoder and pixel-level features from the hint U-Net Encoder. Finally, these aligned feature maps,\nin conjunction with the main image conditioning, facilitate the inpainting of the masked region. Red indicates trainable modules and blue\nindicates frozen modules.\nget garment, ensuring that the garment\u2019s fine-grained de-\ntails are preserved and that it blends seamlessly with its sur-\nrounding context. Since this is a highly domain-specific\nand constrained problem (models and garments are of-\nten not presented in in-the-wild examples), existing mod-\nels generally employ warp and paste (blend) techniques,\nleveraging extra inputs such as pose and human parse\nmaps [3, 5, 7, 12, 17, 18, 37\u201339]. VITON [8] uses a two-\nstep synthesis and refinement process, initially generating\na coarse image with the desired clothing and then refining\nit to enhance the details. VITON-HD [5] aims for higher-\nresolution images and employs alignment-aware segment\nnormalization to correct misaligned parts. TryOnGAN [18]\nuses pose conditioning but relies on a purely latent model,\nwhich often lacks fine-grained details when representing\ngarments. Given that GAN-based approaches are tailored\nexclusively for virtual try-on, which is a more narrowly de-\nfined problem compared to ours, and they do not possess\nthe broad mode coverage inherent to diffusion models, we\nhave chosen to concentrate solely on diffusion models in our\nwork. One of the latest diffusion-based virtual try-on mod-\nels, Tryondiffusion [43], employs a dual U-Net approach\non a pixel-level diffusion model. While it offers impressive\nperformance for virtual try-on, it struggles with in-the-wild\nexamples, supports only upper garments, and is not suitable\nfor real-time use. Thus, a latent diffusion approach is nec-\nessary to ensure real-time inference in practical use cases.\nRegarding furniture, most existing works are AR-based 3D\napproaches from large corporations, which do not provide\nmuch detail about their models. In the 2D domain, there is\nDreamPaint [28], a DreamBooth-based [26] inpainting ap-\nproach that few-shot fine-tunes the U-Net of Stable Diffu-\nsion, but it is not suitable for real-time applications.\nDiffusion Based Image Editing. Creating realistic com-\nposites by superimposing an object from one image onto\nanother is a common practice in photo editing and is closely\n3\nImage Condition\nVAE encoder\nVAE encoder\nVAE decoder\nU-Net\ndecoder\nU-Net \nencoder\nMLP\nImage \nencoder\nGenerated\n+\nGaussian \nNoise\nSource\nMasked Source\nReference\nPaint By Example \npipeline applied on the\nVit-All training dataset\nFigure 3. Pipeline of Paint by Example [40] for Vit-All case. Red\nare trainable and blue are frozen. Orange pathways indicate skip\nconnections between the encoder and the decoder.\naligned with the Vit-All task. Image Editing, particularly\ninpainting, has been extensively explored in diffusion mod-\nels. Initially, there were text-based image editing models\n[2, 9, 13, 14, 25]. However, it is evident that text alone\ncannot capture the fine-grained details necessary for accu-\nrately describing a product, necessitating the use of image\nconditioning. DCCF [38] introduced pyramid filters for im-\nage composition, followed by Paint by Example [40], which\nconditions the diffusion model using CLIP embeddings of\nthe reference image. However, relying solely on the [CLS]\ntoken often leads to an overgeneralization of the reference\nimage, making it unsuitable for the Vit-All task. Similarly,\nObjectStitch [34] combines image and text embeddings to\nguide the model but faces challenges in conveying fine-\ngrained details. In response to these challenges, we intro-\nduce Diffuse to Choose, a novel latent diffusion inpainting\nmodel. Our model effectively leverages fine-grained details\nfrom the reference image, ensuring both the preservation of\nthe product\u2019s fine-grained details and its seamless integra-\ntion into the chosen location, while working in a zero-shot\nsetting with any in-the-wild image.\n3. Method\nWe formulate Vit-All as an image-conditioned inpainting\ntask, wherein a single product image is integrated into a\nuser-specified region within a user-specified image, ensur-\ning the preservation of the product\u2019s fine-grained details\nand its harmonious blend with the target image. A naive\napproach would be using the conventional PBE method,\nshown in Fig. 3. However, due to the information bottle-\nneck caused by PBE\u2019s reliance on only the [CLS] token for\nimage conditioning, it tends to lose significant details of the\nreference image, resulting in unsatisfactory performance.\nTo rectify the shortcomings of the PBE in preserving\nthe reference image\u2019s details, we introduce the \u201cDiffuse to\nChoose\u201d (DTC) method. DTC leverages an auxiliary U-\nNet alongside the primary U-Net within a latent diffusion\nmodel, specifically Stable Diffusion v1.5 [25]. The purpose\nof the auxiliary U-Net is to protect the details of the refer-\nence image that might be lost due to both the latent nature\nof the Stable Diffusion model and the limitations of image\nconditioning. To this end, we directly infuse fine-grained\ndetails of the reference image into the main U-Net\u2019s de-\ncoder via affine transformations, ensuring preservation of\nthe reference product\u2019s details in the generated image. Our\npipeline is shown in Fig. 2.\n3.1. Diffusion Inpainting Models\nFor the Vit-All inpainting task, our objective is as follows:\nGiven a user provided source image, xs \u2208 RH\u00d7W \u00d73, and\na user-defined mask, m, (is a binary matrix of dimension\n0, 1HxW ), with zeros indicating editable regions, and a ref-\nerence image xr, showcasing the desired product, the ob-\njective is to seamlessly incorporate the product image xr\nwithin the mask-defined region of xs, ensuring the preser-\nvation of xr\u2019s details. Diffusion models provide unparal-\nleled success in image generation and specific tasks such as\ninpainting [24, 25, 27, 31]. These models follow a Marko-\nvian process, gradually introducing noise, denoted as \u03f5 from\nN(0, 1), to xs over various timesteps t to transform it into\nan isotropic Gaussian distribution zt. The process is then\nreversed by iteratively predicting and subtracting the added\nnoise to convert zt back to xs, conditioned by c. In the con-\ntext of inpainting, this can be mathematically expressed as:\nL = Ezt,\u03f5,t\nh\n\u2225\u03f5\u03b8 ((m \u2299 xs), zt, t, c) \u2212 \u03f5\u22252\n2\ni\n(1)\nHere, xs is the source image, m the user-defined mask,\nzt the noise-added version of xs, and c denotes the embed-\ndings of xr. PBE uses the [CLS] token of CLIP [23] for\nc, a deliberate information bottleneck, because it relies on\nself-referencing, and additional patches often leads to copy-\npaste artifacts. However, for the Vit-All task, it is practi-\ncal to compile a dataset with distinct source and reference\nimages of the same object, thereby eliminating this bottle-\nneck. Consequently, we introduced a series of enhance-\nments to PBE to explore the upper limits of basic image-\nconditioned inpainting models in the Vit-All context and\nestablish a stronger baseline. Our modifications included\n4\nMask\nMasked Source\nU-Net \nencoder\nAdapter\nHint\n+\nVAE encoder\nSource\nFigure 4. Hint signal is stitched into a blank image within the\nmasked region, then summed up with latent masked input before\nfed into Auxiliary U-Net.\nusing all CLIP patches instead of just the [CLS] token, em-\nploying a larger image encoder DINOV2 [21], and adding\na refinement loss similar to [7] alongside the diffusion loss\ngiven in Eq. 1. Each of these alterations incrementally im-\nproved the performance of the PBE approach.\n3.2. Design of Diffuse to Choose\nCreating the Hint Signal.\nDrawing inspiration from\nControlNet [42], we propose the incorporation of a sec-\nondary U-Net encoder, which serves as a trainable replica\nof the main U-Net encoder.\nIn the ControlNet architec-\nture however, the secondary U-Net is integrated directly\ninto the main U-Net decoder, providing spatial condition-\ning. In contrast, DTC demonstrates that the secondary U-\nNet, rather than providing a spatial layout, can serve to\nguide the main U-Net by exerting a potent pixel-wise influ-\nence from the reference image during the decoding process.\nTo generate the hint signal, we start by creating an image of\nzeros, identical in size to the source image, xs \u2208 RH\u00d7W \u00d73.\nSubsequently, we resize the reference image and insert it\nwithin the designated mask coordinates within the image\nof zeros. The same mask is then applied to xs, and this\nmasked source image undergoes processing by the VAE en-\ncoder to yield a latent representation, sized 64\u00d764\u00d74. The\nHint image is subsequently processed by the Adapter mod-\nule \u2014a shallow convolutional network comprising four lay-\ners\u2014 to match with the dimensions of the masked latent in-\npaint image. Finally, the Hint image and the masked source\nare added elementwise to produce the final representation\nof the hint image, which is then processed by the replicated\nU-Net encoder. This process is not shown in Fig. 2 to keep\nit concise, but is illustrated in Fig. 4 for clarity. Through a\nseries of ablation studies, we demonstrate that maintaining\na distinct representation for the hint image at the pixel level,\nwhile keeping the inpaint image in a latent form, provides\ncomplementary signals that yield superior results.\nCombining Hint Signal with Main U-Net. The Stable Dif-\nfusion U-Net Encoder generates feature maps of varying\nresolutions at each level, consisting of 13 blocks, includ-\ning the middle layer. The direct addition of the Hint En-\ncoder\u2019s outputs to the skip connections of the Main U-Net\nEncoder at every level tends to result in a pronounced spatial\ninfluence from the Hint signal, which is often not spatially\naligned with the source image, thus negatively affecting the\nperformance. In addition to direct addition, we explore two\nstrategies for integrating the Hint Signal into the main U-\nNet: Using Feature-wise Linear Modulation (FiLM) [22],\nand Cross Attention, computed in a manner akin to [43].\nAmong these three approaches\u2014direct addition, FiLM, and\nCross Attention\u2014FiLM emerges as the most effective. We\nargue this is due to the image conditioning already capturing\nthe majority of low-level details from the reference image,\nwith mostly fine-grained details being absent. FiLM specif-\nically enhances those feature channels that are essential for\npreserving the fine-grained details of the reference image.\nHinting Strategies, Refinement Loss and Image En-\ncoder. Our objective is to convey pixel-level, fine-grained\ndetails from the reference image into the main U-Net, and\nthere are several methods to achieve this. One approach is\nto focus on high-frequency details by employing techniques\nlike Canny Edges or Holistically-Nested Edge Detection\n(HED) features. Alternatively, we can directly use the ref-\nerence image itself. In our experiments, we tested Canny\nedge extraction using the implementation in the OpenCV li-\nbrary [4], with minimum and maximum threshold values of\n30 and 140, and for slightly softer edges, we used the HED\nmodel [36]. Despite these strategies yielding comparable\nresults, directly using the reference image proved to be the\nmost effective as it conveys the entire spectrum of details\nfrom the reference image, rather than focusing solely on\nhigh-frequency details. Thus, instead of pre-filtering to only\nconvey the high-frequency details, it is a better approach to\nlet the FiLM layer decide the most important channels, thus\ncapturing the essential nuances of the reference image.\nFor our image encoder, we use DINOV2 [21], which out-\nputs 256\u00d71536 dimensional embeddings to represent a ref-\nerence image, which is then reduced to 256\u00d7768 by a train-\nable MLP layer. Finally, we utilized a perceptual loss using\na pre-trained VGGNet [30], which is computed by compar-\ning the feature maps from the first five layers of VGGNet\nfor both the source and the generated images, thereby im-\nplicitly ensuring the alignment of basic features like color.\n5\nFigure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\n6\n4. Experiments\n4.1. Dataset and Implementation Details\nData. We compiled an in-house training dataset composed\nof product images. Fortunately, e-commerce products often\nhave multiple images available, so, during training, we do\nnot need to adhere to the self-reference setting employed by\nPBE, where the reference image is derived from xs, lead-\ning to potential overfitting. However not all products yield\nuseful xs, xr pairs, as many product images feature only\nthe product against a white background. While these im-\nages are apt for use as xr, they are unsuitable for xs since\nwe require images of products in contextual settings (with\na natural background). To address this, we employed an in-\nhouse model to identify products that have at least one xs\nimage depicting the product in a natural setting, interacting\nwith other elements in the scene, and one image with the\nproduct itself xr, where we collect images against a white\nbackground if it exists. Finally, we use GroundingDINO\n[19] and SAM [15] alongside with the product type of the\nxs to create the inpainting mask within xs. From the result-\ning dataset, we sampled a sample training dataset of 1.2M\nsamples, evenly split between wearables and furniture. To\nensure accessibility and reproducibility, we also train and\ntest our model on a public dataset modified to remove model\nfaces, VITONHD-NoFace [5], which provides xr against a\nwhite background, masks, and xs where individuals (with\nremoved faces) are wearing xr.\nImplementation details. We use a latent diffusion model,\nStable Diffusion [25] V1.5 as our backbone in our exper-\niments. Our image resolution is 512 \u00d7 512 and we train\nour model with DDPM [11] using a constant learning rate\nof 1e \u2212 5 in both PBE and DTC implementations.\nWe\nuse simple augmentations like rotation and flip but avoid\nstrong augmentations given in [40], as we don\u2019t rely on\nself-referencing. We also use classifier free guidance [10]\nin similar fashion to [42]. During inference, we use DDIM\n[32] with guidance scale of 5, and for the hint input we stitch\nthe reference image into the largest rectangular bounding\nbox within the arbitrary-shape binary mask.\nWe use 8\nNVIDIA A100 40G GPUs to train our model for 40 epochs.\n4.2. Paint by Example Ablations\nTo ascertain the optimal performance of the naive image-\nconditioned inpainting approach [40] and create the\nstrongest possible baseline, we implemented a series of\nmodifications to the architecture illustrated in Fig. 3. Orig-\ninally, PBE utilized self-reference conditioning, which in-\nvolved cropping the xr from within xs. However, in the\nVit-All context, we circumvent this limitation by having\nseparate images for xr. As a result, instead of using only\nthe [CLS] token from CLIP [23] in the Image Encoder, we\nincorporated all CLIP patches alongside the [CLS] token.\nSubsequently, we increased the capacity of the image en-\ncoder by adopting DINOV2 [21], a larger and purely image-\nbased model. Furthermore, akin to [7], we integrated a per-\nceptual loss with the diffusion loss. This involved using an\nImageNet pre-trained VGGNet [30] to foster alignment of\nbasic features, such as color and certain textures, between\nthe generated and the source images. A qualitative example\nof this implementation is presented in Fig. 6.\nWithout Perceptual\nWith Perceptual Loss\nFigure 6. Effect of Perceptual Loss.\n4.3. Diffuse-to-Choose Ablations\nHint Pathway Ablations. It is possible to directly insert the\nreference image xr into the masked source image m \u2299 xs\nand process it with a VAE, circumventing the use of the\nadapter network to align the hint image\u2019s resolution with\nthe latent masked image prior to addition. However, this ap-\nproach produce suboptimal results, yielding a Clip Score of\n86.97 and an FID of 6.26 on our Vit-All dataset, in contrast\nto the non-latent Hint insertion\u2019s Clip Score of 88.14 and\nFID of 5.72. We hypothesize that maintaining the hint sig-\nnal at the pixel level could introduce additional information\nthat is overlooked in its VAE encoded latent counterpart.\nThis indicates that the VAE might be excluding certain fea-\ntures during the encoding process.\nAblations on Hinting Strategies. We explored alternatives\nto directly inserting the reference image into the masked\nsource image.\nThese alternatives included using Canny\nEdges and HED features, both of which are designed to con-\nvey the high-frequency details that are absent in image-only\nconditioning. However, we observed a slight underperfor-\nmance with both HED and Canny edges compared to the\ndirect use of the reference image. This was evidenced by\nthe CLIP scores, which were 87.85 for Canny and 86.98\nfor HED, compared to 88.14 for direct usage on our Vit-All\ndataset. Similarly, the FID scores were 6.11 for Canny and\n6.57 for HED, against 5.72 for direct insertion.\nAblations of Techniques for Integrating Hint Signal into\nthe Main U-Net. There are multiple ways to merge the\nsignals from the hint U-Net and the main U-Net before in-\ncorporating the combined signal into the main U-Net de-\ncoder. We explored three approaches: direct addition, the\nuse of an affine transformation layers FiLM [22], and the\n7\nTable 1.\nQuantitative comparison between DTC variants and\nPBEbest, which denotes a PBE variant using DINOv2 and percep-\ntual loss. CA denotes Cross-Attention.\nMethod\nCLIP Score (\u2191)\nFID (\u2193)\nPBEbest\n85.43\n6.65\nOursaddition\n86.94\n6.19\nOursCA\n88.01\n5.68\nOursFiLM\n88.14\n5.72\nintegration of more computationally expensive Cross Atten-\ntion layers [43]. Results shown on Tab. 1 revealed that both\nFiLM and Cross Attention layers outperform direct addi-\ntion. Also, Cross Attention and FiLM yield comparable re-\nsults, and FiLM is cheaper to compute, therefore we chose\nto use FiLM in our final model.\nFigure 7. Failure cases with generating fine-grained text.\n4.4. Evaluation and Comparisons\nComparison Against Paint by Example Variants.\nWe\nimplemented a series of enhancements to PBE and trained\neach variant on VITONHD-NoFace dataset. The results are\npresented in Table 2. As anticipated, using all CLIP patches\nsurpasses the performance of using only the [CLS] token,\nwhich is limited to encoding a generalized version of xr.\nFurthermore, augmenting the size of the image encoder by\nusing DINOv2 notably enhances performance. Notably, the\naddition of perceptual loss provides a marginal improve-\nment in scenarios where the model initially struggled with\nbasic features, such as color. While PBE, particularly with\nDINOv2 and perceptual loss, is adept at handling basic\nitems with minimal details, it often falls short in the inpaint-\ning of detailed items. In contrast, DTC exhibits superior\nperformance, especially in preserving the fine-grained de-\ntails of items. Figure 9 illustrates the outcomes achieved\nwith certain enhancements.\nComparisons Against Few-Shot Personalization Meth-\nTable 2. Quantitative comparison of PBE variants on VITONHD-\nNoFace [5].\nMethod\nCLIP Score (\u2191)\nFID (\u2193)\nPBE CLIPcls[40]\n82.43\n9.54\n+ PBE CLIPall\n84.01\n8.93\n+ PBE DINOv2\n87.48\n6.18\n+ PBE perceptual\n87.79\n5.93\nOurs\n90.14\n5.39\nTable 3. The average results of the small-scale human evaluation\nstudy. Similarity evaluates the resemblance of the inpainted re-\ngion to the reference image, while Semantic Blending assesses the\naccuracy of the reference image\u2019s integration within its context.\nMethod\nSimilarity (\u2193)\nSemantic Blending (\u2193)\nPBEbest\n3.7\n3.13\nDreamPaint [28]\n2.83\n2.53\nOurs\n2.9\n2.5\nods. While personalization methods such as DreamBooth\n[26] do not support inpainting, the recently introduced\nDreamPaint approach [28] enables similar few-shot fine-\ntuning of the U-Net in a masked setting, allowing for the\ngeneration of specified concepts at user-defined locations.\nHowever, DreamPaint requires few-shot fine-tuning with\nmultiple product images, taking about 40 minutes per prod-\nuct to be trained. We manually selected 30 samples to com-\npare DTC with DreamPaint and PBE. Visual comparisons\nare presented in Fig. 8. Furthermore, we conducted a sub-\njective human survey, the results of which are tabulated in\nTable 3. A total of 20 participants scored each image on\na scale from 1 to 5, with 1 being the best, based on both\nthe inpainted region\u2019s similarity to the reference image and\nits contextual blending. The results show that DTC, despite\nbeing a zero-shot model, performs on par with DreamPaint,\nwhich requires few-shot fine-tuning with multiple xr.\n5. Conclusion and Limitations\nLimitations. DTC has limitations. Despite our efforts to in-\nject fine-grained details, the model may still overlook fine-\ngrained details, particularly in text engravings, a challenge\ninherent to Stable Diffusion (see Fig. 7). Additionally, the\nmodel might alter human poses since it doesn\u2019t consider\npose, leading to discrepancies with pose-agnostic mask-\ning, especially for full-body coverage (see Fig. 10 in the\nAppendix). Introducing pose conditioning could mitigate\nthis, but we prioritized a general-purpose model over task-\nspecific auxiliary inputs for broader applicability.\nConclusion.\nIn this paper, we introduced \u201dDiffuse to\nChoose,\u201d a novel image-conditioned diffusion inpainting\nmodel designed for the Virtual Try-All, aiming to integrate\ne-commerce items into user images while preserving item\ndetails. Our main contributions include employing a sec-\nondary U-Net to infuse fine-grained signals from the ref-\nerence image into the primary U-Net decoder using basic\naffine transformation layers within a latent diffusion model.\nMoreover, we refined the PBE model for peak performance\nachievable with straightforward image-conditioned inpaint-\ning models. We compared DTC with upgraded PBE vari-\nants and a few-shot personalization methods using datasets\nlike VITONHD-NoFace and a larger in-house dataset and\nshow that DTC outperforms existing diffusion based in-\n8\nSource & Reference\nPBE Best\nDreamPaint\nDiffuse to Choose (Ours)\nFigure 8. Qualitative comparison against PBE and DreamPaint.\nsource & ref\nPBE\nPBE All CLIP Patches\nPBE DINOV2\nPBE DINOV2+perceptual\nOurs\nFigure 9. PBE variants\u2019 performance vs DTC\npainting approaches in the Virtual Try-All setting.\nReferences\n[1] Mathieu Aubry, Daniel Maturana, Alexei A Efros, Bryan C\nRussell, and Josef Sivic. Seeing 3d chairs: exemplar part-\nbased 2d-3d alignment using a large dataset of cad models.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 3762\u20133769, 2014. 2\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022. 4\n[3] Shuai Bai, Huiling Zhou, Zhikang Li, Chang Zhou, and\nHongxia Yang. Single stage virtual try-on via deformable\nattention flows. In European Conference on Computer Vi-\nsion, pages 409\u2013425. Springer, 2022. 2, 3\n[4] G. Bradski. The OpenCV Library. Dr. Dobb\u2019s Journal of\nSoftware Tools, 2000. 5\n[5] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul\nChoo.\nViton-hd:\nHigh-resolution virtual try-on via\nmisalignment-aware normalization.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 14131\u201314140, 2021. 2, 3, 7, 8\n[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2\n[7] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen\nQian, and Liqing Zhang.\nTaming the power of diffusion\nmodels for high-quality virtual try-on with appearance flow.\narXiv preprint arXiv:2308.06101, 2023. 2, 3, 5, 7\n[8] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S\nDavis. Viton: An image-based virtual try-on network. In\n9\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 7543\u20137552, 2018. 3\n[9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 4\n[10] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 7\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 7\n[12] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 2, 3\n[13] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 4\n[14] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-\nfusionclip: Text-guided diffusion models for robust image\nmanipulation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2426\u2013\n2435, 2022. 4\n[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 7, 1\n[16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2\n[17] Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan\nChoi, and Jaegul Choo. High-resolution virtual try-on with\nmisalignment and occlusion-handled conditions.\nIn Eu-\nropean Conference on Computer Vision, pages 204\u2013219.\nSpringer, 2022. 2, 3\n[18] Kathleen\nM\nLewis,\nSrivatsan\nVaradharajan,\nand\nIra\nKemelmacher-Shlizerman.\nTryongan: Body-aware try-on\nvia layered interpolation. ACM Transactions on Graphics\n(TOG), 40(4):1\u201310, 2021. 2, 3\n[19] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 7\n[20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 1\n[21] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 5, 7, 1\n[22] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In Proceedings of the AAAI con-\nference on artificial intelligence, 2018. 5, 7\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 4, 7, 1\n[24] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n4\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 4, 7, 1\n[26] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 3, 8\n[27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2, 4\n[28] Mehmet Saygin Seyfioglu, Karim Bouyarmane, Suren Ku-\nmar, Amir Tavanaei, and Ismail B Tutar. Dreampaint: Few-\nshot inpainting of e-commerce items for virtual try-on with-\nout 3d modeling. arXiv preprint arXiv:2305.01257, 2023. 2,\n3, 8\n[29] Ka\nChun\nShum,\nHong-Wing\nPang,\nBinh-Son\nHua,\nDuc Thanh Nguyen, and Sai-Kit Yeung. Conditional 360-\ndegree image synthesis for immersive indoor scene decora-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 4478\u20134488, 2023. 2\n[30] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 5, 7\n[31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n4\n[32] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 7\n[33] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[34] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price,\nJianming Zhang, Soo Ye Kim, and Daniel Aliaga. Object-\n10\nstitch: Object compositing with diffusion model.\nIn Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18310\u201318319, 2023. 4\n[35] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2\n[36] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In Proceedings of the IEEE international conference\non computer vision, pages 1395\u20131403, 2015. 5\n[37] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye\nDong, Xijin Zhang, Feida Zhu, and Xiaodan Liang.\nGp-\nvton: Towards general purpose virtual try-on via collabora-\ntive local-flow global-parsing learning.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 23550\u201323559, 2023. 3\n[38] Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang\nZhao, and Xing Tang. Dccf: Deep comprehensible color fil-\nter learning framework for high-resolution image harmoniza-\ntion. In European Conference on Computer Vision, pages\n300\u2013316. Springer, 2022. 2, 4, 3\n[39] Keyu Yan, Tingwei Gao, Hui Zhang, and Chengjun Xie.\nLinking garment with person via semantically associated\nlandmarks for virtual try-on.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17194\u201317204, 2023. 3\n[40] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18381\u201318391,\n2023. 2, 4, 7, 8, 3\n[41] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zis-\nserman. What does stable diffusion know about the 3d scene?\narXiv preprint arXiv:2310.06836, 2023. 2\n[42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2, 5, 7\n[43] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William\nChan, Chitwan Saharia, Mohammad Norouzi, and Ira\nKemelmacher-Shlizerman.\nTryondiffusion: A tale of two\nunets. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 4606\u20134615,\n2023. 3, 5, 8\n11\nDiffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion\nModels for Virtual Try-All\nSupplementary Material\n5.1. Masking Strategy During Training and Infer-\nence\nDuring training, with equal probability, we alternate be-\ntween a fine-grained mask (where we only mask the item\nspecifically) and bounding box shaped masks (covering the\nlargest bounding box spanned by the fine-grained mask).\nFor each case, we stitch the reference image within the\nlargest rectangular shape inside the mask. This approach\nis straightforward in the case of rectangular masks. How-\never, for fine-grained masks, we calculate the largest rect-\nangular area within the binary mask. Initially, we construct\na histogram for each row of the matrix, with each entry in\nthe histogram representing the cumulative height of masked\nareas in the column up to that row. We then calculate the\nmaximum area rectangle that can be formed in each his-\ntogram, updating the coordinates of the largest rectangle as\nwe iterate through the rows. This process ultimately yields\nthe top-left and bottom-right coordinates of the largest rect-\nangle fitting inside the mask. An example is shown in Fig.\n11. During inference, we stitch the hint image within the\nlargest rectangular region of the mask.\nGarment-Only Mask\nPose Agnostic Mask\nFigure 10. Pose agnostic masking case.\n5.2. Implementation Details and Inference Perfor-\nmance\nIn all our experiments, we used Stable Diffusion v1-5 [25].\nFor our image encoder, we employed DINOV2 [21], which\noutputs 1536-dimensional vectors for every patch of the ref-\nerence image, of shape 224 \u00d7 224 \u00d7 3.\nThus, it yields\n256 \u00d7 1536-dimensional outputs.\nAdditionally, we ap-\npended the CLS token to obtain 257 \u00d7 1536 image condi-\ntioning vectors. Subsequently, these vectors were processed\nthrough a single layer of a fully connected network, which\nwas trained from scratch, to reduce them to 257 \u00d7 768 di-\nmensions. We trained our model using AdamW [20] with a\nconstant learning rate of 1e \u2212 5 and used horizontal flip and\nrotation as augmentations. To calculate the CLIP score, we\nused ViT-B/32 [23]. Finally, the model is efficient in infer-\nence, taking \u2248 6 seconds to run a single pass on an A100\n(40GB) GPU with 40 DDIM steps.\n5.3. The effect of masking\nSince our approach relies on collages, the mask serves as\na strong prior for the DTC model. As illustrated in Fig.\n12, the use of masking enables users during inference to\nmanipulate clothing styles. Consequently, users can guide\nthe model to generate a t-shirt in a tucked-in style, or with\nsleeves rolled up, among other variations.\n5.4. Iterative Inpainting\nDTC enables a range of enjoyable applications.\nFor in-\nstance, users can begin with an empty room and iteratively\ndecorate it, designing as shown in Fig. 13. The same prin-\nciple applies to clothing; users can generate multiple items\nof clothing in combination with one another to experiment\nwith different outfit combinations, as shown in Fig. 14.\n5.5. Visualization of Hint Signal\nAs mentioned, in addition to direct stitching, we also uti-\nlized Canny Edges and HED edges on our hint pathway, as\ndemonstrated in Fig. 16. For Canny Edges, we used sobel\nfilters for each color channel independently and then com-\nbined the results to obtain RGB edge information, which\nwe believed could more effectively convey the details of e-\ncommerce items.\n5.6. More on Limitations\nFor certain items, such as shoes, the model frequently fails\nto generate satisfactory results. We argue that this issue\nstems from SAM\u2019s [15] inability to generate appropriate\nmasks specifically for shoes or, more broadly, for items pre-\nsented in pairs. SAM often masks only one shoe of a pair,\nleading the model to learn shortcut features from the un-\nmasked shoe during training, rather than acquiring useful,\ngeneralizable features. And as mentioned, since we use a la-\ntent diffusion model as our backbone, no matter how much\nextra information we guide it with, we are subject to the ca-\npacity of VAE decoder, which often fails to generate very\nfine grained concepts like detailed engravings etc.\n5.7. Comparison Against Other Methods.\nMost state-of-the-art GAN-based methods are tailored for\nsingle-domain applications, such as virtual try-ons in con-\n1\nResize & \nStitch\nReference Image\nFind the \nlargest \nbbox\nMask\nInsert \nbbox into \nan image \nof zeros\nFigure 11. We find the largest rectangular bounding box inside an fine-grained binary mask. Then the same coordinates are used to stitch\nthe reference image into an image of zeros to create the initial hint signal.\nFigure 12. DTC allows users to manipulate different styles of the same clothing by adjusting the mask (given in the row above for each\nimage). The first two columns display variations of the same t-shirt, showcasing it both tucked out and tucked in. The third and fourth\ncolumns illustrate the same shirt with normal sleeves and with sleeves rolled up.\ntrolled environments with sanitized backgrounds, and often\nnecessitate additional inputs like pose or depth maps. Also,\nit is already established that diffusion-based approaches are\nsuperior to GANs in performance, possessing more compre-\nhensive world models [41]. Consequently, diffusion-based\nmodels are more apt for the Vit-All use case.\nThe scope of our comparison models is intentionally lim-\nited. Personalization models such as ELITE [35], Custom\nDiffusion [16], DreamBooth [26], and Textual Inversion [6]\nlack inpainting capabilities, as they aim to directly generate\nentire views. DreamPaint [28] is the only exception with in-\npainting support. Among the models that facilitate inpaint-\n2\nFigure 13. Diffuse to Choose allows users to iteratively decorate an empty room from scratch.\nFigure 14. Diffuse to Choose allows users to iteratively try out combination of clothes.\ning, including PBE [40] and DreamPaint, we attempted to\nemploy DCCF [38]. However, its tendency to create copy-\npaste artifacts made it unsuitable for the Vit-All task, where\nsemantically blending the item with its environment is as\ncrucial as preserving its detailed features.\n5.8. More Examples\nWe provide more qualitative examples to showcase DTC\u2019s\ncapabilities. Please see Fig. 17, 18, 15 for more examples.\n3\nFigure 15. Some clothing try-on results. Note that DTC can handle in the wild reference and source images.\n4\nReference Image\nCanny Edges\nHED\nFigure 16. Different reference image representations that we use on hint pathway.\n5\nFigure 17. Additional examples showcasing different products. Note that DTC can infer how the product should look like, given a zero-shot\nexample.\n6\nFigure 18. Additional examples showcasing different products.\n7\n"
  },
  {
    "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
    "link": "https://arxiv.org/pdf/2401.14196.pdf",
    "upvote": "41",
    "text": "DeepSeek-Coder: When the Large Language Model Meets\nProgramming - The Rise of Code Intelligence\nDaya Guo*1, Qihao Zhu\u22171,2, Dejian Yang1, Zhenda Xie1, Kai Dong1, Wentao Zhang1\nGuanting Chen1, Xiao Bi 1, Y. Wu1, Y.K. Li1, Fuli Luo1, Yingfei Xiong2, Wenfeng Liang1\n1DeepSeek-AI\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\n{zhuqh, guodaya}@deepseek.com\nhttps://github.com/deepseek-ai/DeepSeek-Coder\nAbstract\nThe rapid development of large language models has revolutionized code intelligence in\nsoftware development. However, the predominance of closed-source models has restricted\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\ntrillion tokens. These models are pre-trained on a high-quality project-level code corpus and\nemploy a fill-in-the-blank task with a 16K window to enhance code generation and infilling.\nOur extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art\nperformance among open-source code models across multiple benchmarks but also surpasses\nexisting closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted commercial use.\nFigure 1 | The Performance of DeepSeek-Coder\n*Core contributors, ordered alphabetically by the name.\narXiv:2401.14196v2  [cs.SE]  26 Jan 2024\n1. Introduction\nThe field of software development has been significantly transformed by the swift advancement\nof large language models (OpenAI, 2023; Touvron et al., 2023), which have brought about\na new era of code intelligence. These models have the potential to automate and streamline\nmany aspects of coding, from bug detection to code generation, thereby enhancing productivity\nand reducing the likelihood of human error. However, a major challenge in this field is the\nperformance gap between open-source models (Li et al., 2023; Nijkamp et al., 2022; Roziere\net al., 2023; Wang et al., 2021) and closed-source models (Gemini Team, 2023; OpenAI, 2023).\nThe giant closed-source models, while powerful, are often inaccessible to many researchers and\ndevelopers due to their proprietary nature.\nIn response to this challenge, we present the DeepSeek-Coder series. This series comprises\na range of open-source code models, varying in size from 1.3B to 33B, including the base\nversion and instructed version for each size. Each model in the series has been trained from\nscratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive\nunderstanding of coding languages and syntax. Besides, we attempt to organize the pre-\ntraining data at the repository level to enhance the pre-trained model\u2019s understanding capability\nwithin the context of cross-files within a repository. In addition to employing the next token\nprediction loss during pre-training, we have also incorporated the Fill-In-Middle (FIM) approach\n(Bavarian et al., 2022; Li et al., 2023). This approach is designed to further bolster the model\u2019s\ncode completion capabilities. To meet the requirements of handling longer code inputs, we\nhave extended the context length to 16K. This adjustment allows our models to handle more\ncomplex and extensive coding tasks, thereby increasing their versatility and applicability in\nvarious coding scenarios.\nWe have carried out comprehensive experiments using a variety of public code-related\nbenchmarks. The findings reveal that among open-source models, DeepSeek-Coder-Base 33B\nconsistently delivers superior performance across all benchmarks. Furthermore, DeepSeek-\nCoder-Instruct 33B surpasses OpenAI GPT-3.5 Turbo in the majority of the evaluation benchmarks,\nsignificantly narrowing the performance gap between OpenAI GPT-4 and open-source models.\nRemarkably, despite having fewer parameters, DeepSeek-Coder-Base 7B demonstrates competi-\ntive performance when compared to models that are five times larger, such as CodeLlama-33B\n(Roziere et al., 2023). To summarize, our main contributions are:\n\u2022 We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code-\nfocused large language models (LLMs). Developed through extensive training on an\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\nlanguages. Additionally, they are available in various model scales to cater to a wide range\nof computational and application needs.\n\u2022 We make the first attempt to incorporate repository-level data construction during the\npre-training phase of our models. We find that it can significantly boost the capability of\ncross-file code generation.\n\u2022 Our analysis rigorously examines the impact of FIM training strategies on the pretraining\nphase of code models. The outcomes of these comprehensive studies shed light on intriguing\naspects of FIM configurations, offering valuable insights that significantly contribute to the\nenhancement and development of code pretrained models.\n\u2022 We conduct extensive evaluations of our code LLMs against a wide array of benchmarks en-\ncompassing numerous code-related tasks. The findings demonstrate that DeepSeek-Coder-\nBase surpasses all existing open-source code LLMs across these benchmarks. Furthermore,\n2\nwith meticulous fine-tuning using instructional data, DeepSeek-Coder-Instruct achieves\nbetter performance compared to the OpenAI GPT-3.5 Turbo model in code-related tasks.\n2. Data Collection\nThe training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code-\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\nEnglish corpus consists of materials from GitHub\u2019s Markdown and StackExchange1, which are\nused to enhance the model\u2019s understanding of code-related concepts and improve its ability\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\nhigh-quality articles aimed at improving the model\u2019s proficiency in understanding the Chinese\nlanguage. In this section, we will provide an overview of how we construct the code training\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository-\nlevel deduplication, and quality screening, as illustrated in Figure 2. In the following, we will\ndescribe the data creation procedure step by step.\nRule \nFiltering\nDependency \nParsing\nRepo-level \nDeduplication\nQuality \nScreening\nData \nCrawling\nFigure 2 | The Procedure of Dataset Creation\n2.1. GitHub Data Crawling and Filtering\nWe collect public repositories created before February 2023 on GitHub and retain only 87\nprogramming languages, as listed in Table 1. To reduce the amount of data to be processed, we\napply filtering rules similar to those used in the StarCoder project (Li et al., 2023) to preliminarily\nfilter out lower-quality code. By applying these filtering rules, we reduce the total amount of\ndata to only 32.8% of its original size. To make the paper self-contained, we briefly describe the\nfilter rules used in the StarCoder Data project:\nFirstly, we filter out files with an average line length exceeding 100 characters or a maximum\nline length surpassing 1000 characters. Additionally, we remove files with fewer than 25%\nalphabetic characters. Except for the XSLT programming language, we further filter out files\nwhere the string \"<?xml version=\" appeared in the first 100 characters. For HTML files, we\nconsider the ratio of visible text to HTML code. We retain files where the visible text constitutes\nat least 20% of the code and is no less than 100 characters. For JSON and YAML files, which\ntypically contain more data, we only keep files that have a character count ranging from 50 to\n5000 characters. This effectively removes most data-heavy files.\n2.2. Dependency Parsing\nIn previous works (Chen et al., 2021; Li et al., 2023; Nijkamp et al., 2022; Roziere et al., 2023),\nlarge language models for code are mainly pre-trained on file-level source code, which ignores\nthe dependencies between different files in a project. However, in practical applications, such\nmodels struggle to effectively scale to handle entire project-level code scenarios. Therefore, we\n1https://stackexchange.com\n3\nAlgorithm 1 Topological Sort for Dependency Analysis\n1: procedure T O P O L O G I C A LS O R T( \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc60)\n2:\n\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e\ud835\udc60 \u2190 {}\n\u22b2 Initialize an empty adjacency list\n3:\n\ud835\udc56\ud835\udc5b\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52 \u2190 {}\n\u22b2 Initialize an empty dictionary for in-degrees\n4:\nfor each \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52 in \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc60 do\n5:\n\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e\ud835\udc60[ \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52] \u2190 []\n6:\n\ud835\udc56\ud835\udc5b\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52[ \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52] \u2190 0\n7:\nend for\n8:\n9:\nfor each \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc34 in \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc60 do\n10:\nfor each \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc35 in \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc60 do\n11:\nif H A SD E P E N D E N C Y( \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc34, \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc35) then\n\u22b2 If fileA depends on fileB\n12:\n\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e\ud835\udc60[ \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc35].append( \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc34)\n\u22b2 Add edge from B to A\n13:\n\ud835\udc56\ud835\udc5b\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52[ \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc34] \u2190 \ud835\udc56\ud835\udc5b\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52[ \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc34] + 1\n\u22b2 Increment in-degree of A\n14:\nend if\n15:\nend for\n16:\nend for\n17:\n18:\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e\ud835\udc60 \u2190 getDisconnectedSubgraphs(\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e\ud835\udc60)\n\u22b2 Identify disconnected subgraphs\n19:\n\ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60 \u2190 []\n20:\nfor each \ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e in \ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e\ud835\udc60 do\n21:\n\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60 \u2190 []\n22:\nwhile length(\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60) \u2260 NumberOfNodes(\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e) do\n23:\n\ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52 \u2190 argmin({\ud835\udc56\ud835\udc5b\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52[ \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52] | \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52 \u2208 \ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e and \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52 \u2209 \ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60})\n24:\nfor each \ud835\udc5b\ud835\udc5c\ud835\udc51\ud835\udc52 in \ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e\ud835\udc60[ \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52] do\n25:\n\ud835\udc56\ud835\udc5b\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52[\ud835\udc5b\ud835\udc5c\ud835\udc51\ud835\udc52] \u2190 \ud835\udc56\ud835\udc5b\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52[\ud835\udc5b\ud835\udc5c\ud835\udc51\ud835\udc52] \u2212 1\n26:\nend for\n27:\n\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60.append( \ud835\udc53 \ud835\udc56\ud835\udc59\ud835\udc52)\n28:\nend while\n29:\n\ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60.append(\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60)\n30:\nend for\n31:\n32:\nreturn \ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60\n33: end procedure\nwill consider how to leverage the dependencies between files within the same repository in\nthis step. Specifically, we first parse the dependencies between files and then arrange these\nfiles in an order that ensures the context each file relies on is placed before that file in the\ninput sequence. By aligning the files in accordance with their dependencies, our dataset more\naccurately represents real coding practices and structures. This enhanced alignment not only\nmakes our dataset more relevant but also potentially increases the practicality and applicability\nof the model in handling project-level code scenarios. It\u2019s worth noting that we only consider\nthe invocation relationships between files and use regular expressions to extract them, such as\n\"import\" in Python, \"using\" in C#, and \"include\" in C.\nThe algorithm 1 describes a topological sort for dependency analysis on a list of files within\nthe same project. Initially, it sets up two data structures: an empty adjacency list named \"graphs\"\nto represent dependencies between files and an empty dictionary called \"inDegree\" for storing\nthe in-degrees of each file. The algorithm then iterates over each file pair to identify depen-\n4\ndencies, updating \"graphs\" and \"inDegree\" accordingly. Next, it identifies any disconnected\nsubgraphs within the overall dependency graph. For each subgraph, the algorithm employs a\nmodified topological sort. Unlike the standard approach that selects nodes with zero in-degrees,\nthis algorithm selects nodes with minimal in-degrees, which allows it to handle cycles within\nthe graph. Selected nodes are added to a \"results\" list, and the in-degrees of their connected\nnodes are decreased. This process continues until a topologically sorted sequence is generated\nfor each subgraph. The algorithm concludes by returning a list of these sorted sequences, and\neach sequence\u2019s files are concatenated to form a single training sample. To incorporate file path\ninformation, a comment indicating the file\u2019s path is added at the beginning of each file. This\nmethod ensures that the path information is preserved in the training data.\n2.3. Repo-Level Deduplication\nRecent studies have demonstrated the significant performance improvements that can be\nachieved by deduplicating training datasets for Large Language Models (LLMs). Lee et al. (2022)\nhave shown that language model training corpora often contain numerous near-duplicates, and\nthe performance of LLMs can be enhanced by removing long repetitive substrings. Kocetkov\net al. (2022) have applied a near-deduplication method to training data, resulting in dramatic\nimprovements, and they emphasize that near-deduplication is a crucial preprocessing step for\nachieving competitive performance on code benchmark tasks. In our dataset, we have also\nemployed near-deduplication. However, there is a distinction in our approach compared to\nprevious works. We perform deduplication at the repository level of code, rather than at the file\nlevel, as the latter approach may filter out certain files within a repository, potentially disrupting\nthe structure of the repository. Specifically, we treat the concatenated code from the repository\nlevel as a single sample and apply the same near-deduplication algorithm to ensure the integrity\nof the repository structure.\n2.4. Quality Screening and Decontamination\nIn addition to applying the filtering rules mentioned in Section 2.1, we also employ a compiler\nand a quality model, combined with heuristic rules, to further filter out low-quality data. This\nincludes code with syntax errors, poor readability, and low modularity. We provide the statistical\nsummary of source code in Table 1, which includes a total of 87 languages, detailing the disk\nsize, number of files, and percentage for each language. The total data volume is 798 GB with\n603 million files. To ensure that our code training data is not contaminated by information\nfrom the test set, which may be present on GitHub, we\u2019ve implemented an n-gram filtering\nprocess. This process involves the removal of any code segments that match specific criteria.\nSpecifically, we filter out files containing docstrings, questions, and solutions from sources such\nas HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), GSM8K (Cobbe et al., 2021) and\nMATH (Hendrycks et al., 2021). For the filtering criteria, we apply the following rules: if a\npiece of code includes a 10-gram string identical to any in the test data, it is excluded from our\ntraining data. In cases where the test data comprises strings that are shorter than 10-grams but\nno less than 3-grams, we use an exact match approach for filtering.\n5\nLanguage\nSize (GB)\nFiles (k)\nProp. (%)\nLanguage\nSize (GB)\nFiles (k)\nProp. (%)\nAda\n0.91\n126\n0.11\nLiterate Haskell\n0.16\n20\n0.02\nAgda\n0.26\n59\n0.03\nLua\n0.82\n138\n0.10\nAlloy\n0.07\n24\n0.01\nMakefile\n0.92\n460\n0.12\nANTLR\n0.19\n38\n0.02\nMaple\n0.03\n6\n0.00\nAppleScript\n0.03\n17\n0.00\nMathematica\n0.82\n10\n0.10\nAssembly\n0.91\n794\n0.11\nMATLAB\n0.01\n1\n0.00\nAugeas\n0.00\n1\n0.00\nOCaml\n0.91\n139\n0.11\nAWK\n0.09\n53\n0.01\nPascal\n0.79\n470\n0.10\nBatchfile\n0.92\n859\n0.12\nPerl\n0.81\n148\n0.10\nBluespec\n0.10\n15\n0.01\nPHP\n58.92\n40,627\n7.38\nC\n28.64\n27,111\n3.59\nPowerShell\n0.91\n236\n0.11\nC#\n58.56\n53,739\n7.34\nProlog\n0.03\n5\n0.00\nClojure\n0.90\n295\n0.11\nProtocol Buffer\n0.92\n391\n0.12\nCMake\n0.90\n359\n0.11\nPython\n120.68\n75,188\n15.12\nCoffeeScript\n0.92\n361\n0.12\nR\n0.92\n158\n0.11\nCommon Lisp\n0.92\n105\n0.11\nRacket\n0.09\n13\n0.01\nC++\n90.87\n36,006\n11.39\nRMarkdown\n6.83\n1,606\n0.86\nCSS\n5.63\n11,638\n0.71\nRuby\n15.01\n18,526\n1.88\nCUDA\n0.91\n115\n0.11\nRust\n0.61\n692\n0.08\nDart\n0.89\n264\n0.11\nSAS\n0.92\n70\n0.11\nDockerfile\n0.04\n48\n0.00\nScala\n0.81\n971\n0.10\nElixir\n0.91\n549\n0.11\nScheme\n0.92\n216\n0.12\nElm\n0.92\n232\n0.12\nShell\n13.92\n10,890\n1.74\nEmacs Lisp\n0.91\n148\n0.11\nSmalltalk\n0.92\n880\n0.12\nErlang\n0.92\n145\n0.12\nSolidity\n0.85\n83\n0.11\nF#\n0.91\n340\n0.11\nSparql\n0.10\n88\n0.01\nFortran\n1.67\n654\n0.21\nSQL\n15.14\n7,009\n1.90\nGLSL\n0.92\n296\n0.11\nStan\n0.20\n41\n0.03\nGo\n2.58\n1,365\n0.32\nStandard ML\n0.74\n117\n0.09\nGroovy\n0.89\n340\n0.11\nStata\n0.91\n122\n0.11\nHaskell\n0.87\n213\n0.11\nSystemVerilog\n0.91\n165\n0.11\nHTML\n30.05\n14,998\n3.77\nTCL\n0.90\n110\n0.11\nIdris\n0.11\n32\n0.01\nTcsh\n0.17\n53\n0.02\nIsabelle\n0.74\n39\n0.09\nTex\n20.46\n2,867\n2.56\nJava\n148.66\n134,367\n18.63\nThrift\n0.05\n21\n0.01\nJava Server Pages\n0.86\n1072\n0.11\nTypeScript\n60.62\n62,432\n7.60\nJavaScript\n53.84\n71,895\n6.75\nVerilog\n0.01\n1\n0.00\nJSON\n4.61\n11956\n0.58\nVHDL\n0.85\n392\n0.11\nJulia\n0.92\n202\n0.12\nVisual Basic\n0.75\n73\n0.09\nJupyter Notebook\n14.38\n2,555\n1.80\nXSLT\n0.36\n48\n0.04\nKotlin\n6.00\n3,121\n0.75\nYacc\n0.72\n67\n0.09\nLean\n0.52\n68\n0.07\nYAML\n0.74\n890\n0.09\nLiterate Agda\n0.05\n4\n0.01\nZig\n0.81\n70\n0.10\nLiterate CoffeeScript\n0.01\n3\n0.00\nTotal\n797.92\n603,173\n100.00\nTable 1 | A summary of the cleaned training data for the selected programming languages.\n3. Training Policy\n3.1. Training Strategy\n3.1.1. Next Token Prediction\nThe first training objective for our model is known as next token prediction. In this process,\nvarious files are concatenated to form a fixed-length entry. Then, these entries are used to train\nthe model, enabling it to predict the subsequent token based on the provided context.\n6\n3.1.2. Fill-in-the-Middle\nThe second training objective for our model is known as fill-in-the-middle. In the code pre-training\nscenario, it is often necessary to generate corresponding inserted content based on the given\ncontext and subsequent text. Due to specific dependencies in a programming language, relying\nsolely on next token prediction is insufficient to learn this fill-in-the-middle capability. Therefore,\nseveral approaches (Bavarian et al., 2022; Li et al., 2023) propose the pretraining method of\nFill-in-the-Midlle (FIM). This approach involves randomly dividing the text into three parts,\nthen shuffling the order of these parts and connecting them with special characters. This method\naims to incorporate a fill-in-the-blank pretraining task during the training process. Within the\nFIM methodology, two distinct modes are employed: PSM (Prefix-Suffix-Middle) and SPM\n(Suffix-Prefix-Middle). In the PSM mode, the training corpus is organized in the sequence\nof \ud835\udc43\ud835\udc5f\ud835\udc52 \ud835\udc53 \ud835\udc56\ud835\udc65, \ud835\udc46\ud835\udc62 \ud835\udc53 \ud835\udc53 \ud835\udc56\ud835\udc65, \ud835\udc40\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc59\ud835\udc52, aligning the text in a way that the middle segment is flanked by the\nprefix and suffix. Conversely, the SPM mode arranges the segments as \ud835\udc46\ud835\udc62 \ud835\udc53 \ud835\udc53 \ud835\udc56\ud835\udc65, \ud835\udc43\ud835\udc5f\ud835\udc52 \ud835\udc53 \ud835\udc56\ud835\udc65, \ud835\udc40\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc59\ud835\udc52,\npresenting a different structural challenge. These modes are instrumental in enhancing the\nmodel\u2019s capability to handle various structural arrangements in code, providing a robust training\nframework for advanced code prediction tasks.\n0\n2000\n4000\n6000\n8000\nSteps\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nValue\nBenchmark/HumanEval-Pass@1\n0\n2000\n4000\n6000\n8000\nSteps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nValue\nBenchmark/HumanFIM-Pass@1\n0\n2000\n4000\n6000\n8000\nSteps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nValue\nBenchmark/MBPP-Pass@1\nfim_0\nfim_0.5\nfim_1.0\nmsp_0.5\nFigure 3 | The effectiveness of using FIM objective.\nTo determine the effectiveness of various hyperparameters within the FIM approach, we\nconducted a series of ablation experiments.\nExperiment Settings: In this experiment, we employ DeepSeek-Coder-Base 1.3B as our model\narchitecture. We focused on a Python subset from our training dataset to streamline the ex-\nperimental process. Our primary objective was to assess the efficacy of the Fill-in-the-Middle\n(FIM) technique, utilizing the HumanEval-FIM benchmark (Fried et al., 2022). This benchmark\nspecializes in a single-line FIM task for Python, in which one line of code from a HumanEval\nsolution is randomly obscured, testing the model\u2019s proficiency in predicting the missing line.\nWe hypothesize that the PSM mode may exhibit subtle differences compared to the traditional\nnext-token prediction objective. This is primarily because PSM involves rearranging the order\nof the original text, potentially impacting the learning dynamics of the model. Therefore, we\nimplement the PSM mode for FIM across four distinct configurations: 0% FIM rate, 50% FIM\nrate, 100% FIM rate, and 50% MSP rate. The Masked Span Prediction (MSP) strategy, initially\nintroduced in T5 (Raffel et al., 2023), conceals multiple text spans and trains the model to recon-\nstruct these segments. According to CodeGen2.5 (Nijkamp et al., 2023), MSP may enhance FIM\nperformance compared to PSM. Thus, we include this method in our comparative analysis.\nResults: The outcomes of our experiment are illustrated in Figure 3. While the model demon-\nstrates peak performance on the HumanEval-FIM with a 100% FIM rate, this configuration also\nresults in the weakest code completion capability. This indicates a trade-off between FIM and\n7\ncode completion abilities. Moreover, we observe that with a 50% PSM rate, the model outper-\nforms the MSP strategy. To achieve a balance between FIM efficiency and code completion\nproficiency, we ultimately choose the 50% PSM rate as our preferred training policy.\nIn our implementation, we have introduced three sentinel tokens specifically for this task.\nFor each code file, we initially divide its content into three segments, denoted as \ud835\udc53\ud835\udc5d\ud835\udc5f\ud835\udc52, \ud835\udc53\ud835\udc5a\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc59\ud835\udc52, and\n\ud835\udc53\ud835\udc60\ud835\udc62 \ud835\udc53. Using the PSM mode, we construct the training example as follows:\n<\uff5cfim_start\uff5c> \ud835\udc53\ud835\udc5d\ud835\udc5f\ud835\udc52<\uff5cfim_hole\uff5c> \ud835\udc53\ud835\udc60\ud835\udc62 \ud835\udc53<\uff5cfim_end\uff5c> \ud835\udc53\ud835\udc5a\ud835\udc56\ud835\udc51\ud835\udc51\ud835\udc59\ud835\udc52<|eos_token|>\nWe implement the Fill-in-the-Middle (FIM) method at the document level before the packing\nprocess, as proposed in the original work by Bavarian et al. (2022). This is done with an FIM\nrate of 0.5, following the PSM mode.\n3.2. Tokenizer\nFor the tokenization process, we employ the HuggingFace Tokenizer library2 to train Byte Pair\nEncoding (BPE) tokenizers, as outlined in Sennrich et al. (2015) (Sennrich et al., 2015), on a\nsubset of our training corpus. Ultimately, we utilize a tokenizer configured with a vocabulary\nsize of 32,000.\n3.3. Model Architecture\nWe develop a range of models with varying parameters to cater to diverse applications, including\nmodels with 1.3B, 6.7B, and 33B parameters. These models are built upon the same framework\nas the DeepSeek Large Language Model (LLM) outlined by DeepSeek-AI (2024). Each model is a\ndecoder-only Transformer, incorporating Rotary Position Embedding (RoPE) as described by Su\net al. (2023). Notably, the DeepSeek 33B model integrates Grouped-Query-Attention (GQA) with\na group size of 8, enhancing both training and inference efficiency. Additionally, we employ\nFlashAttention v2 (Dao, 2023) to expedite the computation involved in the attention mechanism.\nThe architectural details of our models are summarized in Table 2.\n3.4. Optimization\nFollowing DeepSeek LLM (DeepSeek-AI, 2024), we use AdamW (Loshchilov and Hutter, 2019)\nas the optimizer with \ud835\udefd1 and \ud835\udefd2 values of 0.9 and 0.95. We adapt batch sizes and learning rates by\nthe scaling laws suggested in DeepSeek LLM. For the learning rate scheduling, we implement a\nthree-stage policy, which includes 2000 warm-up steps, and set the final learning rate to 10% of\nthe initial rate. Notably, the learning rate at each stage is scaled down to\n\u221a\ufe03\n1\n10 of the preceding\nstage\u2019s rate, following the guidelines established in DeepSeek LLM (DeepSeek-AI, 2024).\n3.5. Environments\nOur experiments are conducted using the HAI-LLM (High-Flyer, 2023) framework, known for\nits efficiency and lightweight approach in training large language models. This framework\nincorporates a variety of parallelism strategies to optimize computational efficiency. These in-\nclude tensor parallelism (Korthikanti et al., 2023), alongside ZeRO data parallelism (Rajbhandari\net al., 2020) and PipeDream pipeline parallelism (Narayanan et al., 2019). Our experiments\n2https://github.com/huggingface/tokenizers\n8\nHyperparameter\nDeepSeek-Coder 1.3B\nDeepSeek-Coder 6.7B\nDeepSeek-Coder 33B\nHidden Activation\nSwiGLU\nSwiGLU\nSwiGLU\nHidden size\n2048\n4096\n7168\nIntermediate size\n5504\n11008\n19200\nHidden layers number\n24\n32\n62\nAttention heads number\n16\n32\n56\nAttention\nMulti-head\nMulti-head\nGrouped-query (8)\nBatch Size\n1024\n2304\n3840\nMax Learning Rate\n5.3e-4\n4.2e-4\n3.5e-4\nTable 2 | Hyperparameters of DeepSeek-Coder.\nutilize clusters outfitted with NVIDIA A100 and H800 GPUs. In the A100 cluster, each node\nis configured with 8 GPUs, interconnected in pairs using NVLink bridges. The H800 cluster is\nsimilarly arranged, with each node containing 8 GPUs. These GPUs are interconnected using\na combination of NVLink and NVSwitch technologies, ensuring efficient data transfer within\nnodes. To facilitate seamless communication between nodes in both A100 and H800 clusters, we\nemploy InfiniBand interconnects, known for their high throughput and low latency. This setup\nprovides a robust and efficient infrastructure for our computational experiments.\n3.6. Long Context\nTo enhance the capabilities of DeepSeek-Coder in handling extended contexts, particularly for\nscenarios like repository-level code processing, we have reconfigured the RoPE (Su et al., 2023)\nparameters to extend the default context window. Following previous practices (Chen et al.,\n2023; kaiokendev, 2023), we employed a linear scaling strategy, increasing the scaling factor from\n1 to 4 and altering the base frequency from 10000 to 100000. The model underwent an additional\n1000 steps of training, using a batch size of 512 and a sequence length of 16K. The learning rate\nwas maintained as in the final pre-training phase. Theoretically, these modifications enable our\nmodel to process up to 64K tokens in context. However, empirical observations suggest that\nthe model delivers its most reliable outputs within a 16K token range. Future research will\ncontinue to refine and evaluate the long-context adaptation methodology, aiming to further\nenhance DeepSeek-Coder\u2019s efficiency and user-friendliness in processing extended contexts.\n3.7. Instruction Tuning\nWe develop DeepSeek-Coder-Instruct by enhancing the DeepSeek-Coder-Base through instruction-\nbased fine-tuning using high-quality data. This data comprises helpful and impartial human\ninstructions, structured by the Alpaca Instruction format (Taori et al., 2023). To demarcate each\ndialogue turn, we employed a unique delimiter token <|EOT|> to signify the conclusion of each\nsegment. For training, we use a cosine schedule with 100 warm-up steps and an initial learning\nrate 1e-5. We also use a batch size of 4M tokens and 2B tokens in total.\nAn example of using DeepSeek-Coder-Instruct 34B is depicted in Figure 4. This example is\na multi-turn dialogue scenario for building a snake game. Initially, we ask the model to write\na game snake using pygame. The model successfully creates a basic snake game that can run\nwithout bugs. To improve the game, we further request adding a scoring system in the top left\ncorner. The model then introduces a \"score\" variable and a \"display_score\" function, along with\nan explanation of how to integrate these features. This example illustrates DeepSeek-Coder-\nInstruct\u2019s ability to provide complete solutions in multi-turn dialogue settings. More cases can\nbe found in the Appendix A.\n9\nQ1: Write a game snake using pygame\nSure, here is a simple implementation of the classic Snake \ngame using Pygame. This code will create a window with a \nsnake that can be controlled using the arrow keys. The snake \nwill grow in length each time it eats the food. The game will \nend if the snake hits the edge of the window or its own tail.\n```python\nimport pygame \nimport random\n\u2026\nrunning = True\nWhile running:\n\u2026\n# Draw everything \nscreen.fill(BLACK) \nsnake.draw() \nfood.draw() \npygame.display.flip() \n# Cap the frame rate \nclock.tick(SPEED) \n# Quit Pygame \npygame.quit()\n```\nThis code sets up a basic game loop where the snake moves in \nresponse to key presses, and the game ends if the snake hits the \nedge of the window or its own tail. The snake grows in length \neach time it eats the food. The game continues until the player \ncloses the window.\nRun Code\nQ2: Add a scoring system in the top left corner\nTo add a scoring system to the top left corner of the game \nwindow, you can create a function to display the score and \nincrement it each time the snake eats the food. Here's how you \ncan modify the code to include a scoring system:\n```python\n\u2026\nrunning = True\nWhile running:\n\u2026\n# Draw everything \nscreen.fill(BLACK) \nsnake.draw() \nfood.draw()\ndisplay_score(score) # Display the score\npygame.display.flip() \n# Cap the frame rate \nclock.tick(SPEED) \n# Quit Pygame \npygame.quit()\n```\nIn this updated code, we've added a `score` variable to keep \ntrack of the player's score, which is incremented each time the \nsnake eats the food. The `display_score` function is used to \nrender the score text and display it in the top left corner of the \nwindow. The `display_score` function is called after the snake \nhas moved and the screen has been filled with the black color, \nbut before the `pygame.display.flip()` call, so that the score is \ndrawn on top of everything else.\nRun Code\nFigure 4 | An example of responses from DeepSeek-Coder-Instruct 33B in a multi-turn setting.\n4. Experimental Results\nIn this section, we evaluate DeepSeek-Coder on four tasks, including code generation (\u00a74.1), FIM\ncode completion (\u00a74.2), cross-file code completion (\u00a74.3) and program-based math reasoning\n(\u00a74.4). We compare DeepSeek-Coder with the previous state-of-the-art large language models:\n\u2022 CodeGeeX2 (Zheng et al., 2023) represents the second generation of the multilingual\ncode generation model CodeGeeX. It is developed using the ChatGLM2 (Du et al., 2022)\narchitecture and is enhanced with an extensive dataset of coding examples.\n\u2022 StarCoder (Li et al., 2023) is a publicly accessible model with a substantial parameter count\nof 15 billion. It is specifically trained on a meticulously curated subset of the Stack dataset\n(Kocetkov et al., 2022), covering 86 programming languages, ensuring its proficiency\nacross a wide range of coding tasks.\n\u2022 CodeLlama (Roziere et al., 2023) encompasses a series of code-centric Large Language\nModels (LLMs) that are derivatives of LLaMA2 (Touvron et al., 2023). Available in three\nsizes \u2014 7B, 13B, and 34B \u2014 these models undergo continued training on a vast 500 billion\ntoken code corpus, building upon the foundational LLaMA2 architecture.\n\u2022 code-cushman-001 Chen et al. (2021) is a 12 billion parameter model developed by OpenAI\nand served as the initial model for Github Copilot.\n\u2022 GPT-3.5 and GPT-4 (OpenAI, 2023) are advanced generative AI models developed by\nOpenAI. While they are not explicitly trained for code generation, they also demonstrate\n10\nnotable performance in this domain. Their effectiveness in handling code generation tasks\nis largely attributed to their massive scale in terms of parameter count.\n4.1. Code Generation\nHumanEval and MBPP Benchmarks\nThe HumanEval (Chen et al., 2021) and MBPP (Austin\net al., 2021) benchmarks are widely used for evaluating code LLMs. HumanEval consists of 164\nhand-written Python problems that are validated using test cases to assess the code generated\nby a Code LLM in a zero-shot setting, while the MBPP benchmark includes 500 problems\nin a few-shot setting. To evaluate the model\u2019s multilingual capabilities, we expanded the\nPython problems of Humaneval Benchmark to seven additional commonly used programming\nlanguages, namely C++, Java, PHP, TypeScript (TS), C#, Bash, and JavaScript (JS) (Cassano et al.,\n2023). For both benchmarks, We adopted a greedy search approach and re-implemented the\nbaseline results using the same script and environment for fair comparison.\nModel\nSize\nPython\nC++\nJava\nPHP\nTS\nC#\nBash\nJS\nAvg\nMBPP\nMultilingual Base Models\ncode-cushman-001\n12B\n33.5%\n31.9%\n30.6%\n28.9%\n31.3%\n22.1%\n11.7%\n-\n-\n-\nCodeGeeX2\n6B\n36.0%\n29.2%\n25.9%\n23.6%\n20.8%\n29.7%\n6.3%\n24.8%\n24.5%\n36.2%\nStarCoderBase\n16B\n31.7%\n31.1%\n28.5%\n25.4%\n34.0%\n34.8%\n8.9%\n29.8%\n28.0%\n42.8%\nCodeLlama\n7B\n31.7%\n29.8%\n34.2%\n23.6%\n36.5%\n36.7%\n12.0%\n29.2%\n29.2%\n38.6%\nCodeLlama\n13B\n36.0%\n37.9%\n38.0%\n34.2%\n45.2%\n43.0%\n16.5%\n32.3%\n35.4%\n48.4%\nCodeLlama\n34B\n48.2%\n44.7%\n44.9%\n41.0%\n42.1%\n48.7%\n15.8%\n42.2%\n41.0%\n55.2%\nDeepSeek-Coder-Base\n1.3B\n34.8%\n31.1%\n32.3%\n24.2%\n28.9%\n36.7%\n10.1%\n28.6%\n28.3%\n46.2%\nDeepSeek-Coder-Base\n6.7B\n49.4%\n50.3%\n43.0%\n38.5%\n49.7%\n50.0%\n28.5%\n48.4%\n44.7%\n60.6%\nDeepSeek-Coder-Base\n33B\n56.1%\n58.4%\n51.9%\n44.1%\n52.8%\n51.3%\n32.3%\n55.3%\n50.3%\n66.0%\nInstruction-Tuned Models\nGPT-3.5-Turbo\n-\n76.2%\n63.4%\n69.2%\n60.9%\n69.1%\n70.8%\n42.4%\n67.1%\n64.9%\n70.8%\nGPT-4\n-\n84.1%\n76.4%\n81.6%\n77.2%\n77.4%\n79.1%\n58.2%\n78.0%\n76.5%\n80.0%\nDeepSeek-Coder-Instruct\n1.3B\n65.2%\n45.3%\n51.9%\n45.3%\n59.7%\n55.1%\n12.7%\n52.2%\n48.4%\n49.4%\nDeepSeek-Coder-Instruct\n6.7B\n78.6%\n63.4%\n68.4%\n68.9%\n67.2%\n72.8%\n36.7%\n72.7%\n66.1%\n65.4%\nDeepSeek-Coder-Instruct\n33B\n79.3%\n68.9%\n73.4%\n72.7%\n67.9%\n74.1%\n43.0%\n73.9%\n69.2%\n70.0%\nTable 3 | Performance of approaches on the Multilingual HumanEval and MBPP Benchmarks.\nThe results are presented in Table 3. As we can see, DeepSeek-Coder-Base achieves state-\nof-the-art performance with an average accuracy of 50.3% on HumanEval and 66.0% on MBPP.\nIn comparison to the similarly sized open-source model CodeLlama-Base 34B, our model has\ndemonstrated a notable improvement of 9% and 11% in accuracy, respectively. It\u2019s worth\nnoting that even our smaller model, DeepSeek-Coder-Base 6.7B, surpasses the performance\nof CodeLlama-Base 34B. After instruction fine-tuning, our model surpasses the closed-source\nGPT-3.5-Turbo model in HumanEval benchmark, significantly reducing the performance gap\nbetween OpenAI GPT-4 and open-source models.\nDS-1000 Benchmark\nHumanEval and MBPP have a significant drawback in that they rely\nheavily on straightforward programming tasks that may not accurately represent the kind of\ncode most programmers typically write. In contrast, the DS-1000 benchmark, as introduced in\nthe work by Lai et al. (2023), offers a comprehensive collection of 1,000 practical and realistic data\nscience workflows across seven different libraries. This benchmark evaluates code generation by\nexecuting it against specific test cases. What sets DS-1000 apart is its categorization of problems\nbased on the libraries involved, which encompass Matplotlib, NumPy, Pandas, SciPy, Scikit-\n11\nLearn, PyTorch, and TensorFlow. The benchmark assesses the performance of base models in the\ncode completion setting and we provide pass@1 results for each library, as well as overall score.\nThe results of DS-1000 benchmark are shown in Table 4. As can be seen from the table, the\nDeepSeek-Coder model achieves relatively high accuracy in all libraries, demonstrating that our\nmodel is not only capable of generating good code but also of using libraries more accurately in\nreal data science workflows.\nModel\nSize\nMatplotlib\nNumpy\nPandas\nPytorch\nScipy\nScikit-Learn\nTensorflow\nAvg\nCodeGeeX2\n6B\n38.7%\n26.8%\n14.4%\n11.8%\n19.8%\n27.0%\n17.8%\n22.9%\nStarCoder-Base\n16B\n43.2%\n29.1%\n11.0%\n20.6%\n23.6%\n32.2%\n15.6%\n24.6%\nCodeLlama-Base\n7B\n41.9%\n24.6%\n14.8%\n16.2%\n18.9%\n17.4%\n17.8%\n22.1%\nCodeLlama-Base\n13B\n46.5%\n28.6%\n18.2%\n19.1%\n18.9%\n27.8%\n33.3%\n26.8%\nCodeLlama-Base\n34B\n50.3%\n42.7%\n23.0%\n25.0%\n28.3%\n33.9%\n40.0%\n34.3%\nDeepSeek-Coder-Base\n1.3B\n32.3%\n21.4%\n9.3%\n8.8%\n8.5%\n16.5%\n8.9%\n16.2%\nDeepSeek-Coder-Base\n6.7B\n48.4%\n35.5%\n20.6%\n19.1%\n22.6%\n38.3%\n24.4%\n30.5%\nDeepSeek-Coder-Base\n33B\n56.1%\n49.6%\n25.8%\n36.8%\n36.8%\n40.0%\n46.7%\n40.2%\nTable 4 | Performance of different approaches on the DS-1000-Tasks.\nLeetCode Contest Benchmark\nTo further validate the model\u2019s capability in real-world pro-\ngramming problems, we construct the LeetCode Contest benchmark3. LeetCode4 presents\ncompetition-level problems, offering significant challenges that test the model\u2019s problem under-\nstanding and code generation skills. We collected the latest problems from LeetCode Contests to\nprevent the appearance of both the problems or their solutions in our pre-training data. A total\nof 180 problems were collected from July 2023 to January 2024. For each problem, we collected\n100 test cases to ensure the test coverage. We use the template \"{problem_description}\\nPlease\ncomplete the code below to solve the above problem:\\n```python\\n{code_template}\\n```\" to\nbuild the instruction prompt.\nThe evaluation results are shown in Table 5. In our evaluation, the DeepSeek-Coder models\ndemonstrate remarkable performance over current open-source coding models. Specifically, the\nDeepSeek-Coder-Instruct 6.7B and 33B achieve Pass@1 scores of 19.4% and 27.8% respectively\nin this benchmark. This performance notably surpasses existing open-sourced models such as\nCode-Llama-33B. The DeepSeek-Coder-Instruct 33B is the only open-sourced model that outper-\nforms OpenAI\u2019s GPT-3.5-Turbo in this task. However, there remains a substantial performance\ngap when compared to the more advanced GPT-4-Turbo.\nOur analysis indicates that the implementation of Chain-of-Thought (CoT) prompting no-\ntably enhances the capabilities of DeepSeek-Coder-Instruct models. This improvement becomes\nparticularly evident in the more challenging subsets of tasks. By adding the directive, \"You\nneed first to write a step-by-step outline and then write the code.\" following the initial prompt,\nwe have observed enhancements in performance. This observation leads us to believe that the\nprocess of first crafting detailed code descriptions assists the model in more effectively under-\nstanding and addressing the intricacies of logic and dependencies in coding tasks, particularly\nthose of higher complexity. Therefore, we strongly recommend employing CoT prompting strate-\ngies when utilizing DeepSeek-Coder-Instruct models for complex coding challenges. Such an\napproach promotes a more methodical and logical framework for problem-solving, potentially\nresulting in more precise and efficient outcomes in code generation tasks.\n3We have published this benchmark in https://github.com/deepseek-ai/DeepSeek-Coder/tree/main\n/Evaluation/LeetCode.\n4https://leetcode.com/\n12\nModel\nSize\nEasy (45)\nMedium (91)\nHard (44)\nOverall(180)\nWizardCoder-V1.0\n15B\n17.8%\n1.1%\n0.0%\n5.0%\nCodeLlama-Instruct\n34B\n24.4%\n4.4%\n4.5%\n9.4%\nPhind-CodeLlama-V2\n34B\n26.7%\n8.8%\n9.1%\n13.3%\nGPT-3.5-Turbo\n-\n46.7%\n15.4 %\n15.9%\n23.3%\nGPT-3.5-Turbo + CoT\n-\n42.2%\n15.4%\n20.5%\n23.3%\nGPT-4-Turbo\n-\n73.3%\n31.9%\n25.0%\n40.6%\nGPT-4-Turbo + CoT\n-\n71.1%\n35.2%\n25.0%\n41.8%\nDeepSeek-Coder-Instruct\n1.3B\n22.2%\n1.1%\n4.5%\n7.2%\nDeepSeek-Coder-Instruct + CoT\n1.3B\n22.2%\n2.2%\n2.3%\n7.2%\nDeepSeek-Coder-Instruct\n6.7B\n44.4%\n12.1%\n9.1%\n19.4%\nDeepSeek-Coder-Instruct + CoT\n6.7B\n44.4%\n17.6%\n4.5%\n21.1%\nDeepSeek-Coder-Instruct\n33B\n57.8%\n22.0%\n9.1%\n27.8%\nDeepSeek-Coder-Instruct + CoT\n33B\n53.3%\n25.3%\n11.4%\n28.9%\nTable 5 | Performance of different models on the LeetCode Contest Benchmark.\nIt is important to acknowledge that despite our diligent efforts to gather the most recent\ncode questions for model testing, the possibility of data contamination cannot be entirely ruled\nout. We observed that the GPT-4-Turbo and DeepSeek-Coder models achieved higher scores\nin the LeetCode Contest held in July and August. We encourage the research community to\nconsider the potential issue of data contamination when evaluating models in future studies\nusing our released LeetCode data.\n4.2. Fill-in-the-Middle Code Completion\nDeepSeek-Coder models are trained with a 0.5 FIM (Fill-In-the-Middle) rate during their pre-\ntraining phase. This specialized training strategy empowers the model to proficiently generate\ncode by filling in blanks based on the surrounding context, both prefix and suffix, of the given\ncode snippet. This capability is particularly advantageous in the realm of code completion\ntools. Several open-source models have emerged with similar capabilities. Notable among these\nare SantaCoder (Allal et al., 2023), StarCoder (Li et al., 2023), and CodeLlama (Roziere et al.,\n2023). These models have set a precedent in the field of code generation and completion. In\nevaluating the performance DeepSeek-Coder models, we conducted a comparative analysis with\nthe aforementioned models. The benchmark for this comparison was the Single-Line Infilling\nbenchmarks, encompassing three different programming languages, as proposed by Allal et al.\n(2023). This benchmark uses the line exact match accuracy as the evaluation metric.\nModel\nSize\npython\njava\njavascript\nMean\nSantaCoder\n1.1B\n44.0%\n62.0%\n74.0%\n69.0%\nStarCoder\n16B\n62.0%\n73.0%\n74.0%\n69.7%\nCodeLlama-Base\n7B\n67.6%\n74.3%\n80.2%\n69.7%\nCodeLlama-Base\n13B\n68.3%\n77.6%\n80.7%\n75.5%\nDeepSeek-Coder-Base\n1B\n57.4%\n82.2%\n71.7%\n70.4%\nDeepSeek-Coder-Base\n7B\n66.6%\n88.1%\n79.7%\n80.7%\nDeepSeek-Coder-Base\n33B\n65.4%\n86.6%\n82.5%\n81.2%\nTable 6 | Performance of different approaches on the FIM-Tasks.\nThe evaluation results are shown in Table 6. Despite being the smallest model with a capacity\n13\nof 1.3 billion parameters, DeepSeek-Coder outperforms its larger counterparts, StarCoder and\nCodeLlama, in these benchmarks. This superior performance can be attributed to the high\nquality of the pre-trained data utilized by DeepSeek-Coder. Furthermore, a notable trend\nobserved is the correlation between the size of the model and its performance. As the model\nsize increases, there is a corresponding and responsible enhancement in performance. This\ntrend underscores the importance of model capacity in achieving higher accuracy in code\ncompletion tasks. Based on these findings, we recommend the deployment of the DeepSeek-\nCoder-Base 6.7B model in code completion tools. This recommendation is grounded in the\nmodel\u2019s demonstrated balance between efficiency and accuracy. The DeepSeek-Coder-Base 6.7B\nmodel, with its substantial parameter size, has proven to be highly effective in the context of\ncode completion, making it an ideal choice for integrating advanced computational capabilities\ninto coding environments.\n4.3. Cross-File Code Completion\nIn this section, we will evaluate the performance of existing open-source models in cross-file\ncode completion tasks. Unlike code generation discussed in the previous section, cross-file code\ncompletion requires the model to access and understand repositories that span multiple files\nwith numerous cross-file dependencies. We use CrossCodeEval (Ding et al., 2023) to evaluate the\ncapabilities of currently available open-source code models of 7B scale in cross-file completion\ntasks. This dataset is constructed on a diverse set of real-world, open-sourced, permissively\nlicensed repositories in four popular programming languages: Python, Java, TypeScript, and C#.\nThe dataset is specifically designed to strictly require cross-file context for accurate completion.\nNotably, this dataset was constructed from repositories created between March and June 2023,\nwhile our pre-training data only includes code created before February 2023, which ensures that\nthis dataset was not present in our pre-training data, thus avoiding data leakage.\nModel\nSize\nPython\nJava\nTypeScript\nC#\nEM\nES\nEM\nES\nEM\nES\nEM\nES\nCodeGeex2\n6B\n8.11%\n59.55%\n7.34%\n59.60%\n6.14%\n55.50%\n1.70%\n51.66%\n+ Retrieval\n10.73%\n61.76%\n10.10%\n59.56%\n7.72%\n55.17%\n4.64%\n52.30%\nStarCoder-Base\n7B\n6.68%\n59.55%\n8.65%\n62.57%\n5.01%\n48.83%\n4.75%\n59.53%\n+ Retrieval\n13.06%\n64.24%\n15.61%\n64.78%\n7.54%\n42.06%\n14.20%\n65.03%\nCodeLlama-Base\n7B\n7.32%\n59.66%\n9.68%\n62.64%\n8.19%\n58.50%\n4.07%\n59.19%\n+ Retrieval\n13.02%\n64.30%\n16.41%\n64.64%\n12.34%\n60.64%\n13.19%\n63.04%\nDeepSeek-Coder-Base\n6.7B\n9.53%\n61.65%\n10.80%\n61.77%\n9.59%\n60.17%\n5.26%\n61.32%\n+ Retrieval\n16.14%\n66.51%\n17.72%\n63.18%\n14.03%\n61.77%\n16.23%\n63.42%\n+ Retrieval w/o Repo Pre-training\n16.02%\n66.65%\n16.64%\n61.88%\n13.23%\n60.92%\n14.48%\n62.38%\nTable 7 | Performance of different models on cross-file code completion.\nIn our evaluation of various models, we set the maximum sequence length to 2048 tokens,\nthe maximum output length to 50 tokens, and a limit of 512 tokens for the cross-file context. For\nthe cross-file context, we utilize the official BM25 search results provided by Ding et al. (2023).\nEvaluation metrics include exact match and edit similarity. The results, presented in Table 7,\ndemonstrate that DeepSeek-Coder consistently outperforms other models in cross-file comple-\ntion tasks across multiple languages, showcasing its superior practical application capabilities.\nWhen only utilizing file-level code corpus (w/o Repo Pre-training) to pre-train DeepSeek-Coder,\nwe observe a decrease in performance in the Java, TypeScript, and C# languages, indicating the\neffectiveness of the repository-level pre-training.\n14\n4.4. Program-based Math Reasoning\nProgram-based math reasoning involves evaluating a model\u2019s ability to understand and solve\nmathematical problems through programming. This type of reasoning is critical in fields\nsuch as data analysis and scientific computing. To conduct this assessment, we utilize the\nProgram-Aided Math Reasoning (PAL) method as outlined in Gao et al. (2023). This approach\nis applied across seven distinct benchmarks, each offering unique challenges and contexts.\nThese benchmarks includes GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), GSM-\nHard (Gao et al., 2023), SVAMP (Patel et al., 2021), TabMWP (Lu et al., 2022), ASDiv (Miao\net al., 2020) and MAWPS (Gou et al., 2023). In each of these benchmarks, the model is prompted\nto alternately describe a solution step in natural language and then execute that step with\ncode. As seen in Table 8, DeepSeek-Coder models achieve a remarkable performance across all\nbenchmarks, especially the 33B variant, which demonstrates the potential of using such models\nin applications that require complex mathematical computations and problem-solving abilities.\nModel\nSize\nGSM8k\nMATH\nGSM-Hard\nSVAMP\nTabMWP\nASDiv\nMAWPS\nAvg\nMultilingual Base Models\nCodeGeex-2\n7B\n22.2%\n9.7%\n23.6%\n39.0%\n44.6%\n48.5%\n66.0%\n36.2%\nStarCoder-Base\n16B\n23.4%\n10.3%\n23.0%\n42.4%\n45.0%\n54.9%\n81.1%\n40.0%\nCodeLlama-Base\n7B\n31.2%\n12.1%\n30.2%\n54.2%\n52.9%\n59.6%\n82.6%\n46.1%\nCodeLlama-Base\n13B\n43.1%\n14.4%\n40.2%\n59.2%\n60.3%\n63.6%\n85.3%\n52.3%\nCodeLlama-Base\n34B\n58.2%\n21.2%\n51.8%\n70.3%\n69.8%\n70.7%\n91.8%\n62.0%\nDeepSeek-Coder-Base\n1.3B\n14.6%\n16.8%\n14.5%\n36.7%\n30.0%\n48.2%\n62.3%\n31.9%\nDeepSeek-Coder-Base\n6.7B\n43.2%\n19.2%\n40.3%\n58.4%\n67.9%\n67.2%\n87.0%\n54.7%\nDeepSeek-Coder-Base\n33B\n60.7%\n29.1%\n54.1%\n71.6%\n75.3%\n76.7%\n93.3%\n65.8%\nTable 8 | Performance of different approaches on the program-aid math reasoning tasks.\n5. Continue Pre-Training From General LLM\nTo further enhance the natural language understanding and mathematical reasoning abilities\nof the DeepSeek-Coder model, we perform additional pre-training from the general language\nmodel DeepSeek-LLM-7B Base (DeepSeek-AI, 2024) on 2 trillion tokens, resulting in DeepSeek-\nCoder-v1.5 7B. For this pre-training, we specifically use the data sources listed in Table 9. Unlike\nDeepSeek-Coder, DeepSeek-Coder-v1.5 employs solely a next token prediction objective with a\n4K context length during its pre-training phase.\nData Source\nPercentage\nSource Code\n70%\nMarkdown and StackExchange\n10%\nNatural language related to code\n7%\nNatural language related to math\n7%\nBilingual (Chinese-English) natural language\n6%\nTable 9 | Data sources for DeepSeek-Coder-v1.5 7B pre-training\nWe conduct a comparison between DeepSeek-Coder-v1.5 7B and DeepSeek-Coder 6.7B, and\nre-run all benchmarks using our evaluation pipeline to ensure a fair comparison. We evaluate\nperformance across a wide range of tasks, which can be categorized as follows:\n15\n\u2022 Programming: This category includes evaluations in a multilingual setting using the\nHumanEval dataset by Chen et al. (2021), as well as evaluations in a Python setting using\nthe MBPP dataset by Austin et al. (2021)\n\u2022 Math Reasoning: We assess performance on math reasoning tasks using the GSM8K\nbenchmark (Cobbe et al., 2021) and the MATH (Hendrycks et al., 2021) benchmark [4].\nThese tasks involve solving math problems by generating programs.\n\u2022 Natural Language Our evaluation in natural language tasks includes MMLU (Hendrycks\net al., 2020), BBH (Suzgun et al., 2022), HellaSwag (Zellers et al., 2019), Winogrande (Sak-\naguchi et al., 2021), and ARC-Challenge (Clark et al., 2018) benchmarks.\nThe results for the Base and Instruct models are presented in Table 10. It is observed\nthat the DeepSeek-Coder-Base-v1.5 model, despite a slight decrease in coding performance,\nshows marked improvements across most tasks when compared to the DeepSeek-Coder-Base\nmodel. In particular, in the Math Reasoning and Natural Language categories, DeepSeek-\nCoder-Base-v1.5 significantly outperforms its predecessor across all benchmarks, which also\ndemonstrates significant improvements in its mathematical reasoning and natural language\nprocessing capabilities.\nProgramming\nMath Reasoning\nNatural Language\nModels\nSize HumanEval MBPP GSM8K MATH MMLU\nBBH\nHellaSwag WinoG ARC-C\nDeepSeek-Coder-Base\n6.7B\n44.7%\n60.6%\n43.2%\n19.2%\n36.6%\n44.3%\n53.8%\n57.1%\n32.5%\nDeepSeek-Coder-Base-v1.5\n6.9B\n43.2%\n60.4%\n62.4%\n24.7%\n49.1%\n55.2%\n69.9%\n63.8%\n47.2%\nDeepSeek-Coder-Instruct\n6.7B\n66.1%\n65.4%\n62.8%\n28.6%\n37.2%\n46.9%\n55.0%\n57.6%\n37.4%\nDeepSeek-Coder-Instruct-v1.5 6.9B\n64.1%\n64.6%\n72.6%\n34.1%\n49.5%\n53.3%\n72.2%\n63.4%\n48.1%\nTable 10 | Comparative analysis of performance between DeepSeek-Coder-Base and\nDeepSeek-Coder-Base-v1.5. Math tasks are solved through programming.\n6. Conclusion\nIn this technical report, we introduce a series of specialized Large Language Models (LLMs) for\ncoding, named DeepSeek-Coder, available in three distinct scales: 1.3B, 6.7B, and 33B parameters.\nThese models are uniquely trained on a meticulously curated project-level code corpus, utilizing\na \"fill-in-the-blank\" pre-training objective to enhance code infilling capabilities. A significant\nadvancement is the extension of the models\u2019 context window to 16,384 tokens, thereby greatly\nimproving their effectiveness in handling extensive code generation tasks. Our evaluations\nreveal that the most advanced model in our series, DeepSeek-Coder-Base 33B surpasses existing\nopen-source code models across a variety of standard tests. Impressively, the DeepSeek-Coder-\nBase 6.7B model, despite its smaller scale, delivers performance on par with the 34B parameter\nCodeLlama, a testament to the high quality of our pretraining corpus.\nTo augment the zero-shot instruction capabilities of the DeepSeek-Coder-Base models, we\nhave fine-tuned them with high-quality instructional data. This has led to the DeepSeek-Coder-\nInstruct 33B model outperforming OpenAI\u2019s GPT-3.5 Turbo in a range of coding-related tasks,\nshowcasing its exceptional proficiency in code generation and understanding.\nTo further improve the natural language understanding capabilities of the DeepSeek-Coder-\nBase models, we have conducted additional pretraining based on the DeepSeek-LLM 7B check-\npoint. This additional training involved processing a diverse dataset comprising 2 billion tokens,\nincluding natural language, code, and mathematical data. The result is the creation of a new\n16\nand improved code model, DeepSeek-Coder-v1.5. Our observations indicate that DeepSeek-\nCoder-v1.5 not only maintains its predecessor\u2019s high-level coding performance but also exhibits\nenhanced natural language comprehension. This advancement underscores our belief that the\nmost effective code-focused Large Language Models (LLMs) are those built upon robust general\nLLMs. The reason is evident: to effectively interpret and execute coding tasks, these models\nmust also possess a deep understanding of human instructions, which often come in various\nforms of natural language. Looking ahead, our commitment is to develop and openly share\neven more powerful code-focused LLMs based on larger-scale general LLMs.\nAcknowledgements\nWe would like to express our gratitude to Bo Liu, Chengqi Deng, Chong Ruan, Damai Dai,\nJiashi Li, Kang Guan, Mingchuan Zhang, Panpan Huang, Shuiping Yu, Shirong Ma, Yaofeng\nSun, Yishi Piao, Zhihong Shao, and Zhewen Hao for their invaluable discussions and assistance\nduring training DeepSeek-Coder models.\nReferences\nL. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra,\nA. Gu, M. Dey, et al. Santacoder: don\u2019t reach for the stars! arXiv preprint arXiv:2301.03988,\n2023.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\nQ. Le, and C. Sutton. Program synthesis with large language models, 2021.\nM. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, and M. Chen. Efficient\ntraining of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.\nF. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee,\nY. Zi, C. J. Anderson, M. Q. Feldman, et al. Multipl-e: a scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Transactions on Software Engineering, 2023.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021.\nS. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via\npositional interpolation. arXiv preprint arXiv:2306.15595, 2023.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457, 2018.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nT. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\nDeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv\npreprint arXiv:2401.02954, 2024.\n17\nY. Ding, Z. Wang, W. U. Ahmad, H. Ding, M. Tan, N. Jain, M. K. Ramanathan, R. Nallapati,\nP. Bhatia, D. Roth, et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file\ncode completion. In Thirty-seventh Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2023.\nZ. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model\npretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335,\n2022.\nD. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer,\nand M. Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint\narXiv:2204.05999, 2022.\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-\naided language models. In International Conference on Machine Learning, pages 10764\u2013\n10799. PMLR, 2023.\nG. Gemini Team. Gemini: A family of highly capable multimodal models, 2023. URL https:\n//goo.gle/GeminiPaper.\nZ. Gou, Z. Shao, Y. Gong, Y. Yang, M. Huang, N. Duan, W. Chen, et al. Tora: A tool-integrated\nreasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measur-\ning massive multitask language understanding. In International Conference on Learning\nRepresentations, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.\nHigh-Flyer. Hai-llm: An efficient and lightweight tool for training large models. 2023. URL\nhttps://www.high-flyer.cn/en/blog/hai-llm.\nkaiokendev. Things i\u2019m learning while training superhot. https://kaiokendev.github.io\n/til#extending-context-to-8k, 2023.\nD. Kocetkov, R. Li, L. Jia, C. Mou, Y. Jernite, M. Mitchell, C. M. Ferrandis, S. Hughes, T. Wolf,\nD. Bahdanau, et al. The stack: 3 tb of permissively licensed source code. Transactions on\nMachine Learning Research, 2022.\nV. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro.\nReducing activation recomputation in large transformer models. Proceedings of Machine\nLearning and Systems, 5, 2023.\nY. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih, D. Fried, S. Wang, and T. Yu.\nDs-1000: A natural and reliable benchmark for data science code generation. In International\nConference on Machine Learning, pages 18319\u201318345. PMLR, 2023.\nK. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicat-\ning training data makes language models better. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424\u20138445,\n2022.\n18\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim,\net al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019.\nP. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan. Dynamic\nprompt learning via policy gradient for semi-structured mathematical reasoning. In The\nEleventh International Conference on Learning Representations, 2022.\nS.-Y. Miao, C.-C. Liang, and K.-Y. Su. A diverse corpus for evaluating and developing english\nmath word problem solvers. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 975\u2013984, 2020.\nD. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B.\nGibbons, and M. Zaharia. Pipedream: Generalized pipeline parallelism for dnn training.\nIn Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1\u201315,\n2019.\nE. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen:\nAn open large language model for code with multi-turn program synthesis. arXiv preprint\narXiv:2203.13474, 2022.\nE. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou. Codegen2: Lessons for training\nllms on programming and natural languages, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nA. Patel, S. Bhattamishra, and N. Goyal. Are nlp models really able to solve simple math\nword problems? In Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pages 2080\u2013\n2094, 2021.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer, 2023.\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training tril-\nlion parameter models. In SC20: International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pages 1\u201316. IEEE, 2020.\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,\net al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd\nschema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\nR. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword\nunits. arXiv preprint arXiv:1508.07909, 2015.\nJ. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with\nrotary position embedding, 2023.\nM. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, , and J. Wei. Challenging big-bench tasks and whether chain-of-thought\ncan solve them. arXiv preprint arXiv:2210.09261, 2022.\n19\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab\n/stanford_alpaca, 2023.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\nY. Wang, W. Wang, S. Joty, and S. C. Hoi. Codet5: Identifier-aware unified pre-trained encoder-\ndecoder models for code understanding and generation. arXiv preprint arXiv:2109.00859,\n2021.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really\nfinish your sentence?\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4791\u20134800, 2019.\nQ. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, L. Shen, Z. Wang, A. Wang, Y. Li, et al.\nCodegeex: A pre-trained model for code generation with multilingual benchmarking on\nhumaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pages 5673\u20135684, 2023.\n20\nA. Cases of Chatting with DeepSeek-Coder-Instruct\nWe will present two cases of interactions with DeepSeek-Coder-Instruct, with one involving a\nmulti-turn conversation about creating a database and performing data analysis, and the other\ncentered around using a model to solve a sample problem from LeetCode.\nIn the first scenario, depicted in Figure 5, we instruct the model to build a student database\nusing Python and randomly insert 10 pieces of information. Subsequently, in the second round\nof the conversation, we continue to ask the model by analyzing the age distribution of the\nstudents. From Figure 5, it\u2019s evident that the model can generate bug-free and comprehensive\ncode, accompanied by explanatory details. In the second scenario, as illustrated in Figure 6,\nwe further assess the model\u2019s capabilities by testing it on an out-of-domain LeetCode contest\nproblem. This particular problem was released in November 2023, after our data collection, and\nthus, isn\u2019t part of our model\u2019s training data. The results show that our model excels at solving\nproblems that extend beyond its training distribution.\nQ1: Build a database in Python that includes \nvarious pieces of information about students, \ninsert ten random records and display.\nSure, here is a simple example of how you can do this using \nSQLite in Python.\n```python\nimport sqlite3\nimport random\n# Connect to SQLite database\nconn = sqlite3.connect('students.db')\n# Create a cursor object\ncur = conn.cursor()\n\u2026\n# Create table\ncur.execute(''\u2019\nCREATE TABLE IF NOT EXISTS students (\nid INTEGER PRIMARY KEY,\nname TEXT,\nage INTEGER,\ngrade TEXT) ''')\n\u2026\n# Display all records\ncur.execute('SELECT * FROM students')\n\u2026\n```\nThis script will create a SQLite database named 'students.db' \nand a table named 'students' with four columns: 'id', 'name', 'age', \nand 'grade'. It will then insert 10 random records into the table \nand display all records. \u2026\nRun Code\nQ2: create a graph depicting the age distribution.\nTo create a graph depicting the age distribution of the students, you \ncan use the matplotlib library in Python. Here's how you can do it:\n```python\nimport sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Connect to SQLite database\nconn = sqlite3.connect('students.db')\n# Read the data from the database into a pandas DataFrame\ndf = pd.read_sql_query(\"SELECT * from students\", conn)\n# Create a histogram of the age distribution\nplt.hist(df['age'], bins=range(10, 30, 2), edgecolor='black')\nplt.title('Age Distribution of Students')\nplt.xlabel('Age')\nplt.ylabel('Number of Students')\nplt.show()\n# Close the connection\nconn.close()\n```\nThis script will connect to the SQLite database, read the data into a \npandas DataFrame, then create a histogram of the age distribution. \nThe histogram will show the number of students in each age range.\nRun Code\nFigure 5 | An example of building database and data analysis.\n21\nQ: There are n teams numbered from 0 to n - 1 in a tournament; each team is \nalso a node in a DAG.\nYou are given the integer n and a 0-indexed 2D integer array edges of \nlength m representing the DAG, where edges[i] = [ui, vi] indicates that there is a \ndirected edge from team ui to team vi in the graph.\nA directed edge from a to b in the graph means that team a is stronger than \nteam b and team b is weaker than team a.\nTeam a will be the champion of the tournament if there is no team b that \nis stronger than team a.\nReturn the team that will be the champion of the tournament if there is \na unique champion, otherwise, return -1.\nNotes\n\u2022A cycle is a series of nodes a1, a2, ..., an, an+1 such that node a1 is the same node \nas node an+1, the nodes a1, a2, ..., an are distinct, and there is a directed edge from \nthe node ai to node ai+1 for every i in the range [1, n].\n\u2022A DAG is a directed graph that does not have any cycle.\n\u2022Example 1:\n\u2026\n\u2022Example 2:\n\u2026\nConstraints:\n\u2026\nPlease follow the template below to solve the problem:\n```python\nclass Solution:\ndef findChampion(self, n: int, edges: List[List[int]]) -> int: \n```\nTo solve this problem, we can use a topological sorting algorithm. The idea is to \u2026\n```python\nfrom typing import List\nclass Solution:\ndef findChampion(self, n: int, edges: List[List[int]]) -> int:\n# Create an adjacency list to represent the DAG\nadj_list = {i: [] for i in range(n)}\nfor u, v in edges:\nadj_list[u].append(v)\n# Initialize a list to keep track of the number of \u2026\nin_degree = [0] * n\n# Calculate the in-degree for each team\nfor u in adj_list:\nfor v in adj_list[u]:\nin_degree[v] += 1\n# Initialize a list to keep track of the teams with no incoming edges\nno_incoming_edges = [i for i in range(n) if in_degree[i] == 0]\n# If there is more than one team with no incoming edges, there is no unique \nchampion\nif len(no_incoming_edges) != 1:\nreturn -1\n# Return the team with no incoming edges, which is the champion\nreturn no_incoming_edges[0]\n```\nThis code first creates an adjacency list to represent the DAG. Then, \u2026\nSubmit Code\nFigure 6 | An example of solving LeetCode Problem.\nB. Benchmark curves during training of DeepSeek-Coder-Base\nIn Figure 7, we present the benchmark curves illustrating the performance of DeepSeek-Coder-\nBase models during their training phase. For validation, a carefully curated subset of the training\ncorpus was employed, consisting of 8,000 code files. This subset was deliberately chosen to\nensure a diverse and representative sample, critical for an accurate assessment of the models\u2019\ncapabilities. The performance metrics of these models are specifically detailed in the final two\nsub-figures of Figure 7, offering a clear visual representation of their efficacy throughout the\ntraining process.\n22\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nValue\nHumanEval-Pass@1\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nValue\nHumanEval-cpp-Pass@1\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nValue\nHumanEval-java-Pass@1\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nValue\nHumanEvalFIM-EM\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.0\n0.2\n0.4\n0.6\nValue\nMBPP-Pass@1\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nValue\nMeanHumanEval\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.0\n0.1\n0.2\n0.3\n0.4\nValue\nValidation-Completion-EM\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTokens (Billions)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nValue\nValidation-CompletionFIM-EM\nDeepSeek-Coder-Base-1.3B\nDeepSeek-Coder-Base-6.7B\nDeepSeek-Coder-Base-33B\nFigure 7 | Benchmark curves during training of DeepSeek-Coder-Base.\n23\n"
  },
  {
    "title": "Rethinking Patch Dependence for Masked Autoencoders",
    "link": "https://arxiv.org/pdf/2401.14391.pdf",
    "upvote": "22",
    "text": "Rethinking Patch Dependence for Masked Autoencoders\nLetian Fu1\u2217\nLong Lian1\u2217\nRenhao Wang1\nBaifeng Shi1\nXudong Wang1\nAdam Yala1,2\u2020\nTrevor Darrell1\u2020\nAlexei A. Efros1\u2020\nKen Goldberg1\u2020\n1UC Berkeley\n2UCSF\nAbstract\nIn this work, we re-examine inter-patch dependencies in\nthe decoding mechanism of masked autoencoders (MAE). We\ndecompose this decoding mechanism for masked patch re-\nconstruction in MAE into self-attention and cross-attention.\nOur investigations suggest that self-attention between mask\npatches is not essential for learning good representations. To\nthis end, we propose a novel pretraining framework: Cross-\nAttention Masked Autoencoders (CrossMAE). CrossMAE\u2019s\ndecoder leverages only cross-attention between masked and\nvisible tokens, with no degradation in downstream perfor-\nmance. This design also enables decoding only a small sub-\nset of mask tokens, boosting efficiency. Furthermore, each\ndecoder block can now leverage different encoder features,\nresulting in improved representation learning. CrossMAE\nmatches MAE in performance with 2.5 to 3.7\u00d7 less decoding\ncompute. It also surpasses MAE on ImageNet classification\nand COCO instance segmentation under the same compute.\nCode and models: https://crossmae.github.io.\n1. Introduction\nMasked image modeling [5, 31, 46, 60] has emerged as a\npivotal unsupervised learning technique in computer vision.\nOne such recent work following this paradigm is masked\nautoencoders (MAE): given only a small, random subset of\nvisible image patches, the model is tasked to reconstruct the\nmissing pixels. By operating strictly on this small subset of\nvisible tokens, MAE can efficiently pre-train high-capacity\nmodels on large-scale vision datasets, demonstrating impres-\nsive results on a wide array of downstream tasks [34, 39, 49].\nThe MAE framework uses multi-headed self-attention\nthroughout the model to perform the self-supervised recon-\nstruction task, where the masked and visible tokens not only\nattend to each other but also to themselves, to generate a\nholistic and contextually aware representation. Yet, the mask\ntokens themselves do not contain information. Intuitively,\n*Equal contribution.\n\u2020Equal advising.\nFigure 1. Method Overview. (A) Masked autoencoder (MAE) starts\nby masking random patches of the input image. (B) To reconstruct\na mask token (i.e. marked by the blue star), MAE attends to both\nthe masked tokens (B.Left) and the visible tokens (B.Right). A\nquantitative comparison over the ImageNet validation set shows\nthat the masked tokens in MAE disproportionally attend to the\nvisible tokens (1.42 vs 0.39), questioning the necessity of attention\nwithin mask tokens. (C) We propose CrossMAE, where each mask\ntoken can only attend to the visible tokens. (D) CrossMAE is\nequivalent to or better than MAE (Tabs. 1 and 3), with significantly\nless decoder FLOPS (2.5x lower).\nallowing the exchange of information within nearby mask\ntokens is supposed to allow the model to synthesize a more\nconsistent image; however, is dense self-attention necessary\nfor learning good representation? We analyze the two paral-\nlel components involved in decoding each mask token: self-\nattention with other mask tokens, as well as cross-attention\nto the encoded visible tokens. If MAE relies on the self-\nattention with other mask tokens, its average should be on\npar with the cross-attention. Yet, the quantitative comparison\nin Figure 1.(b) shows the magnitude of mask token-to-visible\ntoken cross-attention (1.42) in the MAE decoder evaluated\nover the entire ImageNet validation set far exceeds that of\nmask token-to-mask token self-attention (0.39).\nThis initial observation prompts two questions: 1) Is the\nself-attention mechanism within mask tokens in the decoder\ntruly necessary for effective representation learning? 2) If\n1\narXiv:2401.14391v1  [cs.CV]  25 Jan 2024\nFigure 2. Example reconstructions of ImageNet validation images. For each set of 5 images, from left to right, are the original image,\nmasked image with a mask ratio of 75%, MAE [31], CrossMAE (trained to reconstruct 25% of image tokens, or 1/3 of the mask tokens),\nand CrossMAE (trained to reconstruct all masked tokens). Since CrossMAE does not reconstruct them, all model outputs have the visible\npatches overlaid. Intriguingly, CrossMAE, when trained for partial reconstruction, can decode all mask tokens in one forward pass (shown\nabove), which deviates from its training methodology. Its comparable reconstruction quality to full-image-trained models suggests that\nfull-image reconstruction might not be essential for effective representation learning.\nnot, can each patch be independently generated, allowing\nthe reconstruction of only a small subset of masked patches,\nwhich in turn, can facilitate faster pretraining without sacri-\nficing downstream performance?\nIn addressing these questions, we introduce CrossMAE,\nwhich diverges from MAE in three ways:\n1. Cross-attention for decoding. Rather than passing a\nconcatenation of mask and visible tokens to a self-attention\ndecoder, CrossMAE uses mask tokens to query the visible\ntokens in a cross-attention decoder to reconstruct the masked\npatches. In this setting, mask tokens incorporate information\nfrom the visible tokens but do not interact with other mask\ntokens, thereby reducing the sequence length for the decoder\nand cutting down computational costs.\n2. Partial reconstruction.\nAfter the removal of self-\nattention, given the encoded features from the visible tokens,\nthe decoding of each mask token becomes conditionally in-\ndependent from one another. This enables the decoding of\nonly a fraction of masked tokens rather than the entire image.\n3. Inter-block attention. Due to the separation of visible\nand mask tokens, we are able to use features from different\nencoder blocks for each decoder block. Empirically, we find\nsolely relying on the last encoder feature map for reconstruc-\ntion, the design present in MAE, hurts feature learning. We\npropose a lightweight inter-block attention mechanism that\nallows the CrossMAE decoder to dynamically leverage a mix\nof low-level and high-level feature maps from the encoder,\nimproving the learned representation.\nUpon training the model, contrary to prior belief, we find\nthat a ViT encoder pretrained with the reconstruction objec-\ntive can learn a holistic representation of the image regardless\nof whether self-attention is used in the decoder. Visually,\nin Fig. 1.(c) and 2, the reconstruction results of CrossMAE\nare similar to MAE, although CrossMAE can only attend to\nthe visible patches instead of having diffuse attention over\nall nearby patches. To our surprise, the downstream perfor-\nmance of CrossMAE is on par with MAE, while maintaining\na higher efficiency enabled by cross-attention and partial\nreconstruction. We show that a ViT-B model trained with\nCrossMAE partial reconstruction achieves a classification\naccuracy of 83.5% on the ImageNet validation set, which\nsurpasses its full-reconstruction MAE counterpart. In object\ndetection and instance segmentation on COCO, CrossMAE\nachieves 52.1 AP and 46.3 AP, again surpassing MAE. Fi-\nnally, we show that with ViT-L, CrossMAE enjoys improved\nscalability when compared to MAE.\n2\n2. Related Works\n2.1. Self-Supervised Learning\nIn self-supervised representation learning (SSL), a model\ntrains on a pretext task where the supervision comes from the\ninput data itself without relying on labeled data. Contrastive\nlearning is a popular approach to SSL that aims to learn\nrepresentations by contrasting positive and negative sam-\nples, such as SimCLR [13], CPC [45], MoCo [30], MoCo\nv2 [14], BYOL [27], and DINO [10]. Additionally, group-\ninstance contrastive learning works, such as DeepCluster [8],\nCLD [58] and SwAV [9], integrate clustering into contrastive\nlearning to improve the quality of learned representations.\nAn alternative method for self-supervised learning is gen-\nerative modeling, which focuses on acquiring a generative\nmodel capable of capturing the underlying data distribution.\nVAE/GAN [36] merges the strengths of variational autoen-\ncoders (VAEs) and generative adversarial networks (GANs)\nto acquire disentangled representations of data. PixelCNN,\nPixelVAE, and PixelTransformer [28, 54, 55] generate im-\nages pixel by pixel, taking into account the context of previ-\nously generated pixels. Masked modeling, a large subclass\nof generative modeling, is discussed in the following subsec-\ntion. After the pre-training stage, these generative models\ncan be finetuned for many downstream applications.\n2.2. Masked Modeling\nMasked modeling learns representations by reconstructing\na masked portion of the input. Pioneering works in natural\nlanguage processing (NLP) present various such pretrain-\ning objectives. BERT [20] and its extensions [35, 42] use a\nbidirectional transformer and present few-shot learning ca-\npabilities from masked language modeling. GPT [6, 47, 48],\nuses autoregressive, causal masking and demonstrates multi-\ntask, few-shot, and in-context learning capabilities.\nEarly works in computer vision, such as Stacked De-\nnoising Autoencoders [57] and Context Encoder [46], in-\nvestigated masked image modeling as a form of denoising\nor representation learning. Recently, with the widespread\nuse of transformer [21] as a backbone vision architecture,\nwhere images are patchified and tokenized as sequences,\nresearchers are interested in how to transfer the success in\nlanguage sequence modeling to scale vision transformers.\nBEiT [4], MAE [31], and SimMIM [60] are a few of the\nearly works that explored BERT-style pretraining of vision\ntransformers. Compared to works in NLP, both MAE and\nSimMIM [31, 60] find that a much higher mask ratio com-\npared to works in NLP is necessary to learn good visual\nrepresentation. Many recent works further extend masked\npretraining to hierarchical architectures [41, 60] and study\ndata the role of data augmentation [11, 22]. Many subse-\nquent works present similar successes of masked pretraining\nfor video [23, 29, 52], language-vision and multi-modal\nFigure 3. MAE [31] concatenates all mask tokens with the visible\npatch features from a ViT encoder and passes them to a decoder\nwith self-attention blocks to reconstruct the original image. Patches\nthat correspond to visible tokens are then dropped, and an L2 loss is\napplied to the rest of the reconstruction as the pretraining objective.\nCrossMAE instead uses cross-attention blocks in the decoder to\nreconstruct only a subset of the masked tokens.\npretraining [2, 24, 40] and for learning both good represen-\ntations and reconstruction capabilities [38, 59].\nHowever, BERT-style pretraining requires heavy use of\nself-attention, which makes computational complexity scale\nas a polynomial of sequence length. PixelTransformer [54]\nand DiffMAE [59] both use cross-attention for masked\nimage generation and representation learning.\nSiamese\nMAE [29] uses an asymmetric masking pattern and decodes\nframes of a video condition on an earlier frame. In these\nsettings, all masked patches are reconstructed. In this work,\nwe investigate if learning good features necessitates high\nreconstruction quality and if the entire image needs to be\nreconstructed to facilitate representation learning. Similar\nin spirit, PCAE [37] progressively discards redundant mask\ntokens through its network, leading to a few tokens for re-\nconstruction. In comparison, we minimally modify MAE\nand start decoding with a random subset of mask tokens.\n2.3. Applications of Cross-Attention\nIn addition to the prevalent use of self-attention in computer\nvision, cross-attention has shown to be a cost-effective way\nto perform pooling from a large set of visible tokens. In-\ntuitively, cross-attention can be seen as a parametric form\nof pooling, which learnably weighs different features. [53]\nreplaces mean pooling with cross-attention pooling and finds\nimprovement in ImageNet classification performance. [33]\nuses cross-attention to efficiently process large volumes of\nmulti-modal data. Cross-attention is also widely used for\nobject detection. [7] utilizes query tokens as placeholders\nfor potential objects in the scene. [16, 17] further extend this\nconcept by introducing additional query tokens to specif-\nically tackle object segmentation in addition to the query\ntokens for object detection. In this work, we are interested\n3\nFigure 4. Overview of CrossMAE. (a) The vanilla version of CrossMAE uses the output of the last encoder block as the keys and queries\nfor cross-attention. The first decoder block takes the sum of mask tokens and their corresponding positional embeddings as queries, and\nsubsequent layers use the output of the previous decoder block as queries to reconstruct the masked patches. (b) Unlike the decoder\nblock in [56], the cross-attention decoder block does not contain self-attention, decoupling the generation of different masked patches. (c)\nCrossMAE\u2019s decoder blocks can leverage low-level features for reconstruction via inter-block attention. It weighs the intermediate feature\nmaps, and the weighted sum of feature maps is used as the key and value for each decoder block.\nin cross-attention as an efficient method for self-supervised\nrepresentation learning.\n3. CrossMAE\nThis section is organized as follows. In Sec. 3.1, we first re-\nvisit vanilla Masked Autoencoders. In Sec. 3.2, we propose\nto use cross-attention instead of self-attention in the decoder\nfor reconstruction. Thanks to a decoding architecture with-\nout self-attention, we achieve further efficiency gains by\nreconstructing only a subset of mask tokens for faster pre-\ntraining in Sec. 3.3. Since the use of cross-attention allows\ndifferent features for different decoder blocks, in Sec. 3.4,\nwe further propose inter-block attention to allow different\ndecoder blocks to focus on different encoder features, which\nrelieves the need to carry all information throughout the\nneural networks and allows for enhanced feature learning.\n3.1. Preliminaries: Masked Autoencoders\nMasked Autoencoders (MAE) [31] pretrain Vision Trans-\nformers (ViTs) [21]. Each image input is first patchified,\nand then a random subset of the patches is selected as the\nvisible patches. As depicted in Fig. 3, the visible patches,\nconcatenated with a learnable class token (<cls>), are sub-\nsequently fed into the ViT encoder, which outputs a set of\nfeature latents. The latent vectors, concatenated with the\nsum of the positional embeddings of the masked patches\nand the learnable mask token, are passed into the MAE de-\ncoder. The decoder blocks share the same architecture as\nthe encoder blocks (i.e., both are transformer blocks with\nself-attention layers). Note that the number of tokens fed\ninto the decoder is the same length as the original input,\nand the decoding process assumes that the decoded tokens\ndepend on both visible and masked tokens. Decoder outputs\nare then passed through a fully connected layer per patch for\nimage reconstruction. After the reconstruction is generated,\nthe loss is applied only to the masked positions, while the\nreconstructions for visible spatial locations are discarded.\nRecall in Sec. 1, to study the properties of MAE, we\nmeasure the mean attention value across all attention maps\nover the ImageNet validation set. We group the attention val-\nues by cross-attention and self-attention between visible and\nmasked tokens. We observe that in the decoding process of\nan MAE, mask tokens attend disproportionately to the class\ntoken and the visible tokens (see Figure 1.(b)). This moti-\nvates us to make design decisions and conduct experiments\nspecifically to answer the following two questions:\n1. Can good representations be learned if masked tokens\ncan attend only to visible tokens?\n2. Can we improve pretraining efficiency by reconstructing\nonly part of an image?\n4\n3.2. Reconstruction with Cross-Attention\nTo address the first question, our method substitutes the\nself-attention mechanism in the decoder blocks with cross-\nattention.\nSpecifically, the decoder employs multi-head\ncross-attention where the queries are the output from previ-\nous decoder blocks (or the sum of position embedding of the\nmasked patches and mask token for the first decoder block.)\nThe keys and values are derived from the encoded features.\nIn the most basic CrossMAE, the output from the final en-\ncoder block is used as the key and value tokens for all layers\nof the decoder, as illustrated in Fig. 4(a). Further exploration\nin Sec.3.4 reveals that utilizing a weighted mean of selected\nencoder feature maps can be beneficial. The residual con-\nnections in each decoder block enable iterative refinement\nof decoded tokens as they progress through decoder blocks.\nDiverging from the original transformer architecture [56],\nour decoder omits the precursory causal self-attention stage\nbefore the introduction of multi-head cross-attention. This\nelimination, coupled with the fact that layer normalization\nand residual connections are only applied along the feature\naxis but not the token axis, enables the independent decoding\nof tokens. This design choice is evaluated in the ablation\nstudy section to determine its impact on performance.\nGiven the disparity in the dimensions of the encoder and\ndecoder, MAE adapts the visible features to the decoder\u2019s\nlatent space using a Multilayer Perceptron (MLP). However,\nin CrossMAE, as encoder features are integrated at various\ndecoder blocks, we embed the projection within the multi-\nhead cross-attention module.\nThis approach also mirrors aspects of Perceiver IO [33],\nwhere cross-attention is employed to amalgamate features\npertinent to the task at hand. However, in contrast, we do\nnot restrict the architecture to a single cross-attention unit.\nIn our method, the masked tokens are decoded in a manner\nmore akin to the traditional transformer paradigm [56].\n3.3. Partial Reconstruction\nThe fact that CrossMAE uses cross-attention rather than self-\nattention in the decoder blocks brings an additional benefit\nover the original MAE architecture. Recall that mask tokens\nare decoded independently and thus there is no exchange of\ninformation between them, to obtain the reconstructions at a\nspecific spatial location, CrossMAE only needs to pass the\ncorresponding mask tokens to the cross-attention decoder.\nThis allows efficient partial reconstruction in contrast to\nthe original MAE architecture which needs to pass all the\nmasked tokens as the input of the decoder blocks due to the\nexistence of self-attention in the decoder blocks.\nTo address the second question in Sec. 3.1, rather than\ndecoding the reconstruction for all masked locations, we\nonly compute the reconstruction on a random subset of the\nlocations and apply the loss to the decoded locations. Specif-\nically, we name the ratio of predicted tokens to all image\ntokens as prediction ratio (\u03b3), and the mask ratio (p). Then\nthe prediction ratio is bounded between \u03b3 \u2208 (0, p]. Because\nwe are sampling within the masked tokens uniformly at ran-\ndom and the reconstruction loss is a mean square error on\nthe reconstructed patches, the expected loss is the same as\nin MAE, while the variance is (p/\u03b3) times larger than the\nvariance in MAE. Empirically, we find that scaling the learn-\ning rate of MAE (\u03b2) to match the variance (i.e. setting the\nlearning rate as \u03b3\u03b2/p)) helps with model performance. Since\ncross-attention has linear complexity with respect to the num-\nber of masked tokens, this partial reconstruction paradigm\nleads to a decrease in computation complexity. Empirically,\nwe find that the quality of the learned representations is not\ncompromised by this approach.\n3.4. Inter-block Attention\nMAE combines the feature of the last encoder block with the\nmask tokens as the input to the self-attention decoder. This\ndesign poses the latent feature as an information bottleneck.\nConcretely, it leaves no room for any residual connection\nfor the decoder to leverage early encoder features since the\ndecoder blocks sequentially update and improve on the de-\ncoded features. In contrast, the cross-attention decoder in\nCrossMAE decouples queries from keys and values. This\nadds flexibility as different cross-attention blocks can take\ndifferent feature maps as keys and values and thus no longer\nneed to only use features from the last encoder block.\nBesides simply using the feature from the last encoder\nblock, one naive choice is to give the feature of the ith\nencoder block to the last ith decoder (e.g., feed the feature of\nthe first encoder to the last decoder), in a U-Net-like fashion.\nHowever, this assumes the depth of the decoder to be the\nsame as the depth of the encoder, which does not hold in\nMAE where an asymmetric design is adopted.\nInstead of manually selecting the feature for each decoder\nblock, we follow our design choice of using cross-attention\nblocks for inter-patch spatial cross-attention and propose\ninter-block attention for feature fusion for each decoder\nblock 4(c). Analogous to the inter-patch cross-attention\nthat takes a weighted sum of the visible token embeddings\nacross the patch dimensions to update the embeddings of\nmasked tokens, inter-block attention takes a weighted sum\nof the visible token embeddings across different input blocks\nat the same spatial location to fuse the input features from\nmultiple blocks into one feature map for each decoder block.\nConcretely, each decoder block takes a weighted linear\ncombination of encoder feature maps {fi} as keys and values.\nSpecifically, for each key/value token tk in decoder block\nk in a model with encoder depth n, we initialize a weight\nwk \u2208 Rn \u223c N(0, 1/n). Then tk is defined as\ntk =\nn\nX\nj=1\nwk\nj fj.\n(1)\n5\nMethod\nPre-train Data\nViT-S\nViT-B\nViT-L\nSupervised [31, 50]\n-\n79.0\n82.3\n82.6\nDINO [10]\nIN1K\n-\n82.8\n-\nMoCo v3 [15]\nIN1K\n81.4\n83.2\n84.1\nBEiT [4]\nIN1K+DALLE\n-\n83.2\n85.2\nMultiMAE [3]\nIN1K\n-\n83.3\n-\nMixedAE [11]\nIN1K\n-\n83.5\n-\nCIM [22]\nIN1K\n81.6\n83.3\n-\nMAE [31]\nIN1K\n78.9\n83.3\n85.4\nCrossMAE (25%)\nIN1K\n79.2\n83.5\n85.4\nCrossMAE (75%)\nIN1K\n79.3\n83.7\n85.4\nTable 1. ImageNet-1K classification accuracy. CrossMAE per-\nforms on par, if not better than MAE without self-attention in the\ndecoder. All experiments are run with 800 epochs. The best results\nare in bold while the second best results are underlined.\nIn addition to feature maps from different encoder blocks,\nwe also include the inputs to the first encoder block to al-\nlow the decoder to leverage more low-level information to\nreconstruct the original image. We can select a subset of\nthe feature maps from the encoder layers instead of all fea-\nture maps. This reduces the computation complexity of the\nsystem. We ablate over this design choice in Tab. 3d.\nWe show that using the weighted features rather than sim-\nply using the features from the last block greatly improves\nthe performance of our CrossMAE. Interestingly, as we will\nshow in Sec. 4.4, early decoder blocks focus on the informa-\ntion from the later encoder blocks, and later decoder blocks\nfocus on the information from the early encoder blocks in\nthe process of learning to achieve better reconstructions.\n4. Experiments\nWe perform self-supervised pretraining on ImageNet-1K,\nfollowing MAE [31]. We perform minimal hyperparame-\nter tuning, maintaining consistency with MAE\u2019s parameters\nexcept for the learning rate and decoder depth. The hyper-\nparameters were initially determined on ViT-Base and then\ndirectly applied to both ViT-Small and ViT-Large. Both\nCrossMAE and MAE are trained for 800 epochs. Please re-\nfer to the supplementary material for implementation details.\n4.1. ImageNet Classification\nSetup. The model performance is evaluated with end-to-end\nfine-tuning, with top-1 accuracy used for comparison. Same\nas in Figure. 2, we compare two versions of CrossMAE: one\nwith a prediction ratio of 25% (1/3 of the mask tokens) and\nanother with 75% (all mask tokens). Both models are trained\nwith a mask ratio of 75% and a decoder depth of 12.\nResults. As shown in Tab. 1, CrossMAE outperforms vanilla\nMAE trained on the same ViT-B encoder in terms of fine-\ntuning accuracy. This shows that replacing the self-attention\nwith cross-attention does not degrade the quality of the pre-\ntrained model in terms of downstream classification. Cross-\nAPbox\nAPmask\nMethod\nPre-train Data\nViT-B\nViT-L\nViT-B\nViT-L\nSupervised [39]\nIN1K w/ labels\n47.6\n49.6\n42.4\n43.8\nMoCo v3 [15]\nIN1K\n47.9\n49.3\n42.7\n44.0\nBEiT [5]\nIN1K+DALLE\n49.8\n53.3\n44.4\n47.1\nMixedAE [11]\nIN1K\n50.3\n-\n43.5\n-\nMAE [39]\nIN1K\n51.2\n54.6\n45.5\n48.6\nCrossMAE\nIN1K\n52.1\n54.9\n46.3\n48.8\nTable 2. COCO instance segmentation. Compared to previous\nmasked visual pretraining works, CrossMAE performs favorably\non object detection and instance segmentation tasks.\nMAE outperforms other strong baselines such as DINO [10],\nMoCo v3 [15], BEiT [4], and MultiMAE [3].\n4.2. Object Detection and Instance Segmentation\nSetup. We additionally evaluate models pretrained with\nCrossMAE for object detection and instance segmentation,\nwhich require deeper spatial understanding than ImageNet\nclassification. Specifically, we follow ViTDet [39], a method\nthat leverages a Vision Transformer backbone for object\ndetection and instance segmentation. We report box AP\nfor object detection and mask AP for instance segmentation,\nfollowing MAE [31]. We compare against 4 baselines: super-\nvised pre-training, MoCo-v3 [15], BEiT [5], and MAE [31].\nResults. As listed in Tab. 2, CrossMAE, with the default\n75% prediction ratio, performs better compared to these\nstrong baselines, including vanilla MAE. This suggests that\nsimilar to MAE, CrossMAE performance on ImageNet posi-\ntively correlates with instance segmentation. Additionally,\nCrossMAE\u2019s downstream performance scales similarly to\nMAE as the model capacity increases from ViT-B to ViT-L.\n4.3. Ablations\nCross-Attention vs Self-Attention.\nAs summarized in\nTab. 3a, CrossMAE that uses a cross-attention-only de-\ncoder has better downstream performance than vanilla MAE,\nwhich is consistent with the primary observations in Sec. 4.1.\nWe further show that applying both cross-attention and self-\nattention together does not lead to additional benefits in\nterms of fine-tuning performance compared to using cross-\nattention only. This suggests that using only cross-attention\nis sufficient for learning good representations.\nMask Ratio and Prediction Ratio. We ablate over a range\nof mask ratio (i.e., the ratio of mask tokens to all tokens) and\nthe prediction ratio (i.e., the ratio of reconstructed tokens\nto all tokens) in Tab. 3b and Tab. 3c. We observe that our\nmethod is not sensitive to varying masked tokens. Further-\nmore, although predicting the reconstructions from all the\nmask tokens leads to the best pre-trained model, the differ-\nence between full prediction and partial reconstruction is\nmarginal. Specifically, CrossMAE can be trained to recon-\nstruct as few as 15% of the tokens in the decoder rather than\n100% of the tokens as required by the vanilla MAE baseline,\n6\nMethod\nAcc. (%)\nMAE\n83.0\nCrossMAE\n83.3\nCrossMAE + Self-Attn\n83.3\n(a) Attention type in decoder blocks.\nAdding back self-attention between mask\ntokens does not improve performance.\nMask Ratio\nAcc. (%)\n65%\n83.5\n75%\n83.3\n85%\n83.3\n(b) Mask ratio. CrossMAE has consistent\nperformance across high mask ratios.\nPred. Ratio\nAcc. (%)\n15%\n83.1\n25%\n83.2\n75%\n83.3\n(c) Prediction ratio. CrossMAE performs\nwell even when only a fraction of mask to-\nkens are reconstructed.\n# Feature\nMaps Fused\nAcc.\n(%)\n1\n82.9\n3\n83.3\n6\n83.5\n12\n83.3\n(d) Inter-block attention. A combination\nof six select encoder feature maps is best.\nDecoder\nDepth\nAcc.\n(%)\n1\n83.0\n4\n83.1\n8\n83.1\n12\n83.3\n(e) Decoder depth.\nCrossMAE perfor-\nmance scales with decoder depth.\nImage\nResolution\nAcc.\n(%)\n224\n83.2\n448\n84.6\n(f) Input resolution. CrossMAE scales to\nlonger input sequences.\nTable 3. Ablations on CrossMAE. We report fine-tuning performance on ImageNet-1K classification with 400 epochs (i.e., half of the full\nexperiments) with ViT-B/16. MAE performance is reproduced using the official MAE code. Underline indicates the default setting for\nCrossMAE. Bold indicates the best hyperparameter among the tested ones. 1 feature map fused (row 1, Table 2d) indicates using only the\nfeature from the last encoder block. We use 25% prediction ratio for both settings in Table 2f to accelerate training.\nFigure 5. We compare ViT-B which is pre-trained for 800 epochs\nwith different variants of CrossMAE v.s. MAE. For CrossMAE,\nwe vary the prediction ratio p and number of decoder blocks n,\nand we denote each as (p, n). While all experiments are run with\ninter-block attention, CrossMAE has lower decoder FLOPS than\nMAE [31] and performs on par or better.\nyet achieving similar downstream finetuning performance.\nThis result suggests that a good representation can be learned\nby reconstructing only part of an image.\nInter-block Attention. We also vary the number of encoder\nfeature maps that are fused with our inter-block attention as\nan ablation. In addition to simply taking the feature from\nthe last encoder block (i.e., using only one feature map)\nand all encoder blocks (i.e., using all 12 feature maps), we\nuniformly select feature maps to be fused in terms of their\nencoder block index. As shown in Tab. 3d, using only the\nlast feature map leads to a minor degradation of performance\ncompared to using all feature maps. Furthermore, adding\neven a subset of feature maps boosts the performance of\nCrossMAE, with the best performance reached when 6 fea-\nture maps are fused. This indicates that CrossMAE does not\nrequire all feature maps to obtain its optimal performance,\nwhich further justifies the efficiency of CrossMAE.\nDecoder Depth.\nAs shown in Tab. 3e, we show that us-\ning a decoder of 12 blocks slightly improves downstream\nMethod\nPred.\nRatio\nDecoder\nDepth\nMemory\nRuntime\nMAE\n0.75\n8\nOOM (>81920)\n103.45\nCrossMAE\n0.25\n12\n41630\n65.80\nTable 4. Pretraining runtime and GPU memory comparison on\nViT-B for 10 epochs using 2 NVIDIA A100 80GB GPUs. Memory\nis measured in MB per GPU, and runtime is in minutes. Please\nrefer to Tab. 5 for a complete comparison. MAE trained without\nusing gradient accumulation at the default batch size does not fit\nonto 2 GPUs, thus the memory usage is not reported.\nperformance compared to shallower decoders. CrossMAE\nperforms on par with the vanilla MAE even with as few as\none decoder block, which shows CrossMAE\u2019s capability for\nefficiently pre-train vision transformers. We further conduct\nfull-scale experiments to compare the impact of decoder\ndepth and prediction ratio, as they can both significantly\nimpact the efficiency of the model. Results are summarized\nin Fig. 5 and the runtimes is compared in Tab. 4. We find\nthat a model trained with a small prediction ratio can benefit\nmore from a deeper decoder.\nInput Resolution. We test CrossMAE on longer token\nlengths by increasing the image resolution without changing\nthe patch size. As the resolution grows from 224 to 448,\nthe image token length increases from 197 to 785, which\nposes great challenges to the scalability of existing methods.\nThus, we deliberately choose the CrossMAE variant with\na 25% prediction ratio for higher efficiency. In Tab. 3f, we\nobserve that the classification accuracy positively correlates\nwith the input resolution, suggesting that CrossMAE can\nscale to long input sequences.\n7\nFigure 6. We visualize the output of each decoder block. (a-b) Different decoder blocks play different roles in the reconstruction, with\nmost details emerging at later decoder blocks, which confirms the motivation for inter-block attention. (c) Visualizations of inter-block\nattention shows that different decoder blocks indeed attend to feature from different encoder blocks, with later blocks focusing on\nearlier encoder features to achieve reconstruction.\n4.4. Visualizations\nTo further understand the properties of the updated recon-\nstruction objective, we devise a method to visualize the re-\nconstruction of CrossMAE\u2019s decoder blocks. Additionally,\nwe visualize the weights applied to each encoder feature map\nto provide intuition for inter-block attention.\nVisualizing Per-block Reconstruction. Rather than only\nvisualizing the final reconstruction, we propose a more fine-\ngrained visualization approach that allows us to precisely\nunderstand the effect and contribution of each decoder block.\nTwo key observations allow for such visualization. 1)\nTransformer blocks have residual connections from their\ninputs to outputs. We denote fi as the output of decoder\ni, gi(\u00b7) as its residual path, with fi = fi\u22121 + gi(fi\u22121).\n2) The output of the last decoder block is processed by\nthe reconstruction head h to generate the reconstruction.\nNote that h(\u00b7) is linear because it is composed of two linear\nfunctions: a layer-norm [1] and a linear layer. Let D be\nthe decoder depth, f0 be the input to the first decoder block\nand y be the final reconstruction. We can recursively define\ny = h(fD\u22121 + gD(fD\u22121)). We expand it and by linearity\nof h we have:\ny = h(f0 + g1(f0) + \u00b7 \u00b7 \u00b7 + gD(fD\u22121))\n=\nh(f0)\n| {z }\nPos Embed. + Mask Token\n+ h(g1(f0))\n|\n{z\n}\nBlock 1\n+ \u00b7 \u00b7 \u00b7 + h(gD(fD\u22121))\n|\n{z\n}\nBlock D\nThis decomposition allows us to express the reconstruc-\ntion as an image stack, where the sum of all the levels gives\nus the final reconstruction. We present the visualization in\nFig. 6 and analyze the contribution of each layer in the final\nreconstruction. We denormalize the output by patch mean\nand std. for visualization.\nFrom Fig. 6 (a) and (b), we observe that different decoder\nblocks play different roles in reconstruction, with most de-\ntails emerging at later decoder blocks. This supports the\nhypothesis for the need to get low-level information from\nearly encoder blocks, motivating inter-block attention.\nVisualizing Inter-block Attention Maps We visualize the\nattention maps of inter-block attention in 6(c). This shows\nthat the CrossMAE model naturally leverages the inter-block\nattention to allow the later decoder blocks to focus on earlier\nencoder features to achieve reconstruction and allow the\nearlier decoder blocks to focus on later encoder features.\nThis also motivates the need for different decoder blocks to\nattend to different encoder features, which is aligned with\nthe performance gains obtained with inter-block attention.\n5. Discussion and Conclusion\nIn this paper, we reassess the decoding mechanisms within\nMAE. Our exploration of MAE questions the necessity of\nusing self-attention for reconstructing masked patches. To\ntest this hypothesis, we design CrossMAE, an MAE-based\n8\nframework that 1) uses cross-attention for reconstruction,\n2) decodes a fraction of the masked patches, and 3) lever-\nages different encoder features for reconstruction. Cross-\nMAE shows similar performance and scaling properties as\nMAE while being more efficient. On one hand, CrossMAE\nopens the possibility to scale visual pretraining to longer\ncontexts, especially in the setting of video pretraining, cov-\nering large swaths of in-the-wild visual data that have so\nfar been computationally prohibitive to fully utilize. On the\nother hand, our investigations also give rise to worry: intu-\nitively, self-attention among mask tokens in MAE should aid\nin consistent image reconstruction. However, as we show,\nthe presence or absence of self-attention is almost uncorre-\nlated with the quality of the learned representations in MAE.\nThis may suggest that there exists a better way to lever-\nage self-attention in masked visual pretraining. We hope\nthat CrossMAE can serve as a starting point for the field\nto better explore the trade-off between self-attention and\ncross-attention for masked pretraining methods, potentially\nleading to truly scalable vision learners.\nAcknowledgments. We thank Sophia Koepke, Yossi Gan-\ndelsman, and Qianqian Wang for their helpful discussions.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450, 2016.\n8\n[2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir\nZamir. Multimae: Multi-modal multi-task masked autoen-\ncoders. arXiv:2204.01678, 2022. 3\n[3] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir\nZamir. Multimae: Multi-modal multi-task masked autoen-\ncoders. In European Conference on Computer Vision, pages\n348\u2013367. Springer, 2022. 6\n[4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021. 3, 6\n[5] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\nof image transformers. In ICLR, 2022. 1, 6\n[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. 2020. 3\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In European con-\nference on computer vision, pages 213\u2013229. Springer, 2020.\n3\n[8] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning of\nvisual features. In Proceedings of the European conference\non computer vision (ECCV), pages 132\u2013149, 2018. 3\n[9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. Ad-\nvances in neural information processing systems, 33:9912\u2013\n9924, 2020. 3\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 3, 6\n[11] Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li,\nand Dit-Yan Yeung. Mixed autoencoder for self-supervised vi-\nsual representation learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 22742\u201322751, 2023. 3, 6\n[12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo\nJun, David Luan, and Ilya Sutskever. Generative pretraining\nfrom pixels. 2020. 2\n[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on\nmachine learning, pages 1597\u20131607. PMLR, 2020. 3\n[14] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Im-\nproved baselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020. 3\n[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\nstudy of training self-supervised vision transformers. arXiv\npreprint arXiv:2104.02057, 2021. 6\n[16] Bowen Cheng, Alexander G. Schwing, and Alexander Kir-\nillov. Per-pixel classification is not all you need for semantic\nsegmentation. 2021. 3\n[17] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander\nKirillov, and Rohit Girdhar. Masked-attention mask trans-\nformer for universal image segmentation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 1290\u20131299, 2022. 3\n[18] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le.\nRandaugment: Practical automated data augmentation with\na reduced search space. arxiv e-prints, page. arXiv preprint\narXiv:1909.13719, 4, 2019. 2\n[19] Tri Dao. FlashAttention-2: Faster attention with better paral-\nlelism and work partitioning. 2023. 2, 3\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transform-\ners for language understanding. 2019. 3\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2020. 3,\n4\n[22] Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, and\nFuru Wei. Corrupted image modeling for self-supervised\nvisual pre-training. In The Eleventh International Conference\non Learning Representations, 2023. 3, 6\n[23] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaim-\ning He. Masked autoencoders as spatiotemporal learners. In\nAdvances in Neural Information Processing Systems, 2022. 3\n9\n[24] Xinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey\nLevine, and Pieter Abbeel.\nMultimodal masked autoen-\ncoders learn transferable representations.\narXiv preprint\narXiv:2205.14204, 2022. 3\n[25] Priya Goyal, Piotr Doll\u00b4ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large minibatch\nsgd: Training imagenet in 1 hour. arXiv:1706.02677, 2017. 2\n[26] Priya Goyal, Piotr Doll\u00b4ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He.\nAccurate, large mini-\nbatch sgd: Training imagenet in 1 hour.\narXiv preprint\narXiv:1706.02677, 2017. 2\n[27] Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\nto self-supervised learning. Advances in neural information\nprocessing systems, 33:21271\u201321284, 2020. 3\n[28] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali\nTaiga, Francesco Visin, David Vazquez, and Aaron Courville.\nPixelvae: A latent variable model for natural images. arXiv\npreprint arXiv:1611.05013, 2016. 3\n[29] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese\nmasked autoencoders.\narXiv preprint arXiv:2305.14344,\n2023. 3\n[30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n9729\u20139738, 2020. 3\n[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n16000\u201316009, 2022. 1, 2, 3, 4, 6, 7\n[32] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nComputer Vision\u2013ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11\u201314, 2016, Proceed-\nings, Part IV 14, pages 646\u2013661. Springer, 2016. 2\n[33] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,\nCarl Doersch, Catalin Ionescu, David Ding, Skanda Kop-\npula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al.\nPerceiver io: A general architecture for structured inputs &\noutputs. arXiv preprint arXiv:2107.14795, 2021. 3, 5\n[34] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross\nGirshick. Segment anything. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n4015\u20134026, 2023. 1\n[35] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert\nfor self-supervised learning of language representations. In\nInternational Conference on Learning Representations, 2020.\n3\n[36] Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo\nLarochelle, and Ole Winther. Autoencoding beyond pixels\nusing a learned similarity metric. In International conference\non machine learning, pages 1558\u20131566. PMLR, 2016. 3\n[37] Jin Li, Yaoming Wang, XIAOPENG ZHANG, Yabo Chen,\nDongsheng Jiang, Wenrui Dai, Chenglin Li, Hongkai Xiong,\nand Qi Tian. Progressively compressed auto-encoder for\nself-supervised representation learning. In The Eleventh In-\nternational Conference on Learning Representations, 2023.\n3\n[38] Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han\nZhang, Dina Katabi, and Dilip Krishnan. Mage: Masked\ngenerative encoder to unify representation learning and image\nsynthesis. arXiv preprint arXiv:2211.09117, 2022. 3\n[39] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In European Conference on Computer Vision, pages\n280\u2013296. Springer, 2022. 1, 6, 2\n[40] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-\nhofer, and Kaiming He. Scaling language-image pre-training\nvia masking. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 23390\u2013\n23400, 2023. 3\n[41] Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, and Hong-\nsheng Li.\nMixmae:\nMixed and masked autoencoder\nfor efficient pretraining of hierarchical vision transformers.\narXiv:2205.13137, 2022. 3\n[42] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019. 3\n[43] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. 2017. 2\n[44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 2\n[45] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 3\n[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros. Context encoders: Feature learn-\ning by inpainting. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2536\u20132544,\n2016. 1, 3\n[47] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 3\n[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsuper-\nvised multitask learners. 2019. 3\n[49] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel,\nJitendra Malik, and Trevor Darrell. Real-world robot learning\nwith masked visual pre-training. In Conference on Robot\nLearning, pages 416\u2013426. PMLR, 2023. 1\n[50] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai,\nRoss Wightman, Jakob Uszkoreit, and Lucas Beyer. How\nto train your vit? data, augmentation, and regularization\n10\nin vision transformers. Transactions on Machine Learning\nResearch, 2022. 6\n[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n2818\u20132826, 2016. 2\n[52] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Video-\nMAE: Masked autoencoders are data-efficient learners for\nself-supervised video pre-training. In Advances in Neural\nInformation Processing Systems, 2022. 3\n[53] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr\nBojanowski, Armand Joulin, Gabriel Synnaeve, and Herv\u00b4e\nJ\u00b4egou. Augmenting convolutional networks with attention-\nbased aggregation, 2021. 3\n[54] Shubham Tulsiani and Abhinav Gupta. Pixeltransformer:\nSample conditioned signal generation. In Proceedings of the\n38th International Conference on Machine Learning, pages\n10455\u201310464. PMLR, 2021. 3\n[55] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,\nOriol Vinyals, Alex Graves, et al. Conditional image genera-\ntion with pixelcnn decoders. Advances in neural information\nprocessing systems, 29, 2016. 3\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. 2017. 4, 5, 1\n[57] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua\nBengio, Pierre-Antoine Manzagol, and L\u00b4eon Bottou. Stacked\ndenoising autoencoders: Learning useful representations in\na deep network with a local denoising criterion. Journal of\nmachine learning research, 11(12), 2010. 3\n[58] Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised\nfeature learning by cross-level instance-group discrimination.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 12586\u201312595, 2021. 3\n[59] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao\nLi, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille,\nand Christoph Feichtenhofer. Diffusion models as masked\nautoencoder. In ICCV, 2023. 3\n[60] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 9653\u20139663, 2022. 1, 3\n[61] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 6023\u20136032, 2019. 2\n[62] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In International Conference on Learning Representa-\ntions, 2018. 2\n11\nRethinking Patch Dependence for Masked Autoencoders\nSupplementary Material\n1. Implementation details\n1.1. Attention Calculation\nTo compare the attention values for mask tokens in vanilla\nMAE (Fig. 1), we trained a ViT-B/16 MAE for 800 epochs\nusing the default hyperparameters provided in [31]. For each\nimage, we randomly generate a 75% binary mask (m) for all\ntokens, with mi = 1 representing a token being masked and\nmi = 0 otherwise. During the forward pass of the decoder,\nfor each self-attention operation, the attention map is stored.\nThis means that for the default MAE, a total of 8 attention\nmaps, each with 16 attention heads are stored. Based on the\nmask pattern, we calculate the outer product (m \u00b7 m\u22a4) for\nthe self-attention among mask tokens, and m \u00b7 (1 \u2212 m\u22a4) for\nthe cross-attention from the mask token to the visible tokens.\nWe then calculate the average across all feature maps and\nattention heads for self-attention and cross-attention to get\nthe image average values. Lastly, we averaged across the\nentire ImageNet validation set to obtain the final values.\n1.2. Inter-Block Attention\nWe tried a few implementations for inter-block attention and\nfound the following implementation to be the fastest and\nmost memory-efficient. In this implementation, we combine\ninter-block attention for all encoder layers as a single forward\npass of a linear layer. For each decoder block, we index into\nthe output tensor to extract the corresponding feature map,\nand a layer norm will be applied before the feature map\nis fed into the decoder block. Other alternatives we tried\ninclude 1) performing separate inter-block attentions before\neach decoder block, and 2) 1x1 convolution on the stacked\nencoder feature maps.\nIn MAE, there exists a layer norm after the last encoder\nfeature map before feeding into the decoder. In our imple-\nmentation, we only add layer norm after inter-block attention.\nWe find that adding an additional layer norm before inter-\nblock attention to each encoder feature map does not lead to\nimprovements in model performance but will significantly\nincrease GPU memory usage.\nThe pseudo-code of inter-block attention is the following:\n1 class InterBlockAttention():\n2\ndef __init__(self, num_feat_maps,\ndecoder_depth):\n3\nself.linear = Linear(num_feat_maps,\ndecoder_depth, bias=False)\n4\nstd_dev = 1. / sqrt(num_feat_maps)\n5\ninit.normal_(self.linear.weight, mean=0.,\nstd=std_dev)\n6\n7\ndef forward(self, feature_maps : list):\n8\n\"\"\"\n9\nfeature_maps: a list of length\nnum_feat_maps, each with dimension\n10\nBatch Size x Num. Tokens x Embedding Dim.\n11\n\"\"\"\n12\nstacked_feature_maps = stack(feature_maps\n, dim=-1)\n13\nreturn self.linear(stacked_feature_maps)\n1.3. Ablation that Adds Self-Attention\nIn Section 4.3 (a), we propose adding self-attention back to\nCrossMAE as an ablation. In that particular ablation study,\nwe analyze the effect of self-attention between the masked\ntokens, which can be used to improve the consistency for\nreconstruction. Specifically, we modify the formulation in\nthe original transformer paper [56], where the mask/query\ntokens are first passed through a multi-head self-attention\nand a residual connection before being used in the multi-\nheaded cross-attention with the features from the encoder.\nThe primary difference with the vanilla transformer decoder\nimplementation [56] is we do not perform casual masking in\nthe multi-head self-attention. Please reference Fig. 7 for a\nmore visual presentation of the method.\nFigure 7. Modification for self-attention ablation\n1\nMethod\nMask\nRatio\nPrediction\nRatio\nDecoder\nDepth\nInterblock\nAttention\nFlashAttn-2 [19]\nMemory\u2193\nRuntime (Minutes)\u2193\nMAE\n0.75\n0.75\n8\n-\nx\n-\n103.45\nMAE\n0.75\n0.75\n8\n-\n\u2713\n53858\n74.80\nMAE\n0.75\n0.75\n12\n-\n\u2713\n68386\n93.48\nCrossMAE\n0.75\n0.75\n8\n\u2713\n\u2713\n46930\n69.33\nCrossMAE\n0.75\n0.75\n12\nx\n\u2713\n55358\n73.75\nCrossMAE\n0.75\n0.75\n12\n\u2713\n\u2713\n57987\n84.05\nCrossMAE\n0.75\n0.25\n8\n\u2713\n\u2713\n32055\n54.88\nCrossMAE\n0.75\n0.25\n12\nx\n\u2713\n34191\n52.45\nCrossMAE\n0.75\n0.25\n12\n\u2713\nx\n41630\n65.80\nCrossMAE\n0.75\n0.25\n12\n\u2713\n\u2713\n36805\n63.23\nTable 5. Pretraining runtime and GPU memory under different configurations. All time trials are conducted for 10 epochs (instead of the\nfull 800 epochs) using 2 NVIDIA A100 GPUs. Memory is measured in MB per GPU, and runtime is measured in minutes. Note that the\nmemory measured here contains both the encoder and decoder.\n1.4. Ablation on Inter-block Attention\nIn Table 3d, the following cases are considered. 1 feature\nmap (row 1) does not use inter-block attention. Each decoder\nblock only takes the last feature map from the encoder as the\nkeys and values. For scenarios where more than one feature\nmap is used, the output of the patch embedding (input to the\nViT) is also used.\n1.5. Hyperparameters\nPre-training: The default setting is in Table 6, which is\nconsistent with the official MAE [31] implementation. As\nmentioned in Sec. 3.4, we scale the learning rate by the ratio\nbetween mask ratio (p) and prediction ratio (\u03b3) to ensure the\nvariance of the loss is consistent with [31]. Additionally, we\nuse the linear learning rate scaling rule [26]. This results in\nlr = \u03b3 \u2217 base lr \u2217 batchsize/(256 \u2217 p). For Table 1, we use\n12 decoder blocks, with mask ratio and prediction ratio both\n75%, and interblock attention takes in all encoder feature\nmaps. For the 400 epochs experiments in Table 2, we scale\nthe warm-up epochs correspondingly. Other hyperparame-\nters are the same as MAE.\nFinetuning: We use the same hyperparameters as MAE\nfinetuning. We use global average pooling for finetuning.\nIn MAE, the layer norm for the last encoder feature map\nis removed for finetuning, which is consistent with our pre-\ntraining setup. Please refer to Table 7 for more detail.\n1.6. Compute Infrastructure\nEach of the pretraining and finetuning experiments is run on\n2 or 4 NVIDIA A100 80GB GPUs. The batch size per GPU\nis scaled accordingly and we use gradient accumulation to\navoid out-of-memory errors. ViTDet [39] experiments use\na single machine equipped with 8 NVIDIA A100 (80GB)\nGPUs. We copy the datasets to the shared memory on the\nmachines to accelerate dataloading. We use FlashAttention-\n2 [19] to accelerate attention calculation.\nConfig\nValue\noptimizer\nAdamW [44]\nbase learning rate\n1.5e-4\nlearning rate schedule\ncosine decay [43]\nbatch size\n4096\nweight decay\n0.05\noptimizer momentum\n\u03b21, \u03b22 = 0.9, 0.95 [12]\nwarm up epoch [25]\n20, 40\ntotal epochs\n400, 800\naugmentation\nRandomResizedCrop,\nRandomHorizontalFlip\nTable 6. Pretraining Hyperparameters\nConfig\nValue\noptimizer\nAdamW\nbase learning rate\n1e-3\nlearning rate schedule\ncosine decay\nbatch size\n1024\nweight decay\n0.05\noptimizer momentum\n\u03b21, \u03b22 = 0.9, 0.999\nwarm up epoch\n5\ntotal epochs\n100 (B), 50 (L)\naugmentation\nRandAug (9, 0.5) [18]\nlabel smoothing [51]\n0.1\nmixup [62]\n0.8\ncutmix [61]\n1.0\ndrop path [32]\n0.1\nTable 7. Finetuning Hyperparameters\n1.7. Runtime and GPU Memory\nIn this section, we provide quantitative comparisons of the\neffect of mask ratios, prediction ratios, and interblock at-\ntention on GPU memory usage (for both the encoder and\ndecoder) and runtime. We provide runtimes (in minutes) of\ndifferent settings of MAE and CrossMAE for 10 epochs with\n2\ntheir corresponding GPU memory usage (in MB). All ex-\nperiments here are conducted with 2 NVIDIA A100 (80GB)\nGPUs, with the standard hyperparameters provided above\nfor pretraining. The results are listed in Tab. 5. Addition-\nally, to compare against the vanilla MAE implementation,\nwe provide rows where FlashAttention-2[19] is not enabled.\nNote that for MAE, the model does not fit on 2 A100 when\nFlashAttention-2 is not enabled, yet by adjusting the predic-\ntion ratio of CrossMAE, the model fits within the memory\nlimit.\n3\n"
  },
  {
    "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
    "link": "https://arxiv.org/pdf/2401.13919.pdf",
    "upvote": "19",
    "text": "WebVoyager\n: Building an End-to-End Web Agent with\nLarge Multimodal Models\nHongliang He1,3\u2217, Wenlin Yao2, Kaixin Ma2, Wenhao Yu2, Yong Dai2,\nHongming Zhang2, Zhenzhong Lan3, Dong Yu2\n1Zhejiang University, 2Tencent AI Lab, 3Westlake University\nhehongliang@westlake.edu.cn, wenlinyao@global.tencent.com\nAbstract\nThe rapid advancement of large language mod-\nels (LLMs) has led to a new era marked by\nthe development of autonomous applications\nin real-world scenarios, which drives innova-\ntion in creating advanced web agents. Exist-\ning web agents typically only handle one input\nmodality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly\nlimiting their applicability in real-world scenar-\nios. To bridge this gap, we introduce WebVoy-\nager, an innovative Large Multimodal Model\n(LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with\nreal-world websites. Moreover, we establish a\nnew benchmark by compiling real-world tasks\nfrom 15 popular websites and introduce an au-\ntomatic evaluation protocol leveraging multi-\nmodal understanding abilities of GPT-4V to\nevaluate open-ended web agents. We show that\nWebVoyager achieves a 59.1% task success rate\non our benchmark, significantly surpassing the\nperformance of both GPT-4 (All Tools) and the\nWebVoyager (text-only) setups, underscoring\nthe exceptional capability of WebVoyager. The\nproposed automatic evaluation metric achieves\n85.3% agreement with human judgment, indi-\ncating its effectiveness in providing reliable and\naccurate assessments of web agents.1\n1\nIntroduction\nThe recent advancement of large language models\n(LLMs), such as ChatGPT and GPT-4 (OpenAI,\n2023), have sparked significant interest in devel-\noping LLM-based autonomous agents (AutoGPT,\n2022) for complex task execution (Qin et al., 2023;\nSchick et al., 2023). Recent studies have explored\nthe construction of text-based web browsing envi-\nronments and how to instruct large language model\nagents to perform web navigation (Nakano et al.,\n2021; Gur et al., 2023; Zhou et al., 2023; Lu et al.,\n\u2217Work done during the internship at Tencent AI Lab.\n1Our code and data will be released at https://github.\ncom/MinorJerry/WebVoyager\n2023). The primary challenge in these works lies\nin managing complex and verbose HTML texts,\nand solutions include simplifying and structuring\nHTML (Nakano et al., 2021; Zhou et al., 2023; Gur\net al., 2023; Deng et al., 2023).\nHowever, existing approaches overlook a critical\nfunctionality of browsing: rendering HTML into\nvisual webpages. Particularly, vision capability is\ncrucial for utilizing tools such as web browsers,\nas rendered web pages are inherently designed\nwith user experience (UX), emphasizing intuitive\ninformation and structured presentation. This de-\nsign principle of rendering makes visual analysis\nmore effective than mere HTML representation. At\npresent, large multimodal models (LMMs), partic-\nularly GPT-4V(ision) (OpenAI, 2023) and Gemini\n(Team et al., 2023), demonstrate a remarkable abil-\nity to integrate intricate visual cues with textual\ninformation. Existing studies such as Pix2Struct\n(Lee et al., 2023) and WebArena (Zhou et al., 2023),\nhave initiated explorations into using screenshots\nas inputs for decision-making in web navigation,\nyet these are preliminary and do not represent a\ndeep exploration. Therefore, building multimodal\nweb agents to leverage the environment rendered\nby browsers through screenshots, thus mimicking\nhuman web browsing behavior, is now a viable\napproach to enhance web navigation abilities.\nWe introduce WebVoyager (Figure 1), a multi-\nmodal web agent designed to autonomously ac-\ncomplish web tasks online from start to finish,\nmanaging the entire process end-to-end without\nany intermediate human intervention. WebVoyager\nprocesses the user query by making observations\nfrom screenshots and textual content in interactive\nweb elements, formulates a thought on what ac-\ntion to take (such as clicking, typing, or scrolling,\netc.), and then executes that action on the web-\nsites. Inspired by Set-of-Mark Prompting (Yang\net al., 2023a), we mark interactive web elements\non screenshots (see Figure 2) to facilitate decision-\narXiv:2401.13919v3  [cs.CL]  29 Feb 2024\nAvailable Websites\nScreenshot (Image)\nWeb Elements (Text)\n[1]: Delivering to Santa Clara 95050\n[2]: <input> \"Search Amazon\"\n                      ...\nObservation\nThought: Based on observation...\n        Click? Type? Scroll? ...\nSend \nQuery\nReturn \nAnswer\nUser\n...\nThought\nAction\nFigure 1: The overall workflow of WebVoyager. WebVoyager takes web tasks assigned by a human and automatically\nbrowses the web online. At each step, WebVoyager selects actions based on screenshots and text (the \u2018type\u2019 of the\nweb element and its contents). Once the task is completed, the answers will be returned to the user. For example,\na user queries: \"Find the cost of a 2-year protection for PS4 on Amazon.\" Agent interacts with Amazon online,\nlocates the PS4, identifies the 2-year protection price, and returns \"$30.99\" to the user.\nmaking for WebVoyager.\nAnother challenge is the evaluation of an end-\nto-end web agent. Existing benchmarks, such as\nMind2Web (Deng et al., 2023), primarily focus\non stepwise and offline evaluation, where agents\nfollow a predefined \u201cgolden\u201d trajectory for action\nselection. This approach, however, may not fully\naccount for the variety of viable strategies to ac-\ncomplish a task, as it only reflects one possible plan.\nThis limitation could lead to a biased evaluation\nand difficulties in fairly comparing different meth-\nods. To accurately evaluate the capabilities of web\nagents in end-to-end task completion, we propose\nan automated evaluation protocol using GPT-4V.\nSpecifically, we save screenshots throughout the\nonline navigation process and then use GPT-4V\nto evaluate these trajectories together with the fi-\nnal results automatically. Human evaluations are\nalso conducted to verify the results and the analysis\nshows that our evaluation protocol achieves 85.3%\nagreement with human judges, indicating GPT-4V\ncan serve as a reliable evaluator for online agents.\nWe conduct evaluations on a newly-collected\ndataset, which is semi-automatically generated us-\ning a self-instruct (Wang et al., 2022) method, com-\nprising 643 web tasks from 15 commonly accessed\nwebsites. We also evaluate WebVoyager on 90 web-\nrelated tasks of level 1 and level 2 from the GAIA\n(Mialon et al., 2023), and 50 interactive open-web\ntasks from SeeAct (Zheng et al., 2024). We com-\npare our WebVoyager with 1) GPT-4 (All Tools)2,\nand 2) WebVoyager in a text-only setting which em-\nploys the textual accessibility tree proposed in We-\nbArena (Zhou et al., 2023) to describe web pages.\n2GPT-4 (All Tools) is an integrated tool-based agent re-\nleased by OpenAI in Oct. 2023. See https://chat.openai.com/\nThe results show that WebVoyager achieves a Task\nSuccess Rate of 59.1% on our new benchmark, sig-\nnificantly outperforming GPT-4 (All Tools) with\na rate of 30.8% and the text-only setting with a\nrate of 40.1%, demonstrating the effectiveness of\nour method. Our research demonstrates the effec-\ntiveness of the WebVoyager method for web tasks,\noffering insights into the development of more in-\ntelligent and efficient web automation solutions.\n2\nRelated Work\nAutonomous web navigation (Shi et al., 2017; Yang\net al., 2023b) requires an agent to follow instruc-\ntions, construct plans, comprehend complex web\nstructures, and decompose tasks into step-by-step\ndecisions (Weng, 2023).\nTo study web agents\nin a controlled environment, previous works con-\nstructed web simulators that contain simplified\nwebsites (Shi et al., 2017; Yao et al., 2022a). More\nrecently, there has been a surge of interest in build-\ning more challenging and realistic benchmarks\nsuch as Mind2Web (Deng et al., 2023) and We-\nbArena (Zhou et al., 2023).\nAlong with these new benchmarks, numerous\nefforts have been made to build autonomous web\nagents. WebGPT (Nakano et al., 2021) constructs\na text-based web browsing environment and fine-\ntunes GPT-3 as a web agent. WebAgent (Gur et al.,\n2023) pretrains a T5 model to extract HTML snip-\npets and leverages Flan-U-PaLM (Chowdhery et al.,\n2023) to generate Python code to interact with the\nenvironment. Besides fine-tuning, another line of\nwork tries to build web agents by prompting LLMs\n(Yao et al., 2022b; Shinn et al., 2023; Ma et al.,\n2023). Multimodal web agents that integrate visual\nsignals have also been explored, WebGUM (Fu-\nruta et al., 2023) combines T5 (Raffel et al., 2020)\nwith a Vision Transformer (ViT) to navigate using\nboth screenshots and HTML text. PIX2ACT (Shaw\net al., 2023) instead solely relies on web screen-\nshots as inputs to predict agent actions. Unlike pre-\nvious works that only consider a single modality\nor simplified web environments, we build a multi-\nmodal agent that can complete tasks on real-world\nwebsites in this work. Concurrently with our work,\nSeeAct (Zheng et al., 2024) also leverages Large\nMultimodal Models (LMMs) for integrated visual\nunderstanding and actions on websites. However,\nthe best SeeAct agent still relies on a finetuned\ncross-encoder model to select candidate elements\nfor interaction. In contrast, WebVoyager do not\nrequire any additional modules.\n3\nWebVoyager\nWe aim to build an agent that can browse the open\nweb autonomously without human intervention to\ncomplete user instructions. Given an instruction,\nour WebVoyager first instantiates a web browser\nand then performs actions with visual (i.e., screen-\nshots) and textual (i.e., HTML elements) signals\nfrom the web. The agent produces an action based\non the inputs at every step, which is then executed\nin the browser environment. The process continues\nuntil the agent decides to stop. The details of Web-\nVoyager, including environment, interaction cycle,\nobservation space, and action space, are as follows.\n3.1\nBrowsing Environment\nWe develop an automated web-browsing environ-\nment using Selenium3. Unlike WebArena (Zhou\net al., 2023), we do not host any websites locally\nand allow the agent to explore the open web instead,\nwhich poses unique challenges such as floating ads,\npop-up windows, constant updates, etc.4 Still, we\nopt for online interaction with real websites as we\nbelieve that this setting truly reflects the real-world\nuse cases (e.g., the agent needs access to real-time\ninformation from the web), and a successful web\nagent should be able to adapt to these challenges\nand consistently solve the problem robustly.\n3https://www.selenium.dev/\n4Regarding CAPTCHAs (Completely Automated Public\nTuring test to tell Computers and Humans Apart) challenges,\nwe believe it is important to respect the rules of these websites\nand prompt the agent to retrieve information from alternative\nsources.\nFigure 2: Examples of webpage screenshots provided\nto the agent. We add borders to most of the interactive\nelements on the web pages and label them with numeri-\ncal tags in the top left corner.\n3.2\nInteraction Formulation\nFormally, we denote the Environment as E, the\nlarge Multimodal Model as M, the Observa-\ntion Space as O, and the Action Space as A.\nAt time step t, the model receives the con-\ntext ct as inputs, which consist of historical ac-\ntions ai and observations oi, defined as: ct =\n(o1, a1, ..., ot\u22121, at\u22121, ot, I) The the model pro-\nduces the action at at time t, at = M(ct), which\nis then executed in the environment. After execu-\ntion, the environment sends back the observation at\ntime t + 1, ot+1 = E(ot, at). Then the context will\nbe updated and this interaction process continues\nuntil the model generates a terminating action or\nthe maximum step is reached.\nInspired by the paradigm of ReAct Prompting\n(Yao et al., 2022b), we also prompt our agent to\ngenerate a thought process first before generating\nthe action code. Hence at can be further composed\ninto (st, \u02c6at) where st and \u02c6at represent the natural\nlanguage thought and action code respectively. Fig-\nure 7 in Appendix A presents the System Prompt\nwe designed for the action prediction step. Also,\nit\u2019s worth noting that excessive observations of web\npages from longer episodes may confuse the agent.\nTherefore, we perform context clipping to remove\noutdated web page information and only keep the\nthree most recent observations in the inputs, and\nwe keep the entire history of thoughts and actions\nto better guide the agent.\n3.3\nObservation Space\nSimilar to how humans browse the web, our agent\nalso takes the visual information from the web\n(screenshots) as the primary source of input. Using\nscreenshots allows us to avoid the burden of pro-\ncessing HTML DOM tree or accessibility tree to\nportray the overall structure of webpages, which\ncan lead to overly verbose texts and impact the\ndecision-making process of the agent. Inspired by\nSet-of-Mark Prompting (Yang et al., 2023a), we\noverlay bounding boxes of the interactive elements\non the websites to better guide the agent\u2019s action\nprediction. Unlike Yang et al. (2023a), we do not\nneed any object detection module (Zou et al., 2023).\nInstead, we utilize GPT-4-ACT5, a Javascript tool\nto extracts the interactive elements based on web\nelement types and then overlays bounding boxes\nwith numerical labels on the respective regions of\nthe elements.\nAs illustrated in Figure 2, the nature of webpages\nallows us to locate and outline each interactive el-\nement using this tool precisely. The numerical\nlabels assigned to each element are also essential\nfor the model to identify the elements requiring\ninteraction, thereby facilitating accurate action de-\ntermination. We empirically choose black color\nfor the borders and the background of the labels\nto enhance clarity. We observe that using a single\nblack color yields higher success rates than using\nmultiple colors. We also provide the agent with\nauxiliary text as inputs, including the textual con-\ntent embedded within the interactive element, the\ntype of the element, and possibly some comment\ntext in the aria-label attribute. To simplify the ob-\nservation, we have disabled multiple tabs, i.e., all\ninteractions occur within the current tab instead of\nopening new ones.\nAt every step, the agent receives the current\nscreenshot, auxiliary text, and history as inputs,\nas discussed in (\u00a73.2). In case the agent\u2019s action\nraised an exception during execution, we addition-\nally incorporated the error messages in the prompt\nand asked the model to regenerate the response.\nNote that each error correction attempt also con-\nsumes one step from the total exploration budget.\n3.4\nAction Space\nWe define the action space of our agent similar\nto how human browse the web. To this end, we\nimplement the most commonly used mouse and\nkeyboard actions, sufficient for the agent to browse\nvarious web pages and locate the content required\nfor the task. With the help of numerical labels in\nscreenshots, we enable the agent to respond with\n5https://github.com/ddupont808/GPT-4V-Act\na concise Action Format. This approach precisely\nlocates the elements requiring interaction and ex-\necutes the corresponding actions. The usage of\nactions is as follows (more details in Appendix\nC): 1) Click. This action involves clicking on an\nelement within a webpage, typically a link or a\nbutton. 2) Input. This composite action involves\nselecting a text box, deleting any existing content\nwithin it, and then inputting new content. 3) Scroll.\nScrolling is a common operation for browsing web-\npages, usually involving the vertical movement of\nthe entire page. 4) Wait. Action execution requires\ntime, and this action is often used to wait for web\npages to load. 5) Back. This action is used to\nreturn to the previous page. 6) Jump to Search\nEngine. There are often situations where agents\nget stuck at a certain website without finding an\nanswer. This action enables the agent to jump to\na search engine and start anew. 7) Answer. Once\nall questions in the task are resolved, this action\nconcludes the iteration and provides an answer in\nline with the task requirements.\n4\nBenchmark for WebVoyager\n4.1\nWebsite Selection\nWe select 15 representative websites that cover dif-\nferent aspects of our daily life to ensure diversity in\nour evaluation, including Allrecipes, Amazon, Ap-\nple, ArXiv, BBC News, Booking, Cambridge Dic-\ntionary, Coursera, ESPN, GitHub, Google Flights,\nGoogle Map, Google Search, Huggingface, and\nWolfram Alpha.\nDue to technical limitations,\nwe regretfully omit websites requiring login or\nCAPTCHA to access their content. Additionally,\nGoogle Search is a universal website that can serve\nas a starting point for any website, making our\nframework applicable to various scenarios.\n4.2\nData Construction\nWe employ a combination of self-instruct (Wang\net al., 2022) and human verification to construct our\nevaluation set. Figure 3 illustrates our data creation\nprocess. Initially, we manually sample and rewrite\nsome tasks from Mind2Web (Yin et al., 2023; Deng\net al., 2023) for websites including Google Flights,\nGoogle Map, Google Search, Booking, and Wol-\nfram Alpha. This process yields initial seed tasks\nin the Task Pool for subsequent generation. In step\ntwo, we sample tasks from Task Pool as in-context\nexamples (Dong et al., 2022) and prompt GPT-4\nTurbo to generate approximately 100 new tasks (20\nStep 1: Tasks written by \nHuman (for 3 - 5 webs)\nGPT-4\nSeveral tasks \nfor 15 webs\nManual Filtering\nTask Pool \n(15 webs)\nGPT-4\nManual Filtering\n(Find answers on Web)\nFinal Web Tasks\nIn-context \nexamples\n(Quality of Tasks)\nFiltered\n tasks\nIn-context \nexamples\nStep 2: seed tasks for each web\nGenerated tasks \nStep 3: Generate web tasks and \ncheck answers online.\nFigure 3: Data creation process using self-instruct.\niterations). Then we manually verify each gener-\nated task and rewrite them if necessary to ensure\nits high quality and the answers can be found on\nthe corresponding website, then we add them to the\nTask Pool as additional seed tasks. This step allows\nus to create human-validated seed tasks for each\nwebsite. Finally, in step three, we sample more\ndiverse in-context examples in the Task Pool and\ndirectly add the generated tasks to the Task Pool\nin each iteration. We manually verify that the gen-\nerated tasks have low repetition, and the answers\nto the generated tasks can be found on the web-\nsites. In total, we collected 40+ tasks per website,\nresulting in a total of 643 tasks.\n4.3\nAnnotation Process\nAfter collecting the full task pool, we annotate an-\nswers for each task. Since some questions are open-\nended and the web information may change, these\nquestions may not have a fixed golden response.\nThus, we label each data entry with an answer, cat-\negorized as \u201cPossible\u201d or \u201cGolden.\u201d For answers\nlabeled as \u201cGolden,\u201d we provide a comprehensive\nlisting of possible responses and consider them\nstable in the short term. The \u201cPossible\u201d category\ncovers the following scenarios: 1) Answers for\nopen-ended tasks where it\u2019s hard to find an exact\nmatch answer, such as summarization. 2) multiple\nanswers satisfy the task, making it impractical to\nlist all of them. Therefore, we provide a partial\nlisting. 3) Tasks related to real-time information,\nwhere the answer might change, e.g., flight ticket\nprices. Hence, the \u201cPossible\u201d answers were also\ncorrect during our experiments. In total, 22.3% of\nquestions are annotated with golden responses, and\nthe rest only have possible answers.\n5\nExperiment\nDataset and Metrics\nBesides the benchmark in-\ntroduced in Section 4, we also evaluated on 90\nweb browsing tasks (Level 1 and Level 2) from\nthe GAIA dataset (Mialon et al., 2023), which also\ncome with golden responses. Since GAIA does not\nprovide specific websites, we instruct the agent to\nstart with Google Search. We further evaluated our\nagent on the 50 tasks used in SeeAct agent\u2019s online\nevaluation (Zheng et al., 2024) and compared with\ntheir results. Following WebArena (Zhou et al.,\n2023), the primary evaluation metric we adopt is\nthe Task Success Rate, measuring the successful\ncompletion of tasks without considering whether\nthe steps are optimal.\nExperimental Details\nWe employ GPT-4 Turbo\nwith vision (gpt-4-vision-preview) as the backbone\nmodel of our agent, which showcases strong seman-\ntic and visual understanding capabilities equivalent\nto GPT-4V (OpenAI, 2023). We include the GPT-4\n(All Tools) for baselines, which integrates vision,\nweb browsing, code analysis, and various plugins\nin one model. Additionally, we consider a text-only\nbaseline where the agent only receives the websites\u2019\naccessibility tree as input to prediction actions. We\nused a fixed browser window size of 1024 * 768\npixels for our environment, ensuring a consistent\nsize for the screenshots in our observations. We set\nthe temperature to 1 during generation and allow\nthe agent to explore at most 15 steps.\n5.1\nEvaluation Methods\nWe adopt human evaluation as our main evaluation\nmetric since most of the questions in our bench-\nmark have open-ended answers. In particular, we\nprovide the human evaluators the complete trajec-\ntories of the agent\u2019s interaction with the web (all\nscreenshots and all actions), and ask them to pro-\nvide a binary judgement of whether the agent suc-\ncessfully completed the task. For a subset of 300\ntasks, we invite three annotators to judge each tra-\njectory to understand the agreement among human\nannotators.\nEven though human evaluations are accurate,\nthey are often not scalable. Hence, we want to see\nif leveraging a LMM for automatic evaluation is\nfeasible. To this end, we propose to use GPT-4V\nas an auto-evaluator that emulates the behavior of\nhuman evaluators to evaluate the navigation trajec-\ntories of WebVoyager. In particular, we provide the\ntask, the responses from WebVoyager, and the last\nStep 1: Cilck [10]\nStep 2: Type [17]: Smart Folio for iPad\nStep 3: Cilck [46]\nStep 4: Cilck [22] \nStep 5: Type [0]: 90038 \nStep 6: ANSWER \n[10]: Accessories\n[17]: Search accessories\n[46]: Smart Folio ...\n[22]: Apple Valley Fair\n[0]: City or Zip\nFigure 4: Screenshots of a complete trajectory of online web browsing. Given the task: \u2018Search Apple for the\naccessory Smart Folio for iPad and check the closest pickup availability next to zip code 90038.\u2019 The agent interacts\nwith the Apple website and obtains the answer: \u2018Apple Tower Theatre.\u2019\nk screenshots to the evaluator and ask it to judge\nwhether the agent has completed the task, where k\nis a hyper-parameter. The prompt of the GPT-4V\nevaluator is shown in Appendix B.\n5.2\nResult\nFigure 4 presents an example that demonstrates\nhow the agent interacts with the Apple website step\nby step in an online fashion to complete a task. In\nthe final screenshot, the Agent acquires the desired\ninformation, then selects the \u201cANSWER\u201d action to\nrespond and conclude the navigation. Additional\nexamples are provided in the Appendix D.\nWe present the results for our dataset and the\nextracted GAIA web tasks in Table 1 and Figure\n5. WebVoyager outperforms text-only and GPT-\n4 (All Tools) baselines by large margins in most\nwebsite tasks, while it is slightly lower than Text-\nonly on Allrecipes and similar to Text-only on\nGithub, ESPN, Cambridge Dictionary and Wolfram\nAlpha. This is primarily because these websites\nare more text-heavy than others. Since WebVoy-\nager mostly relies on web screenshots for decision-\nmaking, dense text might not be easily recogniz-\nable from the image. We think extracting such text\nfrom the HTML to augment the input could be a\npotential solution to this problem, suggesting a di-\nrection for future work. In Figure 5, WebVoyager\nalso achieves much stronger performance than both\nbaselines. Finally, WebVoyager has a success rate\nof 30% on the SeeAct online test set whereas the\nbest SeeAct autonomous agent has 26%, showing\nthe efficacy of our proposed agent.\nWe report the Agreement (the ratio of overlap)\nand Kappa (\u03ba; Cohen 1960) between consolidated\nhuman labels6 and GPT-4V\u2019s judgments on the\nsubset of 300 tasks in Table 2. Here, k denotes the\nnumber of screenshots provided to GPT-4V, with\n\u201cfull\u201d implying the full trajectory. GPT-4V\u2019s agree-\nment with human annotators gradually improves as\nit receives more information, and its final Kappa\nscore also reaches 0.7, which is on par with the\nagreement among human annotators. The consis-\ntency between GPT-4V and humans suggests that\nGPT-4V is a promising automatic evaluator for\nmulti-modal web agents.\n5.3\nDiscussions\nDirect interaction with the websites is necessary\nFrom our experience of using GPT-4 (All Tools),\nthe primary limitation of GPT-4 (All Tools)\u2019s per-\nformance is rooted in its reliance on Bing search for\nweb browsing, predominantly depending on web\npages fetched by Bing. It cannot directly access cer-\ntain websites (such as Apple, Amazon, BBC News,\netc.) for searching, clicking, or utilizing their sort-\ning functions. This greatly limits the agent\u2019s ability\nto complete certain types of tasks.\nBoth text and vision are necessary for general-\nist web agents. As discussed earlier, WebVoyager\n6the Fleiss\u2019s Kappa (Fleiss, 1971) of human annotators\nbefore any discussion is 0.7, which is substantial agreement.\nAllrecipes\nAmazon\nApple\nArXiv\nGitHub\nBooking\nESPN\nCoursera\nGPT-4 (All Tools)\n11.1%\n17.1%\n44.2%\n14.0%\n48.8%\n22.7%\n31.8%\n31.0%\nWebVoyagerText-only\n55.6%\n31.7%\n34.9%\n32.6%\n61.0%\n2.3%\n36.4%\n23.8%\nWebVoyager\n53.3%\n58.5%\n65.1%\n51.2%\n63.4%\n43.2%\n38.6%\n73.8%\nWebVoyagerText-only\u2217\n57.8%\u00b10.0%\n43.1%\u00b11.4%\n36.4%\u00b13.5%\n50.4%\u00b11.4%\n63.4%\u00b12.5%\n2.3%\u00b10.0%\n38.6%\u00b12.3%\n24.6%\u00b11.4%\nWebVoyager\u2217\n51.1%\u00b12.2%\n52.9%\u00b11.4%\n62.8%\u00b12.3%\n52.0%\u00b11.3%\n59.3%\u00b13.7%\n32.6%\u00b12.7%\n47.0%\u00b11.3%\n57.9%\u00b12.7%\nCambridge\nBBC\nGoogle\nGoogle\nGoogle\nHuggingface\nWolfram\nOverall\nDictionary\nNews\nFlights\nMap\nSearch\nAlpha\nGPT-4 (All Tools)\n25.6%\n9.5%\n2.4%\n53.7%\n60.5%\n37.2%\n52.2%\n30.8%\nWebVoyagerText-only\n62.8%\n45.2%\n7.1%\n61.0%\n67.4%\n20.9%\n58.7%\n40.1%\nWebVoyager\n65.1%\n61.9%\n59.5%\n70.7%\n76.7%\n44.2%\n63.0%\n59.1%\nWebVoyagerText-only\u2217\n66.7%\u00b13.6%\n45.2%\u00b12.4%\n7.1%\u00b10.0%\n62.6%\u00b12.8%\n75.2%\u00b11.3%\n31.0%\u00b11.4%\n60.2%\u00b11.3%\n44.3%\u00b10.6%\nWebVoyager\u2217\n71.3%\u00b11.3%\n60.3%\u00b12.8%\n51.6%\u00b11.4%\n64.3%\u00b12.8%\n77.5%\u00b12.7%\n55.8%\u00b12.3%\n60.9%\u00b12.2%\n57.1%\u00b10.2%\nTable 1: The main result for WebVoyager. Each website contains 40 to 45 tasks, and we report the Task Success\nRate in the table. We show the results of GPT-4 (All Tools), WebVoyagerText-only using the accessibility tree, and\nWebVoyager by comparing with human expert labels. WebVoyager\u2217 and WebVoyagerText-only\u2217 are results evaluated\nby GPT-4V (full trajectory, kappa = 0.70). For each automatic evaluation, we run GPT-4V evaluator three times to\ncalculate the performance mean and standard deviation.\nSuccess Rate\nConsistency\nAgreement\n\u03ba\nk=1\n47.7%\n75.3%\n0.51\nk=2\n55.3%\n79.7%\n0.59\nk=3\n54.3%\n81.3%\n0.62\nFull\n58.3%\n85.3%\n0.70\nTable 2: Consistency between GPT-4V and Human.\nSuccess Rate is the overall success rate of all tasks given\nby GPT-4V. Based on the annotations given by GPT-4V\nand Human (after alignment), we compute Agreement,\ni.e., the label overlap, and the Kappa values.\n23.1\n12.5\n19.2\n12.5\n38.5\n15.6\n0\n10\n20\n30\n40\nLevel 1\nLevel 2\nTask Success Rate (%)\nGPT-4V (All Tools)\nWebVoyager (Text only)\nWebVoyager\nFigure 5: Success Rate for GAIA Level 1 and Level 2.\nstruggles with text-heavy websites. On the other\nhand, we observe that text-only agent demonstrates\nsignificantly poorer performance on websites with\ncomplex visual elements, such as Booking and\nFlights, which require interactions with calendars\nand other intricate components. In these scenarios,\nthe textual input such as accessibility tree becomes\nhighly complex and verbose, making it far less\nintuitive than using screenshots. Hence it\u2019s neces-\nsary to incorporate both modalities of inputs when\nbuilding the general purpose agents.\n3\n4\n5\n6\n7\n8\n9\n10\nAverage Trajectory Length\n20\n30\n40\n50\n60\n70\nAverage elements num per step\nAllrecipes\nAmazon\nApple\nArXiv\nBBC News\nBooking\nCambridge Dict.\nCoursera\nESPN\nGitHub\nGoogle Flights\nGoogle Map\nGoogle Search\nHuggingface\nWolfram Alpha\n30\n40\n50\n60\n70\n80\nTask Success Rate\nFigure 6: Factors related to task success rate. We show\nthe average number of elements per page and the aver-\nage trajectory length for each website type. The darker\ncolors indicate a higher task success rate.\nWebsites with more interactable elements are\nmore challenging for agents. We also calculate\nthe average trajectory length of tasks completed\nwithin the maximum number of iterations, as well\nas the average number of interactive web elements\npresent on the webpage screenshots. Figure 6 il-\nlustrates their relationship with the Task Success\nRate. We posit that the average trajectory length\nserves as a measure of a task\u2019s complexity to some\nextent, while the average number of numerical la-\nbels related to the decision-making process reflects\nthe complexity of a webpage. Intuitively, websites\ndepicted in the lower-left corner of Figure 6, char-\nacterized by relatively simple webpages and shorter\ntrajectory lengths, are expected to exhibit higher\nTask Success Rates. As observed in Figure 6, the\nresults largely align with this intuition.\n5.4\nError Analysis\nIn this section, we discuss and summarize the pri-\nmary issues encountered by WebVoyager in the task\ncompletion process. These challenges will serve\nas critical entry points for future enhancements of\nthe Task Success Rate and for devising strategies\nto obtain an Optimal Trajectory. We sampled 300\ntasks from our benchmark and manually labeled\nthe error category for each failed case, we show the\ndistribution of errors in table 3. In Appendix F, we\nalso provide specific examples for each issue.\nNavigation Stuck\nThe most common failure is\nrunning out of steps before completing the task.\nThere are three failure scenarios: 1) When the\nagent\u2019s search query is not precise and explicit\nenough, it will be overwhelmed by irrelevant search\nresults. The agent may prefer to browse different\nresults or wait for incorrect outcomes rather than\ncorrect its previous action; 2) When the scroll-able\narea is very small, the agent might not be able to\nlocate the correct scrolling area and repeatedly re-\nquest the execution of useless scrolling actions; 3)\nSometimes in the middle of the page, the agent has\ntrouble deciding whether to scroll up or down. The\nagent also tends to repeat its previous mistakes due\nto the input clipping, as mentioned in section 3.2.\nThese meaningless or repetitive actions may hinder\nthe completion of the task.\nVisual Grounding Issue\nThe visual grounding\nability of our agent still has a large room for im-\nprovement. We observe the following issues: 1)\nThe agent cannot interpret less frequently observed\npatterns, such as misidentifying characters repre-\nsenting the pronunciations or math formulas; 2)\nThe agent doesn\u2019t recognize the subtle difference\nbetween two observations and thinks the execution\nhas failed; 3) The agent selects the wrong element\nfor action execution due to proximity. For example,\nthe model sometimes confuses adjacent elements\nand misinterprets numbers on a calendar as numer-\nical labels. Sometimes textual information plays a\nsignificant role, offering valuable cues and assist-\ning in distinguishing between overly dense web el-\nements. We find that incorporating the text content\nincluded in Web Elements can alleviate these prob-\nlems to some extent. However, a stronger visual\nencoder or additional text inputs might be needed.\nHallucination\nAgents sometimes produce seem-\ningly correct answers, which may require checking\ncarefully to identify errors. We mainly see the fol-\nMain reasons for\nFailure\nRatio\nNavigation Stuck\n44.4%\nVisual Grounding Issue\n24.8%\nHallucination\n21.8%\nPrompt Misalignment\n9.0%\nTable 3: Distribution of main failure reasons.\nlowing two scenarios: 1) The agent may overlook\ncertain task requirements and settle for an answer\nthat is only partially correct. For instance, when\nasked for the cheapest product, the agent might\nrespond with a cheap product visible in a screen-\nshot, neglecting the need to sort the options first. 2)\nThe agent might execute a seemingly correct action\nwithout raising any errors, which deviate it from\nthe correct reasoning path. For example, inputting\ncontent to the wrong text box when there are many\ntext boxes on the webpage is still valid, yet it would\nguide the agent to obtain a wrong answer.\nPrompt Misalignment\nUnderstanding and fol-\nlowing complex prompts, as illustrated in Figure\n7, often poses significant challenges. Moreover,\nlonger trajectories may result in excessively lengthy\ncontexts, hindering effective instruction following.\nAlthough many of the errors in Navigation Stuck\nand Hallucination categories can also be attributed\nto prompt design, we use Prompt Misalignment to\ncategorize the following situations: 1) the agent\nfails to generate outputs that can be parsed into ex-\necutable actions, e.g. providing only the \u2018Thought\u2019\nwithout the corresponding \u2018Action\u2019; 2) Prematurely\nterminating the process using the ANSWER ac-\ntion, though agent knows that the task is not yet\ncomplete (explicitly mentioned in its answer).\n6\nConclusion\nWe introduce WebVoyager, an innovative web\nagent powered by large multimodal models (LMM)\nthat can complete real-world web tasks end-to-\nend by interacting with websites. We have shown\nthrough evaluations that WebVoyager outperforms\nseveral baselines by leveraging both visual and tex-\ntual signals. We also propose an automatic evalu-\nation protocol by leveraging GPT-4V as the eval-\nuator for online agents. Our work demonstrates\nthe promise of using advanced LMM capabilities\nfor building intelligent web agents. We hope We-\nbVoyager provides a strong foundation for future\nresearch toward building more versatile and capa-\nble web assistants.\nLimitations\nWe recognize the following limitations of our work.\nFirst, we haven\u2019t supported all possible actions in\nour environment compared to actions a human user\nmight take when browsing the web. e.g. the Drag\naction on web pages. Supporting such an action is\nchallenging since the degree of Drag is not a finite\nset. We may allow it to choose the pixel values to\nbe dragged if the Visual Grounding capabilities of\nLMMs are further enhanced. Second, our agent\ncurrently can only analyze basic file formats (such\nas text files and PDF files) and doesn\u2019t support all\nfile formats, especially videos. Enhancing support\nfor additional file formats is a crucial step in the\ndevelopment of web agents and we leave it for\nfuture work.\nRegarding the potential risks of our work, we be-\nlieve that it requires a substantial amount of safety\nchecks before deploying web agents like WebVoya-\ngar into real-world applications, as the agent might\nunintentionally download malicious content from\nunauthorized websites, or input private/confidential\ninformation on public websites. Therefore it\u2019s nec-\nessary to take extra caution when using and testing\nour agent.\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\net al. 2022.\nCm3:\nA causal masked multi-\nmodal model of the internet.\narXiv preprint\narXiv:2201.07520.\nAutoGPT. 2022. AutoGPT.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nQi Chen, Dileepa Pitawela, Chongyang Zhao, Gengze\nZhou, Hsiang-Ting Chen, and Qi Wu. 2023. Webvln:\nVision-and-language navigation on websites.\nKanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu,\nYantao Li, Jianbing Zhang, and Zhiyong Wu. 2024.\nSeeclick: Harnessing gui grounding for advanced\nvisual gui agents.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research, 24(240):1\u2013113.\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and psychological mea-\nsurement, 20(1):37\u201346.\nYong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan,\nCong Zhou, Jingquan Wang, Zhangyin Feng, Fan\nZhang, Xueyu Hu, and Shuming Shi. 2022. One\nmodel, multiple modalities: A sparsely activated ap-\nproach for text, sound, image, video and code. arXiv\npreprint arXiv:2205.06126.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,\nSamuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\n2023. Mind2web: Towards a generalist agent for the\nweb. arXiv preprint arXiv:2306.06070.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale.\narXiv preprint\narXiv:2010.11929.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yu-\ntaka Matsuo, Shixiang Shane Gu, and Izzeddin\nGur. 2023.\nMultimodal web navigation with\ninstruction-finetuned foundation models.\narXiv\npreprint arXiv:2305.11854.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904\u20136913.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-\ndra Faust. 2023. A real-world webagent with plan-\nning, long context understanding, and program syn-\nthesis. arXiv preprint arXiv:2307.12856.\nJack Hessel, Jena D Hwang, Jae Sung Park, Rowan\nZellers, Chandra Bhagavatula, Anna Rohrbach, Kate\nSaenko, and Yejin Choi. 2022. The abduction of\nsherlock holmes: A dataset for visual abductive rea-\nsoning. In European Conference on Computer Vision,\npages 558\u2013575. Springer.\nJing Yu Koh, Robert Lo, Lawrence Jang, Vikram\nDuvvur, Ming Chong Lim, Po-Yu Huang, Graham\nNeubig, Shuyan Zhou, Ruslan Salakhutdinov, and\nDaniel Fried. 2024.\nVisualwebarena: Evaluating\nmultimodal agents on realistic visual web tasks.\nKenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexi-\nang Hu, Fangyu Liu, Julian Martin Eisenschlos, Ur-\nvashi Khandelwal, Peter Shaw, Ming-Wei Chang,\nand Kristina Toutanova. 2023. Pix2struct: Screen-\nshot parsing as pretraining for visual language under-\nstanding. In International Conference on Machine\nLearning, pages 18893\u201318912. PMLR.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\narXiv preprint arXiv:1908.03557.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. Advances in Neural Information\nProcessing Systems, 35:2507\u20132521.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nKaixin Ma, Hongming Zhang, Hongwei Wang, Xiao-\nman Pan, and Dong Yu. 2023. Laser: Llm agent with\nstate-space exploration for web navigation. arXiv\npreprint arXiv:2309.08172.\nGr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Craig Swift,\nThomas Wolf, Yann LeCun, and Thomas Scialom.\n2023. Gaia: a benchmark for general ai assistants.\narXiv preprint arXiv:2311.12983.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.\nWebgpt: Browser-assisted question-\nanswering with human feedback.\narXiv preprint\narXiv:2112.09332.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nPeter Shaw, Mandar Joshi, James Cohan, Jonathan Be-\nrant, Panupong Pasupat, Hexiang Hu, Urvashi Khan-\ndelwal, Kenton Lee, and Kristina Toutanova. 2023.\nFrom pixels to ui actions: Learning to follow instruc-\ntions via graphical user interfaces. arXiv preprint\narXiv:2306.00245.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Her-\nnandez, and Percy Liang. 2017. World of bits: An\nopen-domain platform for web-based agents. In In-\nternational Conference on Machine Learning, pages\n3135\u20133144. PMLR.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023.\nGemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. arXiv preprint arXiv:2108.10904.\nLilian Weng. 2023. Llm-powered autonomous agents.\nlilianweng.github.io.\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin,\nLinjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong,\nJulian McAuley, Jianfeng Gao, Zicheng Liu, and\nLijuan Wang. 2023. Gpt-4v in wonderland: Large\nmultimodal models for zero-shot smartphone gui nav-\nigation.\nJianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun-\nyuan Li, and Jianfeng Gao. 2023a.\nSet-of-mark\nprompting unleashes extraordinary visual grounding\nin gpt-4v. arXiv preprint arXiv:2310.11441.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng\nWang, Chung-Ching Lin, Zicheng Liu, and Lijuan\nWang. 2023b.\nThe dawn of lmms: Preliminary\nexplorations with gpt-4v (ision).\narXiv preprint\narXiv:2309.17421, 9(1).\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022a. Webshop: Towards scalable\nreal-world web interaction with grounded language\nagents.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022b.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nDa Yin, Faeze Brahman, Abhilasha Ravichander, Khy-\nathi Chandu, Kai-Wei Chang, Yejin Choi, and\nBill Yuchen Lin. 2023. Lumos: Learning agents\nwith unified data, modular design, and open-source\nllms. arXiv preprint arXiv:2311.05657.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019.\nFrom recognition to cognition: Vi-\nsual commonsense reasoning. In Proceedings of the\nIEEE/CVF conference on computer vision and pat-\ntern recognition, pages 6720\u20136731.\nChi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin\nChen, Zebiao Huang, Bin Fu, and Gang Yu. 2023.\nAppagent: Multimodal agents as smartphone users.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\nBisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:\nA realistic web environment for building autonomous\nagents. arXiv preprint arXiv:2307.13854.\nZhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo,\nand Jieping Ye. 2023. Object detection in 20 years:\nA survey. Proceedings of the IEEE.\nA\nPrompt for WebVoyager\nThe System Prompt for WebVoyager is shown in\nFigure 7. The Prompt\u2019s guidelines hold potential\nfor optimization and should be generalist rather\nthan website-specific in design. Incorporating spe-\ncific issues from websites into the system prompt\nmay compromise the agent\u2019s universality.\nB\nPrompt for Auto Evaluation\nFigure 8 demonstrates using GPT-4V as an evalu-\nator for web tasks, involving web task instruction,\nscreenshots in the trajectory, and WebVoyager re-\nsponses. We require GPT-4V to mark success or\nnot success. The temperature is set to 0 to reduce\nrandomness during evaluation.\nC\nAction Space\nWe detail the interaction actions that WebVoyager\nemploys to navigate and operate within web envi-\nronments. These actions are fundamental to how\nthe agent interacts with web pages, retrieves in-\nformation, and performs specific tasks as part of\nits operational protocol. The actions range from\nbasic web navigation to more complex operations,\ntailored to efficiently gather data and respond to\nqueries. Each action is designed with a specific\nformat for easy identification and execution.\n\u2022 Click. This action involves clicking on an ele-\nment within a webpage, typically a link or a\nbutton. If clicking a link results in the down-\nload of a PDF file, we automatically parse its\ncontent using the OpenAI Assistant API7 and\nincorporate it into the Observation. Action\nFormat: Click [Numerical_Label].\n\u2022 Input.\nThis is a composite action that in-\nvolves selecting a text box, deleting any ex-\nisting content within it, and then inputting\nnew content.\nTo minimize interaction fre-\nquency, an automatic ENTER key press fol-\nlows the input completion. Action Format:\nType [Numerical_Label]; [Content].\n\u2022 Scroll. Scrolling is a common operation for\nbrowsing webpages, usually involving the ver-\ntical movement of the entire page. However,\nthere are instances where only a specific sec-\ntion within the webpage is scrollable. In such\n7https://platform.openai.com/docs/assistants/overview\ncases, we expect the Agent to select an ele-\nment within the scrollable area and navigate\nto that particular region for scrolling. Ac-\ntion Format: Scroll [Numerical_Label or\nWINDOW]; [up or down].\n\u2022 Wait. Action execution requires time, and this\naction is often used to wait for web pages to\nload. Action Format: Wait.\n\u2022 Back. This action is used to return to the\nprevious page. We consider the forward action\nunnecessary because it can be achieved by\nrepeating previous actions. Action Format:\nGoBack.\n\u2022 Jump to Search Engine. There are often situa-\ntions where agents get stuck at a certain web-\nsite, without finding an answer. This action\nenables the agent to jump to a search engine\nand start anew. In this work, we just adopt\nGoogle Search. Action Format: Google.\n\u2022 Answer. Once all questions in the task are\nresolved, this action concludes the iteration\nand provides an answer in line with the\ntask requirements. Action Format: ANSWER;\n[Content].\nD\nAdditional Trajectories\nIn our experiment, we present the complete trajec-\ntory of WebVoyager accessing the Apple website\nand addressing the task. In this section, we ex-\nhibit the trajectories for the remaining websites\nthat complete web tasks successfully. We provide\na screenshot for each step, accompanied by the\naction generated by WebVoyager. The specific nav-\nigation trajectories for each website are illustrated\nin Figures 9 to 22.\nE\nAdditional Related Work\nVision-based Agents Concurrent to our work, a\nfew related works also studied vision-based au-\ntonomous agents. VisualWebArena (Koh et al.,\n2024) extends WebArena with additional websites\nand tasks that focus on visual reasoning to facili-\ntate research on vision-based web agents. SeeClick\n(Cheng et al., 2024) focused on finetuning a LMM\nto solely leverage screenshots as inputs to interact\nwith websites. WebVLN (Chen et al., 2023) intro-\nduced a web simulator that provides both HTML\nImagine you are a robot browsing the web, just like humans. Now you need to complete a task. In each iteration, \nyou will receive an Observation that includes a screenshot of a webpage and some texts. This screenshot will \nfeature Numerical Labels placed in the TOP LEFT corner of each Web Element. Carefully analyze the visual \ninformation to identify the Numerical Label corresponding to the Web Element that requires interaction, then follow \nthe guidelines and choose one of the following actions:\n1. Click a Web Element. \n2. Delete existing content in a textbox and then type content. \n3. Scroll up or down. \n  ...\nCorrespondingly, Action should STRICTLY follow the format:\n- Click [Numerical_Label]\n- Type [Numerical_Label]; [Content]\n- Scroll [Numerical_Label or WINDOW]; [up or down]\n- Wait\n- GoBack\n- Google\n- ANSWER; [content]\nKey Guidelines You MUST follow:\n* Action guidelines *\n1) Execute only one action per iteration. \n  ...\n* Web Browsing Guidelines *\n1) Don't interact with useless web elements like Login, Sign-in, donation that appear in Webpages.\n ...\nYour reply should strictly follow the format:\nThought: {Your brief thoughts (briefly summarize the info that will help ANSWER)}\nAction: {One Action format you choose}\nThen the User will provide:\nObservation: {A labeled screenshot Given by User}\nFigure 7: System Prompt for WebVoyager. We instruct agents to perform web navigation, along with specific\nbrowsing actions and action formats. To enhance efficiency and accuracy, we can incorporate additional general into\nthe prompts. These guidelines should be generic and not about a specific website to ensure generalizability.\ntext and screenshots to finetune supervised vision-\nlanguage models. GPT-4V Wonderland (Yan et al.,\n2023) and AppAgent (Zhang et al., 2023) instead\nfocuses on building agents that can operate smart\nphone apps using the GPT-4V as the backbone.\nThese works further underscore the promising\nprospects in this field.\nLarge Multimodal Models. In recent years, sig-\nnificant strides have been made in unifying image\nand text representations within a single multimodal\nmodel through joint training with image and text\n(Li et al., 2019; Dosovitskiy et al., 2020; Wang\net al., 2021; Dai et al., 2022; Aghajanyan et al.,\n2022). Large Multimodal Models (LMMs), fol-\nlowing in the footsteps of Large Language Models\n(Brown et al., 2020; Chen et al., 2021; Chowdh-\nery et al., 2023), attain the capability of instruction\nfollowing (Ouyang et al., 2022) and exhibit robust\nmultimodal comprehension. Represented by GPT-\n4V (OpenAI, 2023) and Gemini (Team et al., 2023),\nLMMs have demonstrated impressive performance\non benchmarks (Goyal et al., 2017; Lu et al., 2022;\nZellers et al., 2019; Hessel et al., 2022), establish-\ning a foundation for the construction of multimodal\nagents in subsequent research.\nF\nError Cases\nIn this section, we provide specific examples of\nthe four types of errors mentioned in the Error\nAnalysis section. Figure 23 illustrates a snippet\nof WebVoyager navigating on Google Flights and\na Visual Grounding Issue appears. The task is\nto retrieve one-way flight information for January\n22nd; however, it selects December 22nd on the\nCalendar and fails to make the necessary correc-\ntions. Although it attempts to modify the date in\nstep 6, it ultimately fails to do so. Figure 24 il-\nlustrates a situation of WebVoyager navigating on\nAllrecipes, encountering the Navigation Stuck is-\nsue. The agent requires multiple downward scrolls\nto locate the correct ingredients. However, it ex-\nperiences confusion during the process, uncertain\nwhether to scroll up or down. Figure 25 depicts the\nAs an evaluator, you will be presented with three primary components to assist you in your role:\n1. Web Task Instruction: This is a clear and specific directive provided in natural language, detailing the online activity to be carried \nout. These requirements may include conducting searches, verifying information, comparing prices, checking availability, or any other \naction relevant to the specified web service (such as Amazon, Apple, ArXiv, BBC News, Booking etc).\n2. Result Screenshots: This is a visual representation of the screen showing the result or intermediate state of performing a web task. \nIt serves as visual proof of the actions taken in response to the instruction.\n3. Result Response: This is a textual response obtained after the execution of the web task. It serves as textual result in response to \nthe instruction.\n-- You DO NOT NEED to interact with web pages or perform actions such as booking flights or conducting searches on websites.\n-- You SHOULD NOT make assumptions based on information not presented in the screenshot when comparing it to the instructions.\n-- Your primary responsibility is to conduct a thorough assessment of the web task instruction against the outcome depicted in the \nscreenshot and in the response, evaluating whether the actions taken align with the given instructions.\n-- NOTE that the instruction may involve more than one task, for example, locating the garage and summarizing the review. Failing to \ncomplete either task, such as not providing a summary, should be considered unsuccessful.\n-- NOTE that the screenshot is authentic, but the response provided by LLM is generated at the end of web browsing, and there may \nbe discrepancies between the text and the screenshots.\n-- Note the difference: 1) Result response may contradict the screenshot, then the content of the screenshot prevails, 2) The content \nin the Result response is not mentioned on the screenshot, choose to believe the content.\nYou should elaborate on how you arrived at your final evaluation and then provide a definitive verdict on whether the task has been \nsuccessfully accomplished, either as 'SUCCESS' or 'NOT SUCCESS'.\nFigure 8: System Prompt for Auto Evaluation by GPT-4V.\nHallucination issue encountered by WebVoyager\non the Coursera website. In the task, we query the\nnumber of quizzes in the \"Artificial Intelligence\nfor Healthcare\" course. However, the agent only\nidentifies the quiz in module 1 of the course, which\nis not the optimal answer and does not fulfill the\ntask requirements. Figure 26 illustrates the issue of\nPrompt Misalignment encountered while browsing\nBBC News. WebVoyager fails to fulfill all the task\nrequirements. Instead of completing the navigation,\nit provides partial answers and tells me how to find\ncomplete answers, which is not end-to-end.\nStep 1: Click [2]\nStep 2: Type [2]; Baked Salmon\nStep 3: Scroll down\nStep 4: Click [6]\nStep 5: Scroll down\nStep 6: ANSWER\nFigure 9: Screenshots of a complete trajectory of browsing Allrecipes. Given the task: \u201cFind a recipe for Baked\nSalmon that takes less than 30 minutes to prepare and has at least a 4-star rating based on user reviews.\u201d The agent\ninteracts with the Allrecipes website and obtains the answer: \u201cThe \"Baked Dijon Salmon\" recipe meets the user\u2019s\ncriteria, with a 4.6-star rating and a preparation time of 15 minutes.\u201d\nStep 1: Type [2]; green \nXbox Wireless controller \n4 stars\nStep 2: Click [33]\nStep 3: ANSWER\nFigure 10: Screenshots of a complete trajectory of browsing Amazon. Given the task: \u201cSearch for an Xbox Wireless\ncontroller with green color and rated above 4 stars.\u201d The agent interacts with the Amazon website and obtains the\nanswer: \u201cThe green Xbox Wireless controller (\"Xbox Core Wireless Gaming Controller \u2013 Velocity Green\") rated\nabove 4 stars has been found on Amazon with a rating of 4.7 out of 5 stars.\u201d\nStep 1: Click [5]\nStep 2: Click [8]\nStep 3: Click [9]\nStep 4: Click [10]\nStep 5: Click [13]\nStep 6: Click [34]\nStep 7: ANSWER\nFigure 11: Screenshots of a complete trajectory of browsing ArXiv. Given the task: \u201cFor non-English submissions,\ndo I need to provide a multi-language abstract, if need, answer the separator between the multiple abstracts.\u201d The\nagent interacts with the ArXiv website and obtains the answer: \u201cLine (\"-----\")\u201d\nStep 1: Click [10]\nStep 2: Click [17]\nStep 3: ANSWER\nFigure 12: Screenshots of a complete trajectory of browsing BBC News. Given the task: \u201cFind out which musician\nmade the headlines in Music News.\u201d The agent interacts with the BBC News website and obtains the answer: \u201cThe\nmusician who made the headlines in Music News is Taylor Swift.\u201d\nStep 1: Type [15]; Jakarta\nStep 2: Click [60]\nStep 3: Click [63]\nStep 4: Click [99]\nStep 5: Scroll down\nStep 6: Scroll up\nStep 7: Click [41]\nStep 8: Click [54]\nStep 9: ANSWER\nFigure 13: Screenshots of a complete trajectory of browsing Booking. Given the task: \u201cFind the cheapest available\nhotel room for a three-night stay from 1st Jan in Jakarta. The room is for 2 adults, just answer the cheapest hotel\nroom and the price.\u201d The agent interacts with the Booking website and obtains the answer: \u201cThe cheapest hotel\nroom is at OYO 3755 Sweet Home for US$14 for a three-night stay.\u201d\nStep 1: Click [24]\nStep 2: Click [25]\nStep 3: Click [21]\nStep 4: Click [3]\nStep 5: Click [22]\nStep 6: Click [25]\nStep 7: Click [27]\nStep 8: Click [26]\nStep 9: Click [25]\nStep 10: Click [27]\nStep 11: Click [26]\nStep 12: ANSWER\nFigure 14: Screenshots of a complete trajectory of browsing Cambridge Dictionary. Given the task: \u201cGo to the\nPlus section of Cambridge Dictionary, find Image quizzes and do an easy quiz about Animals and tell me your final\nscore.\u201d The agent interacts with the Cambridge Dictionary website and obtains the answer: \u201cThe final score on the\neasy Animals quiz is 6 out of 6.\u201d\nStep 1: Click [2]\nStep 2: Type [0]; Introduction \nto Finance: The Basics\nStep 3: Click [17]\nStep 4: Type [2]; Xi Yang\nStep 5: ANSWER\nFigure 15: Screenshots of a complete trajectory of browsing Coursera. Given the task: \u201cIdentify a course on\nCoursera named \u2019Introduction to Finance: The Basics\u2019, who is the course instructor, and what other courses does\nhe/she teach.\u201d The agent interacts with the Coursera website and obtains the answer: The course instructor for\n\u2018Introduction to Finance: The Basics\u2019 is Xi Yang. Xi Yang also teaches \u2018Introduction to Finance: The Role of\nFinancial Markets\u2019 and \u2018Introduction to Financial Analysis - The \"Why?\"\u2019\nStep 1: Click [16]\nStep 2: Click [23]\nStep 3: Scroll down\nStep 4: Scroll down\nStep 5: ANSWER\nFigure 16: Screenshots of a complete trajectory of browsing ESPN. Given the task: \u201cHow many NBA teams are\nthere and list all the teams with \u2018New\u2019 in their name.\u201d The agent interacts with the ESPN website and obtains the\nanswer: \u201cThere are 30 NBA teams. The teams with \u2018New\u2019 in their name are the New York Knicks and the New\nOrleans Pelicans.\u201d\nStep 1: Click [5]\nStep 2: Type [0]; climate \nchange data visualization\nStep 3: Click [29]\nStep 4: Click [52]\nStep 5: ANSWER\nFigure 17: Screenshots of a complete trajectory of browsing GitHub. Given the task: \u201cSearch for an open-source\nproject related to \u2018climate change data visualization\u2019 on GitHub and report the project with the most stars.\u201d The\nagent interacts with the GitHub website and obtains the answer: \u201cresource-watch/resource-watch with 63 stars\u201d\nStep 1: Click [2]\nStep 2: Type [7]; Boston Logan Airport\nStep 3: Type [8]; North Station\nStep 4: ANSWER\nFigure 18: Screenshots of a complete trajectory of browsing Google Map. Given the task: \u201cPlan a trip from Boston\nLogan Airport to North Station.\u201d The agent interacts with the Google Map website and obtains the answer: \u201cThe\nfastest route from Boston Logan International Airport to North Station is via MA-1A S and takes approximately 8\nminutes in current traffic conditions.\u201d\nStep 1: Click [10]\nStep 2: Click [12]\nStep 3: Type [13]; Dublin\nStep 4: Type [14]; Athens, Greece\nStep 5: Click [16]\nStep 6: Click [41]\nStep 7: Click [78]\nStep 8: Click [19]\nStep 9: Click [34]\nStep 10: Click [2]\nStep 11: ANSWER\nFigure 19: Screenshots of a complete trajectory of browsing Google Flights. Given the task: \u201cSearch a one-way\nflight from Dublin To Athens Greece for 1 Adult that leaves on December 30 and analyse the price graph for the\nnext 2 months.\u201d The agent interacts with the Google Flights website and obtains the answer: \u201cThe price graph has\nbeen analyzed, and it shows the trends for flight prices from Dublin to Athens over the next two months.\u201d\nStep 1: Type [7]; comedy \nmovies sorted by user ratings\nStep 2: Click [29]\nStep 3: Scroll down\nStep 4: Scroll down\nStep 5: ANSWER\nFigure 20: Screenshots of a complete trajectory of browsing Google Search. Given the task: \u201cShow me a list of\ncomedy movies, sorted by user ratings. Show me the Top 5 movies.\u201d The agent interacts with the Google Search\nwebsite and obtains the answer: \u201cThe Top 5 comedy movies sorted by user ratings are:Life Is Beautiful; Back to the\nFuture; The Intouchables; City Lights; Modern Times\u201d\nStep 1: Click [1]\nStep 2: Click [15]\nStep 3: Click [30]\nStep 4: Click [68]\nStep 5: Click [70]\nStep 6: ANSWER\nFigure 21: Screenshots of a complete trajectory of browsing Huggingface. Given the task: \u201cLook up a model\nwith a license of cc-by-sa-4.0 with the most likes on Huggingface.\u201d The agent interacts with the Huggingface\nwebsite and obtains the answer: \u201cThe model with a license of cc-by-sa-4.0 with the most likes on Hugging Face is\n\"replit/replit-code-v1-3b\" with 703 likes.\u201d\nStep 1: Type [8]; x^5-20x^4\n+163x^3-676x^2+1424x-1209\nStep 2: Scroll down\nStep 3: ANSWER\nFigure 22: Screenshots of a complete trajectory of browsing Wolfram Alpha. Given the task: \u201cSimplify x^5-\n20x^4+163x^3-676x^2+1424x-1209 so that it has fewer items.\u201d The agent interacts with the Wolfram Alpha website\nand obtains the answer: \u201c(x - 4)^5 + 3(x - 4)^3 + 7\u201d\nStep 1: Click [34]\nStep 2: Click [13]\nStep 3: Click [14]\nStep 4: Click [16]\nStep 5: Click [78]\nStep 6: Click [16]\nFigure 23: An error case for Google Flights. Given the task:\u201cFind the lowest fare from all eligible one-way flights\nfor 1 adult from JFK to Heathrow on Jan. 22.\u201d Agent fails to select the correct numerical label though it really wants\nto select 22 January.\nStep 1: Scroll down\nStep 2: Scroll down\nStep 3: Scroll down\nStep 4: Scroll up\nStep 6: Scroll down\nStep 5: Scroll up\nFigure 24: An error case for Allrecipes. Given the task:\u201cSearch for a recipe for Beef Wellington on Allrecipes that\nhas at least 200 reviews and an average rating of 4.5 stars or higher. List the main ingredients required for the dish.\u201d\nAgent fails to scroll the page correctly and find ingredients.\nStep 1: Click [2]\nStep 2: Type [0]; Artificial \nIntelligence for Healthcare\nStep 3: Click [17]\nStep 4: Click [13]\nStep 5: Click [8]\nStep 6: ANSWER\nFigure 25: An error case for Coursera. Given the task: \u201cIdentify a course on Coursera related to \u2019Artificial\nIntelligence for Healthcare\u2019 and note the course duration along with the number of quizzes in Assessments.\u201d The\nagent provides the answer: \u201cModule 1 includes 3 quizzes in the Assessments.\u201d However, we also require other\nModules and agent fails to provide the results.\nStep 1: Click [44]\nStep 2: Click [24]\nStep 3: ANSWER\nFigure 26: An error case for BBC News. Given the task: \u201cFind out how many teams are in the Scottish Premiership\nof the Football Tournament and when did the Hibernian team\u2019s most recent match start?\u201d The agent provides the\nanswer: \u201cThere are 12 teams in the Scottish Premiership. To find out the exact start time of Hibernian\u2019s most recent\nmatch, further interaction with the website would be required.\u201d The Agent knows that the task is not yet complete,\nbut it ends its navigation early, even though it can find the Hibernian team\u2019s most recent match.\n"
  },
  {
    "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI",
    "link": "https://arxiv.org/pdf/2401.14019.pdf",
    "upvote": "18",
    "text": "Unitxt: Flexible, Shareable and Reusable\nData Preparation and Evaluation for Generative AI\nElron Bandel\nYotam Perlitz\nElad Venezian\nRoni Friedman-Melamed\nOfir Arviv\nMatan Orbach\nShachar Don-Yehyia\nDafna Sheinwald\nAriel Gera\nLeshem Choshen\nMichal Shmueli-Scheuer\nYoav Katz\nIBM Research\nelron.bandel@ibm.com\nAbstract\nIn the dynamic landscape of generative NLP,\ntraditional text processing pipelines limit re-\nsearch flexibility and reproducibility, as they\nare tailored to specific dataset, task, and model\ncombinations. The escalating complexity, in-\nvolving system prompts, model-specific for-\nmats, instructions, and more, calls for a shift to\na structured, modular, and customizable solu-\ntion. Addressing this need, we present Unitxt,\nan innovative library for customizable textual\ndata preparation and evaluation tailored to gen-\nerative language models. Unitxt natively inte-\ngrates with common libraries like HuggingFace\nand LM-eval-harness and deconstructs process-\ning flows into modular components, enabling\neasy customization and sharing between practi-\ntioners. These components encompass model-\nspecific formats, task prompts, and many other\ncomprehensive dataset processing definitions.\nThe Unitxt Catalog centralizes these compo-\nnents, fostering collaboration and exploration\nin modern textual data workflows. Beyond be-\ning a tool, Unitxt is a community-driven plat-\nform, empowering users to build, share, and\nadvance their pipelines collaboratively. Join\nthe Unitxt community;\nProject: https://github.com/IBM/unitxt.\nUI: https://bit.ly/unitxt-explore\nVideo: https://bit.ly/unitxt-video\n1\nIntroduction\nTextual data processing has always been at the heart\nof NLP, but in the current landscape it has taken\non new roles. A prominent one comes from LLMs\u2019\nrole as general interfaces, that receive an exam-\nple, but also the task they should perform, general\nsystem instruction and other specifications, all in\nnatural language. Thus, the inputs \u2013 or prompts \u2013\nthat a model receives now consist of many compo-\nnents, that can be combined in different ways: task\ninstructions (Wei et al., 2022), in-context demon-\nstrations (Brown et al., 2020), system prompts and\nmore. At the same time, for text generation models,\nmodel outputs are themselves rich textual data, and\nthus can be processed and evaluated with a range\nof different approaches and paradigms. Therefore,\ntextual data processing for LLMs is growing in-\ncreasingly complex. It incorporates a large number\nof non-trivial design choices and parameters, which\npose new challenges for maintaining flexibility and\nreproducibility in LLM research.\nBroadly, research in computer science, and in\nparticular within NLP, thrives on that combination\nof flexibility and reproducibility. On the one hand,\nit should be simple to try new ideas: to compare\ndifferent approaches, choose parameters, and eas-\nily switch out one workflow or architecture with\nanother. On the other hand, the results of these\nexplorations must be shared in such a way that\nothers are able to \u2013 and crucially, are likely to \u2013\nreproduce and try them. To enable the above, code\nreuse, a well-defined API and ease of use are piv-\notal, ensuring reproducibility and applicability in\npractice. How such traits allow for widespread\nadoption is epitomized by the Hugging Face trans-\nformers library (Wolf et al., 2020). Today, a modest\nset of hyperparameters is sufficient to reproduce\na training or inference workflow. This has had an\nundeniable and dramatic impact on the ability to\nmake progress in the field.\nSuch is not the case, however, for textual data\npipelines.\nUnfortunately, data-preparation for\nLLMs has no standards, Processing model inputs or\noutputs of the same data often comes with rewriting\nthe code, leading to mismatches in reported values\n(Post, 2018), unanswerable examples and hidden\nbugs (Fourrier et al., 2023) and general time waste.\nCrucially, the additional components beyond tra-\nditional processing, such as in-context demonstra-\ntions, have no canonical API. This prevents fair\ncomparisons between different studies, discourages\nexploring combinations, hinders integrating a par-\nticular approach (say, a new type of system prompt)\ninto an existing NLP system, and prevents major\narXiv:2401.14019v1  [cs.CL]  25 Jan 2024\nscale-ups in terms of datasets, tasks and metrics.\nTo address these gaps, we introduce a new collab-\norative framework for unified textual data process-\ning named Unitxt. This new Python library sup-\nports multilingual textual data processing through\nflexible pipelines called recipes. A recipe (see \u00a74.1\nand examples in \u00a73) is a sequence of textual data\nprocessing operators, including, among others, op-\nerators that load data, pre-process it, handle the\npreparation of different parts of a prompt, or evalu-\nate model predictions (see Figure 1).\nAiming for reuse, Unitxt ships with a catalog\ncontaining a wide variety of pre-defined recipes for\nvarious tasks. These are all based on a diverse set of\nbuilt-in operators that are also shared in the catalog.\nHaving a centralized location for these components,\nwhere anyone can add new ingredients (such as\nrecipes or operators), or share existing ones, fosters\ncollaboration, transparency and reproducibility.\nAs fitting a Recipe, the modularity of Unitxt en-\nables mixing and matching of ingredients to create\nnew recipes. This ability to mix and match in-\ngredients enables Unitxt to support 100K+ recipe\nconfigurations, allowing users to experiment with a\nlarge set of such recipes by to obtain multiple con-\nfigurations of tasks, datasets and new formatting\n(see \u00a73 for example).\nChanging libraries is always a nuisance; there-\nfore, Unitxt is designed to seamlessly integrate\nwith preexisting code, offering a hassle-free ex-\nperience without even needing a pip install. For\ninstance, Unitxt can load HuggingFace datasets\nand produce outputs that adhere to the same format,\nallowing it to integrate seamlessly with other parts\nof your codebase (\u00a74.4.1). Demonstrating this, in-\ncorporating Unitxt, with all its tasks, datasets, tem-\nplates and metrics into LM-eval-harness (Gao et al.,\n2023) required only 30 lines of code, while pre-\nserving the current API and ensuring a smooth tran-\nsition and compatibility with existing workflows\n(App. A).\nUnitxt, an open-source library, is under ac-\ntive development by IBM and the community.\nThe code and documentation are available on\nGitHub at:\nhttps://github.com/IBM/unitxt,\nthe UI, at https://bit.ly/unitxt-explore\nwhile the demo video is at https://bit.ly/\nunitxt-video.\n2\nUse cases\nUnitxt for evaluation: The increasing capabilities\nof LLMs require evaluation frameworks that test\nmodels over an unprecedented number of datasets,\ntasks and configurations (Liang et al., 2022; Gao\net al., 2023; Contributors, 2023). Unitxt can serve\nas the backbone of such evaluation efforts, by sup-\nporting easy changes across multiple important\naxes, including tasks, languages, prompt struc-\nture (e.g. instructions, verbalizations, etc.), aug-\nmentation robustness and more. Moreover, with\nthe Unitxt Catalog, different distinct projects\ncan share their full evaluation pipelines, making\ntheir data-preparation and evaluation metrics repro-\nducible.\nUnitxt for training: Modern LLM training\nframeworks have extensive data requirements to at-\ntain state-of-the-art performance. Multiple datasets\nacross diverse domains and languages need to be\nleveraged to impart broad capabilities; Various\nprompt formulations and verbalizations are neces-\nsary to enable instruction-following, where verbal-\nizations are the final text form. However, combin-\ning heterogeneous data sources and textual repre-\nsentations poses significant engineering challenges.\nWithout a common underlying framework, data\naugmentation, multitask learning and few-shot tun-\ning become prohibitively complex. This is where\nUnitxt steps in, as an indispensable data backend.\nUnitxt enables seamless fusion of diverse\ndatasets. Moreover, the standardized format also\nfacilitates changes to the datasets, dynamic prompt\ngeneration, data augmentations and model-specific\nformat, to name just a few.\nBy handling the\ndata wrangling complexity, Unitxt empowers re-\nsearchers to focus on creating performant, robust\nand safe LLMs.\nFor both evaluation and training, Unitxt has al-\nready been adopted as a core utility for LLMs in\nIBM by multiple teams working on various NLP\ntasks, including classification, extraction, summa-\nrization, generation, question answering, code, bi-\nases and more. In total, the open source catalog\ncontains more than 100K possible pipeline config-\nurations.\n3\nUnitxt: Library Tour\nTo introduce unitxt, we begin with a tour of the li-\nbrary, and specifically, with the creation of a recipe.\nA recipe contains all the data-processing and met-\nric configurations needed, including the data, task,\nStandardize \nDataset Fields\nVerbalize\nGenerate with \nModel\nDe-verbalize\nStandardize \nMetrics Inputs\nTask\nTemplate\nFormat\nLoad Data\nCalculate \nMetric\nResources\n\u2026\nFormat for \nModel \nunitxt.evaluate()\nunitxt.load_dataset()\nOperators:\n\u2026\nRename \nField\nSplit \nTrain/Val\nAdd \nNoise \nLabels\nSample \nDemo\nAdd \nSystem \nPrompt\nFinalize \nFormat\nAugment \nSpaces\nFigure 1: Unitxt flow: The upper section illustrates the data-preparation pipeline \u00a74.4.1, encompassing raw dataset\nloading, standardization according to the task interface, verbalization using templates, and application of formatting.\nThe lower section showcases the evaluation pipeline \u00a74.4.2, involving de-verbalization operations and output\nstandardization before performance evaluation with task-defined metrics. All components are described in \u00a74.2.\ntemplate and formatting (see details in \u00a74). Here\nwe define a recipe that loads the STS-B dataset for\na sentence similarity task:\n1 recipe = \"\"\"\n2\ncard=cards.stsb , # dataset info card\n3\ntemplate=templates.text_similarity ,\n4\nsys_prompt=prompts.helpful ,\n5\nformat=formats.user_agent ,\n6\nnum_demos =1\n7 \"\"\"\nWith a recipe, a concrete dataset can be loaded:\ndataset = unitxt.load_dataset(recipe)\nImportantly, every data instance in a dataset loaded\nwith a unitxt recipe contains a fully prepared source\ntext, which can be directly passed as input to the\nmodel. For example, here is such source text for\none sentence-similarity data instance, integrated\nwith three formatting decisions, a \u201chelpful model\u201d\nsystem-prompt, a user-agent response schema and\none demonstration:\n[System] you are helpful model [/ System]\n[User]: for the following texts rank the\nsimilarity between 1 to 5.\nText 1: \"i love ice cream\"\nText 2: \"i like ice cream\"\n[Agent ]: 4.8\n[User]: Text 1: \"i hate pizza\"\nText 2: \"i like pizza\"\n[Agent ]:\nLoading a dataset with a unitxt recipe also adds\na metric-ready target text (created from the origi-\nnal target) to each data instance. To Evaluate the\nmodel\u2019s textual predictions, we call:\nresults = unitxt.evaluate(\ndataset ,\npredictions=predictions ,\n)\nThe evaluation results are a dictionary of task de-\nfined metric names and the values computed for\nthem.\n4\nDesign\nIn this section we outline the design of Unitxt.\nUnitxt processes data by applying a modular se-\nquence of operators, which are segmented into 5\nkey ingredients (\u00a74.2) color-coded as in Fig. 1:\n\u25a0 Resources, \u25a0 Task, \u25a0 Template, \u25a0 Format and\n\u25a0 Extensions. These ingredients are then used to\nbuild the data preparation (\u00a74.4.1) and evalua-\ntion (\u00a74.4.2) pipelines.\n4.1\nUnitxt Building Blocks\nWhen loading a dataset (as demonstrated in \u00a73),\nthe Unitxt ingredients are retrieved based on a\nData-Task Card and a Recipe.\n\u25a0\u25a0 Data-Task Card\nDefines how raw data (in-\nputs and targets) are standardized for a certain task.\nTypically, this includes data wrangling actions, e.g.\nrenaming fields, filtering data instances, modifying\nvalues, train/test/val splitting etc. It also describes\nthe resource from which the data is loaded.\n\u25a0\u25a0\u25a0\u25a0\u25a0 Recipe\nA Recipe holds a complete\nspecification of a Unitxt pipeline: including the\nResources, Task, Template, Format and Extensions.\n4.2\nUnitxt Ingredients\n\u25a0 Resources\nRaw data and metrics are external\nresources utilized by Unitxt. Unitxt implements\nseveral APIs for raw-data and metric loading (e.g.,\nfrom Huggingface Hub, local files, and cloud stor-\nage).\n\u25a0 Task\nA Unitxt Task follows the formal defi-\nnition of an NLP task, such as multi-label classi-\nfication, named entity extraction, abstractive sum-\nmarization or translation. A task is defined by its\nstandard interface \u2013 namely, input and output fields\n\u2013 and by its evaluation metrics. Given a dataset, its\ncontents are standardized into the fields defined by\nan appropriate task by a Data-Task Card (\u00a74.1).\nAs an example of a defined task, consider sen-\ntence similarity: it has two input fields (named\n\u201csentence1\u201d and, \u201csentence2\u201d), one output field\n(named \u201clabel\u201d) and the conventional metric is\nSpearman correlation (Spearman, 1904).\n\u25a0 Template\nA Unitxt Template defines the ver-\nbalizations to be applied to the inputs and targets,\nas well as the de-verbalization operations over the\nmodel predictions. For example, in Fig 2, applying\nthe template to I like toast verbalizes it into\nclassify the sentence: \u201cI like toast\u201d.\nIn the other direction, template de-verbalization\ninvolves two steps. First, a general standardization\nof the output texts: taking only the first non-empty\nline of a model\u2019s predictions, lowercasing, strip-\nping whitespaces, etc. The second step standard-\nizes the output to the specific task at-hand. For\nexample, in Sentence Similarity, a prediction may\nbe a quantized float number outputted as a string\n(e.g \u201c2.43\u201d), or a verbally expressed numeric ex-\npression (e.g \u201ctwo and a half\u201d). This depends on\nthe verbalization defined by the template and the\nin-context demonstrations it constructs. Both types\nof outputs should be standardized before evaluation\nbegins \u2013 e.g. to a float for sentence similarity. Hav-\ning the de-verbalization steps defined within the\ntemplate enables templates reuse across different\nmodels and datasets.\nCrucially, in contrast to existing solutions (e.g.,\nBach et al., 2022) the templates, datasets and tasks\nin Unitxt are not exclusively tied. Each task can\nharness multiple templates and a template can be\nused for different datasets. Thus, the modularity of\nUnitxt allows mixing and matching, significantly\nenhancing re-usability and flexibility.\nTarget:\npositive\nSource:\nResource          Template       Format\n<SYS>You are a helpful agent</SYS>\nInstruction: Classify the sentence to one of the \nfollowing categories: positive, negative\nUser: classify this sentence: \u2018I like pizza\u2019\nAgent: positive\nUser: classify this sentence: \u2018I hate pizza\u2019\nAgent: negative\nUser: classify this sentence: \u2018I like toast\u2019\nAgent:\nFigure 2: Illustration of the data preparation pipeline\n(\u00a74.4.1), depicting the transformation from raw data and\nformatting specifications to the final text output. Com-\nponents include Resources (raw data), Format (model-\nspecific formatting requirements), and Template (ver-\nbalization).\n\u25a0 Format\nA Unitxt Format defines a set of ex-\ntra formatting requirements, unrelated to the under-\nlying data or task, including those pertaining to sys-\ntem prompts, special tokens or user/agent prefixes,\nand in-context demonstrations. Continuing the ex-\nample from Figure 2, the Unitxt format receives\nthe text produced by the template classify the\nsentence: \u201cI like toast\u201d, and adds the system\nprompt <SYS>You are a helpful agent</SYS>,\nthe Instruction-User-Agent schema cues, and the\ntwo presented demonstrations.\n\u25a0 Extensions\nUnitxt supports Extensions such\nas input-augmentation (for example, adding ran-\ndom whitespace, introducing spelling mistakes, or\nreplacing words with their synonyms) or label-\nnoising (replaces the labels in the demonstrations\nrandomly from a list of options). Such extensions\ncan be added anywhere in the data-preparation\npipeline between any two operators, depending on\nthe desired logic (see Fig. 1). Unitxt supports\nthe addition of custom extensions to the Catalog.\nEach extension is an independent unit, reusable\nacross different datasets and tasks, templates and\nformats.\n4.3\nUnitxt Catalog\nAll Unitxt artifacts \u2013 recipes, data-task cards, tem-\nplates, pre-processing operators, formats and met-\nrics \u2013 are stored in the Unitxt Catalog. In addi-\ntion to the open-source catalog, that can be found in\nthe documentation, users can choose to define a pri-\nvate catalog. This enables teams and organizations\nto harness the open Unitxt Catalog while uphold-\ning organizational requirements for additional pro-\nprietary artifacts.\n4.4\nUnitxt Pipelines\n4.4.1\nData Preparation Pipeline\nThe data preparation pipeline (top part ot Fig. 1) be-\ngins with standardizing the raw data into the task in-\nterface, as defined in the data-task card (\u00a74.1). The\nexamples are then verbalized by the template, and\nthe format operator applies system prompts, special\ntokens and in-context learning examples (\u00a74.2), as\nillustrated in Figure 2. To maintain compatibility,\nthe output of this pipeline is an HF dataset, that can\nbe saved or pushed to the hub.\n4.4.2\nEvaluation Pipeline\nThe evaluation pipeline (bottom part of Fig. 1) is\nresponsible for producing a list of evaluation scores\nthat reflect model performance. It includes a de-\nverbalization of the model outputs (as defined in\nthe template, see \u00a74.2), and a computation of per-\nformance by the metrics defined in the task. The\nstandardization of the task interface, namely, hav-\ning fixed names and types for its input and output\nfields, allows the use of any metric that accept such\nfields as input. In addition to the computed eval-\nuation scores, Unitxt metrics supports a built in\nmechanism for confidence interval reporting, using\nstatistical bootstrap (Perlitz et al., 2023).\n5\nUnitxt UI: Explore & Preview\nThe objective of the user interface is to guide users\nthrough the essential steps of recipe creation, il-\nlustrated with pertinent examples. Additionally,\nit allows for catalog exploration. The UI comple-\nments the experience with the option to execute\nthe examples on some pre-set model (e.g., flan-t5-\nbase), get the predictions and associated scores.\nThe interaction entry point is the tasks. Upon\nclicking, the tasks taxonomy is presented, and the\nusers have the option to choose the applicable task\ntype. Selecting a task results in showing only the\nrelevant datasets and templates. Once the user se-\nlects a dataset, and a template, and presses \u201cGener-\nate Prompts\u201d a random example enhanced with the\ntemplate is loaded. If the user wants to augment\nthe input with system prompt, or response-schema\nthose will be instantly added when opted for. As\nin-context learning evaluations are supported, the\nuser can select the preferred number of shots. Once\nsatisfied with the example, the user has the option\nto proceed with executing it on a model, wherein\nthe predictions and corresponding scores will be\ndisplayed for this specific example. Further, going\nto the code tab, the user can copy the associated\ncode into a notebook and run. Users have the op-\ntion to explore various examples, enhancing their\ncomprehension and confidence in the chosen con-\nfiguration.\n6\nRelated work\nStandardized data processing for evaluation and\ntraining has been a longstanding need in the NLP\ncommunity and has been repeatedly addressed in\nthe past. Datasets (Lhoest et al., 2021) and Eval-\nuate1 are community-driven libraries, providing a\nstandardized interface to diverse corpora and met-\nrics, as well as supporting many data processing\noperations. These packages, however, fall short\nof providing a standardized, shareable and repro-\nducible framework to cast the raw data into textual\nprompts and cast them back from text to a metric\ndigestible format. The lack of such a framework\nhinders reproducibility, as often slight variations in\nad-hoc text processing code may yield significantly\ndifferent evaluation scores. Moreover, it also pre-\nvents users from easily scaling up their experiment,\nas each task and dataset often requires specific code\nfor processing and evaluation. Unitxt builds on\ntop of these frameworks, harnessing them as re-\nsources (\u00a74.1) to produce a full data-preparation\nand evaluation framework.\nWhile several existing frameworks have con-\ntributed to data pipeline management workflows, a\ncommon drawback, for those we are aware of, is the\nabsence of a well-defined and flexible modularity\nin their design, such as the ability to define specific\ncomponents for system prompts, task instructions\nand model-specific formats. This absence of clearly\ndefined components makes it challenging to share\nand customize such pipelines effectively, across\ndifferent datasets and tasks.\nLike Unitxt, Tasksource (Sileo, 2023) supplies\ntools for consistent preprocessing over different\ndatasets, simplifying their usage. However, it is\nprimarily designed for discriminative tasks, uses\nfixed formats and lacks a modular design that en-\nables sharing, mixing and matching, and overall\n1https://github.com/huggingface/evaluate/\nSystem-prompt\nResponse scheme\nsysprompts\nFigure 3: Exploration UI showcasing configuration options for model input creation on the left, including param-\neters such as task, dataset card, template, system-prompt, response-schema, number of examples, and optional\naugmentations. The resulting model input is displayed in the prompt window.\nflexibility in processing steps. Promptsource (Bach\net al., 2022) focuses on making and sharing natural\nlanguage prompts but doesn\u2019t handle other types\nof data processing. Each prompt is tied to just one\ndataset, making it hard to reuse and share. Further-\nmore, prompts aren\u2019t split into system, instruction,\nand format parts, limiting options for flexibility\nand reuse. SeqIO (Roberts et al., 2022), offers\ntask-based pipelines encompassing pre-processing,\npost-processing, and handling metrics. However, a\nstructured breakdown of these processing steps is\nabsent, limiting the creation of shareable catalogs\nwithin the community. In this framework, each pro-\ncess is a generic function and specialized steps are\nmissing, like those designed for system prompts.\nA different branch of solutions are language\nmodel evaluation frameworks such as OpenCom-\npass (Contributors, 2023), HELM (Liang et al.,\n2022) and LM-eval-harness (Gao et al., 2023) also\nimplement their own standardized data processing\npipelines in order to obtain verbalized prompts for\nLMs. These, however, are highly coupled with the\ninference engine and cannot be used as standalone\ndata-processing pipelines or integrated into other\ncode bases.\n7\nConclusion\nIn this paper, we have introduced Unitxt, an open-\nsource Python library aimed at unifying textual\ndata processing pipelines for large language mod-\nels. Unitxt provides a modular, flexible frame-\nwork that enables mixing and matching of vari-\nous pipeline components like loaders, templates,\nformats and metrics. Unitxt key capabilities are,\nstandardization, flexibility, collaboration and scale.\nUnitxt has already been successfully deployed\nfor large language model evaluation and training\nwithin IBM. As the library matures through open-\nsource community involvement, we hope its adop-\ntion will grow to push the frontiers of textual data\nprocessing for LLMs. We believe Unitxt has the\npotential to significantly impact research and de-\nvelopment of large language models by unifying\ntextual data processing. Through its emphasis on\nflexibility, reproducibility and collaboration, unitxt\ncan help drive progress towards more capable, safer\nand trustworthy LLMs.\n8\nLimitations\nWhile unitxt makes significant progress towards\nunified textual data processing for LLMs, some\nlimitations still remain:\n\u2022 The Unitxt Catalog, while already substan-\ntial in coverage, needs expansion to encom-\npass more datasets, languages, and niche tasks.\nCommunity contributions will be key to en-\nhancing catalog diversity.\n\u2022 Coverage of evalution metrics, especially for\ngenerative tasks, needs improvement. We plan\nto incorporate more reference-free and LLM-\nbased metrics going forward.\n\u2022 Training data augmentation abilities, while\nflexible currently, can be expanded further\nwith techniques like back-translation for mul-\ntilinguality.\n\u2022 While using Unitxt recipes is as simple as\nspecifying the recipe ingrediants, adding new\ndatasets or operators requires learning the\nUnitxt operator language. Additional doc-\numentation, examples and IDE support could\nhelp alleviate this.\nAddressing these limitations through open-\nsource community involvement is the major focus\ngoing forward. By tapping into collective expertise,\nwe envision unitxt becoming an indispensable tex-\ntual data processing backbone for the responsible\ndevelopment, evaluation and deployment of large\nlanguage models.\nReferences\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert\nWebson, Colin Raffel, Nihal V. Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea San-\ntilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,\nGunjan Chhablani, Han Wang, Jason Alan Fries,\nMaged S. Al-shaibani, Shanya Sharma, Urmish\nThakker, Khalid Almubarak, Xiangru Tang, Xian-\ngru Tang, Mike Tian-Jian Jiang, and Alexander M.\nRush. 2022. Promptsource: An integrated develop-\nment environment and repository for natural language\nprompts.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nOpenCompass Contributors. 2023.\nOpencompass:\nA universal evaluation platform for foundation\nmodels.\nhttps://github.com/open-compass/\nopencompass.\nCl\u00e9mentine Fourrier, Alex Cabrera, Stella Biderman,\nNathan Habib, and Thomas Wolf. 2023. Open llm\nleaderboard: Drop deep dive.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,\nSid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li,\nKyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf,\nAviya Skowron, Lintang Sutawika, Eric Tang, An-\nish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n2023. A framework for few-shot language model\nevaluation.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario \u0160a\u0161ko, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Cl\u00e9ment Delangue, Th\u00e9o Matus-\nsi\u00e8re, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, Fran\u00e7ois\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175\u2013184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv,\nLiat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal\nShmueli-Scheuer, and Leshem Choshen. 2023. Ef-\nficient benchmarking (of language models). arXiv\npreprint arXiv:2308.11696.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186\u2013\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz\nMohiuddin, Curtis Hawthorne, Aitor Lewkowycz,\nAlex Salcianu, Marc van Zee, Jacob Austin, Se-\nbastian Goodman, Livio Baldini Soares, Haitang\nHu, Sasha Tsvyashchenko, Aakanksha Chowdh-\nery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-\ncia, Jianmo Ni, Andrew Chen, Kathleen Kenealy,\nJonathan H. Clark, Stephan Lee, Dan Garrette, James\nLee-Thorp, Colin Raffel, Noam Shazeer, Marvin\nRitter, Maarten Bosma, Alexandre Passos, Jeremy\nMaitin-Shepard, Noah Fiedel, Mark Omernick, Bren-\nnan Saeta, Ryan Sepassi, Alexander Spiridonov,\nJoshua Newlan, and Andrea Gesmundo. 2022. Scal-\ning up models and data with t5x and seqio. arXiv\npreprint arXiv:2203.17189.\nDamien Sileo. 2023. tasksource: Structured dataset\npreprocessing annotations for frictionless extreme\nmulti-task learning and evaluation. arXiv preprint\narXiv:2301.05948.\nC. Spearman. 1904. The proof and measurement of as-\nsociation between two things. The American Journal\nof Psychology, 15(1):72\u2013101.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38\u201345, Online. Association\nfor Computational Linguistics.\nA\nLM Eval Harness Integration\nLM-eval-harness (Gao et al., 2023) is one of the\nmost commonly used open source evaluation frame-\nworks. It leverages a yaml-based declarative lan-\nguage which defines loading of the dataset, the\ndataset splits, the prompt used and the metrics in a\nsingle file for each task. Many tasks are supported,\nincluding multi-class classification, multiple choice\nquestion answering, and generation tasks. Unitxt\nwas integrated into LM-eval-harness to extend LM-\neval-harness to support new tasks and metrics that\ncurrently are not supported, including multi-label\nclassification, named entity extraction, and target\nsentiment analysis.\nSince Unitxt recipes can be loaded as standard\nHF datasets, no code changes were required to add\nthe Unitxt data preparation pipeline to LM-eval-\nharness. Adding a Unitxt recipe requires only\none line change in a LM-eval-harness yaml (see\nFigure 4 in Appendix). Adding the Unitxt met-\nrics required about 30 lines of code, to register\nthe Unitxt metrics to the LM-eval-harness metrics\nregistry.\n1 group: glue\n2 task: unitxt_unfair_tos\n3 dataset_path: unitxt/data\n4 dataset_name: card=cards.unfair_tos ,template_card_index=templates.classification.\nmulti_label.default ,format=formats.user_agent\n5 output_type:\ngenerate_until\n6 training_split: train\n7 validation_split: validation\n8 doc_to_text: \"{{ source }}\"\n9 doc_to_target: target\n10 generation_kwargs:\n11\nuntil:\n12\n- \"</s>\"\n13 metric_list:\n14\n- metric: unitxt_f1_micro_multi_label\n15 metadata:\n16\nversion: 1.0\nFigure 4: Unitxt and LM-eval-harness integration. A Unitxt recipe can be integrated as an LM-eval-harness task,\nby setting the dataset_path (line 3) to unitxt/data and the setting the recipe in the dataset_name (line 4). Unitxt\nmetrics can be used like any other metric (line 14).\n"
  },
  {
    "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
    "link": "https://arxiv.org/pdf/2401.14404.pdf",
    "upvote": "16",
    "text": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning\nXinlei Chen1\nZhuang Liu1\nSaining Xie2\nKaiming He1\n1FAIR, Meta\n2New York University\nAbstract\nIn this study, we examine the representation learning\nabilities of Denoising Diffusion Models (DDM) that were\noriginally purposed for image generation. Our philosophy\nis to deconstruct a DDM, gradually transforming it into a\nclassical Denoising Autoencoder (DAE). This deconstruc-\ntive procedure allows us to explore how various components\nof modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern compo-\nnents are critical for learning good representations, while\nmany others are nonessential. Our study ultimately arrives\nat an approach that is highly simplified and to a large extent\nresembles a classical DAE. We hope our study will rekindle\ninterest in a family of classical methods within the realm of\nmodern self-supervised learning.\n1. Introduction\nDenoising is at the core in the current trend of genera-\ntive models in computer vision and other areas. Popularly\nknown as Denoising Diffusion Models (DDM) today, these\nmethods [36, 37, 38, 23, 29, 11] learn a Denoising Au-\ntoencoder (DAE) [39] that removes noise of multiple lev-\nels driven by a diffusion process. These methods achieve\nimpressive image generation quality, especially for high-\nresolution, photo-realistic images [33, 32]\u2014in fact, these\ngeneration models are so good that they appear to have\nstrong recognition representations for understanding the vi-\nsual content.\nWhile DAE is a powerhouse of today\u2019s generative mod-\nels, it was originally proposed for learning representations\n[39] from data in a self-supervised manner. In today\u2019s com-\nmunity of representation learning, the arguably most suc-\ncessful variants of DAEs are based on \u201cmasking noise\u201d\n[39], such as predicting missing text in languages (e.g.,\nBERT [10]) or missing patches in images (e.g., MAE [21]).\nHowever, in concept, these masking-based variants remain\nsignificantly different from removing additive (e.g., Gaus-\nsian) noise: while the masked tokens explicitly specify un-\nknown vs. known content, no clean signal is available in\nthe task of separating additive noise.\nNevertheless, to-\ninv\nPCA\ndenoised\nimage\nPCA\nencoder\ndecoder\nnoise\nnoised\nclean image\nnoised\ndenoised\nFigure 1. The latent Denoising Autoencoder (l-DAE) architecture\nwe have ultimately reached, after a thorough exploration of decon-\nstructing Denoising Diffusion Models (DDM) [23], with the goal\nof approaching the classical Denoising Autoencoder (DAE) [39]\nas much as possible. Here, the clean image (left) is projected onto\na latent space using patch-wise PCA, in which noise is added (mid-\ndle). It is then projected back to pixels via inverse PCA. An au-\ntoencoder is learned to predict a denoised image (right). This sim-\nple architecture largely resembles classical DAE (with the main\ndifference that noise is added to the latent) and achieves competi-\ntive self-supervised learning performance.\nday\u2019s DDMs for generation are dominantly based on ad-\nditive noise, implying that they may learn representations\nwithout explicitly marking unknown/known content.\nMost recently, there has been an increasing interest\n[40, 28] in inspecting the representation learning ability of\nDDMs.\nIn particular, these studies directly take off-the-\nshelf pre-trained DDMs [23, 32, 11], which are originally\npurposed for generation, and evaluate their representation\nquality for recognition. They report encouraging results us-\ning these generation-oriented models. However, these pio-\nneering studies obviously leave open questions: these off-\nthe-shelf models were designed for generation, not recog-\nnition; it remains largely unclear whether the representa-\ntion capability is gained by a denoising-driven process, or a\ndiffusion-driven process.\nIn this work, we take a much deeper dive into the direc-\ntion initialized by these recent explorations [40, 28]. Instead\n1\narXiv:2401.14404v1  [cs.CV]  25 Jan 2024\nof using an off-the-shelf DDM that is generation-oriented,\nwe train models that are recognition-oriented. At the core of\nour philosophy is to deconstruct a DDM, changing it step-\nby-step into a classical DAE. Through this deconstructive\nresearch process, we examine every single aspect (that we\ncan think of) of a modern DDM, with the goal of learning\nrepresentations. This research process gains us new under-\nstandings on what are the critical components for a DAE to\nlearn good representations.\nSurprisingly, we discover that the main critical compo-\nnent is a tokenizer [33] that creates a low-dimensional latent\nspace.\nInterestingly, this observation is largely indepen-\ndent of the specifics of the tokenizer\u2014we explore a stan-\ndard VAE [26], a patch-wise VAE, a patch-wise AE, and\na patch-wise PCA encoder. We discover that it is the low-\ndimensional latent space, rather than the tokenizer specifics,\nthat enables a DAE to achieve good representations.\nThanks to the effectiveness of PCA, our deconstructive\ntrajectory ultimately reaches a simple architecture that is\nhighly similar to the classical DAE (Fig. 1). We project\nthe image onto a latent space using patch-wise PCA, add\nnoise, and then project it back by inverse PCA. Then we\ntrain an autoencoder to predict a denoised image. We call\nthis architecture \u201clatent Denoising Autoencoder\u201d (l-DAE).\nOur deconstructive trajectory also reveals many other\nintriguing properties that lie between DDM and classical\nDAE. For one example, we discover that even using a sin-\ngle noise level (i.e., not using the noise scheduling of DDM)\ncan achieve a decent result with our l-DAE. The role of us-\ning multiple levels of noise is analogous to a form of data\naugmentation, which can be beneficial, but not an enabling\nfactor. With this and other observations, we argue that the\nrepresentation capability of DDM is mainly gained by the\ndenoising-driven process, not a diffusion-driven process.\nFinally, we compare our results with previous baselines.\nOn one hand, our results are substantially better than the off-\nthe-shelf counterparts (following the spirit of [40, 28]): this\nis as expected, because these are our starting point of decon-\nstruction. On the other hand, our results fall short of base-\nline contrastive learning methods (e.g., [7]) and masking-\nbased methods (e.g., [21]), but the gap is reduced. Our study\nsuggests more room for further research along the direction\nof DAE and DDM.\n2. Related Work\nIn the history of machine learning and computer vision, the\ngeneration of images (or other content) has been closely\nintertwined with the development of unsupervised or self-\nsupervised learning. Approaches in generation are concep-\ntually forms of un-/self-supervised learning, where models\nwere trained without labeled data, learning to capture the\nunderlying distributions of the input data.\nThere has been a prevailing belief that the ability of\na model to generate high-fidelity data is indicative of its\npotential for learning good representations.\nGenerative\nAdversarial Networks (GAN) [18], for example, have ig-\nnited broad interest in adversarial representation learning\n[13, 12]. Variational Autoencoders (VAEs) [26], originally\nconceptualized as generative models for approximating data\ndistributions, have evolved to become a standard in learning\nlocalized representations (\u201ctokens\u201d), e.g., VQVAE [30] and\nvariants [16]. Image inpainting [2], essentially a form of\nconditional image generation, has led to a family of mod-\nern representation learning methods, including Context En-\ncoder [31] and Masked Autoencoder (MAE) [21].\nAnalogously, the outstanding generative performance of\nDenoising Diffusion Models (DDM) [36, 37, 38, 23, 11] has\ndrawn attention for their potential in representation learn-\ning. Pioneering studies [40, 28] have begun to investigate\nthis direction by evaluating existing pre-trained DDM net-\nworks. However, we note that while a model\u2019s generation\ncapability suggests a certain level of understanding, it does\nnot necessarily translate to representations useful for down-\nstream tasks. Our study delves deeper into these issues.\nOn the other hand, although Denoising Autoencoders\n(DAE) [39] have laid the groundwork for autoencoding-\nbased representation learning, their success has been mainly\nconfined to scenarios involving masking-based corruption\n(e.g., [21, 41, 17, 5]). To the best of our knowledge, lit-\ntle or no recent research has reported results on classical\nDAE variants with additive Gaussian noise, and we believe\nthat the underlying reason is that a simple DAE baseline\n(Fig. 2(a)) performs poorly1 (e.g., in \u223c20% Fig. 5).\n3. Background: Denoising Diffusion Models\nOur deconstructive research starts with a Denoising Diffu-\nsion Model (DDM) [36, 37, 38, 23, 11]. We briefly describe\nthe DDM we use, following [11, 32].\nA diffusion process starts from a clean data point (z0)\nand sequentially adds noise to it. At a specified time step t,\nthe noised data zt is given by:\nzt = \u03b3tz0 + \u03c3t\u03f5\n(1)\nwhere \u03f5\u223cN(0, I) is a noise map sampled from a Gaussian\ndistribution, and \u03b3t and \u03c3t define the scaling factors of the\nsignal and of the noise, respectively. By default, it is set\n\u03b32\nt + \u03c32\nt = 1 [29, 11].\nA denoising diffusion model is learned to remove the\nnoise, conditioned on the time step t.\nUnlike the orig-\ninal DAE [39] that predicts a clean input, the modern\nDDM [23, 29] often predicts the noise \u03f5. Specifically, a\n1According to the authors of MoCo [20] and MAE [21], significant\neffort has been devoted to DAE baselines during the development of those\nworks, following the best practice established. However, it has not led to\nmeaningful results (<20% accuracy).\n2\nimage\nlatent\nencoder\ndecoder\ntokenizer\nnoised\ndenoised\ndenoised\nimage\nencoder\ndecoder\nnoise\nnoised\n(a) a classical Denoising Autoencoders (DAE)\nimage\nlatent\nencoder\ndecoder\nnoise\ntokenizer\nnoised\ndenoised\ndenoised\nimage\nencoder\ndecoder\nnoise\nnoised\n(b) a modern Denoising Diffusion Model (DDM) on a latent space\nFigure 2. A classical DAE and a modern DDM. (a) A classical\nDAE that adds and predicts noise on the image space. (b) State-of-\nthe-art DDMs (e.g., LDM [33], DIT [32]) that operate on a latent\nspace, where the noise is added and predicted.\nloss function in this form is minimized:\n\u2225\u03f5 \u2212 net(zt)\u22252\n(2)\nwhere net(zt) is the network output. The network is trained\nfor multiple noise levels given a noise schedule, conditioned\non the time step t. In the generation process, a trained model\nis iteratively applied until it reaches the clean signal z0.\nDDMs can operate on two types of input spaces. One\nis the original pixel space [11], where the raw image x0\nis directly used as z0. The other option is to build DDMs\non a latent space produced by a tokenizer, following [33].\nSee Fig. 2(b). In this case, a pre-trained tokenizer f (which\nis often another autoencoder, e.g., VQVAE [30]) is used to\nmap the image x0 into its latent z0=f(x0).\nDiffusion Transformer (DiT). Our study begins with\nthe Diffusion Transformer (DiT) [32].\nWe choose this\nTransformer-based DDM for several reasons: (i) Unlike\nother UNet-based DDMs [11, 33], Transformer-based ar-\nchitectures can provide fairer comparisons with other self-\nsupervised learning baselines driven by Transformers (e.g.,\n[7, 21]); (ii) DiT has a clearer distinction between the en-\ncoder and decoder, while a UNet\u2019s encoder and decoder are\nconnected by skip connections and may require extra effort\non network surgery when evaluating the encoder; (iii) DiT\ntrains much faster than other UNet-based DDMs (see [32])\nwhile achieving better generation quality.\nWe use the DiT-Large (DiT-L) variant [32] as our DDM\nbaseline. In DiT-L, the encoder and decoder put together\nhave the size of ViT-L [15] (24 blocks). We evaluate the rep-\nresentation quality (linear probe accuracy) of the encoder,\nwhich has 12 blocks, referred to as \u201c 1\n2L\u201d (half large).\nTokenizer.\nDiT instantiated in [32] is a form of Latent\nDiffusion Models (LDM) [33], which uses a VQGAN tok-\nenizer [16]. Specifically, this VQGAN tokenizer transforms\nthe 256\u00d7256\u00d73 input image (height\u00d7width\u00d7channels)\ninto a 32\u00d732\u00d74 latent map, with a stride of 8.\nStarting baseline. By default, we train the models for 400\nepochs on ImageNet [9] with a resolution of 256\u00d7256 pix-\nels. Implementation details are in Sec. A.\nOur DiT baseline results are reported in Tab. 1 (line 1).\nWith DiT-L, we report a linear probe accuracy of 57.5% us-\ning its 1\n2L encoder. The generation quality (Fr\u00b4echet Incep-\ntion Distance [22], FID-50K) of this DiT-L model is 11.6.\nThis is the starting point of our destructive trajectory.\nDespite differences in implementation details, our start-\ning point conceptually follows recent studies [40, 28]\n(more specifically, DDAE [40]), which evaluate off-the-\nshelf DDMs under the linear probing protocol.\n4. Deconstructing Denoising Diffusion Models\nOur deconstruction trajectory is divided into three stages.\nWe first adapt the generation-focused settings in DiT to be\nmore oriented toward self-supervised learning (Sec. 4.1).\nNext, we deconstruct and simplify the tokenizer step by step\n(Sec. 4.2). Finally, we attempt to reverse as many DDM-\nmotivated designs as possible, pushing the models towards\na classical DAE [39] (Sec. 4.3). We summarize our learn-\nings from this deconstructing process in Sec. 4.4.\n4.1. Reorienting DDM for Self-supervised Learning\nWhile a DDM is conceptually a form of a DAE, it was orig-\ninally developed for the purpose of image generation. Many\ndesigns in a DDM are oriented toward the generation task.\nSome designs are not legitimate for self-supervised learn-\ning (e.g., class labels are involved); some others are not\nnecessary if visual quality is not concerned. In this sub-\nsection, we reorient our DDM baseline for the purpose of\nself-supervised learning, summarized in Tab. 1.\nRemove class-conditioning. A high-quality DDM is of-\nten trained with conditioning on class labels, which can\nlargely improve the generation quality. But the usage of\nclass labels is simply not legitimate in the context of our\nself-supervised learning study. As the first step, we remove\nclass-conditioning in our baseline.\nSurprisingly, removing class-conditioning substantially\nimproves the linear probe accuracy from 57.5% to 62.1%\n(Tab. 1), even though the generation quality is greatly hurt\nas expected (FID from 11.6 to 34.2). We hypothesize that\ndirectly conditioning the model on class labels may reduce\nthe model\u2019s demands on encoding the information related\nto class labels. Removing the class-conditioning can force\nthe model to learn more semantics.\n3\nacc. (\u2191)\nFID (\u2193)\nDiT baseline\n57.5\n11.6\n+ remove class-conditioning\n62.5\n30.9\n+ remove VQGAN perceptual loss\n58.4\n54.3\n+ remove VQGAN adversarial loss\n59.0\n75.6\n+ replace noise schedule\n63.4\n93.2\nTable 1. Reorienting DDM for self-supervised learning. We\nbegin with the DiT [32] baseline and evaluate its linear probe ac-\ncuracy (acc.) on ImageNet. Each line is based on a modification\nof the immediately preceding line. The entries in gray, in which\nclass labels are used, are not legitimate results for self-supervised\nlearning. See Sec. 4.1 for description.\nDeconstruct VQGAN. In our baseline, the VQGAN to-\nkenizer, presented by LDM [33] and inherited by DiT,\nis trained with multiple loss terms: (i) autoencoding re-\nconstruction loss; (ii) KL-divergence regularization loss\n[33];2 (iii) perceptual loss [44] based on a supervised VGG\nnet [35] trained for ImageNet classification; and (iv) adver-\nsarial loss [18, 16] with a discriminator. We ablate the latter\ntwo terms in Tab. 1.\nAs the perceptual loss [44] involves a supervised pre-\ntrained network, using the VQGAN trained with this loss is\nnot legitimate. Instead, we train another VQGAN tokenizer\n[33] in which we remove the perceptual loss. Using this to-\nkenizer reduces the linear probe accuracy significantly from\n62.5% to 58.4% (Tab. 1), which, however, provides the first\nlegitimate entry thus far. This comparison reveals that a to-\nkenizer trained with the perceptual loss (with class labels)\nin itself provides semantic representations. We note that the\nperceptual loss is not used from now on, in the remaining\npart of this paper.\nWe train the next VQGAN tokenizer that further removes\nthe adversarial loss. It slightly increases the linear probe\naccuracy from 58.4% to 59.0% (Tab. 1). With this, our to-\nkenizer at this point is essentially a VAE, which we move\non to deconstruct in the next subsection. We also note that\nremoving either loss harms generation quality.\nReplace noise schedule. In the task of generation, the goal\nis to progressively turn a noise map into an image. As a\nresult, the original noise schedule spends many time steps\non very noisy images (Fig. 3). This is not necessary if our\nmodel is not generation-oriented.\nWe study a simpler noise schedule for the purpose of\nself-supervised learning. Specifically, we let \u03b32\nt linearly de-\ncay in the range of 1>\u03b32\nt \u22650 (Fig. 3). This allows the model\nto spend more capacity on cleaner images. This change\ngreatly improves the linear probe accuracy from 59.0% to\n63.4% (Tab. 1), suggesting that the original schedule fo-\ncuses too much on noisier regimes. On the other hand, as\nexpected, doing so further hurts the generation ability, lead-\ning to a FID of 93.2.\n2The KL form in [33] does not perform explicit vector quantization\n(VQ), interpreted as \u201cthe quantization layer absorbed by the decoder\u201d [33].\n0\n200\n400\n600\n800\n1000\ntime step\n0.0\n0.5\n1.0\n2   \noriginal\nlinear\nFigure 3. Noise schedules. The original schedule [23, 32], which\nsets \u03b32\nt =\u03a0t\ns=1(1 \u2212 \u03b2s) with a linear schedule of \u03b2, spends many\ntime steps on very noisy images (small \u03b3). Instead, we use a sim-\nple schedule that is linear on \u03b32, which provides less noisy images.\nSummary. Overall, the results in Tab. 1 reveal that self-\nsupervised learning performance is not correlated to gen-\neration quality. The representation capability of a DDM is\nnot necessarily the outcome of its generation capability.\n4.2. Deconstructing the Tokenizer\nNext, we further deconstruct the VAE tokenizer by making\nsubstantial simplifications. We compare the following four\nvariants of autoencoders as the tokenizers, each of which is\na simplified version of the preceding one:\n\u2022 Convolutional VAE. Our deconstruction thus far leads us\nto a VAE tokenizer. As common practice [26, 33], the en-\ncoder f(\u00b7) and decoder g(\u00b7) of this VAE are deep convo-\nlutional (conv) neural networks [27]. This convolutional\nVAE minimizes the following loss function:\n\u2225x \u2212 g(f(x))\u22252 + KL [f(x)|N] .\nHere, x is the input image of the VAE. The first term is the\nreconstruction loss, and the second term is the Kullback-\nLeibler divergence [3, 16] between the latent distribution\nof f(x) and a unit Gaussian distribution.\n\u2022 Patch-wise VAE. Next we consider a simplified case in\nwhich the VAE encoder and decoder are both linear pro-\njections, and the VAE input x is a patch. The training\nprocess of this patch-wise VAE minimizes this loss:\n\u2225x \u2212 U T V x\u22252 + KL [V x|N] .\nHere x denotes a patch flattened into a D-dimensional\nvector. Both U and V are d\u00d7D matrixes, where d is the\ndimension of the latent space. We set the patch size as\n16\u00d716 pixels, following [15].\n\u2022 Patch-wise AE. We make further simplification on VAE\nby removing the regularization term:\n\u2225x \u2212 U T V x\u22252.\nAs such, this tokenizer is essentially an autoencoder (AE)\non patches, with the encoder and decoder both being lin-\near projections.\n4\n(a) patch-wise VAE\n(b) patch-wise AE\n(c) patch-wise PCA\nFigure 4. Visualization of the patch-wise tokenizer. Each filter\ncorresponds to a row of the linear projection matrix V (d\u00d7D),\nreshaped to 16\u00d716\u00d73 for visualization. Here d=16.\n\u2022 Patch-wise PCA. Finally, we consider a simpler variant\nwhich performs Principal Component Analysis (PCA) on\nthe patch space. It is easy to show that PCA is equivalent\nto a special case of AE:\n\u2225x \u2212 V T V x\u22252.\nin which V satisfies V V T =I (d\u00d7d identity matrix).\nThe PCA bases can be simply computed by eigen-\ndecomposition on a large set of randomly sampled\npatches, requiring no gradient-based training.\nThanks to the simplicity of using patches, for the three\npatch-wise tokenizers, we can visualize their filters in the\npatch space (Fig. 4).\nTab. 2 summarizes the linear probe accuracy of DiT us-\ning these four variants of tokenizers. We show the results\nw.r.t. the latent dimension \u201cper token\u201d.3 We draw the fol-\nlowing observations.\nLatent dimension of the tokenizer is crucial for DDM to\nwork well in self-supervised learning.\nAs shown in Tab. 2, all four variants of tokenizers exhibit\nsimilar trends, despite their differences in architectures and\nloss functions. Interestingly, the optimal dimension is rel-\natively low (d is 16 or 32), even though the full dimension\nper patch is much higher (16\u00d716\u00d73=768).\nSurprisingly, the convolutional VAE tokenizer is neither\nnecessary nor favorable; instead, all patch-based tokeniz-\ners, in which each patch is encoded independently, perform\nsimilarly with each other and consistently outperform the\nConv VAE variant. In addition, the KL regularization term\nis unnecessary, as both the AE and PCA variants work well.\nTo our further surprise, even the PCA tokenizer works\nwell. Unlike the VAE or AE counterparts, the PCA tok-\nenizer does not require gradient-based training. With pre-\ncomputed PCA bases, the application of the PCA tokenizer\n3For patch-wise VAE/AE/PCA (patch stride is 16), we treat each patch\nas a token, so the latent dimension is simply d for each patch. For the\ndefault convolutional VAE that has a stride of 8, the DiT implementation\n[32] treats each 2\u00d72 patch on the latent space as a \u201ctoken\u201d; as a result, its\nlatent dimension \u201cper token\u201d should be multiplied by 4 for calibration.\n8\n16\n32\n64\n128\nlatent dim per token (log-scale)\n45\n50\n55\n60\n65\nconv. VAE\npatch-wise VAE\npatch-wise AE\npatch-wise PCA\nlatent dim. d\n8\n16\n32\n64\n128\nconv. VAE (baseline)\n54.5\n63.4\n62.8\n57.0\n48.1\npatch-wise VAE\n58.3\n64.9\n64.8\n56.8\n-\npatch-wise AE\n59.9\n64.7\n64.6\n59.9\n-\npatch-wise PCA\n56.0\n63.4\n65.1\n60.0\n53.9\nTable 2. Linear probe accuracy vs. latent dimension. With a\nDiT model, we study four variants of tokenizers for computing the\nlatent space. We vary the dimensionality d (per token) of the latent\nspace. The table is visualized by the plot above. All four variants\nof tokenizers exhibit similar trends, despite their differences in\narchitectures and loss functions. The 63.4% entry of \u201cconv. VAE\u201d\nis the same entry as the last line in Tab. 1.\n12\n48\n192\n768\nlatent dim per token (log-scale)\n20\n30\n40\n50\n60\n56.0\n63.4\n65.1\n53.9\n44.7\n60.0\n43.7\n58.5\n53.0\n23.6\npatch-wise PCA\npixel-based\nFigure 5. Linear probe results of the pixel-based tokenizer, op-\nerated on an image size of 256, 128, 64, and 32, respectively with\na patch size of 16, 8, 4, 2. The \u201clatent\u201d dimensions of these tok-\nenized spaces are 768, 192, 48, and 12 per token. Similar to other\ntokenizers we study, this pixel-based tokenizer exhibits a similar\ntrend: a relatively small dimension of the latent space is optimal.\nis analogous to a form of image pre-processing, rather than\na \u201cnetwork architecture\u201d. The effectiveness of a PCA to-\nkenizer largely helps us push the modern DDM towards a\nclassical DAE, as we will show in the next subsection.\nHigh-resolution, pixel-based DDMs are inferior for self-\nsupervised learning.\nBefore we move on, we report an extra ablation that is con-\nsistent with the aforementioned observation. Specifically,\nwe consider a \u201cna\u00a8\u0131ve tokenizer\u201d that performs identity map-\nping on patches extracted from resized images. In this case,\na \u201ctoken\u201d is the flatten vector consisting all pixels of a patch.\nIn Fig. 5, we show the results of this \u201cpixel-based\u201d tok-\nenizer, operated on an image size of 256, 128, 64, and 32,\nrespectively with a patch size of 16, 8, 4, 2. The \u201clatent\u201d\ndimensions of these tokenized spaces are 768, 192, 48, and\n12 per token. In all case, the sequence length of the Trans-\nformer is kept unchanged (256).\n5\nacc.\npatch-wise PCA baseline\n65.1\n+ predict clean data (rather than noise)\n62.4\n+ remove input scaling (fix \u03b3t \u2261 1)\n63.6\n+ operate on image input with inv. PCA\n63.6\n+ operate on image output with inv. PCA\n63.9\n+ predict original image\n64.5\nTable 3.\nMoving toward a classical DAE, starting from our\npatch-wise PCA tokenizer. Each line is based on a modification\nof the immediately preceding line. See Sec. 4.3 for descriptions.\nInterestingly, this pixel-based tokenizer exhibits a sim-\nilar trend with other tokenizers we have studied, although\nthe optimal dimension is shifted. In particular, the optimal\ndimension is d=48, which corresponds to an image size of\n64 with a patch size of 4. With an image size of 256 and\na patch size of 16 (d=768), the linear probe accuracy drops\ndramatically to 23.6%.\nThese comparisons show that the tokenizer and the re-\nsulting latent space are crucial for DDM/DAE to work com-\npetitively in the self-supervised learning scenario. In partic-\nular, applying a classical DAE with additive Gaussian noise\non the pixel space leads to poor results.\n4.3. Toward Classical Denoising Autoencoders\nNext, we go on with our deconstruction trajectory and aim\nto get as close as possible to the classical DAE [39]. We\nattempt to remove every single aspect that still remains be-\ntween our current PCA-based DDM and the classical DAE\npractice. Via this deconstructive process, we gain better un-\nderstandings on how every modern design may influence\nthe classical DAE. Tab. 3 gives the results, discussed next.\nPredict clean data (rather than noise). While modern\nDDMs commonly predict the noise \u03f5 (see Eq. (2)), the clas-\nsical DAE predicts the clean data instead. We examine this\ndifference by minimizing the following loss function:\n\u03bbt\u2225z0 \u2212 net(zt)\u22252\n(3)\nHere z0 is the clean data (in the latent space), and net(zt)\nis the network prediction. \u03bbt is a t-dependent loss weight,\nintroduced to balance the contribution of different noise lev-\nels [34]. It is suggested to set \u03bbt = \u03b32\nt /\u03c32\nt as per [34]. We\nfind that setting \u03bbt = \u03b32\nt works better in our scenario. In-\ntuitively, it simply puts more weight to the loss terms of the\ncleaner data (larger \u03b3t).\nWith the modification of predicting clean data (rather\nthan noise), the linear probe accuracy degrades from 65.1%\nto 62.4% (Tab. 3). This suggests that the choice of the pre-\ndiction target influences the representation quality.\nEven though we suffer from a degradation in this step,\nwe will stick to this modification from now on, as our goal\nis to move towards a classical DAE.4\n4We have revisited undoing this change in our final entry, in which we\nhave not observed this degradation.\nRemove input scaling. In modern DDMs (see Eq. (1)),\nthe input is scaled by a factor of \u03b3t. This is not common\npractice in a classical DAE. Next, we study removing input\nscaling, i.e., we set \u03b3t \u2261 1. As \u03b3t is fixed, we need to define\na noise schedule directly on \u03c3t. We simply set \u03c3t as a linear\nschedule from 0 to\n\u221a\n2. Moreover, we empirically set the\nweight in Eq. (3) as \u03bbt = 1/(1 + \u03c32\nt ), which again puts\nmore emphasis on cleaner data (smaller \u03c3t).\nAfter fixing \u03b3t \u2261 1, we achieve a decent accuracy of\n63.6% (Tab. 3), which compares favorably with the varying\n\u03b3t counterpart\u2019s 62.4%. This suggests that scaling the data\nby \u03b3t is not necessary in our scenario.\nOperate on the image space with inverse PCA. Thus far,\nfor all entries we have explored (except Fig. 5), the model\noperates on the latent space produced by a tokenizer (Fig. 2\n(b)). Ideally, we hope our DAE can work directly on the im-\nage space while still having good accuracy. With the usage\nof PCA, we can achieve this goal by inverse PCA.\nThe idea is illustrated in Fig. 1. Specially, we project the\ninput image into the latent space by the PCA bases (i.e., V ),\nadd noise in the latent space, and project the noisy latent\nback to the image space by the inverse PCA bases (V T ).\nFig. 1 (middle, bottom) shows an example image with noise\nadded in the latent space. With this noisy image as the input\nto the network, we can apply a standard ViT network [15]\nthat directly operate on images, as if there is no tokenizer.\nApplying this modification on the input side (while still\npredicting the output on the latent space) has 63.6% accu-\nracy (Tab. 3). Further applying it to the output side (i.e.,\npredicting the output on the image space with inverse PCA)\nhas 63.9% accuracy. Both results show that operating on the\nimage space with inverse PCA can achieve similar results as\noperating on the latent space.\nPredict original image. While inverse PCA can produce\na prediction target in the image space, this target is not the\noriginal image. This is because PCA is a lossy encoder for\nany reduced dimension d. In contrast, it is a more natural\nsolution to predict the original image directly.\nWhen we let the network predict the original image, the\n\u201cnoise\u201d introduced includes two parts: (i) the additive Gaus-\nsian noise, whose intrinsic dimension is d, and (ii) the PCA\nreconstruction error, whose intrinsic dimension is D \u2212 d (D\nis 768). We weight the loss of both parts differently.\nFormally, with the clean original image x0 and network\nprediction net(xt), we can compute the residue r projected\nonto the full PCA space: r \u225c V (x0 \u2212 net(xt)). Here V is\nthe D-by-D matrix representing the full PCA bases. Then\nwe minimize the following loss function:\n\u03bbt\nD\nX\ni=1\nwir2\ni .\n(4)\nHere i denotes the i-th dimension of the vector r. The per-\n6\n56\n58\n60\n62\n64\n66\nlinear probing accuracy\nDiT baseline\nremove cls-cond\nremove VQGAN perc. loss\nremove VQGAN adv. loss\nreplace noise sched.\nconv. VAE to patch-wise VAE\nto patch-wise AE\nto patch-wise PCA\npredict clean data\nremove input scaling\nuse image input\nuse image output\npredict original image\n57.5\n62.5\n58.4\n59.0\n63.4\n64.9\n64.7\n65.1\n62.4\n63.6\n63.6\n63.9\n64.5\nFigure 6. The overall deconstructive trajectory from a modern\nDDM to l-DAE, summarizing Tab. 1, Tab. 2, and Tab. 3. Each line\nis based on a modification of the immediately preceding line.\ndimension weight wi is 1 for i \u2264 d, and 0.1 for d < i \u2264 D.\nIntuitively, wi down-weights the loss of the PCA recon-\nstruction error. With this formulation, predicting the origi-\nnal image achieves 64.5% linear probe accuracy (Tab. 3).\nThis variant is conceptually very simple: its input is a\nnoisy image whose noise is added in the PCA latent space,\nits prediction is the original clean image (Fig. 1).\nSingle noise level. Lastly, out of curiosity, we further study\na variant with single-level noise. We note that multi-level\nnoise, given by noise scheduling, is a property motived by\nthe diffusion process in DDMs; it is conceptually unneces-\nsary in a classical DAE.\nWe fix the noise level \u03c3 as a constant (\np\n1/3). Using this\nsingle-level noise achieves decent accuracy of 61.5%, a 3%\ndegradation vs. the multi-level noise counterpart (64.5%).\nUsing multiple levels of noise is analogous to a form of data\naugmentation in DAE: it is beneficial, but not an enabling\nfactor. This also implies that the representation capability\nof DDM is mainly gained by the denoising-driven process,\nnot a diffusion-driven process.\nAs multi-level noise is useful and conceptually simple,\nwe keep it in our final entries presented in the next section.\n4.4. Summary\nIn sum, we deconstruct a modern DDM and push it towards\na classical DAE (Fig. 6). We undo many of the modern\ndesigns and conceptually retain only two designs inherited\nfrom modern DDMs: (i) a low-dimensional latent space in\nwhich noise is added; and (ii) multi-level noise.\nWe use the entry at the end of Tab. 3 as our final DAE\ninstantiation (illustrated in Fig. 1). We refer to this method\nas \u201clatent Denoising Autoencoder\u201d, or in short, l-DAE.\nFigure 7. Visualization: pixel noise vs. latent noise. Left: clean\nimage, 256\u00d7256 pixels. Middle: Gaussian noise added to the\npixel space. Right: Gaussian noise added to the latent space pro-\nduced by the PCA tokenizer, visualized by back projection to the\nimage space using inverse PCA. \u03c3=\np\n1/3 in both cases.\n5. Analysis and Comparisons\nVisualizing latent noise. Conceptually, l-DAE is a form of\nDAE that learns to remove noise added to the latent space.\nThanks to the simplicity of PCA, we can easily visualize the\nlatent noise by inverse PCA.\nFig. 7 compares the noise added to pixels vs. to the latent.\nUnlike the pixel noise, the latent noise is largely indepen-\ndent of the resolution of the image. With patch-wise PCA\nas the tokenizer, the pattern of the latent noise is mainly de-\ntermined by the patch size. Intuitively, we may think of it as\nusing patches, rather than pixels, to resolve the image. This\nbehavior resembles MAE [21], which masks out patches in-\nstead of individual pixels.\nDenoising results. Fig. 8 shows more examples of denois-\ning results based on l-DAE. Our method produces reason-\nable predictions despite of the heavy noise. We note that\nthis is less of a surprise, because neural network-based im-\nage restoration [4, 14] has been an intensively studied field.\nNevertheless, the visualization may help us better un-\nderstand how l-DAE may learn good representations. The\nheavy noise added to the latent space creates a challenging\npretext task for the model to solve. It is nontrivial (even\nfor human beings) to predict the content based on one or\na few noisy patches locally; the model is forced to learn\nhigher-level, more holistic semantics to make sense of the\nunderlying objects and scenes.\nData augmentation. Notably, all models we present thus\nfar have no data augmentation: only the center crops of im-\nages are used, with no random resizing or color jittering,\nfollowing [11, 32]. We further explore a mild data augmen-\ntation (random resized crop) for our final l-DAE:\naug.\ncenter crop\nrandom crop\nacc.\n64.5\n65.0\nwhich has slight improvement. This suggests that the rep-\nresentation learning ability of l-DAE is largely independent\nof its reliance on data augmentation. A similar behavior\nwas observed in MAE [21], which sharply differs from the\nbehavior of contrastive learning methods (e.g., [6]).\n7\nFigure 8. Denoising results of l-DAE, evaluated on ImageNet validation images. This denoising problem, serving as a pretext task,\nencourages the network to learn meaningful representations in a self-supervised manner. For each case, we show: (left) clean image;\n(middle) noisy image that is the input to the network, where the noise is added to the latent space; (right) denoised output.\nTraining epochs. All our experiments thus far are based\non 400-epoch training. Following MAE [21], we also study\ntraining for 800 and 1600 epochs:\nepochs\n400\n800\n1600\nacc.\n65.0\n67.5\n69.6\nAs a reference, MAE [21] has a significant gain (4%) ex-\ntending from 400 to 800 epochs, and MoCo v3 [7] has\nnearly no gain (0.2%) extending from 300 to 600 epochs.\nModel size. Thus far, our all models are based on the DiT-L\nvariant [32], whose encoder and decoder are both \u201cViT- 1\n2L\u201d\n(half depth of ViT-L). We further train models of different\nsizes, whose encoder is ViT-B or ViT-L (decoder is always\nof the same size as encoder):\nencoder\nViT-B\nViT- 1\n2 L\nViT-L\nacc.\n60.3\n65.0\n70.9\nWe observe a good scaling behavior w.r.t. model size: scal-\ning from ViT-B to ViT-L has a large gain of 10.6%. A sim-\nilar scaling behavior is also observed in MAE [21], which\nhas a 7.8% gain from ViT-B to ViT-L.\nComparison with previous baselines.\nFinally, to have\na better sense of how different families of self-supervised\nlearning methods perform, we compare with previous base-\nlines in Tab. 4. We consider MoCo v3 [7], which belongs to\nthe family of contrastive learning methods, and MAE [21],\nwhich belongs to the family of masking-based methods.\nInterestingly, l-DAE performs decently in comparison\nwith MAE, showing a degradation of 1.4% (ViT-B) or 0.8%\n(ViT-L). We note that here the training settings are made as\nmethod\nViT-B (86M)\nViT-L (304M)\nMoCo v3\n76.7\n77.6\nMAE\n68.0\n75.8\nl-DAE\n66.6\n75.0\nTable 4. Comparisons with previous baselines of MoCo v3 [7]\nand MAE [21]. The entries of both MAE and l-DAE are trained\nfor 1600 epochs and with random crop as the data augmentation.\nLinear probe accuracy on ImageNet is reported. In the brackets\nare the number of parameters of the encoder.\nfair as possible between MAE and l-DAE: both are trained\nfor 1600 epochs and with random crop as the data augmen-\ntation. On the other hand, we should also note that MAE\nis more efficient in training because it only operates on un-\nmasked patches. Nevertheless, we have largely closed the\naccuracy gap between MAE and a DAE-driven method.\nLast, we observe that autoencoder-based methods (MAE\nand l-DAE) still fall short in comparison with contrastive\nlearning methods under this protocol, especially when the\nmodel is small. We hope our study will draw more atten-\ntion to the research on autoencoder-based methods for self-\nsupervised learning.\n6. Conclusion\nWe have reported that l-DAE, which largely resembles the\nclassical DAE, can perform competitively in self-supervised\nlearning. The critical component is a low-dimensional la-\ntent space on which noise is added. We hope our discovery\nwill reignite interest in denoising-based methods in the con-\ntext of today\u2019s self-supervised learning research.\n8\nAcknowledgement. We thank Pascal Vincent, Mike Rab-\nbat, and Ross Girshick for their discussion and feedback.\nA. Implementation Details\nDiT architecture.\nWe follow the DiT architecture de-\nsign [32]. The DiT architecture is similar to the original\nViT [15], with extra modifications made for conditioning.\nEach Transformer block accepts an embedding network (a\ntwo-layer MLP) conditioned on the time step t. The output\nof this embedding network determines the scale and bias\nparameters of LayerNorm [1], referred to as adaLN [32].\nSlightly different from [32], we set the hidden dimension\nof this MLP as 1/4 of its original dimension, which helps\nreduce model sizes and save memory, at no accuracy cost.\nTraining. The original DiTs [32] are trained with a batch\nsize of 256. To speed up our exploration, we increase the\nbatch size to 2048. We perform linear learning rate warm up\n[19] for 100 epochs and then decay it following a half-cycle\ncosine schedule. We use a base learning rate blr = 1e-4 [32]\nby default, and set the actual lr following the linear scaling\nrule [19]: blr \u00d7 batch size / 256. No weight decay is used\n[32]. We train for 400 epochs by default. On a 256-core\nTPU-v3 pod, training DiT-L takes 12 hours.\nLinear probing. Our linear probing implementation fol-\nlows the practice of MAE [21]. We use clean, 256\u00d7256-\nsized images for linear probing training and evaluation.\nThe ViT output feature map is globally pooled by average\npooling. It is then processed by a parameter-free Batch-\nNorm [25] layer and a linear classifier layer, following [21].\nThe training batch size is 16384, learning rate is 6.4\u00d710\u22123\n(cosine decay schedule), weight decay is 0, and training\nlength is 90 epochs. Randomly resized crop and flipping\nare used during training and a single center crop is used for\ntesting. Top-1 accuracy is reported.\nWhile the model is conditioned on t in self-supervised\npre-training, conditioning is not needed in transfer learning\n(e.g., linear probing). We fix the time step t value in our\nlinear probing training and evaluation. The influence of dif-\nferent t values (out of 1000 time steps) is shown as follows:\nfixed t\n0\n10\n20\n40\n80\nw/ clean input\n64.1\n64.5\n64.1\n63.3\n62.2\nw/ noisy input\n64.2\n65.0\n65.0\n65.0\n64.5\nWe note that the t value determines: (i) the model weights,\nwhich are conditioned on t, and (ii) the noise added in trans-\nfer learning, using the same level of t. Both are shown in\nthis table. We use t = 10 and clean input in all our experi-\nments, except Tab. 4 where we use the optimal setting.\nFixing t also means that the t-dependent MLP layers,\nwhich are used for conditioning, are not exposed in transfer\nlearning, because they can be merged given the fixed t. As\nsuch, our model has the number of parameters just similar\nto the standard ViT [15], as reported in Tab. 4.\nThe DiT-L [32] has 24 blocks where the first 12 blocks\nare referred to as the \u201cencoder\u201d (hence ViT- 1\n2L) and the oth-\ners the \u201cdecoder\u201d. This separation of the encoder and de-\ncoder is artificial. In the following table, we show the linear\nprobing results using different numbers of blocks in the en-\ncoder, using the same pre-trained model:\nenc. blocks\n9\n10\n11\n12\n13\n14\n15\nacc.\n58.5\n62.0\n64.1\n64.5\n63.6\n61.9\n59.7\nThe optimal accuracy is achieved when the encoder and de-\ncoder have the same depth. This behavior is different from\nMAE\u2019s [21], whose encoder and decoder are asymmetric.\nB. Fine-tuning Results\nIn addition to linear probing, we also report end-to-end fine-\ntuning results. We closely followed MAE\u2019s protocol [21].\nWe use clean, 256\u00d7256-sized images as the inputs to the\nencoder. Globally average pooled outputs are used as fea-\ntures for classification.\nThe training batch size is 1024,\ninitial learning rate is 4\u00d710\u22123, weight decay is 0.05, drop\npath [24] is 0.1, and training length is 100 epochs. We use\na layer-wise learning rate decay of 0.85 (B) or 0.65 (L).\nMixUp [43] (0.8), CutMix [42] (1.0), RandAug [8] (9, 0.5),\nand exponential moving average (0.9999) are used, similar\nto [21]. The results are summarized as below:\nmethod\nViT-B\nViT-L\nMoCo v3\n83.2\n84.1\nMAE\n83.6\n85.9\nl-DAE\n83.7\n84.7\nOverall, both autoencoder-based methods are better than\nMoCo v3. l-DAE performs similarly with MAE with ViT-B,\nbut still fall short of MAE with ViT-L.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv:1607.06450, 2016.\n[2] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and\nColoma Ballester. Image inpainting. In SIGGRAPH, 2000.\n[3] Christopher M Bishop and Nasser M Nasrabadi.\nPattern\nrecognition and machine learning. Springer, 2006.\n[4] Harold Christopher Burger, Christian J Schuler, and Stefan\nHarmeling.\nImage denoising: Can plain neural networks\ncompete with BM3D? In CVPR, 2012.\n[5] Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li,\nand Dit-Yan Yeung. Mixed autoencoder for self-supervised\nvisual representation learning. In CVPR, 2023.\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020.\n[7] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\nstudy of training self-supervised Vision Transformers.\nIn\nICCV, 2021.\n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In CVPR Workshops, 2020.\n9\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019.\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. In NeurIPS, 2021.\n[12] Jeff Donahue and Karen Simonyan. Large scale adversarial\nrepresentation learning. arXiv:1907.02544, 2019.\n[13] Jeff Donahue, Philipp Kr\u00a8ahenb\u00a8uhl, and Trevor Darrell. Ad-\nversarial feature learning. In ICLR, 2017.\n[14] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang.\nLearning a deep convolutional network for image\nsuper-resolution. In ECCV, 2014.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021.\n[16] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021.\n[17] Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, and\nFuru Wei. Corrupted image modeling for self-supervised vi-\nsual pre-training. arXiv:2202.03382, 2022.\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014.\n[19] Priya Goyal, Piotr Doll\u00b4ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large minibatch\nSGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017.\n[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020.\n[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, 2022.\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local Nash equi-\nlibrium. In NeurIPS, 2017.\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020.\n[24] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In ECCV,\n2016.\n[25] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, 2015.\n[26] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In ICLR, 2013.\n[27] Yann LeCun, Bernhard Boser, John S Denker, Donnie\nHenderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwrit-\nten zip code recognition. Neural computation, 1989.\n[28] Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agar-\nwal,\nNamitha\nPadmanabhan,\nArchana\nSwaminathan,\nSrinidhi Hegde, Tianyi Zhou, and Abhinav Shrivastava.\nDiffusion models beat GANs on image classification.\narXiv:2307.08702, 2023.\n[29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In ICML, 2021.\n[30] Aaron\nvan\nden\nOord,\nOriol\nVinyals,\nand\nKoray\nKavukcuoglu.\nNeural discrete representation learning.\nIn NeurIPS, 2017.\n[31] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros.\nContext encoders: Feature\nlearning by inpainting. In CVPR, 2016.\n[32] William Peebles and Saining Xie. Scalable diffusion models\nwith Transformers. In ICCV, 2023.\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022.\n[34] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2022.\n[35] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In ICLR,\n2015.\n[36] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015.\n[37] Yang Song and Stefano Ermon. Generative modeling by es-\ntimating gradients of the data distribution. NeurIPS, 2019.\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv:2011.13456, 2020.\n[39] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In ICML, 2008.\n[40] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang.\nDenoising diffusion autoencoders are unified self-supervised\nlearners. In ICCV, 2023.\n[41] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple\nframework for masked image modeling. In CVPR, 2022.\n[42] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In ICCV, 2019.\n[43] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In ICLR, 2018.\n[44] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018.\n10\n"
  },
  {
    "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design",
    "link": "https://arxiv.org/pdf/2401.14112.pdf",
    "upvote": "14",
    "text": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric\nAlgorithm-System Co-Design\nHaojun Xia\nUniversity of Sydney\nZhen Zheng\nMicrosoft\nXiaoxia Wu\nMicrosoft\nShiyang Chen\nRutgers University\nZhewei Yao\nMicrosoft\nStephen Youn\nMicrosoft\nArash Bakhtiari\nMicrosoft\nMichael Wyatt\nMicrosoft\nDonglin Zhuang\nUniversity of Sydney\nZhongzhu Zhou\nUniversity of Sydney\nOlatunji Ruwase\nMicrosoft\nYuxiong He\nMicrosoft\nShuaiwen Leon Song\nMicrosoft\nAbstract\nSix-bit quantization (FP6) can effectively reduce the size of\nlarge language models (LLMs) and preserve the model quality\nconsistently across varied applications. However, existing sys-\ntems do not provide Tensor Core support for FP6 quantization\nand struggle to achieve practical performance improvements\nduring LLM inference. It is challenging to support FP6 quanti-\nzation on GPUs due to (1) unfriendly memory access of model\nweights with irregular bit-width and (2) high runtime overhead\nof weight de-quantization. To address these problems, we pro-\npose TC-FPx, the first full-stack GPU kernel design scheme\nwith unified Tensor Core support of float-point weights for var-\nious quantization bit-width. We integrate TC-FPx kernel into\nan existing inference system, providing new end-to-end sup-\nport (called FP6-LLM) for quantized LLM inference, where\nbetter trade-offs between inference cost and model quality\nare achieved. Experiments show that FP6-LLM enables the\ninference of LLaMA-70b using only a single GPU, achieving\n1.69\u00d7-2.65\u00d7 higher normalized inference throughput than\nthe FP16 baseline. The source code is publicly available at\nhttps://github.com/usyd-fsalab/fp6_llm.\n1\nIntroduction\nLarge Language Models (LLMs) [1, 28, 32\u201334, 41] are\nrenowned for their capacity to process diverse language-\nrelated tasks [2, 9, 10, 27]. However, it is challenging to\ndeploy LLMs as these models are also characterized by\ntheir expansive size, e.g., 175 billion parameter GPT-3 [1]\nand 1.76 trillion parameter GPT-4 [28]. On one hand, it re-\nquires large amounts of GPU memory (326 GB for GPT-3\nin FP16) only to accommodate model weights, whereas an\nA100/H100 GPU [20,21] only has up to 80 GB memory. On\nthe other hand, LLM inference faces severe \"memory wall\"\nissues [12,36] during token generation, where the speed of\nLLM inference is mainly limited by the time reading model\nweights from GPU DRAM. It makes LLM inference memory\nbounded, under-utilizing the computational power of GPUs.\n0\n0.5\n1\n1.5\n2\n2.5\n1\n2\n4\n8\n16\n32\n64\n128\n256\n512\nKernel Speedup\nInference Batch Size\nTC-FPx_W6A16\nTensorRT_LLM_W8A16\nAWQ_W4A16_SIMT\ncuBLAS_W16A16\nFP16\nTensor Cores\nFP32 SIMT Cores\nFigure 1: Performance of a linear layer within the llama-\n65b [32] model. The shapes of the weight/activation matrices\nare (8192, 22016) and (22016, Batch Size).\nModel quantization [4, 7, 14, 30, 38, 42, 44] reduces both\nGPU memory footprint and DRAM data access. It uses fewer\nbits to represent each model weight, resulting in a more com-\npact representation of the model. However, only a small set of\nbit-widths (i.e. 4-bit and 8-bit) are efficiently supported in ex-\nisting systems [3,14,15,26] on modern GPUs. Recent studies\nshow that 6-bit quantization is a good trade-off between in-\nference cost and model quality for LLM deployment [30,35].\nHowever, there is still no efficient system support for the 6-bit\nlinear layer execution (i.e., matrix multiplication) on modern\nGPUs. It is urgent to develop the system support for 6-bit\nquantization fully leveraging the computing power of GPUs.\nOn one hand, more efficient LLM inference can be achieved\nwith 6-bit quantization compared to larger-bit quantization\n(e.g., 8-bit). Firstly, more GPU memory can be saved, e.g.\naround 40 GB memory can be saved if deploying the GPT-3\nmodel with 6-bit rather than 8-bit quantization. Secondly,\nLLM inference can be further accelerated as the time of\nreading model weights from GPU DRAM can be effec-\ntively reduced. As shown in Figure 1, the linear layer im-\nplemented with our newly proposed 6-bit quantization system\ndesign (TC-FPx_W6A16) is constantly faster (up to 1.45\u00d7)\n1\narXiv:2401.14112v2  [cs.LG]  4 Mar 2024\nthan the state-of-the-art support for 8-bit quantization (Ten-\nsorRT_LLM_W8A16). On the other hand, 6-bit quantiza-\ntion can more effectively preserve model quality compared\nto smaller-bit quantization (e.g., 4-bit). Despite the potential\nshown by recent 4-bit techniques [6,39] in compressing LLMs\nwith minimal quality loss, they are predominantly geared\ntowards zero-shot evaluations. Recent research [35] demon-\nstrates that in tasks extending beyond zero-shot measurements,\nsuch as code generation and summarization, 4-bit methods\nunderperform and lack robustness, whereas 6-bit quantiza-\ntion displays strong and consistent performance across these\nvaried applications.\nMotivated by the above observations, we propose TC-FPx,\nthe first full-stack GPU system design scheme with unified\nTensor Core [20,21] support of float-point weights for various\nquantization bit-width (6-bit, 5-bit, 3-bit, etc.), mitigating the\n\"memory wall\" issues during LLM inference. TC-FPx breaks\nthe limitations of the underlying GPU hardware, allowing\nthe GPU to support linear layer calculations involving model\nweights of arbitrary bit width. In TC-FPx, Tensor Cores are\nutilized for intensive computation of matrix multiplications,\nwhile SIMT cores are effectively leveraged for weight de-\nquantization, transforming the x-bit model weights to FP16\ntype during runtime before feeding them to Tensor Cores. We\npropose Ahead-of-time Bit-level Pre-packing (Section 5.2)\nto resolve the challenge of unfriendly memory access for\nweights with irregular bit-width (Section 4.2.1), enabling opti-\nmal GPU memory access. Besides, we propose SIMT-Efficient\nGPU Runtime (Section 5.3) to minimize the runtime overhead\nof weight de-quantization (Section 4.2.2). Last but not least,\nwe present the software pipeline of TC-FPx kernel, where\nSIMT cores, Tensor Cores, and the GPU memory hierarchy\ncooperate efficiently with high performance.\nWe integrate TC-FPx kernel into a state-of-the-art inference\nsystem [19], providing new end-to-end support (called FP6-\nLLM) for quantized LLM inference, where better trade-offs\nbetween inference cost and model quality are achieved. Cur-\nrently, FP6-LLM mainly supports 6-bit quantization (FP6) for\npopular LLMs such as LLaMA [32], OPT [41] with various\nsizes. Evaluations show that FP6-LLM enables the inference\nof LLaMA-70b using only a single GPU, achieving 1.69\u00d7-\n2.65\u00d7 higher normalized inference throughput than the FP16\nbaseline. Besides, FP6-LLM improves the inference through-\nput of OPT-30b by 1.72\u00d7-4.05\u00d7.\nIn summary, we make the following contributions:\n\u2022 We identify the significance and key challenges in support-\ning FP6 quantization on modern GPUs.\n\u2022 We propose TC-FPx, the first full-stack GPU kernel de-\nsign scheme with unified Tensor Core support of float-point\nweights with various bit-width, e.g. FP6.\n\u2022 We provide new end-to-end inference support for quantized\nLLMs through the integration of TC-FPx, achieving better\ntrade-offs between inference cost and model quality.\n\u2022 We evaluate FP6-LLM on various LLM models and demon-\nstrate that it substantially outperforms the baseline.\n2\nBackground\n2.1\nQuantization of Large Language Models\nAlthough large language models (LLMs) are known for\ntheir impressive performance, their large size also creates\nchallenges for model deployment. Thus, model quantiza-\ntion [4,7,14,30,38,42,44] is commonly used for LLM deploy-\nment, resulting in a more compact representation of the model.\nWeight-only quantization [7,14] only reduces the precision of\nmodel weights (e.g., INT8, using an 8-bit integer to represent\neach weight) while still using an FP16 value to represent each\nactivation. The major targets to be quantized are the weights\nof linear layers (i.e., matrix multiplication), which account\nfor more than 99% of the overall LLM weights. The activa-\ntions can also be quantized during inference [4,38]. In this\npaper, we describe the precision of Weights and Activations\nwith the term \"WxAy\", where x/y denotes the bit-width of\nweights/activations. Besides, the process of \"dequantization\"\nrefers to transforming the quantized weights back to FP16.\n2.2\nIEEE Standard for Floating-Point\nThe IEEE 754 float-point standard defines a binary format\nfor representing real numbers. Each floating point number\nconsists of three parts: the sign bit (S), the exponent bits (E),\nand the mantissa bits (M). The corresponding value f of a\nfloat-point number can be calculated via:\nf = (\u22121)S \u00d7(1.M)\u00d72E\u2212bias;\nbias = 2len(E)\u22121 \u22121\n(1)\nPlease refer to [11] for details, where special cases for values\nlike infinity, zero, and NaN (Not a Number) are also defined.\n2.3\nTensor Cores vs. SIMT Cores\nSIMT cores 1 are responsible for general-purpose process-\ning tasks in GPUs, which handle a wide range of instruc-\ntions including integer operations, floating-point operations,\nload/store operations, etc. SIMT cores execute scalar (or vec-\ntor) instructions operating on individual (or vector) data el-\nements. Tensor cores [20,21] are specialized hardware de-\nsigned for accelerating matrix multiplication. Tensor cores\nhave 16.0\u00d7/14.8\u00d7 higher FLOPS than SIMT cores on A100\n[20]/H100 [21] GPUs. Besides, Tensor cores work at a coarse-\ngrained granularity, e.g. performing a matrix multiplication\nbetween two FP16 matrices of shape 16 \u00d7 16 and 16 \u00d7 8 with\na single mma (matrix multiply and accumulate) instruction.\n1Or referred to as CUDA cores on NVIDIA GPUs.\n2\n3\nMotivations\n8-bit [4, 38] and 4-bit quantization [7, 14, 42] are the most\nwidely applied schemes for the current post-training LLMs.\nHowever, recent algorithmic research [30, 35] has demon-\nstrated that superior trade-offs between inference cost and\nmodel quality can be achieved with FP6 quantization, com-\npared to 8-bit and 4-bit quantization.\n(I) Lower inference cost than 8-bit quantization. Com-\npared to the 8-bit quantization, the cost of deploying LLMs\ncan be further reduced through more aggressive 6-bit quanti-\nzation without a visible accuracy drop. On one hand, the size\nof LLM weights can be significantly reduced, nearly 2.7\u00d7\nsmaller than the FP16 baseline. Less GPU memory is required\nto store model weights, thereby requiring fewer GPUs and\nreducing the serving cost of deploying LLMs. On the other\nhand, 6-bit quantization can also more effectively accelerate\nthe inference of LLMs. Given that the LLM inference is usu-\nally memory-bounded2 during token generation, faster LLM\ninference can be achieved through reducing GPU DRAM\naccess of the model weights. As shown in Figure 1, the ex-\necution of the linear layer within llama-65b model [32] is\nconsistently faster (up to 1.42\u00d7 faster) with our newly pro-\nposed 6-bit quantization system design (TC-FPx_W6A16)\ncompared to the state-of-the-art 8-bit quantization support\n(TensorRT-LLM_W8A16 [26]). Given that linear layers are\nthe most time-consuming part of the large language models,\nthis speedup will directly translate to performance improve-\nments for end-to-end inference scenarios (See Section 7.3).\n(II) Better model quality than 4-bit quantization. Although\n4-bit quantization more aggressively reduces memory foot-\nprint and DRAM access, it unavoidably causes degradation in\nmodel quality. In contrast, near-lossless model compression\ncan be achieved with 6-bit quantization. As shown in Table 1\nand Table 2, FP6 displays strong and consistent performance\nacross various tasks including code generation and zero-shot\nperplexity performance. It also shows high robustness across\nvarious model sizes, e.g., 1B, 13B, and 65B LLaMA [32]\nmodels. We also find that INT4 quantization heavily relies on\nFine-Grained Quantization (FGQ) methods to maintain high\nmodel quality, whereas our FP6 quantization already works\nwell on coarse-grained quantization. Note that the data points\nin Table 1 and Table 2 are picked from [35]. For more details,\nplease refer to this paper. In conclusion, at algorithmic level,\nFP6 quantization is a practical alternative to further democra-\ntize the deployment of LLMs without significantly sacrificing\nmodel quality on complex tasks and various model sizes.\n2When the execution is memory-bounded, it means that the rate at which\ndata is transferred to or from the GPU\u2019s memory is the bottleneck, rather\nthan the computational capabilities of the GPU cores.\nTable 1: Zero-shot evaluations, averaging over five datasets\nincluding PTB [17], Wikitext [18], and C4 [29]. Metric: per-\nplexity, lower is better.\nFP16\nFP6\nINT4\nINT4\nFine-Grain Quantization\n/\n\u2717\n\u2713\n\u2717\nLLaMA-1B\n24.13\n24.83\n564.73\n288.22\nLLaMA-13B\n13.16\n13.09\n14.19\n14.13\nLLaMA-65B\n6.41\n6.42\n6.61\n7.17\nTable 2: Code Generation in HumanEval-X (JavaScript) [43].\nMetric: pass@1\u2191, higher is better\nFP16\nFP6\nINT4\nINT4\nFine-Grain Quantization\n/\n\u2717\n\u2713\n\u2717\nCodeGeeX2-6B [43]\n31.50\n31.61\n28.35\n25.15\nStarCoder-15B [13]\n33.67\n33.6\n32.32\n32.18\nCodeLLaMA-34B [16]\n45.05\n44.51\n43.22\n43.45\n4\nDesign Choices and Challenges\n4.1\nDesign Choices\nAlthough there is an increasing demand for high-performance\nsupport of post-training FP6 quantization, currently there is\nno such efficient FP6-centric system design available that\nenables the aforementioned trade-offs against 4-bit and 8-bit\nquantization. Specifically, existing supports for linear layers\nare mainly designed for data types whose bit-width is an\nexponent of 2 (e.g., 4-bit, 8-bit, and 16-bit). Given that it is\nnot clear how to support FP6 efficiently on modern GPUs, we\nillustrate two important design choices in this section.\nNecessity in enabling Tensor Cores. We find it essential to\nsupport Tensor Cores when performing inference of quantized\nLLMs. For example, we have evaluated the performance of\nAWQ\u2019s [14,15] pure SIMT-core execution on various batch\nsizes to test its scalability. As shown in Figure 1, the runtime\nperformance of linear layers without Tensor Core support\n(AWQ_W4A16_SIMT) becomes extremely low as the infer-\nence batch size increases. The reason behind this is twofold.\nOn one hand, traditional SIMT cores are an order of magni-\ntude slower than Tensor Cores for linear layer execution as\ndescribed in Section 2.3. On the other hand, a large fraction\nof the SIMT core\u2019s computational power will be used to de-\nquantize the model weights at runtime, which further reduces\nthe available computational power of SIMT cores for comput-\ning matrix multiplication. This motivates us to enable tensor\ncores for intensive computation of matrix multiplication while\nleveraging versatile SIMT cores for weight de-quantization.\nUnified kernel solution rather than dual kernels. The\nunique character of WxA16 quantization is that the activation\nmatrices use FP16 but the weight matrices are stored in a\nnarrower bit-width. However, Tensor Cores require both the\nweights and activations matrices to be stored in the same\n3\nDe-quantization\nMatMul (FP16)\nW (FPx)\nA (FP16)\nW\u2019 (FP16)\nO (FP16)\non-the-fly\nDeQuant\nMatMul (FP16)\nW (FPx)\nA (FP16)\nW\u2019 (FP16)\nO (FP16)\nKernel-1\nKernel-2\nSingle \nKernel\nFigure 2: Two different methods to support weight-only\nWxA16 quantization during LLM inference. (Left) Dual ker-\nnels. (Right) Unified kernel.\ndata type, e.g. FP16/INT8/INT4. The straightforward solution\n(i.e., dual kernel solution) adds an extra GPU kernel that de-\nquantizes the weights to FP16 before calling the normal FP16\nkernel. However, such inference speed would be even slower\nthan that of the model without quantization. As shown in\nFigure 2 (Left), two GPU kernels will be launched for the\nlinear layer execution, and the de-quantized FP16 weights\nwill be written to GPU DRAM before being read by the\nsecond GPU kernel, resulting in 2\u00d7 DRAM access. It is more\nefficient to fuse the de-quantization and the matrix-multiply\nprocess into a single GPU kernel, eliminating the read/write\nof the de-quantized weights (W\n\u2032 in FP16).\n4.2\nDesign Challenges\nGiven the design choices in Section 4.1, it is challenging to\ndesign a unified GPU kernel supporting FP6\u00d7FP16 matrix\nmultiplication on modern GPUs. On one hand, modern GPU\nmemory systems do not naturally support irregular bit-width\n(not an exponent of 2) because the minimal access size of\nGPU global/shared memory is 8/32 bits per thread and the\nmemory addresses to access must be aligned. The complex\ndata layout requirement of Tensor Cores makes it even more\nchallenging for irregular bit-widths. On the other hand, the\nde-quantization computation is expensive as it requires a large\namount of complex bit-level operations. Thus, how to fuse\nthe de-quantization into the linear layer computation without\nhurting the overall performance is also non-trivial.\n4.2.1\nHardware-Unfriendly Memory Access\nDuring the execution of linear layers on modern GPUs, model\nweights should be loaded from DRAM to registers before the\ncorresponding multiplication calculations can be performed.\nUsually, the model weights are loaded in two steps, to hide the\nhigh access latency of DRAM for high performance. Specifi-\ncally, model weights are first loaded from GPU DRAM and\nbuffered into on-chip memory (e.g., shared memory) for data\nreusing. After that, the buffered weights are then read from\nshared memory to registers for the actual computation.\nGiven that each GPU thread cannot directly access other\nT3\nT2\nT1\nT0\nT7\nT6\nT5\nT4\nT11\nT10\nT9\nT8\nT15\nT14\nT13\nT12\nT19\nT18\nT17\nT16\nT23\nT22\nT21\nT20\nT27\nT26\nT25\nT24\nT32\nT30\nT29\nT28\nFP-x\nFP-x\n2*x bits\n8\n8\nWeights \nMatrix\nDistributing a \nsubmatrix to \ndifferent GPU \nthreads.\n8 9 101112131415 \u2026 5657585960616263 \u2026\n\u2026\n1 2 3 4 5 6 7\n0\nT0\nT1\nT2\nT3\nT4\nT5\nT6\nT7\nT28 T29 T30 T31\nTranslating to Linear Memory Space\nX-bit Weight\n(a) Required Data Layout of Tensor Cores Input. T0 Means Thread #0.\n\u2026\nT28 T29 T30 T31\n\u2026\nT7\nT6\nT5\nT4\n\u2026\nT3\nT2\nT1\nT0\nW2\nW1\nAligned 32-bit Words\nTwo 6-bit Weights needed by Thread #4.\nW4\nW3\nAligned 32-bit Words\nW16\nW15\nAligned 32-bit Words\n(b) Accessing 6-bit weights at the granularity of 32-bit Words.\nFigure 3: Memory Access of X-bit Weights for each Thread.\nthreads\u2019 registers3, each thread must put the model weights\nthat are needed by itself to its private registers on its own.\nThis process can become extremely challenging when the\nweights are stored with irregular bit-width (not 2n, e.g., 6 bit),\ngiven the rigid data layout requirements of Tensor Cores. As\nshown in Figure 3a, the minimal input of FP16 Tensor Cores\nis a 8\u00d78 sub-matrix in modern GPU architecture, and each\nGPU thread should hold a pair of weights in its register. In\nnormal cases, each weight is stored with 16 bits, and each\npair of weights can be naturally read from shared memory\nat the granularity of 32-bit words. However, each weight is\nstored with x-bits in our work4, which makes memory access\nextremely unfriendly to modern GPU memory hierarchy.\nOn-chip Memory Access with Unused Bits: We use 6-bit\nquantization as an example to show the inefficiency in ac-\ncessing weights with irregular bit-width. As shown in Figure\n3b, weights are already buffered in shared memory, and each\nGPU thread needs to read a pair of weights (12 bits, 2\u22176bits)\nfrom shared memory. However, shared memory has 32 mem-\nory banks and each memory bank outputs a 32-bit word per\nmemory request on GPU. Thus, a large fraction of bits read\nfrom shared memory will be unused, resulting in a signifi-\ncant waste of shared memory bandwidth. For instance, T0\n(Thread #0) in Figure 3b only needs 12 bits. However, a 32-\nbit word (W1) will be read, resulting in 20 out of 32 bits\n(62.5%) unused. The waste of unused bits can get even more\nsevere due to the requirement of aligned memory access5\nin modern GPU memory hierarchy. As shown in Figure 3b,\nthe bits needed by T2 (Thread #2) are distributed in both W1\n3Each GPU thread is allocated and owns a distinct portion of the whole\nregisters available on GPU processors.\n4Our design principles support not only 6-bit but also any other bit widths.\n5Memory access must be aligned, i.e., its address is a multiple of its size.\n4\nand W2. Thus, T2 needs to read both W1 and W2, reading\n2\u221732 bits from shared memory. However, only 6\u22172 bits will\nbe eventually used, resulting in 52 out of 64 bits (81.25%)\nunused and wasted. It is also worth noting that the memory\nmanagement and access on GPU DRAM and registers suffer\nfrom similar problems due to the irregular bit-width.\n4.2.2\nHigh Computation Overhead of De-quantization\nThe runtime overhead of FPx-FP16 de-quantization can be\nextremely high, which easily slows down the overall execu-\ntion. On one hand, large amounts of model weights need to be\nde-quantized at runtime, e.g. 70 billion FPx weights should\nbe de-quantized for each LLM decoding step6 for LLaMA-\n70b [33] inference. On the other hand, the runtime overhead\nto de-quantize each FPx weight is high, requiring complex\nbit-wise operations. According to Equation 2, new sign, expo-\nnent, and mantissa all need to be calculated during runtime,\nto obtain the FP16 with the equivalent value of a given FPx.\n2E f p16\u2212biasf p16 \u00d7(1.M f p16) = 2E f px\u2212biasf px \u00d7(1.M f px) (2)\nIn Equation 2, bias f p16 = 15 and biasf px = 2len(E f px)\u22121 \u2212 1.\nThe sign field of the FP16 is identical to that of the FPx, and\nthe mantissa of the FP16 can also be calculated by padding\nzeros to that of the FPx. What\u2019s more, the exponent of FP16\nshould be E f p16 = E f px +biasf p16 \u2212biasf px, which is more\ncomputationally expensive. In summary, how to de-quantize\nFPx values efficiently also becomes a major challenge.\n5\nDesign Methodology\nIn this section, we first provide an overview of our designs\nin Section 5.1. To solve the challenge of unfriendly memory\naccess (Section 4.2.1), we propose Ahead-of-time Bit-level\nPre-packing in Section 5.2. To deal with the challenge of\nthe high computational overhead of de-quantization (Section\n4.2.2), we presented our designs to achieve SIMT-Efficient\nGPU Runtime in Section 5.3. At last, we presented our soft-\nware pipeline designs in Section 5.4, where SIMT cores, Ten-\nsor Cores, and GPU memory hierarchy work collaboratively\nwith full performance.\n5.1\nOverview\nFigure 4 compares TC-FPx, the x-bit weight-only quantized\nlinear layer kernel in our design, with the traditional design for\ngeneral-purpose matrix multiplication (GEMM) where both\ninput matrices are in FP16. The model weight is stored with\na reduced number of bits for TC-FPx. Consequently, an addi-\ntional de-quantization stage (Dequant W) is introduced at the\nregister level, where the FP6 weights are de-quantized to FP16\nlocally within each thread using SIMT cores. It is worth noting\n6To generate a sequence with n tokens, n-1 decoding steps are required.\nGlobal Memory\nL2 Cache\nRegisters\nRegisters\nTensor Core\nmma\nDequant [W]\nShared Memory\nLdmatrix [A]\nLds [W]\nAsync_Copy\nW: FP-x\nA: FP16\n(a) TC-FPx Design.\nGlobal Memory\nL2 Cache\nShared Memory\nAsync_Copy\nRegisters\nTensor Core\nmma\nW: FP16\nA: FP16\nLdmatrix \n[Activation]\nLdmatrix\n[Weight]\n(b) Traditional GEMM Design.\nFigure 4: Design Overview.\nthat the FP16 weights are not written back to shared memory\nbut stored in registers for future use, eliminating unnecessary\nround-trip access to shared memory. Another difference is\nthat TC-FPx loads x-bit weights from shared memory to regis-\nters using fine-grained lds (load shared) instructions instead\nof using the coarse-grained intrinsic ldmatrix (load matrix),\nwhich has a strict layout requirement and is less flexible.\n5.2\nAhead-of-time Bit-level Pre-packing\nAs described in Section 4.2.1, memory access to weights with\nirregular bit-width is unfriendly to modern GPU memory hi-\nerarchy. To address this problem, we propose the insight that\nwe can combine the memory read of every 32 x-bit weights,\nresulting in x request of 4-byte word per GPU thread. In this\ncase, all the memory access would be aligned at the granular-\nity of 32-bit words rather than the irregular bit-width.\nHowever, it is not trivial to combine the memory read of\nweights due to the rigid data layout requirements of Tensor\nCores, because the weights needed by each GPU thread are\nnot stored in continuous memory space. To solve this problem,\nwe propose to optimize the runtime memory access pattern\nby reordering the weights within each weight matrix and\npre-pack the weights ahead of time. As model weights are\nstatically determined after the model is trained and quantized,\ncomplicated memory layout transformation can be applied\nto the weights ahead of time and thus introduces no runtime\noverhead. Besides, we only need to pre-pack the weights once,\nthus the overhead of weight pre-packing can be effectively\namortized by each inference service and becomes negligible.\nIn general, weight pre-packing consists of two steps. In\nthe first step, we gather all the weights needed by each GPU\nthread and combine these weights locally. Given that the\nweights needed by each GPU thread are not originally in con-\ntinuous locations (see Figure 3a) within each weight matrix,\nwe must pick the weights for each GPU thread carefully. The\nweights picked for each thread are then combined locally in\nrelative temporal order as they are consumed by Tensor Cores\n5\nT3\nT2\nT1\nT0\nT7\nT6\nT5\nT4\nT10 T11\nT9\nT8\nT13 T14 T15\nT12\nT17 T18 T19\nT16\nT21 T22 T23\nT20\nT25 T26 T27\nT24\nT29 T30 T32\nT28\nT3\nT2\nT1\nT0\nT7\nT6\nT5\nT4\nT10 T11\nT9\nT8\nT13 T14 T15\nT12\nT17 T18 T19\nT16\nT21 T22 T23\nT20\nT25 T26 T27\nT24\nT29 T30 T32\nT28\nT3\nT2\nT1\nT0\nT7\nT6\nT5\nT4\nT10 T11\nT9\nT8\nT13 T14 T15\nT12\nT17 T18 T19\nT16\nT21 T22 T23\nT20\nT25 T26 T27\nT24\nT29 T30 T32\nT28\nT3\nT2\nT1\nT0\nT7\nT6\nT5\nT4\nT10 T11\nT9\nT8\nT13 T14 T15\nT12\nT17 T18 T19\nT16\nT21 T22 T23\nT20\nT25 T26 T27\nT24\nT29 T30 T32\nT28\nT0\n128 * [x bits]\n\u2026\u2026\n4x * [32 bits]\n64\nStep1\n\u2026\u2026\n64\nA Tile of Weights\nA Slice Inputs for a mma instruction\n16\n16\nT1\nT2\nT30\nT31\nStep2\nFigure 5: Ahead-of-time Bit-level Weight Pre-packing.\nat runtime. In the second step, we combine all the weights\nneeded by the whole GPU WARP (consisting of 32 GPU\nthreads) into a unified linear memory space, in which order\nthe weights will be stored in GPU DRAM before runtime. To\nfully eliminate shared memory bank conflict7, we propose to\ncombine the 32-bit word of each thread in a jagged order.\nIt is worth noting that all the techniques discussed in this\nsubsection are independent of the actual bit-width (denoted\nusing x the whole time) of the model weights. Thus, our\nweight pre-packing can be naturally applied to any bit-width.\nStep 1: Per-thread Weight Gathering\nFigure 5 demon-\nstrates the weights picked by T0 (Thread #0) and the order\nto combine them. We suppose the WARP-level tiling size\nis 64\u00d764, which means each weight matrix is divided into\n64\u00d764 data tiles and loaded to GPU\u2019s shared memory at this\ngranularity for each WARP. Each weight tile is then further\ndivided into four slices, as the weights are loaded from shared\nmemory and used for Tensor Core computations on a slice-\nby-slice basis. What\u2019s more, each slice is divided into four\n16\u00d716 chunks, as Tensor Core processes 16\u00d716 data items\nin each instruction. Within each 16 \u00d7 16 chunk, four pairs\nof FPx weights are picked for T0 and combined. As shown\nin Figure 5, we get 32 (i.e., the WARP size) groups of FPx\nweights after Step 1. The weights are combined and stored\ncontinuously within each group and each group of weights\nwill be consumed by a certain GPU thread. In summary, each\n64 \u00d7 64 weight tile is eventually assigned to 32 threads (a\nWARP), and each thread will consume 128 x-bit weights.\nStep 2: Bit-level Assembling per WARP\nIn Step 2, we\nassemble all the weights of different groups into a unified\n7Bank conflicts occur in shared memory when multiple threads access\ndata in the same memory bank simultaneously, leading to lower throughput.\nmemory space. During this bit-level pre-packing process, we\nconsider the combined weights as continuous data to copy,\ntemporarily ignoring the meaning of each bit. Specifically,\n128 items with x-bit are considered as 4x items with 32 bits.\nWe propose to assemble the weights of all groups in the\njagged order shown in Figure 5. To begin with, the first 32-\nbit item of each thread is concatenated together. After that,\nthe second 32-bit item of each thread is concatenated and\nappended to the previous results. By repeating this process, all\nweights can be stored continuously in a linear memory space\nand well-aligned (128-byte aligned). In this way, all weights\ncan be simply copied from DRAM to shared memory at the\ngranularity of 128-byte blocks without any changes, easily\nachieving optimal DRAM access. Besides, these weights can\nthen be loaded from shared memory with optimal performance\nas well during runtime. Specifically, a WARP of threads will\nread consecutive 32-bit items in shared memory for each\nmemory request, fully avoiding bank conflict.\n5.3\nSIMT-Efficient GPU Runtime\nParallel De-quantization\nTo reduce the runtime overhead\nof FP-x weight de-quantization, we implemented FP-x de-\nquantization with optimized bit-wise SIMT core instructions.\nBesides, we propose to de-quantize multiple FPx weights\nin parallel, further reducing the SIMT overhead by 4\u00d7 by\nexploiting the bit-level parallelism within each 32-bit register.\n(1) Optimized Bit-wise Operations: As described in Sec-\ntion 4.2.2, the exponent for FP16 should be E f p16 = E f px +\nbiasf p16 \u2212 biasf px, when casting an FPx to the equivalent\nFP16. To simplify this process, we adopted the mathe-\nmatical transformation in [35], calculating the exponent of\nFP16 with E f p16 = E f px instead. To maintain correctness,\nthe result FP16 is then multiplied with the FP16 constant\n2bias f p16\u2212biasf px:\ncast(Wf px) = new_cast(Wf px)\u00d72biasf p16\u2212bias f px.\n(3)\nFig.6a shows the optimized FP16 to FP6 conversion. Al-\nthough we only draw the cast from FP6 to FP16 for demon-\nstration, it can be applied to any bit-width. The sign field of\nFP16 is identical to that of FPx. Besides, the lower bits of the\nexponent field and the higher bits of the mantissa field can\nbe copied from FPx to FP16 together for efficiency. What\u2019s\nmore, other bits of FP16 should be padded with zeros.\nWith careful designs, we succeeded in achieving cast from\nFP6 to FP16 with only two bit-wise \"and\", one \"shifting\", and\none \"or\" as shown in \u2776 of Figure 6b. The sign field is copied\nfrom FP6 to FP16 with the first \"and\" and all other bits of the\nFP16 are initialized to zeros at the same time eliminating the\nneed to pad zeros to the exponent and mantissa fields later.\nAll bits of the FP6 are then shifted right with the bit-wise\n\"right shifting\". After that, the lower bits of the exponent and\nthe higher bits of the mantissa in FP6 are first selected via the\n6\n2 3 4 5 6\n1\n2 3 4 5 6 7 8 9 10111213141516\n1\n2 3 4 5 6\n1\n2 3 4 5 6 7 8 9 10111213141516\n1\nS\nE\nM\n(1) S_[1]\n= S[1]\n(2) S_[2:6] = S[2:4]+12\n(3) S_[7:8] = S[5:6]\n(4) S_[9:16] = 00\u20260\n(1) S_[1]\n= S[1]\n(2) S_[2:3]   = 00\n(3) S_[4:8] = S[2:6]\n(4) S_[9:16] = 00\u20260\n(a) Optimized FP6 to FP16 cast.\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n2 3 4 5 6\n1\n2 3 4 5 6\n2 3 4 5 6 1\n2 3 4 5 6 1\n2 3 4 5 6 1\n1\nSlot 2\n2 3 4 5 6\n1\n2 3 4 5 6\n1\n2 3 4 5 6\n1\n2 3 4 5 6\n1\n2 3 4 5 6\n1\n2 3 4 5 6\n1\nR2 = R1 & 0x80808080\nR1 = R1 >> 2\nR1 = R1 & 0x1f1f1f1f\nR2 = R2 | R1\nR1 = R2 & 0x9f009f00\n[#2 FP16, #1 FP16]\nR2 =R2&0x009f009f\nR2 = R2 << 8\n[#4 FP16, #3 FP16]\n1\n2\n4\n3\nSlot 4\nSlot 1\nSlot 3\nR1\nR2\nR1\nR2\nR2\n(b) 4-Way parallel de-quantization within 32-bit registers.\nFigure 6: SIMT-Efficient On-the-fly Parallel De-quantization.\n\"and\" between the FP6 and the bit mask \"0x1f1f1f1f\", and\nthen copied to the FP16 with the bit-wise operation \"or\".\n(2) Bit-level Parallelism: Given the insight that we can ex-\nploit the bit-level parallelism within each 32-bit word, we\npropose to de-quantize multiple FPx weights in parallel, fur-\nther reducing the runtime overhead of de-quantization. The\ndetailed design is demonstrated in Figure 6b using FP6 as an\nexample. The 32-bit registers are treated as four processing\nslots, where each slot works independently with the same\ninstruction but different input FP6 data. Before the start of\nde-quantization, four FP6 should be stored in R1 (Register #1)\nwith the initial data layout shown in the figure. With the code\nsnippet \u2776, these four FP6 can be simultaneously de-quantized\ninto four FP16, where only the first 8 bits of each FP16 are\nstored in R2. After that, the first and the second FP16 are\nextracted to R1 with their last 8 bits padded with zeros, with\nthe code snippet \u2777. Finally, with the code snippet \u2778 and \u2779,\nthe third and the fourth FP16 are extracted to R2.\nWeight Split and Stitching\nWe will then demonstrate the\nmethod to efficiently reconstruct the 6-bit weights from the\n2+4 scheme [35] on GPUs with a carefully designed memory\nlayout, which can also be applied to other bit-width.\n(1) Ahead-of-time Weight Split: To store the weights in a\nwell-aligned manner in GPU\u2019s 32-bit registers, we split each\nweight into several segments, where the bit-width of each\nsegment is 2n, e.g. each 6-bit weight can be split into either\n2+4 or 4+2. Based on this scheme, the index calculations for\nthe following designs are significantly simplified. Note that\nthe techniques described in Section 5.2 can be applied to any\nbit-width, thus the 2-bit and 4-bit segments can be pre-packed\nseparately and efficiently according to Section 5.2.\n(2) Runtime Weight Stitching: Before the de-quantization,\nthe weights are first loaded from shared memory to registers.\nAs each weight is split into several segments, the complete\nweights need to be reconstructed at the register level during\nruntime. To reduce this runtime overhead, we propose to\nextract and stitch the weights in parallel. As shown in Figure\n7, two sets of registers are used to store 32 FP6 weights, where\n2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n1\n# 6 # 10# 14 # 4 # 8 # 12# 16\n# 2\n# 6\n# 4\n# 8\n# 1\n# 5\n# 3\n# 7\n4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6\n3\n1 2\n1 2\n1 2\n1 2\n# 2\n# 1\n# 4\n# 3\n1 2 3 4 5 6\n1 2 3 4 5 6\n1 2 3 4 5 6\n2 3 4 5 6\n1\n3 4 5 6\n3 4 5 6\n3 4 5 6\n4 5 6\n3\n3 4 5 6\n3 4 5 6\n3 4 5 6\n4 5 6\n3\nInt* Frag1_PTR\n2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n1\n# 2\n# 5 # 9 # 13 # 3 # 7 # 11# 15\n# 1\nInt* Frag2_PTR\n4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6\n3\n4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6\n3\n4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6 3 4 5 6\n3\nWeight Extraction & Stitching\n# 2\n# 1\n# 4\n# 3\n1\n2\n3\n4\nFigure 7: Parallel Weight Stitching.\nFrag1_PTR points to two 32-bit registers containing 32 2-bit\nsegments while Frag2_PTR points to four 32-bit registers\ncontaining 32 4-bit segments. With our parallel stitching, four\nFP6 weights are reconstructed simultaneously, reducing the\nnumber of SIMT core instructions by 4\u00d7. As shown in Figure\n7, four 2-bit segments are first extracted to Register #1 (\u2776),\nand four 4-bit segments are then extracted to Register #2 (\u2777).\nAfter that, Register #2 is right-shifted (\u2778) and its valid bits are\ncopied to Register #1 (\u2779), resulting in complete 6-bit weights.\n(3) Bit Reordering: To extract and stitch the weight in par-\nallel, it is necessary to enforce the initial data layout in Figure\n7. The key observation is that each four continuous segments\nmust be placed in the order shown in the figure, e.g. the first\nfour segments must be stored in the order of #2, #4, #1, and\n#3. Besides, the stride between each pair of 2/4-bit segments\nshould be 6/4, respectively. Otherwise, it is not possible to\nstitch four segments simultaneously with only four SIMT core\ninstructions. To satisfy the initial data layout requirements in\nFigure 7, we propose to ensure this layout via reordering the\nweight segments before runtime with no runtime overhead.\nBesides, this technique is supposed to be superimposed on\nthe technique described in Section 5.2 as an additional pass.\n7\nOverall Pseudo Code\nAlgorithm 1 shows the pseudo code\n(GPU code) including both Parallel De-quantization and\nWeight Stitching. All the input and output variables in the\npseudo code are stored in registers. As demonstrated in Fig-\nure 7, Algorithm 1 de-quantizes 32 FP6 weights in total. For\neach outer loop, four FP16 weights are generated and stored\nwith two registers at the end of the code. The transformations\nin Figure 7 (\u2776, \u2777, \u2778, and \u2779) are achieved with the SIMT core\noperations of lines 6, 7, 9, and 10 in Algorithm 1, respectively.\nThe output register array (OutputReg) is then directly used by\nTensor Cores as inputs.\nAlgorithm 1 Weight Stitching & De-quantization.\n1: Inputs: int Frag1_ptr[], int Frag2_ptr[], half Scales[]\n2: Output: int OutputReg[]\n3: #pragma unroll\n4: for int i = 0; i < 8; i++ do\n5:\n//Weight Extraction\n6:\nunsigned int R1 = (\u2217Frag1_ptr)&0xc0c0c0c0;\n\u25b7 \u2776\n7:\nunsigned int R2 = (\u2217Frag2_ptr)&0x f0f0f0 f0;\n\u25b7 \u2777\n8:\n//Weight Stitching\n9:\nR2 = R2 >> 2;\n\u25b7 \u2778\n10:\nR1 = R1|R2;\n\u25b7 \u2779\n11:\n//Advancing to next register or shifting current register.\n12:\nif i%4 == 3 then\n13:\nFrag1_PTR++;\n14:\nelse\n15:\n(\u2217Frag1_PTR) = (\u2217Frag1_PTR) << 2;\n16:\nif i%2 == 1 then\n17:\nFrag2_PTR++;\n18:\nelse\n19:\n(\u2217Frag2_PTR) = (\u2217Frag2_PTR) << 4;\n20:\n//4-Way Parallel de-quantization.\n21:\n\u2217R2 = \u2217R1&0x80808080;\n22:\n\u2217R1 = \u2217R1 >> 2;\n23:\n\u2217R1 = \u2217R1&0x1f1f1 f1 f;\n24:\n\u2217R2 = \u2217R2|\u2217R1;\n25:\n\u2217R1 = \u2217R2&0x9f009f00;\n26:\n\u2217R2 = \u2217R2&0x009 f009f;\n27:\n\u2217R2 = \u2217R2 << 8;\n\u25b7 R1 and R2 now each has 2 FP16 weights.\n28:\n//Multiplying with quantization scales & Output to registers.\n29:\nOutputReg[i\u22172] = Multiply(R1,Scales[i/2\u22172]);\n30:\nOutputReg[i\u22172+1] = Multiply(R1,Scales[i/2\u22172+1]);\n5.4\nSoftware Pipeline Design\nTo reduce the usage of GPU registers, we de-quantize the\nweights slice by slice. Besides, we seamlessly fuse the pro-\ncess of de-quantization into the traditional software pipeline\nof linear layer execution, completely hiding the runtime over-\nhead of de-quantization via effective instruction parallelism.\nSlice-by-slice De-quantization\nInstead of de-quantizing all\nthe weights at once, we de-quantize the FPx weights slice\nby slice. As shown in Figure 8a, we assume that an FPx\nweights tile and an FP16 activation tile are already copied\nfrom DRAM to shared memory. The whole tile of weight\nin shared memory is then de-quantized in several steps. In\nTensor cores\nDouble Buffers Implemented with GPU Registers\nATile [Shared Mem]\nA1\nB1\n(1)  Load Shared;\n(2)  SIMT-efficient     \nDe-quantization\nLoad Matrix\n(\u201cldmatrix\u201d)\nB0\nA0\nBTile [Shared Mem]\nFP-x \nWeights\nFP16 \nActivation\nASlice\nBSlice\n(a) Slice-by-slice De-quantization.\ncp.async\ncp.async\ncp.async\nmma\nDe-quant\nk=0\nInitializing Pipeline\nMore Loops\u2026\nB1\nB0\nA0\nasync_copy_barrier();\n__sync_threads();\nWrite/Read\nShared\n1st Loop\nDe-quant\nA1\nDequant\nDe-quant\nDe-quant\nDequant\nldmatrix\nldmatrix\nldmatrix\nldmatrix\nldmatrix\nldmatrix\nmma\nmma\nmma\nmma\nk=1\nk=2\nk=3\nTime\nOn-chip Buffers\nWrite/Read\nRegisters\nData \nDependency\n(b) Space-time Diagram of the Kernel Pipeline.\nFigure 8: Software Pipeline of TC-FPx GPU Kernel.\neach step, only a slice of FPx weights is loaded from shared\nmemory to registers, de-quantized into FP16 weights with\nSIMT-Efficient GPU Runtime (Section 5.3), and then stored in\nthe register buffer A1 or A2 as inputs for Tensor Cores. ASlice\nand BSlice are then multiplied using Tensor Cores.\nCompared to de-quantizing the whole tile at once, our slice-\nby-slice de-quantization reduces the number of registers re-\nquired to store the FP16 weights by 4\u00d7, significantly reducing\nregister pressure. Besides, more opportunities are created for\ninstruction-level parallelism, since Censor Cores can be used\nimmediately for computations once a slice of weights is de-\nquantized, rather than waiting for the entire tile.\nEffective Overlapping\nThe software pipeline is illustrated\nvia the space-time diagram in Figure 8b, where SIMT cores\n(working on de-quantization), Tensor Cores (working on ma-\ntrix multiplication), and GPU memory hierarchy work collab-\noratively, achieving high instruction-level parallelism.\nFirstly, global memory read is executed asynchronously\nusing the cp.async [20] intrinsic, fully overlapped with other\noperations. Memory barrier and thread block synchronization\nare issued after the third slice is processed (at the end of k=2),\nmaking sure that the data for the next main loop is ready in\nshared memory so that the \"De-quant\" (de-quantization) and\nthe \"ldmatrix\" operations can be started when k=3.\nSecondly, shared memory read is also overlapped with ten-\nsor core operations. When the ith slice is being computed,\nthe data of the (i + 1)th slice are read from shared memory\nsimultaneously via \"De-quant\" and \"ldmatrix\".\nLast but not least, the SIMT core operations for weight\n8\nde-quantization are also effectively overlapped with Tensor\nCore operations. Within the \"De-quant\" process of the ith\nslice, the FPx weights are first loaded from shared memory\nto registers using the hardware intrinsic load shared (LDS),\nand then immediately de-quantized into FP16 weights with\nSIMT cores. At the same time, Tensor Cores are computing\nthe (i\u22121)th slice with no data dependency.\n6\nImplementation\nWe implemented the TC-FPx kernel supporting matrix multi-\nply C = A\u00d7B, where A is the weight matrix of shape [M,K]\nand B is the activation matrix of shape [K,N]. The weight\nmatrices are stored in our customized format described in\nSection 5.2, and the input and output activation matrices are\nstored in column-major. Thus, our TC-FPx kernel could be a\ndrop-in replacement of cuBLAS kernels in inference frame-\nworks for quantized LLMs. Our GPU kernel is implemented\nwith more than 1.2K lines of CUDA codes, on top of the code\nof Flash-LLM [37]. Our TC-FPx kernels could be compiled\nseparately into a .so dynamic link-able library, and we provide\na set of C++ APIs to call the kernels. Thus, our kernels could\nbe easily used and integrated. Besides, we also provided C++\nAPIs to pre-pack the weight matrices (See Section 5.2). More\nimportantly, we provide new system support for end-to-end\ninference of quantized LLMs, by integrating our kernel into\nthe state-of-the-art inference framework DeepSpeed [19].\n7\nEvaluation\nWe evaluate the performance at two levels: kernel-level bench-\nmarking using TC-FPx GPU kernels and model-level end-to-\nend inference using DeepSpeed integration (which we call\nFP6-LLM). The kernel-level evaluation is conducted on the\nNVIDIA A100-40GB platform with CUDA 11.8, and we\nmainly evaluate the performance of linear layers within LLMs\nduring the token generation phase. The utilization of each\nGPU hardware unit during runtime (Section 7.1) is measured\nusing NVIDIA Nsight Compute [23]. For end-to-end eval-\nuations, we conduct the inference of typical LLMs on the\nNVIDIA A100-SXM4-80GB DGX platform with CUDA\n11.8. The inference latency and the latency breakdown (Sec-\ntion 7.3) are measured using NVIDIA Nsight System [24].\n7.1\nLinear Layer Speedups to 8-/16- bit\nWorkloads.\nWe evaluate the performance of TC-FPx on\nlinear layers under different shapes, coming from the shapes\nof the weight matrices within LLaMA models [32] (llama-7b,\nllama-13b, llama-33b, and llama-65b) and OPT models [41]\n(OPT-30b, OPT-65b, and OPT-175b). For each model, we\nevaluated the latency of each GPU kernel at three typical\ninference batch sizes, i.e. 8, 16, and 32.\nBaselines.\nThe baselines we compare include the W16A16\nkernels from cuBLAS [22] and the W8A16 kernels from\nTensorRT-LLM (commit: 6837c81) [26]. What\u2019s more, we\nalso include the W4A16 (FP4) support from BitsandBytes\n(commit: f1ef74f) [3] as a baseline.\nResults.\nFigure 9 shows the latency speedups of TC-FPx\nand other baselines. We use the performance of cuBLAS to\nnormalize the performance of all GPU kernels. As shown\nin Figure 9, TC-FPx outperforms BitsandBytes (W4A16),\ncuBLAS (W16A16), and TensorRT_LLM (W8A16, INT8\nweights) by up to 8.9\u00d7, 2.6\u00d7, and 1.9\u00d7. On average, TC-\nFPx outperforms BitsandBytes, cuBLAS and TensorRT_LLM\nby 7.6\u00d7/7.5\u00d7/6.6\u00d7, 2.2\u00d7/2.2\u00d7/2.0\u00d7, and 1.3\u00d7/1.3\u00d7/1.2\u00d7\nwhen the batch size is 8/16/32, respectively.\nPerformance Analysis\nWith extensive kernel profiling, We\ndemonstrate the utilization8 of each GPU hardware unit and\nprovide more in-depth insights into the source of our perfor-\nmance improvements. During the execution of linear layers,\nas for the cuBLAS baseline, the DRAM bandwidth (shown\nas the yellow lines in Figure 10a) is almost exhausted (>80%)\nwhile the GPU Tensor Cores (shown as the yellow bar in Fig-\nure 10a) are not fully used (<50%), when the inference batch\nsize is smaller than 128. It is a common issue during the infer-\nence of large language models caused by the auto-regressive\ninference scheme of large language models. With our sup-\nport of 6-bit quantization, the DRAM access is significantly\nreduced (up to 2.7\u00d7), mitigating the bottleneck of insufficient\nDRAM bandwidth. Consequently, the Tensor Cores can be\nmore effectively utilized for matrix computations, shown as\nblue bars compared to yellow bars in Figure 10a. In summary,\nour kernel mitigated the \"memory wall\" issue and achieved\nhigher computational efficiency (higher utilization of Tensor\nCores) by supporting 6-bit quantization on Tensor Cores.\nFurthermore, it explains that our kernel can outperform\nTensorRT-LLM\u2019s W8A16 kernel because we are more effec-\ntive in reducing DRAM access of model weights. Note that\nthe performance of our TC-FPx kernel, cuBLAS kernel, and\nTensorRT-LLM\u2019s W8A16 kernel will eventually converge to\nthe same performance when the inference batch size is larger\n(bigger than 128), as their performance will all be bounded\nby the peak computing power of Tensor Cores.\nWe also observed that BitsandBytes is constantly slower\nthan cuBLAS, which is 29.6% as fast as cuBLAS on aver-\nage. After further investigation, we found that BitsandBytes\nadopted the dual-kernel method (discussed in Section 4.1) to\nsupport FP4 quantization. During the execution of the first\nkernel, the FP4 model weights will be first loaded from global\nmemory, de-quantized into FP16, and then written back to\nglobal memory in the FP16 data type. After that, a normal\n8\"Utilization\" typically refers to the degree to which a particular hardware\nresource is being actively used during the execution of a GPU kernel.\n9\n0\n0.5\n1\n1.5\n2\n2.5\nBitsandBytes_W4A16_FP4\ncuBLAS_W16A16\nTensorRT_LLM_W8A16\nTC_FPx_W6A16\n0\n0.5\n1\n1.5\n2\n2.5\n12k*4k\n4k*4k\n10.75k*4k\n4k*10.75k\n15k*5k\n5k*5k\n13.5k*5k\n5k*13.5k\n19.5k*6.5k\n6.5k*6.5k\n17.5k*6.5k\n6.5k*17.5k\n24k*8k\n8k*8k\n21.5k*8k\n8k*21.5k\n21k*7k\n7k*7k\n28k*7k\n7k*28k\n27k*9k\n9k*9k\n36k*9k\n9k*36k\n36k*12k\n12k*12k\n48k*12k\n12k*48k\n0\n0.5\n1\n1.5\n2\n2.5\nBatch Size = 8\nBatch Size = 16\nBatch Size = 32\nLlama-7b\nLlama-13b\nLlama-33b\nLlama-65b\nOPT-30b\nOPT-66b\nOPT-175b\nFigure 9: Linear layer speedups compared to the baselines for token generation phase.\n0\n20\n40\n60\n80\n100\n1\n8\n16\n32\n64\n128\n256\n512\nUtilization %\nTC_cuBLAS\nTC_TC-FPx\nDRAM_cuBLAS\nDRAM_TC-FPx\n(a) Tensor core and DRAM utilization at different batch sizes.\n0\n10\n20\n30\n40\n1\n8\n16\n32\n64\n128\n256\n512\nUtilization %\nALU_cuBLAS\nALU_TC-FPx\nFMA_cuBLAS\nFMA_TC-FPx\n(b) ALU and FMA unit utilization at different batch sizes.\nFigure 10: Performance Analysis.\ncuBLAS kernel is launched computing the matrix multipli-\ncation as the second kernel. Thus, the FP4 GPU kernel is\nalways slower than the original FP16 cuBLAS kernel due to\nthe overhead of the extra GPU kernel for FP4 de-quantization.\nAnalysis of on-the-fly De-quantization\nFigure 10b shows\nthe overhead of FP6-to-FP16 de-quantization in two aspects.\nOn one hand, the FP6-to-FP16 de-quantization introduces\na significant number of bit-wise operations even with our\nSIMT-efficient designs. As a result, the utilization of the Arith-\nmetic/Logic Unit (ALU) has increased from 6.36% to 38.8%\non average. It is also strong evidence that the SIMT-efficient\ndesigns (Section 5.3) for de-quantization are essential. On the\nother hand, the FP6-to-FP16 de-quantization also introduces\nmore float-point multiplications, computing the multiplication\nbetween the weights and the quantization scales. On average,\nthe utilization of the FMA unit is increased from 0.33% to\n16.64%. Given that both ALU and FMA units are part of the\nSIMT cores, the de-quantization operations will not consume\nthe computing power of Tensor Cores. More importantly, the\nruntime overhead of SIMT cores can be effectively hidden by\noverlapping these SIMT instructions with other operations,\nwith our novel designs described in Section 5.4.\n7.2\nPerformance Comparison to 4-bit\nWorkloads\nAs described in Section 3, 6-bit quantization is\nmore appealing than 4-bit quantization in terms of preserving\nmodel quality. However, we still compare the performance\nof our W6A16 kernels to the state-of-the-art W4A16 kernels,\nfully demonstrating that our 6-bit quantization can achieve\ncomparable inference speed to the existing 4-bit quantization\nmethods. We evaluate the performance of the linear layers\nwithin the LLaMA-65b model [32] under different batch sizes.\nBaselines\nThe major baselines here include the W4A16\nsupport of row-wise quantization (Coarse-grained_W4A16)\nand the W4A16 support of group-wise quantization (Fine-\ngrained_W4A16)\nfrom\nTensorRT-LLM\n[26]\n(commit:\n6837c81) with state-of-the-art performance. We also include\ncuBLAS [22] here as the performance baseline, clearly show-\ning the benefits of each quantization method.\nResults\nFigure 11 shows the latency speedups of TC-FPx\nand other baselines running four different linear layers (i.g.\nL1, L2, L3, and L4) within the LLaMA-65b models. We use\ncuBLAS\u2019 performance to normalize the performance of other\nGPU kernels. As shown in Figure 11, TC-FPx_W6A16, Fine-\ngrained_W4A16, and Coarse-grained_W4A16 outperform\n10\nL1\nL2\nL3\nL4 Sum L1\nL2\nL3\nL4 Sum L1\nL2\nL3\nL4 Sum\ncuBLAS_W16A16\nTC-FPx_W6A16\nFine-grained_W4A16\nCoarse-grained_W4A16\nBatch Size = 8\nBatch Size = 16\nBatch Size = 32\n1\n2\n3\n0\nFigure 11: Linear layer speedups compared to using 4-bit\nweights for token generation phase of the LLaMA-65b model.\ncuBLAS_W16A16 by up to 2.4\u00d7, 3.0\u00d7, and 3.3\u00d7. More\nimportantly, TC-FPx achieves similar performance with Fine-\ngrained_W4A16, which is 1.06\u00d7/1.04\u00d7/0.94\u00d7 faster than\nFine-grained_W4A16 when running all these linear layers at\nbatch size 8/16/32, respectively. Besides, TC-FPx is only 16%\n/ 17% / 24% slower than Coarse-grained_W4A16 at batch size\n8/16/32. Since 6-bit quantization can provide significantly\nhigher model quality, it is a worthwhile trade-off.\n7.3\nEnd2End Inference\nWorkloads\nWe evaluate the end-to-end inference perfor-\nmance of FP6-LLM on large language models of vari-\nous model sizes, i.g. LLaMA-13b [33], OPT-30b [41], and\nLLaMA-70b [33]. For each model, we evaluated its token\ngeneration throughput at different batch sizes, starting from 1\nuntil GPU memory is exhausted.\nMetric.\nWe use the metric tokens per GPU-second to in-\ndicate the normalized inference throughput with the consid-\neration of both execution time and hardware cost (i.e., the\nnumber of GPUs used). It is calculated with this equation:\nIn ference_Per formance =\nNtoken\n\u2211NGPU\ni=1 Ti\n(4)\nNtoken means the number of tokens generated, whereas NGPU\nand Ti mean the GPU number and the time spent on the i\u2019th\nGPU for execution. We use this metric to evaluate the end-to-\nend inference performance in this section.\nSettings and Baselines\nWe set the prefill/prompt length\nof each request to 0.5K, and generate 1.5K tokens for each\nrequest ignoring the \"EOS\" (end of sequence) token. We inte-\ngrate our TC-FPx kernel into DeepSpeed [19] for end-to-end\nevaluation and call this new system support FP6-LLM. The\nbaseline for comparison is the FP16 execution of the original\nDeepSpeed system. With our FP6-LLM, only a single 80GB\nA100 GPU is used for the inference for all the workloads,\nincluding the LLaMA-70b model [33]. In contrast, two 80GB\nA100 GPUs are used for the inference of the LLaMA-70b\n0\n50\n100\n150\n200\n250\n300\n1\n2\n4\n8\n16\n32\nToken / GPU-Second\nFP6-LLM  - 1GPU\nFP16 - 2GPU\n(a) Generation throughput.\n0\n50\n100\n150\n200\n1 2 4 8 1632 1 2 4 8 1632\nFP16\n2 GPU\nFP6-LLM\n1 GPU\nTims (s)\nMatMul\nMHA\nNCCL\nOthers\n(b) Inference latency breakdown.\nFigure 12: LLaMA-70b inference at different batch sizes. Mat-\nMul: linear layers, implemented with cuBLAS or our TC-FPx;\nMHA: multi-head attention; NCCL: cross-GPU communica-\ntions; Others: other GPU kernels or GPU idle time.\n0\n50\n100\n150\n200\n250\n300\n350\n1\n2\n4\n8\n16\nToken / GPU-Second\nFP6-LLM  - 1GPU\nFP16 - 1GPU\n(a) Generation throughput.\n0\n20\n40\n60\n80\n1\n2\n4\n1\n2\n4\n8\n16\nFP16\n1 GPU\nFP6-LLM\n1 GPU\nTims (s)\nMatMul\nMHA\nNCCL\nOthers\n(b) Inference latency breakdown.\nFigure 13: OPT-30b inference at different batch sizes.\nmodel for the FP16 baseline, since the model weights (\u2248 130\nGB) can not be fit into a single GPU.\nLLaMA-70b\nFigure 12a shows token generation through-\nput on the LLaMA-70b model using our FP6-LLM (FP6-\nLLM-1GPU) and the FP16 baseline (FP16-2GPU). Accord-\ning to our experiments, both our FP6-LLM and FP16 baseline\ncan at most set the inference batch size to 32 before running\nout of GPU memory, whereas FP6-LLM only requires a sin-\ngle GPU and the baseline uses two GPUs. The results show\nthat FP6-LLM can achieve 1.69\u00d7-2.65\u00d7 higher normalized\ninference throughput than the FP16 baseline.\nWe conduct a careful latency breakdown of this end-to-end\ninference process. As shown in Figure 12b, our TC-FPx ker-\nnel (used in FP6-LLM) is 1.20\u00d7 faster than cuBLAS kernel\n(used in FP16 baseline) on average, even with half number\nof GPUs. Besides, the NCCL [25] overhead (cross-GPU com-\nmunications) is fully avoided using FP6-LLM since only a\nsingle GPU is required. We also notice that the FP16 base-\nline has accelerated the computation of MHA (multi-head\nattention) with 2-way tensor parallelism [31]. Overall, our\nFP6-LLM achieves up to 2.65\u00d7 higher throughput than the\nFP16 baseline as for tokens per GPU-second.\n11\n0\n100\n200\n300\n400\n500\n600\n700\n1\n2\n4\n8\n16\n32\nToken / GPU-Second\nFP6-LLM  - 1GPU\nFP16 - 1GPU\n(a) Generation throughput.\n0\n20\n40\n60\n80\n100\n1 2 4 8 1632 1 2 4 8 1632\nFP16\n1 GPU\nFP6-LLM\n1 GPU\nTims (s)\nMatMul\nMHA\nNCCL\nOthers\n(b) Inference latency breakdown.\nFigure 14: LLaMA-13b inference at different batch sizes.\nOPT-30b\nFigure 13a shows token generation throughput\non the OPT-30b model using FP6-LLM (FP6-LLM-1GPU)\nand the FP16 baseline (FP16-1GPU). According to our ex-\nperiments, FP6-LLM can at most set the inference batch size\nto 16 before running out of GPU memory while the FP16\nbaseline can at most serve 4 requests in a batch. As a result,\nFP6-LLM can at most achieve 319.1 tokens per GPU-second\n(4.05\u00d7 higher) with batch size 16 while the FP16 baseline\ncan at most achieve 78.8 tokens per GPU-second with batch\nsize 4, given the same GPU budget. Besides, FP6-LLM can\nachieve 1.91\u00d7/1.84\u00d7/1.72\u00d7 higher generation throughput\ncompared to the FP16 baseline when their batch sizes are set\nto 1/2/4. These overall performance improvements mainly\ncome from the reduction of time in executing linear layers.\nAs shown in Figure 13b, TC-FPx kernel is 2.39\u00d7 faster than\nthe FP16 cuBLAS kernel on average.\nLLaMA-13b\nFigure 14a shows the token generation\nthroughput on the LLaMA-13b model using FP6-LLM (FP6-\nLLM-1GPU) and the FP16 baseline (FP16-1GPU). According\nto the experiments, FP6-LLM and the FP16 baseline can at\nmost set the inference batch size to 32 before running out of\nmemory. On average, FP6-LLM can achieve 1.23\u00d7 higher\ngeneration throughput compared to the FP16 baseline using\nthe same batch size. The overall performance improvements\non this model are less significant compared to the previous\ntwo models due to the non-kernel overhead. According to\nFigure 14b, the execution time of linear layers has been sig-\nnificantly reduced (2.11\u00d7 faster on average) with TC-FPx\nkernel. However, the portion of running other GPU kernels\nplus the GPU idle time increases, weakening the overall per-\nformance gains. The reason is that GPUs tend to have a larger\nproportion of idle time due to kernel launch latency and GPU\nsynchronizations as the model size gets smaller.\n8\nRelated Work\nSix-bit Quantization\n[35] shows that FP6 performs ro-\nbustly across various algorithms and tasks, demonstrating its\nsuperiority in accuracy and versatility. Besides, [30] verified\nthat the FP6 data format can closely match FP32 for infer-\nence after quantization-aware fine-tuning. However, there is\nno hardware support for the proposed data types. Their in-\nference/training experiments can only be done via software\nemulations. FP6-LLM can provide high-performance GPU\nsupport for the inference of LLMs after FP6 quantization.\nSystem Supports for Quantization\nTensorRT-LLM [26]\nhas state-of-the-art kernel supports for weight-only quanti-\nzation. However, it only supports weights in INT4 (W4A16\n[7,14]) or INT8 (W8A16 and W8A8 [38]) data types while we\nprovide better trade-offs by supporting weights in 6 bits. Be-\nsides, TensorRT-LLM does not support float-point data type\n(e.g. FP6), which is much more complicated to de-quantize\nduring runtime than the integer type. Bitsandbytes [3] mainly\nsupports INT8 weights (W8A8) and has very naive support\nfor FP4 (W4A16) with poor performance. Llama.cpp [8] has\n2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit quantization support\non CPUs or GPU SIMT cores. However, it does not support\nweights in float point data type and it can not make use of\nGPU tensor cores. AWQ [14] has GPU kernel implemen-\ntation [15] for memory-efficient 4-bit Linear (W4A16) in\nPyTorch. OPTQ [7] has a basic GEMV implementation [5]\nfor weights in INT3 (W3A16) data type. To the best of our\nknowledge, this work is the first system supporting weight-\nonly quantization with FP6 weights on Tensor cores.\nRelated Design Techniques\n[40] and [35] has previously\nproposed 4+2 weight split. However, they only presented intu-\nitive thoughts without comprehensive system designs. Flash-\nLLM [36] has proposed the load-as-sparse and compute-as-\ndense approach for the weight-only-pruning. It does not tackle\nthe problems of supporting quantization.\n9\nConclusions\nIn this paper, we introduce TC-FPx, the first full-stack GPU\nkernel design scheme with unified tensor core support for\nfloat-point weights of various quantization bit-width, miti-\ngating the \"memory wall\" issues during LLM inference. We\nintegrate TC-FPx kernel into a state-of-the-art inference sys-\ntem, providing new end-to-end support (called FP6-LLM) for\nquantized LLM inference, where better trade-offs between\ninference cost and model quality are achieved. FP6-LLM tack-\nles the problems of hardware-unfriendly memory access and\nhigh computation overhead of de-quantization with a set of\nnovel techniques, achieving faster inference speed with signif-\nicantly less GPU memory. Evaluations show that FP6-LLM\nenables the inference of LLaMA-70b using only a single GPU,\nachieving 1.69\u00d7-2.65\u00d7 higher normalized inference through-\nput than the FP16 baseline. Besides, FP6-LLM improves the\ninference throughput of OPT-30b by 1.72\u00d7-4.05\u00d7.\n12\nReferences\n[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learn-\ners, 2020.\n[2] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374, 2021.\n[3] Tim Dettmers. bitsandbytes. \"https://github.com/\nTimDettmers/bitsandbytes\", 2023.\n[4] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. Llm.int8(): 8-bit matrix multiplication for\ntransformers at scale, 2022.\n[5] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh. gptq. \"https://github.com/IST-DASLab/\ngptq\", 2022.\n[6] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh.\nGptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv preprint\narXiv:2210.17323, 2022.\n[7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh. Optq: Accurate quantization for generative\npre-trained transformers. In The Eleventh International\nConference on Learning Representations, 2022.\n[8] Georgi Gerganov. llama.cpp. \"https://github.com/\nggerganov/llama.cpp\", 2023.\n[9] Github. Copilot. \"https://github.com/features/\ncopilot\", 2022.\n[10] Google. Bard. \"https://bard.google.com/\", 2023.\n[11] William Kahan. Ieee standard 754 for binary floating-\npoint arithmetic. Lecture Notes on the Status of IEEE,\n754(94720-1776):11, 1996.\n[12] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong,\nMinwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh,\nQijing Huang, Kurt Keutzer, Michael W Mahoney, et al.\nFull stack optimization of transformer inference: a sur-\nvey. arXiv preprint arXiv:2302.14017, 2023.\n[13] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, Qian\nLiu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas\nWang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-\nPoirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gon-\ntier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Lo-\ngesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-\ntasham Oblokulov, Zhiruo Wang, Rudra Murthy, Ja-\nson Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov,\nMarco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy,\nUrvashi Bhattacharyya, Wenhao Yu, Swayam Singh,\nSasha Luccioni, Paulo Villegas, Maxim Kunakov, Fe-\ndor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor,\nJennifer Ding, Claire Schlesinger, Hailey Schoelkopf,\nJan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jen-\nnifer Robinson, Carolyn Jane Anderson, Brendan Dolan-\nGavitt, Danish Contractor, Siva Reddy, Daniel Fried,\nDzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Fer-\nrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Lean-\ndro von Werra, and Harm de Vries. Starcoder: may the\nsource be with you!, 2023.\n[14] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu\nDang, Chuang Gan, and Song Han. Awq: Activation-\naware weight quantization for llm compression and ac-\nceleration, 2023.\n[15] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu\nDang, and Song Han. llm-awq. \"https://github.\ncom/mit-han-lab/llm-awq\", 2023.\n[16] Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong,\nHuan Wang, Zhichao Lei, Ming Liang, Dajun Chen,\nMin Shen, Hailian Zhou, Hang Yu, and Jianguo Li. Mft-\ncoder: Boosting code llms with multitask fine-tuning.\narXiv preprint arXiv, 2023.\n[17] Mary Ann Marcinkiewicz. Building a large annotated\ncorpus of english: The penn treebank. Using Large\nCorpora, page 273, 1994.\n[18] Stephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. Pointer sentinel mixture models. In\nInternational Conference on Learning Representations,\n2017.\n[19] Microsoft. Deepspeed github. \"https://github.com/\nmicrosoft/DeepSpeed\", 2023.\n[20] NVIDIA.\nNvidia\na100\ntensor core\ngpu\nar-\nchitecture.\n\"https://images.nvidia.com/\naem-dam/en-zz/Solutions/data-center/\nnvidia-ampere-architecture-whitepaper.pdf\",\n2020.\n13\n[21] NVIDIA.\nNvidia h100 tensor core gpu archi-\ntecture.\n\"https://www.hpctech.co.jp/catalog/\ngtc22-whitepaper-hopper_v1.01.pdf\", 2022.\n[22] NVIDIA.\ncublas.\n\"https://developer.nvidia.\ncom/cublas\", 2023.\n[23] NVIDIA.\nNsight\ncompute\nprofiling\nguide.\n\"https://docs.nvidia.com/nsight-compute/\nProfilingGuide/#introduction\", 2023.\n[24] NVIDIA.\nNsight system.\n\"https://developer.\nnvidia.com/nsight-systems\", 2023.\n[25] NVIDIA. Nvidia collective communications library\n(nccl).\n\"https://developer.nvidia.com/nccl\",\n2023.\n[26] NVIDIA.\nTensorrt-llm.\n\"https://github.com/\nNVIDIA/TensorRT-LLM/\", 2023.\n[27] OpenAI.\nChatgpt.\n\"https://openai.com/blog/\nchatgpt\", 2022.\n[28] OpenAI. Gpt-4 technical report, 2023.\n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a unified text-to-text transformer. The Journal\nof Machine Learning Research, 21(1):5485\u20135551, 2020.\n[30] Bita Darvish Rouhani, Ritchie Zhao, Ankit More,\nMathew Hall, Alireza Khodamoradi, Summer Deng,\nDhruv Choudhary, Marius Cornea, Eric Dellinger,\nKristof Denolf, Stosic Dusan, Venmugil Elango, Maxi-\nmilian Golub, Alexander Heinecke, Phil James-Roxby,\nDharmesh Jani, Gaurav Kolhe, Martin Langhammer,\nAda Li, Levi Melnick, Maral Mesmakhosroshahi, An-\ndres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei\nShao, Michael Siu, Pradeep Dubey, Paulius Micikevi-\ncius, Maxim Naumov, Colin Verrilli, Ralph Wittig, Doug\nBurger, and Eric Chung. Microscaling data formats for\ndeep learning, 2023.\n[31] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter lan-\nguage models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar,\nAurelien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foun-\ndation language models, 2023.\n[33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,\nDan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya\nChen, Guillem Cucurull, David Esiobu, Jude Fernan-\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,\nVedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu,\nYuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Sub-\nramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie\nKambadur, Sharan Narang, Aurelien Rodriguez, Robert\nStojnic, Sergey Edunov, and Thomas Scialom. Llama 2:\nOpen foundation and fine-tuned chat models, 2023.\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances\nin neural information processing systems, 30, 2017.\n[35] Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng,\nShiyang Chen, Arash Bakhtiari, Michael Wyatt, Yux-\niong He, Olatunji Ruwase, Leon Song, and Zhewei Yao.\nZeroquant(4+2): Redefining llms quantization with a\nnew fp6-centric strategy for diverse generative tasks.\narXiv preprint arXiv: 2312.08583, 2023.\n[36] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang,\nZhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and\nShuaiwen Leon Song.\nFlash-llm: Enabling cost-\neffective and highly-efficient large generative model\ninference with unstructured sparsity. Proc. VLDB En-\ndow., 17(2):211\u2013224, oct 2023.\n[37] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang,\nZhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and\nShuaiwen Leon Song. Flash-llm github. \"https://\ngithub.com/AlibabaResearch/flash-llm\", 2023.\n[38] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,\nJulien Demouth, and Song Han. Smoothquant: Accu-\nrate and efficient post-training quantization for large\nlanguage models, 2023.\n[39] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and\nYuxiong He. Zeroquant-v2: Exploring post-training\nquantization in llms from comprehensive study to low\nrank compensation. arXiv preprint arXiv:2303.08302,\n2023.\n14\n[40] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and\nYuxiong He. Zeroquant-v2: Exploring post-training\nquantization in llms from comprehensive study to low\nrank compensation, 2023.\n[41] Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\nand Luke Zettlemoyer. Opt: Open pre-trained trans-\nformer language models, 2022.\n[42] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn\nChen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,\nTianqi Chen, and Baris Kasikci. Atom: Low-bit quanti-\nzation for efficient and accurate llm serving, 2023.\n[43] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan\nWang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang,\nYang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex:\nA pre-trained model for code generation with multilin-\ngual evaluations on humaneval-x. In KDD, 2023.\n[44] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping\nWang. A survey on model compression for large lan-\nguage models. arXiv preprint arXiv:2308.07633, 2023.\n15\n"
  },
  {
    "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
    "link": "https://arxiv.org/pdf/2401.14405.pdf",
    "upvote": "11",
    "text": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\nModalities\nYiyuan Zhang1\nXiaohan Ding2\nKaixiong Gong1\nYixiao Ge2\nYing Shan2\nXiangyu Yue1*\n1MMLab, The Chinese University of Hong Kong\n2Tencent AI Lab\nyiyuanzhang.ai@gmail.com, xiaohding@gmail.com, xyyue@ie.cuhk.edu.hk\nhttps://ailab-cvc.github.io/M2PT/\nAbstract\nWe propose to improve transformers of a specific modal-\nity with irrelevant data from other modalities, e.g., improve\nan ImageNet model with audio or point cloud datasets. We\nwould like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distin-\nguishes our method from other works utilizing paired (e.g.,\nCLIP) or interleaved data of different modalities. We pro-\npose a methodology named Multimodal Pathway - given a\ntarget modality and a transformer designed for it, we use an\nauxiliary transformer trained with data of another modal-\nity and construct pathways to connect components of the\ntwo models so that data of the target modality can be pro-\ncessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers\nobtained from two modalities. As a concrete implementa-\ntion, we use a modality-specific tokenizer and task-specific\nhead as usual but utilize the transformer blocks of the aux-\niliary model via a proposed method named Cross-Modal\nRe-parameterization, which exploits the auxiliary weights\nwithout any inference costs. On the image, point cloud,\nvideo, and audio recognition tasks, we observe significant\nand consistent performance improvements with irrelevant\ndata from other modalities. The code and models are avail-\nable at https://github.com/AILab-CVC/M2PT.\n1. Introduction\nTransformers [12, 14, 36, 37] are widely adopted in var-\nious tasks across modalities, such as text classification [8],\nobject detection [3], point cloud analysis [47], and au-\ndio spectrogram recognition [16]. Apart from numerous\nunimodal tasks, transformers are also effective on multi-\nmodal data, e.g., CLIP [32] uses image-text pairs to achieve\n*Corresponding Author\nsuperior performance in image recognition.\nTransform-\ners\u2019 success in multiple modalities demonstrates their abil-\nities to universally establish sequence-to-sequence model-\ning, given the input sequences (i.e., tokens) which can be\nseen as the universal embeddings of data of multiple modal-\nities [3, 12, 16, 46, 47]. For brevity, we refer to such ability\nas the universal modeling ability.\nWe would like to note that CLIP [32] represents the sig-\nnificant success of a methodology that improves a model\u2019s\nperformance on a certain modality (i.e., image) with the\nhelp of data from another modality (i.e., text), but the lim-\nitation is also apparent - the data samples from the two\nmodalities must be relevant (e.g., paired, in this case).\nThis limitation seems so inevitable that it hardly attracts\nresearch interest from the literature. Taking another two\nmodalities, image and audio, as an example, we may expect\nthat training with image-audio pairs may help the model\nrecognize images (if we build a dataset with enough image-\naudio pairs and re-design the model to use the audio la-\nbels as the supervision, just like CLIP does with image-\ntext pairs), but it seems hard to believe that a pure audio\ndataset would improve a model\u2019s performance on Ima-\ngeNet classification without any relevance between the\naudio and image samples.\nIn this paper, we propose to improve a transformer\u2019s per-\nformance on a certain modality even with irrelevant data\nfrom another modality, as shown in Figure 1. The motiva-\ntion is that we can see a training process on a certain modal-\nity as converting the data of the modality to sequences (i.e.,\ntokens) and establishing sequence-to-sequence mappings\nwith the transformer blocks. For a specific modality, we\nreckon that the trained model has knowledge encoded in the\nsequence-to-sequence modeling that can facilitate another\nmodeling process whose input sequences are obtained from\nanother modality. In other words, apart from the obvious\nmodality-specific knowledge acquired through training on\na specific modality, we seek the modality-complementary\nknowledge of sequence-to-sequence modeling in trans-\nformers and will show that it does exist.\n1\narXiv:2401.14405v2  [cs.CV]  18 Mar 2024\nIrrelevant Data\nRelevant data\nLAION-400M\nLAION-2B\nLAION-5B\n21K train set\n(Irrelevant datasets of multiple modalities)\n(ours)\nSingle \nModality\nMultiple\nModalities\n(Transfer a model to another task of the same modality)\n(Train and test model on the same or a similar task)\n1K train set\n(Paired or interleaved multi-modal data\ne.g., CLIP, Flamingo, BLIP)\nval set\ndetection\nsegmentation\nclassification\nvideo\nimage\npoint cloud\naudio\nFigure 1. Compared to the known paradigms which use well-aligned multimodal data, we focus on scenarios where the data samples are\nfrom multiple modalities but irrelevant, which is an open problem in the literature.\nHowever, given a target modality, it seems difficult to\ndesign the model to utilize some irrelevant data of another\nmodality because the data samples of different modalities\n(e.g., image and audio) may vary significantly in the se-\nmantics, data format, preprocessing, and it seems hardly\npossible to design a reasonable objective function since\nthere is no relevance between any two samples.\nIn this\npaper, we solve this problem by not directly mixing train-\ning data of two modalities but seeing a model trained on a\nspecific unimodal dataset as a proxy of the corresponding\nmodality and using the model instead. Specifically, given\na target modality and an auxiliary modality, we propose a\nframework named Multimodal Pathway to improve the per-\nformance on the target modality by using two transform-\ners respectively trained with the unimodal data of the two\nmodalities. We construct pathways across the components\nof the target and auxiliary models to exploit the modality-\ncomplementary knowledge encoded in the latter to help the\nformer. Note pathway is an abstract concept that may re-\nfer to any connection between the two models. We name\nthe model as Multimodal Pathway Transformer (M2PT)\nfor brevity.\nThis paper proposes a simple yet effective implementa-\ntion of M2PT, where the key is the concrete implementa-\ntion of pathways that connect the two models. As discussed\nabove, thanks to the universal modeling ability, transform-\ners on different modalities may have different tokenizers,\nbut their main bodies (i.e., transformer blocks) may have\nthe same structure. 1 For a target model and an auxiliary\n1Except for transformers, a recent work, UniRepLKNet [11], reveals\nmodel with the same structure as the main bodies, a layer\nin the main body of the former should have a counterpart in\nthe latter. For example, the counterpart of the Query layer\nin the 9th block of the target model, which is the 9th Query\nlayer in the auxiliary model, should exist, and they play a\nsimilar role in the two models. Considering this, we build\nthe connections between the two models by augmenting ev-\nery linear layer in the transformer blocks of the target model\nwith its counterpart in the auxiliary model. In such a con-\nceptual design, we let the two layers take the same inputs\nand add up their outputs, as shown in Figure 2 (middle).\nHowever, considering the budget on compute and la-\ntency, we desire an implementation of the Multimodal Path-\nway that realizes the pathways and makes good use of the\nauxiliary model but brings only marginal training cost and\ncompletely no inference cost, compared to a regular model\ntrained on the target modality. We note that the conceptual\nstructure described above can be equivalently implemented\nby a re-parameterization method, which equivalently con-\nverts the connections between model structures (i.e., linear\nlayers) into connections between the two models\u2019 weights.\nSpecifically, we construct a pathway for each target linear\nlayer by adding the corresponding weights of its counter-\npart in the trained auxiliary model scaled by a learnable\nmultiplier that indicates the strength of the pathway, so that\nthe method is named Cross-Modal Re-parameterization. A\nConvNets also effectively handle embeddings extracted from different\nmodalities with the same architecture (akin to transformers universally tok-\nenizing and processing data of multiple modalities), achieving state-of-the-\nart performances in tasks including global weather forecasting and audio\nrecognition.\n2\nMultimodal Pathways Transformer (M2PT)\nImplemented Structure   with Cross-Modal Re-parameterization\nTokenizer\nTransformer Blocks\nHead\nPathways\nAuxiliary Modality\nTarget Modality\nQ\nK\nV\nProj\nAttention\nTokens\nstructure\n=\nformulation\nequivalent\n+\nNorm\nNorm\n+\nFC\nFC\n+\nparameterization\nFC\nFC\u2019\nFC\nConceptual Structure    =\nQ\nK\nV\nQ\u2019\nK\u2019\nV\u2019\nProj\nProj\u2019\nAttention\nTokens\nNorm\nNorm\n+\nFC\nFC\u2019\nFC\nFC\u2019\n+\n=\nFigure 2. (Left) Framework of Multimodal Pathway Transformer (M2PT). We use point cloud and image modalities as an example.\nCommon practices with transformers follow the same pipeline: using 1) tokenizers to convert the input data to sequences, 2) transformer\nblocks to process the sequences, and 3) heads to decode the sequences. We upgrade the sequence-to-sequence modeling by establishing\npathways between the components of different modalities so processing the tokens of a specific modality can utilize the transformer\nblocks trained with another modality. (Middle) Conceptual design of M2PT, where the pathways are implemented by letting a linear layer\n(including the Query/Key/Value/projection layers in the attention block and those in the FFN block) in the target model cooperate with its\ncounterpart in the auxiliary model. (Right) Cross-Modal Re-parameterization efficiently realizes M2PT by re-parameterizing the weights\nof the target model with those of the auxiliary model, introducing marginal training costs and completely no inference costs.\nIN-1K\nK400\nPartNet\nAudioset\n+0.4%\n+0.7%\n+0.4%\n+1.0%\n+1.5%\n+0.9%\n+3.8%\n+0.8%\n+0.6%\n+5.7%\n+1.0%\n+0.8%\nTop-1 Acc. Top-1 Acc.\nmIoU\nTop-1 Acc.\nAuxiliary \nTarget\nFigure 3. Consistent improvements brought by M2PT across each\npair of four modalities - image, video, point cloud, and audio. The\nmetrics are ImageNet-1K accuracy, Kinetics-400 accuracy, Part-\nNet mIoU, and AudioSet accuracy, respectively. The numbers rep-\nresent the percentage of improvement of M2PT models relative to\nthe performance of baseline models that are pretrained with MAE-\nstyle methods [22, 23, 30, 49] on the four modalities, respectively.\nsignificant strength of re-parameterization is that the extra\ntraining costs are marginal (i.e., the re-parameterized model\nwill have the same number of linear layers as the original\nmodel, and each linear layer merely needs to compute the\nsum of two weight matrices before projecting the inputs)\nand we can merge the weights after training so that the\nstructure and number of parameters of the resultant model\nwill be identical to a regular model.\nWe experimented with the image, video, point cloud, and\naudio modalities. Figure 3 shows the relative improvements\nM2PT consistently brings among four modalities. Such re-\nsults reveal that the modality-complementary knowledge of\nsequence-to-sequence modeling in transformers does exist.\nAs an early exploration, our empirical studies confirm that\nsuch improvements are not solely due to the more parame-\nters, and suggest that such modality-complementary knowl-\nedge may be related to the ability to generally process hi-\nerarchical representations. Abstraction hierarchy exists in\nmultiple modalities with concepts ranging from low-level\nto high-level, which may explain the universality of the\nlearned knowledge. In other words, as a transformer is be-\ning trained with images, it learns both (ability A) how to\nunderstand images and (ability B) how to generally trans-\nform the tokens from the lower-level patterns to a higher\nlevel without assuming they originally come from images.\nMeanwhile, as another transformer is being pretrained with\naudio data, it learns both a different \u201cability A\u201d for audio\nand a similar \u201cability B\u201d, so that it can help the aforemen-\ntioned transformer in image recognition.\nIn summary, our contributions are as follows:\n\u2022 We propose Multimodal Pathway, which is a framework\nto improve transformers via exploiting models trained on\nother modalities.\n\u2022 We propose an inference-cost-free implementation of\nMultimodal Pathway, which is named Cross-Modal Re-\nparameterization.\n\u2022 Multimodal Pathway represents an early exploration in\nthis direction, which offers a novel perspective. We re-\n3\nalize significant and consistent improvements in four rep-\nresentative modalities, which demonstrates the potential\nof our method as a promising approach.\n2. Related Work\nUnimodal pretraining. The evolution of unimodal pre-\ntraining paradigms has transitioned from supervised to self-\nsupervised paradigms. For instance, Devlin et al. [8] intro-\nduced the mask-reconstruction paradigm and achieved re-\nmarkable outcomes. At that time, visual pretraining largely\nemphasized contrastive learning [4, 6, 21]. Subsequently,\nleveraging the vast amounts of unlabeled data, the BERT\nparadigm gained traction and pioneers like MAE [22] suc-\ncessfully applied it to visual pretraining, while others [16,\n30, 35, 46] extended this paradigm to areas like point cloud,\naudio, and video perception.\nWe use MAE-style unimodal pretraining methods to ob-\ntain the weights on each modality for simplicity. We do\nnot use supervised pretraining because we would like to en-\nsure that two unimodal datasets are completely irrelevant\nby avoiding using labels, considering that the labels of two\ndatasets may somehow overlap.\nMultimodal pretraining.\nExisting multimodal learning\nmethods require paired [19, 39, 40, 50] or interleaved\ndata [1]. In either case, the data samples of different modal-\nities are well-aligned (i.e., strongly related). A recent study\nhighlighted a main trend in the literature - existing mul-\ntimodal pretraining methods are overly dependent on the\nwell-aligned multimodal sample pairs/tuples [43]. For in-\nstance, VideoBERT [34] and CBT [33] utilize well-aligned\nvideo and speech data;\nNowadays, using the weakly-aligned or unpaired/u-\nnaligned multimodal data as the pretraining corpora re-\nmains understudied [43]. This work represents an early ex-\nploration in this direction, which serves to fill this gap in the\nfield and contributes to multimodal calibration [38].\nStructural Re-parameterization is a methodology that\nconstructs extra structures (e.g., convolutional layers) dur-\ning training and converts the trained structures via trans-\nforming the parameters [9\u201311].\nA primary drawback of\nStructural Re-parameterization is that the constructed layers\nmust participate in the computations with the inputs, result-\ning in significant extra training costs.\nIn contrast, Cross-Modal Re-parameterization is a sim-\nple re-parameterization method that is more efficient than\nStructural Re-parameterization. Specifically, the extra com-\nputation of each re-parameterized layer in the forward com-\nputation adds up two weight matrices,\n3. Method\n3.1. Architectural Design\nWe design a transformer for a specific modality as three\nmodules - the modality-specific tokenizer, the modality-\nagnostic transformer blocks, and the modality-specific\nhead. We assume the dimension of tokens is D, which is\na pre-defined architectural hyper-parameter, and describe\nhow to tokenize the input data of multiple modalities into\nD-dimensional tokens.\nImage tokenizer.\nWe represent an image by xI\n\u2208\nRH\u00d7W \u00d7C, where (H, W) specifies the image\u2019s resolution,\nand C is the number of channels. With an image patch of\n(S, S), we obtain:\nxI \u2208 RH\u00d7W \u00d7C \u2192 x\u2032\nI \u2208 R\nHW\nS2 \u00d7D .\n(1)\nVideo tokenizer. Analogous to 2D images, we use video\npatches as the basic units for learning video representations.\nGiven an N-frame video x \u2208 RN\u00d7H\u00d7W \u00d7C, similar to im-\nages, we use an S\u00d7S embedding layer so that\nxV \u2208 RN\u00d7H\u00d7W \u00d7C \u2192 x\u2032\nV \u2208 R\nNHW\nS2\n\u00d7D .\n(2)\nFollowing ViT [12], we use S = 16 by default.\nPoint cloud tokenizer. Given a point cloud X = {xi}P\ni=1\ncomprising P points, each point xi is defined as xi =\n(pi, fi), where pi \u2208 R3 denotes the 3D coordinates and\nfi \u2208 Rc encodes the attributes, e.g., color, viewpoint, nor-\nmal, etc. We use the Farthest Point Sampling to sample a\nrepresentative skeleton from the original points at a fixed\nsampling ratio of 1/4, then K-Nearest Neighbor method\nto group proximate points. Then we model the geomet-\nric relevance by constructing an adjacency matrix R\nP\n4 \u00d7 P\n4\nbetween each pair of groups, which is then projected into\nD-dimensional tokens. That is\nxP \u2208 RP \u00d7(3+c) \u2192 x\u2032\nP \u2208 R\nP\n4 \u00d7 P\n4 \u2192 x\u2032\u2032\nP \u2208 R\nP\n4 \u00d7D . (3)\nAudio spectrogram tokenizer. Let T and F be the num-\nbers of time frames and frequency bins, we use xA \u2208 RT \u00d7F\nto represent a sample. Analogous to 2D images, we see an\naudio sample as a single-channel image and use a similar\nembedding layer so that\nxA \u2208 RT \u00d7F \u2192 x\u2032\nA \u2208 R\nT F\nS2 \u00d7D.\n(4)\nIn our AudioSet experiments, we have T=F=128, S=16.\nTransformer blocks. We adopt the structural design of the\ntransformer blocks in Vision Transformer (ViT) [12], where\neach transformer block comprises a self-attention block and\na Feed-Forward Network (FFN) block. The linear layers\ninclude the Query/Key/Value/projection layers in the atten-\ntion block and two layers in the FFN block. For fairness\nand reproducibility, we use the same architectural hyper-\nparameters (e.g., dimension of tokens, number of blocks,\nand number of heads) as ViT-Base for every M2PT model\non every modality.\n4\n3.2. Cross-Modal Re-parameterization\nFor an M2PT model on a specific modality, we use Cross-\nModal Re-parameterization in the transformer blocks to\nutilize another model\u2019s weights trained on another modal-\nity.\nSpecifically, let \u03b8 be an arbitrary trainable parame-\nter of a layer in the transformer, x be the input, and y be\nthe output, we use f to denote the operation so that y =\nf(x; \u03b8). With Cross-Modal Re-parameterization, we simply\nre-parameterize the layer with parameters of its counterpart\nin another modal that is trained on another modality. Let \u03b8\u2032\nbe the parameter of the counterpart, the operation becomes\ny = f(x; \u03b8 + \u03bb\u03b8\u2032) .\n(5)\nWe refer to \u03bb as the Cross-Modal Scale and \u03b8\u2032 as the Cross-\nModal Parameter. After training, we merge the model by\ncomputing and saving \u02c6\u03b8 = \u03b8 + \u03bb\u03b8\u2032 so that the model will\nno longer have extra parameters and the inference costs and\nmodel size will be identical to a regular model.\nWith Cross-Modal Re-parameterization, we equivalently\nrealize the proposed M2PT transformer block with marginal\ntraining costs and completely no inference costs. For a lin-\near layer whose parameters form a matrix W \u2208 RDin\u00d7Dout\nand the inputs and outputs are matrices x \u2208 RB\u00d7Din and\ny \u2208 RB\u00d7Dout. We omit the bias term for brevity and the\noriginal operation is formulated by\ny = xW .\n(6)\nAs described in the conceptual structure depicted in Fig-\nure 2, the linear layer and its counterpart take the same in-\nput. The output will be\ny = xW + \u03bb(xW \u2032) .\n(7)\nNote\nxW + \u03bb(xW \u2032) = x(W + \u03bbW \u2032) ,\n(8)\nso that the two layers can be equivalently implemented by\na single layer that has a trainable scalar \u03bb and an additional\ntrainable matrix which is initialized with the counterpart in\nthe auxiliary model. Both the original weight matrix and the\nadditional one are trainable. At each forward computation,\nthe layer computes the equivalent weight matrix and then\nuses it to project the input, which is\ny = x(W + \u03bbW \u2032) .\n(9)\nAfter training, we merge the parameters by computing\n\u02c6\nW = W +\u03bbW \u2032 and save it only. For inference, we simply\nconstruct a regular linear layer and load \u02c6\nW .\nIn summary, to construct and use an M2PT with Cross-\nModal Re-parameterization, we\n\u2022 Construct the tokenizer and head according to the target\nmodality.\n\u2022 Construct the transformer blocks with Cross-Modal Re-\nparameterization. For each linear layer, except for the\noriginal weight matrix, we add an extra trainable weight\nmatrix and initialize it with the corresponding one from a\ntransformer trained on the auxiliary modality, and add a\ntrainable scalar parameter initialized with 0.\n\u2022 Train the re-parameterized cross-modal model just like\nwe train a regular model.\n\u2022 After training, convert the trained model and save the con-\nverted one for inference.\n4. Experiments\n4.1. Setup\nDatasets.\nFor image recognition, we evaluate the mod-\nels\u2019 performance on three representative image datasets.\n1) ImageNet-1K [7] contains nearly 1.3 million images\nof 1000 categories.\n2) MSCOCO 2017 [27] is a com-\nmon benchmark for object detection. M2PT is trained on\nthe train set and evaluated on the val set with Mask\nRCNN [20]. 3) ADE-20K [48] is used for semantic seg-\nmentation experiments with UperNet [41] and we adopt the\nsingle-scale evaluation setting. For point cloud, we evalu-\nate the performance of M2PT on ShapeNetPart [44], which\ncontains 16,880 models and 16 categories. For audio recog-\nnition, following AudioMAE [23], we utilize the AudioSet-\n2k [15] dataset. For video, we experiment on the action\nrecognition dataset, Kinetics-400 [24], which contains 240k\ntraining videos and 20k validation videos from 400 classes.\nExperimental details.\nFor a pair of target modality\nand auxiliary modality, we obtain the auxiliary model\nby self-supervised training on a dataset of the auxiliary\nmodality. Specifically, the auxiliary image model is pre-\ntrained with MAE [22] on ImageNet-1K [7], the auxil-\niary point cloud model is pretrained with Point-MAE [30]\non ShapeNet [5], the auxiliary audio model is pretrained\nwith AudioMAE [23] on AudioSet-2M [15], the auxiliary\nvideo model is pretrained with VideoMAE [35] on Kinetics-\n700 [24]. We do not use supervised pretraining because we\nwould like to eliminate the effects of labels in the pretrain-\ning datasets so that we can ensure the irrelevance of the data\nsamples, considering that the labels of two datasets may\nsomehow overlap. In terms of the initialization of the target\nmodel, we adopt two settings. 1) The target model (i.e., the\nparameters denoted by W in Eq. 9) is initialized with the\naforementioned weights pretrained with the self-supervised\nmethods on the target modality.\nWe finetune the M2PT\nmodel with the default finetuning configurations described\nby the corresponding pretraining methods.\nThe baseline\nmodel is also initialized with the pretrained weights and\nfine-tuned with identical configurations so that this setting\nis referred to as the pretrained setting for brevity. 2) The\n5\nTable 1. Experimental results on image recognition tasks. On ImageNet, we report the results with the linear layers in transformer\nblocks finetuned (tune acc) or fixed (fix acc). The architecture of every model is ViT-B. The relative improvements over the baselines are\nshown in green. * The standard error of M2PT on image recognition tasks is 0.04.\nMethod\nImageNet\nMS COCO\nADE20K\ntune acc(%)\nfix acc(%)\nAPbox(%)\nAPmask(%)\nmIOU(%)\nPretrained setting\nSemMAE[25]\n83.4\n65.0\n-\n-\n46.3\nMFF [28]\n83.6\n67.0\n48.1\n43.1\n47.9\nMAE\u2217[22]\n83.3\n65.6\n47.3\n42.4\n46.1\nM2PT-Video (Ours)\n83.6 \u2191 0.4%\n67.1 \u2191 2.3%\n-\n-\n-\nM2PT-Audio (Ours)\n83.7 \u2191 0.4%\n67.3 \u2191 2.6%\n-\n-\n-\nM2PT-Point (Ours)\n83.9 \u2191 0.7%\n67.8 \u2191 3.4%\n50.0 \u2191 5.7%\n44.0 \u2191 3.8%\n47.9 \u2191 3.9%\nFrom-scratch setting\nViT [12]\n76.5\n14.5\n46.2\n40.5\n39.7\nM2PT-Point (Ours)\n81.9 \u2191 7.1%\n19.5 \u2191 34.5%\n48.9 \u2191 5.8%\n42.2 \u2191 4.2%\n42.5 \u2191 7.1%\ntarget model is initialized as usual, and we use the widely\nadopted training configurations to train the M2PT model.\nThe baseline model is trained from scratch with identical\nconfigurations for fair comparisons so that the setting is re-\nferred to as the from-scratch setting for brevity.\nMetrics.\nWe report the performance of M2PT mod-\nels on various datasets, including top-1 accuracy for\nImageNet-1K, AudioSet, Kinetics-400, mIoU for ADE20K,\nShapeNetPart and PartNet, and box/mask AP for MS\nCOCO. To fairly assess the performance improvements over\nthe baselines in multiple metrics, we also report the relative\npercentage of improvement in Table 1, 2, 3, and 4.\n4.2. Main Results\nImage recognition. We first conduct a group of experi-\nments under the pretrained setting, where the target weights\nare initialized with a ViT pretrained with MAE on Ima-\ngeNet, and the auxiliary weights are from the models pre-\ntrained on video, audio, and point datasets, respectively.\nSuch three models, which are labeled as M2PT-Video,\nM2PT-Audio, and M2PT-Point, respectively, and the base-\nline (the original MAE-pretrained ViT) are trained on Ima-\ngeNet with the finetuning configurations originally adopted\nby MAE [22], and the resultant accuracies are reported in\nthe \u201ctune acc\u201d column in Table 1. Then we transfer the best-\nperforming model, which is M2PT-Point, to COCO object\ndetection and ADE20K semantic segmentation tasks. The\nimprovements are significant: the ImageNet accuracy im-\nproves from 83.3 to 83.9, the COCO box AP improves from\n47.3 to 50.0, and the ADE20K mIoU improves from 46.1\nto 47.9, so the relative improvements are 0.7%, 5.7%, and\n3.9%, respectively.\nApart from finetuning the target and auxiliary weights,\nwe test another setting where the parameters of linear\nweights in transformer blocks are fixed, and only the Cross-\nModal Scales together with the classifier are trainable. The\naccuracies are reported in the \u201cfix acc\u201d column.\nNatu-\nrally, under this setting, the baseline should be the MAE-\npretrained ViT where only the classifier is trainable. Im-\npressively, the relative improvement becomes more signifi-\ncant (65.6\u219267.8 so that the relative improvement is 3.4%),\ndemonstrating that the weights obtained from the auxiliary\nmodality work on another modality, even if the weights are\nfixed. We would like to note MAE is a powerful pretraining\nmethod, and it is challenging to gain further improvements\non top of MAE. Some insightful recent methods [25, 28]\nimproved MAE but our results are more significant.\nOn the other hand, under the from-scratch setting, the\nbaseline is a ViT trained from scratch, and the target weights\nof M2PT are also randomly initialized. The accuracy is\ndrastically improved from 81.9 to 76.5 so the relative im-\nprovement is 7.1%, suggesting the auxiliary weights signif-\nicantly facilitate the training process. Intuitively, the Cross-\nModal Scales are initialized with 0 but will soon become\nnon-zero as the training proceeds so the model will be grad-\nually influenced by the auxiliary weights and benefit from\nthe modality-complementary knowledge. When we trans-\nfer such two models to COCO and ADE20K, we observe\nconsistent improvements in the box AP and mIoU.\n3D point cloud understanding. Table 2 presents the ex-\nperimental results on ShapeNetPart and PartNet datasets,\nwhere we compare M2PT with existing point cloud pre-\ntraining methods such as Point-BERT [30] and Point-\nMAE [45]. M2PT consistently improves the class mIoU\nfrom 84.2 to 85.6 and instance mIoU from 86.1 to 87.5 on\nShapeNetPart and raises the mIoU from 47.4 to 50.1 on\nPartNet. Under the from-scratch setting, we also observe\nconsistent improvements.\nAudio recognition.\nFor the pretrained setting, the tar-\n6\nTable 2. Experimental results on point cloud datasets. We re-\nport the class mIoU (mIoUC) and instance mIoUI on ShapeNet-\nPart and mIoU on PartNet. The relative improvements over the\nbaselines are shown in green.\nMethod\nShapeNetPart\nPartNet\nmIoUC (%)\nmIoUI (%)\nmIoU (%)\nPretrained setting\nPointNet++ [31]\n81.9\n85.1\n42.5\nPoint-BERT [45]\n84.1\n85.6\n-\nPoint-MLP [29].\n84.6\n86.1\n48.1\nPoint-MAE [45]\n84.2\n86.1\n47.4\nM2PT-Video\n85.6 \u2191 1.7%\n87.5 \u2191 1.6%\n50.1 \u2191 5.7%\nM2PT-Image\n85.6 \u2191 1.7%\n87.5 \u2191 1.6%\n49.2 \u2191 3.8%\nM2PT-Audio\n85.6 \u2191 1.7%\n87.5 \u2191 1.6%\n48.1 \u2191 1.5%\nFrom-scratch setting\nN/A\n50.2\n68.4\n-\nM2PT-Video\n50.8 \u2191 1.2%\n68.8 \u2191 0.6%\n-\nTable 3. Experimental results on AudioSet-2k. The relative im-\nprovements over the baselines are shown in green.\nMethod\nModel\nTop-1 Acc. (%)\nPretrained setting\nPSLA [17]\nCNN+Trans\n31.9\nAST [16]\nViT-B\n34.7\nSSAST [18]\nViT-B\n31.0\nAudioMAE [23]\nViT-B\n35.3\nM2PT-Point\nViT-B\n35.6 \u2191 0.8%\nM2PT-Video\nViT-B\n35.5 \u2191 0.6%\nM2PT-Image\nViT-B\n35.6 \u2191 0.8%\nFrom-scratch setting\nN/A\nViT-B\n11.0\nM2PT-Point\nViT-B\n11.4 \u2191 3.6%\nTable 4. Experimental results on Kinetics-400. The relative im-\nprovements over the baselines are shown in green\nMethod\nModel\nTop-1 Acc. (%)\nSlowFast-101 [13]\nResNet-101\n79.8\nMViTv2-B [26]\nViT-B\n81.2\nTimeSFormer [2]\nViT-B\n80.7\nVideoMAE [35]\nViT-B\n81.5\nM2PT-Point\nViT-B\n82.3 \u2191 1.0%\nM2PT-Image\nViT-B\n82.2 \u2191 0.9%\nM2PT-Audio\nViT-B\n82.3 \u2191 1.0%\nget weights are initialized with an AudioMAE-pretrained\nmodel. As shown in Table 3, we compare M2PT with exist-\ning competitive methods including SSAST [18], AST [16],\nand AudioMAE [23]. M2PT improves the top-1 accuracy\nby 0.8% relatively on the Audioset balanced split, demon-\nstrating that M2PT is also effective in audio recognition.\nUnder the from-scratch setting, M2PT brings out a relative\nimprovement of 3.6%.\nVideo understanding. For the experiments on Kinetics-\n400, we adopt only the pretrained setting because it is not a\ncommon practice to train a model from scratch on a video\ndataset, which would deliver inferior performance.\nWe\nuse the Video-MAE-pretrained ViT to initialize the target\nweights. Naturally, the baseline should be the VideoMAE-\npretrained model directly finetuned on Kinetics-400. Ta-\nble 4 shows that compared with previous works including\nSlowFast [13], MViTv2 [26], TimeSFormer [2], and Video-\nMAE [35], M2PT outperforms by at least +0.8 top-1 accu-\nracy (82.3 vs. 81.5), which reveals that the temporal aware-\nness for video understanding can also be enhanced with ir-\nrelevant data from other modalities.\n4.3. Ablation Studies\nAs shown in Table 5, we evaluate the design choices of\nM2PT separately through a group of ablation studies under\nthe pretrained setting on ImageNet and the auxiliary modal-\nity is the point cloud. We make the following observations.\n1) Applying Cross-Modal Re-parameterization to every\nlinear layer delivers the best performance. In each trans-\nformer block, we may choose to apply our method to any\nof the Query/Key/Value/projection layers in the attention\nblock and the two linear layers in the FFN. Table 5 shows\nchanging any one of the layers brings improvements, and\nthe best result is achieved by changing them all.\n2) Cross-Modal Scale should be initialized with 0. By\ndefault, we initialize the Cross-Modal Scale \u03bb with 0 for\nevery layer. We observe that initializing it to a higher value\ndegrades the performance, suggesting that the initial state\nof the M2PT should be identical to the target weights (i.e.,\nthe weights pretrained with MAE, in this case).\n3) Cross-Modal Scale should be learnable. Fixing the\nCross-Modal Scale degrades the performance, suggesting\nit is important to let the model learn how to combine the\ntarget weights and the corresponding auxiliary weights.\n4.4. Empirical Discussions\n4.4.1\nOn the Modality-Complementary Knowledge\nThe observed improvements on multiple modalities have\nshown that the auxiliary transformer has learned some\nknowledge that can be transferred to the target modality.\nWe continue to investigate the properties of such modality-\ncomplementary knowledge through two groups of experi-\nments (Table 6).\n1) Modality-complementary knowledge & Abstraction\nHierarchy. Vision Transformers excel in general hierar-\nchical representations by stacking blocks [12]. For exam-\nple, in the image and point cloud modalities, this hierar-\nchy may include textures (in images) or individual points\n(in point clouds), object parts, and whole objects. In Ta-\nble 6, we construct the multimodal pathway by connect-\ning transformer blocks of different depths.\nSpecifically,\nthe counterpart of the first target block should be the first\n7\nTable 5. Ablation studies on design choices of M2PT including the layers to re-parameterize and configurations of Cross-Modal Scale \u03bb.\nWe use the point cloud and video as auxiliary modalities for image and 3D evaluation. The first row reports the results of direct tuning.\nMultimodal Pathway Components\nCross-Modal Scale\nImageNet\nShapeNetPart\nPartNet\nAttn QKV\nAttn Proj\nFFN 1st\nFFN 2nd\nInit.\nTrainable\n(%)\n(%)\n(%)\n-\n-\n83.3\n84.2/86.1\n47.4\n\u2714\n0\n\u2714\n83.4\n84.6/86.5\n48.3\n\u2714\n0\n\u2714\n83.6\n84.8/87.1\n48.2\n\u2714\n0\n\u2714\n83.6\n84.9/87.0\n48.4\n\u2714\n0\n\u2714\n83.7\n85.2/87.2\n48.3\n\u2714\n\u2714\n\u2714\n\u2714\n0\n\u2714\n83.9\n85.6/87.5\n50.1\n\u2714\n\u2714\n\u2714\n\u2714\n10\u22122\n\u2718\n83.5\n84.6/86.3\n48.2\n\u2714\n\u2714\n\u2714\n\u2714\n10\u22122\n\u2714\n83.6\n84.3/86.2\n48.0\n\u2714\n\u2714\n\u2714\n\u2714\n10\u22124\n\u2714\n83.6\n84.7/86.2\n48.1\n\u2714\n\u2714\n\u2714\n\u2714\n10\u22126\n\u2714\n83.7\n84.7/86.4\n48.2\nTable 6.\nImageNet accuracy with changed order of auxiliary\nweights or fewer pretraining epochs.\nOrder of aux weights\nEpochs pretrained\nTop-1 acc\nNormal\n20\n83.55\nNormal\n220\n83.69\nNormal\n300\n83.93\nReversed\n300\n83.61\nTable 7. Training efficiency of Multimodal Pathway.\nModel\nTrain Time\nTrain Param.\nInference Time\nInference Param.\nMAE\n16.95 Hours\n86.3M\n11.64 ms\n86.3M\nM2PT\n22.84 Hours\n172.6M\n11.64ms\n86.3M\nauxiliary block. Under the reverse-order setting, we ob-\nserve that doing so decreases the accuracy to 83.61%, which\nis 0.32% lower than the normal M2PT. We observe that\nmodality-complementary knowledge in the auxiliary trans-\nformer can transfer to another modality but can be harmed\nif the low-to-high correspondence is interrupted, suggesting\nthat modality-complementary knowledge reinforces hierar-\nchical representations of the transformer architecture.\n2) More trainable parameters?\nJust better initializa-\ntion? For this part, we use insufficiently pretrained aux-\niliary weights. Specifically, the default auxiliary weights\nare pretrained for 300 epochs with mask modeling on point\ncloud data, but we alternatively use the checkpoints saved\nat the 20th and 220th epoch, respectively, as the auxiliary\nweights. Not surprisingly, we observe that the performance\ndegrades to 83.55% and 83.69%, respectively, which is still\nhigher than the baseline. This phenomenon suggests that\nthe improvements brought by the auxiliary weights cannot\nbe explained as better initialization, because after pretrain-\ning the auxiliary model from 20 to 300 epochs, the accuracy\nincreases from 83.5 to 83.9. If improvements were due to\ninitialization, the results of pretraining 20 epochs should be\nclose to random initialization (83.5 v.s. 81.9).\n4.4.2\nDiscussion on the Data Scale\n1) From small-scale data to large-scale data.\nPrevi-\nous works such as Image2Point [42] introduces image-\npretrained models to data-insufficient 3D perception tasks.\nDifferently, M2PT sets up a brand new methodology and\nbreaks the former consensus - we discover that even though\nthe data scale of point clouds is limited, such data still\nbrings out impressive improvements to the image, video,\nand audio perception tasks. Impressively, the pretraining\ndata of the latter modalities is larger in magnitude than that\nof the point cloud, but the point cloud data makes a differ-\nence. 2) From large-scale data to small-scale data. On\nthe other hand, the effectiveness of M2PT highlights that\nfor 3D vision research and other areas that lack large-scale\ndata for pretraining, M2PT introduces a promising direction\nto leverage irrelevant large-scale data from other modalities.\n5. Conclusion and Limitation\nThis paper explores the feasibility and advantages of im-\nproving a transformer\u2019s performance on a specific modal-\nity with irrelevant data from other modalities.\nWe pro-\npose the Multimodal Pathway and a concrete implemen-\ntation of no additional inference cost named Cross-Modal\nRe-parameterization. It represents an early exploration in\nthis direction, which offers a novel perspective. We real-\nize significant and consistent improvements on four rep-\nresentative modalities, demonstrating the potential of our\nmethod as a promising approach. In the future, we will ex-\nplore to construct multimodal pathways among CNNs and\ncross-architecture. The primary limitation is that the the-\nory behind the improvements remains to be revealed. Apart\nfrom empirical explanations, we believe further investiga-\ntions (e.g., a mathematically provable bound) will be useful.\nAcknowledgements. This work is partially supported\nby the National Natural Science Foundation of China (Grant\nNo. 8326014).\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatie Millican, Malcolm Reynolds, et al. Flamingo: a vi-\nsual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022. 4\n[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\nIn ICML, page 4, 2021. 7\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part I 16, pages 213\u2013229.\nSpringer, 2020. 1\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 4\n[5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-\nlis Savva, Shuran Song, Hao Su, et al.\nShapenet: An\ninformation-rich 3d model repository.\narXiv:1512.03012,\n2015. 5\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709,\n2020. 4\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248\u2013255. Ieee, 2009. 5\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n1, 4\n[9] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,\nGuiguang Ding, and Jian Sun. Repvgg: Making vgg-style\nconvnets great again. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n13733\u201313742, 2021. 4\n[10] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang\nDing. Scaling up your kernels to 31x31: Revisiting large\nkernel design in cnns. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n11963\u201311975, 2022.\n[11] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin\nSong, Xiangyu Yue, and Ying Shan. Unireplknet: A univer-\nsal perception large-kernel convnet for audio, video, point\ncloud, time-series and image recognition.\narXiv preprint\narXiv:2311.15599, 2023. 2, 4\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 1, 4, 6, 7\n[13] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 6202\u20136211, 2019. 7\n[14] Chongjian Ge, Xiaohan Ding, Zhan Tong, Li Yuan, Jian-\ngliu Wang, Yibing Song, and Ping Luo.\nAdvancing vi-\nsion transformers with group-mix attention. arXiv preprint\narXiv:2311.15157, 2023. 1\n[15] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren\nJansen, Wade Lawrence, R Channing Moore, Manoj Plakal,\nand Marvin Ritter.\nAudio set: An ontology and human-\nlabeled dataset for audio events.\nIn 2017 IEEE interna-\ntional conference on acoustics, speech and signal processing\n(ICASSP), pages 776\u2013780. IEEE, 2017. 5\n[16] Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio\nspectrogram transformer. arXiv preprint arXiv:2104.01778,\n2021. 1, 4, 7\n[17] Yuan Gong, Yu-An Chung, and James Glass. Psla: Improv-\ning audio tagging with pretraining, sampling, labeling, and\naggregation. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 29:3292\u20133306, 2021. 7\n[18] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass.\nSsast: Self-supervised audio spectrogram transformer.\nIn\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, pages 10699\u201310709, 2022. 7\n[19] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang,\nKaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xi-\nangyu Yue. Onellm: One framework to align all modalities\nwith language. arXiv preprint arXiv:2312.03700, 2023. 4\n[20] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, pages 2961\u20132969, 2017. 5\n[21] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n9729\u20139738, 2020. 4\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022. 3, 4, 5, 6\n[23] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,\nMichael Auli,\nWojciech Galuba,\nFlorian Metze,\nand\nChristoph Feichtenhofer. Masked autoencoders that listen.\narXiv preprint arXiv:2207.06405, 2022. 3, 5, 7\n[24] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 5\n[25] Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing\nSu, and Changwen Zheng. Semmae: Semantic-guided mask-\ning for learning masked autoencoders. Advances in Neural\nInformation Processing Systems, 35:14290\u201314302, 2022. 6\n[26] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-\ngalam, Bo Xiong, Jitendra Malik, and Christoph Feichten-\nhofer. Mvitv2: Improved multiscale vision transformers for\n9\nclassification and detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4804\u20134814, 2022. 7\n[27] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC. Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In ECCV, 2014. 5\n[28] Yuan Liu, Songyang Zhang, Jiacheng Chen, Zhaohui Yu,\nKai Chen, and Dahua Lin.\nImproving pixel-based mim\nby reducing wasted modeling capability.\narXiv preprint\narXiv:2308.00261, 2023. 6\n[29] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Re-\nthinking network design and local geometry in point cloud:\nA simple residual mlp framework. ICLR, 2022. 7\n[30] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,\nYonghong Tian, and Li Yuan.\nMasked autoencoders\nfor point cloud self-supervised learning.\narXiv preprint\narXiv:2203.06604, 2022. 3, 4, 5, 6\n[31] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. In NeurIPS, 2017. 7\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 1\n[33] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia\nSchmid.\nLearning video representations using contrastive\nbidirectional transformer. arXiv preprint arXiv:1906.05743,\n2019. 4\n[34] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,\nand Cordelia Schmid. Videobert: A joint model for video\nand language representation learning.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 7464\u20137473, 2019. 4\n[35] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learn-\ners for self-supervised video pre-training.\narXiv preprint\narXiv:2203.12602, 2022. 4, 5, 7\n[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. pages 10347\u201310357. PMLR, 2021. 1\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 1\n[38] Weiyao Wang, Du Tran, and Matt Feiszli. What makes train-\ning multi-modal classification networks hard? In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 12695\u201312705, 2020. 4\n[39] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei.\nVlmo: Unified vision-language pre-training with mixture-of-\nmodality-experts. arXiv preprint arXiv:2111.02358, 2021. 4\n[40] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\nmodel pretraining with weak supervision.\narXiv preprint\narXiv:2108.10904, 2021. 4\n[41] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In ECCV, pages 418\u2013434, 2018. 5\n[42] Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu,\nXiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt\nKeutzer, and Masayoshi Tomizuka. Image2point: 3d point-\ncloud understanding with 2d image pretrained models. In\nEuropean Conference on Computer Vision, pages 638\u2013656.\nSpringer, 2022. 8\n[43] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal\nlearning with transformers: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2023. 4\n[44] Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan\nYan, Hao Su, ARCewu Lu, Qixing Huang, Alla Sheffer,\nLeonidas Guibas, et al. A scalable active framework for re-\ngion annotation in 3d shape collections. ACM TOG, 35(6):\n210, 2016. 5\n[45] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nZhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud\ntransformers with masked point modeling. In CVPR, 2022.\n6, 7\n[46] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng\nLi, Yu Qiao, Wanli Ouyang, and Xiangyu Yue.\nMeta-\ntransformer: A unified framework for multimodal learning.\narXiv preprint arXiv:2307.10802, 2023. 1, 4\n[47] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and\nVladlen Koltun. Point transformer. In ICCV, pages 16259\u2013\n16268, 2021. 1\n[48] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017. 5\n[49] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun,\nJing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong,\nMeng Wang, and Yiran Zhong. Audio\u2013visual segmentation.\nIn Computer Vision\u2013ECCV 2022: 17th European Confer-\nence, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XXXVII, pages 386\u2013403. Springer, 2022. 3\n[50] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie\nZhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan.\nVl-gpt: A generative pre-trained transformer for vision and\nlanguage understanding and generation.\narXiv preprint\narXiv:2312.09251, 2023. 4\n10\n"
  },
  {
    "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models",
    "link": "https://arxiv.org/pdf/2401.13974.pdf",
    "upvote": "11",
    "text": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in\nPretrained Diffusion Models\nSenthil Purushwalkam*\nAkash Gokul*\nShafiq Joty\nNikhil Naik\nSalesforce AI Research\n{spurushwalkam,agokul,sjoty,nnaik}@salesforce.com\n\u201cA photo of \nmy pig \nwearing \nboots\u201d\n\u201cA photo of a young \nboy, swinging on a \nswing set in a \nsunlit playground.\u201d\nMy Pig\nText-to-Image\nSynthesized Training Data\nBootPIG       \nText-to-Image\nText-to-Image\n\u201cA photo of \nmy pig \nwearing boots\u201d\nFigure 1. Existing text-to-image models demonstrate exceptional image synthesis capabilities. However, they fail to \"personalize\"\ngenerations according to a specific subject. BootPIG (Ours) enables zero-shot subject-driven generation through a bootstrapped training\nprocess that uses images synthesized by the text-to-image model (Bottom). BootPIG trained text-to-image models can synthesize novel\nscenes containing the input subject without test-time finetuning while maintaining high fidelity to the prompt and subject.\nAbstract\nRecent text-to-image generation models have demon-\nstrated incredible success in generating images that faith-\nfully follow input prompts. However, the requirement of\nusing words to describe a desired concept provides limited\ncontrol over the appearance of the generated concepts. In\nthis work, we address this shortcoming by proposing an\napproach to enable personalization capabilities in existing\ntext-to-image diffusion models. We propose a novel archi-\ntecture (BootPIG) that allows a user to provide reference\nimages of an object in order to guide the appearance of a\nconcept in the generated images.\nThe proposed BootPIG architecture makes minimal modi-\nfications to a pretrained text-to-image diffusion model and\nutilizes a separate UNet model to steer the generations to-\nward the desired appearance. We introduce a training proce-\ndure that allows us to bootstrap personalization capabilities\nin the BootPIG architecture using data generated from pre-\ntrained text-to-image models, LLM chat agents, and image\nsegmentation models. In contrast to existing methods that\nrequire several days of pretraining, the BootPIG architec-\nture can be trained in approximately 1 hour. Experiments on\nthe DreamBooth dataset demonstrate that BootPIG outper-\nforms existing zero-shot methods while being comparable\nwith test-time finetuning approaches. Through a user study,\nwe validate the preference for BootPIG generations over\nexisting methods both in maintaining fidelity to the reference\nobject\u2019s appearance and aligning with textual prompts.\n1\narXiv:2401.13974v1  [cs.CV]  25 Jan 2024\n1. Introduction\nOver the last few years, research in generative model-\ning has pushed the boundaries of technological innovation\nand enabled new avenues of creative expression. In com-\nputer vision, text-to-image generation models [4, 5, 16, 30\u2013\n32, 49, 50] have showcased remarkable proficiency in gener-\nating high-fidelity images corresponding to novel captions.\nHowever, the ability to generate arbitrary images depict-\ning a chosen caption has limited applications. Recent ad-\nvancements in research have expanded upon these text-to-\nimage models, leveraging their capabilities to solve more\nwidely applicable tasks such as image inpainting[46], image\nediting[25, 44], style transfer[52], and even generation of 3D\nmodels from text[27] and images[24, 28, 47]. One such task\nwith numerous applications is the problem of Personalized\nImage Generation.\nPersonalized Image Generation (also know as subject-\ndriven generation) is the ability to generate images of spe-\ncific personal objects in various user-specified contexts. For\ninstance, one may want to visualize what their pet would\nlook like if it wore cowboy boots. Beyond such personal\nvisualization experiments, this capability has the potential\nto serve as a versatile tool with applications ranging from\npersonalized storytelling to interactive design.\nMore concretely, the Personalized Image Generation prob-\nlem can be formulated as follows: given a few reference\nimages depicting an object and a target caption, the goal is\nto generate an image that corresponds to the caption while\nfaithfully capturing the object\u2019s appearance. In recent re-\nsearch, several methods have been developed to leverage pre-\ntrained text-to-image models to accomplish this task. These\napproaches can be divided into two categories\u2013 Test-time\nFinetuning and Zero-Shot Inference methods. The former\ncategory involves methods that finetune the text-to-image\nmodel parameters or learn additional parameters to learn the\nsubject\u2019s appearancee. These methods often take a few min-\nutes to a few hours to update parameters before personalized\nimages are generated. The latter category includes methods\nthat rely on pretraining the text-to-image models and do not\ninvolve test-time updates. While Zero-Shot Inference-based\nmethods are more user-friendly and efficient for deployment,\nthey fall short compared to Test-time Finetuning methods\nin terms of faithfulness to the reference object and textual\nprompt following capabilities.\nIn this work, we propose a novel solution to the Personal-\nized Image Generation problem that provides the efficiency\nof Zero-Shot Inference methods while outperforming exist-\ning Test-time Finetuning methods. We introduce a novel ar-\nchitecture, named BootPIG, designed to enable personalized\nimage generation capabilities in a pretrained text-to-image\ngeneration model. The BootPIG architecture comprises two\nreplicas of the pretrained text-to-image model \u2014 one ded-\nicated to extracting visual features from reference images\nand another for the actual image generation process.\nIn order to train our proposed model, we present a novel\nprocedure that does not directly utilize any human curated\ndataset. Instead, we bootstrap the personalization capabil-\nity by learning from data synthetically generated using pre-\ntrained text-to-image generation models, state-of-the-art chat\nagents and image segmentation models. Furthermore, unlike\nexisting Zero-Shot Inference methods that require several\ndays of compute for pretraining, our proposed model can be\ntrained in approximately 1 hour on 16 A100 GPUs.\nExperiments on the DreamBooth dataset show that Boot-\nPIG generations outperform existing zero-shot methods\nwhile performing comparably to test-time finetuned ap-\nproaches based on existing metrics. After conducting a user\nstudy, which asks users to compare generations from two\napproaches based on subject fidelity and prompt fidelity, we\nfind that users generally prefer BootPIG generations over\nexisting zero-shot and test-time finetuned methods.\nThe contributions of our work are summarized as follows:\n\u2022 We propose a novel architecture that enables zero-shot\nsubject-driven generation while minimally modifying the\narchitecture of the pretrained text-to-image model and\nrequiring only 1 hour to train.\n\u2022 We demonstrate an effective bootstrapped learning pro-\ncedure which does not require human-curated data and\nallows a pretrained text-to-image to use its own features\nto learn subject-driven generation.\n\u2022 BootPIG excels in zero-shot personalized image genera-\ntion outperforming existing zero-shot method and test-time\nfinetuned methods based on quantitative evaluations and\nuser studies.\n2. Related Work\n2.1. Text-to-Image Synthesis\nProgress in generative models has led to breakthroughs\nin text-to-image synthesis. Existing text-to-image models\n[4, 5, 16, 30\u201332, 49, 50] are capable of generating high-\nquality images in accordance with a given text prompt.\nThese models fall into one of the following categories: dif-\nfusion models [31, 32, 36], autoregressive image models\n[30, 49, 50], non-autoregressive image models [3][4], and\ngenerative adversarial networks (GANs) [8][16]. While\nthese models demonstrate exceptional image generation and\nfidelity to an input prompt, their outputs are constrained by\nthe limitations of the text interface. This hinders their ability\nto generate images with a specific subject or follow addi-\ntional controls from signals such as images. In this work,\nwe extend the capability of pretrained text-to-image diffu-\nsion models to enable zero-shot subject-driven generation\nby introducing subject images as an additional source of\ncontrol.\n2\n2.2. Subject-Driven Text-to-Image Synthesis via\nTest-time Finetuning\nFollowing the breakthroughs in text-to-image generation,\nmany works extended these models to enable subject-driven\ngeneration. Subject-driven generation (personalized image\ngeneration) focuses on synthesizing an input subject in novel\nscenes. The seminal works of Textual Inversion [6] and\nDreamBooth [34] introduced test-time finetuning approaches\nthat customized a text-to-image generative model given a\nfew images of a subject. Textual Inversion enables subject-\ndriven generation by learning a unique text token for each\nsubject. In contrast, DreamBooth finetunes the entire text-to-\nimage backbone and demonstrates superior results in fidelity\nto the subject. Subsequent works [1, 7, 9, 10, 19, 35, 38, 43]\nextended these approaches by developing improved finetun-\ning methods. In particular, CustomDiffusion [19] and ViCo\n[10] improve subject-driven generation by combining ideas\nfrom Textual Inversion and DreamBooth, by learning a spe-\ncific text embedding for each concept while finetuning the\ncross-attention layers of the text-to-image model. Unlike\nthese works, our method enables zero-shot subject-driven\ngeneration and avoids the need to train unique models for\neach subject.\n2.3. Zero-Shot Subject-Driven Text-to-Image Syn-\nthesis\nSubject-driven generation approaches, such as DreamBooth\nand Textual Inversion, require hundreds, sometimes even\nthousands, of steps to learn the user-provided subject. Recent\nworks have sought to avoid this tedious finetuning process\nby developing zero-shot methods for subject-driven gen-\neration. These methods typically pretrain image encoders\nacross large datasets to learn image features that improve\nthe generative model\u2019s ability to render the subject. Instant-\nBooth [37] introduces adapter layers in the text-to-image\nmodel and pretrains an adapter for each concept, e.g. cats. In\ncomparison, our method is not restricted to learning single\ncategories and, instead, can generalize to novel concepts.\nThe authors of Jia et al. [15] propose to use frozen CLIP\n[29] image encoders and continue training the generative\nbackbone to use CLIP image features. Similar to this work,\nour approach continues training the generative backbone.\nHowever, our approach does not introduce additional image\nencoders and does not require real data. Along the lines of\npretraining image encoders, BLIP-Diffusion [21] pretrains a\nQ-Former [22] that learns image features which are aligned\nwith text embeddings. Similarly, ELITE [45] pretrains two\nimage encoders\u2013 one that learns image features that align\nwith token embeddings, and a second encoder that learns\nimage features that align with the text encoder\u2019s features. In\ncomparison, our approach avoids introducing a bottleneck\nin the form of text-aligned features and, instead, uses the\nfeatures from the generative backbone to guide generation.\n2.4. Multimodal Controls for Text-to-Image Models\nConcurrent to the work in subject-driven generation, many\nworks have explored extending the conditional controls in\ntext-to-image models. These methods introduce learnable\ncomponents, such as modality-specific adapters [14, 17, 23,\n26, 48, 53], to enable additional control via inputs such as\ndepth maps or color palettes. In particular, ControlNet [51]\ndemonstrated exceptional results in controllable generation\nby training a learnable copy of the generative model\u2019s back-\nbone to accept new input conditions. Our architecture shares\nsimilarities with the ControlNet architecture, as both works\nuse a copy of the generative backbone. However unlike\nControlNet, our work focuses on zero-shot subject-driven\ngeneration.\n3. Method\nIn this section, we present the details of our proposed ap-\nproach for enabling personalized image generation capabili-\nties in existing latent diffusion models. Given a caption and\na set of reference images that depict an object, the goal is to\ngenerate an image that follows the caption while ensuring\nthat the appearance of the object matches that of the refer-\nence images. We accomplish this by introducing a novel\narchitecture (BootPIG) that builds on top of existing diffu-\nsion models for text-to-image generation.\n3.1. Preliminary: Diffusion Models\nDiffusion models [13, 39, 40] learn a data distribution by\niteratively denoising samples from a Gaussian distribution.\nGiven a data sample x, a noised version xt := \u03b1tx + \u03c3t\u03f5,\nis generated by applying noise, \u03f5 \u223c N(0, 1) according to\na timestep t \u2208 {1, . . . , T}, where T is the total number of\ntimesteps, and \u03b1t, \u03c3t control the noise schedule. The model,\n\u03f5\u03b8, is trained using the mean squared error objective between\nthe noise \u03f5 and the predicted noise \u03f5\u03b8(xt, t, c), where c refers\nto a conditioning vector e.g. a text prompt (Equation 1).\nEx,c,\u03f5,t\u223cU([0,T ]) \u2225\u03f5 \u2212 \u03f5\u03b8(xt, t, c)\u22252\n2\n(1)\nIn this work, we use pretrained text-to-image diffusion\nmodels and do not alter the diffusion objective during our\ntraining. Instead, we introduce additional image features\ninto the architecture which are trained using Equation 1.\n3.2. BootPIG Model Architecture\nWe present an overview of the BootPIG architecture in Fig-\nure 2. The key idea of our proposed architecture is to inject\nthe appearance of a reference object into the features of\na pretrained text conditioned image diffusion model, such\nthat the generated images imitate the reference object. In\nthis work, we use Stable Diffusion [32] as our pretrained\ntext-to-image diffusion model. Stable Diffusion is a Latent\nDiffusion Model [32] that uses a U-Net [33] architecture\n3\nconsisting of Transformer[41] and Residual[11] blocks. Our\nproposed BootPIG architecture modifies the information\nprocessed by the self-attention layers in the Transformer\nblocks in order to control the appearance of the generated\nobjects. Let the Stable Diffusion U-Net model be denoted\nby U\u03b8(x, c, t) where xt are noisy input latents, c is an input\ntextual prompt and t is the timestep in the diffusion process.\nInjecting Reference Features\nA self-attention (SA) layer\nthat receives a latent feature f \u2208 Rn\u00d7d, performs the follow-\ning operation:\nSA(f) = Wo\n\u0010\nsoftmax\n\u0010\nq(f)k(f)T \u0011\nv(f)\n\u0011\n(2)\nwhere q, k, v are linear mappings known as the query, key\nand value functions with parameters Wq, Wk, Wv \u2208 Rd\u00d7d\u2032\nrespectively that project the features to a chosen dimension\nd\u2032. Wo \u2208 Rd\u2032xd projects the output back to the original\ndimension d. We propose to replace all the Self-Attention\n(SA) layers with an operation that we refer to as Reference\nSelf-Attention (RSA), that allows us to inject reference fea-\ntures. The RSA operator takes as input the latent features\nf \u2208 Rn\u00d7d and reference features of the same dimension\nfref \u2208 Rnref\u00d7d, and performs the following operation:\nRSA(f, fref) = Wo\n\u0010\nsoftmax\n\u0010\nq(f)\n\u0014\nk(f)\nk(fref)\n\u0015T \"\nv(f)\ni\nv(fref)\n# \u0011\n(3)\nwhere [:] indicates concatenation along the first dimen-\nsion. Intuitively, the RSA operator facilitates the injection\nof reference features, allowing the diffusion model to \u201cat-\ntend\u201d to them in the computation of the output latent feature.\nLet us denote this new U-Net, referred to as Base U-Net, by\nU RSA\n\u03b8\n(x, c, t, {f (1)\nref, f (2)\nref, ..., f (L)\nref }) containing L RSA layers.\nFor simplicity, we use the notation Fref to denote the set\nof L reference features. Note that the RSA operator does\nnot introduce any new parameters and reuses the weight\nparameters Wo, Wq, Wk, Wv.\nExtracting Reference Features\nGiven a reference image\nIref, we need to extract the appropriate features fref that can\nbe passed to each RSA layer. We propose to extract features\nusing a separate U-Net U\u03d5(x, c, t), referred to as Reference\nU-Net, that follows the same architecture as the Base U-Net\nand is initialized with the same parameters (\u03d5 = \u03b8). For\na given t, we perform the forward diffusion process on the\nreference image Iref to compute the noised reference latents\nx\u2032\nreft. We pass x\u2032\nreft as input along with the textual prompt,\nand extract the features before the L\u2212 SA layers as Fref.\nThis ensures that the extracted reference features have the\nappropriate dimensions and are compatible with the weights\nof the RSA layers.\nReference UNet\nBase UNet\nSelf-Attention Layer\nReference Self-Attention Layer\nf\nq\nk\nv\n Queries\nKeys\nValues\nf\nQueries\nKeys\nValues\nq\nv\nvref\n[   ]\nk\nkref\n[   ]\nfref\nReference Image\nImage\nSA\nSA\nSA\nRSA\nRSA\nRSA\n\"A photo of a wooden \nviolin, cradled gently \ninside a velvet case.\u201d\nPrompt\nFigure 2. Model Architecture: We propose a novel architecture,\nthat we refer to as BootPIG, for personalized image generation.\nThe model comprises of two replicas of a latent diffusion model -\nReference UNet and Base UNet. The Reference UNet processes\nreference images to collect the features before each Self-Attention\n(SA) layer. The Base UNet\u2019s SA layers are modified to Reference\nSelf-Attention (RSA) layers that allow conditioning on extra fea-\ntures. Using the collected reference features as input, the Base\nUNet equipped with the RSA layers estimates the noise in the input\nto guide the image generation towards the reference objects.\n3.3. Training\nThe BootPIG architecture allows us to pass features of a\nreference image to the RSA layers. However, since the\noriginal diffusion model U\u03b8 was not trained with RSA layers,\nwe observe that the generated images are corrupted (see\nsupplementary material) and do not correctly follow the input\nprompts. In order to rectify this, we propose to finetune the\nparameters of the Reference U-Net \u03d5 in order to extract better\nreference features and the parameters of the RSA layers\n(Wo, Wq, Wk, Wv) to better utilize the reference features.\nGiven a dataset of triplets containing a reference image,\na textual prompt and a target image following the textual\nprompt while accurately depicting the reference object, we\nfinetune the BootPIG architecture using the same objective\nas the original latent diffusion model (see Section 3.1). The\nReference U-Net takes as input noisy VAE latents (noised\naccording to timestep t) corresponding to the reference im-\nage as input, along with the timestep t and the target caption.\nThe Base U-Net receives as input noisy VAE latents corre-\nsponding to the target image (similarly noised), the timestep\nt, the target caption and the reference features collected from\nthe Reference U-Net. The parameters of the Reference U-\nNet and the RSA layers are updated to accurately estimate\nthe noise in the input latents (see Eq 1). In order to pre-\nserve the Base U-Net\u2019s prompt following capabilities, we\nrandomly drop the reference image\u2019s features (with proba-\nbility 0.15), thereby reverting the Base U-Net model back\n4\nGenerate a caption \nthat \u2026. in the \nformat:                  \n\u201cA photo of a \n[object], [object \npositioning] \n[describe scene]\u2019\u2019\nA photo of a \nchild's tricycle, \nleft on a \ndriveway, as \nnightfall settles \nin.\nChatGPT\nStable \nDiffusion\nSAM\nA photo of a pair of \nbrand new running \nshoes, laces untied \nand ready to go.\n\u201cA photo of a black \nleather jacket, \nhanging on a hook by \nthe front door.\u201d\n\u201cA photo of a cool mint \npopsicle, melting and \nleaving droplets on a \nchild's hand.\u201d\nTarget Image\nReference Image\nAdditional Examples\nFigure 3. Synthetic Training Data: We propose an automated\ndata generation pipeline to generate (reference image, target image,\ntarget caption) triplets for training BootPIG. The pipeline uses\nChatGPT to generate captions, Stable Diffusion to generate images\nand the Segment Anything Model to segment the foreground which\nserves as the reference image.\nto the SA based architecture. We provide a more detailed\ndescription of the training pipeline along with pseudocode\nin the supplementary material.\n3.4. Bootstrapping Training Data\nCollecting a large curated source of training data for optimiz-\ning the BootPIG architecture is an expensive if not infeasible\nprocess. This would involve gathering paired reference and\ntarget images depicting the same object instance in different\ncontexts. Instead, we propose a synthetic data generation\npipeline that leverages the capabilities of existing pretrained\nComputer Vision and NLP models.\nIn Figure 3, we provide an overview of the data genera-\ntion pipeline. First, we utilize ChatGPT[2], a state-of-the-art\nconversational agent, to generate captions for potential tar-\nget images. For each caption, we generate an image using\nStable Diffusion[32]. We then use Segment Anything Model\n(SAM)[18], a state-of-the-art segmentation model, to extract\na foreground mask corresponding to the main object in the\ncaption. We treat the Stable Diffusion generated image as the\ntarget image, the foreground object pasted on a white back-\nground as the reference image and the ChatGPT generated\ncaption as the textual prompt. While the reference image\ndoes not depict the object in an entirely different context, we\nobserve empirically that this synthetic data is sufficient to\nlearn personalization capabilities.\n3.5. Inference\nDuring inference, at each timestep, we extract reference\nfeatures Fref by passing a noised reference image, the target\ncaption c and the timestep t to the Reference UNet. We use\na classifier free guidance [12] strategy to estimate the noise\nin the noisy generation latents x\u2032\nt using the Base UNet as:\n\u03f5 = U\nSA\n\u03b8 (x\u2032\nt, t) + \u03b7im(U\nRSA\n\u03b8 (x\u2032\nt, t, fref) \u2212 U\nSA\n\u03b8 (x\u2032\nt, t))\n+\u03b7text+im \u2217 (U\nRSA\n\u03b8 (x\u2032\nt, t, fref, c) \u2212 U\nRSA\n\u03b8 (x\u2032\nt, t, fref, c))\n(4)\nwhere the first U-Net term estimates the noise without any\nconditioning information, the second U-Net term estimates\nnoise with just image information and the fourth U-Net term\nuses both images and captions.\nThe BootPIG architecture described so far takes as input a\nsingle reference image. This allows us to train the parameters\nof the model with synthesized (reference, target) image pairs.\nA common scenario with several applications is the case\nwhere multiple reference images are available to us. In\norder to handle this scenario, we now propose an inference\nprocedure to leverage a trained BootPIG model and utilize\nappearance information from multiple reference images.\nLet frefi be the reference feature for image i at a specific\nlayer. At each RSA layer, we first compute the outputs\nwithout any reference feature (fref = f) and using each\nreference feature:\no = RSA(f, f)\n(5)\noi = RSA(f, frefi)\n\u2200i \u2208 {1, 2, ..., k}\n(6)\nFor the output generated by each reference oi \u2208 Rn\u00d7d, we\ncompute the pixelwise norm of difference to the reference-\nless output o. We denote this by ni = ||oi \u2212 o|| \u2208 Rn\u00d71.\nThe pixelwise softmax of these norms is then used to weight\nthe contribution of each reference feature. Specifically, we\ncompute the final output as:\npi = softmax(n1, n2, ..., nk)[i] \u2208 Rn\u00d71\n(7)\nomultiref = o +\nX\ni\npi \u2217 (oi \u2212 o)\n(8)\nIntuitively, at each pixel location, we want to use the ref-\nerence features that make the largest contribution compared\nto the reference-less output.\n4. Experiments\nIn this section, we describe the experimental setup and\npresent results of our proposed model, BootPIG, for per-\nsonalized image generation.\n4.1. Experimental Setup\nImplementation Details\nThe dataset synthesis pipeline\ndiscussed in 3.3 was used to synthesize 200000 (reference\n5\nBLIP Diffusion \nBootPIG (ours)\nReference Samples\nDreamBooth\nELITE\n\u201ca dog in a chef outfit\u201d\n\u201ca dog in a firefighter outfit\u201d\n\u201ca cat on top of a purple rug in a forest\u201d\n\u201ca toy floating on top of water\u201d\n\u201ca can in the snow\u201d\nFigure 4. Qualitative Comparision: We provide visual comparisions of subject-driven generations from related methods such as BLIP-\nDiffusion, ELITE, and DreamBooth. BootPIG exhibits high subject and prompt fidelity, outperforming related methods while avoiding\ntest-time finetuning.\nimage, target image, target caption) triplets as training data.\nWe provide the ChatGPT prompt used to generate these\nimages in the Supplementary. We use the VAE and text\nencoder weights from the publicly available Stable Diffusion\nversion 2.1 [32]. The Base U-Net and Reference U-Net are\nboth initialized from the pretrained Stable Diffusion U-Net\nweights. During training, we augment the reference images\nby randomly resizing the foreground, horizontally flipping\n6\nand moving the foreground to random locations. We train\nthe model using 16 A100 GPUs, with a total batch size\nof 64 for 10k updates. We use a learning rate of 5e-6 for\nthe Reference U-Net and 1e-6 for the RSA layers of the\nBase U-Net. The entire training takes 70 minutes, whereas\nBLIP-Diffusion requires 6 days with the same amount of\ncompute. During inference, we use the UniPC Scheduler[53]\nand generate images using 50 inference steps. For classifier\nfree guidance (as described in Section 3.5), we use \u03b7im =\n5.0 and \u03b7text+im = 7.5 in all the experiments.\nEvaluation\nWe evaluate our method on the DreamBooth\ndataset. The dataset contains 30 personalization subjects,\neach with multiple 3-6 reference images, and 25 novel\nprompts per subject. Unless specified, we use all available\nreference images for each generation. We evaluate the zero-\nshot performance of our method on this dataset using the\nmetrics introduced by the authors of DreamBooth: CLIP-T,\nCLIP-I, and DINO. CLIP-T is used to measure the align-\nment between the generated image and the textual prompt.\nCLIP-I and DINO are used to evaluate the faithfulness of the\ngenerated image to the appearance of the reference object.\nAdditionally, we perform a user-study to compare our\nmethod to existing work. We perform a subject fidelity\nuser study and a text alignment user study following the\nprocedure outlined in ELITE [45]. In the subject fidelity\nstudy, users are provided the original subject image, and\ntwo generated images of that subject, generated by different\nmethods (both methods use the same prompt). Users are\nasked to select which image better represents the original\nsubject. In the text alignment study, users are provided the\ncaption and two generated images, generated by different\nmethods, for that caption and are asked to select which image\nbetter represents the caption.\nFor comparisons with Textual Inversion and DreamBooth,\nwe use the corresponding implementations in the diffusers\npackage [42] to train on each of the 30 subjects. We report\nCustomDiffusion\u2019s evaluation metrics using the results re-\nproduced in Wei et al. [45]. For zero-shot methods, such as\nBLIP-Diffusion and ELITE, we use their official pretrained\nmodels to generate images for each subject, following their\nrespective inference procedures.\n4.2. Qualitative Comparisons\nIn Figure 4, we provide qualitative comparisons between\nBootPIG and several personalized image generation methods.\nIn comparison to zero-shot methods such as BLIP-Diffusion\nand ELITE, BootPIG exhibits significantly higher fidelity to\nthe original subject. This improved fidelity to the subject is\nseen at both the object-level and at the level of fine-grained\ndetails as seen in examples such as the dog (row 3) or the can\n(row 5). We attribute this improvement in subject alignment\nto our reference feature extraction method which, unlike\nBLIP-Diffusion and ELITE, does not introduce a bottleneck\nby forcing image features to be aligned with the text features.\nCompared to DreamBooth, a test-time finetuned approach,\nwe also see improvements in subject fidelity. For example,\nBootPIG better preserves the markings and colors of the\ncat\u2019s fur (row 4) and the facial features, e.g. eyes, of the dog\n(row 3).\n4.3. Quantitative Comparisons\nIn Table 1, we present the main quantitative comparison\nof BootPIG against existing methods on the DreamBooth\ndataset. BootPIG outperforms all existing zero-shot meth-\nods in prompt fidelity (+1.1 CLIP-T) and subject fidelity\n(+0.8 CLIP-I, +2.4 DINO) metrics. Compared to test-time\nfinetuned methods, BootPIG exhibits state-of-the-art perfor-\nmance in prompt fidelity (+0.6 CLIP-T) while performing\ncomparably in subject fidelity.\nWin Rate\n0\n25\n50\n75\n100\n vs. BLIP-Diffusion\n vs. ELITE\n vs. Dreambooth\nPrompt Matching\nSubject Matching\nFigure 5. User Study: We report the win rate (% of users who fa-\nvored BootPIG generations) against existing methods. We perform\ntwo studies per head-to-head comparision, one evaluating prompt\nfidelity and the other evaluating subject fidelity.\nTable 1. Quantitative Comparisons: We report average metrics\nfor subject fidelity (CLIP-I, DINO) and prompt fidelity (CLIP-T)\non the DreamBooth dataset.\nMethod\nZero-shot\nCLIP-T\nCLIP-I\nDINO\nBLIP Diffusion [21]\n\u2713\n30.0\n77.9\n59.4\nELITE [45]\n\u2713\n25.5\n76.2\n65.2\nELITE (reproduced)\n\u2713\n29.6\n78.8\n61.4\nOurs\n\u2713\n31.1\n79.7\n67.4\nTextual Inversion [6]\n\u2717\n25.5\n78.0\n56.9\nDreamBooth [34]\n\u2717\n30.5\n80.3\n66.8\nCustomDiffusion [19]\n\u2717\n24.5\n80.1\n69.5\nBLIP Diffusion + FT\n\u2717\n30.2\n80.5\n67.0\nReference Images\n-\n-\n88.5\n77.4\nFrom the results of our user study, we find that users\nconsistently prefer BootPIG generations over both zero-shot\n7\nMultiple Ref \n(Concat)\nMultiple Ref \n(Average)\nMultiple Ref \n(Ours)\nSingle Ref\nReference Example\nFigure 6. Effect of Inference Strategy: We compare different\nstrategies for pooling information from multiple reference images\nin the BootPIG architecture. Refer to Section 4.4 for more details.\nTable 2. Effect of training different components: We perform a\nquantitative study on the effect of updating or fixing different com-\nponents of the BootPIG architecture during training. We observe\nthat the optimal strategy is to train the entire Reference U-Net and\nonly train the RSA layers of the Base U-Net. Refer to Section 4.4\nfor more details.\nTrainable Parameters\nUse Aug.\nCLIP-T\nCLIP-I\nDINO\nBase UNet\nRef. UNet\nAll\nAll\n\u2713\nOOM\nNone\nAll\n\u2713\n30.6\n72.8\n43.8\nAll\nNone\n\u2713\n31.6\n78.8\n66.4\nRSA\nNone\n\u2713\n30.9\n78.2\n64.9\nRSA\nAll\n\u2717\n30.2\n78.8\n67.0\n(Ours)\nRSA\nAll\n\u2713\n31.1\n79.7\n67.4\nand test-time finetuned methods. Human evaluators find\nBootPIG generations to have significantly greater subject\nfidelity (69% win rate versus ELITE 65% win rate versus\nBLIP-Diffusion, and 62% win rate versus DreamBooth). Ad-\nditionally, the user study demonstrates that generations from\nBootPIG exhibit higher fidelity to the caption than existing\nmethods (60% win rate versus ELITE, 54% win rate ver-\nsus BLIP-Diffusion, and 52% win rate versus DreamBooth).\nThese results underscore the efficacy of our training method.\n4.4. Ablative Studies\nIn order to understand the effect of different implementation\nchoices, we further conduct ablative studies using the Boot-\nPIG architecture. First, we study the effect of training/freez-\ning different components of the BootPIG architecture during\nthe training process. In Table 2, we present the results on\nthe three evaluation metrics. As discussed in Section 3.3, for\nour best model, we train all the parameters of the Reference\nU-Net and only train the RSA layers of the Base U-Net. We\nNumber of Reference Images\n% Relative Change \n-2\n0\n2\n4\n6\n8\n1\n2\n3\n4\n5\nCLIP-T\nCLIP-Image\nDINO-Image\nFigure 7. Effect of multiple references: We quantify the effect of\nusing one or more reference images during inference. Increasing\nthe number of reference images significantly improves subject\nfidelity (+7.23 DINO, +1.74 CLIP-I) while slightly worsening\nprompt fidelity (\u22121.03 CLIP-T)\nfind that the following a ControlNet style training approach,\nwhere only the Reference U-Net is trainable, leads to poor\nperformance. We attribute this to the fact that the attention\nlayers in the Base U-Net are not trained to handle features\nfrom multiple images. We also find that only training the\nRSA layers of the Base U-Net also leads to poor subject\nfidelity and text fidelity. This result highlight the importance\nof training the Reference U-Net to extract better reference\nfeatures. Lastly, we find that finetuning both networks, is\nextremely memory intensive and hits our available mem-\nory limits even when training with small batch sizes. We\nalso present results obtained after training our model with-\nout randomly augmenting the reference images (2nd last\nrow). While we only see a minor drop in the subject fidelity\nmetrics (CLIP-I, DINO), we observe in qualitative visualiza-\ntions that the generated images are extremely corrupted (see\nsupplementary material).\nNext, we study the effect of the number of references used\nas input to BootPIG. Since each reference image provides\nmore information about the subject\u2019s appearance, the subject\nfidelity of an ideal personalized generation model should\nimprove as the number of references increase. In Figure 7,\nwe present the relative change in the three metrics compared\nto outputs generated using only one reference image. We\nobserve that the our subject fidelity metrics (CLIP-Image,\nDINO) consistently increase as the number of references\nincreases. This highlights the efficacy of our proposed in-\nference strategy that accounts for multiple reference images\n(introduced in Section 3.5).\nFinally, we study different the efficacy of different infer-\nence strategies to handle multiple reference images with the\nBootPIG architecture. In Figure 6, we present personalized\ngenerations on a few subjects using different inference strate-\ngies. \u201cConcat\u201d refers to concatenating the features from all\nreference images into one sequence before passing to the\nRSA layer. \u201cAverage\u201d refers to averaging the outputs of an\n8\nRSA layer after passing each reference image\u2019s feature in\nindividually i.e.\n1\nK\nPK\ni=1 RSA(f, frefi). We observe that\n\u201cConcat\u201d often leads to generations where the object is re-\npeated multiple times. This is expected since the model is\nreceiving multiple copies of the same feature as input. On\nthe other hand, the \u201cAverage\u201d produces good quality images\nbut smoothens out all the details in the objects. Our proposed\ninference strategy avoids these issues and generally produces\nhigher quality personalized generations.\n5. Limitations\nBootPIG possesses many of the same failures as related meth-\nods. Specifically, in many instances, BootPIG generation\nmay fail to render fine-grained features of the subject and\nstruggle to accurately adhere to the prompt. Some of these\nfailure cases highlight the need to learn stronger fine-grained\nfeatures, while other failure cases, such as prompt fidelity\nin certain conditions and text rendering (see Fig 4 row 5),\nare limitations inherited from the underlying text-to-image\nmodel. We provide further illustrations of failure cases in the\nSupplementary Material. Additionally, the weaknesses and\nthe biases, including harmful stereotypes, of the underlying\ngenerative model will be perpetuated by BootPIG genera-\ntions. Subject-driven generation also opens the possibility\nof generating unwanted images of individuals without their\nconsent. We urge that users of this technology are mind-\nful of these implications and recommend that they use such\ntechnology responsibly.\n6. Discussion\nIn this paper, we presented a bootstrapped training proce-\ndure that enables a text-to-image model to synthesize user-\nprovided subjects in novel scenes without subject-specific\nfinetuning. Our method, BootPIG, does not require real\ndata or additional pretrained image encoders. Instead, it\nuses images generated by the text-to-image models, and\nutilizes a copy of the generative backbone to capture the\nappearance of user-provided subjects. The proposed model\ncan be trained in approximately 1 hour, outperforms similar\nzero-shot inference methods and performs comparably to\ntest-time finetuning methods. We believe that bootstrapped\ntraining of pretrained text-to-image models can be a promis-\ning paradigm for learning new capabilities and unlocking\nother modalities of controlled image generation.\nReferences\n[1] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel\nCohen-Or, Ariel Shamir, and Amit H Bermano. Domain-\nagnostic tuning-encoder for fast personalization of text-to-\nimage models. arXiv preprint arXiv:2307.06925, 2023. 3\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot\nlearners. ArXiv, abs/2005.14165, 2020. 5\n[3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315\u201311325, 2022. 2\n[4] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\nWilliam T Freeman, Michael Rubinstein, et al. Muse: Text-to-\nimage generation via masked generative transformers. arXiv\npreprint arXiv:2301.00704, 2023. 2\n[5] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems, 35:16890\u201316902, 2022. 2\n[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. arXiv preprint arXiv:2208.01618,\n2022. 3, 7\n[7] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Encoder-based domain\ntuning for fast personalization of text-to-image models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201313, 2023. 3\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Communi-\ncations of the ACM, 63(11):139\u2013144, 2020. 2\n[9] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yun-\npeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning\nChang, Weijia Wu, et al. Mix-of-show: Decentralized low-\nrank adaptation for multi-concept customization of diffusion\nmodels. arXiv preprint arXiv:2305.18292, 2023. 3\n[10] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee K Wong.\nVico: Detail-preserving visual condition for personalized text-\nto-image generation. arXiv preprint arXiv:2306.00971, 2023.\n3, 14, 15\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 4\n[12] Jonathan Ho.\nClassifier-free diffusion guidance.\nArXiv,\nabs/2207.12598, 2022. 5\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[14] Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng,\nChaoyue Wang, Dacheng Tao, and Tat-Jen Cham. Cocktail:\nMixing multi-modality control for text-conditional image gen-\neration. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023. 3\n9\n[15] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models. arXiv\npreprint arXiv:2304.02642, 2023. 3\n[16] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli\nShechtman, Sylvain Paris, and Taesung Park. Scaling up gans\nfor text-to-image synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10124\u201310134, 2023. 2\n[17] Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim,\nand Namhyuk Ahn. Diffblender: Scalable and composable\nmultimodal text-to-image diffusion models. arXiv preprint\narXiv:2305.15194, 2023. 3\n[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and\nRoss B. Girshick. Segment anything. ArXiv, abs/2304.02643,\n2023. 5, 12\n[19] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht-\nman, and Jun-Yan Zhu. Multi-concept customization of text-\nto-image diffusion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1931\u20131941, 2023. 3, 7, 14, 15\n[20] Min Seok Lee, WooSeok Shin, and Sung Won Han. Tracer:\nExtreme attention guided salient object tracing network (stu-\ndent abstract). In Proceedings of the AAAI Conference on\nArtificial Intelligence, pages 12993\u201312994, 2022. 13\n[21] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion: Pre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 3, 7\n[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 3\n[23] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 3\n[24] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8446\u20138455, 2023. 2\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image\nsynthesis and editing with stochastic differential equations.\narXiv preprint arXiv:2108.01073, 2021. 2\n[26] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 3\n[27] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 2\n[28] Senthil Purushwalkam and Nikhil Naik.\nConrad: Image\nconstrained radiance fields for 3d generation from a single\nimage. arXiv preprint arXiv:2311.05230, 2023. 2\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n2\n[31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. arXiv preprint arXiv:2204.06125, 1(2):\n3, 2022. 2\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 5, 6, 12\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 3\n[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 22500\u201322510,\n2023. 3, 7, 14, 15\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo\nHou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and\nKfir Aberman. Hyperdreambooth: Hypernetworks for fast\npersonalization of text-to-image models.\narXiv preprint\narXiv:2307.06949, 2023. 3\n[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022. 2\n[37] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. In-\nstantbooth: Personalized text-to-image generation without\ntest-time finetuning. arXiv preprint arXiv:2304.03411, 2023.\n3\n[38] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting\nHua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual\ndiffusion: Continual customization of text-to-image diffusion\nwith c-lora. arXiv preprint arXiv:2304.06027, 2023. 3\n[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International confer-\n10\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n3\n[40] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. Advances in\nneural information processing systems, 32, 2019. 3\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[42] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca,\nNathan Lambert, Kashif Rasul, Mishig Davaadorj, and\nThomas Wolf. Diffusers: State-of-the-art diffusion models. 7,\n14\n[43] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir\nAberman. p+: Extended textual conditioning in text-to-image\ngeneration. arXiv preprint arXiv:2303.09522, 2023. 3\n[44] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact\ndiffusion inversion via coupled transformations. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 22532\u201322541, 2023. 2\n[45] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang,\nand Wangmeng Zuo. Elite: Encoding visual concepts into\ntextual embeddings for customized text-to-image generation.\narXiv preprint arXiv:2302.13848, 2023. 3, 7\n[46] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun\nZhang. Smartbrush: Text and shape guided object inpainting\nwith diffusion model. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22428\u201322437, 2023. 2\n[47] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360deg views. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4479\u20134489, 2023. 2\n[48] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 3\n[49] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei\nYang, Burcu Karagol Ayan, et al. Scaling autoregressive mod-\nels for content-rich text-to-image generation. arXiv preprint\narXiv:2206.10789, 2(3):5, 2022. 2\n[50] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning. arXiv\npreprint arXiv:2309.02591, 2023. 2\n[51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3\n[52] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang,\nChongyang Ma, Weiming Dong, and Changsheng Xu.\nInversion-based style transfer with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10146\u201310156, 2023. 2\n[53] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin\nBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-\ncontrolnet: All-in-one control to text-to-image diffusion mod-\nels. arXiv preprint arXiv:2305.16322, 2023. 3, 7\n11\nA. Additional Details: Data Generation Pipeline\nAs described in the main text, we utilize state-of-the-art pretrained models to synthesize training data. First, we use ChatGPT\nto generate captions for potential images. We use the following prompt for generating captions:\nGenerate a caption for an image where there is one main object and possibly other secondary objects. The object needs\nto be finite and solid. The main object should not be things like lakes, fields, sky, etc.\nYou should only respond in the following format: Caption: A photo of a [object], [describe object positioning in scene]\n[describe scene]\nSome examples of the generated captions are:\n\u2022 A photo of a red rose, nestled alone in a glass vase upon a lace table runner.\n\u2022 A photo of a vintage typewriter, resting on a weathered desk.\n\u2022 A photo of a faded Polaroid camera, lying on a sun-warmed picnic blanket.\n\u2022 A photo of a sewing machine, buzzing with creativity amidst colorful fabric swatches in a designer\u2019s studio.\n\u2022 A photo of a delicate porcelain teacup, delicately placed on a lace doily.\nFor each of these captions, we generate an image using Stable Diffusion 2.1[32] which is used as the target image. The\nformatting of the captions allows us to automatically extract the object categories by parsing the [object] part of the\ncaption. For example, for the above captions, the corresponding objects categories are: red rose, vintage typewriter,\nfaded Polaroid camera, sewing machine, delicate porcelain teacup. These object categories are used as input\nthe Segment Anything Model[18] to extract foreground images. These foreground images pasted on a white background are\nused as reference images.\n12\nB. Additional Details: Training and Inference\nIn this section, we present additional implementation details for training and inference with the BootPIG architecture. The\nsource code to reproduce our experiments will be released with the next update to the paper.\nTraining\nIn Algorithm 1, we present a pseudocode describing the training pipeline. For optimizing the parameters, we\nuse the AdamW algorithm with learning rate 5e-6, betas (0.9, 0.999), weight decay 1e-2 and epsilon 1e-8. The norm of the\ngradients is clipped to 1.0.\nAlgorithm 1: Training algorithm for BootPIG.\n1\nInitialize:\n2\nU\u03b8, U\u03d5 = Stable Diffusion UNet\n3\nVAE = Stable Diffusion VAE\n4\nnoise_scheduler = Stable Diffusion Noise Scheduler\n5\nU RSA\n\u03b8\n= Replace SA with RSA in U\u03b8\n6\ndataset = synthetic dataset containing 200k reference, target images and captions\n7\noptimizer = AdamW( (RSA layers of U\u03b8) + (All layers of U\u03d5)\n)\n8\n9\ndef collect_features(model, latents, timestep, caption):\n10\nPerform forward pass model(latents, timestep, caption)\n11\nfeatures = Collected features before each SA layer\n12\nreturn features\n13\n14\ncount = 0\n15\nfor reference, target, caption\nin dataset:\n16\nref = VAE(reference)\n17\ntar = VAE(target)\n18\nt = random.randint(0, 1000)\n19\n20\nnoise = randn_like(ref)\n21\nnoisedref = noise_sceduler.add_noise(ref, noise, t)\n22\nnoise = randn_like(tar)\n23\nnoisedtar = noise_sceduler.add_noise(tar, noise, t)\n24\n25\nFr = collect_features(U\u03d5, noisedref, t, caption)\n26\nif random.rand()<0.15:\n27\ncaption = \"\"\n28\n29\npredtar = U RSA\n\u03b8\n(noisedtar, t, caption, Fr)\n# Use corresponding features from F_r in RSA layers\n30\n31\nloss = ((noisedtar - predtar)**2).mean()\n32\nloss.backward()\n33\ncount += 1\n34\nif count %\n35\noptimizer.step()\n36\noptimizer.zero_grad()\n37\nInference\nDuring inference, we first perform foreground segmentation on each reference image using the TRACER[20]\nmodel implemented in the carvekit python package. Then we use the Reference UNet in the BootPIG architecture to collect\nreference features for each reference image. We then perform denoising using the Base UNet with the pooling strategy\ndescribed in Section 3.5.\n13\nC. Additional Comparisons to Test-Time Finetuning Methods\nCustomDiffusion\nBootPIG (ours)\nReference Samples\nViCo\n\u201ca dog wearing pink glasses\u201d\n\u201ca toy with a mountain in the background\u201d\n\u201ca sneaker on a cobblestone street\u201d\n \n\u201ca stuffed animal on top of a dirt road\n\u201ca boot on top of a white rug\u201d\nFigure 8. Qualitative Comparision: We provide additional visual comparisons of Custom Diffusion (reproduced), ViCo (reproduced), and\nBootPIG. In comparision, BootPIG generations demonstrate higher prompt fidelity (rows 2-4) and higher subject fidelity (rows 2, 3, 5).\nWe compare our proposed method, BootPIG, with two recent test-time finetuning approaches: CustomDiffusion [19] and\nViCo [10]. For both approaches, we train subject-specific models using each method on the DreamBooth [34] dataset. We\nuse the diffusers [42] implementation of CustomDiffusion and the official code provided by the authors of ViCo. We find\n14\nthat quantitatively (Table 3), BootPIG outperforms CustomDiffusion (+2.2 CLIP-T, +1.9 DINO) and ViCo (+3.1 CLIP-T,\nand +2.0 CLIP-I & +8.9 DINO) despite being zero-shot. These results highlight the benefits of BootPIG as the method\noutperforms existing methods in prompt and subject fidelity, both quantitatively (CLIP-T, DINO) and qualitatively (Figure 8).\nWe present additional qualitative comparisions with CustomDiffusion and ViCo on the DreamBooth dataset. Qualitative\nresults, provided in Figure 8, demonstrate the improvements in subject fidelity and prompt fidelity when using BootPIG. In\ncomparision to CustomDiffusion, we find that BootPIG provides exhibits greater fidelity to the prompt. Examples of this\nimprovement include: in row 2, CustomDiffusion\u2019s generation is missing the white rug; in row 3, CustomDiffusion adds an\nunnecessary pink bowtie to the subject; and in row 5 CustomDiffusion fails to place the stuffed animal on a dirt road. We also\nfind noticeable improvements in subject fidelity when comparing BootPIG to CustomDiffusion (shape of the duck in row 1,\nthe length of the fringes on the boot in row 2, and color and thickness of dog\u2019s fur in row 3). Similarly, BootPIG visually\noutperforms ViCo (e.g. fails to match the details of the subject in rows 2, 3, 5 and does not follow the prompt in row 4).\nMethod\nZero-shot\nCLIP-T\nCLIP-I\nDINO\nOurs\n\u2713\n31.1\n79.7\n67.4\nDreamBooth [34]\n\u2717\n30.5\n80.3\n66.8\nCustomDiffusion[19] (reproduced)\n\u2717\n28.9\n80.6\n65.5\nViCo [10] (reproduced)\n\u2717\n28.0\n77.7\n58.5\nReference Images\n-\n-\n88.5\n77.4\nTable 3. Quantitative Comparisons: We provide average metrics for subject fidelity (CLIP-I, DINO) and prompt fidelity (CLIP-T) on the\nDreamBooth dataset.\n15\nD. Additional Qualitative Results\nIn Figure 9, we present additional visualizations using our proposed method on several DreamBooth dataset images. We\nobserve that across different subjects, our proposed method is able to successfully maintain subject fidelity while accurately\nfollowing input prompts.\nBootPIG Generations\nReference Samples\n[object] = stuffed animal\n[object] = bowl\n[object] = cat\n[object] = dog\n[object] = red cartoon\n\u201c[object] in a forest\u201d\n\u201c[object] in the snow\u201d\n\u201c[object] with a city in the \nbackground\u201d\n\u201c[object] with a blue house \nin the background\u201d\n\u201c[object] in a forest\u201d\n\u201c[object] in the snow\u201d\n\u201c[object] on the beach\u201d\n\u201c[object] on top of green grass \nwith sunflowers around it\u201d\n\u201c[object] on top of a purple \nrug in a forest\u201d\n\u201c[object] in a chef outfit\u201d\n\u201c[object] in a purple wizard \noutfit\u201d\n\u201c[object] in a chef outfit\u201d\n\u201c[object] wearing pink \nglasses\u201d\n\u201ccube shaped [object]\u201d\n\u201c[object] on top of a dirt \nroad\u201d\n\u201c[object] wearing a red \nhat\u201d\n\u201c[object] wearing a santa \nhat\u201d\n\u201c[object] wearing a rainbow \nscarf\u201d\n\u201c[object] on top of a purple \nrug in a forest\u201d\n\u201c[object] in the snow\u201d\nFigure 9. Additional Visualizations: We present additional qualitative results on the DreamBooth dataset using our proposed method.\n16\nE. Additional Qualitative Ablation\nIn Figure 10, we present additional qualitative results to ablate the effect of training the BootPIG architecture and the\nimportance of augmenting the reference images during training. At initialization, we observe that the BootPIG architecture\ncan already copy some aspects of the subject\u2019s appearance. However, both the subject and prompt fidelity is limited, and the\nReference Samples\n\u201ca dog in a chef outfit\u201d\n\u201ca cat on top of a purple rug in a forest\u201d\n\u201ca can in the snow\u201d\nBootPIG \n(Initialization)\nBootPIG              \n(w/o augmentation)\nBootPIG\n\u201ca toy floating on top of water\u201d\n\u201ca dog in a firefighter outfit\u201d\nFigure 10. Qualitative Ablation:\n17\noverall quality of generated images is poor.\nIn Section 4.4 of the main text, we quantitatively show that synthetically augmenting the reference images leads to improve-\nments in final performance. Specifically, we random horizontal flipping, randomly resize the foreground in the images and\nplace the foreground at random locations in the white background images. We observed that the subject fidelity metrics\n(CLIP-I, DINO-I) demonstrate minor improvements when BootPIG is trained with such augmented images. However, in\npractice, we observed that training without augmentations led to significantly worse subject preservation and overall image\nquality. In Figure 10 Column 2 & 3, we demonstrate this by qualitatively comparing the effect of the synthetic augmentations.\n18\nF. Subject-Driven Inpainting\nWe additionally qualiatively explore the zero-shot capabilities of BootPIG for subject-driven inpainting. Given an input image,\nprompt, and binary segmentation mask, we use BootPIG to inpaint a given subject into the masked region of the image. We\nprovide examples of BootPIG inpainted images in Figure 11. Similar to the results in the text-to-image experiments, we find\nthat BootPIG can preserve the details of the target subject in the new scene (e.g. face and fur of the dog in row 1, the dog\nface on the backpack in row 2, and the face and wing of the duck in row 3). We note that BootPIG was not trained using an\ninpainting objective, thus limiting its capabilities as an image editing model. As a result, we find that BootPIG can struggle to\ninpaint target subjects when they are drastically different, in terms of size and/or shape, to the existing subject in the scene.\nFigure 11. Zero-shot Subject-Driven Inpainting: We provide visual comparisons of BootPIG inpainted images according to a given\nsubject. Despite not being trained using an inpainting objective, we find that BootPIG is able to accurately render the subject within the\noriginal scene.\n19\nG. Failures Cases\nThe BootPIG architecture does occasionally fail on certain (subject, caption) pairs. In Figure 12, we present examples of\nsome failed generations. We observe that a commonly occurring failure is in the case where the prompt attempts to modify\nthe appearance of the subject. For example, prompts of the form \"a cube shaped [object]\" attempts to modify the shape of\nreference [object] to a cube. Since the BootPIG architecture was trained on synthetic data where the target and reference\nappearance accurately matches, it fails to capture such modifications to the appearance during inference. We also observed\nthat the BootPIG architecture occasionally generates images depicting multiple instances of the reference object. Furthermore,\nas with any existing personalization method, our proposed model also occasionally fails at accurately capturing the appearance\nof the object (see Figure 12 row 3, column 2). As a guideline for practitioners, we find that regenerating the image with a\ndifferent sample of Gaussian noise often resolves the issue.\na can with a blue house \nin the background\na purple backpack\nrepeated subject\nappearance changing \nprompt \na cube shaped boot \na cube shaped toy \nappearance changing \nprompt \nappearance changing \nprompt \na purple stuffed animal \na teapot on top of a \ndirt road \nappearance changing \nprompt \npoor subject fidelity\nFigure 12. Failure Cases: We present examples of a few failure modes demonstrated by the BootPIG architecture.\n20\n"
  },
  {
    "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
    "link": "https://arxiv.org/pdf/2401.14257.pdf",
    "upvote": "9",
    "text": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation\nMinglin Chen1*\nWeihao Yuan2\nYukun Wang1\nZhe Sheng2\nYisheng He2\nZilong Dong2\nLiefeng Bo2\nYulan Guo1\u2020\n1 The Shenzhen Campus of Sun Yat-sen University, Sun Yat-sen University\n2 Alibaba Group\nFigure 1. Sketch2NeRF is a sketch-guided text-to-3D generative model that produces high-fidelity 3D objects resembling multi-view\nsketches. Top: our method can use an arbitrary number of sketches (usually more than 3) as input. Middle: generated 3D objects (shown\nas rendered RGB and normal images) of which the shapes are controlled by input sketches. Bottom: rendered RGB images at novel views.\nNote that, these 3D objects are generated using the same prompt of \u201ca teapot\u201d.\nAbstract\nRecently, text-to-3D approaches have achieved high-\nfidelity 3D content generation using text description. How-\never, the generated objects are stochastic and lack fine-\ngrained control. Sketches provide a cheap approach to in-\ntroduce such fine-grained control. Nevertheless, it is chal-\nlenging to achieve flexible control from these sketches due to\ntheir abstraction and ambiguity. In this paper, we present a\nmulti-view sketch-guided text-to-3D generation framework\n(namely, Sketch2NeRF) to add sketch control to 3D gen-\neration. Specifically, our method leverages pretrained 2D\ndiffusion models (e.g., Stable Diffusion and ControlNet) to\nsupervise the optimization of a 3D scene represented by\n* Work done during an internship supervised by Weihao Yuan at Alibaba\nGroup.\n\u2020 Corresponding author: guoyulan@mail.sysu.edu.cn\na neural radiance field (NeRF). We propose a novel syn-\nchronized generation and reconstruction method to effec-\ntively optimize the NeRF. In the experiments, we collected\ntwo kinds of multi-view sketch datasets to evaluate the pro-\nposed method. We demonstrate that our method can synthe-\nsize 3D consistent contents with fine-grained sketch control\nwhile being high-fidelity to text prompts. Extensive results\nshow that our method achieves state-of-the-art performance\nin terms of sketch similarity and text alignment.\n1. Introduction\nDistilling pretrained large-scale text-to-image diffusion\nmodels [33, 36] to neural radiance fields (NeRF) [2, 6, 9,\n27, 29, 39, 43] has emerged as a powerful paradigm for text-\nconditioned 3D object generation [7, 21, 25, 30, 46, 48, 49].\n1\narXiv:2401.14257v2  [cs.CV]  27 Jan 2024\nThe success of these models comes from the development\nof score distillation sampling (SDS) [30], which is further\nimproved within a variational framework like variational\nscore distillation (VSD) [49]. In spite of the high-fidelity,\ndiverse, and 3D-consistent results achieved by text-to-3D\nmethods, existing works lack fine-grained controllable abil-\nity during the generation procedure.\nTo achieve fine-grained controllable 3D generation, a\nstraightforward way is prompt-based approaches that con-\nstruct a fine-grained prompt with more specific words. Ex-\nisting text-to-3D approaches [17, 21] have shown the po-\ntential controllability of compositional prompts that com-\nbine two concrete concepts in a prompt template for 3D\ngeneration. Still, prompt-based approaches are difficult to\nachieve fine-grained control since a certain spatial geome-\ntry and concept is difficult to describe in words. In addition,\nseveral shape-guided approaches [5, 7, 25] use a coarse 3D\nmodel (e.g, SMPL model [24], morphable face model [20],\nand simple 3D primitives) as initial geometry to guide the\ngeneration. The generated model is highly sensitive to the\ninitial geometry, while the fine geometry model is not easy\nto obtain. More recently, image-based approaches explore\nto generate a 3D asset using one image [23, 44] or a few\nimages [32]. However, it is challenging for these methods\nto achieve fine-grained control in the 3D space with only\nmodifications in the images. Because humans find it hard to\ndraw a real image with rich texture. Therefore, fine-grained\ncontrol in 3D generation which guides spatial properties of\nthe 3D asset remains an open problem.\nIn 2D image generation, sketches offer a feasible and\npromising approach to provide controllable structure cues,\nbut it is challenging to generate plausible 3D assets from\nthe sketch-based constraint due to its simplicity and abstrac-\ntion. Some pioneering methods make attempts to generate\n3D shapes from single-view sketches [37]. Nevertheless,\nthe geometry constraints provided by the one-view sketch\nare insufficient to synthesize plausible 3D objects.\nIn this paper, we make the first attempt to tackle the\nmulti-view sketch-guided 3D object generation problem by\nproposing a novel framework (namely Sketch2NeRF). We\nfirst employ a neural radiance field (NeRF) [27] to repre-\nsent the underlying 3D object, and then leverage the pre-\ntrained 2D diffusion models (i.e., Stable Diffusion and Con-\ntrolNet) to supervise the optimization of NeRF. In partic-\nular, we propose a novel synchronized generation and re-\nconstruction mechanism to effectively optimize the NeRF.\nFurthermore, the annealed time schedule is introduced to\nimprove the quality of the generated object. For perfor-\nmance evaluation of the proposed sketch-guided 3D gen-\neration framework, we introduce sketch-based datasets and\nmetrics to evaluate the controllability of 3D object genera-\ntion. Experimental results demonstrate that our method can\nfine-grainedly control the 3D generation based on sketches,\nwhich offers flexible controllability to existing text-to-3D\nmethods.\nTo the best of our knowledge, this is the first work\nfor controllable generation of 3D shapes from multi-view\nsketches. Our main contributions in this paper are summa-\nrized as follows:\n\u2022 We propose a novel framework for multi-view sketch-\nguided 3D object generation, which enables fine-grained\ncontrol during generation.\n\u2022 We leverage sketch-conditional 2D diffusion models to\nguide the 3D generation, which eliminates the need for a\nlarge sketch-3D paired dataset.\n\u2022 We collect sketch-based generation datasets and evalu-\nation metrics to show the superior fine-grained control\nability of the proposed method over existing text-to-3D\ncounterparts.\n2. Related Work\nIn this section, we first review the 3D generation approaches\nand controllable generation. Then, we briefly describe the\ndevelopment of sketch-based generation.\n2.1. 3D Generation\nPrompt-based With the emergence of pretrained text-\nconditioned image generative diffusion models [36], lift-\ning these 2D models to achieve text-to-3D generation has\ngained popularity.\nPoole et al. [30] proposed DreamFu-\nsion with SDS for text-to-3D generation. The lack of 3D\nawareness in the 2D diffusion models destabilizes score\ndistillation-based methods from reconstructing a plausible\n3D object. To address this issue, Seo et al. [38] proposed\n3DFuse, a novel framework that incorporates 3D aware-\nness into pretrained 2D diffusion models, enhancing the\nrobustness and 3D consistency of score distillation-based\nmethods. Chen et al. [7] proposed Fantasia3D for high-\nquality text-to-3D content creation. This method disentan-\ngles modeling and learning of geometry and appearance\nto achieve high-resolution supervision from 2D diffusion\nmodels. Xu et al. [54] proposed a simple yet effective ap-\nproach that directly bridges the text and image modalities\nwith a powerful text-to-image diffusion model. Wang et\nal. [49] proposed to model the 3D parameter as a random\nvariable instead of a constant as in SDS and present VSD\nto explain and address over-saturation, over-smoothing, and\nlow-diversity problems.\nImage-based 3D generation from a single image or a few\nimages is an ill-posed problem. Previously, PixelNeRF [55]\nand GRF [45] are proposed to employ pretrained visual\nmodels (e.g., CNN [12]) as a prior. Recently, one-shot or\nfew-shot 3D generation approaches explore diffusion mod-\nels as a prior. Deng el al. [8] proposed NeRDi, a single-\nview NeRF generation framework with general image priors\nfrom 2D diffusion models. As an alternative, Wimbauer et\n2\nal. [50] proposed to predict an implicit density field from\na single image. Gu et al. [10] proposed NeRFDiff, which\naddresses the occlusion issue by distilling the knowledge of\na 3D-aware conditional diffusion model (CDM) into NeRF\nthrough synthesizing and refining a set of virtual views at\ntest-time. Liu et al. [23] proposed Zero-1-to-3 to generate\na 3D object from a single image by learning to control the\ncamera perspective in large-scale diffusion models. Tang et\nal. [44] proposed Make-it-3D to create high-fidelity 3D con-\ntent from a single image using pretrained diffusion priors.\n2.2. Controllable Generation\n2D Image The 2D generative approaches [25] based on\nthe diffusion probabilistic model [15] can synthesize high-\nfidelity images according to text description. To append\nmore controllable conditions (e.g., Canny edge, human\npose, semantic map, depth) in generation, ControlNet [57],\nT2I-Adapter [28], and Uni-ControlNet [59] are proposed to\nlearn task-specific conditions. Besides, DreamBooth [35]\ncontrols the content of synthesized images using a few im-\nages of the same subject. Recently, DragDiffusion [40] is\nproposed to achieve point-based interactive image genera-\ntion.\n3D Shape These controllable 3D generation approaches\nbased on 2D generative diffusion models use either coarse\nintialized geometry or controllable 2D models.\nLatent-\nNeRF [25] and Fantasia3D [7] use a coarse 3D model (e.g.,\nsimple 3D primitives) as initialized geometry, which is fur-\nther refined and textured during generation. In the other di-\nrecitons, DreamBooth3D [32] leverages DreamBooth [35]\nto specify the content of generated a 3D asset using 3\u223c5\nimages, while Instruct-NeRF2NeRF [11] and Instruct 3D-\nto-3D [18] employs Instruct-Pix2Pix [4] to edit a pretrianed\nNeRF.\n2.3. Sketch-based 3D Generation\n3D generation from the sketch is a challenging task, due to\nthe high-level abstraction and ambiguity of sketches. He et\nal. [13] proposed Sketch2Cloth, a sketch-based 3D garment\ngeneration system using the unsigned distance fields from\nthe user\u2019s sketch input.\nApplying sketches to neural ra-\ndiance fields [27] is also challenging due to the inherent\nuncertainty for 3D generation with coarse 2D constraints,\na significant gap in content richness, and potential incon-\nsistencies for sequential multi-view editing given only 2D\nsketch inputs. To address these challenges, Gao et al. [22]\npresent SketchFaceNeRF, a novel sketch-based 3D facial\nNeRF generation and editing method, to produce free-view\nphoto-realistic images. Recently, Sanghi et al. [37] pro-\nposed Sketch-A-Shape to generate a 3D shape from a single\nsketch by leveraging the pretrained CLIP model [31]. Wu et\nal. [52] proposed a sketch-guided diffusion model for col-\nored point cloud generation. Mikaeili et al. [26] present\nSKED to edit a NeRF using the sketch cues.\nExisting sketch-based 3D generation methods are\ninference-based and have the following limitations: 1) can\nonly input a single sketch; 2) require a large amount of\ndata to train the network; and 3) the generated objects are\nlimited to a few specific categories (e.g., face [22]). By\nleveraging the pretained 2D diffusion models, our method is\nan optimization-based 3D generation method that achieves\nopen-vocabulary object generation from an arbitrary num-\nber of sketches as input without any training data.\n3. Methodology\nGiven a set of N multiview sketch images {Sn}N\n1 with\nposes {\u03c0n}N\n1 , and a text prompt T, our goal is to gener-\nate a 3D asset that resembles specific sketches at all given\nangles. We employ an optimization-based framework that\nleverages pretrained 2D diffusion models (Sec. 3.2) as guid-\nance to fit a NeRF-based 3D representation (Sec. 3.1). The\noptimization details are described in Sec. 3.3. The overview\nof our method is shown in Fig. 2.\n3.1. 3D Representation\nWe represent a 3D object using the neural radiance fields\n(NeRF) [27], which is flexible and capable of rendering\nphoto-realistic images [1\u20133, 47]. Suppose the 3D object is\nlocated at the center of a bounded region (e.g., [\u22121, 1]3).\nFor each 3D point xxx = (x, y, z) and its view direction\nddd = (\u03b8, \u03d5), NeRF represents its density and view-dependent\ncolor as a continuous function:\n(\u03c3,ccc) = f\u03b8(xxx,ddd),\n(1)\nwhere \u03b8 is the parameters of the continuous function f. For\nfaster coverage and efficient memory cost, we implement f\nfollowing [29].\nWe employ the volume rendering technique [27] to ren-\nder the image with pose. In particular, the ray rrr = ooo+t\u00b7ddd is\ncalculated according to the image pose for each pixel. Then,\nwe sample points along the ray as {xxxi}. The density \u03c3i and\ncolor ccci of the sampled points xxxi are evaluated using Eq.\n(1). Finally, we obtain the ray color (i.e., the pixel color) by\nray marching:\nC(rrr) =\nX\ni\nTi(1 \u2212 exp(\u2212\u03c3i\u03b4i))ccci,\n(2)\nwhere Ti = exp(\u2212 Pi\u22121\nj=1 \u03c3j\u03b4j), and \u03b4i = ti+1 \u2212 ti is the\ninterval between adjacent sampled points. Image rendering\nis performed by repeating the rendering of each pixel.\n3.2. Sketch-conditioned Guidance\nGiven a randomly initialized NeRF, we iteratively update its\nweights using a 2D pretrained diffusion model as the guid-\nance. To incorporate the constraint of multi-view sketches,\n3\nFigure 2. Sketch2NeRF Overview. We represent a 3D object using a neural radiance field (NeRF) which is optimized based on the\nproposed synchronized generation and reconstruction optimization method. At the generation stage, the ControlNet is used to generate\nreal images at specific poses of sketches, while the Stable Diffusion is employed to generate real images at randomly sampled poses. At\nthe reconstruction stage, we update the NeRF parameters such that the reconstruction loss between the generated and rendered images is\nminimized.\nwe employ a pretrained 2D sketch-conditioned diffusion\nmodel.\nIn particular, the sketch-conditioned diffusion model\ncomprises of a variational auto-encoder (VAE) and a de-\nnoising network. The VAE consists of an encoder E and a\ndecoder D, where the encoder transforms an image xxx to a\nlatent variable as zzz = E(xxx) and the decoder reconstruct the\nimage from the latent variable as \u02c6xxx = D(zzz). The denoising\nnetwork implemented by an improved U-Net [34] is used to\ngenerate the latent variable zzz0 of images by iteratively de-\nnoising from a Gaussian noise zzzT . The denoising network\nestimates the noise \u02c6\u03f5\u03f5\u03f5t at level t:\n\u02c6\u03f5\u03f5\u03f5t := \u03f5\u03d5(zzzt; t,yyy),\n(3)\nwhere yyy is the conditional prompt. We improve the gen-\neration quality with classifier-free guidance [14], thus the\nestimated noise is obtained as:\n\u02c6\u03f5\u03f5\u03f5t := (1 + \u03c9) \u00b7 \u03f5\u03d5(zzzt; t,yyy) \u2212 \u03c9 \u00b7 \u03f5\u03d5(zzzt; t, \u2205),\n(4)\nwhere \u03c9 (\u03c9 = 7.5 in our experiments) is the weight for\nclassifier-free guidance, and \u2205 denotes the empty prompt.\nWe incorporate the sketch condition in generation fol-\nlowing [57]. The sketch condition is a binary image SSS \u2208\n{0, 1}H\u00d7W , where 0 denotes the canvas and 1 denotes the\nstroke of the sketch. The latent denoising network has an\nadditional sketch condition:\n\u02c6\u03f5\u03f5\u03f5t := \u03f5\u03d5(zzzt; t,yyy,SSS).\n(5)\n3.3. Optimization\nA naive approach for the 3D generation with multi-view\nsketch constraints is replacing Stable Diffusion in the ex-\nisting text-to-3D methods [49] with ControlNet. In prac-\ntice, we found that such methods lead to severely degraded\nperformance (as shown in Fig. 5). This is because the score-\nbased distillation optimization method [30, 49] is prone to\nfocusing on edges rather than the overall content with the\nSketch-conditioned ControlNet guidance.\nTo effectively\noptimize NeRF with ControlNet guidance, we propose a\nsynchronized generation and reconstruction method.\nGeneration. We leverage pretrained ControlNet and Stable\nDiffusion to generate real images at the specific poses of\nsketches and randomly sampled poses, respectively.\n1) Specific poses of sketches. For specific poses \u03c0s with\nthe sketch image constraint, we render an image IIIs from\nthe NeRF. The latent variable zzz of the image is added to the\nGaussian noise at level t:\nzzznoise = \u221a\u00af\u03b1tzzz +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03f5\u03f5,\n(6)\nwhere t \u223c U(0, 1), \u03f5\u03f5\u03f5 is the Gaussian noise, and \u00af\u03b1t :=\nQt\ns=1 \u03b1t, \u03b1s is a variance schedule for adding Gaussian\nnoise to the data.\nGiven the noisy latent variable zzznoise, we employ the\nsketch-conditioned ControlNet to iteratively denoise the la-\ntent variable for T steps using DDIM [41], resulting in a\nlatent variable zzz0. The generated image is obtained by de-\ncoding the latent variable as \u02c6IIIs.\n4\n(a) w/o\n(b) w/\nFigure 3. The rendered RGB and opacity images of the generated\n3D objects w/o and w/ the random viewpoint regularization. The\nrandom viewpoint regularization effectively eliminates the near-\nplane artifacts and the floaters for the generated 3D objects.\n2) Randomly sampled poses. The number of sketch im-\nages is usually small, and insufficient to reconstruct a NeRF.\nTraining NeRF with only supervision from sketch images\nproduces near-plane artifacts and many floaters (Fig. 3 (a)).\nWe introduce random viewpoint regularization to eliminate\nthis issue (Fig. 3 (b)). Specifically, we randomly sample\nthe pose \u03c0r at the upper hemisphere. The image IIIr is ren-\ndered from the NeRF. The corresponding generated image\n\u02c6IIIr is obtained using a similar way as in the specific poses\nof sketches, except for using Stable Diffusion.\n3) Annealed time schedule. During optimization, pre-\nvious text-to-3D approaches [30] add noise to the NeRF-\nrendered image at uniformly distributed noise level t \u223c\nU(tmin, tmax), where tmin and tmax are close to 0 and 1,\nrespectively. However, the generation cannot converge at\nthe last optimization iteration, leading to unrealistic results.\nSince the rendered image is realistic at the last generation,\na high level of noise will introduce large randomness. As\nshown in Fig. 4, the generated images become more differ-\nent from the original one when a higher level of noise is\nadded.\nBased on this observation, we propose an annealed time\nschedule for optimization. We linearly decrease tmax as fol-\nlows:\ntmax = t1 \u2212 (t1 \u2212 t0) n\nN ,\n(7)\nwhere t0 and t1 are the minimum and the maximum value\nof tmax, respectively. n and N are the current and the total\nsteps of the optimization, respectively.\nReconstruction. With images generated at specific poses\nand random poses, we optimize the NeRF using the recon-\nstruction loss. We compute the reconstruction loss for spe-\ncific poses as follows:\nLs(IIIs, \u02c6IIIs) = LLPIPS(IIIs, \u02c6IIIs) + LL1(IIIs, \u02c6IIIs),\n(8)\nwhere LLPIPS(\u00b7, \u00b7) and LL1(\u00b7, \u00b7) are the perceptual loss [58]\nand the L1 loss, respectively.\nWe also regularize the NeRF at randomly sampled view-\npoints. Similar to Ls, the regularization loss Lr is defined\nas:\nLr(IIIr, \u02c6IIIr) = LLPIPS(IIIr, \u02c6IIIr) + LL1(IIIr, \u02c6IIIr).\n(9)\nOriginal\nt = 0.2\nt = 0.4\nt = 0.6\nt = 0.8\nt = 0.98\nFigure 4. Images generated with different levels of noise. The gen-\nerated images are far different from the original when the added\nnoise is large (e.g., t = 0.98).\nTotally, we optimize the NeRF with the following recon-\nstruction loss:\nLtotal = \u03bbsLs + \u03bbrLr + \u03bbaLa,\n(10)\nwhere \u03bbs, \u03bbr, and \u03bba are balancing weights for losses Ls,\nLr, and La. La is the additional geometry regularizaition\nof NeRF used in DreamFusion.\n3.4. Implementation\nWe implement the proposed method based on the ThreeStu-\ndio* framework. We use the improved Mip-NeRF 360 [2]\nwith neural hashing encoding [29] as the 3D representation.\nWe use the Stable Diffuson v1.5 [25] along with ControlNet\nv1.1 [57] as our sketch-conditioned guidance. We set tmin,\nt0, and t1 to 0.02, 0.5, and 0.98, respectively. We optimize\nthe proposed method for 25000 iterations for each object\n(i.e., sketch images and a prompt). We use the Adam opti-\nmizer with an initial learning rate of 0.01. We set \u03bbs and \u03bbr\nto 10.0 and 1.0, respectively. We set \u03bba to 1.0 after 20000\niterations, otherwise 0. We found that the strategy of \u03bba\nis critical to generate high-quality geometry. Our method\ntakes around 2 hours to generate a 3D object on a single\nNVIDIA RTX 3090 GPU.\n4. Experiments\nWe first introduce the datasets (Sec. 4.1) and metrics\n(Sec. 4.2) for performance evaluation.\nWe then provide\ncomparisons with several baseline methods (Sec. 4.3 and\n4.4). Finally, we show the ablative results (Sec. 4.5).\n* https://github.com/threestudio-project/threestudio\n5\n4.1. Datasets\nSince the multi-view sketch dataset does not exist, we\ncollect two kinds of multi-view sketch datasets (i.e., the\nOmniObject3D-Sketch dataset and the THuman-Sketch\ndataset) to evaluate the proposed method.\nOmniObject3D-Sketch. We collect 20 categories of ob-\njects from the OmniObject3D dataset [51]. In particular,\nwe use 24 rendered images of each object provided by [51].\nThe sketch images are extracted by performing the HED\nboundary detector [53] on these rendered images, while the\nprompt is obtained by performing the image caption model\n(i.e., BLIP-2 [19]) on a rendered image for each object. The\nOmniObject3D-Sketch dataset covers a wide range of daily\n3D objects from the real world.\nTHuman-Sketch. We collect 20 different human meshes\nfrom the THuman 3.0 dataset [42]. After rendering 16 im-\nages with different poses located at the upper hemisphere\nof each mesh, we employ the HED boundary detector to\nobtain the sketch of each rendered image.\nThe prompts\nof all humans are fixed to \u201ca DSLR photo of a human\u201d.\nThe Thuman-Sketch dataset contains several human pos-\ntures which are hard to describe with texts but easy to depict\nwith sketches.\n4.2. Evaluation Metrics\nSketch Similarity We evaluate the similarity between the\ninput sketches and the generated object sketches.\nSince\nsketches can be considered as image edges, we introduce\ntwo edge-based metrics (i.e., chamfer distance (CD) [56],\nand hausdorff distance (HD) [16]) to evaluate the sketch\nsimilarity.\nText Alignment We evaluate the alignment of generated 3D\nobjects with their prompt texts using the CLIP R-Precision\nmetric [17, 30]. Specifically, we render images of size 512\u00d7\n512 for each generated object, where its pose is randomly\nsampled on the upper hemisphere. We use all prompts from\nthe OmniOBject3D-Sketch dataset. The CLIP R-Precision\nmeasures the retrieval accuracy of the rendered image and\nthe prompts. We provided the CLIP R-Precision metrics\nusing three different CLIP models [31] (i.e., ViT-B/32, ViT-\nB/16, and ViT-L-14).\n4.3. Baselines\nWe include two types of baseline methods for comparison.\nWe first compare our method to DreamFusion [30] and Pro-\nlificDreamer [49], which are proposed for text-conditioned\n3D object generation. DreamFusion is the first to distill\nknowledge from 2D diffusion model to 3D objects using\nthe SDS technique, while ProlificDreamer achieves state-\nof-the-art performance in the text-conditioned 3D genera-\ntion. To achieve multi-view sketch control, we replace the\nStable Diffusion used in DreamFusion and ProlificDreamer\nwith ControlNet.\nWe refer to the modified versions of\nDreamFusion and ProlificDreamer as C-DreameFusion and\nC-ProlificDreamer, respectively. Note that we use the im-\nplementation of DreamFusion in the ThreeStudio frame-\nwork, since the official codebase is not released.\n4.4. Results\nQualitative Comparisons. Fig. 5 shows the visual results\nof our method and other competitive baseline methods. The\nresults demonstrate that our method can generate a wide\nrange of plausible 3D objects. In addition, the generation\nof our method can be controlled by the input sketches. The\ngenerated 3D objects resemble the specific sketches in de-\ntail, e.g., the zipper of the backpack, and the flower position\non the hat (as shown in Fig. 5). In contrast, the generated\n3D objects of DreamFusion and ProlificDream are stochas-\ntic and cannot be constrained by multi-view sketches. Al-\nthough C-DreamFusion and C-ProlificDream can be guided\nby multi-view sketches, C-DreamFusion produces more\nnoise on the edge of 3D objects, while C-ProlificDreamer\nproduces an unpleasant background. This demonstrates the\nfine-grained controllability of our method in 3D generation.\nFig. 6 shows the human generation results produced by\nour method on the THuman-Sketch dataset. Human gener-\nation with only text conditions is highly challenging due to\nthe large variation of posture space in a generation. In prac-\ntice, we found that DreamFusion and ProlificDreamer failed\nto synthesize humans, resulting in empty or low-quality 3D\nshapes. However, our method can synthesize high-quality\nhumans with various postures and a fixed simple prompt\ntext (Fig. 6).\nThe generated humans not only resemble\nthe given sketches but also have high-fidelity clothes. This\ndemonstrates the promising performance of our method on\nhuman generation without explicit geometry constraints.\nFig. 7 shows that our method can generate consistent\n3D objects. DreamFusion faces the Janus problem, that\nis, the generated 3D object has multiple faces.\nFor ex-\nample, the teapot generated by DreamFusion has multiple\nspouts. This is because the 2D generative diffusion model\nused by DreamFusion is unaware of 3D constraints. With\nthe multi-view sketch constraint, our method can alleviate\nthe Janus problem. In addition, we find that our method\nproduces human-made regular 3D objects of higher quality\nthan DreamFusion. For example, DreamFusion can gener-\nate a chair with four legs but struggles to correctly arrange\nthem at four corners. With the sketch constraint, our method\nproduces a regular chair that matches the real world. This\ndemonstrates that our method can generate objects with 3D\nconsistency.\nQuantitative Comparisons. Tab. 1 shows the metrics of\nsketch similarity and text alignment achieved by various\nmethods. Compared to text-to-3D approaches (i.e., Dream-\nFusion and ProlificDreamer), our method achieves signif-\nicantly better sketch similarity in terms of CD and HD.\n6\n(a) Input Sketches\n(b) Ours\n(c) C-DreamFusion\n(d) C-ProlificDreamer\n(e) DreamFusion [30]\n(f) ProlificDreamer [49]\nFigure 5. Qualitative comparisons on 3 different objects with four baseline methods. Results clearly indicate that our method produces\nbetter consistent and high-fidelity 3D objects with multi-view sketch control.\nFigure 6. Visual results of human generation by our method on the THuman-Sketch dataset. We show four generated 3D humans. For each\n3D human, we show the input mult-view sketches (left), and the generated 3D human (right).\nOur method achieves a CD of 0.0091 which outperforms\nDreamFusion and ProlificDreamer by 0.032 and 0.034, re-\nspectively. This demonstrates that our method can be con-\ntrolled by given sketches. In addition, our method achieves\nbetter text alignment than ProlificDreamer in terms of R-\nPrecision CLIP with B/32, B/16, and L/14 ViT models.\nOur method improves ProlificDream by 1.7% in terms of\nR-Precision CLIP B/32.\nCompared to the modified version of DreamFusion and\nProlificDreamer, our method outperforms C-DreamFusion\nand C-ProlificDreamer by 0.0057 and 0.0367 in terms of\nCD, respectively.\nBesides, our method outperforms C-\nDreamFusion by 0.0782 in terms of R-Precision CLIP B/32.\nThis is because the proposed synchronized generation and\n7\nTable 1. Quantitative comparisons on the OmniObject3D-Sketch dataset. (Note that RP denotes R-Precision)\nSketch Input\nSketch Similarity\nText Alignment\nCD \u2193\nHD \u2193\nRP CLIP B/32 \u2191\nRP CLIP B/16 \u2191\nRP CLIP L/14 \u2191\nDreamFusion [30]\n\u00d7\n0.0411\n0.1876\n0.7511\n0.7753\n0.7972\nProlificDreamer [49]\n\u00d7\n0.0438\n0.1784\n0.7687\n0.7907\n0.8457\nC-DreamFusion\n\u2713\n0.0148\n0.1478\n0.7039\n0.7171\n0.7848\nC-ProlificDreamer\n\u2713\n0.0458\n0.3273\n0.0675\n0.0531\n0.0491\nSketch2NeRF (Ours)\n\u2713\n0.0091\n0.1233\n0.7821\n0.8110\n0.8597\n(a) DreamFusion [30]\n(b) Ours\nFigure 7. Comparisons on hard cases with DreamFusion.\nreconstruction method can effectively optimize the NeRF\nwith sketch-conditioned guidance. These results show that\nour method achieves state-of-the-art performance in terms\nof sketch similarity and text alignment.\n4.5. Ablation Study\nVariation of Sketches. Fig. 1 shows the results of the pro-\nposed method with different sketches as input. Note that all\nthe prompts are fixed to \u201da teapot\u201d, our method can produce\nhigh-fidelity 3D objects resembling the specific sketches\n(e.g., the handle shape, the spout position, and the body\nsize). This demonstrates that our method can generate di-\nverse 3D objects specified by varied sketches.\nQuantity of Sketch Images. We generated a 3D sofa by\nusing sketches with quantities of 24, 12, 6, and 3, respec-\ntively. Fig. 8 shows that our method can produce 3D con-\nsistent objects with plausible geometry even using only 3\nsketches. This is because the random viewpoint regulariza-\ntion ensures the reality of the generated object.\nNoisy Poses. We conducted experiments to study the ro-\nFigure 8. Generated objects using various quantities of sketch im-\nages. From top to bottom, each row shows the results (of rendered\nRGB and normal images) using 24, 12, 6, and 3 sketch images as\ninput, respectively.\nbustness of our method for the noise poses. We perturb the\nsketch poses with different intensities (i.e., 0, 0.01, 0.02,\nand 0.03), such that the multi-view sketches cannot main-\ntain consistency. Fig. 9 shows that higher-intensity noises\nlead to blurry contents and coarse geometry, because incon-\nsistent multi-view sketches introduce higher uncertainty in\n3D generation.\n0\n0.01\n0.02\n0.03\nFigure 9. Rendered RGB and normal images of objects generated\nunder various intensities noises of sketch poses.\n5. Conclusion\nIn this paper, we proposed a novel multi-view sketch-guided\ntext-to-3D generation method (namely Sketch2NeRF) to\ngenerate high-fidelity 3D content that resembles given\nsketches.\nIn particular,\nwe proposed a novel syn-\nchronous\ngeneration\nand\nreconstruction\noptimization\nstrategy to effectively fit the NeRF from 2D Control-\n8\nNet guidance.\nIn addition, we employ the annealed\ntime schedule to further improve the quality of NeRF\nin the generation procedure.\nWe collected two types\nof datasets to evaluate the proposed methods.\nEx-\nperimental results demonstrate the superior performance\nof our method on fine-grained control provided by sketches.\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-NeRF: A multiscale representation for anti-aliasing\nneural radiance fields.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5855\u2013\n5864, 2021. 3\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 1, 5\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan,\nand\nPeter\nHedman.\nZip-NeRF:\nAnti-\naliased grid-based neural radiance fields.\narXiv preprint\narXiv:2304.06706, 2023. 3\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n3\n[5] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. DreamAvatar: Text-and-shape guided 3D hu-\nman avatar generation via diffusion models. arXiv preprint\narXiv:2304.00916, 2023. 2\n[6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. TensoRF: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 1\n[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3D: Disentangling geometry and appearance for\nhigh-quality text-to-3D content creation.\narXiv preprint\narXiv:2303.13873, 2023. 1, 2, 3\n[8] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNeRDi: Single-view nerf synthesis with language-guided\ndiffusion as general image priors.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20637\u201320647, 2023. 2\n[9] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 1\n[10] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNeRFDiff: Single-image view synthesis with NeRF-guided\ndistillation from 3D-aware diffusion. In International Con-\nference on Machine Learning, pages 11808\u201311826. PMLR,\n2023. 3\n[11] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander\nHolynski, and Angjoo Kanazawa.\nInstruct-NeRF2NeRF:\nEditing 3D scenes with instructions.\narXiv preprint\narXiv:2303.12789, 2023. 3\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016. 2\n[13] Yi He, Haoran Xie, and Kazunori Miyata. Sketch2Cloth:\nSketch-based 3D garment generation with unsigned distance\nfields. arXiv preprint arXiv:2303.00167, 2023. 3\n[14] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[16] Daniel P Huttenlocher,\nGregory A. Klanderman,\nand\nWilliam J Rucklidge. Comparing images using the Haus-\ndorff distance. IEEE Transactions on pattern analysis and\nmachine intelligence, 15(9):850\u2013863, 1993. 6\n[17] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2, 6\n[18] Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato\nIshii, and Takuya Narihira.\nInstruct 3D-to-3D: Text in-\nstruction guided 3D-to-3D conversion.\narXiv preprint\narXiv:2303.15780, 2023. 3\n[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 6\n[20] Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara,\nOwen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha\nPrasad, Bipin Kishore, Jun Xing, et al.\nLearning forma-\ntion of physically-based face attributes. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 3410\u20133419, 2020. 2\n[21] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution\ntext-to-3D content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 1, 2\n[22] Gao Lin, Liu Feng-Lin, Chen Shu-Yu, Jiang Kaiwen, Li\nChunpeng, Yukun Lai, and Fu Hongbo. SketchFaceNeRF:\nSketch-based facial generation and editing in neural radiance\nfields. ACM Transactions on Graphics, 2023. 3\n[23] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-\n3:\nZero-shot one image to 3D object.\narXiv preprint\narXiv:2303.11328, 2023. 2, 3\n[24] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black.\nSMPL: A skinned\n9\nmulti-person linear model.\nACM Trans. Graphics (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 2\n[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-NeRF for shape-guided generation\nof 3D shapes and textures. arXiv preprint arXiv:2211.07600,\n2022. 1, 2, 3, 5\n[26] Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or,\nand Ali Mahdavi-Amiri. SKED: Sketch-guided text-based\n3d editing. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 14607\u201314619, 2023.\n3\n[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 1,\n2, 3\n[28] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 3\n[29] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 1, 3, 5\n[30] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D diffusion. arXiv\npreprint arXiv:2209.14988, 2022. 1, 2, 4, 5, 6, 7, 8\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 6\n[32] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nBooth3D: Subject-driven text-to-3D generation.\narXiv\npreprint arXiv:2303.13508, 2023. 2, 3\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1\n[34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 4\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 3\n[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1, 2\n[37] Aditya\nSanghi,\nPradeep\nKumar\nJayaraman,\nArianna\nRampini,\nJoseph Lambourne,\nHooman Shayani,\nEvan\nAtherton, and Saeid Asgari Taghanaki.\nSketch-A-Shape:\nZero-shot sketch-to-3D shape generation.\narXiv preprint\narXiv:2307.03869, 2023. 2, 3\n[38] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim. Let 2D diffusion model know 3D-\nconsistency for robust text-to-3D generation. arXiv preprint\narXiv:2303.07937, 2023. 2\n[39] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep Marching Tetrahedra: A hybrid repre-\nsentation for high-resolution 3D shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 1\n[40] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-\ncent YF Tan, and Song Bai.\nDragDiffusion: Harnessing\ndiffusion models for interactive point-based image editing.\narXiv preprint arXiv:2306.14435, 2023. 3\n[41] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 4\n[42] Zhaoqi Su, Tao Yu, Yangang Wang, and Yebin Liu. Deep-\nCloth: Neural garment representation for shape and style\nediting. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 45(2):1581\u20131593, 2022. 6\n[43] Cheng Sun, Min Sun, and Hwann-Tzong Chen.\nDirect\nVoxel Grid Optimization: Super-fast convergence for radi-\nance fields reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5459\u20135469, 2022. 1\n[44] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-It-3D: High-fidelity\n3D creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 2, 3\n[45] Alex Trevithick and Bo Yang. GRF: Learning a general radi-\nance field for 3D representation and rendering. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 15182\u201315192, 2021. 2\n[46] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. TextMesh: Gen-\neration of realistic 3D meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 1\n[47] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T Barron, and Pratul P Srinivasan.\nRef-NeRF:\nStructured view-dependent appearance for neural radiance\nfields. In 2022 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5481\u20135490. IEEE,\n2022. 3\n[48] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score Jacobian Chaining: Lifting\npretrained 2D diffusion models for 3D generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 1\n10\n[49] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity\nand diverse text-to-3D generation with variational score dis-\ntillation. arXiv preprint arXiv:2305.16213, 2023. 1, 2, 4, 6,\n7, 8\n[50] Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel\nCremers. Behind the Scenes: Density fields for single view\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9076\u2013\n9086, 2023. 3\n[51] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,\net al. OmniObject3D: Large-vocabulary 3D object dataset\nfor realistic perception, reconstruction and generation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 803\u2013814, 2023. 6\n[52] Zijie Wu, Yaonan Wang, Mingtao Feng, He Xie, and Ajmal\nMian. Sketch and text guided diffusion model for colored\npoint cloud generation.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 8929\u2013\n8939, 2023. 3\n[53] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In Proceedings of the IEEE international conference\non computer vision, pages 1395\u20131403, 2015. 6\n[54] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3D: Zero-shot\ntext-to-3D synthesis using 3D shape prior and text-to-image\ndiffusion models.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n20908\u201320918, 2023. 2\n[55] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\nPixelNeRF: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 2\n[56] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and\nMartial Hebert. PCN: Point completion network. In 2018 in-\nternational conference on 3D vision (3DV), pages 728\u2013737.\nIEEE, 2018. 6\n[57] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 3, 4, 5\n[58] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[59] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin\nBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-\nControlNet: All-in-one control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2305.16322, 2023. 3\n11\n"
  },
  {
    "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
    "link": "https://arxiv.org/pdf/2401.14403.pdf",
    "upvote": "8",
    "text": "Adaptive Mobile Manipulation for Articulated Objects\nIn the Open World\nHaoyu Xiong\nCMU\nRussell Mendonca\nCMU\nKenneth Shaw\nCMU\nDeepak Pathak\nCMU\nFig. 1: Open-World Mobile Manipulation System: We use a full-stack approach to operate articulated objects such as real-world doors,\ncabinets, drawers, and refrigerators in open-ended unstructured environments.\nAbstract\u2014 Deploying robots in open-ended unstructured en-\nvironments such as homes has been a long-standing research\nproblem. However, robots are often studied only in closed-off\nlab settings, and prior mobile manipulation work is restricted\nto pick-move-place, which is arguably just the tip of the iceberg\nin this area. In this paper, we introduce Open-World Mobile\nManipulation System, a full-stack approach to tackle realistic\narticulated object operation, e.g. real-world doors, cabinets,\ndrawers, and refrigerators in open-ended unstructured environ-\nments. The robot utilizes an adaptive learning framework to ini-\ntially learns from a small set of data through behavior cloning,\nfollowed by learning from online practice on novel objects that\nfall outside the training distribution. We also develop a low-cost\nmobile manipulation hardware platform capable of safe and\nautonomous online adaptation in unstructured environments\nwith a cost of around 25,000 USD. In our experiments we\nutilize 20 articulate objects across 4 buildings in the CMU\ncampus. With less than an hour of online learning for each\nobject, the system is able to increase success rate from 50% of\nBC pre-training to 95% using online adaptation. Video results\nat https://open-world-mobilemanip.github.io/.\nI. INTRODUCTION\nDeploying robotic systems in unstructured environments\nsuch as homes has been a long-standing research problem.\nIn recent years, significant progress has been made in de-\nploying learning-based approaches [1]\u2013[4] towards this goal.\nHowever, this progress has been largely made independently\neither in mobility or in manipulation, while a wide range of\npractical robotic tasks require dealing with both aspects [5]\u2013\n[8]. The joint study of mobile manipulation paves the way\nfor generalist robots which can perform useful tasks in\nopen-ended unstructured environments, as opposed to being\nrestricted to controlled laboratory settings focused primarily\non tabletop manipulation.\nHowever, developing and deploying such robot systems\nin the open-world with the capability of handling unseen\nobjects is challenging for a variety of reasons, ranging from\nthe lack of capable mobile manipulator hardware systems\nto the difficulty of operating in diverse scenarios. Con-\nsequently, most of the recent mobile manipulation results\nend up being limited to pick-move-place tasks [9]\u2013[11],\nwhich is arguably representative of only a small fraction of\nproblems in this space. Since learning for general-purpose\nmobile manipulation is challenging, we focus on a restricted\nclass of problems, involving the operation of articulated\nobjects, such as doors, drawers, refrigerators, or cabinets in\nopen-world environments. This is a common and essential\ntask encountered in everyday life, and is a long-standing\nproblem in the community [12]\u2013[18]. The primary challenge\nis generalizing effectively across the diverse variety of such\nobjects in unstructured real-world environments rather than\nmanipulating a single object in a constrained lab setup.\nFurthermore, we also need capable hardware, as opening a\ndoor not only requires a powerful and dexterous manipulator,\nbut the base has to be stable enough to balance while the door\nis being opened and agile enough to walk through.\nWe take a full-stack approach to address the above\narXiv:2401.14403v2  [cs.RO]  28 Jan 2024\nchallenges. In order to effectively manipulate objects in\nopen-world settings, we adopt a adaptive learning approach,\nwhere the robot keeps learning from online samples collected\nduring interaction. Hence even if the robot encounters a\nnew door with a different mode of articulation, or with\ndifferent physical parameters like weight or friction, it can\nkeep adapting by learning from its interactions. For such\na system to be effective, it is critical to be able to learn\nefficiently, since it is expensive to collect real world samples.\nThe mobile manipulator we use as shown in Figure. 3 has\na very large number of degrees of freedom, corresponding\nto the base as well as the arm. A conventional approach for\nthe action space of the robot could be regular end-effector\ncontrol for the arm and SE2 control for the base to move in\nthe plane. While this is very expressive and can cover many\npotential behaviors for the robot to perform, we will need to\ncollect a very large amount of data to learn control policies\nin this space. Given that our focus is on operating articulated\nobjects, can we structure the action space so that we can get\naway with needing fewer samples for learning?\nConsider the manner in which people typically approach\noperating articulated objects such as doors. This generally\nfirst involves reaching towards a part of the object (such\nas a handle) and establishing a grasp. We then execute con-\nstrained manipulation like rotating, unlatching, or unhooking,\nwhere we apply arm or body movement to manipulate the\nobject. In addition to this high-level strategy, there are also\nlower-level decisions made at each step regarding exact\ndirection of movement, extent of perturbation and amount of\nforce applied. Inspired by this, we use a hierarchical action\nspace for our controller, where the high-level action sequence\nfollows the grasp, constrained manipulation strategy. These\nprimitives are parameterized by learned low-level continu-\nous values, which needs to be adapted to operate diverse\narticulated objects. To further bias the exploration of the\nsystem towards reasonable actions and avoid unsafe actions\nduring online sampling, we collect a dataset of expert demon-\nstrations on 12 training objects, including doors, drawers\nand cabinets to train an initial policy via behavior cloning.\nWhile this is not very performant on new unseen doors\n(getting around 50% accuracy), starting from this policy\nallows subsequent learning to be faster and safer.\nLearning via repeated online interaction also requires\ncapable hardware. As shown in Figure 3, we provide a\nsimple and intuitive solution to build a mobile manipula-\ntion hardware platform, followed by two main principles:\n(1) Versatility and agility - this is essential to effectively\noperate diverse objects with different physical properties in\npotentially challenging environments, for instance a cluttered\noffice. (2) Affordabiluty and Rapid-prototyping - Assembled\nwith off the shelf components, the system is accessible and\ncan be readily be used by most research labs.\nIn this paper, we present Open-World Mobile Manipu-\nlation System, a full stack approach to tackle the problem\nof mobile manipulation of realistic articulated objects in the\nopen world. Efficient learning is enabled by a structured ac-\ntion space with parametric primitives, and by pretraining the\npolicy on a demonstration dataset using imitation learning.\nAdaptive learning allows the robot to keep learning from\nself-practice data via online RL. Repeated interaction for\nautonomous learning requires capable hardware, for which\nwe propose a versatile, agile, low-cost easy to build system.\nWe introduce a low-cost mobile manipulation hardware plat-\nform that offers a high payload, making it capable of repeated\ninteraction with objects, e.g. a heavy, spring-loaded door, and\na human-size, capable of maneuvering across various doors\nand navigating around narrow and cluttered spaces in the\nopen world. We conducted a field test of 8 novel objects\nranging across 4 buildings on a university campus to test\nthe effectiveness of our system, and found adaptive earning\nboosts success rate from 50% from the pre-trained policy to\n95% after adaptation.\nII. RELATED WORK\na) Adaptive Real-world Robot Learning: There has\nbeen a lot of prior work that studies how robots can acquire\nnew behavior by directly using real-world interaction sam-\nples via reinforcement learning using reward [19]\u2013[22], and\neven via unsupervised exploration [23], [24]. More recently\nthere have been approaches that use RL to fine-tune policies\nvery efficiently that have been initialized via by imitating\ndemonstrations [25], [26]. Other methods aim to do so\nwithout access to demonstrations on the test objects, and\npretrain using other sources of data - either using offline\nrobot datasets [27], simulation [28] or human video\n[29],\n[30] or a combination of these approaches [10]. We operate\nin a similar setting, without any demonstrations on test\nobjects, and focus on demonstrating RL adaptation on mobile\nmanipulation systems that can be deployed in open-world\nenvironments. While prior large-scale industry efforts also\ninvestigate this [10], we seek to be able to learn much more\nefficiently with fewer data samples.\nb) Learning-based Mobile Manipulation Systems. : In\nrecent years, the setup for mobile manipulation tasks in both\nsimulated and real-world environments has been a prominent\ntopic of research [5], [31]\u2013[39]. Notably, several studies have\nexplored the potential of integrating Large Language Models\ninto personalized home robots, signifying a trend towards\nmore interactive and user-friendly robotic systems [39]\u2013\n[41]. While these systems display impressive long horizon\ncapabilities using language for planning, these assume fixed\nlow-level primitives for control. In our work we seek to learn\nlow-level control parameters via interaction. Furthermore,\nunlike the majority of prior research which predominantly\nfocuses on pick-move-place tasks [9], we consider operat-\ning articulated objects in unstructured environments, which\npresent an increased level of difficulty.\nc) Door Manipulation: The research area of door open-\ning has a rich history in the robotics community [15]\u2013[18],\n[42]. A significant milestone in the domain was the DARPA\nRobotics Challenge (DRC) finals in 2015. The accomplish-\nment of the WPI-CMU team in door opening illustrated not\nonly advances in robotic manipulation and control but also\nthe potential of humanoid robots to carry out intricate tasks in\nEnv\nGrasp\nMobile-Manipulation\nFig. 2: Adaptive Learning Framework: The policy outputs\nlow-level parameters for the grasping primitive, and chooses\na sequence of manipulation primitives and their parameters.\nreal-world environments [12]\u2013[14]. Nevertheless, prior to the\ndeep learning era, the primary impediment was the robots\u2019\nperception capabilities, which faltered when confronted with\ntasks necessitating visual comprehension of complex and\nunstructured environments. Approaches using deep learning\nto address vision challenges include Wang et al. [43], which\nleverages synthetic data to train keypoint representation for\nthe grasping pose estimation, and Qin et, al. [44], which\nproposed an end-end point cloud RL framework for sim2real\ntransfer. Another approach is to use simulation to learn\npolicies, using environments such as Doorgym [45], which\nprovides a simulation benchmark for door opening tasks.\nThe prospect of large-scale RL combined with sim-to-real\ntransfer holds great promise for generalizing to a diverse\nrange of doors in real-world settings [44]\u2013[46]. However,\none major drawback is that the system can only generalize\nto the space of assets already present while training in the\nsimulation. Such policies might struggle when faced with a\nnew unseen door with physical properties, texture or shape\ndifferent from the training distribution. Our approach can\nkeep on learning via real-world samples, and hence can learn\nto adapt to difficulties faced when operating new unseen\ndoors.\nIII. ADAPTIVE LEARNING FRAMEWORK\nIn this section, we describe our algorithmic framework for\ntraining robots for adaptive mobile manipulation of everyday\narticulated objects. To achieve efficient learning, we use a\nstructured hierarchical action space. This uses a fixed high-\nlevel action strategy and learnable low-level control param-\neters. Using this action space, we initialize our policy via\nbehavior cloning (BC) with a diverse dataset of teleoperated\ndemonstrations. This provides a strong prior for exploration\nand decreases the likelihood of executing unsafe actions.\nHowever, the initialized BC policy might not generalize\nto every unseen object that the robot might encounter due\nto the large scope of variation of objects in open-world\nenvironments. To address this, we enable the robot to learn\nfrom the online samples it collects to continually learn and\nadapt. We describe the continual learning process as well as\ndesign considerations for online learning.\nA. Action Space\nFor greater learning efficiency, we use a parameterized\nprimitive action space. Concretely, we assume access to a\nAlgorithm 1 Adaptive Learning\nRequire: Grasping primitive G(.) taking parameter g\nRequire: Constrained manipulation primitives M(.), taking\nparameter C and c.\n1: Initialize primitive classifier \u03c0\u03c6({Ci}N\ni=1|I)\n2: Initialize conditional action policy\n\u03c0\u03b8(g,{ci}N\ni=1|I,{Ci}N\ni=1)\n3: Collect a dataset D of expert demos\n{I,g,{Ci}N\ni=1,{ci}N\ni=1}\n4: Train \u03c0\u03c6 and \u03c0\u03b8 on D using Imitation Learning 2\n5: for online RL iteration 1:N do\n6:\nGiven image Is, sample {Ci}N\ni=1 \u223c \u03c0\u03c6(.|Is),\nsample (g,{ci}N\ni=1) \u223c \u03c0\u03b8(.|Is)\n7:\nExecute trajectory {G(g),{M(Ci,ci)}N\ni=1},\nobserve reward R\n8:\nUpdate policies \u03c0\u03c6 and \u03c0\u03b8 using RL (Eqs. 5, 4, 2)\n9: end for\ngrasping primitive G(.) parameterized by g. We also have\na constrained mobile-manipulation primitives M(.), where\nprimitive M(.) takes two parameters, a discrete parameter\nC and a continuous parameter c. Trajectories are executed\nin an open-loop manner, a grasping primitive followed by a\nsequence of N constrained mobile-manipulation primitives:\n{Is,G(g),{M(Ci,ci)}N\ni=1,If ,R}\nwhere Is is the initial observed image, G(g), M(Ci,ci))\ndenote the parameterized grasp and constrained manipulation\nprimitives respectively, If is the final observed image, and r\nis the reward for the trajectory. While this structured space is\nless expressive than the full action space, it is large enough to\nlearn effective strategies for the everyday articulated objects\nwe encountered, covering 20 different doors, drawers, and\nfridges in open-world environments. The key benefit of the\nstructure is that it allows us to learn from very few samples,\nusing only on the order of 20-30 trajectories. We describe\nthe implementation details of the primitives in section IV-B.\nB. Adaptive Learning\nGiven an initial observation image Is, we use a classifier\n\u03c0\u03c6({Ci}N\ni=1|I) to predict the a sequence of N discrete pa-\nrameters {Ci}N\ni=1 for constrained mobile-manipulation, and\na conditional policy network \u03c0\u03b8(g,{ci}N\ni=1|I,{Ci}N\ni=1) which\nproduces the continuous parameters of the grasping primi-\ntive and a sequence of N constrained mobile-manipulation\nprimitives. The robot executes the parameterized primitives\none by one in an open-loop manner.\n1) Imitation: We start by initializing our policy using\na small set of expert demonstrations via behavior cloning.\nThe details of this dataset are described in section IV-\nC. The imitation learning objective is to learn policy pa-\nrameters \u03c0\u03b8,\u03c6 that maximize the likelihood of the expert\nactions. Specifically, given a dataset of image observations Is,\nand corresponding actions {g,{Ci}N\ni=1,{ci}N\ni=1}, the imitation\nlearning objective is:\nmax\n\u03c6,\u03b8\n\u0002\nlog\u03c0\u03c6({Ci}N\ni=1 |Is)+log\u03c0\u03b8(g,{ci}N\ni=1 |{Ci}N\ni=1,Is)\n\u0003\n(1)\n2) Online RL: The central challenge we face is operating\nnew articulated objects that fall outside the behavior cloning\ntraining data distribution. To address this, we enable the\npolicy to keep improving using the online samples collected\nby the robot. This corresponds to maximizing the expected\nsum of rewards under the policy :\nmax\n\u03b8,\u03c6 E\u03c0\u03b8,\u03c6\n\"\nT\n\u2211\nt=0\nr(st,at)\n#\n(2)\nSince we utilize a highly structured action space as de-\nscribed previously, we can optimize this objective using a\nfairly simple RL algorithm. Specifically we use the REIN-\nFORCE objective [47]:\n\u2207\u03b8,\u03c6J(\u03b8,\u03c6) = E\u03c0\u03b8,\u03c6\n\"\nT\n\u2211\nt=0\n\u2207\u03b8 log\u03c0(at|st)\u00b7rt\n#\n(3)\n= E\u03c0\u03c6,\u03b8\n\u0002\n(\u2207\u03c6 log\u03c0\u03c6(Ci|I)+\u2207\u03b8 log\u03c0\u03b8(g,ci|Ci,I))\u00b7R\n\u0003\n(4)\nwhere R is the reward provided at the end of trajectory ex-\necution. Note that we only have a single time-step transition,\nall actions are determined from the observed image Is, and\nexecuted in an open-loop manner. Further details for online\nadaptation such as rewards, resets and safety are detailed in\nsection IV-D.\n3) Overall Finetuning Objective: To ensure that the policy\ndoesn\u2019t deviate too far from the initialization of the imitation\ndataset, we use a weighted objective while finetuning, where\nthe overall loss is :\nLoverall = Lonline +\u03b1 \u2217Loffline\n(5)\nwhere loss on online sampled data is optimized via Eq.4\nand loss on the batch of offline data is optimized via BC as\nin Eq.2. We use equal sized batches for online and offline\ndata while performing the update.\nIV. OPEN-WORLD MOBILE MANIPULATION SYSTEMS\nIn this section, we describe details of our full-stack ap-\nproach encompassing hardware, action space for efficient\nlearning, the demonstration dataset for initialization of the\npolicy and crucially details of autonomous, safe execution\nwith rewards. This enables our mobile manipulation system\nto adaptively learn in open-world environments, to manip-\nulate everyday articulated objects like cabinets, drawers,\nrefrigerators, and doors.\nA. Hardware\nThe transition from tabletop manipulation to mobile ma-\nnipulation is challenging not only from algorithmic studies\nbut also from the perspective of hardware. In this project,\nwe provide a simple and intuitive solution to build a mobile\nmanipulation the hardware platform. Specifically, our design\naddresses the following challenges -\n  server\nonboard \ncomputer\nomni-base\nodometer\n  camera\narm & EE\nX\nY\nX\nY\nZ\nFig. 3: Mobile Manipulation Hardware Platform: Differ-\nent components in the mobile manipulator hardware system.\nOur design is low-cost and easy-to-build with off-the-shelf\ncomponents\n\u2022 Versatility and agility: Everyday articulated objects like\ndoors have a wide degree of variation of physical\nproperties, including weight, friction and resistance.\nTo successfully operate these, the platform must offer\nhigh payload capabilities via a strong arm and base.\nAdditionally, we sought to develop a human-sized, agile\nplatform capable of maneuvering across various real-\nworld doors and navigating unstructured and narrow\nenvironments, such as cluttered office spaces.\n\u2022 Affordability and Rapid-Prototyping: The platform is\ndesigned to be low-cost for most robotics labs and em-\nploys off-the-shelf components. This allows researchers\nto quickly assemble the system with ease, allowing the\npossibility of large-scale open-world data collection in\nthe future.\nWe show the different components of the hardware system\nin Figure 3. Among the commercially available options,\nwe found the Ranger Mini 2 from AgileX to be an ideal\nchoice for robot base due to its stability, omni-directional\nvelocity control, and high payload capacity. The system uses\nan xArm for manipulation, which is an effective low-cost\narm with a high payload (5kg), and is widely accessible\nfor research labs. The system uses a Jetson computer to\nsupport real-time communication between sensors, the base,\nthe arm, as well as a server that hosts large models. We\nuse a D435 Intel Realsense camera mounted on the frame to\ncollect RGBD images as ego-centric observations and a T265\nIntel Realsense camera to provide visual odometry which is\ncritical for resetting the robot when performing trials for RL.\nThe gripper is equipped with a 3d-printed hooker and an anti-\nslip tape to ensure a secure and stable grip. The overall cost\nof the entire system is around 25,000 USD, making it an\naffordable solution for most robotics labs.\nWe compare key aspects of our modular platform with\nthat of other mobile manipulation platforms in Table I.This\ncomparison highlights advantages of our system such as cost-\n      train     set\ntest  set\n \nFig. 4: Articulated Objects: Visualization of the 12 training and 8 testing objects used, with location indicators corresponding to the\nbuildings in the map below. The training and testing objects are significantly different from each other, in terms of different visual\nappearances, different modes of articulation, or different physical parameters, e.g. weight or friction.\nGHC\nNSH\nHamburg\nWean\nFig. 5: Field Test on CMU Campus: The system was\nevaluated on articulated objects from across four distinct\nbuildings on the Carnegie Mellon University campus.\neffectiveness, reactivity, ability to support a high-payload\narm, and a base with omnidirectional drive.\nB. Primitive Implementation\nIn this subsection, we describe the implementation details\nof our parameterized primitive action space.\n1) Grasping: Given the RGBD image of the scene ob-\ntained from the realsense camera, we use off-the-shelf visual\nmodels [48], [49] to obtain the mask of the door and handle\ngiven just text prompts. Furthermore, since the door is a\nflat plane, we can estimate the surface normals of the door\nusing the corresponding mask and the depth image. This\nis used to move the base close to the door and align it to\nbe perpendicular, and also to set the orientation angle for\ngrasping the handle. The center of the 2d mask of the handle\nis projected into 3d coordinates using camera calibration,\nand this is the nominal grasp position. The low-level control\nparameters to the grasping primitive indicate an offset for this\nposition at which to grasp. This is beneficial since depending\non the type of handle the robot might need to reach a slightly\ndifferent position which can be learned via the low-level\ncontinuous valued parameters.\n2) Constrained Mobile-Manipulation: We use velocity\ncontrol for the robot arm end-effector and the robot base.\nWith a 6dof arm and 3dof motion for the base (in the SE2\nplane), we have a 9-dimensional vector -\nControl :\n(vx,vy,vz,vyaw,vpitch,vroll,Vx,Vy,V\u03c9)\nWhere the first 6 dimensions correspond to control for the\narm, and the last three are for the base. The primitives we\nuse impose contraints on this space as follows -\nUnlock :\n(0,0,vz,vyaw,0,0,0,0,0)\nRotate :\n(0,0,0,vyaw,0,0,0,0,0)\nOpen :\n(0,0,0,0,0,0,Vx,0,0)\nFor control, the policy outputs an index corresponding to\nwhich primitive is to executed, as well as the corresponding\nlow-level parameters for the motion. The low-level control\ncommand is continuous valued from -1 to 1 and executed\nfor a fixed duration of time. The sign of the parameters\ndictates the direction of the velocity control, either clockwise\nor counter-clockwise for unlock and rotate, and forward or\nbackward for open.\nC. Pretraining Dataset\nThe articulated objects we consider in this project consist\nof three rigid parts: a base part, a frame part, and a handle\nHardware features comparison\nArm payload\nDoF arm\nomni-base\nfootprint\nbase max speed\nprice\nStretch RE1 [8]\n1.5kg\n2\n\u2715\n34 cm, 33 cm\n0.6 m/s\n20k USD\nGo1-air + WidowX 250s [38]\n0.25kg\n6\n\u2713\n59 cm, 22 cm\n2.5 m/s\n10k USD\nFranka + Clearpath Ridgeback [50]\n3kg\n7\n\u2713\n96 cm, 80 cm\n1.1 m/s\n75k USD\nFranka + Omron LD-60 [51]\n3kg\n7\n\u2715\n70 cm, 50 cm\n1.8 m/s\n50k USD\nXarm-6 + Agilex Ranger mini 2 (ours)\n5kg\n6\n\u2713\n74 cm, 50 cm\n2.6 m/s\n25k USD\nTABLE I: Comparison of different aspects of popular hardware systems for mobile manipulation\npart. This covers objects such as doors, cabinets, drawers\nand fridges. The base and frame are connected by either a\nrevolute joint (as in a cabinet) or a prismatic joint (as in\na drawer). The frame is connected to the handle by either\na revolute joint or a fixed joint. We identify four major\ntypes of the articulated objects, which relate to the type\nof handle, and the joint mechanisms. Handle articulations\ncommonly include levers (Type A) and knobs (Type B). For\ncases where handles are not articulated, the body-frame can\nrevolve about a hinge using a revolute joint (Type C), or\nslide back and forth along a prismatic joint, for example,\ndrawers (Type D). While not exhaustive, this categorization\ncovers a wide variety of everyday articulated objects a robot\nsystem might encounter. To provide generalization benefits\nin operating unseen novel articulated objects, we first collect\na offline demonstration dataset. We include 3 objects from\neach category in the BC training dataset, collecting 10\ndemonstrations for each object, producing a total of 120\ntrajectories.\nWe also have 2 held-out testing objects from each category\nfor generalization experiments. The training and testing\nobjects differ significantly in visual appearance (eg. texture,\ncolor), physical dynamics (eg. if spring-loaded), and actu-\nation (e.g. the handle joint might be clockwise or counter-\nclockwise). We include visualizations of all objects used in\ntrain and test sets in Fig. 4, along with which part of campus\nthey are from as visualized in Fig. 5.\nD. Autonomous and Safe Online Adaptation\nThe key challenge we face is operating with new objects\nthat fall outside the BC training domain. To address this, we\ndevelop a system capable of fully autonomous Reinforcement\nLearning (RL) online adaptation. In this subsection, we\ndemonstrate the details of the autonomy and safety of our\nsystem.\n1) Safety Aware Exploration: It is crucial to ensure that\nthe actions the robot takes for exploring are safe for its\nhardware, especially since it is interacting with objects\nunder articulation constraints. Ideally, this could be addressed\nfor dynamic tasks like door opening using force control.\nHowever, low-cost arms like the xarm-6 we use do not\nsupport precise force sensing. For deploying our system, we\nuse a safety mechanism based which reads the joint current\nduring online sampling. If the robot samples an action that\ncauses the joint current to meet its threshold, we terminate\nthe episode and reset the robot, to prevent the arm from\npotentially damaging itself, and also provide negative reward\nto disincentivize such actions.\n2) Reward Specification: In our main experiments, a\nhuman operator provides rewards- with +1 if the robot suc-\ncesfully opens the doors, 0 if it fails, and -1 if there is a safety\nviolation. This is feasible since the system requires very few\nsamples for learning. For autonomous learning however, we\nwould like to remove the bottleneck of relying on humans\nto be present in the loop. We investigate using large vision\nlanguage models as a source of reward. Specifically, we use\nCLIP [52] to compute the similarity score between two text\nprompts and the image observed after robot execution. The\ntwo prompts we use are - \u201ddoor that is closed\u201d and \u201ddoor\nthat is open\u201d. We compute the similarity score of the final\nobserved image and each of these prompts and assign a\nreward of +1 if the image is closer to the prompt indicating\nthe door is open, and 0 in the other case. If a safety protection\nis triggered the reward is -1.\n3) Reset Mechanism: The robot employs visual odometry,\nutilizing the T265 tracking camera mounted on its base,\nenabling it to navigate back to its initial position. At the\nend of every episode, the robot releases its gripper, and\nmoves back to the original SE2 base position, and takes an\nimage of If for computing reward. We then apply a random\nperturbation to the SE2 position of the base so that the policy\nlearns to be more robust. Furthermore, if the reward is 1,\nwhere the door is opened, the robot has a scripted routine to\nclose the door.\nV. RESULTS\nWe conduct an extensive field study involving 12 training\nobjects and 8 testing objects across four distinct buildings on\nthe Carnegie Mellon University campus to test the efficacy\nof our system. In our experiments, we seek to answer the\nfollowing questions:\n1) Can the system improve performance on unseen objects\nvia online adaptation across diverse object categories?\n2) How does this compare to simply using imitation learn-\ning on provided demonstrations?\n3) Can we automate providing rewards using off-the-shelf\nvision-language models?\n4) How does the hardware design compare with other\nplatforms?\nA. Online Improvement\n1) Diverse Object Category Evaluation: : We evaluate our\napproach on 4 categories of held-out articulated objects. As\nsuccess rate\niteration\niteration\niteration\niteration\nFig. 6: Online Improvement: Comparison of our approach to the imitation policy on 4 different categories of articulated\nobjects, each consisting of two different objects. Our adaptive approach is able to improve in performance, while the imitation\npolicy has limited generalization.\nCLIP-reward comparison\nBC-0\nAdapt-GT\nAdapt-CLIP\nSuccess Rate A1 (lever)\n20%\n100%\n80%\nSuccess Rate B1 (knob)\n30%\n80%\n80%\nTABLE II: In this table, we present improvements in online\nadaptation with CLIP reward.\ndescribed in section IV-C, these are determined by handle\narticulation and joint mechanisms. This categorization is\nbased on types of handles, including levers (type A) and\nknobs (type B), as well as joint mechanisms including\nrevolute (type C) and prismatic (type D) joints. We have\ntwo test objects from each category. We report continual\nadaptation performance in Fig. 6 over 5 iterations of fine-\ntuning using online interactions, starting from the behavior\ncloned initial policy. Each iteration of improvement consists\nof 5 policy rollouts, after which the model is updated using\nthe loss in Equation 5.\nFrom Fig. 6, we see that our approach improves the\naverage success rate across all objects from 50 to 95 percent.\nHence, continually learning via online interaction samples\nis able to overcome the limited generalization ability of\nthe initial behavior cloned policy. The adaptive learning\nprocedure is able to learn from trajectories that get high\nreward, and then change its behavior to get higher reward\nmore often. In cases where the BC policy is reasonably\nperformant, such as Type C and D objects with an average\nsuccess rate of around 70 percent, RL is able to perfect the\npolicy to 100 percent performance. Furthermore, RL is also\nable to learn how to operate objects even when the initial\npolicy is mostly unable to perform the task. This can be seen\nfrom the Type A experiments, where the imitation learning\npolicy has a very low success rate of only 10 percent, and\ncompletely fails to open one of the two doors. With continual\npractice, RL is able to achieve an average success of 90\npercent. This shows that RL can explore to take actions that\nare potentially out of distribution from the imitation dataset,\nAction-Replay Comparison\nKNN-open\nKNN-close\nBC-0\nAdapt-GT\nSuccess Rate B1 (knob)\n10%\n0%\n30%\n80%\nSuccess Rate A2 (lever)\n0%\n0%\n0%\n80%\nTABLE III: We compare the performance of our adaptation\npolicies and initialized BC policies with KNN baselines.\nand learn from them, allowing the robot to learn how to\noperate novel unseen articulated objects.\n2) Action-replay baseline: : There is also another very\nsimple approach for utilizing a dataset of demonstrations for\nperforming a task on a new object. This involves replaying\ntrajectories from the closest object in the training set. This\nclosest object can be found using k-nearest neighbors with\nsome distance metric. This approach is likely to perform well\nespecially if the distribution gap between training and test\nobjects is small, allowing the same actions to be effective.\nWe run this baseline for two objects that are particularly\nhard for behavior cloning, one each from Type A and\nB categories (lever and knob handles respectively). The\ndistance metric we use to find the nearest neighbor in the\ntraining set is euclidean distance of the the CLIP encoding of\nobserved images. We evaluate this baseline both in an open-\nloop and closed-loop manner. In the former case, only the\nfirst observed image is used for comparison and the entire\nretrieved action sequence is executed, and in the latter we\nsearch for the closest neighbor after every step of execution\nand perform the corresponding action. From Table III we see\nthat this approach is quite ineffective, further underscoring\nthe distribution gap between the training and test objects in\nour experiments.\n3) Autonomous reward via VLMs: We investigate whether\nwe can replace the human operator with an automated proce-\ndure to provide rewards. The reward is given by computing\nthe similarity score between the observed image at the end\nof execution, and two text prompts, one of which indicate\nthat the door is open, and the other that says the doors is\nclosed, as described in section IV-D.\nsuccess rate\niteration\nFig. 7: Online Adaptation with\nCLIP reward. Adaptive learning\nusing rewards from CLIP, instead\nof a human operator, showing our\nsystem can operate autonomously.\nAs\nwith\nthe\naction-\nreplay baseline, we eva-\nlute\nthis\non\ntwo\ntest\ndoors, on each from the\nhandle\nand\nknob\ncate-\ngories. From Table II, we\nsee that online adapta-\ntion with VLM reward\nachieves a similar perfor-\nmance as using ground-\ntruth human-labeled re-\nward, with an average of\n80 percent compared to\n90 percent. We also report\nthe performance after ev-\nery iteration of training in\nFig. 7. Removing the need for a human operator to be present\nin the learning loop opens up the possiblity for autonomous\ntraining and improvement.\nB. Hardware Teleop Strength\nExpert teleoperation success rate\nlever B\nknob A\nStretch RE1\n0/5\n0/5\nOurs\n5/5\n5/5\nTABLE IV: Human expert teleoperation success rate using stretch\nand our system for opening doors\nIn order to successfully operate various doors the robot\nneeds to be strong enough to open and move through them.\nWe empirically compare against a different popular mobile\nmanipulation system, namely the Stretch RE1 (Hello Robot).\nWe test the ability of the robots to be teleoperated by a\nhuman expert to open two doors from different categories,\nspecifically lever and knob doors. Each object was subjected\nto five trials. As shown is Table IV, the outcomes of these\ntrials revealed a significant limitation of the Stretch RE1: its\npayload capacity is inadequate for opening a real door, even\nwhen operated by an expert, while our system succeeds in\nall trials.\nVI. CONCLUSION\nWe present a full-stack system for adaptive learning in\nopen world environments to operate various articulated ob-\njects, such as doors, fridges, cabinets and drawers. The\nsystem is able to learn from very few online samples since\nit uses a highly structured action space, which consists of\na parametric grasp primitive, followed by a sequence of\nparametric constrained mobile manipulation primitives. The\nexploration space is further structured via a demonstration\ndataset on some training objects. Our approach is able to\nimprove performance from about 50 to 95 percent across\n8 unseen objects from 4 different object categories, selected\nfrom buildings across the CMU campus. The system can also\nlearn using rewards from VLMs without human intervention,\nallowing for autonomous learning. We hope to deploy such\nmobile manipulators to continuously learn a broader variety\nof tasks via repeated practice.\nACKNOWLEDGMENT\nWe thank Shikhar Bahl, Tianyi Zhang, Xuxin Cheng, Sh-\nagun Uppal, and Shivam Duggal for the helpful discussions.\nThis work was supported in part by CMU-AIST Bridge\nproject, AFOSR research grant FA9550-23-1-0747 and Sony\nfaculty award.\nREFERENCES\n[1] S. Bahl, A. Gupta, and D. Pathak, \u201cHuman-to-robot imitation in the\nwild,\u201d RSS, 2022. 1\n[2] A. Kumar, Z. Fu, D. Pathak, and J. Malik, \u201cRma: Rapid motor\nadaptation for legged robots,\u201d arXiv preprint arXiv:2107.04034, 2021.\n1\n[3] M. Chang, T. Gervet, M. Khanna, S. Yenamandra, D. Shah, S. Y. Min,\nK. Shah, C. Paxton, S. Gupta, D. Batra et al., \u201cGoat: Go to any thing,\u201d\narXiv preprint arXiv:2311.06430, 2023. 1\n[4] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose,\nand S. Levine, \u201cVint: A foundation model for visual navigation,\u201d 2023.\n1\n[5] Z. Fu, T. Z. Zhao, and C. Finn, \u201cMobile aloha: Learning bimanual\nmobile manipulation with low-cost whole-body teleoperation,\u201d in\narXiv, 2024. 1, 2\n[6] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y. Liu, I. Misra, S. Chintala,\nand L. Pinto, \u201cOn bringing robots home,\u201d 2023. 1\n[7] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter,\nA. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalash-\nnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla,\nD. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez,\nK. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi,\nK. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Van-\nhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu,\nand B. Zitkovich, \u201cRt-1: Robotics transformer for real-world control\nat scale,\u201d 2023. 1\n[8] R. Yang, Y. Kim, A. Kembhavi, X. Wang, and K. Ehsani, \u201cHarmonic\nmobile manipulation,\u201d 2023. 1, 6\n[9] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna,\nT. Gervet, T.-Y. Yang, V. Jain, A. W. Clegg, J. Turner et al.,\n\u201cHomerobot: Open-vocabulary mobile manipulation,\u201d arXiv preprint\narXiv:2306.11565, 2023. 1, 2\n[10] A. Herzog, K. Rao, K. Hausman, Y. Lu, P. Wohlhart, M. Yan, J. Lin,\nM. G. Arenas, T. Xiao, D. Kappler et al., \u201cDeep rl at scale: Sorting\nwaste in office buildings with a fleet of mobile manipulators,\u201d arXiv\npreprint arXiv:2305.03270, 2023. 1, 2\n[11] C. Sun, J. Orbik, C. Devin, B. Yang, A. Gupta, G. Berseth, and\nS. Levine, \u201cFully autonomous real-world reinforcement learning with\napplications to mobile manipulation,\u201d 2021. 1\n[12] C. G. Atkeson, P. B. Benzun, N. Banerjee, D. Berenson, C. P. Bove,\nX. Cui, M. DeDonato, R. Du, S. Feng, P. Franklin et al., \u201cWhat\nhappened at the darpa robotics challenge finals,\u201d The DARPA robotics\nchallenge finals: Humanoid robots to the rescue, pp. 667\u2013684, 2018.\n1, 3\n[13] M. DeDonato, F. Polido, K. Knoedler, B. P. Babu, N. Banerjee, C. P.\nBove, X. Cui, R. Du, P. Franklin, J. P. Graff et al., \u201cTeam wpi-cmu:\nachieving reliable humanoid behavior in the darpa robotics challenge,\u201d\nJournal of Field Robotics, vol. 34, no. 2, pp. 381\u2013399, 2017. 1, 3\n[14] N. Banerjee, X. Long, R. Du, F. Polido, S. Feng, C. G. Atkeson,\nM. Gennert, and T. Padir, \u201cHuman-supervised control of the atlas\nhumanoid robot for traversing doors,\u201d in 2015 IEEE-RAS 15th In-\nternational Conference on Humanoid Robots (Humanoids).\nIEEE,\n2015, pp. 722\u2013729. 1, 3\n[15] S. Chitta, B. Cohen, and M. Likhachev, \u201cPlanning for autonomous\ndoor opening with a mobile manipulator,\u201d in 2010 IEEE International\nConference on Robotics and Automation. IEEE, 2010, pp. 1799\u20131806.\n1, 2\n[16] A. Jain and C. C. Kemp, \u201cBehaviors for robust door opening and\ndoorway traversal with a force-sensing mobile manipulator.\u201d Georgia\nInstitute of Technology, 2008. 1, 2\n[17] K. Nagatani and S. Yuta, \u201cAn experiment on opening-door-behavior\nby an autonomous mobile robot with a manipulator,\u201d in Proceedings\n1995 IEEE/RSJ International Conference on Intelligent Robots and\nSystems. Human Robot Interaction and Cooperative Robots, vol. 2,\n1995, pp. 45\u201350 vol.2. 1, 2\n[18] L. Peterson, D. Austin, and D. Kragic, \u201cHigh-level control of a\nmobile manipulator for door opening,\u201d in Proceedings. 2000 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS\n2000) (Cat. No.00CH37113), vol. 3, 2000, pp. 2333\u20132338 vol.3. 1, 2\n[19] S. Levine and V. Koltun, \u201cGuided policy search,\u201d in ICML, 2013. 2\n[20] S. Levine, C. Finn, T. Darrell, and P. Abbeel, \u201cEnd-to-end training of\ndeep visuomotor policies,\u201d JMLR, 2016. 2\n[21] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang,\nD. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., \u201cQt-\nopt: Scalable deep reinforcement learning for vision-based robotic\nmanipulation,\u201d arXiv preprint arXiv:1806.10293, 2018. 2\n[22] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jon-\nschkowski, C. Finn, S. Levine, and K. Hausman, \u201cMt-opt: Continuous\nmulti-task robotic reinforcement learning at scale,\u201d arXiv preprint\narXiv:2104.08212, 2021. 2\n[23] V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine, \u201cSkew-\nfit: State-covering self-supervised reinforcement learning,\u201d arXiv\npreprint arXiv:1903.03698, 2019. 2\n[24] R. Mendonca, S. Bahl, and D. Pathak, \u201cAlan: Autonomously exploring\nrobotic agents in the real world,\u201d in ICRA, 2023. 2\n[25] S. Haldar, J. Pari, A. Rai, and L. Pinto, \u201cTeach a robot to fish:\nVersatile imitation from one minute of demonstrations,\u201d arXiv preprint\narXiv:2303.01497, 2023. 2\n[26] S. Haldar, V. Mathur, D. Yarats, and L. Pinto, \u201cWatch and match:\nSupercharging imitation with regularized optimal transport,\u201d CoRL,\n2022. 2\n[27] A. Kumar, A. Singh, F. Ebert, M. Nakamoto, Y. Yang, C. Finn, and\nS. Levine, \u201cPre-training for robots: Offline rl enables learning new\ntasks from a handful of trials,\u201d arXiv preprint arXiv:2210.05178, 2022.\n2\n[28] L. Smith, J. C. Kew, X. B. Peng, S. Ha, J. Tan, and S. Levine, \u201cLegged\nrobots that keep on learning: Fine-tuning locomotion policies in the\nreal world,\u201d 2021. 2\n[29] R. Mendonca, S. Bahl, and D. Pathak, \u201cStructured world models from\nhuman videos,\u201d 2023. 2\n[30] A. Kannan, K. Shaw, S. Bahl, P. Mannam, and D. Pathak, \u201cDeft:\nDexterous fine-tuning for real-world hand policies,\u201d CoRL, 2023. 2\n[31] B. Wu, R. Martin-Martin, and L. Fei-Fei, \u201cM-ember: Tackling long-\nhorizon mobile manipulation via factorized domain transfer,\u201d arXiv\npreprint arXiv:2305.13567, 2023. 2\n[32] N. Yokoyama, A. W. Clegg, E. Undersander, S. Ha, D. Batra, and\nA. Rai, \u201cAdaptive skill coordination for robotic mobile manipulation,\u201d\narXiv preprint arXiv:2304.00410, 2023. 2\n[33] S. Srivastava, C. Li, M. Lingelbach, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, F. Xia, K. E.\nVainio, Z. Lian, C. Gokmen, S. Buch, K. Liu et al., \u201cBehavior:\nBenchmark for everyday household activities in virtual, interactive,\nand ecological environments,\u201d in Conference on Robot Learning.\nPMLR, 2022, pp. 477\u2013490. 2\n[34] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain,\nJ. Straub, J. Liu, V. Koltun, J. Malik et al., \u201cHabitat: A platform for\nembodied ai research,\u201d in Proceedings of the IEEE/CVF international\nconference on computer vision, 2019, pp. 9339\u20139347. 2\n[35] J. Wong, A. Tung, A. Kurenkov, A. Mandlekar, L. Fei-Fei, S. Savarese,\nand R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, \u201cError-aware imitation learning from teleopera-\ntion data for mobile manipulation,\u201d in Conference on Robot Learning.\nPMLR, 2022, pp. 1367\u20131378. 2\n[36] M. Mittal, D. Hoeller, F. Farshidian, M. Hutter, and A. Garg,\n\u201cArticulated object interaction in unknown scenes with whole-body\nmobile manipulation,\u201d in 2022 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS).\nIEEE, 2022, pp. 1647\u20131654.\n2\n[37] Y. Zhao, Q. Gao, L. Qiu, G. Thattai, and G. S. Sukhatme, \u201cOpend:\nA benchmark for language-driven door and drawer opening,\u201d arXiv\npreprint arXiv:2212.05211, 2022. 2\n[38] Z. Fu, X. Cheng, and D. Pathak, \u201cDeep whole-body control: learning\na unified policy for manipulation and locomotion,\u201d in Conference on\nRobot Learning.\nPMLR, 2023, pp. 138\u2013149. 2, 6\n[39] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., \u201cRt-1:\nRobotics transformer for real-world control at scale,\u201d arXiv preprint\narXiv:2212.06817, 2022. 2\n[40] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song,\nJ. Bohg, S. Rusinkiewicz, and T. Funkhouser, \u201cTidybot: Personal-\nized robot assistance with large language models,\u201d arXiv preprint\narXiv:2305.05658, 2023. 2\n[41] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,\nC. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog et al., \u201cDo as i\ncan, not as i say: Grounding language in robotic affordances,\u201d arXiv\npreprint arXiv:2204.01691, 2022. 2\n[42] R. Rusu, W. Meeussen, S. Chitta, and M. Beetz, \u201cLaser-based percep-\ntion for door and handle identification,\u201d 07 2009, pp. 1 \u2013 8. 2\n[43] J. Wang, S. Lin, C. Hu, Y. Zhu, and L. Zhu, \u201cLearning semantic key-\npoint representations for door opening manipulation,\u201d IEEE Robotics\nand Automation Letters, vol. 5, no. 4, pp. 6980\u20136987, 2020. 3\n[44] Y. Qin, B. Huang, Z.-H. Yin, H. Su, and X. Wang, \u201cDexpoint: Gener-\nalizable point cloud reinforcement learning for sim-to-real dexterous\nmanipulation,\u201d in Conference on Robot Learning.\nPMLR, 2023, pp.\n594\u2013605. 3\n[45] Y. Urakami, A. Hodgkinson, C. Carlin, R. Leu, L. Rigazio, and\nP. Abbeel, \u201cDoorgym: A scalable door opening environment and\nbaseline agent,\u201d arXiv preprint arXiv:1908.01887, 2019. 3\n[46] A. Gupta, M. E. Shepherd, and S. Gupta, \u201cPredicting motion plans\nfor articulating everyday objects,\u201d arXiv preprint arXiv:2303.01484,\n2023. 3\n[47] R. J. Williams, \u201cSimple statistical gradient-following algorithms for\nconnectionist reinforcement learning,\u201d Machine learning, vol. 8, pp.\n229\u2013256, 1992. 4\n[48] X. Zhou, R. Girdhar, A. Joulin, P. Kr\u00a8ahenb\u00a8uhl, and I. Misra, \u201cDetecting\ntwenty-thousand classes using image-level supervision,\u201d in Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part IX.\nSpringer, 2022, pp.\n350\u2013368. 5\n[49] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\nT. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., \u201cSegment\nanything,\u201d arXiv preprint arXiv:2304.02643, 2023. 5\n[50] J. Kindle, F. Furrer, T. Novkovic, J. J. Chung, R. Siegwart, and\nJ. Nieto, \u201cWhole-body control of a mobile manipulator using end-\nto-end reinforcement learning,\u201d 2020. 6\n[51] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suen-\nderhauf, \u201cSayplan: Grounding large language models using 3d scene\ngraphs for scalable task planning,\u201d arXiv preprint arXiv:2307.06135,\n2023. 6\n[52] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable\nvisual models from natural language supervision,\u201d in International\nconference on machine learning.\nPMLR, 2021, pp. 8748\u20138763. 6\n"
  },
  {
    "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
    "link": "https://arxiv.org/pdf/2401.14398.pdf",
    "upvote": "8",
    "text": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes\nEge Ozguroglu1 Ruoshi Liu1 D\u00b4\u0131dac Sur\u00b4\u0131s1 Dian Chen2 Achal Dave2 Pavel Tokmakov2 Carl Vondrick1\n1Columbia University\n2Toyota Research Institute\ngestalt.cs.columbia.edu\nAbstract\nWe introduce pix2gestalt, a framework for zero-shot\namodal segmentation, which learns to estimate the shape\nand appearance of whole objects that are only partially\nvisible behind occlusions. By capitalizing on large-scale\ndiffusion models and transferring their representations to\nthis task, we learn a conditional diffusion model for recon-\nstructing whole objects in challenging zero-shot cases, in-\ncluding examples that break natural and physical priors,\nsuch as art. As training data, we use a synthetically curated\ndataset containing occluded objects paired with their whole\ncounterparts. Experiments show that our approach outper-\nforms supervised baselines on established benchmarks. Our\nmodel can furthermore be used to significantly improve the\nperformance of existing object recognition and 3D recon-\nstruction methods in the presence of occlusions.\n1. Introduction\nAlthough only parts of the objects in Figure 1 are visible,\nyou are able to visualize the whole object, recognize the\ncategory, and imagine its 3D geometry. Amodal completion\nis the task of predicting the whole shape and appearance of\nobjects that are not fully visible, and this ability is crucial\nfor many downstream applications in vision, graphics, and\nrobotics. Learned by children from an early age [30], the\nability can be partly explained by experience, but we seem\nto be able to generalize to challenging situations that break\nnatural priors and physical constraints with ease. In fact,\nwe can imagine the appearance of objects during occlusions\nthat cannot exist in the physical world, such as the horse in\nMagritte\u2019s The Blank Signature.\nWhat makes amodal completion challenging compared\nto other synthesis tasks is that it requires grouping for both\nthe visible and hidden parts of an object. To complete an ob-\nject, we must be able to first recognize the object from par-\ntial observations, then synthesize only the missing regions\nfor the object. Computer vision researchers and gestalt psy-\nchologists have extensively studied amodal completion in\nthe past [10, 17, 18, 21, 33, 35, 49, 53], creating mod-\nels that explicitly learn figure-ground separation. However,\nthe prior work has been limited to representing objects in\nclosed-world settings, restricted to only operating on the\ndatasets on which they trained.\nIn this paper, we propose an approach for zero-shot\namodal segmentation and reconstruction by learning to syn-\nthesize whole objects first.\nOur approach capitalizes on\ndenoising diffusion models [14], which are excellent rep-\nresentations of the natural image manifold and capture all\ndifferent types of whole objects and their occlusions. Due\nto their large-scale training data, we hypothesize such pre-\ntrained models have implicitly learned amodal representa-\ntions (Figure 2), which we can reconfigure to encode ob-\nject grouping and perform amodal completion. By learning\nfrom a synthetic dataset of occlusions and their whole coun-\nterparts, we create a conditional diffusion model that, given\nan RGB image and a point prompt, generates whole objects\nbehind occlusions and other obstructions.\nOur main result is showing that we are able to achieve\nstate-of-the-art amodal segmentation results in a zero-shot\nsetting, outperforming the methods that were specifically\nsupervised on those benchmarks. We furthermore show that\nour method can be used as a drop-in module to significantly\nimprove the performance of existing object recognition and\n3D reconstruction methods in the presence of occlusions.\nAn additional benefit of the diffusion framework is that it al-\nlows sampling several variations of the reconstruction, nat-\nurally handling the inherent ambiguity of the occlusions.\n2. Related Work\nWe briefly review related work in amodal completion, anal-\nysis by synthesis, and denoising diffusion models for vision.\n2.1. Amodal Completion and Segmentation\nIn this work, we define amodal completion as the task of\ngenerating the image of the whole object [10, 49], amodal\nsegmentation as generating the segmentation mask of the\nwhole object [18, 21, 33, 35, 53], and amodal detection as\npredicting the bounding box of the whole object [15, 17].\nMost prior work focuses on the latter two tasks, due to\nthe challenges in generating the (possibly ambiguous) pix-\nels behind an occlusion. In addition, to our knowledge, all\nprior work on these tasks is limited to a small closed-world\n1\narXiv:2401.14398v1  [cs.CV]  25 Jan 2024\n\u201cseagull\u201d\n\u201cchair\u201d\n\u201ccouch\u201d\n\u201chorse\u201d\n\u201cperson\u201d\n\u201ccow\u201d\nInput\nAmodal \nCompletion\nAmodal \nSegmentation\nAmodal Novel \nView Synthesis\nAmodal 3D\nReconstruction\nFigure 1. Amodal Segmentation and Reconstruction via Synthesis. We present pix2gestalt, a method to synthesize whole objects from\nonly partially visible ones, enabling amodal segmentation, recognition, novel-view synthesis, and 3D reconstruction of occluded objects.\n2\nFigure 2. Whole Objects. Pre-trained diffusion models are able to\ngenerate all kinds of whole objects. We show samples conditioned\non a category from Stable Diffusion. We leverage this synthesis\nability for zero-shot amodal reconstruction and segmentation.\nof objects [17, 18, 21, 33, 49] or to synthetic data [10]. For\nexample, PCNet [49], the previous state-of-the-art method\nfor amodal segmentation, operates only on a closed-world\nset of classes in Amodal COCO [53].\nOur approach, by contrast, provides rich image comple-\ntions with accurate masks, generalizing to diverse zero-shot\nsettings, while still outperforming state-of-the-art methods\nin a closed-world. To achieve this degree of generaliza-\ntion, we capitalize on large-scale diffusion models, which\nimplicitly learn internal representations of whole objects.\nWe propose to unlock this capability by fine-tuning a diffu-\nsion model on a synthetically generated, realistic dataset of\nvaried occlusions.\n2.2. Analysis by Synthesis\nOur approach is heavily inspired by analysis by synthe-\nsis [47] \u2013 a generative approach for visual reasoning. Im-\nage parsing [42] was a representative work that unifies seg-\nmentation, recognition, and detection by generation. Prior\nworks have applied the analysis by synthesis approaches on\nvarious problems including face recognition [5, 42], pose\nestimation [27, 51], 3D reconstruction [22, 23], semantic\nimage editing [1, 24, 52]. In this paper, we aim to harness\nthe power of generative models trained with internet-scale\ndata for the task of amodal completion, thereby aiding vari-\nous tasks such as recognition, segmentation, and 3D recon-\nstruction in the presence of occlusions.\n2.3. Diffusion Models\nRecently, Denoising Diffusion Probabilistic Model [14], or\nDDPM, has emerged as one of the most widely used gen-\nerative architectures in computer vision due to its ability\nto model multi-modal distributions, training stability, and\nscalability. [8] first showed that diffusion models outper-\nform GANs [12] in image synthesis. Stable Diffusion [36],\ntrained on LAION-5B [39], applied diffusion model in\nthe latent space of a variational autoencoder [19] to im-\nprove computational efficiency.\nLater, a series of major\nimprovements were made to improve diffusion model per-\nformance [13, 41]. With the release of Stable Diffusion as a\nstrong generative prior, many works have adapted it to solve\ntasks in different domain such as image editing [6, 11, 37],\n3D [7, 25, 45], and modal segmentation [2, 3, 46]. In this\nwork, we leverage the strong occlusion and complete ob-\nject priors provided by internet-pretrained diffusion model\nto solve the zero-shot amodal completion task.\n3. Amodal Completion via Generation\nGiven an RGB image x with an occluded object that is par-\ntially visible, our goal is to predict a new image with the\nshape and appearance of the whole object, and only the\nwhole object. Our approach will accept any point or mask\nas a prompt p indicating the modal object:\n\u02c6xp = f\u03b8(x, p)\nwhere \u02c6xp is our estimate of the whole object indicated by p.\nMapping from x to this unified whole form, i.e. gestalt of\nthe occluded object, we name our method pix2gestalt. We\nwant \u02c6x to be perceptually similar to the true but unobserved\nwhole of the object as if there was no occlusion. We will\nuse a conditional diffusion model (see Figure 3) for f\u03b8.\nAn advantage of this approach is that, once we estimate\nan image of the whole object \u02c6x, we are able to perform any\nother computer vision task on it, providing a unified method\nto handle occlusions across different tasks. Since we will di-\nrectly synthesize the pixels of the whole object, we can aid\noff-the-shelf approaches to perform segmentation, recogni-\ntion, and 3D reconstruction of occluded objects.\nTo perform amodal completion, f needs to learn a repre-\nsentation of whole objects in the visual world. Due to their\nscale of training data, we will capitalize on large pretrained\ndiffusion models, such as Stable Diffusion, which are ex-\ncellent representations of the natural image manifold and\nhave the support to generate unoccluded objects. However,\nalthough they generate high-quality images, their represen-\ntations do not explicitly encode the grouping of objects and\ntheir boundaries to the background.\n3.1. Whole-Part Pairs\nTo learn the conditional diffusion model f with the ability\nfor grouping, we build a large-scale paired dataset of oc-\ncluded objects and their whole counterparts. Unfortunately,\ncollecting a natural image dataset of these pairs is challeng-\ning at scale. Prior datasets provide amodal segmentation an-\nnotations [33, 53], but they do not reveal the pixels behind\nan occlusion. Other datasets have relied on graphical simu-\nlation [16], which lack the realistic complexity and scale of\neveryday object categories.\nWe build paired data by automatically overlaying objects\nover natural images. The original images provide ground-\ntruth for the content behind occlusions. However, we need\nto ensure that we only occlude whole objects in this con-\nstruction, as otherwise our model could learn to generate\n3\nInput Image + Prompt\nNovel View \nSynthesis\n3D \nReconstruction\nVAE conditioning:\nCLIP conditioning:\nconcat(      ,        ,        )\npix2gestalt\nGaussian\nNoise\nVisible (Modal) Mask\nLatent Diffusion\nSynthesized Whole\nAmodal \nSegmentation\n\u201cbench\u201d\nFigure 3. pix2gestalt is an amodal completion model using a latent diffusion architecture. Conditioned on an input occlusion image and a\nregion of interest, the whole (amodal) form is synthesized, thereby allowing other visual tasks to be performed on it too. For conditioning\ndetails, see section 3.2.\nincomplete objects. To this end, we use a heuristic that,\nif the object is closer to the camera than its neighboring\nobjects, then it is likely a whole object. We use Segment\nAnything [20] to automatically find object candidates in the\nSA-1B dataset, and use the off-the-shelf monocular depth\nestimator MiDaS [4] to select which objects are whole. For\neach image with at least one whole object, we sample an\noccluder and superimpose it, resulting in a paired dataset of\n837K images and their whole counterparts. Figure 4 illus-\ntrates this construction and shows examples of the heuristic.\n3.2. Conditional Diffusion\nGiven pairs of an image x and its whole counterpart \u02c6xp, we\nfine-tune a conditional diffusion model to perform amodal\ncompletion while maintaining the zero-shot capabilities of\nthe pre-trained model. We solve for the following latent\ndiffusion objective:\nmin\n\u03b8\nEz\u223cE(x),t,\u03f5\u223cN (0,1)\n\u0002\n||\u03f5 \u2212 \u03f5\u03b8(zt, E(x), t, E(p), C(x))||2\n2\n\u0003\nwhere 0 \u2264 t < 1000 is the diffusion time step, zt is the em-\nbedding of the noised amodal target image \u02c6xp. C(x) is the\nCLIP embedding of the input image, and E(\u00b7) is a VAE em-\nbedding. Following [6, 25], we apply classifier-free guid-\nance (CFG) [13] by setting the conditional information to\na null vector randomly.\nAmodal completion requires reasoning about the whole\nshape, its appearance, and contextual visual cues of the\nscene. We adapt the design in [6, 25] to condition the dif-\nfusion model \u03f5\u03b8 in two separate streams. C(x) conditions\nthe diffusion model \u03f5\u03b8 via cross-attention on the semantic\nfeatures of the partially visible object in x as specified by\np, providing high-level perception. On the VAE stream, we\nchannel concatenate E(x) and zt, providing low-level visual\ndetails (shade, color, texture), as well as E(p) to indicate the\nvisible region of the object.\nAfter \u03f5\u03b8 is trained, f can generate \u02c6xp by performing it-\nerative denoising [36]. The CFG can be scaled to control\nimpact of the conditioning on the completion.\n3.3. Amodal Base Representations\nSince we synthesize RGB images of the whole object, our\napproach makes it straightforward to equip various com-\nputer vision methods with the ability to handle occlusions.\nWe discuss a few common cases.\nImage Segmentation aims to find the spatial bound-\naries of an object given an image x and an initial prompt\np.\nWe can perform amodal segmentation by completing\nan occluded object with f, then thresholding the result to\nobtain an amodal segmentation map. Note that this prob-\nlem is under-constrained as there are multiple possible solu-\ntions. Given the uncertainty, we found that sampling multi-\nple completions and performing a majority vote on the seg-\nmentation masks works best in practice.\nObject Recognition is the task of classifying an object\nlocated in an bounding box or mask p. We can zero-shot\nrecognize significantly occluded objects by first completing\nthe whole object with f, then classifying the amodal com-\npletion with CLIP.\n3D Reconstruction estimates the appearance and geom-\netry of an object. We can zero-shot reconstruct objects with\npartial occlusions by first completing the whole object with\nf, then applying SyncDreamer and Score Distillation Sam-\npling [32] to estimate a textured mesh.\n4\nFigure 4. Constructing Training Data. To ensure we only occlude whole objects, we use a heuristic that objects closer to the camera than\nits neighbors are likely whole objects. The green outline around the object shows where the estimated depth is closer to the camera than\nthe background (the red shows when it is not).\n4. Experiments\nWe evaluate pix2gestalt\u2019s ability to perform zero-shot\namodal completion for three tasks: amodal segmentation,\noccluded object recognition, and amodal 3D reconstruction.\nWe show that our method provides amodal completions that\ndirectly lead to strong results in all tasks.\n4.1. Amodal Segmentation\nSetup. Amodal segmentation requires segmenting the full\nextent of a (possibly occluded) object. We evaluate this task\non the Amodal COCO (COCO-A) [53] and Amodal Berke-\nley Segmentation (BSDS-A) datasets [28]. For evaluation,\nCOCO-A provides 13,000 amodal annotations of objects in\n2,500 images, while BSDS-A provides 650 objects from\n200 images. For both datasets, we evaluate methods that\ntake as input an image and a (modal) mask of the visible\nextent of an object, and output an amodal mask of the full-\nextent of the object. Following [49], we evaluate segmen-\ntations using mean intersection-over-union (mIoU). We fol-\nlow the strategy in Section 3.3 to convert our amodal com-\npletions into segmentation masks.\nWe evaluate three baselines for amodal segmentation.\nThe first method is PCNet [49], which is trained for amodal\nsegmentation specifically for COCO-A. Next, we compare\nto two zero-shot methods, which do not train on COCO-A:\nSegment Anything (SAM) [20], a strong modal segmen-\ntation method, and Inpainting using Stable Diffusion-XL\n[31]. To evaluate inpainting methods, we provide as input\nan image with all but the visible object region erased, and\nconvert the completed image output by the method into an\namodal segmentation mask following the same strategy as\nfor our method.\nResults. Table 1 compares pix2gestalt with prior work.\nDespite never training on the COCO-A dataset, our method\noutperforms all baselines, including PCNet, which uses\nCOCO-A images for training, and even PCNet-Sup, which\nis supervised using human-annotated amodal segmentations\nfrom COCO-A\u2019s training set. Compared to other zero-shot\nmethods, our improvements are dramatic, validating the\ngeneralization abilities of our method. Notably, we also out-\nperform the inpainting baseline which is based off a larger,\nmore recent variant of Stable Diffusion [31]. This demon-\nstrates that internet-scale training alone is not sufficient and\nour fine-tuning approach is key to reconfigure priors from\npre-training for amodal completion.\nWe further analyze amodal completions qualitatively in\nFigure 6. While SD-XL often hallucinates extraneous, un-\nrealistic details (e.g. person in front of the bus in the sec-\nond row), PCNet tends to fail to recover the full extent of\nobjects\u2014often only generating the visible region, as in the\nMario example in the third row. In contrast, pix2gestalt pro-\nvides accurate, complete reconstructions of occluded ob-\njects on both COCO-A (Figure 6) and BSDS-A (Figure 7).\nOur method generalizes well beyond the typical occlusion\nscenarios found in those benchmarks. Figure 5 shows sev-\neral examples of out-of-distribution images, including art\npieces, illusions, and images taken by ourselves that are suc-\ncessfully handled by our method. Note that no prior work\nhas shown open-world generalization (see 2.1).\nFigure 8 illustrates the ability of the approach to gen-\nerate diverse samples in shape and appearance when there\nis uncertainty in the final completion. For example, it is\n5\nFigure 5. In-the-wild Amodal Completion and Segmentation. We find that pix2gestalt is able to synthesize whole objects in novel\nsituations, including artistic pieces, images taken by an iPhone, and illusions.\nTable 1. Amodal Segmentation Results. We report mIoU (%)\n\u2191 on Amodal COCO [53] and on Amodal Berkeley Segmentation\nDataset [28, 53].\n\u2217PCNet-Sup trains using ground truth amodal\nmasks from COCO-Amodal. See Section 4.1 for analysis.\nZero-shot Method\nCOCO-A BSDS-A\n\u2717\nPCNet [49]\n81.35\n-\n\u2717\nPCNet-Sup\u2217 [49]\n82.53\u2217\n-\nD\nSAM [20]\n67.21\n65.25\nD\nSD-XL Inpainting [31]\n76.52\n74.19\nD\nOurs\n82.87\n80.76\nD\nOurs: Best of 3\n87.10\n85.68\nable to synthesize several plausible completions of the oc-\ncluded house in the painting. We quantitatively evaluate the\ndiversity of our samples in the last row of Table 1 by sam-\npling from our model three times and reporting the perfor-\nmance for the best sample (\u201cBest of 3\u201d). Finally, we found\nlimitations of our approach in situations that require com-\nmonsense or physical reasoning. We show two examples in\nFigure 9.\n4.2. Occluded Object Recognition\nNext, we evaluate the utility of our method for recognizing\noccluded objects.\nSetup.\nWe use the Occluded and Separated COCO\nbenchmarks [48] for evaluating classification accuracy un-\nder occlusions. The former consists of partially occluded\nobjects, whereas Separated COCO contains objects whose\nmodal region is separated into disjoint segments by the oc-\ncluder(s), resulting in a more challenging problem setting.\nWe evaluate on all 80 COCO semantic categories in the\ndatasets using Top 1 and Top 3 accuracy.\nWe use CLIP [34] as the base open-vocabulary classifier.\nAs baselines, we evaluate CLIP without any completion,\nreporting three variants: providing the entire image (CLIP),\nproviding the entire image with a visual prompt (a red cir-\ncle, as in Shtedritski et al. [40]) around the occluded object,\nor providing an image with all but the visible portion of the\noccluded object masked out. To evaluate our approach, we\nfirst utilize it to complete the occluded object, and then clas-\nsify the output image using CLIP.\nResults. Table 2 compares our method with the base-\nlines. Visual prompting with a red circle (RC) and masking\nall but the visible object (Vis. Obj.) provide improvements\nover directly passing the image to CLIP on the simpler Oc-\ncluded COCO benchmark, but fail to improve, and some\ntimes even decreases the performance of the baseline CLIP\non the more challenging Separated COCO variant.\nOur\nmethod (Ours + CLIP), however, strongly outperforms all\nbaselines for both the occluded and separated datasets, ver-\nifying the quality of our completions.\n4.3. Amodal 3D Reconstruction\nFinally, we evaluate our method for improving 3D recon-\nstruction of occluded objects.\nSetup. We focus on two tasks: Novel-view synthesis and\n6\nInput Occlusion\nInput Modal Mask\nSD-XL Inpainting\nPCNet\nOurs\nGT Amodal Mask\nFigure 6. Amodal Completion and Segmentation Qualitative Results on Amodal COCO. In blue circles, we highlight completion\nregions that, upon a closer look, have a distorted texture in the PCNet baseline, and a correct one in our results.\nsingle-view 3D reconstruction.\nTo demonstrate pix2gestalt\u2019s performance as a drop-in\nmodule to 3D foundation models [25, 26, 38], we replicate\nthe evaluation procedure of Zero-1-to-3 [25, 26] on Google\nScanned Objects (GSO) [9], a dataset of common house-\nhold objects 3D scanned for use in embodied, synthetic, and\n3D perception tasks. We use 30 randomly sampled objects\nfrom GSO ranging from daily objects to animals. For each\nobject, we render a 256x256 image with synthetic occlu-\nsions sampled from the full dataset of 1,030 objects in GSO.\n7\nFigure 7. Amodal Berkeley Segmentation Dataset Qualitative Results. Our method provides accurate, complete reconstructions of\noccluded objects.\nFigure 8. Diversity in Samples. Amodal completion has inherent\nuncertainties. By sampling from the diffusion process multiple\ntimes, the method synthesizes multiple plausible wholes that are\nconsistent with the input observations.\nTable 2. Occluded Object Recognition. We report zero-shot clas-\nsification accuracy on Occluded and Separated COCO [48]. While\nsimple baselines fail to improve CLIP performance in the more\nchallenging Separated COCO setting, our method consistently im-\nproves recognition accuracy by large margins. See Section 4.2 for\nanalysis.\nMethod\nTop 1 Acc. (%) \u2191 Top 3 Acc. (%) \u2191\nOccluded\nSep.\nOccluded\nSep.\nCLIP [34]\n23.33\n26.04\n43.84\n43.19\nCLIP + RC [40]\n23.46\n25.64\n43.86\n43.24\nVis. Obj. + CLIP\n34.00\n21.10\n49.26\n34.70\nOurs + CLIP\n43.39\n31.15\n58.97\n45.77\nWe render from a randomly sampled view to avoid canoni-\ncal poses, and generate two occluded images for each of the\n30 objects, resulting in 60 samples.\nFor amodal novel-view synthesis, we quantitatively eval-\nuate our method using 3 metrics: PSNR, SSIM [44], and\nLPIPS [50], measuring the image-similarity of the input and\nground truth views. For 3D reconstruction, we use the Volu-\nFigure 9. Common-sense and Physics Failures. Left: recon-\nstruction has the car going in the wrong direction. Right: recon-\nstruction contradicts physics, failing to capture that a hand must\nbe holding the donut box.\nmetric IoU and Chamfer Distance metrics. We compare our\napproach with SyncDreamer [26], a 3D generative model\nthat fine-tunes Zero123-XL [7, 25] for multi-view consis-\ntent novel view synthesis and consequent 3D reconstruction\nwith NeuS [43] and NeRF [29]. Our first baseline provides\nas input to SyncDreamer the segmentation mask of all fore-\nground objects, following the standard protocol. To avoid\nreconstructing occluded objects, we additionally evaluate\ntwo variants that use SAM [20] to estimate the mask of only\nthe object of interest, or the ground truth mask for the object\nof interest (GT Mask). Finally, to evaluate our method, we\nprovide as input the full object completed by our method,\nalong with the corresponding amodal mask. We evaluate\ntwo variants of our method: One where we provide a modal\nmask for the object of interested as estimated by SAM (Ours\n(SAM Mask)) and one where we use the ground truth modal\nmask (Ours (GT Mask)).\nResults. We compare our approach with the two base-\nlines in Table 4 for novel view synthesis and Table 3 for\n3D reconstruction. Quantitative results demonstrate that we\nstrongly outperform the baselines for both tasks. In novel-\n8\nInput View\n3D Geometry\nNovel View\nNovel View\n3D Geometry\nInput View\nFigure 10. Amodal 3D Reconstruction qualitative results. The object of interest is specified by a point prompt, shown in yellow.\nIncorporating pix2gestalt as a drop-in module to state-of-the-art 3D reconstruction models allows us to address challenging and diverse\nocclusion scenarios with ease.\nTable 3. Single-view 3D Reconstruction. We report Chamfer\nDistance and Volumetric IoU for Google Scanned Objects. See\nSection 4.3 for analysis.\nCD \u2193\nIoU \u2191\nSyncDreamer [26]\n0.0884\n0.2741\nSAM Mask + SyncDr.\n0.1182\n0.0926\nOurs (SAM Mask) + SyncDr.\n0.0784\n0.3312\nGT Mask + SyncDr.\n0.1084\n0.1027\nOurs (GT Mask) + SyncDr.\n0.0681\n0.3639\nview synthesis, we outperform SAM + SyncDreamer on\nthe image reconstruction metrics, LPIPS [50] and PSNR\n[44].\nCompared to SAM as a modal pre-processor, we\nobtain these improvements as a drop-in module to Sync-\nDreamer while still retaining equivalent image quality (Ta-\nble 4, SSIM [44]). With ground truth mask inputs, we ob-\ntain further image reconstruction gains.\nMoreover, even\nthough our approach utilizes an additional diffusion step\ncompared to SyncDreamer only, we demonstrate less image\nquality degradation.\nFor reconstruction of the 3D geometry, our fully auto-\nmatic method outperforms all of the baselines for both vol-\numetric IoU and Chamfer distance metrics, even the base-\nlines that use ground masks. Providing the ground truth to\nour approach further improves the results. Figure 10 shows\nTable 4. Novel-view synthesis from one image. We report re-\nsults on Google Scanned Objects [9]. Note SSIM measures image\nquality, not novel-view accuracy. See Section 4.3 for analysis.\nLPIPS \u2193 PSNR \u2191 SSIM \u2191\nSyncDreamer [26]\n0.3221\n11.914\n0.6808\nSAM + SyncDr.\n0.3060\n12.432\n0.7248\nOurs (SAM Mask) + SyncDr.\n0.2848\n13.868\n0.7211\nGT Mask + SyncDr.\n0.2905\n12.561\n0.7322\nOurs (GT Mask) + SyncDr.\n0.2631\n14.657\n0.7328\nqualitative evaluation for 3D reconstruction of occluded ob-\njects, ranging from an Escher lithograph to in-the-wild im-\nages.\n5. Conclusion\nIn this work, we proposed a novel approach for zero-shot\namodal segmentation via synthesis. Our model capitalizes\non whole object priors learned by internet-scale diffusion\nmodels and unlocks them via fine-tuning on a synthetically\ngenerated dataset of realistic occlusions. We then demon-\nstrated that synthesizing the whole object makes it straight-\nforward to equip various computer vision methods with the\nability to handle occlusions. In particular, we reported state-\nof-the art results on several benchmarks for amodal segmen-\n9\ntation, occluded object recognition and 3D reconstruction.\nAcknowledgements: This research is based on work\npartially supported by the Toyota Research Institute, the\nDARPA MCS program under Federal Agreement No.\nN660011924032, the NSF NRI Award #1925157, and the\nNSF AI Institute for Artificial and Natural Intelligence\nAward #2229929. DS is supported by the Microsoft PhD\nFellowship.\nReferences\n[1] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nIm-\nage2StyleGAN: How to embed images into the stylegan la-\ntent space? In ICCV, 2019. 3\n[2] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior\nWolf. Segdiff: Image segmentation with diffusion proba-\nbilistic models. arXiv preprint arXiv:2112.00390, 2021. 3\n[3] Dmitry\nBaranchuk,\nIvan\nRubachev,\nAndrey\nVoynov,\nValentin Khrulkov, and Artem Babenko. Label-efficient se-\nmantic segmentation with diffusion models. arXiv preprint\narXiv:2112.03126, 2021. 3\n[4] Reiner Birkl, Diana Wofk, and Matthias M\u00a8uller. Midas v3.1\n\u2013 a model zoo for robust monocular relative depth estimation.\narXiv preprint arXiv:2307.14460, 2023. 4\n[5] Volker Blanz and Thomas Vetter. A morphable model for the\nsynthesis of 3d faces. In Seminal Graphics Papers: Pushing\nthe Boundaries, Volume 2, pages 157\u2013164. 2023. 3\n[6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023. 3, 4\n[7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 3, 8\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. NeurIPS, 2021. 3\n[9] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B. McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3D scanned household items. In ICRA,\n2022. 7, 9\n[10] Kiana Ehsani, Roozbeh Mottaghi, and Ali Farhadi. Segan:\nSegmenting and generating the invisible. In CVPR, 2018. 1,\n3\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 3\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\n3\n[13] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3, 4\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33, 2020. 1, 3\n[15] Cheng-Yen Hsieh, Tarasha Khurana, Achal Dave, and Deva\nRamanan. Tracking any object amodally, 2023. 1\n[16] Y.-T. Hu, H.-S. Chen, K. Hui, J.-B. Huang, and A. G.\nSchwing.\nSAIL-VOS: Semantic Amodal Instance Level\nVideo Object Segmentation \u2013 A Synthetic Dataset and Base-\nlines. In Proc. CVPR, 2019. 3\n[17] Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jiten-\ndra Malik. Amodal completion and size constancy in natural\nscenes. In ICCV, 2015. 1, 3\n[18] Lei Ke, Yu-Wing Tai, and Chi-Keung Tang. Deep occlusion-\naware instance segmentation with overlapping bilayers. In\nCVPR, 2021. 1, 3\n[19] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything. In ICCV, 2023. 4, 5,\n6, 8\n[21] Huan Ling, David Acuna, Karsten Kreis, Seung Wook Kim,\nand Sanja Fidler.\nVariational amodal object completion.\nNeurIPS, 2020. 1, 3\n[22] Ruoshi Liu and Carl Vondrick. Humans as light bulbs: 3d hu-\nman reconstruction from thermal reflection. In CVPR, 2023.\n3\n[23] Ruoshi Liu, Sachit Menon, Chengzhi Mao, Dennis Park, Si-\nmon Stent, and Carl Vondrick. Shadows shed light on 3d\nobjects. arXiv preprint arXiv:2206.08990, 2022. 3\n[24] Ruoshi Liu, Chengzhi Mao, Purva Tendulkar, Hao Wang,\nand Carl Vondrick. Landscape learning for neural network\ninversion. In ICCV, 2023. 3\n[25] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In ICCV, 2023. 3, 4, 7, 8\n[26] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n7, 8, 9\n[27] Wufei Ma, Angtian Wang, Alan Yuille, and Adam Ko-\nrtylewski. Robust category-level 6D pose estimation with\ncoarse-to-fine rendering of neural features. In ECCV, 2022.\n3\n[28] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database\nof human segmented natural images and its application to\nevaluating segmentation algorithms and measuring ecologi-\ncal statistics. In ICCV, 2001. 5, 6\n[29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 8\n[30] Jean Piaget. The construction of reality in the child. Rout-\nledge, 2013. 1\n[31] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\n10\nRobin Rombach. Sdxl: Improving latent diffusion models\nfor high-resolution image synthesis, 2023. 5, 6\n[32] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 4\n[33] Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia.\nAmodal instance segmentation with KINS dataset. In CVPR,\n2019. 1, 3\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 6, 8\n[35] N Dinesh Reddy,\nRobert Tamburo,\nand Srinivasa G\nNarasimhan. Walt: Watch and learn 2d amodal represen-\ntation from time-lapse imagery. In CVPR, 2022. 1\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 3, 4\n[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 3\n[38] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann,\nHong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry La-\ngun, Li Fei-Fei, Deqing Sun, and Jiajun Wu.\nZeroNVS:\nZero-shot 360-degree view synthesis from a single real im-\nage. arXiv preprint arXiv:2310.17994, 2023. 7\n[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5B: An open large-scale dataset for training\nnext generation image-text models. NeurIPS, 2022. 3\n[40] Aleksandar Shtedritski, Christian Rupprecht, and Andrea\nVedaldi. What does clip know about a red circle? visual\nprompt engineering for vlms. In ICCV, 2023. 6, 8\n[41] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3\n[42] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-\nChun Zhu. Image parsing: Unifying segmentation, detection,\nand recognition. International Journal of computer vision,\n63:113\u2013140, 2005. 3\n[43] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 8\n[44] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE Transactions on Image Process-\ning, 13(4):600\u2013612, 2004. 8, 9\n[45] Rundi Wu, Ruoshi Liu, Carl Vondrick, and Changxi Zheng.\nSin3dm: Learning a diffusion model from a single 3d tex-\ntured shape. arXiv preprint arXiv:2305.15399, 2023. 3\n[46] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xi-\naolong Wang, and Shalini De Mello.\nOpen-Vocabulary\nPanoptic Segmentation with Text-to-Image Diffusion Mod-\nels. arXiv preprint arXiv:2303.04803, 2023. 3\n[47] Alan Yuille and Daniel Kersten. Vision as bayesian infer-\nence: analysis by synthesis?\nTrends in cognitive sciences,\n10(7):301\u2013308, 2006. 3\n[48] Guanqi Zhan, Weidi Xie, and Andrew Zisserman. A tri-layer\nplugin to improve occluded detection. BMVC, 2022. 6, 8\n[49] Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua\nLin, and Chen Change Loy.\nSelf-supervised scene de-\nocclusion. In CVPR, 2020. 1, 3, 5, 6\n[50] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 8, 9\n[51] Yi Zhang, Pengliang Ji, Angtian Wang, Jieru Mei, Adam Ko-\nrtylewski, and Alan Yuille. 3D-Aware neural body fitting for\nocclusion robust 3d human pose estimation. In ICCV, 2023.\n3\n[52] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-\ndomain GAN inversion for real image editing.\nIn ECCV,\n2020. 3\n[53] Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr\nDoll\u00b4ar. Semantic amodal segmentation. In CVPR, 2017. 1,\n3, 5, 6\n11\n"
  },
  {
    "title": "CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion",
    "link": "https://arxiv.org/pdf/2401.14066.pdf",
    "upvote": "7",
    "text": "CreativeSynth: Creative Blending and Synthesis of Visual Arts based on\nMultimodal Diffusion\nNisha Huang1,2\nWeiming Dong1,2\nYuxin Zhang1,2\nFan Tang3\nRonghui Li4\nChongyang Ma5\nXiu Li6\nChangsheng Xu1,2\n1School of Artificial Intelligence, UCAS\n2MAIS, Institute of Automation, CAS\n3 Institute of Computing Technology, CAS\n4Tsinghua University\n5ByteDance Inc.\n6Shenzhen International Graduate School, Tsinghua University\nhttps://anonymous.4open.science/r/CreativeSynth-52A8\nFigure 1. Our CreativeSynth unified framework is capable of generating personalized digital art when supplied with an art image, drawing\non prompts from either unimodal or multimodal prompts. This methodology not only yields artwork with high-fidelity realism but also\neffectively upholds the foundational concepts, composition, stylistic elements, and visual symbolism intrinsic to genuine artworks. Cre-\nativeSynth supports a wide array of intriguing applications, including (a) image variation, (b) image editing, (c) style transfer, (d) image\nfusion, and (e) multimodal blending.\nAbstract\nLarge-scale text-to-image generative models have made\nimpressive strides, showcasing their ability to synthesize a\nvast array of high-quality images. However, adapting these\nmodels for artistic image editing presents two significant\nchallenges. Firstly, users struggle to craft textual prompts\nthat meticulously detail visual elements of the input image.\nSecondly, prevalent models, when effecting modifications in\nspecific zones, frequently disrupt the overall artistic style,\ncomplicating the attainment of cohesive and aesthetically\nunified artworks. To surmount these obstacles, we build the\ninnovative unified framework CreativeSynth, which is based\non a diffusion model with the ability to coordinate multi-\nmodal inputs and multitask in the field of artistic image\ngeneration. By integrating multimodal features with cus-\ntomized attention mechanisms, CreativeSynth facilitates the\nimportation of real-world semantic content into the domain\nof art through inversion and real-time style transfer. This\nallows for the precise manipulation of image style and con-\narXiv:2401.14066v2  [cs.CV]  30 Jan 2024\ntent while maintaining the integrity of the original model\nparameters. Rigorous qualitative and quantitative evalu-\nations underscore that CreativeSynth excels in enhancing\nartistic images\u2019 fidelity and preserves their innate aesthetic\nessence. By bridging the gap between generative models\nand artistic finesse, CreativeSynth becomes a custom digi-\ntal palette.\n1. Introduction\nIf a picture is worth a thousand words, then two pic-\ntures can weave a narrative beyond measure. The bound-\naries of digital art continue to be pushed as artificial intel-\nligence technologies [1\u20134] flourish in the field of art cre-\nation. These innovations, rooted in the synergy of natural\nlanguage processing and generative paradigms, are redefin-\ning our approach to conceiving and creating digital master-\npieces that resonate with the essence of human creativity\nand intent.\nDiffusion models [5\u20137] have set the benchmark in terms\nof generation quality, skillfully transforming noise into rec-\nognizable high-resolution visual content. However, these\napproaches often encounter limitations in the granularity of\ncontrol over subtle visual aspects and deep semantic ele-\nments, attributed to the disproportionate influence textual\nprompts exercise upon the resultant imagery.\nTo break\nthrough this limitation and improve the ability to accu-\nrately edit artistic images, we introduce a more effective im-\nage prompt adapter [8] for enhancing the versatility of the\nmodel and the realism of the visual input, seamlessly trans-\nforming the image into a piece of vivid expressive space.\nThis allows textual prompts to become flexible mediators of\nartistic transformation, responding with agility to changes\ninduced by textual editing (see Fig. 1).\nPrecise control and editing of the content within any\nsingle image remain a significant challenge [1\u20134].\nCur-\nrent methods typically fail to capture subtle details [9\u201311]\nor require tedious manual adjustments for each input im-\nage [12,13]. Furthermore, users often find it challenging to\naccurately describe the visual elements of a specific input\nimage through text. Additionally, models tend to disrupt\nthe overall artistic style when modifying certain specific ar-\neas, which complicates the maintenance of style unity and\naesthetic integrity of the artwork. To address those prob-\nlems, in this paper, we propose CreativeSynth, a revolu-\ntionary unified framework that integrates multimodal inputs\nwith digital artworks. CreativeSynth not only focuses on\ngenerating images with realism but also retains fundamen-\ntal artistic elements such as conceptual integrity, stylistic\nfidelity, and visual symbolism.\nCreativeSynth introduces an innovative concept of cus-\ntomized art, transcending the limitations of traditional style\ntransfer and text-guided image generation techniques, as\nFigure 2. Comparative analysis of conceptual distinctions among\n(a) classical style transfer [14], (b) text-to-image synthesis [15],\nand (c) our CreativeSynth framework.\nshown in Fig. 2. The two core mechanisms we propose,\naesthetic maintenance and semantic fusion, enable Cre-\nativeSynth to perform a series of art image editing and gen-\neration tasks while preserving the individual visual char-\nacteristics of the artwork.\nDuring the image generation\nprocess, the aesthetic maintenance mechanism ensures the\nartist\u2019s original intent and the artwork\u2019s original style are\nnot lost, as well as the overall aesthetic harmony when new\nvisual elements are added.\nThe semantic fusion mecha-\nnism emphasizes the synergy between multimodal inputs\nand visual art creation, enhancing the work\u2019s capacity for\npersonalized customization and reflecting the user\u2019s inten-\ntions and narrative. Specific applications of CreativeSynth\ninclude image variation, image editing, style transfer, image\nfusion, and multimodal blending, as demonstrated in Fig. 1.\nTo summarize, our contributions are:\n\u2022 We introduce CreativeSynth, an innovative multimodal,\nmultitasking unified art framework that enables the ability\nto edit arbitrary art images on a single platform.\n\u2022 We employ advanced mechanisms for aesthetic mainte-\nnance, semantic fusion, and inversion coding. These tech-\nniques enable the maintenance of the intrinsic expression\nof the art image while integrating multimodal semantic\ninformation. The coherence of the artwork is greatly im-\nproved from both macro and micro aspects, ultimately re-\nalizing truly personalized creation.\n\u2022 The\nexperimental\nresults\nprove\nthat\nCreativeSynth\ndemonstrates superior performance compared to other\nstate-of-the-art methods in the current field of art image\nblending and synthesis.\n2. Related Work\nImage style transfer.\nStyle transfer has been extensively\nstudied as a common mechanism for generating artistic im-\nages guided by examples. Traditional style transfer research\nhas evolved from patch matching methods [16] to deep con-\nvolutional neural networks learning approaches [17, 18].\nAdaIN [14] employs conditional instance normalization for\nstyle transfer by aligning content and style feature statis-\ntics. ArtFlow [19] technique uses reversible neural flows\nto prevent content leakage.\nCAST, developed by [20],\nenhances arbitrary style transfer via contrastive learning,\nwhile the visual transformer-based StyTr2 addresses image\ndependencies to maintain content integrity [21].\nRecent\ndiffusion-based text-driven methods including InST [12],\nStyleDrop [22], and DiffStyler [23] have broadened the do-\nmain with stylistically expressive and parameter-efficient\napproaches. While existing image style transfer methods\nprimarily focus on learning and transferring artistic ele-\nments into a given content image (see Fig. 2(a)), our ap-\nproach aims to create the appearance of specific content\nwithin a target painting.\nText-to-image generation.\nWith the ability of neural net-\nworks to understand intricate natural language and vi-\nsual representations, the field of image synthesis has\nmade significant progress from textual descriptions [24].\nTransformer-based architectures such as DALL-E [25] and\nits follow-up studies [6,7] incorporate powerful attentional\nmechanisms to efficiently transform textual prompts into\nhigh-fidelity images.\nSimilarly, VQ-VAE-2 [26] and its\nautoregressive model demonstrate the strong potential of\ncombining textual and visual patterns through discrete la-\ntent spaces. These methods have achieved remarkable re-\nsults, but they often do not allow for fine control of struc-\ntural details [10, 27]. Diffusion models similar to Stable\nDiffusion [5] also exemplify the ability to generate high-\nquality images based on descriptions.\nNonetheless, as\nshown in Fig. 2(b), these methods still face the challenge\nof generating images with styles that are inconsistent with\ntextual prompts. Our research closely follows the previ-\nous work [1, 4, 27, 28], focusing on converting multimodal\nprompts into realistic artistic images and achieving innova-\ntions in reconstructing and editing existing images.\nPersonalized image generation.\nIn order to incorporate\nspecific styles or personalities into image generation, per-\nsonalization, and style alignment has become an important\narea of research. For example, StyleGAN [29] has made im-\npressive progress in personalized face generation. Control-\nNet [30] leverages \u201czero-convolution\u201d fine-tuning on pre-\ntrained diffusion models to enable diverse, prompt-driven\nimage generation with spatial conditioning.\nIn terms of\nimage restoration with constraints, ProSpect [13] attempts\nto preserve the style features of the reference image while\nadapting its content to fit the new context.\nIn terms of\nachieving multi-image style consistency, Style Aligned [27]\nshows how multiple images can be stylistically consistent\nthrough a shared attention layer. Textual Inversion [4] intro-\nduces a method for embedding new \u201dwords\u201d into the model\nspace using as few as 3-5 images, allowing for nuanced lin-\nguistic guidance customization and demonstrating superior\nconcept depiction capabilities across a range of tasks. [31]\nenables intuitive text-guided image edits by inverting im-\nages into the domain of a pre-trained model using meaning-\nful text prompts. As demonstrated in Fig. 2(c), our work ex-\ntends the above idea by enhancing the interaction between\ntextual and artistic visual features made achievable through\nmultimodal fusion.\n3. Method\n3.1. Overview\nCreativeSynth incorporates information from text and\nimage modalities to sample artwork based on guiding con-\nditions.\nAs illustrated in Fig. 3, this approach begins\nwith encodings of semantic cues from images and textual\nprompts to lay the groundwork for condition guidance. Our\nframework then focuses on aesthetic maintenance by a ded-\nicated processor that adjusts the semantic image style to\nbe consistent with the artistic image through adaptive in-\nstance normalization. In the semantic fusion section, Cre-\nativeSynth employs a decoupled cross-attention mechanism\nthat meticulously coordinates the interplay between visual\nand textual features, resulting in a cohesive synthesis rather\nthan a sum of its parts. Finally, the sampling process is\nbased on the principle of image inversion, which utilizes de-\nnoising techniques to reverse sample the image from the ini-\ntial noise. Ultimately, CreativeSynth generates customized\nartworks that resonate with the given semantic prompts and\nchosen aesthetic style.\n3.2. Condition Guidance\nCondition encoding.\nThe encoding process integrates\ntext and image features using a decoupled cross-attention\nmechanism within the framework of a pre-trained Stable\nDiffusion model [5]. For a given text prompt P, the tok-\nenizer and the text encoder from the pre-trained diffusion\nmodel are used to generate the text embeddings Etext \u2208\nRn\u00d7dtext:\nEtext = E(Tokenizer(P)),\n(1)\nwhere n is the sequence length and dtext is the text embed-\nding dimension.\nImage encoding requires converting images into a suit-\nable latent representation that can be processed by the gen-\nerative model. For an input image I, the encoding is com-\nFigure 3. The overall structure of CreativeSynth. Text features and image features are first acquired from separate text and image encoders,\nrespectively. Then, target and semantic images are interacted with by applying AdaIN to focus on image art features. An innovative\ndecoupled cross-attention mechanism is employed to fuse the attention between the multimodal inputs, which is subsequently integrated\ninto a U-Net architecture. The target image is transformed into a latent variable zT via DDIM Inversion, and the final output is refined\nthrough a denoising network.\nputed by a forward pass through the VAE\u2019s encoder network\nof the Stable Diffusion model:\nzI = EVAE(I).\n(2)\nUnlike the existing U-Net cross-attention architecture,\nwhich uses two paths to process text and image features\nseparately, each path consists of specialized cross-attention\nlayers that are dedicated to either text features or image fea-\ntures without interfering with each other.\n3.3. Aesthetic Maintenance\nStyle alignment.\nWe introduce a style alignment proces-\nsor to adjust the model\u2019s attention mechanism and nor-\nmalization layers.\nIt achieves an adaptive style blend-\ning between the aesthetic image and the semantic image.\nSpecifically, we utilize the adaptive instance normalization\n(AdaIN) [14] technique. It adjusts the semantic image Is\nto have the same mean and variance as the features of the\naesthetic image Ia. The AdaIN formula is as follows:\n\u02c6\nQs = AdaIN(Qs, Qa),\n(3)\n\u02c6\nKs = AdaIN(Ks, Ka),\n(4)\nwhere Qs and Ks are the query and key of the semantic im-\nage, and Qa and Ka are the query and key of the aesthetic\nimage, respectively. The AdaIN operation is defined as:\nAdaIN (x, y) = \u03c3 (y)\n\u0012x \u2212 \u00b5(x)\n\u03c3(x)\n\u0013\n+ \u00b5(y),\n(5)\nwhere \u00b5(x) is the mean of the semantic features, \u03c3(x) is the\nstandard deviation of the semantic features, and \u00b5(y) is the\nmean of the style features.\nShared attention.\nShared attention combines the charac-\nteristics of artistic images and semantic images, updating\nthe information in the semantic images based on the style\nof the artistic image. \u02c6\nQs and Kas represent the normalized\nquery and shared key, respectively, while V as denotes the\nvalue:\nKas =\n\u0014Ka\n\u02c6\nKs\n\u0015\n,\nV as =\n\u0014V a\nV s\n\u0015\n.\n(6)\nThe keys and values are aggregated together from the target\nimage and the reference image, while the query only repre-\nsents the attributes of the target image. The application of\nthe scaled dot-product attention mechanism is as follows:\nZ\u2032 = Attention( \u02c6Qs, KT\nas, V as) = Softmax\n \u02c6QsK\u22a4\nas\n\u221a\nd\n!\nV as,\n(7)\nwhere d is the dimensionality of the keys and queries.\n3.4. Semantic fusion\nDecoupled cross attention.\nText features are regarded as\nthe context for attention, and the editing text undergoes a\ncross-attention mechanism without sharing attention with\nthe aesthetic image features.\nBy decoupling the cross-\nattention mechanism, the shared attention results of images\nand the cross-attention results of texts are combined for\nthe final image generation. Each information stream (im-\nage and text features) is processed through its own cross-\nattention layer, after which they are merged to produce the\nfinal modified image feature Z\u2032\u2032:\nZ\u2032\u2032 = Z\u2032 + Softmax\n \nQK\u22a4\n\u221a\nd\n!\nV ,\n(8)\nhere Q, K, V are the transformed query, key, and value ma-\ntrices of the text features. The contribution of each decou-\npled attention operation is summed up to influence the final\nfeature representation.\n3.5. Sample Process\nImage inversion.\nIn order to reconstruct a real image un-\nder a given conditional text, we need to perform a reverse\nprocess to recover the image from random noise. We em-\nploy the deterministic Denoising Diffusion Implicit Models\n(DDIM) as our core denoising technique. Specifically, we\nuse the following reverse formula of DDIM to restore the\noriginal image:\nxt\u22121 =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t)\n\u0013\n+ \u03c3tz,\n(9)\nwhere \u03b1t is the step size factor in the denoising process, \u03f5\u03b8\nis the predicted noise, z is an optional noise vector used to\nincrease randomness, and \u03c3t is a factor that regulates the\nnoise intensity.\nIn the sampling process, we design an inversion callback\nfunction, which aims to adjust the latent space vectors at the\nend of each inversion step to ensure the text alignment of the\nimage. For this, we define the following callback function:\nCallback(zt, t) =\n(\nzT ,\nif t = T,\nzt,\notherwise.\n(10)\nHere zt denotes the latent variable corresponding to the\ntemporal index t, which is substituted with a pre-computed\nvector derived via a technique DDIM inversion [31]). This\nensures that throughout the diffusion process, our optimized\nlatent space vector remains highly consistent with the inher-\nent attributes of the target image.\nAesthetic maintenance image generation.\nUsing pre-\nprocessed latent vectors, our model generates a series of im-\nages that follow user-provided text input while maintaining\nstyle alignment provided by a reference image. To achieve\nthis, we introduce the IP-Adapter [8], which leverages em-\nbeddings from the reference image to guide the generation\nprocess. Given a target prompt (P) and a negative prompt\n(NP), the IP-Adapter (IPA) calculates the embeddings of\nthe prompts based on the following formula:\n(EP , ENP ) = IPA(P, NP, Ia),\n(11)\nwhere Ia symbolizes the artistic image.\n4. Experiments\n4.1. Implementation Details\nThe Stable Diffusion XL (SDXL) [15] model employed\nin our experiments has undergone pre-training on an ex-\ntensive corpus of text-image pairs. This pre-training regi-\nmen enables the exploitation of the model\u2019s full potential in\nterms of representational capacity. To ensure experimental\nconsistency, we have standardized the number of genera-\ntion steps and the guidance scale at 30 and 5.0, respectively.\nFurthermore, the input images utilized in the experiments\nare uniformly scaled to a resolution of 1024 \u00d7 1024 pixels\nfor both the image reversal process and subsequent image\nsynthesis tasks. On a single NVIDIA L40 GPU, each im-\nage takes five seconds to generate.\n4.2. Qualitative Evaluation\nImage fusion\nIn this analysis, we conduct baseline com-\nparisons for image fusion tasks involving models such as\nImage Mixer [32], Kosmos-G [33], and VD [11]. Qualita-\ntive results can be seen in Fig. 4. Image Mixer and Kosmos-\nG tend to generate results with subdued stylistic expression,\noften producing images that are more realistic than artis-\ntic. Meanwhile, VD demonstrates a uniform high saturation\nin artistic expressiveness across various outputs but fails\nto capture the nuances of distinct styles. In contrast, our\nmethod consistently excels in style rendering, effectively\nincorporates semantic information, and yields harmonious\nfusion results that exhibit both aesthetic appeal and artistic\nquality.\nText guided image editing\nTo accurately assess model\nperformance, we conduct baseline comparisons for the task\nof single-image text editing. As shown in Fig. 5, our model\ntakes succinct personalized text descriptions as input and\nsuccessfully performs operations such as semantic introduc-\ntion, facial attribute modification, and complex scene recre-\nation across a range of different scenes and objects. For\na comprehensive evaluation of our method, we select sev-\neral advanced baseline models for comparison, including\nIP-Adapter [8], ProSpect [13], DreamBooth [1], Textual In-\nversion [4], SDXL I2I [15], Instruct Pix2Pix [9], Masac-\ntrl [10], and Versatile Diffusion (VD) [11].\nBased on the results, although the IP-Adapter generates\nresults of superior quality, it fails to preserve the non-editing\ninformation of the target image. In terms of style consis-\ntency, some models like ProSpect, DreamBooth, and SDXL\nI2I exhibit high congruence with the target image. How-\never, Instruct Pix2Pix and Masactrl often damage the com-\nposition and content of the target image during editing, in-\ntroducing distortions and unnatural artifacts. For instance,\nFigure 4. Qualitative comparisons of our proposed CreativeSynth with other extant methods. The results offer a visualization of image\nfusion between artistic and real images.\nTable 1. Statistics of quantitative comparison with state-of-the-art methods. Specific metrics include aesthetic scores, CLIP-T, CLIP-I and\nhuman rates. The best results are in bold while the second best results are marked with underline.\nOurs\nIP-Adapter\nProSpect\nDreamBooth\nTI\nSDXL I2I\nInstruct P2P\nMasactrl\nVD\nImage Mixer\nKosmos-G\nAesthetic Score \u2191\n7.563\n7.249\n6.297\n6.216\n6.441\n6.636\n5.344\n5.707\n6.818\n7.151\n6.125\nCLIP-T \u2191\n59.123\n57.956\n58.004\n46.792\n48.576\n57.981\n55.203\n45.147\n53.516\n-\n-\nCLIP-I \u2191\n52.067\n-\n-\n-\n-\n-\n-\n-\n44.973\n48.349\n50.564\nOverall Preference\n-\n24.1%\n19.2%\n1.5%\n3.0%\n12.9%\n3.3%\n14.7%\n8.6%\n20.3%\n26.1%\nAesthetic Preference\n-\n34.2%\n13.4%\n44.1%\n4.6%\n18.7%\n10.1%\n24.1%\n8.6%\n28.4%\n31.6%\nSemantic Preference\n-\n17.7%\n10.6%\n4.1%\n5.1%\n18.0%\n6.1%\n9.4%\n12.9%\n21.5%\n32.4%\nimages processed by Instruct Pix2Pix show obvious ghost-\ning effects on headphones and ice cream, while Masactrl\nfaces challenges in generating specific and realistic human\nfaces. ProSpect and SDXL I2I perform admirably in pre-\nserving the semantic content of the original image, but of-\nten experience significant alterations in key features such as\nfacial details of people, impacting the image\u2019s authenticity\nand credibility. In contrast, DreamBooth\u2019s results display\nvery limited input image information changes, leading to\nthe production of images that nearly do not align with the\ntext editing requirements, thus limiting their potential for\npractical application. Lastly, Textual Inversion and VD can\ngenerate quite distinctive artworks, which are creative but\nstill deviate significantly from the target image in terms of\nstyle and semantic preservation.\nCompared to baselines, our results guarantee a high level\nof content fidelity and stylistic coherence during image\nmodification. The altered images retain the principal struc-\nture of the original while integrating new attributes or al-\nterations in accordance with text-based directives. In the\ndomain of facial attribute editing, our method yields facial\nfeatures that are both more natural and realistic, minimiz-\ning visual anomalies and undue artistic alterations. Further-\nmore, our approach facilitates the effective reconstruction\nand editing of intricate scenes without disrupting the global\ncomposition of the image.\n4.3. Quantitative Evaluation\nTo extensively evaluate the performance of our pro-\nposed method, this paper uses three key metrics\u2014aesthetic\nscore [34], CLIP-T [35], and CLIP-I [35]\u2014for quantitative\ncomparison with the current state-of-the-art methods. The\naesthetic score reflects the visual appeal and artistic qual-\nity of the generated images, CLIP-T characterizes the se-\nmantic consistency between the generated images and the\nedited text, while CLIP-I indicates the visual and content\ncoherence between the generated images and the target im-\nages. The comparison results are shown in Table 1. In terms\nof aesthetic score, our method significantly surpasses other\nmethods, achieving the highest average score of 7.563,\nFigure 5. Visual comparison of our proposed CreativeSynth with state-of-the-art methods for text-guided editing of diverse types of art\nimages.\ndemonstrating its outstanding performance in overall aes-\nthetic appeal. The CLIP-T score quantifies the consistency\nof the generated images with the given text description. The\nhigher the score, the more accurately the image reflects the\ncontent of the text description. Our method tops this assess-\nment with a score of 59.123, indicating its efficient capabil-\nity to reproduce the semantic information specified by the\ntext in images. Our work also exhibit superior performance\non the CLIP-I metric, highlighting our method\u2019s capabil-\nity in maintaining visual semantic. The score indicates that\nour method is able to preserve consistent visual quality and\ndetail fidelity when considering image fusion. In summary,\nour method offers a balanced high-quality image generation\nsolution as a whole, enabling it to meet the diverse require-\nments of different usage scenarios.\nUser study\nWe benchmark CreativeSynth with ten other\nleading-edge image-generation techniques to determine\nwhich generates the most favored artistic outcomes. We\npresented each participant with 50 randomly selected sets\nof results, displaying the images produced by CreativeSynth\nand an alternative method in no particular order. We asked\nparticipants to identify the results that (1) were the most vi-\nsually pleasing overall, (2) most closely matched the artistic\nexpression of the target image, and (3) most closely related\nto the editorial semantics of the text or image. In the end,\nwe obtained 11, 850 votes from 79 participants, and the per-\ncentage of votes for each method is detailed in rows 4\u22126 of\nTable 1. It is worth noting that CreativeSynth is particularly\npopular in the categories of ink drawing, oil painting, and\ndigital art.\n4.4. Ablation Study\nMechanism dissection\nTo deeply investigate the under-\nlying mechanisms of our proposed method, we conduct a\nseries of ablation experiments. We remove AdaIN, inver-\nsion, and IPA, respectively, to observe their impact on the\ngenerated image variations. As Fig. 6(a) demonstrates, the\nabsence of AdaIN leads to discrepancies in the colors of\nthe results compared to the style image. Without inversion,\nthere is a significant difference in the facial feature details\nof the generated image relative to the input semantic image.\nFigure 6. Results of ablation study on (a) AdaIN, inversion, and\nIPA, (b) IPA scale, and (c) guidance scale.\nIn the absence of IPA, consistent character information is\nnot accurately introduced. However, the complete model\nretains style information and better incorporates semantic\ninformation.\nIPA\nAs Fig. 6(b) shows, we fine-tune the scale of IPA and\nrecord the changes in the generated images across various\nscales. With an increase in the IPA scale, the generated im-\nages more accurately reflect the expected style and content\nunder the given conditions, demonstrating IPA\u2019s crucial role\nin controlling the generation process. Furthermore, we note\nthat a proper increase in IPA scale significantly enhances\nthe flexibility of image editing, allowing for more refined\nstyle adjustments and content updates.\nCondition guidance\nBy analyzing the visual results pro-\nvided in Fig. 6(c), we discover that as the guidance scale\nincreases, the details of the results become richer and more\nprecise, aligning closer to the target style. This set of ex-\nperiments, supported by both the visual demonstration in\nFig. 6(c), confirms that increasing the guidance scale sig-\nnificantly improves the clarity and detail representation of\nthe images, as well as the controllability over the gener-\nated images, thereby enhancing their accuracy and editabil-\nity. Consequently, adjusting the guidance scale parameter\neffectively optimizes the performance of our image genera-\ntion algorithm.\n5. Conclusions and Future Work\nIn this paper, we present CreativeSynth, a unifying\nframework designed to enable creative fusion and synthesis\nof visual artworks. The primary aim is to infuse multimodal\nsemantic information into the world of artworks. This pro-\ncess ensures the preservation of the inherent themes, emo-\ntions, and narratives of the art pieces, transcending a mere\noverlay of style onto natural images. In this way, each syn-\nthesized work is not only a visual fusion, but also an inter-\nsection of meaning and story; with a strong personality, a\nunique visual narrative, and an exclusive emotional depth.\nExperimental results have shown that CreativeSynth is not\nonly popular for its visual results, but also highly effective\nin executing user-specific artistic editorial intent. In the fu-\nture, we plan to apply this approach to different image gen-\neration architectures and to broaden its application to en-\ncompass other forms of media, such as video. With subse-\nquent refinements and applications, our approach will help\ncreators realize creative expression like never before.\nReferences\n[1] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 22500\u201322510, 2023.\n2, 3, 5\n[2] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.\nKey-locked rank one editing for text-to-image personaliza-\ntion.\nIn ACM SIGGRAPH 2023 Conference Proceedings,\nSIGGRAPH \u201923, New York, NY, USA, 2023. Association\nfor Computing Machinery. 2\n[3] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Encoder-based domain\ntuning for fast personalization of text-to-image models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201313, 2023. 2\n[4] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. In International Conference\non Learning Representations (ICLR), 2023. 2, 3, 5\n[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n10684\u201310695, 2022. 2, 3\n[6] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 3\n[7] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In Inter-\nnational Conference on Machine Learning (ICML), 2022. 2,\n3\n[8] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 2, 5\n[9] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 18392\u201318402, 2023. 2, 5\n[10] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 2, 3, 5\n[11] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang,\nand Humphrey Shi. Versatile diffusion: Text, images and\nvariations all in one diffusion model. In IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 7754\u2013\n7765, 2023. 2, 5\n[12] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang,\nChongyang Ma, Weiming Dong, and Changsheng Xu.\nInversion-based style transfer with diffusion models.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10146\u201310156, 2023. 2, 3\n[13] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang,\nHaibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver\nDeussen, and Changsheng Xu.\nProspect: Prompt spec-\ntrum for attribute-aware personalization of diffusion models.\nACM Transactions on Graphics (TOG), 42(6):244:1\u2013244:14,\n2023. 2, 3, 5\n[14] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization. In IEEE In-\nternational Conference on Computer Vision (ICCV), pages\n1501\u20131510, 2017. 2, 3, 4\n[15] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 5\n[16] Bin Wang, Wenping Wang, Huaiping Yang, and Jiaguang\nSun. Efficient example-based painting and synthesis of 2D\ndirectional texture. IEEE Transactions on Visualization and\nComputer Graphics (TVCG), 10(3):266\u2013277, 2004. 3\n[17] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.\nImage style transfer using convolutional neural networks.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2414\u20132423, 2016. 3\n[18] Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron\nHertzmann, and Eli Shechtman. Controlling perceptual fac-\ntors in neural style transfer. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n3730\u20133738, 2017. 3\n[19] Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, and\nJiebo Luo. ArtFlow: Unbiased image style transfer via re-\nversible neural flows. In IEEE/CVF Conferences on Com-\nputer Vision and Pattern Recognition (CVPR), pages 862\u2013\n871, 2021. 3\n[20] Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang,\nChongyang Ma, Tong-Yee Lee, and Changsheng Xu. Do-\nmain enhanced arbitrary image style transfer via contrastive\nlearning. In ACM SIGGRAPH 2022 Conference Proceed-\nings, 2022. 3\n[21] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma,\nXingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image\nstyle transfer with transformers. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n11326\u201311336, 2022. 3\n[22] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel\nRuiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan\nEssa, Michael Rubinstein, et al. Styledrop: Text-to-image\nsynthesis of any style. In Advances in Neural Information\nProcessing Systems (NIPS), 2023. 3\n[23] Nisha Huang, Yuxin Zhang, Fan Tang, Chongyang Ma,\nHaibin Huang, Weiming Dong, and Changsheng Xu. Diff-\nstyler: Controllable dual diffusion for text-driven image styl-\nization. IEEE Transactions on Neural Networks and Learn-\ning Systems, pages 1\u201314, 2024. 3\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 3\n[25] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Con-\nference on Machine Learning (ICML), pages 8821\u20138831.\nPMLR, 2021. 3\n[26] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin Neural Information Processing Systems (NIPS), 32, 2019.\n3\n[27] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel\nCohen-Or. Style aligned image generation via shared atten-\ntion. arXiv preprint arXiv:2312.02133, 2023. 3\n[28] Nisha Huang, Fan Tang, Weiming Dong, and Changsheng\nXu. Draw your art dream: Diverse digital art synthesis with\nmultimodal guided diffusion. In ACM International Confer-\nence on Multimedia, page 1085\u20131094, 2022. 3\n[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4401\u20134410, 2019. 3\n[30] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 3836\u20133847, 2023. 3\n[31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models.\nIn IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n6038\u20136047, 2023. 3, 5\n[32] Pinkney, Justin, 2024. Last accessed on 2024-01-10. 5\n[33] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. Kosmos-g: Generating images\nin context with multimodal large language models. arXiv\npreprint arXiv:2310.02992, 2023. 5\n[34] Beaumont, Romain and Schuhmann, Christoph, 2024. Last\naccessed on 2024-01-15. 6\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning\n(ICML), pages 8748\u20138763, 2021. 6\n[36] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 10\nAppendix\nA. Diffusion\nThe diffusion process is simulated through a gradual\nnoise addition process, where noise is progressively intro-\nduced to the clear original image x0, generating a series of\ntransitional latent variables (x1, ..., xT ). In the denoising\ndiffusion model, this process is defined as:\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03b8(xt\u22121, t),\nwhere \u00af\u03b1t = Qt\ni=1 \u03b1i is the cumulative product factor for\ntime step t, and \u03f5 is a neural network model learned from\nrandom noise. In this process, we gradually apply forward\nnoising to the original image x0, creating a series of in-\ncreasingly noised images x1, x2, . . . , xT until an image that\nis nearly pure noise xT is generated.\nSubsequently, we\nreconstruct the image using a reverse process, that is, by\ndenoising steps learned step by step, we trace back from\nthe noisy image xT to the original image x0. The key to\nour approach is the Denoising Diffusion Implicit Models\n(DDIMs) [36], which enables precise control over synthe-\nsis, serving as the backbone of the algorithm. DDIM em-\nploys a non-Markovian diffusion process, characterized by\na sequence of forward noising steps followed by a reverse\ndenoising procedure.\nIn our method, the reverse diffusion process follows the\nformal representation below to progressively restore the la-\ntent clean image:\nzt\u22121 = \u221a\u03b1t\u22121\n\u0012zt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(zt, t)\n\u221a\u03b1t\n\u0013\n+\np\n1 \u2212 \u03b1t\u22121\u03f5\u03b8(zt, t),\nwhere \u03b1t represents the steps of the predetermined variance\nschedule, and \u03f5\u03b8 is a parameterized neural network respon-\nsible for predicting the noise component in the image at\ntime t. This process starts from the initial latent represen-\ntation zT obtained from a prior noise distribution N(0, I)\nand gradually decreases the noise until the original image\nrepresentation in the latent space z0 is completely restored.\nIn this way, our model can perform precise image synthe-\nsis while providing good control and predictability over the\nsynthesis process.\nB. Additional User Study\nTo further understand the necessity of our proposed idea,\nwe also designed the following questions in the user re-\nsearch questionnaire: (1) whether the idea is necessary, (2)\nwhether it creates realistic artworks, and (3) whether the\nresult is novel and interesting. Finally, we collected 237\nvotes from 79 participants. The voting results showed that\na percentage of 94.9% supported the necessity of our idea,\nthe potential of the idea to be applied in practice, and the\npossibility to fulfill a specific need. In addition, 91.1% of\nthe participants believed that it is indeed possible to create\nhighly realistic works of art based on our idea, which shows\nthe credibility of our technical realization and the recogni-\ntion of the expected results. Most strikingly, a whopping\n96.2% found the artworks generated by our idea to be inno-\nvative and appealing. In view of this, our idea is not only\nwidely recognized by the potential user community, but also\nhas some prospects for practical application.\nC. More results\nThe additional results of CreativeSynth are presented in\nFigs. 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19. These\nresults demonstrate the performance of our model in terms\nof diversity and quality.\nFigure 7. More results by CreativeSynth.\nFigure 8. More results by CreativeSynth.\nFigure 9. More results by CreativeSynth.\nFigure 10. More results by CreativeSynth.\nFigure 11. More results by CreativeSynth.\nFigure 12. More results by CreativeSynth.\nFigure 13. More results by CreativeSynth.\nFigure 14. More results by CreativeSynth.\nFigure 15. More results by CreativeSynth.\nFigure 16. More results by CreativeSynth.\nFigure 17. More results by CreativeSynth.\nFigure 18. More results by CreativeSynth.\nFigure 19. More results by CreativeSynth.\n"
  },
  {
    "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
    "link": "https://arxiv.org/pdf/2401.14367.pdf",
    "upvote": "6",
    "text": "Published as a conference paper at ICLR 2024\nGENIE: ACHIEVING HUMAN PARITY IN\nCONTENT-GROUNDED DATASETS GENERATION\nAsaf Yehudai \u2662 \u2663, Boaz Carmeli \u2662, Yosi Mass \u2662, Ofir Arviv \u2662, Nathaniel Mills \u2662,\nAssaf Toledo \u2662, Eyal Shnarch \u2662, Leshem Choshen \u2662 \u2661\nIBM Israel Research Lab \u2662, Hebrew University of Jerusalem \u2663, MIT \u2661\n{Asaf.Yehudai, leshem.choshen}@ibm.com\nABSTRACT\nThe lack of high-quality data for content-grounded generation tasks has been iden-\ntified as a major obstacle to advancing these tasks. To address this gap, we pro-\npose Genie, a novel method for automatically generating high-quality content-\ngrounded data. It consists of three stages: (a) Content Preparation, (b) Genera-\ntion: creating task-specific examples from the content (e.g., question-answer pairs\nor summaries). (c) Filtering mechanism aiming to ensure the quality and faith-\nfulness of the generated data. We showcase this methodology by generating three\nlarge-scale synthetic data, making wishes, for Long-Form Question-Answering\n(LFQA), summarization, and information extraction. In a human evaluation, our\ngenerated data was found to be natural and of high quality. Furthermore, we com-\npare models trained on our data with models trained on human-written data \u2013 ELI5\nand ASQA for LFQA and CNN-DailyMail for Summarization. We show that\nour models are on par with or outperforming models trained on human-generated\ndata and consistently outperforming them in faithfulness. Finally, we applied our\nmethod to create LFQA data within the medical domain and compared a model\ntrained on it with models trained on other domains.\n1\nINTRODUCTION\nContent-grounded generation is needed in various tasks, such as Retrieval-Augmented Generation\n(RAG), and content-based virtual assistants. In such tasks, the model is expected to generate a\nresponse based on a given content (i.e., information). For example, answer a question given a\ndocument that includes information needed for the answer. Zheng et al. (2023) found those types of\ntasks to be the second most common use cases of Language Models.\nCreating datasets with elaborate responses, which rely on long content, requires an expensive and\ndemanding manual process. This may explain why such datasets are scarce even for popular tasks\nsuch as question-answering generation. Moreover, most existing datasets were collected from noisy\navailable resources, such as news providers (Hermann et al., 2015) and Reddit user posts (Fan et al.,\n2019). This lack of high-quality content-grounded data has been identified as one of the obstacles\nto advancing long-form QA (Stelmakh et al., 2022) and domain-specific summarization (Zhu et al.,\n2020), among other content-based tasks.\nTo address this gap, we suggest Genie, Generate information & elucidate1, a method for creating\nsynthetic training data for any domain and any content-grounded task. We propose a three-step pro-\ncess: (a) Content Preparation, (b) Generating, and (c) Filtering. Preparation is fairly straightforward,\nas the data may be noisy; it is best to clean it. The generation is done using a few-shot prompting ap-\nproach with a large language model (LLM). See an example in App. D. Finally, since the generation\nis automatic, we filter its outputs to ensure their faithfulness, well-formedness, and overall quality.\nGenie offers flexibility and can generate synthetic data for different domains and content-grounded\ngeneration tasks. We apply it to the tasks of long-form QA (LFQA), summarization (\u00a7 3), and\ninformation extraction (IE) (App. C) by creating wish-QA, wish-summarization, and wish-IE. We\nthen show in a manual evaluation that it generates high-quality data that is natural, faithful, and\n1We wished for a cool name and that is what we\u2019ve got.\n1\narXiv:2401.14367v1  [cs.CL]  25 Jan 2024\nPublished as a conference paper at ICLR 2024\nCorpus\n1: Content Preparation\n2: Generation\nQuestion: What is the lowest point on dry land?\nAnswer: The lowest point on dry land is the \nshore of the Dead Sea, shared by Israel, and \nJordan. It is 418 m (1,371 feet) below sea level.\n3: Filtering\nFaithfulness \nModel\nReward \nModel\nTask\nData  \nLLM\nGenerated Task Example\nExtracted Content\nTask description\nContent:Example\n\u2026\nContent:Example\n<New Content Here>\nFew-Shot Learning Prompt\nFigure 1: Genie\u2019s three steps are: (1) Extract content from the source data (2) Prompt an LLM to\ngenerate task-specific examples based on the provided content (3) filter low-quality and unfaithful\nexamples to ensure data quality.\nlexically diverse (\u00a74). For the task of LFQA, we compare the performance of models that were\ntrained with wish-QA generated by Genie to those trained with the same amount of data, generated\nby humans (\u00a75). We show that the former models outperform or are on par with the latter models.\nAdditionally, faithfulness scores show that models trained on our synthetic data are more faithful to\nthe grounding content. Those results show the overall efficacy and faithfulness of our data as training\ndata compared to that of human-generated data. We replicate our success with summarization,\nshowcasing the generality of the method. We publicly release all three wishes datasets.\n2\nAUTOMATICALLY CURATING DATASET FOR CONTENT-GROUNDED TASKS\nNext, we detail the steps of Genie for automatically curating high-quality content-grounded datasets.\nFigure 1 depicts the three steps in Genie: Content Preparation, Generation, and Filtering. For sim-\nplicity, we refer to content-grounded data point, like a question-answer pair or a summary, as an\nexample.\n2.1\nCONTENT PREPARATION\nIn the preparation step, we obtain the grounding content. This is done in a rule-based fashion by\nextracting passages from the raw documents. This step is the least general of our approach as it relies\non the specific format in which the data is stored. If the data already exists in easy-to-use passages,\nit can be used as is. For example, broken by lines, found in a table, or conveniently extracted from\nanother dataset. As the general case, we describe the extraction of content passages directly from\nweb pages.\nImplementation details.\nWe crawled Wikipedia pages using browser emulation to allow dynamic\ncontent to be retrieved. Then we pass the full HTML DOM through filters to remove noise (e,g.,\nheaders, footers, sidebars, etc.). We are left with the main page content which is then transformed\ninto Markdown, preserving the document structure (e.g., lists, tables, links, image references, arti-\ncles, and sections). From this structure a table of contents is derived and based on it we break the\nMarkdown page into passages.\n2\nPublished as a conference paper at ICLR 2024\n2.2\nGENERATION\nIn the generation step, we prompt a large language model to generate a synthetic example. For that,\nwe use the in-context capabilities of the model. We prompt the model with four content-example\npairs followed by the extracted content from the corpus with no example (see the prompt in appendix\n\u00a7D). The LLM generates a new example to match the extracted content.\nImplementation details.\nWe decode greedily, which encourages the models to produce more\ngrounded responses (Honovich et al., 2022b). In addition, we create two variants of the data, one\nby generating examples using Falcon-40B (Penedo et al., 2023) and another by generating with\nLlama-2-70B (Touvron et al., 2023). In general, the results using the different models have similar\ntendencies, with Llama being slightly better (see replications with Llama in an appendix \u00a7B). As\nFalcon is purely pre-trained, without additional steps we mainly report results relying on Falcon, to\nshowcase that our method is not dependent on further alignment and instruction steps.\n2.3\nFILTERING\nIn the filtering step, we score each content-example pair for its format, faithfulness (i.e., grounded),\nand quality. For each such aspect, we implement a scoring function and filter low-scoring pairs.\nFormat.\nWe filter out examples where parts of the template are missing (e.g., in QA, when the\nprefixes signifying the start of the question or the answer are absent). Furthermore, we filter ex-\namples that are too short (less than ten words) or too long (surpassing 1.5 times the length of the\ngrounding content for LFQA, and 0.25 for Summarization).\nFor QA, we use [document], [question], and [answer] as prefixes before each corresponding element.\nFor summarization [document], [summarize], and [summary] with [summarize] representing the\nspecific summarization instruction. It is important to note that we did not fine-tune these prompts.\nFaithfulness.\nTo validate that the model-generated examples are grounded in the content, we adopt\nan off-the-shelf faithfulness metric and filter low-scoring examples. When deployed with trustwor-\nthy data, this can serve as a measure of correctness.\nWe test faithfulness by mapping the problem into a Textual Entailment (Dagan et al., 2005) or Nat-\nural Language Inference (Bowman et al., 2015) (NLI) problem. NLI involves two input sentences:\na hypothesis and a premise. The objective is to determine whether the hypothesis can be inferred\nfrom the premise, contradicts it, or is neutral with respect to it. NLI models were widely utilized\nfor faithfulness consistency evaluation (Honovich et al., 2021; Dziri et al., 2022), and most simply\nby taking the grounding text as the premise and the generated example as the hypothesis (Maynez\net al., 2020). Here, we use the fine-tuning T5-11B NLI model presented in Honovich et al. (2022a)\nfor assessing the generated example faithfulness.\nQuality.\nAn important aspect of our methodology involves evaluating the quality of the generated\nexamples, specifically quantifying their relevance to the corresponding task. Note that the task may\nbe constant throughout a dataset (as is often the case of summarization) or be dependent upon an\ninstruction (such as the question in question answering). To judge the quality automatically we use\na reward model.\nReward models are trained on human preference data to give a high reward for answers that human\nannotators prefer. Such models can quantify quality in a human-like way, considering dimensions\nthat are hard to isolate and measure independently by dedicated metrics. Reward models are used\nas quality scores for Reinforcement Learning optimization (Ouyang et al., 2022), and also serve as\nreference-less evaluation metrics for text generation tasks (Touvron et al., 2023).\nHere, we use the reward model for both purposes and rely on the Open-Assistant model (K\u00a8opf\net al., 2023), using the DeBERTa-v3 architecture (He et al., 2021). We filter generated examples\nwhose score is below 0.5 by the reward model reward-model-deberta-v3-large-v2. 2 We chose 0.5\n2https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2\n3\nPublished as a conference paper at ICLR 2024\nas a threshold based on experimentation. Similarly, we use t5 xxl true nli mixture 3 model to filter\nexamples deemed unfaithful by it.\n3\nEXPERIMENTAL SETUP\nHere we describe the datasets we utilized in our content-grounded generation tasks: LFQA and\nsummarization (\u00a73.1). Subsequently, we outline the various synthetic datasets we generated (\u00a73.2),\nand finally, we discuss the models employed for training and the evaluation metrics (\u00a73.4)\n3.1\nDATASETS\nELI5.\n(Explain Like I\u2019m Five) (Fan et al., 2019) comprises open-ended questions and extensive\nresponses authored by users within the Reddit forum of the same name. To these questions and\nanswers, retrieved documents were added as grounding content. In their manual analysis, they have\nfound that the content is sufficient to answer 65% of the questions and have information relevant to\n92% of the questions. In this work, we use the KILT version of the dataset (Petroni et al., 2020).\nASQA.\n(Answer Summaries for Questions which are Ambiguous) (Stelmakh et al., 2022) is a\ndataset that pairs ambiguous questions from the AmbigQA dataset (Min et al., 2020) with metic-\nulously crafted long-form answers generated through crowdsourcing. To add grounding they have\nused the same method presented in ELI5, but specifically retrieved documents from Wikipedia.\nNQ.\n(Natural Questions) (Kwiatkowski et al., 2019) is a dataset of real user questions sourced\nfrom the Google search engine. It includes questions and their corresponding passages (named long\nanswers) from Wikipedia which provide potential answers and contain extractive short answers.\nThis dataset does not have long-form answers, and here we will use only its documents for our\nsynthetic data generation process \u00a73.2 and will compare our synthetic questions with the questions\nfrom NQ.\nCNN-DailyMail.\n(Kwiatkowski et al., 2019) is a dataset commonly used for text summarization.\nIt consists of news articles from CNN and the DailyMail along with their human-written summaries.\n3.2\nGENERATING SYNTHETIC DATASETS\nThe datasets described above were used to create datasets of synthetic data;\nWish-QA-NQ.\nTo create this dataset, we draw upon NQ passages (Kwiatkowski et al., 2019),\nfor our synthetic data generation process. These passages are well-suited for our process because\nthey were originally extracted from Wikipedia pages by annotators and typically consist of well-\nstructured paragraphs, each centered around a specific topic.\nWish-QA ELI5/ASQA.\nFor the creation of a dataset that mimics the conditions of ELI5 and\nASQA, where answers can be derived from multiple documents, we rely on the top three retrieved\npassages from either of the corresponding corpus. These passages are used as the grounding docu-\nments for constructing this synthetic dataset.\nIn addition, we make a new wish dataset entirely from crawled data:\nWish-QA.\nstands for Wikipedia from Scratch4, is a novel data we constructed following the gen-\neral approach for crawling and processing as detailed in Section \u00a72.1. It represents a realistic data\ngeneration use case from unprocessed content. We note that the extracted passages may exhibit\nnoise and lack of coherence and conciseness.\n3https://huggingface.co/google/t5 xxl true nli mixture\n4Wish-QA is also the general name for all our synthetic QA datasets\n4\nPublished as a conference paper at ICLR 2024\n3.3\nMODELS FOR EXTRINSIC EVALUATION\nIn the Extrinsic evaluation, our goal is to compare the performance of models trained on our synthetic\ncontent-grounded data with those trained on data generated by humans. To ensure a fair comparison,\nwe maintain an equal number of examples from each dataset (10,000) and employ identical models\nfor training, using the same set of hyperparameters. The models we use for training are Flan-xl (Wei\net al., 2021) and llama-2-13b-Chat (Touvron et al., 2023). These models serve as the foundation for\nfacilitating comparisons across architectural variations, including Encoder-Decoder and Decoder-\nonly models, as well as different variations of instruction fine-tuning and alignment training.\n3.4\nEVALUATION METRICS\nWe evaluate the performance with ROUGE as a lexical similarity metric (Lin, 2004), BERT-Score\nas a model-based reference-based metric (Zhang et al., 2019b), and Reward model as a model-based\nreference-less metric. We reuse the ANLI faithfulness metric and reward mentioned in the filtering\nfor evaluation. For faithfulness evaluation, we also calculate the K-Precision lexical similarity metric\n(Adlakha et al., 2023). Different performance metrics (Post, 2018; Zhang et al., 2019a, and more)\nshowed similar results in initial trials, showing reliability of different forms (Perlitz et al., 2023).\nROUGE.\nFollowing the conventional approach of assessing generated text quality, including long-\nform answers (Fan et al., 2019), we report the ROUGE-L score (Lin, 2004).\nBERT Score.\n(Zhang et al., 2019b) is a semantic similarity-based metric that leverages pre-trained\nlanguage models to predict if the model response is semantically equivalent to the gold answer.\nKasai et al. (2022) have shown BERT Score F1 is effective in evaluating many generation tasks.\nK-Precision.\nFollowing Adlakha et al. (2023) we report K-Precision, as it showed the highest\ncorrelation with human judgments from all lexical metrics. The metric follows the intuition, that in\nfaithful response most words need to come from the content.\n4\nINTRINSIC EVALUATION\nIn this section, we perform intrinsic evaluation and validation of Wish-QA. We conduct a micro\nTuring Test, presenting synthetic and human questions side by side. We show that the questions\ngenerated synthetically are more natural than most of those found in available datasets. We also test\nthe whole workflow and show that the filters contribute to the generated data quality and that Genie\nis cost and time-efficient and creates diverse data.\nNaturalness Evaluation.\nTo assess the naturalness of our questions, we conducted a human-\nevaluation experiment. In the experiment, an expert annotator5 was provided with two questions:\none human-created and the other synthetic. Both questions were based on the same content. The\nannotator\u2019s task was to identify the question they believed was human-written. For this experiment,\nwe sampled 100 questions from ELI5, ASQA, and NQ, along with their 100 synthetic counterparts.\nThe results in Table 1 (and App. \u00a7 B) indicate that for ELI5, the synthetic question was selected as\nthe human-written one in 72% of the cases, for NQ it was 63%, and for ASQA it was 49%. These\nresults suggest that our synthetic questions are more natural and human-like than questions collected\nfrom sources like Reddit and Google Search engine. Additionally, they are indistinguishable from\nquestions written by experts, such as those in the ASQA dataset. As a side finding, we also find that\nthe ASQA dataset is of higher quality than the others, which experiments below replicate.\nMulti-Dimensional Quality Assessment.\nIn this assessment, we aimed to investigate the qualities\nof the generated data and the impact of the filtration processes. We focused on the following dimen-\nsions: relevance and clarity of the questions, and faithfulness and overall quality of the answers. To\naccomplish that, we randomly selected 100 questions from the unfiltered and filtered Wish-QA. For\neach content-question-answer triplet, we asked annotators to answer a list of questions as shown in\n5A non-author, native English speaker with an MA degree.\n5\nPublished as a conference paper at ICLR 2024\nTable 1. The first two assessment questions aim to assess the relevance and clarity of the question.\nThe clarity question is inspired by the findings of Min et al. (2020), which revealed that more than\nhalf of naturally occurring factoid questions are ambiguous. Following that, we include three ques-\ntions related to the answer quality. These questions are designed to ascertain whether the answer\nadequately addresses the question while remaining faithful to the underlying content. Lastly, we ask\nfor an overall quality rating on a 5-level Likert scale.\nHuman assessment results in Table 1 demonstrate that the filtration process had a significant impact\non the relevance of the questions. Although our filtration setup does not directly assess the questions,\nwe find that our faithfulness filter together with the reward filter provides an indirect signal about\nthe relevance of the question. We also observed an improvement in the percentage of answers that\nwere found to address the question. Faithfulness results show decent improvement, but there is still\nroom for enhancement. Annotators\u2019 interviews reveal that despite the presence of unfaithful cases\nin the dataset, their granularity was often more subtle. In some instances, the model added missing\npieces of information that were subsequently found to be factually correct.\nWe observe a slight improvement in the clarity of questions, coupled with almost all answers ad-\ndressing the questions. This highlights that our answer is a single relevant response from a wide\nspace of plausible answers, a well-documented phenomenon in LFQA (Krishna et al., 2021). Lastly,\nwe identify an improvement in the overall score, which leads us to the conclusion that the filtering\nprocess substantially contributes to the quality and faithfulness of our dataset.\nTable 1: Multi-Dimensional Quality assessment for synthetic data generated from scratch. Results\nshow a large improvement in question relevance and the percentage of answers that address the\nquestion, answers that are faithful, and overall answer scores.\nQuality Review Question\nWiFS w/o filters\nWiFS w/ filters\nIs the question relevant to the content?\n67%\n92%\nIs the question clear? (not ambiguous)\n63%\n67%\nDoes the answer address the question?\n80%\n98%\nIs the answer faithful to the content?\n53%\n76%\nGrade the overall quality of the answer\n3.48\n4.58\nDiversity.\nOur synthetic data is built on top of large-scale content that covers many different dis-\ntinct topics. As a result, our data contain diverse lexicons. We compute vocd-D (McCarthy & Jarvis,\n2010) to measure the lexical diversity of our data. We found that the lexical diversity of all synthetic\ndata is higher than their human-generated counterparts (see Table 6). We also can see that most\nresponse lengths are similar to the ones in the human writing datasets.\nScale.\nWith 300K samples overall (full statistics in App. A), our dataset collection balances scale\nand quality. ELI5 is of a similar size but noisy, and ASQA is carefully annotated but much smaller.\nMonetary and Time Cost.\nGenie is more cost-efficient and time-efficient than the traditional\napproach of crowd-sourced dataset curation. The cost of API\u2019s calls of models like the ones used\ntypically ranges from $0.02 to $0.04, while the cost of an expert annotator to create a question is\napproximately $4.45 (Stelmakh et al., 2022). According to this rate, the 300K examples in our\nsynthetic dataset would have cost over $1M. The time it takes to generate 10 examples is less than a\nminute, i.e. much faster than the time that it would take a human to read the context.\n5\nEXTRINSIC EVALUATION\nFinding the synthetic data to be of high quality, we test its usefulness for improving training. We\npresent quantitative results from our extrinsic experiments, evaluating models trained on synthetic\nand human-generated data on the ASQA and ELI5 test sets.\nIn Table 2 (and App. \u00a7 B) we present Flan-xl results trained on human and synthetic data. We note\nthat here by synthetic in-domain we refer to the case where the train and test come from the same\ndataset, either ELI5 or ASQA.\n6\nPublished as a conference paper at ICLR 2024\nResults indicate that synthetic data is a competitive alternative even when human-generated data\nalready exists. In all cases, we see substantial gains from training on the synthetic data. For example,\nRouge-L almost triples from 10.5 to 28.2 for Synthetic NQ. This gain is over the already strong\nmultitask baseline (Flan) that trained on thousands of tasks, many of which are forms of question\nanswering.\nMoreover, the synthetic data provides better or comparable results in all metrics even for cases where\ntrain and test data come from the same dataset. While \u2013 for ASQA \u2013 Rouge-L and Bert-Score are\nslightly lower than the in-domain training data, the synthetic data is even better than the human data\non the rest of the scores on ELI5. We conclude that, if no human-generated data exists, automatically\ngenerating it has the potential to be as good.\nASQA performs better on both ASQA and ELI5 test sets. This observation implies that ASQA\nis, on the whole, a superior dataset compared to ELI5. This aligns with the substantial annotation\nefforts invested in the creation of ASQA, in contrast to the noisy and automatically scraped ELI5\ndata. However, it is important to note that this meticulous curation has led to a considerably smaller\ndataset for ASQA, totaling approximately 6k examples including the development and test sets\n(compared to 272k examples in ELI5). This emphasizes the contribution of our approach which\nallows large-scale high-quality data generation.\nAnother strong support for the effectiveness of our data generation approach is exemplified by the\nmodel\u2019s outputs being favored by the preference reward model, achieving comparable or higher\nresults than the gold standard of both datasets.\nWish-QA seems to work well even with the noisy content. Wish-QA-NQ data outperformed the\nsynthetic in-domain data across all metrics. This can be due to the quality of the Wish-QA-NQ\nbeing favorable or that a signal document generation setup is slightly preferable.\nThe performance on CNN-DailyMail, presented in Table 4, shows that Wish-summarization data\nimproves upon the strong Flan-xl baseline, in Bert-Score and Reward score but not on ROUGE-L.\nOverall, the dataset seems comparable, attesting to the flexibility of the method.\n5.1\nFAITHFULNESS RESULTS\nGenerally, we find (Table 3) that training on our synthetic data leads to more content-faithful models.\nModels trained on Wish-QA-NQ and Synthetic in-domain data, and Wish-QA were more faithful\nthan models trained on both ASQA and ELI5 data by the k-Precision, and ANLI metrics. This result\nis aligned with the finding of Krishna et al. (2021) that find that LFQA models generated answers\nthat are not grounded in the retrieved documents, and assert that this is one of the hurdles for filed\nprogress.\nFlan-xl achieves the highest Faithfulness scores followed by the synthetic datasets. Flan\u2019s achieve-\nment can be the result of its shorter and almost extractive answers. Taking into account that it is\nalso substantially underperforming, we deduce that the synthetic datasets achieve the best trade-off\nacross performance and faithfulness.\nThe faithfulness results for CNN-DailyMail are consistently high. As we observed, Flan-xl tends\nto produce more extractive responses. Since CNN-DailyMail primarily contains extractive summa-\nrization, it\u2019s no surprise that it exhibits high faithfulness scores. However, the model trained on our\ndata, which doesn\u2019t emphasize extractiveness as a hard requirement, outperforms Flan-xl in terms\nof k-Precision, matches it in terms of NLI, and achieves the highest average level of faithfulness.\nIn summary, our quantitative analysis affirms that the utilization of synthetic data substantially en-\nhances answer quality in both ASQA and ELI5 datasets. Our approach not only matches human-\ngenerated responses but also quantitatively surpasses them in terms of reward, highlighting its po-\ntential for generating higher-quality answers. Additionally, our method ensures high faithfulness\nand grounding in the generated responses, setting it apart from existing datasets.\n6\nDOMAIN ADAPTATION\nWe have demonstrated that our method can generate synthetic data that is as good as human-\ngenerated data. Next, we raise the hypothesis that given a task in a target domain it may be more\n7\nPublished as a conference paper at ICLR 2024\nTable 2: Performance comparison of Flan-xl models trained on human-generated and synthetic data.\nThe results reveal that our synthetic data consistently outperforms or achieves comparable perfor-\nmance to human-generated data, as indicated by ROUGE-L and Bert-Score metrics. Additionally,\nby reward score, models trained on our synthetic data exhibit superior or comparable performance\nto the gold standard responses.\nTest-set\nASQA\nELI5\nTrain Set\nROUGE-L\nBert-Score\nReward\nROUGE-L\nBert-Score F1\nReward\nFlan-xl\n10.5\n49.7\n28.8\n6.2\n46.7\n9.2\nASQA\n31.4\n66.0\n68.6\n13.5\n52.2\n24.4\nELI5\n18.7\n58.7\n37.2\n13.1\n51.3\n11.3\nWiFS\n28.0\n67.5\n85.1\n13.8\n55.2\n26.7\nWish-QA-NQ\n28.2\n64.8\n80.3\n13.2\n54.0\n30.3\nWish-QA in-domain\n27.0\n63.4\n73.3\n13.1\n52.8\n22.7\nGold\n-\n-\n72.1\n-\n-\n30.3\nTable 3: Fiathfullnes performance Comparison of Flan-xl Models Trained on Human-Created and\nSynthetic Data.\nThe results demonstrate that our synthetic data consistently outperforms both\nhuman-generated data and gold responses, as indicated by the k-Precision, and ANLI metrics. Flan-\nxl stands out with the highest scores, which can be attributed to the extractive nature of its responses.\nTest-set\nASQA\nELI5\nTrain Set\nk-Precision\nANLI\nk-Precision\nANLI\nFlan-xl\n98.2\n88.7\n89.2\n84.9\nASQA\n67.5\n55.7\n52.2\n34.3\nELI5\n52.9\n33.5\n29.0\n5.6\nWiFS\n77.9\n74.9\n58.5\n37.9\nWish-QA-NQ\n79.3\n75.5\n60.4\n43.3\nWish in-domain\n81.9\n79.1\n68.3\n52.8\nGold\n46.3\n25.3\n20.6\n2.7\neffective to generate synthetic data directly in the target domain than generating it, for the same\ntask, from another domain. To investigate this hypothesis, we define our test set as PubMed-QA,\nwhich specifically involves the task of LFQA in the medical domain. Accordingly, we create syn-\nthetic question-answering data on papers from PubMed (Wish-QA-MED) as task data in the target\ndomain. We then compare the performance of models trained on Wish-QA-MED dataset with those\ntrained on Wish-QA-NQ data, as well as with models trained on the human-created ELI5 and ASQA\ndatasets.\nThe results in Table 5 demonstrate that the synthetic dataset outperforms ELI5 and is comparable to\nor slightly better than ASQA in ROUGE-L and Bert-Score. Additionally, there is a more substantial\ngap in terms of reward and faithfulness.\nInterestingly, Wish-QA-NQ and Wish-QA-MED achieve similar results, echoing the finding that\nWish-QA outperforms other datasets. This suggests that out-of-domain data holds little disadvantage\nTable 4: Performance Comparison of Flan-xl Models Trained on Human-Created and Wish-\nSummarization Data. The results reveal that our synthetic data achieves comparable performance to\nhuman-generated data.\nTest-set\nCNN-DailyMail\nTrain-set\nROUGE-L\nBert-Score\nReward\nk-Precision\nANLI\nFlan-xl\n30.2\n70.9\n96.3\n97.6\n98.7\nCNN-DailyMail\n33.3\n72.7\n96.5\n97.0\n99.1\nWish-Summarization\n28.6\n71.3\n97.5\n98.2\n98.7\n8\nPublished as a conference paper at ICLR 2024\nTable 5: Performance of Flan-xl Models on PubMed test data. The results reveal that our syn-\nthetic data consistently outperforms or achieves comparable performance to human-generated data\nin general and faithfulness metrics. Results suggest that in-domain data don\u2019t provide additional\nimprovement for content-grounded generation, but may help the faithfulness of the model.\nPubMed\nTrain-set\nROUGE-L\nBert-Score\nReward\nK-Precision\nANLI\nFlan-xl\n12.8\n53.8\n10.7\n60.6\n38.2\nASQA\n20.5\n61.4\n37.3\n77.2\n60.8\nELI5\n15.0\n56.3\n16.8\n32.2\n2.2\nWish-QA-MED\n22.1\n61.6\n39.4\n78.2\n81.8\nWish-QA-NQ\n22.0\n62.9\n44.5\n84.2\n73.1\nover in-domain data and can often surpass it. One explanation may be that providing the content\nwith the task (e.g. QA) makes the model rely less on the training domain. Supportive evidence\nis the finding of Onoe et al. (2023), who found that, in their task, update strategies lag behind the\nperformance of simply concatenating the content to the prompt. This may mean that the model relies\non the content more than was previously thought (Neeman et al., 2023).\nThe faithfulness scores are inconclusive, while ANLI indicates that in-domain synthetic improves\nfaithfulness, the k-Precision says otherwise, suggesting at least parity.\nWe conclude that Genie can be beneficial in creating human-level data for many tasks and domains,\nhowever, it can be that LFQA is flexible in terms of its training data domain. We leave for future\nresearch to check this finding and to show tasks or dimensions that exhibit improvement due to target\ndomain data and can benefit from our method.\n7\nRELATED WORK\nOur work is far from the first to propose synthetic data for training or experimentation (Choshen\n& Abend, 2019; Agarwal et al., 2020). Recently, generating data from a large language model to\ntrain a smaller one was suggested as a weak form of distillation to improve smaller models (West\net al., 2022). Our method does not focus on distillation. Apart from using a stronger model for the\nsynthetic data, those methods differ from ours as the learned model mimics a diverse set of skills,\nrather than becoming an expert on a task.\nStill, there are a few synthetic methods for specific tasks. Most notably, methods that rely on a\n2-step process, generation, and filtering. West et al. (2022) presented a 2-step pipeline for Symbolic\nKnowledge Distillation, rather than for creating content-grounded data. Kim et al. (2022) apply this\nmethod to create a social dialogue dataset. In Unnatural Instructions and Self-Instruct (Honovich\net al., 2022b; Wang et al., 2022), they applied this method for the creation of an instruction dataset.\nTheir method relies on model knowledge for content-grounded tasks. Similarly, Bitton et al. (2023)\nq2d approach uses a 2-step process for creating information-seeking dialogs. Those works share\nsimilar mechanisms with our method but differ in the content-grounded aspect of our work.\nThe dialog inpainting approach (Dai et al., 2022), shares a common objective with ours, to generate\ncontent-grounded question answering. They add questions between the document sentences to cre-\nate a dialogue. This approach ensures the groundedness of the dialogue but it comes at the cost of\nless fluent and neutral conversation. In our approach, we generate the question and answer using the\nLLM and verify its groundedness and quality to allow both faithfulness and naturalness.\n8\nDISCUSSION\nOur work introduces Genie, an efficient and cost-effective automated approach for curating content-\ngrounded datasets. Our method incorporates a novel filtering mechanism to ensure data quality. We\ndemonstrate that our synthetic wish-QA and wish-summarization data achieves parity with expert\nhuman datasets in both intrinsic and extrinsic evaluations. Furthermore, we illustrate that our data\n9\nPublished as a conference paper at ICLR 2024\nsurpasses human-written datasets in terms of lexical diversity and faithfulness. We have also proven\nthe applicability of our method to noisy crawled data.\nWe want to emphasize the immense potential this approach holds for facilitating the development\nof content-focused datasets and, consequently, generative models, minimizing the need for costly\nhuman annotation. Therefore, our method democratizes the creation of such datasets and models,\nmaking them more accessible to the entire community.\nREFERENCES\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Eval-\nuating correctness and faithfulness of instruction-following models for question answering. arXiv\npreprint arXiv:2307.16877, 2023.\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based syn-\nthetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint\narXiv:2010.12688, 2020.\nYonatan Bitton, Shlomi Cohen-Ganor, Ido Hakimi, Yoad Lewenberg, Roee Aharoni, and Enav\nWeinreb. q2d: Turning questions into dialogs to teach models how to search. arXiv preprint\narXiv:2304.14318, 2023.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-\ntated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.\nLeshem Choshen and Omri Abend.\nAutomatically extracting challenge sets for non-local phe-\nnomena in neural machine translation. Conference on Computational Natural Language Learn-\ning (CoNLL), pp. 291\u2013303, November 2019.\ndoi: 10.18653/v1/K19-1028.\nURL https:\n//aclanthology.org/K19-1028.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\nThe pascal recognising textual entailment\nchallenge. In Machine learning challenges workshop, pp. 177\u2013190. Springer, 2005.\nZhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao, Aida Amini, Qazi Mamunur Rashid, Mike\nGreen, and Kelvin Guu. Dialog inpainting: Turning documents into dialogs. In International\nConference on Machine Learning, pp. 4558\u20134586. PMLR, 2022.\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. Evaluating attribution in dialogue\nsystems: The begin benchmark. Transactions of the Association for Computational Linguistics,\n10:1066\u20131083, 2022.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:\nLong form question answering. arXiv preprint arXiv:1907.09190, 2019.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style\npre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543,\n2021.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural\ninformation processing systems, 28, 2015.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend.\nq2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and\nquestion answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 7856\u20137870, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.619. URL\nhttps://aclanthology.org/2021.emnlp-main.619.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen,\nThomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating\nfactual consistency evaluation. arXiv preprint arXiv:2204.04991, 2022a.\n10\nPublished as a conference paper at ICLR 2024\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022b.\nJungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander\nFabbri, Yejin Choi, and Noah A. Smith. Bidimensional leaderboards: Generate and evaluate\nlanguage hand in hand. In Proceedings of the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pp. 3540\u20133557,\nSeattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.naacl-main.259. URL https://aclanthology.org/2022.naacl-main.259.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,\nMalihe Alikhani, Gunhee Kim, Maarten Sap, et al. Soda: Million-scale dialogue distillation with\nsocial commonsense contextualization. arXiv preprint arXiv:2212.10465, 2022.\nAndreas K\u00a8opf, Yannic Kilcher, Dimitri von R\u00a8utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00b4ard Nagyfi, et al. Openassistant\nconversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327,\n2023.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answer-\ning. In Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pp. 4940\u20134957, Online, June\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.393. URL\nhttps://aclanthology.org/2021.naacl-main.393.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics, 7:453\u2013466, 2019.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\nin abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 1906\u20131919, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.173. URL https://aclanthology.org/\n2020.acl-main.173.\nPhilip M McCarthy and Scott Jarvis. Mtld, vocd-d, and hd-d: A validation study of sophisticated\napproaches to lexical diversity assessment. Behavior research methods, 42(2):381\u2013392, 2010.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering\nambiguous open-domain questions. arXiv preprint arXiv:2004.10645, 2020.\nElla Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, and Omri Abend. Dis-\nentQA: Disentangling parametric and contextual knowledge with counterfactual question answer-\ning. Annual Meeting of the Association for Computational Linguistics, pp. 10056\u201310070, July\n2023. doi: 10.18653/v1/2023.acl-long.559. URL https://aclanthology.org/2023.\nacl-long.559.\nYasumasa Onoe, Michael JQ Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. Can\nlms learn new entities from descriptions? challenges in propagating injected knowledge. arXiv\npreprint arXiv:2305.01651, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\n11\nPublished as a conference paper at ICLR 2024\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim,\nMichal Shmueli-Scheuer, and Leshem Choshen. Efficient benchmarking (of language models).\narXiv preprint arXiv:2308.11696, 2023.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark for knowl-\nedge intensive language tasks. arXiv preprint arXiv:2009.02252, 2020.\nMatt Post.\nA call for clarity in reporting BLEU scores.\nIn Proceedings of the Third Con-\nference on Machine Translation: Research Papers, pp. 186\u2013191, Brussels, Belgium, Octo-\nber 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/W18-6319.\nURL\nhttps://aclanthology.org/W18-6319.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet\nlong-form answers. arXiv preprint arXiv:2204.06092, 2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Xim-\ning Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language\nmodels to commonsense models. In Proceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, pp. 4602\u20134625, Seattle, United States, July 2022. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2022.naacl-main.341. URL https://aclanthology.org/2022.\nnaacl-main.341.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.\nBertscore:\nEvaluating text generation with bert. ArXiv, abs/1904.09675, 2019a. URL https://api.\nsemanticscholar.org/CorpusID:127986044.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-\ning text generation with bert. arXiv preprint arXiv:1904.09675, 2019b.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. Lmsys-chat-1m: A large-scale real-world llm\nconversation dataset. arXiv preprint arXiv:2309.11998, 2023.\nChenguang Zhu, Ziyi Yang, Robert Gmyr, Michael Zeng, and Xuedong Huang. Make lead bias in\nyour favor: Zero-shot abstractive news summarization. In International Conference on Learning\nRepresentations, 2020.\nA\nDATASETS STATISTICS\nWhile the amount of synthetic data is capped merely by the actual amount produced and the con-\ntent available, meaning the others can generate more data, we supply the actual sizes we release.\nStatistics are shown in Table 6\n12\nPublished as a conference paper at ICLR 2024\nTable 6: Data statistic for several datasets including the size of the data, the number of words in the\nresponse, and the average lexical diversity. Overall, our dataset spans 300K+ samples, similar to the\nsize of ELI5, and CNN-DailyMail which were collected from available resources and are noisy by\nnature. On the other hand, our data is 50 times larger than the carefully annotated ASQA data. We\nalso can see that most response lengths are similar to the ones in the human writing datasets, and the\nlexical diversity of all synthetic data is higher than their human writing counterparts.\n# samples\n# words in response\nlexical diversity\nELI5\n272K\n107.0\n70.9\nASQA\n6,316\n73.3\n65.8\nCNN-DailyMail\n311K\n44.8\n40.1\nWish\n27K\n65.1\n54.3\nWish-QA-NQ\n88K\n73.4\n62.3\nWish-QA-ELI5\n78K\n98.3\n71.8\nWish-QA-ASQA\n6140\n87.5\n67.0\nWish-Summarization\n37K\n66.6\n76.0\nWish-QA-Med\n69K\n84.2\n71.8\nTable 7: Human assessment for WiFS without and with filtering, and Synthetic NQ. Results show\na big improvement in question relevance and the percentage of answers that address the question,\nanswers that are faithful, and overall answer scores.\nQuality Review Question\nWiFS w/o filters\nWiFS w/ filters\nSynthetic NQ\nIs the question relevant to the content?\n67%\n92%\n98%\nIs the question clear? (not ambiguous)\n63%\n67%\n91%\nDoes the answer address the question?\n80%\n98%\n98%\nIs the answer faithful to the content?\n53%\n76%\n88%\nGrad the overall quality of the answer\n3.48\n4.58\n4.74\nB\nCOMPLEMENTARY RESULTS.\nWe provide additional human experiments (Table 7) demonstrating that using clean content from\nexciting datasets can further improve the synthetic data quality.\nWe also provide the results of synthetic datasets created with the stronger Lamma model in Table 8\nand on PubMed on Table 9.\nWe also include here results with Llama-2-13B-Chat as the base model. General results are pre-\nsented in Table 11 and Faithfulness results are in Table 10.\nTable 8: Results on ASQA and ELI5 including models trained with Llama-2-70B-chat synthetic\ndata.\nTest-set\nASQA\nELI5\nTrain Set\nROUGE-L\nBert-Score F1\nReward\nROUGE-L\nBert-Score F1\nReward\nFlan-xl\n10.5\n49.7\n28.8\n6.2\n46.7\n9.2\nHuman in-domain\n31.4\n66.0\n68.6\n13.1\n51.3\n11.3\nHuman out of domain\n18.7\n58.7\n37.2\n13.5\n52.2\n24.4\nWish-QA\n28.0\n67.5\n85.1\n13.8\n55.2\n26.7\nWish-QA-NQ Falcon\n28.2\n64.8\n80.3\n13.2\n54.0\n30.3\nWish-QA-NQ Llama\n28.1\n64.8\n81.7\n13.6\n53.9\n44.1\nWish-QA in-domain Falcon\n27.0\n63.4\n73.3\n13.1\n52.8\n22.7\nWish-QA in-domain Llama\n28.0\n64.6\n76.4\n13.7\n53.6\n30.5\nGold\n-\n-\n72.1\n-\n-\n30.3\n13\nPublished as a conference paper at ICLR 2024\nTable 9: Full results on PubMed dataset. Here we include results where Llama-2-70B is the data\ngenerator.\nPubMed\nTrain-set\nROUGE-L\nBert-Score\nReward\nk-Precision\nANLI\nFlan-xl\n12.8\n53.8\n10.7\n60.6\n38.2\nASQA\n20.5\n61.4\n37.3\n77.2\n60.8\nELI5\n15.0\n56.3\n16.8\n32.2\n2.2\nWish-QA-Med Falcon\n22.1\n61.6\n39.4\n78.2\n81.8\nWish-QA-NQ Falcon\n22.0\n62.9\n44.5\n84.2\n73.1\nWish-QA-Med Llama\n21.4\n62.1\n30.0\n74.6\n92.4\nWish-QA-NQ Llama\n21.4\n62.5\n51.2\n80.6\n83.0\nTable 10: Faithfulness results on ASQA and ELI5 when Llama-2-13b-Chat is the base model.\nTest-set\nASQA\nELI5\nTrain-set\nk-Precision\nANLI\nk-Precision\nANLI\nLlama-13b-chat\n40.4\n65.5\n23.3\n24.6\nASQA\n77.3\n73.1\n48.0\n37.3\nELI5\n59.0\n42.6\n33.3\n13.8\nWish-QA-NQ\n81.1\n76.2\n58.2\n45.1\nWish-QA-ASQA\n79.2\n73.1\n51.8\n40.3\nWish-QA-ELI5\n77.6\n75.9\n52.0\n46.1\nTable 11: Results on ASQA and ELI5 when Llama-2-13b-Chat is the base model.\nTest-set\nASQA\nELI5\nTrain-set\nRougeL\nBert-Score\nReward\nRougeL\nBert-Score\nReward\nLlama-13b-chat\n21.4\n57.6\n62.7\n11.8\n51.2\n46\nASQA\n29.0\n65.2\n91.1\n12.4\n53.5\n66.9\nELI5\n13.4\n55.0\n86.4\n11.4\n52.8\n66.7\nSynthetic NQ\n26.3\n64.4\n86.9\n12.5\n53.5\n52.6\nSynthetic ASQA\n26.3\n63.6\n87.1\n12.3\n53.1\n57.7\nSynthetic ELI5\n26.3\n63.6\n89.2\n12.4\n53.0\n57.1\n14\nPublished as a conference paper at ICLR 2024\nTable 12: Your Table Caption\nModel\nROUGE-L\nBert-Score\nReward\nK-Precision\nANLI\nFlan-xl\n42.3\n67.3\n35.9\n39.1\n89.1\nDatabricks\n54.6\n77.2\n70.1\n56.7\n89.9\nWish-IE\n47.9\n74.5\n87.4\n60.0\n86.8\nC\nINFORMATION EXTRACTION RESULTS.\nTo test the efficacy of the method in another task, we conducted an experiment focusing on informa-\ntion extraction (IE). For this experiment, we utilized the information extraction part of the Databricks\ndataset , which we divided into train and test sets (1000/50). Synthetic information extraction data\nwas generated over Google-NQ passages, requiring only a new set of few-shot examples to demon-\nstrate the task, without any other modifications. The table below 12 shows that our model improved\nin all metrics compared to the Flan-xl baseline, except for ANLI. Furthermore, it achieves higher\nscores than the model trained on human data in Reward and K-Precision, while slightly lagging\nbehind in ROUGE-L, Bert-Score, and ANLI. The average of the faithfulness metrics indicates com-\nparable performances in this dimension for the models trained on human and synthetic data. These\nexperiments reinforce our belief that our method is general and can be readily applied to various\ntasks, yielding improved results through the generation of high-quality synthetic data.\nD\nPROMPT\nWe provide examples of the prompts used to generate data in Fig.2.\n15\nPublished as a conference paper at ICLR 2024\nFigure 2: An illustration of our data generation prompt. In black is the few-shot prompt we give the\nmodel. In pink a new QA that the model generated based on the the provided content.\nInstruction:  Given the next [document], create a [question] and [answer] pair that are \ngrounded in the main point of the document, don't add any additional information that is \nnot in the document. The [question] is by an information-seeking user and the [answer] is \nprovided by a helping AI Agent.\n[document]: Scrumptious Sweet Co. factory ..\n### Response:\n[question]: Where was the movie Chitty Chitty Bang Bang filmed?\n[answer]: Chitty Chitty Bang Bang was filmed in a variety of locations in England and \nGermany. Some ..\n[document]: The series stars Eugene Levy as Johnny Rose \u2026\n### Response:\n[question]: What is the plot of the show Schitt's Creek?\n[answer]: The show Schitt's Creek is about a wealthy family who loses their fortune and is \nforced to rebuild their lives in a small town. The show follows the family as they adjust to \ntheir new life in the town and learn to appreciate the simple things in life.\n[document]: 2016's countdown broke several Hottest 100 records\u2026\n### Response:\n[question]: What was the most popular song on the 2016 Hottest 100?\n[answer]: The most popular song on the 2016 Hottest 100 was \"Never Be Like You\" by \nFlume. This was the first time that an electronic dance music producer topped the \ncountdown.\n[document]: In Greek mythology, Persephone ..\n### Response:\n[question]: Who is Persephone in Greek mythology and what is her significance?\n[answer]: In Greek mythology, Persephone, also known as Kore, is the daughter of Zeus and \nDemeter. She ..\n[document]: Terry, the dog that played Toto in the 1939 screen \u2026\n### Response:\n[question]: What breed of dog was Toto in the Wizard of Oz?\n[answer]: Terry, the dog that played Toto in the 1939 screen adaptation of The Wizard of Oz, \nwas a brindle Cairn terrier.\n16\n"
  }
]