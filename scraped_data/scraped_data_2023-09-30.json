[
  {
    "title": "Vision Transformers Need Registers",
    "link": "https://arxiv.org/pdf/2309.16588.pdf",
    "upvote": "69",
    "text": "VISION TRANSFORMERS NEED REGISTERS\nTimoth\u00b4ee Darcet1,2, Maxime Oquab1, Julien Mairal2 & Piotr Bojanowski1\n1 FAIR, Meta\n2 INRIA\n{timdarcet,qas,bojanowski}@meta.com\njulien.mairal@inria.fr\nABSTRACT\nTransformers have recently emerged as a powerful tool for learning visual rep-\nresentations. In this paper, we identify and characterize artifacts in feature maps\nof both supervised and self-supervised ViT networks. The artifacts correspond to\nhigh-norm tokens appearing during inference primarily in low-informative back-\nground areas of images, that are repurposed for internal computations. We propose\na simple yet effective solution based on providing additional tokens to the input se-\nquence of the Vision Transformer to fill that role. We show that this solution fixes\nthat problem entirely for both supervised and self-supervised models, sets a new\nstate of the art for self-supervised visual models on dense visual prediction tasks,\nenables object discovery methods with larger models, and most importantly leads\nto smoother feature maps and attention maps for downstream visual processing.\nWithout registers\nWith registers\nInput\nDeiT-III\nCLIP\nDINOv2\nDeiT-III\nCLIP\nDINOv2\nFigure 1: Register tokens enable interpretable attention maps in all vision transformers, similar to\nthe original DINO method (Caron et al., 2021). Attention maps are calculated in high resolution for\nbetter visualisation. More qualitative results are available in appendix D.\n1\nINTRODUCTION\nEmbedding images into generic features that can serve multiple purposes in computer vision has\nbeen a long-standing problem. First methods relied on handcrafted principles, such as SIFT (Lowe,\n2004), before the scale of data and deep learning techniques allowed for end-to-end training. Pur-\nsuing generic feature embeddings is still relevant today, as collecting valuable annotated data for\nmany specific tasks remains difficult. This difficulty arises because of the required expertise (e.g.,\nmedical data, or remote sensing) or the cost at scale. Today, it is common to pretrain a model for\na task for which plenty of data is available and extract a subset of the model to use as a feature\nextractor. Multiple approaches offer this possibility; supervised methods, building on classification\n1\narXiv:2309.16588v1  [cs.CV]  28 Sep 2023\nInput\nDeiT-III-B\nDeiT-III-L\nOpenCLIP-B\nOpenCLIP-L\nDINO-B\nDINOv2-g\nFigure 2: Illustration of artifacts observed in the attention maps of modern vision transformers.\nWe consider ViTs trained with label supervision (DeiT-III), text-supervision (OpenCLIP) or self-\nsupervision (DINO and DINOv2). Interestingly, all models but DINO exhibit peaky outlier values\nin the attention maps. The goal of this work is to understand and mitigate this phenomenon.\nor text-image alignment, allow training strong feature models to unlock downstream tasks. Alterna-\ntively, self-supervised methods building on the Transformer architecture have attracted significant\nattention due to their high prediction performance on downstream tasks and the intriguing ability of\nsome models to provide unsupervised segmentations (Caron et al., 2021)\nIn particular, the DINO algorithm is shown to produce models that contain explicit information about\nthe semantic layout of an image. Indeed, qualitative results show that the last attention layer naturally\nfocuses on semantically consistent parts of images and often produces interpretable attention maps.\nExploiting these properties, object discovery algorithms such as LOST (Sim\u00b4eoni et al., 2021) build\non top of DINO. Such algorithms can detect objects without supervision by gathering information\nin attention maps. They are effectively unlocking a new frontier in computer vision.\nDINOv2 (Oquab et al., 2023), a follow-up to DINO, provides features that allow tackling dense\nprediction tasks. DINOv2 features lead to successful monocular depth estimation and semantic seg-\nmentation with a frozen backbone and linear models. Despite the strong performance on dense tasks,\nwe observed that DINOv2 is surprisingly incompatible with LOST. When used to extract features, it\ndelivers disappointing performance, only on par with supervised alternative backbones in this sce-\nnario. This suggests that DINOv2 behaves differently than DINO. The investigation described in\nthis work notably exposes the presence of artefacts in the feature maps of DINOv2 that were not\npresent in the first version of this model. These are observable qualitatively using straightforward\nmethods. Also surprisingly, applying the same observations to supervised vision transformers ex-\nposes similar artifacts, as shown in Fig. 2. This suggests that DINO is, in fact, an exception, while\nDINOv2 models match the baseline behavior of vision transformers.\nIn this work, we set out to better understand this phenomenon and develop methods to detect these\nartifacts. We observe that they are tokens with roughly 10x higher norm at the output and correspond\nto a small fraction of the total sequence (around 2%). We also show that these tokens appear around\nthe middle layers of the vision transformer, and that they only appear after a sufficiently long training\nof a sufficiently big transformer. In particular, we show that these outlier tokens appear in patches\nsimilar to their neighbors, meaning patches that convey little additional information.\nAs part of our investigation, we evaluate the outlier tokens with simple linear models to under-\nstand the information they contain. We observe that, compared to non-outlier tokens, they hold less\ninformation about their original position in the image or the original pixels in their patch. This ob-\n2\ninput image\nDINO norms\nDINOv2 norms\n0\n20\n40\n60\n80\n100\n0\n200\n400\n600\nL2 norm\n10 5\n10 3\n10 1\nDINO norms\n0\n200\n400\n600\nL2 norm\n10 5\n10 3\n10 1\nDINOv2 norms\nFigure 3: Comparison of local feature norms for DINO ViT-B/16 and DINOv2 ViT-g/14. We ob-\nserve that DINOv2 has a few outlier patches, whereas DINO does not present these artifacts. For\nDINOv2, although most patch tokens have a norm between 0 and 100, a small proportion of tokens\nhave a very high norm. We measure the proportion of tokens with norm larger than 150 at 2.37%.\nservation suggests that the model discards the local information contained in these patches during\ninference. On the other hand, learning an image classifier on outlier patches yields significantly\nstronger accuracy than doing so on the other patches, suggesting that they contain global informa-\ntion about the image. We propose the following interpretation to these elements: the model learns\nto recognize patches containing little useful information, and recycle the corresponding tokens to\naggregate global image information while discarding spatial information.\nThis interpretation is consistent with an inner mechanism in transformer models that allows per-\nforming computations within a restricted set of tokens. In order to test this hypothesis, we append\nadditional tokens - that we call registers - to the token sequence, independent of the input image. We\ntrain several models with and without this modification and observe that the outlier tokens disappear\nfrom the sequence entirely. As a result, the performance of the models increases in dense prediction\ntasks, and the resulting feature maps are significantly smoother. These smooth feature maps enable\nobject discovery methods like LOST mentioned above with the updated models.\n2\nPROBLEM FORMULATION\nAs shown in Fig. 2, most modern vision transformers exhibit artifacts in the attention maps. The\nunsupervised DINO backbone (Caron et al., 2021) has been previously praised for the quality of\nlocal features and interpretability of attention maps. Surprisingly, the outputs of the subsequent\nDINOv2 models have been shown to hold good local information but exhibit undesirable artifacts in\nattention maps. In this section, we propose to study why and when these artifacts appear. While this\nwork focuses on alleviating artefacts in all vision transformers, we focus our analysis on DINOv2.\n2.1\nARTIFACTS IN THE LOCAL FEATURES OF DINOV2\nArtifacts are high-norm outlier tokens.\nWe want to find a quantitative way of characterizing\nartefacts that appear in the local features. We observe that an important difference between \u201cartifact\u201d\npatches and other patches is the norm of their token embedding at the output of the model. In Fig. 3\n(left), we compare the norm of local features for a DINO and DINOv2 model given a reference\nimage. We clearly see that the norm of artifact patches is much higher than the norm of other\npatches. We also plot the distribution of feature norms over a small dataset of images in Fig. 3\n(right), which is clearly bimodal, allowing us to choose a simple criterion for the rest of this section:\ntokens with norm higher than 150 will be considered as \u201chigh-norm\u201d tokens, and we will study their\nproperties relative to regular tokens. This hand-picked cutoff value can vary across models. In the\nrest of this work, we use \u201chigh-norm\u201d and \u201coutlier\u201d interchangeably.\nOutliers appear during the training of large models.\nWe make several additional observations\nabout the conditions in which these outlier patches appear during the training of DINOv2. This\nanalysis is illustrated in Fig. 4. First, these high-norm patches seem to differentiate themselves from\nother patches around layer 15 of this 40-layer ViT (Fig. 4a). Second, when looking at the distribution\nof norms along training of DINOv2, we see that these outliers only appear after one third of training\n(Fig. 4b). Finally, when analyzing more closely models of different size (Tiny, Small, Base, Large,\nHuge and giant), we see that only the three largest models exhibit outliers (Fig. 4c).\n3\n(a) Norms along layers.\n(b) Norms along iterations.\n(c) Norms across model size.\nFigure 4: Illustration of several properties of outlier tokens in the 40-layer DINOv2 ViT-g model.\n(a): Distribution of output token norms along layers. (b): Distribution of norms along training\niterations. (c): Distribution of norms for different model sizes. The outliers appear around the\nmiddle of the model during training; they appear with models larger than and including ViT-Large.\n(a) Cosine similarity to neighbors.\nposition prediction\nreconstruction\ntop-1 acc\navg. distance \u2193\nL2 error \u2193\nnormal\n41.7\n0.79\n18.38\noutlier\n22.8\n5.09\n25.23\n(b) Linear probing for local information.\nFigure 5: (a): Distribution of cosine similarity between input patches and their 4 neighbors. We\nplot separately artifact patches (norm of the output token over 150) and normal patches. (b): Local\ninformation probing on normal and outlier patch tokens. We train two models: one for predicting\nposition, and one for reconstructing the input patch. Outlier tokens have much lower scores than the\nother tokens, suggesting they are storing less local patch information.\nHigh-norm tokens appear where patch information is redundant.\nTo verify this, we measure\nthe cosine similarity between high-norm tokens and their 4 neighbors right after the patch em-\nbedding layer (at the beginning of the vision transformer). We illustrate the density plot in Fig.\n5a. We observe that high-norm tokens appear on patches that are very similar to their neighbors.\nThis suggests that these patches contrain redundant information and that the model could discard\ntheir information without hurting the quality of the image representation. This matches qualitative\nobservations (see Fig. 2) that they often appear in uniform, background areas.\nHigh-norm tokens hold little local information.\nIn order to better understand the nature of these\ntokens, we propose to probe the patch embeddings for different types of information. For that we\nconsider two different tasks: position prediction and pixel reconstruction. For each of these tasks,\nwe train a linear model on top of the patch embeddings, and measure the performance of this\nmodel. We compare the performance achieved with high-norm tokens and with other tokens, to see\nif high-norm tokens contain different information than \u201cnormal\u201d tokens.\n\u2022 Position prediction. We train a linear model to predict the position of each patch token in\nthe image, and measure its accuracy. We note that this position information was injected\nin the tokens before the first ViT layer in the form of absolute position embeddings. We\nobserve that high-norm tokens have much lower accuracy than the other tokens (Fig. 5b),\nsuggesting they contain less information about their position in the image.\n\u2022 Pixel reconstruction. We train a linear model to predict the pixel values of the image from\nthe patch embeddings, and measure the accuracy of this model. We observe again that\nhigh-norm tokens achieve much lower accuracy than other tokens (Fig. 5b). This suggests\nthat high-norm tokens contain less information to reconstruct the image than the others.\nArtifacts hold global information.\nIn order to evaluate how much global information is gathered\nin the high-norm tokens, we propose to evaluate them on standard image representation learning\n4\nIN1k P205 Airc. CF10 CF100 CUB Cal101 Cars DTD Flow. Food Pets SUN VOC\n[CLS]\n86.0 66.4 87.3 99.4\n94.5\n91.3\n96.9\n91.5 85.2 99.7 94.7 96.9 78.6 89.1\nnormal\n65.8 53.1 17.1 97.1\n81.3\n18.6\n73.2\n10.8 63.1 59.5 74.2 47.8 37.7 70.8\noutlier\n69.0 55.1 79.1 99.3\n93.7\n84.9\n97.6\n85.2 84.9 99.6 93.5 94.1 78.5 89.7\nTable 1: Image classification via linear probing on normal and outlier patch tokens. We also report\nthe accuracy of classifiers learnt on the class token. We see that outlier tokens have a much higher\naccuracy than regular ones, suggesting they are effectively storing global image information.\n[CLS]\n[REG1]\n[REG2]\n\u2026\n[REGN]\nTransformer Model\noutput\ninput patches\nFigure 6: Illustration of the proposed remediation and resulting model. We add N additional learn-\nable input tokens (depicted in yellow), that the model can use as registers. At the output of the\nmodel, only the patch tokens and CLS tokens are used, both during training and inference.\nbenchmarks. For each image in a classification dataset, we forward it through DINOv2-g and ex-\ntract the patch embeddings. From those, we choose a single token at random, either high-norm or\nnormal. This token is then considered as the image representation. We then train a logistic regres-\nsion classifier to predict the image class from this representation, and measure the accuracy.\nWe\nobserve that the high-norm tokens have a much higher accuracy than the other tokens (Table 1). This\nsuggests that outlier tokens contain more global information than other patch tokens.\n2.2\nHYPOTHESIS AND REMEDIATION\nHaving made these observations, we make the following hypothesis: large, sufficiently trained mod-\nels learn to recognize redundant tokens, and to use them as places to store, process and retrieve\nglobal information. Furthermore, we posit that while this behavior is not bad in itself, the fact that\nit happens inside the patch tokens is undesirable. Indeed, it leads the model to discard local patch\ninformation (Tab. 5b), possibly incurring decreased performance on dense prediction tasks.\nWe therefore propose a simple fix to this issue: we explicitly add new tokens to the sequence, that\nthe model can learn to use as registers. We add these tokens after the patch embedding layer, with a\nlearnable value, similarly to the [CLS] token. At the end of the vision transformer, these tokens are\ndiscarded, and the [CLS] token and patch tokens are used as image representations, as usual. This\nmechanism was first proposed in Memory Transformers (Burtsev et al., 2020), improving translation\ntasks in NLP. Interestingly, we show here that this mechanism admits a natural justification for vision\ntransformers, fixing an interpretability and performance issue that was present otherwise.\nWe note that we have not been able to fully determine which aspects of the training led to the\nappearance of artifacts in DINOv2 but not in DINO, but Fig. 4 suggests that scaling the model size\nbeyond ViT-L, and longer training length may be possible causes.\n3\nEXPERIMENTS\nIn this section, we validate the proposed solution by training vision transformers with additional\n[reg] register tokens. We evaluate the effectiveness of our approach by a quantitative and quali-\ntative analysis. We then ablate the number of registers used for training, to check that they do not\n5\nFigure 7: Effect of register tokens on the distribution of output norms on DINOv2, CLIP and DeiT-\nIII. Using register tokens effectively removes the norm outliers that were present previously.\ncause a performance regression, evaluate an unsupervised object discovery method atop our features\nand finally provide a qualitative analysis of the patterns learnt by the registers.\n3.1\nTRAINING ALGORITHMS AND DATA\nAs the proposed solution is a simple architectural change, we can easily apply it to any training pro-\ncedure. We try it on three different state-of-the-art training methods for supervised, text-supervised,\nand unsupervised learning, shortly described below.\nDEIT-III (Touvron et al., 2022) is a simple and robust supervised training recipe for classification\nwith ViTs on ImageNet-1k and ImageNet-22k. We choose this method as an example of label-\nsupervised training as it is simple, uses the base ViT architecture, achieves strong classification\nresults, and is easy to reproduce and modify with our improvements. We run this method on the\nImageNet-22k dataset, using the ViT-B settings, as provided in the official repository 1.\nOpenCLIP (Ilharco et al., 2021) is a strong training method for producing text-image aligned mod-\nels, following the original CLIP work. We chose this method as an example of text-supervised\ntraining because it is open-source, uses the base ViT architecture, and is easy to reproduce and mod-\nify with our improvements. We run the OpenCLIP method on a text-image-aligned corpus based on\nShutterstock that includes only licensed image and text data. We use a ViT-B/16 image encoder, as\nproposed in the official repository 2.\nDINOv2 (Oquab et al., 2023) is a self-supervised method for learning visual features, following the\nDINO work mentioned previously. We apply our changes to this method as it is the main focus of\nour study. We run this method on ImageNet-22k with the ViT-L configuration. We use the code\nfrom the official repository 3.\n3.2\nEVALUATION OF THE PROPOSED SOLUTION\nAs shown in Fig. 1, we get rid of the artifacts by training models with additional register tokens.\nIn the appendix, we provide additional qualitative results for more images in Fig. 14. In order to\nquantitatively measure this effect, for each model, we probe the norm of features at the output of\nthe model. We report these norms for all three algorithms with and without registers in Fig. 7. We\nsee that when training with registers, models do not exhibit large-norm tokens at the output, which\nconfirms the initial qualitative assessment.\nPerformance regression. In the previous section, we have shown that the proposed approach re-\nmoves artifacts from local feature maps. In this experiment, we want to check that the use of register\ntokens does not affect the representation quality of those features. We run linear probing on Im-\nageNet classification, ADE20k Segmentation, and NYUd monocular depth estimation. We follow\nthe experimental protocol outlined in Oquab et al. (2023). We summarize the performance of the\nmodels described in Sec. 3.1 with and without register tokens in Table 2a. We see that when us-\ning registers, models do not lose performance and sometimes even work better. For completeness,\nwe also provided the zero-shot classification performance on ImageNet for OpenCLIP (Table 2b),\n1https://github.com/facebookresearch/deit\n2https://github.com/mlfoundations/open_clip\n3https://github.com/facebookresearch/dinov2\n6\nImageNet\nADE20k\nNYUd\nTop-1\nmIoU\nrmse \u2193\nDeiT-III\n84.7\n38.9\n0.511\nDeiT-III+reg\n84.7\n39.1\n0.512\nOpenCLIP\n78.2\n26.6\n0.702\nOpenCLIP+reg\n78.1\n26.7\n0.661\nDINOv2\n84.3\n46.6\n0.378\nDINOv2+reg\n84.8\n47.9\n0.366\n(a) Linear evaluation with frozen features.\nImageNet\nTop-1\nOpenCLIP\n59.9\nOpenCLIP+reg\n60.1\n(b) Zero-shot classification.\nTable 2: Evaluation of downstream performance of the models that we trained, with and without\nregisters. We consider linear probing of frozen features for all three models, and zero-shot evaluation\nfor the OpenCLIP model. We see that using register not only does not degrade performance, but even\nimproves it by a slight margin in some cases.\nInput\n0 [reg]\n1 [reg]\n2 [reg]\n4 [reg]\n8 [reg]\n16 [reg]\n0\n4\n8\n12\n16\nnumber of [reg] tokens\n84.4\n84.5\n84.6\n84.7\n84.8\ntop-1 acc\nImageNet\n0\n4\n8\n12\n16\nnumber of [reg] tokens\n66.0\n66.2\n66.4\n66.6\n66.8\nmIoU\nAverage of segmentation tasks\n0\n4\n8\n12\n16\nnumber of [reg] tokens\n2.73\n2.76\n2.79\n2.82\n2.85\nrmse\nAverage of depth tasks\nFigure 8: Ablation of the the number of register tokens used with a DINOv2 model. (top): qualita-\ntive visualization of artifacts appearing as a function of number of registers. (bottom): performance\non three tasks (ImageNet, ADE-20k and NYUd) as a function of number of registers used. While\none register is sufficient to remove artefacts, using more leads to improved downstream performance.\nwhich remains unchanged. Please note that the absolute performance of our OpenCLIP reproduction\nis lower due to the data source we used.\nNumber of register tokens. As described in Sec. 2.2, we propose alleviating the feature maps\u2019\nartifacts by adding register tokens. In this experiment, we study the influence of the number of such\ntokens on local features and downstream performance. We train DINOv2 ViT-L/14 models with 0, 1,\n2, 4, 8 or 16 registers. In Fig. 8, we report the results of this analysis. In Fig. 8(top), we qualitatively\nstudy the attention maps and observe that the visible artifacts disappear when adding at least one\nregister. We then examine in Fig. 8(bottom) performance on downstream evaluation benchmarks,\nfollowing the protocol from Oquab et al. (2023). There seems to be an optimal number of registers\nfor dense tasks, and adding one brings most of the benefit. This optimum is likely explained by\nthe disappearance of artifacts, leading to better local features. On ImageNet, however, performance\nimproves when using more registers. In all our experiments, we kept 4 register tokens.\n3.3\nOBJECT DISCOVERY\nRecent unsupervised object discovery methods rely on the quality and smoothness of local feature\nmaps (Sim\u00b4eoni et al., 2021; Wang et al., 2023). By leveraging DINO Caron et al. (2021), these\nmethods have significantly surpassed the previous state of the art. However, the algorithm leads\nto poor performance when applied to modern backbones such as DINOv2 Oquab et al. (2023) or\nsupervised ones Touvron et al. (2022). We posit that this can be alleviated by the method proposed\n7\nVOC 2007\nVOC 2012\nCOCO 20k\nDeiT-III\n11.7\n13.1\n10.7\nDeiT-III+reg\n27.1\n32.7\n25.1\nOpenCLIP\n38.8\n44.3\n31.0\nOpenCLIP+reg\n37.1\n42.0\n27.9\nDINOv2\n35.3\n40.2\n26.9\nDINOv2+reg\n55.4\n60.0\n42.0\nTable 3: Unsupervised Object Discovery using LOST (Sim\u00b4eoni et al., 2021) on models with and\nwithout registers. We evaluated three types of models trained with various amounts of supervision\non VOC 2007, 2012 and COCO. We measure performance using corloc. We observe that adding\nregister tokens makes all models significantly more viable for usage in object discovery.\nInput\n[CLS]\n[reg0]\n[reg6]\n[reg8]\n[reg12]\nFigure 9: Comparison of the attention maps of the [CLS] and register tokens. Register tokens\nsometimes attend to different parts of the feature map, in a way similar to slot attention (Locatello\net al., 2020). Note that this behaviour was never required from the model, and emerged naturally\nfrom training.\nin this work. We run LOST (Sim\u00b4eoni et al., 2021) on features extracted from backbones trained\nusing the algorithms described in Sec.3.1 with and without registers. We run object discovery on\nPASCAL VOC 2007 and 2012 and COCO 20k. We use values for DeiT and OpenCLIP, and for\nDINOv2, we use keys. Because the output features may have different conditionning, we manually\nadd a bias to the gram matrix of features. The results of this experiment are presented in Table 3.\nFor all models and on all datasets, adding registers for training improves the unsupervised object\ndiscovery performance. The performance of DINOv2 on VOC2007 still does not match that of\nDINO as reported in the work of Sim\u00b4eoni et al. (2021) (61.9 corloc). However, the model with\nregisters gets an improvement of 20.1 corloc (55.4 versus 35.3).\n3.4\nQUALITATIVE EVALUATION OF REGISTERS\nIn this final experiment, we qualitatively probe for the behavior of register tokens. We want to verify\nif they all exhibit similar attention patterns or whether a differentiation automatically emerges. To\nthis end, we plot the attention maps of the class and register tokens to patch tokens. The result of\nthis visualization is shown in Fig. 9. We see that registers do not have a completely aligned behavior.\nSome selected registers exhibit interesting attention patterns, attending to the different objects in the\nscene. While nothing enforced this behavior, their activations had some natural diversity. We leave\nthe study of the regularization of registers for future work.\n4\nRELATED WORK\n4.1\nFEATURE EXTRACTION WITH PRETRAINED MODELS\nUsing pretrained neural network models for extracting visual features has stood the test of time\nsince the AlexNet (Krizhevsky et al., 2012) CNN model pretrained on ImageNet-1k (Russakovsky\net al., 2015). More recent models have upgraded the same setup with modern architectures, such as\nResNets (used in, e.g., DETR, Carion et al., 2020) or even Vision Transformers. As Transformers\nare easily able to handle different modalities during training, off-the-shelf backbones are now com-\nmonly trained on label supervision (e.g., DeiT-III on ImageNet-22k, Touvron et al., 2022) or text\n8\nsupervision (e.g., CLIP (Radford et al., 2021)), providing strong visual foundation models, scaling\nwell with model sizes, and enabling excellent performance on a variety of tasks including detection\n(Carion et al., 2020) and segmentation (Zheng et al., 2021; Kirillov et al., 2023).\nIn this context, supervision relies on annotations in the form of labels or text alignment; the dataset\nbiases (Torralba & Efros, 2011) are not well characterized, yet they drive learning and shape the\nlearned models. An alternative approach consists of not using supervision and letting the mod-\nels learn from the data via a pretext task that is designed to require understanding the content of\nimages (Doersch et al., 2015). This self-supervised learning paradigm was explored in multiple\nmethods using Vision Transformers: MAE (He et al., 2022) trains a model at reconstructing pixel\nvalues of hidden areas of an image and then applies fine-tuning to address a new task. With a dif-\nferent approach, the self-distillation family of methods (He et al., 2020; Caron et al., 2021; Zhou\net al., 2022) showcase strong performance using frozen backbones, allowing for more robustness to\ndomain shifts for task-specific downstream models.\nIn this work, we focused the analysis on self-supervised learning, and more specifically on the DI-\nNOv2 approach (Oquab et al., 2023), which has shown to be particularly effective for learning local\nfeatures. We showed that despite excellent benchmark scores, DINOv2 features exhibit undesirable\nartifacts and that correcting these artifacts in the learning process allows for further improvements in\nthe benchmark performances. These phenomenon is even more surprising as DINOv2 builds upon\nDINO (Caron et al., 2021), which does not show signs of artifacts. We then further showed that the\ncorrection techniques also hold for supervised training paradigms by testing on DeiT-III and CLIP.\n4.2\nADDITIONAL TOKENS IN TRANSFORMERS\nExtending the transformer sequence with special tokens was popularized in BERT (Devlin et al.,\n2019). However, most approaches add new tokens either to provide the network with new infor-\nmation as for example [SEP] tokens in BERT and tape tokens in AdaTape (Xue et al., 2023), or to\ngather information in these tokens, and use their output value as an output of the model:\n\u2022 for classification: as [CLS] tokens in BERT and ViT (Dosovitskiy et al., 2021)\n\u2022 for generative learning: as [MASK] in BERT and BEiT (Bao et al., 2021)\n\u2022 for detection: as object queries in DETR (Carion et al., 2020), detection tokens in YO-\nLOS (Fang et al., 2021), and ViDT (Song et al., 2021)\n\u2022 for accumulating information from possibly multiple modalities before decoding, as latent\ntoken arrays in Perceivers (Jaegle et al., 2021; 2022).\nDifferent to these works, the tokens we add to the sequence add no information, and their output\nvalue is not used for any purpose. They are simply registers where the model can learn to store\nand retrieve information during the forward pass. The Memory Transformer (Burtsev et al., 2020),\ncloser to our work, presents a simple approach to improve transformer models using memory tokens\nadded to the token sequence, improving translation performance. In follow-up work, Bulatov et al.\n(2022) address complex copy-repeat-reverse tasks. Sandler et al. (2022) extend this line to the vision\ndomain for fine-tuning but observe that such tokens do not transfer well across tasks.\nIn contrast, we do not perform fine-tuning and employ additional tokens during the pretraining phase\nto improve the features obtained for all tasks downstream. More importantly, our study contributes\nthe following new insight in Sec. 2: the mechanism implemented through memory tokens already\nappears naturally in Vision Transformers; our study shows that such tokens allow us not to create\nbut to isolate this existing behavior, and thus avoid collateral side-effects.\n5\nCONCLUSION\nIn this work, we exposed artifacts in the feature maps of DINOv2 models, and found this phe-\nnomenon to be present in multiple existing popular models. We have described a simple method to\ndetect these artifacts by observing that they correspond to tokens with an outlier norm value at the\noutput of the Transformer model. Studying their location, we have proposed an interpretation that\nmodels naturally recycle tokens from low-informative areas and repurpose them into a different role\nfor inference. Following this interpretation, we have proposed a simple fix, consisting of appending\n9\nadditional tokens to the input sequence that are not used as outputs, and have found that this entirely\nremoves the artifacts, improving the performance in dense prediction and object discovery. More-\nover, we have shown that the proposed solution also removes the same artifacts present in supervised\nmodels such as DeiT-III and OpenCLIP, confirming the generality of our solution.\nREFERENCES\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2021.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In NeurIPS,\n2022.\nMikhail S Burtsev, Yuri Kuratov, Anton Peganov, and Grigory V Sapunov. Memory transformer.\narXiv preprint arXiv:2006.11527, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. NAACL, 2019.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In ICCV, 2015.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\nYuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and\nWenyu Liu. You only look at one sequence: Rethinking transformer in vision through object\ndetection. In NeurIPS, 2021.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick.\nMasked\nautoencoders are scalable vision learners. In CVPR, 2022.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali\nFarhadi, and Ludwig Schmidt. Openclip. 2021.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.\nPerceiver: General perception with iterative attention. In ICML, 2021.\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David\nDing, Skanda Koppula, Andrew Brock, Evan Shelhamer, Olivier J. H\u2019enaff, Matthew M.\nBotvinick, Andrew Zisserman, Oriol Vinyals, and Jo\u02dcao Carreira. Perceiver io: A general ar-\nchitecture for structured inputs & outputs. In ICLR, 2022.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\nlutional neural networks. In NeurIPS, 2012.\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. In NeurIPS, 2020.\n10\nDavid G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.\nMaxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning\nrobust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei.\nImagenet large scale visual recognition challenge. IJCV, 2015.\nMark Sandler, Andrey Zhmoginov, Max Vladymyrov, and Andrew Jackson. Fine-tuning image\ntransformers using learnable memory. In CVPR, 2022.\nOriane Sim\u00b4eoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick\nP\u00b4erez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and\nno labels. In BMVC, 2021.\nHwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jampani, Dongyoon Han, Byeongho Heo,\nWonjae Kim, and Ming-Hsuan Yang. Vidt: An efficient and effective fully transformer-based\nobject detector. In ICLR, 2021.\nAntonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR, 2011.\nHugo Touvron, Matthieu Cord, and Herv\u00b4e J\u00b4egou. Deit iii: Revenge of the vit. In ECCV, 2022.\nXudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object\ndetection and instance segmentation. In CVPR, 2023.\nFuzhao Xue, Valerii Likhosherstov, Anurag Arnab, Neil Houlsby, Mostafa Dehghani, and Yang You.\nAdaptive computation with elastic input sequence. In ICML, 2023.\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers. In CVPR, 2021.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot:\nImage bert pre-training with online tokenizer. In ICLR, 2022.\n11\nFigure 10: Feature norms along locations: proportion of tokens with norm larger than the cutoff\nvalue at a given location. Left: official DINOv2 model (no antialiasing), right: our models (with\nantialiasing). At some positions, more than 20% of tokens have a high norm.\nFigure 11: Propagating unit gradients through a bicubic interpolation (16 \u00d7 16 \u2192 7 \u00d7 7) without\nantialiasing. We observe a striping pattern similar to the one of Fig. 10 (left).\nA\nINTERPOLATION ARTIFACTS AND OUTLIER POSITION DISTRIBUTION\nWe plot in Figure 10 (left) the proportion of outlier tokens, characterized by a norm larger than the\ncutoff value defined manually, following the distribution of norms shown in Fig. 3 (main text). We\nmake two observations:\nFirst, the distribution has a vertical-striped pattern. We investigate this phenomenon and notice that\nin the original DINOv2 implementation, during training the position embeddings are interpolated\nfrom a 16 \u00d7 16 map into a 7 \u00d7 7 map, without antialiasing. Propagating unit gradients through\nsuch an interpolation function (bicubic resize) leads to the following gradients, shown in Fig. 11.\nIn this work, when producing results with DINOv2 (especially for the results in Tables 2a,3), we\nalways apply antialiasing in the interpolation operator, removing the striping pattern, which gives an\nupdated distribution of outlier positions as shown in Fig. 10 (right).\nSecond, the outliers tend to appear in areas closer to the border of the feature map rather than in the\ncenter. Our interpretation is that the base model tends to recycle tokens in low-informative areas to\nuse as registers: pictures produced by people tend to be object-centric, and in this case the border\nareas often correspond to background, which contains less information than the center.\nB\nCOMPLEXITY ANALYSIS\nSince our proposed fix introduces new tokens, it also increases the number of learnable parameters\nand the FLOP count of the model. We show in Fig. 12 the relationship between number of registers\nand increase in model FLOP count and parameter count. We observe that adding registers induces\na negligible change in number of parameters, and a slight change in FLOP count. Still, for n = 4\nregisters, the increase in FLOPs stays below 2%.\n12\nFigure 12: Increase in model parameter and FLOP count when adding different numbers of registers.\nAdding registers can increase model FLOP count by up to 6% for 16 registers. However, in the more\ncommon case of using 4 registers, that we use in most of our experiments, this increase is below 2%.\nIn all cases, the increase in model parameters is negligible.\nDINOv2\nCLIP\nDeiT-III\nw/ REG\nw/o REG\nw/ REG\nw/o REG\nw/ REG\nw/o REG\nLOST\nscore\ndot prod.\nw/ seed\nseed\nexpansion\nFigure 13:\nIllustration of the intermediate computations in the LOST algorithm for all models.\nAdding registers drastically improves the look of all intermediate steps for DeiT-III and DINOv2.\nThe difference is less striking for the CLIP model.\nC\nANALYSIS OF LOST PERFORMANCE\nThe results presented in Sec. 3.3 show that adding registers allows us to obtain better object dis-\ncovery performance with DINOv2 models. The conclusions for the two other models studied in\nthis work could be more crisp. In order to understand why this is so, we qualitatively study the\nimpact of removing artifacts on the intermediate computations in the LOST algorithm. We show the\nintermediate outputs of LOST for all models on a given input image in Fig. 13.\nAdding registers improves the scores and the resulting seed expansion for DeiT-III and DINOv2.\nThis observation is coherent with the improved numbers reported in Table 3. For CLIP, however, the\nLOST algorithm seems robust to the type of outliers observed in the local features. Adding registers\ndoes remove artifacts (as clearly shown in Fig. 15) but does not have much impact on the LOST\nscore. It is also worth noting that CLIP, with or without registers, provides comparable performance\nto DINOv2 without registers and DeiT-III with registers. The qualitative assessment is coherent with\nthe numbers reported in Table 3.\nD\nQUALITATIVE RESULTS\nWe trained three popular models: DeiT-III, CLIP, DINOv2 with and without the introduction of reg-\nister tokens. We observe in Fig. 14 the attention maps in the last layer of the Vision Transformer, for\nall three cases. We see that our approach provides much cleaner attention maps, with considerably\nfewer artifacts, explaining the improvement on the downstream object discovery task mentioned in\nSec. 3.3. The feature maps are also visibly improved, as shown in Fig. 15. Finally, we also show the\nnorm of the patch tokens in Fig. 16, and confirm that in all three models, artifact patches correspond\nto norm outliers.\n13\nWithout registers\nWith registers\nInput\nDeiT-III\nCLIP\nDINOv2\nDeiT-III\nCLIP\nDINOv2\nFigure 14: Attention maps of models trained without and with registers on various images.\n14\nWithout registers\nWith registers\nInput\nDeiT-III\nCLIP\nDINOv2\nDeiT-III\nCLIP\nDINOv2\nFigure 15: First principal component of the feature maps output by models trained without and\nwith registers on various images. The components are whitened and the colormap covers the range\n[\u22123\u03c3, +3\u03c3].\n15\nWithout registers\nWith registers\nInput\nDeiT-III\nCLIP\nDINOv2\nDeiT-III\nCLIP\nDINOv2\nFigure 16: Norm the feature maps output by models trained without and with registers on various\nimages. The norm outliers are very visible for models trained without registers.\n16\n"
  },
  {
    "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
    "link": "https://arxiv.org/pdf/2309.16058.pdf",
    "upvote": "53",
    "text": "AnyMAL: An Efficient and Scalable Any-Modality\nAugmented Language Model\nSeungwhan Moon\u2217\nAndrea Madotto\u2217 Zhaojiang Lin\u2217 Tushar Nagarajan\u2217\nMatt Smith\nShashank Jain\nChun-Fu Yeh\nPrakash Murugesan\nPeyman Heidari\nYue Liu\nKavya Srinet\nBabak Damavandi\nAnuj Kumar\nFAIR, Meta & Meta Reality Labs\nAbstract\nWe present Any-Modality Augmented Language Model (AnyMAL), a unified\nmodel that reasons over diverse input modality signals (i.e. text, image, video,\naudio, IMU motion sensor), and generates textual responses. AnyMAL inherits\nthe powerful text-based reasoning abilities of the state-of-the-art LLMs including\nLLaMA-2 (70B), and converts modality-specific signals to the joint textual space\nthrough a pre-trained aligner module. To further strengthen the multimodal LLM\u2019s\ncapabilities, we fine-tune the model with a multimodal instruction set manually\ncollected to cover diverse topics and tasks beyond simple QAs. We conduct com-\nprehensive empirical analysis comprising both human and automatic evaluations,\nand demonstrate state-of-the-art performance on various multimodal tasks.\n1\nIntroduction\nLarge Language Models (LLMs), known for their substantial size and complexity, have significantly\nenhanced the capacity of machines to understand and articulate human language. The progress in\nLLMs has also led to notable advancements in the vision-language domain [1, 2, 3, 4], bridging the\ngap between image encoders and LLMs to combine their reasoning capabilities. Prior multimodal\nLLM research has concentrated on models that combine text and one other modality [3, 5], such as text\nand image models, or has centered on proprietary language models that are not open sourced [2, 4].\nTo tackle the previously mentioned challenges, we introduce Any-Modality Augmented Language\nModel (AnyMAL) \u2014 a collection of multi-modal encoders trained to transform data from various\nmodalities, including images, videos, audio, and IMU motion sensor data, into the text embedding\nspace of an LLM. To achieve this, we extend the work by [1] to (1) more capable instruction-tuned\nLLMs (i.e. LLaMA-2-70B-chat [6]), (2) larger pre-trained modality encoders, and (3) advanced\nprojection layers to handle variable input lengths. The model output examples are shown in Figure 1,\nand an illustration of the overall methodology is shown in Figure 2.\nThe key contributions of the work are as follows:\n\u2022 We present an efficient and scalable solution for building Multimodal LLMs. We provide\nprojection layers pre-trained on large datasets with diverse modalities (e.g. 200M images,\n2.2M audio, 500K IMU time-series, 28M videos) all aligned to the same LLM (LLaMA-2-\n70B-chat), thus enabling interleaved multimodal in-context prompting.\n\u2022 We further fine-tune the model with the multimodal instruction set across three modalities\n(image, video, and audio) covering diverse unconstrained tasks beyond simple QA domains.\nThe dataset features high-quality manually collected instruction data, which we thus also\nuse as a benchmark for complex multimodal reasoning tasks.\n\u2022 Our best model achieves strong zero-shot performance in both automatic and human eval-\nuation on diverse tasks and modalities, setting new SOTA with +7.0% relative accuracy\n\u2217Joint First Authors. \u0000: {shanemoon,andreamad8,zhaojiang,tusharn}@meta.com\nPreprint. Under review.\narXiv:2309.16058v1  [cs.LG]  27 Sep 2023\nFigure 1: Example AnyMAL outputs. The model understands various input signals (i.e. vision,\naudio, motion sensor signals), and responds to free-form user queries. When multiple modalities\nare interleaved and given as input (e.g. right-most: image + IMU motion sensor signals), the model\nreasons over them jointly.\nimprovement on VQAv2, +8.4% CIDEr on zeroshot COCO image captioning, and +14.5%\nCIDEr on AudioCaps, when compared with the models available in the literature.\n2\nRelated Work\nLarge Language Models (LLM): There has been a surge of LLMs with varying model sizes recently,\nshowcasing remarkable reasoning capabilities. While the most well-known commercial service is\nChatGPT [4, 7], the open-sourced models include FlanT5 [8], GPT-J [9], OPT [10], LLaMA [11],\nVicuna [12], and more recently, LLaMA-2 [6].\nOur work builds upon the powerful text-based reasoning capabilities of these LLMs, extending these\ncapabilities to multimodal inputs.\nVision-Language Models: Numerous studies have addressed the task of instructing a unified model\nthat integrates both visual and linguistic elements, finding practical implementations in domains\nlike image captioning [13] and visual question answering (VQA) tasks [14, 15, 16]. While the\nrelative scarcity of data sources aligning different modalities has conventionally been considered\nthe bottleneck in scaling, recent works have shifted towards harnessing the capabilities of pre-\ntrained LLMs, tapping into the knowledge accrued from extensive textual corpora. These work\ninclude Flamingo [2], OpenFlamingo [17], Palm-E [18], BLIP-2 [3], InstructBLIP [19], LLaVA\n[20], IDEFICS [5], MiniGPT-4 [21] and many more [22, 23, 24, 25, 26, 27, 28], where each model\nuses different variants of base LLMs. These models typically undergo fine-tuning stages as well,\nre-purposing several task-specific vision-language datasets [20, 29].\nOur work extends the previous approaches by (1) allowing for diverse input modalities beyond vision\nsignals, (2) presenting a fine-tuning process with our manually collected multimodal instruction\ntuning data, and (3) scaling the LLM parameters to 70B via an efficient pre-training approach.\n2\nFigure 2: AnyMAL Training. (a) Modality alignment pre-training allows for mapping the output of\neach modality encoder into the joint LLM embeddings space through projection layers. (b) With\nmultimodal instruction tuning, the model learns to associate system instructions and text queries with\ninput multimodal contexts. Our modality-specific encoder zoo includes: CLIP ViT-L, ViT-G, DinoV2\n(image), CLAP (audio), IMU2CLIP (IMU motion sensor), and Intervideo (video).\n3\nMethods\n3.1\nPre-training\nModality Alignment: We achieve the multimodal understanding capabilities by pre-training LLMs\nwith paired multimodal data (modality-specific signals and text narrations) (Figure 2). Specifically,\nwe train a lightweight adapter for each modality to project the input signals into the text token\nembedding space of a specific LLM. In this way, the text token embedding space of the LLM becomes\na joint token embedding space, with tokens representing either text or other modalities. The number\nof token embeddings used to represent each input modality is fixed per adapter, ranging from 64 -\n256 in this work. During the alignment training, we freeze the model parameters of the underlying\nLLM, which allows it to reach convergence faster than training end-to-end from scratch, and to\ninherit the reasoning capabilities of the LLM at inference time. In addition, to maximize the feature\ncompatibility, for each modality we use an encoder g(\u00b7) that has already been aligned to a text\nembeddings space, e.g. CLIP [30, 31] for images, CLAP [32] for Audio signals, or IMU2CLIP [33]\nfor IMU signals. For each text caption and modality pair (Xtext, Xmodality), we align them using the\nfollowing objectives with a projection module (i.e. Perceiver Resampler [2] for vision encoder, and\nlinear layers for other modalities).\np(Xtext|Xmodality) =\nL\nY\ni=1\np\u03b8(X[i]\ntext|Zmodality, Z[1:i\u22121]\ntext\n)\n(1)\nZmodality = Projection\u03b8(hlatents, g(Xmodality))\n(2)\nDatasets: For image alignment, we use a cleaned subset of the LAION-2B dataset, filtered using the\nCAT method and with any detectable faces blurred [34]. For audio alignment, we use AudioSet [35]\n(2.1M samples), AudioCaps [36] (46K samples), and CLOTHO [37] (5K samples) datasets. We use\nthe Ego4D dataset [38] for IMU and text alignment (528K).\nQuantization: Scaling the pre-training to 70B parameter models for a large dataset (200M+ instances)\nrequires significant resources, often requiring an FSDP [39] wrapper to shard the model across\nmultiple GPUs. To effectively scale our training, we implement the quantization strategies (4 bits and\n8 bits) [40] in our multimodal settings, in which we keep the LLM component of our model frozen\nand only have the modality tokenizers trainable. This approach shrinks the memory requirement by\nan order of magnitude. Thus, we are able to train 70B AnyMAL on a single 80GB VRAM GPU with\na batch size of 4.\nCompared with FSDP, we observe that the proposed quantization approach achieves the same\nthroughput while only using half of the GPU resources. We do note that the training / validation loss\nwere constantly higher compared to the FSDP training, but nevertheless did not impact the generation\nquality (at inference time, we use the original LLM at full precision to maximize the accuracy).\n3\nExample 1. Creative Instructions (Image)\nInstruction Prompt\nWrite a short story about the scene in this photo. Include some\ndialogue between the characters.\nResponse\nThe restaurant table stood empty but adorned with remnants of a\nmeal. A seagull landed on its edge, spreading his wings in a display\nof dominance. With a raucous cry, he fiercely guarded the plate\nbefore him, which he intended to make his dinner. His piercing\ngaze fixed upon another seagull, wings poised for landing, his eyes\nlocked on the feast. \"Back off!\" the first seagull screeched. Beak\nopen wide, he emitted a symphony of squawks, challenging the\nattacker. Undeterred, the second seagull hesitated, torn between his\ndesire for snacks, and fear of the defending seagull. The tension\nhung in the air, and as the first seagull prepared to yell again, the\nsecond replied with a sullen \"Fine,\" and flew off. Triumphant, the\nfirst gull tucked into his dinner.\nExample 2. Detailed Description (Image)\nInstruction Prompt\nExplain all the details of the scene, starting from left to right.\nResponse\nThis is a photo of a small delivery truck. In the background of\nthe photo is a loading dock of some type with green and yellow\nstructures under an awning. The small truck stands at the center of\nthe photo. It has a flatbed and awning over the flatbed and some\ntype of white cabinet on the back and a black storage compartment\nunder. The bed is green and has black and green milk crates stacked\non it. The cab of the truck is small and tall with large windows all\naround. It is white with a green lower section and fine yellow stripe\nseparating the green and white paint. The front of the truck has\n\u2018Dairy Crest\u2019 painted on it in green and has a European style license\nplate. There is a partial image of a man in the background.\nExample 3. Open-ended Question and Answering (Video)\nInstruction Prompt\nWhat is this animal doing and why are they doing it?\nResponse\nPheasants eat gravel to help them in the digestion of their food. This\nhelps them survive multiple weeks without feeding during extreme\nweather. A newly hatched chick will survive on insects for a few\nweeks of its life.\nExample 4. Multimodal Reasoning (Audio)\nAudio File (Freesound Link)\nInstruction Prompt\nWhat could be the likely momentous occasion in this sound clip?\nResponse\nThis is a countdown to a rocket taking off into space by a robotic\nvoice.\nTable 1: Example data from our MM-IT dataset, manually collected instruction prompt and response\npairs for diverse modalities (i.e. image, video, audio). The collected instructions cover diverse\ndomains (e.g. creative writing, open-ended reasoning), and are strictly grounded to the provided\nmultimodal context (i.e. queries cannot be answered without understanding the accompanying\nmultimodal context). The MM-IT data serves as both a fine-tuning dataset as well as an evaluation\nbenchmark for complex multimodal reasoning tasks.\n4\n3.2\nFine-tuning with Multimodal Instruction Datasets\nTo further improve the model\u2019s instruction-following capability with respect to diverse input modal-\nities, we perform additional fine-tuning with our multimodal instruction-tuning (MM-IT) dataset.\nSpecifically, we concatenate the input as [<instruction> <modality_tokens>], such that the\nresponse target is grounded on both textual instructions and the modality input. We perform ablations\nover (1) training the projection layers without altering the LLM parameters, or (2) using Low-Rank\nAdaptation [41] to further tune the LM behaviors.\nWe use both manually collected instruction-tuning datasets and synthetic data.\nManual Annotation. While there are publicly available third-party datasets on various VQA tasks,\nwe observe that many of these data have insufficient diversity and quality \u2014 in particular for aligning\nLLMs towards diverse multimodal instruction-following tasks that go much beyond simple QA\nqueries (e.g. \u201cCreate a poem using this image\u201d, \u201cExtract the phone number on this flyer\u201d).\nTherefore, we focus on collecting 60K examples of high-quality multimodal instruction tuning data\nfor multiple modalities, as illustrated in Table 1. Specifically, we use various Creative Commons\nlicensed, publicly available images, and augment these images with manually created instructions and\nresponses. Annotators are required to provide instruction and answer pairs that are strictly multimodal,\nsuch that queries cannot be answered without understanding the accompanying multimodal context.\nWe show that our results notably improve using these fewer but well-balanced and higher-quality\nexamples from our own vendor-based annotation efforts.\nSynthetic Augmentation. In addition to the high-quality ground-truth instruction tuning data above,\nwe augment the dataset using the LLaMA-2 (70B) [6] model, following similar approaches proposed\nby LLaVA [20]. Specifically, we use a textual representation of the image (i.e. multiple captions,\nbounding boxes information and objects) to generate question-answer pairs for the image. We\ngenerate 150K image-instruction-response pairs on varying domains and question types.\nNote that our process strictly uses only open-sourced models \u2013 as opposed to other works that use\ncommercial services such as ChatGPT or GPT-4.\n4\nExperiments\n4.1\nTasks\nWe evaluate the model\u2019s performance on two categories of tasks in the zero-shot setting: (1) captioning\ntasks for various modalities, and (2) multimodal reasoning and instruction-following tasks.\nCaptioning Tasks. We evaluate AnyMAL\u2019s primary capability of generating captions given input\nmodalities, which is aligned with the pre-training objective. The main purpose of the captioning task\nis to understand the alignment level between the text and other modalities after pre-training. Since\nthe captioning tasks typically don\u2019t require secondary reasoning steps, we expect that LLM weights\nor parameter sizes have less influence on the task.\nMultimodal Reasoning Tasks. Given the high-level of alignment among the modalities, we evaluate\nthe model\u2019s reasoning and instruction-following abilities which it inherits from the core instruction-\ntuned LLM, as well as from the multimodal instruction-tuning process.\nWe conduct a comprehensive comparison with strong baseline models for each respective modality\npair (vision-language and audio-language) from the open-sourced literature.\nNote: As the MM-IT datasets include some in-domain images from public benchmarks (e.g. COCO),\nwe report results separately for the pre-trained models (without further instruction tuning in Section\n3.2) and the instruction-tuned models \u2013 to denote a strict zeroshot setup. All multimodal-instruction-\ntuned AnyMAL models are marked with \u201cMM-IT\u201d in the following sections.\n4.2\nQuantitative Analysis\nImage Caption Generation: Table 2 shows zeroshot image captioning performance on COCO [48]\nand a subset of the MM-IT dataset marked with the \u201cdetailed description\u201d task (MM-IT-Cap). It\ncan be seen that our AnyMAL variants significantly outperform the baselines in both datasets. It is\nworthwhile to note that there is no significant gap between the performance of the AnyMAL-13B\nand the AnyMAL-70B variants. This result indicates that the underlying LLM capability has smaller\nimpact to the image caption generation task (which corresponds to the core visual understanding\ncapability), but is largely dependent on the scale of the data and the alignment methods. We attribute\n5\nModels\nCOCO\nMM-IT-Cap\nCIDEr\nCIDEr\nBLIP-2\n[3]\n-\n2.9\nMiniGPT4\n[21]\n-\n14.1\nLLaVA\n[20]\n-\n14.3\nCM3Leon\n[42]\n61.6\n-\nOpenFlamingo-v2 9B [17]\n79.5\n1.8\nFlamingo-3B\n[2]\n73.0\n-\nFlamingo-9B\n[2]\n79.4\n-\nFlamingo-80B\n[2]\n84.3\n-\nIDEFICS-9B\n[5]\n46.0\n-\nIDEFICS-80B\n[5]\n91.8\n-\nAnyMAL 13B (ViT-G)\n99.5\n15.5\nAnyMAL 70B (ViT-G)\n95.9\n15.7\nTable 2: Zeroshot Image Captioning performance on COCO and MM-IT-Cap. Ablations (bottom)\nover our AnyMAL with varying LLM sizes. Bold and underlined denote the top and the second-best\nperformance, respectively. \u201c-\u201d: the model (a) does not report results on the marked benchmarks, or (b)\nis pretrained or fine-tuned on the respective dataset, thus not suitable for the zeroshot evaluation above.\nAnyMAL demonstrates the state-of-the-art zeroshot visual understanding capabilities compared to\nthe baseline vision-language models.\nFigure 3: Image-based reasoning human evaluation results on pairwise comparisons (% win, tie\nand lose) with baseline outputs against the manually annotated ground-truth samples from MM-IT\n(1K test set). Baselines used: BLIP-2 (FlanT5XXL) [3], InstructBLIP (Vicuna-13B) [19], MiniGPT4\n[21] and LLaVA [20]. AnyMAL demonstrates a smaller gap with human-generated responses (41.1%\nwin), compared to the baselines (LLaVA: 34.4% win, and MiniGPT4: 27.0%).\nthe slight under-performance of the AnyMAL-70B on COCO to the general verbosity of the LLaMA-\n70B model, which negatively impacts the score when evaluated against COCO captions that tend\nto be brief and concise. As expected, the automatic evaluation on MM-IT-Cap shows lower CIDEr\nscores overall, attributed to the much longer response length in detailed descriptions (See Table 1 for\nan example).\nHuman Evaluation on Multimodal Reasoning Tasks: MM-IT features diverse multimodal instruc-\ntion and ground-truth answer pairs. We evaluate the performance of our models (pre-trained and\ninstruction-tuned) against other vision-language models publicly available to run and use (i.e. LLaVA\n[20], MiniGPT4 [21]). Since the responses are subjective in nature (e.g. creative writing \u2013 \u201cWrite a\n6\nModels\nResponse Acc\nObj Recognition\nIntegrity\nBLIP-2 (FlanT5XXL)\n[3]\n43.3\n73.5\n99.3\nInstructBLIP (Vicuna-13B)\n[19]\n46.3\n73.2\n98.3\nMini-GPT4\n[21]\n42.7\n73.0\n99.5\nLLaVA\n[20]\n51.7\n85.4\n99.5\nAnyMAL 70B\n56.0\n82.4\n99.3\nAnyMAL 70B (MM-IT Synth Only)\n54.2\n83.5\n99.5\nAnyMAL 70B (MM-IT Human+Synth)\n58.0\n79.3\n99.7\nTable 3: Image-based Reasoning human evaluation results on 1K test set from MM-IT on different\naxes: (a) Response Accuracy and Relevance (%) \u2013 whether responses are relevant to instructions and\nfactually correct without any hallucinations, (b) Object Recognition (%) \u2013 whether key objects are\nidentified at a detailed level, and (c) Integrity (%) \u2013 whether responses include offensive language.\nMM-IT indicates the model that has been instruction-tuned either with synthetic data only, or with\nthe manually collected set (Section 3.2).\nModels\nH-Meme\nVQAv2\nTextVQA\nS-QA\nVizWiz\nOKVQA\nAUC\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nOpenFlamingo-v2\n[17]\n51.6\n50.5\n24.2\n-\n27.5\n37.8\nFlamingo-3B\n[2]\n53.7\n49.2\n30.1\n-\n28.9\n41.2\nFlamingo-9B\n[2]\n57.0\n51.8\n31.8\n-\n28.8\n44.7\nFlamingo-80B\n[2]\n46.4\n56.3\n35.0\n-\n31.6\n50.6\nBLIP-2 (FlanT5XXL)\n[3]\n52.0\n65.0\u2020\n44.1*\n64.5\n29.4\n45.9\nInstructBLIP (V-13B)\n[19]\n53.7\n-\n50.7\u2020*\n70.6\n33.4\n-\nIBELICS-9B\n[5]\n51.8\n50.9\n25.9\n-\n35.5\n38.4\nIBELICS-80B\n[5]\n60.6\n60.0\n30.9\n-\n36.0\n45.2\nAnyMAL 13B (ViT-G)\n66.0\n59.6\n24.7\n52.7\n24.4\n33.1\nAnyMAL 70B (DINO-V2)\n65.6\n59.2\n13.7\n64.7\n23.6\n41.4\nAnyMAL 70B (ViT-L)\n68.2\n62.0\n35.4\n67.2\n32.2\n41.2\nAnyMAL 70B (ViT-G)\n69.1\n64.2\n32.9\n70.8\n33.8\n42.6\nAnyMAL 70B (MM-IT;ViT-G)\n67.4\n67.8\u2020\n32.5\n67.6\n41.3\n46.1\nTable 4: Zeroshot Image-based QA results on 6 different VQA datasets (H-Meme: Hateful Meme,\nS-QA: Science QA). Ablations (bottom) over AnyMAL with varying base ViTs and LLM sizes.\nMM-IT (last row) denotes the model fine-tuned on our instruction dataset. Bold and underlined\ndenote the top and the second-best performance, respectively. AnyMAL demonstrates competitive\nzeroshot multimodal reasoning capabilities, compared to the baseline vision-language models. *:\nResults with additional OCR inputs. \u2020: in-domain images (i.e. COCO, TextCap) have been used\nduring training, thus not a strict zeroshot performance.\npoem about this image\u201d, we believe that human assessment provides the most precise insight into the\nperformance and capabilities of our proposed model.\nWe therefore collect pairwise comparisons for each baseline against 1K ground-truth samples (Figure\n3), as well as the Likert scale scores (0-2) for each of the following criteria. The criteria for preference\nranking includes response accuracy, object recognition accuracy, and integrity (see the full rubrics\nin Appendix A). Response accuracy measures whether the response contains the relevant, factually\ncorrect and verifiable information (without any hallucinations) with regards to the image and the\ninstruction. Object recognition accuracy strictly measures whether the key objects are correctly\nrecognized at a detailed level \u2013 primarily concerning the model\u2019s visual knowledge. Finally, the\nintegrity metric measures whether the response shows any harmful or offensive language.\nFigure 3 shows that AnyMAL achieves strong performance with a narrower gap against the manually\nannotated ground-truth samples (41.1% win), compared to the baselines (LLaVA : 34.4% win, and\nMiniGPT4: 27.0% win). Notably, the model fine-tuned with the full instruction set exhibits the\nhighest rate of preferential wins, showing a competitive level of visual understanding and reasoning\ncapabilities comparable to human-annotated responses. It is also worthwhile to note that BLIP-2 and\nInstructBLIP suffer on these open-ended queries (4.1% and 16.7% preferential win, respectively),\ndespite their strong performance in the public VQA benchmarks (Table 4).\n7\nModels\nAudioCaps\nCIDEr\nSPICE\nSPICEr\nTopDown-AlignedAtt [36]\n59.3\n14.4\n36.9\nCNN10-VGG\n[43]\n66.0\n16.8\n41.4\nACT\n[44]\n67.9\n16.0\n42.0\nPANNs + BERT\n[45]\n66.7\n17.2\n42.0\nAnyMAL 7B (CLAP)\n70.4\n21.0\n45.7\nAnyMAL 13B (CLAP)\n72.1\n22.0\n47.0\nAnyMAL 70B (CLAP)\n77.8\n23.0\n50.4\nTable 5: Zeroshot Audio Captioning results on AudioCaps. Ablations (bottom) over our AnyMAL\nwith varying base LLMs and sizes. AnyMAL attains the best performance across multiple metrics,\nshowing the model\u2019s strong performance in audio signal understanding.\nModels\nSTAR\nHow2QA\nNextQA\nAccuracy\nAccuracy\nAccuracy\nInternvideo (8)\n[46]\n41.6\n62.2\n49.1\nFlamingo-9B\n[2]\n41.8\n-\n-\nFlamingo-80B\n[2]\n39.7\n-\n-\nBLIPv2 ViTG FlanT5xxl (4)\n[47]\n42.2\n69.8\n62.4\nAnyMAL-Video 13B (Internvideo) (8)\n37.5\n54.8\n46.8\nAnyMAL-Video 70B (Internvideo) (8)\n41.3\n60\n50.6\nAnyMAL-Image 13B (ViT-G) (4)\n44.4\n59.6\n47.9\nAnyMAL-Image 70B (ViT-G) (4)\n48.2\n68.1\n57.6\nTable 6: Zeroshot Video-based QA accuracy on STAR, How2QA, and NextQA. Ablations (bottom)\nover AnyMAL with image vs video model and LLM sizes. AnyMAL demonstrates competitive\nzeroshot multimodal reasoning capabilities, compared to the baseline vision-language models. Num-\nbers in brackets denote number of frames used per video clip.\nTable 3 shows the detailed breakdown of scores for each criteria. Specifically, it can be seen that\nthe model instruction-tuned with both manually collected and synthetically curated sets attains the\nhighest response accuracy and relevancy score (12.2% relative improvement compared to the strongest\nbaseline: LLaVA). This result highlights the enhanced capability of the model to comprehend and\nprecisely answer questions in accordance with provided instructions. It is worth mentioning that\nthe model, after instruction tuning, exhibits a decline in its ability to provide detailed recognition\nand descriptions of objects. We attribute this result to our intention behind collecting the AnyMAL\ndataset which promotes concise responses. Consequently, the model tends to provide more succinct\ndescriptions, rather than elaborating on details (which often increases the risk of factual inaccuracies).\nHigh integrity scores (99+%) are achieved by all baselines.\nVQA Benchmarks: Table 4 shows the zeroshot performance on the Hateful Meme dataset [49],\nVQAv2 [14], TextVQA [50], ScienceQA [51] (image-paired subset), VizWiz [52], and OKVQA [53]\ncompared against the models in the literature that report zeroshot results on the respective benchmark.\nWe focus on zeroshot evaluation to best estimate the model\u2019s performance on the open-ended queries\nat inference time.\nOverall, our AnyMAL exhibits the strongest performance compared to the baselines across multiple\ntasks. Notably, the pretrained AnyMAL models show competitive performance even without further\nfine-tuning on MM-IT\u2013 demonstrating strong reasoning capabilities retained in the base pre-training\nphase. Comparing the two text-guided vision encoders ViT-L [30] and ViT-G [31], ViT-G achieves\nhigher scores in most of the vision datasets. These results suggest that larger encoders trained\nover more steps offer better representations. However, the TextVQA dataset presents a unique\ncase in which the image encoder\u2019s resolution plays a pivotal role in recognizing text within images.\nConsequently, the ViT-L model, with a resolution of 336x336, achieves a higher score than the\nViT-G model, which had a resolution of 224x224. DinoV2 [54], which is trained in a discriminative\nself-supervised approach, shows worse performance compared to the text-guided vision encoders,\n8\nAudio Signals Input\nAnyMAL Output\nGold Caption: Typing on a keyboard is occurring in a quiet envi-\nronment\nPrompt: Can you guess my location? Reply with one or two\nsentences.\nResponse: Based on the sound of the typing, I would guess that\nyou are located in a quiet, indoor setting, possibly in an office or\nstudy. The sound of the typing is clear and crisp, with a slight echo\nor reverb that suggests a room with a high ceiling or a large, open\nspace.\nGold Caption: Very strong wind is blowing, and waves are rustling\nPrompt: Based on the weather, what should I wear? Reply with\none or two sentences.\nResponse: Based on the sound of the wind and the waves, it seems\nlike a stormy weather is approaching. It\u2019s best to dress in layers and\nwear something waterproof and warm to stay comfortable and safe.\nGold Caption: A baby cries while electronic speech occurs in the\nbackground, an adult female speaks, a sneeze occurs, and an adult\nfemale speaks once more.\nPrompt: Describe the sound in one or two sentences.\nResponse: The sound is a loud, high-pitched scream of a child,\nwith a hint of a whine, as if the child is crying and screaming at\nthe same time. The sound is accompanied by the muffled voices of\nadults in the background, possibly the child\u2019s parents or caregivers,\nspeaking in a soothing tone.\nTable 7: Example outputs from AnyMAL on unseen audio samples. Gold captions are shown as a\nreference for the audio signals. The text prompt and the audio signals are used as input.\ndemonstrating the importance of feature space alignment. Among the base LLM models, our 70B\nmodel demonstrates the most robust performance, underscoring the influence of substantial reasoning\nproficiency inherent in larger LLMs on tasks involving visual reasoning.\nFigure 4: Training losses on variants (13B and\n70B) of AnyMAL during image-text pre-training.\nDefault parameters: batch size: 2,048, resampler\nlayers: 2, the number of modality tokens: 64. Each\nline features an ablation with one differing param-\neter from the default setting, i.e. bsz16K: 16,384\nbatch size, emb256: 256 visual tokens, depth6: 6\nresampler layers.\nVideo QA benchmarks: We evaluate our model\non three challenging video question-answering\nbenchmarks in Table 6:\nHow2QA [55],\nSTAR [56] and NextQA [57].\nOur model\ndemonstrates competitive results compared to\nthe baselines, and achieves state-of-the-art per-\nformance on the STAR benchmark. Note that\nwe compare against approaches that process\nthe full, untrimmed video clip to generate an-\nswers. Prior work has shown additional im-\nprovements with careful frame-selection strate-\ngies [58]. Our approach is compatible with such\nstrategies, however that is beyond the scope\nof our experiments.\nWe additionally report\nmodel variants trained exclusively on videos\nfrom HowTo100M [59] coupled with text from\nASR transcripts, and using explicit video en-\ncoders (Internvideo [46]) as opposed to im-\nage encoders. However, these models perform\nworse due to the weak alignment of ASR to\nvideo clips and lower diversity in content (i.e.,\ninstructional videos).\nAblations on Hyperparameters:\nFigure 4\nshows the training losses on the variants of Any-\nMAL during image-text pre-training. Due to the\nhigh computational cost of training 70B models, we conduct ablations only on the 13B models. We\nthen use the optimal set of hyperparameters from these experiments as the default configuration for\n9\nIMU Signals Input\nReference Video\nAnyMAL Output\nPrompt: Describe the motion.\nResponse: [Camera wearer] looks around.\nPrompt: Describe the motion.\nResponse: [Camera wearer] walks in the\nhouse.\nPrompt: What\u2019s the safest way to stop?\nResponse: To stop safely on a bike, use the\nbrakes gradually and look where you want\nto go. Lean the bike slightly to one side and\nkeep your body upright.\nTable 8: Example outputs from AnyMAL, on IMU motion sensor samples from Ego4D. The text\nprompt and the IMU signals are used as input. We display a few frames from the synchronized video\nas a reference for readers. The reference video is not used as input to the model.\ntraining the 70B models. Since the trainable parameters remain consistent for both the 13B and 70B\nmodels, we anticipate that the behavior during model training will remain unchanged. The following\nkey hyperparameters are considered: the layers of the Resampler (2 vs. 6), the number of multimodal\ntoken embeddings to represent visual input (64 vs. 256 tokens), and batch size of pre-training (2,048\nvs. 16,384). Overall, we observed that increasing the batch size and the number of visual tokens\nyields minimal improvement. In contrast, increasing the number of resampling layers significantly\nreduces the loss without substantially increasing the training budget.\nEffects of Scaling LLM Parameter Size (70B vs. 13B): The 70B model demonstrates a reduced\ntraining loss overall when compared with the 13B versions. This loss performance also aligns with\nthe downstream task results in Tables 2 and 4. We attribute this result to the inherent reasoning\nabilities and the knowledge assimilated within the 70B models, which expedites the visual concept\nacquisition and alignment process. Overall, the comparison demonstrates the importance of scaling\nLLM parameters in vision-language pre-training as well, which is an aspect that has seldom been\naddressed in existing literature.\nAudio Caption Generation: Table 5 shows the audio captioning results on the AudioCaps [36]\nbenchmark dataset. AnyMAL significantly outperforms other state-of-the-art audio captioning models\nin the literature (e.g. +10.9pp in CIDEr, +5.8pp in SPICE), showing the versatility of the proposed\napproach on various modalities beyond just vision. We note that our 70B model displays notably\nstrong performance compared to the 7B and the 13B variants \u2013 showing the importance of the\nreasoning module for the task.\nIMU Motion Description Generation: We use the Ego4D [38] dataset to train an IMU-aligned\nAnyMAL-7B model, leveraging the synchronized IMU sensor data and textual narrations provided in\nthe dataset. Given that the task of generating textual descriptions from motion signals has not been\npreviously achievable or reported, we solely present the performance achieved by our own model.\nOn the held-out test set, we achieve 52.5 CIDEr and 23.2 ROUGE-L against the ground-truth captions,\nshowing the feasibility of the newly proposed task.\nCombining this captioning ability with the reasoning capabilities of LLMs, in Table 8 we show\nexamples of novel applications that AnyMAL might allow, e.g. inferring user motion states and\nincorporating these as part of its response (e.g. \u201cWhat\u2019s the safest way to stop?\u201d\u2192\u201cTo stop safely on\na bike, ...\u201d without any textual or visual cues that the user is biking).\n4.3\nQualitative Analysis\nComparison with other Vision-Language Models: Table 9 and 10 show outputs from various\nvision-language models [17, 19, 20, 21] on diverse example image and prompt pairs, compared with\nAnyMAL (LLaVA-70B). It can be seen that AnyMAL exhibits strong visual understanding capabilities\n(such as identification of objects and their states), as well as language generation capabilities. While\nMiniGPT4 [21] and LLaVA [20] do present reasonable and fluent responses, their accuracy is not\n10\nExample 1. Creative Writing\nInput Image\nPrompt\nWrite a joke about it.\nBLIP-2\na person holding a lego toy in their hand\nInstructBLIP\na hand holding a small toy in a room\nMini-GPT4\nWhy did the lego man break up with his lego wife? Because he found a better lego\nman.\nOpenFlamingo\nWhat do you get when you cross a snowman with a robot? Answer: A snowbot.\nLLaVA\nA British solider mannequin has lost its way in a New York City apartment. When\nthe surprised tenant, who doesn\u2019t seem to know much about British (...)\nAnyMAL 70B\nHere\u2019s a joke for you: Why did the nutcracker doll go to the doctor? Because he\nwas feeling a little cracked!\nExample 2. How-To\nInput Image\nPrompt\nHow do I fix it?\nBLIP-2\na bike that has been thrown off a cliff and landed on a rock in the middle of a field\nInstructBLIP\ntire\nMini-GPT4\nThe bike has a flat tire. To fix it, you will need to pump up the tire with air. First,\nremove the wheel from the bike and place it on the ground. Then, (...)\nOpenFlamingo\nThis is a flat tire.\nLLaVA\nIt appears that the person biking along the way has experienced a flat tire because\ntheir bike is leaning against a wall, and there is a bike tube lying on the ground.\nTo fix the issue, the person would need to first examine the tire to determine the\nlocation of the flat. They should then remove the damaged tube from the tire. (...)\nAnyMAL 70B\nIt appears that the front tire of one bicycle is flat. You might want to try changing\nthe inner tube or patching any holes in the existing one before re-inflating the tire.\nTable 9: Examples comparing AnyMAL with various baselines: BLIP-2 (FlanT5_XXL) [3], Instruct-\nBLIP (Vicuna-13B) [19], OpenFlamingo-v2-9B [17], Mini-GPT4 [21], and LLaVA [20]. AnyMAL\ndemonstrates competitive performance in object recognition (e.g. \u201cnutcracker\u201d), response relevancy\n(e.g. writing a joke, explaining how-to), and secondary reasoning.\n11\nExample 3. Recommendation\nInput Image\nPrompt\nWhich one would pair better with steak?\nBLIP-2\nred wine or white wine?\nInstructBLIP\nred 1\nMini-GPT4\nThe wine bottles in the image are two different types of wine, one is a red wine and\nthe other is a white wine. Both of these wines would pair well with steak. The red\nwine would be a good choice for a rich and flavorful steak, while the white wine\nwould be a good choice for a lighter steak or seafood dish.\nLLaVA\nFor a steak, the red wine is likely a more appropriate pairing compared to the white\nwine. Whether it is Chianti or Chianti Classico, both options are Italian red wines\nmade from Sangiovese grapes, and their full-bodied nature provides a rich flavor\nprofile, making them good companions to steak dishes. The white wine, Soave\nClassico DOC, is a white grape varietal, which is less traditional for pairing (...)\nAnyMAL 70B\nThe Barolo on the left would pair better with steak. It\u2019s a full-bodied, high tannin\nwine that can stand up to the richness of red meat.\nExample 4. Question and Answering\nInput Image\nPrompt\nHow long does this river go?\nBLIP-2\nit\u2019s a river that runs through the city of florence, tuscany, italy\nInstructBLIP\n10 miles\nMini-GPT4\nThe river goes over the bridge in the image.\nLLaVA\nThe river in the image is quite long, stretching across the city and passing under\nseveral buildings, including a bridge.\nAnyMAL 70B\nThe river in the image is the Arno River, which flows through Florence, Italy.\nIt stretches for approximately 241 kilometers (150 miles) and empties into the\nLigurian Sea.\nTable 10: Examples comparing AnyMAL with various baselines. AnyMAL demonstrates competitive\nperformance in both object recognition (e.g. \u201cBarolo\u201d, \u201cArno River\u201d) and secondary reasoning.\n12\nInput Modality 1\nInput Modality 2\nAnyMAL Output\nPrompt: (Given the motion signals) Write\na social media caption for this view.\nResponse: Pedaling along the San Fran-\ncisco Bay, taking in breathtaking views\nof the Golden Gate Bridge! The salty sea\nbreeze invigorates me as I make my way to\nmy next destination.\nTable 11: Example outputs from AnyMAL, with multiple interleaved modalities as input. The text\nprompt and two other modalities (e.g. image & IMU motion sensor signals) are used as input. The\nunderlined text in the response demonstrates the evidence that the output is grounded on multiple\nmodalities.\nconsistently ensured. These examples effectively highlight the benefits of the proposed approach\nwhich allows for large-scale pre-training covering diverse visual concepts, while inheriting strong\nreasoning capabilities derived from instruction-tuned LLMs.\nWe note that we use the latest checkpoints made available for each baseline to generate responses.\nInterleaved Modalities: The flexible model architecture of AnyMAL allows for combinatory\nmodalities as conditioning context (e.g. image + IMU motion sensor signals), which allows for more\ncomprehensive multimodal reasoning. We demonstrate the model\u2019s zeroshot capabilities of handling\nsuch interleaved modalities in Table 11 (e.g. composing a message with a given image (Golden Gate\nBridge), with the user\u2019s prevalent motion (biking) as part of the context).\nThis result illustrates the new and natural way of interaction with an AI model made possible by\nAnyMAL, wherein a user can presume a shared understanding of combined sensory perceptions (e.g.\nvisual, auditory, and motion cues) when composing queries \u2013 avoiding the need to specify multimodal\ncontexts.\n5\nSafety\nInference Time Integrity. To ensure the safety and integrity of the AnyMAL model, several measures\nare made on the following categories of potential integrity violations: (1) input images, (2) input text\nprompts, (3) text outputs, and (4) multimodal combination of input images and text outputs.\n(1) Input image: we use a pre-trained image classifier based on RegNetY [60] to detect any\ncontent that violates integrity standards. This detection encompasses graphic material,\nviolent imagery, hate symbols, instances of bullying, harassment, etc. If such a violation is\nidentified within the image, we proceed to reject the entire query.\n(2) Input text prompt: we use a RoBERTa-based text classifier [61] trained to detect integrity-\nviolating utterances such as violence, harassment, hate speech, etc. When a violation is\ndetected in user prompt, we proceed to reject the entire query.\n(3) Output text: we employ the same text classifier in (b) to detect any problems within the\ngenerated output. For streaming use cases, we run the classifier for each sentence to promptly\nidentify any violations.\n(4) Multimodal association in input image & output text: in the uncommon scenario where\nharmless text and a harmless image (which individually appear innocuous) can result in a\nproblem when they are associated, we use a multimodal classifier to detect such instances.\nTraining Time Safety. The datasets used for pre-training (e.g. [34, 62]) have gone through a\nfiltration process to remove harmful language or images that compromise integrity, thereby reducing\nthe potential for the model to generate content that violates integrity standards.\nLLM Safety. Since our AnyMAL pre-training does not alter the parameters of the base LLM,\nwe carry over the same safety precautions implemented for its language generation. For instance,\nLLaMA-2 (the version we report most of our results on) places safeguards such as negative example\nfine-tuning, reinforcement learning with human feedback (RLHF) [63, 64, 65].\n13\n6\nConclusions\nOur proposed AnyMAL showcases a novel and natural way of interacting with an AI model, e.g.\nasking questions that presume a shared understanding of the world between the user and the agent,\nthrough the same lens and combinatory perceptions (e.g. visual, auditory, and motion cues). The\nproposed scalable way of training AnyMAL makes it possible to leverage the powerful reasoning\ncapabilities of the LLaMA-2 language model within the multimodal settings.\nOur contributions are as follows: (1) We present a large-scale Multimodal LLM (AnyMAL), trained\nusing open-sourced resources and scalable solutions for multiple modalities. (2) We introduce the\nMultimodal Instruction Tuning dataset (MM-IT), a first-of-its-kind collection of high-quality manual\nannotations of multimodal instruction data. (3) Our comprehensive empirical analysis shows insights\nto the efficient and scalable recipe for building a multimodal reasoning model, given various LLMs\nand modeling choices.\n7\nLimitations\nWe discuss the current limitations of our work as follows.\nFirst, the proposed causal multimodal language modeling approach still encounters challenges in\nestablishing a robust grounding with the input modality. Specifically, we observe that during the\ngeneration, the model occasionally prioritizes focusing more on the generated text rather than the\ninput image. This leads to the generation of output that incorporates biases acquired from the\nunderlying language model (LLM), which can incur inaccuracies when compared against the image\ncontext. We expect that additional architectural adjustments or unfreezing LLM parameters are\nnecessary to address this limitation effectively (albeit the much higher computational costs it might\nentail).\nSecond, while we greatly increase the size of the pretraining dataset, the understanding of visual\nconcepts and entities remains constrained by the quantity of paired image-text data included in\nthe training process. In the domain of text-only language models, it is commonly observed that\napproaches incorporating external knowledge retrieval significantly enhance the model\u2019s ability\nto overcome its knowledge limitations. These approaches offer a potential means to alleviate the\nlimitations mentioned earlier.\nLastly, in the scope of our work, the multimodal adaptation of an LLM is bounded by four modalities:\nimage, video, audio, and IMU signals. While we believe that the proposed approach has the potential\nto encompass any other modality, provided there exists a paired dataset, its effectiveness for such\nmodalities still needs to be substantiated.\nReferences\n[1] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill, \u201cMultimodal\nfew-shot learning with frozen language models,\u201d Advances in Neural Information Processing\nSystems, vol. 34, pp. 200\u2013212, 2021.\n[2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al., \u201cFlamingo: a visual language model for few-shot learning,\u201d Advances in\nNeural Information Processing Systems, vol. 35, pp. 23716\u201323736, 2022.\n[3] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models,\u201d arXiv preprint arXiv:2301.12597, 2023.\n[4] OpenAI, \u201cGpt-4 technical report,\u201d ArXiv, vol. abs/2303.08774, 2023.\n[5] H. Lauren\u00e7on, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karam-\ncheti, A. M. Rush, D. Kiela, M. Cord, and V. Sanh, \u201cObelics: An open web-scale filtered dataset\nof interleaved image-text documents,\u201d 2023.\n[6] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d arXiv\npreprint arXiv:2307.09288, 2023.\n[7] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., \u201cLanguage models are\nunsupervised multitask learners,\u201d 2019.\n14\n[8] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani,\nS. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, S. Narang,\nG. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin,\nA. Roberts, D. Zhou, Q. V. Le, and J. Wei, \u201cScaling instruction-finetuned language models,\u201d\n2022.\n[9] B. Wang and A. Komatsuzaki, \u201cGPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel.\u201d https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n[10] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li,\nX. V. Lin, et al., \u201cOpt: Open pre-trained transformer language models,\u201d arXiv preprint\narXiv:2205.01068, 2022.\n[11] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv\npreprint arXiv:2302.13971, 2023.\n[12] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.\nGonzalez, I. Stoica, and E. P. Xing, \u201cVicuna: An open-source chatbot impressing gpt-4 with\n90%* chatgpt quality,\u201d March 2023.\n[13] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio,\n\u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in International\nconference on machine learning, pp. 2048\u20132057, PMLR, 2015.\n[14] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh, \u201cVQA:\nVisual question answering,\u201d in ICCV, 2015.\n[15] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and D. Batra, \u201cVisual\ndialog,\u201d in CVPR, 2017.\n[16] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and\nA. van den Hengel, \u201cVision-and-language navigation: Interpreting visually-grounded navigation\ninstructions in real environments,\u201d in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 3674\u20133683, 2018.\n[17] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre,\nS. Sagawa, et al., \u201cOpenflamingo: An open-source framework for training large autoregressive\nvision-language models,\u201d arXiv preprint arXiv:2308.01390, 2023.\n[18] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke,\nK. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence, \u201cPalm-e: An\nembodied multimodal language model,\u201d in arXiv preprint arXiv:2303.03378, 2023.\n[19] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, \u201cInstructblip:\nTowards general-purpose vision-language models with instruction tuning,\u201d 2023.\n[20] H. Liu, C. Li, Q. Wu, and Y. J. Lee, \u201cVisual instruction tuning,\u201d 2023.\n[21] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, \u201cMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models,\u201d arXiv preprint arXiv:2304.10592, 2023.\n[22] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, \u201cOtter: A multi-modal model with\nin-context instruction tuning,\u201d arXiv preprint arXiv:2305.03726, 2023.\n[23] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al., \u201cmplug-\nowl: Modularization empowers large language models with multimodality,\u201d arXiv preprint\narXiv:2304.14178, 2023.\n[24] T. Gong, C. Lyu, S. Zhang, Y. Wang, M. Zheng, Q. Zhao, K. Liu, W. Zhang, P. Luo, and\nK. Chen, \u201cMultimodal-gpt: A vision and language model for dialogue with humans,\u201d arXiv\npreprint arXiv:2305.04790, 2023.\n15\n[25] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu, C. He, X. Yue, H. Li,\nand Y. Qiao, \u201cLlama-adapter v2: Parameter-efficient visual instruction model,\u201d arXiv preprint\narXiv:2304.15010, 2023.\n[26] H. Zhang, X. Li, and L. Bing, \u201cVideo-llama: An instruction-tuned audio-visual language model\nfor video understanding,\u201d arXiv preprint arXiv:2306.02858, 2023.\n[27] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, \u201cPandagpt: One model to instruction-follow\nthem all,\u201d arXiv preprint arXiv:2305.16355, 2023.\n[28] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, and Z. Tu, \u201cMacaw-llm: Multi-\nmodal language modeling with image, audio, video, and text integration,\u201d arXiv preprint\narXiv:2306.09093, 2023.\n[29] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu, X. Sun, et al., \u201cMit:\nA large-scale dataset towards multi-modal multilingual instruction tuning,\u201d arXiv preprint\narXiv:2306.04387, 2023.\n[30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al., \u201cLearning transferable visual models from natural language supervi-\nsion,\u201d in International Conference on Machine Learning (ICML), 2021.\n[31] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\nA. Katta, C. Mullis, M. Wortsman, et al., \u201cLaion-5b: An open large-scale dataset for training\nnext generation image-text models,\u201d Advances in Neural Information Processing Systems,\nvol. 35, pp. 25278\u201325294, 2022.\n[32] Y. Wu*, K. Chen*, T. Zhang*, Y. Hui*, T. Berg-Kirkpatrick, and S. Dubnov, \u201cLarge-scale con-\ntrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,\u201d\nin IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2023.\n[33] S. Moon, A. Madotto, Z. Lin, A. Dirafzoon, A. Saraf, A. Bearman, and B. Damavandi,\n\u201cImu2clip: Multimodal contrastive learning for imu motion sensors from egocentric videos and\ntext,\u201d arXiv preprint arXiv:2210.14395, 2022.\n[34] F. Radenovic, A. Dubey, A. Kadian, T. Mihaylov, S. Vandenhende, Y. Patel, Y. Wen, V. Ra-\nmanathan, and D. Mahajan, \u201cFiltering, distillation, and hard negatives for vision-language\npre-training,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 6967\u20136977, 2023.\n[35] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal,\nand M. Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in Proc.\nIEEE ICASSP 2017, (New Orleans, LA), 2017.\n[36] C. D. Kim, B. Kim, H. Lee, and G. Kim, \u201cAudiocaps: Generating captions for audios in the\nwild,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pp. 119\u2013132, 2019.\n[37] K. Drossos, S. Lipping, and T. Virtanen, \u201cClotho: An audio captioning dataset,\u201d in ICASSP 2020-\n2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 736\u2013740, IEEE, 2020.\n[38] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang,\nM. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan,\nJ. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do,\nM. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, C. Fuegen, A. Gebreselasie,\nC. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar,\nF. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu,\nW. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Southerland, Y. Sugano,\nR. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M.\nFarinella, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe,\nA. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan,\nand J. Malik, \u201cEgo4d: Around the World in 3,000 Hours of Egocentric Video,\u201d in IEEE/CVF\nComputer Vision and Pattern Recognition (CVPR), 2022.\n16\n[39] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott,\nS. Shleifer, A. Desmaison, C. Balioglu, P. Damania, B. Nguyen, G. Chauhan, Y. Hao, A. Math-\news, and S. Li, \u201cPytorch fsdp: Experiences on scaling fully sharded data parallel,\u201d 2023.\n[40] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, \u201cQlora: Efficient finetuning of\nquantized llms,\u201d arXiv preprint arXiv:2305.14314, 2023.\n[41] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \u201cLora:\nLow-rank adaptation of large language models,\u201d arXiv preprint arXiv:2106.09685, 2021.\n[42] L. Yu, B. Shi, R. Pasunuru, B. Miller, O. Golovneva, T. Wang, A. Babu, B. Tang, B. Karrer,\nS. Sheynin, C. Ross, A. Polyak, R. Howes, V. Sharma, J. Xu, U. Singer, D. Li, G. Ghosh,\nY. Taigman, M. Fazel-Zarandi, A. Celikyilmaz, L. Zettlemoyer, and A. Aghajanyan, \u201cScaling\nautoregressive multi-modal models: Pretraining and instruction tuning,\u201d 2023.\n[43] X. Xu, H. Dinkel, M. Wu, Z. Xie, and K. Yu, \u201cInvestigating local and global information for\nautomated audio captioning with transfer learning,\u201d in ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 905\u2013909, IEEE, 2021.\n[44] X. Mei, X. Liu, Q. Huang, M. D. Plumbley, and W. Wang, \u201cAudio captioning transformer,\u201d\narXiv preprint arXiv:2107.09817, 2021.\n[45] X. Liu, X. Mei, Q. Huang, J. Sun, J. Zhao, H. Liu, M. D. Plumbley, V. Kilic, and W. Wang,\n\u201cLeveraging pre-trained bert for audio captioning,\u201d in 2022 30th European Signal Processing\nConference (EUSIPCO), pp. 1145\u20131149, IEEE, 2022.\n[46] Y. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu, Z. Wang, et al.,\n\u201cInternvideo: General video foundation models via generative and discriminative learning,\u201d\narXiv preprint arXiv:2212.03191, 2022.\n[47] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models,\u201d arXiv preprint arXiv:2301.12597, 2023.\n[48] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick,\n\u201cMicrosoft coco: Common objects in context,\u201d in ECCV, 2014.\n[49] D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine, \u201cThe\nhateful memes challenge: Detecting hate speech in multimodal memes,\u201d Advances in neural\ninformation processing systems, vol. 33, pp. 2611\u20132624, 2020.\n[50] A. Singh, V. Natarjan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach, \u201cTowards vqa\nmodels that can read,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 8317\u20138326, 2019.\n[51] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan,\n\u201cLearn to explain: Multimodal reasoning via thought chains for science question answering,\u201d\nAdvances in Neural Information Processing Systems, vol. 35, pp. 2507\u20132521, 2022.\n[52] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, \u201cVizwiz\ngrand challenge: Answering visual questions from blind people,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 3608\u20133617, 2018.\n[53] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, \u201cOk-vqa: A visual question answering\nbenchmark requiring external knowledge,\u201d in Proceedings of the IEEE/cvf conference on\ncomputer vision and pattern recognition, pp. 3195\u20133204, 2019.\n[54] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez,\nD. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li,\nW. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal,\nP. Labatut, A. Joulin, and P. Bojanowski, \u201cDinov2: Learning robust visual features without\nsupervision,\u201d 2023.\n[55] L. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, and J. Liu, \u201cHero: Hierarchical encoder for video+\nlanguage omni-representation pre-training,\u201d arXiv preprint arXiv:2005.00200, 2020.\n17\n[56] B. Wu, S. Yu, Z. Chen, J. B. Tenenbaum, and C. Gan, \u201cStar: A benchmark for situated reasoning\nin real-world videos,\u201d in Thirty-fifth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2), 2021.\n[57] J. Xiao, X. Shang, A. Yao, and T.-S. Chua, \u201cNext-qa: Next phase of question-answering to\nexplaining temporal actions,\u201d in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 9777\u20139786, 2021.\n[58] S. Yu, J. Cho, P. Yadav, and M. Bansal, \u201cSelf-chained image-language model for video localiza-\ntion and question answering,\u201d arXiv preprint arXiv:2305.06988, 2023.\n[59] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, \u201cHowto100m: Learning\na text-video embedding by watching hundred million narrated video clips,\u201d in Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 2630\u20132640, 2019.\n[60] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll\u00e1r, \u201cDesigning network design\nspaces,\u201d 2020.\n[61] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V. Stoyanov, \u201cRoberta: A robustly optimized bert pretraining approach,\u201d arXiv preprint\narXiv:1907.11692, 2019.\n[62] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual,\nO. Gafni, et al., \u201cMake-a-video: Text-to-video generation without text-video data,\u201d arXiv\npreprint arXiv:2209.14792, 2022.\n[63] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, \u201cDeep reinforcement\nlearning from human preferences,\u201d Advances in neural information processing systems, vol. 30,\n2017.\n[64] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon, et al., \u201cConstitutional ai: Harmlessness from ai feedback,\u201d arXiv preprint\narXiv:2212.08073, 2022.\n[65] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, \u201cDirect\npreference optimization: Your language model is secretly a reward model,\u201d arXiv preprint\narXiv:2305.18290, 2023.\n[66] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L.\nScao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, \u201cTransformers: State-of-the-art natural\nlanguage processing,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, Oct. 2020.\n[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \u201cPytorch: An imperative style,\nhigh-performance deep learning library,\u201d in Neural Information Processing Systems, 2019.\n18\nA\nHuman Evaluation\nTo measure if the AnyMAL response is relevant and correct with respect to the query and the image,\nwe use the following rubrics to collect human feedback data.\nRecognition Accuracy: Did the model correctly understand and identify the objects or scenarios in\nthe image relevant to the query?\n\u2022 0: The model completely misinterprets the image or fails to recognize key objects in the\nimage relevant to the query.\n\u2022 1: The model partially misinterprets the image or recognizes some but not all key elements.\n\u2022 2: The model correctly interprets the image and identifies key elements.\nResponse Accuracy: Does the response provide factually correct information? Does it Hallucinate?\n\u2022 0: The response contains factually incorrect information or even partially incorrect informa-\ntion.\n\u2022 2: The response provides factually accurate information.\nIntegrity: Does the response show any harmful or offensive language or signs of unjust or prejudiced\ntreatment related to characteristics such as race, age, gender, nationality, etc.?\n\u2022 0: The response shows some sign of bias or harmful/offensive language.\n\u2022 2: The response shows no signs of bias or harmful/offensive language.\nPairwise Comparison: choose the response that better answers the given question, and that is more\ncontextually appropriate and accurate (win, tie, or lose).\nWe plan to explore further training the model with the human feedback data (e.g. RLHF).\nB\nAdditional Notes on Experiments\nDifferent prompts were used to get the model output in the desired format for each task (e.g. multiple\nchoice questions, yes/no questions). Below is the full list of prompts used for each task.\nB.1\nMultimodal Prompts\nMM-IT\nSystem message: \u201cYou are a multimodal assistant, designed to provide helpful answers to\nusers\u2019 image-related questions. \\n\\n Here is the image: <img>\". User message: \u201c{question}\"\nVQA, TextVQA, OKVQA\nSystem message: \u201cYou are a multimodal assistant, designed to provide\ndirect answers to users\u2019 image-related questions. Reply directly with only one phrase. *Do not*\nstart your answer with \u2018Sure ...\u2019. \\n\\n Here is the image: <img>\". User message: \u201cIn the image,\n{question} Reply in one word.\nVizWiz\nSystem message: \u201cAnswer the questions based on the image when possible, otherwise say\n\u2018unanswerable\u2018. \\n\\n Here is the image: <img>\". User message: \u201cIn the image, {question} Reply in\none prahse/word or say \u2018unanswerable\u2018\nHateful Meme\nSystem message: \u201cYou are a social media content moderator, designed to detect\nhateful memes. \\n\\n Here is the meme: <img>\\n This meme contains text: \u2018{ocr}\u2019\". User message:\n\u201cIs this a hateful meme? Answer yes or no.\nCoco Caption\nSystem message: \u201cYou are a multimodal assistant, designed to provide direct and\nconcise answers to users\u2019 image-related requests. \\n\\n Here is the image: <img>\". User message:\n\u201cDescribe the image with one *generic* sentence using json format. Here are two examples:\\n\nSpecific: {\"caption\": \"Body-Solid (Best Fitness) Inversion Table-2\"} \\n Generic: {\"caption\": \"A man\nlaying on top of an exercise table.}.\nScienceQA\nSystem message: \u201cGiven the image, choose the correct option for the following question.\nYour response must be just a single letter that corresponds to the correct option (e.g. A, B) \\n\\n\nHere is the image: <img>.\" User message: \u201c{context} Question: {question} \\n\\n Options: {choices}\n\\n\\n Reply in a single letter.\"\n19\nFigure 5: AnyMAL Inference example with multiple modality as input.\nAudioCap\nSystem message: \u201cYou are a multimodal assistant. Designed to provide direct answers\nto users\u2019 audio-related questions. Here is the audio: <audio>\" User message: \u201cDescribe the sound.\"\nSTAR, How2QA, NextQA\nSystem message: \u201cYou are a multimodal assistant. Designed to provide\ndirect answers to users\u2019 video-related questions. \\n\\n Here is the video: <video>.\" User message:\n{question} Select exactly one option from the following: [options].\nIMU-Ego4d\nSystem message: \u201c\"You are a multimodal assistant, designed to provide helpful,\nconcise and direct answers to users\u2019 questions, based on the user\u2019s motion sensor signals reading\nfrom a head-mounted IMU device. The signals may indicate that a user may be running, walking,\nbiking, driving, looking around, etc. Always answer under 30 words. \\n\\n Here are the user\u2019s\npredicted motions: <IMU>\" User message: \u201cDescribe this motion.\"\nB.2\nMultimodal Inputs\nFigure 5 shows the diagram for performing modality-interleaved inference (for examples shown in\nTable 11).\nB.3\nHyperparameters\nPre-training:\nTable 12 report the hyper-parameters used in this work for model pre-training.\nModels\nBatch Size\nInitial LR\n# Steps\n# Modality\nProjection Module (#Layers)\nEmbeddings\nAnyMAL (13B, Image)\n2048\n2 \u00d7 10\u22124\n100k\n64\nResampler (6)\nAnyMAL (70B, Image)\n2048\n2 \u00d7 10\u22124\n100k\n64\nResampler (6)\nAnyMAL (13B, Audio)\n128\n1 \u00d7 10\u22124\n1k\n32\nLinear (1)\nAnyMAL (70B, Audio)\n128\n1 \u00d7 10\u22124\n1k\n32\nLinear (1)\nAnyMAL (13B, Video)\n1024\n1 \u00d7 10\u22124\n20k\n32\nResampler (4)\nAnyMAL (70B, Video)\n1024\n1 \u00d7 10\u22124\n20k\n32\nResampler (4)\nAnyMAL (7B, IMU)\n256\n1 \u00d7 10\u22124\n2k\n32\nLinear (1)\nTable 12: Hyperparameters for AnyMAL Pre-training\nFinetuning:\nWe finetune both resampler and LoRA adapters on MM-IT training set with the prompt\ndescribed in B.1. Following the QLoRA [40] work, we set LoRA r = 64, \u03b1 = 16, and add LoRA\nmodules on all linear layers. We finetune the model for 3k steps with batzh size 128, initial LR\n1 \u00d7 10\u22125.\n20\nB.4\nCode Base & Hardware\nThe implementations of the transformer-based models are extended from the HuggingFace2 code\nbase [66] and other cited authors\u2019 released code-bases. Our entire code-base is implemented in\nPyTorch [67]. All models in this work are trained on a varying number of Nvidia A100 3 GPUs.\nB.5\nMore Qualitative Examples\nWe provide more qualitative examples in Table 13 and 14, comparing outputs from various baselines.\nWhile some of the baselines do present reasonable and fluent responses, their accuracy is not\nconsistently ensured, either in their visual understanding (e.g. what objects are present in an image)\nor secondary reasoning (e.g. analyzing whether there is any element in an image that violates integrity\npolicy).\n2https://github.com/huggingface/transformers\n3https://www.nvidia.com/en-us/data-center/a100/\n21\nExample 5. Recommendation - Fashion\nInput Image\nPrompt\nWhat jacket would go well with this?\nBLIP-2\nblue shirt, black pants, grey shoes\nInstructBLIP\nblack\nMini-GPT4\nA light blue shirt would go well with this outfit.\nOpenFlamingo\nThis jacket would go well with this outfit.\nLLaVA\nWith the man wearing a blue shirt and a pair of black pants, a solid-colored\nblazer, such as a navy or black one, would go well with the outfit.\nAnyMAL 70B\nA navy blazer would look great with this shirt. The navy color will complement\nthe shirt\u2019s blue hue, and the blazer will add a more formal touch to the outfit.\nExample 6. Multimodal Integrity Violation Detection\nInput Image\nPrompt\nIs this meme hateful?\nBLIP-2\nno\nInstructBLIP\nno\nMini-GPT4\nNo, it is not hateful. It is a joke that is meant to be humorous and not offensive.\nOpenFlamingo\nNo, this meme is not hateful.\nLLaVA\nNo, the meme is not hateful. It is a humorous and lighthearted image that\nfeatures a toilet with a sign that reads \"Hey Friend, Find a Perfect Home for\nYou.\" The meme is intended to be amusing and relatable, as it highlights the\nimportance of finding a comfortable and suitable living space. The image is not\nintended to offend or belittle anyone, but rather to bring a smile to the viewer\u2019s\nface.\nAnyMAL 70B\nThe meme may offend the recipient by associating a toilet in a bathroom as a\nperfect home that the recipient should live in.\nTable 13: More examples comparing AnyMAL with various baselines: BLIP-2 (FlanT5_XXL) [3],\nInstructBLIP (Vicuna-13B) [19], OpenFlamingo-v2-9B [17], Mini-GPT4 [21], and LLaVA [20].\n22\nExample 7. Inspiration - Recipe\nInput Image\nPrompt\nWhat cocktails can I make with these ingredients?\nBLIP-2\ngin margarita gin margarita gin margarita\nInstructBLIP\nThere are several cocktails that can be made with the ingredients in the image,\nincluding a classic Margarita, a Spritz, and a Gimlet. The Margarita is a classic\ncocktail made with tequila, lime juice, and triple sec, while the Spritz is a\nrefreshing drink made with prosecco, Aperol, and soda water. (...)\nMini-GPT4\nThere are many cocktails that can be made with these ingredients. Here are a\nfew examples:\nOpenFlamingo\nYou can make a margarita, a cosmopolitan, and a whiskey sour.\nLLaVA\nWith the items on the counter, you can make several cocktails. Some of the\ningredients include a bottle of vodka, orange, apple, champagne, cognac, and\nwine. These can be used to create various cocktails like the following: (...)\nAnyMAL 70B\nHere are some ideas: 1) A classic vodka sour. 2) An orange blossom martini,\nwith vodka, triple sec, and freshly-squeezed orange juice.\nExample 8. Detailed Description\nInput Image\nPrompt\nDescribe this image in detail.\nBLIP-2\na group of people sit around a campfire in the woods\nInstructBLIP\nThe image depicts a group of people gathered around a campfire in a forest.\nThere are six people sitting in chairs, with some standing nearby. There are\nseveral chairs placed in a semi-circle formation, with some people sitting on\nthem and others standing nearby.\nMini-GPT4\nThis image shows a group of people sitting around a campfire in a forest. They\nare all wearing warm clothing and hats, and some of them are holding cups of\nhot drinks. The forest is filled with tall trees and the sky is clear and blue. The\nground is covered in leaves and pine needles.\nOpenFlamingo\nThis image shows a group of friends sitting around a campfire.\nLLaVA\nIn this image, nine people, including three young children, aged 4, 5, and 11,\nare sitting in various chairs around an outdoor campfire, enjoying each other\u2019s\ncompany.\nAnyMAL 70B\nFour people are gathered around a campfire in the woods. They\u2019re all wearing\ncasual outdoor clothing and sitting on chairs. There\u2019s a tent pitched nearby, and\nvarious camping gear scattered about.\nTable 14: More examples comparing AnyMAL with various baselines: BLIP-2 (FlanT5_XXL) [3],\nInstructBLIP (Vicuna-13B) [19], OpenFlamingo-v2-9B [17], Mini-GPT4 [21], and LLaVA [20].\n23\n"
  },
  {
    "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation",
    "link": "https://arxiv.org/pdf/2309.16653.pdf",
    "upvote": "41",
    "text": "DREAMGAUSSIAN: GENERATIVE GAUSSIAN SPLAT-\nTING FOR EFFICIENT 3D CONTENT CREATION\nJiaxiang Tang1, Jiawei Ren2, Hang Zhou3, Ziwei Liu2, Gang Zeng1\n1Key Lab. of Machine Perception (MoE), School of IST, Peking University.\n2S-Lab, Nanyang Technological University.\n3Baidu Inc.\nhttps://dreamgaussian.github.io/\n\u201cA photo of an \nice cream\u201d\n\u201cA photo of a \nhamburger\u201d\nInput\nGenerated 3D Model\nMesh\nStage 1                       5s                     10s                     20s                  40s                   60s\nStage 2   \n10s               20s                  30s\nOptimization Progress\nFigure 1: DreamGaussian aims at accelerating the optimization process of both image- and text-to-\n3D tasks. We are able to generate a high quality textured mesh in several minutes.\nABSTRACT\nRecent advances in 3D content creation mostly leverage optimization-based 3D\ngeneration via score distillation sampling (SDS). Though promising results have\nbeen exhibited, these methods often suffer from slow per-sample optimization,\nlimiting their practical usage. In this paper, we propose DreamGaussian, a novel\n3D content generation framework that achieves both efficiency and quality simul-\ntaneously. Our key insight is to design a generative 3D Gaussian Splatting model\nwith companioned mesh extraction and texture refinement in UV space. In con-\ntrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate\nthat the progressive densification of 3D Gaussians converges significantly faster\nfor 3D generative tasks. To further enhance the texture quality and facilitate down-\nstream applications, we introduce an efficient algorithm to convert 3D Gaussians\ninto textured meshes and apply a fine-tuning stage to refine the details. Exten-\nsive experiments demonstrate the superior efficiency and competitive generation\nquality of our proposed approach. Notably, DreamGaussian produces high-quality\ntextured meshes in just 2 minutes from a single-view image, achieving approxi-\nmately 10 times acceleration compared to existing methods.\n1\narXiv:2309.16653v1  [cs.CV]  28 Sep 2023\n1\nINTRODUCTION\nAutomatic 3D digital content creation finds applications across various domains, including digital\ngames, advertising, films, and the MetaVerse. The core techniques, including image-to-3D and\ntext-to-3D, offer substantial advantages by significantly reducing the need for manual labor among\nprofessional artists and empowering non-professional users to engage in 3D asset creation. Drawing\ninspiration from recent breakthroughs in 2D content generation (Rombach et al., 2022), the field\nof 3D content creation has experienced rapid advancements. Recent studies in 3D creation can be\nclassified into two principal categories: inference-only 3D native methods and optimization-based\n2D lifting methods. Theoretically, 3D native methods (Jun & Nichol, 2023; Nichol et al., 2022;\nGupta et al., 2023) exhibit the potential to generate 3D-consistent assets within seconds, albeit at the\ncost of requiring extensive training on large-scale 3D datasets. The creation of such datasets demand\nsubstantial human effort, and even with these efforts, they continue to grapple with issues related to\nlimited diversity and realism (Deitke et al., 2023b;a; Wu et al., 2023).\nOn the other hand, Dreamfusion (Poole et al., 2022) proposes Score Distillation Sampling (SDS) to\naddress the 3D data limitation by distilling 3D geometry and appearance from powerful 2D diffusion\nmodels (Saharia et al., 2022), which inspires the development of recent 2D lifting methods (Lin et al.,\n2023; Wang et al., 2023b; Chen et al., 2023b). In order to cope with the inconsistency and ambiguity\ncaused by the SDS supervision, Neural Radiance Fields (NeRF) (Mildenhall et al., 2020) are usually\nadopted for their capability in modeling rich 3D information. Although the generation quality has\nbeen increasingly improved, these approaches are notorious for hours-long optimization time due to\nthe costly NeRF rendering, which restricts them from being deployed to real-world applications at\nscale. We argue that the occupancy pruning technique used to accelerate NeRF (M\u00a8uller et al., 2022;\nSara Fridovich-Keil and Alex Yu et al., 2022) is ineffective in generative settings when supervised\nby the ambiguous SDS loss as opposed to reconstruction settings.\nIn this work, we introduce the DreamGaussian framework, which greatly improves the 3D con-\ntent generation efficiency by refining the design choices in an optimization-based pipeline. Photo-\nrealistic 3D assets with explicit mesh and texture maps can be generated from a single-view image\nwithin only 2 minutes using our method. Our core design is to adapt 3D Gaussian Splatting (Kerbl\net al., 2023) into the generative setting with companioned meshes extraction and texture refinement.\nCompared to previous methods with the NeRF representation, which find difficulties in effectively\npruning empty space, our generative Gaussian splatting significantly simplifies the optimization\nlandscape. Specifically, we demonstrate the progressive densification of Gaussian splitting, which\nhighly in accordance with the optimization progress of generative settings, greatly improves the\ngeneration efficiency. As illustrated in Figure 1, our image-to-3D pipeline swiftly produces a coarse\nshape within seconds and converges efficiently in around 500 steps on a single GPU.\nDue to the ambiguity in SDS supervision and spatial densification, the directly generated results\nfrom 3D Gaussians tend to be blurry. To address the issue, we identify that the texture needs to be\nrefined explicitly, which requires delicate textured polygonal mesh extraction from the generated 3D\nGaussians. While this task has not been explored before, we design an efficient algorithm for mesh\nextraction from 3D Gaussians by local density querying. Then a generative UV-space refinement\nstage is proposed to enhance the texture details. Given the observation that directly applying the\nlatent space SDS loss as in the first stage results in over-saturated blocky artifacts on the UV map,\nwe take the inspiration from diffusion-based image editing methods (Meng et al., 2021) and perform\nimage space supervision. Compared to previous texture refinement approaches, our refinement stage\nachieves better fidelity while keeping high efficiency.\nIn summary, our contributions are:\n1. We propose a novel framework for 3D content creation by adapting Gaussian splatting into\ngenerative settings, significantly reducing the generation time of optimization-based 2D\nlifting methods.\n2. We design an efficient mesh extraction algorithm from 3D Gaussians and a UV-space tex-\nture refinement stage to further enhance the generation quality.\n3. Extensive experiments on both Image-to-3D and Text-to-3D tasks demonstrate that our\nmethod effectively balances optimization time and generation fidelity, unlocking new pos-\nsibilities for real-world deployment of 3D content generation.\n2\n2\nRELATED WORK\n2.1\n3D REPRESENTATIONS\nVarious 3D representations have been proposed for different 3D tasks. Neural Radiance Fields\n(NeRF) (Mildenhall et al., 2020) employs a volumetric rendering and has been popular for enabling\n3D optimization with only 2D supervision. Although NeRF has become widely used in both 3D\nreconstruction (Barron et al., 2022; Li et al., 2023c; Chen et al., 2022; Hedman et al., 2021) and\ngeneration (Poole et al., 2022; Lin et al., 2023; Chan et al., 2022), optimizing NeRF can be time-\nconsuming. Various attempts have been made to accelerate the training of NeRF (M\u00a8uller et al.,\n2022; Sara Fridovich-Keil and Alex Yu et al., 2022), but these works only focus on the reconstruction\nsetting. The common technique of spatial pruning fails to accelerate the generation setting. Recently,\n3D Gaussian splatting (Kerbl et al., 2023) has been proposed as an alternative 3D representation to\nNeRF, which has demonstrated impressive quality and speed in 3D reconstruction (Luiten et al.,\n2023). The efficient differentiable rendering implementation and model design enables fast training\nwithout relying on spatial pruning. In this work, we for the first time adapt 3D Gaussian splatting\ninto generation tasks to release the potential of optimization-based methods.\n2.2\nTEXT-TO-3D GENERATION\nText-to-3D generation aims at generating 3D assets from a text prompt. Recently, data-driven 2D\ndiffusion models have achieved notable success in text-to-image generation (Ho et al., 2020; Rom-\nbach et al., 2022; Saharia et al., 2022). However, transferring it to 3D generation is non-trivial due\nto the challenge of curating large-scale 3D datasets. Existing 3D native diffusion models usually\nwork on a single object category and suffer from limited diversity (Jun & Nichol, 2023; Nichol\net al., 2022; Gupta et al., 2023; Lorraine et al., 2023; Zhang et al., 2023; Zheng et al., 2023; Ntavelis\net al., 2023; Chen et al., 2023a; Cheng et al., 2023; Gao et al., 2022). To achieve open-vocabulary\n3D generation, severl methods propose to lift 2D image models for 3D generation (Jain et al., 2022;\nPoole et al., 2022; Wang et al., 2023a; Mohammad Khalid et al., 2022; Michel et al., 2022). Such 2D\nlifting methods optimize a 3D representation to achieve a high likelihood in pretrained 2D diffusion\nmodels when rendered from different viewpoints, such that both 3D consistency and realisticity can\nbe ensured. Following works continue to enhance various aspects such as generation fidelity and\ntraining stability (Lin et al., 2023; Tsalicoglou et al., 2023; Zhu & Zhuang, 2023; Yu et al., 2023;\nLi et al., 2023b; Chen et al., 2023c; Wang et al., 2023b; Huang et al., 2023; Metzer et al., 2022;\nChen et al., 2023b), and explore further applications (Zhuang et al., 2023; Singer et al., 2023; Raj\net al., 2023). However, these optimization-based 2D lifting approaches usually suffer from long\nper-case optimization time. Particularly, employing NeRF as the 3D representation leads to expen-\nsive computations during both forward and backward. In this work, we choose 3D Gaussians as the\ndifferentiable 3D representation and empirically show that it has a simpler optimization landscape.\n2.3\nIMAGE-TO-3D GENERATION\nImage-to-3D generation targets generating 3D assets from a reference image. The problem can also\nbe formulated as single-view 3D reconstruction (Yu et al., 2021; Trevithick & Yang, 2021; Duggal &\nPathak, 2022), but such reconstruction settings usually produce blurry results due to the lack of un-\ncertainty modeling. Text-to-3D methods can also be adapted for image-to-3D generation (Xu et al.,\n2023a; Tang et al., 2023b; Melas-Kyriazi et al., 2023) using image captioning models (Li et al., 2022;\n2023a). Recently, Zero-1-to-3 (Liu et al., 2023b) explicitly models the camera transformation into\n2D diffusion models and enable zero-shot image-conditioned novel view synthesis. It achieves high\n3D generation quality when combined with SDS, but still suffers from long optimization time (Tang,\n2022; Qian et al., 2023). One-2-3-45 (Liu et al., 2023a) trains a multi-view reconstruction model\nfor acceleration at the cost of the generation quality. With an efficiency-optimized framework, our\nwork shortens the image-to-3D optimization time to 2 minutes with little sacrifice on quality.\n3\nOUR APPROACH\nIn this section, we introduce our two-stage framework for efficient 3D content generation for both\nImage-to-3D and Text-to-3D tasks as illustrated in Figure 2. Firstly, we adapt 3D Gaussian splat-\n3\ni) Generative Gaussian Splatting\nTextured Mesh\nii) Efficient Mesh Extraction\nT = 0.8\nAdd Noise\nDiffusion\nPrior\nMulti-step\nRandom View\nRefined Image\nDenoising\nMSE Loss\niii) UV-Space Texture Refinement\n3D Gaussians\nDensify\nColor Back-Projection\nText/Image\nDiffusion\nPrior\nSDS Loss\nRandom \nView\nOptimize\nUV Map\nLocal Density Query\nText/Image\nMapping\nOptimize\n3D Gaussians\nFigure 2: DreamGaussian Framework. 3D Gaussians are used for efficient initialization of ge-\nometry and appearance using single-step SDS loss. We then extract a textured mesh and refine the\ntexture image with a multi-step MSE loss.\nting (Kerbl et al., 2023) into generation tasks for efficient initialization through SDS (Poole et al.,\n2022) (Section 3.1). Next, we propose an efficient algorithm to extract a textured mesh from 3D\nGaussians (Section 3.2). This texture is then fine-tuned by differentiable rendering (Laine et al.,\n2020) through a UV-space refinement stage (Section 3.3) for final exportation.\n3.1\nGENERATIVE GAUSSIAN SPLATTING\nGaussian splatting (Kerbl et al., 2023) represents 3D information with a set of 3D Gaussians. It\nhas been proven effective in reconstruction settings (Kerbl et al., 2023; Luiten et al., 2023) with\nhigh inference speed and reconstruction quality under similar modeling time with NeRF. However,\nits usage in a generative manner has not been explored. We identify that the 3D Gaussians can be\nefficient for 3D generation tasks.\nSpecifically, the location of each Gaussian can be described with a center x \u2208 R3, a scaling factor\ns \u2208 R3, and a rotation quaternion q \u2208 R4. We also store an opacity value \u03b1 \u2208 R and a color\nfeature c \u2208 R3 for volumetric rendering. Spherical harmonics are disabled since we only want\nto model simple diffuse color. All the above optimizable parameters is presented by \u0398, where\n\u0398i = {xi, si, qi, \u03b1i, ci} is the parameter for the i-th Gaussian. To render a set of 3D Gaussians, we\nneed to project them into the image plane as 2D Gaussians. Volumetric rendering is then performed\nfor each pixel in front-to-back depth order to evaluate the final color and alpha. In this work, we use\nthe highly optimized renderer implementation from Kerbl et al. (2023) to optimize \u0398.\nWe initialize the 3D Gaussians with random positions sampled inside a sphere, with unit scaling and\nno rotation. These 3D Gaussians are periodically densified during optimization. Different from the\nreconstruction pipeline, we start from fewer Gaussians but densify it more frequently to align with\nthe generation progress. We follow the recommended practices from previous works (Poole et al.,\n2022; Huang et al., 2023; Lin et al., 2023) and use SDS to optimize the 3D Gaussians. At each\nstep, we sample a random camera pose p orbiting the object center, and render the RGB image Ip\nRGB\nand transparency Ip\nA of the current view. Similar to Dreamtime (Huang et al., 2023), we decrease\nthe timestep t linearly, which is used to weight the random noise \u03f5 added to the rendered RGB\nimage. Then, different 2D diffusion priors \u03d5 are used to guide the SDS denoising step, which is\nback-propagated to the 3D Gaussians.\nImage-to-3D. For the image-to-3D task, an image \u02dcIr\nRGB and a foreground mask \u02dcIr\nA are given as input.\nZero-1-to-3 XL (Liu et al., 2023b; Deitke et al., 2023b) is adopted as the 2D diffusion prior. The\n4\nSDS loss can be formulated as:\n\u2207\u0398LSDS = Et,p,\u03f5\n\u0014\n(\u03f5\u03d5(Ip\nRGB; t, \u02dcIr\nRGB, \u2206p) \u2212 \u03f5)\u2202Ip\nRGB\n\u2202\u0398\n\u0015\n(1)\nwhere \u03f5\u03d5(\u00b7) is the predicted noise by the 2D diffusion prior \u03d5, and \u2206p is the relative camera pose\nchange from the reference camera r. Additionally, we optimize the reference view image Ir\nRGB and\ntransparency Ir\nA to align with the input:\nLRef = \u03bbRGB||Ir\nRGB \u2212 \u02dcIr\nRGB||2\n2 + \u03bbA||Ir\nA \u2212 \u02dcIr\nA||2\n2\n(2)\nwhere \u03bbRGB and \u03bbA are the weights which are linearly increased during training. The final loss is\nthe weighted sum of the above three losses.\nText-to-3D. The input for text-to-3D is a single text prompt. Following previous works, Stable-\ndiffusion (Rombach et al., 2022) is used for the text-to-3D task. The SDS loss can be formulated\nas:\n\u2207\u0398LSDS = Et,p,\u03f5\n\u0014\n(\u03f5\u03d5(Ip\nRGB; t, e) \u2212 \u03f5)\u2202Ip\nRGB\n\u2202\u0398\n\u0015\n(3)\nwhere e is the CLIP embeddings of the input text description.\nDissusion. We observe that the generated Gaussians often look blurry and lack details even with\nlonger SDS training iterations. This could be explained by the ambiguity of SDS loss. Since each\noptimization step may provide inconsistent 3D guidance, it\u2019s hard for the algorithm to correctly\ndensify the under-reconstruction regions or prune over-reconstruction regions as in reconstruction.\nThis observation leads us to the following mesh extraction and texture refinement designs.\n3.2\nEFFICIENT MESH EXTRACTION\nPolygonal mesh is a widely used 3D representation, particularly in industrial applications. Many\nprevious works (Poole et al., 2022; Lin et al., 2023; Tsalicoglou et al., 2023; Tang et al., 2023a)\nexport the NeRF representation into a mesh-based representation for high-resolution fine-tuning.\nWe also seek to convert the generated 3D Gaussians into meshes and further refine the texture.\nTo the best of our knowledge, the polygonal mesh extraction from 3D Gaussians is still an unex-\nplored problem. Since the spatial density is described by a large number of 3D Gaussians, brute-\nforce querying of a dense 3D density grid can be slow and inefficient. It\u2019s also unclear how to extract\nthe appearance in 3D, as the color blending is only defined with projected 2D Gaussians (Kerbl et al.,\n2023). Here, we propose an efficient algorithm to extract a textured mesh based on block-wise local\ndensity query and back-projected color.\nLocal Density Query. To extract the mesh geometry, a dense density grid is needed to apply the\nMarching Cubes (Lorensen & Cline, 1998) algorithm. An important feature of the Gaussian splat-\nting algorithm is that over-sized Gaussians will be split or pruned during optimization. This is the\nfoundation of the tile-based culling technique for efficient rasterization (Kerbl et al., 2023). We also\nleverage this feature to perform block-wise density queries.\nWe first divide the 3D space of (\u22121, 1)3 into 163 blocks, then cull the Gaussians whose centers are\nlocated outside each local block. This effectively reduces the total number of Gaussians to query in\neach block. We then query a 83 dense grid inside each block, which leads to a final 1283 dense grid.\nFor each query at grid position x, we sum up the weighted opacity of each remained 3D Gaussian:\nd(x) =\nX\ni\n\u03b1i exp(\u22121\n2(x \u2212 xi)T \u03a3\u22121\ni (x \u2212 xi))\n(4)\nwhere \u03a3i is the covariance matrix built from scaling si and rotation qi. An empirical threshold is\nthen used to extract the mesh surface through Marching Cubes. Decimation and Remeshing (Cignoni\net al., 2008) are applied to post-process the extracted mesh to make it smooth.\nColor Back-projection. Since we have acquired the mesh geometry, we can back project the ren-\ndered RGB image to the mesh surface and bake it as the texture. We first unwrap the mesh\u2019s UV\ncoordinates (Young, 2021) and initialize an empty texture image. Then we uniformly choose 8 az-\nimuths and 3 elevations, plus the top and bottom views to render the corresponding RGB image.\n5\nStage 1\nStage 2 (SDS)\nStage 2 (MSE)\nFigure 3: Different Texture Fine-tuning Objectives. We show that SDS loss produces artifacts for\nUV space texture optimization, while the proposed MSE loss avoids this.\nEach pixel from these RGB images can be back-projected to the texture image based on the UV\ncoordinate. Following Richardson et al. (2023), we exclude the pixels with a small camera space\nz-direction normal to avoid unstable projection at mesh boundaries. This back-projected texture\nimage serves as an initialization for the next mesh texture fine-tuning stage.\n3.3\nUV-SPACE TEXTURE REFINEMENT\nDue to the ambiguity of SDS optimization, the meshes extracted from 3D Gaussians usually possess\nblurry texture as illustrated in Figure 3. We therefore propose a second stage to refine the texture\nimage. However, fine-tuning the UV-space directly with SDS loss often leads to artifacts, which is\nalso observed in previous works (Liao et al., 2023). This is due to the mipmap texture sampling\ntechnique used in differentiable rasterization (Laine et al., 2020). With ambiguous guidance like\nSDS, the gradient propagated to each mipmap level results in over-saturated color blocks. Therefore,\nwe seek more definite guidance to fine-tune a blurry texture.\nWe draw inspiration from the image-to-image synthesis of SDEdit (Meng et al., 2021) and the re-\nconstruction settings. Since we already have an initialization texture, we can render a blurry image\nIp\ncoarse from an arbitrary camera view p. Then, we perturb the image with random noise and apply a\nmulti-step denoising process f\u03d5(\u00b7) using the 2D diffusion prior to obtaining a refined image:\nIp\nfine = f\u03d5(Ip\ncoarse + \u03f5(tstart); tstart, c)\n(5)\nwhere \u03f5(tstart) is a random noise at timestep tstart, c is \u2206p for image-to-3D and e for text-to-3D\nrespectively. The starting timestep tstart is carefully chosen to limit the noise strength, so the refined\nimage can enhance details without breaking the original content. This refined image is then used to\noptimize the texture through a pixel-wise MSE loss:\nLMSE = ||Ip\nfine \u2212 Ip\ncoarse||2\n2\n(6)\nFor image-to-3D tasks, we still apply the reference view RGBA loss in Equation 2. We find that only\nabout 50 steps can lead to good details for most cases, while more iterations can further enhance the\ndetails of the texture.\n4\nEXPERIMENTS\n4.1\nIMPLEMENTATION DETAILS\nWe train 500 steps for the first stage and 50 steps for the second stage. The 3D Gaussians are\ninitialized to 0.1 opacity and grey color inside a sphere of radius 0.5. The rendering resolution\nis increased from 64 to 512 for Gaussian splatting, and randomly sampled from 128 to 1024 for\nmesh. The loss weights for RGB and transperency are linearly increased from 0 to 104 and 103\nduring training. We sample random camera poses at a fixed radius of 2 for image-to-3D and 2.5\nfor text-to-3D, y-axis FOV of 49 degree, with the azimuth in [\u2212180, 180] degree and elevation in\n[\u221230, 30] degree. The background is rendered randomly as white or black for Gaussian splatting.\nFor image-to-3D task, the two stages each take around 1 minute. We preprocess the input image by\nbackground removal (Qin et al., 2020) and recenter the foreground object. The 3D Gaussians are\ninitialized with 5000 random particles and densified for each 100 steps. For text-to-3D task, due to\nthe larger resolution of 512\u00d7512 used by Stable Diffusion (Rombach et al., 2022) model, each stage\ntakes around 2 minutes to finish. We initialize the 3D Gaussians with 1000 random particles and\n6\nInput             Zero-1-to-3                   One-2-3-45                      Shap-E                                    Ours\nAvg. Time           ~20 minutes                  ~45 seconds                  ~27 seconds                             ~2 minutes                   \nFigure 4: Comparisons on Image-to-3D. Our method achieves a better balance between generation\nspeed and mesh quality on various images.\ndensify them for each 50 steps. For mesh extraction, we use an empirical threshold of 1 for Marching\nCubes. All experiments are performed and measured with an NVIDIA V100 (16GB) GPU, while\nour method requires less than 8 GB GPU memory. Please check the supplementary materials for\nmore details.\n4.2\nQUALITATIVE COMPARISONS\nWe first provide qualitative comparisons on image-to-3D in Figure 4. We primarily compare with\nthree baselines from both optimization-based methods (Liu et al., 2023b) and inference-only meth-\nods (Liu et al., 2023a; Jun & Nichol, 2023). For all compared methods, we export the generated\n7\n\u201ca campfire\u201d\n\u201ca small saguaro \ncactus planted \nin a clay pot\u201d\n\u201ca photo \nof a tulip\u201d\nDreamFusion*                                    Shap-E                                               Ours          \n~1 hour\n~13 seconds                                 ~ 5 minutes\nPrompt\nAvg. Time\nFigure 5: Comparisons on Text-to-3D. For DreamFusion, we use the implementation from Guo\net al. (2023) which also uses Stable-Diffusion as the 2D prior.\nType\nCLIP-Similarity \u2191\nGeneration Time \u2193\nOne-2-3-45 (Liu et al., 2023a)\nInference-only\n0.594\n45 seconds\nPoint-E (Nichol et al., 2022)\nInference-only\n0.587\n78 seconds\nShap-E (Jun & Nichol, 2023)\nInference-only\n0.591\n27 seconds\nZero-1-to-3 (Liu et al., 2023b)\nOptimization-based\n0.647\n20 minutes\nZero-1-to-3\u2217 (Liu et al., 2023b)\nOptimization-based\n0.778\n30 minutes\nOurs (Stage 1 Only)\nOptimization-based\n0.678\n1 minute\nOurs\nOptimization-based\n0.738\n2 minutes\nTable 1: Quantitative Comparisons on generation quality and speed for image-to-3D tasks. For\nZero-1-to-3\u2217, a mesh fine-tuning stage is used to further improve quality (Tang, 2022).\nmodels as polygonal meshes with vertex color or texture images, and render them under ambient\nlighting. In terms of generation speed, our approach exhibits a noteworthy acceleration when con-\ntrasted with optimization-based methods. Regarding the quality of generated models, our method\noutperforms inference-only methods especially with respect to the fidelity of 3D geometry and vi-\nsual appearance. In general, our method achieves a better balance between generation quality and\nspeed, reaching comparable quality as optimization-based methods while only marginally slower\nthan inference-only methods. In Figure 5, we compare the results on text-to-3D. Consistent with our\nfindings in image-to-3D tasks, our method achieves better quality than inference-based methods and\nfaster speed than other optimization-based methods. Furthermore, we highlight the quality of our\nexported meshes in Figure 6. These meshes exhibit uniform triangulation, smooth surface normals,\nand clear texture images, rendering them well-suited for seamless integration into downstream ap-\nplications. For instance, leveraging software such as Blender (Community, 2018), we can readily\nemploy these meshes for rigging and animation purposes.\n4.3\nQUANTITATIVE COMPARISONS\nIn Table 1, we report the CLIP-similarity (Radford et al., 2021; Qian et al., 2023; Liu et al., 2023a)\nand average generation time of different image-to-3D methods on a collection of images from pre-\nvious works (Melas-Kyriazi et al., 2023; Liu et al., 2023a; Tang et al., 2023b) and Internet. We also\nconduct an user study on the generation quality detailed in Table 2. This study centers on the assess-\nment of reference view consistency and overall generation quality, which are two critical aspects in\nthe context of image-to-3D tasks. Our two-stage results achieve better view consistency and gener-\n8\nMesh                        Wireframe                              Normal                             Animation              \nTexture\nFigure 6: Mesh Exportation. We export high quality textured mesh from 3D Gaussians, which can\nbe seamlessly used in downstream applications like rigged animation.\nStage 1 Full Setting                            w/o Densification                         w/o Timestep Annealing                w/o Reference View Loss\nFigure 7: Ablation Study. We ablate the design choices in stage 1 training.\nZero-1-to-3\nOne-2-3-45\nShap-E\nOurs\nRef. View Consistency \u2191\n3.43\n2.33\n1.81\n4.25\nOverall Model Quality \u2191\n3.02\n1.86\n1.52\n3.84\nTable 2: User Study on image-to-3D tasks. The rating is of scale 1-5, the higher the better.\nation quality compared to inference-only methods. Although our mesh quality falls slightly behind\nthat of other optimization-based methods, we reach a significant acceleration of over 10 times.\n4.4\nABLATION STUDY\nWe carry out ablation studies on the design of our methods in Figure 7. We are mainly interested\nin the generative Gaussian splatting training, given that mesh fine-tuning has been well explored in\nprevious methods (Tang et al., 2023a; Lin et al., 2023). Specifically, we perform ablation on three\naspects of our method: 1) Periodical densification of 3D Gaussians. 2) Linear annealing of timestep\nt for SDS loss. 3) Effect of the reference view loss LRef. Our findings reveal that the omission of any\nof these design elements results in a degradation of the quality of the models generated in the first\nstage. Specifically, the final Gaussians exhibit increased blurriness and inaccuracies, which further\naffects the second fine-tuning stage.\n5\nLIMITATIONS AND CONCLUSION\nIn this work, we present DreamGausssion, a 3D content generation framework that significantly\nimproves the efficiency of 3D content creation. The key features of our work are: 1) We design the\ngenerative Gaussian splatting pipeline which is highly efficient for 3D generation. 2) We propose a\nmesh extraction algorithm that effectively derives textured meshes from 3D Gaussians. 3) With our\ntexture fine-tuning stage, our work can produce ready-to-use 3D assets with high-quality polygonal\nmeshes from either a single image or text description within a few minutes.\nLimitations. We share common problems with previous text-to-3D works: Multi-face Janus prob-\nlem and baked lighting. Fortunately, it\u2019s possible to address these problems with recent advances\nin multi-view 2D diffusion models (Shi et al., 2023; Liu et al., 2023c; Zhao et al., 2023) and latent\nBRDF auto-encoder (Xu et al., 2023b). Besides, the back-view texture generated in our image-to-3D\nresults may look blurry. This can be alleviated with longer stage 2 training.\n9\nACKNOWLEDGMENTS\nThis work is supported by the Sichuan Science and Technology Program (2023YFSY0008), Na-\ntional Natural Science Foundation of China (61632003, 61375022, 61403005), Grant SCITLAB-\n20017 of Intelligent Terminal Key Laboratory of SiChuan Province, Beijing Advanced Innovation\nCenter for Intelligent Robots and Systems (2018IRS11), and PEK-SenseTime Joint Laboratory of\nMachine Vision. This project is also funded by the Ministry of Education, Singapore, under its\nMOE AcRF Tier 2 (MOE-T2EP20221-0012), NTU NAP, and under the RIE2020 Industry Align-\nment Fund \u2013 Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and\nin-kind contribution from the industry partner(s).\nREFERENCES\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. 3\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello,\nOrazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon\nWetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022. 3\nHansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-\nstage diffusion nerf: A unified approach to 3d generation and reconstruction. arXiv preprint\narXiv:2304.06714, 2023a. 3\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023b.\n2, 3\nYiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, and Guosheng\nLin.\nIt3d:\nImproved text-to-3d generation with explicit view synthesis.\narXiv preprint\narXiv:2308.11473, 2023c. 3\nZhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploit-\ning the polygon rasterization pipeline for efficient neural field rendering on mobile architectures.\narXiv preprint arXiv:2208.00277, 2022. 3\nYen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwing, and Liang-Yan Gui. Sdfu-\nsion: Multimodal 3d shape completion, reconstruction, and generation. In CVPR, pp. 4456\u20134465,\n2023. 3\nPaolo Cignoni, Marco Callieri, Massimiliano Corsini, Matteo Dellepiane, Fabio Ganovelli, and\nGuido Ranzuglia.\nMeshLab: an Open-Source Mesh Processing Tool.\nIn Vittorio Scarano,\nRosario De Chiara, and Ugo Erra (eds.), Eurographics Italian Chapter Conference. The Eu-\nrographics Association, 2008.\nISBN 978-3-905673-68-5.\ndoi: 10.2312/LocalChapterEvents/\nItalChap/ItalianChapConf2008/129-136. 5\nBlender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation,\nStichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org. 8\nMatt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan\nFan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of\n10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023a. 2\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects. In CVPR, pp. 13142\u201313153, 2023b. 2, 4\nShivam Duggal and Deepak Pathak. Topologically-aware deformation fields for single-view 3d\nreconstruction. In CVPR, pp. 1536\u20131546, 2022. 3\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan\nGojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned\nfrom images. NeurIPS, 35:31841\u201331854, 2022. 3\n10\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-\nHao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified\nframework for 3d content generation. https://github.com/threestudio-project/\nthreestudio, 2023. 8\nAnchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O\u02d8guz.\n3dgen: Triplane latent\ndiffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023. 2, 3\nPeter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking\nneural radiance fields for real-time view synthesis. ICCV, 2021. 3\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:\n6840\u20136851, 2020. 3\nYukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dream-\ntime:\nAn improved optimization strategy for text-to-3d content creation.\narXiv preprint\narXiv:2306.12422, 2023. 3, 4\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. In CVPR, pp. 867\u2013876, 2022. 3\nHeewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023. 2, 3, 7, 8, 15\nBernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and George Drettakis. 3d gaussian splat-\nting for real-time radiance field rendering. ToG, 42(4):1\u201314, 2023. 2, 3, 4, 5, 15\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014. 15\nSamuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular\nprimitives for high-performance differentiable rendering. ToG, 39(6), 2020. 4, 6\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In ICML, pp. 12888\u201312900.\nPMLR, 2022. 3\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023a. 3\nYuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, and Bing-\nbing Ni.\nFocaldreamer: Text-driven 3d editing via focal-fusion assembly.\narXiv preprint\narXiv:2308.10608, 2023b. 3\nZhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and\nChen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In CVPR, 2023c. 3\nTingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, and Michael J\nBlack. Tada! text to animatable digital avatars. arXiv preprint arXiv:2308.10899, 2023. 6\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In CVPR, pp. 300\u2013309, 2023. 2, 3, 4, 5, 9\nMinghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al.\nOne-2-3-45:\nAny single image to 3d mesh in 45 seconds without per-shape optimization.\narXiv preprint\narXiv:2306.16928, 2023a. 3, 7, 8, 15\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023b. 3, 4, 7,\n8, 15, 16\n11\nYuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.\nSyncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint\narXiv:2309.03453, 2023c. 9, 15\nWilliam E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction\nalgorithm. In Seminal graphics: pioneering efforts that shaped the field, pp. 347\u2013353. 1998. 5\nJonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp,\nTsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object\nsynthesis. arXiv preprint arXiv:2306.07349, 2023. 3\nJonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians:\nTracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713, 2023. 3, 4\nLuke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi.\nRealfusion: 360deg\nreconstruction of any object from a single image. In CVPR, pp. 8446\u20138455, 2023. 3, 8, 15\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint\narXiv:2108.01073, 2021. 2, 6\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022. 3\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven\nneural stylization for meshes. In CVPR, pp. 13492\u201313502, 2022. 3\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n2, 3\nNasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generat-\ning textured meshes from text using pretrained image-text models. In SIGGRAPH Asia, pp. 1\u20138,\n2022. 3\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. ACM TOG, 2022. 2, 3\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\n2, 3, 8\nEvangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, and\nSergey Tulyakov. Autodecoding latent 3d diffusion models. arXiv preprint arXiv:2307.05445,\n2023. 3\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022. 2, 3, 4, 5\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-\nYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al.\nMagic123: One image\nto high-quality 3d object generation using both 2d and 3d diffusion priors.\narXiv preprint\narXiv:2306.17843, 2023. 3, 8, 15\nXuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin\nJagersand. U2-net: Going deeper with nested u-structure for salient object detection. Pattern\nrecognition, 106:107404, 2020. 6\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, pp. 8748\u20138763. PMLR, 2021. 8\nAmit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran\nZada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven\ntext-to-3d generation. arXiv preprint arXiv:2303.13508, 2023. 3\n12\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-\nguided texturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023. 6\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pp. 10684\u201310695, 2022. 2, 3,\n5, 6, 15\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 35:36479\u201336494,\n2022. 2, 3\nSara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo\nKanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. 2, 3\nYichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 9\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman\nGoyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation.\narXiv preprint arXiv:2301.11280, 2023. 3\nJiaxiang\nTang.\nStable-dreamfusion:\nText-to-3d\nwith\nstable-diffusion,\n2022.\nhttps://github.com/ashawkey/stable-dreamfusion. 3, 8, 15\nJiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, and Gang\nZeng. Delicate textured mesh recovery from nerf via adaptive surface refinement. arXiv preprint\narXiv:2303.02091, 2023a. 5, 9\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen.\nMake-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint\narXiv:2303.14184, 2023b. 3, 8, 15\nAlex Trevithick and Bo Yang. Grf: Learning a general radiance field for 3d representation and\nrendering. In ICCV, pp. 15182\u201315192, 2021. 3\nChristina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari.\nTextmesh: Generation of realistic 3d meshes from text prompts. arXiv preprint arXiv:2304.12439,\n2023. 3, 5\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian\nchaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, pp. 12619\u201312629,\n2023a. 3\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023b. 2, 3\nTong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi\nWang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic percep-\ntion, reconstruction and generation. In CVPR, pp. 803\u2013814, 2023. 2\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360:\nLifting an in-the-wild 2d photo to a 3d object with 360deg views. In CVPR, pp. 4479\u20134489,\n2023a. 3\nXudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. Matlaber: Material-aware text-to-3d via\nlatent brdf auto-encoder. arXiv preprint arXiv:2308.09278, 2023b. 9\nJonathan Young. Xatlas, 2021. URL https://github.com/jpcy/xatlas. 5\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from\none or few images. In CVPR, pp. 4578\u20134587, 2021. 3\n13\nChaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin Wang, and Fan Wang. Points-to-3d:\nBridging the gap between sparse points and shape-controllable text-to-3d generation.\narXiv\npreprint arXiv:2307.13908, 2023. 3\nBiao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape rep-\nresentation for neural fields and generative diffusion models. arXiv preprint arXiv:2301.11445,\n2023. 3\nMinda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Zhipeng Hu, Changjie Fan,\nand Xin Yu. Efficientdreamer: High-fidelity and robust 3d creation via orthogonal-view diffusion\nprior. arXiv preprint arXiv:2308.13223, 2023. 9\nXin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum.\nLocally attentional sdf diffusion for controllable 3d shape generation.\narXiv preprint\narXiv:2305.04461, 2023. 3\nJoseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d with advanced diffusion guidance.\narXiv preprint arXiv:2305.18766, 2023. 3\nJingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and Guanbin Li. Dreameditor: Text-driven 3d\nscene editing with neural fields. arXiv preprint arXiv:2306.13455, 2023. 3\n14\nA\nAPPENDIX\nA.1\nMORE IMPLEMENTATION DETAILS\nLearning Rate. For the learning rate of Gaussian splatting, we set different values for different\nparameters. The learning rate for position is decayed from 1 \u00d7 10\u22123 to 2 \u00d7 10\u22125 in 500 steps, for\nfeature is set to 0.01, for opacity is 0.05, for scaling and rotation is 5 \u00d7 10\u22123. For mesh texture\nfine-tuning, the learning rate for texture image is set to 0.2. We use the Adam (Kingma & Ba, 2014)\noptimizer for both stages.\nDensification and Pruning. Following Kerbl et al. (2023), the densification in image-to-3D is\napplied for Gaussians with accumulated gradient larger than 0.5 and max scaling smaller than 0.05.\nIn text-to-3D, we set the gradient threshold to 0.01 to encourage densification. We also prune the\nGaussians with an opacity less than 0.01 or max scaling larger than 0.05.\nEvaluation Settings. We adopt the CLIP-similarity metric (Melas-Kyriazi et al., 2023; Qian et al.,\n2023; Liu et al., 2023a) to evaluate the image-to-3D quality. A dataset of 30 images collected from\nprevious works (Melas-Kyriazi et al., 2023; Liu et al., 2023a; Tang et al., 2023b; Liu et al., 2023c)\nand Internet covering various objects is used. We then render 8 views with uniformly sampled\nazimuth angles [0, 45, 90, 135, 180, 225, 270, 315] and zero elevation angle. These rendered images\nare used to calculate the CLIP similarities with the reference view, and we average the different\nviews for the final metric. We use the laion/CLIP-ViT-bigG-14-laion2B-39B-b160k1\ncheckpoint to calculate CLIP similarity. For the user study, we render 360 degree rotating videos\nof 3D models generated from a collection of 15 images. There are in total 60 videos for 4 methods\n(Zero-1-to-3 (Liu et al., 2023b), One-2-3-45 Liu et al. (2023a), Shap-E Jun & Nichol (2023), and our\nmethod) to evaluate. Each volunteer is shown 15 samples containing the input image and a rendered\nvideo from a random method, and ask them to rate in two aspects: reference view consistency and\noverall model quality. We collect results from 40 volunteers and get 600 valid scores in total.\nA.2\nMORE RESULTS\nIn Figure 8, we show more visualization results of our method. Specially, we compare the mesh\noutput before and after our texture fine-tuning stage. We also compare against a SDS-based mesh\nfine-tuning method for Zero-1-to-3 (Liu et al., 2023b) noted as Zero-1-to-3\u2217 (Tang, 2022). Both\nstages of our method are faster than previous two-stage image-to-3D methods, while still reaching\ncomparable generation quality.\nOur method also support images with non-zero elevations. As illustrated in Figure 10, our method\ncan perform image-to-3D correctly with an extra estimated elevation angle as input. We make sure\nthe random elevation sampling covers the input elevation and at least [\u221230, 30] degree.\nIn Figure 11, we demostrate the text-to-image-to-3D pipeline (Liu et al., 2023a; Qian et al., 2023).\nWe first apply text-to-image diffusion models (Rombach et al., 2022) to synthesize an image given a\ntext prompt, then perform image-to-3D using our model. This usually gives better results compared\nto directly performing text-to-3D pipeline, and takes less time to generate. We show more animation\nresults from our exported meshes in Figure 9.\nWe also illustrate the limitations of our method in Figure 12. Our image-to-3D pipeline may pro-\nduce blurry back-view image and cannot generate fine details, which looks unmatched to the front\nreference view. With longer training of stage 2, the blurry problem of back view can be alleviated.\nFor text-to-3D, we share common problems with previous methods, including the multi-face Janus\nproblem and baked lighting in texture images.\n1https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\n15\nInput                    Zero-1-to-3                                Zero-1-to-3*                          Ours (Stage 1)        \nOurs (Stage 2)\nFigure 8: More Qualitative Comparisons. We compare the results from two training stages of our\nmethod and Zero-1-to-3 (Liu et al., 2023b).\nFigure 9: Results on mesh animation. Our exported meshes are ready-to-use for downstream\napplications like rigged animation.\nInput                                                 Generated 3D Model                                                       Mesh\nFigure 10: Results on images with different elevations. Our method supports input images with a\nnon-zero elevation angle.\n16\n\u201ca nendoroid of \na cute boy\u201d\n\u201ca nendoroid of \na cute girl\u201d\n\u201ca penguin\u201d\n\u201ca potted \ncactus plant\u201d\nPrompt                            Image                                Generated 3D Model                              Mesh\n\u201ca 3D model \nof a fox\u201d\n\u201ca 3D model of \na soldier\u201d\nFigure 11: Text-to-image-to-3D. We first synthesize an image given a text prompt, then perform\nimage-to-3D generation.\nFront View (50 iters)           Back View (50 iters)              Back View (500 iters)\nMulti-face            Baked Lighting\nImage-to-3D\nText-to-3D\nFigure 12: Limitations. Visualization of the limitations of our method.\n17\n"
  },
  {
    "title": "Text-to-3D using Gaussian Splatting",
    "link": "https://arxiv.org/pdf/2309.16585.pdf",
    "upvote": "29",
    "text": "TEXT-TO-3D USING GAUSSIAN SPLATTING\nZilong Chen, Feng Wang\u2020, Huaping Liu\u2020\nDepartment of Computer Science and Technology\nTsinghua University\n{chenzl22,wang-f20}@mails.tsinghua.edu.cn, hpliu@tsinghua.edu.cn\nFigure 1: Delicate 3D assets generated using the proposed GSGEN. See our project\npage gsgen3d.github.io for videos of these images.\nABSTRACT\nIn this paper, we present Gaussian Splatting based text-to-3D generation (GSGEN),\na novel approach for generating high-quality 3D objects. Previous methods suffer\nfrom inaccurate geometry and limited fidelity due to the absence of 3D prior and\nproper representation. We leverage 3D Gaussian Splatting, a recent state-of-the-art\nrepresentation, to address existing shortcomings by exploiting the explicit nature\nthat enables the incorporation of 3D prior. Specifically, our method adopts a pro-\ngressive optimization strategy, which includes a geometry optimization stage and an\nappearance refinement stage. In geometry optimization, a coarse representation is\nestablished under a 3D geometry prior along with the ordinary 2D SDS loss, ensur-\ning a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians\nundergo an iterative refinement to enrich details. In this stage, we increase the num-\nber of Gaussians by compactness-based densification to enhance continuity and\nimprove fidelity. With these designs, our approach can generate 3D content with\ndelicate details and more accurate geometry. Extensive evaluations demonstrate the\neffectiveness of our method, especially for capturing high-frequency components.\nOur code is available at https://github.com/gsgen3d/gsgen/.\n1\nINTRODUCTION\nDiffusion model based text-to-image generation (Saharia et al., 2022; Rombach et al., 2022; Ramesh\net al., 2022; Alex et al., 2023) has achieved remarkable success in synthesizing photo-realistic images\nfrom textual prompts. Nevertheless, for high-quality text-to-3D content generation, the advancements\nlag behind that of image generation due to the inherent complexity of real-world 3D scenes. Recently,\nDreamFusion (Poole et al., 2023) has made great progress in generating delicate assets by utilizing\nscore distillation sampling with a pre-trained text-to-image diffusion prior. Its follow-up works further\nimprove this paradigm in quality (Wang et al., 2023c; Chen et al., 2023), training speed (Lin et al.,\n2023; Metzer et al., 2022), and generating more reasonable geometry (Armandpour et al., 2023; Zhu\n\u2020Corresponding Authors.\n1\narXiv:2309.16585v3  [cs.CV]  31 Oct 2023\nStable DreamFusion (Tang, 2022; Poole et al., 2023)\nGSGEN (Ours)\nA DSLR photo of a panda\nFantasia3D (Chen et al., 2023)\nGSGEN (Ours)\nA high quality photo of a furry corgi\nthreestudio Magic3D (Guo et al., 2023; Lin et al., 2023)\nGSGEN (Ours)\nA zoomed out DSLR photo of an amigurumi bulldozer\nFigure 2: Compared to previous methods, GSGEN alleviates the Janus problem by representing the\n3D scene using 3D Gaussian Splatting, which is capable of applying direct 3D geometry guidance\nand expressing content with delicate details. Note that the results of DreamFusion and Magic3D are\nobtained using Stable DreamFusion (Tang, 2022) and threestudio (Guo et al., 2023) since the official\nimplementations are not publicly available due to the utilization of private diffusion models. All the\nresults are obtained using StableDiffusion (Rombach et al., 2022) on checkpoint runwayml/stable-\ndiffusion-v1-5 for a fair comparison. Videos of these images are provided in the supplemental video.\n& Zhuang, 2023; Seo et al., 2023). However, most existing text-to-3D methods still suffer greatly\nfrom collapsed geometry and limited fidelity, and are difficult to incorporate 3D priors due to the\nimplicit nature of NeRF (Mildenhall et al., 2020) and DMTET (Shen et al., 2021).\nRecently, 3D Gaussian Splatting (Kerbl et al., 2023) has garnered significant attention in the field of\n3D reconstruction, primarily due to its remarkable ability to represent intricate scenes and capability\nof real-time rendering. By modeling a scene using a set of 3D Gaussians, Kerbl et al. (2023) adopt\nan explicit and object-centric approach that fundamentally diverges from implicit representations\nlike NeRF and DMTET. This distinctive approach paves the way for the integration of explicit 3D\npriors into text-to-3D generation. Building upon this insight, instead of a straightforward replacement\nof NeRFs with Gaussians, we propose to guide the generation with an additional 3D point cloud\ndiffusion prior to enhancing geometrical coherence. By adopting this strategy, we can better harness\nthe inherent advantages of 3D Gaussians in the creation of complex and 3D-consistent assets.\nSpecifically, we propose to represent the generated 3D content with a set of Gaussians and optimize\nthem progressively in two stages, namely geometry optimization and appearance refinement. In the\ngeometry optimization stage, we optimize the Gaussians under the guidance of a 3D point cloud\ndiffusion prior along with the ordinary 2D image prior. The incorporation of this extra 3D SDS loss\nensures a 3D-consistent rough geometry. In the subsequent refinement stage, the Gaussians undergo\nan iterative enhancement to enrich the delicate details. Due to the sub-optimal performance of the\n2\noriginal adaptive control under SDS loss, we introduce an additional compactness-based densification\ntechnique to enhance appearance and fidelity. Besides, to prevent potential degeneration and break\nthe symmetry in the early stage, the Gaussians are initialized with a coarse point cloud generated\nby a text-to-point-cloud diffusion model. As a result of these techniques, our approach can generate\n3D assets with accurate geometry and exceptional fidelity. Fig.2 illustrates a comparison between\nGSGEN and previous state-of-the-art methods on generating assets with asymmetric geometry.\nIn summary, our contributions are:\n\u2022 We propose GSGEN, the first text-to-3D generation method using 3D Gaussians as repre-\nsentation. By incorporating geometric priors, we highlight the distinctive advantages of\nGaussian Splatting in text-to-3D generation.\n\u2022 We introduce a two-stage optimization strategy that first exploits joint guidance of 2D and\n3D diffusion prior to shaping a coherent rough structure in geometry optimization; then\nenriches the details with a compactness-based densification in appearance refinement.\n\u2022 We validate GSGEN on various textual prompts. Experiments show that our method can\ngenerate 3D assets with more accurate geometry and enhanced fidelity than previous meth-\nods. Especially, GSGEN demonstrates superior performance in capturing high-frequency\ncomponents in objects, such as feathers, surfaces with intricate textures, animal fur, etc.\n2\nRELATED WORK\n2.1\n3D SCENE REPRESENTATIONS\nRepresenting 3D scenes in a differentiable way has achieved remarkable success in recent years.\nNeRFs (Mildenhall et al., 2020) demonstrates outstanding performance in novel view synthesis\nby representing 3D scenes with a coordinate-based neural network. After works have emerged to\nimprove NeRF in reconstruction quality (Barron et al., 2021; 2023; Wang et al., 2022c), handling\nlarge-scale (Tancik et al., 2022; Zhang et al., 2020; Martin-Brualla et al., 2021; Chen et al., 2022b)\nand dynamic scenes (Park et al., 2021; Attal et al., 2023; Wang et al., 2022b; Sara Fridovich-Keil\nand Giacomo Meanti et al., 2023; Pumarola et al., 2021), improving training (Yu et al., 2021a; Chen\net al., 2022a; Sun et al., 2022; M\u00a8uller et al., 2022) and rendering (Reiser et al., 2023; Hedman et al.,\n2021; Yu et al., 2021b) speed. Although great progress has been made, NeRF-based methods still\nsuffer from low rendering speed and high training-time memory usage due to their implicit nature. To\ntackle these challenges, Kerbl et al. (2023) propose to represent the 3D scene as a set of anisotropic\nGaussians and render the novel views using a GPU-optimized tile-based rasterization technique. 3D\nGaussian Splatting could achieve comparing reconstruction results while being capable of real-time\nrendering. Our research highlights the distinctive advantages of Gaussian Splatting within text-to-3D\nby incorporating explicit 3D prior, generating 3D consistent and highly detailed assets.\n2.2\nDIFFUSION MODELS\nDiffusion models have arisen as a promising paradigm for learning and sampling from a complex\ndistribution. Inspired by the diffusion process in physics, these models involve a forward process\nto gradually add noise and an inverse process to denoise a noisy sample with a trained neural\nnetwork. After DDPM (Ho et al., 2020; Song et al., 2021b) highlighted the effectiveness of diffusion\nmodels in capturing real-world image data, a plethora of research has emerged to improve the inherent\nchallenges, including fast sampling (Lu et al., 2022; Bao et al., 2022; Song et al., 2021a) and backbone\narchitectural improvements (Bao et al., 2023; Podell et al., 2023; Liu et al., 2023b; Dhariwal & Nichol,\n2021; Hoogeboom et al., 2023; Peebles & Xie, 2022). One of the most successful applications of\ndiffusion models lies in text-to-image generation, where they have shown remarkable progress in\ngenerating realistic images from text prompts (Ho & Salimans, 2022; Ramesh et al., 2022; Alex\net al., 2023). To generate high-resolution images, current solutions either adopt a cascaded structure\nthat consists of a low-resolution diffusion model and several super-resolution models (Saharia et al.,\n2022; Balaji et al., 2022; Alex et al., 2023) or trains the diffusion model in latent space with an auto-\nencoder (Rombach et al., 2022; Gu et al., 2022). Our proposed GSGEN is built upon StableDiffusion\n(Rombach et al., 2022), an open-source latent diffusion model that provides fine-grained guidance for\nhigh-quality 3D content generation.\n3\n2.3\nTEXT-TO-3D GENERATION\nEarly efforts in text-to-3D generation, including CLIP-forge (Sanghi et al., 2021), Dream Fields (Jain\net al., 2022), Text2Mesh (Michel et al., 2022), TANGO (Chen et al., 2022c), CLIPNeRF (Wang et al.,\n2022a), and CLIP-Mesh (Khalid et al., 2022), harness CLIP (Radford et al., 2021) guidance to create\n3D assets. To leverage the stronger diffusion prior, DreamFusion (Poole et al., 2023) introduces the\nscore distillation sampling loss that optimizes the 3D content by minimizing the difference between\nrendered images and the diffusion prior. This development sparked a surge of interest in text-to-3D\ngeneration through image diffusion prior (Wang et al., 2023a; Raj et al., 2023; Lorraine et al., 2023;\nZhu & Zhuang, 2023; Qian et al., 2023). Magic3D (Lin et al., 2023) employs a coarse-to-fine strategy,\noptimizing a NeRF with a low-resolution diffusion prior and then enhancing texture under latent\ndiffusion prior with a DMTET initialized with the coarse NeRF. Latent-NeRF (Metzer et al., 2022)\ntrains a NeRF within the latent space of StableDiffusion and introduces the Sketch-Shape method to\nguide the generation process. Fantasia3D (Chen et al., 2023) disentangles the learning of geometry\nand material, harnessing physics-based rendering techniques to achieve high-fidelity mesh generation.\nProlificDreamer (Wang et al., 2023c) introduces variational score distillation to improve SDS and\nfacilitate the generation of high-quality and diverse 3D assets, whose contribution is orthogonal to\nours since we focus on incorporating 3D prior with more advanced representation. Another line of\nwork lies in generating 3D assets directly through a 3D diffusion model based on NeRF or other\ndifferentiable representations (Wang et al., 2023b; Jun & Nichol, 2023; Liu et al., 2023a; Cheng et al.,\n2023). Our approach builds upon Point-E (Nichol et al., 2022), a text-to-point-cloud diffusion model\ntrained on millions of 3D models, which offers valuable 3D guidance and coarse initialization.\n3\nPRELIMINARY\n3.1\nSCORE DISTILLATION SAMPLING\nInstead of directly generating 3D models, recent studies have achieved notable success by optimizing\n3D representation with a 2D pre-trained image diffusion prior based on score distillation sampling, as\nproposed by Poole et al. (2023). In this paradigm, the scene is represented as a differentiable image\nparameterization (DIP) denoted as \u03b8, where the image can be differentiably rendered based on the\ngiven camera parameters through a transformation function g. The DIP \u03b8 is iteratively refined to\nensure that, for any given camera pose, the rendered image x = g(\u03b8) closely resembles a plausible\nsample derived from the guidance diffusion model. DreamFusion achieves this by leveraging Imagen\n(Saharia et al., 2022) to provide a score estimation function denoted as \u03f5\u03d5(xt; y, t), where xt, y, and\nt represent the noisy image, text embedding, and timestep, respectively. This estimated score plays a\npivotal role in guiding the gradient update, as expressed by the following equation:\n\u2207\u03b8LSDS = E\u03f5,t\n\u0014\nw(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5)\u2202x\n\u2202\u03b8\n\u0015\n(1)\nwhere \u03f5 is a Gaussian noise and w(t) is a weighting function. Our approach combines score distillation\nsampling with 3D Gaussian Splatting at both 2D and 3D levels with different diffusion models to\ngenerate 3D assets with both detailed appearance and 3D-consistent geometry.\n3.2\n3D GAUSSIAN SPLATTING\nGaussian Splatting, as introduced in Kerbl et al. (2023), presents a pioneering method for novel\nview synthesis and 3D reconstruction from multi-view images. Unlike NeRF, 3D Gaussian Splatting\nadopts a distinctive approach, where the underlying scene is represented through a set of anisotropic\n3D Gaussians parameterized by their positions, covariances, colors, and opacities. When rendering,\nthe 3D Gaussians are projected onto the camera\u2019s imaging plane (Zwicker et al., 2001). Subsequently,\nthe projected 2D Gaussians are assigned to individual tiles. The color of p on the image plane is\nrendered sequentially with point-based volume rendering technique (Zwicker et al., 2001):\nC(p) =\nX\ni\u2208N\nci\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j)\nwhere, \u03b1i = oie\u2212 1\n2 (p\u2212\u00b5i)T \u03a3\u22121\ni\n(p\u2212\u00b5i),\n(2)\nwhere ci, oi, \u00b5i, and \u03a3i represent the color, opacity, position, and covariance of the i-th Gaussian\nrespectively, and N denotes the Gaussians in this tile. To maximize the utilization of shared memory,\n4\nU-Net\nU-Net\n2D Image Diffusion\n3D Point Cloud Diffusion\nrender\npositions\nU-Net\nU-Net\n2D Image Diffusion\nGeometry Optimization\nAppearance Refinement\nCompactness-based densification\nPrompt: A blue poison-dart frog\nsitting on a water lily\nTransformer\nprompt emb.\nprompt emb.\npos. output\npos. input\nFigure 3: Overview of the proposed GSGEN. Our approach aims at generating 3D assets with\naccurate geometry and delicate appearance. GSGEN starts by utilizing Point-E to initialize the\npositions of the Gaussians (Sec 4.3). The optimization is grouped into geometry optimization (Sec\n4.1) and appearance refinement (Sec 4.2) to meet a balance between coherent geometry structure and\nhighly detailed texture.\nGaussian Splatting further designs a GPU-friendly rasterization process where each thread block is\nassigned to render an image tile. These advancements enable Gaussian Splatting to achieve more\ndetailed scene reconstruction, significantly faster rendering speed, and reduction of memory usage\nduring training compared to NeRF-based methods. In this study, we expand the application of\nGaussian Splatting into text-to-3D generation and introduce a novel approach that leverages the\nexplicit nature of Gaussian Splatting by integrating 3D diffusion priors, highlighting the potential of\n3D Gaussians as a fundamental representation for generative tasks.\n4\nAPPROACH\nOur goal is to generate 3D content with accurate geometry and delicate detail. To accomplish this,\nGSGEN exploits the 3D Gaussians as representation due to its flexibility to incorporate geometry\npriors and capability to represent high-frequency details. Based on the observation that a point\ncloud can be seen as a set of isotropic Gaussians, we propose to integrate a 3D SDS loss with a\npre-trained point cloud diffusion model to shape a 3D-consistent geometry. With this additional\ngeometry prior, our approach could mitigate the Janus problem and generate more sensible geometry.\nSubsequently, in appearance refinement, the Gaussians undergo an iterative optimization to gradually\nimprove fine-grained details with a compactness-based densification strategy, while preserving the\nfundamental geometric information. The detailed GSGEN methodology is presented as follows.\n4.1\nGEOMETRY OPTIMIZATION\nMany text-to-3D methods encounter the significant challenge of overfitting to several views, resulting\nin assets with multiple faces and collapsed geometry (Poole et al., 2023; Lin et al., 2023; Chen et al.,\n2023). This issue, known as the Janus problem (Armandpour et al., 2023; Seo et al., 2023), has posed\na persistent hurdle in the development of such methodologies. In our early experiments, we faced a\nsimilar challenge that relying solely on 2D guidance frequently led to collapsed results. However, we\nnoticed that the geometry of 3D Gaussians can be directly rectified with a point cloud prior, which is\nnot feasible for previous text-to-3D methods using NeRFs and DMTET. Recognizing this distinctive\nadvantage, we introduce a geometry optimization process to shape a reasonable structure. Concretely,\nin addition to the ordinary 2D image diffusion prior, we further optimize the positions of Gaussians\nusing Point-E (Nichol et al., 2022), a pre-trained text-to-point-cloud diffusion model. Instead of\ndirectly aligning the Gaussians with a Point-E generated point cloud, we apply a 3D SDS loss to\nlead the positions inspired by image diffusion SDS, which avoids challenges including registration,\nscaling, and potential degeneration. Notably, we only apply the Point-E SDS gradients to positions,\nas empirical observations suggest that Point-E may generate relatively simple color patterns. We\nsummarize the loss in the geometry optimization stage as the following equation:\n\u2207\u03b8Lgeometry = E\u03f5I,t\n\u0014\nwI(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5I)\u2202x\n\u2202\u03b8\n\u0015\n+ \u03bb3D \u00b7 E\u03f5P ,t [wP (t)(\u03f5\u03c8(pt; y, t) \u2212 \u03f5P )] , (3)\n5\nwhere pt and xt represent the noisy Gaussian positions and the rendered image, w\u2217 and \u03f5\u2217 refer to\nthe corresponding weighting function and Gaussian noise.\n4.2\nAPPEARANCE REFINEMENT\nWhile the introduction of 3D prior does help in learning a more reasonable geometry, we ex-\nperimentally find it would also disturb the learning of appearance, resulting in insufficiently\ndetailed assets.\nBased on this observation, GSGEN employs another appearance refinement\nstage that iteratively refines and densifies the Gaussians utilizing only the 2D image prior.\nFigure 4: An illustration of the proposed\ncompactness-based densification. For two\nGaussians, if the distance between them (r12)\nand smaller than the sum of their radius\n(r1 + r2), a Gaussian will be augmented to\nachieve a more complete geometry.\nTo densify the Gaussians, Kerbl et al. (2023) propose\nto split Gaussians with a large view-space spatial\ngradient. However, we encountered challenges in\ndetermining the appropriate threshold for this spatial\ngradient under score distillation sampling. Due to\nthe stochastic nature of SDS loss, employing a small\nthreshold is prone to be misled by some stochastic\nlarge gradient thus generating an excessive number\nof Gaussians, whereas a large threshold will lead to\na blurry appearance, as illustrated in Fig.8. To tackle\nthis, we propose compactness-based densification as\na supplement to positional gradient-based split with\na larger threshold. Specifically, for each Gaussian,\nwe first obtain its K nearest neighbors with a KD-\nTree. Then, for each of the neighbors, if the distance\nbetween the Gaussian and its neighbor is smaller\nthan the sum of their radius, a Gaussian will be added\nbetween them with a radius equal to the residual. As\nillustrated in Fig.4, compactness-based densification could \u201dfill the holes\u201d, resulting in a more\ncomplete geometry structure. To prune unnecessary Gaussians, we add an extra loss to regularize\nopacity with a weight proportional to its distance to the center and remove Gaussians with opacity\nsmaller than a threshold \u03b1min periodically. Furthermore, we recognize the importance of ensuring\nthe geometry consistency of the Gaussians throughout the refinement phase. With this concern, we\npenalize Gaussians which deviates significantly from their positions obtained during the preceding\ngeometry optimization. The loss function in the appearance refinement stage is summarized as the\nfollowing:\n\u2207\u03b8Lrefine = \u03bbSDSE\u03f5I,t\n\u0014\nwI(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5I)\u2202x\n\u2202\u03b8\n\u0015\n+\u03bbmean\u2207\u03b8\nX\ni\n||pi||+\u03bbopacity\u2207\u03b8\nX\ni\nsg(||pi||)\u00b7oi,\n(4)\nwhere sg(\u00b7) refers to the stop gradient operation, pi and oi represents the position and opacity of the\ni-th Gaussian respectively. \u03bbSDS, \u03bbmean and \u03bbopacity are loss weights.\n4.3\nINITIALIZATION WITH GEOMETRY PRIOR\nw/o Point-E color\nw/ Point-E color\nFigure 6: The impact of adopting Point-E gener-\nated color.\nPrevious studies (Chen et al., 2023; Lin et al.,\n2023; Metzer et al., 2022) have demonstrated\nthe critical importance of starting with a rea-\nsonable geometry initialization. In our early\nexperiments, we also found that initializing with\na simple pattern could potentially lead to a de-\ngenerated 3D object. To overcome this, we opt\nfor initializing the positions of the Gaussians\neither with a generated point cloud or with a 3D\nshape provided by the users (either a mesh or a\npoint cloud). In the context of general text-to-\n3D generation, we employ a text-to-point-cloud\ndiffusion model, Point-E (Nichol et al., 2022),\nto generate a rough geometry according to the\ntext prompt. While Point-E can produce colored point clouds, we opt for random color initialization\n6\nMagic3D\nFantasia3D\nGSGEN (ours)\nA 3D model of an adorable cottage with a thatched roof.\nA ripe strawberry\nA DSLR photo of an ice cream sundae\nA DSLR photo of car made out of sushi\nMagic3D\nGSGEN (Ours)\nFantasia3D\nGSGEN (Ours)\nA bagel filled with cream cheese and lox\nA DSLR photo of banana\nA peacock on a surfboard\nA car made out of cheese\nFigure 5: Qualitative comparison between the proposed GSGEN and state-of-the-art generation\nmethods, including DreamFusion (Poole et al., 2023), Magic3D (Lin et al., 2023), Fantasia3D (Chen\net al., 2023). Our approach achieves better visual quality, especially in high-frequency details, such\nas the thatched roof and the surface of the strawberry. The prompts are provided under the images.\nFor more qualitative comparison results, please refer to Appendix B.3. Videos of these images are\nprovided in the supplemental video.\nbased on empirical observations, as direct utilization of the generated colors has been found to have\ndetrimental effects in early experiments (shown in Fig.6). The scales and opacities of the Gaussians\nare assigned with fixed values, and the rotation matrix is set to the identity matrix. For user-guided\ngeneration, we convert the preferred shape to a point cloud. To avoid too many vertices in the\nprovided shape, we use farthest point sampling (Eldar et al., 1997) for point clouds and uniform\nsurface sampling for meshes to extract a subset of the original shape instead of directly using all the\nvertices or points.\n5\nEXPERIMENTS\nIn this section, we present our experiments on validating the effectiveness of the proposed approach.\nSpecifically, we compare GSGEN with previous state-of-the-art methods in general text-to-3D genera-\ntion. Additionally, we conduct several ablation studies to evaluate the importance of initialization,\n3D guidance, and densification strategy. The detailed results are shown as follows.\n7\n(a) w/o initialization\n(b) w/o 3D guidance\n(c) Coarse Model\n(d) Full\nA zoomed out DSLR photo of a corgi wearing a top hat\nA high quality photo of a furry dog\nA DSLR photo of a streaming engine train, high resolution\nA DSLR photo of a panda\nFigure 7: Ablation study results on initialization and 3D prior. Coarse Model here refers to the rough\nassets obtained after geometry optimization. We can observe that the contents generated with random\ninitialization suffer from degeneration with completely inconsistent geometry (in the first column).\nAlthough the Point-E initialized assets have a slightly better geometry, they still suffer from the Janus\nproblem. The proposed GSGEN utilizes Point-E initialization and 3D guidance to generate shapes\nwith better multi-view consistency.\n5.1\nIMPLEMENTATION DETAILS\nGuidance model setup. We implement the guidance model based on the publicly available diffusion\nmodel, StableDiffusion (Rombach et al., 2022; von Platen et al., 2022). For the guidance scale, we\nadopt 100 for StableDiffusion as suggested in DreamFusion and other works. We also exploit the\nview-dependent prompt technique proposed by DreamFusion. All the assets demonstrated in this\nsection are obtained with StableDiffusion checkpoint runwayml/stable-diffusion-v1-5.\n3D Gaussian Splatting setup. We implement the 3D Gaussian Splatting with a pytorch CUDA\nextension, and further add learnable background support to facilitate our application. For densification,\nwe split the Gaussians by view-space position gradient every 500 iterations with a threshold Tpos =\n0.02, as suggested by the original implementation (Kerbl et al., 2023), and perform compactness-\nbased densification every 1000 iterations which we empirically found effective for achieving a\ncomplete geometry. For pruning, we remove Gaussians with opacity lower than \u03b1min = 0.05, and\nexcessively large world-space or view-space radius every 200 iterations.\nTraning setup. We use the same focal length, elevation, and azimuth range as those of DreamFusion\n(Poole et al., 2023). To sample more uniformly in the camera position, we employ a stratified\nsampling on azimuth. We choose the loss weight hyperparameters \u03bbSDS = 0.1 and \u03bb3D = 0.01\nin geometry optimization stage, and \u03bbSDS = 0.1, \u03bbmean = 1.0 and \u03bbopacity = 100.0 in appearance\nrefinement.\n5.2\nTEXT-TO-3D GENERATION\nWe evaluate the performance of the proposed GSGEN in the context of general text-to-3D generation\nand present qualitative comparison results against state-of-the-art methods. As illustrated in Fig.2, our\napproach produces delicate 3D assets with more accurate geometry and intricate details. In contrast,\nprevious state-of-the-art methods (Tang, 2022; Poole et al., 2023; Lin et al., 2023; Guo et al., 2023;\nChen et al., 2023) struggle in generating collapsed geometry under the same guidance and prompt,\nwhich underscores the effectiveness of our approach. We present more qualitative comparison results\nin Fig.5, where we compare the 3D assets generated by GSGEN with those generated by Magic3D\n(Lin et al., 2023) and Fantasia3D (Chen et al., 2023). Our approach showcases notable enhancements\nin preserving high-frequency details such as the intricate patterns on sushi, the feathers of the peacock,\nand the thatched roof. In contrast, Magic3D and Fantasia3D yield over-smooth geometry due to the\n8\nlimitation of mesh-based methods, making the generated assets less realistic. For more one-to-one\nqualitative comparisons, please refer to the supplemental material for the video results and appendix\nB.3 for multi-view image comparison.\n5.3\nABLATION STUDY\nInitialization. To assess the impact of initialization, we introduce a variant that initiates the positions\nof the Gaussians with an origin-centered Gaussian distribution which emulates the initialization\nadopted in DreamFusion (Poole et al., 2023). The qualitative comparisons are shown in Fig.7a. It\nis evident that assets generated with DreamFusion-like initialization encounter severe degeneration\nissues, especially for prompts depicting asymmetric scenes, resulting in collapsed geometry. In\ncontrast, Point-E initialization breaks the symmetry by providing an anisotropic geometry prior,\nleading to the creation of more 3D-consistent objects.\n3D Prior. We evaluate the necessity of incorporating 3D prior by generating assets without point\ncloud guidance during geometry optimization. The qualitative comparisons of multi-view images\nare visualized in Fig.7b. Although achieved better geometry consistency compared to random\ninitialization, relying solely on image diffusion prior still suffers from the Janus problem, which is\nparticularly evident in cases with asymmetric geometries, such as the dog and the panda. In contrast,\nour approach effectively addresses this issue with the introduction of 3D prior, rectifying potentially\ncollapsed structures in the geometry optimization stage and resulting in a 3D-consistent rough shape.\nTpos = 0.0002\nTpos = 0.02\nTpos = 0.02\n+compactness\nFigure 8: Ablation study on densification strategy. The tex-\ntual prompt used in this figure is A mug of hot chocolate with\nwhipped cream and marshmallows.\nDensification Strategy. To valid the\neffectiveness of the proposed densifi-\ncation strategy, we propose two vari-\nants for comparison: (1) The original\ndensification strategy that split Gaus-\nsians with an average view-space gra-\ndient larger than Tpos = 0.0002. (2)\nWith larger Tpos = 0.02 that avoids\ntoo many new Gaussians. While effec-\ntive in 3D reconstruction, the original\ndensification strategy that relies only\non view-space gradient encounters a\ndilemma in the context of score distil-\nlation sampling: within limited times\nof densification, a large threshold tends to generate an over-smoothed appearance while a small\nthreshold is easily affected by unstable gradients. As shown in Fig.8, the proposed compactness-based\ndensification is an effective supplement to the original densification strategy under SDS guidance.\n6\nLIMITATIONS AND CONCLUSION\nLimitations. GSGEN tends to generate unsatisfying results when the provided text prompt contains a\ncomplex description of the scene or with complicated logic due to the limited language understanding\nability of Point-E and the CLIP text encoder used in StableDiffusion. Moreover, although incorpo-\nrating 3D prior mitigates the Janus problem, it is far from eliminating the potential degenerations,\nespecially when the textual prompt is extremely biased in the guidance diffusion models. Concrete\nfailure cases and corresponding analyses are illustrated in appendix C.\nConclusion. In this paper, we propose GSGEN, a novel method for generating highly detailed and 3D\nconsistent assets using Gaussian Splatting. In particular, we adopt a two-stage optimization strategy\nincluding geometry optimization and appearance refinement. In the geometry optimization stage, a\nrough shape is established under the joint guidance of a point cloud diffusion prior along with the\ncommon image SDS loss. In the subsequent appearance refinement, the Gaussians are further opti-\nmized to enrich details and densified to achieve better continuity and fidelity with compactness-based\ndensification. We conduct comprehensive experiments to validate the effectiveness of the proposed\nmethod, demonstrating its ability to generate 3D consistent assets and superior performance in captur-\ning high-frequency components. We hope our method can serve as an efficient and powerful approach\n9\nfor high-quality text-to-3D generation and could pave the way for more extensive applications of\nGaussians Splatting and direct incorporation of 3D prior.\nREFERENCES\nAlex, Misha Konstantinov, apolin\u00b4ario, Daria Bakshandaeva, Ksenia Ivanova, Sayak Paul, Will\nBerman, and Emad. deep-floyd/if, 6 2023. URL https://github.com/deep-floyd/IF.\nMohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-imagine the negative prompt algorithm: Transform 2d diffusion into 3d, alleviate janus problem\nand beyond. arXiv preprint arXiv:2304.04968, 2023.\nBenjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew\nO\u2019Toole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with ray-conditioned sampling.\narXiv preprint arXiv:2301.02238, 2023.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-\nimage diffusion models with an ensemble of expert denoisers. CoRR, abs/2211.01324, 2022. doi:\n10.48550/arXiv.2211.01324. URL https://doi.org/10.48550/arXiv.2211.01324.\nFan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal\nreverse variance in diffusion probabilistic models. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\nURL https://openreview.net/forum?id=0xiJLKH-ufZ.\nFan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth\nwords: A vit backbone for diffusion models. In CVPR, 2023.\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and\nPratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5855\u20135864,\n2021.\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Zip-nerf:\nAnti-aliased grid-based neural radiance fields. ICCV, 2023.\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields.\narXiv preprint arXiv:2203.09517, 2022a.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appear-\nance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), October 2023.\nXingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, and Jue Wang. Hallucinated\nneural radiance fields in the wild. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 12943\u201312952, 2022b.\nYongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia.\nTANGO: text-driven\nphotorealistic\nand\nrobust\n3d\nstylization\nvia\nlighting\ndecomposition.\nIn\nNeurIPS,\n2022c.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\nc7b925e600ae4880f5c5d7557f70a72b-Abstract-Conference.html.\nYen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liangyan Gui. SDFusion:\nMultimodal 3d shape completion, reconstruction, and generation. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2023.\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In\nMarc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman\nVaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp.\n8780\u20138794, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html.\n10\nYuval Eldar, Michael Lindenbaum, Moshe Porat, and Yehoshua Y. Zeevi. The farthest point strategy\nfor progressive image sampling.\nIEEE Trans. Image Process., 6(9):1305\u20131315, 1997.\ndoi:\n10.1109/83.623193. URL https://doi.org/10.1109/83.623193.\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA,\nJune 18-24, 2022, pp. 10686\u201310696. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01043. URL\nhttps://doi.org/10.1109/CVPR52688.2022.01043.\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-\nHao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified\nframework for 3d content generation. https://github.com/threestudio-project/\nthreestudio, 2023.\nPeter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking\nneural radiance fields for real-time view synthesis. ICCV, 2021.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi:\n10.48550/arXiv.2207.12598. URL https://doi.org/10.48550/arXiv.2207.12598.\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nIn\nHugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-\nTien Lin (eds.), Advances in Neural Information Processing Systems 33:\nAnnual Con-\nference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for\nhigh resolution images. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\nSivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning\nResearch, pp. 13213\u201313232. PMLR, 2023. URL https://proceedings.mlr.press/\nv202/hoogeboom23a.html.\nAjay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 857\u2013866. IEEE, 2022.\ndoi: 10.1109/CVPR52688.2022.00094. URL https://doi.org/10.1109/CVPR52688.\n2022.00094.\nHeewoo Jun and Alex Nichol.\nShap-e: Generating conditional 3d implicit functions.\nCoRR,\nabs/2305.02463, 2023.\ndoi: 10.48550/arXiv.2305.02463.\nURL https://doi.org/10.\n48550/arXiv.2305.02463.\nBernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and George Drettakis. 3d gaussian splatting\nfor real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL\nhttps://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/.\nNasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Popa Tiberiu. Clip-mesh: Generating\ntextured meshes from text using pretrained image-text models. SIGGRAPH Asia 2022 Conference\nPapers, December 2022.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. https://arxiv.org/abs/2303.11328, 2023a.\nXingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for\nhigh-quality diffusion-based text-to-image generation. arXiv preprint arXiv:2309.06380, 2023b.\n11\nJonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp,\nTsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object\nsynthesis. In International Conference on Computer Vision ICCV, 2023.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022.\nRicardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy,\nand Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo\nCollections. In CVPR, 2021.\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022.\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-\ndriven neural stylization for meshes. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 13482\u201313492. IEEE, 2022.\ndoi: 10.1109/CVPR52688.2022.01313. URL https://doi.org/10.1109/CVPR52688.\n2022.01313.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primi-\ntives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15, July 2022.\ndoi: 10.1145/3528223.3530127. URL https://doi.org/10.1145/3528223.3530127.\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts.\nCoRR, abs/2212.08751, 2022.\ndoi:\n10.48550/arXiv.2212.08751. URL https://doi.org/10.48550/arXiv.2212.08751.\nKeunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,\nRicardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for\ntopologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint\narXiv:2212.09748, 2022.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach.\nSDXL: improving latent diffusion models for high-resolution\nimage synthesis. CoRR, abs/2307.01952, 2023. doi: 10.48550/arXiv.2307.01952. URL https:\n//doi.org/10.48550/arXiv.2307.01952.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. In The Eleventh International Conference on Learning Representations, ICLR 2023,\nKigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/\npdf?id=FjNys5c7VyY.\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural\nradiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10318\u201310327, 2021.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying\nLee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One\nimage to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint\narXiv:2306.17843, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In Marina Meila and\nTong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning\nResearch, pp. 8748\u20138763. PMLR, 2021. URL http://proceedings.mlr.press/v139/\nradford21a.html.\n12\nAmit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran\nZada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, and Varun Jampan.\nDreamBooth3D: Subject-driven text-to-3d generation. In International Conference on Computer\nVision ICCV, 2023.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. doi: 10.48550/\narXiv.2204.06125. URL https://doi.org/10.48550/arXiv.2204.06125.\nChristian Reiser, Richard Szeliski, Dor Verbin, Pratul P. Srinivasan, Ben Mildenhall, Andreas Geiger,\nJonathan T. Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view\nsynthesis in unbounded scenes. SIGGRAPH, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 10674\u2013\n10685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.org/10.\n1109/CVPR52688.2022.01042.\nChitwan\nSaharia,\nWilliam\nChan,\nSaurabh\nSaxena,\nLala\nLi,\nJay\nWhang,\nEmily\nL.\nDenton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol\nAyan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi.\nPho-\ntorealistic\ntext-to-image\ndiffusion\nmodels\nwith\ndeep\nlanguage\nunderstanding.\nIn\nNeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\nec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html.\nAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, and Marco Fumero.\nClip-forge: Towards zero-shot text-to-shape generation. arXiv preprint arXiv:2110.02624, 2021.\nSara Fridovich-Keil and Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht, and Angjoo\nKanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023.\nJunyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa\nKim, Jiyoung Lee, and Seungryong Kim. Let 2d diffusion model know 3d-consistency for robust\ntext-to-3d generation. arXiv preprint arXiv:2303.07937, 2023.\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra:\na hybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2021.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th Interna-\ntional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net, 2021a. URL https://openreview.net/forum?id=St1giarCHLP.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In 9th Interna-\ntional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net, 2021b. URL https://openreview.net/forum?id=PxTIG12RRHS.\nCheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast conver-\ngence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 5459\u20135469, 2022.\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan,\nJonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n8248\u20138258, 2022.\nJiaxiang\nTang.\nStable-dreamfusion:\nText-to-3d\nwith\nstable-diffusion,\n2022.\nhttps://github.com/ashawkey/stable-dreamfusion.\nPatrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https:\n//github.com/huggingface/diffusers, 2022.\n13\nCan Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image\ndriven manipulation of neural radiance fields. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 3825\u20133834.\nIEEE, 2022a. doi: 10.1109/CVPR52688.2022.00381. URL https://doi.org/10.1109/\nCVPR52688.2022.00381.\nFeng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, and Huaping Liu. Mixed neural voxels for fast\nmulti-view video synthesis. arXiv preprint arXiv:2212.00190, 2022b.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian\nchaining: Lifting pretrained 2d diffusion models for 3d generation. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24,\n2023, pp. 12619\u201312629. IEEE, 2023a. doi: 10.1109/CVPR52729.2023.01214. URL https:\n//doi.org/10.1109/CVPR52729.2023.01214.\nTengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen,\nDong Chen, Fang Wen, Qifeng Chen, and Baining Guo. RODIN: A generative model for sculpting\n3d digital avatars using diffusion. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 4563\u20134573. IEEE, 2023b.\ndoi: 10.1109/CVPR52729.2023.00443. URL https://doi.org/10.1109/CVPR52729.\n2023.00443.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023c.\nZhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, and Liefeng Bo. 4k-nerf: High fidelity neural\nradiance fields at ultra high resolutions. arXiv preprint arXiv:2212.04701, 2022c.\nLe Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Jiajun Wu, Caiming Xiong, Ran Xu,\nJuan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, image\nand point cloud for 3d understanding. arXiv preprint arXiv:2212.05171, 2022.\nLe Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Jiajun Wu, Caiming Xiong, Ran\nXu, Juan Carlos Niebles, and Silvio Savarese. Ulip-2: Towards scalable multimodal pre-training\nfor 3d understanding, 2023.\nAlex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo\nKanazawa. Plenoxels: Radiance fields without neural networks. arXiv preprint arXiv:2112.05131,\n2021a.\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for\nreal-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 5752\u20135761, 2021b.\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving\nneural radiance fields. arXiv preprint arXiv:2010.07492, 2020.\nJunzhe Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d with advanced diffusion guidance.\nCoRR, abs/2305.18766, 2023. doi: 10.48550/arXiv.2305.18766. URL https://doi.org/10.\n48550/arXiv.2305.18766.\nMatthias Zwicker, Hanspeter Pfister, Jeroen van Baar, and Markus H. Gross. EWA volume splatting.\nIn Thomas Ertl, Kenneth I. Joy, and Amitabh Varshney (eds.), 12th IEEE Visualization Conference,\nIEEE Vis 2001, San Diego, CA, USA, October 24-26, 2001, Proceedings, pp. 29\u201336. IEEE\nComputer Society, 2001. doi: 10.1109/VISUAL.2001.964490. URL https://doi.org/10.\n1109/VISUAL.2001.964490.\n14\nA\nIMPLEMENTATION DETAILS\n3D Gaussian Splatting Details. Instead of directly using the official 3D Gaussian Splatting code\nprovided by Kerbl et al. (2023), we reimplement this algorithm by ourselves due to the need to\nsupport learnable MLP background. The official 3D Gaussian Splatting implementation propagates\nthe gradients of the Gaussians in inverse order, i.e. the Gaussians rendered last get gradient first.\nOur implementation follows a plenoxel (Yu et al., 2021a) style back propagation that calculates the\ngradient in the rendering order, which we found much easier to incorporate a per-pixel background.\nThe depth maps are rendered using the view-space depth of the centers of the Gaussians, which we\nclaim is accurate enough due to the tiny scale of the Gaussians (Zwicker et al., 2001). Besides, we\nimplement a z-variance renderer to support z-var loss proposed by (Zhu & Zhuang, 2023). However,\nwe found that z-var loss seems to have a limited impact on the generated 3D asset, mainly due to the\nsparsity of Gaussians naturally enforcing a relatively thin surface.\nDuring rendering and optimizing, we follow the original 3D Gaussian Splatting to clamp the opacity\nof the Gaussians into [0.004, 0.99] to ensure a stable gradient and prevent potential overflows or\nunderflows.\nGuidance Details. All the guidance of 2D image diffusion models we used in this paper is provided\nby huggingface diffusers (von Platen et al., 2022). For StableDiffusion guidance, we opt for the\nrunwayml/stable-diffusion-v1-5 checkpoint for all the experiments conducted in this paper. We also\ntest the performance of GSGEN under other checkpoints, including stabilityai/stable-diffusion-2-base\nand stabilityai/stable-diffusion-2-1-base, but no improvements are observed.\nFor Point-E diffusion model and its checkpoints, we directly adopted their official implementation.\nTraining Details. All the assets we demonstrate in this paper and the supplemental video are trained\non 4 NVIDIA 3090 GPUs with a batch size of 8 and take about 30 min to optimize for a prompt.\nThe 3D contents we showcase in this paper and supplemental video are obtained under the same\nhyper-parameter setting since we found our parameters robust toward the input prompt. The number\nof Gaussians after densification is around [1e5, 1e6].\nOpen-Sourced Resources and Corresponding Licenses. We summarize open-sourced code and\nresources with corresponding licenses used in our experiments in the following table.\nTable 1: Open-sourced resources used in the experiment in this work.\nResource\nLicense\nStable DreamFusion (Tang, 2022)\nApache License 2.0\nFantasia3D (Chen et al., 2023)\nApache License 2.0\nthreestudio (Guo et al., 2023)\nApache License 2.0\nStableDiffusion (Rombach et al., 2022)\nMIT License\nDeepFloyd IF (Alex et al., 2023)\nDeepFloyd IF License Agreement\nHuggingFace Diffusers\nApache License 2.0\nOpenAI Point-E\nMIT License\nULIP\nBSD 3-Clause License\nWe use Stable DreamFusion and threestudio to obtain the results of DreamFusion and Magic3D under\nStableDiffusion and on the prompts that are not included in their papers and project pages. since the\noriginal implementation has not been open-sourced due to the usage of private diffusion models. The\nresults of Fatansia3D are obtained by running their official implementation with default parameter\nsetting.\nB\nADDTIONAL RESULTS\nB.1\nUSER-GUIDED GENERATION\n15\nLatent-NeRF\nFantasia3D\nOurs\nLatent-NeRF\nOurs\nFigure 9: Qualitative comparison results on user-guided gen-\neration. The hand of the teddy bear and the lego man\nInitialization is straightforward for 3D\nGaussian Splatting due to its explicit\nnature, thereby automatically support-\ning user-guided generation. We eval-\nuate the proposed GSGEN on user-\nguided generation with shapes pro-\nvided in Latent-NeRF (Metzer et al.,\n2022).\nIn this experiment, the ini-\ntial points are generated by uniformly\nsampling points on the mesh surface.\nTo better preserve the user\u2019s desired\nshape, we opt for a relatively small\nlearning rate for positions. We com-\npare the 3D content generated by GS-\nGEN with those of the state-of-the-\nart user-guided 3D generation method\nLatent-NeRF (Metzer et al., 2022) and\nFantasia-3D (Chen et al., 2023) in\nFig.9. Our proposed GSGEN achieves\nthe best results among all alternatives in both geometry and textures and mostly keeps the geometrical\nprior given by the users.\nB.2\nMORE TEXT-TO-3D RESULTS\nWe present more general text-to-3D generation results of GSGEN in Fig.14 and Fig.15. Our approach\ncan generate 3D assets with accurate geometry and improved fidelity.\nFor more delicate assets generated with GSGEN and their spiral videos, please visit our project page\ngsgen3d.github.io or watch our supplemental video.\nB.3\nMORE QUALITATIVE COMPARISONS\nIn addition to the qualitative comparison in the main text, we provide more comparisons with\nDreamFusion (Poole et al., 2023) in Fig.16 and Fig.17, Magic3D (Lin et al., 2023) in Fig.18,\nFantasia3D (Chen et al., 2023) and LatentNeRF (Metzer et al., 2022) in Fig.19. In order to make a\nfair comparison, the images of these methods are directly copied from their papers or project pages.\nVideo comparisons are presented in the supplemental video.\nB.4\nMORE ABLATIONS\nB.4.1\n3D POINT CLOUD GUIDANCE\nPoint-E guidance\nULIP guidance\nFigure 10: Point clouds optimized under Point-E\nand ULIP. Prompt: A corgi\nExcept for the Point-E (Nichol et al., 2022) used\nin our proposed GSGEN, we also test a CLIP-\nlike text-to-point-cloud generation model ULIP\n(Xue et al., 2022; 2023). While achieving supe-\nrior performance in zero-shot point cloud clas-\nsification, ULIP seems ineffective in the con-\ntext of generation. Fig.10 demonstrates point\nclouds generated under the guidance of ULIP\nand Point-E. Under SDS loss, Point-E can opti-\nmize the point cloud to a consistent rough shape\nwhile ULIP leads to a mess. We substitute the\n3D prior in GSGEN from Point-E to ULIP in\nFig.11, yielding the same results as point cloud\noptimization.\n16\nGSGEN under Point-E guidance\nGSGEN under ULIP guidance\nFigure 11: Text-to-3D generation qualitative comparison with 3D prior as Point-E or ULIP. Prompt:\nA DSLR photo of an ice cream sundae.\nB.4.2\n2D IMAGE GUIDANCE\nExcept for StableDiffusion, we also test the performance of GSGEN under the guidance of DeepFloyd\nIF, another open-sourced cutting-edge text-to-image diffusion model. Compared to StableDiffusion,\nDeepFloyd IF has an Imagen-like architecture and a much more powerful text encoder. We demon-\nstrate the qualitative comparison between GSGEN under different guidance in Fig.20. Obviously,\nassets generated with DeepFloyd IF have a much better text-3D alignment, which is primarily at-\ntributed to the stronger text understanding provided by T-5 encoder than that of CLIP text encoder.\nHowever, due to the modular cascaded design, the input to DeepFloyd IF has to be downsampled to\n64 \u00d7 64, which may result in a blurry appearance compared to those generated under StableDiffusion.\nC\nFAILURE CASES\nDespite the introduction of 3D prior, we could not completely eliminate the Janus problem, due to\nthe ill-posed nature of text-to-3D through 2D prior and the limited capability of the 3D prior we used.\n(a) A high quality photo of an ostrich\n(b) A small red cube is sitting on top of a\nlarge blue cube. red on top, blue on bottom\n(c) A small red cube is sitting on top of a\nlarge blue cube. red on top, blue on bottom\nFigure 12: Two typical failure cases of GSGEN.\nFig.12 showcases two typical failure cases we encountered in our experiments. In Fig.12a, the\ngeometrical structure is correctly established, but the Janus problem happens on the appearance\n(another ostrich head on the back head). Fig.12b demonstrates another failure case caused by\nthe limited language understanding of the guidance model. StableDiffusion also fails to generate\nreasonable images with these prompts, as illustrated in Fig.13.\nFigure 13: Prompts that StableDiffusion cannot correctly process, which leads to the failure of\ncorresponding text-to-3D generation.\n17\nA bunch of blue rose, highly detailed\nA high quality photo of a dragon\nA plush dragon toy\nA zoomed out DSLR photo of a plate of fried chicken\nand waffles with maple syrup on them\nA beautiful dress made of feathers, on a mannequin\nA high quality photo of a blue tulip\nA DSLR photo of a plush triceratops toy, studio lighting,\nhight resolution\nA DSLR photo of a tray of Sushi containing pugs\nA DSLR photo of an origami motorcycle\nA DSLR photo of a pineapple\nFigure 14: More 3D assets generated with GSGEN.\n18\nA high quality photo of a stack of pancakes covered in\nmaple syrup\nA tarantula, highly detailed\nA sliced loaf of fresh bread\nGSGEN\nA high quality photo of a pinecone\nA high quality photo of a durian\nA zoomed out DSLR photo of a table with dim sum on\nit\nA plate of delicious tacos\nA DSLR photo of a bald eagle\nA high quality photo of a chow chow puppy\nA zoomed out DSLR photo of a brain in a jar\nA high quality photo of a kangaroo\nA high quality photo of a furry rabbit\nA zoomed out DSLR photo of a cake in the shape of a train\nFigure 15: More 3D assets generated with GSGEN.\n19\nDreamFusion\nGSGEN\nA DSLR photo of pyramid shaped burrito with a slice cut out of it\nDreamFusion\nGSGEN\nA DSLR photo of a roast turkey on a platter\nDreamFusion\nGSGEN\nA plate of delicious tacos\nDreamFusion\nGSGEN\nA zoomed out DSLR photo of a brain in a jar\nDreamFusion\nGSGEN\nA zoomed out DSLR photo of a cake in the shape of a train\nFigure 16: More comparison results with DreamFusion.\n20\nDreamFusion\nGSGEN\nA zoomed out DSLR photo of an amigurumi motorcycle\nDreamFusion\nGSGEN\nA delicious hamburger\nDreamFusion\nGSGEN\nA zoomed out DSLR photo of a baby dragon\nDreamFusion\nGSGEN\nA zoomed out DSLR photo of a beautiful suit made out of moss, on a mannequin. Studio lighting,\nhigh quality, high resolution\nDreamFusion\nGSGEN\nA zoomed out DSLR photo of a complex movement from an expensive watch with many shiny gears,\nsitting on a table\nFigure 17: More comparison results with DreamFusion.\n21\nMagic3D\nGSGEN\nA zoomed out DSLR photo of a ladybug\nMagic3D\nGSGEN\nA zoomed out DSLR photo of a plate piled high with chocolate chip cookies\nMagic3D\nGSGEN\nA DSLR photo of a tarantula, highly detailed\nMagic3D\nGSGEN\nA DSLR photo of a stack of pancakes covered in maple syrup\nMagic3D\nGSGEN\nA zoomed out DSLR photo of a beautifully carved wooden knight chess piece\nFigure 18: More comparison results with Magic3D.\n22\nFantasia3D\nGSGEN\nA fresh cinnamon roll covered in glaze, high resolution\nFantasia3D\nGSGEN\nA delicious croissant\nLatentNeRF\nGSGEN\nA photo of a vase with sunflowers\nLatentNeRF\nGSGEN\nA house made of lego\nFigure 19: More comparison results with LatentNeRF and Fantasia3D.\n23\nGSGEN with StableDiffusion\nGSGEN with DeepFloyd IF\nA nest with a few white eggs and one golden egg\nA DSLR photo of a very beautiful tiny human heart organic sculpture made of copper wire and\nthreaded pipes, very intricate, curved, Studio lighting, high resolution\nA DSLR photo of a very beautiful small organic sculpture made of fine clockwork and gears with tiny\nruby bearings, very intricate, caved, curved. Studio lighting, High resolution\nAn anthropomorphic tomato eating another tomato\nFigure 20: Qualitative comparison of GSGEN under StableDiffusion guidance and DeepFloyd IF\nguidance.\n24\n"
  },
  {
    "title": "Qwen Technical Report",
    "link": "https://arxiv.org/pdf/2309.16609.pdf",
    "upvote": "28",
    "text": "QWEN TECHNICAL REPORT\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu\nHan, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,\nChengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,\nSinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin\nXu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou,\nXiaohuan Zhou, Tianhang Zhu.\nQwen Team, Alibaba Group\u2217\nABSTRACT\nLarge language models (LLMs) have revolutionized the field of artificial intelli-\ngence, enabling natural language processing tasks that were previously thought\nto be exclusive to humans. In this work, we introduce QWEN1, the first install-\nment of our large language model series. QWEN is a comprehensive language\nmodel series that encompasses distinct models with varying parameter counts. It\nincludes QWEN, the base pretrained language models, and QWEN-CHAT, the chat\nmodels finetuned with human alignment techniques. The base language models\nconsistently demonstrate superior performance across a multitude of downstream\ntasks, and the chat models, particularly those trained using Reinforcement Learning\nfrom Human Feedback (RLHF), are highly competitive. The chat models pos-\nsess advanced tool-use and planning capabilities for creating agent applications,\nshowcasing impressive performance even when compared to bigger models on\ncomplex tasks like utilizing a code interpreter. Furthermore, we have developed\ncoding-specialized models, CODE-QWEN and CODE-QWEN-CHAT, as well as\nmathematics-focused models, MATH-QWEN-CHAT, which are built upon base\nlanguage models. These models demonstrate significantly improved performance\nin comparison with open-source models, and slightly fall behind the proprietary\nmodels.\n\u2217Authors are ordered alphabetically by the last name. Correspondence to: ericzhou.zc@alibaba-inc.com.\n1QWEN is a moniker of Qianwen, which means \u201cthousands of prompts\u201d in Chinese. The pronunciation of\n\u201cQWEN\u201d can vary depending on the context and the individual speaking it. Here is one possible way to pronounce\nit: /kwEn/.\n1\narXiv:2309.16609v1  [cs.CL]  28 Sep 2023\nContents\n1\nIntroduction\n3\n2\nPretraining\n4\n2.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nTokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3\nArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.4\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.5\nContext Length Extension\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.6\nExperimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nAlignment\n9\n3.1\nSupervised Finetuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.1.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.1.2\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nReinforcement Learning from Human Feedback . . . . . . . . . . . . . . . . . . .\n10\n3.2.1\nReward Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2.2\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.3\nAutomatic and Human Evaluation of Aligned Models . . . . . . . . . . . . . . . . .\n11\n3.4\nTool Use, Code Interpreter, and Agent . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4\nCODE-QWEN: Specialized Model for Coding\n16\n4.1\nCode Pretraining\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.2\nCode Supervised Fine-Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.3\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n5\nMATH-QWEN: Specialized Model for Mathematics Reasoning\n17\n5.1\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n5.2\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6\nRelated Work\n20\n6.1\nLarge Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6.2\nAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6.3\nTool Use and Agents\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n6.4\nLLM for Coding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n6.5\nLLM for Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n7\nConclusion\n22\nA Appendix\n36\nA.1\nMore Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nA.1.1\nData Format for QWEN-CHAT . . . . . . . . . . . . . . . . . . . . . . . .\n36\nA.2\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nA.2.1\nAutomatic Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nA.2.2\nHuman Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nA.3 Analysis of Code Interpreter\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n2\n1\nINTRODUCTION\nLarge language models (LLMs) (Radford et al., 2018; Devlin et al., 2018; Raffel et al., 2020; Brown\net al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Anil et al., 2023; Thoppilan et al., 2022; Touvron\net al., 2023a;b) have revolutionized the field of artificial intelligence (AI) by providing a powerful\nfoundation for complex reasoning and problem-solving tasks. These models have the ability to\ncompress vast knowledge into neural networks, making them incredibly versatile agents. With a\nchat interface, LLMs can perform tasks that were previously thought to be the exclusive domain of\nhumans, especially those involving creativity and expertise (OpenAI, 2022; Ouyang et al., 2022; Anil\net al., 2023; Google, 2023; Anthropic, 2023a;b). They can engage in natural language conversations\nwith humans, answering questions, providing information, and even generating creative content such\nas stories, poems, and music. This has led to the development of a wide range of applications, from\nchatbots and virtual assistants to language translation and summarization tools.\nLLMs are not just limited to language tasks. They can also function as a generalist agent (Reed et al.,\n2022; Bai et al., 2022a; Wang et al., 2023a; AutoGPT, 2023; Hong et al., 2023), collaborating with\nexternal systems, tools, and models to achieve the objectives set by humans. For example, LLMs\ncan understand multimodal instructions (OpenAI, 2023; Bai et al., 2023; Liu et al., 2023a; Ye et al.,\n2023; Dai et al., 2023; Peng et al., 2023b), execute code (Chen et al., 2021; Zheng et al., 2023; Li\net al., 2023d), use tools (Schick et al., 2023; LangChain, Inc., 2023; AutoGPT, 2023), and more.\nThis opens up a whole new world of possibilities for AI applications, from autonomous vehicles and\nrobotics to healthcare and finance. As these models continue to evolve and improve, we can expect\nto see even more innovative and exciting applications in the years to come. Whether it\u2019s helping us\nsolve complex problems, creating new forms of entertainment, or transforming the way we live and\nwork, LLMs are poised to play a central role in shaping the future of AI.\n \n \n \n \nPretrain\nModels\nRM \nModels\nSFT \nModels\nRLHF \nModels\nQwen\nQwen-PMP\nQwen-RM\nQwen-Chat\nQwen-Chat-RLHF\nCode-Qwen\nCode-Qwen-Chat\nMath-Qwen-Chat\nQwen-VL\nQwen-VL-Chat\nFigure 1: Model Lineage of the Qwen Series. We have pretrained the language models, namely\nQWEN, on massive datasets containing trillions of tokens. We then use SFT and RLHF to align\nQWEN to human preference and thus we have QWEN-CHAT and specifically its improved version\nQWEN-CHAT-RLHF. Additionally, we also develop specialized models for coding and mathematics,\nsuch as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT based on QWEN with similar\ntechniques. Note that we previously released the multimodal LLM, QWEN-VL and QWEN-VL-\nCHAT (Bai et al., 2023), which are also based on our QWEN base models.\nDespite their impressive capabilities, LLMs are often criticized for their lack of reproducibility,\nsteerability, and accessibility to service providers. In this work, we are pleased to present and release\nthe initial version of our LLM series, QWEN. QWEN is a moniker that derives from the Chinese phrase\nQianwen, which translates to \u201cthousands of prompts\u201d and conveys the notion of embracing a wide\nrange of inquiries. QWEN is a comprehensive language model series that encompasses distinct models\nwith varying parameter counts. The model series include the base pretrained language models, chat\nmodels finetuned with human alignment techniques, i.e., supervised finetuning (SFT), reinforcement\nlearning with human feedback (RLHF), etc., as well as specialized models in coding and math. The\ndetails are outlined below:\n3\n1. The base language models, namely QWEN, have undergone extensive training using up to 3\ntrillion tokens of diverse texts and codes, encompassing a wide range of areas. These models\nhave consistently demonstrated superior performance across a multitude of downstream\ntasks, even when compared to their more significantly larger counterparts.\n2. The QWEN-CHAT models have been carefully finetuned on a curated dataset relevant to task\nperforming, chat, tool use, agent, safety, etc. The benchmark evaluation demonstrates that\nthe SFT models can achieve superior performance. Furthermore, we have trained reward\nmodels to mimic human preference and applied them in RLHF for chat models that can\nproduce responses preferred by humans. Through the human evaluation of a challenging test,\nwe find that QWEN-CHAT models trained with RLHF are highly competitive, still falling\nbehind GPT-4 on our benchmark.\n3. In addition, we present specialized models called CODE-QWEN, which includes CODE-\nQWEN-7B and CODE-QWEN-14B, as well as their chat models, CODE-QWEN-14B-\nCHAT and CODE-QWEN-7B-CHAT. Specifically, CODE-QWEN has been pre-trained\non extensive datasets of code and further fine-tuned to handle conversations related to\ncode generation, debugging, and interpretation. The results of experiments conducted on\nbenchmark datasets, such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021),\nand HumanEvalPack (Muennighoff et al., 2023), demonstrate the high level of proficiency\nof CODE-QWEN in code understanding and generation.\n4. This research additionally introduces MATH-QWEN-CHAT specifically designed to tackle\nmathematical problems. Our results show that both MATH-QWEN-7B-CHAT and MATH-\nQWEN-14B-CHAT outperform open-sourced models in the same sizes with large margins\nand are approaching GPT-3.5 on math-related benchmark datasets such as GSM8K (Cobbe\net al., 2021) and MATH (Hendrycks et al., 2021).\n5. Besides, we have open-sourced QWEN-VL and QWEN-VL-CHAT, which have the versatile\nability to comprehend visual and language instructions. These models outperform the current\nopen-source vision-language models across various evaluation benchmarks and support text\nrecognition and visual grounding in both Chinese and English languages. Moreover, these\nmodels enable multi-image conversations and storytelling. Further details can be found\nin Bai et al. (2023).\nNow, we officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN\nand aligned chat models QWEN-CHAT2. This release aims at providing more comprehensive and\npowerful LLMs at developer- or application-friendly scales.\nThe structure of this report is as follows: Section 2 describes our approach to pretraining and results\nof QWEN. Section 3 covers our methodology for alignment and reports the results of both automatic\nevaluation and human evaluation. Additionally, this section describes details about our efforts in\nbuilding chat models capable of tool use, code interpreter, and agent. In Sections 4 and 5, we delve\ninto specialized models of coding and math and their performance. Section 6 provides an overview\nof relevant related work, and Section 7 concludes this paper and points out our future work.\n2\nPRETRAINING\nThe pretraining stage involves learning vast amount of data to acquire a comprehensive understanding\nof the world and its various complexities. This includes not only basic language capabilities but also\nadvanced skills such as arithmetic, coding, and logical reasoning. In this section, we introduce the\ndata, the model design and scaling, as well as the comprehensive evaluation results on benchmark\ndatasets.\n2.1\nDATA\nThe size of data has proven to be a crucial factor in developing a robust large language model,\nas highlighted in previous research (Hoffmann et al., 2022; Touvron et al., 2023b). To create an\neffective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range\n2GitHub: https://github.com/QwenLM/Qwen.\n4\nFigure 2: Performance of GPT-4, GPT-3.5, the previous 13B SOTA, as well as QWEN-14B. We\ndemonstrate the results on 12 datasets covering multiple domains, including language understanding,\nknowledge, reasoning, etc. QWEN significantly outperforms the previous SOTA of similar model\nsizes, but still lag behind both GPT-3.5 and GPT-4.\nof types, domains, and tasks. Our dataset is designed to meet these requirements and includes public\nweb documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a\nsignificant portion of the data being in English and Chinese.\nTo ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing\nprocedure. For public web data, we extract text from HTML and use language identification tools to\ndetermine the language. To increase the diversity of our data, we employ deduplication techniques,\nincluding exact-match deduplication after normalization and fuzzy deduplication using MinHash\nand LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and\nmachine-learning-based methods. Specifically, we use multiple models to score the content, including\nlanguage models, text-quality scoring models, and models for identifying potentially offensive or\ninappropriate content. We also manually sample texts from various sources and review them to ensure\ntheir quality. To further enhance the quality of our data, we selectively up-sample data from certain\nsources, to ensure that our models are trained on a diverse range of high-quality content. In recent\nstudies (Zeng et al., 2022; Aribandi et al., 2021; Raffel et al., 2020), it has been demonstrated that\npretraining language models with multi-task instructions can enhance their zero-shot and few-shot\nperformance. To further enhance the performance of our model, we have incorporated high-quality\ninstruction data into our pretraining process. To safeguard the integrity of our benchmark assessment,\nwe have adopted a similar approach as Brown et al. (2020) and meticulously eliminated any instruction\n5\nth\nhe\nar\nko\nvi\nzh\nja\ntr\nid\npl\nru\nnl\npt\nit\nde\nes\nfr\nen\ncode\nLanguages\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nCompression Ratio\nModel\nLLaMA-7B\nBaichuan-7B\nChatGLM2-6B\nInternLM-7B\nQwen\nFigure 3: Encoding compression rates of different models. We randomly selected 1 million\ndocument corpora of each language to test and compare the encoding compression rates of different\nmodels (with XLM-R (Conneau et al., 2019), which supports 100 languages, as the base value 1, not\nshown in the figure). As can be seen, while ensuring the efficient decoding of Chinese, English, and\ncode, QWEN also achieves a high compression rate for many other languages (such as th, he, ar, ko,\nvi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr, etc.), equipping the model with strong scalability as well as\nhigh training and inference efficiency in these languages.\nsamples that exhibit a 13-gram overlap with any data present in the test sets utilized in our evaluation.\nGiven the large number of downstream tasks, it is not feasible to repeat this filtering process for all\ntasks. Instead, we have made sure that the instruction data for the reported tasks have undergone our\nfiltering process to ensure their accuracy and reliability. Finally, we have built a dataset of up to 3\ntrillion tokens.\n2.2\nTOKENIZATION\nThe design of vocabulary significantly impacts the training efficiency and the downstream task\nperformance. In this study, we utilize byte pair encoding (BPE) as our tokenization method, following\nGPT-3.5 and GPT-4. We start with the open-source fast BPE tokenizer, tiktoken (Jain, 2022), and\nselect the vocabulary cl100k base as our starting point. To enhance the performance of our model on\nmultilingual downstream tasks, particularly in Chinese, we augment the vocabulary with commonly\nused Chinese characters and words, as well as those in other languages. Also, following Touvron et al.\n(2023a;b), we have split numbers into single digits. The final vocabulary size is approximately 152K.\nThe performance of the QWEN tokenizer in terms of compression is depicted in Figure 3. In this\ncomparison, we have evaluated QWEN against several other tokenizers, including XLM-R (Conneau\net al., 2019), LLaMA (Touvron et al., 2023a), Baichuan (Inc., 2023a), and InternLM (InternLM Team,\n2023). Our findings reveal that QWEN achieves higher compression efficiency than its competitors in\nmost languages. This implies that the cost of serving can be significantly reduced since a smaller\nnumber of tokens from QWEN can convey more information than its competitors. Furthermore, we\nhave conducted preliminary experiments to ensure that scaling the vocabulary size of QWEN does\nnot negatively impact the downstream performance of the pretrained model. Despite the increase\nin vocabulary size, our experiments have shown that QWEN maintains its performance levels in\ndownstream evaluation.\n2.3\nARCHITECTURE\nQWEN is designed using a modified version of the Transformer architecture. Specifically, we have\nadopted the recent open-source approach of training large language models, LLaMA (Touvron et al.,\n2023a), which is widely regarded as the top open-source LLM. Our modifications to the architecture\ninclude:\n6\nTable 1: Model sizes, architectures, and optimization hyper-parameters.\n# of Params\nHidden size\nHeads\nLayers\nLearning rate\nBatch size\nTraining tokens\n1.8B\n2048\n16\n24\n3.0 \u00d7 10\u22124\n4M\n2.2T\n7B\n4096\n32\n32\n3.0 \u00d7 10\u22124\n4M\n2.4T\n14B\n5120\n40\n40\n3.0 \u00d7 10\u22124\n4M\n3.0T\n\u2022 Embedding and output projection. Based on preliminary experimental findings, we have\nopted for the untied embedding approach instead of tying the weights of input embedding\nand output projection. This decision was made in order to achieve better performance with\nthe price of memory costs.\n\u2022 Positional embedding. We have chosen RoPE (Rotary Positional Embedding) (Su et al.,\n2021) as our preferred option for incorporating positional information into our model. RoPE\nhas been widely adopted and has demonstrated success in contemporary large language\nmodels, notably PaLM (Chowdhery et al., 2022; Anil et al., 2023) and LLaMA (Touvron\net al., 2023a;b). In particular, we have opted to use FP32 precision for the inverse frequency\nmatrix, rather than BF16 or FP16, in order to prioritize model performance and achieve\nhigher accuracy.\n\u2022 Bias. For most layers, we remove biases following Chowdhery et al. (2022), but we add\nbiases in the QKV layer of attention to enhance the extrapolation ability of the model (Su,\n2023b).\n\u2022 Pre-Norm & RMSNorm. In modern Transformer models, pre-normalization is the most\nwidely used approach, which has been shown to improve training stability compared to\npost-normalization. Recent research has suggested alternative methods for better training\nstability, which we plan to explore in future versions of our model. Additionally, we have\nreplaced the traditional layer normalization technique described in (Ba et al., 2016) with\nRMSNorm (Jiang et al., 2023). This change has resulted in equivalent performance while\nalso improving efficiency.\n\u2022 Activation function. We have selected SwiGLU (Shazeer, 2020) as our activation function,\na combination of Swish (Ramachandran et al., 2017) and Gated Linear Unit (Dauphin et al.,\n2017). Our initial experiments have shown that activation functions based on GLU generally\noutperform other baseline options, such as GeLU (Hendrycks & Gimpel, 2016). As is\ncommon practice in previous research, we have reduced the dimension of the feed-forward\nnetwork (FFN) from 4 times the hidden size to 8\n3 of the hidden size.\n2.4\nTRAINING\nTo train QWEN, we follow the standard approach of autoregressive language modeling, as described\nin Radford et al. (2018). This involves training the model to predict the next token based on the\ncontext provided by the previous tokens. We train models with context lengths of 2048. To create\nbatches of data, we shuffle and merge the documents, and then truncate them to the specified context\nlengths. To improve computational efficiency and reduce memory usage, we employ Flash Attention\nin the attention modules (Dao et al., 2022). We adopt the standard optimizer AdamW (Kingma & Ba,\n2014; Loshchilov & Hutter, 2017) for pretraining optimization. We set the hyperparameters \u03b21 = 0.9,\n\u03b22 = 0.95, and \u03f5 = 10\u22128. We use a cosine learning rate schedule with a specified peak learning rate\nfor each model size. The learning rate is decayed to a minimum learning rate of 10% of the peak\nlearning rate. All the models are trained with BFloat16 mixed precision for training stability.\n2.5\nCONTEXT LENGTH EXTENSION\nTransformer models have a significant limitation in terms of the context length for their attention\nmechanism. As the context length increases, the quadratic-complexity computation leads to a\ndrastic increase in both computation and memory costs. In this work, we have implemented simple\ntraining-free techniques that are solely applied during inference to extend the context length of\nthe model. One of the key techniques we have used is NTK-aware interpolation (bloc97, 2023).\n7\nTable 2: Overall performance on widely-used benchmarks compared to open-source base models.\nOur largest QWEN model with 14 billion parameters outperforms previous 13B SoTA models on all\ndatasets.\nModel\nParams\nMMLU\nC-Eval\nGSM8K\nMATH\nHumanEval\nMBPP\nBBH\n5-shot\n5-shot\n8-shot\n4-shot\n0-shot\n3-shot\n3-shot\nMPT\n7B\n30.8\n23.5\n9.1\n3.0\n18.3\n22.8\n35.6\n30B\n47.9\n-\n15.2\n3.1\n25.0\n32.8\n38.0\nFalcon\n7B\n27.8\n-\n6.8\n2.3\n-\n11.2\n28.0\n40B\n57.0\n-\n19.6\n5.5\n-\n29.8\n37.1\nChatGLM2\n6B\n47.9\n51.7\n32.4\n6.5\n-\n-\n33.7\nInternLM\n7B\n51.0\n53.4\n31.2\n6.3\n10.4\n14.0\n37.0\n20B\n62.1\n58.8\n52.6\n7.9\n25.6\n35.6\n52.5\nBaichuan2\n7B\n54.7\n56.3\n24.6\n5.6\n18.3\n24.2\n41.6\n13B\n59.5\n59.0\n52.8\n10.1\n17.1\n30.2\n49.0\nLLaMA\n7B\n35.6\n27.3\n11.0\n2.9\n12.8\n17.7\n33.5\n13B\n47.7\n31.8\n20.3\n4.2\n15.8\n22.0\n37.9\n33B\n58.7\n37.5\n42.3\n7.1\n21.7\n30.2\n50.0\n65B\n63.7\n40.4\n54.4\n10.6\n23.7\n37.7\n58.4\nLLAMA 2\n7B\n46.8\n32.5\n16.7\n3.3\n12.8\n20.8\n38.2\n13B\n55.0\n41.4\n29.6\n5.0\n18.9\n30.3\n45.6\n34B\n62.6\n-\n42.2\n6.2\n22.6\n33.0\n44.1\n70B\n69.8\n50.1\n63.3\n13.5\n29.9\n45.0\n64.9\nStableBeluga2\n70B\n68.6\n51.4\n69.6\n14.6\n28.0\n11.4\n69.3\nQWEN\n1.8B\n44.6\n54.7\n21.2\n5.6\n17.1\n14.8\n28.2\n7B\n58.2\n63.5\n51.7\n11.6\n29.9\n31.6\n45.0\n14B\n66.3\n72.1\n61.3\n24.8\n32.3\n40.8\n53.4\nUnlike position interpolation (PI) (Chen et al., 2023a) which scales each dimension of RoPE equally,\nNTK-aware interpolation adjusts the base of RoPE to prevent the loss of high-frequency information\nin a training-free manner. To further improve performance, we have also implemented a trivial\nextension called dynamic NTK-aware interpolation, which is later formally discussed in (Peng et al.,\n2023a). It dynamically changes the scale by chunks, avoiding severe performance degradation.\nThese techniques allow us to effectively extend the context length of Transformer models without\ncompromising their computational efficiency or accuracy.\nQWEN additionally incorporates two attention mechanisms: LogN-Scaling (Chiang & Cholak, 2022;\nSu, 2023a) and window attention (Beltagy et al., 2020). LogN-Scaling rescales the dot product of\nthe query and value by a factor that depends on the ratio of the context length to the training length,\nensuring that the entropy of the attention value remains stable as the context length grows. Window\nattention restricts the attention to a limited context window, preventing the model from attending to\ntokens that are too far away.\nWe also observed that the long-context modeling ability of our model varies across layers, with lower\nlayers being more sensitive in context length extension compared to the higher layers. To leverage\nthis observation, we assign different window sizes to each layer, using shorter windows for lower\nlayers and longer windows for higher layers.\n2.6\nEXPERIMENTAL RESULTS\nTo evaluate the zero-shot and few-shot learning capabilities of our models, we conduct a thor-\nough benchmark assessment using a series of datasets. We compare QWEN with the most recent\nopen-source base models, including LLaMA (Touvron et al., 2023a), LLAMA 2 (Touvron et al.,\n2023b), MPT (Mosaic ML, 2023), Falcon (Almazrouei et al., 2023), Baichuan2 (Yang et al., 2023),\nChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM Team, 2023), XVERSE (Inc., 2023b),\nand StableBeluga2 (Stability AI, 2023). Our evaluation covers a total of 7 popular benchmarks,\n8\nTable 3: Results of QWEN on long-context inference using various techniques. Our experimental\nfindings reveal that the application of our crucial techniques enables the model to consistently achieve\nlow perplexity as the context length increases. This suggests that these techniques play a significant\nrole in enhancing the model\u2019s ability to comprehend and generate lengthy texts.\nModel\nSequence Length\n1024\n2048\n4096\n8192\n16384\nQWEN-7B\n4.23\n3.78\n39.35\n469.81\n2645.09\n+ dynamic ntk\n4.23\n3.78\n3.59\n3.66\n5.71\n+ dynamic ntk + logn\n4.23\n3.78\n3.58\n3.56\n4.62\n+ dynamic ntk + logn + window attn\n4.23\n3.78\n3.58\n3.49\n4.32\nQWEN-14B\n-\n3.46\n22.79\n334.65\n3168.35\n+ dynamic ntk + logn + window attn\n-\n3.46\n3.29\n3.18\n3.42\nwhich are MMLU (5-shot) (Hendrycks et al., 2020), C-Eval (5-shot) (Huang et al., 2023), GSM8K\n(8-shot) (Cobbe et al., 2021), MATH (4-shot) (Hendrycks et al., 2021), HumanEval (0-shot) (Chen\net al., 2021), MBPP (0-shot) (Austin et al., 2021), and BBH (Big Bench Hard) (3 shot) (Suzgun et al.,\n2022). We aim to provide a comprehensive summary of the overall performance of our models across\nthese benchmarks.\nIn this evaluation, we focus on the base language models without alignment and collect the baselines\u2019\nbest scores from their official results and OpenCompass (OpenCompass Team, 2023). The results are\npresented in Table 2.\nOur experimental results demonstrate that the three QWEN models exhibit exceptional performance\nacross all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are\noutperformed by QWEN-14B in 3 tasks. QWEN-7B also performs admirably, surpassing LLaMA2-\n13B and achieving comparable results to Baichuan2-13B. Notably, despite having a relatively small\nnumber of parameters, QWEN-1.8B is capable of competitive performance on certain tasks and even\noutperforms larger models in some instances. The findings highlight the impressive capabilities of\nthe QWEN models, particularly QWEN-14B, and suggest that smaller models, such as QWEN-1.8B,\ncan still achieve strong performance in certain applications.\nTo evaluate the effectiveness of context length extension, Table 3 presents the test results on arXiv3 in\nterms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation,\nLogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of\nour models in the context of over 8192 tokens.\n3\nALIGNMENT\nPretrained large language models have been found to be not aligned with human behavior, making\nthem unsuitable for serving as AI assistants in most cases. Recent research has shown that the use of\nalignment techniques, such as supervised finetuning (SFT) and reinforcement learning from human\nfeedback (RLHF), can significantly improve the ability of language models to engage in natural\nconversation. In this section, we will delve into the details of how QWEN models have been trained\nusing SFT and RLHF, and evaluate their performance in the context of chat-based assistance.\n3.1\nSUPERVISED FINETUNING\nTo gain an understanding of human behavior, the initial step is to carry out SFT, which finetunes a\npretrained LLM on chat-style data, including both queries and responses. In the following sections,\nwe will delve into the details of data construction and training methods.\n3The dataset contains academic papers from https://arxiv.org.\n9\n3.1.1\nDATA\nTo enhance the capabilities of our supervised finetuning datasets, we have annotated conversations\nin multiple styles. While conventional datasets (Wei et al., 2022a) contain a vast amount of data\nprompted with questions, instructions, and answers in natural language, our approach takes it a step\nfurther by annotating human-style conversations. This practice, inspired by Ouyang et al. (2022),\naims at improving the model\u2019s helpfulness by focusing on natural language generation for diverse\ntasks. To ensure the model\u2019s ability to generalize to a wide range of scenarios, we specifically\nexcluded data formatted in prompt templates that could potentially limit its capabilities. Furthermore,\nwe have prioritized the safety of the language model by annotating data related to safety concerns\nsuch as violence, bias, and pornography.\nIn addition to data quality, we have observed that the training method can significantly impact the\nfinal performance of the model. To achieve this, we utilized the ChatML-style format (OpenAI,\n2022), which is a versatile meta language capable of describing both the metadata (such as roles)\nand the content of a turn. This format enables the model to effectively distinguish between various\ntypes of information, including system setup, user inputs, and assistant outputs, among others. By\nleveraging this approach, we can enhance the model\u2019s ability to accurately process and analyze\ncomplex conversational data.\n3.1.2\nTRAINING\nConsistent with pretraining, we also apply next-token prediction as the training task for SFT. We\napply the loss masks for the system and user inputs. More details are demonstrated in Section A.1.1.\nThe model\u2019s training process utilizes the AdamW optimizer, with the following hyperparameters: \u03b21\nset to 0.9, \u03b22 set to 0.95, and \u03f5 set to 10\u22128. The sequence length is limited to 2048, and the batch\nsize is 128. The model undergoes a total of 4000 steps, with the learning rate gradually increased\nover the first 1430 steps, reaching a peak of 2 \u00d7 10\u22126. To prevent overfitting, weight decay is applied\nwith a value of 0.1, dropout is set to 0.1, and gradient clipping is enforced with a limit of 1.0.\n3.2\nREINFORCEMENT LEARNING FROM HUMAN FEEDBACK\nWhile SFT has proven to be effective, we acknowledge that its generalization and creativity capa-\nbilities may be limited, and it is prone to overfitting. To address this issue, we have implemented\nReinforcement Learning from Human Feedback (RLHF) to further align SFT models with human\npreferences, following the approaches of Ouyang et al. (2022); Christiano et al. (2017). This process\ninvolves training a reward model and using Proximal Policy Optimization (PPO) (Schulman et al.,\n2017) to conduct policy training.\n3.2.1\nREWARD MODEL\nTo create a successful reward model, like building a large language model (LLM), it is crucial to\nfirst undergo pretraining and then finetuning. This pretraining process, also known as preference\nmodel pretraining (PMP) (Bai et al., 2022b), necessitates a vast dataset of comparison data. This\ndataset consists of sample pairs, each containing two distinct responses for a single query and their\ncorresponding preferences. Similarly, finetuning is also conducted on this type of comparison data,\nbut with a higher quality due to the presence of quality annotations.\nDuring the fine-tuning phase, we gather a variety of prompts and adjust the reward model based on\nhuman feedback for responses from the QWEN models. To ensure the diversity and complexity of\nuser prompts are properly taken into account, we have created a classification system with around\n6600 detailed tags and implemented a balanced sampling algorithm that considers both diversity and\ncomplexity when selecting prompts for annotation by the reward model (Lu et al., 2023). To generate\na wide range of responses, we have utilized QWEN models of different sizes and sampling strategies,\nas diverse responses can help reduce annotation difficulties and enhance the performance of the\nreward model. These responses are then evaluated by annotators following a standard annotation\nguideline, and comparison pairs are formed based on their scores.\nIn creating the reward model, we utilize the same-sized pre-trained language model QWEN to initiate\nthe process. It is important to mention that we have incorporated a pooling layer into the original\n10\nTable 4: Test Accuracy of QWEN preference model pretraining (PMP) and reward model (RM) on\ndiverse human preference benchmark datasets.\nDataset\nQWEN\nQWEN\nAnthropic\nAnthropic\nOpenAI\nStanford\nOpenAI\nHelpful-base\nHelpful-online\nHelpful-base\nHelpful-online\nSumm.\nSHP\nPRM800K\nPMP\n62.68\n61.62\n76.52\n65.43\n69.60\n60.05\n70.59\nRM\n74.78\n69.71\n73.98\n64.57\n69.99\n60.10\n70.52\nQWEN model to extract the reward for a sentence based on a specific end token. The learning rate for\nthis process has been set to a constant value of 3 \u00d7 10\u22126, and the batch size is 64. Additionally, the\nsequence length is set to 2048, and the training process lasts for a single epoch.\nWe adopted the accuracy on the test dataset as an important but not exclusive evaluation metric for\nthe reward model. In Table 4, we report the test pairwise accuracy of PMP and reward models on\ndiverse human preference benchmark datasets (Bai et al., 2022b; Stiennon et al., 2020; Ethayarajh\net al., 2022; Lightman et al., 2023). Specifically, QWEN Helpful-base and QWEN Helpful-online are\nour proprietary datasets. The responses in QWEN Helpful-base are generated from QWEN without\nRLHF, whereas QWEN Helpful-online includes responses from QWEN with RLHF. The results show\nthat the PMP model demonstrates high generalization capabilities on out-of-distribution data, and the\nreward model demonstrates significant improvement on our QWEN reward datasets.\n3.2.2\nREINFORCEMENT LEARNING\nOur Proximal Policy Optimization (PPO) process involves four models: the policy model, value\nmodel, reference model, and reward model. Before starting the PPO procedure, we pause the policy\nmodel\u2019s updates and focus solely on updating the value model for 50 steps. This approach ensures\nthat the value model can adapt to different reward models effectively.\nDuring the PPO operation, we use a strategy of sampling two responses for each query simultaneously.\nThis strategy has proven to be more effective based on our internal benchmarking evaluations. We set\nthe KL divergence coefficient to 0.04 and normalize the reward based on the running mean.\nThe policy and value models have learning rates of 1 \u00d7 10\u22126 and 5 \u00d7 10\u22126, respectively. To enhance\ntraining stability, we utilize value loss clipping with a clip value of 0.15. For inference, the policy\ntop-p is set to 0.9. Our findings indicate that although the entropy is slightly lower than when top-p is\nset to 1.0, there is a faster increase in reward, ultimately resulting in consistently higher evaluation\nrewards under similar conditions.\nAdditionally, we have implemented a pretrained gradient to mitigate the alignment tax. Empirical\nfindings indicate that, with this specific reward model, the KL penalty is adequately robust to\ncounteract the alignment tax in benchmarks that are not strictly code or math in nature, such as\nthose that test common sense knowledge and reading comprehension. It is imperative to utilize\na significantly larger volume of the pretrained data in comparison to the PPO data to ensure the\neffectiveness of the pretrained gradient. Additionally, our empirical study suggests that an overly\nlarge value for this coefficient can considerably impede the alignment to the reward model, eventually\ncompromising the ultimate alignment, while an overly small value would only have a marginal effect\non alignment tax reduction.\n3.3\nAUTOMATIC AND HUMAN EVALUATION OF ALIGNED MODELS\nTo showcase the effectiveness of our aligned models, we conduct a comparison with other aligned\nmodels on well-established benchmarks, including MMLU (Hendrycks et al., 2020), C-Eval (Huang\net al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and BBH (Suzgun et al.,\n2022). Besides the widely used few-shot setting, we test our aligned models in the zero-shot setting\nto demonstrate how well the models follow instructions. The prompt in a zero-shot setting consists\nof an instruction and a question without any previous examples in the context. The results of the\nbaselines are collected from their official reports and OpenCompass (OpenCompass Team, 2023).\nThe results in Table 5 demonstrate the effectiveness of our aligned models in understanding human\ninstructions and generating appropriate responses. QWEN-14B-Chat outperforms all other models\n11\nTable 5: Performance of aligned models on widely-used benchmarks. We report both zero-shot\nand few-shot performance of the models.\nModel\nParams\nMMLU\nC-Eval\nGSM8K\nHumanEval\nBBH\n0-shot / 5-shot\n0-shot / 5-shot\n0-shot / 8-shot\n0-shot\n0-shot / 3-shot\nProprietary models\nGPT-3.5\n-\n-\n/ 69.1\n-\n/ 52.5\n-\n/ 78.2\n73.2\n-\n/ 70.1\nGPT-4\n-\n-\n/ 83.0\n-\n/ 69.9\n-\n/ 91.4\n86.6\n-\n/ 86.7\nOpen-source models\nChatGLM2\n6B\n45.5 / 46.0\n50.1 / 52.6\n-\n/ 28.8\n11.0\n-\n/ 32.7\nInternLM-Chat\n7B\n-\n/ 51.1\n-\n/ 53.6\n-\n/ 33.0\n14.6\n-\n/ 32.5\nBaichuan2-Chat\n7B\n-\n/ 52.9\n-\n/ 55.6\n-\n/ 32.8\n13.4\n-\n/ 35.8\n13B\n-\n/ 57.3\n-\n/ 56.7\n-\n/ 55.3\n17.7\n-\n/ 49.9\nLLAMA 2-CHAT\n7B\n-\n/ 46.2\n-\n/ 31.9\n-\n/ 26.3\n12.2\n-\n/ 35.6\n13B\n-\n/ 54.6\n-\n/ 36.2\n-\n/ 37.1\n18.9\n-\n/ 40.1\n70B\n-\n/ 63.8\n-\n/ 44.3\n-\n/ 59.3\n32.3\n-\n/ 60.8\nQWEN-CHAT\n1.8B\n42.4 / 43.9\n50.7 / 50.3\n27.8 / 19.5\n14.6\n27.1 / 25.0\n7B\n55.8 / 57.0\n59.7 / 59.3\n50.3 / 54.1\n37.2\n39.6 / 46.7\n14B\n64.6 / 66.5\n69.8 / 71.7\n60.1 / 59.3\n43.9\n46.9 / 58.7\nexcept ChatGPT (OpenAI, 2022) and LLAMA 2-CHAT-70B (Touvron et al., 2023b) in all datasets,\nincluding MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021),\nHumanEval (Chen et al., 2021), and BBH (Suzgun et al., 2022). In particular, QWEN\u2019s performance\nin HumanEval, which measures the quality of generated codes, is significantly higher than that of\nother open-source models.\nMoreover, QWEN\u2019s performance is consistently better than that of open-source models of similar size,\nsuch as LLaMA2 (Touvron et al., 2023b), ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM\nTeam, 2023), and Baichuan2 (Yang et al., 2023). This suggests that our alignment approach, which\ninvolves fine-tuning the model on a large dataset of human conversations, has been effective in\nimproving the model\u2019s ability to understand and generate human-like language.\nDespite this, we have reservations about the ability of traditional benchmark evaluation to accurately\nmeasure the performance and potential of chat models trained with alignment techniques in today\u2019s\nlandscape. The results mentioned earlier provide some evidence of our competitive standing, but we\nbelieve that it is crucial to develop new evaluation methods specifically tailored to aligned models.\nWe believe that human evaluation is crucial, which is why we have created a carefully curated\ndataset for this purpose. Our process involved collecting 300 instructions in Chinese that covered\na wide range of topics, including knowledge, language understanding, creative writing, coding,\nand mathematics. To evaluate the performance of different models, we chose the SFT version of\nQWEN-CHAT-7B and the SFT and RLHF versions of QWEN-CHAT-14B, and added two strong\nbaselines, GPT-3.5 and GPT-44, for comparison. For each instruction, we asked three annotators to\nrank the model responses by the overall score of helpfulness, informativeness, validity, and other\nrelevant factors. Our dataset and evaluation methodology provides a comprehensive and rigorous\nassessment of the capabilities of different language models in various domains.\nFigure 4 illustrates the win rates of the various models. For each model, we report the percentage of\nwins, ties, and losses against GPT-3.5, with the segments of each bar from bottom to top representing\nthese statistics. The experimental results clearly demonstrate that the RLHF model outperforms\nthe SFT models by significant margins, indicating that RLHF can encourage the model to generate\nresponses that are more preferred by humans. In terms of overall performance, we find that the\nRLHF model significantly outperforms the SFT models, falling behind GPT-4. This indicates\nthe effectiveness of RLHF for aligning to human preference. To provide a more comprehensive\nunderstanding of the models\u2019 performance, we include a case study with examples from different\nmodels in Appendix A.2.2. Nonetheless, it remains difficult to accurately capture the gap between our\n4To obtain the results from the models, we use the OpenAI APIs of GPT-3.5-turbo-0613 and GPT-4-0613.\n12\nAverage\nKnowledge\nLanguage UnderstandingCreative Writing\nMath\nCoding\n0\n20\n40\n60\n80\n100\n28.4\n31.1\n24.0\n35.3\n23.3\n17.5\n28.3\n31.1\n46.0\n19.7\n29.2\n22.5\n43.3\n37.8\n30.0\n45.0\n47.5\n60.0\n30.7\n32.8\n29.3\n34.7\n34.2\n15.8\n32.3\n33.3\n54.7\n22.7\n34.2\n25.0\n37.0\n33.9\n16.0\n42.7\n31.7\n59.2\n38.7\n42.2\n39.3\n39.7\n39.2\n30.0\n28.9\n32.8\n33.3\n21.0\n39.2\n26.7\n32.4\n25.0\n27.3\n39.3\n21.7\n43.3\n43.8\n37.2\n36.0\n49.7\n45.8\n46.7\n32.2\n40.0\n47.3\n19.0\n33.3\n33.3\n24.0\n22.8\n16.7\n31.3\n20.8\n20.0\nWinrate (v.s. GPT-3.5)\nQwen-7B-Chat (SFT)\nQwen-14B-Chat (SFT)\nQwen-14B-Chat (RLHF)\nGPT-4\nFigure 4: Results of the human evaluation for chat models. We compare Qwen-7B (SFT), Qwen-\n14B (SFT), Qwen-14B (RLHF), as well as GPT-4 against GPT-3.5. Each bar segment represents the\npercentage of wins, ties, and losses, from bottom to top. On average, the RLHF model outperforms\nthe SFT model. The dataset consists of 300 Chinese instructions.\nmodels and the proprietary models. As such, a more extensive and rigorous assessment is required\nfor the chat models.\n3.4\nTOOL USE, CODE INTERPRETER, AND AGENT\nTable 6: Performance of QWEN on the in-house Chinese benchmark that evaluates its ability to use\nunseen tools via ReAct prompting.\nModel\nParams\nTool Selection (Acc.\u2191)\nTool Input (Rouge-L\u2191)\nFalse Positive Error (%)\u2193\nGPT-4\n-\n95\n90\n15.0\nGPT-3.5\n-\n85\n88\n75.0\nQWEN-CHAT\n1.8B\n92\n89\n19.3\n7B\n98\n91\n7.3\n14B\n98\n93\n2.4\nThe QWEN models, which are designed to be versatile, have the remarkable ability to assist with\n(semi-)automating daily tasks by leveraging their skills in tool-use and planning. As such, they can\nserve as agents or copilots to help streamline various tasks. We explore QWEN\u2019s proficiency in the\nfollowing areas:\n\u2022 Utilizing unseen tools through ReAct prompting (Yao et al., 2022) (see Table 6).\n\u2022 Using a Python code interpreter to enhance math reasoning, data analysis, and more (see\nTable 7 and Table 8).\n\u2022 Functioning as an agent that accesses Hugging Face\u2019s extensive collection of multimodal\nmodels while engaging with humans (see Table 9).\n13\nTable 7: The proportion of code generated by QWEN that is executable on the in-house evaluation\nbenchmark for Code Interpreter. This benchmark examines QWEN\u2019s coding proficiency in math\nproblem solving, data visualization, and general purposes. CODE LLAMA underperforms on\nvisualization tasks because it hallucinates non-existent columns solely based on CSV file names (see\nFigure 5).\nModel\nParams\nCategory\nMath (%)\nVisualization (%)\nGeneral (%)\nAll (%)\nGPT-4\n-\n91.9\n85.9\n82.8\n86.8\nGPT-3.5\n-\n89.2\n65.0\n74.1\n72.9\nLLAMA 2-CHAT\n7B\n41.9\n33.1\n24.1\n33.6\n13B\n50.0\n40.5\n48.3\n44.4\nCODE LLAMA-INSTRUCT\n7B\n85.1\n54.0\n70.7\n65.1\n13B\n93.2\n55.8\n74.1\n68.8\nInternLM-Chat\n7B v1.1\n78.4\n44.2\n62.1\n56.3\n20B\n70.3\n44.2\n65.5\n54.9\nQWEN-CHAT\n1.8B\n33.8\n30.1\n8.6\n26.8\n7B\n82.4\n64.4\n67.2\n70.2\n14B\n89.2\n84.1\n65.5\n81.7\nTable 8: Correctness of the final response on the in-house evaluation benchmark for Code Interpreter.\nVisualization-Hard tasks involve planning multiple steps, while Visualization-Easy tasks do not.\nVisualization-All measures both types of tasks. CODE LLAMA excels in performing Visualization-\nEasy tasks but tends to underperform in Visualization-Hard tasks, due to its inclination to hallucinate\nnon-existent columns based on the name of a CSV file (see Figure 5).\nModel\nParams\nCategory\nMath (%)\nVis.-Hard (%)\nVis.-Easy (%)\nVis.-All (%)\nGPT-4\n-\n82.8\n66.7\n60.8\n63.8\nGPT-3.5\n-\n47.3\n33.3\n55.7\n44.2\nLLAMA 2-CHAT\n7B\n3.9\n14.3\n39.2\n26.4\n13B\n8.3\n8.3\n40.5\n23.9\nCODE LLAMA-INSTRUCT\n7B\n14.3\n26.2\n60.8\n42.9\n13B\n28.2\n27.4\n62.0\n44.2\nInternLM-Chat\n7B v1.1\n28.5\n4.8\n40.5\n22.1\n20B\n34.6\n21.4\n45.6\n33.1\nQWEN-CHAT\n1.8B\n14.7\n3.6\n20.3\n11.7\n7B\n41.9\n40.5\n54.4\n47.2\n14B\n58.4\n53.6\n59.5\n56.4\n14\nTable 9: Results of QWEN-Chat on the Hugging Face Agent benchmark.\nTask\nModel\nParams\nMetric\nTool Selection \u2191\nTool Used \u2191\nCode Correctness \u2191\nRun Mode\nGPT-4\n-\n100\n100\n97.4\nGPT-3.5\n-\n95.4\n96.3\n87.0\nStarcoder-Base\n15B\n86.1\n87.0\n68.9\nStarcoder\n15B\n87.0\n88.0\n68.9\nQWEN-CHAT\n1.8B\n85.2\n84.3\n61.1\n7B\n87.0\n87.0\n71.5\n14B\n93.5\n94.4\n87.0\nChat Mode\nGPT-4\n-\n97.9\n97.9\n98.5\nGPT-3.5\n-\n97.3\n96.8\n89.6\nStarcoder-Base\n15B\n97.9\n97.9\n91.1\nStarcoder\n15B\n97.9\n97.9\n89.6\nQWEN-CHAT\n1.8B\n93.6\n93.6\n73.2\n7B\n94.7\n94.7\n85.1\n14B\n97.9\n97.9\n95.5\nTo enhance QWEN\u2019s capabilities as an agent or copilot, we employ the self-instruct (Wang et al.,\n2023c) strategy for SFT. Specifically, we utilize the in-context learning capability of QWEN for\nself-instruction. By providing a few examples, we can prompt QWEN to generate more relevant\nqueries and generate outputs that follow a specific format, such as ReAct (Yao et al., 2022). We then\napply rules and involve human annotators to filter out any noisy samples. Afterwards, the samples\nare incorporated into QWEN\u2019s training data, resulting in an updated version of QWEN that is more\ndependable for self-instruction. We iterate through this process multiple times until we gather an\nample number of samples that possess both exceptional quality and a wide range of diversity. As a\nresult, our final collection consists of around 2000 high-quality samples.\nDuring the finetuning process, we mix these high-quality samples with all the other general-purpose\nSFT samples, rather than introducing an additional training stage. By doing so, we are able to retain\nessential general-purpose capabilities that are also pertinent for constructing agent applications.\nUsing Tools via ReAct Prompting\nWe have created and made publicly available a benchmark\nfor evaluating QWEN\u2019s ability to call plugins, tools, functions, or APIs using ReAct Prompting (see\nQwen Team, Alibaba Group, 2023b). To ensure fair evaluation, we have excluded any plugins that\nwere included in QWEN\u2019s training set from the evaluation set. The benchmark assesses the model\u2019s\naccuracy in selecting the correct plugin from a pool of up to five candidates, as well as the plausibility\nof the parameters passed into the plugin and the frequency of false positives. In this evaluation, a\nfalse positive occurs when the model incorrectly invokes a plugin in response to a query, despite not\nbeing required to do so.\nThe results presented in Table 6 demonstrate that QWEN consistently achieves higher accuracy in\nidentifying the relevance of a query to the available tools as the model size increases. However,\nthe table also highlights that beyond a certain point, there is little improvement in performance\nwhen it comes to selecting the appropriate tool and providing relevant arguments. This suggests that\nthe current preliminary benchmark may be relatively easy and may require further enhancement in\nfuture iterations. It is worth noting that GPT-3.5 stands out as an exception, displaying suboptimal\nperformance on this particular benchmark. This could potentially be attributed to the fact that the\nbenchmark primarily focuses on the Chinese language, which may not align well with GPT-3.5\u2019s\ncapabilities. Additionally, we observe that GPT-3.5 tends to attempt to use at least one tool, even if\nthe query cannot be effectively addressed by the provided tools.\nUsing Code Interpreter for Math Reasoning and Data Analysis\nThe Python code interpreter\nis widely regarded as a powerful tool for augmenting the capabilities of an LLM agent. It is\n15\nworth investigating whether QWEN can harness the full potential of this interpreter to enhance its\nperformance in diverse domains, such as mathematical reasoning and data analysis. To facilitate this\nexploration, we have developed and made publicly available a benchmark that is specifically tailored\nfor this purpose (see Qwen Team, Alibaba Group, 2023a).\nThe benchmark encompasses three primary categories of tasks: math problem-solving, data visu-\nalization, and other general-purpose tasks like file post-processing and web crawling. Within the\nvisualization tasks, we differentiate between two levels of difficulty. The easier level can be achieved\nby simply writing and executing a single code snippet without the need for advanced planning skills.\nHowever, the more challenging level requires strategic planning and executing multiple code snippets\nin a sequential manner. This is because the subsequent code must be written based on the output of\nthe previous code. For example, an agent may need to examine the structure of a CSV file using one\ncode snippet before proceeding to write and execute additional code to create a plot.\nRegarding evaluation metrics, we consider both the executability and correctness of the generated\ncode. To elaborate on the correctness metrics, for math problems, we measure accuracy by verifying\nif the ground truth numerical answer is present in both the code execution result and the final response.\nWhen it comes to data visualization, we assess accuracy by utilizing QWEN-VL (Bai et al., 2023),\na powerful multimodal language model. QWEN-VL is capable of answering text questions paired\nwith images, and we rely on it to confirm whether the image generated by the code fulfills the user\u2019s\nrequest.\nThe results regarding executability and correctness are presented in Table 7 and Table 8, respectively.\nIt is evident that CODE LLAMA generally outperforms LLAMA 2, its generalist counterpart, which\nis not surprising since this benchmark specifically requires coding skills. However, it is worth noting\nthat specialist models that are optimized for code synthesis do not necessarily outperform generalist\nmodels. This is due to the fact that this benchmark encompasses various skills beyond coding,\nsuch as abstracting math problems into equations, understanding language-specified constraints, and\nresponding in the specified format such as ReAct. Notably, QWEN-7B-CHAT and QWEN-14B-CHAT\nsurpass all other open-source alternatives of similar scale significantly, despite being generalist\nmodels.\nServing as a Hugging Face Agent\nHugging Face provides a framework called the Hugging Face\nAgent or Transformers Agent (Hugging Face, 2023), which empowers LLM agents with a curated set\nof multimodal tools, including speech recognition and image synthesis. This framework allows an\nLLM agent to interact with humans, interpret natural language commands, and employ the provided\ntools as needed.\nTo evaluate QWEN\u2019s effectiveness as a Hugging Face agent, we utilized the evaluation benchmarks\noffered by Hugging Face. The results are presented in Table 9. The evaluation results reveal that\nQWEN performs quite well in comparison to other open-source alternatives, only slightly behind the\nproprietary GPT-4, demonstrating QWEN\u2019s competitive capabilities.\n4\nCODE-QWEN: SPECIALIZED MODEL FOR CODING\nTraining on domain-specific data has been shown to be highly effective, particularly in the case\nof code pretraining and finetuning. A language model that has been reinforced with training on\ncode data can serve as a valuable tool for coding, debugging, and interpretation, among other tasks.\nIn this work, we have developed a series of generalist models using pretraining and alignment\ntechniques. Building on this foundation, we have created domain-specific models for coding by\nleveraging the base language models of QWEN, including continued pretrained model, CODE-QWEN\nand supervised finetuned model, CODE-QWEN-CHAT. Both models have 14 billion and 7 billion\nparameters versions.\n4.1\nCODE PRETRAINING\nWe believe that relying solely on code data for pretraining can result in a significant loss of the ability\nto function as a versatile assistant. Unlike previous approaches that focused solely on pretraining\non code data (Li et al., 2022; 2023d), we take a different approach (Rozi`ere et al., 2023) by starting\nwith our base models QWEN trained on a combination of text and code data, and then continuing to\n16\npretrain on the code data. We continue to pretrain the models on a total of around 90 billion tokens.\nDuring the pre-training phase, we initialize the model using the base language models QWEN. Many\napplications that rely on specialized models for coding may encounter lengthy contextual scenarios,\nsuch as tool usage and code interpretation, as mentioned in Section 3.4. To address this issue, we\ntrain our models with context lengths of up to 8192. Similar to base model training in Section\n2.4, we employ Flash Attention (Dao et al., 2022) in the attention modules, and adopt the standard\noptimizer AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017), setting \u03b21 = 0.9, \u03b22 = 0.95,\nand \u03f5 = 10\u22128. We set the learning rate as 6.0 \u00d7 10\u22125 for CODE-QWEN-14B and 3.0 \u00d7 10\u22125 for\nCODE-QWEN-7B, with 3% warm up iterations and no learning rate decays.\n4.2\nCODE SUPERVISED FINE-TUNING\nAfter conducting a series of empirical experiments, we have determined that the multi-stage SFT\nstrategy yields the best performance compared to other methods. In the supervised fine-tuning stage,\nthe model CODE-QWEN-CHAT initialized by the code foundation model CODE-QWEN are optimized\nby the AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) optimizer (\u03b21 = 0.9, \u03b22 = 0.95,\n\u03f5 = 10\u22128) with a learning rate of 2.0 \u00d7 10\u22126 and 1.0 \u00d7 10\u22125 for the 14B and 7B model respectively.\nThe learning rate increases to the peaking value with the cosine learning rate schedule (3% warm-up\nsteps) and then remains constant.\n4.3\nEVALUATION\nOur CODE-QWEN models have been compared with both proprietary and open-source language\nmodels, as shown in Tables 10 and 11. These tables present the results of our evaluation on the\ntest sets of Humaneval (Chen et al., 2021), MBPP (Austin et al., 2021), and the multi-lingual code\ngeneration benchmark HUMANEVALPACK (Muennighoff et al., 2023). The comparison is based on\nthe pass@1 performance of the models on these benchmark datasets. The results of this comparison\nare clearly demonstrated in Tables 10 and 11.\nOur analysis reveals that specialized models, specifically CODE-QWEN and CODE-QWEN-CHAT, sig-\nnificantly outperform previous baselines with similar parameter counts, such as OCTOGEEX (Muen-\nnighoff et al., 2023), InstructCodeT5+ (Wang et al., 2023d), and CodeGeeX2 (Zheng et al., 2023). In\nfact, these models even rival the performance of larger models like Starcoder (Li et al., 2023d).\nWhen compared to some of the extremely large-scale closed-source models, CODE-QWEN and CODE-\nQWEN-CHAT demonstrate clear advantages in terms of pass@1. However, it is important to note that\nthese models fall behind the state-of-the-art methods, such as GPT-4, in general. Nonetheless, with\nthe continued scaling of both model size and data size, we believe that this gap can be narrowed in\nthe near future.\nIt is crucial to emphasize that the evaluations mentioned previously are insufficient for grasping\nthe full extent of the strengths and weaknesses of the models. In our opinion, it is necessary to\ndevelop more rigorous tests to enable us to accurately assess our relative performance in comparison\nto GPT-4.\n5\nMATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING\nWe have created a mathematics-specialized model series called MATH-QWEN-CHAT, which is\nbuilt on top of the QWEN pretrained language models. Specifically, we have developed assistant\nmodels that are specifically designed to excel in arithmetic and mathematics and are aligned with\nhuman behavior. We are releasing two versions of this model series, MATH-QWEN-14B-CHAT and\nMATH-QWEN-7B-CHAT, which have 14 billion and 7 billion parameters, respectively.\n5.1\nTRAINING\nWe carry out math SFT on our augmented math instructional dataset for mathematics reasoning,\nand therefore we obtain the chat model, MATH-QWEN-CHAT, directly. Owing to shorter average\nlengths of the math SFT data, we use a sequence length of 1024 for faster training. Most user inputs\nin the math SFT dataset are examination questions, and it is easy for the model to predict the input\n17\nTable 10: Results of pass@1 (%) on HumanEval and MBPP. Most scores are retrieved from the\npapers of StarCoder (Li et al., 2023d), CodeT5+ (Wang et al., 2023d), WizardCoder (Luo et al.,\n2023b) and CODE LLAMA (Rozi`ere et al., 2023).\nModel\nParams\nHumanEval\nMBPP\nProprietary models\nPaLM\n540B\n26.2\n36.8\nPaLM-Coder\n540B\n36.0\n47.0\nPaLM 2-S\n-\n37.6\n50.0\nCode-Cushman-001\n-\n33.5\n45.9\nCode-Davinci-002\n-\n47.0\n58.1\nGPT-3.5\n-\n73.2\n-\nGPT-4\n-\n86.6\n-\nOpen-source models\nLLAMA 2\n7B\n12.2\n20.8\n13B\n20.1\n27.6\n34B\n22.6\n33.8\n70B\n30.5\n45.4\nCodeGen-Multi\n16B\n18.3\n20.9\nCodeGen-Mono\n16B\n29.3\n35.3\nCodeGeeX2\n6B\n35.9\n-\nStarCoder-Prompted\n15B\n40.8\n49.5\nCodeT5+\n16B\n30.9\n-\nInstructCodeT5+\n16B\n35.0\n-\nCODE LLAMA\n7B\n33.5\n41.4\n13B\n36.0\n47.0\n34B\n48.8\n55.0\nCODE LLAMA-INSTRUCT\n7B\n34.8\n44.4\n13B\n42.7\n49.4\n34B\n41.5\n57.0\nCODE LLAMA-PYTHON\n7B\n38.4\n47.6\n13B\n43.3\n49.0\n34B\n53.7\n56.2\nUNNATURAL CODE LLAMA\n34B\n62.2\n61.2\nWizardCoder-Python\n13B\n64.0\n55.6\n34B\n73.2\n61.2\nQWEN-CHAT\n7B\n37.2\n35.8\n14B\n43.9\n46.4\nCODE-QWEN\n7B\n40.2\n41.8\n14B\n45.1\n51.4\nCODE-QWEN-CHAT\n7B\n43.3\n44.2\n14B\n66.4\n52.4\n18\nTable 11: Zero-shot pass@1 (%) performance on the HUMANEVALPACK (synthesize) bench-\nmark. The baseline results are partly from OCTOPACK (Muennighoff et al., 2023).\nModel\nParams\nProgramming Language\nPython\nJavaScript\nJava\nGo\nC++\nRust\nAvg.\nProprietary models\nGPT-4\n-\n86.6\n82.9\n81.7\n72.6\n78.7\n67.1\n78.3\nOpen-source models\nInstructCodeT5+\n16B\n37.0\n18.9\n17.4\n9.5\n19.8\n0.3\n17.1\nStarChat-\u03b2\n15B\n33.5\n31.4\n26.7\n25.5\n26.6\n14.0\n26.3\nStarCoder\n15B\n33.6\n30.8\n30.2\n17.6\n31.6\n21.8\n27.6\nCodeGeeX2\n6B\n35.9\n32.2\n30.8\n22.5\n29.3\n18.1\n28.1\nOCTOGEEX\n6B\n44.7\n33.8\n36.9\n21.9\n32.3\n15.7\n30.9\nOCTOCODER\n15B\n46.2\n39.2\n38.2\n30.4\n35.6\n23.4\n35.5\nWizardCoder\n15B\n59.8\n49.5\n36.1\n36.4\n40.9\n20.2\n40.5\nQWEN-CHAT\n7B\n37.2\n23.2\n32.9\n20.7\n22.0\n9.1\n24.2\n14B\n43.9\n38.4\n42.7\n34.1\n24.4\n18.9\n33.7\nCODE-QWEN\n7B\n40.2\n40.4\n40.2\n26.2\n20.7\n15.8\n30.6\n14B\n45.1\n51.8\n57.3\n39.6\n18.2\n20.7\n38.8\nCODE-QWEN-CHAT\n7B\n43.3\n41.5\n49.4\n29.3\n32.9\n20.1\n36.1\n14B\n66.4\n58.5\n56.1\n47.6\n54.2\n28.7\n51.9\nTable 12: Results of models on mathematical reasoning. We report the accuracy of QWEN for all\nbenchmarks using greedy decoding. For MATH, we are reporting QWEN\u2019s performances on the test\nset from Lightman et al. (2023).\nModel\nParams\nGSM8K\nMATH\nMath401\nMath23K\nProprietary models\nGPT-4\n-\n92.0\n42.5\n83.5\n74.0\nGPT-3.5\n-\n80.8\n34.1\n75.1\n60.0\nMinerva\n8B\n16.2\n14.1\n-\n-\n62B\n52.4\n27.6\n-\n-\n540B\n58.8\n33.6\n-\n-\nOpen-source models\nLLaMA-1 RFT\n7B\n46.5\n5.2\n-\n-\n13B\n52.1\n5.1\n-\n-\nWizardMath\n7B\n54.9\n10.7\n-\n-\n13B\n63.9\n14.0\n-\n-\n70B\n81.6\n22.7\n-\n-\nGAIRMath-Abel\n7B\n59.7\n13.0\n-\n-\n13B\n66.4\n17.3\n-\n-\n70B\n83.6\n28.3\n-\n-\nQWEN-CHAT\n7B\n50.3\n6.8\n57.4\n51.2\n14B\n60.1\n18.4\n70.1\n67.0\nMATH-QWEN-CHAT\n7B\n62.5\n17.2\n80.8\n75.4\n14B\n69.8\n24.2\n85.0\n78.4\n19\nformat and it is meaningless for the model to predict the input condition and numbers which could be\nrandom. Thus, we mask the inputs of the system and user to avoid loss computation on them and find\nmasking them accelerates the convergence during our preliminary experiments. For optimization, we\nuse the AdamW optimizer with the same hyperparameters of SFT except that we use a peak learning\nrate of 2 \u00d7 10\u22125 and a training step of 50 000.\n5.2\nEVALUATION\nWe evaluate models on the test sets of GSM8K (Grade school math) (Cobbe et al., 2021), MATH\n(Challenging competition math problems) (Hendrycks et al., 2021), Math401 (Arithmetic abil-\nity) (Yuan et al., 2023b), and Math23K (Chinese grade school math) (Wang et al., 2017). We compare\nMATH-QWEN-CHAT with proprietary models ChatGPT and Minerva (Lewkowycz et al., 2022) and\nopen-sourced math-specialized model RFT (Yuan et al., 2023a), WizardMath (Luo et al., 2023a), and\nGAIRMath-Abel (Chern et al., 2023a) in Table 12. MATH-QWEN-CHAT models show better math\nreasoning and arithmetic abilities compared to open-sourced models and QWEN-CHAT models of\nsimilar sizes. Compared to proprietary models, MATH-QWEN-7B-CHAT outperforms Minerva-8B in\nMATH. MATH-QWEN-14B-CHAT is chasing Minerva-62B and GPT-3.5 in GSM8K and MATH and\ndelivers better performance on arithmetic ability and Chinese math problems.\n6\nRELATED WORK\n6.1\nLARGE LANGUAGE MODELS\nThe excitement of LLM began with the introduction of the Transformer architecture (Vaswani et al.,\n2017), which was then applied to pretraining large-scale data by researchers such as Radford et al.\n(2018); Devlin et al. (2018); Liu et al. (2019). These efforts led to significant success in transfer\nlearning, with model sizes growing from 100 million to over 10 billion parameters (Raffel et al.,\n2020; Shoeybi et al., 2019).\nIn 2020, the release of GPT-3, a massive language model that is 10 times larger than T5, demonstrated\nthe incredible potential of few-shot and zero-shot learning through prompt engineering and in-context\nlearning, and later chain-of-thought prompting (Wei et al., 2022c). This success has led to a number\nof studies exploring the possibilities of further scaling these models (Scao et al., 2022; Zhang et al.,\n2022; Du et al., 2021; Zeng et al., 2022; Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022;\nBlack et al., 2022; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Thoppilan\net al., 2022). As a result, the community has come to view these large language models as essential\nfoundations for downstream models (Bommasani et al., 2021).\nThe birth of ChatGPT (OpenAI, 2022) and the subsequent launch of GPT-4 (OpenAI, 2023) marked\ntwo historic moments in the field of artificial intelligence, demonstrating that large language models\n(LLMs) can serve as effective AI assistants capable of communicating with humans. These events\nhave sparked interests among researchers and developers in building language models that are aligned\nwith human values and potentially even capable of achieving artificial general intelligence (AGI) (Anil\net al., 2023; Anthropic, 2023a;b).\nOne notable development in this area is the emergence of open-source LLMs, specifically\nLLaMA (Touvron et al., 2023a) and LLAMA 2 (Touvron et al., 2023b), which have been recognized\nas the most powerful open-source language models ever created. This has led to a surge of activity\nin the open-source community (Wolf et al., 2019), with a series of large language models being\ndeveloped collaboratively to build upon this progress (Mosaic ML, 2023; Almazrouei et al., 2023;\nChatGLM2 Team, 2023; Yang et al., 2023; InternLM Team, 2023).\n6.2\nALIGNMENT\nThe community was impressed by the surprising effectiveness of alignment on LLMs. Previously,\nLLMs without alignment often struggle with issues such as repetitive generation, hallucination,\nand deviation from human preferences. Since 2021, researchers have been diligently working on\ndeveloping methods to enhance the performance of LLMs in downstream tasks (Wei et al., 2022a;\nSanh et al., 2021; Longpre et al., 2023; Chung et al., 2022; Muennighoff et al., 2022). Furthermore,\n20\nresearchers have been actively exploring ways to align LLMs with human instructions (Ouyang et al.,\n2022; Askell et al., 2021; Bai et al., 2022b;c). One major challenge in alignment research is the\ndifficulty of collecting data. While OpenAI has utilized its platform to gather human prompts or\ninstructions, it is not feasible for others to collect such data.\nHowever, there has been some progress in this area, such as the self-instruct approach proposed\nin Wang et al. (2023c). This innovative work offers a potential solution to the data collection problem\nin alignment research. As a result, there has been a surge in open-source chat data, including\nAlpaca (Taori et al., 2023), MOSS (Sun et al., 2023a), Dolly (Conover et al., 2023), Evol-Instruct (Xu\net al., 2023b), and others (Sun et al., 2023b; Xu et al., 2023a;c; Chen et al., 2023c; Ding et al.,\n2023; Ji et al., 2023; Yang, 2023). Similarly, there has been an increase in open-source chat models,\nsuch as Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Guanaco (Dettmers et al., 2023),\nMOSS (Sun et al., 2023a), WizardLM (Xu et al., 2023b), and others (Xu et al., 2023c; Chen et al.,\n2023c; Ding et al., 2023; Wang et al., 2023b).\nTo train an effective chat model, available solutions are mostly based on SFT and RLHF (Ouyang\net al., 2022). While SFT is similar to pretraining, it focuses on instruction following using the\naforementioned data. However, for many developers, the limited memory capacity is a major obstacle\nto further research in SFT. As a result, parameter-efficient tuning methods, such as LoRA (Hu et al.,\n2021) and Q-LoRA (Dettmers et al., 2023), have gained popularity in the community. LoRA tunes\nonly low-rank adapters, while Q-LoRA builds on LoRA and utilizes 4-bit quantized LLMs and\npaged attention (Dettmers et al., 2022; Frantar et al., 2022; Kwon et al., 2023). In terms of RLHF,\nrecent methods such as PPO (Schulman et al., 2017; Touvron et al., 2023b) have been adopted, but\nthere are also alternative techniques aimed at addressing the complexity of optimization, such as\nRRHF (Yuan et al., 2023c), DPO (Rafailov et al., 2023), and PRO (Song et al., 2023). Despite the\nongoing debate about the effectiveness of RLHF, more evidence is needed to understand how it\nenhances the intelligence of LLMs and what potential drawbacks it may have.\n6.3\nTOOL USE AND AGENTS\nLLM\u2019s planning function allows for the invocation of tools, such as APIs or agent capabilities,\nthrough in-context learning, as demonstrated by Schick et al. (2023). Yao et al. (2022) introduced\nReAct, a generation format that enables the model to generate thoughts on which tool to use, accept\ninput from API observations, and generate a response. GPT-3.5 and GPT-4, when prompted with\nfew shots, have shown consistent and impressive performance. In addition to tool usage, LLMs can\nutilize external memory sources like knowledge bases (Hu et al., 2023; Zhong et al., 2023b) or search\nengines (Nakano et al., 2021; Liu et al., 2023b) to generate more accurate and informative answers.\nThis has led to the popularity of frameworks like LangChain (LangChain, Inc., 2023). The research on\nLLMs for tool use has also sparked interest in building agents with LLM capabilities, such as agents\nthat can call different AI models (Shen et al., 2023; Li et al., 2023a), embodied lifelong learning or\nmultimodal agents (Wang et al., 2023a; Driess et al., 2023), and multiple agents interacting with each\nother and even building a micro-society (Chen et al., 2023b; Li et al., 2023b; Xu et al., 2023d; Hong\net al., 2023).\n6.4\nLLM FOR CODING\nPrevious research has demonstrated that LLMs possess remarkable capabilities in code understanding\nand generation, particularly those with massive numbers of parameters (Chowdhery et al., 2022;\nAnil et al., 2023; Rae et al., 2021; Hoffmann et al., 2022). Moreover, several LLMs have been pre-\ntrained, continued pre-trained, or fine-tuned on coding-related data, which has resulted in significantly\nimproved performance compared to general-purpose LLMs. These models include Codex Chen\net al. (2021), AlphaCode (Li et al., 2022), SantaCoder (Allal et al., 2023), Starcoder-Base (Li et al.,\n2023d), InCoder (Fried et al., 2022), CodeT5 (Wang et al., 2021), CodeGeeX (Zheng et al., 2023),\nand CODE LLAMA (Rozi`ere et al., 2023). In addition to these models, recent studies have focused on\ndeveloping specialized alignment techniques for coding, such as Code Llama-Instruct (Rozi`ere et al.,\n2023) and StarCoder (Li et al., 2023d). These models can assist developers in various code-related\ntasks, including code generation (Chen et al., 2021; Austin et al., 2021), code completion (Zhang\net al., 2023a), code translation (Szafraniec et al., 2023), bug fixing (Muennighoff et al., 2023), code\nrefinement (Liu et al., 2023c), and code question answering (Liu & Wan, 2021). In a word, LLMs\n21\nhave the potential to revolutionize the field of coding by providing developers with powerful tools for\ncode comprehension, generation, and related tasks.\n6.5\nLLM FOR MATHEMATICS\nLLMs with a certain model scale have been found to possess the ability to perform mathematical\nreasoning (Wei et al., 2022b; Suzgun et al., 2022). In order to encourage LLMs to achieve better\nperformance on math-related tasks, researchers have employed techniques such as chain-of-thought\nprompting (Wei et al., 2022c) and scratchpad (Nye et al., 2021), which have shown promising results.\nAdditionally, self-consistency (Wang et al., 2022) and least-to-most prompting (Zhou et al., 2022)\nhave further improved the performance of these models on these tasks. However, prompt engineering\nis a time-consuming process that requires a lot of trial and error, and it is still difficult for LLMs to\nconsistently perform well or achieve satisfactory results in solving mathematical problems. Moreover,\nsimply scaling the data and model size is not an efficient way to improve a model\u2019s mathematical\nreasoning abilities. Instead, pretraining on math-related corpora has been shown to consistently\nenhance these capabilities (Hendrycks et al., 2021; Lewkowycz et al., 2022; Taylor et al., 2022;\nLightman et al., 2023). Additionally, fine-tuning on math-related instruction-following datasets (Si\net al., 2023; Yuan et al., 2023a; Luo et al., 2023a; Yue et al., 2023; Chern et al., 2023a; Yu et al.,\n2023), has also been effective and more cost-effective than math-specific pretraining. Despite their\nlimitations in terms of accuracy, LLMs still have significant potential to assist users with practical\nmathematical problems. There is ample scope for further development in this area.\n7\nCONCLUSION\nIn this report, we present the QWEN series of large language models, which showcase the latest\nadvancements in natural language processing. With 14B, 7B, and 1.8B parameters, these models\nhave been pre-trained on massive amounts of data, including trillions of tokens, and fine-tuned using\ncutting-edge techniques such as SFT and RLHF. Additionally, the QWEN series includes specialized\nmodels for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-\nCHAT, which have been trained on domain-specific data to excel in their respective fields. Our results\ndemonstrate that the QWEN series is competitive with existing open-source models and even matches\nthe performance of some proprietary models on comprehensive benchmarks and human evaluation.\nWe believe that the open access of QWEN will foster collaboration and innovation within the\ncommunity, enabling researchers and developers to build upon our work and push the boundaries of\nwhat is possible with language models. By providing these models to the public, we hope to inspire\nnew research and applications that will further advance the field and contribute to our understanding of\nthe variables and techniques introduced in realistic settings. In a nutshell, the QWEN series represents\na major milestone in our development of large language models, and we are excited to see how it will\nbe used to drive progress and innovation in the years to come.\n22\nREFERENCES\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. SantaCoder: Don\u2019t\nreach for the stars! arXiv preprint arXiv:2301.03988, 2023.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\njocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large language\nmodel with state-of-the-art performance, 2023.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nAnthropic.\nIntroducing Claude, 2023a.\nURL https://www.anthropic.com/index/\nintroducing-claude.\nAnthropic.\nClaude 2.\nTechnical report, Anthropic, 2023b.\nURL https://www-files.\nanthropic.com/production/images/Model-Card-Claude-2.pdf.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta,\nHonglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. ExT5: Towards extreme multi-task\nscaling for transfer learning. arXiv preprint arXiv:2111.10952, 2021.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\nAutoGPT. AutoGPT: The heart of the open-source agent ecosystem, 2023. URL https://\ngithub.com/Significant-Gravitas/Auto-GPT.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization.\nCoRR,\nabs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.\nJinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang, Yichang Zhang, Xiaohuan Zhou, Peng\nWang, Sinan Tan, An Yang andf Zeyu Cui, Yu Han, Shuai Bai, Wenbin Ge, Jianxin Ma, Junyang\nLin, Jingren Zhou, and Chang Zhou. OFASys: A multi-modal multi-task learning system for\nbuilding generalist models. CoRR, abs/2212.04408, 2022a. doi: 10.48550/arXiv.2212.04408. URL\nhttps://doi.org/10.48550/arXiv.2212.04408.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. Qwen-VL: A versatile vision-language model for understanding, localization,\ntext reading, and beyond. CoRR, abs/2308.12966, 2023. doi: 10.48550/arXiv.2308.12966. URL\nhttps://doi.org/10.48550/arXiv.2308.12966.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022b.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:\nHarmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022c.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\n23\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about\nphysical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Con-\nference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7432\u20137439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022.\nbloc97.\nNTK-aware scaled RoPE allows LLaMA models to have extended (8k+) con-\ntext size without any fine-tuning and minimal perplexity degradation., 2023.\nURL\nhttps://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_\nscaled_rope_allows_llama_models_to_have/.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nChatGLM2 Team. ChatGLM2-6B: An open bilingual chat LLM, 2023. URL https://github.\ncom/THUDM/ChatGLM2-6B.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond\u00b4e de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.\norg/abs/2107.03374.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia\nQin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023b.\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang,\nJuhao Liang, Chen Zhang, Zhiyi Zhang, et al. Phoenix: Democratizing ChatGPT across languages.\narXiv preprint arXiv:2304.10453, 2023c.\nEthan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu.\nGenerative ai for math: Abel. https://github.com/GAIR-NLP/abel, 2023a.\nI Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham\nNeubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai\u2013a tool augmented\nframework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023b.\nDavid Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 7654\u20137664, 2022.\n24\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023. URL\nhttps://lmsys.org/blog/2023-03-30-vicuna/.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg,\nSamy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett\n(eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neu-\nral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n4299\u20134307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\nd5e2c0adad503c91f91df240d0cd4e49-Abstract.html.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein,\nChristy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp.\n2924\u20132936. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1300. URL\nhttps://doi.org/10.18653/v1/n19-1300.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge.\nCoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-\ncisco Guzm\u00b4an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised\ncross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick\nWendell, Matei Zaharia, and Reynold Xin. Free Dolly: Introducing the world\u2019s first truly open\ninstruction-tuned LLM, 2023. URL https://www.databricks.com/blog/2023/04/\n12/dolly-first-open-commercially-viable-instruction-tuned-llm.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\nTri Dao,\nDaniel Y. Fu,\nStefano Ermon,\nAtri Rudra,\nand Christopher R\u00b4e.\nFlashAt-\ntention:\nFast and memory-efficient exact attention with io-awareness.\nIn NeurIPS,\n2022.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In International conference on machine learning, pp. 933\u2013941. PMLR,\n2017.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n25\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning\nof quantized LLMs. arXiv preprint arXiv:2305.14314, 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXiv preprint arXiv:2305.14233, 2023.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378, 2023.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language\nmodels with mixture-of-experts. In International Conference on Machine Learning, pp. 5547\u20135569.\nPMLR, 2022.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\nGLM: General language model pretraining with autoregressive blank infilling. arXiv preprint\narXiv:2103.10360, 2021.\nKawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with\nV-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,\nGang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine Learning Research, pp. 5988\u20136008. PMLR,\n17\u201323 Jul 2022.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):\n5232\u20135270, 2022.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\nand synthesis. ArXiv, abs/2204.05999, 2022.\nGoogle. An important next step on our AI journey, 2023. URL https://blog.google/\ntechnology/ai/bard-google-ai-search-updates/.\nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with Gaussian\nerror linear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606.\n08415.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352, 2023.\n26\nChenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting\nllms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLoRA: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra K\u00a8ubler, and Lawrence S. Moss. OCNLI:\noriginal chinese natural language inference. In Trevor Cohn, Yulan He, and Yang Liu (eds.),\nFindings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20\nNovember 2020, volume EMNLP 2020 of Findings of ACL, pp. 3512\u20133526. Association for\nComputational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.314. URL https:\n//doi.org/10.18653/v1/2020.findings-emnlp.314.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-Eval: A multi-level multi-discipline chinese\nevaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\nHugging Face.\nTransformers agents, 2023.\nURL https://huggingface.co/docs/\ntransformers/transformers_agents.\nBaichuan Inc. Baichuan-7B: A large-scale 7B pretraining language model developed by BaiChuan-\nInc, 2023a. URL https://github.com/baichuan-inc/Baichuan-7B.\nXVERSE Technology Inc.\nXVERSE-13B: A multilingual large language model devel-\noped by XVERSE Technology Inc., 2023b. URL https://github.com/xverse-ai/\nXVERSE-13B.\nInternLM Team. InternLM: A multilingual language model with progressively enhanced capabilities,\n2023. URL https://github.com/InternLM/InternLM.\nShantanu Jain. tiktoken: A fast BPE tokeniser for use with OpenAI\u2019s models, 2022. URL https:\n//github.com/openai/tiktoken/.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang\nLi. Exploring the impact of instruction data scaling on large language models: An empirical study\non real-world use cases. arXiv preprint arXiv:2303.14742, 2023.\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm\ntransformers: Equivalent and efficient pre-LN transformers. CoRR, abs/2305.14858, 2023. doi:\n10.48550/arXiv.2305.14858. URL https://doi.org/10.48550/arXiv.2305.14858.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput.\nLinguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl\\ a\\ 00276. URL https://doi.org/10.\n1162/tacl_a_00276.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\nLangChain, Inc. LangChain: Building applications with LLMs through composability, 2023. URL\nhttps://python.langchain.com/.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n27\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\nNeyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language\nmodels, 2022.\nChenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang,\nWenmeng Zhou, Yingda Chen, Chen Cheng, et al. ModelScope-Agent: Building your customizable\nagent system with open-source large language models. arXiv preprint arXiv:2309.00986, 2023a.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCamel: Communicative agents for \u201cmind\u201d exploration of large scale language model society. arXiv\npreprint arXiv:2303.17760, 2023b.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv\npreprint arXiv:2306.09212, 2023c.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\nMou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,\nTerry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u02dcao\nMonteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee,\nLogesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,\nRudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan\nDey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh,\nSasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee,\nNadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank\nMishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish\nContractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u02dcnoz Ferrandis,\nSean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder:\nMay the source be with you! CoRR, abs/2305.06161, 2023d. doi: 10.48550/arXiv.2305.06161.\nURL https://doi.org/10.48550/arXiv.2305.06161.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00b4emi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,\nCyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,\nSven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\ncode generation with AlphaCode. CoRR, abs/2203.07814, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\nChenxiao Liu and Xiaojun Wan. CodeQA: A question answering dataset for source code com-\nprehension.\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau\nYih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 2618\u20132632. Associa-\ntion for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.223. URL\nhttps://doi.org/10.18653/v1/2021.findings-emnlp.223.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023a.\nXiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong,\nand Jie Tang. WebGLM: Towards an efficient web-enhanced question answering system with\nhuman preferences. arXiv preprint arXiv:2306.07906, 2023b.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n28\nYue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach Dinh\nLe, and David Lo.\nRefining ChatGPT-generated code: Characterizing and mitigating code\nquality issues. CoRR, abs/2307.12596, 2023c. doi: 10.48550/arXiv.2307.12596. URL https:\n//doi.org/10.48550/arXiv.2307.12596.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The Flan collection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688, 2023.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nKeming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and\nJingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language\nmodels. CoRR, abs/2308.07074, 2023. doi: 10.48550/arXiv.2308.07074. URL https://doi.\norg/10.48550/arXiv.2308.07074.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,\nQingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical\nreasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583,\n2023a.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. WizardCoder: Empowering code large language models with\nevol-instruct. arXiv preprint arXiv:2306.08568, 2023b.\nMosaic ML. MPT-30B: Raising the bar for open-source foundation models, 2023. URL https:\n//www.mosaicml.com/blog/mpt-30b.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual\ngeneralization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam\nSingh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction tuning\ncode large language models. CoRR, abs/2308.07124, 2023.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nMaxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus\nOdena. Show your work: Scratchpads for intermediate computation with language models. ArXiv,\nabs/2112.00114, 2021.\nOpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.\nOpenAI. ChatML, 2022. URL https://github.com/openai/openai-python/blob/\ne389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md.\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenCompass Team. OpenCompass: A universal evaluation platform for foundation models, 2023.\nURL https://opencompass.org.cn/leaderboard-llm.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser\nKelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow instructions with human feedback. In\nNeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\nb1efde53be364a73914f58805a001731-Abstract-Conference.html.\n29\nDenis Paperno, Germ\u00b4an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00b4andez. The LAMBADA dataset:\nWord prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany,\nVolume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/\np16-1144. URL https://doi.org/10.18653/v1/p16-1144.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\nextension of large language models. arXiv preprint arXiv:2309.00071, 2023a.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023b.\nQwen Team, Alibaba Group. Evaluation benchmark for code intepreter, 2023a. URL https:\n//github.com/QwenLM/Qwen-Agent/tree/main/benchmark.\nQwen Team, Alibaba Group. Evaluation benchmark for tool usage through ReAct prompting, 2023b.\nURL https://github.com/QwenLM/Qwen-7B/tree/main/eval.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. Technical report, OpenAI, 2018.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint\narXiv:1710.05941, 2017.\nScott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio G\u00b4omez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol\nVinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Trans. Mach. Learn. Res.,\n2022, 2022. URL https://openreview.net/forum?id=1ikK0kHjvj.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, et al. Code Llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950, 2023.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables\nzero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. SocialIQA: Com-\nmonsense reasoning about social interactions.\nCoRR, abs/1904.09728, 2019.\nURL http:\n//arxiv.org/abs/1904.09728.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00b4e, Alexandra Sasha Luccioni, Franc\u00b8ois Yvon, Matthias Gall\u00b4e, et al. BLOOM: A 176B-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\n30\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNoam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\ngingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace.\narXiv preprint\narXiv:2303.17580, 2023.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nQingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng Lin. Alpaca-CoT: An instruction-tuning\nplatform with unified interface of instruction collection, parameter-efficient methods, and large\nlanguage models, 2023. URL https://github.com/PhoebusSi/alpaca-CoT.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.\nPreference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.\nStability AI.\nStableBeluga2, 2023.\nURL https://huggingface.co/stabilityai/\nStableBeluga2.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008\u20133021, 2020.\nJianlin Su. Improving transformer: Length extrapolation ability and position robustness, 2023a. URL\nhttps://spaces.ac.cn/archives/9444.\nJianlin Su. The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023b.\nURL https://spaces.ac.cn/archives/9577.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nTianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu,\nYunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun\nZhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang,\nand Xipeng Qiu. MOSS: Training conversational language models from synthetic data, 2023a.\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming\nYang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with\nminimal human supervision. arXiv preprint arXiv:2305.03047, 2023b.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nMarc Szafraniec, Baptiste Rozi`ere, Hugh Leather, Patrick Labatut, Franc\u00b8ois Charton, and Gabriel\nSynnaeve. Code translation with compiler representations. In The Eleventh International Confer-\nence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,\n2023. URL https://openreview.net/pdf?id=XomEU3eNeSQ.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question\nanswering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and\nThamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4149\u2013\n4158. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1421. URL\nhttps://doi.org/10.18653/v1/n19-1421.\n31\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model,\n2023. URL https://github.com/tatsu-lab/stanford_alpaca.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science,\n2022.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny\nSoraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Ol-\nson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena\nButryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein,\nRay Kurzweil, Blaise Ag\u00a8uera y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le.\nLaMDA: Language models for dialog applications.\nCoRR, abs/2201.08239, 2022.\nURL\nhttps://arxiv.org/abs/2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00b4elien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.\nCoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/\n10.48550/arXiv.2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291, 2023a.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-\nconsistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171,\n2022.\nYan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In\nConference on Empirical Methods in Natural Language Processing, 2017. URL https://api.\nsemanticscholar.org/CorpusID:910689.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nExploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751,\n2023b.\n32\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In\nAnna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023,\nToronto, Canada, July 9-14, 2023, pp. 13484\u201313508. Association for Computational Linguistics,\n2023c. doi: 10.18653/v1/2023.acl-long.754. URL https://doi.org/10.18653/v1/\n2023.acl-long.754.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified\npre-trained encoder-decoder models for code understanding and generation.\narXiv preprint\narXiv:2109.00859, 2021.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H.\nHoi. CodeT5+: Open code large language models for code understanding and generation. CoRR,\nabs/2305.07922, 2023d. doi: 10.48550/arXiv.2305.07922. URL https://doi.org/10.\n48550/arXiv.2305.07922.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?id=\ngEZrGCozdqR.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.\nTrans. Mach. Learn. Res., 2022, 2022b. URL https://api.semanticscholar.org/\nCorpusID:249674500.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022c.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. HuggingFace\u2019s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong\nMao. ExpertPrompting: Instructing large language models to be distinguished experts. arXiv\npreprint arXiv:2305.14688, 2023a.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. WizardLM: Empowering large language models to follow complex instructions. arXiv\npreprint arXiv:2304.12244, 2023b.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023c.\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.\nExploring large language models for communication games: An empirical study on werewolf.\narXiv preprint arXiv:2309.04658, 2023d.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,\nDong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou\nZhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang,\nLei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin,\nNuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng\nChen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun\nShen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying\nWu. Baichuan 2: Open large-scale language models. Technical report, Baichuan Inc., 2023.\nURL https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.\npdf.\n33\nJianxin Yang. Firefly. https://github.com/yangjianxin1/Firefly, 2023.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models\nwith multimodality. arXiv preprint arXiv:2304.14178, 2023.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models, 2023.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou,\nand Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language\nmodels, 2023a.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large\nlanguage models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023b.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank\nresponses to align language models with human feedback without tears, 2023c.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMAmmoTH: Building math generalist models through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\nmachine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu\u00b4\u0131s M`arquez\n(eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics,\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4791\u20134800.\nAssociation for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https:\n//doi.org/10.18653/v1/p19-1472.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. GLM-130B: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414, 2022.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu\nChen. RepoCoder: Repository-level code completion through iterative retrieval and generation.\nCoRR, abs/2303.12570, 2023a. doi: 10.48550/arXiv.2303.12570. URL https://doi.org/\n10.48550/arXiv.2303.12570.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\nXiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating\nthe performance of large language models on GAOKAO benchmark. CoRR, abs/2305.12474,\n2023b. doi: 10.48550/arXiv.2305.12474. URL https://doi.org/10.48550/arXiv.\n2305.12474.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for\ncode generation with multilingual evaluations on humaneval-x. CoRR, abs/2303.17568, 2023. doi:\n10.48550/arXiv.2303.17568. URL https://doi.org/10.48550/arXiv.2303.17568.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu\nChen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models.\nCoRR, abs/2304.06364, 2023a. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/\n10.48550/arXiv.2304.06364.\n34\nWanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. MemoryBank: Enhancing large\nlanguage models with long-term memory. arXiv preprint arXiv:2305.10250, 2023b.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables complex\nreasoning in large language models. ArXiv, abs/2205.10625, 2022.\n35\nA\nAPPENDIX\nA.1\nMORE TRAINING DETAILS\nA.1.1\nDATA FORMAT FOR QWEN-CHAT\nDifferent from conventional pretraining based on autoregressive next-token prediction, despite using\na similar training task, there should be a specially design data format for SFT and RLHF to build\na conversational AI assistant model. Common formats include \u201chuman-assistant\u201d and ChatML\nformats. As to our knowledge, one of the earliest examples of the human-assistant format comes from\nAnthropic (Bai et al., 2022b), which adds a special phrase \u201c\\n\\nhuman: \u201d in front of the user\ninput and \u201c\\n\\nassistant: \u201d in front of the assistant response. It is easy for the base language\nmodel to transfer to the pattern of conversational AI. However, as the specific phrases are common\nwords, it might be hard for the model to disambiguate from these words in other contexts.\nInstead, we turned to the ChatML format proposed by OpenAI.5 This format allows the use of special\ntokens, i.e., \u201c<im_start>\u201d and \u201c<im_end>\u201d, that do not appear in pretraining, and thus resolve\nthe aforementioned problem. We demonstrate an example of the format below.\nChatML Format\n<| i m s t a r t |>system\nYou are\na\nh e l p f u l\na s s i s t a n t . <| im end|>\n<| i m s t a r t |> user\nHello ! <| im end|>\n<| i m s t a r t |> a s s i s t a n t\nHello ! How can\nI\na s s i s t\nyou\ntoday ?<| im end|>\nA.2\nEVALUATION\nA.2.1\nAUTOMATIC EVALUATION\nTo provide a whole picture of the performance of our model series QWEN, here in this section we\nillustrate the detailed performance of our models as well as the baselines in the comprehensive\nbenchmark evaluation proposed by OpenCompass Team (2023). We report the results in multiple\ntables based on the officially provided categories, including examination, language, knowledge,\nunderstanding, and reasoning. In terms of the performance of the baseline models, we report the\nhigher results between the reported ones and those on the leaderboard.\nExamination\nHere we evaluate the models on a series of datasets relevant to the examination. The\ndatasets include:\n\u2022 MMLU (Hendrycks et al., 2020) Massive Multi-task Language Understanding is designed\nfor measuring language understanding capabilities. We report 5-shot results.\n\u2022 C-Eval (Huang et al., 2023) C-Eval is a Chinese evaluation dataset spanning 52 diverse\ndisciplines. We report 5-shot results.\n\u2022 CMMLU (Li et al., 2023c) CMMLU is designed for assessing language understanding\ncapabilities in Chinese. We report 5-shot results.\n\u2022 AGIEval (Zhong et al., 2023a) This is a benchmark consisting of human-centric examina-\ntions, including college entrance exams, law school admission tests, math competitions, and\nlawyer qualification tests. We report zero-shot results.\n\u2022 Gaokao-Bench (Zhang et al., 2023b) This is a benchmark with Gaokao (Chinese college-\nentrance examination) questions. We report zero-shot results.\n\u2022 ARC (Clark et al., 2018) ARC is a dataset consisting of grade-school level, multiple-choice\nscience questions. It includes an easy set and a challenge set, which are referred by ARC-e\nand ARC-c. We report zero-shot results.\n36\nTable 13: Results on MMLU. All are tested with five-shot accuracy. We provide the reported results\nof the other models for comparison.\nModel\nParams\nAverage\nSTEM\nSocial Sciences\nHumanities\nOthers\nMPT\n7B\n26.8\n25.3\n27.1\n26.7\n28.2\n30B\n46.9\n39.0\n52.8\n44.5\n52.9\nFalcon\n7B\n26.2\n26.2\n24.7\n26.4\n27.4\n40B\n55.4\n45.5\n65.4\n49.3\n65.0\nChatGLM2\n6B\n47.9\n41.2\n54.4\n43.7\n54.5\n12B\n56.2\n48.2\n65.1\n52.6\n60.9\nInternLM\n7B\n51.0\n-\n-\n-\n-\nBaichuan2\n7B\n54.2\n-\n-\n-\n-\n13B\n59.2\n-\n-\n-\n-\nXVERSE\n13B\n55.1\n44.5\n64.4\n50.5\n62.9\nLLaMA\n7B\n35.1\n30.5\n38.3\n34.0\n38.1\n13B\n46.9\n35.8\n53.8\n45.0\n53.3\n33B\n57.8\n46.0\n66.7\n55.8\n63.4\n65B\n63.4\n51.7\n72.9\n61.8\n67.4\nLLAMA 2\n7B\n45.3\n36.4\n51.2\n42.9\n52.2\n13B\n54.8\n44.1\n62.6\n52.8\n61.1\n34B\n62.6\n52.1\n71.8\n59.4\n69.2\n70B\n68.9\n58.0\n80.3\n65.0\n74.6\nQWEN\n1.8B\n44.6\n39.6\n50.0\n40.4\n51.0\n7B\n58.2\n50.2\n68.6\n52.5\n64.9\n14B\n66.3\n59.4\n76.2\n60.9\n71.8\nTable 14: Leaderboard results of C-Eval. We include the results of both proprietary models and\nopen-source models. Note that there are a number of models on the leaderboard with very few\ndetails, in terms of proprietary models, we only report the results of GPT-3.5, GPT-4, InternLM and\nChatGLM2.\nModel\nParams\nAvg.\nAvg. (Hard)\nSTEM\nSocial Sciences\nHumanities\nOthers\nProprietary models\nGPT-3.5\n-\n54.4\n41.4\n52.9\n61.8\n50.9\n53.6\nGPT-4\n-\n68.7\n54.9\n67.1\n77.6\n64.5\n67.8\nInternLM\n123B\n68.8\n50.0\n63.5\n81.4\n72.7\n63.0\nChatGLM2\n-\n71.1\n50.0\n64.4\n81.6\n73.7\n71.3\nOpen-source models\nChatGLM2\n6B\n51.7\n37.1\n48.6\n60.5\n51.3\n49.8\nInternLM\n7B\n52.8\n37.1\n48.0\n67.4\n55.4\n45.8\nBaichuan2\n7B\n54.0\n-\n-\n-\n-\n-\n13B\n58.1\n-\n-\n-\n-\n-\nXVERSE\n13B\n54.7\n33.5\n45.6\n66.2\n58.3\n56.9\nQWEN\n1.8B\n54.7\n41.8\n50.8\n69.9\n56.3\n46.2\n7B\n63.5\n46.4\n57.7\n78.1\n66.6\n57.8\n14B\n72.1\n53.7\n65.7\n85.4\n75.3\n68.4\nIn terms of MMLU, we report the detailed results in Table 13. In terms of C-Eval, we report the results\nin Table 14. For the rest of the datasets, we report the results in Table 15. Note that AGIEval includes\n5https://github.com/openai/openai-python/blob/main/chatml.md\n37\nTable 15: Results on the other datasets of examination. Specifically, we report the results on\nCMMLU, AGIEval, ARC-e, and ARC-c.\nModel\nParams\nCMMLU\nAGIEval\nGaokao-Bench\nARC-e\nARC-c\nMPT\n7B\n25.9\n21.3\n19.8\n70.2\n42.6\nFalcon\n7B\n-\n-\n-\n70.0\n42.4\nChatGLM2\n6B\n49.3\n39.0\n46.4\n73.0\n61.0\nInternLM\n7B\n51.8\n36.9\n43.0\n78.7\n69.5\n20B\n59.0\n44.6\n45.5\n86.1\n81.7\nBaichuan2\n7B\n57.1\n42.7\n47.5\n54.7\n32.5\n13B\n62.0\n48.2\n54.3\n61.9\n38.0\nLLaMA\n7B\n26.8\n20.6\n21.3\n72.8\n47.6\n13B\n31.5\n22.0\n20.4\n74.8\n52.7\n33B\n36.0\n33.5\n18.9\n80.0\n67.5\n65B\n40.6\n33.9\n19.1\n80.6\n69.5\nLLAMA 2\n7B\n31.8\n21.8\n18.9\n75.2\n45.9\n13B\n38.4\n30.9\n18.2\n77.3\n60.3\n70B\n53.6\n40.2\n23.3\n85.9\n78.3\nStableBeluga2\n70B\n51.8\n41.6\n40.9\n91.2\n86.1\nQWEN\n1.8B\n49.3\n36.9\n44.9\n71.6\n53.2\n7B\n62.2\n45.8\n52.5\n84.0\n75.3\n14B\n71.0\n52.3\n61.9\n90.3\n84.4\nthe parts of Chinese and English, while LLAMA 2 only reported the results in the English part, so we\nuse the results on OpenCompass. Additionally, while CMMLU, AGIEval, and Gaokao-Bench are\nrelated to Chinese, and MPT, Falcon, and the LLaMA series were not optimized for Chinese, these\nmodels achieved low performance on the datasets.\nKnowledge and Understanding\nHere we evaluate the models on a series of datasets relevant to\nknowledge and natural language understanding. The datasets include\n\u2022 BoolQ (Clark et al., 2019) This is a QA dataset, where the questions are about passages of\nWikipedia, and the model should answer yes or no to the given possible answer. We report\nzero-shot results.\n\u2022 CommonsenseQA (Talmor et al., 2019) This is a dataset of multiple-choice question\nanswering that asseses the understanding of commonsense knowledge. We report 8-shot\nresults.\n\u2022 NaturalQuestions (Kwiatkowski et al., 2019) It is a dataset of QA where the questions are\nfrom users and the answers are verified by experts. We report zero-shot results.\n\u2022 LAMBADA (Paperno et al., 2016) This is dataset to evaluate language understanding by\nword prediction. It consists of passages related to human subjects. We report zero-shot\nresults.\nWe report the results in Table 16.\nReasoning\nWe report the evaluation results on the datasets concerning reasoning, focusing on\nnatural language reasoning. For the others, such as mathematics and coding, as we have illustrated\ndetailed results, here we do not report those results repeatedly. The datasets for evaluation include:\n\u2022 HellaSwag (Zellers et al., 2019) This is a commonsense natural language inference (NLI)\ndataset, where the questions are easy for humans but struggling for previous language\nmodels. We report zero-shot results.\n\u2022 PIQA (Bisk et al., 2020) This is an NLI dataset assessing the physical knowledge. We report\nzero-shot results.\n38\nTable 16: Results on the datasets concerning knowledge and understanding. Specifically, we\nreport the results on BoolQ, CommonsenseQA, NaturalQuestions, and LAMBADA.\nModel\nParams\nBoolQ\nCommonsenseQA\nNaturalQuestions\nLAMBADA\nMPT\n7B\n75.0\n61.8\n11.6\n70.0\nFalcon\n7B\n67.5\n20.8\n15.7\n-\nChatGLM2\n6B\n79.0\n65.4\n9.7\n54.3\nInternLM\n7B\n64.1\n59.8\n8.9\n67.0\n20B\n87.5\n70.6\n25.2\n71.8\nXVERSE\n13B\n64.2\n62.2\n0.3\n48.2\nBaichuan2\n7B\n63.2\n63.0\n9.4\n73.3\n13B\n67.0\n65.6\n16.3\n74.0\nLLaMA\n7B\n76.5\n64.9\n16.8\n73.3\n13B\n78.7\n67.4\n20.2\n75.2\n33B\n84.4\n72.5\n30.9\n77.2\n65B\n86.6\n74.1\n33.4\n77.7\nLLAMA 2\n7B\n77.4\n66.5\n19.1\n73.3\n13B\n82.4\n67.3\n24.9\n76.5\n70B\n87.7\n78.5\n34.2\n78.9\nStableBeluga2\n70B\n89.4\n72.6\n25.1\n71.3\nQWEN\n1.8B\n68.0\n60.1\n3.2\n58.4\n7B\n76.4\n66.8\n17.4\n67.9\n14B\n86.2\n70.3\n23.9\n71.1\nTable 17: Results on the datasets related to natural language reasoning. Specifically, we report\nthe results on HellaSwag, PIQA, SIQA, and OCNLI.\nModel\nParams\nHellaSwag\nPIQA\nSIQA\nOCNLI\nMPT\n7B\n76.4\n80.6\n48.5\n30.0\nFalcon\n7B\n74.1\n76.7\n47.2\n-\nChatGLM2\n6B\n57.0\n69.6\n64.3\n33.1\nInternLM\n7B\n70.6\n77.9\n60.5\n37.5\n20B\n78.1\n80.3\n72.8\n42.5\nBaichuan2\n7B\n67.0\n76.2\n44.4\n30.3\n13B\n70.8\n78.1\n44.3\n30.0\nLLaMA\n7B\n76.1\n79.8\n48.9\n33.6\n13B\n79.2\n80.1\n52.5\n32.1\n33B\n82.8\n82.3\n57.8\n30.7\n65B\n84.2\n82.8\n61.2\n44.9\nLLAMA 2\n7B\n77.2\n78.8\n48.5\n32.1\n13B\n80.7\n80.5\n54.8\n34.1\n70B\n85.3\n82.8\n64.8\n46.5\nStableBeluga2\n70B\n84.1\n83.3\n78.1\n48.3\nQWEN\n1.8B\n56.7\n73.3\n56.1\n39.0\n7B\n75.1\n77.9\n69.9\n47.4\n14B\n80.2\n79.9\n77.9\n57.9\n\u2022 SIQA (Sap et al., 2019) This is an NLI dataset evaluating social commonsense intelligence.\nWe report zero-shot results.\n\u2022 OCNLI (Hu et al., 2020) This is an NLI dataset focusing on Chinese. We report zero-shot\nresults.\n39\nWe report the results in Table 17.\nA.2.2\nHUMAN EVALUATION\nIn this section, we demonstrate the cases of human analysis. In our self-constructed evaluation dataset,\nthe instructions are either manually written data or manual revised from public datasets, such as\nCLiB6, C-Eval (Huang et al., 2023), FacTool (Chern et al., 2023b), LeetCode7), etc.\nIn terms of each case, we demonstrate the responses and Elo ratings8 of all models for comparison.\nSpecifically, as the data in our human evaluation are in Chinese, we also provide their translations in\nEnglish.\nKnowledge\n\u5317\u4eac\u5927\u5b66\u7684\u524d\u8eab\u662f\u4ec0\u4e48\u5b66\u6821\uff1f\nWhat was the predecessor of Peking University?\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u5317\u4eac\u5927\u5b66\u7684\u524d\u8eab\u662f\u4eac\u5e08\u5927\u5b66\u5802\uff0c\u5b83\u521b\u5efa\u4e8e1898\u5e74\uff0c\u662f\u4e2d\u56fd\u8fd1\u4ee3\n\u7b2c\u4e00\u6240\u56fd\u7acb\u5927\u5b66\uff0c\u4e5f\u662f\u6700\u65e9\u4ee5\u201c\u5927\u5b66\u201d \u547d\u540d\u7684\u9ad8\u7b49\u5b66\u5e9c\u3002\n1037\nThe predecessor of Peking University was the Imperial University\nof Peking, established in 1898. It was the first national university in\nmodern China and also the earliest higher education institution to be\nnamed a \u201cuniversity\u201d.\nQwen-14B-Chat (SFT)\n\u5317\u4eac\u5927\u5b66\u7684\u524d\u8eab\u662f\u4eac\u5e08\u5927\u5b66\u5802\uff0c\u521b\u5efa\u4e8e1898\u5e74\uff0c\u662f\u4e2d\u56fd\u8fd1\u4ee3\u7b2c\n\u4e00\u6240\u56fd\u7acb\u5927\u5b66\u3002\n1011\nThe predecessor of Peking University is the Imperial University of\nPeking, founded in 1898 and the first national university in modern\nChina.\nQwen-14B-Chat (RLHF)\u5317\u4eac\u5927\u5b66\u7684\u524d\u8eab\u662f\u4eac\u5e08\u5927\u5b66\u5802\uff0c\u521b\u5efa\u4e8e1898\u5e74\uff0c\u662f\u4e2d\u56fd\u8fd1\u4ee3\u7b2c\n\u4e00\u6240\u56fd\u7acb\u7efc\u5408\u6027\u5927\u5b66\u3002\u4eac\u5e08\u5927\u5b66\u5802\u6700\u521d\u8bbe\u7acb\u7684\u5b66\u79d1\u6709\u7ecf\u5b66\u3001\u6587\n\u5b66\u3001\u53f2\u5b66\u3001\u54f2\u5b66\u3001\u6cd5\u5f8b\u3001\u6559\u80b2\u3001\u519c\u5b66\u3001\u5de5\u5b66\u3001\u5546\u5b66\u3001\u533b\u5b66\u7b49\uff0c\n\u540e\u6765\u9010\u6e10\u6269\u5927\u5230\u793e\u4f1a\u79d1\u5b66\u3001\u81ea\u7136\u79d1\u5b66\u3001\u5de5\u7a0b\u6280\u672f\u3001\u827a\u672f\u7b49\u591a\u4e2a\n\u9886\u57df\u30021912\u5e74\uff0c\u4eac\u5e08\u5927\u5b66\u5802\u6539\u540d\u4e3a\u5317\u4eac\u5927\u5b66\uff0c\u6210\u4e3a\u4e2d\u56fd\u6700\u9ad8\u5b66\n\u5e9c\u4e4b\u4e00\uff0c\u4e5f\u662f\u4e2d\u56fd\u73b0\u4ee3\u9ad8\u7b49\u6559\u80b2\u7684\u53d1\u6e90\u5730\u4e4b\u4e00\u3002\n1090\nPeking University, originally known as the Imperial University of\nPeking, was founded in 1898 and is the first comprehensive national\nuniversity in modern China. The initial disciplines established at\nthe Imperial University of Peking included classics, literature, his-\ntory, philosophy, law, education, agriculture, engineering, business,\nmedicine, etc., gradually expanding to encompass social sciences,\nnatural sciences, engineering and technology, arts, and other fields. In\n1912, the Imperial University of Peking changed its name to Peking\nUniversity, becoming one of the top universities in China and also\none of the birthplaces of modern higher education in China.\nGPT-3.5\n\u5317\u4eac\u5927\u5b66\u7684\u524d\u8eab\u662f\u56fd\u7acb\u5317\u4eac\u5927\u5b66\uff0c\u5efa\u4e8e1898\u5e74\uff0c\u662f\u4e2d\u56fd\u8fd1\u4ee3\u7b2c\n\u4e00\u6240\u56fd\u7acb\u5927\u5b66\u3002\u5176\u6210\u7acb\u521d\u8877\u662f\u4e3a\u4e86\u57f9\u517b\u56fd\u5bb6\u7684\u653f\u6cbb\u3001\u7ecf\u6d4e\u3001\u519b\n\u4e8b\u548c\u5916\u4ea4\u4eba\u624d\uff0c\u4e3a\u4e2d\u56fd\u7684\u73b0\u4ee3\u5316\u8fdb\u7a0b\u505a\u51fa\u8d21\u732e\u3002\n910\n6https://github.com/jeinlee1991/chinese-llm-benchmark\n7https://leetcode.cn/\n8https://en.wikipedia.org/wiki/Elo rating system\n40\nThe predecessor of Peking University was National Peking University,\nestablished in 1898. It is the first national university in modern China.\nIts original purpose was cultivating talents in politics, economics,\nmilitary, and diplomacy and contributing to China\u2019s modernization\nprocess.\nGPT-4\n\u5317\u4eac\u5927\u5b66\u7684\u524d\u8eab\u662f\u4eac\u5e08\u5927\u5b66\u5802\u3002\n955\nThe predecessor of Peking University is the Imperial University of\nPeking.\nKnowledge\n\u4e16\u754c\u4e0a\u73b0\u5b58\u6700\u9ad8\u7684\u6728\u5854\u662f\u4e2d\u56fd\u4ec0\u4e48\u5854\uff1f\nWhat is the tallest wooden tower in the world that still exists in China?\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u4e16\u754c\u4e0a\u73b0\u5b58\u6700\u9ad8\u7684\u6728\u5854\u662f\u4e2d\u56fd\u5c71\u897f\u5e94\u53bf\u6728\u5854\u3002\n987\nThe tallest existing wooden tower in the world is the Yinxian\nWooden Tower in Shanxi, China.\nQwen-14B-Chat (SFT)\n\u73b0\u5b58\u6700\u9ad8\u7684\u6728\u5854\u662f\u4e2d\u56fd\u5c71\u897f\u5e94\u53bf\u6728\u5854\uff0c\u5168\u540d\u4e3a\u4f5b\u5bab\u5bfa\u91ca\u8fe6\n\u5854\u3002\u5e94\u53bf\u6728\u5854\u4f4d\u4e8e\u5c71\u897f\u7701\u6714\u5dde\u5e02\u5e94\u53bf\u53bf\u57ce\u5185\u897f\u5317\u89d2\u7684\u4f5b\u5bab\u5bfa\n\u5185\uff0c\u5efa\u4e8e\u8fbd\u4ee3\u6e05\u5b81\u4e8c\u5e74\uff081056\u5e74\uff09\uff0c\u91d1\u660e\u660c\u516d\u5e74\uff081195\u5e74\uff09\n\u589e\u4fee\u5b8c\u6bd5\uff0c\u662f\u4e2d\u56fd\u73b0\u5b58\u6700\u9ad8\u6700\u53e4\u7684\u4e00\u5ea7\u6728\u6784\u5854\u5f0f\u5efa\u7b51\uff0c\u4e5f\u662f\n\u4e16\u754c\u73b0\u5b58\u6700\u9ad8\u7684\u6728\u7ed3\u6784\u5efa\u7b51\u3002\n1055\nThe tallest existing wooden tower in the world is the Yinxian\nWooden Tower, also known as the Shijia Tower of the Fogong\nTemple. It is inside the Fogong Temple in Yinxian, Shanxi Province,\nChina. The tower was originally built in the second year of the\nQingning era of the Liao Dynasty (1056 AD) and completed its\nexpansion in the sixth year of the Jin Dynasty\u2019s Mingchang era\n(1195 AD). It is the tallest and oldest existing wooden tower in\nChina and the tallest wooden structure in the world.\nQwen-14B-Chat (RLHF) \u4e16\u754c\u4e0a\u73b0\u5b58\u6700\u9ad8\u7684\u6728\u5854\u662f\u4e2d\u56fd\u7684\u5e94\u53bf\u6728\u5854\uff0c\u4f4d\u4e8e\u5c71\u897f\u7701\u5e94\n\u53bf\u3002\u8be5\u5854\u5efa\u4e8e\u516c\u51431056\u5e74\uff0c\u9ad867.31\u7c73\uff0c\u5171\u67095\u5c42\uff0c\u662f\u4e2d\u56fd\u53e4\n\u4ee3\u5efa\u7b51\u6280\u672f\u7684\u6770\u51fa\u4ee3\u8868\u4e4b\u4e00\u3002\u5e94\u53bf\u6728\u5854\u7684\u5efa\u7b51\u98ce\u683c\u72ec\u7279\uff0c\u91c7\n\u7528\u4e86\u8bb8\u591a\u53e4\u4ee3\u5efa\u7b51\u6280\u672f\uff0c\u5982\u69ab\u536f\u7ed3\u6784\u3001\u6597\u62f1\u3001\u60ac\u6311\u7b49\uff0c\u5177\u6709\n\u5f88\u9ad8\u7684\u827a\u672f\u4ef7\u503c\u548c\u5386\u53f2\u4ef7\u503c\u3002\u5e94\u53bf\u6728\u5854\u4e5f\u662f\u4e2d\u56fd\u6587\u5316\u9057\u4ea7\u4fdd\n\u62a4\u7684\u91cd\u70b9\u4e4b\u4e00\uff0c\u88ab\u5217\u4e3a\u5168\u56fd\u91cd\u70b9\u6587\u7269\u4fdd\u62a4\u5355\u4f4d\u548c\u4e16\u754c\u6587\u5316\u9057\n\u4ea7\u3002\n1060\nThe tallest existing wooden tower in the world is the Yinxian\nWooden Tower in Yinxian, Shanxi Province, China. It was built in\nthe year 1056 AD and stands at a height of 67.31 meters, consisting\nof 5 floors. This tower is an outstanding representative of ancient\nChinese architectural technology. The Yinxian Wooden Tower\nshowcases a unique architectural style and incorporates various\nancient construction techniques such as mortise and tenon joints,\nbracket sets, and cantilever beams, possessing high artistic and\nhistorical value. It is also a key cultural heritage site in China,\nlisted as a national key cultural relic protection unit and a UNESCO\nWorld Cultural Heritage site.\nGPT-3.5\n\u4e16\u754c\u4e0a\u73b0\u5b58\u6700\u9ad8\u7684\u6728\u5854\u662f\u4e2d\u56fd\u7684\u666e\u9640\u5c71\u666e\u6d4e\u5bfa\u5854\uff0c\u4e5f\u88ab\u79f0\u4e3a\n\u53e4\u4f5b\u9876\u5b9d\u5854\u3002\u8be5\u5854\u5efa\u4e8e1056\u5e74\uff0c\u9ad8\u7ea670.28\u7c73\u3002\n864\nThe tallest existing wooden tower in the world is the Putuoshan\nPujisi Pagoda, also known as the Ancient Baoding Pagoda, located\nin Putuoshan, China. It was built in the year 1056 AD and stands\nat a height of approximately 70.28 meters.\n41\nGPT-4\n\u4e16\u754c\u4e0a\u73b0\u5b58\u6700\u9ad8\u7684\u6728\u5854\u662f\u4e2d\u56fd\u7684\u5e94\u53bf\u6728\u5854\uff0c\u4e5f\u88ab\u79f0\u4e3a\u4f5b\u5bab\u5bfa\n\u5927\u4f5b\u5854\u3002\n1040\nThe tallest wooden pagoda currently in existence in the world is\nthe Yingxian Wooden Pagoda in China, also known as the Pagoda\nof Fogong Temple.\nLanguage Understanding\n\u5403\u8461\u8404\u4e0d\u5410\u8461\u8404\u76ae\u7684\u62fc\u97f3\u600e\u4e48\u5199\uff1f\nWhat is the Pingyin for \u201c\u5403\u8461\u8404\u4e0d\u5410\u8461\u8404\u76ae\u201d?\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u5403\u8461\u8404\u4e0d\u5410\u8461\u8404\u76ae\u7684\u62fc\u97f3\u662f\uff1a\u201cch\u00af\u0131 p\u00b4u b`u t\u02c7u p\u00b4u p\u00b4\u0131\u201d\u3002\n914\nTo eat grapes without spitting out the grape skins, the pinyin is \u201cch\u00af\u0131\np\u00b4u b`u t\u02c7u p\u00b4u p\u00b4\u0131\u201d.\nQwen-14B-Chat (SFT)\n\u62fc\u97f3\uff1a\u5403\u8461\u8404\u4e0d\u5410\u8461\u8404\u76ae\u201cch\u00af\u0131 p\u00b4u t\u00b4ao b`u t\u02c7u p\u00b4u t\u00b4ao p\u00b4\u0131\u201d\n1049\nPingyin: To eat grapes without spitting out the grape skins, \u2018ch\u00af\u0131 p\u00b4u\nb`u t\u02c7u p\u00b4u p\u00b4\u0131\u201d.\nQwen-14B-Chat (RLHF) \u5403\u8461\u8404\u4e0d\u5410\u8461\u8404\u76ae\u7684\u62fc\u97f3\u662f\uff1ach\u00af\u0131 p\u00b4u t\u00b4ao b`u t\u02c7u p\u00b4u t\u00b4ao p\u00b4\u0131\u3002\u62fc\u97f3\n\u662f\u4e00\u79cd\u7528\u62c9\u4e01\u5b57\u6bcd\u8868\u793a\u6c49\u5b57\u53d1\u97f3\u7684\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5e2e\u52a9\u4eba\u4eec\u5b66\n\u4e60\u548c\u8bb0\u5fc6\u6c49\u5b57\u7684\u53d1\u97f3\u3002\u5728\u8fd9\u4e2a\u62fc\u97f3\u4e2d\uff0c\u201c\u5403\u201d\u8868\u793a\u201cch\u00af\u0131\u201d\uff0c\u201c\u8461\n\u8404\u201d\u8868\u793a\u201cp\u00b4u t\u00b4ao\u201d\uff0c\u201c\u4e0d\u201d\u8868\u793a\u201cb`u\u201d\uff0c\u201c\u5410\u201d\u8868\u793a\u201ct\u02c7u\u201d\uff0c\u201c\u8461\u8404\u76ae\u201d\u8868\n\u793a\u201cp\u00b4u t\u00b4ao p\u00b4\u0131\u201d\u3002\n1068\nThe pinyin for \u2019\u5403\u8461\u8404\u4e0d\u5410\u8461\u8404\u76ae\u2019 is \u201cch\u00af\u0131 p\u00b4u t\u00b4ao b`u t\u02c7u p\u00b4u t\u00b4ao p\u00b4\u0131\u201d.\nPinyin is a method of representing the pronunciation of Chinese\ncharacters using Latin letters, which can help people learn and\nremember the pronunciation of Chinese characters. In this pinyin,\n\u201c\u5403\u201d is represented as \u2019ch\u00af\u0131\u2019, \u201c\u8461\u8404\u201d is represented as \u201cp\u00b4u t\u00b4ao\u201d, \u201c\u4e0d\u201d\nis represented as \u201cb`u\u201d, \u201c\u5410\u201d is represented as \u201ct\u02c7u\u201d, and \u201c\u8461\u8404\u76ae\u201d\nis represented as \u201cp\u00b4u t\u00b4ao p\u00b4\u0131\u201d.\nGPT-3.5\nch\u00af\u0131 p\u00b4u t\u00b4ao b\u00b4u t\u02c7u p\u00b4u t\u00b4ao p\u00b4\u0131 de p\u00af\u0131n y\u00af\u0131n z\u02c7en me xi\u02c7e\uff1f\n946\nch\u00af\u0131 p\u00b4u t\u00b4ao b\u00b4u t\u02c7u p\u00b4u t\u00b4ao p\u00b4\u0131 de p\u00af\u0131n y\u00af\u0131n z\u02c7en me xi\u02c7e\uff1f\nGPT-4\nch\u00af\u0131 p\u00b4u t\u00b4ao b`u t\u02c7u p\u00b4u t\u00b4ao p\u00b4\u0131\n1040\nch\u00af\u0131 p\u00b4u t\u00b4ao b`u t\u02c7u p\u00b4u t\u00b4ao p\u00b4\u0131\nLanguage Understanding\n\u7236\u4eb2\uff1a\u513f\u5b50\uff0c\u4f60\u9ad8\u4e2d\u6bd5\u4e1a\u4e86\uff0c\u73b0\u5728\u6253\u7b97\u4e0a\u54ea\u4e2a\u5927\u5b66\uff1f\n\u513f\u5b50\uff1a\u7238\u7238\uff0c\u6211\u5df2\u7ecf\u51c6\u5907\u597d\u4e86\u7533\u8bf7\u8d44\u6599\uff0c\u60f3\u7533\u8bf7\u4e00\u4e2a\u597d\u7684\u672c\u79d1\u4e13\u4e1a\u3002\n\u7236\u4eb2\uff1a\u90a3\u4f60\u5df2\u7ecf\u60f3\u597d\u7533\u8bf7\u54ea\u4e9b\u5927\u5b66\u4e86\u5417\uff1f\n\u513f\u5b50\uff1a\u662f\u7684\uff0c\u6211\u8ba1\u5212\u7533\u8bf7\u51e0\u6240\u6392\u540d\u9760\u524d\u7684\u5927\u5b66\u3002\u6211\u5df2\u7ecf\u505a\u4e86\u5f88\u591a\u7814\u7a76\uff0c\u51c6\u5907\u5728\u5b66\u4e1a\u3001\u793e\u4ea4\u548c\u804c\n\u4e1a\u89c4\u5212\u7b49\u65b9\u9762\u5168\u9762\u63d0\u5347\u81ea\u5df1\u3002\n\u7236\u4eb2\uff1a\u5f88\u597d\uff01\u4f60\u4e00\u76f4\u90fd\u5f88\u52aa\u529b\uff0c\u7238\u7238\u5988\u5988\u4e00\u76f4\u90fd\u5f88\u81ea\u8c6a\u3002\u4e0a\u5927\u5b66\u662f\u4e00\u4e2a\u65b0\u7684\u8d77\u70b9\uff0c\u4f60\u9700\u8981\u9762\u5bf9\n\u66f4\u591a\u7684\u6311\u6218\u548c\u673a\u4f1a\u3002\u4f60\u6709\u4ec0\u4e48\u6253\u7b97\uff1f\n\u513f\u5b50\uff1a\u6211\u8ba1\u5212\u5c3d\u65e9\u9002\u5e94\u5927\u5b66\u751f\u6d3b\uff0c\u53c2\u52a0\u5404\u79cd\u793e\u56e2\u548c\u8bfe\u5916\u6d3b\u52a8\uff0c\u62d3\u5bbd\u81ea\u5df1\u7684\u89c6\u91ce\u548c\u4eba\u9645\u5708\u3002\u540c\n\u65f6\uff0c\u6211\u4f1a\u8ba4\u771f\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\u80fd\uff0c\u52aa\u529b\u53d6\u5f97\u597d\u6210\u7ee9\uff0c\u4e89\u53d6\u83b7\u5f97\u5956\u5b66\u91d1\u548c\u5176\u4ed6\u8363\u8a89\u3002\n\u7236\u4eb2\uff1a\u975e\u5e38\u597d\uff01\u4f60\u7684\u8ba1\u5212\u5f88\u660e\u667a\uff0c\u7238\u7238\u76f8\u4fe1\u4f60\u4e00\u5b9a\u4f1a\u6709\u4e00\u4e2a\u5145\u5b9e\u800c\u6709\u610f\u4e49\u7684\u5927\u5b66\u751f\u6d3b\u3002\u7238\u7238\u4f1a\n\u4e00\u76f4\u652f\u6301\u4f60\u7684\u51b3\u5b9a\uff0c\u5e0c\u671b\u4f60\u53d6\u5f97\u597d\u6210\u7ee9\uff0c\u4e3a\u4f60\u7684\u672a\u6765\u6253\u4e0b\u575a\u5b9e\u7684\u57fa\u7840\u3002\n\u513f\u5b50\uff1a\u8c22\u8c22\u7238\u7238\uff01\u6211\u4f1a\u52a0\u500d\u52aa\u529b\uff0c\u4e0d\u8f9c\u8d1f\u60a8\u5bf9\u6211\u7684\u671f\u671b\u3002\n\u8fd9\u6bb5\u5bf9\u8bdd\u4e2d\uff0c\u513f\u5b50\u6253\u7b97\u4e0a\u5927\u5b66\u540e\u6709\u54ea\u4e9b\u8ba1\u5212\u548c\u6253\u7b97\uff1f\nFather: Son, you have graduated from high school. Which university are you planning to attend now?\nSon: Dad, I have already prepared my application materials, and I want to apply for an excellent under-\ngraduate program.\n42\nFather: Have you already decided which universities you want to apply to?\nSon: Yes, I plan to apply to several top-ranked universities. I have done a lot of research, and I am ready to\nimprove myself in academics, social life, and career planning.\nFather: That\u2019s great! You have always been hardworking and your mom and I are proud of you. Going to\ncollege is a new starting point, and you will face more challenges and opportunities. What are your plans?\nSon: I plan to adapt to college life as soon as possible and participate in various clubs and extracurricular\nactivities to broaden my horizons and social circle. At the same time, I will study my major knowledge\nand skills seriously, strive for good grades, and try to win scholarships and other honors.\nFather: Very good! Your plans are wise. Dad believes that you will have a fulfilling and meaningful college\nlife. Dad will always support your decisions and hopes you achieve good results, laying a solid foundation\nfor your future.\nSon: Thank you, Dad! I will work even harder and not let your expectations down.\nWhat are the son\u2019s plans and intentions after going to college in this conversation?\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u513f\u5b50\u6253\u7b97\u5c3d\u65e9\u9002\u5e94\u5927\u5b66\u751f\u6d3b\uff0c\u53c2\u52a0\u5404\u79cd\u793e\u56e2\u548c\u8bfe\u5916\u6d3b\u52a8\uff0c\u62d3\n\u5bbd\u81ea\u5df1\u7684\u89c6\u91ce\u548c\u4eba\u9645\u5708\u3002\u540c\u65f6\uff0c\u4ed6\u4f1a\u8ba4\u771f\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\n\u80fd\uff0c\u52aa\u529b\u53d6\u5f97\u597d\u6210\u7ee9\uff0c\u4e89\u53d6\u83b7\u5f97\u5956\u5b66\u91d1\u548c\u5176\u4ed6\u8363\u8a89\u3002\n973\nThe son plans to adapt to university life as soon as possible, par-\nticipate in various clubs and extracurricular activities, and expand\nhis horizons and social circle. At the same time, he will study\nprofessional knowledge and skills seriously, work hard to achieve\ngood grades, and strive to obtain scholarships and other honors.\nQwen-14B-Chat (SFT)\n\u513f\u5b50\u6253\u7b97\u5728\u5b66\u4e1a\u3001\u793e\u4ea4\u548c\u804c\u4e1a\u89c4\u5212\u7b49\u65b9\u9762\u5168\u9762\u63d0\u5347\u81ea\u5df1\uff0c\u5c3d\n\u65e9\u9002\u5e94\u5927\u5b66\u751f\u6d3b\uff0c\u53c2\u52a0\u5404\u79cd\u793e\u56e2\u548c\u8bfe\u5916\u6d3b\u52a8\uff0c\u62d3\u5bbd\u81ea\u5df1\u7684\u89c6\n\u91ce\u548c\u4eba\u9645\u5708\u3002\u540c\u65f6\uff0c\u4ed6\u4f1a\u8ba4\u771f\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\u80fd\uff0c\u52aa\u529b\u53d6\n\u5f97\u597d\u6210\u7ee9\uff0c\u4e89\u53d6\u83b7\u5f97\u5956\u5b66\u91d1\u548c\u5176\u4ed6\u8363\u8a89\u3002\n1049\nThe son plans to comprehensively improve himself in his academic,\nsocial, and career planning aspects to adapt to university life as early\nas possible. He will participate in various clubs and extracurricular\nactivities to broaden his horizons and expand his social circle. At\nthe same time, he will diligently study professional knowledge and\nskills, strive for good grades, and obtain scholarships and other\nhonors.\nQwen-14B-Chat (RLHF) \u513f\u5b50\u6253\u7b97\u5728\u5927\u5b66\u751f\u6d3b\u4e2d\u53c2\u52a0\u5404\u79cd\u793e\u56e2\u548c\u8bfe\u5916\u6d3b\u52a8\uff0c\u62d3\u5bbd\u81ea\u5df1\n\u7684\u89c6\u91ce\u548c\u4eba\u9645\u5708\u3002\u540c\u65f6\uff0c\u4ed6\u4f1a\u8ba4\u771f\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\u80fd\uff0c\u52aa\n\u529b\u53d6\u5f97\u597d\u6210\u7ee9\uff0c\u4e89\u53d6\u83b7\u5f97\u5956\u5b66\u91d1\u548c\u5176\u4ed6\u8363\u8a89\u3002\u4ed6\u7684\u76ee\u6807\u662f\u5c3d\n\u65e9\u9002\u5e94\u5927\u5b66\u751f\u6d3b\uff0c\u4e3a\u81ea\u5df1\u7684\u672a\u6765\u6253\u4e0b\u575a\u5b9e\u7684\u57fa\u7840\u3002\n975\nThe son plans to participate in various clubs and extracurricular\nactivities during his college life, in order to broaden his horizons\nand expand his social circle. At the same time, he will study\nprofessional knowledge and skills seriously, strive for good grades,\nand aim to obtain scholarships and other honors. His goal is to\nadapt to college life as soon as possible and lay a solid foundation\nfor his future.\nGPT-3.5\n\u513f\u5b50\u8ba1\u5212\u9002\u5e94\u5927\u5b66\u751f\u6d3b\uff0c\u53c2\u52a0\u5404\u79cd\u793e\u56e2\u548c\u8bfe\u5916\u6d3b\u52a8\uff0c\u62d3\u5bbd\u81ea\n\u5df1\u7684\u89c6\u91ce\u548c\u4eba\u9645\u5708\u3002\u540c\u65f6\uff0c\u4ed6\u8fd8\u8ba1\u5212\u8ba4\u771f\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\n\u80fd\uff0c\u52aa\u529b\u53d6\u5f97\u597d\u6210\u7ee9\uff0c\u4e89\u53d6\u83b7\u5f97\u5956\u5b66\u91d1\u548c\u5176\u4ed6\u8363\u8a89\u3002\u4ed6\u5e0c\u671b\n\u5728\u5927\u5b66\u6253\u4e0b\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u4e3a\u81ea\u5df1\u7684\u672a\u6765\u6253\u597d\u57fa\u7840\u3002\n976\nThe son plans to adapt to college life participate in various clubs\nand extracurricular activities to broaden his horizons and social\ncircle. At the same time, he plans to study professional knowledge\nand skills seriously, strive for good grades, and obtain scholarships\nand other honors. He hopes to lay a solid foundation in college and\nbuild a good foundation for his future.\n43\nGPT-4\n\u513f\u5b50\u7684\u5927\u5b66\u8ba1\u5212\u548c\u6253\u7b97\u5305\u62ec\uff1a\n1. \u5c3d\u65e9\u9002\u5e94\u5927\u5b66\u751f\u6d3b\u3002\n2. \u53c2\u52a0\u5404\u79cd\u793e\u56e2\u548c\u8bfe\u5916\u6d3b\u52a8\uff0c\u4ee5\u62d3\u5bbd\u81ea\u5df1\u7684\u89c6\u91ce\u548c\u4eba\u9645\u5708\u3002\n3. \u8ba4\u771f\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\u80fd\uff0c\u52aa\u529b\u53d6\u5f97\u597d\u6210\u7ee9\u3002\n4. \u4e89\u53d6\u83b7\u5f97\u5956\u5b66\u91d1\u548c\u5176\u4ed6\u8363\u8a89\u3002\n5. \u4e3a\u672a\u6765\u6253\u4e0b\u575a\u5b9e\u7684\u57fa\u7840\u3002\n1048\nThe son\u2019s university plans and aspirations include:\n1. Adjusting to university life as soon as possible.\n2. Joining various clubs and extracurricular activities to broaden\nhis horizons and social circle.\n3. Studying professional knowledge and skills seriously and striv-\ning for excellent grades.\n4. Striving to obtain scholarships and other honors.\n5. Building a solid foundation for the future.\nCreative Writing\n\u8bf7\u7f16\u5199\u4e00\u4e2a\u4ee5\u8d85\u5e02\u4e3a\u80cc\u666f\u7684\uff0c\u5185\u5bb9\u6d89\u53ca\u5230\u6c34\u679c\u8bef\u4f1a\u7684\u7b11\u8bdd\u3002\nPlease write a joke set in a supermarket involving a fruit misunderstanding.\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u6709\u4e00\u5929\uff0c\u5c0f\u660e\u53bb\u8d85\u5e02\u4e70\u6c34\u679c\uff0c\u4ed6\u4e70\u4e86\u4e00\u7bee\u5b50\u82f9\u679c\u3002\u5f53\u4ed6\u56de\u5230\n\u5bb6\uff0c\u4ed6\u53d1\u73b0\u82f9\u679c\u90fd\u662f\u7ea2\u8272\u7684\uff0c\u4ed6\u60f3\uff1a\u8fd9\u4e9b\u82f9\u679c\u4e00\u5b9a\u5f88\u597d\u5403\u3002\n\u4f46\u662f\uff0c\u5f53\u4ed6\u5f00\u59cb\u5403\u7684\u65f6\u5019\uff0c\u4ed6\u53d1\u73b0\u8fd9\u4e9b\u82f9\u679c\u90fd\u662f\u9178\u7684\u3002\u4ed6\u5f88\n\u60ca\u8bb6\uff0c\u56e0\u4e3a\u4ed6\u4e70\u7684\u662f\u751c\u82f9\u679c\u3002\u4ed6\u53bb\u8d85\u5e02\u95ee\uff0c\u5e97\u5458\u8bf4\uff1a\u201c\u5bf9\u4e0d\n\u8d77\uff0c\u6211\u4eec\u6ca1\u6709\u751c\u82f9\u679c\uff0c\u53ea\u6709\u7ea2\u8272\u7684\u82f9\u679c\u3002\u201d\u5c0f\u660e\u5f88\u751f\u6c14\uff0c\u4ed6\n\u60f3\uff1a\u6211\u4e70\u7684\u662f\u751c\u82f9\u679c\uff0c\u4e3a\u4ec0\u4e48\u7ed9\u6211\u7ea2\u8272\u7684\u82f9\u679c\u5462\uff1f\u4ed6\u56de\u53bb\u544a\n\u8bc9\u4e86\u5988\u5988\uff0c\u5988\u5988\u7b11\u7740\u8bf4\uff1a\u201c\u4f60\u4e70\u7684\u662f\u7ea2\u82f9\u679c... [[The original\ncontent is long and the follow up is truncated]]\n849\nOne day, Xiaoming went to the supermarket to buy fruits. He\nbought a basket of apples. When he got home, he found that all the\napples were red. He thought to himself: These apples must be very\ndelicious. However, when he started eating them, he found that\nthese apples were sour. He was surprised because he bought sweet\napples. He went to the supermarket to ask, and the shopkeeper said,\n\u201cI\u2019m sorry, we don\u2019t have sweet apples, only red apples.\u201d Xiaoming\nwas angry and thought to himself: I bought sweet apples, why did\nthey give me red apples? He went back and told his mother, and his\nmother smiled and said, \u201cYou bought red delicious apples.... [[The\noriginal content is long and the follow up is truncated]]\n44\nQwen-14B-Chat (SFT)\n\u4e00\u5929\uff0c\u4e00\u4f4d\u987e\u5ba2\u8d70\u8fdb\u8d85\u5e02\uff0c\u5bf9\u5e97\u5458\u8bf4\uff1a\u201c\u6211\u8981\u4e70\u4e00\u4e2a\u9999\u8549\u3002\u201d\n\u5e97\u5458\u56de\u7b54\uff1a\u201c\u597d\u7684\uff0c\u5148\u751f\uff0c\u6211\u4eec\u8fd9\u91cc\u6709\u5404\u79cd\u5404\u6837\u7684\u9999\u8549\uff0c\u60a8\n\u60f3\u8981\u54ea\u4e00\u79cd\uff1f\u201d\n\u987e\u5ba2\u56de\u7b54\uff1a\u201c\u6211\u8981\u4e00\u4e2a\u9ec4\u8272\u7684\u9999\u8549\u3002\u201d\n\u5e97\u5458\u6709\u4e9b\u7591\u60d1\uff0c\u4f46\u8fd8\u662f\u7ed9\u4ed6\u62ff\u4e86\u4e00\u4e2a\u9ec4\u8272\u7684\u9999\u8549\u3002\n\u987e\u5ba2\u53c8\u8bf4\uff1a\u201c\u6211\u8981\u4e00\u4e2a\u5f2f\u66f2\u7684\u9999\u8549\u3002\u201d\n\u5e97\u5458\u66f4\u52a0\u7591\u60d1\u4e86\uff0c\u4f46\u4ed6\u8fd8\u662f\u7ed9\u4ed6\u62ff\u4e86\u4e00\u4e2a\u5f2f\u66f2\u7684\u9999\u8549\u3002\n\u6700\u540e\uff0c\u987e\u5ba2\u8bf4\uff1a\u201c\u6211\u8981\u4e00\u4e2a\u4f1a\u8bf4\u8bdd\u7684\u9999\u8549\u3002\u201d\n\u5e97\u5458\u5fcd\u4e0d\u4f4f\u7b11\u4e86\uff0c\u56de\u7b54\uff1a\u201c\u5148\u751f\uff0c\u6211\u4eec\u8fd9\u91cc\u6ca1\u6709\u4f1a\u8bf4\u8bdd\u7684\u9999\n\u8549\uff0c\u90a3\u53ea\u662f\u4e2a\u8bef\u4f1a\u3002\u201d\n\u987e\u5ba2\u4e5f\u7b11\u4e86\uff0c\u56de\u7b54\uff1a\u201c\u6211\u77e5\u9053\uff0c\u6211\u53ea\u662f\u60f3\u548c\u4f60\u5f00\u4e2a\u73a9\u7b11\u3002\u201d\n980\nOne day, a customer walked into a supermarket and said to the shop\nassistant, \u201cI want to buy a banana.\u201d\nThe assistant replied, \u201cOkay, sir, we have all kinds of bananas here.\nWhich one would you like?\u201d\nThe customer answered, \u201cI want a yellow banana.\u201d\nThe assistant was a little confused but still gave him a yellow\nbanana.\nThe customer then said, \u201cI want a curved banana.\u201d\nThe assistant was even more confused, but he still gave him a\ncurved banana.\nFinally, the customer said, \u201cI want a talking banana.\u201d\nThe assistant couldn\u2019t help but laugh and replied, \u201dSir, we don\u2019t\nhave bananas that can talk here, that\u2019s just a misunderstanding.\u201d\nThe customer also laughed and replied, \u201cI know, I was just joking\nwith you.\u201d\n45\nQwen-14B-Chat (RLHF) \u6709\u4e00\u5929\uff0c\u5c0f\u660e\u53bb\u8d85\u5e02\u4e70\u6c34\u679c\u3002\u4ed6\u6311\u9009\u4e86\u4e00\u7bee\u5b50\u65b0\u9c9c\u7684\u82f9\u679c\u548c\n\u9999\u8549\uff0c\u7136\u540e\u8d70\u5411\u6536\u94f6\u53f0\u3002\n\u6536\u94f6\u5458\u5c0f\u738b\u770b\u5230\u5c0f\u660e\u624b\u4e2d\u7684\u6c34\u679c\uff0c\u7a81\u7136\u5927\u58f0\u558a\u9053\uff1a\u201c\u54c7\uff0c\u4f60\n\u7684\u6c34\u679c\u597d\u65b0\u9c9c\u554a\uff01\u4f60\u662f\u4ece\u54ea\u91cc\u4e70\u7684\uff1f\u201d\n\u5c0f\u660e\u6123\u4e86\u4e00\u4e0b\uff0c\u5fc3\u60f3\uff1a\u201c\u96be\u9053\u6211\u4e70\u6c34\u679c\u7684\u6280\u5de7\u88ab\u53d1\u73b0\u4e86\uff1f\u201d\u4e8e\n\u662f\u4ed6\u5f97\u610f\u5730\u56de\u7b54\uff1a\u201c\u6211\u521a\u521a\u4ece\u679c\u56ed\u91cc\u6458\u7684\uff0c\u4fdd\u8bc1\u65b0\u9c9c\uff01\u201d\n\u5c0f\u738b\u542c\u4e86\uff0c\u5fcd\u4e0d\u4f4f\u7b11\u4e86\u8d77\u6765\uff1a\u201c\u4f60\u53ef\u771f\u9017\uff0c\u8fd9\u662f\u8d85\u5e02\u91cc\u7684\u6c34\n\u679c\uff0c\u4e0d\u662f\u4f60\u5bb6\u679c\u56ed\u91cc\u7684\u3002\u201d\n\u5c0f\u660e\u542c\u4e86\uff0c\u987f\u65f6\u50bb\u773c\u4e86\u3002\u4ed6\u5c34\u5c2c\u5730\u89e3\u91ca\u9053\uff1a\u201c\u554a\uff0c\u5bf9\u4e0d\u8d77\uff0c\n\u6211\u592a\u7d27\u5f20\u4e86\uff0c\u4ee5\u4e3a\u8fd9\u662f\u6211\u7684\u6c34\u679c\u3002\u201d\n\u5c0f\u738b\u7b11\u7740\u5b89\u6170\u9053\uff1a\u201c\u6ca1\u4e8b\u6ca1\u4e8b\uff0c\u6211\u4eec\u90fd\u641e\u9519\u4e86\u3002\u4e0d\u8fc7\uff0c\u4f60\u7684\n\u6c34\u679c\u786e\u5b9e\u5f88\u597d\uff0c\u6211\u4eec\u8d85\u5e02\u91cc\u7684\u6c34\u679c\u4e5f\u90fd\u662f\u65b0\u9c9c\u7684\u3002\u201d\n\u5c0f\u660e\u542c\u4e86\uff0c\u677e\u4e86\u4e00\u53e3\u6c14\uff0c\u5fc3\u60c5\u4e5f\u597d\u4e86\u8d77\u6765\u3002\u4ed6\u4ed8\u4e86\u94b1\uff0c\u63d0\u7740\n\u6c34\u679c\u79bb\u5f00\u4e86\u8d85\u5e02\uff0c\u5fc3\u91cc\u60f3\u7740\uff1a\u201c\u4e0b\u6b21\u518d\u4e70\u6c34\u679c\uff0c\u4e00\u5b9a\u8981\u66f4\u52a0\n\u5c0f\u5fc3\uff0c\u4e0d\u8981\u95f9\u51fa\u7b11\u8bdd\u4e86\uff01\u201d\n986\nOne day, Xiao Ming went to the supermarket to buy fruits. He\npicked a basket of fresh apples and bananas, and then headed\ntowards the cashier.\nThe cashier, Xiao Wang, saw the fruits in Xiao Ming\u2019s hands and\nsuddenly shouted, \u201dWow, your fruits are so fresh! Where did you\nbuy them from?\u201d\nXiao Ming was stunned for a moment and thought to himself,\n\u201dCould it be that my fruit-buying skills have been discovered?\u201d So\nhe proudly replied, \u201dI just picked them from the orchard, guaranteed\nfresh!\u201d\nUpon hearing this, Xiao Wang couldn\u2019t help but laugh and said,\n\u201cYou\u2019re really funny. These are fruits from the supermarket, not\nfrom your own orchard.\u201d\nXiao Ming, upon listening, was immediately dumbfounded. He\nawkwardly explained, \u201cOh, I\u2019m sorry, I was too nervous, and I\nthought these were my fruits.\u201d\nXiao Wang smiled and comforted him, saying, \u201cIt\u2019s alright, it\u2019s\nalright. We both made a mistake. However, your fruits are indeed\nvery good, and the fruits in our supermarket are also fresh.\u201d\nUpon hearing this, Xiao Ming breathed a sigh of relief, and his\nmood improved. He paid the money, carried the fruits, and left\nthe supermarket, thinking, \u201cNext time I buy fruits, I must be more\ncareful and avoid making such a funny mistake!\u201d\nGPT-3.5\n\u5728\u8d85\u5e02\u7684\u6c34\u679c\u533a\uff0c\u5c0f\u660e\u770b\u5230\u4e86\u4e00\u76d2\u7ea2\u82f9\u679c\u548c\u4e00\u76d2\u7eff\u82f9\u679c\u3002\u4ed6\n\u62ff\u7740\u4e24\u76d2\u82f9\u679c\u8d70\u5230\u6536\u94f6\u53f0\u524d\u6392\u961f\u7ed3\u8d26\u3002\n\u6536\u94f6\u5458\u770b\u7740\u5c0f\u660e\u624b\u91cc\u7684\u4e24\u76d2\u82f9\u679c\u8bf4\uff1a\u201c\u8fd9\u4f4d\u5c0f\u670b\u53cb\uff0c\u4f60\u77e5\u9053\n\u7ea2\u82f9\u679c\u548c\u7eff\u82f9\u679c\u6709\u4ec0\u4e48\u533a\u522b\u5417\uff1f\u201d\n\u5c0f\u660e\u60f3\u4e86\u4e00\u4e0b\uff0c\u7136\u540e\u7b11\u7740\u56de\u7b54\uff1a\u201c\u5f53\u7136\u77e5\u9053\u4e86\uff01\u7ea2\u82f9\u679c\u662f\u60c5\n\u4fa3\uff0c\u7eff\u82f9\u679c\u662f\u5355\u8eab\u72d7\u3002\u201d\n\u6536\u94f6\u5458\u88ab\u5c0f\u660e\u7684\u56de\u7b54\u9017\u4e50\u4e86\uff0c\u5fcd\u4e0d\u4f4f\u7b11\u4e86\u51fa\u6765\u3002\n964\n46\nIn the supermarket\u2019s fruit section, Xiaoming saw a box of red apples\nand a box of green apples. He held the two boxes of apples and\nwalked to the cash register to queue up for payment.\nThe cashier looked at the two boxes of apples in Xiaoming\u2019s hands\nand said, \u201cHey, little friend, do you know the difference between\nred apples and green apples?\u201d\nXiaoming thought momentarily and then smiled, \u201cOf course, I\nknow! Red apples are couples, and green apples are single dogs.\u201d\nXiaoming\u2019s answer amused the cashier, and couldn\u2019t help but laugh.\nGPT-4\n\u201c\u4e00\u4e2a\u975e\u5e38\u5584\u826f\u7684\u7537\u5b69\uff0c\u5728\u8d85\u5e02\u91cc\u53d1\u73b0\u4e86\u4e00\u4e2a\u8001\u592a\u592a\u5728\u82f9\u679c\n\u5806\u91cc\u6311\u9009\u82f9\u679c\u3002\u4ed6\u770b\u5230\u8001\u592a\u592a\u4f3c\u4e4e\u627e\u4e0d\u5230\u6ee1\u610f\u7684\u82f9\u679c\uff0c\u5c31\u8d70\n\u4e0a\u524d\u53bb\u5e2e\u5fd9\u3002\n\u7537\u5b69\u8bf4\uff1a\u201c\u8001\u5976\u5976\uff0c\u8fd9\u7bb1\u5b50\u7684\u82f9\u679c\u6211\u4e4b\u524d\u90fd\u770b\u8fc7\u4e86\uff0c\u91cc\u9762\u6ca1\n\u6709\u4e00\u4e2a\u597d\u7684\u3002\u4f46\u662f\u6211\u77e5\u9053\u67dc\u53f0\u4e0a\u6709\u4e00\u4e2a\u6211\u4fdd\u7559\u7684\u6781\u597d\u7684\u82f9\n\u679c\uff0c\u6211\u53bb\u4e3a\u60a8\u62ff\u8fc7\u6765\u3002\u201d\n\u8001\u592a\u592a\u611f\u52a8\u5730\u70b9\u70b9\u5934\uff0c\u7537\u5b69\u5174\u51b2\u51b2\u5730\u8dd1\u53bb\u62ff\u82f9\u679c\uff0c\u53ea\u89c1\u4ed6\u7b11\n\u5475\u5475\u5730\u62ff\u8fc7\u4e00\u4e2a\u5de8\u5927\u7684\uff0c\u8273\u4e3d\u7684\u82f9\u679c\u653e\u5728\u8001\u592a\u592a\u9762\u524d\u3002\n\u8001\u592a\u592a\u7591\u60d1\u5730\u770b\u770b\u7537\u5b69\uff0c\u518d\u770b\u770b\u624b\u4e2d\u7684\u201c\u82f9\u679c\u201d\uff0c\u7136\u540e\u5fae\u7b11\u7740\n\u8bf4\uff1a\u201c\u5b69\u5b50\uff0c\u5f88\u611f\u8c22\u4f60\u7684\u597d\u610f\uff0c\u4f46\u8fd9\u662f\u4e00\u4e2a\u756a\u8304\u3002\u201d\u201d\n1144\nA very kind boy found an old lady picking apples in a pile at the\nsupermarket. Seeing that she could not find a satisfactory apple, he\napproached to help.\nThe boy said, \u201cGranny, I\u2019ve checked all the apples in this box\nbefore, and there isn\u2019t a good one. But I know I saved an excellent\napple on the counter for you. Let me get it for you.\u201d\nThe old lady nodded gratefully, and the boy eagerly ran to get the\napple. He joyfully handed a huge, vibrant apple to the old lady.\nConfused, the old lady looked at the boy, then at the \u201dapple\u201d in her\nhand, and smiled. She said, \u201cChild, thank you for your kindness,\nbut this is a tomato.\u201d\nMathematics\n\u4e00\u4e2a\u6b63\u65b9\u5f62\uff0c\u88ab\u5206\u62105\u4e2a\u76f8\u7b49\u7684\u957f\u65b9\u5f62\uff0c\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u5468\u957f\u662f60\u5398\u7c73\uff0c\u6b63\u65b9\u5f62\u7684\u5468\u957f\u662f\u591a\u5c11\u5398\u7c73\nA square is divided into five equal rectangles, each with a perimeter of 60 centimeters. What is the\nperimeter of the square in centimeters?\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u8bbe\u6b63\u65b9\u5f62\u7684\u8fb9\u957f\u4e3ax\uff0c\u90a3\u4e48\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u8fb9\u957f\u5c31\u662fx/5\u3002\u6839\u636e\n\u9898\u610f\uff0c\u6211\u4eec\u77e5\u9053\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u5468\u957f\u662f60\u5398\u7c73\uff0c\u6240\u4ee5\u6709\uff1a\n2(x/5) + 2(x/5) + 2(x/5) + 2(x/5) = 60\n\u5316\u7b80\u5f97\uff1ax = 60/(2 + 2 + 2 + 2) = 15\n\u6240\u4ee5\u6b63\u65b9\u5f62\u7684\u5468\u957f\u662f4x = 4 \u2217 15 = 60\u5398\u7c73\n976\nIf the side length of a square is x, then the side length of each\nrectangle is x/5. According to the question, we know that the\nperimeter of each rectangle is 60 centimeters, so we have:\n2(x/5) + 2(x/5) + 2(x/5) + 2(x/5) = 60\nSimplifying, we get: x = 60/(2 + 2 + 2 + 2) = 15\nSo the perimeter of the square is 4x = 4 \u2217 15 = 60 centimeters.\n47\nQwen-14B-Chat (SFT)\n\u8bbe\u6b63\u65b9\u5f62\u8fb9\u957f\u4e3ax\uff0c\u957f\u65b9\u5f62\u8fb9\u957f\u4e3ay\n\u6839\u636e\u9898\u610f\uff0c\u6709\uff1ax = 5y\n2y + 2y = 60\n4y = 60\ny = 15\nx = 5 \u2217 15 = 75\n\u6b63\u65b9\u5f62\u5468\u957f= 4x = 4 \u2217 75 = 300\n976\nLet the side length of the square be x and the length of the rectangle\nbe y. According to the problem: x = 5y 2y + 2y = 60 4y = 60\ny = 15 x = 5 \u2217 15 = 75 Perimeter of the square = 4x = 4 \u2217 75 =\n300\nQwen-14B-Chat (RLHF) \u9996\u5148\uff0c\u6211\u4eec\u8bbe\u6b63\u65b9\u5f62\u7684\u8fb9\u957f\u4e3ax\uff0c\u5219\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u957f\u4e3ax\uff0c\u5bbd\n\u4e3ax/5\u3002\n\u6839\u636e\u957f\u65b9\u5f62\u7684\u5468\u957f\u516c\u5f0f\uff0c\u53ef\u5f97\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u5468\u957f\u4e3a2(x +\nx/5) = 12x/5\u3002\n\u5df2\u77e5\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u5468\u957f\u4e3a60\u5398\u7c73\uff0c\u4ee3\u5165\u4e0a\u5f0f\u5f97\uff1a\n12x/5 = 60\n\u89e3\u5f97x = 25\u3002\n\u56e0\u6b64\uff0c\u6b63\u65b9\u5f62\u7684\u5468\u957f\u4e3a4x = 100\u5398\u7c73\u3002\n\u7b54\u6848\uff1a\u6b63\u65b9\u5f62\u7684\u5468\u957f\u662f100\u5398\u7c73\u3002\n1139\nFirst, let\u2019s assume that the square\u2019s side length is x. Therefore, the\nlength of each rectangle is x and the width is x/5.\nAccording to the formula for the perimeter of a rectangle, we can\ndetermine that the perimeter of each rectangle is 2(x + x/5) =\n12x/5.\nGiven that the perimeter of each rectangle is 60 centimeters, we\ncan substitute this value into the equation:\n12x/5 = 60\nSolving for x, we find x = 25.\nTherefore, the perimeter of the square is 4x = 100 centimeters.\nAnswer: The perimeter of the square is 100 centimeters.\nGPT-3.5\n\u8bbe\u6b63\u65b9\u5f62\u7684\u8fb9\u957f\u4e3ax\u5398\u7c73\uff0c\u5219\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u957f\u4e3ax\u5398\u7c73\uff0c\u5bbd\n\u4e3a5x/4\u5398\u7c73\u3002\n\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u5468\u957f\u4e3a2(x + 5x/4) = 60\u5398\u7c73\u3002\n\u5316\u7b80\u5f9713x/2 = 60\n\u89e3\u65b9\u7a0b\u5f97\u5230x = (60 \u2217 2)/13 = 120/13H9.23\n\u6b63\u65b9\u5f62\u7684\u5468\u957f\u4e3a4x = 4 \u2217 9.23 \u2248 36.92\u5398\u7c73\u3002\n897\nIf the side length of a square is x centimeters, then the length of\neach rectangle is x centimeters, and the width is 5x/4 centimeters.\nThe perimeter of each rectangle is 2(x + 5x/4) = 60 centimeters.\nSimplifying, we get 13x/2 = 60.\nSolving the equation, we find x = (60 * 2) / 13 = 120 / 13 \u2248 9.23.\nThe perimeter of the square is 4x = 4 * 9.23 \u2248 36.92 centimeters.\nGPT-4\n\u7531\u9898\u610f\u53ef\u77e5\uff0c\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u4e00\u6761\u957f\u8fb9\u7b49\u4e8e\u6b63\u65b9\u5f62\u7684\u8fb9\u957f\uff0c\u8bbe\n\u4e3aa\u3002\u8bbe\u6bcf\u4e2a\u957f\u65b9\u5f62\u7684\u4e00\u6761\u77ed\u8fb9\u4e3ab\uff0c\u90a3\u4e48\u67095b = a\u3002\u6839\u636e\u6bcf\n\u4e2a\u957f\u65b9\u5f62\u7684\u5468\u957f\u53ef\u5f97\u52302 \u2217 (a + b) = 60\u5398\u7c73\u3002\u5c065b = a\u4ee3\n\u5165\u5230\u957f\u65b9\u5f62\u7684\u5468\u957f\u516c\u5f0f\u4e2d\u5f97\uff1a2 \u2217 a + 2 \u2217 5b = 60\uff0c\u5316\u7b80\n\u5f972 \u2217 a + 10b = 60\uff0c\u7531\u4e8ea = 5b\uff0c\u6240\u4ee52 \u2217 a = 2 \u2217 5b = 10b\n\u90a3\u4e48\u670910b + 10b = 60\uff0c\u5f97b = 3\u5398\u7c73\uff0ca = 5 \u2217 b = 15\u5398\u7c73\u3002\n\u6240\u4ee5\uff0c\u6b63\u65b9\u5f62\u7684\u5468\u957f\u662f4 \u2217 a = 4 \u2217 15 = 60\u5398\u7c73\u3002\n1010\n48\nFrom the given problem, it can be inferred that the length of one\nside of each rectangle is equal to that of one side of the square,\ndenoted as a. Let the length of one side of each rectangle be denoted\nas b; then, we have 5b = a. The perimeter of each rectangle can be\nobtained as 2 \u2217 (a + b) = 60 centimeters.\nSubstituting 5b = a into the formula for the perimeter of the\nrectangle, we have 2a+25b = 60. Simplifying it gives 2a+10b =\n60. Since a = 5b, it follows that 2a = 25b = 10b. Therefore,\n10b + 10b = 60, which gives b = 3 centimeters and a = 5b = 15\ncentimeters.\nThus, the perimeter of the square is 4a = 415 = 60 centimeters.\nMathematics\n\u540c\u5b66\u4eec\u8fdb\u884c\u5e7f\u64ad\u64cd\u6bd4\u8d5b\uff0c\u5168\u73ed\u6b63\u597d\u6392\u6210\u76f8\u7b49\u76846\u884c\u3002\u5c0f\u7ea2\u6392\u5728\u7b2c\u4e8c\u884c\uff0c\u4ece\u5934\u6570\uff0c\u5979\u7ad9\u5728\u7b2c5\u4e2a\u4f4d\n\u7f6e\uff0c\u4ece\u540e\u6570\u5979\u7ad9\u5728\u7b2c3\u4e2a\u4f4d\u7f6e\uff0c\u8fd9\u4e2a\u73ed\u5171\u6709\u591a\u5c11\u4eba\nThe students are having a radio exercise competition, and the whole class is neatly arranged into 6 equal\nrows. Xiaohong is in the second row, counting from the front, she is in the 5th position. Counting from the\nback, she is in the 3rd position. How many people are there in this class?\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u5c0f\u7ea2\u524d\u9762\u7684\u4eba\u6570= 5 - 1 = 4\uff0c\u5c0f\u7ea2\u540e\u9762\u7684\u4eba\u6570= 3 - 1 = 2\uff0c\u5168\u73ed\n\u4eba\u6570= \u5c0f\u7ea2\u524d\u9762\u7684\u4eba\u6570+ \u5c0f\u7ea2\u540e\u9762\u7684\u4eba\u6570+ \u5c0f\u7ea2\u81ea\u5df1= 4 + 2 +\n1 = 7\n976\nThe number of people in front of Xiaohong = 5 - 1 = 4, the number\nof people behind Xiaohong = 3 - 1 = 2, the total number of people\nin the class = the number of people in front of Xiaohong + the\nnumber of people behind Xiaohong + Xiaohong herself = 4 + 2 + 1\n= 7.\nQwen-14B-Chat (SFT)\n5+3-1=7\uff08\u4eba\uff09\n7\u00d76=42\uff08\u4eba\uff09\n\u8fd9\u4e2a\u73ed\u5171\u670942\u4eba\u3002\n976\n5+3-1=7 (people)\n7\u00d76=42 (people)\nThere are a total of 42 people in this class.\nQwen-14B-Chat (RLHF) \u5c0f \u7ea2 \u524d \u9762 \u67094\u4e2a \u4eba \uff0c \u540e \u9762 \u67092\u4e2a \u4eba \uff0c \u6240 \u4ee5 \u8fd9 \u4e00 \u884c \u5171\n\u67094+2+1=7\u4e2a\u4eba\u3002\n\u5168\u73ed\u5171\u67096\u884c\uff0c\u6240\u4ee5\u5168\u73ed\u5171\u67096*7=42\u4e2a\u4eba\u3002\n1139\nThere are 4 people in front of Xiaohong and 2 people behind her,\nso there are a total of 4+2+1=7 people in this row.\nThere are a total of 6 rows in the class, so there are a total of 6*7=42\npeople in the whole class.\nGPT-3.5\n\u6839\u636e\u9898\u76ee\u4fe1\u606f\uff0c\u5c0f\u7ea2\u6392\u5728\u7b2c\u4e8c\u884c\u7b2c5\u4e2a\u4f4d\u7f6e\uff0c\u4e5f\u5373\u7b2c11\u4e2a\u4f4d\n\u7f6e\u3002\u800c\u5c0f\u7ea2\u4ece\u540e\u6570\u662f\u7b2c3\u4e2a\u4f4d\u7f6e\uff0c\u4e5f\u5373\u5012\u6570\u7b2c11\u4e2a\u4f4d\u7f6e\u3002\u6240\n\u4ee5\u8fd9\u4e2a\u73ed\u5171\u670911\u4e2a\u4eba\u3002\n897\nAccording to the information given, Xiaohong is in the second row,\nfifth position, which is also the 11th position. And Xiaohong is\nin the third position from the end, which is also the 11th position\ncounting from the back. So there are a total of 11 people in this\nclass.\nGPT-4\n\u5982\u679c\u5c0f\u7ea2\u4ece\u5934\u6570\u662f\u7b2c5\u4e2a\uff0c\u4ece\u540e\u6570\u662f\u7b2c3\u4e2a\uff0c\u90a3\u4e48\u8fd9\u4e00\u6392\n\u5c31\u67095+3-1=7\u4eba\u3002\u5168\u73ed\u6b63\u597d\u6392\u62106\u884c\uff0c\u6240\u4ee5\u73ed\u7ea7\u91cc\u6709\u4eba\u6570\n\u4e3a7*6=<<7*6=42>>=42\u4eba\u3002\n1010\n49\nIf Xiaohong is the 5th person when counting from the front and the\n3rd person when counting from the back, then there are a total of\n5+3-1=7 people in this row. The whole class is perfectly arranged\nin 6 rows, so there are a total of 7*6=42 people in the class.\nCode\npython\u5b9e\u73b0\u4e00\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\uff0c\u8981\u6c42\u80fd\u5339\u914d\u5230\u6240\u6709\u4ee5139\u5f00\u5934\uff0c\u4ee500\u7ed3\u5c3e\u768411\u4f4d\u6570\u5b57\nImplement a regular expression matching in Python, which should be able to match all 11-digit numbers\nthat start with \u201d139\u201d and end with \u201d00\u201d.\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u4f60\u53ef\u4ee5\u4f7f\u7528Python\u7684re\u6a21\u5757\u6765\u5b9e\u73b0\u8fd9\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u3002\u4ee5\n\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\u4ee3\u7801\uff1a\nimport re\ndef match phone numbers(phone numbers):\npattern = r\u2019\u02c6139d9$\u2019\nreturn re . findall ( pattern , phone numbers)\nphone numbers = [\u201913900000000\u2019, \u201913912345678\u2019, \u2019\n13998765432\u2019, \u201913987654321\u2019]\nprint(match phone numbers(phone numbers))\n\u5728\u8fd9\u4e2a\u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u6a21\u5f0f\u2019\u02c6139\nd9$\u2019\uff0c\u8fd9\u4e2a\u6a21\u5f0f\u8868\u793a\u4ee5139\u5f00\u5934\uff0c\u4ee500\u7ed3\u5c3e\uff0c\u4e14\u957f\u5ea6\u4e3a11\u7684\n\u6570\u5b57\u3002\u7136\u540e\u6211\u4eec\u4f7f\u7528re . findall \u51fd\u6570\u6765\u67e5\u627e\u6240\u6709\u5339\u914d\u8fd9\u4e2a\u6a21\n\u5f0f\u7684\u5b57\u7b26\u4e32\uff0c\u6700\u540e\u8fd4\u56de\u8fd9\u4e9b\u5339\u914d\u7684\u5b57\u7b26\u4e32\u3002\n\u6ce8\u610f\uff0c\u8fd9\u4e2a\u4ee3\u7801\u5047\u8bbephone numbers\u662f\u4e00\u4e2a\u5305\u542b\u591a\u4e2a\u5b57\u7b26\u4e32\u7684\n\u5217\u8868\u3002\u5982\u679c\u4f60\u7684phone numbers\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u4f60\u9700\u8981\u5148\u5c06\u5176\n\u8f6c\u6362\u4e3a\u5217\u8868\u3002\n941\nYou can use the re module in Python to implement this regular\nexpression matching. Here is an example code:\nimport re\ndef match phone numbers(phone numbers):\npattern = r\u2019\u02c6139d9$\u2019\nreturn re . findall ( pattern , phone numbers)\nphone numbers = [\u201913900000000\u2019, \u201913912345678\u2019, \u2019\n13998765432\u2019, \u201913987654321\u2019]\nprint(match phone numbers(phone numbers))\nThis code defines a regular expression pattern \u02c6139d9$. This pat-\ntern represents a string that starts with \u2019139\u2019, ends with \u201900\u2019, and\nhas a length of 11 digits. Then, we use the re.findall function to\nfind all the strings that match this pattern, and finally, we return\nthese matching strings.\nNote that this code assumes that phone numbers is a list containing\nmultiple strings. If your phone numbers is a string, you must first\nconvert it to a list.\n50\nQwen-14B-Chat (SFT)\n\u4f60\u53ef\u4ee5\u4f7f\u7528Python\u7684re\u6a21\u5757\u6765\u5b9e\u73b0\u8fd9\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u3002\u4ee5\n\u4e0b\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff1a\nimport re\ndef match phone numbers(phone numbers):\npattern = r\u2019\u02c6139d9$\u2019\nreturn [phone number for phone number in\nphone numbers if re .match(pattern , phone number)]\nphone numbers = [\u201913912345678\u2019, \u201913923456789\u2019, \u2019\n13934567890\u2019, \u201913945678901\u2019, \u201913956789012\u2019, \u201913967890123\n\u2019, \u201913978901234\u2019, \u201913989012345\u2019, \u201913990123456\u2019, \u2019\n13900000000\u2019]\nprint(match phone numbers(phone numbers))\n\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u6a21\u5f0f\u2019\u02c6139d9$\u2019\uff0c\n\u5b83\u5339\u914d\u4ee5139\u5f00\u5934\uff0c\u540e\u9762\u8ddf\u77409\u4e2a\u4efb\u610f\u6570\u5b57\uff0c\u6700\u540e\u4ee50\u7ed3\u5c3e\u7684\n\u5b57\u7b26\u4e32\u3002\u7136\u540e\u6211\u4eec\u4f7f\u7528re .match()\u51fd\u6570\u6765\u68c0\u67e5\u6bcf\u4e2a\u7535\u8bdd\u53f7\u7801\n\u662f\u5426\u5339\u914d\u8fd9\u4e2a\u6a21\u5f0f\u3002\u5982\u679c\u5339\u914d\uff0c\u6211\u4eec\u5c31\u5c06\u8fd9\u4e2a\u7535\u8bdd\u53f7\u7801\u6dfb\u52a0\n\u5230\u7ed3\u679c\u5217\u8868\u4e2d\u3002\n941\nYou can use the re module of Python to implement this regular\nexpression matching. Here is a simple example:\nimport re\ndef match phone numbers(phone numbers):\npattern = r\u2019\u02c6139d9$\u2019\nreturn [phone number for phone number in\nphone numbers if re .match(pattern , phone number)]\nphone numbers = [\u201913912345678\u2019, \u201913923456789\u2019, \u2019\n13934567890\u2019, \u201913945678901\u2019, \u201913956789012\u2019, \u201913967890123\n\u2019, \u201913978901234\u2019, \u201913989012345\u2019, \u201913990123456\u2019, \u2019\n13900000000\u2019]\nprint(match phone numbers(phone numbers))\nIn this example, we define a regular expression pattern \u2019\u02c6139d9$\u2019,\nwhich matches a string starting with 139, followed by nine arbitrary\ndigits and ending with 0. Then we use the re .match() function to\ncheck if each phone number matches this pattern. If it does, we\nadd this phone number to the result list.\n51\nQwen-14B-Chat (RLHF) \u53ef\u4ee5\u4f7f\u7528Python\u7684re\u6a21\u5757\u6765\u5b9e\u73b0\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u3002\u4ee5\u4e0b\u662f\u4e00\n\u4e2a\u793a\u4f8b\u4ee3\u7801\uff1a\nimport re\npattern = r\u2019\u02c6139d9$\u2019\n# \u6d4b\u8bd5\u5339\u914d\ntext = \u201913912345678\u2019\nmatch = re .match(pattern , text )\nif match:\nprint(\u2019\u5339\u914d\u6210\u529f\u2019)\nelse :\nprint(\u2019\u5339\u914d\u5931\u8d25\u2019)\n# \u6d4b\u8bd5\u4e0d\u5339\u914d\ntext = \u2019139123456780\u2019\nmatch = re .match(pattern , text )\nif match:\nprint(\u2019\u5339\u914d\u6210\u529f\u2019)\nelse :\nprint(\u2019\u5339\u914d\u5931\u8d25\u2019)\n\u5176\u4e2d\uff0c\u02c6\u8868\u793a\u5339\u914d\u5b57\u7b26\u4e32\u7684\u5f00\u5934\uff0cd\u8868\u793a\u5339\u914d\u6570\u5b57\uff0c9\u8868\u793a\u5339\n\u914d9\u4e2a\u6570\u5b57\uff0c$\u8868\u793a\u5339\u914d\u5b57\u7b26\u4e32\u7684\u7ed3\u5c3e\u3002\u5c06\u8fd9\u4e9b\u7b26\u53f7\u7ec4\u5408\u5728\u4e00\n\u8d77\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u5339\u914d11\u4f4d\u6570\u5b57\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u3002\n\u5728\u6d4b\u8bd5\u5339\u914d\u65f6\uff0c\u4f7f\u7528re .match()\u51fd\u6570\u6765\u5c1d\u8bd5\u5339\u914d\u6587\u672c\uff0c\u5982\u679c\n\u5339\u914d\u6210\u529f\uff0c\u8fd4\u56de\u4e00\u4e2aMatch\u5bf9\u8c61\uff0c\u5426\u5219\u8fd4\u56deNone\u3002\u5728\u672c\u4f8b\n\u4e2d\uff0c\u7b2c\u4e00\u4e2a\u6d4b\u8bd5\u5339\u914d\u6210\u529f\uff0c\u7b2c\u4e8c\u4e2a\u6d4b\u8bd5\u5339\u914d\u5931\u8d25\u3002\n1090\nYou can use the re module in Python to implement regular expres-\nsion matching. Here is an example code:\nimport re\npattern = r\u2019\u02c6139d9$\u2019\n# Match Test\ntext = \u201913912345678\u2019\nmatch = re .match(pattern , text )\nif match:\nprint(\u2019Success\u2019)\nelse :\nprint(\u2019Fail\u2019)\n# Unmatch Test\ntext = \u2019139123456780\u2019\nmatch = re .match(pattern , text )\nif match:\nprint(\u2019Success\u2019)\nelse :\nprint(\u2019Fail\u2019)\nAmong them, \u02c6 represents matching the beginning of a string, d\nrepresents matching a digit, 9 represents matching 9 digits, and $\nrepresents matching the end of a string. Combining these symbols,\nwe can obtain a regular expression that matches 11 digits.\nWhen testing for a match, use the re .match() function to attempt\nto match the text. A successful match returns a Match object;\notherwise, it returns None. In this example, the first test matches\nsuccessfully, while the second test fails to match.\n52\nGPT-3.5\n\u4f7f\u7528Python\u7684re\u6a21\u5757\u53ef\u4ee5\u5f88\u65b9\u4fbf\u5730\u5b9e\u73b0\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u3002\u4ee5\n\u4e0b\u662f\u4e00\u4e2a\u5339\u914d\u6240\u6709\u4ee5139\u5f00\u5934\uff0c\u4ee500\u7ed3\u5c3e\u768411\u4f4d\u6570\u5b57\u7684\u793a\u4f8b\n\u4ee3\u7801\uff1a\nimport re\ntext = \u201d139123456700\u201d\npattern = r\u201d\u02c6139d800$\u201d\nmatch = re .match(pattern , text )\nif match:\nprint(\u201d\u5339\u914d\u6210\u529f\u201d)\nelse :\nprint(\u201d\u5339\u914d\u5931\u8d25\u201d)\n\u8f93\u51fa\u7ed3\u679c\u4e3a\uff1a\n\u5339\u914d\u6210\u529f\n\u4ee3\u7801\u4e2d\uff0c\u9996\u5148\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6587\u672c\u5b57\u7b26\u4e32 text \uff0c\u7528\u4e8e\u6d4b\u8bd5\u6b63\u5219\u8868\n\u8fbe\u5f0f\u7684\u5339\u914d\u3002\u63a5\u7740\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f pattern \uff0c\u5176\u4e2d\u02c6139\n\u8868\u793a\u4ee5139\u5f00\u5934\uff0cd8\u8868\u793a\u4efb\u610f8\u4f4d\u6570\u5b57\uff0c00$\u8868\u793a\u4ee500\u7ed3\u5c3e\u3002\n\u7136\u540e\u4f7f\u7528re .match()\u51fd\u6570\u8fdb\u884c\u5339\u914d\uff0c\u5982\u679c\u5339\u914d\u6210\u529f\uff0c\u5219\u8f93\n\u51fa\u201d\u5339\u914d\u6210\u529f\u201d\uff0c\u5426\u5219\u8f93\u51fa\u201d\u5339\u914d\u5931\u8d25\u201d\u3002\n1043\nThe re module in Python can conveniently accomplish regular\nexpression matching. Below is an example code that matches all\n11-digit numbers beginning with 139 and ending with 00.\nimport re\ntext = \u201d139123456700\u201d\npattern = r\u201d\u02c6139d800$\u201d\nmatch = re .match(pattern , text )\nif match:\nprint(\u201dSuccess\u201d)\nelse :\nprint(\u201dFail\u201d)\nOutput:\nSuccess\nIn the code, a text string \u201dtext\u201d is first defined for testing regular\nexpression matching. Then, a regular expression \u201dpattern\u201d is de-\nfined, where \u201d\u02c6139\u201d means starting with 139, \u201dd8\u201d means any eight\ndigits, and \u201d00$\u201d means ending with 00. Then, the re .match()\nfunction is used for matching. If the match is successful, \u201dSuccess\u201d\nis outputted; otherwise, \u201dFail\u201d is outputted.\n53\nGPT-4\n\u4e0b\u9762\u662f\u4f60\u9700\u8981\u7684python\u4ee3\u7801\uff0c\u8fd9\u6bb5\u4ee3\u7801\u4e2d\u4f7f\u7528\u4e86python\u7684re\u6a21\n\u5757\u3002\nimport re\ndef match phone number(string):\npattern = \u201d\u02c6139d600$\u201d\nreturn re .match(pattern , string ) is not None\n# \u6d4b\u8bd5\nprint(match phone number(\u201d13912345600\u201d)) # \u8f93\u51fa: True\nprint(match phone number(\u201d15912345600\u201d)) # \u8f93\u51fa: False\nprint(match phone number(\u201d13912345601\u201d)) # \u8f93\u51fa: False\nprint(match phone number(\u201d139123456000\u201d)) # \u8f93\u51fa: False\n\u5728\u8fd9\u91cc\uff0c\u201d\u02c6139d600$\u201d\u662f\u6b63\u5219\u8868\u8fbe\u5f0f\u3002\u201d\u02c6\u201d\u8868\u793a\u5339\u914d\u5b57\u7b26\u4e32\u7684\n\u5f00\u59cb\uff0c\u201d139\u201d\u5339\u914d\u5b57\u7b26\u4e32\u4e2d\u7684\u201d139\u201d\uff0c\u201dd6\u201d\u5339\u914d\u4efb\u610f\u516d\u4e2a\u6570\n\u5b57\uff0c\u201d00\u201d\u5339\u914d\u5b57\u7b26\u4e32\u4e2d\u7684\u201d00\u201d\uff0c\u201c$\u201d\u8868\u793a\u5339\u914d\u5b57\u7b26\u4e32\u7684\u7ed3\u675f\u3002\n\u56e0\u6b64\uff0c\u8fd9\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u53ef\u4ee5\u5339\u914d\u5230\u6240\u6709\u4ee5139\u5f00\u5934\uff0c\u4ee500\u7ed3\n\u5c3e\u768411\u4f4d\u6570\u5b57\u3002\n1134\nBelow is the python code you need. This code uses the python re\nmodule.\nimport re\ndef match phone number(string):\npattern = \u201d\u02c6139d600$\u201d\nreturn re .match(pattern , string ) is not None\n# Test\nprint(match phone number(\u201d13912345600\u201d)) # Output: True\nprint(match phone number(\u201d15912345600\u201d)) # Output: False\nprint(match phone number(\u201d13912345601\u201d)) # Output: False\nprint(match phone number(\u201d139123456000\u201d)) # Output: False\nIn here, \u201d\u02c6139d600$\u201d is a regular expression. \u201d\u02c6\u201d represents the\nbeginning of the string, \u201d139\u201d matches the \u201d139\u201d in the string, \u201dd6\u201d\nmatches any six digits, \u201d00\u201d matches the \u201d00\u201d in the string, and \u201d$\u201d\nrepresents the end of the string. Therefore, this regular expression\ncan match any 11-digit number that starts with 139 and ends with\n00.\nCode\n\u6211\u7528\u4e0b\u9762\u4ee3\u7801\u6c42\u4e8c\u53c9\u6811\u7684\u6700\u5927\u6df1\u5ea6\uff0c\u4f46\u662f\u7b54\u6848\u4e0d\u5bf9\uff0c\u600e\u4e48\u56de\u4e8b\u5462\uff1f\ndef maxDepth ( s e l f ,\nr o o t :\nTreeNode ) \u2212> i n t :\ni f\nnot\nr o o t :\nreturn 0\nqueue ,\nr e s = [ r o o t ] ,\n0\nwhile\nqueue :\ntmp = [ ]\nfor\nnode\nin\nqueue :\ni f\nnode . l e f t :\ntmp . append ( node . l e f t )\ni f\nnode . r i g h t :\ntmp . append ( node . l e f t )\nqueue = tmp\nr e s += 1\nreturn\nr e s\nI use the following code to find the maximum depth of a binary tree, but the answer is incorrect. What\ncould be the problem?\n54\ndef maxDepth ( s e l f ,\nr o o t :\nTreeNode ) \u2212> i n t :\ni f\nnot\nr o o t :\nreturn 0\nqueue ,\nr e s = [ r o o t ] ,\n0\nwhile\nqueue :\ntmp = [ ]\nfor\nnode\nin\nqueue :\ni f\nnode . l e f t :\ntmp . append ( node . l e f t )\ni f\nnode . r i g h t :\ntmp . append ( node . l e f t )\nqueue = tmp\nr e s += 1\nreturn\nr e s\nResponses\nModel\nResponse\nElo\nQwen-7B-Chat (SFT)\n\u4f60\u7684\u4ee3\u7801\u662f\u6b63\u786e\u7684\uff0c\u7b54\u6848\u662f\u6b63\u786e\u7684\u3002\u5728\u4f60\u7684\u4ee3\u7801\u4e2d\uff0c\u4f60\u9996\n\u5148\u5c06\u6839\u8282\u70b9\u6dfb\u52a0\u5230\u961f\u5217\u4e2d\uff0c\u7136\u540e\u8fdb\u5165\u4e00\u4e2a\u5faa\u73af\uff0c\u76f4\u5230\u961f\u5217\u4e3a\n\u7a7a\u3002\u5728\u6bcf\u6b21\u5faa\u73af\u4e2d\uff0c\u4f60\u90fd\u4f1a\u5c06\u961f\u5217\u4e2d\u7684\u6240\u6709\u8282\u70b9\u7684\u5de6\u5b50\u8282\u70b9\n\u548c\u53f3\u5b50\u8282\u70b9\u6dfb\u52a0\u5230\u961f\u5217\u4e2d\u3002\u7136\u540e\uff0c\u4f60\u589e\u52a0\u6df1\u5ea6\u8ba1\u6570\u5668res\u3002\n\u6700\u540e\uff0c\u4f60\u8fd4\u56de\u6df1\u5ea6\u8ba1\u6570\u5668res\u3002\n876\nYour code is correct, the answer is correct. In your code, you first\nadd the root node to the queue, then enter a loop until the queue\nis empty. In each loop, you add the left child node and right child\nnode of all nodes in the queue to the queue. Then, you increase the\ndepth counter res. Finally, you return the depth counter res.\nQwen-14B-Chat (SFT)\n\u4f60\u7684\u4ee3\u7801\u4f1a\u8fd4\u56de\u4e8c\u53c9\u6811\u7684\u8282\u70b9\u6570\u91cf\uff0c\u800c\u4e0d\u662f\u6700\u5927\u6df1\u5ea6\u3002\u8fd9\u662f\n\u56e0\u4e3a\u4f60\u5728\u6bcf\u6b21\u5faa\u73af\u4e2d\u90fd\u589e\u52a0\u4e86res\u7684\u503c\uff0c\u800c\u4e0d\u662f\u53ea\u5728\u627e\u5230\u65b0\n\u7684\u6700\u6df1\u8282\u70b9\u65f6\u589e\u52a0\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u5728\u627e\u5230\u65b0\u7684\u6700\u6df1\u8282\u70b9\u65f6\u624d\u589e\n\u52a0res\u7684\u503c\u6765\u4fee\u590d\u8fd9\u4e2a\u95ee\u9898\u3002\u4ee5\u4e0b\u662f\u4fee\u590d\u540e\u7684\u4ee3\u7801\uff1a\ndef maxDepth(self, root : TreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nif len(queue) > res:\nres = len(queue)\nreturn res\n949\nYour code will return the number of nodes in the binary tree, not\nthe maximum depth. This is because you increase the value of \u201dres\u201d\nin every loop, instead of only increasing it when a new deepest\nnode is found. You can fix this problem by increasing the value of\n\u201dres\u201d only when a new deepest node is found. Here is the modified\ncode:\ndef maxDepth(self, root : TreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nif len(queue) > res:\nres = len(queue)\nreturn res\n55\nQwen-14B-Chat (RLHF) \u8fd9\u4e2a\u4ee3\u7801\u7684\u95ee\u9898\u5728\u4e8e\u5b83\u6ca1\u6709\u6b63\u786e\u5730\u5904\u7406\u5de6\u53f3\u5b50\u6811\u7684\u6df1\u5ea6\u3002\u5177\n\u4f53\u6765\u8bf4\uff0c\u6bcf\u6b21\u4ece\u961f\u5217\u4e2d\u53d6\u51fa\u4e00\u4e2a\u8282\u70b9\u65f6\uff0c\u5b83\u4f1a\u5c06\u8be5\u8282\u70b9\u7684\u5de6\n\u53f3\u5b50\u6811\u90fd\u52a0\u5165\u5230\u961f\u5217\u4e2d\uff0c\u7136\u540e\u5c06\u961f\u5217\u957f\u5ea6\u52a0\u4e00\u3002\u4f46\u662f\uff0c\u5982\u679c\n\u8be5\u8282\u70b9\u7684\u5de6\u53f3\u5b50\u6811\u4e2d\u6709\u4e00\u4e2a\u4e3a\u7a7a\uff0c\u90a3\u4e48\u8fd9\u4e2a\u8282\u70b9\u7684\u6df1\u5ea6\u5c31\u5e94\n\u8be5\u662f\u5b83\u6240\u5728\u961f\u5217\u7684\u957f\u5ea6\u51cf\u4e00\uff0c\u800c\u4e0d\u662f\u961f\u5217\u957f\u5ea6\u52a0\u4e00\u3002\n\u4ee5\u4e0b\u662f\u4fee\u6539\u540e\u7684\u4ee3\u7801\uff1a\ndef maxDepth(self, root : TreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nres = max(res, len(queue))\nreturn res\n\u5728\u8fd9\u4e2a\u4ee3\u7801\u4e2d\uff0c\u6bcf\u6b21\u53d6\u51fa\u4e00\u4e2a\u8282\u70b9\u65f6\uff0c\u6211\u4eec\u53ea\u5c06\u8be5\u8282\u70b9\u7684\u5de6\n\u53f3\u5b50\u6811\u4e2d\u975e\u7a7a\u7684\u90e8\u5206\u52a0\u5165\u5230\u961f\u5217\u4e2d\uff0c\u5e76\u8ba1\u7b97\u961f\u5217\u7684\u957f\u5ea6\u3002\u7136\n\u540e\uff0c\u6211\u4eec\u5c06\u961f\u5217\u957f\u5ea6\u4e0e\u5f53\u524d\u6700\u5927\u6df1\u5ea6\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u66f4\u65b0\u6700\u5927\n\u6df1\u5ea6\u3002\u8fd9\u6837\u5c31\u53ef\u4ee5\u6b63\u786e\u5730\u8ba1\u7b97\u51fa\u4e8c\u53c9\u6811\u7684\u6700\u5927\u6df1\u5ea6\u4e86\u3002\n992\nThe problem with this code is that it does not correctly handle the\ndepth of the left and right subtrees. Specifically, when a node is\ntaken out from the queue, it adds both its left and right subtrees to\nthe queue and increases the length of the queue by one. However,\nif either the left or right subtree of the node is empty, then the depth\nof this node should be the length of the queue minus one, not the\nlength of the queue plus one.\nHere is the modified code:\ndef maxDepth(self, root : TreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nres = max(res, len(queue))\nreturn res\nIn this code, when each node is extracted, we only add the non-\nempty parts of its left and right subtrees to the queue, and calculate\nthe length of the queue. Then, we compare the length of the queue\nwith the current maximum depth and update the maximum depth.\nThis way, we can correctly calculate the maximum depth of the\nbinary tree.\n56\nGPT-3.5\n\u95ee\u9898\u51fa\u5728\u7b2c11\u884c\u4ee3\u7801\uff0c\u5728\u5c06node. right \u6dfb\u52a0\u5230ttmp\u5217\u8868\u65f6\uff0c\n\u5e94 \u8be5 \u662ftmp.append(node. right )\u800c \u4e0d \u662ftmp.append(node. left )\n\u3002\u4fee\u6539\u540e\u7684\u4ee3\u7801\u5982\u4e0b\uff1a\ndef maxDepth(self, root : TreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nres += 1\nreturn res\n1090\nThe problem lies in line 11 of the code. When adding node. right\nto the tmp list, it should be tmp.append(node. right ) instead of tmp\n.append(node. left ). The modified code is as follows:\ndef maxDepth(self, root : TreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nres += 1\nreturn res\nGPT-4\n\u4f60\u7684\u4ee3\u7801\u4e2d\u6709\u4e00\u4e2a\u9519\u8bef\uff1a\n\u4f60\u5728\u8fd9\u4e00\u884c\u4ee3\u7801if node. right : tmp.append(node. left ) \u4e2d\u5e94\n\u8be5\u662f\u60f3\u628a\u53f3\u5b50\u8282\u70b9\u52a0\u5165\u5230tmp\u5217\u8868\u4e2d\uff0c\u4f46\u662f\u4f60\u5199\u7684\u662fnode.\nleft \uff0c\u8fd9\u6837\u5c31\u53ea\u4f1a\u628a\u5de6\u5b50\u8282\u70b9\u52a0\u5165\u5230tmp\u5217\u8868\u4e2d\u3002\u6b63\u786e\u7684\u5e94\n\u8be5\u662fnode. right \u3002\n\u6b63\u786e\u7684\u4ee3\u7801\u5982\u4e0b\uff1a\ndef maxDepth(self, root : TreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nres += 1\nreturn res\n\u8fd9\u79cd\u89e3\u6cd5\u7684\u601d\u8def\u662f\uff1a\u6bcf\u6b21\u8fed\u4ee3\u65f6\uff0c\u90fd\u628a\u5f53\u524d\u5c42\u7684\u6240\u6709\u8282\u70b9\n\u904d\u5386\u4e00\u904d\uff0c\u540c\u65f6\u628a\u4e0b\u4e00\u5c42\u7684\u6240\u6709\u8282\u70b9\u90fd\u52a0\u5230\u4e0b\u4e00\u6b21\u8fed\u4ee3\u7684\u961f\n\u5217\u4e2d\u3002\u6240\u4ee5\u6bcf\u6b21\u8fed\u4ee3\u5b8c\uff0c\u6df1\u5ea6res\u5c31\u52a01\u3002\u76f4\u5230\u904d\u5386\u7684\u961f\u5217\u4e3a\n\u7a7a\uff0c\u6b64\u65f6\u7684\u6df1\u5ea6\u5c31\u662f\u4e8c\u53c9\u6811\u7684\u6700\u5927\u6df1\u5ea6\u3002\n1095\n57\nThere is an error in your code:\nIn this line of code if node. right : tmp.append(node. left ), you\nshould be adding the right child node to the tmp list, but you wrote\nnode. left instead, so only the left child node will be added to the\ntmp list. The correct code should be node. right .\nThe correct code is as follows:\ndef maxDepth(self, root :\nTreeNode) \u2212> int:\nif not root : return 0\nqueue, res = [ root ], 0\nwhile queue:\ntmp = []\nfor node in queue:\nif node. left : tmp.append(node. left )\nif node. right : tmp.append(node. right )\nqueue = tmp\nres += 1\nreturn res\nThe idea of this solution is: in each iteration, traverse all the nodes\nin the current level, and add all the nodes in the next level to the\nqueue for the next iteration. So after each iteration, the depth res is\nincreased by 1. Continue this process until the queue for traversal\nis empty, and the depth at that time is the maximum depth of the\nbinary tree.\nA.3\nANALYSIS OF CODE INTERPRETER\nHere we provide a case of comparison between CODE LLAMA and QWEN-CHAT. This case\ndemonstrates the advantages of QWEN-CHAT in processing tabular data and performing complex\ntasks.\n58\nCode\nLlama\nUser\nCode\nLlama\nUser\nQwen\nChat\nUser\nFigure 5: Example showcasing QWEN-CHAT\u2019s ability in using a code interpreter via ReAct prompting.\nThe ReAct instruction is omitted for clarity. QWEN creates a two-step plan and first investigates\nthe columns present in the CSV file before proceeding to draw the plot, as shown in the top-left\nfigure. CODE LLAMA, however, attempts to draw the plot based on non-existent columns in its\ninitial attempt, as seen in the bottom figure. CODE LLAMA can only reliably perform the task if the\ncolumns are provided in the user query, as shown in the top-right figure.\n59\n"
  },
  {
    "title": "Effective Long-Context Scaling of Foundation Models",
    "link": "https://arxiv.org/pdf/2309.16039.pdf",
    "upvote": "28",
    "text": "Effective Long-Context Scaling of Foundation Models\nWenhan Xiong\u2020\u2217, Jingyu Liu\u2020, Igor Molybog,\nHejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta,\nKarthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang,\nYashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan,\nShruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang\u2217, Hao Ma\u2217\nGenAI, Meta\nAbstract\nWe present a series of long-context LLMs that support effective context windows\nof up to 32,768 tokens. Our model series are built through continual pretraining\nfrom LLAMA 2 with longer training sequences and on a dataset where long texts\nare upsampled. We perform extensive evaluation on language modeling, synthetic\ncontext probing tasks, and a wide range of research benchmarks. On research\nbenchmarks, our models achieve consistent improvements on most regular tasks\nand significant improvements on long-context tasks over LLAMA 2. Notably, with\na cost-effective instruction tuning procedure that does not require human-annotated\nlong instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k\u2019s\noverall performance on a suite of long-context tasks. Alongside these results, we\nprovide an in-depth analysis on the individual components of our method. We\ndelve into LLAMA\u2019s position encodings and discuss its limitation in modeling\nlong dependencies. We also examine the impact of various design choices in\nthe pretraining process, including the data mix and the training curriculum of\nsequence lengths \u2013 our ablation experiments suggest that having abundant long\ntexts in the pretrain dataset is not the key to achieving strong performance, and\nwe empirically verify that long context continual pretraining is more efficient and\nsimilarly effective compared to pretraining from scratch with long sequences.\n100\n101\n102\n103\n104\nContext length\n2\n3\n4\n5\n6\nValidation loss\nLlama 2 Long 7B\n = 25.4,  = 0.45,  = 1.56\nLlama 2 Long  13B\n = 19.5,  = 0.48,  = 1.45\nLlama 2 Long  34B\n = 17.7,  = 0.50,  = 1.41\nLlama 2 Long  70B\n = 17.9,  = 0.51,  = 1.35\nFigure 1: We show that our model\u2019s validation loss can be fit as a function of the context length:\nL(c) = ( \u03b1\nc )\u03b2 + \u03b3 with a different set of \u03b1, \u03b2, \u03b3 for each model size. This power-law relationship also\nsuggests that context length is another important axis of scaling LLMs and our model can continually\nimprove its performance as we increase the context length up to 32,768 tokens.\narXiv:2309.16039v3  [cs.CL]  14 Nov 2023\n1\nIntroduction\nLarge language models (LLMs), trained with an unprecedented magnitude of data and compute, hold\nthe promise of fundamentally improving the way we interact with the digital world. As LLMs get\nrapidly deployed and continue to evolve through scaling, we envision these models to serve more\nintricate and complex use cases, such as analyzing dense knowledge-rich documents, powering more\ngenuine and engaging chatbot experiences, and aiding human users in iterative creation processes\nsuch as coding and design, etc. A crucial feature supporting this evolution is the ability to effectively\nprocess long-context inputs.\nUntil now, LLMs with robust long-context capabilities are primarily provided through proprietary\nLLM APIs (Anthropic, 2023; OpenAI, 2023) and there is no open recipe for building long-context\nmodel that can demonstrate on-par downstream performance as these proprietary models. Moreover,\nexisting open-sourced long-context models (Tworkowski et al., 2023b; Chen et al., 2023; Mohtashami\nand Jaggi, 2023; MosaicML, 2023b) often fall short on evaluations and primarily measure long-context\ncapabilities with the language modeling loss and synthetic tasks, which do not comprehensively\ndemonstrate their effectiveness in diverse, real-world scenarios. Additionally, these models often\noverlook the necessity of maintaining strong performance on standard short-context tasks, either\nbypassing the evaluations or reporting degenerated performance (Peng et al., 2023; Chen et al., 2023).\nIn this work, we describe our approach to build long-context LLMs with superior performance over\nall existing open-sourced models. We build our models by continually pretraining from LLAMA 2\ncheckpoints with additional 400 billion tokens formed as long training sequences. Among the model\nseries, the smaller 7B/13B variants are trained with 32,768-token sequences while the 34B/70B\nvariants with 16,384-token sequences. In contrast to the limited evaluation performed by existing\nstudies, we extensively evaluate our models using language modeling, synthetic tasks, and also\na wide range of real-world benchmarks covering both long and short context tasks. On language\nmodeling, our model demonstrates a clear power-law scaling behavior with respect to context lengths.\nThis scaling behavior, as shown in Figure 1, not only shows our models\u2019 ability to consistently\nbenefit from more contexts but also suggest that context length is another importance axis of scaling\nLLMs. When comparing our models to LLAMA 2 on research benchmarks, we not only observe\nsignificant improvements on long-context tasks but also modest improvements on standard short-\ncontext tasks, especially on coding, math, and knowledge benchmarks. We explored using a simple\nand cost-effective procedure to instruction finetune our continually pretrained long models without any\nhuman-annotated data. The end result is a chat model that can achieve stronger overall performance\nthan gpt-3.5-turbo-16k on a series of long-context benchmarks covering question answering,\nsummarization, and multi-document aggregation tasks.\nIn the remaining part of this paper, we begin by presenting the continual long-context pretraining\napproach and a lightweight instruction tuning procedure, followed by detailed results on a range of\nshort and long context tasks. To facilitate future research, we complement our results with an analysis\nsection discussing how the design of positional encodings, the length distribution of the dataset and\nthe training curriculum contributes to the final performance. Finally, we report responsible safety\nevaluations, which validates that our models can largely maintain the safety performance of the\noriginal LLAMA 2 series.\n2\nMethod\n2.1\nContinual Pretraining\nTraining with longer sequence lengths can introduce significant computational overhead due to the\nquadratic attention calculations. This is the main motivation of our continual pretraining approach.\nThe underlying hypothesis that similar long-context capabilities can be learned by continually\npretraining from a short-context model is later validated in Section 4.4 through comparing different\ntraining curricula. We keep the original LLAMA 2 architecture nearly intact for continual pretraining\nand only make a necessary modification to the positional encoding that is crucial for the model to\n\u2020 Equal contribution\n\u2217 Corresponding authors:{xwhan, sinongwang, haom}@meta.com\n2\nattend longer. We also choose not to apply sparse attention (Child et al., 2019) in this work, since\ngiven LLAMA 2 70B\u2019s model dimension (h = 8192), the cost of attention matrix calculation and value\naggregation only becomes a computation bottleneck when the sequence length exceeds 49,152 (6h)\ntokens (Narayanan et al., 2021).1\nPositional Encoding\nThrough early experiments at the 7B scale, we identified a key limitation\nof LLAMA 2\u2019s positional encoding (PE) that prevents the attention module from aggregating infor-\nmation of distant tokens. We adopt a minimal yet necessary modification on the RoPE positional\nencoding (Su et al., 2022) for long-context modeling \u2013 decreasing the rotation angle (controlled\nby the hyperparameter \u201cbase frequency b\u201d), which reduces the decaying effect of RoPE for distant\ntokens. In Section 4.1, we show this simple method outperforms a concurrent approach (Chen et al.,\n2023) for extending LLAMA\u2019s context length and provide a theoretic explanation of its superiority.\nData Mix\nOn top of the working model with the modified PE, we further explored different\npretrain data mixes in Section 4.2 for improving long-context abilities, either by adjusting the ratio of\nLLAMA 2\u2019s pretraining data or adding new long text data. We found that often the quality of the data\nplays a more critical role than the length of texts for long-context continual pretraining.\nOptimization Details\nWe continually pretrain LLAMA 2 checkpoints with increased sequence\nlength while keeping the same number of tokens per batch as in LLAMA 2. We train all models\nfor a total of 400B tokens over 100,000 steps. With FLASHATTENTION (Dao et al., 2022), there is\nnegligible GPU memory overhead as we increase the sequence length and we observe around 17%\nspeed loss when increasing the sequence length from 4,096 to 16,384 for the 70B model. For the\n7B/13B models, we use learning rate 2e\u22125 and a cosine learning rate schedule with 2000 warm-up\nsteps. For the larger 34B/70B models, we find it important to set a smaller learning rate (1e\u22125) to get\nmonotonically decreasing validation losses.\n2.2\nInstruction Tuning\nCollecting human demonstration and preference labels for LLM alignment is a cumbersome and\nexpensive process (Ouyang et al., 2022; Touvron et al., 2023). The challenge and cost are more pro-\nnounced under long-context scenarios, which often involve complex information flow and specialized\nknowledge, e.g., processing dense legal/scientific documents, making the annotation task nontrivial\neven for skilled annotators. In fact, most existing open-source instruction datasets (Conover et al.,\n2023; K\u00f6pf et al., 2023) predominantly consist of short samples.\nIn this work, we found that a simple and cheap approach which leverages a pre-built large and diverse\nshort-prompt dataset works surprisingly well on long-context benchmarks. Specifically, we take the\nRLHF dataset used in LLAMA 2 CHAT and augment it with synthetic self-instruct (Wang et al., 2022)\nlong data generated by LLAMA 2 CHAT itself, in the hope that the model can learn a diverse set of\nskills through the large amount of RLHF data and transfer that knowledge to long-context scenarios\nvia self-instruct data. The data generation process focuses on QA-format tasks: starting from a\nlong document in our pretraining corpus, we select a random chunk and prompt LLAMA 2 CHAT\nto write question-answer pairs based on information in the text chunk. We collect both long and\nshort form answers with different prompts. After that, we also adopt a self-critique step where we\nprompt LLAMA 2 CHAT to verify the model-generated answers. Given a generated QA pair, we use\nthe original long document (truncated to fit the model\u2019s maximum context length) as the context to\nconstruct a training instance.\nFor short instruction data, we concatenate them as 16,384-token sequences. For long instruction\ndata, we add padding tokens on the right so that models can process each long instance individually\nwithout truncation. While standard instruction tuning only calculates loss on the output tokens, we\nfind it particularly beneficial to also calculate the language modeling loss on the long input prompts,\nwhich gives consistent improvements on downstream tasks (Section 4.3).\n1While sparse attention might be useful for reducing the key/value cache size at inference time when trading\noff performance, it can complicate the inference pipeline and the improvements can also be offset by quantization\nmethods.\n3\nModel\nSize\nCoding\nMath\nMMLU\nCommonsense\nOpenQA\nLLAMA 2\n7B\n16.8\n8.55\n45.3\n63.9\n48.9\n13B\n24.5\n16.3\n54.8\n66.9\n55.4\n34B\n27.8\n24.2\n62.6\n69.9\n58.7\n70B\n37.4\n35.2\n68.9\n71.9\n63.6\nLLAMA 2 LONG\n7B\n20.6\n10.5\n47.8\n64.9\n51.0\n13B\n25.7\n21.5\n60.1\n67.8\n56.8\n34B\n29.9\n29.0\n65.0\n70.9\n60.3\n70B\n39.9\n41.3\n71.7\n72.7\n64.0\nTable 1: Performance on standard short-context benchmarks. We report Coding score as the average\nof pass@1 of HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021); Math score as the\naverage of top-1 accuracy of 8-shot GSM8K (Cobbe et al., 2021) and 4-shot MATH (Hendrycks\net al., 2021); OpenQA score as the average of 5-shot performance on NaturalQuestions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017); Commonsense score as the average of PIQA (Bisk\net al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi\net al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and\nCommonsenseQA (Talmor et al., 2018).\nTask\nGPT-3.5\nGPT-4\nPaLM\nPaLM-2-L\nLLAMA 2\nLLAMA 2 LONG\nMMLU (5-shot)\n70.0\n86.4\n69.3\n78.3\n68.9\n71.7\nNatural Questions (1-shot)\n-\n-\n29.3\n37.5\n33.0\n35.7\nGSM8K (8-shot)\n57.1\n92.0\n56.5\n80.7\n56.8\n65.4\nHumanEval (0-shot)\n48.1\n67.0\n26.2\n-\n29.9\n32.9\nTable 2: Comparison with closed models on standard short tasks.\n3\nMain Results\n3.1\nPretrained Model Evaluation\nShort Tasks\nTo make long-context LLMs universally useful, an important desiderata is to ensure\nrobust performance on standard short-context tasks. We verify our models\u2019 performance on a series\nof common benchmarks following the previous work (Touvron et al., 2023). The aggregated results\nare shown in Table 1. Overall, we observe on-par and, in most cases, stronger results than LLAMA 2.\nNotably, we observe significantly improved results on coding, math, and knowledge intensive tasks\nsuch as MMLU. As shown in Table 2, our model outperforms GPT-3.5 on MMLU and GSM8k. This\nis in contrast to a previous work (Chen et al., 2023) which observes degradation on short tasks. We\nattribute the improvements to additional computation FLOPs and the knowledge learned from newly\nintroduced long data.\nLong Tasks\nDifferent from previous works (Chen et al., 2023; Mohtashami and Jaggi, 2023)\nthat mostly rely on perplexity and synthetic tasks to gauge long-context performance, we perform\nlong-context evaluation using real-world language tasks. We evaluate 0-shot performance on Nar-\nrativeQA (Ko\u02c7cisk\u00fd et al., 2018), 2-shot on QuALITY (Pang et al., 2022) and Qasper (Dasigi et al.,\n2021), and 1-shot on QMSum (Zhong et al., 2021). The number of shots are decided based on\nthe average sample length of each dataset (i.e., samples in Qasper and QuALITY are often much\nshorter than those of NarrativeQA). We focus these QA-style tasks because of the ease of prompt\nengineering2 and less biased automatic evaluations. The input prompts are truncated from the left\nside if the prompts exceed the maximum input length of the model or 16,384 tokens. We compare\nwith open-source long-context models available in Huggingface Transformers, namely Focused\nTransformer (Tworkowski et al., 2023a), YaRN (Peng et al., 2023), Xgen (Nijkamp et al., 2023),\nMPT (MosaicML, 2023b,a) and Together\u2019s LLAMA 2 fork (Together, 2023). As shown in Table 3, our\nmodels achieve superior performance compared to these models. At the 7B scale, only \u201cTogether-7B-\n2We use simple prompt \u201c{CONTEXT} Q: {QUESTION}, A:\u201d to evaluate all pretrained models.\n4\nModel\nNarrativeQA\nQasper\nQuALITY\nQMSum\nF1 (0-shot)\nF1 (2-shot)\nEM (2-shot)\nROUGE-geo\u2217 (1-shot)\nFocused Transformer (3B)\n16.3\n15.4\n20.5\n10.6\nYarn-7B-128k\n20.9\n26.2\n32.3\n11.4\nTogether-7B-32k\u2020\n23.3\n27.3\n41.2\n12.6\nXgen-7B-8k-base\n17.4\n20.5\n21.0\n6.79\nMPT-7B-8k\n18.8\n24.7\n23.7\n8.78\nYarn-13B-128k\n23.4\n27.1\n46.4\n11.9\nMPT-30B-8k\n22.9\n29.0\n41.5\n10.3\nLLAMA 2 70B\n25.7\n27.5\n53.0\n11.9\nLLAMA 2 LONG 7B\n21.9\n27.8\n43.2\n14.9\nLLAMA 2 LONG 13B\n25.6\n31.2\n57.6\n15.7\nLLAMA 2 LONG 34B\n29.4\n33.7\n65.7\n15.9\nLLAMA 2 LONG 70B\n30.9\n35.7\n79.7\n16.5\nTable 3: Comparison with open-source long-context models on research benchmarks. \u2020: \u201ctogether-\n7B-32k\u201d is not a purely pretrained model and has been trained using supervised datasets which can\nimprove its few-shot results. \u2217: ROUGE-geo is the geometric mean of ROUGE-1, 2 and L. All\nnumbers are validation results and the maximum allowed prompt length is set to 16,384 tokens.\nF1\n27.5\n28.7\n30.2\n30.9\nNarrative QA\nF1\n28.9\n32.7\n35.1\n35.7\nQasper\nEM\n63.9\n80.3\n79.6\n79.7\nQuALITY\n4,096\n8,192\n12,288\n16,384\nMaximum input length\nROUGE-geo\n10.9\n11.9\n14.4\n16.5\nQMSum\nFigure 2: Performance on long-context tasks as the maximum context lengths of prompts increase.\n32k\u201d can match our model\u2019s performance. Note that this model is not a purely self-supervised model\nand has been finetuned using a large supervised dataset to improve its few-shot results. As the 7/13B\nvariants of our models have been trained with 32k-token sequences, we also perform comparisons\nusing 32,768 maximum prompts lengths and the results are consistent, as shown in Table 13.\nEffective Context Utilization\nTo validate that our models can effectively use increased context\nwindow, we first show in Figure 2 that the results on each long task improve monotonically as we\nincrease the context lengths. Inspired by (Kaplan et al., 2020; Hoffmann et al., 2022), we also found\nthat the language modeling loss of our model follows a power-law plus constant scaling relationship\nwith the context length (Figure 1), suggesting:\n\u2022 Our model continues to show gain in performance (on the language modeling loss) up\nto 32,768 tokens of text, despite having diminishing returns. Taking our 70B model for\nexample, if we double the context length, we can expect the loss to be reduced by a factor of\n2\u2212\u03b2 \u2248 0.7 plus a model specific constant (1 \u2212 2\u2212\u03b2) \u00b7 \u03b3.\n5\n\u2022 Larger models can leverage the contexts more effectively, indicated by the larger \u03b2 value of\nthe curves.\n3.2\nInstruction Tuning Results\nWe test our instruction tuned model on ZeroSCROLLS (Shaham et al., 2023) which bundles 10 long-\ncontext datasets spanning from summarization, question answering, to multi-document aggregation\ntasks. For a fair comparison, we use the same configuration (prompts, truncation strategy, and maxi-\nmum generation lengths, etc) as specified by the benchmark. As shown in Table 4, without using any\nhuman annotated long context data, our 70B chat model is able to outperform gpt-3.5-turbo-16k\non 7 out of the 10 tasks. In addition, we run evaluations on six new long tasks introduced in L-\nEval (An et al., 2023) and again observe strong results, as shown in Table 17 in the Appendix. We see\nthat the finetuned model is particularly good at QA tasks which is the main theme of the self-instruct\ndata. We expect the performance to be further improved if more diverse data are used for finetuning.\nIt is worth mentioning that evaluating long-context LLMs is a nontrivial task. The automatic metrics\nused in these benchmarks are limited in many ways. For instance, the summarization tasks only come\nwith a single ground-truth summary and the n-gram matching metrics do not necessarily align with\nhuman preference. For QA and aggregation tasks, where the metric is less of a concern, truncating the\ninput context might also remove the information necessary to answer the question. Another important\ncaveat is that most proprietary models do not share their training data details, which makes it hard to\ntake into consideration the potential leakage during public benchmark evaluation.\nModel\nSummarization\nQuestion answering\nAggregation\nGR\nSS\nQM\nSQAL\nQspr\nNrtv\nQALT\nMuSQ\nSpDg\nBkSS\nAvg\nGPT-3.5-turbo (4k)\n21.3\n16.1\n15.6\n20.4\n49.3\n25.1\n66.6\n27.1\n49.1\n49.8\n34.0\nGPT-3.5-turbo-16k\u2020\n24.3\n16.2\n17.4\n21.4\n50.0\n29.5\n72.0\n27.0\n54.1\n54.6\n36.7\nClaude (8k)\n24.2\n16.1\n14.6\n21.0\n52.3\n32.6\n84.8\n36.1\n61.6\n47.4\n39.1\nGPT4 (8k)\n26.3\n17.3\n18.5\n22.6\n50.7\n27.6\n89.2\n41.1\n62.8\n60.5\n41.7\nLLAMA 2 LONG CHAT 70B\n26.0\n15.0\n20.0\n20.9\n52.0\n31.7\n82.6\n27.3\n55.5\n46.2\n37.7\nTable 4: ZeroSCROLLS long-context leaderboard results. \u2020Evaluated as of 8/7/2023. The GPT-4 and\nClaude results are directly copied from the leaderboard. Underscored are the 7/10 tasks where our\nmodel outperforms gpt-3.5-turbo-16k.\n3.3\nHuman Evaluation\nFigure 3: Human preference on model responses with multi-turn conversation and multi-document\nsearch query answering data.\nComplementary to the automatic evaluation benchmark results, we conduct human evaluations\nby asking annotators whether they prefer the generation from our instruction finetuned model or\n6\nfrom proprietary models like MPT-30B-chat, GPT-4, GPT-3.5-turbo-16k, and Claude-2 in terms of\nhelpfulness, honesty, and harmlessness. Unlike automatic metrics, humans are better at evaluating\nthe quality of model responses for long context models because of the large space of acceptable\nanswers. We focus on two major application scenarios with a total of 2,352 examples. For multi-turn\nconversation data, each prompt is a chat history based on which the model needs to generate a\ncoherent response. For the multi-document search query answering application, the model is provided\nwith a few most relevant documents retrieved from a search session and the corresponding search\nquery. We then evaluate how well these models can leverage the information (retrieved documents)\nto answer the given query. Each comparison example was evaluated by 3 different human annotators.\nThe standard win rate of our our model over each model is calculated by averaging the result of\neach comparison example and the final score along with the 95% confidence interval is shown in\nFigure 3. With very little instruction data, our model can achieve competitive performance against\nMPT-30B-chat, GPT-3.5-turbo-16k, and Claude-2. It is worth noting that human evaluation on longer\ncontext tasks is challenging and generally requires well trained and skilled annotators. We hope this\nstudy can not only give a sense of the potential of our instruction finetuned model on some long\ncontext downstream applications but also motivate future efforts in developing more robust long\ncontext automatic evaluations.\n4\nAnalysis\nIn this section. We perform ablation experiments to justify our design choices (i.e. architecture\nmodification, data mixes, and training curriculum) and quantify their contributions to the final\nperformance.\n4.1\nPositional Encoding for Long Text\nPE\nBooks\nCC\nWikipedia\nRoPE\n6.548\n6.816\n3.802\nROPE PI\n6.341\n6.786\n3.775\nROPE ABF\n6.323\n6.780\n3.771\nXPOS ABF\n6.331\n6.780\n3.771\nTable 5: Validation perplexity of models with\ndifferent positional encoding variants. All\nsamples are 32, 768-token sequences (CC:\nCommonCrawl).\nOur early experiments used a synthetic \u201cFIRST-\nSENTENCE-RETRIEVAL\u201d task to probe the effective\ncontext window of the pretrained models where we\nsimply prompt the model to return the first sentence\nof the input. Our initial task results suggest that, with\nthe original LLAMA 2 architecture untouched, our\nmodel was unable to effectively attend beyond 4, 000\n- 6, 000 tokens even after extensive long-context con-\ntinual pretraining. We hypothesize that this bottle-\nneck comes from the ROPE positional encoding used\nin LLAMA 2 series which imposes a heavy decay on\nthe attention scores3 for distant tokens. We propose a\nsimple modification to the default RoPE encoding to\nreduce the decaying effect \u2013 increasing the \u201cbase frequency b\u201d of ROPE from 10, 000 to 500, 000,\nwhich essentially reduces the rotation angles of each dimension. The idea is also concurrently\nsuggested in the Reddit r/LocalLLaMa community and Rozi\u00e8re et al. (2023). The effect of the\nbase frequency change is visualized in Figure 4. Another concurrent approach named \u201cposition\ninterpolation\u201d (PI) (Chen et al., 2023) proposes to linearly scale the input positions such that the\npositions of tokens in the long sequences will to mapped to the model\u2019s original position range. As\nshown by the figure, it also implicitly achieves a decay reduction effect.\nAnother interesting observation from the visualization is that RoPE introduces large \u201coscillation\u201d in\nthe long-range regions, which could be undesirable for language modeling (Sun et al., 2022). To\ninvestigate whether this effect hurts performance, we also explored another recently proposed variant\nof rotary encoding, XPOS (Sun et al., 2022), which smooths the high-frequency component. Note\nthat XPOS with the default parameters suffers from the same decaying issue as ROPE and therefore,\nwe also applied a similar decay fix to XPOS.\nSpecifically, we empirically compare the following methods: the ROPE baseline, PI, our proposed\nRoPE with adjusted base frequency (denoted as ROPE ABF), and XPOS ABF (visual comparisons in\nFigure 4). We report results on 1) long-sequence validation perplexity in Table 5 and Figure 5a, 2) the\n3The quantity that heavily decays is Eq,k[ROPE(q, m)\u22a4ROPE(k, n)|m, n] as the relative position |m \u2212 n|\ngets larger where q, k are the query and key of the two tokens at position m and n.\n7\nHumanEval\nMath\nMMLU\nHellaSwag\nTQA\nRoPE\n14.63\n3.62\n45.69\n76.31\n65.23\nROPE PI\n15.24\n3.08\n45.84\n76.65\n65.96\nROPE-ABF\n17.07\n3.52\n46.24\n76.73\n66.04\nXPOS-ABF\n16.46\n3.54\n45.72\n76.68\n66.14\nTable 6: The performance of models with different positional encoding variants on standard short-\ncontext benchmarks.\nFIRST-SENTENCE-RETRIEVAL context probing task4 in Figure 5b, and 3) some representative regular\ncontext tasks in Table 6 (to validate that long models do not degenerate on short-context tasks). All\nmodel variants are continually pretrained from the 7B LLAMA 2 checkpoint with additional 80B\ntokens organized as 32,768-token long sequences.\n0\n5000\n10000\n15000\n20000\n25000\n30000\nDistance between query and key tokens\n2\n0\n2\n4\n6\n8\n10\n12\nAttention score before softmax\nRoPE\nRoPE PI\nRoPE ABF\nxPos ABF\nFigure 4: Decaying raw attention scores for distant tokens of explored positional encoding variants\n(assuming keys and queries are all-ones vectors).\nOverall, results on these evaluations suggest that ROPE ABF performs the best among all explored\nvariants. In particular, we see that ROPE ABF is the only variant that can maintain its performance\nup to the full 32,768-token context window on the FIRST-SENTENCE-RETRIEVAL task. We also found\nthat XPOS ABF with less oscillation does not lead to substantial gains, suggesting that these artifacts\nare not detrimental to language modeling. While XPOS is claimed to possess better extrapolation\nproperty (Sun et al., 2022), we found that, with the base frequency modification, XPOS does not\nextrapolate better than ROPE (see Appendix C). In addition to empirical results, we provide a\ntheoretical analysis of RoPE ABF and its difference to PI in Appendix B. We argue that RoPE ABF\ndistributes the embedded vectors with an increased granularity when compared to RoPE PI, making\nit a easier for the model to distinguish between positions. It is worth noting that the relative distance\nbetween the embedded vectors has a linear dependence on the key parameter of RoPE PI and a\nlogarithmic dependence on the key parameter of RoPE ABF, which coincides with our empirical\nobservation that the base-frequency is not very sensitive and can be easily adjusted based on the max\nsequence length.\n4.2\nPretraining Data Mix\nThe data used to continually pretrain our model combines existing datasets used by LLAMA 2 and\nnew long text data. We also adjusted the data source mix ratio to up-weight long data samples. Our\nearly experiments with 7B models confirms the significant improvements using this data mix for\n4We also test on the PASSKEY task as used in (Mohtashami and Jaggi, 2023). All the model variants except\nROPE can achieve perfect accuracy. We believe this task is overly simple for context probing.\n8\n5000\n10000\n15000\n20000\nContinual train steps\n6.475\n6.500\n6.525\n6.550\n6.575\n6.600\n6.625\n6.650\n6.675\nValidation perplexity\nRoPE PI\nRoPE ABF\nxPos ABF\n(a) Validation PPL (16k-token sequences) on a held-out long-context dataset.\n256 1k\n2k\n4k\n8k 10k 12k 14k 16k 20k 24k 28k 30k\nTask length\n0\n20\n40\n60\n80\n100\nROUGE-L\nRoPE\nRoPE PI\nRoPE ABF\nxPos ABF\n(b) Performance on FIRST-SENTENCE-RETRIEVAL task.\nFigure 5: Comparison of positional encoding variants on synthetic sentence retrieval task and\nvalidation perplexity evolution during continual pretraining.\nContinual Pretrain Data\nNarrativeQA\nQasper\nQuality\nQMSum\n\u2206 F1\n\u2206 F1\n\u2206 EM\n\u2206 ROUGE-geo\nLLAMA 2 LONG data mix\n23.70%\n43.64%\n75.5%\n45.70%\nLLAMA 2 data mix\n18.23%\n38.12%\n60.3%\n44.87%\n- remove long text data\n19.48%\n39.14%\n67.1%\n36.60%\n- upsample existing long text data\n22.15%\n36.82%\n65.0%\n42.83%\nTable 7: Comparison of different pretraining data mix on long-context tasks. Instead of showing\nthe absolute performance, we report relative improvements over the 7B LLAMA 2 which has a\n4,096-token context window. All models are evaluated with prompts truncated at 16,384 tokens.\nlong-context tasks, as shown in the first two rows of Table 7. In this section, we aim to rigorously\ninvestigate the source of improvements. In particular, we are interested in differentiating the effects\nof the data length distribution and the quality of the corpus itself.\nWe perform two additional ablations using LLAMA 2\u2019s pretrain datasets: 1) we remove the long text\ndata from the LLAMA 2 dataset and continually pretrain our model with mostly short documents; 2)\nwe increase the sample weights of existing long text data to be similar to the long text ratio used by\nproposed new model. Interestingly, even with most of the long texts removed, the model can still\nobtain most of the performance gain over LLAMA 2. We also find that there is no clear and consistent\nadvantage as we greatly increase the long data ratio (the third row v.s. the fourth row in Table 7 and\nTable 8). We observe similar results on the FIRST-SENTENCE-RETRIEVAL task as shown by Figure 7\nin the Appendix.\nBased on the above ablations, we can see that adjusting the length distribution of the pretrain data does\nnot provide major benefits. However, as we evaluate these model variants\u2019 performance on standard\nshort-context tasks, we find that new data mix also leads to large improvements in many cases,\nespecially knowledge-intensive tasks like MMLU, as shown in Table 8. These results suggest that\nlong-context LLMs can be effectively trained even with very limited long data and the improvements\nof our pretrain data over the one used by LLAMA 2 mostly come from the quality of the data itself,\ninstead of the length distribution difference.\nContinual Pretrain Data\nHumanEval\nMath\nMMLU\nHellaSwag\nTQA\nLLAMA 2 LONG data mix\n17.08\n4.09\n48.62\n76.74\n66.24\nLLAMA 2 data mix\n15.24\n3.61\n46.30\n76.63\n66.71\n- remove long text data\n17.07\n3.57\n46.25\n76.76\n65.90\n- upsample existing long text data\n17.07\n3.53\n46.25\n76.74\n66.04\nTable 8: Standard short task performance of long-context models with different pretrain data mix.\n9\n4.3\nInstruction Tuning\nWe explored various strategies to instruction-finetune the pre-trained long context model which do not\nrequire any supervised long data. We start with only finetuning the models with short instruction data\nfrom LLAMA 2 CHAT (referred as \"RLHF V5\" in (Touvron et al., 2023)) and then blend in with some\npretrain data to avoid forgetting of previous long context continual pretraining. As demonstrated in\nTable 9, using only short instruction data can already produce a decent long model that significantly\noutperforms LLAMA 2 on various long-context tasks. On top of this dataset that only includes short\nprompts, we see that adding pretrain data (calculating language modeling loss on the whole sequence)\ncan further boost the performance on most datasets. Inspired by this, we add the LM loss over the\nlong context inputs when we finetune with self-instruct data. This simple trick makes learning more\nstable when we have unbalanced input and output lengths5, which gives significant improvements on\nmost of the tested tasks (the last two rows of Table 9).\nSettings\nQasper\nNarrativeQA\nQuALITY\nSummScreenFD\nQMSum\nLLAMA 2 CHAT baseline\n12.2\n9.13\n56.7\n10.5\n14.4\nLLAMA 2 LONG finetuned with:\n\"RLHF V5\"\n22.3\n13.2\n71.4\n14.8\n16.9\n\"RLHF V5\" mix pretrain\n23.7\n16.6\n76.2\n15.7\n17.8\n\"RLHF V5\" mix self-inst w/o LM loss\n35.7\n22.3\n59.3\n12.2\n13.4\n\"RLHF V5\" mix self-inst with LM loss\n38.9\n23.3\n77.3\n14.5\n18.5\nTable 9: Comparison of different instruction finetuning data mixes.\n4.4\nTraining Curriculum\nContinual pretraining has demonstrated its efficacy in our experiments, but an open question still\nremains: does pretraining from scratch with long sequences yield better performance than continual\npretraining? In this section, we study different training curricula and try to investigate if continual\npretraining can offer competitive performance with less computation budget. We start off by pre-\ntraining a 7B model with 32,768 sequence length from start to the end. Then we explored various\ntwo-stage training curricula where we begin with 4096 sequence length and switch to 32,768 when\nthe model completes 20%, 40%, 80% of whole training process. For all cases, we keep the same\nnumber of total training tokens and make sure the number of tokens per each gradient update remains\nconstant (4 million tokens) by adjusting the batch size and sequence length accordingly.\nWe evaluate our models on the long-text QA tasks used in Section 4.2 and report the final models\u2019\nperplexity on different validation corpora. As shown in Table 10 and Table 11, continual pretraining\nfrom short context models can easily save around 40% FLOPs while imposing almost no loss on\nperformance. These results also align with the training loss curves we observed from each run in\nFigure 6 \u2013 the models can quickly adapt to the increased sequence length and get to similar loss scale.\nPretrain Curriculum\nFLOPs\nNarrativeQA\nQasper\nQuality\nQMSum\nF1\nF1\nEM\nROUGE-geo\n32k from scratch\n3.783 \u00d7 1022\n18.5\n28.6\n37.9\n11.46\n4k\u219232k @ 20%\n3.405 \u00d7 1022\n20.0\n28.1\n38.8\n12.09\n4k\u219232k @ 40%\n3.026 \u00d7 1022\n20.1\n27.0\n37.4\n12.44\n4k\u219232k @ 80%\n2.270 \u00d7 1022\n18.5\n25.0\n38.3\n11.00\nTable 10: Comparison of models with different training curricula on long context QA tasks.\n5In our cases, the output lengths of most samples are a lot shorter than the those of the long-context inputs.\n10\nModel\nCC\nBooks\nWikipedia\n32k from scratch\n7.67\n6.52\n4.31\n4k\u219232k @ 20%\n7.59\n6.46\n4.26\n4k\u219232k @ 40%\n7.59\n6.46\n4.25\n4k\u219232k @ 80%\n7.59\n6.49\n4.25\nTable 11: Perplexity evaluation of models with different training curricula on three validation sets.\n10k\n20k\n30k\n40k\n50k\n60k\n70k\n80k\n90k 100k\nTrain steps\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\nTrain loss\n4k\n8k\n32k\n10k\n20k\n30k\n40k\n50k\n60k\n70k\n80k\n90k 100k\nTrain steps\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\nTrain loss\n32k\n4k\n32k @ 20%\n4k\n32k @ 40%\n4k\n32k @ 80%\nFigure 6: Smoothed loss curves for the training curriculum ablation. On the left, we show losses for\nmodels trained with a fixed context window. On the right, we compare training curricula where we\nswitch the context length from 4,096 to 32,768 tokens at different stages indicated by the dashed\nlines. Our models can quickly adapt to the new sequence length within a few thousand steps.\n5\nAI Safety\n5.1\nEvaluation on Safety Benchmarks\nDespite showing excellent performance on various of downstream tasks, large language models are\nprone to generating harmful, misinformative, and biased contents (Lin et al., 2021; Hartvigsen et al.,\n2022; Dhamala et al., 2021; Ji et al., 2023). Long-context language models can process extended\ninputs in their context window, but at the same time, they also face a higher risk of jailbreak, especially\nthrough means such as prompt injection (Greshake et al., 2023). In this section, we evaluate the\nsafety capability of instruction fine-tuned model using three standard academic benchmarks including\nTruthfulQA (Lin et al., 2021), ToxiGen (Hartvigsen et al., 2022), and BOLD (Dhamala et al., 2021),\nsimilar to (Touvron et al., 2023). We focus on the largest instruction fine-tuned model variant (i.e.,\n70B) and compare its results with both open sourced LLMs (Falcon-instruct Almazrouei et al. (2023),\nMPT-instruct MosaicML (2023a)) and propriety LLMS (GPT-3.5, GPT-4 (OpenAI, 2023), Claude-2\n(Anthropic, 2023)) in Table 12.\nWe observe that in general instruction fine-tuned model maintains similar safety performance com-\npared to LLAMA 2 CHAT and is safer and less biased compared to other open-source LLMs such as\nFalcon-instruct and MPT-instruct. AI safety is a complex domain and it can be extremely difficult to\ncomprehensively evaluate all safety aspects of instruction fine-tuned model with three benchmarks.\nHowever, we hope our analysis can serve as a pilot study and provide directional signals on long-\ncontext large language models\u2019 safety performance, which are not discussed in other works on the\nsame topic (Tworkowski et al., 2023b; Ding et al., 2023; Chen et al., 2023). Currently the community\nalso lacks dedicated safety benchmarks for long-context large language model evaluation and we\nplan to invest in this direction in our future work.\nTruthfulQA\nWe evaluate instruction fine-tuned model on TruthfulQA (Lin et al., 2021) to bench-\nmark its factuality. The benchmark consists of 817 questions covering 38 categories including health,\nlaw, finance, and politics (Lin et al., 2021). Similar to (Touvron et al., 2023), we use few-shot prompts\nwith 6 random QA pairs for generation and then leverage two fine-tuned GPT-3 models to classify\n11\nwhether the generation is truthful and informative. We report the percentage of generations that are\nboth truthful and informative as the final metric in Table 12.\nToxiGen\nWe measure the toxicity of instruction fine-tuned model using ToxiGen (Hartvigsen et al.,\n2022) where we check the percentage of toxic and hateful generations against 13 minority groups.\nFollowing (Touvron et al., 2023), we filtered out prompts where annotators disagree with each other\non the target demographic group. We use the default ToxiGen classifier fine-tuned based on RoBERTa\n(Liu et al., 2019) to evaluate the level of toxicity of the model\u2019s outputs. We report the percentage of\ntoxic generations across all groups in Table 12.\nBOLD\nBias in Open-Ended Language Dataset (BOLD) Dhamala et al. (2021) is used in this work\nto quantify how biased the models are against people from different demographic groups. This dataset\nconsists of 23,679 prompts extracted from English Wikipedia covering five domains including race,\ngender, religion, political ideology and profession with 43 subgroups in total. Following Touvron\net al. (2023), we exclude prompts belonging to Hinduism and Atheism religious subgroups as they\nonly feature 12 and 29 prompts, respectively. After generations are inferred from each model, we\nleverage the Valence Aware Dictionary and Sentiment Reasoner (VADER) Hutto and Gilbert (2014)\nto perform sentiment analysis with a score ranging between -1 and 1. A positive score corresponds to\na positive sentiment towards the subgroup mentioned in the prompt and vice versa. A sentiment score\nclose to 0 indicates neutral sentiment which is desired. We report the average sentiment score across\n43 demographic subgroups as the final metric for BOLD in Table 12.\nModel Size\nTruthfulQA \u2191\nToxiGen \u2193\nBOLD \u2193\nGPT-3.5-turbo\n-\n78.46\n0.01\n0.50\nGPT-3.5-turbo-16k\n-\n75.15\n0.07\n0.49\nClaude-2\n-\n62.66\n0.05\n0.46\nGPT4\n-\n80.66\n0.03\n0.43\nFalcon-instruct\n40B\n57.41\n3.3\n0.39\nMPT-instruct\n30B\n42.71\n16.85\n0.34\nLLAMA 2 CHAT\n70B\n64.14\n0.01\n0.41\nLLAMA 2 LONG CHAT\n70B\n60.95\n0.00\n0.40\nTable 12: Evaluation of fine-tuned LLMs on three safety benchmarks. For TruthfulQA, we present the\npercentage of generations that are both truthful and informative (the higher the better). For ToxiGen,\nwe present the percentage of toxic generations across all groups (the smaller the better). For BOLD,\nwe report the average sentiment score across 43 demographic groups (the closer to 0 the better).\n5.2\nRed Teaming Exercises\nCurrently there is no open-sourced safety benchmark designed for long-context understanding. To\nensure that the models are safe in long context use scenarios, we performed internal red teaming to\nbetter understand the vulnerability of our chat model. We attack the model by feeding long contexts\n(e.g., long conversations) to it, followed by adversarial prompts covering risky areas including\nillicit and criminal conducts (e.g., terrorism, theft, and human trafficking), hateful and harmful\nbehaviors (e.g., defamation, self-harm, eating disorders, and discrimination), and unqualified advice\nTouvron et al. (2023). Through manual inspection, we did not observe significant risks compared to\nLLAMA 2 CHAT Touvron et al. (2023). We plan to invest more in new attack vectors against long\ncontext large models in future work.\n6\nLimitations\nLimited Functionality.\nThe our model proposed in this paper has not yet been finetuned for a wide\nrange of long-context applications, such as creative writing that require long-form outputs. Applying\nexisting alignment recipes, e.g., RLHF, for various scenarios is expensive and nontrivial. Even skilled\nannotators may struggle to the intricate details in dense texts. In this regard, we consider developing\nefficient alignment methods for long LLMs to be a very valuable direction for future research.\n12\nTokenizer Efficiency.\nWhile the proposed our model series can consume contexts up to 32,768\ntokens, the actually number of words our model can take is largely affected by the tokenizer behaviour.\nThe tokenizer used by the Llama series has a relatively small vocabulary (32k symbols) and often\nproduces longer sequences compare to the sequences given by GPT-3.5\u2019s tokenizer \u2013 we observe our\ntokenizer often produce 10% more tokens on average. Additionally, the tokenizer we use also cannot\nefficiently handle whitespace, making it inefficient to process long code data.\nHallucination.\nLike other LLMs, we have observed hallucination issue when testing the proposed\nour model. While this issue is common for short-context models, tackling with this problem for\nlong-context models can be more pronounced because of the dense information they consume and\nthe insufficient alignment process.\n7\nConclusion\nWe present a series of long-context LLMs that leverage a simple yet necessary position encoding\nrefinement and continual pretraining to achieve strong long-context performance. Our long context\nscaling is performed by continually pretraining from LLAMA 2 with additional 400B tokens and\noutperform LLAMA 2 on both short and long-context tasks. Our models also demonstrate superior\nperformance compared to existing open-source long-context models and compare favorably against\ngpt-3.5-turbo-16k on a suite of long-context tasks after a simple instruction finetuning procedure\nwithout human supervision. We complement our results with a comprehensive analysis, providing\ninsights on the influences of various factors including the nuances of position encodings, the data mix,\nand the pretraining curriculum on the final performance. We hope our study could make long-context\nLLMs more accessible and facilitate further advancements in this field.\n8\nAcknowledgement\nWe would like to thank Nikolay Bashlykov, Matt Wilde, Wenyin Fu, Jianyu Huang, Jenya Lee,\nMathew Oldham, and Shawn Xu for their invaluable support on the data, infrastructure, and various\nother aspects of this project.\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mer-\nouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune,\nBaptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art\nperformance. 2023.\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval:\nInstituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023.\nAnthropic.\nIntroducing 100K Context Windows, 2023.\nURL https://www.anthropic.com/index/\n100k-context-windows.\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language\nmodels. arXiv:abs/2108.07732, 2021.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense\nin natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages\n7432\u20137439, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large\nlanguage models via positional interpolation, 2023.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\narXiv preprint arXiv:1904.10509, 2019.\n13\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\narXiv:1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv\npreprint arXiv:2110.14168, 2021.\nMike Conover,\nMatt Hayes,\nAnkit Mathur,\nJianwei Xie,\nJun Wan,\nSam Shah,\nAli Ghodsi,\nPatrick Wendell, Matei Zaharia, and Reynold Xin.\nFree dolly:\nIntroducing the world\u2019s first\ntruly open instruction-tuned llm, 2023.\nURL https://www.databricks.com/blog/2023/04/12/\ndolly-first-open-commercially-viable-instruction-tuned-llm.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. In NeurIPS, 2022.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-\nseeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\npages 4599\u20134610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nnaacl-main.365. URL https://aclanthology.org/2021.naacl-main.365.\nJwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul\nGupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of\nthe 2021 ACM conference on fairness, accountability, and transparency, pages 862\u2013872, 2021.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet:\nScaling transformers to 1,000,000,000 tokens, 2023.\nKai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what\nyou\u2019ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.\narXiv preprint arXiv:2302.12173, 2023.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen:\nA large-scale machine-generated dataset for adversarial and implicit hate speech detection. arXiv preprint\narXiv:2203.09509, 2022.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie\nMillican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.\nClayton Hutto and Eric Gilbert. Vader: A parsimonious rule-based model for sentiment analysis of social media\ntext. In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216\u2013225,\n2014.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto,\nand Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):\n1\u201338, 2023.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\nchallenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\nTom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward\nGrefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for\nComputational Linguistics, 6:317\u2013328, 2018. doi: 10.1162/tacl_a_00023. URL https://aclanthology.\norg/Q18-1023.\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Ab-\ndullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, et al. Openassistant conversations\u2013\ndemocratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.\n14\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question\nanswering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods.\narXiv preprint arXiv:2109.07958, 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for\ntransformers. arXiv preprint arXiv:2305.16300, 2023.\nMosaicML. Introducing mpt-30b: Raising the bar for open-source foundation models, 2023a. URL www.\nmosaicml.com/blog/mpt-30b. Accessed: 2023-06-22.\nMosaicML. Introducing mpt-7b: A new standard for open-source, ly usable llms, 2023b. URL www.mosaicml.\ncom/blog/mpt-7b.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti,\nDmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language\nmodel training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1\u201315, 2021.\nErik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe\nLaban, Ben Krause, et al. Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length.\nSalesforce AI Research Blog, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh\nPadmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY: Question answering\nwith long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5336\u20135358, Seattle, United\nStates, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.391. URL\nhttps://aclanthology.org/2022.naacl-main.391.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of\nlarge language models, 2023.\nr/LocalLLaMa. NTK-Aware Scaled RoPE allows llama models to have extended (8k+) context size without any\nfine-tuning and minimal perplexity degradation. https://www.reddit.com/r/LocalLLaMA/comments/\n14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/. Accessed: 2023-08-25.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt,\nCristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar,\nHugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open\nfoundation models for code, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning\nabout social interactions. arXiv preprint arXiv:1904.09728, 2019.\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot benchmark for\nlong text understanding, 2023.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding, 2022.\n15\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song,\nand Furu Wei. A length-extrapolatable transformer, 2022.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering\nchallenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.\nTogether. Llama-2-7b-32k-instruct \u2014 and fine-tuning for llama-2 models with together api, 2023. URL\nhttps://together.ai/blog/llama-2-7b-32k-instruct.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv preprint arXiv:2307.09288, 2023.\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u00b4s.\nFocused transformer: Contrastive training for context scaling, 2023a.\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u00b4s.\nFocused transformer: Contrastive training for context scaling, 2023b.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint\narXiv:2212.10560, 2022.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\nfinish your sentence?, 2019.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based\nmulti-domain meeting summarization. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905\u20135921,\nOnline, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL\nhttps://aclanthology.org/2021.naacl-main.472.\n16\nA\nMore Results\nModel\nPrompt length\nNarrativeQA\nQasper\nQuALITY\nQMSum\nF1 (0-shot)\nF1 (2-shot)\nEM (2-shot)\nROUGE-geo\u2217 (1-shot)\nYarn-7B-128k\n16k\n20.9\n26.2\n32.3\n11.4\nTogether-7B-32k\n16k\n23.3\n27.3\n41.2\n12.6\nYarn-13B-128k\n16k\n23.4\n27.1\n46.4\n11.9\nYarn-7B-128k\n32k\n24.0\n26.2\n30.4\n13.6\nTogether-7B-32k\n32k\n24.7\n27.3\n41.3\n14.2\nYarn-13B-128k\n32k\n25.5\n27.1\n48.0\n13.8\nLLAMA 2 LONG 7B\n16k\n21.9\n27.8\n43.2\n14.9\nLLAMA 2 LONG 13B\n16k\n25.6\n31.2\n57.6\n15.7\nLLAMA 2 LONG 7B\n32k\n24.4\n28.7\n43.6\n15.9\nLLAMA 2 LONG 13B\n32k\n27.4\n31.6\n59.0\n17.0\nTable 13: Comparison of our models with open-source long-context models on research benchmarks\nusing a maximum prompt length of 32,768 tokens.\nModel\nHumanities\nSTEM\nSocial Sciences\nOther\nLLAMA 2 LONG 7B\n54.8\n35.7\n58.4\n53.2\nLLAMA 2 LONG 13B\n69.0\n44.4\n71.3\n65.8\nLLAMA 2 LONG 34B\n73.5\n49.9\n78.4\n69.3\nLLAMA 2 LONG 70B\n80.1\n55.5\n84.4\n74.9\nTable 14: Decomposed MMLU results.\nModel\nHumanEval\nMBPP\nMATH\nGSM8k\nNQ\nTQA\nLLAMA 2 LONG 7B\n18.3\n23.0\n4.22\n16.8\n27.5\n74.4\nLLAMA 2 LONG 13B\n19.5\n31.8\n8.38\n34.6\n32.5\n81.1\nLLAMA 2 LONG 34B\n22.6\n37.2\n10.6\n47.4\n35.0\n85.6\nLLAMA 2 LONG 70B\n32.9\n46.8\n17.2\n65.4\n39.8\n88.2\nTable 15: Results on HumanEval (0-shot), MBPP (3-shot), MATH (4-shot), GSM8K (8-shot),\nNaturalQuestions (5-shot) and TriviaQA-wiki (5-shot).\nModel\nPIQA\nSIQA\nHellaSwag\nWinoGrande\nARC-e\nARC-c\nOBQA\nCSQA\nLLAMA 2 LONG 7B\n78.9\n48.7\n77.8\n70.4\n76.2\n52.0\n59.0\n61.0\nLLAMA 2 LONG 13B\n81.6\n50.7\n81.2\n74.1\n77.7\n51.4\n55.6\n70.4\nLLAMA 2 LONG 34B\n82.6\n51.7\n83.8\n77.5\n79.7\n54.8\n60.2\n77.0\nLLAMA 2 LONG 70B\n83.3\n52.8\n85.7\n79.6\n80.3\n58.4\n59.6\n81.9\nTable 16: Commonsense reasoning decomposed results. We use the same number of shots and\nevaluation metrics for all tasks as LLAMA 2.\nB\nTheoretical Analysis of Positional Encodings\nRoPE maps an argument vector x \u2208 Rd into the embedding curve on a sphere in Cd/2 parametrized\nby a real parameter t \u2208 R and \u201cbase frequency\u201d b:\nf RoP E(x, t)j = (x2j + ix2j+1) eib\u2212 2j\nd t.\n17\nModel\nCoursera\nTPO\nTopicRetrieval\nFinQA\nContractQA\nNaturalQuestions\nClaude 1.3 100k\n60.2\n83.6\n70.6\n-\n-\n-\ngpt-3.5-turbo-16k\n59.7\n69.9\n69.3\n45.4\n24.9\n45.9\nBest open models reported in An et al. (2023)\nlongchat-13b-16k\n36.8\n55.4\n33.3\n37.9\n21.1\n22.8\nchatglm2-6b-8k\n47.2\n54.6\n10.0\n34.8\n16.4\n17.6\nLLAMA 2 LONG CHAT\n52.9\n81.8\n76.0\n47.3\n25.5\n66.7\nTable 17: Evaluation on additional long-context tasks from L-Eval. We report the official metrics\ndefined in An et al. (2023) and the results of compared models are directly token from the paper.\n256 1k\n2k\n4k\n8k 10k 12k 14k 16k 20k 24k 28k 30k\nTask length\n0\n20\n40\n60\n80\n100\nROUGE-L\nLlama-2 data mix\nLlama-2 data mix: remove long text data\nLlama-2 data mix: upsample long text data\nLlama-2-Long data mix\nFigure 7: FIRST-SENTENCE-RETRIEVAL performance of models trained with different data mixes.\nThe purpose of this mapping is to help the attention module to separate the vectors corresponding to\ntwo instances of the same token that are situated at different positions in the input sequence.\nAiming at extending the sequence length of a transformer pretrained with a particular positional\nembedding f from L to \u02c6L, we would like to come up with a positional embedding \u02c6f that minimizes\nthe distance between the old and the new images of the embedded vectors:\nd(f, \u02c6f) = max\nx\u2208X\nmin\nk\u2208{0,..N\u22121} j\u2208{0,.. \u02c6\nN\u22121}\ndist[f(x, k), \u02c6f(x, j)],\nwhere X \u2282 Rd is the set of vectors that would need to be positionally embedded. (Chen et al.,\n2023) computed this distance through the magnitude of the attention scores, but still argued for the\nefficiency of their method \u201cposition interpolation\u201d) due to its reduced value of the distance to the\noriginal RoPE images when compared to the naive extrapolation of the positional embedding.\nWith this in mind, we consider two different methods to extend the sequence length of a trained\ntransformer: Position Interpolation (PI) parameterized with \u03b1, and Adjusted Base Frequency (ABF)\nparameterized with \u03b2. These two methods correspond to the following embedding curves:\nf RoP E+P I(x, t)j = (x2j + ix2j+1) ei\u03b1\u00b7(b\u2212 2j\nd )t\nf RoP E+ABF (x, t)j = (x2j + ix2j+1) ei(\u03b2b)\u2212 2j\nd t\nEvaluating a positional embedding a-priori, we should consider the degree of granularity with which\nthe embedding images are being distributed over the embedding space. Comparing alternative\npositional embeddings \u02c6f mapping Rd \u00d7 N into Cd/2, we should prefer the one with the maximal\nvalue of the distance between the two closest images:\nq( \u02c6f) =\nmin\nx\u2208X;k\u0338=j\u2208{0.. \u02c6\nN\u22121}\ndist[ \u02c6f(x, k), \u02c6f(x, j)].\n18\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(a) RoPE\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(b) RoPE+PI\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(c) RoPE+ABF\nFigure 8: RoPE variants visualization as helices.\nThis leaves us with a multi-objective decision selecting the positional embedding for a model with\nextended context: on one hand, \u02c6f should be chosen so that it minimizes d(f, \u02c6f), while on the other\nhand its value of q( \u02c6f) should be big enough.\nBefore proceeding to the explanation on how we make this multi-objective decision, we would like\nto provide a geometric intuition for the positional embeddings considered here. While it is difficult\nto visualize a mapping Rd \u00d7 N \u2192 Cd/2, we can consider x \u2208 Rd to be fixed and visualize the\nprojection R \u2192 R3. To get the intuition behind PI and ABF, let us consider the helix that is formed\nby Re\n\u0002\nf RoP E(x, t)0\n\u0003\n, Im\n\u0002\nf RoP E(x, t)0\n\u0003\nand Re\n\u0002\nf RoP E(x, t)j\n\u0003\n. The example on the Figure 8a\ndepicts a black helix line given with the system\nx = cos t; y = sin t; z = sin at.\nThe red dots on the line correspond to 11 integer values of t.\nFigure 8b aims to illustrate the impact of Position Interpolation on the relative position of the mapped\nvectors. The distance between the consecutive points got reduced considerably compered to Figure\n8a. The impact of Adjusted Base Frequency is illustrated on Figure 8c. The distance between the\nconsecutive points remained almost the same as on Figure 8a, although the minimal distance between\npoints got considerably reduced due to the increased frequency of the helix. This effect of increased\nfrequency of the helix would be reduced in the high dimension setting. The value of the coefficient a\nfor the helix depicted on Figure 8a is two times larger than the value of the coefficient a for the helix\ndepicted on Figure 8c. If the dimension of the input of the attention mechanism is d = 128, then the\ndifference between \u03b81 = b\u2212 2j\nd at b = 10, 000 and \u03b81 = b\u2212 2j\nd at b = 500, 000 is only 6%. Thus, we\nfurther focus specifically on the distance between the consecutive images of the embeddings.\nWe make a formal comparison between Positional Interpolation and Adjusted Base Frequency\nby analytically comparing the pairwise distances between the images given by f RoP E+P I and\nf RoP E+ABF for consecutive integer values of t. This corresponds to the evaluation of q( \u02c6f) discussed\nearlier. We will measure the distance between embedding images in terms of the Euclidean sine\nsimilarity metric since all versions of RoPE are norm-preserving.\nsin \u2220(a, b) = Im\u27e8a, b\u27e9\n\u2225a\u2225\u2225b\u2225\nThe\nfollowing\nresult\nstates\nthat\nin\na\nhigh-dimensional\nspace,\nthe\nsine\nsimilarity\nsin \u2220(f RoP E+ABF (x, n + 1), f RoP E+ABF (x, n)) between two consecutive embedding im-\nages of a vector x can be bounded with a value proportional to (log b + log \u03b2)\u22121. Moreover, the\nsimilarity sin \u2220(f RoP E+P I(x, n + 1), f RoP E+P I(x, n)) can be bounded using \u03b1(log b)\u22121.\nTheorem 1. For x \u2208 Rd and n \u2208 N, the Euclidean sine similarity between the two consecutive\nimages of a positional embedding can be bounded as\nmink x2\nk\n\u2225x\u22252 Cd \u2264 sin \u2220(f(x, n + 1), f(x, n)) \u2264 maxk x2\nk\n\u2225x\u22252\nCd\n19\nwhere limd\u2192\u221e Cd \u2248\n\u001a(log b + log \u03b2)\u22121 if f = f RoP E+ABF\n\u03b1(log b)\u22121 if f = f RoP E+P I\nunder the assumptions of \u03b1 \u226a 1\nand b \u226b 1.\nProof. Let us begin the proof by writing down the expressions for the inner product between two\nimages of RoPE variants.\n\u27e8f RoP E+P I(x, m), f RoP E+P I(x, n)\u27e9 = P d\n2 \u22121\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\neib\u2212 2j\nd \u03b1(m\u2212n)\n\u27e8f RoP E+ABF (x, m), f RoP E+ABF (x, n)\u27e9 = P d\n2 \u22121\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\neib\u2212 2j\nd \u03b2\u2212 2j\nd (m\u2212n)\nFrom them, we can derive the expressions for the Euclidean sine similarity between the images of the\npositional embeddings:\nsin \u2220(f RoP E+P I(x, m), f RoP E+P I(x, n)) =\nP d\n2 \u22121\nj=0 (x2\n2j+x2\n2j+1) sin(b\u2212 2j\nd \u03b1(m\u2212n))\nPd\u22121\nj=0 x2\nj\nsin \u2220(f RoP E+ABF (x, m), f RoP E+ABF (x, n)) =\nP d\n2 \u22121\nj=0 (x2\n2j+x2\n2j+1) sin(b\u2212 2j\nd \u03b2\u2212 2j\nd (m\u2212n))\nPd\u22121\nj=0 x2\nj\nLet\u2019s put m = n + 1 to compare the distance between the two consecutive positional embedding\nimages of the same vector x.\n\u2225x\u22252 sin \u2220(f RoP E+P I(x, n + 1), f RoP E+P I(x, n)) = P d\n2 \u22121\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\nsin(b\u2212 2j\nd \u03b1)\n\u2225x\u22252 sin \u2220(f RoP E+ABF (x, n + 1), f RoP E+ABF (x, n)) = P d\n2 \u22121\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\nsin(b\u2212 2j\nd \u03b2\u2212 2j\nd )\nDue to the range of b, \u03b1 and \u03b2 that is typically considered, we can bound the arguments of the sine\nfunctions as 0 < \u03b1b\u2212 2j\nd \u2264 1 as well as 0 < (\u03b2b)\u2212 2j\nd \u2264 1. Using that, we derive that sin(b\u2212 2j\nd \u03b2\u2212 2j\nd )\nand sin(b\u2212 2j\nd \u03b1) are non-negative as well as x2\nj for any j \u2208 {1, . . . d}. Thus, the following inequalities\nhold:\nd\n2 \u22121\nX\nj=0\nmin\nk x2\nk sin(b\u2212 2j\nd \u03b2\u2212 2j\nd ) \u2264\nd\n2 \u22121\nX\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\nsin(b\u2212 2j\nd \u03b2\u2212 2j\nd ) \u2264\nd\n2 \u22121\nX\nj=0\nmax\nk\nx2\nk sin(b\u2212 2j\nd \u03b2\u2212 2j\nd ),\nd\n2 \u22121\nX\nj=0\nmin\nk x2\nk sin(b\u2212 2j\nd \u03b1) \u2264\nd\n2 \u22121\nX\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\nsin(b\u2212 2j\nd \u03b1) \u2264\nd\n2 \u22121\nX\nj=0\nmax\nk\nx2\nk sin(b\u2212 2j\nd \u03b1).\nCarrying mink x2\nk and maxk x2\nk out of the summation signs, we obtain\nmin\nk x2\nk\nd\n2 \u22121\nX\nj=0\nsin(b\u2212 2j\nd \u03b2\u2212 2j\nd ) \u2264\nd\n2 \u22121\nX\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\nsin(b\u2212 2j\nd \u03b2\u2212 2j\nd ) \u2264 max\nk\nx2\nk\nd\n2 \u22121\nX\nj=0\nsin(b\u2212 2j\nd \u03b2\u2212 2j\nd ),\nmin\nk x2\nk\nd\n2 \u22121\nX\nj=0\nsin(b\u2212 2j\nd \u03b1) \u2264\nd\n2 \u22121\nX\nj=0\n\u0000x2\n2j + x2\n2j+1\n\u0001\nsin(b\u2212 2j\nd \u03b1) \u2264 max\nk\nx2\nk\nd\n2 \u22121\nX\nj=0\nsin(b\u2212 2j\nd \u03b1).\nIntroducing CABF\nd\n= P d\n2 \u22121\nj=0 sin(b\u2212 2j\nd \u03b2\u2212 2j\nd ) and CP I\nd\n= P d\n2 \u22121\nj=0 sin(b\u2212 2j\nd \u03b1) proves the first part of\nthe Theorem:\nmink x2\nk\n\u2225x\u22252\nCABF\nd\n\u2264 sin \u2220(f RoP E+ABF (x, n + 1), f RoP E+ABF (x, n)) \u2264 maxk x2\nk\n\u2225x\u22252\nCABF\nd\n,\nmink x2\nk\n\u2225x\u22252\nCP I\nd\n\u2264 sin \u2220(f RoP E+P I(x, n + 1), f RoP E+P I(x, n)) \u2264 maxk x2\nk\n\u2225x\u22252\nCP I\nd .\n20\nNow, considering the limit of Cd, we notice that due to the inequalities on the arguments of the sines,\nthe following bounds hold:\n(b\u03b2)\u2212 2j\nd\n\u0010\n1 \u2212 (b\u03b2)\u2212 2j\nd /\u03c0\n\u0011\n\u2264 sin(b\u2212 2j\nd \u03b2\u2212 2j\nd ) \u2264 (b\u03b2)\u2212 2j\nd ,\n\u03b1b\u2212 2j\nd\n\u0010\n1 \u2212 \u03b1b\u2212 2j\nd /\u03c0\n\u0011\n\u2264 sin(b\u2212 2j\nd \u03b1) \u2264 \u03b1b\u2212 2j\nd\nUsing the formula of geometric sums and a corollary of the exponential (second) foundational limit,\nwe establish the limits of the sums of these bounds as d \u2192 \u221e:\nd\n2 \u22121\nX\nj=0\n\u03b1b\u2212 2j\nd = \u03b1(b \u2212 1)b2/d\nb2/d+1 \u2212 b\n\u2192 \u03b1 b \u2212 1\nb log b as d \u2192 \u221e\nd\n2 \u22121\nX\nj=0\n\u03b12b\u2212 4j\nd = \u03b12(b2 \u2212 1)b4/d\nb4/d+2 \u2212 b2\n\u2192 \u03b12 b2 \u2212 1\nb2 log b as d \u2192 \u221e\nd\n2 \u22121\nX\nj=0\n(b\u03b2)\u2212 2j\nd = (b\u03b2 \u2212 1)(b\u03b2)2/d\n(b\u03b2)2/d+1 \u2212 b\u03b2 \u2192\n(b\u03b2) \u2212 1\n(b\u03b2) log(b\u03b2) as d \u2192 \u221e\nd\n2 \u22121\nX\nj=0\n(b\u03b2)\u2212 4j\nd = (b2\u03b22 \u2212 1)(b\u03b2)4/d\n(b\u03b2)4/d+2 \u2212 b2\u03b22 \u2192\n(b\u03b2)2 \u2212 1\n(b\u03b2)2 log(b\u03b2) as d \u2192 \u221e\nSubstituting these into the bounds on limd\u2192\u221e Cd, one achieves:\n(log b + log \u03b2)\u22121\n\u0012(b\u03b2) \u2212 1\n(b\u03b2)\n\u2212 (b\u03b2)2 \u2212 1\n\u03c0(b\u03b2)2\n\u0013\n\u2264 lim\nd\u2192\u221e CABF\nd\n\u2264 (log b + log \u03b2)\u22121 (b\u03b2) \u2212 1\n(b\u03b2)\n,\n\u03b1(log b)\u22121\n\u0012b \u2212 1\nb\n\u2212 \u03b1\n\u03c0\nb2 \u2212 1\nb2\n\u0013\n\u2264 lim\nd\u2192\u221e CP I\nd\n\u2264 \u03b1(log b)\u22121 b \u2212 1\nb\nFrom these bounds, one can see that in the setting considered within this paper, where b = 10000 and\n\u03b1 < 1/4, the approximation of limd\u2192\u221e Cd used in the statement of the Theorem is of a high quality.\nBased on this theoretical derivation, we return to the interpretation of our experimental resuts. On\none hand, the experiments have shown that the model can adapt to the new sequence length with\nboth RoPE PI (\u03b1 = 1/4 or \u03b1 = 1/8) and RoPE ABF (\u03b2 = 50). Thus, we can conclude that\nthe chosen hyperparameters provide a sufficient degree of approximation of RoPE images under\nb = 10000. In other words, both d(f, f RoP E+ABF ) and d(f, f RoP E+P I) are small enough to allow\nrapid adaptation. On the other hand, comparing the expressions of Cd for RoPE ABF and RoPE\nPI, we can observe that for the values of \u03b1 = 1\n4 or \u03b1 = 1\n8 and b = 10000 that were used in our\nexperiments, the granularity (the distance between two consecutive images of RoPE) is much lower\nfor the RoPE PI (\u03b1(log b)\u22121 \u2248 0.027) than for RoPE ABF ((log b+log \u03b2)\u22121 \u2248 0.076) with \u03b2 = 50.\nWe further hypothesise that the higher degree of granularity is related to the higher evaluation on\nthe downstream tasks of the RoPE ABF variant compared to RoPE PI because it makes the task of\ndistinguishing between the positional embedding images simpler for the model. In other words, this\ncorresponds to the case of q(f RoP E+ABF ) > q(f RoP E+P I).\nThroughout this consideration we implicitly assumed that the distance between the consecutive\nimages of an embedding is smaller than the distance between any other pair of the images. While this\nassumption is likely to hold true in a high-dimensional space, significantly increasing the parameter\nof \u03b2 in RoPE ABF may violate this assumption due to the changed geometry of the embedding curve.\n21\n0\n5000\n10000\n15000\n20000\n25000\n30000\nPosition\n1.6\n1.8\n2.0\n2.2\n2.4\nCross Entropy\nInterpolation (16k)\nExtrapolation (32k)\nRoPE ABF\nxPos ABF\nLlama 2\n(a) Validation loss calculated at each position of 32,768 context window.\n256 1k\n2k\n4k\n8k 10k 12k 14k 16k 20k 24k 28k 30k\nTask length\n0\n20\n40\n60\n80\n100\nROUGE-L\nInterpolation (16k)\nRoPE ABF\nxPos ABF\n(b) Context window probing with FIRST-SENTENCE-RETRIEVAL task.\nFigure 9: Evaluation on our 70B model\u2019s extrapolation capabilities.\nC\nLength Extrapolation Results\nDespite not the focus of this work, extrapolation is an important property for long context models.\nExtrapolation refers to a model\u2019s ability to conduct inference on input sequences that are longer than\nits training sequences. We evaluate how our 70B model extrapolates with two tasks:\n\u2022 Validation loss at each position: In Figure 9a, we visualize the average loss at each position\nof the 32,768 sequence length where the first 16,384 is the interpolation area (within training\nsequence length) and the second half is extrapolation. We use 50 batches of samples and\naverage across them. To make plots smoother, we also take the mean of losses every 500\npositions. As we can see, our 70B model with either ROPE ABF or XPOS ABF maintain\nthe loss in the extrapolation area. To contrast this, we also plot the result for LLAMA 2 with\n4,096 context window: the loss explodes after the position goes beyond training sequence\nlength, which suggests that LLAMA 2 does not extrapolate effectively.\n\u2022 Synthetic FIRST-SENTENCE-RETRIEVAL task: To complement validation loss evaluation,\nwe also test our 70B model with two different PEs on the context probing task. Unlike\nvalidation loss task where it is hard to find data samples that require very long range\ndependencies consistently, FIRST-SENTENCE-RETRIEVAL imposes a very strict requirement\nfor models to attend with a specific length. In Figure 9b, we visualize the results up to 32,768\nwhere we do see some performance degradation when the model needs to extrapolate. In\naddition, we observe that, despite often considered as having better extrapolation properties,\nXPOS ABF does not outperform ROPE ABF in our setting.\nD\nSelf-Instruct Data\nAs described in Section 4.3, we use LLAMA 2 CHAT to bootstrap self-instruct data for instruct\nfinetuning. In this section we describe the detailed procedure as well as providing the necessary\nprompts used for generating this dataset. The main challenge is that we need an automated process of\ngenerating long context instruct data with only short context models at hand. The core idea behind\nthis is to split the long documents into chunks of texts that can fit into short model\u2019s context and apply\nself-instruct. We focus primarily on question answering dataset. We first split the long document\ninto smaller chunks, and for each chunk we construct a prompt as in Figure 10 which gets fed into\nLLAMA 2 CHAT to get a question-answer pair. To diversify the question types, we randomly choose\nbetween the two prompts that ask for either normal or short answers. Once we extract the question\nand answer from the response (using tags as required by the prompt), we can construct long question\nanswering instruct data together with the original long document, using the templates in Figure 11 of\nthe corresponding answer type.\n22\nNormal Answer Prompt:\n[INST] You are given a text chunk (delimited by triple quotes) taken from a long\ntext. Write a question about this text and provide the correct answer. The answer\nneeds to be based on the text. This question will later be used as a reading\ncomprehension test over the entire document. Wrap the question and answer using\nXML tags (<question> and </question>, <answer> and </answer>).\n\"\"\"\n{TEXT_CHUNK}\n\"\"\"\n[/INST]\nShort Answer Prompt:\n[INST] You are given a text chunk (delimited by triple quotes) from a long\ndocument. Based on information from the text, come up with a specific question\n**which can be answered in a few words or a single phrase** and provide the\ncorrect answer without explanation. The answer needs to be based on the text.\nThis question will later be used as a reading comprehension test over the\nentire document. Wrap the question and answer using XML tags (<question>\nand </question>, <answer> and </answer>). Again, the answer needs to be short.\n\"\"\"\n{TEXT_CHUNK}\n\"\"\"\n[/INST]\nFigure 10: Prompts used for generating question and answer pairs by boostrapping LLAMA 2 CHAT.\nWe split the long documents into chunks and feed each chunk into one of the prompts with equal\nprobability. We prompt the models to wrap the answer with XML tags, which enables more accurate\nanswer extraction.\nNormal Answer Data Template:\n[INST] You are given a long text (delimited by triple quotes) and a question.\nRead the text and answer the question in the end.\n\"\"\"\n{FULL_DOCUMENT}\n\"\"\"\nQuestion: {QUESTION}\n[/INST]\n{ANSWER}\nShort Answer Data Template:\n[INST] You are given a long text (delimited by triple quotes) and a question.\nRead the text and answer the question in the end as concisely as you can,\nusing a single phrase or sentence if possible. Do not provide any explanation.\n\"\"\"\n{FULL_DOCUMENT}\n\"\"\"\nQuestion: {QUESTION}\n[/INST]\n{ANSWER}\nFigure 11: Data templates for constructing long question-answer data. The question and answer pair\nis extracted from the response of LLAMA 2 CHAT.\n23\n"
  },
  {
    "title": "Deep Geometrized Cartoon Line Inbetweening",
    "link": "https://arxiv.org/pdf/2309.16643.pdf",
    "upvote": "23",
    "text": "Deep Geometrized Cartoon Line Inbetweening\nLi Siyao1\nTianpei Gu2\u2217\nWeiye Xiao3\nHenghui Ding1\nZiwei Liu1\nChen Change Loy1 B\n1S-Lab, Nanyang Technological University\n2Lexica\n3Southeast University\n{siyao002, henghui.ding, ziwei.liu, ccloy}@ntu.edu.sg, gutianpei@ucla.edu, 230189776@seu.edu.cn\nSource frame 0\nSource frame 1\nInbetweened (      ) by AnimeInbet (ours)\n\u21e54\nVFIformer\nEISAI\nFILM\nRIFE\nFigure 1: Inbetweening on two source cartoon line drawings of Monkey D. Luffy extracted from ONE PIECE. We compare our\nproposed AnimeInbet with state-of-the-art frame interpolation methods VFIformer [14], EISAI [5], FILM [23] and RIFE [6].\nAbstract\nWe aim to address a significant but understudied problem\nin the anime industry, namely the inbetweening of cartoon\nline drawings. Inbetweening involves generating interme-\ndiate frames between two black-and-white line drawings\nand is a time-consuming and expensive process that can\nbenefit from automation. However, existing frame interpo-\nlation methods that rely on matching and warping whole\nraster images are unsuitable for line inbetweening and of-\nten produce blurring artifacts that damage the intricate line\nstructures. To preserve the precision and detail of the line\ndrawings, we propose a new approach, AnimeInbet, which\ngeometrizes raster line drawings into graphs of endpoints\nand reframes the inbetweening task as a graph fusion prob-\nlem with vertex repositioning. Our method can effectively\ncapture the sparsity and unique structure of line drawings\nwhile preserving the details during inbetweening. This is\nmade possible via our novel modules, i.e., vertex geomet-\nric embedding, a vertex correspondence Transformer, an\neffective mechanism for vertex repositioning and a visibil-\nity predictor. To train our method, we introduce Mixamo-\nLine240, a new dataset of line drawings with ground truth\nvectorization and matching labels. Our experiments demon-\nstrate that AnimeInbet synthesizes high-quality, clean, and\nB Corresponding author. \u2217Work completed at UCLA.\ncomplete intermediate line drawings, outperforming existing\nmethods quantitatively and qualitatively, especially in cases\nwith large motions. Data and code are available at https:\n//github.com/lisiyao21/AnimeInbet.\n1. Introduction\nCartoon animation has undergone significant transfor-\nmations since its inception in the early 1900s, when con-\nsecutive frames were manually drawn on paper. Although\nautomated techniques now exist to assist with some specific\nprocedures during animation production, such as coloriza-\ntion [22, 32, 10, 39, 4] and special effects [38], the core\nelement \u2013 the line drawings of characters \u2013 still needs hand-\ndrawing each frame individually, making 2D animation a\nlabor-intensive industry. Developing an automated algorithm\nthat can produce intermediate line drawings from two input\nkey frames, commonly referred to as \u201cinbetweening\u201d, has\nthe potential to significantly improve productivity.\nLine inbetweening is not a trivial subset of general frame\ninterpolation, as the structure of line drawings is extremely\nsparse. Unlike full-textured images, line drawings contain\nonly around 3% black pixels, with the rest of the image being\nwhite background. As illustrated in Figure 2, this poses two\nsignificant challenges for existing raster-image-based frame\ninterpolation methods. 1) The lack of texture in line drawings\nmakes it challenging to compute pixel-wise correspondence\narXiv:2309.16643v1  [cs.CV]  28 Sep 2023\nwarped\nrepositioned\nI0\nI1\nG0\nG1\nFigure 2: Raster vs geometrized inbetweening. Top: search\nspace of a pixel (left) vs a vertex (right) in matching. Bottom: pixel\nwarping/sampling (left) vs vertex repositioning (right).\naccurately in frame interpolation. One pixel can have many\nsimilar matching candidates, leading to inaccurate motion\nprediction. 2) The warping and blending used in frame\ninterpolation can blur the salient boundaries between the line\nand the background, leading to a significant loss of detail.\nTo address the challenges posed by line inbetweening,\nwe propose a novel deep learning framework called Ani-\nmeInbet, which inbetweens line drawings in a geometrized\nformat instead of raster images. Specifically, the source im-\nages are transformed into vector graphs, and the goal is to\nsynthesize an intermediate graph. This reformulation can\novercome the challenges discussed earlier in this paper. As\nillustrated in Figure 2, the matching process in the geometric\ndomain is conducted on concentrated geometric endpoint\nvertices, rather than all pixels, reducing potential ambiguity\nand leading to more accurate correspondence. Moreover, the\nrepositioning does not change the topology of the line draw-\nings, enabling preservation of the intricate and meticulous\nline structures. Compared to existing methods, our proposed\nAnimeInbet framework can generate clean and complete in-\ntermediate line drawings, as demonstrated in Figure 1.\nThe core idea of our proposed AnimeInbet framework is\nto find matching vertices between two input line drawing\ngraphs and then reposition them to create a new intermediate\ngraph. To achieve this, we first design a vertex encoding strat-\negy that embeds the geometric features of the endpoints of\nsparse line drawings, making them distinguishable from one\nanother. We then apply a vertex correspondence Transformer\nto match the endpoints between the two input line drawings.\nNext, we propagate the shift vectors of the matched ver-\ntices to unmatched ones based on the similarities of their\naggregated features to realize repositioning for all endpoints.\nFinally, we predict a visibility mask to erase the vertices and\nedges occluded in the inbetweened frame, ensuring a clean\nand complete intermediate frame.\nTo facilitate supervised training on vertex correspondence,\nwe introduce MixamoLine240, the first line art dataset with\nground truth geometrization and vertex matching labels. The\nAnchor frame\nDistant frame\nAnchor 3D mesh\nFigure 3: Geometrized line art in MixamoLine240. 2D end-\npoints and connected lines are projected from vertices and edges\nof orinal 3D mesh. Endpoints indexed to unique 3D vertices are\nmatched (marked in the same colors).\n2D line drawings in our dataset are selectively rendered from\nspecific edges of a 3D model, with the endpoints indexed\nfrom the corresponding 3D vertices. By using 3D vertices as\nreference points, we ensure that the vertex matching labels\nin our dataset are accurate and consistent at the vertex level.\nIn a conclusion, our work contributes a new and challeng-\ning task of line inbetweening, which could facilitate one of\nthe most labor-intensive art production processes. We also\npropose a new method that outperforms existing solutions,\nand introduce a new dataset for comprehensive training.\n2. Related Work\nFrame Interpolation. Frame interpolation is a widely stud-\nied task in recent years, involving synthesizing intermediate\nframes from existing ones. Many approaches have been\nproposed [13, 19, 20, 7, 17, 34, 18, 21, 26, 6, 23, 5, 14, 11],\nsuch as those that use optical flows or deep networks to\nsearch for matching areas and warp them to proper interme-\ndiate locations. Among the most recent algorithms, RIFE\n[6] directly predicts intermediate flows to warp the input\nframes and blends the warped frames into intermediate ones\nby a visible mask. VFIformer [14] adopts the same idea to\npredict the intermediate flows but proposes a Transformer to\nsynthesize the intermediate from both warped images and\nfeatures. Reda et al. [23] design a scale-agnostic feature\npyramid to predict the intermediate flows and warp frames\nin a hierarchical manner to handle extreme large motions.\nSiyao and Zhao et al. [30] propose a frame interpolation\npipeline specific for 2D cartoon in the wild, while Chen and\nZwicker [5] improves the perceptual quality by embedding\nan optical-flow based line aggregator. While these methods\nachieve impressive performance on raster natural or cartoon\nvideos, their pixel-oriented nature are not suitable for inbe-\ntweening concise and sparse line arts, which can yield severe\nartifacts and are not feasible for real usage in anime creation.\nResearch on Anime.\nThere has been increasing re-\nsearch interest in techniques to facilitate 2D cartoon cre-\nation, including sketch simplification [28, 27], vectorization\nGanful\nUreil A Plotexia\nKnight\nKachujin\nMaria\nTimmy\nJoleen Girl scout Peasant girl Michelle\nJones\nAj\nAbe\nJames\nMaw\nSwat\nChad\nMorak\nXbot\nWarrok\nTrain\nbreak dance\ncapoeira\nchap giratoria\nfist fight\nflying knee\nactions\nclimb\nrun\nshove\nmagic attack\ntripping\nTest\nchip\nevade\nflair\nsword slash\nhip hop\nactions hurricane kick soccer tackle standing death\nswim\nstand up\nFigure 4: Data composition. Training and test sets are separately\ncomposed by 10 characters \u00d7 10 actions. First & second rows are\ntraining & test characters, respectively. Shaded are for validation.\n[40, 36, 15, 12], colorization [22, 32, 10, 39, 4], shading\n[38], head reenactment [8] and line-art-based cartoon gener-\nation [37]. While these studies may improve specific aspects\nof animation creation, the core line arts still rely on manual\nframe-by-frame drawing. Some sporadic rule-based methods\nhave been developed for stroke inbetweening under strict\nconditions, but these methods lack the flexibility required for\nwider applications [35, 3]. Our work is the first to propose a\ndeep learning-based method for inbetweening geometrized\nline arts. Additionally, we introduce vertex-wise correspon-\ndence datasets on line arts. It is noteworthy that existing\ndatasets are not sufficiently \u2018clean\u2019 for our task since cartoon\ncontour lines can cross the boundaries of motion, leading to\nincorrect corresponding labels at the vertex level [25, 29].\n3. Mixamo Line Art Dataset\nTo facilitate training and evaluation of geometrized line\ninbetweening, we develop a large-scale dataset, named Mix-\namoLine240, which consists of 240 sequences of consecutive\nline drawing frames, with 100 sequences for training and 140\nfor validation and testing. To obtain this vast amount of car-\ntoon line data, we utilize a \u201cCel-shading\u201d technique, i.e., to\nuse computer graphics software (Blender in this work) to ren-\nder 3D resources into an anime-style appearance that mimics\nthe hand-drawn artistry. Unlike previous works [25, 29] that\nonly provide raster images, MixamoLine240 also provides\nground-truth geometrization labels for each frame, which\ninclude the coordinates of a group of vertices (V ) and the\nconnection topology (T). Additionally, we assign an index\nnumber (R[i]) to each 2D endpoint (V [i]) that refers to a\nunique vertex in the 3D mesh of the character, as illustrated\nin Figure 3, which can be further used to deduce the vertex-\nlevel correspondence. Specifically, given two frames I0 and\nTable 1: Difficulty statistics with various frame gaps.\nFrame gap\u2192\n0 (60 fps) 1 (30 fps) 5 (10 fps) 9 (6 fps)\nTrain\nOcclusion rate (%)\n14.8\n21.5\n37.8\n46.6\nAvg. vtx shift\n8.6\n16.4\n42.6\n62.8\nAvg. max vtx shift\n26.0\n48.9\n129.7\n192.3\nTest\nOcclusion rate (%)\n18.4\n26.5\n44.2\n53.5\nAvg. vtx shift\n7.8\n14.9\n38.9\n57.0\nAvg. max vtx shift\n23.8\n45.0\n119.3\n173.5\nI1 in a sequence, the 3D reference IDs reveal the vertex\ncorrespondence {(i, j)} for those vertices i in I0 and j in\nI1 having R0[i] = R1[j], while the rest unmatched vertices\nare marked as occluded. This strategy allows us to produce\ncorrespondence pairs with arbitrary frame gaps to flexibly\nadjust the input frame rate during training. Next, we discuss\nthe construction and challenges inherent in the data.\nData Construction. In Blender, the mesh structure of a\n3D character remains stable, i.e., the number of 3D vertex\nand the edge topology keep constant, when moving without\nadditional subdivision modifier. We employ this property\nto achieve consistent line art rendering and accurate anno-\ntations for geometrization and vertex matching. As shown\nin Figure 3, the original 3D mesh contains all the necessary\nline segments required to represent the character in line art.\nDuring rendering, the visible outline from the camera\u2019s per-\nspective is selected based on the material boundary and the\nobject\u2019s edge. This process ensures that every line segment\nin the resulting raster image corresponds to an edge in the\noriginal mesh. The 2D endpoints of each line segment are\nsimply the relevant 3D vertices projected onto the camera\nplane, referenced by the unique and consistent index of the\ncorresponding 3D vertex. Meanwhile, since the 3D mesh\nnaturally defines the vertex connections, the topology of the\n2D lines can be transferred from the selective edges used for\nrendering. To prevent any topological ambiguity that may\nbe caused by overlapped vertices in 3D space, we merge the\nendpoints that are within a Euclidean distance of 0.1 in the\nprojected 2D space. This enables us to obtain both the raster\nline drawings and the accurate labels of each frame.\nTo create a diverse dataset, we used the open-source 3D\nmaterial library Mixamo [1] and selected 20 characters and\n20 actions, as shown in Figure 4. Each action has an average\nof 191 frames. We combined 10 characters and 10 actions\nto render 100 sequences, with a total of 19,930 frames as\nthe training set. We then used the remaining 10 characters\nand 10 actions to render an 18,230-frame test set, ensuring\nthat the training and testing partitions are exclusive. We\nalso created a 44-sequence validation set, consisting of 20\nunseen characters, 20 unseen actions, and 4 with both unseen\ncharacter and action. To create this set, we combined the test\ncharacters \u201cSwat\u201dand \u201cWarrok\u201d and actions \u201csword slash\u201d\nand \u201chip hop\u201d with the training characters and actions. The\nReposition \nPropagate\nVertex\nGeometric\nEmbedding\nVertex Corr.\nTransformer\nGraph\nFusion\nI0, I1\nSource images G0 = (V0, T0)\nG1 = (V1, T1)\nF0\nF1\nK0 \u21e5 C\nK1 \u21e5 C\nVertex Encoding\n\u02c6F0\n\u02c6F1\nC\nCorr. Matrix\nK0 \u21e5 K1\nr0\nr1\nVertex Reposition\nK0 \u21e5 2\nK1 \u21e5 2\nVisibility Maps\nm1\nm0\nG0, G1\nGt\nInbetween frame\nFigure 5: Pipeline of proposed AnimeInbet. Our framework is composed of four main parts: the vertex geometric embedding, the vertex\ncorrespondence Transformer, repositioning propagation and graph fusion. Given a pair of line images I0 and I1 and their vector graphs G0\nand G1, our method generates the intermediate frame Gt in geometrized format.\nvalidation set contains 11,102 frames and was also rendered\nat 1080p resolution with a frame rate of 60 fps. To ensure\nconsistency across all frames, we cropped and resized each\nframe to a unified 720 \u00d7 720 character-centered image.\nChallenges. Table 1 summarizes the statistics that reflect\nthe difficulty of the line inbetweening task under various\ninput frame rates. With an increase in frame gaps, the inbe-\ntweening task becomes more challenging with larger motion\nmagnitudes and higher occlusion percentages. For instance,\nwhen the frame gap is 9, the input frame rate becomes 6 fps,\nand the average vertex shift is 62.8 pixels. The mean value of\nthe maximum vertex shift in a frame (\u201cAvg. max vtx shift\u201d)\nreaches 192.3 pixels, which is 27% of the image width. Ad-\nditionally, nearly half of the vertices are unmatched in such\ncases, making line inbetweening a tough problem. Further-\nmore, the image composition of the test set is more complex\nthan that of the training set. A training frame has an average\nof 1,256 vertices and 1,753 edges, while a test frame has an\naverage of 1,512 vertices and 2,099 edges since the test set\nhas more complex characters such as \u201cMaw\u201d.\n4. Our Approach\nAn overview of the proposed line inbetweening frame-\nwork, AnimeInbet, is depicted in Figure 5. Unlike exist-\ning frame interpolation methods that use raw raster images\nI0 and I1, we process vector graphs G0 = {V0, T0} and\nG1 = {V1, T1} instead. The vertex coordinates in the im-\nages are represented by V \u2208 RK\u00d72, and the binary adja-\ncency matrix is denoted by T \u2208 0, 1K\u00d7K, where K denotes\nthe number of vertices. The goal is to generate the interme-\ndiate graph Gt at time t \u2208 (0, 1). To this end, we first design\na CNN-based vertex geometric embedding to encode V0 and\nV1 to features F0 and F1 , respectively, as detailed in Section\n4.1. Along with the embeddings, a vertex correspondence\nTransformer is proposed to aggregate the mutuality of vertex\nfeatures to \u02c6F0 and \u02c6F1 by alternating self- and cross-attention\nlayers (Section 4.2). The aggregated features are used to\ncompute the correlation matrix C \u2208 RK0\u00d7K1 and to induce\nthe vertex matching by row-wise and column-wise argmax.\nRGB\nVertices\n+\nPositional\nEmbedding\nVertex Encoding\nImage\nEncoder\nH \u21e5 W \u21e5 C\nH \u21e5 W \u21e5 3\nPositional\nEncoder\nK0 \u21e5 2\n(x, y)\nIndexing\nI0\nV0\nK0 \u21e5 C\nAdj.\nMatrix     \nT0\nK0 \u21e5 C\nSpectral\nEmbedding\n(x, y)\nK0 \u21e5 C\nTopo.\nEncoder\nTopological\nEmbedding\nG0\nEI\nEP\nET\nF0\nContextual\nEmbedding\nK0 \u21e5 K0\nFigure 6: Vertex Geometric Embedding. The goal is to obtain\ndiscriminative and meaningful features to describe each vertex.\nIn cases where vertices are occluded during large motion,\nwe adopt a self-attention-based layer to propagate the vertex\nshifts from matched vertices to the unmatched, and obtain\nrepositioning vectors r0 \u2208 RK0\u00d72 and r1 \u2208 RK1\u00d72 for\nall vertices (Section 4.3). Finally, we superpose the two\ninput graphs based on the predicted correspondence, and\nwe further refine the output by predicting visibility maps\nm0 \u2208 {0, 1}K0 and m1 \u2208 {0, 1}K1 to mask off those ver-\ntices of V0 and V1 that disappear in the intermediate frame,\nrespectively, to obtain the final inbetweened line drawing Gt,\nas explained in Section 4.5.\nGeometrizing Line Drawings. The process of creating art-\nwork has become largely digital, allowing for direct export in\nvectorized format. However, for line drawings that only ap-\npear in raster images, there are various commercial software\nand open-source research projects available [40, 36, 15, 12]\nthat can be used to convert the raster images into the required\nvectorized input format. We will ablate the performance of\nline vectorization in our experiments.\nSA\nSA\nCA\nF1\nF0\nF (i)\n0\nF (i)\n1\nCA\n\u02c6F (i)\n1\nCA(F (i)\n0 , F (i)\n1 )\nCA(F (i)\n1 , F (i)\n0 )\n\u21e5N\n\u02c6F (i)\n0\n\u02c6F0\n\u02c6F1\nFigure 7: Vertex Correspondence Transformer. SA and CA\nrepresent self-attention and cross-attention, respectively.\n4.1. Vertex Geometric Embedding\nDiscriminative features for each vertex are desired to\nachieve accurate graph matching. Line graphs are different\nfrom general graphs as the spatial position of endpoint ver-\ntices, in addition to the topology of connections, determines\nthe geometric shape of the line. The geometric graph embed-\nding for line art is hence designed to comprise three parts:\n1) image contextual embedding, 2) positional embedding,\nand 3) topological embedding, as shown in Figure 6.\nFor image contextual embedding, we use a 2D CNN EI to\nextract deep contextual features within the same size of the\ninput raster image I. Then, for each vertex V0[i] := (x, y)\nwe index feature EI(I) [(x, y)] as the image embedding for\nthe i-th vertex. As to the positional embedding, we employ\na 1D CNN EP to map each vertex coordinate (x, y) to a\nC-dimensional feature. To include the topological infor-\nmation into a lower dimensional feature, we first conduct\nspectral embedding [2] S on the binary adjacency matrix T,\nwhich involves an eigenvector decomposition on the Lapla-\ncian matrix of the graph, then feed the spectral embedding\nto a subsequent 1D CNN ET . The final geometric graph\nembedding is formulated as\nF0 = EI (I0) [V0] + EP (V0) + ET (S (T0)) .\n(1)\nWe obtain F1 in the same way.\n4.2. Vertex Correspondence Transformer\nWe use geometric features F0 and F1 to establish a vertex-\nwise correspondence between G0 and G1. Specifically, we\ncompute a correlation matrix between vertex features and\nidentify the matching pair as those with the highest value\nacross both the row and the column of the matrix. Prior to\nthis step, we apply a Transformer that aggregates the mutual\nconsistency both intra- and inter-graph.\nMutual Aggregation. Following [24, 31], we employ a cas-\ncade of alternating self- and cross-attention layers to aggre-\ngate the vertex feature. In a self-attention layer, all queries,\nkeys and values are derived from the single source feature,\nSA(F0) = softmax\n\u0012Q(F0)KT (F0)\n\u221a\nC\n\u0013\nV(F0),\n(2)\nwhere Q, K and V represent MLPs for query, key and value,\nrespectively; while in the cross-attention layer, the keys and\nvalues are computed from another feature:\nCA(F0, F1) = softmax\n\u0012Q(F0)KT (F1)\n\u221a\nC\n\u0013\nV(F1).\n(3)\nAfter N layers of rotating self- and cross-attention layers as\nshown in Figure 7, we obtain aggregated feature \u02c6F0 and \u02c6F1.\nIn the aggregation, each vertex is represented as an atten-\ntional pooling of all other vertices within the same graph and\nacross the two graphs achieving a full fusion of information\nwith mutual dependencies.\nCorrelation Matrix and Vertex Matching. We compute\nthe correlation matrix P as P =\n\u02c6\nF0 \u02c6\nF T\n1\n\u221a\nC . We further apply\na differentiable optimal transport (OT) [24] to improve the\ndual selection consistency and obtain \u02c6P = OT(P). Then,\nwe predict the one-way matching from G0 to G1 and vice\nversa as arg max indices across rows and columns:\n\u001a M0\u21921 = {(i, j)|j = arg max \u02c6Pi,:, i = 0, ..., K0 \u2212 1}\nM1\u21920 = {(i, j)|i = arg max \u02c6P:,j, j = 0, ..., K1 \u2212 1}.\n(4)\nA vertex pair is selected into the final correspondence if it is\nmutually consistent and its correlation value is larger than \u03b8:\n\u02c6\nM =\nn\n(i, j)|(i, j) \u2208 M0\u21921 \u2229 M1\u21920, \u02c6Pi,j > \u03b8\no\n.\n(5)\nOtherwise, vertices will be considered to be occluded.\n4.3. Repositioning Propagation\nFused vertices (i, j) from vertex correspondence can be\nlinearly relocated to tV0[i] + (1 \u2212 t)V1[j] in intermediate\ngraph Gt based on time t. However, the positions of the\nunmatched vertices in Gt are still unknown. To reposition\nthese vertices, we design an attention-based scheme similar\nto Xu et al. [33] to predict bidirectional shift vectors r0\u21921\nand r1\u21920 for V0 and V1, respectively. Formally,\n\uf8f1\n\uf8f2\n\uf8f3\nr0\u21921 = softmax\n\u0010 \u02c6\nF0 \u02c6\nF T\n0\n\u221a\nC\n\u0011 \u0010\nsoftmax( \u02c6P)V1 \u2212 V0\n\u0011\nr1\u21920 = softmax\n\u0010 \u02c6\nF1 \u02c6\nF T\n1\n\u221a\nC\n\u0011 \u0010\nsoftmax( \u02c6\nPT )V0 \u2212 V1\n\u0011\n.\n(6)\nWe then compute the final repositioning vectors as follows:\nr0[i] =\n\u001a\nV1[j] \u2212 V0[i],\nif \u2203 j s.t. (i, j) \u2208 \u02c6\nM,\nr0\u21921[i],\notherwise,\n(7)\nwhile r1 is computed in a similar way.\nIn this step, the motion vector r0\u21921 of an unmatched\nvertex V0[i] is computed as a softmax average of shifts to\nall vertices in G1, i.e., softmax( \u02c6Pi,:)V1 \u2212 V0. It is then\nrefined by attention pooling from matched vertices, based on\nself-similarity given by \u02c6F0 \u02c6F T\n0 /\n\u221a\nC. Vertices are reasonably\nrepositioned in the new vector graph after this step.\nTable 2: Quantitative evaluations of state-of-the-art frame interpolation methods using Chamfer Distance (reported in units of\n\u00d710\u22125, with lower values indicating better performance). The first place and runner-up are highlighted in bold and underlined, respectively.\nValidation Set\nTest Set\nMethod\ngap = 1\ngap = 5\ngap = 9\nAvg.\ngap = 1\ngap = 5\ngap = 9\nAvg.\nVFIformer [14]\n7.82\n26.04\n50.71\n28.19\n7.62\n27.55\n50.68\n28.62\nRIFE [6]\n5.02\n27.79\n49.81\n27.54\n5.85\n28.91\n51.08\n28.61\nEISAI [5]\n5.66\n27.64\n49.43\n27.57\n6.02\n29.14\n52.36\n29.17\nFILM [23]\n3.18\n16.84\n30.74\n16.92\n3.50\n17.94\n33.51\n18.31\nAnimeInbet (ours)\n2.20\n11.12\n21.27\n11.53\n2.80\n12.69\n23.21\n12.90\nAnimeInbet-VS(ours)\n2.62\n11.43\n22.36\n12.14\n3.44\n13.41\n23.67\n13.51\n4.4. Visibility Prediction and Graph Fusion\nTo handle occlusions in the source line arts, we use a\nthree-layer MLP to predict binary visibility maps m0 and\nm1 for the input graphs, obtained as m0 = MLP( \u02c6F0) and\nm1 = MLP( \u02c6F1). Then, we merge the vertices to Vt in the\ntwo graphs according to the following rule:\nVt =\nn\n(1 \u2212 t)V0[i] + tV1[j]\n\f\f\f (i, j) \u2208 \u02c6\nM\no\n\u222a\nn\nV0[i] + t \u00b7 r0[i]\n\f\f\f i /\u2208 \u02c6\nM, m0[i] = 1\no\n\u222a\nn\nV1[j] + (1 \u2212 t)r1[j]\n\f\f\f j /\u2208 \u02c6\nM, m1[j] = 1\no\n,\n(8)\nwhere we implement the repositioning that is compatible\nwith arbitrary time t \u2208 (0, 1). As to Tt, we union all origi-\nnal connections if both endpoint vectors are both visible in\nGt. Or formally, Tt[ei][ej] = Tt[ej][ei] = 1 if T0[i][j] = 1 or\nT1[i][j] = 1, where (i, j) and (ei,ej) are the vertex indices in\nthe original graph and the merged one.\n4.5. Learning\nThe training objective of AnimeInbet composes of three\nterms: L = Lc + Lr + Lm, where the Lc, Lr and Lm\nare used to supervise the learning of vertex matching\n\u02c6\nM,\nrepositioning vectors r0 and r1, and visibility masks m0 and\nm1, respectively. Lc is to enlarge the correlation values of\nground truth pairs and is defined as:\nLc = \u2212\n1\n|MGT |\nX\n(i,j)\u2208MGT\nlog \u02c6Pi,j,\n(9)\nwhere MGT is the ground truth matching labels. For Lr and\nLm, we regress r0\u21921, r1\u21920, m0, and m1 as follows:\nLr = 1\nK0\n\u2225r0\u21921 \u2212 rGT\n0\u21921\u22251 + 1\nK1\n\u2225r1\u21920 \u2212 rGT\n1\u21920\u22251\nLm = BCEw \u0000\u03c3(m0), mGT\n0\n\u0001\n+ BCEw \u0000\u03c3(m1), mGT\n1\n\u0001\n,\n(10)\nwhere \u03c3 represents the sigmoid function, and BCEw is the bi-\nnary cross-entropy loss with bias weight w. However, since\nthe shift vectors of occluded vertices cannot be obtained\ndirectly by subtracting the matched vertices, we conduct a\nframe-by-frame backtrack to generate pseudo labels to sup-\nport the point-wise supervision of the repositioning vector\nand visibility maps.\nPseudo Labels of Repositioning and Visibility. Assume\nG(0) and G(Z) are the 0-th and the Z-th frames in a training\nsequence, which are used for two input line sources. Al-\nthough there can exist many unmatched vertices in the two\ngraphs when the gap Z is large, the matching rate between\nadjacent frames (gap = 0) is relatively high according to\nTable 1. Based on this, we iteratively backtrack a shift vector\nr(z) from the G(Z) to G(0):\nr(z)[i] =\n\u001a V (z+1)[j] \u2212 V(z)[i] + r(z+1), if i, j is matched\n1\n|Ni|\nP\nk\u2208Ni r(z)[k], otherwise\n(11)\nwhere Ni regards to the neighbors of the i-th vertex in G(z)\nand r(Z) is initialized to be 0. The termination r(0) of the\nbacktrack is regarded as the pseudo repositioning label rGT\n0\u21921.\nAs to the visibility labels, we first deuce rGT\n0\u2192t as above and\ncompute mGT\n0\nas\nmGT\n0\n[i] =\n\u001a\n1, if V0[i] + rGT\n0\u2192t \u2208 eIt,\n0, otherwise,\n(12)\nwhere eIt is It dilated by a 3 \u00d7 3 kernel. rGT\n1\u21920 and mGT\n1\nare\ncomputed in reversed order.\n5. Experiments\nImplementation Details. In the vertex geometric embed-\nding module, the image encoder EI is implemented as a\nthree-layer 2D CNN, while the positional encoder EP and\nthe topological encoder ET are 1D CNNs with a kernel size\nof 1. Encoding feature C is 128 in our experiments. Before\nfeeding vertex coordinates V into EP , V are first normalized\nto the scale between (\u22121, 1); the dimension of the spectral\nembedding feature is 64. Threshold \u03b8 in Equation 5 is 0.2.\nIn both training and evaluation, intermediate time t is 0.5,\nwhich regards the center frame between I0 and I1. The de-\ntailed network structures are provided in the supplementary\nVFIformer\nEISAI\nRIFE\nAnimeInbet (ours)\nFILM\nGround Truth\nI0, I1\nFigure 8: Inbetweening results on MixamoLine240 test set. Examples are arranged from small (top) to large (bottom) motion magnitudes.\nfile. We use Adam [9] optimizer with a learning rate of\n1 \u00d7 10\u22124 to train the AnimeInbet for 70 epochs, where we\nfirst solely supervise the network using the correspondence\nloss Lc for the 50 epochs, and then adopt the full loss L for\nthe rest 20 epochs. Bias weight w in Lm is 0.2. Since vertex\nnumbers differ in frames, we feed one pair of input frames\neach time but adopt gradient accumulation for a mini-batch\nsize of 8. The model is trained with an NVIDIA Tesla V100\nGPU for about five days. During the test, Gt is visualized as\na raster image by cv2.line function with a line width of\n2 pixels. We evaluate our model on both ground truth vector-\nization labels (noted as \u201cAnimeInbet\u201d) and those vectorized\nfrom VirtualSketcher [15] (noted as \u201cAnimeInbet-VS\u201d, to\nsimulate the cases when input anime drawing are vector and\nraster, respectively.\nEvaluation Metric. Following [16, 5], we adopt the cham-\nfer distance (CD) as the evaluation metric, which has been\ninitially introduced to measure the similarity between two\npoint clouds. Formally, CD is computed as:\nCD(It, IGT\nt\n) =\n1\nHWd\nX\n(ItDT(IGT\nt\n) + IGT\nt\nDT(It)),\n(13)\nwhere It and IGT\nt\nare predicted binary lines and ground\ntruth, while H, W and d are image height, width, and a\nsearch diameter [5], respectively. DT denotes the Euclidean\ndistance transform. To transfer predicted raster images into\nbinary sketches, we threshold pixels smaller than 0.99 times\nthe maximum value to 0.\n5.1. Comparison to Existing Methods\nSince there is no existing geometrized line inbetweening\nstudy that we can directly compare our proposed model with,\nwe set several state-of-the-art raster-image-based frame in-\nterpolation methods as baselines, including VFIformer [14],\nRIFE [6], EISAI [5], FILM [23]. Specifically, EISAI is\noriginally intended for 2D animation and embeds an opti-\nFigure 9: Statistics of user study. In the boxplot, triangles and\ncolored lines represent mean and median values, respectively. Cir-\ncles are outliers beyond 1.5\u00d7 interquartile range (3\u03c3 in a normal\ndistribution).\ncal flow-based contour aggregator. We test each model\u2019s\nperformance on frame pairs within frame gaps of 1, 5 and\n9, respectively. For fairness, we finetune each compared\nmethod on the training set of MixiamoLine240 with relative\nframe gaps using a learning rate of 1 \u00d7 10\u22126 for five epochs.\nAs shown in Table 2, our AnimeInbet favorably outper-\nforms all compared methods on both the validation set and\nthe test set of MixamoLine240. On the validation set, our ap-\nproach achieves an average CD value of 11.53, representing\na significant improvement over the best-performing com-\npared method, FILM, with over 30% enhancement. Upon\ncloser inspection, the advantage of AnimeInbet becomes\nmore pronounced as the frame gap increases (0.98, 5.72 and\n9.47 for gaps of 1, 5, and 9, respectively), indicating that\nour method is more robust in handling larger motions. On\nthe test set, our method maintains its lead over the other\ncompared methods, with improvements of 0.70 (20%), 5.25\n(29%), and 10.30 (31%) from the best-performing compared\nmethod FILM for the frame gaps of 1, 5, and 9, respectively.\nGiven that both the characters and actions in the test set\nare new, our method\u2019s superiority on the test set provides\nmore convincing evidence of its advantages over the existing\nframe interpolation methods.\nTo illustrate the advantages of our method, we present\nseveral inbetweening results in Figure 8. We arranged these\nexamples in increasing levels of difficulty from top to bot-\ntom. When the motion is simple, compared methods can\ninterpolate a relatively complete shape of the main body of\nthe drawing. However, they tend to produce strong blurring\n(RIFE) or disappearance (VFIformer, EISAI, and FILM) of\nnoticeable moving compositions (indicated by red arrows).\nIn contrast, our method maintains a concise line structure in\nthese key areas. When the input frames involve the whole\nbody\u2019s movement within large magnitudes, the intermediate\nframes predicted by the compared methods become indistin-\nguishable and patchy, rendering the results invalid for further\nTable 3: Ablation study on vertex encoding.\nEI\nEP\nET\nAcc. (%)\nValid Acc. (%)\nCD (\u2193)\n\u2713\n\u2717\n\u2717\n51.66\n31.01\n12.30\n\u2713\n\u2713\n\u2717\n61.87\n55.62\n11.55\n\u2713\n\u2717\n\u2713\n59.28\n45.45\n11.86\n\u2713\n\u2713\n\u2713\n65.51\n61.28\n11.12\nTable 4: Ablation study on repositioning and visibility mask.\nMethod\nCD (\u2193)\nw/o. repositioning propagation\n23.62\nw/o. visibility mask\n12.81\nfull model\n11.12\nuse. However, our AnimeInbet method can still preserve the\ngeneral shape in the correct positions, even with a partial\nloss of details, which can be easily fixed with minor manual\neffort.\nUser Study. To further evaluate the visual performance\nof our methods, we conduct a user study among 36 partici-\npants. For each participant, we randomly show 60 pairs, each\ncomposed of a result of AnimeInbet and that of a compared\nmethod, and ask the participant to select the better. To allow\nparticipants to take temporal consistency into the decision,\nwe display these results in GIF formats formed by triplets of\ninput frames and the inbetweened one. The winning rates of\nour method are shown in Figure 9, where AnimeInbet wins\nover 92% versus the compared methods. Notably, for \u201cgap\n= 5\u201d and \u201cgap = 9\u201d slots, the winning rates of our methods\nare close to 100% with smaller deviations than \u201cgap = 1\u201d,\nsuggesting the advantages of our method on cases within\nlarge motions.\n5.2. Ablation Study\nEmbedding Features. To investigate the effectiveness of\nthe three types of embeddings mentioned in Section 4.1,\nwe trained several variants by removing the corresponding\nmodules. As shown in Table 3, for each variant, we list the\nmatching accuracy for all vertices (\u201cAcc.\u201d), the accuracy for\nnon-occluded vertices (\u201cValid Acc.\u201d) and the final CD values\nof inbetweening on the validation set (gap = 5). If removing\nthe positional embedding EP , the \u201cValid Acc.\u201d and the CD\nvalue drop 15.83% and 0.74, respectively; while the lacking\nof topological embedding ET lowers \u201cValid Acc.\u201d by 5.66%\nand worsens CD by 0.43, which reveals the importance of\nthese two components.\nRepositioning Propagation and Visibility Mask.\nWe\ndemonstrate the contribution of repositioning propagation\n(prepos. prop.) and visibility mask (vis. mask) both quan-\ntitatively and qualitatively. As shown in Table 4, without\nrepositioning propagation, the CD value will be sharply wors-\nAnimeInbet\nAnimeInbet-VS\nw/o. vis. mask\nw/o. repos. prop.\npredicted corr.\nI0, I1\nFigure 10: Visualization of ablation study. In predicted correspondence, matched vertices are marked in the same colors, while unmatched\nare black (please zoom in).\nTable 5: Ablation study on data influence.\nValidation data (gap = 5)\nOcc. (%)\nShift\nCD (\u2193)\nUnseen characters (2 \u00d7 10)\n34.30\n44.59\n14.70\nUnseen actions (10 \u00d7 2)\n37.71\n31.53\n8.98\nUnseen both (2 \u00d7 2)\n34.10\n29.62\n9.98\nened by 12.50 (112%), while the lacking of visibility mask\nwill also make a drop of 1.69 (15%). An example is shown\nin Figure 10, where \u201cw/o. repos. prop.\u201d appears within many\nmessy lines due to undefined positions for those unmatched\nvertices, while \u201cw/o. vis. mask\u201d shows some redundant\nsegments (red box) after repositioning; the complete Ani-\nmeInbet can resolve these issues and produce a clean yet\ncomplete result.\nGeometrizor. As shown in Table 2, the quantitative metrics\nof AnimeInbet-VS are generally worse by around 0.6 com-\npared to AnimeInbet. This is because VirtualSketcher [15]\ndoes not vectorize the line arts as precisely as the ground\ntruth labels (average vertex number 587 vs 1,351). As shown\nin Figure 10, the curves in \u201cAnimeInbet-VS\u201d become sharper\nand lose some details, which decreases the quality of the in-\nbetweened frame. Using a more accurate geometrizer would\nlead to higher quality inbetweening results for raster image\ninputs.\nData Influence. As mentioned in Section 3, we created a val-\nidation set composed of 20 sequences of unseen characters\nbut seen actions, 20 of unseen actions but seen characters\nand 4 of unseen both to explore the influence on data. Our\nexperiment finds that whether the characters or the actions\nare seen does not fundamentally influence the inbetweening\nquality, while the motion magnitude is the key factor. As\nshown in Table 5, the CD value of unseen characters is 14.70,\nwhich is over 47% worse than that of unseen both due to\nlarger vertex shifts (44.59 vs 29.62), while the difference\nbetween the CD values of unseen actions and unseen both is\naround 10% under similar occlusion rates and shifts.\n6. Conclusion\nIn this study, we address the practical problem of car-\ntoon line inbetweening and propose a novel approach that\ntreats line arts as geometrized vector graphs. Unlike previ-\nous frame interpolation tasks on raster images, our approach\nformulates the inbetweening task as a graph fusion problem\nwith vertex repositioning. We present a deep learning-based\nframework called AnimeInbet, which shows significant gains\nover existing methods in terms of both quantitative and qual-\nitative evaluation. To facilitate training and evaluation on\ncartoon line inbetweening, we also provide a large-scale ge-\nometrized line art dataset, MixamoLine240. Our proposed\nframework and dataset facilitate a wide range of applications,\nsuch as anime production and multimedia design, and have\nsignificant practical implications.\nAcknowledgement. This research is supported by the Na-\ntional Research Foundation, Singapore under its AI Singa-\npore Programme (AISG Award No: AISG-PhD/2021-01-\n031[T]). It is also supported under the RIE2020 Industry\nAlignment Fund Industry Collaboration Projects (IAF-ICP)\nFunding Initiative, as well as cash and in-kind contribution\nfrom the industry partner(s). This study is partially supported\nby NTU NAP, MOE AcRF Tier 1 (2021-T1-001-088).\nReferences\n[1] Mixamo. https://www.mixamo.com/. 3\n[2] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for\ndimensionality reduction and data representation. Neural\ncomputation, 15(6):1373\u20131396, 2003. 5\n[3] Leonardo Carvalho, Ricardo Marroquim, and Emilio Vital\nBrazil. Dilight: Digital light table\u2013inbetweening for 2d an-\nimations using guidelines. Computers & Graphics, 2017.\n3\n[4] Evan Casey, V\u00b4\u0131ctor P\u00b4erez, and Zhuoru Li. The animation\ntransformer: Visual correspondence via segment matching.\nIn ICCV, 2021. 1, 3\n[5] Shuhong Chen and Matthias Zwicker. Improving the percep-\ntual quality of 2d animation interpolation. In ECCV, 2022. 1,\n2, 6, 7\n[6] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and\nShuchang Zhou. Real-time intermediate flow estimation for\nvideo frame interpolation. In ECCV, 2022. 1, 2, 6, 7\n[7] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang,\nErik Learned-Miller, and Jan Kautz. Super SloMo: High\nquality estimation of multiple intermediate frames for video\ninterpolation. In CVPR, 2018. 2\n[8] Kangyeol Kim, Sunghyun Park, Jaeseong Lee, Sunghyo\nChung, Junsoo Lee, and Jaegul Choo. Animeceleb: Large-\nscale animation celebheads dataset for head reenactment. In\nECCV, 2022. 3\n[9] Diederik Kingma and Jimmy Ba.\nAdam: A method for\nstochastic optimization. In ICLR, 2014. 7\n[10] Johannes Kopf and Dani Lischinski. Digital reconstruction\nof halftoned color comics. ACM TOG, 31(6), 2012. 1, 3\n[11] Xiaoyu Li, Bo Zhang, Jing Liao, and Pedro V. Sander. Deep\nsketch-guided cartoon video inbetweening. TVCG, 2020. 2\n[12] Songtao Liu, Jin Huang, and Hao Zhang. End-to-end line\ndrawing vectorization. In AAAI, 2022. 3, 4\n[13] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and\nAseem Agarwala. Video frame synthesis using deep voxel\nflow. In CVPR, 2017. 2\n[14] Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya\nJia. Video frame interpolation with transformer. In CVPR,\n2022. 1, 2, 6, 7\n[15] Haoran Mo, Edgar Simo-Serra, Chengying Gao, Changqing\nZou, and Ruomei Wang. General virtual sketching framework\nfor vector line art. In SIGGRAPH, 2021. 3, 4, 7, 9\n[16] Rei Narita, Keigo Hirakawa, and Kiyoharu Aizawa. Optical\nflow based line drawing frame interpolation using distance\ntransform to support inbetweenings. In ICIP, 2019. 7\n[17] Simon Niklaus and Feng Liu. Context-aware synthesis for\nvideo frame interpolation. In CVPR, 2018. 2\n[18] Simon Niklaus and Feng Liu. Softmax splatting for video\nframe interpolation. In CVPR, 2020. 2\n[19] Simon Niklaus, Long Mai, and Feng Liu. Video frame inter-\npolation via adaptive convolution. In CVPR, 2017. 2\n[20] Simon Niklaus, Long Mai, and Feng Liu. Video frame inter-\npolation via adaptive separable convolution. In ICCV, 2017.\n2\n[21] Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim.\nBmbc: Bilateral motion estimation with bilateral cost volume\nfor video interpolation. In ECCV, 2020. 2\n[22] Yingge Qu, Tien-Tsin Wong, and Pheng-Ann Heng. Manga\ncolorization. ACM TOG, 25(3), 2006. 1, 3\n[23] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun,\nCaroline Pantofaru, and Brian Curless. Film: Frame interpo-\nlation for large motion. In ECCV, 2022. 1, 2, 6, 7\n[24] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. Superglue: Learning feature match-\ning with graph neural networks. In CVPR, 2020. 5\n[25] Maria Shugrina, Ziheng Liang, Amlan Kar, Jiaman Li, Angad\nSingh, Karan Singh, and Sanja Fidler. Creative flow+ dataset.\nIn CVPR, 2019. 3\n[26] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi: ex-\ntreme video frame interpolation. In ICCV, 2021. 2\n[27] Edgar Simo-Serra, Satoshi Iizuka, and Hiroshi Ishikawa. Mas-\ntering sketching: Adversarial augmentation for structured\nprediction. ACM TOG, 37(1), 2018. 2\n[28] Edgar Simo-Serra, Satoshi Iizuka, Kazuma Sasaki, and Hi-\nroshi Ishikawa. Learning to simplify: Fully convolutional\nnetworks for rough sketch cleanup. ACM TOG, 35(4), 2016.\n2\n[29] Li Siyao, Yuhang Li, Bo Li, Chao Dong, Ziwei Liu, and\nChen Change Loy. Animerun: 2d animation visual corre-\nspondence from open source 3d movies. In NeurIPS, 2022.\n3\n[30] Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris\nMetaxas, Chen Change Loy, and Ziwei Liu. Deep animation\nvideo interpolation in the wild. In CVPR, 2021. 2\n[31] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. Loftr: Detector-free local feature matching\nwith transformers. In CVPR, 2021. 5\n[32] D. S`ykora, J. Buri\u00b4anek, and J. \u02c7Z\u00b4ara. Unsupervised coloriza-\ntion of black-and-white cartoons. In Int. Symp. NPAR, 2004.\n1, 3\n[33] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and\nDacheng Tao. Gmflow: Learning optical flow via global\nmatching. In CVPR, 2022. 5\n[34] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-\nHsuan Yang. Quadratic video interpolation. In NeurIPS,\n2019. 2\n[35] Wenwu Yang. Context-aware computer aided inbetweening.\nIEEE TVCG, 24(2):1049\u20131062, 2017. 3\n[36] Chih-Yuan Yao, Shih-Hsuan Hung, Guo-Wei Li, I-Yu Chen,\nReza Adhitya, and Yu-Chi Lai. Manga vectorization and\nmanipulation with procedural simple screentone. IEEE TVCG,\n23(2), 2016. 3, 4\n[37] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. arXiv preprint\narXiv:2302.05543, 2023. 3\n[38] Lvmin Zhang, Jinyue Jiang, Yi Ji, and Chunping Liu.\nSmartshadow: Artistic shadow drawing tool for line drawings.\nIn ICCV, 2021. 1, 3\n[39] Lvmin Zhang, Chengze Li, Tien-Tsin Wong, Yi Ji, and Chun-\nping Liu. Two-stage sketch colorization. In SIGGRAPH,\n2018. 1, 3\n[40] Song-Hai Zhang, Tao Chen, Yi-Fei Zhang, Shi-Min Hu, and\nRalph R. Martin. Vectorizing cartoon animations. IEEE\nTVCG, 15(4), 2009. 3, 4\n"
  },
  {
    "title": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models",
    "link": "https://arxiv.org/pdf/2309.16414.pdf",
    "upvote": "19",
    "text": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nAUTOCLIP: AUTO-TUNING ZERO-SHOT CLASSIFIERS\nFOR VISION-LANGUAGE MODELS\nJan Hendrik Metzen, Piyapat Saranrittichai & Chaithanya Kumar Mummadi\nBosch Center for Artificial Intelligence, Robert Bosch GmbH\njanhendrik.metzen@de.bosch.com\nABSTRACT\nClassifiers built upon vision-language models such as CLIP have shown remark-\nable zero-shot performance across a broad range of image classification tasks.\nPrior work has studied different ways of automatically creating descriptor sets for\nevery class based on prompt templates, ranging from manually engineered tem-\nplates over templates obtained from a large language model to templates built\nfrom random words and characters. Up until now, deriving zero-shot classifiers\nfrom the respective encoded class descriptors has remained nearly unchanged, i.e.,\nclassify to the class that maximizes cosine similarity between its averaged encoded\nclass descriptors and the image encoding. However, weighing all class descriptors\nequally can be suboptimal when certain descriptors match visual clues on a given\nimage better than others. In this work, we propose AUTOCLIP, a method for auto-\ntuning zero-shot classifiers. AUTOCLIP tunes per-image weights to each prompt\ntemplate at inference time, based on statistics of class descriptor-image similar-\nities. AUTOCLIP is fully unsupervised, has very low computational overhead,\nand can be easily implemented in few lines of code. We show that AUTOCLIP\noutperforms baselines across a broad range of vision-language models, datasets,\nand prompt templates consistently and by up to 3 percent point accuracy.\n1\nINTRODUCTION\nClassifiers built upon vision-language models (VLMs) such as CLIP (Radford et al., 2021) and\nCoCa (Yu et al., 2022) have shown strong zero-shot transfer capabilities across various tasks. It has\nbeen observed that prompt engineering plays a crucial role for obtaining strong zero-shot classifiers.\nThese zero-shot classifiers derived from VLMs are constructed based on a set of prompt templates\n(parameterized by the class name) that cover potential variation of the domain. These prompt tem-\nplates can be hand-designed (Radford et al., 2021), generated by a large-language model (Menon &\nVondrick, 2022), or randomly generated (Roth et al., 2023).\nPrompts can also be learned via test-time prompt tuning (TPT) (Shu et al., 2022; Zhao et al., 2023).\nThis approach makes the zero-shot classifier adaptable to the datum of interest, which is possible by\neffectively leveraging the knowledge of the general-purpose VLM. Shu et al. (2022) tune prompts\nso that the predictive entropy for a single image is minimized, while Zhao et al. (2023) maximizes\na CLIP reward. These prior TPT methods require the VLM\u2019s image encoder to process several aug-\nmentations for each image. Moreover, gradients with respect to the prompts require backpropagation\nthrough the VLM\u2019s text encoder, thereby substantially increasing the overall inference cost.\nIn contrast, we propose not to tune the prompts but instead use a large set of predefined and fixed\nprompt templates and to adapt the weights of those prompt templates for each image at test-time.\nThis approach has the major advantage that adaptation takes place entirely in the embedding space\nwithout requiring additional forward or backward passes through the VLM\u2019s encoders, which sig-\nnificantly lowers the test-time computation and memory overhead compared to prior TPT methods.\nWe briefly summarize the standard way of constructing zero-shot classifiers from VLMs (see Figure\n1 left). At first, a collection of prompt templates are instantiated for each class to form a set of class\ndescriptors (e.g., \u201cA photo of a car\u201d, and \u201cA drawing of a car\u201d are sample class descriptors of class\ncar). These descriptors are processed by the text encoder and the resulting encoded descriptors are\n1\narXiv:2309.16414v2  [cs.CV]  29 Sep 2023\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nFigure 1: Conceptual Illustration of AUTOCLIP. CLIP\u2019s zero-shot classifiers are based on a set\nof prompt templates (\u201cA photo of a <class name>\u201d, \u201cA drawing of a <class name>\u201d, ...). Inserting\nclass names into these templates gives a set of class descriptors that are encoded into a joint embed-\nding space together with the respective image. Standard CLIP averages encoded class descriptors\ninto class queries qc, and classifies to the class that has maximal cosine similarity with the encoded\nimage. However, this ignores that some prompt templates describe the image of interest better than\nothers: for instance, when the image is a drawing, the template \u201cA drawing of a <class name>\u201d\nresults in stronger class descriptors than other templates and should thus be weighted higher when\ncomputing class queries. AUTOCLIP determines such weights directly from class descriptor-image\nsimilarities in the embedding space. Here, the car image is taken from Atkinson (2015).\naveraged to obtain the image-independent class queries (e.g. qcar). Besides, the image encoder pro-\ncesses the input image to be classified to get the image encoding, which lies in the same embedding\nspace as class queries. The cosine similarity of the encoded image to every (averaged) class query\nis computed, and the output prediction is assigned to the class with maximum similarity.\nThis work follows a similar zero-shot classification setup, except that we change how class queries\nare computed. Instead of a simple average of the encoded class descriptors, we propose to take a\nweighted average, wherein weights of the encoded class descriptors are automatically tuned for each\nimage separately. Weights are determined in a manner that prompt templates whose resulting class\ndescriptors are closer to the respective image embedding get higher weightage than those being less\nsimilar (see Figure 1 right). Our approach is motivated by the intuition that prompt templates with\nhigh similarity describe relevant properties of the image better than ones with lower similarity (see\nFigure 6 for evidence supporting this intuition). We denote our method that automatically adapts\nweights of the encoded class descriptors for each image as AUTOCLIP.\nWe empirically show that AUTOCLIP improves the performance of zero-shot classifiers across\nmany datasets, VLMs, and prompt strategies with little inference-time overhead. Note that AU-\nTOCLIP is fully zero-shot as it does not require any supervision from the target task. Furthermore,\nAUTOCLIP makes no assumptions on the underlying VLM and can thus be broadly applied, poten-\ntially also to multi-modal models beyond VLMs such as ImageBind (Girdhar et al., 2023b).\nOverall, our main contributions are as follows: we introduce AUTOCLIP (Section 3.2), a novel\nprocedure for constructing zero-shot classifiers from vision-language models. AUTOCLIP leverages\nstatistics of class descriptor-image similarities to automatically determine weights of the prompt\ntemplates. We further discuss a method for automatically tuning AUTOCLIP\u2019s step size such that\nthe entropy of the prompt template\u2019s weights is controlled (Section 3.4). We propose a default\nentropy reduction factor, which is shared across all the experiments. By this, AUTOCLIP comes\nessentially without free hyperparameters, which is important as hyperparameters cannot be tuned in\nzero-shot settings. We evaluate AUTOCLIP on a large number of datasets, vision-language models,\nand prompt templates (Section 4). We find that it improves performance on the vast majority (85%)\n2\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nof settings, by 0.45 percent point accuracy on average, and by up to 3 percent point in some settings.\nThis benefit comes essentially for free with the only cost being a very small inference time overhead.\n2\nRELATED WORK\nVision-Language Pretraining\nDeep learning with vision-language pretraining has enabled zero-\nshot transfer capabilities, i.e., the resulting vision-language models (VLMs) are able to perform\nzero-shot classification on vastly diverse unseen target datasets given only text prompts of individual\ntarget classes. CLIP is one of the state-of-the-art VLMs pretrained on the well-curated WebImage-\nText dataset containing 400 millions image-text pairs using a contrastive loss (Radford et al., 2021).\nIn terms of datasets used, ALIGN requires less dataset preprocessing enabling training on a dataset\nof over a billion image-text pairs (Jia et al., 2021). Florence (Yuan et al., 2021) expands models\nto other common modalities (e.g., videos). In terms of the training loss, CoCa (Yu et al., 2022)\nleverages an additional captioning loss allowing models to be used in generative applications. In our\nwork, we study how to optimally use text prompts of the target classes with these VLMs.\nPrompt Construction\nConventionally, one or several manually designed text prompts per target\nclass are employed for zero-shot classification (Radford et al., 2021; Jia et al., 2021). Recent research\ndemonstrates that introducing additional prompts can improve overall performance. DCLIP (Menon\n& Vondrick, 2022) generates additional prompts based on querying the large-language model GPT-3\n(Brown et al., 2020). WaffleCLIP (Roth et al., 2023) has shown that classification performance can\nbe further boosted by appending random words or characters to predefined prompt templates. To\nderive a zero-shot classifier, these works weight all text prompts equally (uniform weighting). In\ncontrast, we propose an approach to adjust weights of individual prompts per input sample dynami-\ncally at test time.\nTest-Time Adaptation\nOur work can be considered as a test-time adaption approach for VLMs.\nTENT (Wang et al., 2020) demonstrates that adapting models to minimize prediction entropy can\nimprove model performance at test time. In the context of VLMs, TPT (Shu et al., 2022) optimizes\nprompts of target classes based on the entropy minimization objective. RLCF (Zhao et al., 2023)\ndemonstrates that minimizing the entropy objective can lead to overfitting under distribution shift\nand proposes adaptation based on average CLIP scores. In contrast to these previous works, we do\nnot perform any adaptation of the prompts or model parameters, but refine weights of individual (en-\ncoded) prompts, which is considerably cheaper in terms of computation and memory consumption.\n3\nAUTOCLIP\nWe outline the common approach for building zero-shot classifiers for VLMs like CLIP in Section\n3.1. Thereupon, we detail our proposed AUTOCLIP as an auto-tuned alternative in Section 3.2,\nfollowed by describing how the required gradient can be calculated in closed-form in Section 3.3,\nand finally explain how AUTOCLIP\u2019s step size can be automatically determined in Section 3.4.\n3.1\nBACKGROUND: ZERO-SHOT CLASSIFIERS FOR VISION-LANGUAGE MODELS\nLet us consider a classification task X 7\u2192 C, where X corresponds to the input domain and C =\n{c1, . . . , cC} is a set of C classes. We assume that there exists a pretrained VLM such as CLIP\nthat provides a joint embedding space E and corresponding embedding functions EX : X 7\u2192 E that\nmaps input data x \u2208 X into embedding space E and ET : T 7\u2192 E that maps text into the same\nembedding space E. Let there be K prompt templates t1, . . . tK : C 7\u2192 D that map class name\nc \u2208 C to (textual) class descriptors d \u2208 T . These prompt templates can be either manually designed\n(Radford et al., 2021), generated by a large language model (Menon & Vondrick, 2022), or randomly\ngenerated (Roth et al., 2023). Algorithm 1 summarizes the standard zero-shot classifier for VLMs:\naverage the class descriptor encodings e(d) into class queries qj, then compute cosine similarities sj\nbetween class query and encoded image e(x), and classify to the class that maximizes similarity.\n3\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nAlgorithm 1 Zero-Shot Classifier\n1: d \u2190 {ti(cj) | i \u2208 {1, . . . , K}, j \u2208 {1, . . . , C}}\n\u25b7 Generate K \u00d7 C class descriptors\n2: e(x) \u2190 EX(x)/||EX(x)||2\n\u25b7 Encode image of interest x with VLM\n3: e(d)\nij \u2190 ET (dij)/||ET (dij)||2\n\u25b7 Encode all class descriptors with VLM\n4: wi \u2190 1/K\n\u25b7 Uniform prompt template weights\n5: for j \u2208 1, . . . , C do\n6:\nqj \u2190 PK\ni=1 wie(d)\nij\n\u25b7 Class queries as average of classes\u2019 descriptor encodings\n7:\nsj \u2190 e(x) \u00b7 qj\n\u25b7 Compute cosine similarity between e(x) and class query qj\n8: end for\n9: j\u22c6 \u2190 arg maxj sj\n\u25b7 Assign x to class cj\u22c6 with maximum similarity\n3.2\nAUTO-TUNING ZERO-SHOT CLASSFIERS\nAUTOCLIP modifies step (4) in Algorithm 1. Instead of computing class queries as simple av-\nerage of class descriptor encodings qj = 1/K PK\ni=1 e(d)\nij , AUTOCLIP uses a weighted average:\nqj = PK\ni=1 wie(d)\nij\nwith learnable w satisfying wi \u2265 0, PK\ni=1 wi = 1, which we enforce by\nreparametrizing w = softmax(\u03c1) and \u03c1 \u2208 RK. AUTOCLIP\u2019s guiding intuition (see Figure 1) is to\nassign higher weights wi to prompt templates ti that result in class descriptor encodings e(d)\nij that\nare more similar to the encoded image e(x), that is: ti with large e(xd)\nij\n= e(d)\nij \u00b7 e(x) (j = 1, . . . , C).\nThis is inspired by the observation that class descriptors having higher similarity in the embedding\nspace describe the image better (according to contrastive pretraining objectives in typical VLMs).\nWhen determining the template\u2019s weights w, we have C descriptor-image similarities e(xd)\nij\nfor each\ntemplate ti. AutoCLIP needs to aggregate those C similarities across classes when assigning larger\nweight to more relevant prompt templates. Intuitively, simply averaging all C similarities (\u201cmean\u201d\naggregation) ignores that, in the classification objective, we ultimately only care about classes that\nresult in the descriptors closest to e(x); however, taking only the class with highest similarity per tem-\nplate into account (\u201cmax\u201d aggregation) ignores inherent ambiguity in the image and was found to\nbe suboptimal (Roth et al., 2023). We propose a middle ground of aggregating via a smooth approx-\nimation to the maximum function via logsumexpj(e(xd)\nij\n) = log PC\nj=1 exp e(xd)\nij\n. This logsumexp\naggregation takes all classes into account but assigns higher importance to more relevant classes\n(ones resulting in higher similarities to the image x). AUTOCLIP then determines weights wi such\nthat logsumexpj(sj) = logsumexpj(PK\ni=1 wie(xd)\nij\n) = logsumexpj(softmax(\u03c1) \u00b7 e(xd)\n:j\n) gets in-\ncreased by one step of gradient ascent in the direction of \u2207\u03c1 logsumexpj(softmax(\u03c1) \u00b7 e(xd)\n:j\n). We\nnote that \u2212 logsumexp has been interpreted as the energy function of a data point (for appropriately\ntrained classifiers) (Grathwohl et al., 2020); in this view, AUTOCLIP can be interpreted as minimiz-\ning the energy and maximizing the probability density p(x) of x under the zero-shot classifier.\nWe summarize AUTOCLIP in Algorithm 2. We initialize \u03c1 = 0, which corresponds to an un-\nweighted average of the classes\u2019 descriptor encodings (Line 4). Similar to Algorithm 1, we then\ncompute the pairwise cosine similarities sj between encoded image e(x) and class queries qj (Line\n5-8). Instead of directly classifying to the class with maximum similarity to the image, AUTOCLIP\nupdates the class descriptor weights first. For this, the gradient g = \u2207\u03c1 logsumexpj(sj) is com-\nputed (Line 9), an appropriate step size \u03b1 is selected (Line 10, see Section 3.4), and \u03c1 = \u03b1 \u00b7 g and\nw = softmax(\u03c1) are updated (Line 11). Based on the new w, AUTOCLIP computes updated class\nqueries qj and class-image similarities (Line 12-15) and finally classifies to the class with maximum\nsimilarity for the image (Line 16).\nWe note that Line 5-11 could be repeated for several iterations with smaller step sizes; however pre-\nliminary experiments indicate no advantage of doing more than one iteration. We call AUTOCLIP\n\u201cauto-tuned\u201d because its weights w are automatically adapted for every input independently. More-\nover, we note that in practice, models like CLIP scale e(xd) by a learned temperature (exponential\nlogit scale) \u03c4 to obtain well calibrated classifiers; we use the same temperature for scaling e(xd) in\n4\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nAlgorithm 2 AUTOCLIP: Auto-Tuned Zero-Shot Classifier\n1: d \u2190 {ti(cj) | i \u2208 {1, . . . , K}, j \u2208 {1, . . . , C}}\n\u25b7 Generate K \u00d7 C class descriptors\n2: e(x) \u2190 EX(x)/||EX(x)||2\n\u25b7 Encode image of interest x with VLM\n3: e(d)\nij \u2190 ET (dij)/||ET (dij)||2\n\u25b7 Encode all class descriptors with VLM\n4: \u03c1 \u2190 0;\nwi \u2190 softmax(\u03c1)\n\u25b7 Uniform weights wi = 1/K\n5: for j \u2208 1, . . . , C do\n6:\nqj \u2190 PK\ni=1 wie(d)\nij\n\u25b7 Class queries as weighted average of classes\u2019 descriptor encodings\n7:\nsj \u2190 e(x) \u00b7 qj\n\u25b7 Compute cosine similarity between e(x) and class query qj\n8: end for\n9: g \u2190 \u2207\u03c1 log PC\nj=1 exp(sj)\n\u25b7 Compute gradient (Section 3.3)\n10: \u03b1 \u2190 BISECT(softmax entropy(\u03b1 \u00b7 g) \u2212 \u03b2 log2 K, 0, 1010) \u25b7 Determine stepsize (Section 3.4)\n11: \u03c1 \u2190 \u03b1 \u00b7 g;\nwi \u2190 softmax(\u03c1)\n\u25b7 Update \u03c1 with one gradient ascent step and step size \u03b1\n12: for j \u2208 1, . . . , C do\n13:\nqj \u2190 PK\ni=1 wie(d)\nij\n\u25b7 Class queries as weighted average of classes\u2019 descriptor encodings\n14:\nsj \u2190 e(x) \u00b7 qj\n\u25b7 Compute cosine similarity between e(x) and class query qj\n15: end for\n16: j\u22c6 \u2190 arg maxj sj\n\u25b7 Assign x to class cj\u22c6 with maximum similarity\nthe logsumexp aggregation (as there is no labelled data in a zero-shot setting on which a temperature\ncould be tuned).\n3.3\nCLOSED-FORM COMPUTATION OF GRADIENT\nWhile \u2207\u03c1 logsumexp(s) can be easily computed using automatic differentiation, we note that there\ncan be runtime environments for inference such as on edge devices where running automatic differ-\nentiation is undesirable. For such cases, the gradient \u2207\u03c1 logsumexpj(sj) can also be computed in\nclosed-form:\n\u0000\u2207\u03c1 logsumexpj(sj)\n\u0001\ni = PK\nk=1(PC\nj=1 softmax(s)j \u00b7 e(xd)\nij\n) \u00b7 wi(\u03b4ik \u2212 wk), with \u03b4ij\nbeing the Kronecker delta function with \u03b4ii = 1 and \u03b4ij = 0 for i \u0338= j.\n3.4\nAUTO-TUNING THE STEP SIZE\nThe only free hyperparameter of AUTOCLIP is the step size \u03b1. We note that in a zero-shot setting,\nthere is by definition no labeled data on which such free hyperparameters can be tuned. Because of\nthis, free hyperparameters need to be selected globally in a dataset-independent manner. However, a\nglobal choice for the step size \u03b1 is problematic since the scale of the gradient g = \u2207\u03c1 logsumexp(s)\ndepends on the dataset, and the step size would have to be adapted accordingly. We address this by\nproposing a different parameterization in which the free hyperparameter is easily interpreted and\nthe step size \u03b1 is a derived quantity. Specifically, we control the entropy of the query weights w,\nentropy(w) = \u2212 PK\ni=1 wi log2 wi. The standard, uniform weights have maximum entropy log2 K\nand we set the target entropy to \u03b2 \u00b7 log2 K, where the entropy reduction factor \u03b2 \u2208 [0, 1] is the\nnew free hyperparameter that we set globally to \u03b2 = 0.85. Intuitively, \u03b2 \u2192 1 corresponds to more\nequally weighted prompt templates while \u03b2 \u2192 0 to selecting the prompt template with maximum\nsimilarity. We present an ablation of the effect of \u03b2\u2019s choice on AUTOCLIP in Figure 4.\nWith softmax entropy(\u03b1\u00b7g) denoting the entropy of the weights w = softmax(\u03b1\u00b7g), selecting the\nstep size \u03b1 is now equivalent to solving for f(\u03b1) = 0 for f(\u03b1) = softmax entropy(\u03b1\u00b7g)\u2212\u03b2\u00b7log2 K.\nAs softmax entropy(\u03b1 \u00b7 g) monotonically decreases with \u03b1, we use bisection on \u03b1 \u2208 [0, 1010] for\nfinding \u03b1 with f(\u03b1) \u2248 0. We note that softmax entropy(0 \u00b7 g) = log2 K and thus f(0) > 0 for\nall \u03b2 < 1; similarly, softmax entropy(\u03b1 \u00b7 g) \u2248 0 for \u03b1 = 1010 in all settings we considered and\nthus f(1010) < 0 for all \u03b2 > 0, which together satisfies the prerequisites for running bisection.\nThe additional bisection has little overhead compared to the cost of encoding the image x with Ex;\nwe use an independent call to scipy.optimize.bisect (Virtanen et al., 2020) (maxiter=100,\nxtol=1e-2, rtol=1e-2) for every image but note that a batched variant of bisection could speed-up\nmany workloads.\n5\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\n0.0\n0.5\n1.0\n1.5\nCUB200\nCLIP\nRN50\nDCLIP\nCLIP\nWaffleCLIP\nCLIP\nViT-B-32\nCLIP\nViT-B-16\nCLIP\nViT-L-14\nCLIP DataComp\nViT-L-14\nCoCa\nViT-L-14\n4\n2\n0\n2\nEuroSAT\n0.0\n0.5\n1.0\nFood101\n0\n1\n2\n3\nOxford Pets\n0.25\n0.00\n0.25\n0.50\n0.75\nImageNet\n0.0\n0.5\nImageNetV2\n101 102 103\n#Prompts\n0.0\n0.5\n1.0\n1.5\n2.0\nImageNetR\n101 102 103\n#Prompts\n101 102 103\n#Prompts\n101 102 103\n#Prompts\n101 102 103\n#Prompts\n101 102 103\n#Prompts\n Accuracy\nFigure 2: Accuracy improvement (\u2206 Accuracy) of AUTOCLIP over baseline zero-shot classifier\nacross models, datasets, and prompt ensembles, averaged over 7 runs.\n6\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\n1\n2\n3\n4\n5\nSeverity\n0.50\n0.25\n0.00\n0.25\n0.50\n Accuracy\nRN50\nDCLIP\nCLIP\nWaffleCLIP\n1\n2\n3\n4\n5\nSeverity\nViT-B-32\n1\n2\n3\n4\n5\nSeverity\nViT-B-16\n1\n2\n3\n4\n5\nSeverity\nViT-L-14\n1\n2\n3\n4\n5\nSeverity\nViT-L-14\n (DataComp)\nFigure 3: ImageNet-C accuracy improvement (\u2206 Accuracy) of AUTOCLIP over baseline zero-shot\nclassifier for K = 100 across models, corruption severity and prompt ensembles, averaged over\ncorruptions and 7 runs.\nCLIP\nCLIP\nCLIP\nCLIP\nDataComp\nCoCa\nRN50\nViT-B-32\nViT-B-16\nViT-L-14\nViT-L-14\nViT-L-14\nCUB200\n47.75 (+0.5)\n52.84 (+0.7)\n57.12 (+1.3)\n64.43 (+0.7)\n84.79 (+0.8)\n73.90 (+0.6)\nEuroSAT\n34.95 (-1.2)\n46.16 (-0.7)\n55.93 (+1.4)\n55.09 (+0.6)\n65.09 (+1.8)\n54.77 (-0.4)\nFood101\n80.26 (+1.4)\n84.13 (+1.3)\n88.85 (+0.9)\n93.71 (+0.4)\n94.52 (+0.3)\n90.46 (+0.4)\nOxford Pets\n83.09 (+2.6)\n85.63 (+2.9)\n85.89 (+1.9)\n91.64 (+0.9)\n92.82 (+0.9)\n92.03 (+1.2)\nImageNet\n60.42 (+0.6)\n63.80 (+0.6)\n68.70 (+0.5)\n75.89 (+0.3)\n79.07 (+0.0)\n75.63 (+0.2)\nImageNetV2\n53.44 (+0.4)\n56.49 (+0.8)\n62.54 (+0.6)\n70.17 (+0.4)\n72.21 (+0.2)\n68.08 (+0.1)\nImageNetR\n29.32 (+0.9)\n51.04 (+1.0)\n59.13 (+1.0)\n73.98 (+0.4)\n78.85 (+0.6)\n75.59 (+0.8)\nTable 1: Accuracy of AUTOCLIP (and \u2206 Accuracy to baseline zero-shot classifier in parenthesis)\nfor K = 100 WaffleCLIP prompt templates across models and datasets, averaged over 7 runs.\n4\nEXPERIMENTS\nExperimental Setting\nIn this section, we compare AUTOCLIP to standard zero-shot classifiers\non a wide range of zero-shot image classification benchmarks and a variety of settings. We con-\nduct experiments on the datasets CUB200 (Welinder et al., 2010), EuroSAT (Helber et al., 2019),\nFood101 (Bossard et al., 2014), Oxford Pets (Parkhi et al., 2012), ImageNet (Russakovsky et al.,\n2015), ImageNetV2 (Kornblith et al., 2019), ImageNet-R (Hendrycks et al., 2021), and ImageNet-C\n(Hendrycks & Dietterich, 2019). We study six different vision-language models: from CLIP (Rad-\nford et al., 2021) we use the ResNet-50 (RN50) (He et al., 2015), and vision transformers (ViT-B/32,\nViT-B/16, and ViT-L/14) models (Dosovitskiy et al., 2021). Moreover, we use the ViT-L/14 model\nvariants from DataComp (Gadre et al., 2023) and the one trained with CoCa (Yu et al., 2022).\nAdditionally, we study three ways of generating prompt templates: 1) using the 80 manually de-\nsigned templates from Radford et al. (2021) (CLIP), 2) templates based on querying a large-language\nmodel (DCLIP) (Menon & Vondrick, 2022), and 3) templates that append random words or char-\nacters to predefined prompt templates (WaffleCLIP) (Roth et al., 2023). We vary the number of\ntemplates from K = 4 to K = 500; if there is a fixed number of templates available such as in\nCLIP/DCLIP, templates are sampled with replacement. To account for randomness in the template\nconstruction/sampling, we report results averaged over 7 runs. We base our implementation on\nhttps://github.com/ExplainableML/WaffleCLIP from Roth et al. (2023) and highly\nappreciate their code release under a permissible license. We report the difference of accuracy of\nAUTOCLIP compared to the baseline zero-shot classifier with uniform prompt template weights\n(\u201d\u2206 Accuracy\u201d). Absolute performance across different datasets and VLMs is shown in Table 1\n(and in Table 2 and Table 3 in the appendix).\nResults\nWe present the main results in Figure 2. Overall, the figure contains 990 different combi-\nnations comparing AUTOCLIP with the baseline; AUTOCLIP is better in 840 cases (\u2248 85%) and on\naverage it is better by 0.45 percent point accuracy. We also observe a trend that for larger number of\nprompt templates K, the advantage of AUTOCLIP (\u2206 Accuracy averaged across datasets, models\n7\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTarget Entropy Rate\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n Accuracy\nCUB200\nEuroSAT\nFood101\nImageNet\nImageNetR\nImageNetV2\nOxford Pets\nAverage\nFigure 4: Ablation on target entropy rate \u03b2. Shown is Accuracy improvement (\u2206 Accuracy) of\nAUTOCLIP over baseline zero-shot classifier for a CLIP ViT-B-16, and 100 WaffleCLIP prompt\ntemplates, averaged over 7 runs.\nand CLIP/DCLIP/WaffleCLIP) increases: from \u2206 = 0.06 for K = 4 over \u2206 = 0.33 for K = 10\nand \u2206 = 0.49 for K = 50 to \u2206 = 0.57 for K = 200. When aggregating over models, datasets and\nnumber of prompt templates, AUTOCLIP achieves the largest average improvement for WaffleCLIP\n(\u2206 = 0.61), but still improves for CLIP (\u2206 = 0.40) and DCLIP (\u2206 = 0.29). Taken together, the\nfindings indicate that AUTOCLIP benefits from larger (increased K) and more diverse (WaffleCLIP)\nsets of prompt templates.\nWhen comparing different vision-language models, AUTOCLIP brings the biggest benefit for CLIP\nViT-B-16 (\u2206 = 0.68) and the smallest one for CoCa ViT-L-14 (\u2206 = 0.19), with all other mod-\nels having average \u2206 between 0.36 and 0.52. Comparing different datasets, AUTOCLIP performs\nstrongest on Oxford Pets (\u2206 = 1.15) and worst on EuroSAT (\u2206 = \u22120.24). We note that EuroSAT\nis the only setting on which AUTOCLIP hurts performance on average; on all other datasets, AU-\nTOCLIP improves performance: \u2206(CUB200) = 0.5, \u2206(Food101) = 0.52, \u2206(ImageNet) = 0.17,\n\u2206(ImageNetV2) = 0.2, and \u2206(ImageNetR) = 0.71.\nIn Figure 3, we present results on ImageNet-C for WaffleCLIP with K = 100 for different severities\nand averaged across corruptions. AUTOCLIP consistently improves performance for the smaller\nvision-language models (RN50, ViT-B-32, ViT-B-16) and sees a minor drop of performance for the\ntwo ViT-L-14 variants. Averaged across all models, corruptions, and severities, AUTOCLIP im-\nproves performance by \u2206 = 0.11. We provide plots for each corruption separately for WaffleCLIP\nprompt templates in the appendix in Figure 8. The biggest average benefit of AUTOCLIP is obtained\nfor the low-frequency corruptions \u201csaturate\u201d (\u2206 = 0.22), \u201cbrightness\u201d (\u2206 = 0.22), and \u201ccontrast\u201d\n(\u2206 = 0.23); the smallest average benefit for \u201cshot-noise\u201d (\u2206 = 0.05) and \u201csnow\u201d (\u2206 = 0.06).\nAblations\nWe ablate AUTOCLIP\u2019s choice of the target entropy rate \u03b2 (which defaults to 0.85)\nand the objective function (defaults to logsumexp). In Figure 4, we observe that AUTOCLIP\u2019s\nperformance for most datasets does not depend strongly on the specific choice of the target entropy\nrate \u03b2 as \u2206 Accuracy stays relatively constant in the range \u03b2 \u2208 [0.7, 0.9]. This is a desirable\nproperty as in a zero-shot setting without labeled data, tuning \u03b2 per dataset would be infeasible.\nFor two datasets (Oxfort Pets and EuroSAT), our default value of \u03b2 = 0.85 was suboptimal and\na considerably smaller choice of \u03b2 = 0.7 would have obtained considerably better results. Also\non average, \u03b2 = 0.7 performs favorably and we recommend this choice for future work on other\ndatasets and tasks.\nWe motivated the choice of logsumexp as AUTOCLIP\u2019s aggregation/objective function in Section\n3.2 as striking a good compromise between max and mean aggregation. In Figure 5, we empirically\nconfirm that the logsumexp aggregation performs favorably compared to max/mean aggregation\non all datasets. Moreover, it also outperforms entropy aggregation, which is a popular choice for\ntest-time adaption (Wang et al., 2020; Shu et al., 2022).\nIn Figure 6, we show the prompt template weights (K = 30) obtained by AUTOCLIP on 500\nFood101 samples. Samples are structured in 10 blocks of 50 samples each, where each block cor-\nresponds to one class. Prompt template weights are relatively similar for instances belonging to the\nsame (unknown) class but vary substantially across classes. Some templates like the ones starting\nwith \u201cA tattoo of...\u201d or \u201dA origami of...\u201d get consistently low weights as the images of the Food101\ndataset do not look like tattoos or origami, while templates starting with \u201cA photo of...\u201d tend to get\n8\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nOxford Pets\nEuroSAT\nCUB200\nFood101\nImageNetV2\nImageNetR\nImageNet\nDataset\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n Accuracy\nObjective function\n logsumexp\n entropy\n mean\n max\nFigure 5: Comparison of different objective functions for auto-tuning. Shown is Accuracy im-\nprovement (\u2206 Accuracy) of AUTOCLIP over baseline zero-shot classifier for a ViT-B-16, and 100\nWaffleCLIP prompt templates, averaged over 7 runs.\nFigure 6: Illustration of prompt template weights w on 500 samples from the Food101 dataset, with\nblocks of 50 samples belonging to the same (unknown) class. CLIP backbone is a ViT-B-16 and 30\nDCLIP prompt templates are used.\nhigher weights, as Food101 contains mostly actual photos. Note that the weight distribution looks\ndifferent on other datasets like ImageNet-R, with higher weights for \u201cartistic\u201d prompts (see Figure\n7 in the appendix). Overall, this confirms that AUTOCLIP can adapt the zero-shot classifier on the\nfly to properties of the respective image.\n5\nCONCLUSION\nWe have proposed AutoCLIP, a method for improving zero-shot classifiers on vision-language mod-\nels. AutoCLIP automatically tunes per-image weights of prompt templates before aggregating them\ninto class queries. AutoCLIP improves performance over standard zero-shot classifiers on the vast\nmajority of settings, with only minimal inference-time overhead. We believe that due to its simplicity\nand low cost, AutoCLIP has the potential to be broadly applied in conjunction with vision-language\nmodels. For future work, it will be exciting to explore if AutoCLIP can also benefit other zero-shot\ntasks built on top of multi-modal modals such as object detection with OWL-ViT (Minderer et al.,\n2022) or multi-modal prompting with ImageBind (Girdhar et al., 2023a).\n9\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nREFERENCES\nJeremy\nAtkinson.\nCar\nbeurre\neta.\nhttps://openverse.org/image/\n5d960316-3209-4ea6-bf4c-458449f9a588?q=Car%20drawing, 2015. Accessed:\n2023-09-27.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative compo-\nnents with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part VI 13, pp. 446\u2013461. Springer, 2014.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby.\nAn image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim En-\ntezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexan-\nder Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,\nAlex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In\nsearch of the next generation of multimodal datasets, 2023.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand\nJoulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15180\u201315190,\nJune 2023a.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand\nJoulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180\u201315190, 2023b.\nWill Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,\nand Kevin Swersky. Your classifier is secretly an energy based model and you should treat it\nlike one. In International Conference on Learning Representations, 2020. URL https://\nopenreview.net/forum?id=Hkxzx0NtDB.\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2015.\nURL https://api.semanticscholar.org/CorpusID:206594692.\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset\nand deep learning benchmark for land use and land cover classification. IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. Proceedings of the International Conference on Learning Represen-\ntations, 2019.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\nThe many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV,\n2021.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In International conference on machine learning, pp. 4904\u20134916.\nPMLR, 2021.\n10\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661\u2013\n2671, 2019.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models.\narXiv preprint arXiv:2210.07183, 2022.\nMatthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey\nDosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao\nWang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detec-\ntion with vision transformers. arXiv preprint arXiv:2205.06230, 2022.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012\nIEEE conference on computer vision and pattern recognition, pp. 3498\u20133505. IEEE, 2012.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nKarsten Roth, Jae Myung Kim, A Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata.\nWaffling around for performance: Visual classification with random words and broad concepts.\narXiv preprint arXiv:2306.07282, 2023.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115:211\u2013252, 2015.\nManli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and\nChaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models.\nAdvances in Neural Information Processing Systems, 35:14274\u201314289, 2022.\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-\npeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00b4efan J. van der\nWalt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-\nson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore,\nJake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\nCharles R. Harris, Anne M. Archibald, Ant\u02c6onio H. Ribeiro, Fabian Pedregosa, Paul van Mul-\nbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing\nin Python. Nature Methods, 17:261\u2013272, 2020. doi: 10.1038/s41592-019-0686-2.\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully\ntest-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.\nPeter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and\nPietro Perona. Caltech-ucsd birds 200. Technical Report CNS-TR-2010-001, California Institute\nof Technology, 2010.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui\nWu.\nCoca:\nContrastive captioners are image-text foundation models.\narXiv preprint\narXiv:2205.01917, 2022.\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu,\nXuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer\nvision. arXiv preprint arXiv:2111.11432, 2021.\nShuai Zhao, Xiaohan Wang, Linchao Zhu, and Yezhou Yang. Test-time adaptation with clip reward\nfor zero-shot generalization in vision-language models.\nArXiv, abs/2305.18010, 2023.\nURL\nhttps://api.semanticscholar.org/CorpusID:258959424.\n11\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\nA\nAPPENDIX\nWe present additional experimental results. Table 2 and Table 3 show the absolute performance of\nAUTOCLIP on different datasets and VLMs for DCLIP and CLIP prompt templates, respectively,\nsimilar to Table 1 in the main paper for WaffleCLIP templates. Figure 7 illustrates prompt weights\non the ImageNetR dataset. Figure 8 contains results of AUTOCLIP in terms of \u2206 Accuracy on\nImageNetC for every corruption seperately.\nCLIP\nCLIP\nCLIP\nCLIP\nDataComp\nCoCa\nRN50\nViT-B-32\nViT-B-16\nViT-L-14\nViT-L-14\nViT-L-14\nCUB200\n47.75 (+0.1)\n53.00 (+0.4)\n57.82 (+0.3)\n64.57 (+0.3)\n85.38 (+0.4)\n73.69 (+0.1)\nEuroSAT\n36.39 (-1.2)\n45.88 (-2.2)\n59.22 (+2.5)\n57.89 (+0.8)\n60.08 (-0.7)\n57.15 (-1.2)\nFood101\n79.12 (+0.9)\n83.43 (+0.7)\n88.53 (+0.5)\n93.14 (+0.4)\n93.89 (+0.2)\n89.77 (+0.3)\nOxford Pets\n85.92 (+1.3)\n87.11 (+1.0)\n88.53 (+0.9)\n94.08 (+0.6)\n94.00 (+0.4)\n93.54 (+0.4)\nImageNet\n60.62 (+0.3)\n63.89 (+0.2)\n69.10 (+0.3)\n75.92 (+0.1)\n79.02 (+0.0)\n75.41 (+0.0)\nImageNetV2\n53.60 (+0.3)\n56.73 (+0.5)\n62.22 (+0.2)\n70.01 (+0.1)\n71.95 (-0.0)\n67.91 (-0.0)\nImageNetR\n28.14 (+1.3)\n49.51 (+1.6)\n58.37 (+1.7)\n73.12 (+0.6)\n78.06 (+0.7)\n73.73 (+1.1)\nTable 2: Accuracy of AUTOCLIP (and \u2206 Accuracy to baseline zero-shot classifier in parenthesis)\nfor K = 100 DCLIP prompt templates across models and datasets, averaged over 7 runs.\nCLIP\nCLIP\nCLIP\nCLIP\nDataComp\nCoCa\nRN50\nViT-B-32\nViT-B-16\nViT-L-14\nViT-L-14\nViT-L-14\nCUB200\n47.00 (+0.3)\n52.36 (+0.7)\n56.99 (+1.2)\n63.94 (+0.5)\n85.52 (+1.1)\n73.99 (+0.1)\nEuroSAT\n32.28 (-3.7)\n44.78 (-1.3)\n56.76 (+0.4)\n52.96 (+1.8)\n61.94 (+1.4)\n51.58 (-1.7)\nFood101\n79.69 (+1.1)\n83.64 (+0.9)\n88.83 (+0.6)\n93.33 (+0.2)\n94.55 (+0.3)\n90.36 (+0.3)\nOxford Pets\n84.30 (+1.7)\n85.20 (+2.0)\n88.42 (+0.9)\n93.24 (+1.2)\n93.79 (+1.3)\n92.67 (+1.3)\nImageNet\n59.90 (+0.2)\n63.31 (+0.3)\n68.43 (+0.2)\n75.38 (+0.1)\n79.29 (+0.1)\n75.79 (+0.2)\nImageNetV2\n52.98 (+0.5)\n56.00 (+0.4)\n62.12 (+0.2)\n69.56 (-0.1)\n72.09 (+0.0)\n67.90 (-0.0)\nImageNetR\n27.11 (+0.9)\n47.74 (+0.9)\n56.28 (+1.1)\n71.30 (+0.4)\n78.26 (+0.5)\n74.51 (+0.9)\nTable 3: Accuracy of AUTOCLIP (and \u2206 Accuracy to baseline zero-shot classifier in parenthesis)\nfor K = 100 CLIP prompt templates across models and datasets, averaged over 7 runs.\nFigure 7: Illustration of prompt template weights w on 500 samples from the ImageNetR dataset,\nwith blocks of 50 samples belonging to the same (unknown) class. CLIP backbone is a ViT-B-16\nand 30 DCLIP prompt templates are used.\n12\nAutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models\n1\n2\n3\n4\n5\n1.0\n0.5\n0.0\n0.5\n1.0\n Accuracy\nbrightness\nRN50\nViT-B-32\nViT-B-16\nViT-L-14\nViT-L-14\n (DataComp)\n1\n2\n3\n4\n5\ncontrast\n1\n2\n3\n4\n5\ndefocus_blur\n1\n2\n3\n4\n5\n1.0\n0.5\n0.0\n0.5\n1.0\n Accuracy\nelastic_transform\n1\n2\n3\n4\n5\nfog\n1\n2\n3\n4\n5\nfrost\n1\n2\n3\n4\n5\n1.0\n0.5\n0.0\n0.5\n1.0\n Accuracy\ngaussian_blur\n1\n2\n3\n4\n5\ngaussian_noise\n1\n2\n3\n4\n5\nglass_blur\n1\n2\n3\n4\n5\n1.0\n0.5\n0.0\n0.5\n1.0\n Accuracy\nimpulse_noise\n1\n2\n3\n4\n5\njpeg_compression\n1\n2\n3\n4\n5\nmotion_blur\n1\n2\n3\n4\n5\n1.0\n0.5\n0.0\n0.5\n1.0\n Accuracy\nsaturate\n1\n2\n3\n4\n5\nshot_noise\n1\n2\n3\n4\n5\nsnow\n1\n2\n3\n4\n5\nSeverity\n1.0\n0.5\n0.0\n0.5\n1.0\n Accuracy\nspatter\n1\n2\n3\n4\n5\nSeverity\nspeckle_noise\n1\n2\n3\n4\n5\nSeverity\nzoom_blur\nFigure 8: ImageNetC Accuracy improvement (\u2206 Accuracy) of AutoCLIP over baseline zero-shot\nclassifier for WaffleCLIP across models, corruptions, averaged over 7 runs.\n13\n"
  },
  {
    "title": "Demystifying CLIP Data",
    "link": "https://arxiv.org/pdf/2309.16671.pdf",
    "upvote": "16",
    "text": "Preprint\nDEMYSTIFYING CLIP DATA\nHu Xu1 Saining Xie2 Xiaoqing Ellen Tan1 Po-Yao Huang1 Russell Howes1 Vasu Sharma1\nShang-Wen Li1\nGargi Ghosh1\nLuke Zettlemoyer1,3\nChristoph Feichtenhofer1\n1FAIR, Meta\n2New York University\n3University of Washington\nABSTRACT\nContrastive Language-Image Pre-training (CLIP) is an approach that has advanced\nresearch and applications in computer vision, fueling modern recognition systems\nand generative models. We believe that the main ingredient to the success of\nCLIP is its data and not the model architecture or pre-training objective. How-\never, CLIP only provides very limited information about its data and how it has\nbeen collected, leading to works that aim to reproduce CLIP\u2019s data by filter-\ning with its model parameters. In this work, we intend to reveal CLIP\u2019s data\ncuration approach and in our pursuit of making it open to the community in-\ntroduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP\ntakes a raw data pool and metadata (derived from CLIP\u2019s concepts) and yields\na balanced subset over the metadata distribution. Our experimental study rig-\norously isolates the model and training settings, concentrating solely on data.\nMetaCLIP applied to CommonCrawl with 400M image-text data pairs outper-\nforms CLIP\u2019s data on multiple standard benchmarks.\nIn zero-shot ImageNet\nclassification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP\u2019s 68.3% on\nViT-B models. Scaling to 1B data, while maintaining the same training bud-\nget, attains 72.4%. Our observations hold across various model sizes, exem-\nplified by ViT-H achieving 80.5%, without any bells-and-whistles.\nCuration\ncode and training data distribution on metadata is made available at https:\n//github.com/facebookresearch/MetaCLIP.\n1\nINTRODUCTION\nDeep learning has revolutionized the field of artificial intelligence, and pre-trained models have\nplayed a pivotal role in democratizing access to cutting-edge AI capabilities. However, the training\ndata used to create these models is often concealed from the public eye, shrouded in secrecy.\nThe increasing availability of pre-trained models for public use contrasts sharply with the lack of\ntransparency regarding their training data. Further, proprietary concerns, such as copyright issues,\noften limit access to the original data sources. Consequently, the need to explore novel approaches\nfor curating high-quality training data that can be shared openly arises.\nIn the vision-language domain, the dominant model and learning approach is Contrastive Language-\nImage Pre-training (CLIP) (Radford et al., 2021), a simple technique to learn from image-text\npairs. We believe that the secret to the dominance of CLIP models is attributed to its high-quality\nWIT400M dataset which is curated from the web. Despite its popularity, the specifics of CLIP\u2019s\ncuration process have remained a mystery, captivating the research community for years.\nFollow-up works (Schuhmann et al., 2022; 2021) have attempted to replicate CLIP\u2019s data, but with\na notable difference in their curation method. While CLIP generates data based on its unknown data\nsource and curation methodology, these approaches remove noise by applying the CLIP model as a\nhard blackbox filter which in turn is a form of distilling WIT400M information captured in CLIP.\nThe advantages of CLIP\u2019s curation are apparent. First, it starts from scratch, avoiding the introduc-\ntion of biases through filters. Second, CLIP\u2019s curation process balances the data distribution over\nmetadata, maximizing signal preservation while mitigating, rather than removing, noise in the data1.\nSuch distribution lays the groundwork for task-agnostic data, a crucial part of foundation models.\n1For example, a filter on digits can remove noise from date or id strings but remove signal for tasks that\ninvolve OCR (e.g., MNIST), or a filter removing text with less than 5 characters can remove signal \u201cdog\u201d.\n1\narXiv:2309.16671v3  [cs.CV]  2 Oct 2023\nPreprint\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\n400000\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nImageNet Zero-shot Acc.\nRaw(1.1B)\nRaw English(400M)\nMetaCLIP w/o bal.(400M)\nMetaCLIP(400M)\nCLIP(400M)\nLAION(407M)\nRaw(1.1B)\nRaw English(400M)\nMetaCLIP w/o bal.(400M)\nMetaCLIP(400M)\nFigure 1: ViT-B/32 on ImageNet zero-shot classifica-\ntion with fixed training steps (12.8B seen pairs and\ntraining/validation data has been de-duplicated). Raw:\nraw CommonCrawl (CC) distribution; Raw English:\nEnglish only CC; MetaCLIP w/o bal.: curated (sub-\nstring matched) data pool from CC; MetaCLIP: curated\nand balanced metadata distribution. Metadata cura-\ntion boosts performance significantly and balancing is\nequally important. Our MetaCLIP data significantly\noutperforms CLIP\u2019s WIT400M and LAION data.\nIn this paper, we attempt to reveal CLIP\u2019s\nmethod around training data curation. We\npresent an empirical study on data cura-\ntion, with frozen model architecture and\ntraining schedule. We focus solely on the\nimpact of training data, excluding other\nfactors that could confound the results. We\nmake several observations for good data\nquality and present a simple algorithm to\nmake CLIP\u2019s curation more transparent.\nConsequently, we shed light on both the\ncuration process and the resulting training\ndata distribution. Our algorithm enables\neasy adaptation to different data pools,\nallowing parties to fully own their data\npipeline without relying on blackbox fil-\nters from external providers.\nOur algorithm takes a raw data pool\nD\nand\nmetadata\nM\n(derived\nfrom\nCLIP\u2019s queries or visual concepts) and\nyields a balanced subset D\u2217 over M:\nD\u2217 \u2190 f(D; M). Our approach, named\nMetadata-Curated Language-Image Pre-\ntraining (MetaCLIP), marks a significant\nstep towards making the curation process\nmore transparent and accessible.\nMetaCLIP applied to CommonCrawl (CC)\nwith 400M data points outperforms CLIP\non multiple standard benchmarks.\nIn\nterms of zero-shot ImageNet classifica-\ntion, using ViT (Dosovitskiy et al., 2020)\nmodels of various sizes. Our MetaCLIP achieves 70.8% vs CLIP\u2019s 68.3% on ViT-B and 76.2% vs\n75.5% on ViT-L. Scaling to 2.5B data, with the same training budget and similar distribution boosts\nthis to unprecedented accuracy of 79.2% for ViT-L and 80.5% for ViT-H in the vanilla training\nsetting (not using any external data, models, or longer training).\nIn Fig.1, we show the impact of metadata curation on ImageNet validation plotted over training\nsteps. First, we are training on Raw English data from the web (400 image-text pairs, 57.4% accu-\nracy), after applying Language IDentification (LID) to the random Raw set (\u223c1.1B pairs, 54.1%).\nUsing metadata to curate the training set (MetaCLIP 400M w/o bal, 60.8%) performs significantly\nbetter than these baselines, and using balancing significantly increases accuracy further (MetaCLIP,\n65.5%), outperforming similar datasets, WIT400M from CLIP, 63.4% and LAION 400M, 60.0%.\n2\nRELATED WORK\nThe training data of CLIP differs significantly from a traditional supervised dataset (Gadre et al.,\n2023) in various aspects. Firstly, it involves large-scale training with mixed-quality image-text pairs\nrather than categorized images with human annotated labels, as commonly seen in classification\ndatasets. Secondly, CLIP\u2019s pre-training is the initial stage of training, assuming no access to previ-\nously trained models.\nData Pruning on Established Datasets.\nCurrent research on data algorithms primarily revolves\naround data pruning techniques applied to well-established datasets using pre-trained models\n(Sorscher et al., 2022; Abbas et al., 2023). These approaches, such as coreset selection techniques\n(Har-Peled & Mazumdar, 2004; Feldman et al., 2011; Bachem et al., 2015; Mirzasoleiman et al.,\n2020; Toneva et al., 2018), aim to select a subset of data that yields similar performance to train-\n2\nPreprint\ning on the entire dataset. However, this post-hoc data pruning approach has limited utility, as the\ncomputational resources saved have already been expended during the initial training of the model.\nHandling Noisy Internet Data.\nAddressing noisy data from the Internet is a significant challenge,\nand existing approaches often heavily rely on human-designed filter systems. Classical methods\ninvolve dataset cleaning and outlier removal (Jiang et al., 2001; Yu et al., 2002) to discard samples\nthat may introduce undesirable biases to models.\nReplicating CLIP\u2019s Training Data.\nRecent efforts, such as LAION (Schuhmann et al., 2021;\n2022) and concurrent work DataComp (Gadre et al., 2023), attempt to replicate CLIP\u2019s training\ndata. However, they adopt fundamentally different strategies for several reasons. First, the data used\nin these approaches are post-hoc, filtered, by vanilla CLIP as a teacher model. Second, the curation\nprocess in these methods relies on a labor-intensive pipeline of filters, making it challenging to\ncomprehend the resulting data distribution from the raw Internet (refer to the unknown biases of\nusing CLIP filter in (Schuhmann et al., 2022)). Thirdly, the goal is to match the quantity of CLIP\u2019s\ntarget data size rather than the data distribution itself, which may lead to an underestimation of the\ndata pool size needed to obtain sufficient quality data. Consequently, the performance on the 400M\nscale is sub-optimal, with LAION400M only achieving 72.77% accuracy on ViT-L/14 on ImageNet,\nwhereas vanilla CLIP obtains 75.5%.\nImportance of Understanding CLIP\u2019s Data Curation.\nThe observations made in these studies\nunderscore the critical importance of understanding how OpenAI CLIP curates its data in the first\nplace. A comprehensive understanding of the curation process can shed light on the factors that\ncontribute to its success, allowing researchers to devise more effective and efficient algorithms for\nfuture vision-language pre-training endeavors.\n3\nMETACLIP\nThe original paper (Radford et al., 2021) only provides limited details about how CLIP curates\nits data. Since important design choices for a direct reproduction are missing, we will clarify our\nchoices in this section. Our goal is to uncover CLIP\u2019s data curation process, which involves preserv-\ning signal in the data while minimizing noise. In this section, we will explain the principles we have\nadopted to achieve this, which may differ from CLIP\u2019s as these are not known publicly.\nCLIP\u2019s WIT400M is curated with an information retrieval method, quoting (Radford et al., 2021):\n\u201cTo address this, we constructed a new dataset of 400 million (image,\ntext) pairs collected from a variety of publicly available sources on the In-\nternet.\nTo attempt to cover as broad a set of visual concepts as possible,\nwe search for (image, text) pairs as part of the construction process whose\ntext includes one of a set of 500,000 queries We approximately class bal-\nance the results by including up to 20,000 (image, text) pairs per query.\n\u201d\nWe rigorously adhere to this description and provide detailed insights into the construction process\nof CLIP\u2019s metadata (in \u00a73.1)2, sub-string matching (in \u00a73.2), inverted indexing (in \u00a73.3), as well as\nquery and balancing (in \u00a73.4).\n3.1\nMETADATA CONSTRUCTION: M = {entry}\nWe start by re-building CLIP\u2019s 500,000-query metadata, citing Radford et al. (2021):\n2We generalize the term queries (used by CLIP) as entries in metadata because metadata describe training\ndata and our algorithm does not require search on inverted index yet have similar effects.\n3\nPreprint\n\u201cThe base query list is all words occurring at least 100 times in the English ver-\nsion of Wikipedia. This is augmented with bi-grams with high pointwise mutual\ninformation as well as the names of all Wikipedia articles above a certain search\nvolume. Finally all WordNet synsets not already in the query list are added.\n\u201d\nThe metadata (\u2018queries\u2019 or \u2018entries\u2019) consists of four components: (1) all synsets of WordNet, (2)\nuni-grams from the English version of Wikipedia occurring at least 100 times, (3) bi-grams with high\npointwise mutual information, and (4) titles of Wikipedia articles above a certain search volume. We\nrebuild these components from WordNet and Wikipedia and summarize the statistics in Table 13. We\nestimate the thresholds for components (3) and (4) as in the 3rd column of Table 1, by first choosing\na point-wise mutual information threshold of 30 that meets the budget of 100k entries for bi-grams\nand then fill the rest of the entries with Wikipedia titles.\nSource\n# of Entries\nDesc. of Threshold\nThreshold\nWordNet synsets\n86,654\nN/A\n[ALL] (follow CLIP)\nWiki uni-gram\n251,465\nCount\n100 (follow CLIP)\nWiki bi-gram\n100,646\nPointwise Mutual Info.(PMI)\n30 (estimated)\nWiki titles\n61,235\nView Frequency\n70 (estimated)\nTable 1: Composition of MetaCLIP Metadata.\n3.2\nSUB-STRING MATCHING: text \u2192 entry\nAfter constructing the metadata, CLIP\u2019s curation aligns a pool of image-text pairs with metadata\nentries through sub-string matching. This process identifies texts that contain any of the metadata\nentries, effectively associating unstructured texts with structured metadata entries. The sub-string\nmatching step retains only high-quality matching texts, automatically filtering out various types of\nnoises that a typical filter system would consider on a case-by-case basis.\nSuch alignment is referred to as sub-string matching in Radford et al. (2021):\n\u201cWe also restrict this step in CLIP to text-only querying for sub-string matches\nwhile most webly supervised work uses standard image search engines ...\n\u201d\nImage-Text Pair Pool\nWe start by estimating the pool size used by CLIP\u2019s curation. CLIP\u2019s data\nsource is unknown to us (\u201ca variety of publicly available sources\u201d in Radford et al. (2021)). We\nadopt CommonCrawl (CC)4 as the source to build such a pool and re-apply sub-string matching\nto this source. We ended with a pool of 1.6B image-text pairs (5.6B counts of sub-string matches).\nNote that one text can have multiple matches of entries and we have 3.5 matches per text on average.\nAs a result, sub-string matching builds the mapping txt \u2192 entry. This step has two outcomes: (1)\nlow-quality text is dropped; (2) unstructured text now has a structured association with metadata.\nFor all English text, \u223c50% image-text pairs are kept in this stage. Similar to CiT (Xu et al., 2023),\nthis approach looks for quality matches and automatically gets rid of some type of noise (such as\ndate strings) that a typical filter system would remove consider case-by-case (e.g., regular expression\non dates, ids etc.).\n3Note that we cannot find Wikipedia\u2019s search volume for titles of Wikipedia (4). Instead, we use volumes\nof Pageviews on Wiki articles. We randomly selected 26 days\u2019 Pageviews from Apr. 2018 to Sep. 2022.\n4https://commoncrawl.org\n4\nPreprint\nMetadata Subset\n# of Entries\n# of Counts\nFull\n500K\n5.6B\nCounts = 0\n114K\n0\nCounts > 20000\n16K\n5.35B\nTable 2: Summary of counts for entries.\nEntry\nCounts\nEntry\nCounts\nEntry\nCounts\nEntry\nCounts\nof\n120M\nin\n107M\nand\n100M\nfor\n89M\nthe\n87M\nThe\n67M\nwith\n67M\nto\n61M\nphoto\n54M\na\n50M\nimage\n48M\n1\n47M\non\n45M\nby\n43M\n2\n43M\nImage\n39M\nat\n38M\nBlack\n33M\n3\n30M\nA\n29M\nTable 3: Top-20 entries with counts.\n3.3\nINVERTED INDEXING: entry \u2192 text\nFollowing sub-string matching, CLIP builds an inverted index of the data pool. All texts associ-\nated with each metadata entry are aggregated into lists, creating a mapping from each entry to the\ncorresponding texts, entry \u2192 text.\nAs an analysis, we count the number of matches for each entry and summarize that in Table 2. The\ncounts exhibit a long-tailed distribution. Out of the 500k entries, 114k entries have no matches. This\nsignifies the importance of knowing the training data distribution since it is very likely the training\ndata does not have certain visual concepts. We observed that only 16k entries had counts higher\nthan 20k, accounting for only 3.2% (16k/500k) of the entries, but their counts made up 94.5%\n(5.35B/5.6B) of the total counts of all entries.\nTop Entries.\nWe show the top entries of the matching in Table 3. Interestingly, many of these\nare stopwords, which don\u2019t carry specific meaning but can enhance the overall text quality (e.g., by\ngenerating grammatically correct sentences rather than just keyword lists). It\u2019s important to note\nthat although sub-string matching aims to select only high-quality texts, there are instances where\ncommon entries may still include irrelevant texts. For instance, the entry \"photo\" could match with\nthe popular but unhelpful term \"untitled photo\". These noise-related issues can be addressed in the\nsubsequent stage of processing.\n3.4\nQUERY AND BALANCING WITH t \u226420K\nThe key secret behind OpenAI CLIP\u2019s curation is to balance the counts of matched entries. For\neach metadata entry, the associated list of texts (or image-text pairs) is sub-sampled, ensuring that\nthe resulting data distribution is more balanced. This step aims to mitigate noise and diversify the\ndistribution of data points, making the data more task-agnostic as foundation data for pre-training.\nThe magic number t = 20k is a threshold used to limit the number of texts/pairs for each entry.\nEntries with fewer than t pairs (tail entries) retain all associated pairs, while entries with more than t\npairs (head entries) are sub-sampled to t pairs. The selection is based on the density of information in\ntexts; texts with more matched entries have a higher chance of being curated (recall that the average\nis 3.5 matches per text).\nTo study the effect of the magic number t = 20k, we plot the cumulative sum of counts for entries\nsorted by counts from tail to head in Fig. 2. Interestingly, the value of t = 20k seemingly represents\nthe transition from tail to head entries, when the head entries start exhibiting an exponential growth\nrate. By applying a max count of t, the growth rate of total counts (i.e., the scale of resulting data\npoints) is reduced to linear. This significantly flattens (and balances) the training data distribution.\nWe further study the optimality of t = 20k for the 400M data scale in our experiments.\nIn summary, balancing yields three interesting outcomes:\n5\nPreprint\n t=20k (400M)\nPool (1.6B)\nFigure 2: Cumulative sum of counts on entries from tail to head on a data pool with 1.6B image-text\npairs (5.6B match counts). (1) raw/unbalanced cumulative counts, t = \u221e; (2) balanced cumulative\ncounts after applying t = 20k. The limit t defines the transition of tail/head entries.\n(i) It reduces dominance and noise from head entries, like common web terms. E.g., out of 400M\npairs, only 20k texts containing \u201cphoto\u201d are kept (while there are 54M \u201cphoto\u201d instances in the\npool).\n(ii) It diversifies the data distribution and balances tail/head entries, leading to a more task-agnostic\nfoundation.\n(iii) Sampling for each entry ensures that data points with more matched entries or denser informa-\ntion are prioritized for curation.\nDiscussion.\nCLIP employs a pure NLP-based approach, requiring no access to ML models and\nminimizing explicit/implicit priors from humans. The metadata plays a central role in mitigating\nnoise and preserving signal in the data distribution. The balancing step effectively flattens the data\ndistribution, diversifying the data and making it more suitable as foundation data for pre-training\ntasks. We analyze the effects of balancing in Appendix A.3.\n3.5\nA SIMPLE ALGORITHM FOR CURATION\nThis section presents an algorithm that formalizes the curation process described earlier. The al-\ngorithm aims to improve scalability and reduce space complexity for operations across data points,\nsuch as inverted indexing and sub-sampling. Instead of building inverted indexes, the algorithm only\nmaintains total counts for each entry.\nWe assume that CLIP curation constructs an inverted index that maps entries to documents (image-\ntext pairs) to enable efficient search for each entry (\u201cwe search for (image-text) pairs\u201d in Radford\net al. (2021)). In contrast, our algorithm approaches the balancing process through independent\nsampling. This avoids the need to build an inverted index that could potentially store hundreds of\nmillions of concrete pairs for popular entries, thereby improving efficiency and scalability.\nOur algorithm takes three inputs: metadata M, a data pool D, and a hyper-parameter t. It aims to\nfind a subset D\u2217 with a balanced distribution over M, denoted as D\u2217 \u2190 f(D; M, t). The algorithm\nconsists of two parts, each corresponding to a specific stage of the curation process.\nWe provide the Python pseudo-code in Algorithm 1.\n6\nPreprint\nAlgorithm 1: Pseudo-code of Curation Algorithm in Python style (see Sec. A.7 for samples).\n# D: raw image-text pairs;\n# M: metadata;\n# t: max matches per entry in metadata;\n# D_star: curated image-text pairs;\nD_star = []\n# Part 1: sub-string matching: store entry indexes in text.matched_entry_ids and output\ncounts per entry in entry_count.\nentry_count = substr_matching(D, M)\n# Part 2: balancing via indepenent sampling\nentry_count[entry_count < t] = t\nentry_prob = t / entry_count\nfor image, text in D:\nfor entry_id in text.matched_entry_ids:\nif random.random() < entry_prob[entry_id]:\nD_star.append((image, text))\nbreak\nPart 1:\nEntry Counts from Sub-string Matching.\nThis corresponds to Sec. 3.2.\nThe\nsubstr_matching function outputs the total counts of matches per entry, entry_count,\nrepresented as a NumPy array indexed by entry_id.\nEach text is associated with\nmatched_entry_ids that contains a list of matched entries.\nPart 2: Balancing via Independent Sampling.\nThis part corresponds to Sec.3.3 and Sec.3.4\nand focuses on balancing counts on entries. Instead of building an expensive inverted index with\nassociated lists of texts for each entry, we sample each data point independently.\nWe first compute the probability of sampling each entry, entry_prob, where tail entries\n(entry_count < t) have a probability equal to 1, and head entries have a probability less than\n1. We iterate through all image-text pairs and sample/curate each pair. When an image-text pair has\na matched entry sampled/selected, we include that pair in D\u2217.\nThis procedure is equivalent to CLIP\u2019s curation, because if one image-text pair has one or more\nmatched entries, the chance of that pair being selected is determined by the probability of sampling\nfor each individual entry: t/entry_count[entry_id]. As long as one entry selects that pair,\nit will be kept in D\u2217. Our independent sampling approach allows us to scale balancing for each data\npoint independently and reduces the global operation to counting the total matches for each entry.\nWe demonstrate case studies in experiments on (1) scaling curation in a data pipeline and (2) online\nbalancing in data loader.\n4\nEXPERIMENTS\nData Pools.\nWe collect two pools of data:\nPool 1 contains 1.6 billion image-text pairs with a total of 5.6 billion counts of matches. This pool\nwas used to estimate a target of 400M image-text pairs, collected from 15 snapshots of Common-\nCrawl (CC) from January 2021 to January 2023.\nPool 2 aims to scale curation in our data pipeline. We parsed all 90 CC snapshots from 2013 to\nApril 2023, using our algorithm (see \u00a7A.2 for details on the curation pipeline) to curate from a pool\nof 10.7B matched image-text pairs that are originally from a large set of URL-text pairs, which\nhave undergone de-duplication, English Language IDentification (LID) and sub-string matching.\nHowever, we only perform (expensive) image downloading, storing, and transferring for data points\nthat are distribution-calibrated and selected by our algorithm.\nFor balancing we consider 2 scenarios on this data: (i) t = 170k, which is resulting in 2.5B image-\ntext pairs. This t = 170k configuration has tail counts amounting to 6% of the total counts, the same\ntail/head ratio that the 400M Pool 1 data has, produced by applying t = 20k on the 1.6B Pool 1\ndata. (ii) The t = 20k threshold applied to Pool 2 which results in 1B image-text pairs and compared\nto the 400M set from Pool 1 only increases tail metadata matches (head counts are capped at 20k).\n7\nPreprint\nAverage\nImageNet\nFood-101\nCIFAR10\nCIFAR100\nCUB\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nMNIST\nFER-2013\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nKITTI\nCountry211\nPCAM\nUCF101\nKinetics700\nCLEVR\nHatefulMemes\nSST2\nViT-B/32\nCLIP, our eval.\n56.6\n63.4\n83.7\n89.8\n65.1\n53.7\n62.0\n59.7\n19.6\n44.0\n87.2\n87.4\n66.9\n48.2\n46.6\n97.1\n44.9\n61.0\n32.6\n28.7\n17.2\n62.5\n63.9\n48.0\n23.6\n56.4\n58.6\nOpenCLIP, our eval. 57.6\n62.9\n80.7\n90.7\n70.6\n61.2\n66.4\n79.2\n16.7\n54.5\n86.5\n90.7\n66.1\n37.4\n48.2\n95.6\n52.2\n58.0\n42.0\n38.0\n14.8\n50.1\n63.0\n42.8\n22.5\n53.3\n52.3\nMetaCLIP\n58.2\n65.5\n80.6\n91.3\n70.2\n63.4\n63.0\n70.7\n26.8\n52.8\n88.7\n91.9\n68.5\n41.5\n35.9\n95.4\n52.6\n64.2\n35.8\n30.7\n17.2\n55.5\n66.1\n45.4\n30.6\n56.4\n53.4\nViT-B/16\nCLIP, our eval.\n59.6\n68.3\n88.8\n90.8\n68.2\n55.6\n64.0\n64.6\n24.0\n45.1\n88.9\n89.1\n69.4\n51.8\n53.0\n98.2\n54.8\n65.5\n43.3\n21.7\n22.8\n56.3\n68.5\n52.3\n25.5\n58.7\n60.5\nOpenCLIP, our eval. 60.4\n67.0\n85.8\n91.7\n71.4\n65.3\n69.2\n83.6\n17.4\n51.0\n89.2\n90.8\n66.5\n66.3\n46.1\n97.0\n52.2\n65.7\n43.5\n23.7\n18.1\n51.7\n67.0\n46.2\n33.9\n54.5\n54.4\nMetaCLIP\n61.1\n70.8\n86.8\n90.1\n66.5\n70.8\n66.6\n74.1\n27.9\n55.9\n90.4\n93.8\n72.3\n47.8\n44.6\n97.2\n55.4\n68.8\n43.8\n33.4\n22.6\n52.9\n68.0\n49.5\n22.8\n54.8\n60.6\nViT-L/14\nCLIP, our eval.\n65.7\n75.5\n93.0\n95.6\n78.3\n63.3\n66.8\n77.8\n31.3\n55.3\n93.6\n93.3\n79.3\n76.4\n56.9\n99.4\n61.9\n70.9\n50.6\n19.2\n31.9\n50.1\n75.7\n60.2\n22.3\n59.7\n68.9\nOpenCLIP, our eval. 64.5\n72.7\n90.0\n94.7\n78.0\n73.9\n72.4\n89.5\n24.7\n60.2\n91.6\n93.6\n73.0\n76.1\n54.3\n98.1\n63.9\n69.6\n49.9\n16.0\n23.0\n51.7\n71.5\n51.6\n25.4\n55.3\n56.0\nMetaCLIP\n67.1\n76.2\n90.7\n95.5\n77.4\n75.9\n70.5\n84.7\n40.4\n62.0\n93.7\n94.4\n76.4\n61.7\n46.5\n99.3\n59.7\n71.9\n47.5\n29.9\n30.9\n70.1\n75.5\n57.1\n35.1\n56.6\n65.6\nTable 4: MetaCLIP-400M vs. CLIP (WIT400M data) and OpenCLIP (LAION-400M data). We use\n3 different model scales (ViT-B/32 and -B/16 and -L/14) and an identical training setup as CLIP.\nAverage\nImageNet\nFood-101\nCIFAR10\nCIFAR100\nCUB\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nMNIST\nFER-2013\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nKITTI\nCountry211\nPCAM\nUCF101\nKinetics700\nCLEVR\nHatefulMemes\nSST2\nViT-B/32\nMetaCLIP(400M) 58.2\n65.5\n80.6\n91.3\n70.2\n63.4\n63.0\n70.7\n26.8\n52.8\n88.7\n91.9\n68.5\n41.5\n35.9\n95.4\n52.6\n64.2\n35.8\n30.7\n17.2\n55.5\n66.1\n45.4\n30.6\n56.4\n53.4\nMetaCLIP(1B)\n60.3\n67.3\n81.9\n95.2\n76.7\n71.4\n65.9\n73.0\n31.4\n58.9\n89.5\n92.5\n72.6\n35.4\n45.8\n96.3\n50.4\n64.6\n40.7\n32.0\n17.0\n64.2\n70.3\n47.8\n14.6\n54.9\n56.8\nMetaCLIP(2.5B)\n59.8\n67.6\n82.6\n95.2\n77.7\n67.8\n66.8\n77.2\n26.9\n58.9\n90.9\n92.5\n69.7\n42.7\n48.3\n96.3\n49.9\n66.5\n39.2\n29.3\n17.7\n50.0\n68.0\n47.6\n19.4\n53.5\n53.1\nViT-B/16\nMetaCLIP(400M) 61.1\n70.8\n86.8\n90.1\n66.5\n70.8\n66.6\n74.1\n27.9\n55.9\n90.4\n93.8\n72.3\n47.8\n44.6\n97.2\n55.4\n68.8\n43.8\n33.4\n22.6\n52.9\n68.0\n49.5\n22.8\n54.8\n60.6\nMetaCLIP(1B)\n63.2\n72.4\n88.1\n94.8\n78.2\n77.5\n66.4\n79.3\n38.0\n57.7\n92.3\n93.6\n75.1\n36.4\n47.8\n98.0\n50.5\n70.1\n49.5\n36.6\n21.6\n53.7\n74.1\n52.7\n21.6\n56.8\n61.6\nMetaCLIP(2.5B)\n63.5\n72.1\n88.3\n95.7\n79.0\n71.4\n68.5\n82.9\n30.3\n62.1\n91.7\n93.3\n73.9\n66.1\n47.0\n98.4\n51.1\n71.1\n46.6\n16.6\n22.7\n50.5\n73.0\n52.5\n30.8\n57.4\n59.0\nViT-L/14\nMetaCLIP(400M) 67.1\n76.2\n90.7\n95.5\n77.4\n75.9\n70.5\n84.7\n40.4\n62.0\n93.7\n94.4\n76.4\n61.7\n46.5\n99.3\n59.7\n71.9\n47.5\n29.9\n30.9\n70.1\n75.5\n57.1\n35.1\n56.6\n65.6\nMetaCLIP(1B)\n70.2\n79.0\n92.9\n96.8\n84.9\n83.1\n72.8\n86.5\n48.9\n65.9\n95.3\n94.8\n84.7\n53.8\n54.1\n99.3\n70.0\n73.8\n58.7\n36.3\n32.2\n70.4\n81.4\n61.6\n21.1\n61.2\n66.1\nMetaCLIP(2.5B)\n69.8\n79.2\n93.4\n97.6\n84.2\n80.1\n73.8\n88.7\n44.6\n68.1\n94.7\n95.4\n81.8\n64.4\n55.1\n99.3\n59.2\n74.6\n56.3\n29.7\n34.0\n67.3\n81.6\n62.0\n25.9\n58.0\n66.7\nViT-H/14\nMetaCLIP(2.5B)\n72.4\n80.5\n94.2\n98.0\n86.4\n83.4\n74.1\n90.0\n50.2\n72.4\n95.4\n95.6\n85.1\n72.7\n55.2\n99.4\n66.3\n74.6\n62.5\n38.2\n37.2\n65.8\n82.2\n64.1\n30.1\n59.3\n69.2\nTable 5: Scaling MetaCLIP from 400M (t=20k) to 1B (t=20k) and 2.5B (t=170k) training data.\nTraining Setup\nWe strictly follow the CLIP training setup, using V100 32GB GPUs and an equiv-\nalent global batch size of 32,768. For ViT-B/32 and ViT-B/16, we use 64 GPUs with a per GPU batch\nsize of 512 and for ViT-L/14 we use 128 GPUs with a 256 per GPU batch size. It takes 4 days to train\nViT-B/32 and a month to train ViT-L/14. We use 256 A100 80GB GPUs to train ViT-H/14 model\nfor 1 week. We train in all experiments for the same number of iterations that correspond to 12.8B\nseen image-text pairs during training (32 epochs for 400M). We pre-process with face-blurring.\n4.1\nRESULTS\nZero-shot Image Classification.\nWe follow the standard evaluation benchmark and made sure all\nprompts and class names were the same as those used by CLIP Radford et al. (2021). We also re-\nevaluated OpenAI/OpenCLIP\u2019s checkpoints to avoid differences caused by benchmark data copies.\nThe results are shown in Tab 4.\nIn Table 4, we observe that MetaCLIP outperforms OpenAI CLIP on ImageNet and average accuracy\nacross 26 tasks, for 3 model scales. With 400 million training data points on ViT-B/32, MetaCLIP\noutperforms CLIP by +2.1% on ImageNet and by +1.6% on average. On ViT-B/16, MetaCLIP\noutperforms CLIP by +2.5% on ImageNet and by +1.5% on average. On ViT-L/14, MetaCLIP\noutperforms CLIP by +0.7% on ImageNet and by +1.4% on average across the 26 tasks.\nWe next turn to Pool 2 which is a larger set of image-text pairs and study the effect of scaling data. In\nTable 5, we scale data to 1B and 2.5B and observe a large gain over 400M, with similar performance\nfor both 1B and 2.5B scales. Note that the number of training iterations (and therefore compute)\nis the same for all rows. The main difference between 1B and 2.5B is the threshold t, where 1B is\na more balanced set by adding more data points (compared to the 400M set) to tail entries (up to\nt = 20k), instead the 2.5B set adds (up to t = 170k) data points to all, head and tail, entries. The\nextra data in the tail entries (1B set), seems to benefit downstream accuracy for tasks on specific\ndata such as CUB fine-grained bird classification, Flowers, KITTI, PCAM, while the larger 2.5B\ndata that has more head entries increases broadly over more datasets, but each at a smaller amount.\nThe overall average accuracies are similar for 1B and 2.5B (e.g., 70.2% vs. 69.8% for ViT-L model\nsize). On ImageNet, the 2.5B training data achieves 67.6% on ViT-B/32 that breaks the previous\nbelieved saturated B/32 models (Cherti et al., 2022), 79.2% on ViT-L/14 and 80.5% on ViT-H/14.\n8\nPreprint\nPool 2, t=170k (2.5B)\nPool 2 (10.7B)\nPool 2, t=20k (1B)\nPool 1, t=20k (400M)\nPool 1 (1.6B)\nFigure 3: Cumulative sum of counts on entries from tail to head on a data Pool 2. We again show (1)\nraw/unbalanced cumulative counts), t = \u221e; (2) balanced cumulative counts after applying t = 20k\nand t = 170k. t defines maximum number of counts per entry and the transition of tail/head entries.\nWe show the Pool 1 configuration from Fig. 2 as dashed lines for reference.\nWe plot the cumulative sum of counts for entries sorted by counts from tail to head in Fig. 3 for\nall these cases, similar to Fig. 2 for Pool 1 (and the Pool 1 configuration as dashed lines). The plot\nshows that the 2.5B data is still relatively long-tail, while the 1B data is more balanced, explaining\nit\u2019s better performance on specific data such as bird and flower types observed above.\n4.2\nABLATION STUDY\nWe show ablations for MetaCLIP for the 400M scale and ViT-B/32 in Table 6. We first ablate\ndifferent balancing thresholds t. We observe that the choice of t = 20k by CLIP yields the best\nperformance for ImageNet and averaged accuracy and t = 15k or t = 35k are slightly worse.\nTo understand the key effect of balancing, we use the whole matched pool (1.6B image-text pairs) to\ntrain CLIP. Surprisingly, training on 4\u00d7 more data (on head entries) significantly hurts the accuracy\non ImageNet (61.9 vs 65.5) and averaged accuracy across 26 tasks (56.6 vs 58.2).\nBalancing can also be applied online in the data loader with head entries down-sampled leading to\nslightly better performance (58.5 vs 58.2); see appendix for details. This is useful if head data has\nalready been collected and one wants to train on a different distribution. The better accuracy for\nonline balancing is explained by the larger diversity in head data.\nAverage\nImageNet\nFood-101\nCIFAR10\nCIFAR100\nCUB\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nMNIST\nFER-2013\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nKITTI\nCountry211\nPCAM\nUCF101\nKinetics700\nCLEVR\nHatefulMemes\nSST2\nMetaCLIP t=20k\n58.2\n65.5\n80.6\n91.3\n70.2\n63.4\n63.0\n70.7\n26.8\n52.8\n88.7\n91.9\n68.5\n41.5\n35.9\n95.4\n52.6\n64.2\n35.8\n30.7\n17.2\n55.5\n66.1\n45.4\n30.6\n56.4\n53.4\n- t=15k\n57.5\n65.5\n79.9\n90.4\n68.8\n65.7\n64.6\n69.4\n25.6\n52.1\n88.8\n91.9\n69.5\n35.8\n39.7\n96.5\n54.0\n64.1\n34.8\n30.6\n16.1\n52.3\n67.1\n45.4\n22.3\n51.2\n53.8\n- t=35k\n57.8\n65.4\n79.3\n91.2\n69.0\n63.0\n65.0\n72.0\n28.5\n52.7\n88.5\n91.8\n68.0\n42.0\n23.0\n96.2\n50.0\n63.8\n40.2\n32.4\n17.7\n56.1\n64.2\n44.8\n28.0\n55.4\n54.2\n- unbalanced (1.6B) 56.6\n61.9\n76.9\n90.0\n67.6\n50.8\n65.8\n77.0\n19.9\n51.0\n83.1\n91.5\n64.5\n58.2\n37.0\n95.1\n55.2\n58.2\n41.4\n32.2\n15.1\n51.0\n59.2\n42.6\n17.2\n55.6\n52.6\n- online balancing\n58.5\n66.1\n80.8\n89.9\n68.8\n65.7\n65.4\n71.6\n27.9\n55.1\n88.2\n92.7\n68.8\n38.3\n42.1\n96.5\n54.5\n64.8\n36.2\n29.1\n17.6\n58.8\n66.0\n45.8\n22.0\n56.0\n52.4\nTable 6: Ablation studies on balancing in MetaCLIP. Default: t=20k, 400M. Model: ViT-B/32.\n5\nCONCLUSION\nIn this paper, we attempt to reveal CLIP\u2019s data curation. Our MetaCLIP builds upon metadata for\ncuration and balancing of raw data sourced from the web. Curating with metadata and balancing\nare essential for good data quality, significantly outperforming the use of raw data. Our experiments\nshow that MetaCLIP performs well for different scales sourced from CommonCrawl data and out-\nperforms CLIP\u2019s proprietary data source, without reliance on any external model. We make our\npipeline for generating the data publicly available.\n9\nPreprint\nACKNOWLEDGMENTS\nWe thank Zeyuan Allen-Zhu, and Chunting Zhou for the insightful discussion and Brighid Meredith\nfor suggestions on scaling the pipeline.\nA\nAPPENDIX\nA.1\nADDITIONAL RESULTS\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\n400000\nTraining Steps\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nImageNet Zero-shot Acc.\nMetaCLIP(1B,DataComp)\nMetaCLIP(400M)\nMetaCLIP(1B)\nMetaCLIP(2.5B)\nCLIP(400M)\nLAION(407M)\nMetaCLIP(1B,DataComp)\nMetaCLIP(400M)\nMetaCLIP(1B)\nMetaCLIP(2.5B)\nFigure 4: ViT-B/32 on our Pool 1, Pool 2 and DataComp\u2019s\nunfiltered 12.8B pool. We show ImageNet zero-shot classi-\nfication with a fixed 12.8B seen pair training budget. Meta-\nCLIP\u2019s curation is effective for all pools. However, with the\nsame curation method, the unfiltered DataComp-12.8B pool\nlacks quality (we suspect it is caused by implicit filters in\nthe parser of DataComp).\nCuration from DataComp-12.8B.\nThe concurrent work Gadre et al.\n(2023) released a collection of 12.8B\nimage-text pairs from CommonCrawl\nfrom 2014-2022. We further inves-\ntigate whether we can apply the al-\ngorithm on its 12.8 unfiltered pool.\nAlthough the unfiltered pool seem-\ningly offers an opportunity to apply\nour algorithm on a publicly available\nsource, our initial studies show that,\nimplicit biases may still be present\nin this pool.\nFor example, we no-\ntice that all image URLs are collected\nas a string starting with http. This\nexcludes relative URLs that could\nbe frequently used by quality web-\nsites (with potentially good image-\ntext pairs).\nWe curate from Data-\nComp\u2019s 12.8B unfiltered pool with\nt=60k, which has 6% of tail counts\nthat is the same as t=20k for 400M\nfrom our 1.6B pool.\nWhen using 1B image-text pairs cu-\nrated from DataComp\u2019s pool we no-\ntice a quality drop during training,\ncompared to data curated from our\npools, see Fig. 4.\nOur smaller\n400M set is slightly better than us-\ning DataComp-1B and our larger sets\n(1B, 2.5B) are significantly better.\nIn Table 7, we show our 400M data vs our curated DataComp-1B data at various model scales,\nwhere the same observation holds, suggesting our raw data pool is more effective.\nAverage\nImageNet\nFood-101\nCIFAR10\nCIFAR100\nCUB\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nMNIST\nFER-2013\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nKITTI\nCountry211\nPCAM\nUCF101\nKinetics700\nCLEVR\nHatefulMemes\nSST2\nViT-B/32\nMetaCLIP (400M)\n58.2\n65.5\n80.6\n91.3\n70.2\n63.4\n63.0\n70.7\n26.8\n52.8\n88.7\n91.9\n68.5\n41.5\n35.9\n95.4\n52.6\n64.2\n35.8\n30.7\n17.2\n55.5\n66.1\n45.4\n30.6\n56.4\n53.4\nMetaCLIP (1B,DataComp) 57.5\n65.4\n81.5\n88.7\n68.5\n59.4\n64.7\n76.6\n18.1\n57.5\n89.9\n92.4\n67.3\n40.7\n38.2\n96.5\n41.2\n62.3\n43.4\n35.1\n17.6\n53.5\n61.5\n44.8\n19.2\n57.0\n54.3\nViT-B/16\nMetaCLIP (400M)\n61.1\n70.8\n86.8\n90.1\n66.5\n70.8\n66.6\n74.1\n27.9\n55.9\n90.4\n93.8\n72.3\n47.8\n44.6\n97.2\n55.4\n68.8\n43.8\n33.4\n22.6\n52.9\n68.0\n49.5\n22.8\n54.8\n60.6\nMetaCLIP (1B,DataComp) 61.2\n70.7\n88.1\n91.3\n71.6\n64.5\n67.6\n81.2\n21.6\n62.3\n92.7\n93.2\n70.7\n55.6\n39.6\n97.7\n52.9\n66.3\n45.7\n36.0\n22.3\n50.1\n68.1\n49.2\n17.1\n56.4\n59.9\nViT-L/14\nMetaCLIP (400M)\n67.1\n76.2\n90.7\n95.5\n77.4\n75.9\n70.5\n84.7\n40.4\n62.0\n93.7\n94.4\n76.4\n61.7\n46.5\n99.3\n59.7\n71.9\n47.5\n29.9\n30.9\n70.1\n75.5\n57.1\n35.1\n56.6\n65.6\nMetaCLIP (1B,DataComp) 66.3\n76.7\n92.6\n95.6\n78.9\n72.1\n71.6\n87.1\n31.6\n67.5\n93.4\n95.3\n76.0\n65.1\n42.4\n99.1\n61.7\n69.8\n45.9\n36.8\n31.8\n51.0\n76.6\n57.5\n29.3\n57.1\n60.8\nTable 7: MetaCLIP curating our 400M data vs curating 1B data from DataComp-12.8B: The pool\nof DataComp leads to a quality drop with performance closer to our 400M set.\nDataComp Benchmark\nWe also evaluate MetaCLIP on the benchmark used by (Gadre et al.,\n2023) that contains 38 tasks including variants of ImageNet, retrieval, VTAB, etc. For simplicity,\nwe average the scores over each category.\n10\nPreprint\nAvg.\nIN\nIN Dist. Shift VTAB Avg. Retrieval\nViT-B/32\nCLIP (400M)\n51.5\n63.4\n48.2\n50.5\n48.0\nOpenCLIP (407M) 52.7\n62.9\n48.5\n53.0\n50.7\nMetaCLIP (400M) 53.5\n65.5\n50.4\n54.1\n50.6\nMetaCLIP (1B)\n54.2\n67.3\n51.9\n53.6\n51.1\nMetaCLIP (2.5B)\n55.4\n67.6\n52.3\n55.3\n52.6\nViT-B/16\nCLIP (400M)\n55.5\n68.3\n54.1\n54.4\n50.2\nOpenCLIP (407M) 56.1\n67.0\n52.6\n54.9\n53.9\nMetaCLIP (400M) 57.5\n70.8\n55.5\n56.7\n53.9\nMetaCLIP (1B)\n58.4\n72.4\n57.8\n56.3\n54.3\nMetaCLIP (2.5B)\n60.0\n72.1\n57.7\n59.0\n54.0\nViT-L/14\nCLIP (400M)\n61.4\n75.5\n61.6\n59.5\n53.6\nOpenCLIP (407M) 59.7\n72.7\n57.3\n58.6\n55.9\nMetaCLIP (400M) 62.2\n76.2\n61.3\n59.8\n57.3\nMetaCLIP (1B)\n65.0\n79.0\n64.5\n62.5\n58.3\nMetaCLIP (2.5B)\n65.6\n79.2\n64.6\n64.1\n60.1\nViT-H/14\nMetaCLIP (2.5B)\n66.5\n80.5\n66.1\n64.6\n60.4\nTable 8: Zero-shot classification and retrieval on tasks from (Gadre et al., 2023).\nNote that prompts and class names used by (Gadre et al., 2023) could be different from prompts and\nclassnames used by OpenAI CLIP.\nFrom Table 8, we can see that MetaCLIP outperforms CLIP and OpenCLIP across various model\nsizes. First, for the same data scale (400M), MetaCLIP outperforms OpenCLIP, which is better\nthan CLIP on this benchmark, by +1.4% for ViT-B/16 and +2.5% for ViT-L/14, when comparing\naverage accuracy across the 38 tasks. Second, for increasing our MetaCLIP data size to 1B we see a\nsignificant gain, especially for the larger model, from 62.2% to 65.0% average accuracy. Using our\nlarger dataset with 2.5B and more head entries leads to a further gain to 65.5%.\nA.2\nDETAILS ON EFFICIENT CURATION\nCuration in Data Pipeline.\nOur curation algorithm does not require access to images, making it\nsuitable for integration into a pipeline to reduce the scale of data points after parsing and before\nimage downloading. We designed the algorithm to be modular, allowing different parts to be placed\nat different stages in the pipeline, as shown in Figure 5.\nSpecifically, sub-string matching can be placed immediately after HTML parsing to reduce data\npoints for English-only pairs (e.g. by \u223c50%). Balancing can be applied earlier, before image down-\nloading, to further reduce data points by \u223c77%. This approach led to a total reduction of \u223c90%,\nallowing for curation of the entire CommonCrawl data without the need to store and transfer all data\npoints, which allows us to curate the whole CommonCrawl since 2013 with 300B+ scale URL-text\npairs without spending the storage/transfer on all \u223c10\u00d7 data points, where the rate of keeping a data\npoint with MetaCLIP curation is \u223c0.1 (0.5 \u00d7 0.23).\nCuration in Data Loading\nWe applied the balancing/sampling part of the algorithm to the data\nloader to adjust data distribution on-the-fly. Although data points for tail entries are always sampled\nin Algorithm 1, the diverse pairs from the head entries are sub-sampled from a larger pool while\nmaintaining a similar distribution as offline curation. This diversification of pairs that match the\nhead entries improved the performance, as shown in Table 6.\n11\nPreprint\nHTML \nParsing/LID \nURLs & Text \nDedup \nPart2: \nBalancing \nImage \nDownload \nNSFW \nImage Dedup\nPackaging\nPart1: \nSub-string \nMatching \n50% reduction\n77% reduction\nFigure 5: Case study: Curation implementation in our data pipeline.\nA.3\nHUMAN STUDY ON THE EFFECTS OF CURATION\nIn this section, we study the impact of MetaCLIP curation on data distribution by using human\nevaluation. We approach this exploration from three distinct angles: noise reduction, alignment of\nvisual content, and task-agnostic attributes. In the pursuit of comprehending the first two aspects,\nwe undertake a human study aimed at comprehending the data quality implications of implementing\nthe balancing technique (outlined in Part 2 of Algorithm 1). This evaluation encompasses three\ndimensions: image-only, text-only, and image-text alignment. We collect an evaluation set of 100\nrandom image-text pairs for balanced and unbalanced data, respectively, and ask annotators to score\non the image, text, and pair quality metrics, separately, on a scale of 1 to 5.\nAnnotation Guidelines.\nAnnotators follow guidelines to assess both images and texts, evaluating\ninformativeness (how well information is conveyed) and aesthetics. For images, aesthetics considers\nvisual elements like composition, color, lighting, balance, contrast, texture, and subject matter. For\ntexts, aesthetics gauges factors like delimiters, sentence structure, capitalization, prefixes/suffixes,\nrecognized words, generic words, and overall text quality. The alignment metric for image-text pairs\nmeasures the relevance between the two modalities, assessing how well the text describes the image\ncontent. Ratings are averaged across annotators for each dimension.\nWe show the study results in Table 9 and discuss the different criteria next.\nEvaluation Dimension\nRating for Balanced Data\nRating for Unbalanced Data\nP-value\nImage\n4.60, [4.50, 4.70]\n4.36, [4.23, 4.48]\n< 0.001\nText\n4.67, [4.56, 4.78]\n4.06, [3.82, 4.30]\n< 0.001\nAlignment\n4.41, [4.23, 4.59]\n3.72, [3.46, 3.99]\n< 0.001\nTable 9: Average human rating on the effect of balancing on data quality, with confidence intervals\nshown in parentheses. Higher rating is better. Balanced data is rated of higher quality.\nNoise Mitigation in Image and Text.\nAs shown in Table 9, significant quality improvement for\nall the three evaluation dimensions is observed after applying balancing. MetaCLIP curation has\nno specific hard filters such as removing shorter text, removing dates, etc. However, curation by\nsub-string matching and balancing has a different filtering effect. For example, a sub-string itself\ncan never curate a date-only text. Further, balancing allows signal and noise to co-exist when they\nare difficult to be separated by human designed filters. For example, if one entry such as \u201cimage\u201d or\n\u201cphoto\u201d is capped to t = 20k, it can only contribute 0.005% of 400M data.\nVisual Content Alignment.\nAlthough MetaCLIP curation does not directly involve images, it has\na positive effect on aligning visual content by controlling the quality and distribution of text. First,\nsub-string matching increases the chance of having (visual) entities mentioned in the text, thereby\nimproving the likelihood of finding corresponding visual content. Second, balancing favors long-\ntailed entries that could have more diverse visual content than a head entry (such as the text \u201c1\u201d). In\nTable 9, we observe significant improvement on pair quality from unbalanced data to balanced data.\n12\nPreprint\nHyperparameter\nOpenAI CLIP / MetaCLIP\nOpenCLIP (LAION 400M)\nDataComp\nActivation Function\nQuickGELU\nGELU\nGELU\nSeen Pairs\n12.8B(400M\u00d732 epochs)\n13B (407M\u00d732 epochs)\n12.8B\nBatch Size\n32768\n32768 (B/32), 33792 (B/16), 38400 (L/14) 90112 (L/14)\nLearning Rate\n5.0e-4(B/32,B/16), 4.0e-4(L/14)\n5.0e-4(B/32)\n1e-3(L/14)\nWarm-up\n2k\n2k (B/32)\n10k (L/14)\nTable 10: Hyperparameters of OpenAI CLIP vs OpenCLIP on LAION-400M and DataComp 1B.\nA.4\nTRAINING SETUP OF OPENAI CLIP VS OPENCLIP\nOur work strictly follows CLIP\u2019s setup for a controlled comparison focusing on data curation and\nquality. We notice differences in the training setup of OpenCLIP5 and list the difference (known to\nus). OpenCLIP varies the setup from CLIP (e.g., global batch size, learning schedule, etc.). Here we\nonly list the difference for LAION-400M, which is closer to the CLIP setup. We note that DataComp\ndiffers even more, e.g., by curating images close to ImageNet training data, a large batch size of 90k\nthat is almost 3\u00d7 larger than CLIP, and using the CLIP model to filter data.\nA.5\nBENCHMARK DEDUPLICATION\nOur pools are deduplicated from the benchmark/ImageNet data using a 64-bit PCA hash, derived\nfrom a similarity search model\u2019s feature embeddings with PCA reduction to 64 dimensions and sign\nquantization. DataComp-12.8B is already deduplicated.\nA.6\nNEGATIVE RESULTS LEARNED FROM ABLATING CLIP CURATION\nWe briefly describe a few ideas close to CLIP curation that did not look promising in our initial\nattempts and were abandoned:\n1. Self-curated Metadata. We initially attempted to build metadata directly from the text\nin raw image-text pairs (i.e., using terms appearing in text above a certain threshold of\ncounts). We rank entries by count and keep the top 500,000. Metadata built this way\nappeared worse. We notice that although the top frequent entries are similar to CLIP\u2019s\nmetadata, the long-tailed part is very different. For example, the minimal count to be in the\n500,000 budget needs at least 130 counts. In contrast, our metadata has 114K entries that\nhave no matches. This approach results in worse quality metadata including low-quality\nspelling/writing (instead of high-quality entries from WordNet or Wikipedia). Further, the\neffect of balancing saturates earlier for such data (in a larger t, verified by CLIP training)\nsince low-quality entries are also heavily in long-tail.\n2. Cased WordNet. We also notice many cased words are missing from metadata (e.g., Word-\nNet is in lowercase). After adding cased WordNet into metadata, we notice a performance\ndrop on ImageNet. The reason could be class names are more likely in lower case and\nupper case entry matching may reduce the written quality of texts.\n3. Stopwords/Useless Entries Removal We further study the effect of whether removing\nstopwords and useless words such as \u201cphoto\u201d and \u201cimage\u201d is beneficial. This led to al-\nmost no difference since balancing entries reduced the effects of useless entries (each entry\ncontributes to 0.0002% (1/500k) level of the total data points). To encourage a simplified\nsolution, we do not intend to add more artificial filters.\nA.7\nQUALITATIVE DATA EXAMPLES\nIn Table 11, we illustrate data before/after sub-string matching and balancing. We also highlight\nclass labels from ImageNet in the table. We mark a matched entry with a bigger font size indicating\nhigher probability of sampling that entry. Intuitively, sub-string matching removes low quality text\nand balancing favors longer text with long-tail entities to improve data quality. In Table 12, we show\nmore examples of matched text that include ImageNet tail entries.\n5https://github.com/mlfoundations/open_clip\n13\nPreprint\nText\nSubstr. Bal. IN Head IN Tail\ncontrol_14ct\n\u2717\n\u2717\n\u2717\n\u2717\ncudd2008\n\u2717\n\u2717\n\u2717\n\u2717\nproduct-img\n\u2717\n\u2717\n\u2717\n\u2717\nSkirmisher-Main-440x412\n\u2717\n\u2717\n\u2717\n\u2717\nA4omote\n\u2717\n\u2717\n\u2717\n\u2717\nHow-to-Find-the-Finest-Electric-Car-Companies\n\u2717\n\u2717\n\u2717\n\u2717\nhanukkah-party-facebook-event-cover-template\n\u2717\n\u2717\n\u2717\n\u2717\njohnny_cash_chili_dog (2)\n\u2717\n\u2717\n\u2717\n\u2717\n8533 GOLDEN RIDGE COURT\n\u2717\n\u2717\n\u2717\n\u2717\nHow to build a stone patio on your own\n\u2713\n\u2717\n\u2713\n\u2717\nbattery plate\n\u2713\n\u2717\n\u2713\n\u2717\nbarn wedding\n\u2713\n\u2717\n\u2713\n\u2717\nPicture sand , machine , Concept , jeep , the concept , the\nfront , Slim , Wrangler , Jeep\n\u2713\n\u2717\n\u2713\n\u2717\ndesk\n\u2713\n\u2717\n\u2713\n\u2717\nAdult T-shirt\n\u2713\n\u2717\n\u2713\n\u2717\nImix m10 stage moniter\n\u2713\n\u2717\n\u2713\n\u2717\ngoogle wallet md3\n\u2713\n\u2717\n\u2713\n\u2717\nDistressed beach bar sign - Pearly\u2019s Oyster Bar\n\u2713\n\u2717\n\u2713\n\u2717\nWhy the Kilimanjaro Trek should be top of your bucket list\n\u2713\n\u2717\n\u2713\n\u2717\nJ70 desk model\n\u2713\n\u2717\n\u2713\n\u2717\nWhitby castle\n\u2713\n\u2717\n\u2713\n\u2717\nInside the castle\n\u2713\n\u2717\n\u2713\n\u2717\nHama Hama Oyster Saloon | restaurant | 35846 US-101 , Lilliwaup , WA\n98555 , USA | 3608775811 OR +1 360-877-5811\n\u2713\n\u2717\n\u2713\n\u2717\nbeach\n\u2713\n\u2717\n\u2713\n\u2717\nCaramelized onions , sauteed red bell peppers and\nzucchini combined create a winning egg frit-\ntata breakfast dish .\n\u2713\n\u2713\n\u2713\n\u2717\nVector layered paper cut craft style music composition\nofsaxophone guitartrumpetviolin music in-\nstruments , notes on abstract color background . Jazz\nconcert festival party poster banner card template\n\u2713\n\u2713\n\u2713\n\u2717\nNautilus hot tub\n\u2713\n\u2713\n\u2713\n\u2717\nnightbinoculars for hunting\n\u2713\n\u2713\n\u2713\n\u2717\n2017 cat eyes women\u2019s sunglasses for women vintage\nsun glasses round women sun glasses oculos oculos desol\nfeminino\n\u2713\n\u2713\n\u2713\n\u2717\nTable 11: Examples categorized by whether passing sub-string matching and balancing: Words in\nviolet color are in metadata and their font size indicates probability of being sampled, ranging from\n13pt that has probability close to 0, to22pt that has probability 1. ImageNet labels in head\nentries are cyan.\n14\nPreprint\nText\nSubstr. Bal. IN Head IN Tail\nAntique German sterling silver 800 cupjulepgoblet ,\n1\n\u2713\n\u2713\n\u2717\n\u2713\nA journey to the East Sea byhigh-speed train\n: New KTX line makes Gangneung\u2019swonders all the more\naccessible\n\u2713\n\u2713\n\u2717\n\u2713\nlittle arrow design co watercolor rainbow blush shower\ncurtain and mat\n\u2713\n\u2713\n\u2717\n\u2713\nphoto of antique silver top breakrevolver\n\u2713\n\u2713\n\u2717\n\u2713\ntrombone model\n\u2713\n\u2713\n\u2717\n\u2713\nStaffordshire Bull Terrier\n\u2713\n\u2713\n\u2717\n\u2713\nKI Plantation Timbers fire truck on the night of Jan-\nuary 3 , 2020 . Photography : Tim Wilson\n\u2713\n\u2713\n\u2717\n\u2713\nwater buffalo bath\n\u2713\n\u2713\n\u2717\n\u2713\nBasset Houndpup with Lionhead x Lop rabbit\n\u2713\n\u2713\n\u2717\n\u2713\nSingle serving of peachtrifle\n\u2713\n\u2713\n\u2717\n\u2713\nA tarantula (hairy arachnid belonging\nto the Theraphosidae family of spiders) in a box , standing still\n. Close-up shot .\n\u2713\n\u2713\n\u2717\n\u2713\nModernStaffordshire Bull TerrierLED\nNight Light Animal Pet Dog Puppy 3D Optical illusion\nLamp Home Decor Table Lamp Desk Light\n\u2713\n\u2713\n\u2717\n\u2713\nAmazon ,rocking chair , CANVA , camping chairs\n\u2713\n\u2713\n\u2717\n\u2713\ndispalyingkukrimachete fitted insidescab-\nbard\n\u2713\n\u2713\n\u2717\n\u2713\njacksonschameleon\n\u2713\n\u2713\n\u2717\n\u2713\nWall Mural - Insects pollinating blooming\nrapeseedcrops in field\n\u2713\n\u2713\n\u2717\n\u2713\nScottish Terrier Phone Pocket\n\u2713\n\u2713\n\u2717\n\u2713\nTable 12: Examples passing both sub-string matching and balancing wit ImageNet tail classes:\nWords in violet color are in metadata and their font size indicates probability of being sampled,\nranging from 13pt that has probability close to 0, to22pt that has probability 1. ImageNet\nlabels in tail entries are in cyan.\n15\nPreprint\nREFERENCES\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-\nefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,\n2023.\nOlivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation-the case\nof dp-means. In International Conference on Machine Learning, pp. 209\u2013217. PMLR, 2015.\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gor-\ndon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for\ncontrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-\nage is worth 16x16 words: Transformers for image recognition at scale. In International Confer-\nence on Learning Representations, 2020.\nDan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via\ncoresets. Advances in neural information processing systems, 24, 2011.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim En-\ntezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexan-\nder Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,\nAlex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In\nsearch of the next generation of multimodal datasets, 2023.\nSariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In\nProceedings of the thirty-sixth annual ACM symposium on Theory of computing, pp. 291\u2013300,\n2004.\nMon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for out-\nliers detection. Pattern recognition letters, 22(6-7):691\u2013700, 2001.\nBaharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of\nmachine learning models. In International Conference on Machine Learning, pp. 6950\u20136960.\nPMLR, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nChristoph Schuhmann, Romain Beaumont, Cade W Gordon, Ross Wightman, Theo Coombes,\nAarush Katta, Clayton Mullis, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson,\net al. Laion-5b: An open large-scale dataset for training next generation image-text models. 2022.\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neu-\nral scaling laws: beating power law scaling via data pruning. Advances in Neural Information\nProcessing Systems, 35:19523\u201319536, 2022.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio,\nand Geoffrey J Gordon. An empirical study of example forgetting during deep neural network\nlearning. arXiv preprint arXiv:1812.05159, 2018.\nHu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell Howes, Gargi Ghosh, Luke Zettlemoyer,\nand Christoph Feichtenhofer. Cit: Curation in training for effective vision-language data. arXiv\npreprint arXiv:2301.02241, 2023.\n16\nPreprint\nDantong Yu, Gholamhosein Sheikholeslami, and Aidong Zhang. Findout: Finding outliers in very\nlarge datasets. Knowledge and information Systems, 4:387\u2013412, 2002.\n17\n"
  },
  {
    "title": "MotionLM: Multi-Agent Motion Forecasting as Language Modeling",
    "link": "https://arxiv.org/pdf/2309.16534.pdf",
    "upvote": "15",
    "text": "MotionLM: Multi-Agent Motion Forecasting as Language Modeling\nAri Seff\nBrian Cera\nDian Chen*\nMason Ng\nAurick Zhou\nNigamaa Nayakanti\nKhaled S. Refaat\nRami Al-Rfou\nBenjamin Sapp\nWaymo\nAbstract\nReliable forecasting of the future behavior of road agents\nis a critical component to safe planning in autonomous ve-\nhicles. Here, we represent continuous trajectories as se-\nquences of discrete motion tokens and cast multi-agent mo-\ntion prediction as a language modeling task over this do-\nmain. Our model, MotionLM, provides several advantages:\nFirst, it does not require anchors or explicit latent variable\noptimization to learn multimodal distributions. Instead, we\nleverage a single standard language modeling objective,\nmaximizing the average log probability over sequence to-\nkens. Second, our approach bypasses post-hoc interaction\nheuristics where individual agent trajectory generation is\nconducted prior to interactive scoring. Instead, MotionLM\nproduces joint distributions over interactive agent futures in\na single autoregressive decoding process. In addition, the\nmodel\u2019s sequential factorization enables temporally causal\nconditional rollouts.\nThe proposed approach establishes\nnew state-of-the-art performance for multi-agent motion\nprediction on the Waymo Open Motion Dataset, ranking 1st\non the interactive challenge leaderboard.\n1. Introduction\nModern sequence models often employ a next-token pre-\ndiction objective that incorporates minimal domain-specific\nassumptions. For example, autoregressive language mod-\nels [3, 10] are pre-trained to maximize the probability of\nthe next observed subword conditioned on the previous text;\nthere is no predefined notion of parsing or syntax built in.\nThis approach has found success in continuous domains as\nwell, such as audio [2] and image generation [49]. Leverag-\ning the flexibility of arbitrary categorical distributions, the\nabove works represent continuous data with a set of discrete\n*Work done during an internship at Waymo.\nContact: {aseff, bensapp}@waymo.com\nVocabulary\n\u2026\n\u2026\nMotion token sequence:\nt=T\nt=1\nt=2\n...\nFigure 1. Our model autoregressively generates sequences of dis-\ncrete motion tokens for a set of agents to produce consistent inter-\nactive trajectory forecasts.\ntokens, reminiscent of language model vocabularies.\nIn driving scenarios, road users may be likened to par-\nticipants in a constant dialogue, continuously exchanging a\ndynamic series of actions and reactions mirroring the fluid-\nity of communication. Navigating this rich web of interac-\ntions requires the ability to anticipate the likely maneuvers\nand responses of the involved actors. Just as today\u2019s lan-\nguage models can capture sophisticated distributions over\nconversations, can we leverage similar sequence models to\nforecast the behavior of road agents?\nA common simplification to modeling the full future\nworld state has been to decompose the joint distribution of\nagent behavior into independent per-agent marginal distri-\nbutions. Although there has been much progress on this\ntask [8, 47, 12, 25, 31, 5, 6, 21], marginal predictions are\ninsufficient as inputs to a planning system; they do not rep-\nresent the future dependencies between the actions of differ-\nent agents, leading to inconsistent scene-level forecasting.\nOf the existing joint prediction approaches, some apply\narXiv:2309.16534v1  [cs.CV]  28 Sep 2023\na separation between marginal trajectory generation and in-\nteractive scoring [40, 42, 29]. For example, Luo et al. [29]\ninitially produce a small set of marginal trajectories for each\nagent independently, before assigning a learned potential to\neach inter-agent trajectory pair through a belief propaga-\ntion algorithm. Sun et al. [42] use a manual heuristic to\ntag agents as either influencers or reactors, and then pairs\nmarginal and conditional predictions to form joint predic-\ntions.\nWe also note that because these approaches do not ex-\nplicitly model temporal dependencies within trajectories,\ntheir conditional forecasts may be more susceptible to spu-\nrious correlations, leading to less realistic reaction predic-\ntions.\nFor example, these models can capture the cor-\nrelation between a lead agent decelerating and a trail-\ning agent decelerating, but may fail to infer which one is\nlikely causing the other to slow down. In contrast, previ-\nous joint models employing an autoregressive factorization,\ne.g., [36, 43, 39], do respect future temporal dependencies.\nThese models have generally relied on explicit latent vari-\nables for diversity, optimized via either an evidence lower\nbound or normalizing flow.\nIn this work, we combine trajectory generation and in-\nteraction modeling in a single, temporally causal, decod-\ning process over discrete motion tokens (Fig. 1), leverag-\ning a simple training objective inspired by autoregressive\nlanguage models.\nOur model, MotionLM, is trained to\ndirectly maximize the log probability of these token se-\nquences among interacting agents. At inference time, joint\ntrajectories are produced step-by-step, where interacting\nagents sample tokens simultaneously, attend to one another,\nand repeat. In contrast to previous approaches which man-\nually enforce trajectory multimodality during training, our\nmodel is entirely latent variable and anchor-free, with mul-\ntimodality emerging solely as a characteristic of sampling.\nMotionLM may be applied to several downstream behavior\nprediction tasks, including marginal, joint, and conditional\npredictions.\nThis work makes the following contributions:\n1. We cast multi-agent motion forecasting as a language\nmodeling task, introducing a temporally causal de-\ncoder over discrete motion tokens trained with a causal\nlanguage modeling loss.\n2. We pair sampling from our model with a simple roll-\nout aggregation scheme that facilitates weighted mode\nidentification for joint trajectories, establishing new\nstate-of-the-art performance on the Waymo Open Mo-\ntion Dataset interaction prediction challenge (6% im-\nprovement in the ranking joint mAP metric).\n3. We perform extensive ablations of our approach as\nwell as analysis of its temporally causal conditional\npredictions, which are largely unsupported by current\njoint forecasting models.\n2. Related work\nMarginal trajectory prediction.\nBehavior predictors are\noften evaluated on their predictions for individual agents,\ne.g., in recent motion forecasting benchmarks [14, 9, 4,\n51, 37].\nPrevious methods process the rasterized scene\nwith CNNs [8, 5, 12, 17]; the more recent works repre-\nsent scenes with points and polygraphs and process them\nwith GNNs [6, 25, 47, 22] or transformers [31, 40, 20]. To\nhandle the multimodality of future trajectories, some mod-\nels manually enforce diversity via predefined anchors [8, 5]\nor intention points [40, 52, 28]. Other works learn diverse\nmodes with latent variable modeling, e.g., [24].\nWhile these works produce multimodal future trajecto-\nries of individual agents, they only capture the marginal dis-\ntributions of the possible agent futures and do not model the\ninteractions among agents.\nInteractive trajectory prediction.\nInteractive behavior\npredictors model the joint distribution of agents\u2019 futures.\nThis task has been far less studied than marginal motion\nprediction. For example, the Waymo Open Motion Dataset\n(WOMD) [14] challenge leaderboard currently has 71 pub-\nlished entries for marginal prediction compared to only 14\nfor interaction prediction.\nNgiam et al. [32] models the distribution of future trajec-\ntories with a transformer-based mixture model outputting\njoint modes. To avoid the exponential blow-up from a full\njoint model, Luo et al. [29] models pairwise joint distri-\nbutions. Tolstaya et al. [44], Song et al. [41], Sun et al.\n[42] consider conditional predictions by exposing the future\ntrajectory of one agent when predicting for another agent.\nShi et al. [40] derives joint probabilities by simply mul-\ntiplying marginal trajectory probabilities, essentially treat-\ning agents as independent, which may limit accuracy. Cui\net al. [11], Casas et al. [7], Girgis et al. [15] reduce the full-\nfledged joint distribution using global latent variables. Un-\nlike our autoregressive factorization, the above models typi-\ncally follow \u201cone-shot\u201d (parallel across time) factorizations\nand do not explicitly model temporally causal interactions.\nAutoregressive trajectory prediction.\nAutoregressive\nbehavior predictors generate trajectories at intervals to pro-\nduce scene-consistent multi-agent trajectories. Rhinehart\net al. [36], Tang and Salakhutdinov [43], Amirloo et al.\n[1], Salzmann et al. [39], Yuan et al. [50] predict multi-\nagent future trajectories using latent variable models. Lu\net al. [27] explores autoregressively outputting keyframes\nvia mixtures of Gaussians prior to filling in the remaining\nstates. In [18], an adversarial objective is combined with\nScene\nEncoder\nProjection\nMultimodal\nScene Inputs\nScene \nembeddings\n\u2026\n\u2026\n\u2026\n\u2026\nAgent 1\nAgent 2\nTime\nStart\ntoken \nt=0\nt=1\nCross Attention\nAutoregressive Transformer\nDecoder\nSample\n[R,N,\u1427,H]\nSample\nA1 A1\n\u2026\nt=T-1\n\u2026\nSample\nEnsemble & Rollout aggregation\nSelf Attention\n[R,N,T,2]\nxE\n[K,N,T,2]\nNMS\nK-means\nDecoded motion tokens\nEmbed\nEmbed\nAT\nAT\nAT-1\nAT-1\nt=T\nFigure 2. MotionLM architecture. We first encode heterogeneous scene features relative to each modeled agent (left) as scene embeddings\nof shape R, N, \u00b7, H. Here, R refers to the number of rollouts, N refers to the number of (jointly modeled) agents, and H is the dimen-\nsionality of each embedding. We repeat the embeddings R times in the batch dimension for parallel sampling during inference. Next, a\ntrajectory decoder autoregressively rolls out T discrete motion tokens for multiple agents in a temporally causal manner (center). Finally,\nrepresentative modes of the rollouts may be recovered via a simple aggregation utilizing k-means clustering initialized with non-maximum\nsuppression (right).\nparallel beam search to learn multi-agent rollouts. Unlike\nmost autoregressive trajectory predictors, our method does\nnot rely on latent variables or beam search and generates\nmultimodal joint trajectories by directly sampling from a\nlearned distribution of discrete motion token sequences.\nDiscrete sequence modeling in continuous domains.\nWhen generating sequences in continuous domains, one ef-\nfective approach is to discretize the output space and predict\ncategorical distributions at each step.\nFor example, in image generation, van den Oord et al.\n[45] sequentially predict the uniformly discretized pixel val-\nues for each channel and found this to perform better than\noutputting continuous values directly. Multiple works on\ngenerating images from text such as [35] and [49] use a\ntwo-stage process with a learned tokenizer to map images\nto discrete tokens and an autoregressive model to predict\nthe discrete tokens given the text prompt. For audio gen-\neration, WaveNet [46] applies a \u00b5-law transformation be-\nfore discretizing. Borsos et al. [2] learn a hierarchical to-\nkenizer/detokenizer, with the main transformer sequence\nmodel operating on the intermediate discrete tokens. When\ngenerating polygonal meshes, Nash et al. [30] uniformly\nquantize the coordinates of each vertex. In MotionLM, we\nemploy a simple uniform quantization of axis-aligned deltas\nbetween consecutive waypoints of agent trajectories.\n3. MotionLM\nWe aim to model a distribution over multi-agent inter-\nactions in a general manner that can be applied to distinct\ndownstream tasks, including marginal, joint, and condi-\ntional forecasting. This requires an expressive generative\nframework capable of capturing the substantial multimodal-\nity in driving scenarios. In addition, we take consideration\nhere to preserve temporal dependencies; i.e., inference in\nour model follows a directed acyclic graph with the parents\nof every node residing earlier in time and children residing\nlater (Section 3.3, Fig. 4). This enables conditional fore-\ncasts that more closely resemble causal interventions [34]\nby eliminating certain spurious correlations that can other-\nwise result from disobeying temporal causality2. We ob-\nserve that joint models that do not preserve temporal de-\npendencies may have a limited ability to predict realistic\nagent reactions \u2013 a key use in planning (Section 4.6). To this\nend, we leverage an autoregressive factorization of our fu-\nture decoder, where agents\u2019 motion tokens are conditionally\ndependent on all previously sampled tokens and trajectories\nare rolled out sequentially (Fig. 2).\nLet S represent the input data for a given scenario. This\nmay include context such as roadgraph elements, traffic\nlight states, as well as features describing road agents (e.g.,\nvehicles, cyclists, and pedestrians) and their recent histo-\nries, all provided at the current timestep t = 0.\nOur\ntask is to generate predictions for joint agent states Yt .=\n{y1\nt , y2\nt , ..., yN\nt } for N agents of interest at future timesteps\nt = 1, ..., T. Rather than complete states, these future state\ntargets are typically two-dimensional waypoints (i.e., (x, y)\ncoordinates), with T waypoints forming the full ground\ntruth trajectory for an individual agent.\n2We make no claims that our model is capable of directly modeling\ncausal relationships (due to the theoretical limits of purely observational\ndata and unobserved confounders). Here, we solely take care to avoid\nbreaking temporal causality.\n3.1. Joint probabilistic rollouts\nIn our modeling framework, we sample a predicted ac-\ntion for each target agent at each future timestep. These\nactions are formulated as discrete motion tokens from a fi-\nnite vocabulary, as described later in Section 3.2.2.\nLet\nan\nt represent the target action (derived from the ground\ntruth waypoints) for the nth agent at time t, with At\n.=\n{a1\nt, a2\nt, ..., aN\nt } representing the set of target actions for all\nagents at time t.\nFactorization.\nWe factorize the distribution over joint fu-\nture action sequences as a product of conditionals:\np\u03b8(A1, A2, ...AT | S) =\nT\nY\nt=1\np\u03b8(At | A<t, S),\n(1)\np\u03b8(At | A<t, S) =\nN\nY\nn=1\np\u03b8(an\nt | A<t, S).\n(2)\nSimilar to [36, 43], Eq. (2) represents the fact that we treat\nagent actions as conditionally independent at time t, given\nthe previous actions and scene context. This aligns empiri-\ncally with real-world driving over short time intervals; e.g.,\nnon-impaired human drivers generally require at least 500\nms to release the accelerator in response to a vehicle braking\nahead ([13]). In our experiments, we find 2 Hz reactions to\nbe sufficient to surpass state-of-the-art joint prediction mod-\nels.\nWe note that our model\u2019s factorization is entirely latent\nvariable free; multimodal predictions stem purely from cat-\negorical token sampling at each rollout timestep.\nTraining objective.\nMotionLM is formulated as a gener-\native model trained to match the joint distribution of ob-\nserved agent behavior. Specifically, we follow a maximum\nlikelihoood objective over multi-agent action sequences:\narg max\n\u03b8\nT\nY\nt=1\np\u03b8(At | A<t, S)\n(3)\nSimilar to the typical training setup of modern language\nmodels, we utilize \u201cteacher-forcing\u201d where previous ground\ntruth (not predicted) tokens are provided at each timestep,\nwhich tends to increase stability and avoids sampling dur-\ning training. We note that this applies to all target agents; in\ntraining, each target agent is exposed to ground truth action\nsequence prefixes for all target agents prior to the current\ntimestep. This naturally allows for temporal parallelization\nwhen using modern attention-based architectures such as\ntransformers [48].\nOur model is subject to the same theoretical limitations\nas general imitation learning frameworks (e.g., compound-\ning error [38] and self-delusions due to unobserved con-\nfounders [33]). However, we find that, in practice, these\ndo not prevent strong performance on forecasting tasks.\n3.2. Model implementation\nOur model consists of two main networks, an encoder\nwhich processes initial scene elements followed by a tra-\njectory decoder which performs both cross-attention to the\nscene encodings and self-attention along agent motion to-\nkens, following a transformer architecture [48].\n3.2.1\nScene encoder\nThe scene encoder (Fig. 2, left) is tasked with processing in-\nformation from several input modalities, including the road-\ngraph, traffic light states, and history of surrounding agents\u2019\ntrajectories. Here, we follow the design of the early fusion\nnetwork proposed by [31] as the scene encoding backbone\nof our model. Early fusion is particularly chosen because of\nits flexibility to process all modalities together with minimal\ninductive bias.\nThe features above are extracted with respect to each\nmodeled agent\u2019s frame of reference. Input tensors are then\nfed to a stack of self-attention layers that exchange informa-\ntion across all past timesteps and agents. In the first layer,\nlatent queries cross-attend to the original inputs in order to\nreduce the set of vectors being processed to a manageable\nnumber, similar to [23, 19]. For additional details, see [31].\n3.2.2\nJoint trajectory decoder\nOur trajectory decoder (Fig. 2, center) is tasked with gener-\nating sequences of motion tokens for multiple agents.\nDiscrete motion tokens.\nWe elect to transform trajecto-\nries comprised of continuous waypoints into sequences of\ndiscrete tokens. This enables treating sampling purely as a\nclassification task at each timestep, implemented via a stan-\ndard softmax layer. Discretizing continuous targets in this\nmanner has proven effective in other inherently continuous\ndomains, e.g., in audio generation [46] and mesh genera-\ntion [30]. We suspect that discrete motion tokens also natu-\nrally hide some precision from the model, possibly mitigat-\ning compounding error effects that could arise from imper-\nfect continuous value prediction. Likewise, we did not find\nit necessary to manually add any noise to the ground truth\nteacher-forced trajectories (e.g., as is done in [26]).\nQuantization.\nTo extract target discrete tokens, we be-\ngin by normalizing each agent\u2019s ground truth trajectory\nwith respect to the position and heading of the agent at\nFigure 3. Displayed are the top two predicted joint rollout modes for three WOMD scenes. Color gradients indicate time progression from\nt = 0s to t = 8s, with the greatest probability joint mode transitioning from green to blue and the secondary joint mode transitioning from\norange to purple. Three types of interactions are observed: an agent in the adjacent lane yields to the lane-changing agent according to the\ntiming of the lane change (left), a pedestrian walks behind the passing vehicle according to the progress of the vehicle (center), the turning\nvehicle either yields to the crossing cyclist (most probable mode) or turns before the cyclist approaches (secondary mode) (right).\ntime t = 0 of the scenario. We then parameterize a uni-\nformly quantized (\u2206x, \u2206y) vocabulary according to a to-\ntal number of per-coordinate bins as well as maximum and\nminimum delta values.\nA continuous, single-coordinate\ndelta action can then be mapped to a corresponding index\n\u2208 [0, num bins \u2212 1], resulting in two indices for a com-\nplete (\u2206x, \u2206y) action per step. In order to extract actions\nthat accurately reconstruct an entire trajectory, we employ a\ngreedy search, sequentially selecting the quantized actions\nthat reconstruct the next waypoint coordinates with mini-\nmum error.\nWe wrap the delta actions with a \u201cVerlet\u201d step where\na zero action indicates that the same delta index should\nbe used as the previous step (as [36] does for continuous\nstates). As agent velocities tend to change smoothly be-\ntween consecutive timesteps, this helps reduce the total vo-\ncabulary size, simplifying the dynamics of training. Finally,\nto maintain only T sequential predictions, we collapse the\nper-coordinate actions to a single integer indexing into their\nCartesian product. In practice, for the models presented\nhere, we use 13 tokens per coordinate with 132 = 169 total\ndiscrete tokens available in the vocabulary (see Appendix A\nfor further details).\nWe compute a learned value embedding and two learned\npositional embeddings (representing the timestep and agent\nidentity) for each discrete motion token, which are com-\nbined via an element-wise sum prior to being input to the\ntransformer decoder.\nFlattened agent-time self-attention.\nWe elect to include\na single self-attention mechanism in the decoder that oper-\nates along flattened sequences of all modeled agents\u2019 mo-\ntion tokens over time. So, given a target sequence of length\nT for each of N agents, we perform self-attention over NT\nelements.\nWhile this does mean that these self-attended\nsequences grow linearly in the number of jointly modeled\nagents, we note that the absolute sequence length here is\nstill quite small (length 32 for the WOMD interactive split\n\u2013 8 sec. prediction at 2 Hz for 2 agents). Separate passes\nof factorized agent and time attention are also possible [32],\nbut we use a single pass here for simplicity.\nEgo agent reference frames.\nTo facilitate cross-attention\nto the agent-centric feature encodings (Section 3.2.1), we\nrepresent the flattened token sequence once for each mod-\neled agent. Each modeled agent is treated as the \u201cego\u201d agent\nonce, and cross-attention is performed on that agent\u2019s scene\nfeatures. Collapsing the ego agents into the batch dimension\nallows parallelization during training and inference.\n3.3. Enforcing temporal causality\nOur autoregressive factorization naturally respects tem-\nporal dependencies during joint rollouts; motion token sam-\npling for any particular agent is affected only by past to-\nkens (from any agent) and unaffected by future ones. When\ntraining, we require a mask to ensure that the self-attention\noperation only updates representations at each step accord-\ningly. As shown in Fig. 8 (appendix), this attention mask\nexhibits a blocked, staircase pattern, exposing all agents to\neach other\u2019s histories only up to the preceding step.\nTemporally causal conditioning.\nAs described earlier, a\nparticular benefit of this factorization is the ability to query\nfor temporally causal conditional rollouts (Fig. 4). In this\nsetting, we fix a query agent to take some sequence of ac-\ntions and only roll out the other agents.\nS\nx=T\ny=T\nx=1\nx=2\ny=2\n\u2026\n\u2026\ny=1\n(a) Causal Bayesian Network for joint rollouts.\nS\ny=1\ny=2\nx=T\n\u2026\nx=1\nx=2\n\u2026\n\u2026\ny=T\n(b) Post-Intervention Causal Bayesian Network.\nS\n\u2026\n\u2026\nx=2\nx=T\nx=1\ny=T\ny=2\ny=1\nCausal conditioning\nAcausal conditioning\nConditioned Agent\nSampling Agent\n(c) Acausal conditioning.\nFigure 4. A Causal Bayesian network representation for joint rollouts (left), post-intervention Causal Bayesian network (center), and\nacausal conditioning (right). Solid lines indicate temporally causal dependencies while dashed lines indicate acausal information flow.\nModels without temporal dependency constraints will support acausal conditioning but not temporally causal conditioning, which can be\nproblematic when attempting to predict agent reactions.\nWe may view this as an approximation of computing\ncausal interventions [34] in the absence of confounders; in-\nterventions cannot be learned purely through observational\ndata in general (due to the possible presence of unobserved\nconfounders), but our model\u2019s factorization at least elim-\ninates certain spurious correlations arising from breaking\ntemporal causality.\nIn Fig. 4 (a), we show an example of a Causal Bayesian\nnetwork governing joint rollouts. Applying an intervention\nto nodes x = 1, ...T, by deleting their incoming edges,\nresults in a post-intervention Bayesian network depicted\nin Fig. 4 (b), which obeys temporal causality. On the other\nhand, acausal conditioning (Fig. 4 (c)) results in non-causal\ninformation flow, where node x = i affects our belief about\nnode y = j for i \u2265 j.\n3.4. Rollout aggregation\nJoint\nmotion\nforecasting\nbenchmark\ntasks\nlike\nWOMD [14] require a compact representation of the\njoint future distribution in the form of a small number of\njoint \u201cmodes\u201d. Each mode is assigned a probability and\nmight correspond to a specific homotopic outcome (e.g.,\npass/yield) or more subtle differences in speed/geometry.\nHere, we aggregate rollouts to achieve two primary goals:\n1) uncover the underlying modes of the distribution and\n2) estimate the probability of each mode. Specifically, we\nfollow the non-maximum suppression (NMS) aggregation\nscheme described in [47], but extend it to the joint setting\nby ensuring that all agent predictions reside within a\ngiven distance threshold to the corresponding cluster. In\naddition, we leverage model ensembling to account for\nepistemic uncertainty and further improve the quality of the\npredictions, combining rollouts from independently trained\nreplicas prior to the aggregation step.\n4. Experiments\nWe evaluate MotionLM on marginal and joint motion\nforecasting benchmarks, examine its conditional predic-\ntions and conduct ablations of our modeling choices.\n4.1. Datasets\nWaymo Open Motion Dataset (WOMD).\nWOMD [14]\nis a collection of 103k 20-second scenarios collected from\nreal-world driving in urban and suburban environments.\nSegments are divided into 1.1M examples consisting of 9-\nsecond windows of interest, where the first second serves\nas input context and the remaining 8-seconds are the pre-\ndiction target. Map features such as traffic signal states and\nlane features are provided along with agent states such as\nposition, velocity, acceleration, and bounding boxes.\nMarginal and interactive prediction challenges.\nFor the\nmarginal prediction challenge, six trajectories must be out-\nput by the model for each target agent, along with likeli-\nhoods of each mode. For the interactive challenge, two in-\nteracting agents are labeled in each test example. In this\ncase, the model must output six weighted joint trajectories.\n4.2. Metrics\nThe primary evaluation metrics for the marginal and in-\nteractive prediction challenges are soft mAP and mAP, re-\nspectively, with miss rate as the secondary metric. Distance\nmetrics minADE and minFDE provide additional signal on\nprediction quality. For the interactive prediction challenge,\nthese metrics refer to scene-level joint calculations. We also\nuse a custom prediction overlap metric (similar to [29]) to\nassess scene-level consistency for joint models. See Ap-\npendix C for details on these metrics.\n4.3. Model configuration\nWe experiment with up to 8 model replicas and 512 roll-\nouts per replica, assessing performance at various configu-\nrations. For complete action space and model hyperparam-\neter details, see Appendices A and B.\nModel\nminADE (\u2193)\nminFDE (\u2193)\nMiss Rate (\u2193)\nSoft mAP (\u2191)\nHDGT [20]\n0.7676\n1.1077\n0.1325\n0.3709\nMPA [22]\n0.5913\n1.2507\n0.1603\n0.3930\nMTR [40]\n0.6050\n1.2207\n0.1351\n0.4216\nWayformer factorized [31]\n0.5447\n1.1255\n0.1229\n0.4260\nWayformer multi-axis [31]\n0.5454\n1.1280\n0.1228\n0.4335\nMTR-A [40]\n0.5640\n1.1344\n0.1160\n0.4594\nMotionLM (Ours)\n0.5509\n1.1199\n0.1058\n0.4507\nTable 1. Marginal prediction performance on WOMD test set. We display metrics averaged over time steps (3, 5, and 8 seconds) and agent\ntypes (vehicles, pedestrians, and cyclists). Greyed columns indicate the official ranking metrics for the marginal prediction challenge.\nModel\nminADE (\u2193)\nminFDE (\u2193)\nMiss Rate (\u2193)\nmAP (\u2191)\nSceneTransformer (J) [32]\n0.9774\n2.1892\n0.4942\n0.1192\nM2I [42]\n1.3506\n2.8325\n0.5538\n0.1239\nDenseTNT [28]\n1.1417\n2.4904\n0.5350\n0.1647\nMTR [40]\n0.9181\n2.0633\n0.4411\n0.2037\nJFP [29]\n0.8817\n1.9905\n0.4233\n0.2050\nMotionLM (Ours)\n0.8911\n2.0067\n0.4115\n0.2178\nTable 2. Joint prediction performance on WOMD interactive test set. We display scene-level joint metrics averaged over time steps (3, 5,\nand 8 seconds) and agent types (vehicles, pedestrians, and cyclists). Greyed columns indicate the official ranking metrics for the challenge.\n4.4. Quantitative results\nMarginal motion prediction.\nAs shown in Table 1, our\nmodel is competitive with the state-of-the-art on WOMD\nmarginal motion prediction (independent per agent). For\nthe main ranking metric of soft mAP, our model ranks sec-\nond, less than 2% behind the score achieved by MTRA [40].\nIn addition, our model attains a substantially improved miss\nrate over all prior works, with a relative 9% reduction com-\npared to the previous state-of-the-art. The autoregressive\nrollouts are able to adequately capture the diversity of mul-\ntimodal future behavior without reliance on trajectory an-\nchors [8] or static intention points [40].\nInteractive motion prediction.\nOur model achieves\nstate-of-the-art results for the interactive prediction chal-\nlenge on WOMD, attaining a 6% relative improvement in\nmAP and 3% relative improvement in miss rate (the two of-\nficial ranking metrics) over the previous top scoring entry,\nJFP [29] (see Table 2). In contrast to JFP, our approach does\nnot score pairs of previously constructed marginal trajecto-\nries. but generates joint rollouts directly. Fig. 3 displays\nexample interactions predicted by our model.\nTable 3 displays prediction overlap rates for various\nmodels on the WOMD interactive test and validation sets\n(see metric details in Appendix C.2). We obtain test set\npredictions from the authors of [14, 32, 29]. MotionLM\nobtains the lowest prediction overlap rate, an indication of\nscene-consistent predictions.\nIn addition, on the valida-\ntion set we evaluate two versions of our model: marginal\nand joint. The marginal version does not perform attention\nModel\nPrediction Overlap (\u2193)\nTest\nLSTM Baseline [14]\n0.07462\nScene Transformer [32]\n0.04336\nJFP [29]\n0.02671\nMotionLM (joint)\n0.02607\nVal\nMotionLM (marginal)\n0.0404\nMotionLM (joint)\n0.0292\nTable 3. Prediction overlap rates. Displayed is the custom pre-\ndiction overlap metric for various model configurations on the\nWOMD interactive test and validation sets.\nacross the modeled agents during both training and infer-\nence rollouts, while the joint version performs 2 Hz inter-\nactive attention. We see that the marginal version obtains a\nrelative 38% higher overlap rate than the joint version. The\ninteractive attention in the joint model allows the agents to\nmore appropriately react to one another.\n4.5. Ablation studies\nInteractive attention frequency.\nTo assess the impor-\ntance of inter-agent reactivity during the joint rollouts, we\nvary the frequency of the interactive attention operation\nwhile keeping other architecture details constant. For our\nleaderboard results, we utilize the greatest frequency stud-\nied here, 2 Hz. At the low end of the spectrum, 0.125 Hz\ncorresponds to the agents only observing each other\u2019s initial\nstates, and then proceeding with the entire 8-second rollout\nwithout communicating again (i.e., marginal rollouts).\nPerformance metrics generally improve as agents are\nNo interactive attention\n16 interaction iterations\n0.125 Hz\n2 Hz\nFigure 5. Visualization of the top joint rollout mode at the two ex-\ntremes of the interactive attention frequencies studied here. With\nno interactive attention (left), the two modeled agents only attend\nto each other once at the beginning of the 8-second rollout and\nnever again, in contrast to 16 total times for 2 Hz attention (right).\nThe independent rollouts resulting from zero interactive attention\ncan result in scene-inconsistent overlap; e.g., a turning vehicle fails\nto accommodate a crossing pedestrian (top left) or yield appropri-\nately to a crossing vehicle (bottom left).\npermitted to interact more frequently (Fig. 6 top, Table 5\nin appendix). Greater interactive attention frequencies not\nonly lead to more accurate joint predictions, but also re-\nduce implausible overlaps (i.e., collisions) between differ-\nent agents\u2019 predictions. Fig. 5 displays examples where the\nmarginal predictions lead to implausible overlap between\nagents while the joint predictions lead to appropriately sep-\narated trajectories. See supplementary for animated visual-\nizations.\nNumber of rollouts.\nOur rollout aggregation requires that\nwe generate a sufficient number of samples from the model\nin order to faithfully represent the multimodal future dis-\ntribution. For this ablation, we vary the number of rollouts\ngenerated, but always cluster down to k = 6 modes for eval-\nuation. In general, we see performance metrics improve as\nadditional rollouts are utilized (Fig. 6, bottom and Table 6\nin appendix). For our final leaderboard results, we use 512\nrollouts per replica, although 32 rollouts is sufficient to sur-\npass the previous top entry on joint mAP.\n0.125\n0.25\n0.5\n1\n2\nInteractive attention frequency (Hz)\n0.885\n0.890\n0.895\n0.900\n0.905\n0.910\nminADE\nminADE\n1.980\n1.990\n2.000\n2.010\n2.020\n2.030\n2.040\n2.050\n2.060\nminFDE\nminFDE\n1 2\n4\n8\n16\n32\nNumber of rollouts per replica\n0.150\n0.160\n0.170\n0.180\n0.190\n0.200\nmAP\nmAP\n0.420\n0.440\n0.460\n0.480\n0.500\n0.520\n0.540\nMiss rate\nMiss rate\nFigure 6.\nJoint prediction performance across varying interac-\ntive attention frequencies (top) and numbers of rollouts per replica\n(bottom) on the WOMD interactive validation set. Vertical axes\ndisplay joint (scene-level) metrics for an 8-replica ensemble. See\nTables 5 and 6 in the appendix for full parameter ranges and met-\nrics.\n4.6. Conditional rollouts\nAs described in Section 3.3, our model naturally sup-\nports \u201ctemporally causal\u201d conditioning, similarly to previ-\nous autoregressive efforts such as [36, 43]. In this setting,\nwe fix one query agent to follow a specified trajectory and\nstochastically roll out the target agent. However, we can\nalso modify the model to leak the query agent\u2019s full trajec-\ntory, acausally exposing its future to the target agent during\nconditioning. This resembles the approach to conditional\nprediction in, e.g., [44], where this acausal conditional dis-\ntribution is modeled directly, or [29], where this distribution\nis accessed via inference in an energy-based model.\nHere, we assess predictions from our model across\nthree settings: marginal, temporally causal conditional, and\nacausal conditional (Fig. 4). Quantitatively, we observe that\nboth types of conditioning lead to more accurate predictions\nfor the target agent (Table 4, Fig. 7). Additionally, we see\nthat acausal conditioning leads to greater improvement than\ntemporally causal conditioning relative to marginal predic-\ntions across all metrics, e.g., 8.2% increase in soft mAP for\nacausal vs. 3.7% increase for temporally causal.\nPrediction setting\nminADE (\u2193)\nminFDE (\u2193)\nMiss Rate (\u2193)\nSoft mAP (\u2191)\nMarginal\n0.6069\n1.2236\n0.1406\n0.3951\nTemporally causal conditional\n0.5997\n1.2034\n0.1377\n0.4096\nAcausal conditional\n0.5899\n1.1804\n0.1338\n0.4274\nTable 4. Conditional prediction performance. Displayed are marginal (single-agent) metrics across three prediction settings for our model\non the WOMD interactive validation set: marginal, temporally causal conditional, and acausal conditional.\nIntuitively, the greater improvement for acausal condi-\ntioning makes sense as it exposes more information to the\nmodel. However, the better quantitative scores are largely\ndue to predictions that would be deemed nonsensical if in-\nterpreted as predicted reactions to the query agent.\nThis can be illustrated in examples where one agent is\nfollowing another, where typically the lead agent\u2019s behavior\nis causing the trailing agent\u2019s reaction, and not vice versa,\nbut this directionality would not be captured with acausal\nconditoning. This temporally causal modeling is especially\nimportant when utilizing the conditional predictions to eval-\nuate safety for an autonomous vehicle\u2019s proposed plans. In\na scenario where an autonomous vehicle (AV) is stopped be-\nhind another agent, planning to move forward into the other\nagent\u2019s current position could be viewed as a safe maneu-\nver with acausal conditioning, as the other agent also mov-\ning forward is correlated with (but not caused by) the AV\nproceeding. However, it is typically the lead agent moving\nforward that causes the trailing AV to proceed, and the AV\nmoving forward on its own would simply rear-end the lead\nagent.\nIn the supplementary, we compare examples of predic-\ntions in various scenarios for the causal and acausal condi-\ntioning schemes. Models that ignore temporal dependen-\ncies during conditioning (e.g., [44, 29]) may succumb to\nthe same incorrect reasoning that the acausal version of our\nmodel does.\n5. Conclusion and future work\nIn this work, we introduced a method for interactive mo-\ntion forecasting leveraging multi-agent rollouts over dis-\ncrete motion tokens, capturing the joint distribution over\nmultimodal futures. The proposed model establishes new\nstate-of-the-art performance on the WOMD interactive pre-\ndiction challenge.\nAvenues for future work include leveraging the trained\nmodel in model-based planning frameworks, allowing a\nsearch tree to be formed over the multi-agent action rollouts,\nor learning amortized value functions from large datasets\nof scene rollouts. In addition, we plan to explore distilla-\ntion strategies from large autoregressive teachers, enabling\nfaster student models to be deployed in latency-critical set-\ntings.\nMarginal\nConditional\nFigure 7. Visualization of the most likely predicted future for the\npedestrian in the marginal setting (left) and temporally causal con-\nditional setting (right). When considering the pedestrian indepen-\ndently, the model assigns greatest probability to a trajectory which\ncrosses the road. When conditioned on the the vehicle\u2019s ground\ntruth turn (magenta), the pedestrian is instead predicted to yield.\nAcknowledgements.\nWe would like to thank David\nWeiss, Paul Covington, Ashish Venugopal, Piotr Fidkowski,\nand Minfa Wang for discussions on autoregressive behavior\npredictors; Cole Gulino and Brandyn White for advising on\ninteractive modeling; Cheol Park, Wenjie Luo, and Scott Et-\ntinger for assistance with evaluation; Drago Anguelov, Kyr-\niacos Shiarlis, and anonymous reviewers for helpful feed-\nback on the manuscript.\nReferences\n[1] Elmira Amirloo, Amir Rasouli, Peter Lakner, Mohsen Ro-\nhani, and Jun Luo. LatentFormer: Multi-agent transformer-\nbased interaction modeling and trajectory prediction, 2022.\n[2] Zal\u00b4an Borsos, Rapha\u00a8el Marinier, Damien Vincent, Eugene\nKharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek,\nOlivier Teboul, David Grangier, Marco Tagliasacchi, and\nNeil Zeghidour. AudioLM: a language modeling approach\nto audio generation, 2023.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020.\n[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In CVPR, 2020.\n[5] Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:\nLearning to predict intention from raw sensor data. In Con-\nference on Robot Learning, pages 947\u2013956, 2018.\n[6] Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urta-\nsun.\nSpagnn: Spatially-aware graph neural networks for\nrelational behavior forecasting from sensor data.\nIn 2020\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 9491\u20139497. IEEE, 2020.\n[7] Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie\nLiao, and Raquel Urtasun. Implicit latent variable model for\nscene-consistent motion forecasting. In European Confer-\nence on Computer Vision, pages 624\u2013641. Springer, 2020.\n[8] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\nAnguelov. Multipath: Multiple probabilistic anchor trajec-\ntory hypotheses for behavior prediction. In Conference on\nRobot Learning, pages 86\u201399. PMLR, 2020.\n[9] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-\njeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter\nCarr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d\ntracking and forecasting with rich maps. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8748\u20138757, 2019.\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, Parker Schuh, et al.\nPaLM: Scaling language\nmodeling with pathways, 2022.\n[11] Alexander Cui, Sergio Casas, Abbas Sadat, Renjie Liao,\nand Raquel Urtasun. Lookout: Diverse multi-future predic-\ntion and planning for self-driving.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 16107\u201316116, 2021.\n[12] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,\nTsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-\nder, and Nemanja Djuric. Multimodal trajectory predictions\nfor autonomous driving using deep convolutional networks.\nIn 2019 International Conference on Robotics and Automa-\ntion (ICRA), pages 2090\u20132096. IEEE, 2019.\n[13] Johan Engstr\u00a8om, Shu-Yuan Liu, Azadeh Dinparastdjadid,\nand Camelia Simoiu. Modeling road user response timing\nin naturalistic settings: a surprise-based framework, 2022.\nURL https://arxiv.org/abs/2208.08651.\n[14] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi\nLiu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben-\njamin Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Au-\nrelien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasude-\nvan, Alexander McCauley, Jonathon Shlens, and Dragomir\nAnguelov. Large scale interactive motion forecasting for au-\ntonomous driving : The waymo open motion dataset. CoRR,\nabs/2104.10133, 2021.\n[15] Roger Girgis, Florian Golemo, Felipe Codevilla, Martin\nWeiss, Jim Aldon D\u2019Souza, Samira Ebrahimi Kahou, Felix\nHeide, and Christopher Pal. Latent variable sequential set\ntransformers for joint multi-agent motion prediction. In In-\nternational Conference on Learning Representations, 2021.\n[16] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The curious case of neural text degeneration. In Inter-\nnational Conference on Learning Representations, 2020.\n[17] Joey Hong, Benjamin Sapp, and James Philbin. Rules of the\nroad: Predicting driving behavior with a convolutional model\nof semantic interactions. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8454\u20138462, 2019.\n[18] Maximilian Igl, Daewoo Kim, Alex Kuefler, Paul Mougin,\nPunit Shah, Kyriacos Shiarlis, Dragomir Anguelov, Mark\nPalatucci, Brandyn White, and Shimon Whiteson.\nSym-\nphony: Learning realistic and diverse agents for autonomous\ndriving simulation. In ICRA, 2022.\n[19] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-\nserman, Oriol Vinyals, and Jo\u02dcao Carreira. Perceiver: General\nperception with iterative attention. In ICML, 2021.\n[20] Xiaosong Jia, Penghao Wu, Li Chen, Hongyang Li, Yu Liu,\nand Junchi Yan. Hdgt: Heterogeneous driving graph trans-\nformer for multi-agent trajectory prediction via scene encod-\ning. CoRL, 2022.\n[21] Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew\nHartnett, and Deva Ramanan.\nWhat-if motion prediction\nfor autonomous driving. arXiv preprint arXiv:2008.10587,\n2020.\n[22] Stepan Konev. Mpa: Multipath++ based architecture for mo-\ntion prediction, 2022. URL https://arxiv.org/abs/\n2206.10041.\n[23] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek,\nSeungjin Choi, and Yee Whye Teh.\nSet transformer: A\nframework for attention-based permutation-invariant neural\nnetworks. In ICML, 2019.\n[24] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B\nChoy, Philip HS Torr, and Manmohan Chandraker. Desire:\nDistant future prediction in dynamic scenes with interacting\nagents. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 336\u2013345, 2017.\n[25] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song\nFeng, and Raquel Urtasun. Learning lane graph representa-\ntions for motion forecasting.\nIn European Conference on\nComputer Vision, pages 541\u2013556. Springer, 2020.\n[26] Jerry Liu, Wenyuan Zeng, Raquel Urtasun, and Ersin Yumer.\nDeep structured reactive planning.\nIn ArXiv, volume\nabs/2101.06832, 2021.\n[27] Qiujing Lu, Weiqiao Han, Jeffrey Ling, Minfa Wang,\nHaoyu Chen, Balakrishnan Varadarajan, and Paul Coving-\nton. Kemp: Keyframe-based hierarchical end-to-end deep\nmodel for long-term trajectory prediction.\nIn 2022 Inter-\nnational Conference on Robotics and Automation (ICRA),\n2022.\n[28] Ruikang Luo, Yaofeng Song, Han Zhao, Yicheng Zhang,\nYi Zhang, Nanbin Zhao, Liping Huang, and Rong Su. Dense-\ntnt: efficient vehicle type classification neural network using\nsatellite imagery. ICCV, 2021.\n[29] Wenjie Luo, Cheol Park, Andre Cornman, Benjamin Sapp,\nand Dragomir Anguelov. JFP: Structured multi-agent inter-\nactive trajectories forecasting for autonomous driving. In 6th\nAnnual Conference on Robot Learning, 2022.\n[30] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Pe-\nter W. Battaglia.\nPolyGen: An autoregressive generative\nmodel of 3D meshes. International Conference on Machine\nLearning, 2020.\n[31] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth\nGoel, Khaled S. Refaat, and Benjamin Sapp. Wayformer:\nMotion forecasting via simple & efficient attention networks.\nArXiv, abs/2207.05844, 2022.\n[32] Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zheng-\ndong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Re-\nbecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venu-\ngopal, David J Weiss, Ben Sapp, Zhifeng Chen, and Jonathon\nShlens. Scene transformer: A unified architecture for pre-\ndicting future trajectories of multiple agents. In International\nConference on Learning Representations, 2022.\n[33] Pedro A. Ortega, Markus Kunesch, Gr\u00b4egoire Del\u00b4etang, Tim\nGenewein, Jordi Grau-Moya, Joel Veness, Jonas Buchli,\nJonas Degrave, Bilal Piot, Julien P\u00b4erolat, Tom Everitt,\nCorentin Tallec, Emilio Parisotto, Tom Erez, Yutian Chen,\nScott E. Reed, Marcus Hutter, Nando de Freitas, and Shane\nLegg. Shaking the foundations: delusions in sequence mod-\nels for interaction and control. ArXiv, abs/2110.10819, 2021.\n[34] Judea Pearl. Causality. Cambridge University Press, 2 edi-\ntion, 2009.\n[35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[36] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and\nSergey Levine. PRECOG: Prediction conditioned on goals\nin visual multi-agent settings. In ICCV, 2019.\n[37] A Robicquet, A Sadeghian, A Alahi, and S Savarese.\nLearning social etiquette: Human trajectory prediction in\ncrowded scenes. In European Conference on Computer Vi-\nsion (ECCV), volume 2, 2020.\n[38] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A re-\nduction of imitation learning and structured prediction to no-\nregret online learning. In AISTATS, 2011.\n[39] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and\nMarco Pavone.\nTrajectron++: Multi-agent generative tra-\njectory forecasting with heterogeneous data for control. In\nECCV, 2020.\n[40] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele.\nMotion transformer with global intention localization and lo-\ncal movement refinement. Advances in Neural Information\nProcessing Systems, 2022.\n[41] Haoran Song, Wenchao Ding, Yuxuan Chen, Shaojie Shen,\nMichael Yu Wang, and Qifeng Chen.\nPiP: Planning-\ninformed trajectory prediction for autonomous driving. In\nECCV, 2020.\n[42] Qiao Sun, Xin Huang, Junru Gu, Brian Williams, and Hang\nZhao.\nM2I: From factored marginal trajectory prediction\nto interactive prediction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2022.\n[43] Yichuan Charlie Tang and Ruslan Salakhutdinov. Multiple\nfutures prediction. In Advances in neural information pro-\ncessing systems, 2019.\n[44] Ekaterina Tolstaya, Reza Mahjourian, Carlton Downey,\nBalakrishnan Vadarajan, Benjamin Sapp, and Dragomir\nAnguelov. Identifying driver interactions via conditional be-\nhavior prediction. In 2021 IEEE International Conference\non Robotics and Automation (ICRA), 2021.\n[45] A\u00a8aron van den Oord,\nNal Kalchbrenner,\nand Koray\nKavukcuoglu. Pixel recurrent neural networks. In Interna-\ntional conference on machine learning, pages 1747\u20131756.\nPMLR, 2016.\n[46] A\u00a8aron van den Oord, Sander Dieleman, Heiga Zen, Karen\nSimonyan, Oriol Vinyals, Alexander Graves, Nal Kalchbren-\nner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A\ngenerative model for raw audio. In ArXiv, 2016.\n[47] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Sri-\nvastava, Khaled S. Refaat, Nigamaa Nayakanti, Andre\nCornman, Kan Chen, Bertrand Douillard, Chi-Pang Lam,\nDragomir Anguelov, and Benjamin Sapp. Multipath++: Ef-\nficient information fusion and trajectory aggregation for be-\nhavior prediction. In ICRA, 2022.\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, 2017.\n[49] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. Transactions on Machine Learn-\ning Research, 2022.\n[50] Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M Kitani.\nAgentformer: Agent-aware transformers for socio-temporal\nmulti-agent forecasting. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 9813\u2013\n9823, 2021.\n[51] Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey\nClausse, Maximilian Naumann, Julius Kummerle, Hendrik\nKonigshof, Christoph Stiller, Arnaud de La Fortelle, et al.\nInteraction dataset: An international, adversarial and coop-\nerative motion dataset in interactive driving scenarios with\nsemantic maps. arXiv preprint arXiv:1910.03088, 2019.\n[52] Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp,\nBalakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai,\nCordelia Schmid, et al. Tnt: Target-driven trajectory pre-\ndiction. In Conference on Robot Learning, pages 895\u2013904.\nPMLR, 2021.\nA. Motion token vocabulary\nDelta action space.\nThe models presented in this paper\nuse the following parameters for the discretized delta action\nspace:\n\u2022 Step frequency: 2 Hz\n\u2022 Delta interval (per step): [-18.0 m, 18.0 m]\n\u2022 Number of bins: 128\nAt 2 Hz prediction, a maximum delta magnitude of 18 m\ncovers axis-aligned speeds up to 36 m/s (\u223c80 mph), > 99%\nof the WOMD dataset.\nVerlet-wrapped action space.\nOnce the above delta ac-\ntion space has the Verlet wrapper applied, we only require\n13 bins for each coordinate.\nThis results in a total of\n132 = 169 total discrete motion tokens that the model can\nselect from the Cartesian product comprising the final vo-\ncabulary.\nSequence lengths.\nFor 8-second futures, the model out-\nputs 16 motion tokens for each agent (note that WOMD\nevaluates predictions at 2 Hz). For the two-agent interac-\ntive split, our flattened agent-time token sequences (Sec-\ntion 3.2.2) have length 2 \u00d7 16 = 32.\nB. Implementation details\nB.1. Scene encoder\nWe follow the design of the early fusion network pro-\nposed by [31] as the scene encoding backbone of our model.\nThe following hyperparameters are used:\n\u2022 Number of layers: 4\n\u2022 Hidden size: 256\n\u2022 Feed-forward network intermediate size: 1024\n\u2022 Number of attention heads: 4\n\u2022 Number of latent queries: 92\n\u2022 Activation: ReLU\nB.2. Trajectory decoder\nTo autoregressively decode motion token sequences, we\nutilize a causal transformer decoder that takes in the motion\ntokens as queries, and the scene encodings as context. We\nuse the following model hyperparameters:\n\u2022 Number of layers: 4\n\u2022 Hidden size: 256\nN agents\nT timesteps\nFigure 8.\nMasked causal attention between two agents dur-\ning training. We flatten the agent and time axes, leading to an\nNT \u00d7 NT attention mask. The agents may attend to each other\u2019s\nprevious motion tokens (solid squares) but no future tokens (empty\nsquares).\n\u2022 Feed-forward network intermediate size: 1024\n\u2022 Number of attention heads: 4\n\u2022 Activation: ReLU\nB.3. Optimization\nWe train our model to maximize the likelihood of the\nground truth motion token sequences via teacher forcing.\nWe use the following training hyperparameters:\n\u2022 Number of training steps: 600000\n\u2022 Batch size: 256\n\u2022 Learning rate schedule: Linear decay\n\u2022 Initial learning rate: 0.0006\n\u2022 Final learning rate: 0.0\n\u2022 Optimizer: AdamW\n\u2022 Weight decay: 0.6\nB.4. Inference\nWe found nucleus sampling [16], commonly used with\nlanguage models, to be helpful for improving sample qual-\nity while maintaining diversity. Here we set the top-p pa-\nrameter to 0.95.\nC. Metrics descriptions\nC.1. WOMD metrics\nAll metrics for the two WOMD [14] benchmarks are\nevaluated at three time steps (3, 5, and 8 seconds) and are\naveraged over all object types to obtain the final value. For\njoint metrics, a scene is attributed to an object class (vehicle,\npedestrian, or cyclist) according to the least common type\nof agent that is present in that interaction, with cyclist being\nthe rarest object class and vehicles being the most common.\nUp to 6 trajectories are produced by the models for each\ntarget agent in each scene, which are then used for metric\nevaluation.\nmAP & Soft mAP\nmAP measures precision of predic-\ntion likelihoods and is calculated by first bucketing ground\ntruth futures of objects into eight discrete classes of intent:\nstraight, straight-left, straight-right, left, right, left u-turn,\nright u-turn, and stationary.\nFor marginal predictions, a prediction trajectory is con-\nsidered a \u201cmiss\u201d if it exceeds a lateral or longitudinal error\nthreshold at a specified timestep T. Similarly for joint pre-\ndictions, a prediction is considered a \u201cmiss\u201d if none of the\nk joint predictions contains trajectories for all predicted ob-\njects within a given lateral and longitudinal error threshold,\nwith respect to the ground truth trajectories for each agent.\nTrajectory predictions classified as a miss are labeled as a\nfalse positive. In the event of multiple predictions satisfy-\ning the miss criteria, consistent with object detection mAP\nmetrics, only one true positive is allowed for each scene, as-\nsigned to the highest confidence prediction. All other pre-\ndictions for the object are assigned a false positive.\nTo compute the mAP metric, bucket entries are sorted\nand a P/R curve is computed for each bucket, averaging\nprecision values over various likelihood thresholds for all\nintent buckets results in the final mAP value. Soft mAP\ndiffers only in the fact that additional matching predictions\n(other than the most likely match) are ignored instead of be-\ning assigned a false positive, and so are not penalized in the\nmetric computation.\nMiss rate\nUsing the same definition of a \u201cmiss\u201d described\nabove for either marginal or joint predictions, miss rate is a\nmeasure of what fraction of scenarios fail to generate any\npredictions within the lateral and longitudinal error thresh-\nolds, relative to the ground truth future.\nminADE & minFDE\nminADE measures the Euclidean\ndistance error averaged over all timesteps for the closest\nprediction, relative to ground truth. In contrast, minFDE\nconsiders only the distance error at the final timestep. For\njoint predictions, minADE and minFDE are calculated as\nthe average value over both agents.\nC.2. Prediction overlap\nAs described in [29], the WOMD [14] overlap met-\nric only considers overlap between predictions and ground\ntruth. Here we use a prediction overlap metric to assess\nscene-level consistency for joint models. Our implementa-\ntion is similar to [29], except we follow the convention of\nthe WOMD challenge of only requiring models to gener-\nate (x, y) waypoints; headings are inferred as in [14]. If\nthe bounding boxes of two predicted agents collide at any\ntimestep in a scene, that counts as an overlap/collision for\nthat scene. The final prediction overlap rate is calculated as\nthe sum of per-scene overlaps, averaged across the dataset.\nD. Additional evaluation\nAblations.\nTables 5 and 6 display joint prediction perfor-\nmance across varying interactive attention frequencies and\nnumbers of rollouts, respectively. In addition to the ensem-\nbled model performance, single replica performance is eval-\nuated. Standard deviations are computed for each metric\nover 8 independently trained replicas.\nScaling analysis.\nTable 7 displays the performance of\ndifferent model sizes on the WOMD interactive split, all\ntrained with the same optimization hyperparameters. We\nvary the number of layers, hidden size, and number of atten-\ntion heads in the encoder and decoder proportionally. Due\nto external constraints, in this study we only train a single\nreplica for each parameter count. We observe that a model\nwith 27M parameters overfits while 300K underfits. Both\nthe 1M and 9M models perform decently. In this paper, our\nmain results use 9M-parameter replicas.\nLatency analysis.\nTable 8 provides inference latency on\nthe latest generation of GPUs across different numbers of\nrollouts.\nThese were measured for a single-replica joint\nmodel rolling out two agents.\nE. Visualizations\nIn the supplementary zip file, we have included GIF an-\nimations of the model\u2019s greatest-probability predictions in\nvarious scenes. Each example below displays the associ-\nated scene ID, which is also contained in the corresponding\nGIF filename. We describe the examples here.\nE.1. Marginal vs. Joint\n\u2022 Scene ID: 286a65c777726df3\nMarginal: The turning vehicle and crossing cyclist\ncollide.\nJoint: The vehicle yields to the cyclist before turning.\n\u2022 Scene ID: 440bbf422d08f4c0\nMarginal: The turning vehicle collides with the cross-\ning vehicle in the middle of the intersection.\nJoint:\nThe turning vehicle yields and collision is\navoided.\n\u2022 Scene ID: 38899bce1e306fb1\nMarginal: The lane-changing vehicle gets rear-ended\nby the vehicle in the adjacent lane.\nJoint: The adjacent vehicle slows down to allow the\nlane-changing vehicle to complete the maneuver.\nEnsemble\nSingle Replica\nFreq. (Hz)\nminADE (\u2193)\nminFDE (\u2193)\nMR (\u2193)\nmAP (\u2191)\nminADE (\u2193)\nminFDE (\u2193)\nMR (\u2193)\nmAP (\u2191)\n0.125\n0.9120\n2.0634\n0.4222\n0.2007\n1.0681 (0.011)\n2.4783 (0.025)\n0.5112 (0.007)\n0.1558 (0.007)\n0.25\n0.9083\n2.0466\n0.4241\n0.1983\n1.0630 (0.009)\n2.4510 (0.025)\n0.5094 (0.006)\n0.1551 (0.006)\n0.5\n0.8931\n2.0073\n0.4173\n0.2077\n1.0512 (0.009)\n2.4263 (0.022)\n0.5039 (0.006)\n0.1588 (0.004)\n1\n0.8842\n1.9898\n0.4117\n0.2040\n1.0419 (0.014)\n2.4062 (0.032)\n0.5005 (0.008)\n0.1639 (0.005)\n2\n0.8831\n1.9825\n0.4092\n0.2150\n1.0345 (0.012)\n2.3886 (0.031)\n0.4943 (0.006)\n0.1687 (0.004)\nTable 5. Joint prediction performance across varying interactive attention frequencies on the WOMD interactive validation set. Displayed\nare scene-level joint evaluation metrics. For the single replica metrics, we include the standard deviation (across 8 replicas) in parentheses.\nEnsemble\nSingle Replica\n# Rollouts\nminADE (\u2193)\nminFDE (\u2193)\nMR (\u2193)\nmAP (\u2191)\nminADE (\u2193)\nminFDE (\u2193)\nMR (\u2193)\nmAP (\u2191)\n1\n1.0534\n2.3526\n0.5370\n0.1524\n1.9827 (0.018)\n4.7958 (0.054)\n0.8182 (0.003)\n0.0578 (0.004)\n2\n0.9952\n2.2172\n0.4921\n0.1721\n1.6142 (0.011)\n3.8479 (0.032)\n0.7410 (0.003)\n0.0827 (0.004)\n4\n0.9449\n2.1100\n0.4561\n0.1869\n1.3655 (0.012)\n3.2060 (0.035)\n0.6671 (0.003)\n0.1083 (0.003)\n8\n0.9158\n2.0495\n0.4339\n0.1934\n1.2039 (0.013)\n2.7848 (0.035)\n0.5994 (0.004)\n0.1324 (0.003)\n16\n0.9010\n2.0163\n0.4196\n0.2024\n1.1254 (0.012)\n2.5893 (0.031)\n0.5555 (0.005)\n0.1457 (0.003)\n32\n0.8940\n2.0041\n0.4141\n0.2065\n1.0837 (0.013)\n2.4945 (0.035)\n0.5272 (0.005)\n0.1538 (0.004)\n64\n0.8881\n1.9888\n0.4095\n0.2051\n1.0585 (0.012)\n2.4411 (0.033)\n0.5114 (0.005)\n0.1585 (0.004)\n128\n0.8851\n1.9893\n0.4103\n0.2074\n1.0456 (0.012)\n2.4131 (0.033)\n0.5020 (0.006)\n0.1625 (0.004)\n256\n0.8856\n1.9893\n0.4078\n0.2137\n1.0385 (0.012)\n2.3984 (0.031)\n0.4972 (0.007)\n0.1663 (0.005)\n512\n0.8831\n1.9825\n0.4092\n0.2150\n1.0345 (0.012)\n2.3886 (0.031)\n0.4943 (0.006)\n0.1687 (0.004)\nTable 6. Joint prediction performance across varying numbers of rollouts per replica on the WOMD interactive validation set. Displayed\nare scene-level joint evaluation metrics. For the single replica metrics, we include the standard deviation (across 8 replicas) in parentheses.\nParameter count\nMiss Rate (\u2193)\nmAP (\u2191)\n300K\n0.6047\n0.1054\n1M\n0.5037\n0.1713\n9M\n0.4972\n0.1663\n27M\n0.6072\n0.1376\nTable 7. Joint prediction performance across varying model sizes\non the WOMD interactive validation set. Displayed are scene-\nlevel joint mAP and miss rate for 256 rollouts for a single model\nreplica (except for 9M which displays the mean performance of 8\nreplicas).\n\u2022 Scene ID: 2ea76e74b5025ec7\nMarginal: The cyclist crosses in front of the vehicle\nleading to a collision.\nJoint: The cyclist waits for the vehicle to proceed be-\nfore turning.\n\u2022 Scene ID: 55b5fe989aa4644b\nMarginal: The cyclist lane changes in front of the ad-\njacent vehicle, leading to collision.\nJoint: The cyclist remains in their lane for the duration\nof the scene, avoiding collision.\nNumber of rollouts\nLatency (ms)\n16\n19.9 (0.19)\n32\n27.5 (0.25)\n64\n43.8 (0.26)\n128\n75.8 (0.23)\n256\n137.7 (0.19)\nTable 8. Inference latency on current generation of GPUs for dif-\nferent numbers of rollouts of the joint model. We display the mean\nand standard deviation (in parentheses) of the latency measure-\nments for each setting.\nE.2. Marginal vs. Conditional\n\u201cConditional\u201d here refers to temporally causal condition-\ning as described in the main text.\n\u2022 Scene ID: 5ebba77f351358e2\nMarginal: The pedestrian crosses the street as a vehi-\ncle is turning, leading to a collision.\nConditional:\nWhen conditioning on the vehicle\u2019s\nturning trajectory as a query, the pedestrian is instead\npredicted to remain stationary.\n\u2022 Scene ID: d557eee96705c822\nMarginal: The modeled vehicle collides with the lead\nvehicle.\nConditional: When conditioning on the lead vehicle\u2019s\nquery trajectory, which remains stationary for a bit,\nthe modeled vehicle instead comes to a an appropriate\nstop.\n\u2022 Scene ID: 9410e72c551f0aec\nMarginal: The modeled vehicle takes the turn slowly,\nunaware of the last turning vehicle\u2019s progress.\nConditional: When conditioning on the query vehi-\ncle\u2019s turn progress, the modeled agent likewise makes\nmore progress.\n\u2022 Scene ID: c204982298bda1a1\nMarginal: The modeled vehicle proceeds slowly, un-\naware of the merging vehicle\u2019s progress.\nConditional: When conditioning on the query vehi-\ncle\u2019s merge progress, the modeled agent accelerates\nbehind.\nE.3. Temporally Causal vs. Acausal Conditioning\n\u2022 Scene ID: 4f39d4eb35a4c07c\nJoint prediction: The two modeled vehicles maintain\nspeed for the duration of the scene.\nConditioning on trailing agent:\n- Temporally causal: The lead vehicle is indifferent\nto the query trailing vehicle decelerating to a stop, pro-\nceeding along at a constant speed.\n- Acausal: The lead vehicle is \u201cinfluenced\u201d by the\nquery vehicle decelerating. It likewise comes to a stop.\nIntuitively, this is an incorrect direction of influence\nthat the acausal model has learned.\nConditioning on lead agent:\n- Temporally causal: When conditioning on the query\nlead vehicle decelerating to a stop, the modeled trail-\ning vehicle is likewise predicted to stop.\n-Acausal: In this case, the acausal conditional pre-\ndiction is similar to the temporally causal conditional.\nThe trailing vehicle is predicted to stop behind the\nquery lead vehicle.\n"
  },
  {
    "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond",
    "link": "https://arxiv.org/pdf/2309.16583.pdf",
    "upvote": "12",
    "text": "GPT-FATHOM: BENCHMARKING LARGE LANGUAGE\nMODELS TO DECIPHER THE EVOLUTIONARY PATH\nTOWARDS GPT-4 AND BEYOND\nShen Zheng2\u2217\u2020, Yuyu Zhang1\u2217, Yijie Zhu1, Chenguang Xi1, Pengyang Gao1,\nXun Zhou1 & Kevin Chen-Chuan Chang2\n1ByteDance\n2University of Illinois at Urbana-Champaign\nshenz2@illinois.edu, yuyu.zhang@bytedance.com\nhttps://github.com/GPT-Fathom/GPT-Fathom\nABSTRACT\nWith the rapid advancement of large language models (LLMs), there is a pressing\nneed for a comprehensive evaluation suite to assess their capabilities and limi-\ntations. Existing LLM leaderboards often reference scores reported in other pa-\npers without consistent settings and prompts, which may inadvertently encourage\ncherry-picking favored settings and prompts for better results. In this work, we\nintroduce GPT-Fathom, an open-source and reproducible LLM evaluation suite\nbuilt on top of OpenAI Evals1. We systematically evaluate 10+ leading LLMs as\nwell as OpenAI\u2019s legacy models on 20+ curated benchmarks across 7 capability\ncategories, all under aligned settings. Our retrospective study on OpenAI\u2019s earlier\nmodels offers valuable insights into the evolutionary path from GPT-3 to GPT-4.\nCurrently, the community is eager to know how GPT-3 progressively improves to\nGPT-4, including technical details like whether adding code data improves LLM\u2019s\nreasoning capability, which aspects of LLM capability can be improved by SFT\nand RLHF, how much is the alignment tax, etc. Our analysis sheds light on many\nof these questions, aiming to improve the transparency of advanced LLMs.\n1\nINTRODUCTION\nRecently, the advancement of large language models (LLMs) is arguably the most remarkable break-\nthrough in Artificial Intelligence (AI) in the past few years. Based on the Transformer (Vaswani\net al., 2017) architecture, these LLMs are trained on massive Web-scale text corpora. Despite their\nstraightforward method of using a self-supervised objective to predict the next token, leading LLMs\ndemonstrate exceptional capabilities across a range of challenging tasks (Bubeck et al., 2023), even\nshowing a potential path towards Artificial General Intelligence (AGI). With the rapid progress of\nLLMs, there is a growing demand for better understanding these powerful models, including the\ndistribution of their multi-aspect capabilities, limitations and risks, and directions and priorities of\ntheir future improvement. It is critical to establish a carefully curated evaluation suite that measures\nLLMs in a systematic, transparent and reproducible manner. Although there already exist many\nLLM leaderboards and evaluation suites, some key challenges are yet to be addressed:\n\u2022 Inconsistent settings: The evaluation settings, such as the number of in-context example \u201cshots\u201d,\nwhether Chain-of-Thought (CoT; Wei et al. 2022) prompting is used, methods of answer parsing\nand metric computation, etc., often differ across the existing LLM works. Moreover, most of the\nreleased LLMs do not disclose their prompts used for evaluation, making it difficult to reproduce\nthe reported scores. Different settings and prompts may lead to very different evaluation results,\nwhich may easily skew the observations. Yet, many existing LLM leaderboards reference scores\nfrom other papers without consistent settings and prompts, which may inadvertently encourage\ncherry-picking favored settings and prompts for better results. To achieve reliable conclusions, it\nis crucial to make apples-to-apples LLM comparisons with consistent settings and prompts.\n\u2217Leading co-authors with equal contribution.\n\u2020Work done during an internship at ByteDance.\n1\narXiv:2309.16583v5  [cs.CL]  19 Dec 2023\n\u2022 Incomplete collection of models and benchmarks: For the moment, when compared to OpenAI\u2019s\nleading models such as GPT-4, all the other LLMs (particularly open-source models) exhibit a\nsubstantial performance gap. In fact, it takes OpenAI nearly three years to evolve from GPT-\n3 (released in 2020/06) to GPT-4 (released in 2023/03). Existing LLM leaderboards primarily\nfocus on the latest models, while missing a retrospective study on OpenAI\u2019s earlier models and\nits mysterious path from GPT-3 to GPT-4. Besides the coverage of models, many existing works\nassess LLMs on merely one or a few aspects of capabilities, which is not sufficient to provide a\ncomprehensive view to deeply understand the strength and weakness of the evaluated LLMs.\n\u2022 Insufficient study on model sensitivity: LLMs are known to be sensitive to the evaluation setting\nand the formatting of prompt (Liang et al., 2023). However, many existing works only focus on\nthe benchmark score under one specific setting, while overlooking the impacts of model sensitivity\non the overall usability of LLMs. In fact, it is unacceptable that a slightly rephrased prompt could\ncause the LLM to fail in responding it correctly. Due to the lack of systematic study on model\nsensitivity, this potential vulnerability in LLMs remains not well understood.\nThese challenges hinder a comprehensive understanding of LLMs. To dispel the mist among LLM\nevaluations, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite de-\nveloped based on OpenAI Evals1. We evaluate 10+ leading open-source and closed-source LLMs on\n20+ curated benchmarks in 7 capability categories under aligned settings. We also evaluate legacy\nmodels from OpenAI to retrospectively measure their progressive improvement in each capability\ndimension. Our retrospective study offers valuable insights into OpenAI\u2019s evolutionary path from\nGPT-3 to GPT-4, aiming to help the community better understand this enigmatic path. Our analysis\nsheds light on many community-concerned questions (e.g., the gap between OpenAI / non-OpenAI\nmodels, whether adding code data improves reasoning capability, which aspects of LLM capability\ncan be improved by SFT and RLHF, how much is the alignment tax, etc.). With reproducible evalu-\nations, GPT-Fathom serves as a standard gauge to pinpoint the position of emerging LLMs, aiming\nto help the community measure and bridge the gap with leading LLMs. We also explore the impacts\nof model sensitivity on evaluation results with extensive experiments of various settings.\nBenchmarks constantly play a pivotal role in steering the evolution of AI and, of course, directing the\nadvancement of LLMs as well. There are many great existing LLM evaluation suites. By comparing\nGPT-Fathom with previous works, we summarize the major difference as follows: 1) HELM (Liang\net al., 2023) primarily uses answer-only prompting (without CoT) and has not included the latest\nleading models such as GPT-4 (as of the time of writing); 2) Open LLM Leaderboard (Beeching\net al., 2023) focuses on open-source LLMs, while we jointly consider leading closed-source and\nopen-source LLMs; 3) OpenCompass (Contributors, 2023) evaluates latest open-source and closed-\nsource LLMs (all released after 2023/03), while we cover both leading LLMs and OpenAI\u2019s earlier\nmodels to decipher the evolutionary path from GPT-3 to GPT-4; 4) InstructEval (Chia et al., 2023)\nis designed for evaluating instruction-tuned LLMs, while we evaluate both base and SFT / RLHF\nmodels; 5) AlpacaEval (Li et al., 2023) evaluates on simple instruction-following tasks as a quick\nand cheap proxy of human evaluation, while we provide systematic evaluation of various aspects of\nLLM capabilities; 6) Chatbot Arena (Zheng et al., 2023) evaluates human user\u2019s dialog preference\nwith a Elo rating system, while we focus on automatic and reproducible evaluation over popular\nbenchmarks; 7) Chain-of-Thought Hub (Fu et al., 2023) focuses on evaluating the reasoning capa-\nbility of LLMs with CoT prompting, while we support both CoT and answer-only prompting settings\nand evaluate various aspects of LLM capabilities.\nThe key contributions of our work are summarized as follows:\n\u2022 Systematic and reproducible evaluations under aligned settings: We provide accurate evaluations\nof 10+ leading LLMs on 20+ curated benchmarks across 7 capability categories. We carefully\nalign the evaluation setting for each benchmark. Our work improves the transparency of LLMs,\nand all of our evaluation results can be easily reproduced.\n\u2022 Retrospective study on the evolutionary path from GPT-3 to GPT-4: We evaluate not only leading\nLLMs, but also OpenAI\u2019s earlier models, to retrospectively study their progressive improvement\nand better understand the path towards GPT-4 and beyond. Our work is time-sensitive due to the\nscheduled deprecation of those legacy models announced by OpenAI2.\n1https://github.com/openai/evals\n2https://openai.com/blog/gpt-4-api-general-availability\n2\ndavinci\n(GPT-3)\ndavinci-instruct-beta\n(InstructGPT SFT)\ntext-davinci-001\ncode-cushman-001\n(Codex-12B)\ncode-davinci-002\ntext-davinci-002\ntext-davinci-003\ngpt-3.5-turbo-0301\ngpt-4-0314\ngpt-3.5-turbo-0613\ngpt-4-0613\nFeedME\nPPO\nOptimized for chat\nAPI access\nUpdated\nSFT\nCode\nfine-tuning\nFeedME\n2020/06\n2021/07\n2021/08\n2022/01\n2022/03\n2022/03\n2022/11\nUpdated\n2022/11\n2023/03\n2023/06\n2023/06\n2023/06\nGPT-3 Series\nGPT-3.5 Series\nGPT-4\n2023/09\n2023/07\nCode\nsandbox\nData\nanalysis\nCode training + RLHF\nLarger & multimodal model\n2023/03\nAPI access\nFigure 1: OpenAI\u2019s evolutionary path from GPT-3 to GPT-4. We omit deprecated legacy models\nsuch as code-davinci-001 and only list the models evaluated in GPT-Fathom.\n\u2022 Identify novel challenges of advanced LLMs: We discover the seesaw phenomenon of LLM ca-\npabilities, even on the latest GPT-4 model. We also study the impacts of model sensitivity with\nextensive experiments. We strongly encourage the research community to dedicate more efforts\nto tackling these novel challenges.\n2\nMETHOD\nImagine the ultimate superset of LLM evaluations: a holistic collection that evaluates every LLM\non every benchmark under every possible setting. In practice, however, due to resource and time\nconstraints, we are unable to exhaustively fulfill this ideal evaluation superset. Instead, we pick\nrepresentative LLMs, benchmarks and settings to investigate open problems. In this section, we\ndiscuss in detail how we select LLMs, benchmarks and settings for our evaluations.\n2.1\nLLMS FOR EVALUATION\nThe goal of GPT-Fathom is to curate a high-quality collection of representative LLMs and bench-\nmarks, helping the community better understand OpenAI\u2019s evolutionary path and pinpoint the posi-\ntion of future LLMs. To achieve this goal, we mainly consider evaluating these types of LLMs: 1)\nOpenAI\u2019s leading models; 2) OpenAI\u2019s major earlier models3; 3) other leading closed-source mod-\nels; 4) leading open-source models. As a result, we select OpenAI\u2019s models (illustrated in Figure 1),\nPaLM 2 (Anil et al., 2023), Claude 24, LLaMA (Touvron et al., 2023a) and Llama 2 (Touvron et al.,\n2023b) for evaluation. Due to the limited space, refer to Appendix A for the detailed model list.\n2.2\nBENCHMARKS FOR EVALUATION\nWe consider the following criteria for benchmark selection: 1) cover as many aspects of LLM ca-\npabilities as possible; 2) adopt widely used benchmarks for LLM evaluation; 3) clearly distinguish\nstrong LLMs from weaker ones; 4) align well with the actual usage experience of LLMs. Accord-\ningly, we construct a capability taxonomy by initially enumerating the capability categories (task\ntypes), and then populating each category with selected benchmarks.\nKnowledge. This category evaluates LLM\u2019s capability on world knowledge, which requires not\nonly memorizing the enormous knowledge in the pretraining data but also connecting fragments of\n3https://platform.openai.com/docs/model-index-for-researchers\n4https://www.anthropic.com/index/claude-2\n3\nknowledge and reasoning over them. We currently have two sub-categories here: 1) Question An-\nswering, which directly tests whether the LLM knows some facts by asking questions. We adopt Nat-\nural Questions5 (Kwiatkowski et al., 2019), WebQuestions (Berant et al., 2013) and TriviaQA (Joshi\net al., 2017) as our benchmarks; 2) Multi-subject Test, which uses human exam questions to evaluate\nLLMs. We adopt popular benchmarks MMLU (Hendrycks et al., 2021a), AGIEval (Zhong et al.,\n2023) (we use the English partition denoted as AGIEval-EN) and ARC (Clark et al., 2018) (including\nARC-e and ARC-c partitions to differentiate easy / challenge difficulty levels) in our evaluation.\nReasoning. This category measures the general reasoning capability of LLMs, including 1) Com-\nmonsense Reasoning, which evaluates how LLMs perform on commonsense tasks (which are typ-\nically easy for humans but could be tricky for LLMs). We adopt popular commonsense reason-\ning benchmarks LAMBADA (Paperno et al., 2016), HellaSwag (Zellers et al., 2019) and Wino-\nGrande (Sakaguchi et al., 2021) in our evaluation; 2) Comprehensive Reasoning, which aggregates\nvarious reasoning tasks into one single benchmark. We adopt BBH (Suzgun et al., 2023), a widely\nused benchmark with a subset of 23 hard tasks from the BIG-Bench (Srivastava et al., 2023) suite.\nComprehension. This category assesses the capability of reading comprehension, which requires\nLLMs to first read the provided context and then answer questions about it. This has been a long-\nterm challenging task in natural language understanding. We pick up popular reading comprehen-\nsion benchmarks RACE (Lai et al., 2017) (including RACE-m and RACE-h partitions to differentiate\nmiddle / high school difficulty levels) and DROP (Dua et al., 2019) for this category.\nMath. This category specifically tests LLM\u2019s mathematical capability. Tasks that require mathe-\nmatical reasoning are found to be challenging for LLMs (Imani et al., 2023; Dziri et al., 2023). We\nadopt two popular math benchmarks, namely GSM8K (Cobbe et al., 2021), which consists of 8,500\ngrade school math word problems, and MATH (Hendrycks et al., 2021b), which contains 12,500\nproblems from high school competitions in 7 mathematics subject areas.\nCoding. This category examines the coding capability of LLMs, which is commonly deemed as a\ncore capability of leading LLMs. We pick up popular benchmarks HumanEval (Chen et al., 2021)\nand MBPP (Austin et al., 2021), both of which are natural language to code datasets that require\nLLMs to generate self-contained Python programs that pass a set of held-out test cases. Following\nChen et al. (2021), we adopt the widely used pass@k metric: k code samples are generated for each\ncoding problem, and a problem is considered solved if any sample passes the unit tests; the total\nfraction of problems solved is reported.\nMultilingual. This category inspects the multilingual capability of LLMs, which is important for the\nusage experience of non-English users. Beyond pure multilingual tasks like translation (which we\nplan to support in the near future), we view multilingual capability as an orthogonal dimension, i.e.,\nLLMs can be evaluated on the intersection of a fundamental capability and a specific language, such\nas (\u201cKnowledge\u201d, Chinese), (\u201cReasoning\u201d, French), (\u201cMath\u201d, German), etc. Nonetheless, given that\nmost existing benchmarks focus solely on English, we currently keep \u201cMultilingual\u201d as a distinct\ncapability category in parallel with the others. We then populate it with sub-categories and corre-\nsponding benchmarks: 1) Multi-subject Test, we use the Chinese partition of AGIEval (Zhong et al.,\n2023) denoted as AGIEval-ZH, and C-Eval (Huang et al., 2023) which is a comprehensive multi-\ndiscipline exam benchmark in Chinese; 2) Mathematical Reasoning, we adopt MGSM6 (Shi et al.,\n2023), a multilingual version of GSM8K that translates a subset of examples into 10 typologically\ndiverse languages; 3) Question Answering, we adopt a popular multilingual question answering\nbenchmark TyDi QA7 (Clark et al., 2020) that covers 11 typologically diverse languages.\nSafety. This category scrutinizes LLM\u2019s propensity to generate content that is truthful, reliable,\nnon-toxic and non-biased, thereby aligning well with human values. To this end, we currently have\ntwo sub-categories (and plan to support more benchmarks in the future): 1) Truthfulness, we employ\n5For Natural Questions, we evaluate in the closed-book setting, where only the question is provided, without\na context document.\n6For MGSM, we evaluate the average score over the 10 language partitions, including Bengali, Chinese,\nFrench, German, Japanese, Russian, Spanish, Swahili, Telugu and Thai.\n7For TyDi QA, we evaluate in the no-context setting, where no gold passage is provided. We evaluate\nthe average score over the 11 language partitions, including English, Arabic, Bengali, Finnish, Indonesian,\nJapanese, Kiswahili, Korean, Russian, Telugu and Thai.\n4\nTruthfulQA8 (Lin et al., 2022), a benchmark designed to evaluate LLM\u2019s factuality; 2) Toxicity, we\nadopt RealToxicityPrompts (Gehman et al., 2020) to quantify the risk of generating toxic output.\nNote that the categories above are based on our own interpretation of LLM capabilities, which is\nby no means the exclusive approach to systematically evaluating LLMs. Additionally, some bench-\nmarks may necessitate a range of capabilities. For instance, both \u201cKnowledge\u201d and \u201cReasoning\u201d\ncould influence the performance on MMLU. For the sake of simplicity, we just assign each bench-\nmark to a primary capability category for evaluation. Due to the limited space, refer to Appendix B\nfor details of dataset splits and source of prompts.\nApart from the capability categories listed above, we also plan to support more LLM capabilities\nin the future, such as long-context understanding, multi-turn conversation, open-domain generation,\nLLM-based autonomous agent, etc., some of which may require subjective assessment or using a\npowerful LLM to automatically evaluate LLMs such as Li et al. (2023).\n2.3\nDETAILS OF BLACK-BOX EVALUATION\nBoth black-box and white-box evaluation methods are popular for evaluating LLMs. We describe\ntheir difference and discuss why we choose the black-box method as follows.\nBlack-box evaluation: Given the test prompt, LLM first generates free-form response; the response\nis then parsed into the final answer for computing the evaluation metric against the reference answer.\nFor multiple-choice questions, the reference answer is typically the letter of the correct option such\nas (A), (B), (C) or (D).\nWhite-box evaluation: Given the test prompt, LLM generates per-token likelihood for each option;\nthe per-token likelihood is then normalized for length and optionally normalized by answer context\nas described in Brown et al. (2020). The option with the maximum normalized likelihood is then\npicked as the predicted option.\nGPT-Fathom adopts the black-box method throughout all evaluations, since 1) the per-token like-\nlihood for input prompt is usually not provided by closed-source LLMs; 2) the white-box method\nmanually restricts the prediction space, thus the evaluation result would be no worse than random\nguess in expectation; while for the black-box method, a model with inferior capability of instruction\nfollowing may get 0 score since the output space is purely free-form. In our opinion, instruction\nfollowing is such an important LLM capability and should be taken into consideration in evaluation.\nBase models are known to have weaker capability of instruction following due to lack of fine-tuning.\nTo reduce the variance of black-box evaluation on base models, we use 1-shot setting for most tasks.\nWith just 1-shot example of question and answer, we observe that stronger base models are able to\nperform in-context learning to follow the required output format of multiple-choice questions. Due\nto the limited space, refer to Appendix C for details of sampling parameters, answer parsing method\nand metric computation for each benchmark. For the sampling variance under black-box evaluation,\nrefer to Section 3.2 for our extensive experiments and detailed discussions.\n3\nEXPERIMENTS\n3.1\nOVERALL PERFORMANCE\nTable 1 summarizes the main evaluation results of GPT-Fathom. For PaLM 2-L, since its API access\nis not currently available yet, we instead cite the numbers from PaLM 2 (Anil et al., 2023). By\naveraging the benchmark scores of each capability category, Figure 2 plots radar charts to visualize\nthe capabilities of evaluated LLMs. Table 2 compares the performance of Claude 2 and OpenAI\u2019s\nlatest models. We\u2019re still on the waitlist of Claude 2\u2019s API access, so we evaluate OpenAI\u2019s latest\nmodels (including Web-version GPT-3.5 and GPT-4) under the same settings used by Claude 24.\nFrom the overall performance of OpenAI\u2019s models, we observe a remarkable leap from GPT-3 to\nGPT-4 across all facets of capabilities, with the GPT-3.5 series serving as a pivotal intermediary\nstage, which was kicked off by code-davinci-002, a fairly strong base model pretrained on a\n8For TruthfulQA, we evaluate in the multiple-choice setting.\n5\nTable 1: Main evaluation results of GPT-Fathom. Note that GPT-Fathom supports various settings\nfor evaluation. For simplicity, we pick one commonly used setting for each benchmark and report\nLLMs\u2019 performance under this aligned setting. We use the Exact Match (EM) accuracy in percentage\nas the default metric, except when otherwise indicated. For clarity, we also report the number of\n\u201cshots\u201d used in prompts and whether Chain-of-Thought (CoT; Wei et al. 2022) prompting is used.\nFor the AGIEval (Zhong et al., 2023) benchmark, we use the official few-shot (3-5 shots) setting.\nFor PaLM 2-L, since its API access is not currently available yet, we instead cite the numbers from\nPaLM 2 (Anil et al., 2023). Numbers that are not from our own experiments are shown in brackets.\nNumbers with \u22c6 are obtained from optimized prompts, which is discussed in Section 3.2.\nCapability Category\nBenchmark\nSetting\nLLaMA-\n65B\nLlama 2-\n70B\nPaLM 2-\nL\ndavinci\n(GPT-3)\ndavinci-\ninstruct-beta\n(InstructGPT)\ntext-\ndavinci-\n001\ncode-\ndavinci-\n002\ntext-\ndavinci-\n002\ntext-\ndavinci-\n003\ngpt-3.5-\nturbo-\n0301\ngpt-3.5-\nturbo-\n0613\ngpt-3.5-\nturbo-\ninstruct-\n0914\ngpt-3.5-\nturbo-\n1106\ngpt-4-\n0314\ngpt-4-\n0613\ngpt-4-\n1106-\npreview\nKnowledge\nQuestion Answering\nNatural Questions\n1-shot\n27.7\n27.0\n(37.5)\n17.8\n7.1\n23.5\n29.2\n28.2\n38.1\n39.6\n38.8\n44.4\n37.2\n48.4\n48.6\n49.6\nWebQuestions\n1-shot\n42.2\n38.2\n(28.2)\n37.3\n11.1\n42.1\n43.3\n45.8\n55.4\n53.0\n53.4\n58.2\n50.2\n60.3\n58.6\n61.5\nTriviaQA\n1-shot\n73.4\n74.0\u22c6\n(86.1)\n61.5\n51.6\n68.0\n82.6\n78.6\n82.5\n83.2\n84.9\n87.2\n84.0\n92.3\n92.1\n92.6\nMulti-subject Test\nMMLU\n5-shot\n60.1\u22c6\n67.8\u22c6\n(78.3)\n34.3\n39.9\n46.7\n69.1\n62.1\n63.7\n66.6\n67.4\n69.6\n61.9\n83.7\n81.3\n78.3\nAGIEval-EN\nfew-shot\n38.0\n44.0\n\u2013\n22.0\n25.1\n31.0\n48.4\n43.6\n44.3\n43.3\n44.5\n47.6\n43.1\n57.1\n56.7\n48.2\nARC-e\n1-shot\n87.2\n93.4\n(89.7)\n57.2\n60.6\n74.7\n92.8\n90.1\n91.5\n94.1\n92.7\n94.3\n89.2\n98.9\n98.6\n98.1\nARC-c\n1-shot\n71.8\n79.6\n(69.2)\n35.9\n40.9\n53.2\n81.7\n75.7\n79.5\n82.9\n81.7\n83.6\n79.1\n94.9\n94.6\n94.2\nReasoning\nCommonsense Reasoning\nLAMBADA\n1-shot\n30.9\n30.4\n(86.9)\n53.6\n13.8\n51.1\n84.9\n66.0\n56.2\n67.8\n68.2\n67.6\n61.2\n78.6\n87.8\n79.9\nHellaSwag\n1-shot\n47.8\n68.4\n(86.8)\n22.8\n18.9\n34.6\n56.4\n64.9\n60.4\n78.9\n79.4\n82.8\n60.8\n92.4\n91.9\n92.7\nWinoGrande\n1-shot\n54.6\n69.8\n(83.0)\n48.0\n49.6\n54.6\n67.6\n65.5\n70.6\n65.8\n55.3\n68.0\n54.0\n86.7\n87.1\n81.8\nComprehensive Reasoning\nBBH\n3-shot CoT\n58.2\n65.0\n(78.1)\n39.1\n38.1\n38.6\n71.6\n66.0\n69.0\n63.8\n68.1\n66.8\n35.2\n84.9\n84.6\n79.8\nComprehension\nReading Comprehension\nRACE-m\n1-shot\n77.0\n87.6\n(77.0)\n37.0\n43.0\n54.4\n87.7\n84.5\n86.3\n86.0\n84.1\n87.2\n78.3\n93.5\n94.0\n93.4\nRACE-h\n1-shot\n73.0\n85.1\n(62.3)\n35.0\n33.5\n44.3\n82.3\n80.5\n79.5\n81.4\n81.2\n82.6\n77.0\n91.8\n90.8\n89.7\nDROP\n3-shot, F1\n10.0\n12.1\n(85.0)\n2.5\n8.6\n33.1\n10.7\n47.7\n56.4\n39.1\n53.4\n59.1\n33.2\n78.9\n74.4\n45.3\nMath\nMathematical Reasoning\nGSM8K\n8-shot CoT\n53.6\n56.4\n(80.7)\n12.1\n10.8\n15.6\n60.2\n47.3\n59.4\n78.2\n76.3\n75.8\n73.8\n92.1\n92.1\n89.8\nMATH\n4-shot CoT\n2.6\n3.7\n(34.3)\n0.0\n0.0\n0.0\n10.2\n8.5\n15.6\n33.4\n20.4\n32.2\n20.9\n38.6\n35.7\n25.3\nCoding\nCoding Problems\nHumanEval\n0-shot, pass@1\n10.7\n12.7\n\u2013\n0.0\n0.1\n0.6\n24.2\n29.3\n57.6\n53.9\n80.0\n61.2\n61.4\n66.3\n66.4\n84.6\nMBPP\n3-shot, pass@1\n44.8\n58.0\n\u2013\n4.6\n7.6\n11.9\n67.3\n70.2\n77.0\n82.3\n98.0\n80.4\n78.5\n85.5\n85.7\n86.3\nMultilingual\nMulti-subject Test\nAGIEval-ZH\nfew-shot\n31.7\n37.9\n\u2013\n23.6\n23.9\n28.0\n41.4\n38.6\n39.3\n41.9\n38.4\n44.4\n30.7\n56.5\n56.7\n53.4\nC-Eval\n5-shot\n10.7\n38.0\n\u2013\n5.5\n1.6\n20.7\n50.3\n44.5\n49.7\n51.8\n48.5\n54.2\n39.2\n69.2\n69.1\n65.1\nMathematical Reasoning\nMGSM\n8-shot CoT\n3.6\n4.0\n(72.2)\n2.4\n5.1\n7.4\n7.9\n22.9\n33.7\n53.5\n53.7\n48.8\n54.3\n82.2\n68.7\n56.1\nQuestion Answering\nTyDi QA\n1-shot, F1\n12.1\n18.8\n(40.3)\n5.7\n3.7\n9.3\n14.3\n12.5\n16.3\n21.2\n25.1\n25.4\n17.3\n31.3\n31.2\n29.9\nSafety\nTruthfulness\nTruthfulQA\n1-shot\n51.0\n59.4\n\u2013\n21.4\n5.4\n21.7\n54.2\n47.8\n52.2\n57.4\n61.4\n59.4\n60.7\n79.5\n79.7\n75.7\nToxicity\nRealToxicityPrompts \u2193\n0-shot\n14.8\n15.0\n\u2013\n15.6\n16.1\n14.1\n15.0\n15.0\n9.6\n8.0\n7.7\n12.9\n8.5\n7.9\n7.9\n6.8\n             Knowledge\n            Reasoning\nComprehension\nMath    \nCoding       \nMultilingual\n      Safety\ndavinci (GPT-3)\ndavinci-instruct-beta (InstructGPT)\ntext-davinci-001\n(a) GPT-3 Series\n             Knowledge\n            Reasoning\nComprehension\nMath    \nCoding       \nMultilingual\n      Safety\ncode-davinci-002\ntext-davinci-002\ntext-davinci-003\ngpt-3.5-turbo-0301\ngpt-3.5-turbo-instruct-0914\ngpt-4-0314\n(b) GPT-3.5 Series and GPT-4\n             Knowledge\n            Reasoning\nComprehension\nMath    \nCoding       \nMultilingual\n      Safety\ngpt-3.5-turbo-0301\ngpt-3.5-turbo-0613\ngpt-4-0314\ngpt-4-0613\nLLaMA-65B\nLlama 2-70B\n(c) Llama 2-70B\nFigure 2: Radar charts to visualize the capabilities of evaluated LLMs. We exclude PaLM 2-L and\nClaude 2 due to the missing of reported performance on some benchmarks.\nhybrid of text and code data. In the following section, we conduct detailed analysis on the progres-\nsive performance of OpenAI\u2019 models, as well as the performance of other leading closed-source /\nopen-source LLMs. Our study aims to unveil OpenAI\u2019s mysterious path from GPT-3 to GPT-4, and\nshed light on many community-concerned questions.\n3.2\nANALYSIS AND INSIGHTS\nCaveats\n\u2022 The analysis below is based on our benchmarking results and publicly available in-\nformation of the evaluated LLMs.\n\u2022 As claimed in OpenAI\u2019s model index3, their models generally used the best available\ndatasets at the time of training. As a result, our analysis on OpenAI\u2019s models may\nnot serve as a rigorous ablation study.\nOpenAI vs. non-OpenAI LLMs. The overall performance of GPT-4, which is OpenAI\u2019s lead-\ning model, is crushing the competitors on most benchmarks. As reported in Table 1, PaLM 2-L\nclearly outperforms gpt-3.5-turbo-0613 on \u201cReasoning\u201d and \u201cMath\u201d tasks, but still falls be-\n6\nTable 2: Performance of Claude 2 and OpenAI\u2019s latest models under aligned settings. Note that the\nWeb-version models (evaluated in 2023/09) could be updated at anytime and may not have the same\nbehavior as the dated API-based models.\nCapability Category\nBenchmark\nSetting\nClaude 2\ngpt-3.5-\nturbo-\n0613\nWeb-version\nGPT-3.5\ngpt-4-\n0613\nWeb-version\nGPT-4\nWeb-version\nGPT-4\nAdvanced Data Analysis\n(Code Interpreter)\nKnowledge\nQuestion Answering\nTriviaQA\n5-shot\n(87.5)\n80.6\n80.5\n92.7\n90.8\n88.8\nMulti-subject Test\nMMLU\n5-shot CoT\n(78.5)\n67.1\n61.8\n82.7\n80.0\n81.5\nARC-c\n5-shot\n(91.0)\n84.1\n79.6\n94.9\n94.4\n95.1\nComprehension\nReading Comprehension\nRACE-h\n5-shot\n(88.3)\n82.3\n80.0\n92.0\n90.0\n90.8\nMath\nMathematical Reasoning\nGSM8K\n0-shot CoT\n(88.0)\n60.2\n61.3\n83.9\n79.8\n72.0\nCoding\nCoding Problems\nHumanEval\n0-shot, pass@1\n(71.2)\n80.0\n69.6\n66.4\n84.8\n85.2\nhind gpt-4-0613 on all capability categories except for \u201cMultilingual\u201d. As described in Anil\net al. (2023), PaLM 2 is pretrained on multilingual data across hundreds of languages, confirming\nthe remarkable multilingual performance achieved by PaLM 2-L that beats GPT-4.\nTable 2 indicates that Claude 2 indeed stands as the leading non-OpenAI model. Compared to\ngpt-4-0613 (up-to-date stable API version of GPT-4), Claude 2 achieves slightly worse perfor-\nmance on \u201cKnowledge\u201d and \u201cComprehension\u201d tasks, but slightly better performance on \u201cMath\u201d and\n\u201cCoding\u201d tasks. Noticeably, the upgraded gpt-3.5-turbo-0613 has significantly improved on\ncoding benchmarks compared to its predecessor gpt-3.5-turbo-0301 with striking pass@1\nscores: 80.0 on HumanEval and 98.0 on MBPP. Although such improvement have yet to manifest in\ngpt-4-0613, we observe a similar leap of coding benchmark scores on the Web-version GPT-4.\nClosed-source vs. open-source LLMs. LLaMA (Touvron et al., 2023a) and Llama 2 (Touvron\net al., 2023b) have been widely recognized as the most powerful open-source LLMs, which largely\nfacilitate the open-source community to develop advanced LLMs. Following their official perfor-\nmance report of base models, we pick the largest variants of their base models (LLaMA-65B and\nLlama 2-70B) as the leading open-source LLMs for evaluation. Compared to LLaMA, Llama 2\nis trained on 40% more pretraining data with doubled context length (Touvron et al., 2023b). As\nexpected, Llama 2-70B outperforms LLaMA-65B on most benchmarks, especially on \u201cReasoning\u201d\nand \u201cComprehension\u201d tasks. The radar chart in Figure 2c highlights the capability distribution of\nLlama 2-70B, which achieves similar performance on \u201cSafety\u201d against gpt-3.5-turbo-0613,\nbut still clearly underperforms on the other dimensions, especially \u201cMath\u201d, \u201cCoding\u201d and \u201cMulti-\nlingual\u201d. We strongly encourage the open-source community to improve these capabilities of open-\nsource LLMs.\nOpenAI API-based vs. Web-version LLMs. According to OpenAI\u2019s blog9, the dated API models\n(such as gpt-4-0613) are pinned to unchanged models, while the Web-version models are subject\nto model upgrades at anytime and may not have the same behavior as the dated API-based models.\nWe then compare the performance of OpenAI API-based and Web-version models in Table 2. We\nobserve that the dated API models gpt-3.5-turbo-0613 and gpt-4-0613, consistently per-\nform slightly better than their front-end counterparts, i.e., Web-version GPT-3.5 (serving ChatGPT)\nand Web-version GPT-4. Noticeably, the latest GPT-4 Advanced Data Analysis (previously known\nas Code Interpreter) has significantly improved the coding benchmark performance, which achieves\na striking 85.2 pass@1 score on HumanEval.\nSeesaw phenomenon of LLM capabilities. By comparing the performance of OpenAI API models\ndated in 2023/03 and 2023/06, we note the presence of a so-called \u201cseesaw phenomenon\u201d, where\ncertain capabilities exhibit improvement, while a few other capabilities clearly regress. As reported\nin Table 1, we observe that gpt-3.5-turbo-0613 significantly improves on coding benchmarks\ncompared to gpt-3.5-turbo-0301, but its score on MATH dramatically degrades from 32.0 to\n15.0. GPT-4 also shows similar phenomenon, where gpt-4-0314 achieves 78.6 on LAMBADA\nand gpt-4-0613 boosts its performance to a remarkable 87.8, but its score on MGSM plummets\nfrom 82.2 to 68.7. OpenAI also admits9 that when they release a new model, while the majority of\nmetrics have improved, there may be some tasks where the performance gets worse. The seesaw\n9https://openai.com/blog/function-calling-and-other-api-updates\n7\nTable 3: Breakdown of coding performance with temperature T = 0.8 and topp = 1.0.\nBenchmark\nSetting\ncode-\ncushman-\n001\n(Codex-12B)\ncode-\ndavinci-\n002\n(base model)\ntext-\ndavinci-\n002\n(+SFT)\ntext-\ndavinci-\n003\n(+PPO)\ngpt-3.5-\nturbo-\n0301\ngpt-4-\n0314\nHumanEval\n0-shot, pass@1\n21.2\n24.2\n29.3\n57.6\n53.9\n66.3\n0-shot, pass@10\n52.8\n68.9\n71.9\n81.3\n72.2\n79.6\n0-shot, pass@100\n79.3\n91.5\n89.0\n89.6\n78.7\n82.9\nMBPP\n3-shot, pass@1\n50.2\n67.3\n70.2\n77.0\n82.3\n85.5\n3-shot, pass@80\n94.8\n97.5\n95.7\n96.1\n95.3\n95.3\nphenomenon of LLM capabilities is likely a universal challenge, not exclusive to OpenAI\u2019s models.\nThis challenge may obstruct LLM\u2019s path towards AGI, which necessitates a model that excels across\nall types of tasks. Therefore, we invite the research community to dedicate more efforts to tackling\nthe seesaw phenomenon of LLM capabilities.\nImpacts of pretraining with code data. Codex-12B (Chen et al., 2021) represents OpenAI\u2019s pre-\nliminary effort to train LLMs on code data. Despite its modest model size, Codex-12B demonstrates\nnotable performance on coding problems. Following this initial attempt, OpenAI trains a brand new\nbase model code-davinci-002 on a mixture of text and code data, which kicks off the new\ngeneration of GPT models, namely the GPT-3.5 Series. As reported in Table 1, the performance of\ncode-davinci-002 surges on all capability categories, compared to the GPT-3 Series, which\nis also visualized in Figure 2a and 2b. On some reasoning tasks such as LAMBADA and BBH,\ncode-davinci-002 shows fairly strong performance that even beats gpt-3.5-turbo-0301\nand gpt-3.5-turbo-0613. This suggests that incorporating code data into LLM pretraining\ncould universally elevate its potential, particularly in the capability of reasoning.\nImpacts of SFT and RLHF. InstructGPT (Ouyang et al., 2022) demonstrates the effectiveness of\nsupervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) approaches\nto aligning language models, which can largely improve the win rate of head-to-head human eval-\nuation. By applying SFT and its variant FeedME (as explained by OpenAI3, FeedME means SFT\non human-written demonstrations and on model samples rated 7/7 by human labelers on an over-\nall quality score) to GPT-3 base model davinci, the obtained model text-davinci-001\nsignificantly improves on most benchmarks, as illustrated in Figure 2a. However, when the base\nmodel becomes stronger, we notice the opposite effect: text-davinci-002 performs slightly\nworse than code-davinci-002 on most benchmarks, except on coding benchmarks. This phe-\nnomenon can also be observed on open-source models: SFT boosts the performance of LLaMA-65B\non MMLU (Touvron et al., 2023a), while all SFT models within the extensive Llama2-70B family on\nthe Open LLM Leaderboard (Beeching et al., 2023) show only marginal improvements on MMLU.\nThis implies that SFT yields more benefits for weaker base models, while for stronger base models,\nit offers diminishing returns or even incurs an alignment tax on benchmark performance.\nOn top of the SFT model text-davinci-002, by applying RLHF with PPO algorithm (Schul-\nman et al., 2017), the obtained model text-davinci-003 has comparable or slightly worse\nperformance on most benchmarks compared to the strong base model code-davinci-002, ex-\ncept for coding benchmarks. To better understand the impacts of SFT and RLHF, we further break\ndown the performance on coding benchmarks in Table 3. Intriguingly, while SFT and RLHF models\nexcel in the pass@1 metric, they slightly underperform in pass@100. We interpret these results\nas follows: 1) A larger k in the pass@k metric, such as pass@100, gauges the intrinsic ability to\nsolve a coding problem, while pass@1 emphasizes the capability for one-take bug-free coding; 2)\nSFT and RLHF models still have to pay the alignment tax, exhibiting a minor performance drop in\npass@100. This trend aligns with their slightly worse performance across other tasks; 3) SFT and\nRLHF can effectively distill the capability of pass@100 into pass@1, signifying a transfer from in-\nherent problem-solving skills to one-take bug-free coding capability; 4) While smaller models, such\nas code-cushman-001 (Codex-12B) and gpt-3.5-turbo-0301, display limited intrinsic\ncapability in terms of pass@100, their pass@1 scores can be dramatically improved by SFT and\nRLHF. This is good news for research on low-cost small-size LLMs.\n8\nTable 4: Ablation study on number of \u201cshots\u201d.\nBenchmark\nSetting\ncode-\ndavinci-\n002\ntext-\ndavinci-\n002\ntext-\ndavinci-\n003\ngpt-3.5-\nturbo-\n0301\ngpt-4-\n0314\nMMLU\n3-shot\n67.9\n62.9\n65.2\n65.8\n82.0\n5-shot\n68.3\n63.5\n65.4\n66.6\n83.7\nARC-c\n0-shot\n78.0\n72.4\n75.8\n81.4\n93.7\n1-shot\n81.7\n75.7\n79.5\n82.9\n94.9\n5-shot\n84.6\n79.3\n82.3\n84.5\n94.8\n25-shot\n85.3\n79.8\n84.4\n84.5\n95.6\nHellaSwag\n0-shot\n39.2\n53.3\n40.1\n59.8\n79.4\n1-shot\n56.4\n64.9\n60.4\n78.9\n92.4\n10-shot\n73.4\n66.4\n65.3\n79.8\n92.5\nTable 5: Ablation study on CoT prompting.\nBenchmark\nSetting\ncode-\ndavinci-\n002\ntext-\ndavinci-\n002\ntext-\ndavinci-\n003\ngpt-3.5-\nturbo-\n0301\ngpt-4-\n0314\nMMLU\n5-shot\n68.3\n63.5\n65.4\n66.6\n83.7\n5-shot CoT\n62.8\n54.8\n64.2\n67.5\n82.2\nBBH\n3-shot\n52.8\n48.2\n51.7\n51.9\n70.8\n3-shot CoT\n71.6\n66.0\n69.0\n63.8\n84.9\nGSM8K\n5-shot\n18.3\n15.4\n15.9\n38.7\n46.6\n5-shot CoT\n56.3\n47.5\n57.3\n78.0\n91.6\n8-shot\n18.3\n15.4\n15.8\n39.1\n45.7\n8-shot CoT\n60.2\n47.3\n59.4\n78.2\n92.1\nTable 6: Benchmark performance with different prompt templates.\nBenchmark\nSetting\nPrompt Template\nLLaMA-\n65B\nLlama 2-\n70B\ncode-\ndavinci-\n002\ntext-\ndavinci-\n002\ntext-\ndavinci-\n003\ngpt-3.5-\nturbo-\n0301\ngpt-4-\n0314\nTriviaQA\n1-shot\n<q1>\\nAnswer: <a1>\\n<q>\\nAnswer:\n75.4\n74.0\n82.9\n77.6\n81.6\n77.8\n92.0\nQ: <q1>\\nA: <a1>\\nQ: <q>\\nA:\n73.4\n55.5\n82.6\n78.6\n82.5\n83.2\n92.3\nMMLU\n5-shot\n<q1>\\nAnswer: <a1>\\n . . . <q5>\\nAnswer: <a5>\\n<q>\\nAnswer:\n60.1\n67.8\n68.3\n64.5\n65.3\n67.7\n82.0\nQ: <q1>\\nA: <a1>\\n . . . Q: <q5>\\nA: <a5>\\nQ: <q>\\nA:\n55.7\n64.8\n68.3\n63.5\n65.4\n66.6\n83.7\nBased on the observations above and recognizing that the state-of-the-art LLMs can inherently tackle\ncomplicated tasks (albeit possibly succeed after many sampling trials), we anticipate that LLMs have\nyet to reach their full potential. This is because techniques like SFT and RLHF can consistently\nenhance their performance with significantly reduced sampling budget, translating their intrinsic\ncapabilities into higher and higher one-take pass rates on reasoning-intensive tasks.\nImpacts of the number of \u201cshots\u201d. To explore the influence of the number of \u201cshots\u201d (in-context\nlearning examples) on LLM benchmark performance, we carry out an ablation study, with the results\nsummarized in Table 4. As expected, performance generally improves with an increased number of\n\u201cshots\u201d, however, the improvement rate quickly shrinks beyond 1-shot in-context examples, par-\nticularly for stronger models. For instance, gpt-4-0314 achieves 94.9 on ARC-c with 1-shot\nexample, and only marginally increases to 95.6 with 25-shot examples. This indicates that 1-shot\nexample typically works well for most tasks, which aligns with our primary evaluation setting.\nImpacts of CoT prompting. We further explore the impact of using Chain-of-Thought (CoT; Wei\net al. 2022) prompting on LLM benchmark performance. As illustrated in Table 5, the influence of\nCoT prompting varies across benchmarks. On tasks that are knowledge-intensive, like MMLU, CoT\nhas minimal or even slightly negative impact on performance. However, for reasoning-intensive\ntasks, such as BBH and GSM8K, CoT prompting markedly enhances LLM performance. For in-\nstance, on the GSM8K with 8-shot examples, gpt-4-0314 elevates its score from 45.7 to an\nimpressive 92.1 when CoT prompting is employed.\nPrompt sensitivity. Many existing works neglect the impacts of prompt sensitivity on the overall us-\nability of LLMs. For advanced LLMs, it is unacceptable that a minor alteration of the prompt (with-\nout changing the inherent meaning) could cause the LLM to fail in solving the problem. Many exist-\ning LLM leaderboards reference scores from other papers without consistent settings and prompts,\nwhich may inadvertently encourage cherry-picking favored settings and prompts for better results.\nIn contrast, we primarily present our own evaluation results under aligned settings and prompts in\nTable 1 and 2, and highlight exceptions where numbers are either sourced from other papers (with\nbrackets) or obtained from optimized prompts (with stars). To figure out the influence of switch-\ning prompt templates on the benchmark performance of LLMs, we conduct experiments and report\nthe results in Table 6. We observe that open-source models LLaMA-65B and Llama 2-70B exhibit\ngreater prompt sensitivity. For instance, a slight change of the prompt template results in the score of\nLlama 2-70B on TriviaQA plummeting from 74.0 to 55.5. We urge the community to place greater\nemphasis on the prompt-sensitive issue and strive to enhance the robustness of LLMs.\nSampling variance. The decoding process of LLMs is repeatedly sampling the next token from\nthe LLM output distribution. Various hyperparameters, including the temperature T and the nucleus\nsampling (Holtzman et al., 2020) parameter topp, can be adjusted to modify the sampling behavior.\nIn our evaluations, we set topp = 1.0 and T = 0 on nearly all tasks, with the exception of coding\n9\nbenchmarks where T = 0.8. We further investigate the sampling variance of evaluation results,\nexamining the effects of the sampling hyperparameters. Due to the limited space, in Appendix D,\nwe report the mean and stand deviation of benchmark scores over 3 runs with different settings\nof T and topp. As expected, a higher temperature T introduces greater variance in benchmark\nscores, since the output becomes less deterministic. Notably, LLMs (especially base models) tend to\nunderperform with a higher temperature T. On coding benchmarks, although a higher temperature T\nstill hurts the pass@1 metric, it boosts the pass@100 metric due to higher coverage of the decoding\nspace with more randomness. As for topp, our results indicate that it has marginal influence on the\nperformance of fine-tuned LLMs. Similarly, a notable exception is observed on coding benchmarks,\nwhere a higher topp diminishes the pass@1 metric but largely enhances the pass@100 metric.\n4\nCONCLUSIONS AND FUTURE WORK\nIn this work, we present GPT-Fathom, an open-source and reproducible evaluation suite that com-\nprehensively measures the multi-dimensional capabilities of LLMs under aligned settings. Our ret-\nrospective study on OpenAI\u2019s models helps the community better understand the evolutionary path\nfrom GPT-3 to GPT-4, and sheds light on many questions that the community is eager to explore,\nsuch as the gap between leading closed-source / open-source LLMs, the benefits of pretraining with\ncode data, the impacts of SFT and RLHF, etc. Moreover, we identify novel challenges of advanced\nLLMs, such as prompt sensitivity and the seesaw phenomenon of LLM capabilities.\nIn the future, we plan to further extend GPT-Fathom by 1) adding additional evaluation benchmarks\nunder existing capability categories; 2) supporting more capability aspects, such as long-context\nunderstanding, multi-turn conversation, open-domain generation, LLM agent and even multi-modal\ncapability; 3) evaluating more leading LLMs, including both open-source and closed-source models.\nAcknowledgments. The authors would like to thank Yao Fu for the suggestions on benchmark\nselection. We also thank Ke Shen, Kai Hua, Yang Liu and Guang Yang for technical discussions. We\ngratefully acknowledge the funding support and feedback from Liang Xiang. This work was made\npossible by all the benchmarks used for evaluation. We appreciate the creators of these benchmarks.\nREFERENCES\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, and et al. PaLM 2 technical report,\n2023. URL https://arxiv.org/abs/2305.10403.\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton.\nPro-\ngram synthesis with large language models.\nCoRR, abs/2108.07732, 2021.\nURL https:\n//arxiv.org/abs/2108.07732.\nEdward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Ra-\njani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open LLM leaderboard. https:\n//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 1533\u20131544, Seattle, Washington, USA, October 2013. Association for\nComputational Linguistics. URL https://aclanthology.org/D13-1160.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\nral Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc.,\n10\n2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith gpt-4, 2023. URL https://arxiv.org/abs/2303.12712.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond\u00e9 de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, and et al. Evaluating large language models trained on code. CoRR, abs/2107.03374,\n2021. URL https://arxiv.org/abs/2107.03374.\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. InstructEval: Towards holistic\nevaluation of instruction-tuned large language models, 2023. URL https://arxiv.org/\nabs/2306.04757.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, and et al. Palm: Scaling language mod-\neling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,\nand Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the Association for Computational Linguistics,\n8:454\u2013470, 2020. doi: 10.1162/tacl_a_00317. URL https://aclanthology.org/2020.\ntacl-1.30.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning chal-\nlenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL\nhttps://arxiv.org/abs/2110.14168.\nOpenCompass Contributors. OpenCompass: A universal evaluation platform for foundation models.\nhttps://github.com/InternLM/OpenCompass, 2023.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n2368\u20132378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter\nWest, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xi-\nang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers\non compositionality, 2023. URL https://arxiv.org/abs/2305.18654.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-Thought Hub: A\ncontinuous effort to measure large language models\u2019 reasoning performance, 2023. URL https:\n//arxiv.org/abs/2305.17306.\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxici-\ntyPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020, pp. 3356\u20133369, Online, November 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL\nhttps://aclanthology.org/2020.findings-emnlp.301.\n11\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-\ncob Steinhardt. Measuring massive multitask language understanding. In International Confer-\nence on Learning Representations, 2021a. URL https://openreview.net/forum?id=\nd7KBjmI3GmQ.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. CoRR,\nabs/2103.03874, 2021b. URL https://arxiv.org/abs/2103.03874.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=rygGQyrFvH.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He.\nC-Eval:\nA multi-level multi-discipline Chinese evaluation suite for foundation models. arXiv preprint\narXiv:2305.08322, 2023. URL https://arxiv.org/abs/2305.08322.\nShima Imani, Liang Du, and Harsh Shrivastava.\nMathPrompter: Mathematical reasoning us-\ning large language models.\nIn Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 5: Industry Track), pp. 37\u201342, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-industry.4. URL\nhttps://aclanthology.org/2023.acl-industry.4.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n1601\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\ndoi:\n10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: A benchmark for question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl_a_00276. URL\nhttps://aclanthology.org/Q19-1026.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.\nRACE: Large-scale\nReAding comprehension dataset from examinations.\nIn Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing, pp. 785\u2013794, Copenhagen, Denmark,\nSeptember 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL\nhttps://aclanthology.org/D17-1082.\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo,\nYuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.\nSolving quantitative rea-\nsoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=IFXTZERXdM7.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instruction-following\nmodels. https://github.com/tatsu-lab/alpaca_eval, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, and\net al. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023.\nISSN 2835-8856. URL https://openreview.net/forum?id=iO4LZibEqW. Featured\nCertification, Expert Certification.\n12\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu-\nman falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 3214\u20133252, Dublin, Ireland, May 2022. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https:\n//aclanthology.org/2022.acl-long.229.\nOpenAI. GPT-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike,\nand Ryan Lowe.\nTraining language models to follow instructions with human feedback.\nIn\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neu-\nral Information Processing Systems, 2022. URL https://openreview.net/forum?id=\nTG8KACxEON.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez.\nThe LAMBADA\ndataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n1525\u20131534, Berlin, Germany, August 2016. Association for Computational Linguistics.\ndoi:\n10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An ad-\nversarial winograd schema challenge at scale. Commun. ACM, 64(9):99\u2013106, aug 2021. ISSN\n0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/\n1707.06347.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi,\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Lan-\nguage models are multilingual chain-of-thought reasoners. In The Eleventh International Confer-\nence on Learning Representations, 2023. URL https://openreview.net/forum?id=\nfR3wGCk-IXp.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, and et al. Be-\nyond the imitation game: Quantifying and extrapolating the capabilities of language mod-\nels.\nTransactions on Machine Learning Research, 2023.\nISSN 2835-8856.\nURL https:\n//openreview.net/forum?id=uyTL5Bvosj.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei.\nChallenging BIG-\nbench tasks and whether chain-of-thought can solve them. In Findings of the Association for\nComputational Linguistics: ACL 2023, pp. 13003\u201313051, Toronto, Canada, July 2023. Associ-\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https:\n//aclanthology.org/2023.findings-acl.824.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation\nlanguage models, 2023a. URL https://arxiv.org/abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, and et al. Llama 2: Open foundation and fine-tuned chat models,\n2023b. URL https://arxiv.org/abs/2307.09288.\n13\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-\nvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n2017.\nURL https://proceedings.neurips.cc/paper_files/paper/2017/\nfile/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Ad-\nvances in Neural Information Processing Systems, 2022. URL https://openreview.net/\nforum?id=_VjQlMeSB_J.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. Associ-\nation for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.6.\nURL https:\n//aclanthology.org/2020.emnlp-demos.6.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4791\u20134800, Florence, Italy, July 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/\nP19-1472.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging LLM-as-a-judge with MT-Bench and Chatbot Arena, 2023. URL https://arxiv.\norg/abs/2306.05685.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation\nmodels, 2023. URL https://arxiv.org/abs/2304.06364.\n14\nAPPENDIX\nA\nDETAILS OF EVALUATED LLMS\nThe LLMs selected for evaluation are organized as follows.\n1. OpenAI\u2019s models (illustrated in Figure 1):\n\u2022 GPT-3 Series: 1) davinci (GPT-3; Brown et al. 2020), the first GPT model ever with over\n100B parameters; 2) davinci-instruct-beta (InstructGPT SFT; Ouyang et al. 2022),\na supervised fine-tuned (SFT) model on top of GPT-3; 3) text-davinci-001, a more\nadvanced SFT model with the FeedME technique (as explained by OpenAI3, FeedME means\nSFT on human-written demonstrations and on model samples rated 7/7 by human labelers on\nan overall quality score); 4) code-cushman-001 (Codex-12B; Chen et al. 2021), a smaller\nexperimental model specifically fine-tuned on code data.\n\u2022 GPT-3.5 Series: 1) code-davinci-002, a base model pretrained on a mixture of text\nand code data; 2) text-davinci-002, a SFT model with the FeedME technique on top\nof code-davinci-002; 3) text-davinci-003, a refined model using PPO (Schul-\nman et al., 2017) on top of text-davinci-002; 4) gpt-3.5-turbo-0301, a chat-\noptimized model on top of text-davinci-003; 5) gpt-3.5-turbo-0613, an up-\ndated API version in lieu of gpt-3.5-turbo-0301; 6) Web-version GPT-3.5, which\nis currently (at the time of writing in 2023/09) serving ChatGPT on OpenAI\u2019s website; 7)\ngpt-3.5-turbo-instruct-0914, a completion model trained similarly to the previ-\nous InstructGPT models such as the text-davinci series, while maintaining the same\nspeed and pricing as the gpt-3.5-turbo models10; 8) gpt-3.5-turbo-1106, an up-\ndated API version in lieu of gpt-3.5-turbo-0613.\n\u2022 GPT-4: 1) gpt-4-0314, the initial API version of GPT-4, which is a new GPT gen-\neration with striking performance improvements over GPT-3.5; 2) gpt-4-0613, an up-\ndated API version in lieu of gpt-4-0314; 3) Web-version GPT-4, which is currently\n(at the time of writing in 2023/09) serving GPT-4 on OpenAI\u2019s website; 4) Web version\nGPT-4 Advanced Data Analysis (Code Interpreter), a recently upgraded Web-version GPT-\n4 with functionalities of advanced data analysis and sandboxed Python code interpreter; 5)\ngpt-4-1106-preview, an early-access API of the upgraded model GPT-4 Turbo11.\n2. Other leading closed-source models:\n\u2022 PaLM 2 (Anil et al., 2023): released by Google in 2023/05, which is a set of strong LLMs with\nhuge improvements over its predecessor PaLM (Chowdhery et al., 2022). For fair comparison,\nwe plan to evaluate the largest model in the PaLM 2 family, which is PaLM 2-L. However,\nsince its API access is not currently available yet, we instead evaluate other models under the\nsame settings of PaLM 2-L and cite the reported performance.\n\u2022 Claude 2: released by Anthropic in 2023/07, which is currently commonly recognized as the\nmost competitive LLM against OpenAI\u2019s leading models. We\u2019re still on the waitlist of its API\naccess, so we evaluate OpenAI\u2019s latest models under the same settings of Claude 2 and cite\nthe reported performance.\n3. Leading open-source models:\n\u2022 LLaMA (Touvron et al., 2023a): released by Meta in 2023/02, which is a set of powerful\nopen-source LLMs with different model sizes. We evaluate LLaMA-65B, the largest variant\nof its base model.\n\u2022 Llama 2 (Touvron et al., 2023b): released by Meta in 2023/07, which is the upgraded version\nof LLaMA. We evaluate the largest variant of its base model, which is Llama 2-70B.\n10https://platform.openai.com/docs/models/gpt-3-5\n11https://openai.com/blog/new-models-and-developer-products-announced-at-devday\n15\nB\nDETAILS OF BENCHMARK DATASETS\nIn Table 7, we clarify the source of few-shot prompts and test samples for each benchmark.\nTable 7: Source of few-shot samples and test samples in our evaluations.\nBenchmark\nSource of few-shot samples\nSource of test samples\nNatural Questions\nsampled from train split\nvalidation split\nWebQuestions\nsampled from train split\ntest split\nTriviaQA\nsampled from train split\nvalidation split\nMMLU\nfew-shot samples from benchmark;\nCoT samples from Chain-of-Thought Hub (Fu et al., 2023)\ntest split\nAGIEval\nbenchmark provided\nbenchmark\nARC\nsampled from validation split\ntest split\nLAMBADA\nsampled from test split\nrest of test split\nHellaSwag\nsampled from train split\nvalidation split\nWinoGrande\nsampled from train split\nvalidation split\nBBH\nbenchmark provided\ntest split\nRACE\nsampled from validation split\ntest split\nDROP\nsampled from train split\nvalidation split\nGSM8K\nCoT samples from Chain-of-Thought Hub (Fu et al., 2023)\ntest split\nMATH\nCoT samples from Minerva (Lewkowycz et al., 2022)\ntest split\nHumanEval\nn/a\ntest split\nMBPP\nbenchmark provided\ntest split\nC-Eval\nsamples in dev split\ntest split\nMGSM\nbenchmark provided\nbenchmark\nTyDi QA\nsampled from train split\nvalidation split\nTruthfulQA\nn/a\nvalidation split\nRealToxicityPrompts\nn/a\nsampled from train split\nC\nDETAILS OF EVALUATION\nC.1\nSAMPLING HYPERPARAMETERS\nFor coding evaluations, we sample 100 responses per question with temperature T = 0.8. For all\nthe other evaluations, we use T = 0. The default topp = 1.0 is applied across all of our evaluations.\nC.2\nEVALUATION PROMPTS\nWe provide our evaluation prompts for all the benchmarks in Table 8. For few-shot settings, earlier\nLLMs with short context window may have the out-of-context issue when feeding the prompts. To\naddress this issue, we use as many \u201cshots\u201d as possible to fit in the context window of LLMs.\nC.3\nANSWER PARSING AND METRIC COMPUTATION\nIn this section, we outline the methods employed to parse the answers of the models from their\nresponses for different tasks:\nMultiple-choice questions.\nWe inspect the output for options such as (A), (B), (C), (D), etc.\nThe option corresponding to a match is determined. If no matches are found, the first character of\nthe output is chosen as the selected option.\n16\nTable 8: Evaluation prompts used for all the benchmarks.\nBenchmark\nPrompt\nNatural Questions\nPlease answer the question:\nWebQuestions\nPlease answer the question:\nTriviaQA\nFollow the given examples and answer the question:\nMMLU\nThe following are multiple choice questions (with answers) about {subtask}\nAGIEval - English MC\nFollow the given samples and answer the following multiple choice question.\nAGIEval - English IMC (Indefinite MC)\nFollow the given samples and answer the following multiple select question.\nAGIEval - English Cloze\nFollow the given samples and answer the following cloze question.\nAGIEval - Chinese MC\n\u56de\u7b54\u4e0b\u5217\u9009\u62e9\u9898\nAGIEval - Chinese IMC (Indefinite MC)\n\u56de\u7b54\u4e0b\u5217\u591a\u9009\u9898\nAGIEval - Chinese Cloze\n\u56de\u7b54\u4e0b\u5217\u586b\u7a7a\u9898\nARC\nThe following are multiple choice questions (with answers) about commonsense reasoning.\nLAMBADA\nPlease answer with the word which is most likely to follow:\nHellaSwag\nComplete the description with an appropriate ending.\nWinoGrande\nChoose the option that fill in the blank best.\nBBH\n{Use the prompt from the benchmark}\nRACE\nThe following are question (with answers) about reading comprehension.\nDROP\nThe following are question (with answers) about reading comprehension.\nGSM8K\nFollow the given examples and answer the question.\nMATH\nFollow the given examples and answer the question.\nHumanEval\nComplete the code:\nMBPP\n{Use the prompt from the benchmark}\nC-Eval\n\u4ee5\u4e0b\u662f\u4e2d\u56fd\u5173\u4e8e{task name}\u8003\u8bd5\u7684\u5355\u9879\u9009\u62e9\u9898\uff0c\u8bf7\u9009\u51fa\u5176\u4e2d\u7684\u6b63\u786e\u7b54\u6848\u3002\nMGSM\nFollow the given examples and answer the question.\nTyDi QA\nFollow the given examples and answer the question.\nTruthfulQA\nAnswer the following multiple choice questions.\nRealToxicityPrompts\nn/a\nCoding problems.\nWe evaluate LLMs on HumanEval and MBPP as the coding benchmarks. Our\nassessment leverages the code evaluation methodology implemented by Hugging Face (Wolf et al.,\n2020). This approach adheres to the evaluation framework outlined in Chen et al. (2021), which\nestimate the pass@k metric using n samples (n > k) to reduce the variance. We use n = 100 for\nall the evaluations on coding benchmarks.\nLAMBADA.\nUtilizing regular expressions, we extract the first word and compare it with the\nground truth.\nDROP.\nWe check if the model\u2019s output aligns with any of the provided candidate answers.\nTyDi QA.\nThe F1 score is employed to measure performance.\nClosed-book question answering.\nThis category encompasses Natural Questions, WebQuestions,\nand TriviaQA. We check if the model\u2019s output aligns with any of the provided candidate answers.\nMGSM.\nThe final number in the output is extracted as the model\u2019s answer.\nGSM8K.\nThe initial step is to extract the first number following the CoT prompt \u201cSo the answer\nis\u201d. If no number is identified, a regular expression is utilized to extract the final number.\nMATH.\nIn line with the official benchmark settings, we initially filter the answers to retain only\nthe last boxed element. The content within the boxed braces is then taken as the answer.\n17\nD\nDETAILS OF EXPERIMENTS\nIn Table 9 and 10, we report the mean and stand deviation of benchmark scores over 3 runs, with\ndifferent settings of T and topp.\nTable 9: Benchmark performance with different temperature T and topp = 1.0. We report the mean\nand standard deviation of scores over 3 runs under each setting.\nBenchmark\nSetting\ncode-davinci-002\ntext-davinci-003\ngpt-3.5-turbo-0301\nT = 0.0\nT = 0.5\nT = 1.0\nT = 0.0\nT = 0.5\nT = 1.0\nT = 0.0\nT = 0.5\nT = 1.0\nMMLU\n5-shot\n68.3 \u00b1 0.0\n65.8 \u00b1 0.0\n59.8 \u00b1 0.4\n65.4 \u00b1 0.0\n65.2 \u00b1 0.2\n65.1 \u00b1 0.3\n66.6 \u00b1 0.0\n68.2 \u00b1 0.1\n67.9 \u00b1 0.1\nGSM8K\n8-shot CoT\n60.2 \u00b1 0.0\n57.7 \u00b1 0.3\n31.2 \u00b1 1.5\n59.4 \u00b1 0.0\n59.9 \u00b1 1.8\n57.2 \u00b1 0.3\n78.2 \u00b1 0.0\n78.9 \u00b1 0.0\n77.5 \u00b1 0.8\nHumanEval\n0-shot, pass@1\n30.3 \u00b1 0.0\n29.4 \u00b1 0.6\n15.6 \u00b1 0.4\n60.1 \u00b1 0.0\n58.6 \u00b1 0.2\n55.3 \u00b1 0.1\n61.4 \u00b1 0.0\n57.3 \u00b1 0.1\n50.8 \u00b1 0.2\n0-shot, pass@100\n31.1 \u00b1 0.0\n88.8 \u00b1 0.9\n86.8 \u00b1 1.8\n61.6 \u00b1 0.0\n87.4 \u00b1 1.8\n92.7 \u00b1 1.2\n62.8 \u00b1 0.0\n75.2 \u00b1 0.3\n79.1 \u00b1 1.0\nTable 10: Benchmark performance with different temperature T and topp. We report the mean and\nstandard deviation of scores over 3 runs under each setting.\nBenchmark\nSetting\ntopp\ncode-davinci-002\ntext-davinci-003\ngpt-3.5-turbo-0301\nT = 0.5\nT = 1.0\nT = 0.5\nT = 1.0\nT = 0.5\nT = 1.0\nMMLU\n5-shot\n0.2\n68.3 \u00b1 0.1\n68.3 \u00b1 0.1\n65.4 \u00b1 0.1\n65.5 \u00b1 0.1\n68.4 \u00b1 0.1\n68.4 \u00b1 0.0\n0.7\n66.9 \u00b1 0.6\n65.7 \u00b1 0.5\n65.3 \u00b1 0.2\n65.4 \u00b1 0.2\n68.2 \u00b1 0.1\n68.4 \u00b1 0.2\n1.0\n65.8 \u00b1 0.0\n59.8 \u00b1 0.4\n65.2 \u00b1 0.2\n65.1 \u00b1 0.3\n68.2 \u00b1 0.1\n67.9 \u00b1 0.1\nGSM8K\n8-shot CoT\n0.2\n60.0 \u00b1 0.7\n60.4 \u00b1 0.7\n59.6 \u00b1 0.4\n59.7 \u00b1 0.5\n78.8 \u00b1 0.3\n78.6 \u00b1 0.2\n0.7\n58.9 \u00b1 1.0\n57.3 \u00b1 0.4\n59.7 \u00b1 0.5\n60.6 \u00b1 0.7\n78.9 \u00b1 0.1\n78.6 \u00b1 1.1\n1.0\n57.7 \u00b1 0.3\n31.2 \u00b1 1.5\n59.9 \u00b1 1.8\n57.2 \u00b1 0.3\n78.9 \u00b1 0.1\n77.5 \u00b1 0.8\nHumanEval\n0-shot, pass@1\n0.2\n29.2 \u00b1 0.0\n15.8 \u00b1 0.4\n58.5 \u00b1 0.3\n55.1 \u00b1 0.4\n61.4 \u00b1 0.2\n61.3 \u00b1 0.1\n0.7\n29.5 \u00b1 0.1\n15.6 \u00b1 0.2\n58.7 \u00b1 0.2\n54.9 \u00b1 0.1\n58.0 \u00b1 0.1\n57.6 \u00b1 0.2\n1.0\n29.4 \u00b1 0.6\n15.6 \u00b1 0.4\n58.6 \u00b1 0.2\n55.3 \u00b1 0.1\n57.3 \u00b1 0.1\n50.8 \u00b1 0.2\n0-shot, pass@100\n0.2\n89.4 \u00b1 0.3\n88.6 \u00b1 1.8\n85.6 \u00b1 1.3\n91.5 \u00b1 1.0\n62.8 \u00b1 0.0\n62.8 \u00b1 0.0\n0.7\n88.8 \u00b1 1.4\n89.6 \u00b1 1.6\n85.1 \u00b1 2.3\n91.1 \u00b1 0.1\n73.8 \u00b1 0.6\n74.4 \u00b1 0.6\n1.0\n88.8 \u00b1 0.9\n86.8 \u00b1 1.8\n87.4 \u00b1 1.8\n92.7 \u00b1 1.2\n75.2 \u00b1 0.3\n79.1 \u00b1 1.0\n18\nE\nCOMPLETE RESULTS OF LLAMA / LLAMA 2 FAMILY\nWe evaluate the entire LLaMA / Llama 2 family, including models ranging from 7B to 65B / 70B\nparameters, and report the complete results in Table 11.\nTable 11: Complete evaluation results of LLaMA and Llama 2 family models.\nCapability Category\nBenchmark\nSetting\nLLaMA-\n7B\nLlama 2-\n7B\nLLaMA-\n13B\nLlama 2-\n13B\nLLaMA-\n30B\nLLaMA-\n65B\nLlama 2-\n70B\nKnowledge\nQuestion Answering\nNatural Questions\n1-shot\n17.6\n19.8\n20.8\n27.6\n24.0\n27.7\n27.0\nWebQuestions\n1-shot\n37.0\n38.3\n37.6\n42.8\n39.0\n42.2\n38.2\nTriviaQA\n1-shot\n52.0\n61.1\n66.6\n70.0\n73.5\n73.4\n74.0\nMulti-subject Test\nMMLU\n5-shot\n25.1\n41.0\n38.5\n49.5\n51.0\n60.1\n67.8\nAGIEval-EN\nfew-shot\n19.1\n25.7\n27.0\n35.7\n34.7\n38.0\n44.0\nARC-e\n1-shot\n30.0\n62.3\n67.6\n76.4\n82.4\n87.2\n93.4\nARC-c\n1-shot\n26.7\n48.6\n49.1\n55.7\n60.8\n71.8\n79.6\nReasoning\nCommonsense Reasoning\nLAMBADA\n1-shot\n19.0\n38.0\n47.0\n56.4\n32.5\n30.9\n30.4\nHellaSwag\n1-shot\n24.6\n25.4\n28.9\n37.2\n31.3\n47.8\n68.4\nWinoGrande\n1-shot\n50.4\n50.2\n48.1\n52.1\n51.3\n54.6\n69.8\nComprehensive Reasoning\nBBH\n3-shot CoT\n33.7\n38.4\n39.1\n46.2\n49.6\n58.2\n65.0\nComprehension\nReading Comprehension\nRACE-m\n1-shot\n26.7\n45.8\n52.4\n57.9\n65.3\n77.0\n87.6\nRACE-h\n1-shot\n29.1\n39.5\n48.5\n55.1\n64.1\n73.0\n85.1\nDROP\n3-shot, F1\n9.6\n7.7\n8.7\n9.3\n9.8\n10.0\n12.1\nMath\nMathematical Reasoning\nGSM8K\n8-shot CoT\n13.9\n17.2\n18.4\n28.6\n35.1\n53.6\n56.4\nMATH\n4-shot CoT\n0.4\n0.1\n0.4\n0.5\n0.5\n2.6\n3.7\nCoding\nCoding Problems\nHumanEval\n0-shot, pass@1\n7.0\n14.6\n9.7\n15.8\n7.2\n10.7\n12.7\nMBPP\n3-shot, pass@1\n23.7\n39.2\n29.5\n46.0\n38.5\n44.8\n58.0\nMultilingual\nMulti-subject Test\nAGIEval-ZH\nfew-shot\n22.3\n23.4\n23.5\n29.7\n28.4\n31.7\n37.9\nC-Eval\n5-shot\n11.5\n10.3\n14.8\n28.9\n10.1\n10.7\n38.0\nMathematical Reasoning\nMGSM\n8-shot CoT\n2.7\n2.3\n2.8\n4.1\n3.1\n3.6\n4.0\nQuestion Answering\nTyDi QA\n1-shot, F1\n2.4\n3.6\n3.2\n4.5\n3.8\n12.1\n18.8\nSafety\nTruthfulness\nTruthfulQA\n1-shot\n37.6\n31.0\n29.5\n38.0\n44.5\n51.0\n59.4\nToxicity\nRealToxicityPrompts \u2193\n0-shot\n14.5\n14.8\n14.9\n14.8\n14.7\n14.8\n15.0\n             Knowledge\n            Reasoning\nComprehension\nMath    \nCoding       \nMultilingual\n      Safety\nLLaMA-7B\nLLaMA-13B\nLLaMA-30B\nLLaMA-65B\nLlama 2-7B\nLlama 2-13B\nLlama 2-70B\nFigure 3: Radar charts to visualize the capabilities of LLaMA and Llama 2 family models.\n19\nF\nOUR RESULTS VS. OFFICIAL SCORES\nTo verify the correctness of our implementation, we first compare our evaluation results with the\nofficially reported scores from GPT-4 technical report (OpenAI, 2023) and Microsoft\u2019s early exper-\niments with GPT-4 (Bubeck et al., 2023). To ensure an apple-to-apple comparison, we align the\nevaluation settings on each benchmark, as summarized in Table 12. This head-to-head comparison\ndemonstrates that our evaluation results are consistent with the official scores, within a margin of\nslight deviation. Since the official prompts and in-context examples for evaluation are not publicly\navailable, the slight deviation is totally reasonable. We also notice that the performance gain with in-\ncontext examples beyond 1-shot is pretty marginal, which aligns with our primary evaluation setting\nin Table 1.\nWe also compare our evaluation results with the official scores reported in LLaMA (Touvron et al.,\n2023a) and Llama 2 (Touvron et al., 2023b). Similarly, in Table 13, we report the benchmarks whose\nofficial evaluation settings match our settings, and compare our results with the official scores. We\nobserve that on some benchmarks, such as BBH, our results are higher than the official scores;\nwhile on some other benchmarks, such as TriviaQA and MATH, our results are lower than the\nofficial scores. This phenomenon is consistent with our conclusion that LLaMA and Llama 2 are\npretty prompt-sensitive (refer to Table 6). To be more specific, take MATH as an example, since\nwe use the exact same setting and prompt as we evaluate OpenAI models on this benchmark, and\nour evaluation result of GPT-4 matches the official scores (Table 12), we argue that the prompt\nsensitivity of LLaMA / Llama 2 models explains the performance gap of our evaluation and their\nofficial scores.\nFor coding benchmarks HumanEval and MBPP, the official LLaMA and Llama 2 papers use differ-\nent temperature T to evaluate pass@1 (T = 0.1) and pass@100 (T = 0.8). In contrast, we follow\nOpenAI\u2019s setting on coding evaluation (Chen et al., 2021) and uniformly use T = 0.8 for all our\nevaluations on coding benchmarks. This explains the performance difference of our results and the\nofficial scores of LLaMA and Llama 2 on HumanEval and MBPP.\nTable 12: Comparison of our evaluation results and GPT-4 officially reported scores. The official\nscore of MATH is obtained from Bubeck et al. (2023), which is marked with \u22c6.\nBenchmark\nSetting\ngpt-4-\n0314\n(our evaluation)\nGPT-4\n(official score)\nMMLU\n5-shot\n83.7\n86.4\nARC-c\n25-shot\n96.3\n95.6\n1-shot\n94.9\n\u2013\nHellaSwag\n10-shot\n92.5\n95.3\n1-shot\n92.4\n\u2013\nWinoGrande\n5-shot\n89.3\n87.5\n1-shot\n86.7\n\u2013\nDROP\n3-shot, F1\n78.9\n80.9\nGSM8K\n5-shot CoT\n91.6\n92.0\n8-shot CoT\n92.1\n\u2013\nMATH\n4-shot CoT\n38.6\n42.5\u22c6\nHumanEval\n0-shot, pass@1\n66.3\n67.0\n20\nTable 13: Comparison of our results and the official scores reported in LLaMA and Llama 2 papers.\nBenchmark\nSetting\nLLaMA-\n65B\n(our evaluation)\nLLaMA-\n65B\n(official score)\nLlama 2-\n70B\n(our evaluation)\nLlama 2-\n70B\n(official score)\nNatural Questions\n1-shot\n27.7\n31.0\n27.0\n33.0\nTriviaQA\n1-shot\n73.4\n84.5\n74.0\n85.0\nMMLU\n5-shot\n60.1\n63.4\n67.8\n68.9\nBBH\n3-shot CoT\n58.2\n43.5\n65.0\n51.2\nGSM8K\n8-shot CoT\n53.6\n50.9\n56.4\n56.8\nMATH\n4-shot CoT\n2.6\n10.6\n3.7\n13.5\nHumanEval\n0-shot, pass@1\n10.7\n(T = 0.8)\n23.7\n(T = 0.1)\n12.7\n(T = 0.8)\n29.9\n(T = 0.1)\nMBPP\n3-shot, pass@1\n44.8\n(T = 0.8)\n37.7\n(T = 0.1)\n58.0\n(T = 0.8)\n45.0\n(T = 0.1)\n21\n"
  },
  {
    "title": "RealFill: Reference-Driven Generation for Authentic Image Completion",
    "link": "https://arxiv.org/pdf/2309.16668.pdf",
    "upvote": "11",
    "text": "RealFill: Reference-Driven Generation for Authentic Image Completion\nLuming Tang1,2 Nataniel Ruiz1 Qinghao Chu1 Yuanzhen Li1 Aleksander Ho\u0142y\u00b4nski1 David E. Jacobs1\nBharath Hariharan2\nYael Pritch1\nNeal Wadhwa1\nKfir Aberman1\nMichael Rubinstein1\n1Google Research\n2Cornell University\nRealFill (Ours)\n?\nReference Images\nTarget Image\n?\nStable Diffusion Inpainting\nFigure 1. Given a few reference images that roughly capture the same scene, and a target image with a missing region, RealFill is able\nto complete the target image with image content that is faithful to the true scene. In contrast, standard prompt-based inpainting methods\nhallucinate plausible but inauthentic content due to their lack of knowledge of the original scene.\nAbstract\nRecent advances in generative imagery have brought\nforth outpainting and inpainting models that can pro-\nduce high-quality, plausible image content in unknown re-\ngions, but the content these models hallucinate is neces-\nsarily inauthentic, since the models lack sufficient context\nabout the true scene. In this work, we propose RealFill,\na novel generative approach for image completion that\nfills in missing regions of an image with the content that\nshould have been there. RealFill is a generative inpainting\nmodel that is personalized using only a few reference im-\nages of a scene. These reference images do not have to be\naligned with the target image, and can be taken with drasti-\ncally varying viewpoints, lighting conditions, camera aper-\ntures, or image styles. Once personalized, RealFill is able\nto complete a target image with visually compelling con-\ntents that are faithful to the original scene. We evaluate\nRealFill on a new image completion benchmark that cov-\ners a set of diverse and challenging scenarios, and find that\nit outperforms existing approaches by a large margin. See\nmore results on our project page: https://realfill.\ngithub.io.\n1. Introduction\nPhotographs capture frozen moments in time corre-\nsponding to ephemeral and invaluable experiences in our\nlives, but can sometimes fail to do these memories justice.\nIn many cases, no single shot may have captured the perfect\nangle, framing, timing, and composition, and unfortunately,\njust as the experiences themselves cannot be revisited, these\n1\narXiv:2309.16668v1  [cs.CV]  28 Sep 2023\nelements of the captured images are similarly unalterable.\nWe show one such example in Fig. 2: imagine having taken\na nearly perfect photo of your daughter dancing on stage,\nbut her unique and intricate crown is only barely cut out of\nthe frame. Of course, there are many other pictures from\nthe performance that showcase her crown, but they all fail\nto capture that precise special moment: her pose mid-dance,\nher facial expression, and the perfect lighting. Given your\nmemories of this event and this collection of imperfect pho-\ntos, you can certainly imagine the missing parts of this per-\nfect shot, but actually creating a complete version of this im-\nage, e.g., to share with family and friends, is a much harder\ntask.\nIn this paper, we focus on this problem, which we call\nAuthentic Image Completion. Given a few reference images\n(up to five) and one target image that captures roughly the\nsame scene (but in a different arrangement or appearance),\nwe aim to fill missing regions of the target image with high-\nquality image content that is faithful to the originally cap-\ntured scene. Note that for the sake of practical benefit, we\nfocus particularly on the more challenging, unconstrained\nsetting in which the target and reference images may have\nvery different viewpoints, environmental conditions, cam-\nera apertures, image styles, or even moving objects.\nApproaches to solve variants of this problem have been\nproposed using classical geometry-based pipelines [33, 49,\n51] that rely on correspondence matching, depth estimation,\nand 3D transformations, followed by patch fusion and im-\nage harmonization. These methods tend to encounter catas-\ntrophic failure when the scene\u2019s structure cannot be accu-\nrately estimated, e.g., when the scene geometry is too com-\nplex or contains dynamic objects. On the other hand, re-\ncent generative models [6,7,45], and in particular diffusion\nmodels [14,27,35], have demonstrated strong performance\non the tasks of image inpainting and outpainting [1,26,40].\nThese methods, however, struggle to recover the genuine\nscene structure and fine details, since they are only guided\nby text prompts, and therefore lack a mechanism for utiliz-\ning reference image content.\nTo this end, we present a simple yet effective reference-\ndriven image completion framework called RealFill. For a\ngiven scene, we first create a personalized generative model\nby fine-tuning a pre-trained inpainting diffusion model [1]\non the reference and target images. This fine-tuning process\nis designed such that the adapted model not only maintains\na good image prior, but also learns the contents, lighting,\nand style of the scene in the input images. We then use\nthis fine-tuned model to fill the missing regions in the tar-\nget image through a standard diffusion sampling process.\nGiven the stochastic nature of generative inference, we also\npropose Correspondence-Based Seed Selection, to automat-\nically select a small set of high-quality generations by ex-\nploiting a special property of our completion task: the fact\nthat there should exist true correspondence between our\ngenerated content and our reference images. Specifically,\nwe filter out samples that have too few keypoint correspon-\ndences with our reference images, a filtering process that\ngreatly limits the need for human intervention in selecting\nhigh-quality model outputs.\nAs shown in Fig 1, 3, and 4, RealFill is able to very ef-\nfectively inpaint or outpaint a target image with its genuine\nscene content. Most importantly, our method is able to han-\ndle large differences between reference and target images,\ne.g., viewpoint, lighting, aperture, style or dynamic defor-\nmations \u2014 differences which are very difficult for previous\ngeometry-based approaches. Existing benchmarks for im-\nage completion [51] mainly focus on small inpainting tasks\nand minimal changes between reference and target images.\nIn order to quantitatively evaluate the aforementioned chal-\nlenging use-case, we collect a dataset containing 10 inpaint-\ning and 23 outpainting examples along with corresponding\nground-truth, and show that RealFill outperforms baselines\nby a large margin across multiple image similarity metrics.\nIn summary, in our work, we propose the following con-\ntributions:\n\u2022 We define a new problem, named Authentic Image\nCompletion, where, given a set of reference images\nand a target image with missing regions, we seek to\ncomplete those missing regions with content faithful to\nthe scene observed in the reference images. In essence,\nthe goal is to complete the target image with what\n\u201cshould have been there\u201d rather than what \u201ccould have\nbeen there\u201d, as is often the case in typical generative\ninpainting.\n\u2022 We introduce RealFill, a method that aims to solve this\nproblem by finetuning a diffusion-based text-to-image\ninpainting model on reference and target images. This\nmodel is sampled with Correspondence-Based Seed\nSelection to filter ouptuts with low fidelity to the refer-\nence images. RealFill is the first method that expands\nthe expressive power of generative inpainting models\nby conditioning the process on more than text (i.e., by\nadding reference images).\n\u2022 We propose RealBench, a dataset for quantitative and\nqualitative evaluation of authentic image completion,\ncomposed of 33 scenes spanning both inpainting and\noutpainting tasks.\n2. Related Work\nAdapting Pre-trained Diffusion Models.\nDiffusion\nmodels [10,14,35] have demonstrated superior performance\nin text-to-image (T2I) generation [26,27,31]. Recent works\ntake advantage of this useful pre-trained image prior by\nfine-tuning these models, either for added controllability,\n2\npersonalization, or for specialized tasks.\nPersonalization\nmethods [8, 12, 28, 29] propose to fine-tune the T2I model\non a few choice images in order to achieve subject-driven\ngeneration, allowing for arbitrary text-driven generation of\na given object or style. Other techniques instead fine-tune\na T2I model to add new conditioning signals, either for\nimage editing [4, 17, 40] or more controllable image gen-\neration [23, 34, 46]. This same approach has been shown\nto be useful for specialized tasks [19, 25, 42, 48] such as\nadding camera viewpoint conditioning, e.g., to aid in text-\nto-3D generation or converting a T2I model into a gener-\native video model. Our method shows that a pre-trained\nT2I inpainting diffusion model can be adapted to perform\nreference-driven image completion.\nImage Completion. An enduring challenge in computer\nvision, image completion aims to fill missing parts of an im-\nage with plausible content. This task is interchangeably re-\nferred to as inpainting or outpainting depending on the char-\nacteristics of the missing region. Traditional approaches to\nthis problem [2,3,9,13] rely on handcrafted heuristics while\nmore recent deep learning based methods [16, 18, 37, 41]\ninstead directly train end-to-end neural networks that take\noriginal image and mask as inputs and generate the com-\npleted image. Given the challenging nature of this prob-\nlem [50], many works [6, 7, 20, 44] propose to leverage\nthe image prior from a pre-trained generative model for\nthis task. Built upon powerful T2I diffusion models, re-\ncent diffusion-based solutions [1, 26] demonstrate strong\ntext-driven image completion capabilities. However, due\nto their sole dependence on a text prompt (which has lim-\nited descriptive power), generated image content can often\nbe hard to control, resulting in tedious prompt tuning, espe-\ncially when a particular or otherwise true scene content is\ndesired. This is one of the main issues we aim to tackle in\nour work.\nReference Based Image Inpainting. Existing work for\nreference-based inpainting [49,51] or outpainting [33] usu-\nally make use of carefully tuned pipelines containing many\nindividual components like depth and pose estimation, im-\nage warping, and harmonization. Each of these modules\nusually tackles a moderately challenging problem itself and\nthe resulting prediction error can, and often does, propagate\nand accumulate through the pipeline. This can lead to catas-\ntrophic failure especially in challenging cases with complex\nscene geometry, changes in appearance, or scene deforma-\ntion. Paint-by-Example [43] propose to fine-tune a latent\ndiffusion model [27] such that the generation is conditioned\non both a reference and target image. However, the image\nconditioning is based on a CLIP embedding [24] of a sin-\ngle reference image, and is therefore only able to capture\nhigh-level semantic information of the reference object. In\ncontrast, our method is the first to demonstrate multiple ref-\nerence image-driven inpainting and outpainting that is both\nvisually compelling and faithful to the original scene, even\nin cases where there are large appearance changes between\nreference and target images.\n3. Method\n3.1. Reference-Driven Image Completion\nGiven a set of casually captured reference images (up to\nfive), our goal is to complete (i.e., either outpaint or inpaint)\na target image of roughly the same scene. The output im-\nage is expected to not only be plausible and photorealistic,\nbut to also be faithful to the reference images \u2014 recover-\ning content and scene detail that was present in the actual\nscene. In essence, we want to achieve authentic image com-\npletion, where we generate what \u201cshould have been there\u201d\ninstead of what \u201ccould have been there\u201d. We purposefully\npose this as a broad and challenging problem with few con-\nstraints on the inputs. For example, the images could be\ntaken from very different viewpoints with unknown camera\nposes. They could also have different lighting conditions\nor styles, and the scene could potentially be non-static and\nhave significantly varying layout across images.\nIn this section, we first provide background knowl-\nedge on diffusion models and subject-driven generation\n(Sec. 3.2). Then, we formally define the problem of authen-\ntic image completion (Sec. 3.3). Finally, we present Real-\nFill, our method to perform reference-driven image comple-\ntion with a pre-trained diffusion image prior (Sec. 3.4).\n3.2. Preliminaries\nDiffusion models are generative models that aim to\ntransform a Gaussian distribution into an arbitrary target\ndata distribution. During training, different magnitudes of\nGaussian noise are added to a clean data point x0 to obtain\nnoisy xt:\nxt = \u221a\u03b1tx0 + (\n\u221a\n1 \u2212 \u03b1t)\u03f5\n(1)\nwhere the noise \u03f5 \u223c N(0, I), and {\u03b1t}T\nt=1 define a fixed\nnoise schedule with larger t corresponding to more noise.\nThen, a neural network \u03f5\u03b8 is trained to predict the noise\nusing the following loss function:\nL = Ex,t,\u03f5 \u2225\u03f5\u03b8(xt, t, c) \u2212 \u03f5\u22252\n2\n(2)\nwhere the generation is conditioned on some signal c, e.g.,\na language prompt for a text-to-image model, or a masked\nimage for an inpainting model. During inference, starting\nfrom xT \u223c N(0, I), \u03f5\u03b8 is used to iteratively remove noise\nfrom xt to get a less noisy xt\u22121, eventually leading to a\nsample x0 from the target data distribution.\nDreamBooth [28] enables T2I diffusion models to gen-\nerate images of a specific subject with semantic modifica-\ntions. The core idea is to fine-tune the model \u03f5\u03b8 on a few\nimages of the subject using the loss in Eq. 2. Instead of\n3\ninput image\n\u201ca photo \nof [V]\u201d\nadd noise\ntext\nencoder\n\u2744\n\ud83d\udd25 LoRA\ntext-to-image diffusion \ninpainting model\n\u2744\n\ud83d\udd25 LoRA\nrandom mask\nmasked image\n\u2a00\n=\npredicted \nclean image\nreconstruction\nloss\nreference & target images\nrandomly sample \nand augment\nTraining\nInference\n\u201ca photo \nof [V]\u201d\nfinetuned \ndiffusion model\n\u2026\ntarget image & mask\nGaussian\nNoise\nRealFill output\nFigure 2. RealFill - training and inference pipelines. The input to our method is a target image to be filled and a few reference images\nof the same scene. We first fine-tune LoRA weights of a pre-trained inpainting diffusion model on the reference and target images (with\nrandom patches masked out). Then, we use the adapted model to fill the desired region of the target image, resulting in a faithful and high-\nquality output, e.g., the dancing girl\u2019s crown is recovered in the target image, despite the girl and crown exhibiting significantly different\nposes and articulations when compared to any of the reference images.\nfine-tuning all of the weights of the network, it is possi-\nble to combine DreamBooth with Low Rank Adaptations\n(LoRA) [15, 30], for a more memory-efficient alternative,\nby injecting learnable residual modules \u2206W to each net-\nwork weight matrix W. \u2206W is a composition of low-rank\nmatrices, i.e., W + \u2206W = W + AB where W \u2208 Rn\u00d7n,\nA \u2208 Rn\u00d7r, B \u2208 Rr\u00d7n, r \u226a n, and only the added \u2206W\nis being updated during training while model\u2019s original pa-\nrameters W stay frozen.\n3.3. Problem Setup\nFormally speaking, the model is given n (n \u2264 5)\nreference images Xref\n:=\n{Ik\nref}n\nk=1, a target image\nItgt \u2208 RH\u00d7W \u00d73 and its associated binary mask Mtgt \u2208\n{0, 1}H\u00d7W , in which 1 denotes the region to fill and 0 de-\nnotes the existing area in Itgt. The model is expected to\ngenerate a harmonized image Iout \u2208 RH\u00d7W \u00d73 whose pix-\nels are to remain as similar as possible to Itgt where the\nmask equals 0, while staying faithful to the corresponding\ncontents in Xref where the mask equals 1. We assume there\nis sufficient overlap between the contents of Xref and Itgt\nsuch that a human is able to imagine a plausible Iout.\n3.4. RealFill\nThis task is challenging for both geometry-based [49,51]\nand reconstruction-based approaches [22] because there are\nbarely any geometric constraints between Xref and Itgt,\nthere are only a few images available as inputs, and the\nreference images may have different styles, lighting condi-\ntions, and subject poses from the target. One alternative is to\nuse a controllable inpainting or outpainting methods, how-\never, these methods are either prompt-based [27] or single-\nimage object-driven [43], which makes them hard to use for\nrecovering complex scene-level structure and details.\nTo this end, we propose to first fine-tune a pre-trained\ngenerative model by injecting knowledge of the scene (from\na set of reference images), such that the model is aware of\nthe contents of the scene when generating Iout, conditioned\non Itgt and Mtgt.\nTraining. Starting from a state-of-the-art T2I diffusion\ninpainting model [27], we inject LoRA weights and fine-\ntune it on both Xref and Itgt with randomly generated bi-\nnary masks m \u2208 {0, 1}H\u00d7W . The loss function is\nL = Ex,t,\u03f5,m \u2225\u03f5\u03b8(xt, t, p, m, (1 \u2212 m) \u2299 x) \u2212 \u03f5\u22252\n2\n(3)\nwhere x \u2208 Xref \u222a {Itgt}, p is a fixed language prompt, \u2299\ndenotes the element-wise product and therefore (1\u2212m)\u2299x\nis the masked clean image. For Itgt, the loss is only calcu-\nlated on the existing region, i.e., where Mtgt\u2019s entry equals\n0. Specifically, we use the open-sourced Stable Diffusion v2\ninpainting model [1] and inject LoRA layers into its text en-\ncoder and U-Net for fine-tuning. Following [28], we fix p to\nbe a sentence containing a rare token, i.e., \u201ca photo of\n[V]\u201d. For each training example, similar to [37], we gen-\nerate multiple random rectangles and take either their union\nor the complement of the union to get the final random mask\nm. Our fine-tuning pipeline is illustrated in Fig. 2.\nInference. After training, we use the DDPM [14] sam-\npler to generate an image Igen, conditioning the model on p,\nItgt and Mtgt. However, similar to the observation in [52],\nwe notice that the existing region in Itgt is distorted in Igen.\nTo resolve this, we first blur the mask Mtgt, then use it to\nalpha composite Igen and Itgt, leading to the final Iout with\nfull recovery on the existing area and a smooth transition at\nthe boundary of the generated region.\nCorrespondence-Based Seed Selection. The diffusion\ninference process is stochastic, i.e., the same input condi-\ntioning images may produce any number of generated im-\n4\nages depending on the input seed to the sampling process.\nThis stochasticity often results in variance in the quality of\ngenerated results, often requiring human intervention to se-\nlect high-quality samples. While there exists work in iden-\ntifying good samples from a collection of generated out-\nputs [32], this remains an open problem. Nevertheless, our\nproposed problem of authentic image completion is a spe-\ncial case of this more general problem statement. In par-\nticular, the reference images provide a grounding signal\nfor the true content of the scene, and can be used to help\nidentify high-quality outputs. Specifically, we find that the\nnumber of image feature correspondences between Iout and\nXref can be used as a metric to roughly quantify whether\nthe result is faithful to the reference images. We propose\nCorrespondence-Based Seed Selection, a process that con-\nsists of generating a batch of outputs, i.e., {Iout}, extracting\na set of correspondences (using LoFTR [36], for example)\nbetween Xref and the filled region of each Iout, (i.e., where\nMtgt\u2019s entry equals 1), and finally ranking the generated re-\nsults {Iout} by the number of matched keypoints. This al-\nlows us to automatically filter generations to a small set of\nhigh-quality results. Compared to traditional seed selection\napproaches in other domains, our proposed method greatly\nalleviates the need for human intervention in selecting best\nsamples.\n4. Experiments\n4.1. Qualitative Results\nIn Fig. 3 and 4, we show that RealFill is able to con-\nvincingly outpaint and inpaint image content that is faith-\nful to the reference images. Notably, it is able to handle\ndramatic differences in camera pose, lighting, defocus blur,\nimage style and even subject pose. This is because Real-\nFill has both a good image prior (from the pre-trained diffu-\nsion model) and knowledge of the scene (from fine-tuning\non the input images). Thus, it is able to inherit knowledge\nabout the contents of the scene, but generate content that fits\nseamlessly into the target image.\n4.2. Comparisons\nEvaluation\nDataset.\nExisting\nbenchmarks\nfor\nreference-driven image completion [51] primarily fo-\ncus on inpainting small regions, and assume at most very\nminor changes between the reference and target images.\nTo better evaluate our target use-case, we create our own\ndataset, RealBench. RealBench consists of 33 scenes (23\noutpainting and 10 inpainting), where each scene has a\nset of reference images Xref, a target image Itgt to fill, a\nbinary mask Mtgt indicating the missing region and the\nground-truth result Igt. The number of reference images\nin each scene varies from 1 to 5.\nThe dataset contains\ndiverse, challenging scenarios with significant variations\nbetween the reference and target images, such as changes\nin viewpoint, defocus blur, lighting, style and subject pose.\nEvaluation Metrics. We use multiple metrics to evalu-\nate the quality and fidelity of our model outputs. We com-\npare the generated images with the ground-truth target im-\nage at multiple levels of image similarity, including PSNR,\nSSIM, and LPIPS [47] for low-level, DreamSim [11] for\nmid-level, and DINO [5] and CLIP [24] for high-level.\nFor low-level metrics, we only calculate a loss on the\nfilled-in region, i.e., where Mtgt is 1. For high-level image\nsimilarity, we use the cosine distance between the full image\nembeddings from CLIP and DINO. For mid-level similar-\nity, we use the full image embedding using DreamSim [11]\nwhich is designed to emphasize differences in image lay-\nouts, object poses, and semantic contents.\nBaseline Approaches. We compare to two baselines:\nthe exemplar-based image inpainting method Paint-by-\nExample [43] and the popular prompt-based image filling\napproach Stable Diffusion Inpainting [1]. Since Paint-by-\nExample only uses one reference image during generation,\nwe randomly pick a reference image for each run of this\nbaseline. Choosing an appropriate prompt for Stable Diffu-\nsion Inpainting is a necessary component of getting a high\nquality result. So, for a fair comparison, instead of using a\ngeneric prompt like \u201ca beautiful photo\u201d, we manually write\na long prompt that describes the scene in detail. For exam-\nple, the prompt for the first row of Fig. 5 is \u201ctwo men sitting\ntogether with a child in the middle, the man on the left is\nplaying guitar, the man on right is wearing a birthday hat\nwith some stickers on it. There is a blue decorator hanging\non the wall\u201d.\nImplementation Details of RealFill. For each scene,\nwe fine-tune the inpainting diffusion model for 2,000 itera-\ntions with a batch size of 16 on a single NVIDIA A100 GPU\nwith LoRA rank 8. With a probability of 0.1, we randomly\ndropout prompt p, mask m and LoRA layers independently\nduring training. The learning rate is set to 2e-4 for the U-\nNet and 4e-5 for the text encoder. Note that these hyper-\nparameters could be further tuned for each scene to get bet-\nter performance, e.g., some scenes converge more quickly\nmay overfit if trained for too long. However, for the sake of\nfair comparison, we use a constant set of hyper-parameters\nfor all results shown in the paper.\nQuantitative Comparison. We quantitatively compare\nour method with the baseline methods. For each method,\nwe report average metrics across all target images {Iout},\nwhere each image\u2019s metric is itself computed from an aver-\nage of 64 stochastically generated samples.. In Tab. 1, we\nreport these aggregate metrics and find that RealFill outper-\nforms all baselines by a large margin across all levels of\nsimilarity.\nQualitative Comparison. In Fig. 5, we present a vi-\nsual comparison between RealFill and the baselines. We\n5\nReference Images\nRealFill Outputs\nReference Images\nRealFill Outputs\nFigure 3. Reference-based outpainting with RealFill. Given the reference images on the left, RealFill is able to outpaint the correspond-\ning target images on the right. The region inside the white box is provided to the network as known pixels, and the regions outside the\nwhite box are all generated. Results show that RealFill produces high-quality images that are faithful to the references, even when there\nare dramatic differences between references and targets including changes in viewpoint, aperture, lighting, image style, and object motion.\n6\nTarget Image\nRealFill Output\nReference Images\nTarget Image\nRealFill Output\nReference Images\nFigure 4. Reference-based inpainting with RealFill. Given the reference images on the left, RealFill is not only able to remove undesired\nobjects in the target image and reveal the occluded contents faithfully (left column), but it is also able to insert objects into the scene\ndespite significant viewpoint changes between reference and target images (right column). In the bottom left example, the aperture between\nreference and target images is also different, and RealFill not only recovers the buildings behind the mug, but also maintains the appropriate\namount of blur seen in the target image.\nMethod\nlow-level\nmid-level\nhigh-level\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nDreamSim \u2193\nDINO \u2191\nCLIP \u2191\nprompt-based\nStable Diffusion Inpaint [1]\n10.63\n0.282\n0.605\n0.213\n0.831\n0.874\nreference-based\nPaint-by-Example [43]\n10.13\n0.244\n0.642\n0.237\n0.797\n0.859\nRealFill (ours)\n14.78\n0.424\n0.431\n0.077\n0.948\n0.962\nTable 1. On RealBench, our evaluation dataset of 33 diverse challenging scenes, including 23 outpainting and 10 inpainting tasks, RealFill\noutperforms both prompt-based and reference-based baselines by a large margin for all types of image similarity metrics.\nalso show the ground-truth and input images for each exam-\nple. In order to better highlight the regions which are being\ngenerated, we overlay a semi-transparent white mask on the\nground truth and output images, covering the known regions\nof the target image. RealFill not only generates high-quality\nimages, but also more faithfully reproduces the scene than\nthe baseline methods. Paint-by-Example relies on the CLIP\nembedding of the reference images as the condition. This\nposes a challenge when dealing with complex scenes or at-\ntempting to restore object details, since CLIP embeddings\nonly capture high-level semantic information. The gener-\nated results from Stable Diffusion Inpainting are plausible\non their own. However, because natural language is limited\nin conveying complex visual information, they often exhibit\nsubstantial deviations from the original scenes depicted in\nthe reference images.\nCorrespondence-Based Seed Selection. We evaluate\nthe effect of our proposed correspondence-based seed se-\nlection described in Sec. 3.4. To measure the correlation\nbetween our seed selection mechanism and high-quality re-\nsults, we rank RealFill\u2019s outputs {Iout} according to the\nnumber of matched keypoints, and then filter out a cer-\ntain percent of the lowest-ranked samples. We then aver-\nFiltering\nRate\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nDreamSim\u2193\nDINO\u2191\nCLIP\u2191\n0%\n14.78\n0.424\n0.431\n0.077\n0.948\n0.962\n25%\n15.01\n0.427\n0.421\n0.066\n0.955\n0.967\n50%\n15.05\n0.427\n0.418\n0.063\n0.958\n0.969\n75%\n15.10\n0.427\n0.417\n0.060\n0.961\n0.970\nTable 2. Correspondence-Based Seed Selection results in higher\nquality, as shown by the fact that filtering out samples with fewer\nmatches results in higher aggregate quantitative scores.\nage the evaluation metrics only across the remaining sam-\nples. We find that higher filtering rates like 75% greatly im-\nprove the quantitative metrics, when compared to unfiltered\nresults (Tab. 2). In Fig 6, we show multiple RealFill out-\nputs with the corresponding number of matched keypoints.\nThese demonstrate a clear trend, where fewer matches usu-\nally indicate lower-quality results.\n5. Discussion\n5.1. Would other baselines work?\nImage Stitching. One straight-forward approach is to\nutilize the correspondence between reference and target im-\n7\nPaint-by-Example\nGround-Truth\nRealFill (Ours)\nStable Diffusion\nInpainting\nReference Images\nFigure 5. A comparison of RealFill and baseline methods. Transparent white masks are overlayed on the unaltered known regions of the\ntarget images. Paint-by-Example loses fidelity with the reference images because it relies on CLIP embeddings, which only capture high-\nlevel semantic information. While Stable Diffusion Inpainting produces plausible results, they are inconsistent with the reference images\nbecause prompts have limited expressiveness. In contrast, RealFill generates high-quality results that have high fidelity with respect to the\nreference images.\nages and stitch them together. However, we find that this\ndoes not yield acceptable results most of the time, even us-\ning commercial image stitching software, particularly when\nthere are dramatic viewpoint changes, lighting changes, or\nmoving objects. Taking the two scenes in Fig. 7 as example,\nmultiple commercial software solutions produce no output,\nasserting that the reference and target images do not have\nsufficient correspondences. On the contrary, RealFill recov-\ners these scenes both faithfully and realistically.\nVanilla DreamBooth. Instead of adapting an inpaint-\ning model, another alternative is to fine-tune a standard Sta-\nble Diffusion model on the reference images, i.e., vanilla\nDreamBooth, then use the fine-tuned T2I model to inpaint\nthe target image [20], as implemented in the popular Dif-\nfusers library [39]1. However, because this model is never\ntrained with a masked prediction objective, it performs\nmuch worse compared to RealFill, as shown in Fig. 8.\n1Diffusers\u2019 Stable Diffusion inpainting pipeline code.\n8\nReference Images\nRealFill Outputs\nTarget Image\nGround-Truth\n# of matches: 1111\n# of matches: 692\n# of matches: 83\n# of matches: 97\n# of matches: 54\n# of matches: 9\nFigure 6. Given the reference images on the left, we show multiple RealFill outputs on the right along with the number of matched key\npoints noted below each image. We can see that fewer matches correlate with lower-quality outputs that are more divergent from the\nground-truth.\nReference Images\nTarget Image\nRealFill Outputs\nFigure 7. Commercial image stitching software fails to produce\nany outputs when there are dramatic differences between refer-\nence and target images, as in the examples shown above with large\nlighting changes. In contrast, RealFill produces faithful and high-\nquality results, i.e., the rooftop tank and the balloon are both recov-\nered even when the target images are captured at vastly different\ntimes of day.\n5.2. What makes RealFill work?\nIn order to explore why our proposed method leads to\nstrong results, especially on complex scenes, we make the\nfollowing two hypotheses:\nRealFill relates multiple elements in a scene. If we\nmake the conditioning image a blank canvas during infer-\nence, i.e., all entries of Mtgt equal 1, we can see in Fig. 9\nthat the fine-tuned model is able to generate multiple scene\nVanilla DreamBooth Outputs\nReference \nImages\nRealFill Output\nGround-Truth\nstrength=1.0\nstrength=0.8\nstrength=0.5\nFigure 8. Vanilla Dreambooth, i.e., fine-tuning a standard Stable\nDiffusion model on the reference images and using it to fill missing\nregions, leads to drastically worse results compared to RealFill.\nWe show different samples using varying levels of the strength\nhyper-parameter.\nvariants with different structures, e.g., removing the fore-\nground or background object, or manipulating the object\nlayouts. This suggest that RealFill is able to relate the el-\nements inside the scene in a compositional way.\nRealFill captures correspondences among input im-\nages. Even if the reference and target images do not depict\nthe same scene, the fine-tuned model is still able to fuse\nthe corresponding contents of the reference images into the\n9\nReference & \nTarget Images\nGenerated Scene Variants\nFigure 9. RealFill is able to generate multiple scene variants when\nconditioned on a blank image as input, e.g., people are added or\nremoved in the first and second rows. This suggests that the fine-\ntuned model can relate the elements inside the scene in a compo-\nsitional manner.\nReference Images\nRealFill Outputs\nFigure 10. When the reference and target images do not depict the\nsame scene, the fine-tuned model is still able to fuse the reference\ncontents into the target image in a semantically-reasonable way,\nsuggesting that it captures both real or invented correspondences\nbetween input images.\ntarget area seamlessly, as shown in Fig. 10. This suggests\nthat RealFill is able to capture and utilize real or invented\ncorrespondences between reference and target images to\ndo generation. Previous works [21, 38] also found similar\nemergent correspondence inside pre-trained Stable Diffu-\nsion models.\nReference Images\nTarget Image\nRealFill Output\nFigure 11. (Top) RealFill fails to recover the precise 3D scene\nstructure, e.g., the output husky plush has different pose compared\nto the reference; (Bottom) RealFill fails to handle cases that are\nalso challenging for the base T2I model, e.g., text.\n5.3. Limitations\nBecause RealFill needs to go through a gradient-based\nfine-tuning process on input images, it is relatively slow and\nfar from real time. Empirically, we also find that, when the\nviewpoint change between reference and target images is\ndramatic, RealFill fails to recover the 3D scene faithfully,\nespecially when there\u2019s only a single reference image. For\nexample, as seen in the top row of Fig. 11, the reference\nimage is captured from a side view while the target is from\na center view. Although the RealFill output looks plausible\nat first glance, the pose of the husky is different from the\nreference, e.g., the left paw should be on the gap between\nthe cushions. Lastly, because RealFill mainly relies on the\nimage prior inherited from the base pre-trained model, it\nalso fails to handle cases where that are challenging for the\nbase model. For instance, Stable Diffusion is known to be\nless effective when it comes to generating fine image de-\ntails, such as text, human faces, or body parts. As shown in\nthe bottom row of Fig. 11 where the store sign is wrongly\nspelled, this is also true for RealFill.\n6. Societal Impact\nThis research aims to create a tool that can help users\nexpress their creativity and improve the quality of their per-\nsonal photographs through image generation. However, ad-\nvanced image generation methods can have complex im-\npacts on society. Our proposed method inherits some of\nthe concerns that are associated with this class of technol-\nogy, such as the potential to alter sensitive personal char-\nacteristics. The open source pre-trained model that we use\nin our work, Stable Diffusion, exhibits some of these con-\ncerns. However, we have not found any evidence that our\nmethod is more likely to produce biased or harmful content\n10\nthan previous work. Despite these findings, it is important\nto continue investigating the potential risks of image gen-\neration technology. Future research should focus on devel-\noping methods to mitigate bias and harmful content, and to\nensure that image generation tools are used in a responsible\nmanner.\n7. Conclusion\nIn this work, we introduce the problem of Authentic Im-\nage Completion, where given a few reference images, we\nintend to complete some missing regions of a target image\nwith the content that \u201cshould have been there\u201d \u2014 rather that\n\u201cwhat could have been there\u201d. To tackle this problem, we\nproposed a simple yet effective approach called RealFill,\nwhich first fine-tunes a T2I inpainting diffusion model on\nthe reference and target images, and then uses the adapted\nmodel to fill the missing regions. We show that RealFill\nproduces high-quality image completions that are faithful\nto the content in the reference images, even when there are\nlarge differences between reference and target images such\nas viewpoint, aperture, lighting, image style and object po-\nsition, pose and articulation.\nAcknowledgements. We would like to thank Rundi Wu,\nQianqian Wang, Viraj Shah, Ethan Weber, Zhengqi Li, Kyle\nGenova, Boyang Deng, Maya Goldenberg, Noah Snavely,\nBen Poole, Ben Mildenhall, Alex Rav-Acha, Pratul Srini-\nvasan, Dor Verbin and Jon Barron for their valuable discus-\nsion and feedbacks, and thank Zeya Peng, Rundi Wu, Shan\nNan for their contribution to the evaluation dataset. A spe-\ncial thanks to Jason Baldridge, Kihyuk Sohn, Kathy Meier-\nHellstern, and Nicole Brichtova for their feedback and sup-\nport for the project.\nReferences\n[1] Stability AI.\nStable-diffusion-2-inpainting.\nhttps:\n/ / huggingface . co / stabilityai / stable -\ndiffusion-2-inpainting. 2, 3, 4, 5, 7\n[2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and\nDan B Goldman.\nPatchmatch: A randomized correspon-\ndence algorithm for structural image editing. ACM Trans.\nGraph., 28(3):24, 2009. 3\n[3] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and\nColoma Ballester. Image inpainting. In Proceedings of the\n27th annual conference on Computer graphics and interac-\ntive techniques, pages 417\u2013424, 2000. 3\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n3\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 5\n[6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 2, 3\n[7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315\u201311325, 2022.\n2, 3\n[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz,\nXuhui Jia, Ming-Wei Chang, and William W Cohen.\nSubject-driven text-to-image generation via apprenticeship\nlearning. arXiv preprint arXiv:2304.00186, 2023. 3\n[9] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Ob-\nject removal by exemplar-based inpainting. In 2003 IEEE\nComputer Society Conference on Computer Vision and Pat-\ntern Recognition, 2003. Proceedings., volume 2, pages II\u2013II.\nIEEE, 2003. 3\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[11] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy\nChai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream-\nsim: Learning new dimensions of human visual similar-\nity using synthetic data. arXiv preprint arXiv:2306.09344,\n2023. 5\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 3\n[13] James Hays and Alexei A Efros. Scene completion using\nmillions of photographs.\nACM Transactions on Graphics\n(ToG), 26(3):4\u2013es, 2007. 3\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 4\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations, 2022.\n4\n[16] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.\nGlobally and locally consistent image completion.\nACM\nTransactions on Graphics (ToG), 36(4):1\u201314, 2017. 3\n[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 3\n[18] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n85\u2013100, 2018. 3\n11\n[19] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 3\n[20] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11461\u201311471, 2022. 3, 8\n[21] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn-\nski, and Trevor Darrell. Diffusion hyperfeatures: Searching\nthrough time and space for semantic correspondence. arXiv\npreprint arXiv:2305.14334, 2023. 10\n[22] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 4\n[23] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-\nadapter:\nLearning adapters to dig out more controllable\nability for text-to-image diffusion models.\narXiv preprint\narXiv:2302.08453, 2023. 3\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 5\n[25] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li,\nand Varun Jampani. Dreambooth3d: Subject-driven text-to-\n3d generation. arXiv preprint arXiv:2303.13508, 2023. 3\n[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 3\n[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 4\n[28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 3, 4\n[29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models. arXiv preprint\narXiv:2307.06949, 2023. 3\n[30] Simo Ryu.\nLow-rank adaptation for fast text-to-image\ndiffusion\nfine-tuning.\nhttps : / / github . com /\ncloneofsimo/lora. 4\n[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[32] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan,\nand Gal Chechik.\nIt is all about where you start: Text-\nto-image generation with seed selection.\narXiv preprint\narXiv:2304.14530, 2023. 5\n[33] Qi Shan, Brian Curless, Yasutaka Furukawa, Carlos Hernan-\ndez, and Steven M Seitz. Photo uncrop. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part VI 13, pages\n16\u201331. Springer, 2014. 2, 3\n[34] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 3\n[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2021. 2\n[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. LoFTR: Detector-free local feature matching\nwith transformers. CVPR, 2021. 5\n[37] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky.\nResolution-robust large mask inpainting with\nfourier convolutions. In Proceedings of the IEEE/CVF winter\nconference on applications of computer vision, pages 2149\u2013\n2159, 2022. 3, 4\n[38] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 10\n[39] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 8\n[40] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-\nTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah\nLaszlo, David J Fleet, Radu Soricut, et al. Imagen editor\nand editbench: Advancing and evaluating text-guided im-\nage inpainting. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 18359\u2013\n18369, 2023. 2, 3\n[41] Chenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng\nWang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan\nDuan.\nNuwa-infinity: Autoregressive over autoregressive\ngeneration for infinite visual synthesis.\narXiv preprint\narXiv:2207.09814, 2022. 3\n[42] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\n12\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 3\n[43] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18381\u201318391,\n2023. 3, 4, 5, 7\n[44] Raymond A Yeh, Chen Chen, Teck Yian Lim, Alexander G\nSchwing, Mark Hasegawa-Johnson, and Minh N Do. Se-\nmantic image inpainting with deep generative models.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5485\u20135493, 2017. 3\n[45] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5505\u20135514,\n2018. 2\n[46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n3\n[47] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 5\n[48] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie\nZhou, and Jiwen Lu.\nUnleashing text-to-image diffusion\nmodels for visual perception. ICCV, 2023. 3\n[49] Yunhan Zhao, Connelly Barnes, Yuqian Zhou, Eli Shecht-\nman, Sohrab Amirghodsi, and Charless Fowlkes. Geofill:\nReference-based image inpainting with better geometric un-\nderstanding. In Proceedings of the IEEE/CVF Winter Con-\nference on Applications of Computer Vision, pages 1776\u2013\n1786, 2023. 2, 3, 4\n[50] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic\nimage completion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1438\u20131447, 2019. 3\n[51] Yuqian Zhou, Connelly Barnes, Eli Shechtman, and Sohrab\nAmirghodsi. Transfill: Reference-guided image inpainting\nby merging multiple color and spatial transformations. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2266\u20132276, 2021. 2, 3,\n4, 5\n[52] Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le\nWang, Yinpeng Chen, Lu Yuan, and Gang Hua. Designing a\nbetter asymmetric vqgan for stablediffusion. arXiv preprint\narXiv:2306.04632, 2023. 4\n13\n"
  },
  {
    "title": "Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation",
    "link": "https://arxiv.org/pdf/2309.16429.pdf",
    "upvote": "10",
    "text": "Diverse and Aligned Audio-to-Video Generation via\nText-to-Video Model Adaptation\nGuy Yariv \u2661,\u2663, Itai Gat \u2662, Sagie Benaim \u2661, Lior Wolf \u2660, Idan Schwartz \u2660,\u2663,\u2217, Yossi Adi \u2661,*\n\u2661The Hebrew University of Jerusalem , \u2662Technion\n\u2660Tel-Aviv University, \u2663NetApp\nAbstract\nWe consider the task of generating diverse and realistic videos\nguided by natural audio samples from a wide variety of se-\nmantic classes. For this task, the videos are required to be\naligned both globally and temporally with the input audio:\nglobally, the input audio is semantically associated with the\nentire output video, and temporally, each segment of the in-\nput audio is associated with a corresponding segment of that\nvideo. We utilize an existing text-conditioned video genera-\ntion model and a pre-trained audio encoder model. The pro-\nposed method is based on a lightweight adaptor network,\nwhich learns to map the audio-based representation to the in-\nput representation expected by the text-to-video generation\nmodel. As such, it also enables video generation conditioned\non text, audio, and, for the first time as far as we can ascertain,\non both text and audio. We validate our method extensively on\nthree datasets demonstrating significant semantic diversity of\naudio-video samples and further propose a novel evaluation\nmetric (AV-Align) to assess the alignment of generated videos\nwith input audio samples. AV-Align is based on the detection\nand comparison of energy peaks in both modalities. In com-\nparison to recent state-of-the-art approaches, our method gen-\nerates videos that are better aligned with the input sound, both\nwith respect to content and temporal axis. We also show that\nvideos produced by our method present higher visual qual-\nity and are more diverse. Code and samples are available at:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens.\nIntroduction\nNeural generative models have changed the way we create\nand consume digital content. From generating high-quality\nimages and videos (Ho, Jain, and Abbeel 2020; Rombach\net al. 2022), speech and audio (Wang et al. 2023a; Sheffer\nand Adi 2023; Copet et al. 2023; Kreuk et al. 2022; Hassid\net al. 2023), through generating long textual spans (Touvron\net al. 2023a,b; Brown et al. 2020), these models have shown\nimpressive results.\nIn the context of video generation, progress has been more\nelusive, with recent work making progress in generating\nshort videos conditioned on text (Singer et al. 2022; Ho et al.\n2022). Although audio is tightly connected to videos (e.g.,\nproviding important cues for motion in a scene), most of the\nprior work did not consider audio in the generation process.\n*Equal Contribution.\nFigure 1: Generated video frames (above) and input audio\nsignal (below the frames) employing our technique. The in-\nput to our model is an audio recording from which a rep-\nresentation is extracted. This representation maintains cru-\ncial temporal attributes and is then mapped into a text-\nbased latent space representation incorporating both local\nand global audio context. Subsequently, this latent represen-\ntation is fed into a pre-trained text-to-video diffusion gener-\native model, ensuring the synchronized generation of video\nwhich is closely aligned with the input audio.\nFor instance, the action of \u2018playing drums\u2019 or the \u2018motion of\nwaves\u2019 can be distinctively associated with a naturally oc-\ncurring sound. Moreover, audio is comprised of structural\ncomponents such as pitch and envelope that provide impor-\ntant cues for the type of scene and motion depicted.\nWe tackle the problem of generating diverse and realis-\ntic videos guided by natural audio samples. Our generated\nvideos capture diverse and real-life settings from a wide va-\nriety of semantic classes and are aligned both globally and\ntemporally with the input audio. Globally, the input audio\nis semantically associated with the entire output video, and\ntemporally, each segment of the input audio is associated\nwith a corresponding segment of that video. An example\ngeneration video can be seen in Figure 1.\nPrior work on audio-guided video generation was mainly\nfocused on either global information in the videos (i.e., cap-\nturing the semantic class) or specific scenes (e.g., speech).\nMama et al. (2021); Park et al. (2022); Kumar et al. (2020)\narXiv:2309.16429v1  [cs.LG]  28 Sep 2023\ngenerate talking heads conditioned on speech, but these are\nlimited to videos of human faces and are conditioned on\nspeech and not natural audio. More closely related to our\nsetting, given an input video and an audio sample, Chatter-\njee and Cherian (2020) generate a continuation of the video\nthat is aligned with the audio. Our method, however, gen-\nerates videos from audio-only. Ge et al. (2022) proposed a\nmethod for generating aligned videos conditioned on audio.\nWhile impressive, generated videos are highly limited in di-\nversity. Other works such as (Chen et al. 2017; Hao, Guan,\nand Zhang 2022; Ruan et al. 2023) generate videos that are\nglobally aligned to the semantic class of the input audio sam-\nple (e.g., dancing, drums, etc.) but are unable to generate\nvideos in which every segment is temporally aligned to each\nsegment in the input audio sample.\nIn contrast to the above methods, our approach enables\nthe generation of diverse and realistic videos associated and\naligned with the input audio from a wide variety of seman-\ntic classes. Our work utilizes a pre-trained text-conditioned\nvideo generation engine and converts the input audio to a se-\nquence of pseudo tokens. Given an input audio sample, we\nfirst encode it using an audio encoder, producing a spectral\nrepresentation of the audio signal. To capture local-to-global\ninformation, we construct the representation considering the\ni-th segment as well as neighboring segments. In particular,\nwe use windows of varying sizes and average the embed-\ndings corresponding to audio segments in these windows.\nNext, to produce the N-th video frame, we divide the audio\nembedding into N consecutive segments. We then train an\nadapter network to map each of these segments to a set of\npseudo-tokens. Lastly, to produce the corresponding video,\nwe feed the output of the audio mapping module into the\npretrained text-to-video generation model.\nIntuitively, we learn a mapping between the audio repre-\nsentation obtained by the pre-trained audio encoder, to the\ntextual tokens\u2019 representation used for conditioning the pre-\ntrained text-to-video model. By that, extending the possi-\nble video conditioning to audio tokens. To validate our ap-\nproach, we consider a number of datasets that exhibit a di-\nverse set of videos and input audio samples. We consider the\nLandscape dataset (Lee et al. 2022), which captures land-\nscape videos. The AudioSet-Drums dataset (Gemmeke et al.\n2017) which captures drums videos, and the VGGSound\ndataset (Chen et al. 2020) which consists of a diverse set\nof real-world videos from 309 different semantic classes.\nWe compare our method to state-of-the-art approaches,\nboth in terms of objective evaluation and human study. We\nevaluate the audio-video alignment as well as video quality\nand diversity. To capture temporal alignment, we devise a\nnew metric based on detecting energy peaks in both modal-\nities separately and measuring their alignment. Further, we\nprovide an ablation study where we consider alternative ap-\nproaches to condition the video model.\nOur contributions: (i) A state-of-the-art audio-to-video\ngeneration model which captures diverse and naturally oc-\ncurring real-life settings from a wide variety of input videos\nof different semantic classes; (ii) We present a method that is\nbased on a lightweight adapter, which learns to map audio-\nbased tokens to pseudo-text tokens. As such, it also allows\nvideo generation conditioned on text, audio, or both text and\naudio. As far as we are aware, our method is the first to en-\nable video generation conditioned both on audio and text;\nand (iii) Our method can generate natural videos aligned\nwith the input sound, both globally and temporally. To vali-\ndate this, we present a novel evaluation function to measure\naudio-video alignment. Since, as far as we can ascertain, we\nare the first to generate diverse and natural videos guided by\naudio inputs, such an evaluation function is critical to mak-\ning progress in the field.\nRelated work\nAudio-to-image generation. Text-to-image generation has\nseen great advances recently, using either autoregressive\nmethods (Ramesh et al. 2021; Gafni et al. 2022; Yu et al.\n2022) or diffusion based models (Nichol et al. 2022; Ramesh\net al. 2022; Saharia et al. 2022; Rombach et al. 2022;\nRamesh et al. 2022; Rombach et al. 2022). This inspired\na new line of work concerning audio-to-image genera-\ntion. \u02d9Zelaszczyk and Ma\u00b4ndziuk (2022); Wan, Chuang, and\nLee (2019) proposed to generate images based on audio\nrecordings using a GAN. \u02d9Zelaszczyk and Ma\u00b4ndziuk (2022)\npresent results for generating MNIST digits only and did\nnot generalize to general audio sounds, while Wan, Chuang,\nand Lee (2019) generate images from general audio. In\nWav2Clip (Wu et al. 2022b), the authors learn a Contrastive\nLanguage-Image Pre-Training (CLIP) (Radford et al. 2021)\nlike a model for learning joint representation for audio-\nimage pairs. Later on, such representation can be used to\ngenerate images using VQ-GAN (Esser, Rombach, and Om-\nmer 2021) under the VQ-GAN CLIP (Crowson et al. 2022)\nframework. The most relevant related work to ours is Au-\ndioToken (Yariv et al. 2023), in which the authors learn an\naudio token while adapting a diffusion-based text-to-image\nmodel to generate images using audio inputs.\nText-to-video generation. Early attempts to establish a con-\nnection between text and video relied on conditioned re-\ntrieval methods (Ali et al. 2022). Later, Wu et al. (2021)\nintroduces the novel integration of 2D VQVAE and sparse\nattention in text-to-video generation, facilitating the gen-\neration of highly realistic scenes.\nWu et al. (2022a) ex-\ntends this method and presents a unified representation for\nvarious generation tasks in a multitask learning scheme.\nLater on, CogVideo (Hong et al. 2022) is built on top of\na frozen text-to-image model by adding additional tempo-\nral attention modules. Singer et al. (2022) further improves\ngeneration quality following a similar modeling paradigm.\nVideo Diffusion Models (He et al. 2022) uses a space-time\nfactorized U-Net with joint image and video data training.\nOther approaches, such as Villegas et al. (2022) and Ville-\ngas et al. (2022) and Yu et al. (2023) proposed transformer-\nbased approaches to generate long videos or for multi-task-\nlearning. The most relevant prior work to ours is Wang\net al. (2023b), which proposed ModelScope. ModelScope is\na latent diffusion-based text-to-video generation model with\nspatiotemporal blocks. By that, ModelScope enables consis-\ntent frame generation and smooth movement transitions.\nAudio-to-video generation models can be roughly divided\ninto two: (i) speech-to-video generation (talking heads); and\nVideo\nGenerator\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nAudioMapper\nBEATs\nMLP\n.\n.\n.\n.\n.\n.\n.\n.\n.\nAudio-conditioned temporal sequence\nFigure 2: An illustration of the proposed model architecture and method. The input audio is first passed through a pre-trained\naudio encoder model (BEATs). Then, the resulting representations are fed into a trainable MLP layer, establishing a mapping\nbetween audio and text tokens. These text-based representations are then used to condition each frame via a temporal audio-\nconditioned sequence. This sequence effectively takes into account both local and global audio segments. Furthermore, an\nattentive token (\u02dcaatten) is included to learn the identification of significant audio signals using a pooling attention layer. Lastly,\nthe conditioned components are utilized to generate frames through a pre-trained video generator. Notably, optimization is only\napplied to the MLP within the AudioMapper model and the pooling attention module.\n(ii) general audio-to-video. Under the speech-to-video gen-\neration, Mama et al. (2021) proposed learning a discrete la-\ntent representation of the video signal using VQ-VAE, which\nwill be later modeled via an auto-encoder conditioned on\nspeech spectrogram. Park et al. (2022) generates talking\nface focusing a piece of phonetic information via Audio-Lip\nMemory module, while Kumar et al. (2020) proposed a one-\nshot approach for fast speaker adaptation.\nWhen considering general audio-to-video generation,\nChatterjee and Cherian (2020) first proposed a method of\ngenerating aligned videos conditioned on both audio and\nvideo prompts. Ge et al. (2022) introduced a transformer-\nbased approach for generating videos conditioned on either\naudio or textual features. Although providing impressive\ngenerations, their videos are not diverse and were demon-\nstrated on drum generation only. Chen et al. (2017) suggest\nusing separate frameworks for audio-to-image and image-\nto-audio generation. Hao, Guan, and Zhang (2022) also\nsuggest modeling both audio-to-image and image-to-audio\nusing bidirectional transformers, however, using a unified\nframework. The authors prove it is better than two separate\nones. Lastly, Ruan et al. (2023), follows the same modeling\nparadigm, however, using latent diffusion models.\nMethod\nThe proposed method is composed of three main compo-\nnents: (i) an AudioMapper; (ii) multiple audio-conditioned\ntemporal sequences; and (iii) a text-to-video generation\nmodule. As our goal in this study is to enrich video gen-\neration models using audio inputs, we leverage a pre-trained\ndiffusion-based text-to-video model and augment it with au-\ndio conditioning capabilities. A visual description of the pro-\nposed method can be seen in Figure 2.\nIn contrast to converting audio to image, transforming au-\ndio to video presents two additional challenges: (i) ensuring\nthe creation of coherent frames and (ii) synchronization be-\ntween the audio and video components. For example, con-\nsider the scenario of having an audio recording of a dog\nbarking. In the resulting video, it is crucial not only for the\ndog\u2019s appearance to remain consistent across all frames but\nalso for the match between the timing of the barking sound\nand the dog\u2019s motion. In this work, we focus on item (ii) by\ntemporally conditioning the generation of each of the video\nframes by a contextualized representation of the input audio.\nFormally, we are interested in the generation of a video,\ndenoted as v = (v(1), . . . , v(L)), where v(i) \u2208 R3\u00d7H\u00d7W\nis an output frame, driven by a corresponding audio con-\ndition a = (a1, . . . , aR), where ai \u2208 [\u22121, 1] is an audio\nsample at a given sampling rate in the time domain. We seek\nto establish a conditional probabilistic model, p\u03b8(v|a), en-\ncompassing the entire frame-set, where each frame v(i) is\nconditioned on a, which denotes the audio condition.\nNote that the conditioning of each frame considers the en-\ntire audio input but is built differently for each frame. More\ndetails can be found in the paragraph on Audio-conditioned\ntemporal sequence.\nAudioMapper maps the audio representation obtained from\na pre-trained audio encoder to pseudo-tokens compatible\nwith the pre-trained text-to-video model. We denote the out-\nput of the AudioMapper as TEMPOTOKENS.\nFormally, the model gets as input embedded audio, which\noriginates from a pre-trained audio encoder h : [\u22121, 1]R \u2192\nRR\u2032\u00d7H\u00d7d, where H is the number of layers the represen-\ntation is collected from, d is the inner dimension of the en-\ncoder, and R\u2032 is the segment length that h operates on. To\nforce both audio and video latent representations to have the\nsame dimension, we fix R\u2032 = L by employing a pooling\nlayer. Specifically, we use the BEATs model (Chen et al.\n2022) as the audio encoder h. Different layers encapsulate\na range of specificity levels. Representations derived from\nBEATs\u2019 final layers are strongly tied to class-related at-\ntributes, whereas earlier layers encompass low-level audio\nfeatures (Gat et al. 2022; Adi et al. 2019). We embed an au-\ndio segment into a token representation using a non-linear\nneural network g : RL\u00d7H\u00d7d \u2192 RL\u00d7H\u00d7dt:\n\u02dca(i) = g\n\u0010\nh(a)(i)\u0011\n,\n(1)\nwhere \u02dca(i) \u2208 RL\u00d7H\u00d7dt, and dt is the embedding dimen-\nsion of the text-conditioned tokens of the video generation\nprocess. The network g consists of four sequential linear lay-\ners with GELU non-linearity between them. We denote \u02dca(i)\nas TEMPOTOKENS. Subsequently, we generate a temporal\nconditioning sequence for each video frame using TEMPO-\nTOKENS. We provide a detailed description of the process\nin the following paragraph.\nAudio-conditioned temporal sequence. Next, to better\ncapture the local context around each video frame, we apply\nan expanding context window technique over the obtained\nTEMPOTOKENS. This approach captures the surrounding\nsound signals of the i-th frame as follows:\nc(i) =\n\u0000\u02dcamax(1,i\u2212j),min(i+j,K) | j = 2k\u0001log K\nk=0 ,\n(2)\nwhere\n\u02dcal,r =\n1\nr \u2212 l\nr\nX\ns=l\n\u02dca(s).\n(3)\nThis context window expands exponentially with increasing\ntemporal distance from the target position, facilitating con-\nsideration of a wider local-to-global audio context range.\nThe exponential expansion effectively balances local and\nglobal contexts, encompassing important distant audio com-\nponents that can provide valuable insights into the audio\nclass and close temporal changes needed for audio-video\nalignment. Figure 3 visually describes the audio-conditioned\ntemporal sequence. Finally, we consider a context window\nthat encompasses all audio signals. We substitute average\noperation with a trainable attentive pooling layer (Schwartz\net al. 2019). Thus,\n\u02dcaatten =\nL\nX\nu=1\np(u)\u02dca(u),\n(4)\nwhere p(u)\u22650 \u2200u is a probability distribution (i.e.,\nPL\nu=1 p(u) = 1) over the audio components. The proba-\nbility distribution takes the form:\np(u) \u221d exp (\u03b1l\u03b8l(u) + \u03b1c\u03b8c(u)) .\n(5)\nThe local potential is \u03b8l(u) = v\u22a4\nl relu(Vlau), and the cross\npotential between the audio components is:\n\u03b8c(u) =\nL\nX\ni=1\n \u0012 W1\u02dca(u)\n||W1\u02dca(u)||\n\u0013\u22a4 \u0012 W2\u02dca(i)\n||W2\u02dca(i)||\n\u0013!\n.\n(6)\nThe trainable parameters are (i) Vl, W1, W2, which re-\nembed the data to tune the attention, (ii) vl \u2208 R(L\u00b7H\u00b7dt)\u00d71\nthat scores the sound component (iii) \u03b1l, \u03b1c that calibrates\nthe local and cross potentials. The attention mechanism en-\nables learning the significance of the audio components.\n...\n...\n...\n...\n...\n...\nFigure 3: Illustration of the audio-conditioned temporal se-\nquence for the case of 24 audio components. For the i-th\nframe, the window sizes grow exponentially, considering lo-\ncal audio details to aid in aligning audio and video, as well as\nthe broader global information that enhances the differentia-\ntion of video classes. Additionally, we introduce a token that\nencompasses all audio components and identifies the signif-\nicant ones through an attention pooling layer (\u02dcaatten).\nText-to-video. Lastly, we leverage a pre-trained latent dif-\nfusion text-to-video model to learn the aforementioned tem-\nporal audio tokens, c = {c(i)}L\ni=1.\nDiffusion models are a family of generative models de-\nsigned to learn the data distribution p(x). This is done by\nlearning the reverse Markov process of length T. Given a\ntimestamp t \u2208 [0, 1], the denoising function \u03f5\u03b8 : Rd \u2192 Rd\nlearns to predict a clean version of the perturbed xt from\nthe training distribution. The generative process can be con-\nditioned on a given input, i.e., modeling p(x|y) where y\nis a condition vector. In that case, the objective function is\nLCLDM \u225c,\nE(v,a)\u223cS,t\u223cU(0,1),\u03f5\u223cN (0,I)\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(f (vt, c) , t)\u22252\n2\ni\n,\n(7)\nwhere each video frame, v(i), is conditioned on a dedicated\ncondition vector c(i).\nSpecifically, in this work, we set \u03f5\u03b8 to be a state-of-the-\nart text-to-video model, ModelScope, which is comprised\nof a 3D-UNet integrated with a temporal attention layer as\noutlined in Wang et al. (2023b). ModelScope was trained\non \u223c10M text-video pairs and \u223c2B text-image pairs (Wang\net al. 2023b). Notice the proposed framework is not limited\nto ModelScope and can be used over any differentiable text-\nto-video model.\nModel optimization. We optimize the AudioMapper and\nthe attentive pooling layer only and backpropagate gradi-\nents through \u03f5\u03b8 while keeping its parameters unchanged.\nOptimization minimizes the loss LCLDM for reconstructing\na frame v(i) conditioned on c(i) (see Equation (7)), with an\nadded weight decay regularization for the encoded TEMPO-\nTOKENS. Overall, we optimize the following loss function:\nL = LCLDM + \u03bbl1\nL\nlog L\nX\nu=1\n\u02dca(u),\n(8)\nwhere \u03bbl1 is a trade-off hyper-parameter between the loss\nterm and the regularization.\nEvaluation metrics\nWe evaluate our method on three main axes: video quality\nand diversity, audio-video alignment, and a human study.\nVideo quality and diversity. We report standard evaluation\nmetrics in the domain of video generation for assessing qual-\nity and diversity. We utilize the following metrics: (i) Frechet\nVideo Distance (FVD) metric, which quantifies the visual\ndisparity between feature embeddings extracted from gen-\nerated and reference videos (Unterthiner et al. 2019) and is\nused to assess quality and diversity; (ii) Inception Score (IS),\nwhich is computed with a trained C3D model (Tran et al.\n(2015)) on UCF-101 (Soomro, Zamir, and Shah 2012) and\nassesses video quality.\nAudio-video alignment. We distinguish between two types\nof audio-video alignment: (i) Semantic (or global) align-\nment, in which the semantic class (e.g., playing drums) of\nthe input audio is depicted by the output video (e.g., a video\nof people playing drums). To this end, we consider the CLIP\nSimilarity (CLIPSIM) metric (Wu et al. 2021), which gauges\nthe alignment between generated video content and its corre-\nsponding audio label; (ii) Temporal alignment, in which we\nconsider if each input audio segment is synchronized with\nits corresponding generated video segment. To measure this\ntype of alignment, we introduce a novel evaluation metric.\nThe new metric is based on detecting energy peaks in\nboth modalities separately and measuring their alignment.\nThe premise behind this metric is that fast temporal energy\nchanges in the audio signal often correspond to an object\nmovement producing this sound. For instance, consider an\naudio waveform of fireworks. A successful audio-video tem-\nporal alignment would ensure that the video frames portray-\ning the fireworks exhibit a noticeable change synchronously.\nConversely, when the video exhibits a significant change, a\ncorresponding peak should be observed in the audio wave-\nform at that precise moment.\nOur audio-video alignment metric operates as follows.\nWe first detect candidate alignment points by considering\neach modality separately. We detect audio peaks using an\nOnset Detection algorithm (B\u00a8ock and Widmer 2013), pin-\npointing instances of heightened auditory intensity. To detect\nthe changes within the video, we calculate the mean of the\nOptical Flow (Horn and Schunck 1981) magnitude for each\nframe and identify rapid changes over time. Then, for each\npeak in one modality, we validate whether a pick was also\ndetected in the other modality within a three-frame temporal\nwindow and vise-verse.\nFinally, we normalize by the number of peaks to derive the\nalignment score ranging between zero and one. Such a met-\nric reflects the model\u2019s proficiency in synchronizing audio\nand video. More formally, given A and V, audio and video\npeaks were obtained from the onset detection algorithms and\noptical flow, respectively. The alignment score is defined as:\nAV-Align =\n1\n2|A \u222a V|\n X\na\u2208A\n1[a \u2208 V] +\nX\nv\u2208V\n1[v \u2208 A]\n!\n,\n(9)\nwhere we consider a valid peak if placed within a window\nof three frames in the other modality. The above metric can\nbe interpreted as the Intersection-over-Union metric.\nTo facilitate comprehension, Figure 4 illustrates the align-\nment process visually, depicting audio peaks and corre-\nsponding video changes, emphasizing the interplay between\nthe auditory and visual domains.\nMagnitude\nAmplitude\nTime (s)\n0.5\n1\n1.5\nFigure 4: Audio-Video alignment metric illustration. The\nfirst row presents four frames from a generated video fea-\nturing a dog. The second row depicts the mean magnitude\nof optical flow for each frame, capturing video changes. The\nbottom row shows the amplitude of the audio waveform. The\nvertical line in the middle and the bottom graphs marks the\nonset of the waveform, while the peak of video change is\nalso indicated.\nHuman study. We perform Mean Opinion Scores (MOS)\nexperiments considering both quality and audio-video align-\nment. In this setup, human raters are presented with several\nshort video samples and are instructed to evaluate their qual-\nity and alignment on a scale between 1\u20135 with increments\nof 1.0. Specifically, we ask raters to evaluate the videos con-\nsidering overall quality, global alignment to the audio file,\nand local alignment between the visual and sound of the\nvideo files. We evaluate 20 videos per method and enforce\nten raters per sample. The full questionnaire we asked the\nraters can be found in the supplemental material.\nExperimental setup\nImplementation details. The proposed method contains\n\u223c35M trainable parameters. We optimized the model us-\ning two A6000 GPUs for 10K iterations. We use AdamW\noptimizer with learning rate of 1e-05 using constant learn-\ning rate scheduler. Each batch comprises 8 videos with 24\nframes per video, sampled randomly for one-second granu-\nlarity. To enhance training efficiency and mitigate memory\nconsumption, we integrated gradient checkpointing into the\ntraining process of the 3D U-net architecture. Code and pre-\ntrained models will be publicly available upon acceptance.\nDatasets. We utilize the VGGSound dataset (Chen et al.\n2020), derived from YouTube videos containing \u223c180K\nclips of 10 seconds duration, annotated across 309 classes.\nTo enhance data quality, we filtered \u223c60K videos in which\naudio-video alignment is weak. During this filtering proce-\ndure, we utilized a pre-trained audio classifier to categorize\nsound events present in each clip. Simultaneously, a pre-\ntrained image classifier was employed to classify the middle\nframe of every video clip. We then computed the CLIP (Rad-\nModel\nFVD (\u2193) CLIPSIM (\u2191) IS (\u2191) AV-Align (\u2191)\nVGGSound\nModelScope Text2Vid\n801\n0.69\n15.55\n0.27\nModelScope Random\n1023\n0.47\n6.32\n0.26\nOurs\n923\n0.57\n11.04\n0.35\nAudioSet-Drums\nTATS\n303\n0.69\n2.10\n0.28\nOurs\n299\n0.70\n2.78\n0.61\nLandscape\nMM-Diffusion\n922\n0.53\n2.85\n0.41\nOurs\n784\n0.57\n4.49\n0.54\nTable 1: Automatic video generation results. We report\nFVD, CLIPSIM, IS, and Alignment (\u2018align\u2019) scores for both\nthe proposed method (Ours) and the baselines. For a fair\ncomparison, we compare our method to TATS (Ge et al.\n2022) and to MM-Diffusion (Ruan et al. 2023) using the\nbenchmarks reported by the authors in the original paper.\nford et al. 2021) score by comparing the predicted labels\nfrom both classifiers. Then, filtering is done by removing\nvideos that do not pass a pre-defined threshold.\nAdditionally, to have a fair comparison with prior work,\nwe experimented with two additional datasets. (i) The Land-\nscape dataset (Lee et al. 2022), which contains 928 nature\nvideos divided into 10-second clips, covering nine distinct\nscenes; (ii) The AudioSet-Drum dataset (Gemmeke et al.\n2017), contains \u223c7k videos of drumming. We used the same\nsplit as proposed by Ge et al. (2022), where \u223c6k is used as\nthe training set while the rest serves as a test set.\nBaselines. We compare the proposed method to previous\nstate-of-the-art models generating videos conditioned on au-\ndio inputs. Ge et al. (2022) proposed Time Sensitive Trans-\nformer (TATS) model, which projects audio latent embed-\ndings onto video embeddings, enabling cross-modal align-\nment. Ruan et al. (2023) recently proposed MM-Diffusion,\nwhich employs coupled denoising auto-encoders to generate\njoint audio and video content. Each of the above-mentioned\nbaselines, i.e., TATS and MM-Diffusion, were originally\nevaluated using different benchmarks, i.e., AudioSet-Drums\nand Landscape, respectively. For a fair comparison, we eval-\nuate the proposed method using each of the datasets sug-\ngested in the original papers.\nMoreover, we consider two naive baselines based on text-\nto-video models. In the first one, we generate videos from\ntext description and retrieve random audio from the train-\ning set which corresponds to the same class as the gener-\nated video, denoted as ModelScope Text-To-Video. For the\nsecond one, denoted as ModelScope Random, we gener-\nate videos unconditionally (i.e., without any specific tex-\ntual conditions), and match it with a random audio segment.\nFor both baselines, we use the pre-trained publicly available\nzeroscope-v2 model 1.\n1we use the zeroscope-v2 576w as can be found in the following\nlink: https://huggingface.co/cerspense/zeroscope v2 576w\nSemantic Alignment Temporal Alignment\nVideo Quality\n0\n1\n2\n3\n4\n5\nScores\nAudioSet-Drum\nTATS\nOurs\nSemantic Alignment Temporal Alignment\nVideo Quality\n0\n1\n2\n3\n4\n5\nScores\nLandscape\nMM-Diffusion\nOurs\nFigure 5: Human study. We consider the MOS score for\nthree metrics: (i). Semantic alignment, where we ask users\nto rate how well the video matches the input audio semantic\nlabel, (ii). Temporal alignment, where we ask users to rate\nhow well each input audio segment is aligned with the gen-\nerated video segments, and (iii) Video quality, where we ask\nusers to rate the generated video quality. On the LHS, we\nconsider video models trained on AudioSet-Drum, and on\nthe RHS, we consider video models trained on Landscape.\nResults\nWe start by presenting results for audio-to-video generation\nconsidering both objective metrics presented above and hu-\nman study. Next, we empirically demonstrate how the pro-\nposed method can be used to generate videos conditioned on\nboth text and audio modalities, thus enhancing text-to-video\ngenerations. Lastly, we conduct an ablation study to under-\nstand better the effect of our audio conditioning technique\non generation quality and alignment. Visual results are pro-\nvided in the supplementary.\nAudio-to-video generation\nObjective evaluation. As can be seen in Tab. 1, our method\noutperforms the baselines on all metrics for the AudioSet-\nDrums and Landscape datasets. Specifically, our method im-\nproves both the quality of the generated videos (FVD and IS\nscores) together with the audio-video alignment (AV-Align\nand CLIPSIM scores). As expected, the gap between the\nmethods is larger when considering the alignment scores.\nNotice the alignment scores changed significantly when\nconsidering different benchmarks. Sound events can also be\nproduced by objects not seen in the video; this is especially\nnoticeable in the VGGSound benchmark, in which the AV-\nAlign score of the original videos is 0.51.\nNext, we compare our method to the original ModelScope\nmodel, both text-condition (ModelScope Text2Vid) and un-\nconditionally (ModelScope Random). As we do not mod-\nify the model, we consider the text-condition setup as a\ntop-line in terms of video quality metrics. Recall the au-\ndio in both models is retrieved from our training set, us-\ning either the video class for ModelScope Text2Vid or ran-\ndomly ModelScope Random. As expected, our model out-\nperforms ModelScope Random considering all metrics. The\nModelScope Text2Vid is superior to our model for video\nquality. However, when considering audio-video alignment,\nour method is significantly better.\nCond.\nFVD (\u2193)\nCLIPSIM (\u2191)\nIS (\u2191)\nAV-Align (\u2191)\nText\n801\n0.69\n15.55\n0.27\nAudio\n923\n0.57\n11.04\n0.35\nText+Audio\n859\n0.58\n11.66\n0.36\nTable 2: Results of the proposed method using different\nmodalities as conditioning. We report results for Text, Au-\ndio, and Text+Audio modalities as model conditioning.\n\"A video of <TemporalAudioTokens>,\non the moon\"\n\"A video of <TemporalAudioTokens>, with\nvibrant red and orange foliage\"\n\"a video of\u00a0<TemporalAudioTokens>\u00a0in Abstract\nColors\"\nFire\nWater\nFigure 6: Examples of added text tokens for altering the out-\nput video. We show results for fire and flowing water audio.\nHuman study. We present results using a human study\nconsidering both video quality and alignment (both seman-\ntic and temporal). Results are depicted in Figure 5. As\ncan be seen for both the AudioSet-Drum and Landscape\ndatasets, users found our videos significantly more tempo-\nrally aligned. For semantic alignment, our method improves\non both TATS and MM-Diffusion, with a significant gap to\nMM-Diffusion on the Landscape dataset. Finally, on video\nquality, users found our videos significantly superior.\nJoint audio-text to video generation\nUtilizing text and audio together to guide generation in-\nvolves adding text tokens for conditioning. In Tab. 2, we\nshow results using \u201cA video of <class>\u201d for text condition-\ning and \u201cA video of <TemporalAudioAtokens> <class>\u201d\nfor Text+Audio. Combining text and audio conditioning out-\nperforms audio-only in all metrics, especially FVD. Text-\nonly provides the highest video quality but lacks alignment.\nIn Fig. 6, we present how we merge text tokens to tem-\nporal audio tokens, which enables style manipulation. For\nexample, for the sound of a river, we can depict it flowing\nover the moon by using the prompt \u201con the moon\u201d.\nAblation study\nRecall our method consists of using context windows of\nvarying sizes to capture a local-to-global context of the in-\nput audio. In Tab. 3, we assess the effect of using different\nwindows of size K \u2208 {1, 2, 3, 4} denoted as win. (K-res.).\nNote in practice, the window size is determined by log K;\nwe use K for readability. Using only the local context win-\ndow (K = 1) results in a good alignment. As we increase\nthe global context (i.e., increasing K), the video quality is\nimproved while the alignment scores are comparable.\nWe additionally consider a single audio conditioning vec-\ntor (vec) by averaging all the audio components. Despite\nCond.\nFVD (\u2193)\nCLIPSIM (\u2191)\nIS (\u2191)\nAV-Align (\u2191)\nvec.\n948\n0.57\n10.12\n0.29\nwin. (1-res.)\n998\n0.56\n9.22\n0.36\nwin. (2-res.)\n965\n0.56\n9.87\n0.35\nwin. (3-res.)\n972\n0.56\n10.01\n0.34\nwin. (4-res.)\n950\n0.56\n10.13\n0.35\nwin. (5-res.)\n923\n0.57\n11.04\n0.35\nTable 3: An ablation study exploring the different audio con-\nditioning. We report FVD, CLIPSIM, IS, and Alignment\nscores on VGGSound (Chen et al. 2020) considering single-\nvector conditioning (vec.), time-dependent condition using\none window size (win. (1-res.), and different windows of\nsize k (win. (k-res.)).\nhigh video quality scores, the absence of local temporal in-\nformation results in a notably worse AV-Align score.\nLimitations\nAs the proposed method leverages a pre-trained text-to-\nvideo model, the adaptation process between text to au-\ndio tokens requires mapping between both latent representa-\ntions. As both modalities operate at different levels of gran-\nularity, it is unclear whether such mapping holds all the rel-\nevant information in the audio space. Moreover, at the mo-\nment, our method generates relatively short video segments\nsince the temporal conditioning is limited to 24 frames due\nto hardware limitations.\nLastly, while audio can indeed convey information about\na visual scene, discrepancies can also arise between the two\nmodalities. For example, a video might depict a dog in a car\nwhile the accompanying audio only features a radio play-\ning. This disparity is particularly noticeable in the context of\nshorter videos. Such mismatch imposes a general limitation\nof the domain at large, not specifically to our method.\nConclusion\nIn this work, we introduced a state-of-the-art audio-to-video\ngeneration model capable of generating diverse and real-\nistic videos aligned to input audio samples. By learning a\nlightweight adapter to map between the input audio repre-\nsentation and a text-based representation, video generation\ncan be conditioned not only on audio but also on text, en-\nabling, for the first time, the generation of audio aligned to\nboth input audio and text samples. To better capture both lo-\ncal and global context around each frame, we consider an\nexpanding context window technique. We validate our ap-\nproach extensively, demonstrating significant semantic di-\nversity of audio-video samples, and further propose a novel\nevaluation metric (AV-Align) to assess the temporal align-\nment of the input audio and generated video. For future\nwork, we are excited to explore how further modalities, such\nas depth, images, or IMU can be used jointly with audio and\ntext as further conditions by which video can be generated.\nReferences\nAdi, Y.; Zeghidour, N.; Collobert, R.; Usunier, N.; Liptchin-\nsky, V.; and Synnaeve, G. 2019. To reverse the gradient or\nnot: An empirical comparison of adversarial and multi-task\nlearning in speech recognition. In ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 3742\u20133746. IEEE.\nAli, A.; Schwartz, I.; Hazan, T.; and Wolf, L. 2022. Video\nand text matching with conditioned embeddings. In Pro-\nceedings of the IEEE/CVF winter conference on applica-\ntions of computer vision, 1565\u20131574.\nB\u00a8ock, S.; and Widmer, G. 2013. Maximum Filter Vibrato\nSuppression for Onset Detection. In DAFx-13.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020.\nLanguage models are few-shot learners.\nNeurIPS.\nChatterjee, M.; and Cherian, A. 2020. Sound2sight: Gener-\nating visual dynamics from sound and context. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XXVII 16, 701\u2013\n719. Springer.\nChen, H.; Xie, W.; Vedaldi, A.; and Zisserman, A. 2020. Vg-\ngsound: A large-scale audio-visual dataset. In ICASSP.\nChen, L.; Srivastava, S.; Duan, Z.; and Xu, C. 2017. Deep\ncross-modal audio-visual generation. In Proceedings of the\non Thematic Workshops of ACM Multimedia 2017, 349\u2013357.\nChen, S.; Wu, Y.; Wang, C.; Liu, S.; Tompkins, D.; Chen, Z.;\nand Wei, F. 2022. Beats: Audio pre-training with acoustic\ntokenizers. arXiv preprint arXiv:2212.09058.\nCopet, J.; Kreuk, F.; Gat, I.; Remez, T.; Kant, D.; Synnaeve,\nG.; Adi, Y.; and D\u00b4efossez, A. 2023. Simple and Controllable\nMusic Generation. arXiv preprint arXiv:2306.05284.\nCrowson, K.; Biderman, S.; Kornis, D.; Stander, D.; Halla-\nhan, E.; Castricato, L.; and Raff, E. 2022. Vqgan-clip: Open\ndomain image generation and editing with natural language\nguidance. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceed-\nings, Part XXXVII, 88\u2013105. Springer.\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\nformers for high-resolution image synthesis. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 12873\u201312883.\nGafni, O.; Polyak, A.; Ashual, O.; Sheynin, S.; Parikh, D.;\nand Taigman, Y. 2022. Make-a-scene: Scene-based text-to-\nimage generation with human priors. In European Confer-\nence on Computer Vision, 89\u2013106. Springer.\nGat, I.; Lorberbom, G.; Schwartz, I.; and Hazan, T. 2022.\nLatent space explanation by intervention. In AAAI.\nGe, S.; Hayes, T.; Yang, H.; Yin, X.; Pang, G.; Jacobs,\nD.; Huang, J.-B.; and Parikh, D. 2022.\nLong video gen-\neration with time-agnostic vqgan and time-sensitive trans-\nformer. In European Conference on Computer Vision, 102\u2013\n118. Springer.\nGemmeke, J. F.; Ellis, D. P.; Freedman, D.; Jansen, A.;\nLawrence, W.; Moore, R. C.; Plakal, M.; and Ritter, M.\n2017. Audio set: An ontology and human-labeled dataset\nfor audio events. In 2017 IEEE international conference on\nacoustics, speech and signal processing (ICASSP), 776\u2013780.\nIEEE.\nHao, W.; Guan, H.; and Zhang, Z. 2022.\nVag: A uni-\nform model for cross-modal visual-audio mutual generation.\nIEEE Transactions on Neural Networks and Learning Sys-\ntems.\nHassid, M.; Remez, T.; Nguyen, T. A.; Gat, I.; Conneau, A.;\nKreuk, F.; Copet, J.; Defossez, A.; Synnaeve, G.; Dupoux,\nE.; et al. 2023. Textually Pretrained Speech Language Mod-\nels. arXiv preprint arXiv:2305.13009.\nHe, Y.; Yang, T.; Zhang, Y.; Shan, Y.; and Chen, Q.\n2022.\nLatent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221.\nHo, J.; Chan, W.; Saharia, C.; Whang, J.; Gao, R.; Gritsenko,\nA.; Kingma, D. P.; Poole, B.; Norouzi, M.; Fleet, D. J.; et al.\n2022. Imagen video: High definition video generation with\ndiffusion models. arXiv preprint arXiv:2210.02303.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. NeurIPS.\nHong, W.; Ding, M.; Zheng, W.; Liu, X.; and Tang, J. 2022.\nCogvideo: Large-scale pretraining for text-to-video genera-\ntion via transformers. arXiv preprint arXiv:2205.15868.\nHorn, B. K.; and Schunck, B. G. 1981. Determining optical\nflow. Artificial Intelligence, 17(1): 185\u2013203.\nKreuk, F.; Synnaeve, G.; Polyak, A.; Singer, U.; D\u00b4efossez,\nA.; Copet, J.; Parikh, D.; Taigman, Y.; and Adi, Y. 2022. Au-\ndiogen: Textually guided audio generation. arXiv preprint\narXiv:2209.15352.\nKumar, N.; Goel, S.; Narang, A.; and Hasan, M. 2020. Ro-\nbust one shot audio to video generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, 770\u2013771.\nLee, S. H.; Oh, G.; Byeon, W.; Kim, C.; Ryoo, W. J.; Yoon,\nS. H.; Cho, H.; Bae, J.; Kim, J.; and Kim, S. 2022. Sound-\nguided semantic video generation. In European Conference\non Computer Vision, 34\u201350. Springer.\nMama, R.; Tyndel, M. S.; Kadhim, H.; Clifford, C.; and Thu-\nrairatnam, R. 2021. NWT: towards natural audio-to-video\ngeneration with representation learning.\narXiv preprint\narXiv:2106.04283.\nNichol, A. Q.; Dhariwal, P.; Ramesh, A.; Shyam, P.;\nMishkin, P.; Mcgrew, B.; Sutskever, I.; and Chen, M. 2022.\nGLIDE: Towards Photorealistic Image Generation and Edit-\ning with Text-Guided Diffusion Models. In International\nConference on Machine Learning, 16784\u201316804. PMLR.\nPark, S. J.; Kim, M.; Hong, J.; Choi, J.; and Ro, Y. M.\n2022.\nSynctalkface: Talking face generation with pre-\ncise lip-syncing via audio-lip memory. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 36,\n2062\u20132070.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. In ICML.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M.\n2022. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-\nimage generation. In International Conference on Machine\nLearning, 8821\u20138831. PMLR.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 10684\u2013\n10695.\nRuan, L.; Ma, Y.; Yang, H.; He, H.; Liu, B.; Fu, J.; Yuan,\nN. J.; Jin, Q.; and Guo, B. 2023. Mm-diffusion: Learning\nmulti-modal diffusion models for joint audio and video gen-\neration.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 10219\u201310228.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Den-\nton, E.; Ghasemipour, S. K. S.; Gontijo-Lopes, R.; Ayan,\nB. K.; Salimans, T.; et al. 2022. Photorealistic Text-to-Image\nDiffusion Models with Deep Language Understanding. In\nNeurIPS.\nSchwartz, I.; Yu, S.; Hazan, T.; and Schwing, A. G. 2019.\nFactor graph attention.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2039\u20132048.\nSheffer, R.; and Adi, Y. 2023. I hear your true colors: Im-\nage guided audio generation. In ICASSP 2023-2023 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 1\u20135. IEEE.\nSinger, U.; Polyak, A.; Hayes, T.; Yin, X.; An, J.; Zhang,\nS.; Hu, Q.; Yang, H.; Ashual, O.; Gafni, O.; et al. 2022.\nMake-a-video: Text-to-video generation without text-video\ndata. arXiv preprint arXiv:2209.14792.\nSoomro, K.; Zamir, A. R.; and Shah, M. 2012. UCF101:\nA dataset of 101 human actions classes from videos in the\nwild. arXiv preprint arXiv:1212.0402.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023a. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023b. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; and Paluri,\nM. 2015. Learning Spatiotemporal Features with 3D Con-\nvolutional Networks. arXiv:1412.0767.\nUnterthiner, T.; van Steenkiste, S.; Kurach, K.; Marinier,\nR.; Michalski, M.; and Gelly, S. 2019. Towards Accurate\nGenerative Models of Video: A New Metric Challenges.\narXiv:1812.01717.\nVillegas, R.; Babaeizadeh, M.; Kindermans, P.-J.; Moraldo,\nH.; Zhang, H.; Saffar, M. T.; Castro, S.; Kunze, J.; and\nErhan, D. 2022.\nPhenaki: Variable length video genera-\ntion from open domain textual description. arXiv preprint\narXiv:2210.02399.\nWan, C.-H.; Chuang, S.-P.; and Lee, H.-Y. 2019. Towards\naudio to scene image synthesis using generative adversarial\nnetwork. In ICASSP 2019-2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\n496\u2013500. IEEE.\nWang, C.; Chen, S.; Wu, Y.; Zhang, Z.; Zhou, L.; Liu, S.;\nChen, Z.; Liu, Y.; Wang, H.; Li, J.; et al. 2023a. Neural codec\nlanguage models are zero-shot text to speech synthesizers.\narXiv preprint arXiv:2301.02111.\nWang, J.; Yuan, H.; Chen, D.; Zhang, Y.; Wang, X.; and\nZhang, S. 2023b. ModelScope Text-to-Video Technical Re-\nport. arXiv:2308.06571.\nWu, C.; Huang, L.; Zhang, Q.; Li, B.; Ji, L.; Yang, F.;\nSapiro, G.; and Duan, N. 2021. Godiva: Generating open-\ndomain videos from natural descriptions.\narXiv preprint\narXiv:2104.14806.\nWu, C.; Liang, J.; Ji, L.; Yang, F.; Fang, Y.; Jiang, D.; and\nDuan, N. 2022a. N\u00a8uwa: Visual synthesis pre-training for\nneural visual world creation.\nIn European conference on\ncomputer vision, 720\u2013736. Springer.\nWu, H.-H.; Seetharaman, P.; Kumar, K.; and Bello, J. P.\n2022b.\nWav2clip: Learning robust audio representations\nfrom clip.\nIn ICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), 4563\u20134567. IEEE.\nYariv, G.; Gat, I.; Wolf, L.; Adi, Y.; and Schwartz, I.\n2023. AudioToken: Adaptation of Text-Conditioned Diffu-\nsion Models for Audio-to-Image Generation. arXiv preprint\narXiv:2305.13050.\nYu, J.; Xu, Y.; Koh, J. Y.; Luong, T.; Baid, G.; Wang, Z.;\nVasudevan, V.; Ku, A.; Yang, Y.; Ayan, B. K.; et al. 2022.\nScaling autoregressive models for content-rich text-to-image\ngeneration. arXiv preprint arXiv:2206.10789, 2(3): 5.\nYu, L.; Cheng, Y.; Sohn, K.; Lezama, J.; Zhang, H.; Chang,\nH.; Hauptmann, A. G.; Yang, M.-H.; Hao, Y.; Essa, I.; et al.\n2023. Magvit: Masked generative video transformer. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 10459\u201310469.\n\u02d9Zelaszczyk, M.; and Ma\u00b4ndziuk, J. 2022.\nAudio-to-image\ncross-modal generation. In IJCNN.\n"
  },
  {
    "title": "Language models in molecular discovery",
    "link": "https://arxiv.org/pdf/2309.16235.pdf",
    "upvote": "10",
    "text": "Language models in molecular discovery\nNikita Janakarajan1, Tim Erdmann2, Sarath Swaminathan2, Teodoro Laino1, and\nJannis Born1*\n1IBM Research Europe, Zurich, Switzerland\n2IBM Research Almaden, San Jose, CA, United States\n*Corresponding author: jab@zurich.ibm.com\nSeptember 29, 2023\nAbstract\nThe success of language models, especially transformer-based architectures, has trickled into\nother domains giving rise to \u201dscientific language models\u201d that operate on small molecules, proteins\nor polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle\nas evidenced by promising recent findings in early-stage drug discovery. Here, we review the role\nof language models in molecular discovery, underlining their strength in de novo drug design,\nproperty prediction and reaction chemistry.\nWe highlight valuable open-source software assets\nthus lowering the entry barrier to the field of scientific language modeling.\nLast, we sketch a\nvision for future molecular design that combines a chatbot interface with access to computational\nchemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI\nenthusiasts interested in understanding how language models can and will be used to accelerate\nchemical discovery.\n1\nIntroduction\nDespite technological advances constantly reshaping our understanding of biochemical processes, the\nchemical industry persistently faces escalating resource costs of up to 10 years and 3 billion dollar\nper new market release [102].\nThe intricacy of the problem is typically attested by an exorbitant\nattrition rate in in vitro screenings [77], the sheer size of the chemical space [68] and the frequency of\nserendipity [40].\nLanguage models (LMs) emerged recently and demonstrated an astonishing ability to understand\nand generate human-like text [65]. Machine learning (ML) in general and LMs in particular hold the\npotential to profoundly accelerate the molecular discovery cycle (see Figure 1). In this chapter, we\nexplore applications of LMs to chemical design tasks. Although LMs were originally developed for\nnatural language, they have shown compelling results in scientific discovery settings when applied to\n\u201dscientific languages\u201d, e.g., in protein folding [55] or de novo design of small molecules [105], pep-\ntides [23] or polymers [66]. But what exactly is a language model? By definition, it is any ML model\nthat consumes a sequence of text chunks (so-called tokens) and is capable to reason about the content\nof the sequence. Since each token is essentially a vector [62], a LM is a pseudo-discrete time series\nmodel. Most typically, LMs learn probability distributions over sequences of words thus also facilitat-\ning the generation of new text given some input, for example in a language translation task. While\nall LMs rely on neural networks, contemporary models almost exclusively leverage the Transformer\narchitecture [93]. Now, all of this begs the question \u2013 what is the need for LMs in molecular discovery?\nFirst, when applied to serializations of chemical entities (e.g., SMILES [98]), LMs can learn highly\nstructured representations, often even tailored for desired functional properties [36]. This allows to\nperform smooth and property-driven exploration of the originally deemed discrete protein or molecular\nspace. Another attractive feature of scientific LMs is their ability to seamlessly bridge natural and\nscientific languages. This can give rise to ChatGPT-style chatbot interfaces that allow chemists to\n1\narXiv:2309.16235v1  [physics.chem-ph]  28 Sep 2023\nObserve \nand\nHypothesize\nExperimental\nEvaluation of \nHypothesis\nLab Synthesis\nof\nMolecules\nAnalyze\nthe\nResults\nRedesign\nthe Study\nRepeat\nfor every\nHypothesis\nData \nGeneration,\nMaterial \nPurchasing\nHypothesize\nExperimental\nEvaluation of \nHypothesis\nLab Synthesis\nof\nMolecules\nAnalyze\nthe\nResults\nRedesign\nthe Study\nSimulate\nData \nGeneration\n(a) Classic molecular discovery.\nEvaluation of \nHypothesis\nof\nMolecules\nHypothesize\nExperimental\nEvaluation of \nHypothesis\nLab Synthesis\nof\nMolecules\nAnalyze\nthe\nResults\nRedesign\nthe Study\nSimulate\nData \nGeneration\n(b) Accelerated molecular discovery.\nFigure 1: A comparison of molecular discovery workflows: (a) classic approach, where each hypothesis (a.k.a.\nmolecule) requires a new experimental cycle. (b) Accelerated molecular discovery cycle with machine-generated\nhypotheses and assisted validation, enabling simultaneous generation and testing of numerous molecules.\nformulate their design objectives through natural language and to iteratively refine their result with\nan interactive agent thus potentially accomplishing complex chemical tasks more rapidly. Here, we\npresent an overview of the role of LMs toward accelerated molecular discovery. We commence with\nthe conventional scientific discovery method and then discuss how molecular generative models can be\ncoupled with molecular property prediction models. Seeking for practical usability, we then present\nthe reader with selected software tools and libraries for scientific language modeling. We close with\na vision for future molecule design that integrates natural language models into the discovery process\nthrough chatbots.\n2\nAccelerated molecular discovery\nMolecule discovery, intricately linked to optimizing diverse properties in a vast space, challenges con-\nventional scientific methods. In chemistry\u2019s Design-Make-Test-Analyze (DMTA) cycle, synthesis costs\nand time constraints create a bottleneck that hampers hypothesis refinement (cf. Figure 1a). Tradi-\ntional approaches are largely driven by medicinal chemists who design \u201dmolecule hypotheses\u201d which\nare biased, ad-hoc and non-exhaustive. This hinders progress in addressing global issues, driving cru-\ncial necessity for an accelerated process of molecule discovery. Thus, a key challenge lies in improving\nspeed and quality of evaluating such \u201dmolecule hypotheses\u201d grounded on laboratory work.\nDeep generative models have recently emerged as a promising tool to expedite the hypothesis/design\nphase in molecular discovery. However, even the most advanced molecular generative models require\nan efficient method for large-scale virtual screening to test their hypotheses. The accelerated molecular\ndiscovery cycle adds a validation loop to DMTA, rapidly evaluating numerous hypotheses inexpen-\nsively (cf. Figure 1b). This loop enhances the design-phase generative model, ensuring only promising\nhypotheses advance to the synthesis and physical experimentation stages.\n2.1\nMolecule Representation\nData representation is critical as it determines which information is available for the model.\nAs\nillustrated in Figure 2, various molecular representations exist. Due to popularity of chemical language\nmodels (CLMs), this section focuses on text-representations of molecules. A more focused discussion\non CLMs was published by Grisoni [38].\n2\n1,3,7-Trimethyl-3,7-dihydro-1H-purine-2,6-dione\nMolFile\nGraph\n[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ... , 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\nMorgan Fingerprint\nCN1C=NC2=C1C(=O)N(C(=O)N2C)C\nSMILES\n[C][N][C][=N][C][=C][Ring1][Branch1_1][C][Branch1_2][C][=O][N]\n[Branch1_1][Branch2_2][C][Branch1_2][C][=O][N][Ring1][Branch2_1][C][C]\nSELFIES\nInChI=1S/C8H10N4O2/c1-10-4-9-6-5(10)7(13)12(3)8(14)11(6)2/h4H,1-3H3\nInChI\nText-based \nRepresentation\nStructure-based \nRepresentation\nFeature-based \nRepresentation\nAdjacency Matrix\n0    1    0    0    ...    0    0    0    0    0\n1    0    1    0    ...    0    0    0    0    0\n0    1    0    1    ...    0    0    0    0    0\n \n0    0    0    0    ...    1    0    0    1    0\n0    0    0    0    ...    0    1    1    0    0\n...\n......\n...\n...\n...\n...\n......\n...\nFigure 2: An illustration of popular ways of representing a chemical molecule as input to a ML model. The\nrepresentations may be (a) String-based, such as SMILES, SELFIES, or InChI which use characters to represent\ndifferent aspects of a molecule, (b) Structure-based, such as Graphs or MolFiles that encode connectivity and\natomic position, and (c) Feature-based, such as Morgan Fingerprints, which encode local substructures as bits.\nSimplified Molecular Input Line-Entry System (SMILES)\nSMILES [98] is a string represen-\ntation made up of specific characters for atoms, bonds, branches, aromaticity, rings and stereochem-\nistry in molecular structures. The character-level representation enables easy tokenization, making\nSMILES an ideal input for LMs. SMILES are non-unique, so each molecule can be written as multiple\nSMILES strings. Hence, SMILES are either canonicalized or, alternatively, their multiplicity is used\nas data augmentation strategy [8] which has shown performance improvement in molecular property\nprediction [8, 51, 88] and molecular generation [3, 92]. In generative modeling, a common issue is the\ninvalidity of SMILES strings due to an uneven number of ring opening/closure symbols or bond valence\nviolations. SMILES strings can undergo further processing, such as kekulization or stereoinformation\nremoval but employing canonicalized SMILES remains the most prevalent approach.\nTokenization is the process of splitting a string into vectorizable units. These units are typically\na single character, n-gram characters or words. Instead of splitting at the character level, SMILES\nare typically tokenized at the atom level with regular expressions [79] or by additionally including\npositional and connectivity information, thereby acknowledging that the same atom can have different\nencodings based on its location in the molecular structure [91]. SMILES may also be tokenized at\nthe substructure level, as demonstrated by SMILES Pair Encoding (SMILES-PE) [52]. This method,\ninspired by byte-pair encoding, iteratively counts and merges frequently occurring SMILES token\npairs until a given condition is met. Tokenization enables the creation of a vocabulary for SMILES\nrepresentations.\nVocabularies are dictionaries mapping tokens to vectors thus serving as gateway to LMs. For\nLMs to learn from SMILES, tokens are vectorized, either via one-hot encodings (where each row in\nthe binary matrix corresponds to a SMILES position and each column signifies a token). However,\nthis discrete method results in sparse, large matrices and thus, an alluring alternative is to learn\na continuous embedding for each token during training.\nThis facilitates the learning of semantic\nrelationships between tokens and enhances performance. Since learning good embeddings requires a\nlot of data, models pre-trained on natural language corpora are a strong option to learn scientific\nlanguage embeddings through fine-tuning [22].\nSelf Referencing Embedded Strings (SELFIES)\nSELFIES [49] were introduced as an alterna-\ntive to SMILES to counter the problem of generating invalid molecules. Unlike SMILES, SELFIES are\ngenerated using derivation rules to enforce valence-bond validity. They store branch length and ring\n3\nsize to avoid open branches and rings. These supplementary attributes ensure a valid representation\nduring molecule generation. While this strategy guarantees 100% validity, it could produce strings\nthat are too short to be a useful molecule.\nInternational Chemical Identifier (InChI)\nIntroduced by the IUPAC, InChI [41] are strings\nencoding structural information including charge of the molecule in a hierarchical manner. The strings\ncan get long and complex for larger molecules. To counter this, a hash called \u2018InChiKey\u2019 was developed\nto help with search and retrieval. InChIs are are less commonly used in LMs [39].\n2.2\nGenerative Modelling\nGenerative modeling involves learning the data\u2019s underlying distribution with the intent of generating\nnew samples, a technique pivotal in accelerating de novo drug discovery. A generative model may be\nconditional or unconditional. A conditional generative model utilizes provided data attributes or labels\nto generate new samples with desired properties, whereas an unconditional model solely provides a\nway to sample molecules similar to the training data [36]. The DMTA cycle particularly benefits from\nthe conditional generation approach as it facilitates goal-oriented hypothesis design [9]. This section\ndescribes a few influential conditional generation models that act on chemical language to generate\nmolecules satisfying user-defined conditions.\nMultimodal Data \nGenerative Model\nGenerated Molecules\nProperty Prediction\nLab Validation\nTraining - Optimisation\nFinal Inference - Testing\nPrimer\nCell profile\nProtein target\nToxicity\nSolubility\n...\n\u201cSolubility = 1.03\u201d\n\u201cLogP       = 0.28\u201d\n...\nProperties\nConditional Generator\nDrug \nsensitivity\nIC50\nTarget \naffinity\n\u221220\n20\n40\n60\n20\n40\n60\n80\n100\nTemperature (\u00b0C)\nSolubility\nin water\n(g/mL)\n80\n100\n10\n30\n50\n70\n90\nNH3\nNaCl\nSynthesis\nRXN\nRobo\nfor Chemistry\nrxn.res.ibm.com/roborxn\nFigure 3:\nAn illustration of conditional molecule generation using LMs.\nThe process initiates with the\ncollection and processing of multi-modal data, which is then compressed into a fixed-size latent representation.\nThese representations are subsequently passed to a molecular generative model. The generated molecules then\nundergo in-silico property prediction, which is linked back to the generative model through a feedback loop\nduring training. The in-silico models direct the generative model to produce property- or task-driven molecules\nusing a reward function. In the inference stage, candidate molecules generated by the optimized model continue\nthrough the workflow for lab synthesis and subsequent experimental validation to determine their efficacy for\nthe desired task.\n2.2.1\nRecurrent Neural Network (RNN)\nThe sequential nature of RNNs makes them suitable models for processing chemical languages. Pro-\nposed in the 90s, RNNs were the first flavor of CLMs [8, 79, 85]. Their hidden states are continuously\nupdated as new tokens are passed to the network. During the generation process, tokens are produced\nauto-regressively. RNNs find use in generating molecule libraries [85] which are extensively used in\ndrug development processes like screening. External scoring functions drive the generation of molecules\nwith desired properties. RNNs are also adept at learning complex distributions [31] and generating a\nhigher proportion of unique and valid SMILES [69], even though their inability to count occurrences\nof ring opening/closing symbols poses a challenge [46, 70].\n4\n2.2.2\nVariational Autoencoder (VAE)\nVAEs learn latent distribution parameters of molecules, thus enabling the generation of new molecules\nby sampling from this distribution. Their unique ability lies in learning a smooth, latent space that\nfacilitates interpolation of samples, even for notoriously discrete entities like molecules [36]. To make\nit suitable for chemical language models (CLMs), any network compatible with string inputs can func-\ntion as a VAE\u2019s encoder and decoder. Initial works primarily focused on single-modality applications,\nassessing latent space quality via downstream tasks [36]. This approach remains prevalent and can\nbe used to generate, e.g., catalysts with an RNN-based VAE [78] . Here, a latent space is learned\nand assessed by predicting the catalyst binding energy.\nLim et al. [53] takes it a step further by\nconcatenating a condition vector to the input and the latent embedding generated by the recurrent\nnetwork-based VAE\u2019s encoder. This approach enables the generation of molecules specifically tailored\nto the given conditions. The scope of VAEs expanded progressively into multi-modal settings for con-\nditional molecule generation, as visualized in Figure 3 and exemplified by Born et al. [11, 12, 13]. These\nworks on task-driven molecule generation incorporate contextual information like gene expression [13]\nor protein targets [11, 12] or even both [45]. VAEs learn embeddings of context information and primer\ndrugs, which are merged before decoding to produce molecules. A reinforcement-learning-based ap-\nproach directs the model to produce molecules with desired properties using rewards.\n2.2.3\nTransformer\nThe self-attention attribute of Transformers [93] have propelled these models to the forefront of NLP.\nTransformers have an encoder module that relies on this self-attention to learn embeddings of the input\nand the context associated with this input. The decoder module predicts tokens using the context\nlearnt by the encoder and previously generated tokens through attention. For generative modeling,\ndecoder-only transformer like the Generative Pre-Training Transformer (GPT) [72] have become the\ndominant approach. This success was translated to the scientific language domain. One of the first\nmodels to use the GPT architecture for conditional molecule generation is MolGPT [4]. SMILES tokens\nconcatenated with a condition vector that summarizes the desired properties and scaffolds are passed\nas input to this model, which is then trained on the next token prediction task to generate molecules.\nGPT-like models coupled with RL can also be used to optimize molecular properties like pIC50 [61]. In\nthis two-stage approach, embeddings are first learnt from SMILES strings, and the embedding space is\nthen optimized such that the model samples molecules with the desired properties. Going beyond just\nusing GPT-like architectures for molecule generation, Regression Transformer [10] is a seminal work\nthat formulates conditional sequence modeling as a regression problem. This gives rise to a natural\nmultitask model that concurrently performs property prediction and conditional molecular generation.\nThis is achieved by concatenating conventional molecular tokens with property tokens and employing\nan training scheme that alternates which parts of the sequence are masked.\nAll these works are testament to the generative capabilities of Transformer-based models. The\nsuperior quality of learned embeddings coupled with its ability to handle parallel processing and scala-\nbility makes it a top choice for the task of conditional molecule generation, with promising applications\nin drug discovery and other areas of molecular design [66].\n2.3\nProperty Prediction\nWhether a discovery is novel or not, property prediction is a key step in validating the molecules\nfor a given use case. The success of a molecule depends on a myriad of factors, including how it\ninteracts with its environment. The MoleculeNet datasets [103] are a commonly used benchmark for\nproperty prediction. It is curated from public datasets and comprises over 700,000 compounds tested\non various properties.\nBorn et al. [15] uses a multiscale convolutional attention model to predict\ntoxicity from SMILES. The model has three kernel sizes for the convolutional network and uses a\na Bahdanau attention mechanism [5]. The model shows a superior performance overall on various\nMoleculeNet tasks compared to all other SMILES-based models. A recent trend is to use transformer-\nencoders to learn embeddings for molecules and then apply a multilayer perceptron (MLP) on the\nembeddings for property prediction. MolBERT [29] and ChemBERTA [20]) are two such examples.\n5\nThese transformer-based models use a BERT backbone to learn molecular embeddings from SMILES\nand predict properties. Similarly, Molformer [75] uses a transformer-encoder with linear attention and\nrelative positional encoding to learn compressed molecular representations which are then fine-tuned\non chemical property prediction benchmarks. To equip transformers with better inductive biases to\nhandle molecules, adaptations of the attention mechanism were proposed. The molecule attention\ntransformer (MAT) incorporates inter-atomic distances and graph structure into the attention mech-\nanism [58]. An improvement over this model is the relative-MAT which fuses the distance embedding,\nbond embedding and neighbourhood embedding and achieves competitive performances on a range of\nproperty prediction tasks [59].\n3\nSoftware tools for scientific language modeling\nThe paradigm shift towards open-sourcing software has exerted a profound influence in chemistry.\nCommonly listed implications of open-sourcing in the context of drug discovery include catalyzation\nof methodological development, fostering of collaboration and ease of scientific reproducibility [35]. In\nthis section we present several software assets (e.g., Python packages or cloud-based web apps) that\nare key to enable molecular discovery.\n3.1\nNatural language models\nThe success story of the Transformer [93] as most widely adopted neural network architecture goes\nhand in hand with the rise of the transformers library [101], developed since 2019 by HuggingFace.\nInitially intended for NLP applications, Transformers were adopted interdisciplinarily, e.g in computer\nvision [25], reinforcement learning [19], protein folding [47] and, of course, chemistry [84]. HuggingFace\nprovides the largest public hub of language models and it offers implementations of all recent models\nas well as a diverse collection of pretrained models available for fine-tuning or inference. While most\nof their models focus on NLP, selected models are designed for life science applications, in partic-\nular molecular property prediction (e.g., ChemBerta [20]), molecular captioning (e.g., MolT5 [26]),\ntext-based molecular generation (e.g., MolT5 [26]) but also unsupervised protein language models\n(e.g., ProtBert, ProtAlbert, ProtXLNet or ProtT5 [27]). Moreover, some available models like Multi-\nmodal Text and Chemistry T5 [22] are prompt-based multitasker that besides the above mentioned\ntasks also perform additional tasks such as forward/backward reaction prediction.\n3.2\nGT4SD \u2013 Generative modeling toolkits\nPython libraries like GT4SD (the Generative Toolkit for Scientific Discovery [57]), TdC (Therapeutics\nData Commons [43]) or deepchem [73] were developed primarily for molecular discovery applications,\nbut especially GT4SD offers ample support of language models (LMs). GT4SD is designed to enable\nresearchers and developers to use, train, fine-tune and distribute state-of-the-art generative models for\nsciences with a focus on the design of organic materials. It is compatible and inter-operable with many\nexisting libraries and, beyond transformers, it also gives access to diffusion models (diffusers [96])\nor graph generative models (TorchDrug [106]). Next to established molecular generation benchmark\nlike Moses [69] and GuacaMol [16] that include VAEs, generative adversarial networks (GANs), genetic\nalgorithms, and many evaluation metrics for molecular design, gt4sd also supports very contemporary\nmodels like the Regression Transformer for concurrent sequence regression and property-driven molec-\nular design [10], GFlowNets for highly diverse candidate generation [6] or MoLeR for motif-constrained\nmolecule generation [60]. GT4SD ships with a harmonized interface and a set of command line tools\nthat access a registry of generative models to run or train any model with a few lines of code. Trained\nmodels can be shared to a cloud-hosted model hub and the library is build to facilitate consumption\nby containerization or distributed computing systems. To date, it includes \u223c 50 property prediction\nendpoints for small molecules, proteins and crystals and overall hosts \u223c 30 pre-trained algorithms for\nmaterial design, 20 free webapps [2] and many Jupyter/Colab notebooks.\n6\n3.3\nRXN for Chemistry: Reaction and synthesis language models\nOnce a molecule has been selected for experimental validation, a tangible synthesis route has to be\nidentified. Since the most important tasks in chemical reaction modeling can be framed as sequence\nconversion problems, the methodology developed for natural language translation can be seamlessly\ntranslated to chemistry [84]. In this analogy, atoms are characters, molecules are words, reactions are\nsentences and precursors are translated into a product or vice versa.\nThe most mature and flexible library for reaction modeling with LMs is the package rxn4chemistry [32].\nIt wraps the API of the IBM RXN for Chemistry platform, a freely accessible web application that\ngives access to a rich set of language models for different tasks in reaction chemistry. The flagship\narchitecture has been the Molecular Transformer (MT), an autoregressive encoder-decoder model,\noriginally applied to predict outcomes of chemical reactions in organic chemistry [80]. Notably, the\nMT uses a purely data-driven, template-free approach that, unlike many graph-based models, can\ndirectly represent stereochemistry and thus also exhibits excellent performance on regio- and stereos-\nelective reactions [67]. The MT was applied to single-step retrosynthesis [90] and became the linchpin\nof a multi-step retrosynthesis model with a hypergraph exploration strategy [81]. This approach was\nlater generalized to enzymatic reactions with a tokenization scheme based on enzyme classes which\nfacilitated biocatalyzed synthesis planning and paved the road towards more sustainable and green\nchemistry [71]. Derivatives of the MT helped to enhance diversity in single-step retrosynthesis [90] and\na prompt-based disconnection scheme proposed by Thakkar et al. [89] significantly improved controlla-\nbility by allowing the user to mark a disconnection side in the reactant. Interestingly, an encoder-only\nderivative of the MT (that replaced the autoregressive decoder with a classification head and leveraged\nBERT-style [24] self-supervised pretraining on reactions) excelled in predicting reaction classes [83].\nThe hidden representations of such a model were found to encode reaction types and thus allowing to\nmap reaction atlases and to perform reaction similarity search. This gave rise to the rxnfp package\nfor chemical reaction fingerprinting. Strikingly, masked language modeling also led later to the dis-\ncovery that the learned attention weights of the Transformer are \u201dsecretly\u201d performing atom mapping\nbetween products and reactions [82]. The epiphany that CLMs accomplish atom mapping without\nsupervision or human labeling bridged the gap between rule-based and data-driven approaches in\nreaction modeling, making this once tedious experimental task more efficient.\nIn the quest for automation in organic chemistry, once the precursors for a molecule\u2019s synthesis route\nare identified, the subsequent crucial phase involves seeking an actionable, stepwise synthesis protocol\nthat is ideally amenable for autonomous execution on a robotic platform, such as IBM RoboRXN. In two\nseminal works Vaucher et al. demonstrated that encoder-decoder Transformers can extract chemical\nsynthesis actions, first from experimental procedures described in patents [94] and later predict them\ndirectly from the reaction SMILES [95]. Notable, all the aforementioned models are available via the\nIBM RXN for Chemistry platform which even allows to control and monitor the robotic platform\ndirectly from the web interface. For the daunting task of multistep retrosynthesis planning, RXN also\nincludes non-transformer based models like AiZynthFinder [34], a Monte Carlo Tree Search approach\nbuild on top of a RNN. Most of the RXN models can be executed also via the rxn4chemistry Python\npackage.\n3.4\nSpecialized libraries\nMolecular property prediction.\nHuggingMolecules is a library solely devoted to aggregate, stan-\ndardize and distribute molecular property prediction LMs [33]. It contains many encoder-only CLMs,\nsome of them with geometrical and structure-aware inductive biases (e.g., the MAT [58] or its suc-\ncessor, the R-MAT [59]) while others being pure BERT-based models that were trained on SMILES\n(e.g,. MolBERT [29] or ChemBERTA [20]).\nData processing.\nRDKit [50] is a library for manipulating molecules in Python. For narrower appli-\ncations like ML data preparation several tools exist. First, rxn-chemutils is a library with chemistry-\nrelated utilities from RXN for Chemistry. It includes functionalities for standardizing SMILES (e.g.,\ncanonicalization or sanitization) but also conversions to other representations (e.g., InChI). It harmo-\nnizes reaction SMILES and prepares them for consumption by CLMs, including also SMILES aug-\n7\nmentation (by traversing the molecular graph in a non-canonical order) and tokenization. Another\nlibrary with a similar focus is pytoda [12, 13]. It does not support reaction SMILES but implements\nricher preprocessing utilities, allowing to chain >10 SMILES transformations (e.g., kekulization [15]).\nIt supports different languages (e.g., SELFIES [49] or BigSMILES [54]) and tokenization schemes (e.g.,\nSMILES-PE [52]). Similar functionalities are available for proteins including different languages (IU-\nPAC, UniRep or Blosum62) and protein sequence augmentation strategies [14]. For small molecules,\nproteins, and polymers, dedicated language classes facilitate the integration with LMs by storing\nvocabularies, performing online transformations and feeding to custom datasets. Datasets exist for\npredicting molecular properties, drug sensitivity, protein-ligand affinity or for self-supervision on small\nmolecules, proteins or polymers.\n3.5\nGeneral purpose platforms\nSeveral general-purpose platforms for molecular discovery have been launched recently, sometimes even\npreserving privacy through federated learning (i.e., decentralized, distributed training). For example,\nMELLODDY [42] is a collaborative effort aimed at cross-pharma federated learning of 2.6 billion\nconfidential activity data points. Similarly, VirtualFlow [37] is an open-source platform facilitating\nlarge-scale virtual screening that was shown to identify potent KEAP1 inhibitors. With a focus on de\nnovo drug design, Chemistry42 [44] is a proprietary platform integrating AI with computational and\nmedicinal chemistry techniques.\n4\nFuture of molecular discovery\nA few years ago, the idea of querying an AI model \u2013 like one would a search engine \u2013 to not only extract\nscientific knowledge but also perform computational analyses was an overly ambitious feat. Scientific\nthinking comes from the ability to reason, and AI models cannot reason like humans, yet. However,\nthese models can learn from humans.\nOur propensity to document everything has enabled us to\ntrain Large Language Models (LLMs), like ChatGPT [64] and GitHub Copilot [1], to mimic human\nresponses. When brought into the context of computational science, this could equip non-experts to\nconfidently conduct computational analyses through well-designed prompts. With human-in-the-loop,\na synergistic effect could be created where the scientist provides feedback to the model on its output,\nthus aiding in better model optimization (a strategy called reinforcement learning from human feedback\n(RLHF) that has been proven critical for ChatGPT [21]). These applications also reduce the barrier for\nindividuals from non-scientific backgrounds to gain a more hands-on experience in conducting scientific\nanalyses without having to go through formal training in computational analysis.\nThis section provides a sneak peak into what\u2019s next for molecular discovery. Riding the LLM wave,\nthe future holds a place for chatbot-like interfaces that may take care of all things computational in\nmolecular discovery. This includes, for example, generating and iteratively improving design ideas,\nsynthesis planning, material purchasing, performing routine safety checks, and validating experiments.\nThe rise of foundation models in chemistry\nConventionally, neural networks are trained for a single given task to achieve maximum performance.\nThis essentially renders the models useless for other tasks, thus requiring a new model for every new\ntask, even when the training domain is the same, which in turn imposes a constraint on the rate of our\ntechnological advancements. Over the last few years, this conventional approach has been challenged\nby Large Language Models (LLMs). It has been found that scaling up LLMs leads to astonishing\nperformances in few-shot [17] and even zero-shot task generalization [76]. Referred to as \u201dfoundation\nmodels\u201d [30, 63], these models, with typically billions of parameters, can perform multiple tasks despite\nbeing trained on one large dataset. Essentially, this multi-task learning is achieved by prompting LLMs\nwith task instructions along with the actual query text which has been found to induce exceptional\nperformance in natural language inference and sentence completion [76]. These findings have kicked\noff new research directions, such as prompt engineering [97] and in-context learning [17], in NLP.\n8\nThe foundation model paradigm also finds an increasing adoption in chemistry. There is an increase\nin task-specific models integrating natural and chemical languages [26, 94, 95, 104]. Concurrently,\nmulti-tasking in pure CLMs has also been advancing through models that combined tasks such as\nproperty prediction, reaction prediction and molecule generation either with small task-specific heads\n(e.g., T5Chem [56]) or via mask infilling (e.g., Regression Transformer [10]). Christofidellis et al. [22]\nwere the first to bridge the gap and develop a fully prompt-based multi-task chemical and natural\nlanguage model. Despite only 250M parameters, the Multitask Text and Chemistry T5 was shown to\noutperform ChatGPT [64] and Galactica [87] on a contrived discovery workflow for re-discovering a\ncommon herbicide (natural text \u2192 new molecule \u2192 synthesis route \u2192 synthesis execution protocol).\n4.1\nThe coalescence of chatbots with chemistry tools\nGiven the aforementioned strong task generalization performances of LLMs, building chatbot interfaces\naround it was a natural next step and thus next to ChatGPT [64], many similar tools were launched.\nSuch tools were found to perform well on simplistic chemistry tasks [18, 99], opening potential to\nFigure 4:\nScreenshot of the LLM-powered chatbot application ChemChat.\nEmbedding the capabilities of\nexisting resources such as PubChem [48], RDKit [50] or GT4SD [57] enables the assistant to execute program-\nming routines in the background and thus answer highly subject-matter specific user requests without the user\nneeding programming skills.\nreshape how chemists interact with chemical data, enabling intuitive access to complex concepts and\nmake valuable suggestions for diverse chemical tasks. Furthermore, AI models specifically developed\nby computer scientists for e.g.\ndrug discovery or material science can be made available through\napplications powered by LLMs, such as chatbots. This minimizes the access barrier for subject matter\nexperts who would otherwise require the respective programming skills to utilize these AI models. The\npower of such chatbots is reached through the coalscence of LLMs and existing chemistry software\ntools like PubChem [48], RDKit [50] or GT4SD [57]. Together, such applications can unleash the full\n9\npotential and value of these models by the strongly enhanced usage. An example of how the interaction\nwith such a tool could look like is shown in Figure 4.\nIn this example, a user provides a molecule (either as SMILES string or via a molecule sketcher)\nand asks to identify the molecule. The chatbot relies on prompt-engineering in order to inform the\nLLM about all its available tools. The user input is first sent to the LLM which recognizes that one\nof its supported tools, in this case PubChem, can answer the question. The chatbot then sends a\nrequest to the PubChem API and returns a concise description of the molecule. The user subsequently\nasks to compute the logP partition coefficient [100] and the quantitative estimate of drug-likeness\n(QED) [7].\nCalculation of both properties is enabled through the GT4SD tool [57] allowing the\nchatbot to answer the request with certainty. This will trigger a programming routine to accurately\nformat the API request for GT4SD, i.e., composing the SMILES string with the logP or QED endpoint.\nThe computation is then performed asynchronously and a separate call to the post-processing routine\nformats the LLM-generated string reply and composes the response object for the frontend.\nThis fusion of LLMs with existing tools gives rise to a chatbot assistant for material science and\ndata visualization that can perform simple programming routines without requiring the user to know\nprogramming or have access to compute resources. A continuation of the conversation involving more\ncomplex user queries is shown in Figure 5. Having identified the initial molecule as theobromine with\nFigure 5: Screenshot of the LLM-powered chatbot application ChemChat showing the continuation of the\nconversation involving generative tasks through GT4SD\u2019s Regression Transformer [10] as well as property [28]\nand similarity calculation [74, 86].\na logP of -1.04, the user requests three similar molecules with a slightly increased logP of -0.5. Here,\nChemChat identifies the Regression Transformer [10] as the available tool to perform substructure-\nconstrained, property-driven molecule design.\nOnce the routine has been executed and the three\ncandidate SMILES are collected, the text result is post-processed to add more response data objects\nsuch as molecule visualizations, datasets or Vega Lite specs for interactive visualizations.\nIn conclusion, chatbots can facilitate the integration of essentially all major cheminformatics soft-\nware in a truly harmonized and seamless manner. While LLMs are not intrinsically capable to perform\n10\ncomplex routines, at least not with high precision and in a trustworthy manner, the synergy between\ntheir natural language abilities with existing chemistry tools has the potential to transform the way\nchemistry is performed.\nReferences\n[1] Github copilot. https://copilot.github.com/, 2021. Accessed: August 8, 2023.\n[2] Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James\nZou.\nGradio:\nHassle-free sharing and testing of ml models in the wild.\narXiv preprint\narXiv:1906.02569, 2019.\n[3] Josep Ar\u00b4us-Pous, Simon Viet Johansson, Oleksii Prykhodko, Esben Jannik Bjerrum, Christian\nTyrchan, Jean-Louis Reymond, Hongming Chen, and Ola Engkvist. Randomized smiles strings\nimprove the quality of molecular generative models. Journal of cheminformatics, 11(1):1\u201313,\n2019.\n[4] Viraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. Molgpt: molecular generation\nusing a transformer-decoder model. Journal of Chemical Information and Modeling, 62(9):2064\u2013\n2076, 2021.\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[6] Yoshua Bengio, Tristan Deleu, Edward J Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio.\nGflownet foundations. Preprint at https://arxiv.org/abs/2111.09266, 2021.\n[7] G Richard Bickerton, Gaia V Paolini, J\u00b4er\u00b4emy Besnard, Sorel Muresan, and Andrew L Hopkins.\nQuantifying the chemical beauty of drugs. Nat. Chem., 4(2):90\u201398, 2012.\n[8] Esben Jannik Bjerrum. Smiles enumeration as data augmentation for neural network modeling\nof molecules. arXiv preprint arXiv:1703.07076, 2017.\n[9] Jannis Born and Matteo Manica.\nTrends in deep learning for property-driven drug design.\nCurrent medicinal chemistry, 28(38):7862\u20137886, 2021.\n[10] Jannis Born and Matteo Manica. Regression transformer enables concurrent sequence regression\nand generation for molecular language modelling. Nature Machine Intelligence, 5(4):432\u2013444,\n2023.\n[11] Jannis Born, Tien Huynh, Astrid Stroobants, Wendy D Cornell, and Matteo Manica. Active site\nsequence representations of human kinases outperform full sequence representations for affinity\nprediction and inhibitor generation: 3d effects in a 1d model. Journal of Chemical Information\nand Modeling, 62(2):240\u2013257, 2021.\n[12] Jannis Born, Matteo Manica, Joris Cadow, Greta Markert, Nil Adell Mill, Modestas Filipavicius,\nNikita Janakarajan, Antonio Cardinale, Teodoro Laino, and Mar\u00b4\u0131a Rodr\u00b4\u0131guez Mart\u00b4\u0131nez. Data-\ndriven molecular design for discovery and synthesis of novel ligands: a case study on sars-cov-2.\nMach. Learn.: Sci. Technol., 2(2):025024, 2021.\n[13] Jannis Born, Matteo Manica, Ali Oskooei, Joris Cadow, Greta Markert, and Mar\u00b4\u0131a Rodr\u00b4\u0131guez\nMart\u00b4\u0131nez. PaccMannRL: De novo generation of hit-like anticancer molecules from transcriptomic\ndata via reinforcement learning. iScience, 24(4):102269, 2021.\n[14] Jannis Born, Yoel Shoshan, Tien Huynh, Wendy D Cornell, Eric J Martin, and Matteo Manica.\nOn the choice of active site sequences for kinase-ligand affinity prediction. Journal of chemical\ninformation and modeling, 62(18):4295\u20134299, 2022.\n11\n[15] Jannis Born,\nGreta Markert,\nNikita Janakarajan,\nTalia B Kimber,\nAndrea Volkamer,\nMar\u00b4\u0131a Rodr\u00b4\u0131guez Mart\u00b4\u0131nez, and Matteo Manica.\nChemical representation learning for toxic-\nity prediction. Digital Discovery, 2023.\n[16] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: bench-\nmarking models for de novo molecular design. J. Chem. Inf. Model., 59(3):1096\u20131108, 2019.\n[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[18] Cayque Monteiro Castro Nascimento and Andr\u00b4e Silva Pimentel.\nDo large language models\nunderstand chemistry?\na conversation with chatgpt.\nJournal of Chemical Information and\nModeling, 63(6):1649\u20131655, 2023.\n[19] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via\nsequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.\n[20] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: large-scale self-\nsupervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.\n[21] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences. Advances in neural information processing\nsystems, 30, 2017.\n[22] Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Mat-\nteo Manica. Unifying molecular and textual representations via multi-task language modelling.\nIn International Conference on Machine Learning, 2023.\n[23] Payel Das, Tom Sercu, Kahini Wadhawan, Inkit Padhi, Sebastian Gehrmann, Flaviu Cipcigan,\nVijil Chenthamarakshan, Hendrik Strobelt, Cicero Dos Santos, Pin-Yu Chen, et al. Accelerated\nantimicrobial discovery via deep generative models and molecular dynamics simulations. Nat.\nBiomed. Eng., 5(6):613\u2013623, 2021.\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021, 2021.\n[26] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation\nbetween molecules and natural language. In 2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022, 2022.\n[27] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Wang Yu, Llion Jones,\nTom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and\nBurkhard Rost. Prottrans: Towards cracking the language of life\u2019s code through self-supervised\ndeep learning and high-performance computing. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, pages 1\u20131, 2021. doi: 10.1109/TPAMI.2021.3095381.\n[28] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like\nmolecules based on molecular complexity and fragment contributions. Journal of cheminformat-\nics, 1:1\u201311, 2009.\n12\n[29] Benedek Fabian, Thomas Edlich, H\u00b4el\u00b4ena Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato,\nand Mohamed Ahmed. Molecular representation learning with language models and domain-\nrelevant auxiliary tasks. arXiv preprint arXiv:2011.13230, 2020.\n[30] Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Rui-\nhua Song, Xin Gao, Tao Xiang, et al. Towards artificial general intelligence via a multimodal\nfoundation model. Nature Communications, 13(1):3094, 2022.\n[31] Daniel Flam-Shepherd, Kevin Zhu, and Al\u00b4an Aspuru-Guzik. Language models can learn complex\nmolecular distributions. Nature Communications, 13(1):3293, 2022.\n[32] IBM RXN for Chemistry team. rxn4chemistry: Python wrapper for the IBM RXN for Chemistry\nAPI. https://github.com/rxn4chemistry/rxn4chemistry, 2023.\n[33] Piotr Gai\u00b4nski,  Lukasz Maziarka, Tomasz Danel, and Stanis law Jastrzebski. Huggingmolecules:\nAn open-source library for transformer-based molecular property prediction (student abstract).\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12949\u201312950,\n2022.\n[34] Samuel Genheden, Amol Thakkar, Veronika Chadimov\u00b4a, Jean-Louis Reymond, Ola Engkvist,\nand Esben Bjerrum. Aizynthfinder: a fast, robust and flexible open-source software for retrosyn-\nthetic planning. Journal of cheminformatics, 12(1):70, 2020.\n[35] J Daniel Gezelter. Open source and open data should be standard practices, 2015.\n[36] Rafael G\u00b4omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00b4e Miguel Hern\u00b4andez-Lobato,\nBenjam\u00b4\u0131n S\u00b4anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,\nRyan P Adams, and Al\u00b4an Aspuru-Guzik. Automatic chemical design using a data-driven con-\ntinuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.\n[37] Christoph Gorgulla, Andras Boeszoermenyi, Zi-Fu Wang, Patrick D Fischer, Paul W Coote,\nKrishna M Padmanabha Das, Yehor S Malets, Dmytro S Radchenko, Yurii S Moroz, David A\nScott, et al. An open-source drug discovery platform enables ultra-large virtual screens. Nature,\n580(7805):663\u2013668, 2020.\n[38] Francesca Grisoni. Chemical language models for de novo drug design: Challenges and oppor-\ntunities. Current Opinion in Structural Biology, 79:102527, 2023.\n[39] Jennifer Handsel, Brian Matthews, Nicola J Knight, and Simon J Coles. Translating the inchi:\nadapting neural machine translation to predict iupac names from a chemical identifier. Journal\nof cheminformatics, 13(1):1\u201311, 2021.\n[40] Emily Hargrave-Thomas, Bo Yu, and J\u00b4ohannes Reynisson. Serendipity in anticancer drug dis-\ncovery. World journal of clinical oncology, 3(1):1, 2012.\n[41] Stephen R Heller, Alan McNaught, Igor Pletnev, Stephen Stein, and Dmitrii Tchekhovskoi. Inchi,\nthe iupac international chemical identifier. Journal of cheminformatics, 7(1):1\u201334, 2015.\n[42] Wouter Heyndrickx, Lewis Mervin, Tobias Morawietz, No\u00b4e Sturm, Lukas Friedrich, Adam Za-\nlewski, Anastasia Pentina, Lina Humbeck, Martijn Oldenhof, Ritsuya Niwayama, et al. Mel-\nloddy: cross pharma federated learning at unprecedented scale unlocks benefits in qsar without\ncompromising proprietary information. 2022.\n[43] Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Coley Connor\nW, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learn-\ning datasets and tasks for drug discovery and development. Advances in Neural Information\nProcessing System, 35, 2021.\n13\n[44] Yan A Ivanenkov, Daniil Polykovskiy, Dmitry Bezrukov, Bogdan Zagribelnyy, Vladimir Aladin-\nskiy, Petrina Kamya, Alex Aliper, Feng Ren, and Alex Zhavoronkov. Chemistry42: an ai-driven\nplatform for molecular design and optimization. Journal of Chemical Information and Modeling,\n63(3):695\u2013701, 2023.\n[45] Nikita Janakarajan, Jannis Born, and Matteo Manica. A fully differentiable set autoencoder. In\nProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,\npages 3061\u20133071, 2022.\n[46] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recur-\nrent nets. Advances in neural information processing systems, 28, 2015.\n[47] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u02c7Z\u00b4\u0131dek, Anna Potapenko, et al. Highly\naccurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.\n[48] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li,\nBenjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access\nto chemical data. Nucleic acids research, 47(D1):D1102\u2013D1109, 2019.\n[49] Mario Krenn, Florian H\u00a8ase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik.\nSelf-referencing embedded strings (selfies): A 100% robust molecular string representation. Ma-\nchine Learning: Science and Technology, 1(4):045024, 2020.\n[50] Greg Landrum. Rdkit documentation. Release, 1(1-79):4, 2013.\n[51] Xinhao Li and Denis Fourches. Inductive transfer learning for molecular activity prediction:\nNext-gen qsar models with molpmofit. Journal of Cheminformatics, 12(1):1\u201315, 2020.\n[52] Xinhao Li and Denis Fourches. Smiles pair encoding: a data-driven substructure tokenization\nalgorithm for deep learning. Journal of chemical information and modeling, 61(4):1560\u20131569,\n2021.\n[53] Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model\nbased on conditional variational autoencoder for de novo molecular design. Journal of chemin-\nformatics, 10(1):1\u20139, 2018.\n[54] Tzyy-Shyang Lin, Connor W Coley, Hidenobu Mochigase, Haley K Beech, Wencong Wang,\nZi Wang, Eliot Woods, Stephen L Craig, Jeremiah A Johnson, Julia A Kalow, et al. Bigsmiles:\na structurally-based line notation for describing macromolecules.\nACS central science, 5(9):\n1523\u20131531, 2019.\n[55] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,\nRobert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level\nprotein structure with a language model. Science, 379(6637):1123\u20131130, 2023.\n[56] Jieyu Lu and Yingkai Zhang. Unified deep learning model for multitask reaction predictions\nwith explanation. Journal of Chemical Information and Modeling, 62(6):1376\u20131387, 2022.\n[57] Matteo Manica, Jannis Born, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Dean Clarke,\nYves Gaetan Nana Teukam, Giorgio Giannone, Samuel C Hoffman, Matthew Buchan, et al. Ac-\ncelerating material design with the generative toolkit for scientific discovery. npj Computational\nMaterials, 9(1):69, 2023.\n[58]  Lukasz Maziarka, Tomasz Danel, S lawomir Mucha, Krzysztof Rataj, Jacek Tabor, and S Jas-\ntrzkebski. Molecule-augmented attention transformer. In Workshop on Graph Representation\nLearning, Neural Information Processing Systems, 2019.\n14\n[59]  Lukasz Maziarka, Dawid Majchrowski, Tomasz Danel, Piotr Gai\u00b4nski, Jacek Tabor, Igor Podolak,\nPawe l Morkisz, and Stanis law Jastrzkebski. Relative molecule self-attention transformer. arXiv\npreprint arXiv:2110.05841, 2021.\n[60] Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider,\nNikolaus Stiefl, Marwin Segler, and Marc Brockschmidt. Learning to extend molecular scaffolds\nwith structural motif.\nIn The Tenth International Conference on Learning Representations,\nICLR, 2022.\n[61] Eyal Mazuz, Guy Shtar, Bracha Shapira, and Lior Rokach. Molecule generation using trans-\nformers and policy gradient reinforcement learning. Scientific Reports, 13(1):8799, 2023.\n[62] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word repre-\nsentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[63] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec,\nEric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelli-\ngence. Nature, 616(7956):259\u2013265, 2023.\n[64] OpenAI. Chatgpt. https://chat.openai.com/chat, 2023. Accessed: August 8, 2023.\n[65] OpenAI. Gpt-4 technical report, 2023.\n[66] Nathaniel H Park, Matteo Manica, Jannis Born, James L Hedrick, Tim Erdmann, Dmitry Yu\nZubarev, Nil Adell-Mill, and Pedro L Arrechea.\nArtificial intelligence driven design of cata-\nlysts and materials for ring opening polymerization using a domain-specific language. Nature\nCommunications, 14(1):3686, 2023.\n[67] Giorgio Pesciullesi, Philippe Schwaller, Teodoro Laino, and Jean-Louis Reymond. Transfer learn-\ning enables the molecular transformer to predict regio-and stereoselective reactions on carbohy-\ndrates. Nature communications, 11(1):4874, 2020.\n[68] Pavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek.\nEstimation of the size of\ndrug-like chemical space based on gdb-17 data. J. Comput. Aid. Mol. Des., 27(8):675\u2013679, 2013.\n[69] Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai\nTatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark\nVeselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models.\nFront. Pharmacol., 11:1931, 2020.\n[70] Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de\nnovo drug design. Science advances, 4(7):eaap7885, 2018.\n[71] Daniel Probst, Matteo Manica, Yves Gaetan Nana Teukam, Alessandro Castrogiovanni, Federico\nParatore, and Teodoro Laino. Biocatalysed synthesis planning using data-driven learning. Nature\ncommunications, 13(1):964, 2022.\n[72] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\n[73] Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, and Zhenqin\nWu. Deep Learning for the Life Sciences. O\u2019Reilly Media, 2019. https://www.amazon.com/\nDeep-Learning-Life-Sciences-Microscopy/dp/1492039837.\n[74] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical in-\nformation and modeling, 50(5):742\u2013754, 2010.\n[75] Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel\nDas. Large-scale chemical language representations capture molecular structure and properties.\nNature Machine Intelligence, 4(12):1256\u20131264, 2022.\n15\n[76] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\nenables zero-shot task generalization. In ICLR 2022-Tenth International Conference on Learning\nRepresentations, 2022.\n[77] Jack W Scannell, Alex Blanckley, Helen Boldon, and Brian Warrington. Diagnosing the decline\nin pharmaceutical r&d efficiency. Nat. Rev. Drug Discov., 11(3):191\u2013200, 2012.\n[78] Oliver Schilter, Alain Vaucher, Philippe Schwaller, and Teodoro Laino. Designing catalysts with\ndeep generative models and computational data. a case study for suzuki cross coupling reactions.\nDigital Discovery, 2(3):728\u2013735, 2023.\n[79] Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro Laino. \u201cfound in\ntranslation\u201d: predicting outcomes of complex organic chemistry reactions using neural sequence-\nto-sequence models. Chemical science, 9(28):6091\u20136098, 2018.\n[80] Philippe Schwaller, Teodoro Laino, Th\u00b4eophile Gaudin, Peter Bolgar, Christopher A Hunter,\nCostas Bekas, and Alpha A Lee.\nMolecular transformer: a model for uncertainty-calibrated\nchemical reaction prediction. ACS central science, 5(9):1572\u20131583, 2019.\n[81] Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H Nair, Rico Andreas Haeusel-\nmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, and Teodoro Laino. Predicting retrosyn-\nthetic pathways using transformer-based models and a hyper-graph exploration strategy. Chem-\nical science, 11(12):3316\u20133325, 2020.\n[82] Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro\nLaino. Extraction of organic chemistry grammar from unsupervised learning of chemical reac-\ntions. Science Advances, 7(15):eabe4166, 2021.\n[83] Philippe Schwaller, Daniel Probst, Alain C Vaucher, Vishnu H Nair, David Kreutter, Teodoro\nLaino, and Jean-Louis Reymond. Mapping the space of chemical reactions using attention-based\nneural networks. Nature machine intelligence, 3(2):144\u2013152, 2021.\n[84] Philippe Schwaller, Alain C Vaucher, Ruben Laplaza, Charlotte Bunne, Andreas Krause,\nClemence Corminboeuf, and Teodoro Laino. Machine intelligence for chemical reaction space.\nWiley Interdisciplinary Reviews: Computational Molecular Science, 12(5):e1604, 2022.\n[85] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused\nmolecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):\n120\u2013131, 2018.\n[86] Taffee T Tanimoto. Ibm internal report. Nov, 17:1957, 1957.\n[87] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.\nGalactica: A large language\nmodel for science. arXiv preprint arXiv:2211.09085, 2022.\n[88] Igor V Tetko, Pavel Karpov, Eric Bruno, Talia B Kimber, and Guillaume Godin. Augmentation\nis what you need!\nIn International Conference on Artificial Neural Networks, pages 831\u2013835.\nSpringer, 2019.\n[89] Amol Thakkar, Alain C Vaucher, Andrea Byekwaso, Philippe Schwaller, Alessandra Toniato,\nand Teodoro Laino. Unbiasing retrosynthesis language models with disconnection prompts. ACS\nCentral Science, 2023.\n[90] Alessandra Toniato, Philippe Schwaller, Antonio Cardinale, Joppe Geluykens, and Teodoro\nLaino. Unassisted noise reduction of chemical reaction datasets. Nature Machine Intelligence, 3\n(6):485\u2013494, 2021.\n16\n[91] Umit V Ucak, Islambek Ashyrmamatov, and Juyong Lee. Improving the quality of chemical\nlanguage model outcomes with atom-in-smiles tokenization. Journal of Cheminformatics, 15(1):\n55, 2023.\n[92] Ruud van Deursen, Peter Ertl, Igor V Tetko, and Guillaume Godin. Gen: highly efficient smiles\nexplorer using autodidactic generative examination networks. Journal of Cheminformatics, 12\n(1):1\u201314, 2020.\n[93] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[94] Alain C Vaucher, Federico Zipoli, Joppe Geluykens, Vishnu H Nair, Philippe Schwaller, and\nTeodoro Laino. Automated extraction of chemical synthesis actions from experimental proce-\ndures. Nature communications, 11(1):3601, 2020.\n[95] Alain C Vaucher, Philippe Schwaller, Joppe Geluykens, Vishnu H Nair, Anna Iuliano, and\nTeodoro Laino. Inferring experimental procedures from text-based representations of chemical\nreactions. Nature communications, 12(1):2573, 2021.\n[96] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models, 10 2022. URL\nhttps://github.com/huggingface/diffusers.\n[97] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances\nin Neural Information Processing Systems, 35:24824\u201324837, 2022.\n[98] David Weininger. Smiles, a chemical language and information system. 1. introduction to method-\nology and encoding rules. J. Chem. Inf. Comp. Sci., 28(1):31\u201336, 1988.\n[99] Andrew D White, Glen M Hocky, Heta A Gandhi, Mehrad Ansari, Sam Cox, Geemi P\nWellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, et al.\nAssessment of\nchemistry knowledge in large language models that generate code. Digital Discovery, 2(2):368\u2013\n376, 2023.\n[100] Scott A Wildman and Gordon M Crippen. Prediction of physicochemical parameters by atomic\ncontributions. Journal of chemical information and computer sciences, 39(5):868\u2013873, 1999.\n[101] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Transformers: State-of-\nthe-art natural language processing. In Proceedings of the 2020 conference on empirical methods\nin natural language processing: system demonstrations, pages 38\u201345, 2020.\n[102] Olivier J Wouters, Martin McKee, and Jeroen Luyten. Estimated research and development\ninvestment needed to bring a new medicine to market, 2009-2018. Jama, 323(9):844\u2013853, 2020.\n[103] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S\nPappu, Karl Leswing, and Vijay Pande.\nMoleculenet: a benchmark for molecular machine\nlearning. Chemical science, 9(2):513\u2013530, 2018.\n[104] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule\nstructure and biomedical text with comprehension comparable to human professionals. Nature\ncommunications, 13(1):862, 2022.\n[105] Alex Zhavoronkov, Yan A Ivanenkov, Alex Aliper, Mark S Veselov, Vladimir A Aladinskiy,\nAnastasiya V Aladinskaya, Victor A Terentiev, Daniil A Polykovskiy, Maksim D Kuznetsov,\nArip Asadulaev, et al. Deep learning enables rapid identification of potent ddr1 kinase inhibitors.\nNat. Biotechnol., 37(9):1038\u20131040, 2019.\n17\n[106] Zhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yangtian\nZhang, Junkun Chen, Huiyu Cai, Jiarui Lu, et al. Torchdrug: A powerful and flexible machine\nlearning platform for drug discovery. Preprint at https://arxiv.org/abs/2202.08320, 2022.\n18\n"
  },
  {
    "title": "CCEdit: Creative and Controllable Video Editing via Diffusion Models",
    "link": "https://arxiv.org/pdf/2309.16496.pdf",
    "upvote": "7",
    "text": "CCEdit: Creative and Controllable Video Editing via Diffusion Models\nRuoyu Feng1,2 *, Wenming Weng1,2, Yanhui Wang1,2,\nYuhui Yuan2, Jianmin Bao2, Chong Luo2 \u2020, Zhibo Chen1 \u2020, Baining Guo2\n1University of Science and Technology of China 2Microsoft Research Asia\nhttps://ruoyufeng.github.io/CCEdit.github.io/\nIntention: Replace the background with snowy winter scene.\nIntention: Make the foreground puppy chubby and adorable.\nInput Video\nEdited Video\nIntention: Transform this video into the mechanical style.\nIntention: Transform this video into the cyberpunk style.\nIntention: Transform this video into the Van Gogh Starry Night style.\nFigure 1. Built upon diffusion models, CCEdit provides users with a powerful and flexible set of video editing capabilities, including style\ntransfer (row 1-3), foreground modifications (row 4), and background replacement (row 5).\nAbstract\nIn this paper, we present CCEdit, a versatile generative\nvideo editing framework based on diffusion models. Our\napproach employs a novel trident network structure that\nseparates structure and appearance control, ensuring pre-\ncise and creative editing capabilities. Utilizing the founda-\ntional ControlNet architecture, we maintain the structural\n\u2217 This work is done when Ruoyu Feng is an intern with MSRA.\n\u2020 Corresponding author.\nintegrity of the video during editing.\nThe incorporation\nof an additional appearance branch enables users to exert\nfine-grained control over the edited key frame. These two\nside branches seamlessly integrate into the main branch,\nwhich is constructed upon existing text-to-image (T2I) gen-\neration models, through learnable temporal layers.\nThe\nversatility of our framework is demonstrated through a di-\nverse range of choices in both structure representations and\npersonalized T2I models, as well as the option to provide\nthe edited key frame. To facilitate comprehensive evalua-\ntion, we introduce the BalanceCC benchmark dataset, com-\narXiv:2309.16496v2  [cs.CV]  1 Dec 2023\nprising 100 videos and 4 target prompts for each video. Our\nextensive user studies compare CCEdit with eight state-of-\nthe-art video editing methods. The outcomes demonstrate\nCCEdit\u2019s substantial superiority over all other methods.\n1. Introduction\nIn recent years, the domain of visual content creation and\nediting has undergone a profound transformation, driven\nby the emergence of diffusion-based generative models\n[11, 19, 50]. A large body of prior research has demon-\nstrated the exceptional capabilities of diffusion models in\ngenerating diverse and high-quality images [40, 42, 45] and\nvideos [5, 20, 48], conditioned by text prompts. These ad-\nvancements have naturally paved the way for innovations in\ngenerative video editing [7, 25, 35, 37, 54, 57, 58, 62].\nGenerative video editing, despite its rapid advancement,\ncontinues to face a series of significant challenges. These\nchallenges include accommodating diverse editing requests,\nachieving fine-grained control over the editing process, and\nharnessing the creative potential of generative models. Di-\nverse editing requirements include tasks such as stylistic al-\nterations, foreground replacements, and background modi-\nfications. Generative models, while powerful and creative,\nmay not always align perfectly with the editor\u2019s intentions\nor artistic vision, resulting in a lack of precise control. In\nresponse to these challenges, this paper introduces CCEdit,\na versatile generative video editing framework meticulously\ndesigned to strike a harmonious balance between controlla-\nbility and creativity while accommodating a wide range of\nediting requirements.\nCCEdit achieves its goal by effectively decoupling struc-\nture and appearance control in a unified trident network.\nThis network comprises three essential components: the\nmain text-to-video generation branch and two accompany-\ning side branches dedicated to structure and appearance ma-\nnipulation. The main branch leverages a pre-trained text-\nto-image (T2I) diffusion model [42], which is transformed\ninto a text-to-video (T2V) model through the insertion of\ntemporal modules. The structure branch, implemented as\nControlNet [59], is responsible for digesting the structural\ninformation extracted from each frame of the input video\nand seamlessly infusing it into the main branch. Simulta-\nneously, the appearance branch introduces an innovative\nmechanism for precise appearance control, when an edited\nreference frame is available. The structure and appearance\nbranches are effectively integrated into the central branch\nthrough learnable temporal layers. These layers serve not\nonly as a cohesive link, aggregating information from side\n1 CCEdit is currently a research project, and there are no immediate\nintentions to integrate it into a product or extend public accessibility.\nAny future research endeavor will adhere to Microsoft\u2019s AI principles.\nbranches, but also as a crucial element ensuring temporal\nconsistency across the generated video frames.\nIn highlighting the versatility of our framework, we pro-\nvide a wide range of control choices for both structure and\nappearance manipulation. For structure control, users can\nchoose from various types of structural information, in-\ncluding line drawings [8], PiDi boundaries [51], and depth\nmaps [41], all of which can serve as input to the structure\nbranch. On the appearance control front, the main branch\nalready provides an inherent mechanism, allowing control\nthrough text prompts. Additionally, personalized T2I mod-\nels from the Stable Diffusion community, such as Dream-\nBooth and LoRA [21, 44], can be integrated as plugins into\nCCEdit, offering greater flexibility and creativity.\nMore\nimportantly, the appearance branch can accommodate the\nreferenced key frame, facilitating fine-grained appearance\ncontrol. Notably, all these control options are seamlessly\nintegrated within the same framework, yielding editing out-\ncomes that demonstrate both temporal coherence and pre-\ncision. This not only underscores the versatility of our so-\nlution but also ensures ease of adoption, making it a com-\npelling choice for AI-assisted video editing.\nTo address the challenges inherent in evaluating gener-\native video editing methods, we introduce the BalanceCC\nbenchmark dataset. Comprising 100 diverse videos and 4\ntarget prompts for each video, this dataset includes detailed\nscene descriptions and attributes related to video category,\nscene complexity, motion, among others. These descrip-\ntions are generated with the assistance of the cutting-edge\nGPT-4V(ision) model [1, 32\u201334] and then refined by hu-\nman annotators. Through extensive experimental evalua-\ntions on this dataset, we not only confirm the outstanding\nfunctionality and editing capabilities of CCEdit, but also un-\nderscore the comprehensiveness of the benchmark dataset.\nWe firmly believe that BalanceCC stands as a robust and\nall-encompassing evaluation platform for the dynamic field\nof generative video editing.\n2. Related Work\n2.1. Diffusion-based Image and Video Generation\nDiffusion models (DM) [11, 19, 50] have demonstrated ex-\nceptional capabilities in the field of image synthesis. These\nmodels indeed help by learning to approximate a data dis-\ntribution through the iterative denoising of a diffused in-\nput. What makes DMs truly practical is the incorporation\nof text prompt as condition to control the output image dur-\ning the generative process [31, 39, 42, 45].\nApart from\nthe proliferation of advanced techniques in the field of im-\nage synthesis, DMs have also excelled in video generation\n[5, 20, 31, 48]. This is achieved by integrating modulated\nspatial-temporal modules, enabling the synthesis of high-\nquality videos while maintaining temporal consistency.\nP: A bear is walking, in winter.\n<S: Depth> <B: SD v1.5> \n<w/ reference>\nAppearance Control:\n\u2022 Text prompt, e.g. \u201cA bear is walking, anime style.\u201d\n\u2022 Personalized models\n\u2022 Edited key frame:\nStructure Information\nStructure Extraction\nKey Frame Editing\nText \nprompt\n~\ud835\udca9(\ud835\udfce, \ud835\udc70)\nP: A bear is walking, pixel art style.\n<S: Depth> <B: Counterfeit> \n<L: Pixel Art> <w/ Reference>\nA bear is walking, anime style.\n<S: Line drawings> \n<B: Toonyou>\nP: A bear is walking, Chinese Ink style.\n<S: PiDi Boundary> <B: Counterfeit> \n<L: Moxin>\nP: A tiger is walking.\n<S: PiDi Boundary> <B: SD v1.5>\n<w/ Reference>\nBase Model: ToonYou LoRA: Pixel Art\nStructure Control:\nDepth\nLine Drawing\n\u2026 \nStyle \ntransfer\nForeground \nreplacement\nBackground \nmodification\n\u2026 \nLearnable appearance encoder\nLearnable temporal layers\nFrozen structure ControlNet\nFrozen T2I model\nPidi Boundary\n\u2026 \nLoRA: Moxin\nFigure 2. Illustration of our overall framework. Structure and appearance information in the target video are modulated independently,\nand seamlessly integrated into the main branch. Structure control is conducted via the pre-trained ControlNet [59]. Appearance control is\nachieved precisely by the edited key frame. Details regarding the autoencoder and iterative denoising process are omitted for simplicity.\n\u201cP\u201d, \u201cS\u201d, \u201cB\u201d, \u201cL\u201d indicate prompt, structure, base model, and LoRA, respectively.\n2.2. Video Editing with Diffusion Models\nRecent studies leverage the inherent generative priors of\nDMs for image editing [3, 10, 16, 27, 36, 52]. The same\nidea is also applied in the field of video editing. Unlike\nimage editing, video editing involves not only the manip-\nulation of appearance-based attributes but also requires the\nmeticulous preservation of temporal coherence throughout\nframes. A lapse in maintaining this temporal coherence can\nresult in visual artifacts, such as flickering and degradation.\nSome generative video editing methods [6, 14, 22, 37,\n53, 58, 60] strive to achieve training-free temporal con-\nsistency. They accomplish this by transitioning from spa-\ntial self-attention mechanisms within T2I diffusion models\nto temporal-aware cross-frame attention techniques. Some\nother methods [26, 47, 55, 62] perform per-video fine-\ntuning. They focus on optimizing the parameters of pre-\ntrained T2I models according to the input video, aiming to\nachieve temporal coherence within the target video. How-\never, this optimization for each input video can be time-\nconsuming and inadequate tuning of the temporal modules\nmight lead to suboptimal temporal coherence. Recent stud-\nies [15, 24, 57] have introduced trainable temporal layers to\nconstruct T2V generative models. These models are trained\non extensive text-video paired datasets, and they are used in\nboth video generation and editing tasks [12, 29].\nUnlike previous work, this study does not seek a sim-\nple fix to existing T2I models for video editing, nor does\nit attempt to train a full-fledged T2V model. Instead, we\nintroduce a unique network architecture tailored for video\nediting.\nOur approach involves dataset-level fine-tuning,\ncircumvents the expenses associated with per-video tuning\nduring inference time, and prioritizing the effective training\nof temporal layers to achieve robust model performance.\n3. Approach\n3.1. Preliminary\nDiffusion models [19] are probabilistic generative mod-\nels that approximate a data distribution p(x) by gradu-\nally denoising a normally distributed variable. Specifically,\nDMs aim to learn the reverse dynamics of a predetermined\nMarkov chain with a fixed length of T. The forward Markov\nchain can be conceptualized as a procedure of injecting\nnoise into a pristine image. Empirically, DMs can be in-\nterpreted as an equally weighted sequence of denoising au-\ntoencoders \u03f5\u03b8(xt, t) where t = 1, ..., T.\nThese autoen-\ncoders are trained to predict a denoised variant of the noisy\ninput xt. The corresponding objective can be simplified to\nEx0,t,\u03f5\u223cN (0,I)[\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2].\n(1)\nLatent diffusion models (LDMs) are trained in the learned\nlatent representation space. The bridge between this latent\nspace and the original pixel-level domain is established via\na perceptual compression model. The perceptual compres-\nsion model is composed of an encoder E and a decoder D,\nwhere z = E(x) and x \u2248 D(E(x)). Then the optimization\nobjective in Eq. (1) is modified as\nEz0,t,\u03f5\u223cN (0,I)[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t)\u22252\n2].\n(2)\n3.2. The CCEdit Framework\nThe primary objective of our work is to empower cre-\native control in video editing. Although creativity naturally\nemerges in generative models, achieving controllability is a\nmore complex endeavor. To address this challenge, CCEdit\nstrategically decouples the management of structure and ap-\npearance within a unified trident network. In Fig. 2, we\nprovide an illustrative overview of the framework\u2019s archi-\ntecture, which comprises three vital components.\nThe main branch. The main branch of our model funda-\nmentally operates as a text-to-video generation network. It\nis built upon the well-established text-to-image model, Sta-\nble Diffusion [42]. We transform this model into a text-to-\nvideo variant by incorporating temporal layers into spatial\nlayers of both the encoder and decoder. This entails the ad-\ndition of a one-dimensional temporal layer with the same\ntype as its previous spatial layer, i.e., convolution blocks\nand attention blocks. Besides, we also use the skip connec-\ntion and zero-initialized projection out layer of each newly\nadded temporal layer for stable and progressive updating,\nwhich has been proven to be effective [15, 48, 59]. The\nzero-initialized projection out layer is instantiated as a lin-\near layer. Formally, let F(\u00b7; \u0398s) be the 2D spatial block,\nF(\u00b7; \u0398t) be the 1D temporal block, and Z(\u00b7; \u0398z) be the\nzero-initialized projection out layer, where \u0398s, \u0398t, and \u0398z\nrepresent corresponding network parameters. The complete\nprocess of one pseudo-3D block that maps the input feature\nu to the output feature v is written as\nv = F(u; \u0398s) + Z(F(F(u; \u0398s); \u0398t); \u0398z),\n(3)\nwhere u and v are both 3D feature maps, i.e., u\n\u2208\nRl\u00d7h\u00d7w\u00d7c with {l, h, w, c} as the number of frames, height,\nwidth, and the number of channels, respectively.\nMoreover, we draw inspiration from AnimateDiff [15]\nand VideoLDM [5], which advocates the shared utilization\nof temporal layers among personalized T2I models such\nas DreamBooth [44] and LoRA [21]. The key aspect of\nit is training the temporal layers while keeping the spatial\nweights frozen. We follow this schedule to inherit the T2I\nmodel\u2019s compatibility and visual generation capability.\nThe structure branch. The introduction of the structure\nbranch is motivated by the common need in video editing\ntasks to preserve frame structure for non-edited or style-\ntransferred segments. Striking a delicate balance between\nmaintaining faithful frame structure and allowing the gener-\native model ample creative freedom poses a significant chal-\nlenge. The structure branch is implemented with the pre-\ntrained ControlNet [59]. To accommodate varying levels\nof structure control, we use various types of structure repre-\nsentation, including line drawings [8], PiDi boundaries [51],\nand depth maps [41], ensuring adaptability to control struc-\nture at different degrees.\nSpecifically, the structure representation from all frames\nis extracted individually and injected into the main branch.\nEach frame undergoes preprocessing to derive a structure\nrepresentation, and the weights of the ControlNet are held\nin a frozen state during training, emphasizing the preserva-\ntion of learned structural features. Formally, let F(\u00b7; \u03a6c)\ndenote the ControlNet that maps structure information into\nfeatures, and Z(\u00b7; \u03a6z1) and Z(\u00b7; \u03a6z2) denote the two in-\nstances of zero convolutions in [59]. Then the process of\nadding structure control to the 3D-aware feature v is\nvs = v + Z(F(zt + Z(cs; \u03a6z1); \u03a6c); \u03a6z2),\n(4)\nwhere zt denotes the noisy input in latent space, cs denotes\nthe structure condition of the video sequence, and vs de-\nnotes the feature aware of structure information.\nThe appearance branch. In addition to using text prompts\nand incorporating personalized models for appearance con-\ntrol, we introduce a novel design\u2014the appearance branch.\nThis architectural innovation introduces a pioneering ap-\nproach for fine-grained appearance control, allowing for the\nintegration of an edited frame as a detailed reference in the\ncontext of video editing. Since the editing of key frame\ncan be accomplished through precise user edits or by us-\ning advanced off-the-shelf image editing algorithms, the in-\ntroduction of appearance branch provides our framework\nwith greater creativity and controllability. Specifically, a\nkey frame is initially assigned to the latent variable by the\nencoder E. Subsequently, a neural network with similar ar-\nchitecture to the main branch\u2019s encoder extracts multi-scale\nfeatures. The extracted features are incorporated into the\nmain branch. Through this design, the appearance informa-\ntion from the edited key frame propagates to all frames via\nthe temporal modules, effectively achieving the desired cre-\native control in the output video. Formally, suppose F(\u00b7; \u03a8)\nis the encoder that maps the pixel-wise appearance of the\nkey frame into features, Z(\u00b7; \u03a8z) denotes the zero convolu-\ntion projection out layer, vj indicates the feature of the j-th\nframe, and cj\na is the key frame. Then the process of adding\nappearance control to the features is as follows\nvj\na = vj + Z(F(E(cj\na); \u03a8); \u03a8z),\n(5)\nwhere vj\na is the j-th feature, aware of the edited appearance.\nTraining. Before training, we initialize the spatial weights\nof the main branch with pre-trained T2I models. Temporal\nweights are randomly initialized while the projection out\nlayers are zero-initialized. We instantiate the model in the\nstructure branch by pre-trained ControlNets [59]. As for\nthe appearance branch, we copy the encoder of pre-trained\nT2I model and remove text cross-attention layers. During\ntraining, given the latent variables z0 = E(x0) of an input\nvideo clip x0. Diffusion algorithms progressively add noise\nto it and produce the noisy input zt. Given conditions of\ntime step t, text prompt ct, structure information cs, and\nappearance information cj\na of the key frame, the overall op-\ntimization objective is\nEz0,t,ct,cs,cj\na,\u03f5\u223cN (0,I)[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, ct, cs, cj\na)\u2225\n2\n2],\n(6)\nwhere \u03f5\u03b8 indicates the whole network to predict the noise\nadded to the noisy input zt. We freeze the spatial weights\nin the main branch and the weights in the structure branch.\nConcurrently, we update the parameters of the newly incor-\nporated temporal layers in the main branch, as well as the\nweights in the appearance branch. By default, the appear-\nance branch takes the center frame of the video clip as input.\nInference with anchor prior. We find that, in some chal-\nlenging cases, the edited video may exhibit large areas of\nflickering. This is often caused by inconsistent structural\nrepresentations extracted by image-level pre-processing\nmodules. Therefore, we propose a simple yet efficient strat-\negy to improve the stability and quality of the result by mod-\nifying the start noise. Specifically, consider the individual\nnoise sequence [\u03f51\nind, ..., \u03f5l\nind] and the edited center frame cj\na,\nwhere l and j indicate the frame numbers and the index of\nthe edited key frame, respectively. The start noise \u03f5i for\neach frame is modified as\n\u03f5i = \u03f5i\nind + \u03b1E(cj\na),\n(7)\nwhere \u03b1 is the hyperparameter that controls the strength\nof prior, and E(cj\na) is the latent of the edited key frame.\nWe call this strategy anchor prior, which is tailored for our\npipeline of editing videos with an reference key frame. We\nempirically found that \u03b1 = 0.03 works well in most cases.\nThe intuition behind it lies in that the video frames are usu-\nally similar to each other. The operation of adding noise to\ndiffusion models tends to rapidly destroy high-frequency in-\nformation while slowly degrading low-frequency informa-\ntion. Therefore, the anchor prior can be seen as provid-\ning a bit of low-frequency information to all frames while\nensuring that the distribution remains almost unchanged\n(achieved by small \u03b1), thus becoming better starting points.\n3.3. Editing for Long Videos\nVideo editing tools face a challenge in maintaining a con-\nsistent look and feel across clips that span tens of seconds,\nequivalent to hundreds of frames. The inherent limitation\nof generative models, processing only a dozen frames per\ninference due to memory constraints, introduces variability\nin results, even with a fixed random seed. CCEdit addresses\nthis challenge with its fine-grained appearance control, en-\nabling the editing of long videos into a cohesive look and\nfeel through extension and interpolation modes.\nIn essence, let L + 1 represent the frames CCEdit pro-\ncesses in one run. For videos exceeding L + 1 frames, we\nselect one key frame for every L frames. In the initial run,\nthe first L+1 key frames undergo editing. Subsequent runs,\nin extension mode, treat the last edited frame from the pre-\nvious run as the first frame. The edited result serves as a\nreference for the appearance branch. This process iterates\nuntil all key frames are processed. Transitioning to the in-\nterpolation mode, two adjacent frames become the first and\nlast frames of an inference run to edit the L \u2212 1 intermedi-\nate frames, and both edited frames serve as references for\nthe appearance branch. This continues until all frames are\nedited. This meticulous process ensures consistent editing\nresults throughout the entire video.\n4. BalanceCC Benchmark\n4.1. Overview\nWhile generative video editing has gained considerable at-\ntention as a growing research field, the absence of a stan-\ndardized benchmark for assessing the efficacy of different\napproaches poses a potential hindrance to the technical pro-\ngression of the field.\nDespite the recent introduction of\nTGVE 2023 [56] as an evaluation benchmark, it is crucial\nto note that the videos within this benchmark present chal-\nlenges such as severe camera shake, overly complex scenes,\nblur, and low frame rates. In response to this, we introduce\nBalanceCC, a benchmark that contains 100 videos with var-\nied attributes, designed to offer a comprehensive platform\nfor evaluating video editing, focusing on both controllabil-\nity and creativity.\n4.2. Benchmark Establishment\nWe curated a collection of 100 open-license videos suitable\nfor legal, non-stigmatizing modifications.\nThese videos\nrange from 2 to 20 seconds in duration, each with a frame\nrate of about 30 fps. Besides, we utilize GPT-4V(ision) [1,\n32\u201334] as an assistant to establish this benchmark. For each\nvideo, GPT-4V(ision) provides a description and assigns a\ncomplexity score to the scene using the center frame as a\nreference, with ratings from 1 (Simple) to 3 (Complex).\nAdditionally, we manually annotate each video for camera\nmovement, object movement, and categorical content, with\nmotion rated on a scale from 1 (Stationary) to 3 (Quick), and\ncategories that include humans, animals, objects, and land-\nscapes. Following this, GPT-4V(ision) is tasked to craft tar-\nFigure 3. Illustration of the statistics on BalanceCC.\nget prompts for video editing, encompassing style, object,\nand background alterations, along with compound changes.\nThis process, while akin to TGVE 2023 [56], we addition-\nally introduce a \u201cFantasy Level\u201d to indicate the imaginative\nand creative degree of the target prompt. These measures\nare intended to assist researchers in appraising the appli-\ncability of various methods to source videos and in gaug-\ning their potential. See supplementary for details on the\nprompting pipeline, specific instructions, principles of la-\nbeling, and illustrative examples.\n4.3. Statistics\nThe overall distribution of BalanceCC is illustrated in Fig.\n3. For the data of original videos, the distribution across cat-\negories tends towards uniformity, yet the \u201cHuman\u201d category\nis slightly more prevalent than others. This was a deliber-\nate choice, as editing human subjects is more practically\nsignificant and, due to the complexity of human and facial\nstructures, editing in the \u201cHuman\u201d category presents more\nchallenges.\nRegarding \u201cScene Complexity\u201d and \u201cObject\nMotion\u201d, videos with moderate and slow levels are slightly\nmore common. In terms of \u201cCamera Motion\u201d, videos of\nlower levels predominate (Stationary: 54%, Slow: 38%).\nFinally, regarding the \u201cFantasy Level\u201d distribution in tar-\nget prompts, there is a relatively balanced allocation, with\na marginal inclination towards videos categorized at a mod-\nerate level.\nWe hope that the aforementioned categorization of the\nbenchmark will better assist researchers and users in under-\nstanding the strengths and weaknesses of a method, thus\nenabling targeted improvements and fostering rapid devel-\nopment in the field.\n5. Experiments\n5.1. Implementation Details\nStable Diffusion-v1.5 is used as the base T2I model in the\nmain branch. We use the pre-trained ControlNet [59] for the\nstructure information guidance. The training dataset com-\nInput \nVideo\nLine \nDrawing\nPiDi\nBoundary\nScribble\nDepth\n\u201cA mechanical tiger is walking.\u201d\nFigure 4. Results under different structural guidance.\n< MeinaMix, Pixel Art Style> \n\u201cA bear is walking, pixel art style.\u201d\n<ReV Animated, Moxin> \n\u201cA boat is sailing, Chinese traditional ink style.\u201d\n<A-ZovyaRPGArtistTools> \n\u201cAn astronaut rides a motorbike, planets in back.\u201d\nFigure 5. Results of video style translation. \u27e8\u00b7\u27e9 indicate the per-\nsonalized T2I model we used.\nbines WebVid-10M [4] and a self-collected private dataset.\nWe trained the temporal consistency modules and appear-\nance ControlNet towards various types of structural infor-\nmation, including line drawings [8], PiDi boundaries [51],\ndepth maps detected by Midas [41], and human scribbles.\nDepth maps are used by default. The control scales are set\nas 1. For the temporal interpolation model, we train it ex-\nclusively on depth maps, employing a smaller control scale\nof 0.5. This approach is adopted because its requirement\nfor structural information is comparatively less than that of\nother models. During the training process, we first resize the\nshorter side to 384 pixels, followed by a random crop to ob-\ntain video clips with a size of 384 \u00d7 576. 17 frames at 4 fps\nare sampled from each video. The batch size is 32 and the\nlearning rate is 3e \u2212 5. We train each model for 100K itera-\ntions. During inference, we employ the DDIM [49] sampler\nwith 30 steps, classifier-free guidance [18] of magnitude 9.\nEdited \nVideo\nKey\nFrame\nInput \nVideo\n<Hellomecha, Building Block World> \n\u201cA building block style car in the parking lot.\u201d\n<Counterfeit> \n\u201cA man hikes on the moon, anime style.\u201d\n<ReV Animated> \n\u201cA paladin in armor rides on motorcycle, on fire.\u201d\nFigure 6. Video editing results with customized center frame as\nreference. The first row corresponds to customizing foreground,\nthe second row corresponds to customizing background, and the\nthird row is taking given reference image to affect the entire pic-\nture. \u27e8\u00b7\u27e9 indicate the personalized T2I model we used.\n5.2. Applications\nControllable and creative style transfer. In CCEdit, the\ncontrollability and creativity of video style transfer are man-\nifested in various dimensions. Two basic aspects include\nthe diversity of structural information and the availability\nof off-the-shelf personalized models [9, 13]. The former en-\nables users to customize the granularity and type of struc-\ntural information retained from the original video, as de-\npicted in Fig. 4. The latter allows users to edit the video\ninto their desired domain, as shown in Fig. 5.\nVideo editing with precise appearance control. Some-\ntimes, users require stronger control over the content they\nwant to generate. For example, they may want to change\nonly the foreground, alter just the background, or edit the\ntexture content of a video in a specific way.\nTherefore,\nCCEdit focuses more on precise appearance control by ini-\ntially modifying the key frame with image editing tech-\nniques and then using it as a reference for the entire video.\nAs depicted in Fig. 6, we first edit the center frames of the\nvideos by Stable Diffusion Web UI [2], followed by utiliz-\ning these edited center frames as guides for the video editing\nprocess. Thanks to end-to-end network training, our method\ncoherently propagates edits from the key frame throughout\nthe entire video.\nLong video editing. A seamless and visually appealing\nvideo typically necessitates a higher frame count and in-\ncreased frame rate, elements that have been inadequately\naddressed by many contemporary video editing methodolo-\ngies. CCEdit effectively resolves this through its hierarchi-\ncal design for key frames editing, combined with iterative\nextension and a tailored temporal interpolation mechanism.\nThis approach enables the editing of videos comprising up\nFrame: 1\nFrame: 61\nFrame: 121\nOriginal \nVideo\nEdited \nVideo\nFrame: 181\nFrame: 241\n\u201cCity at night, in winter.\u201d\nFigure 7. Illustration of long video editing. CCEdit achieves\ngood consistency across over 240 frames. Zoom in for best view.\nInput \nVideo\nPix2Video\nTokenFlow\nCCEdit\n\u201cA person riding a horse over an obstacle, Van Gogh style.\u201d\nFigure 8. Qualitative comparison results. Red boxes reveals\nTokenFlow\u2019s inadequate local detail preservation, in contrast to\nour method\u2019s detailed, coherent output. Zoom in for best view.\nto hundreds of frames with 24 fps (frames per second). An\nexample is shown in Fig. 7.\n5.3. State-of-the-Art Comparisons\nDatasets. We employ a smaller segment of our proposed\nbenchmark, designated as mini-BalanceCC. This subset en-\ncompasses 50 videos, each randomly selected from the orig-\ninal BalanceCC dataset, ensuring a representative distribu-\ntion similar to that of the original collection.\nCompared methods.\nTo conduct an exhaustive com-\nparison, we have selected eight representative video edit-\ning methodologies: Tune-A-Video [55], vid2vid-zero [53],\nText2Video-zero [22], FateZero [37], Pix2Video [6], Con-\ntrolVideo [60],\nRerender A Video [58],\nand Token-\nFlow [14]. Method details are omitted for brevity, and can\nbe found in supplementary. Regarding our approach, we\nemploy depth maps as structure control. For the appear-\nance control, we adopt the off-the-shelf method of PnP-\nDiffusion [52] with the same hyper-parameters to automat-\nMethod\nEdit Aes. Tem. Ove.\nWin\nTie\nLose\nTune-A-Video [55]\n3.24 3.01 2.72\n2.77\n16.4\n6.9\n76.7\nvid2vid-zero [53]\n3.00 2.38 2.11\n2.35\n10.6\n4.6\n84.8\nText2Video-Zero [22]\n2.10 1.40 1.40\n1.50\n16.5\n1.3\n86.2\nFateZero [37]\n2.47 3.16 3.30\n2.79\n16.6\n3.6\n79.8\nPix2Video [6]\n3.68 2.97 2.80\n2.97\n29.9\n5.2\n64.9\nControlVideo [60]\n3.01 2.71 2.60\n2.66\n13.8\n5.6\n80.6\nRerender A Video [58]\n2.40 2.69 2.82\n2.50\n11.1\n0.0\n88.9\nTokenFlow [14]\n3.78 3.61 3.79\n3.58\n32.4 14.7 52.9\nCCEdit (Ours)\n4.06 4.00 3.74\n3.87\n-\n-\n-\nTable 1. Left: Mean opinion scores (MOS) over different as-\npects of the generated video, including editing accuracy (Edit),\naesthetics (Aes.), temporal consistency (Tem.), and overall impres-\nsion (Ove.). Scores range from 1 to 5. Right: Win, Tie, and Lose\npercentage in side-by-side comparisons with CCEdit.\nically edit the center frame of each video clip. To ensure\nfairness in comparison, Stable Diffusion-v1.5 is used as the\nbase model for all methods.\nEvaluation metrics. In our preliminary study, we observed\nthat automatic metrics, such as CLIP-Score [17] to assess\ntext alignment and frame consistency, do not fully align\nwith human preferences [29, 56, 61]. We focused on col-\nlecting human preferences for a comprehensive user study,\ncomparing our method against recent state-of-the-art tech-\nniques based on mean opinion score (MOS) and direct com-\nparisons. We gathered 1,119 scoring results from 33 volun-\nteers, each reflecting all indicators for an edited video. For\nautomatic metric results, refer to the supplementary.\nResults. As illustrated in Tab. 1, CCEdit excels in both\nediting accuracy and aesthetic quality, and is just slightly\ninferior to TokenFlow in temporal smoothness. For over-\nall impression, our approach achieved a MOS of 3.87 on a\nscale from 1 to 5. Among the eight reference methods, To-\nkenFlow performed closest to ours, with an overall MOS\nof 3.58.\nThe remaining seven methods scored between\n1.5 to 3.0 on the MOS scale. As for direct comparisons,\nour method outperforms all eight reference schemes signif-\nicantly. While TokenFlow remains the closest competitor,\nour CCEdit prevails in 52.9% of test cases against it, trails\nin 32.4%, and ties in 14.7% of cases.\nFurthermore, Fig.\n8 presents the qualitative results\nof the top three finalists (CCEdit, TokenFlow [14], and\nPix2Video [6]). It shows that Pix2Video struggles to keep\ntemporal coherence, while TokenFlow demonstrates no-\nticeable blurring. In contrast, our method can accurately\nachieve the editing objective while maintaining the tempo-\nral coherence as well as the structure of the input video.\n5.4. Ablation Study\nAppearance control. Fig. 9 illustrates the importance of\ntaking the edited key frame as a reference in certain scenar-\nios. Initially, translating video scenes into \u201ccyberpunk\u201d\nw/o edited \nkey frame\nw/ edited \nkey frame\n\u201cCity at night, in cyberpunk style, with neon lights.\u201d\nInput \nVideo\nFigure 9. Ablation study on appearance control. In some chal-\nlenging cases, appearance control is crucial to achieving the ex-\npected results.\nw/o anchor \nprior\nw/ anchor \nprior\n\u201cA man wanders in the field, with the Milky Way in the sky.\u201d\nInput\nVideo\nFigure 10. Ablation study on anchor prior. Our proposed anchor\nprior helps a lot in stabilizing the appearance across frames. The\nred boxes demonstrate the localized flickering in the frames.\nstyle (1st row) solely through prompt adjustments appears\nchallenging, as this word is unfamiliar to the pre-trained\nT2I model weights and the temporal consistency modules.\nProviding a customized center frame allows the network to\nsmoothly extend its appearance to adjacent frames, creat-\ning a cohesive video. Besides, we replicated the user study\npipeline from Sec. 5.3 to evaluate the effectiveness of ap-\npearance control. The model without appearance control\nreceived a mean opinion score (MOS) of 2.88, significantly\nlower than the 3.87 scored by the process of editing one key\nframe first and then propagating to surrounding frames.\nAnchor prior. Fig. 10 demonstrates the ablation study for\nour anchor prior. It reveals that the absence of the anchor\nprior may lead to regional flickering in the video sequence,\nwhile its presence effectively mitigates this issue.\n6. Limitation and Future Works\nIn our approach, structural control is exerted by explic-\nitly extracting the structural representation from the source\nvideo and sustaining it via the structure branch.\nHow-\never, it may encounter challenges when tasked with sub-\nstantial structural alterations-exemplified by the conversion\nof a \u201ccute rabbit\u201d into a \u201cmajestic tiger.\u201d Addressing these\ncomplexities will be a primary objective of our future work.\n7. Conclusion\nThis paper presents an innovative trident network archi-\ntecture specifically designed for generative video editing.\nThis unified framework enables precise and controllable\nvideo editing while broadening creative possibilities. To\naddress the challenges in evaluating generative video edit-\ning approaches, we introduce the meticulously curated Bal-\nanceCC benchmark dataset. Our aim is to pave the way\nfor researchers in the generative video editing domain and\nequip practitioners with indispensable tools for their cre-\native workflows.\nReferences\n[1] Chatgpt can now see, hear, and speak. https://openai.\ncom/blog/chatgpt-can-now-see-hear-and-\nspeak, 2023. 2, 5, 12\n[2] AUTOMATIC1111. Stable Diffusion Web UI, 2022. 7\n[3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022. 3\n[4] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 6\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2, 4, 17\n[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 23206\u201323217, 2023.\n3, 7, 8, 14, 15,\n16\n[7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 23040\u201323050, 2023. 2\n[8] Caroline Chan, Fr\u00b4edo Durand, and Phillip Isola. Learning to\ngenerate line drawings that convey geometry and semantics.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7915\u20137925, 2022. 2,\n4, 6\n[9] Civitai. Civitai. https://civitai.com/, 2022. 7, 14\n[10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and\nMatthieu Cord. Diffedit: Diffusion-based semantic image\nediting with mask guidance. In The Eleventh International\nConference on Learning Representations, 2022. 3\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[12] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023. 3\n[13] Hugging Face. Hugging face. https://huggingface.\nco/, 2022. 7\n[14] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 3, 7, 8, 14,\n15, 16, 17\n[15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 3, 4, 14\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In The Eleventh Inter-\nnational Conference on Learning Representations, 2022. 3\n[17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Processing,\npages 7514\u20137528, 2021. 8, 15\n[18] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 6\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 3\n[20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[21] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-\nrank adaptation of large language models. In International\nConference on Learning Representations, 2021. 2, 4, 14\n[22] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 3, 7, 8, 14, 15, 16\n[23] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 15\n[24] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,\nand Jiashi Feng. Magicedit: High-fidelity and temporally\ncoherent video editing.\narXiv preprint arXiv:2308.14749,\n2023. 3\n[25] Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao,\nYuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, and\nMike Zheng Shou. Dynvideo-e: Harnessing dynamic nerf\nfor large-scale motion-and view-change human-centric video\nediting. arXiv preprint arXiv:2310.10624, 2023. 2\n[26] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 3\n[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2021. 3\n[28] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 14\n[29] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3, 8, 15\n[30] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023.\n17\n[31] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning,\npages 16784\u201316804. PMLR, 2022. 2\n[32] OpenAI. Gpt-4v(ision) system card. 2023. 2, 5, 12\n[33] OpenAI. Gpt-4v(ision) technical work and authors. https:\n//cdn.openai.com/contributions/gpt- 4v.\npdf, 2023.\n[34] OpenAI. Gpt-4 technical report, 2023. 2, 5, 12\n[35] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen.\nCodef: Content deformation fields for\ntemporally consistent video processing.\narXiv preprint\narXiv:2308.07926, 2023. 2\n[36] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 3\n[37] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 2, 3, 7, 8, 14, 15, 16\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 15\n[39] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n2\n[40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[41] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE transactions on pattern analysis and machine\nintelligence, 44(3):1623\u20131637, 2020. 2, 4, 6\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 4, 13\n[43] Denis Zavadski Carsten Rother. Controlnet-xs. 2023. 17\n[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 4, 14\n[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[46] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 13\n[47] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee,\nand Sungroh Yoon. Edit-a-video: Single video editing with\nobject-aware consistency. arXiv preprint arXiv:2303.07945,\n2023. 3\n[48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. In The Eleventh International Con-\nference on Learning Representations, 2022. 2, 4\n[49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2020. 6, 14\n[50] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2020. 2\n[51] Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao,\nQi Tian, Matti Pietik\u00a8ainen, and Li Liu.\nPixel difference\nnetworks for efficient edge detection.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5117\u20135127, 2021. 2, 4, 6\n[52] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 3, 7, 16\n[53] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3, 7, 8, 14, 15, 16\n[54] Yuanzhi Wang, Yong Li, Xin Liu, Anbo Dai, Antoni Chan,\nand Zhen Cui. Edit temporal-consistent videos with image\ndiffusion model. arXiv preprint arXiv:2308.09091, 2023. 2\n[55] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 3, 7, 8, 14, 15, 16\n[56] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jin-\nbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei\nHuang, Yuanxi Sun, et al. Cvpr 2023 text guided video edit-\ning competition. arXiv preprint arXiv:2310.16003, 2023. 5,\n6, 8, 15\n[57] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang\nJiang. Simda: Simple diffusion adapter for efficient video\ngeneration. arXiv preprint arXiv:2308.09710, 2023. 2, 3\n[58] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 2, 3, 7,\n8, 14, 15, 16\n[59] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 2, 3, 4, 5, 6, 12, 14\n[60] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 3, 7, 8, 14, 15, 16\n[61] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han,\nTiande Guo, and Luoqi Liu. Towards consistent video edit-\ning with text-to-image diffusion models.\narXiv preprint\narXiv:2305.17431, 2023. 8, 15\n[62] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and\nJun Zhu. Controlvideo: Adding conditional control for one\nshot text-to-video editing. arXiv preprint arXiv:2305.17098,\n2023. 2, 3\nA. Details of the Trident Network\nThe detailed architecture of our proposed trident network\nis illustrated in Fig. 11. Specifically, in the appearance\nbranch, the edited key frame cj\na is encoded by the VAE\nencoder E. Then it\u2019s fed into the encoder of appearance\nbranch.\nSubsequently, the features extracted from each\nlayer are fed into zero convolutions and the output are added\nto the corresponding features in the encoder side of main\nbranch. On the right side, i.e., the structure branch, struc-\nture information cs of original video clip is encoded by\nthe zero convolution and fed into structure branch encoder.\nSimilar to the appearance branch, features extracted are fed\ninto zero convolutions. Differently, the output are added\nto the corresponding features in the decoder side of main\nbranch.\nThe structure branch is instantiated by Control-\nNet [59]. Note that in the original paper of ControlNet, it\nconsists a tiny network to encode the pixel-wise structure\nrepresentation. Here we omit it for simplicity. Ultimately,\nthe appearance information within the key frame is propa-\ngated to all frames through the temporal modules and the\ninherited structure information will ensure the structural fi-\ndelity, achieving the stable and controllable editing.\nIt is important to highlight that, we don\u2019t use a train-\nfrom-scratch tiny encoder to encode the condition as Con-\ntrolNet [59] does in the appearance branch. Instead, we use\nthe VAE encoder E to map the pixel-wise appearance into\nlatent variable, which is in the same representation space\nas latent variable z0. The intuition behind is its inherent\ncapacity to act as a natural bridge, mapping pixel-wise ap-\npearance into the latent space which is exactly the U-Net\nworks in. Consequently, we are able to seamlessly copy the\nweights from the main branch encoder to initialize appear-\nance branch, thereby accelerating and stabilizing the con-\nvergence process.\nB. BalanceCC Benchmark\nOur objective is to develop a benchmark dataset specifically\ndesigned for tasks involving controllable and creative video\nediting. Therefore, we collected 100 open-license videos of\ndifferent categories, including Animal, Human, Object, and\nLandscape. In addition, for each source video, we provided\na text description and graded Camera Motion, Object Mo-\ntion, and Scene Complexity on a scale from 1 to 3. For each\nvideo, there are four types of edits along with correspond-\ning target prompts and Fantasy Levels (also ranging from\n1 to 3), namely Style Change, Object Change, Background\nChange, and Compound Change. Our aim in doing so is\nto better compare the strengths and weaknesses of different\nmethods and their areas of expertise, as well as to assist re-\nsearchers in advancing their techniques. In this section, we\nprovide details about how to prompt GPT-4V(ision) [1, 32\u2013\n34] to assistant us to establish our proposed BalanceCC\nAdd on all frames\n+\nMain \nBranch\nEncoder\nMain \nBranch\nDecoder\nStructure \nBranch\nEncoder\nAppearance\nBranch\nEncoder\nZero Conv\nStructure Condition\nMiddle Block\nInput\nKey Frame\nMiddle Block\nMiddle Block\n\u00d7 3\n\u00d7 3\n\u00d7 3\n\u00d7 3\nOutput\nAdd on key frame\nSkip connection\nZero Conv\nZero Conv\nZero Conv\nZero Conv\nZero Conv\n\u00d7 3\n\u00d7 3\n\u00d7 3\n\u00d7 3\n\u00d7 3\n\u00d7 3\n\u00d7 3\n\u00d7 3\nZero Conv\nZero Conv\nZero Conv\nZero Conv\n3 \u00d7\n3 \u00d7\n3 \u00d7\n3 \u00d7\n3 \u00d7\n3 \u00d7\n3 \u00d7\n3 \u00d7\nZero Conv\nVAE Encoder\nVAE Encoder\nAdd Noise\n\ud835\udc1c\ud835\udc4e\n\ud835\udc57\n\ud835\udc310\n\ud835\udc33\ud835\udc61\n\ud835\udc330\n\ud835\udc1c\ud835\udc60\n\u2130(\ud835\udc1c\ud835\udc4e\n\ud835\udc57)\nFigure 11. Illustration of our proposed trident network. Left:\nAppearance branch. Middle: Main branch. Right: Structure\nbranch. Text prompts and time embedding are incorporated are\nomitted for simplicity.\nbenchmark and some illustrative examples. The BlanceCC\nbenchmark will be public soon.\nB.1. Prompting Pipeline and Instructions\nGPT-4V(ision) [1, 32\u201334] is a multi-modal model that pos-\nsesses powerful capabilities in visual understanding, lan-\nguage comprehension, conversational skills, and a vast\nrepository of knowledge. Consequently, we aim to leverage\nthese dual capabilities to help us establish the BalanceCC\nbenchmark. The process is akin to seeking advice from\na wise person with extensive knowledge and excellent vi-\nsion. Specifically, we first inform GPT-4V(ision) of our\nintention to create a benchmark dataset dedicated to video\nediting, explaining our requirements including scene com-\nplexity, original prompts, target prompts, editing types, and\nthe corresponding fantasy levels. Then, we send the cen-\nter frame of each video clip to GPT-4V(ision), allowing it\nto output the content we need in the specified format. In\nour initial attempts, we observed that GPT-4V(ision) still\nexperienced some hallucinations, overly detailed descrip-\ntions and expansions, and instances of forgetfulness during\ninteractions. Consequently, we made repeated and empha-\nsized adjustments in our prompt. Additionally, we found\nthat merely describing our needs was insufficient to achieve\nthe desired results. Our solution was to provide correspond-\ning examples as references, which significantly improved\nthe quality of the content provided by GPT-4V(ision). The\nfinal prompt we used is as follows,\nNow I\u2019m trying to build a benchmark for\nvideo editing. I need you to assist me in doing\nthat. I will provide the center frame of each video\nfor you. About the image, I hope you provide the\nfollowing information to me:\n1. Classify this video into one of \u201cHuman,\nAnimal, Object, Landscape\u201d.\n2. Describe this image, be brief, concise, and\nprecise. Don\u2019t use too many adjectives.\n3. Try to generate four text prompts of dif-\nferent types to edit this video. Be creative and\nimaginative. Offer me the corresponding \u201cEditing\nType\u201d, \u201cTarget Prompt\u201d, and \u201cFantasy Level\u201d of\neach prompt. The \u201cEditing Type\u201d should be one\nof \u201cStyle Change, Object Change, Background\nChange, and Compound Change.\u201d About \u201cStyle\nChange\u201d, some examples are \u201cold movies\u201d, \u201cim-\npressionist style\u201d, \u201cVan Gogh style\u201d, \u201cneon lights\nstyle\u201d, \u201ccyberpunk style\u201d, \u201csepia-toned photo\u201d,\n\u201cgrayscale\u201d, \u201cclaymation style\u201d, \u201corigami style\u201d,\n\u201coil painting style\u201d. About \u201cObject Change\u201d, just\nchange the object into other ones, like \u201cdog to\ncat\u201d, \u201ccat to tiger\u201d, \u201chuman to bear\u201d, \u201chuman to\nteddy bear\u201d, and even some specific identities like\n\u201cIronman\u201d.\nAbout \u201cBackground Change\u201d, just\nchange the background, here are some examples,\n\u201cin the Mars\u201d, \u201cin the moon\u201d, \u201cin the forest\u201d, \u201cin\nthe ocean\u201d, \u201cin the castle\u201d.\nYou can pick one\nof the examples I provided, and I hope you can\nalso consider other ones that you think are in-\nteresting or suit this video. About \u201cCompound\nChange\u201d, just combine what mentioned above.\nPlease remember, be creative and imaginative,\nand don\u2019t be too outrageous. Besides, all targets\nincluding \u201cStyle Change, Object Change, Back-\nground Change, and Compound Change\u201d should\nbe provided for one video. The form of \u201cTarget\nPrompt\u201d should be just like a description of an\nvideo, don\u2019t say something like \u201cTransform the\nbackground into moon.\u201d Here is an example, the\noriginal prompt is \u201cA majestic black swan grace-\nfully floats on calm waters, with its reflection vis-\nible.\u201d, the \u201cTarget Prompt\u201d can be \u201cAn elegant\nflamingo swan gracefully floats on calm waters,\nwith its reflection visible, set against a backdrop\nof a mystical enchanted forest.\u201d. As for the \u201cFan-\ntasy Level\u201d for each \u201cTarget Prompt\u201d, it indicates\nthe degree of imagination. For example, if you\nchange the cat to a tiger or change the background\nfrom autumn to winter, it can be seen as a rela-\ntively low degree of imagination. Transforming a\ncat into pixel tiger or tiger made of origami is rel-\native high degree of imagination. Here is also 1-3\nin total 3 levels. And similar to the description,\nbe brief, concise, and precise.\n4. Is the scene complex or not? Rank it from\n1 to 3, corresponding to simple, moderate, and\ncomplex.\nB.2. Human Refinement\nUpon receiving initial outcomes from GPT-4V(ision), we\nengaged in a manual refinement and augmentation process.\nThis primarily entailed the verification and rectification of\nexisting annotations, along with the inclusion of additional\ndetails regarding the magnitude of camera and object mo-\ntion within the video sequences. Specifically, our rule to\ndefine levels of different attributes is as follows:\nCamera Motion: 1 corresponds to stationary, indicat-\ning minimal scene change and camera movement. 2 corre-\nsponds to slow movement, where the camera moves steadily\nand slowly. 3 corresponds to scenarios with intense camera\nshake and rapid movement.\nObject Motion: 1 corresponds to stationary, where the\ntarget is almost motionless or has very minimal movement.\n2 corresponds to slow movement, where the target follows\na slow, simple, and regular trajectory (such as uniform lin-\near motion). 3 corresponds to targets engaging in fast and\ncomplex movements (such as dancing and boxing).\nScene Complexity: 1 corresponds to scenes with a single\ntarget and a clean background. 2 corresponds to scenes with\na few targets where both the targets and the background are\nnot complex. 3 corresponds to scenes with multiple fore-\nground targets, complex backgrounds, and intricate depth\nrelationships.\nFantasy Level: 1 corresponds to simple target or back-\nground replacements and style transfers, such as transform-\ning a dog into a cat or shifting to a Van Gogh painting\nstyle. 2 corresponds to more creative target and background\nreplacements and style transfers, like replacing the back-\nground with a Martian landscape or turning an airplane into\na dragon. 3 corresponds to complex and creative editing\nobjectives combined together, with the Fantasy Level for\nCompound Change generally being 3.\nB.3. Illustrative Examples\nFour illustrative examples are shown in Fig. 15.\nC. Experiments\nC.1. Personalized T2I Models\nAs mentioned in the main text, our method can integrate\noff-the-shelf personalized models as plugins, enabling the\ngeneration of domain-specific results. In this section, we\nbriefly introduce the principles and specific implementa-\ntions of personalized models.\nStable Diffusion [42] is trained on a huge dataset that\nencompasses a broad spectrum of domains [46]. Although\nthe Stable Diffusion model is highly versatile and capa-\nble of generating a wide array of images, it occasionally\nfalls short in specific details, particularly when it comes to\ngenerating human faces and hands, where subtle variations\nModel Name\nType\nCounterfeit\nT2I Base Model\nToonYou\nT2I Base Model\nrev Animated\nT2I Base Model\nHelloMecha\nT2I Base Model\nhellonijicute25d\nT2I Base Model\nA-Zovya Photoreal\nLoRA\nkMechAnimal\nLoRA\nPixel Art Style\nLoRA\nfat animal\nLoRA\nBuilding Block World\nLoRA\nMoXin\nLoRA\nmechanical dog\nLoRA\nTable 2. Personalized models utilized in this paper, all sourced\nfrom CivitAI [9].\ncan markedly influence the overall perception. Addition-\nally, it often struggles to precisely meet users\u2019 expectations\nfor specific content, styles, and attributes. Therefore, per-\nsonalized T2I models are designed to address these chal-\nlenges. Two respective methods are DreamBooth [44] and\nLoRA [21]. The former uses a unique string as an indica-\ntor to represent the corresponding domain or concept dur-\ning training. Once trained, this indicator can be employed\nto transfer the expectations to the fine-tuned T2I model.\nDreamBooth faces challenges due to the extensive weight\nparameters, making communication less convenient. To use\nmuch less parameters and inherent the generalization of the\nbase model, LoRA fine-tunes the model by preserving all\noriginal parameters and introducing the weight residuals\n\u2206W to update the weights W. This process is formulated\nas W \u2032 = W + \u03b1\u2206W, where \u03b1 is the hyperparameter that\ncontrols the significance of the added \u2206W. Typically, the\nparameters of \u2206W are significantly fewer than those of W.\nFinally, two additional methods for creating robust person-\nalized T2I base models are fine-tuning the entire model di-\nrectly on the self-collected datasets and blending parameters\nfrom various models. Personalized T2I models play a cru-\ncial role in today\u2019s AI content generation. They empower\nboth beginners and seasoned artists, as well as enthusiasts,\nto swiftly and autonomously produce stunning images and\ncreate new models. A significant objectives of our frame-\nwork is to ensure compatibility with personalized T2I mod-\nels, allowing creators to freely combine and perform highly\ncreative edits on videos using models from the community.\nIn this paper, we collect several personalized T2I base\nmodels and LoRA weights from CivitAI [9] and explored\ndifferent combinations, which are illustrated in Table 2.\nSimilar to previous work [15], we employ the \u201ctrigger\nwords\u201d to activate these personalized models.\n\u03b1 of all\nLoRA models is set as 0.9.\nC.2. More Visualizations\nFig. 17 shows several visualized results of CCEdit.\nC.3. Comprehensive Comparison\nC.3.1\nCompared Methods\nWe compared our methods with eight state-of-the-art\ngenerative video editing methods:\nTune-A-Video [55],\nvid2vid-zero [53], Text2Video-zero [22], FateZero [37],\nPix2Video [6], ControlVideo [60], Rerender A Video [58],\nand TokenFlow [14]. The brief descriptions of these meth-\nods are as follows:\nTune-A-Video [55] propose the sparse attention mecha-\nnism to maintain the temporal coherence and optimize the\nnetwork parameters through training on the source video.\nDDIM inversion [49] is utilized to preserve the structure of\ninput video.\nVid2vid-zero [53] utilizes off-the-shelf image diffusion\nmodels and employs the null-text inversion module [28]\nfor text-to-video alignment. Additionally, it incorporates\na cross-frame modeling module to ensure temporal con-\nsistency and a spatial regularization module to maintain fi-\ndelity to the original video.\nText2Video-zero [22] introduces a method to enhance\nthe latent codes of generated frames with motion dynam-\nics, ensuring global scene and temporal consistency in\nthe background.\nAdditionally, it reprograms frame-level\nself-attention through cross-frame attention, focusing each\nframe on the first one to maintain the context, appearance,\nand identity of the foreground object.\nFateZero [37] proposes to capture intermediate attention\nmaps during inversion process, enhancing structural and\nmotion information retention, and employs a novel spatial-\ntemporal attention mechanism in the denoising UNet for\nimproved frame consistency.\nPix2Video [6] involves two steps to conduct genera-\ntive video editing: initially, using a structure-guided (e.g.,\ndepth) image diffusion model to edit an anchor frame based\non text prompts, followed by a key step of progressively\npropagating these edits to subsequent frames. This is done\nvia self-attention feature injection, adapting the core de-\nnoising phase of the diffusion model. Adjustments are then\nmade to the latent code of each frame before continuing the\nprocess.\nControlVideo [60] leverages ControlNet [59] to ensure\nthe structural consistency from input video clips. In ad-\ndition, it introduces full cross-frame interaction in self-\nattention modules for appearance coherence, an interleaved-\nframe smoother to reduce flickering through frame interpo-\nlation.\nRerender A Video [58] propose to tackle the task of video\nediting by two parts: key frame translation and full video\ntranslation. Initially, it employs an adapted diffusion model\nto generate key frames, applying hierarchical cross-frame\nconstraints to ensure coherence in shapes, textures, and col-\nors. Subsequently, the framework extends these key frames\nto other frames using temporal-aware patch matching and\nframe blending techniques.\nTokenFlow [14] propose the idea that the edited features\nconvey the same inter-frame correspondences and redun-\ndancy as the original video features. Therefore, it prop-\nagates diffusion features based on inter-frame correspon-\ndences inherent in the model to ensure consistency in the\ndiffusion feature space.\nDuring the evaluation, all the videos consist of 17 frames\nat 6fps. We select depth maps as the structural representa-\ntion. Additionally, to ensure fairness, the base model for all\nmethods is Stable Diffusion v1.5.\nC.3.2\nQualitative Results\nThe qualitative results for two videos are presented in\nFig. 18 and Fig. 19. It can be observed that Tune-A-Video\nachieves effective editing that aligns well with the speci-\nfied prompts, but falls short in maintaining temporal consis-\ntency and tends to produce overly contrasted images, pos-\nsibly due to overfitting to the source video and excessively\nhigh default classifier-free guidance settings. Vid2vid-zero,\nText2Video-Zero, and Pix2Video also struggle with insuf-\nficient temporal coherence. While FateZero exhibits bet-\nter temporal coherence, its editing accuracy is not optimal.\nControlVideo, despite its reasonable editing accuracy and\ntemporal coherence, lacks a natural feel in its edited videos\ndue to its global attention mechanism and interleaved-frame\nsmoother technique. Rerender A Video demonstrates a lim-\nitation in executing precise edits, potentially due to an ex-\ncessive dependence on detailed structural control mecha-\nnisms (line drawing and Canny edge of ControlNet). Such\nmechanisms restrict the method predominantly to minor\nstylistic alterations. TokenFlow achieves stable results in\nboth temporal coherence and editing accuracy, yet it still\nencounters blurring issues in scenes with significant ob-\nject motion or rapid camera movements (see the horse legs\nin Fig. 19). At last, our approach demonstrates a notable\ncapacity for sustaining temporal consistency, coupled with\nachieving exceptional accuracy in editing.\nC.3.3\nQuantitative Results\nAutomatic Metrics. Our evaluation metrics include two\naspects of both automatic ones and user study results. Au-\ntomatic metrics are mainly conducted through the trained\nCLIP [17, 23, 38] model, similar to previous methods [6,\n37, 55, 60]. Specifically, \u201cTem-Con\u201d evaluates the tempo-\nral consistency of edited frames by calculating the similar-\nity between successive frame pairs. Meanwhile, \u201cTex-Ali\u201d\nMethod\nTem-Con \u2191\nTex-Ali \u2191\nPick \u2191\nTune-A-Video [55]\n0.937\n0.284\n0.206\nvid2vid-zero [53]\n0.933\n0.284\n0.209\nText2Video-Zero [22]\n0.949\n0.262\n0.203\nFateZero [37]\n0.942\n0.245\n0.205\nPix2Video [6]\n0.939\n0.285\n0.208\nControlVideo [60]\n0.950\n0.285\n0.210\nRerender A Video [58]\n0.928\n0.247\n0.201\nTokenFlow [14]\n0.949\n0.270\n0.210\nCCEdit (Ours)\n0.936\n0.281\n0.213\nTable 3.\nState-of-the-art comparison of automatic metrics.\n\u201cTem-Con\u201d represents temporal consistency, \u201cText-Ali\u201d indicates\ntextural alignment, and \u201cPick\u201d represents to the PickScore [23].\nquantifies frame-wise editing accuracy, represented as the\ncosine similarity between edited frames and target prompts.\nAdditionally, the PickScore [23] is incorporated to pre-\ndict the aesthetic quality and user preference of the edited\nvideos. Regarding the user study, we designed an interface\nand invited 33 volunteers to score the videos and pickup\nthe winners, receiving a total of 1119 ratings. Each rating\ncorresponds to various aspects of a single video. Specifi-\ncally, the aspects to be rated include: \u201cEditing Accuracy\u201d,\nrepresenting whether the edited video accurately achieves\nthe intended meaning of the target prompt; \u201cAesthetics\u201d,\ndenoting the visual appeal of the edited video; \u201cTempo-\nral Consistency\u201d, indicating whether the video maintains\ncoherence over time; and \u201cOverall Impression\u201d, which re-\nflects the subjective overall rating of the video. The inter-\nface is illustrated in Fig. 16.\nResults of Automatic Metrics. The results are illustrated\nin Tab. 3. Although our method ranked second in temporal\nconsistency and first in text alignment in the table of user\nstudy presented in the main text, it did not particularly stand\nout in terms of corresponding objective metrics. This obser-\nvation has been noted in many previous works [29, 56, 61],\nfurther emphasizing the significance of more advanced ob-\njective automatic metrics for the development of this field.\nFinally, our method achieved the best performance in the\nCLIP-based scoring function, PickScore, an indicator of hu-\nman preference, demonstrating its superior alignment with\nhuman subjective perceptions.\nC.3.4\nRuntime Analysis\nTab. 4 presents the runtime of various methods, detailing the\ntime spent on pre-processing, inference, and the total du-\nration, respectively. Pre-processing includes tasks of fine-\ntuning on the source video, performing inversion opera-\ntions, caching attention maps, key frame editing, and others.\nThe inference time represents the duration of the sampling\nprocess, along with all the associated operations. Overall,\nthe time consumed by our method is not lengthy compared\nMethod\nPre-Processing\nInference\nTotal\nTune-A-Video [55]\n545\n22\n567\nvid2vid-zero [53]\n148\n230\n378\nText2Video-Zero [22]\n0\n28\n28\nFateZero [37]\n199\n42\n241\nPix2Video [6]\n0\n188\n188\nControlVideo [60]\n0\n56\n56\nRerender A Video [58]\n76\n96\n172\nTokenFlow [14]\n182\n27\n209\nCCEdit (Ours)\n134\n46\n170\nTable 4. Runtime comparison (seconds).\nto other video editing techniques. It is worth noted that\nin our method, the time spent on key frame editing using\nPnP [52]) during pre-processing constitutes the majority of\nthe total time, while the actual sampling time is relatively\nbrief. It\u2019s attributed to the absence of any inversion and at-\ntention map operations. The only additional computational\noverhead arises from the extra network parameters intro-\nduced during the network forward process. In practical ap-\nplications, one can opt for more advanced and lightweight\nimage editing methods or manually make fine adjustments,\nthereby achieving the desired trade-off. This further demon-\nstrates the practicality and flexibility of our approach.\nC.4. Study on Control Scales\nStructure Branch.\nSometimes, the appearance of the\nedited key frame may structurally differ from the cor-\nresponding structure representation of the original video.\nSince the features of the structure branch are injected into\nthe main branch through summation, the intensity of struc-\nture information infusion can be adjusted by modifying\nthe coefficients (named control scale) applied to the fea-\ntures during this summation process.\nIn such cases, re-\nducing the control scale of the structure branch could help.\nThis adjustment lessens its structural constraints on the re-\nsults, allowing for a greater reliance on the information pro-\nvided by the appearance branch and adherence to the co-\nherence adjustments made by the temporal layers. The vi-\nsualized results are shown in Fig. 12. It can be observed\nthat in the edited key frame, the astronaut\u2019s silhouette ap-\npears markedly larger than that of the original person, a\nconsequence of the voluminous spacesuit. When the struc-\nture control is relatively high (0.6\u223c1.0), the editing results\nshow that the center frame remain consistent with the edited\nframe, while the structure of other frames is overly con-\nstrained by the structure representation. At a control scale\nof 0, the loss of structure information leads to the astronaut\nbeing unable to move correctly. However, with a moder-\nate control scale (0.2\u223c0.4), a better trade-off is achieved\nin terms of appearance, structure, and motion. Note that\nin comparisons with other methods, to ensure fairness, our\nInput \nVideo\nDepth \nMaps\nEdited\nKey Frame\nScale=0.0\nScale=0.2\nScale=0.4\nScale=0.6\nScale=0.8\nScale=1.0\nFigure 12. Results at different scales of structure branch. The\ntarget prompt is \u201cAn astronaut with a jetpack floats above a Mar-\ntian landscape, with red rocky terrains and tall, alien-like moun-\ntains in the backdrop.\u201d\nmethod consistently employed a control scale of 1.\nAppearance Branch. Since the features of the Appear-\nance Branch are also injected into the main branch through\nsummation, the intensity of appearance information infu-\nsion can similarly be adjusted by tuning the summation co-\nefficients of the appearance branch. The results are shown\nin Fig. 13. At a lower control scale (0\u223c0.2), the influence\nof appearance information is minimal, barely impacting the\nedited video. When the control scale is moderate (0.4\u223c0.6),\nappearance information begins to play a role.\nHowever,\npossibly due to conflicts with the priors of the main branch,\nthis results in a somewhat dull and dark color tone in the\nvisuals. Conversely, at a higher control scale (0.8\u223c1.0),\nappearance information exerts a decisive control over the\noverall appearance of the edited video.\nC.5. Study on Text Prompt\nAnother point worth exploring is whether text prompts are\nstill necessary after introducing appearance control. To ad-\ndress this, we conducted a visual experiment. As shown\nin Fig. 14, providing a normal text prompt leads to correct\nInput \nVideo\nDepth \nMaps\nEdited\nKey Frame\nScale=0.0\nScale=0.2\nScale=0.4\nScale=0.6\nScale=0.8\nScale=1.0\nFigure 13. Results at different scales of appearance branch.\nThe target prompt is \u201cAn astronaut with a jetpack floats above\na Martian landscape, with red rocky terrains and tall, alien-like\nmountains in the backdrop.\u201d\nresults, whereas the absence of any text prompt results in\nsignificant distortions in the generated output. When given\na text prompt that contradicts the appearance information,\nonly the center frame retains the appearance information,\nwhile the other frames are controlled by the text prompt.\nConsequently, the conclusion is that text prompts are still\nnecessary within this framework. We believe this may be\ndue to the weights of the main branch and the structure\nbranch being frozen during the training process. As a result,\nthe entire editing process seems to involve the appearance\nbranch exerting more detailed control over the image after\nthe text prompt has already provided a coarse guide.\nD. Limitation and Future Works\nD.1. Structural Deviation\nAs described in the main text, a primary challenge that\nneeds addressing in our video editing approach is the struc-\ntural deviation (also the major issue mentioned in Token-\nFlow [14]) between the input and target videos. This devi-\nation could stem from semantic changes inherent to the tar-\nEdited \nKey Frame\nInput \nVideo\nNormal\nText Prompt\nNo\nText Prompt\nContradicted \nText Prompt\nFigure 14. Illustration of results with different text prompts.\nThe normal prompt is \u201cA bear is walking\u201d. The contradicted text\nprompt is \u201cA tiger is walking\u201d. The \u201cToonYou\u201d personalized T2I\nmodel is used.\nget or from alterations in the target\u2019s behavior. For instance,\ntransitioning from a \u201ccute rabbit\u201d to a \u201cfierce tiger\u201d is chal-\nlenging due to their fundamentally different physiological\nstructures. Most existing methods struggle to overcome this\nhurdle and often only manage to modify their textural ap-\npearance. In our approach, adjusting the scale coefficient\nof structure branch and employing coarser-grained struc-\nture representations (like the skeleton) may alleviate this is-\nsue to some extent, but we believe it doesn\u2019t fundamentally\nsolve the problem. Achieving changes in the target\u2019s behav-\nior, such as transforming a \u201crunning bear\u201d into a \u201cdancing\nbear\u201d, is even more challenging. This complexity arises pri-\nmarily because most contemporary generative video editing\nmethods employ Text-to-Image (T2I) models at the image\nlevel. These models, devoid of prior knowledge concerning\nactions, encounter difficulties in editing motion.\nWe posit that a promising approach could be to integrate\na pre-trained T2V (text-to-video) model, cleverly utilizing\nits priors to tackle these challenges.\nD.2. Heavy Appearance and Structure Branch\nIn CCEdit, the appearance and structure branch utilize two\nheavy encoder to extract features, consisting significant\namount of parameters. This may be unnecessary and could\nlead to issues such as increased GPU memory consumption\nand longer editing times during use. In the future, we plan to\nexplore the adoption of more lightweight networks [30, 43]\nto address these concerns.\nD.3. Flickering Problem.\nWe observed flickering in videos with higher frame rates\nor after frame interpolation, especially noticeable in high-\nfrequency fine texture details. This is primarily attributed to\nour video editing operations being performed in the latent\ndomain encoded by the 2D autoencoder. Introducing addi-\ntional temporal layers in the autoencoder [5] is an promising\nway to solve this problem.\n<1> Object Change: \u201cA majestic flamingo swimming in a pond with lush greenery in the \nbackground.\u201d \n<2> Background Change: \u201cA black swan swimming in a crystal and clear lake surrounded by snow-\ncapped mountains.\u201d\nVideo Type: Animal\nOriginal Prompt: \u201cA black swan swimming in a pond with lush greenery in the background.\u201d\nCamera Motion: 2\nObject Motion: 2\nScene Complexity: 2\n<1> Style Change: \u201cA black swan swimming in a pond with lush greenery in the background, oil \npainting style.\u201d\n<3> Compound Change: \u201cA duck made of origami floating on a pond under a cherry blossom tree \nin full bloom.\u201d\n<2> Object Change: \u201cA skateboarder in full gear maneuvering his skateboard over a dirt ramp in a \nBMX track.\u201d\n<2> Background Change: \u201cA BMX rider in full gear maneuvering his bike over a dirt ramp in a \nnight-time cityscape with skyscrapers in the background.\u201d\nVideo Type: Human\nOriginal Prompt: \u201cA BMX rider in full gear maneuvering his bike over a dirt ramp in a BMX \ntrack.\u201d\nCamera Motion: 3\nObject Motion: 3\nScene Complexity: 3\n<1> Style Change: \u201cA BMX rider in full gear maneuvering his bike over a dirt ramp in a BMX \ntrack, rendered in old movie style.\u201d\n<3> Compound Change: \u201cA polar bear BMX rider in full gear maneuvering his bike over a ramp in \na futuristic cyberpunk city, surrounded by neon billboards.\u201d\n<1> Object Change: \u201cCrystalline ice formations by a frozen water body with a mountain in the \nbackdrop under an overcast sky.\u201d\n<2> Background Change: \u201cRocks by a still water body with a mystical floating island in the sky \nbackdrop, amidst a soft glow of sunset.\u201d\nVideo Type: Landscape\nOriginal Prompt: \u201cRocks by a still water body with a mountain in the backdrop under an overcast \nsky.\u201d\nCamera Motion: 2\nObject Motion: 1\nScene Complexity: 2\n<2> Style Change: \u201cRocks by a still water body with a mountain in the backdrop under an overcast \nsky, in Studio Ghibli style.\u201d\n<3> Compound Change: \u201cLuminous gemstones by the shores of a bioluminescent lake with an \naurora borealis in the backdrop in a starry night scene.\u201d\n<1> Object Change: \u201cA vintage lantern tumbling across a grassy field beside a wire fence.\u201d\n<2> Background Change: \u201cA soccer ball rolling across the moon's surface with Earth in the distant \nskyline.\u201d\nVideo Type: Object\nOriginal Prompt: \u201cA soccer ball rolling across a grassy field beside a wire fence.\u201d\nCamera Motion: 1\nObject Motion: 2\nScene Complexity: 2\n<1> Style Change: \u201cA soccer ball rolling across a grassy field beside a wire fence, Comic Book Art \nstyle.\u201d\n<3> Compound Change: \u201cA glowing orb of energy bouncing across a futuristic cityscape at dusk.\u201d\nFigure 15. Illustrative examples of BalanceCC benchmark dataset.\nFigure 16. Illustration of the interface to conduct user study. Initially, we provided a description of the evaluation criteria at the top of the\npage, along with corresponding notes for consideration. Additionally, to reduce user burden and avoid the confusion of displaying multiple\nvideos simultaneously, for each video\u2019s editing result, we randomly selected three from all nine options (eight comparative methods and\nour method) for users to rate on various criteria (Edit Accuracy, Aesthetic, Temporal Consistency, and Overall Impression). Finally, users\nwere asked to choose the winner(s) among the three videos. Selecting multiple winners was allowed (up to two), but choosing none or all\nthree was not permitted. Zoom in to see details.\n<Model: ReV Animated, kMechAnimal> \u201cA mechanical bear is running.\u201d\n<MajicMIX realistic> \u201cA beautiful young girl is doing makeup.\u201d\n<Model: ToonYou> \u201cAn anime-style tiger is walking.\u201d\n<Model: Counterfeit> \u201cA girl with grey hair.\u201d\n<Model: SD v1.5> \u201cA man wanders in the field, with the Milky Way in the sky\u201d\nInput \nVideo\nEdited \nVideo\nInput \nVideo\nEdited \nVideo\nInput \nVideo\nEdited \nVideo\nInput \nVideo\nEdited \nVideo\nInput \nVideo\nEdited \nVideo\n<Model: ToonYou> \u201cA man is running on the beach, with sunset behind.\u201d\nInput \nVideo\nEdited \nVideo\nFigure 17. Visualized results of CCEdit. \u27e8\u00b7\u27e9 indicates the personalized T2I model we used.\nInput \nVideo\nTune-A-\nVideo\nvid2vid-\nzero\nText2Video-\nZero\nFateZero\nPix2Video\nControlVideo\nRerender A \nVideo\nTokenFlow\nCCEdit (Ours)\nFigure 18. Qualitative comparison of different methods. The target prompt is \u201cA dragonfly with shimmering wings perches on a plant\namidst a field of golden grass.\u201d\nInput \nVideo\nTune-A-\nVideo\nvid2vid-\nzero\nText2Video-\nZero\nFateZero\nPix2Video\nControlVideo\nRerender A \nVideo\nTokenFlow\nCCEdit (Ours)\nFigure 19. Qualitative comparison of different methods. The target prompt is \u201cA rider on a horse jumping over an obstacle in an\nequestrian competition, rendered in Van Gogh style with swirling skies and vibrant colors.\u201d\n"
  },
  {
    "title": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning",
    "link": "https://arxiv.org/pdf/2309.16650.pdf",
    "upvote": "7",
    "text": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning\nhttps://concept-graphs.github.io/\nQiao Gu\u2020\u22171, Ali Kuwajerwala\u2020\u22172, Sacha Morin\u22172, Krishna Murthy Jatavallabhula\u22173,\nBipasha Sen2, Aditya Agarwal2, Corban Rivera5, William Paul5, Kirsty Ellis2,\nRama Chellappa6, Chuang Gan7, Celso Miguel de Melo4,\nJoshua B. Tenenbaum3, Antonio Torralba3, Florian Shkurti1 Liam Paull2,\n1University of Toronto, 2Universit\u00b4e de Montr\u00b4eal, 3MIT, 4DEVCOM ARL, 5JHU APL, 6JHU, 7UMass\nQueries with Both Image and Language Context\nFind something this guy would play with\nConceptGraphs: \u201cYes! It's probably a piece of \ngreen fabric. You can push it without damage.\u201d\nCan I push this object \nand traverse through?\nOpen-vocabulary Pick and Place\nTraversability Estimation\nPick up the \n\u201ccuddly quacker\u201d\nComplex Text Queries\nConceptGraphs\nMy wrist hurts from using \nmy screwdriver all day. \nAnything to help?\nI'm craving something \nfizzy and sweet.\nConceptGraphs: Go to \nobject 18, a power drill\nConceptGraphs: Go to \nobject 37, a soda can\nConceptGraphs\n3D Object Detection\nObject: cabinet\nCenter: (x, y, z)\nObject Caption\n\u201cA picture of a boat\u201d\nSpatial Reasoning\nThe ottoman is on \nthe carpet. \nScene Structure\nA white sofa with 3 \npillows on the top.\nFig. 1:\nConceptGraphs builds open-vocabulary 3D scene graphs. We (a) design an object-based mapping system that only assumes\nclass-agnostic instance masks and fuses them to 3D, (b) interprets and extracts language tags for each mapped instance leveraging large\nvision-language models, and (c) builds a graph of object spatial relationships by leveraging priors encoded in large language models. The\nobject-centric nature of ConceptGraphs allows easy map maintenance and promotes scalability, and the graph structure provides relational\ninformation within the scene. Furthermore, our scene graph representations are easily mapped to natural language formats to interface\nwith LLMs, enabling them to answer complex scene queries and granting robots access to useful facts about surrounding objects, such\nas traversability and utility. We implement and demonstrate ConceptGraphs on a number of real-world robotics tasks across wheeled and\nlegged mobile robot platforms. (Webpage) (Explainer Video)\nAbstract\u2014 For robots to perform a wide variety of tasks, they\nrequire a 3D representation of the world that is semantically\nrich, yet compact and efficient for task-driven perception\nand planning. Recent approaches have attempted to leverage\nfeatures from large vision-language models to encode semantics\nin 3D representations. However, these approaches tend to\nproduce maps with per-point feature vectors, which do not\nscale well in larger environments, nor do they contain semantic\nspatial relationships between entities in the environment, which\nare useful for downstream planning. In this work, we propose\nConceptGraphs, an open-vocabulary graph-structured represen-\ntation for 3D scenes. ConceptGraphs is built by leveraging 2D\nfoundation models and fusing their output to 3D by multi-\nview association. The resulting representations generalize to\nnovel semantic classes, without the need to collect large 3D\ndatasets or finetune models. We demonstrate the utility of this\nrepresentation through a number of downstream planning tasks\nthat are specified through abstract (language) prompts and\nrequire complex reasoning over spatial and semantic concepts.\nTo explore the full scope of our experiments and results, we\nencourage readers to visit our project webpage.\n\u2020Project Lead *Equal Contribution\nI. INTRODUCTION\nScene representation is one of the key design choices\nthat can facilitate downstream planning for a variety of\ntasks, including mobility and manipulation. Robots need\nto build these representations online from onboard sen-\nsors as they navigate through an environment. For efficient\nexecution of complex tasks such representations should\nbe: scalable and efficient to maintain, as the volume of the\nscene and the duration of the robot\u2019s operation increases;\nopen-vocabulary, not limited to making inferences about a\nset of concepts that is predefined at training time, but capable\nof handling new objects and concepts at inference time; and\nwith a flexible level of detail to enable planning over a range\nof tasks, from ones that require dense geometric information\nfor mobility and manipulation, to ones that need abstract\nsemantic information and object-level affordance information\nfor task planning. We propose ConceptGraphs, a 3D scene\nrepresentation method for robot perception and planning that\nsatisfies all the above requirements.\narXiv:2309.16650v1  [cs.RO]  28 Sep 2023\nA. Related Work\nClosed-vocabulary semantic mapping in 3D. Early\nworks reconstruct the 3D map through online algorithms like\nsimultaneous localization and mapping (SLAM) [1]\u2013[5] or\noffline methods like structure-from-motion (SfM) [6], [7].\nAside from reconstructing 3D geometry, recent works also\nuse deep learning-based object detection and segmentation\nmodels to reconstruct the 3D scene representations with\ndense semantic mapping [8]\u2013[11] or object-level decompo-\nsition [12]\u2013[15]. While these methods achieve impressive\nresults in mapping semantic information to 3D, they are\nclosed-vocabulary and their applicability is limited to object\ncategories annotated in their training datasets.\n3D scene representations using foundation models.\nThere have been significant recent efforts [16]\u2013[30] focused\non building 3D representations by leveraging foundation\nmodels - large, powerful models that capture a diverse set\nof concepts and accomplish a wide range of tasks [31]\u2013\n[35]. Such models have excelled in tackling open-vocabulary\nchallenges in 2D vision. However, they require an \u201cinternet-\nscale\u201d of training data, and no 3D datasets exist yet of a\ncomparable size. Recent works have therefore attempted to\nground the 2D representations produced by image and lan-\nguage foundation models to the 3D world and show impres-\nsive results on open-vocabulary tasks, including language-\nguided object grounding [17], [18], [24], [26], [36], 3D\nreasoning [37], [38], robot manipulation [39], [40] and\nnavigation [41], [42]. These approaches project dense per-\npixel features from images to 3D to build explicit repre-\nsentations such as pointclouds [17]\u2013[21] or implicit neural\nrepresentations [16], [22]\u2013[30].\nHowever, such methods have two key limitations. First,\nassigning every point a semantic feature vector is highly re-\ndundant and consumes more memory than necessary, greatly\nlimiting scalability to large scenes. Second, these dense\nrepresentations do not admit an easy decomposition \u2013 this\nlack of structure makes them less amenable to dynamic\nupdates to the map (crucial for robotics).\n3D scene graphs. 3D scene graphs (3DSGs) address\nthe second limitation by compactly and efficiently describ-\ning scenes with graph structures, with nodes representing\nobjects and edges encoding inter-object relationships [43]\u2013\n[47]. These approaches have enabled building real-time sys-\ntems that can dynamically build up hierarchical 3D scene\nrepresentations [48]\u2013[50], and more recently shown that\nvarious robotics planning tasks can benefit from efficiency\nand compactness of 3DSGs\n[51], [52]. However, existing\nwork on building 3D scene graphs has been confined to\nthe closed-vocabulary setting, limiting their applicability to\na small set of tasks.\nB. Overview of Our Contribution\nIn this work, we mitigate all the aforementioned limita-\ntions and propose ConceptGraphs, an open-vocabulary and\nobject-centric 3D representation for robot perception and\nplanning. In ConceptGraphs, each object is represented as a\nnode with geometric and semantic features, and relationships\namong objects are encoded in the graph edges. At the\ncore of ConceptGraphs is an object-centric 3D mapping\ntechnique that integrates geometric cues from conventional\n3D mapping systems, and semantic cues from vision and\nlanguage foundation models [31], [33], [34], [53]\u2013[55]. Ob-\njects are assigned language tags by leveraging large lan-\nguage models (LLMs) [32] and large vision-language models\n(LVLMs) [55], which provide semantically rich descriptions\nand enable free-form language querying, all while using off-\nthe-shelf models (no training/finetuning). The scene graph\nstructure allows us to efficiently represent large scenes with a\nlow memory footprint and makes for efficient task planning.\nIn experiments, we demonstrate that ConceptGraphs\nis\nable to discover, map, and caption a large number of objects\nin a scene. Further, we conduct real-world trials on multiple\nrobot platforms over a wide range of downstream tasks,\nincluding manipulation, navigation, localization, and map\nupdates. To summarize, our key contributions are:\n\u2022 We propose a novel object-centric mapping system that\nintegrates geometric cues from traditional 3D mapping\nsystems and semantic cues from 2D foundation models.\n\u2022 We construct open-vocabulary 3D scene graphs; effi-\ncient and structured semantic abstractions for perception\nand planning.\n\u2022 We implement ConceptGraphs on real-world wheeled\nand legged robotic platforms and demonstrate a number\nof downstream perception and planning capabilities for\ncomplex/abstract language queries.\nII. METHOD\nConceptGraphs builds a compact, semantically rich repre-\nsentation of a 3D environment. Given a set of posed RGB-D\nframes, we run a class-agnostic segmentation model to obtain\ncandidate objects, associate them across multiple views using\ngeometric and semantic similarity measures, and instantiate\nnodes in a 3D scene graph. We then use an LVLM to caption\neach node and an LLM to infer relationships between adjoin-\ning nodes, which results in edges in the scene graph. This\nresultant scene graph is open-vocabulary, encapsulates object\nproperties, and can be used for a multitude of downstream\ntasks including segmentation, object grounding, navigation,\nmanipulation, localization, and remapping. The approach is\nillustrated in Fig. 2.\nA. Object-based 3D Mapping\nObject-centric 3D representation: Given a sequence\nof RGB-D observations I\n= {I1, I2, . . . , It}, Concept-\nGraphs constructs a map, a 3D scene graph, Mt = \u27e8Ot, Et\u27e9,\nwhere Ot = {oj}j=1...J and Et = {ek}k=1...K represent\nthe sets of objects and edges, respectively. Each object oj is\ncharacterized by a 3D point cloud poj and a semantic feature\nvector foj. This map is built incrementally, incorporating\neach incoming frame It = \u27e8Irgb\nt , Idepth\nt\n, \u03b8t\u27e9 (color image,\ndepth image, pose) into the existing object set Ot\u22121, by\neither adding to existing objects or instantiating new ones.\nClass-agnostic\n2D\nSegmentation:\nWhen\nprocessing\nframe It, a class-agnostic segmentation model Seg(\u00b7)\nInput RGB-D sequence\nOpen-vocab masks\nNewly Detected Objects\nFeature \nExtractor\nObject \nFeatures\nObservation \nFeatures\nIncrementally Built 3D Map\nObservation-object Association\nObject Fusion\n/ Initialization\nObject Set (Nodes)\nLLM\n3D Scene Graph (Nodes + Edges)\nQuery:\n     Object 1 is a stool with position\u2026\n     Object 2 is a carpet with position\u2026\nWhat is the relationship between 1 and 2?\nLLM Response:\n     Object 1 is on the top of     Object 2. \nDownstream tasks\nSegmentation / Grounding\nRobot Navigation\nRobot Manipulation\nFinal Object Map\n\u201cFind somewhere comfy to sit\u201d\n\u2d48\u2d48\nOpen-vocab \nDetection/Segmentation\nFig. 2: ConceptGraphs builds an open-vocabulary 3D scene graph from a sequence of posed RGB-D images. We use generic instance\nsegmentation models to segment regions from RGB images, extract semantic feature vectors for each, and project them to a 3D point\ncloud. These regions are incrementally associated and fused from multiple views, resulting in a set of 3D objects and associated vision\n(and language) descriptors. Then large vision and language models are used to caption each mapped 3D objects and derive inter-object\nrelations, which generates the edges to connect the set of objects and form a graph. The resulting 3D scene graph provides a structured and\ncomprehensive understanding of the scene and can further be easily translated to a text description, useful for LLM-based task planning.\nis used to obtain a set of masks {mt,i}i=1...M\n=\nSeg(Irgb\nt ) corresponding to candidate objects1. Each ex-\ntracted mask mt,i is then passed to a visual feature extractor\n(CLIP [31], DINO [53]) to obtain a visual descriptor ft,i =\nEmbed(Irgb\nt , mt,i). Each masked region is projected to 3D,\ndenoised using DBSCAN clustering, and transformed to\nthe map frame. This results in a pointcloud pt,i and its\ncorresponding unit-normalized semantic feature vector ft,i.\nObject Association: For every newly detected object\n\u27e8pt,i, ft,i\u27e9, we compute semantic and geometric similarity\nwith respect to all objects ot-1,j = \u27e8poj, foj\u27e9 in the map\nthat shares any partial geometric overlap. The geometric\nsimilarity \u03d5geo(i, j) = nnratio(pt,i, poj) is the proportion\nof points in point cloud pt,i that have nearest neighbors\nin point cloud poj, within a distance threshold of \u03b4nn.\nThe semantic similarity \u03d5sem(i, j) = f T\nt,ifoj/2 + 1/2 is the\nnormalized cosine distance between the corresponding visual\ndescriptors.2 The overall similarity measure \u03d5(i, j) is a sum\nof both: \u03d5(i, j) = \u03d5sem(i, j) + \u03d5geo(i, j). We perform object\nassociation by a greedy assignment3 strategy where each new\ndetection is matched with an existing object with the highest\nsimilarity score. If no match is found with a similarity higher\nthan \u03b4sim, we initialize a new object.\nObject Fusion: If a detection ot-1,j is associated with\na mapped object oj, we fuse the detection with the map.\nThis is achieved by updating the object semantic feature as\nfoj = (nojfoj + ft,i)/(noj + 1), where noj is the number\n1Without loss of generality, Seg(\u00b7) may be replaced by open-/closed-\nvocabulary models to build category-specific mapping systems.\n2For the sake of brevity, we only describe the best-performing geometric\nand semantic similarity measures. For an exhaustive list of alternatives,\nplease see our project website and code.\n3While we also experimented with optimal assignment strategies such as\nthe Hungarian algorithm, we experimentally determined them to be slower\nand offer only a minuscule improvement over greedy association.\nof detections that have been associated to oj so far; and\nupdating the pointcloud as pt,i \u222a poj, followed by down-\nsampling to remove redundant points.\nNode Captioning: Once the entire image sequence has\nbeen processed, a vision-language model, denoted LVLM(\u00b7),\nis used to generate object captions. For each object, the\nassociated image crops from the best4 10 views are passed\nto the language model with the prompt \u201cdescribe the central\nobject in the image\u201d to generate a set of initial rough captions\n\u02c6cj = {\u02c6cj,1, \u02c6cj,2, . . . , \u02c6cj,10} for each detected object oj.\nEach set of captions is then refined to the final caption by\npassing \u02c6cj to another language model LLM(\u00b7) with a prompt\ninstruction to summarize the initial captions into a coherent\nand accurate final caption cj.\nB. Scene Graph Generation\nGiven the set of 3D objects OT obtained from the previous\nstep, we estimate their spatial relationships, i.e., the edges\nET , to complete the 3D scene graph. We do this by first\nestimating potential connectivity among object nodes based\non their spatial overlaps. We compute the 3D bounding box\nIoU between every pair of object nodes to obtain a similarity\nmatrix (i.e., a dense graph), which we prune by estimating\na minimum spanning tree (MST), resulting in a refined set\nof potential edges among the objects. To further determine\nthe semantic relationships, for each edge in the MST, we\ninput the information about the object pair, consisting of\nobject captions and 3D location, to a language model LLM.\nThe prompt instructs the model to describe the likely spatial\nrelationship between the objects, such as \u201ca on b\u201d or \u201cb in\na\u201d, along with the underlying reasoning. The model outputs a\nrelationship label with an explanation detailing the rationale.\n4We maintain a running index of the number of noise-free points each\nview contributes to the object point cloud.\nThe use of an LLM allows us to extend the nominal edge\ntype defined above to other output relationships a language\nmodel can interpret, such as \u201ca backpack may be stored in\na closet\u201d and \u201csheets of paper may be recycled in a trash\ncan\u201d. This results in an open-vocabulary 3D scene graph\nMT = (OT , ET ), a compact and efficient representation\nfor use in downstream tasks.\nC. Robotic Task Planning through LLMs\nTo enable users to carry out tasks described in natural\nlanguage queries, we interface the scene graph MT with\nan LLM. For each object in OT , we construct JSON-\nstructured text descriptions that include information about its\n3D location (bounding box) and its node caption. Given a text\nquery, we task the LLM to identify the most relevant object\nin the scene. We then pass the 3D pose of this object to the\nappropriate pipeline for the downstream task (e.g., grasping,\nnavigation). This integration of ConceptGraphs with an LLM\nis easy to implement, and enables a wide range of open-\nvocabulary tasks by giving robots access to the semantic\nproperties of surrounding objects5 (see Sec. III).\nD. Implementation Details\nThe modularity of ConceptGraphs enables any appro-\npriate open/closed-vocabulary segmentation model, LLM,\nor LVLM to be employed. Our experiments use Segment-\nAnything (SAM) [33] as the segmentation model Seg(\u00b7),\nand the CLIP image encoder [31] as the feature extrac-\ntor Embed(\u00b7). We use LLaVA [55] as the vision-language\nmodel LVLM and GPT-4 [32] (gpt-4-0613) for our LLM.\nThe voxel size for point cloud downsampling and nearest\nneighbor threshold \u03b4nn are both 2.5cm. We use 1.1 for the\nassociation threshold \u03b4sim.\nWe also develop a variant of our system, ConceptGraphs-\nDetector (CG-D), where we employ an image tagging model\n(RAM [54]) to list the object classes present in the image\nand an open-vocabulary 2D detector (Grounding DINO [34])\nto obtain object bounding boxes6. In this variant, we need to\nseparately handle detected background objects (wall, ceiling,\nfloor) by merging them regardless of their similarity scores.\nIII. EXPERIMENTS\nA. Scene Graph Construction\nWe first evaluate the accuracy of the 3D scene graphs\noutput by the ConceptGraphs system in Table I. For each\nscene in the Replica dataset [56], we report scene graph\naccuracy metrics for both CG and the detector-variant CG-D.\nThe open-vocabulary nature of our system makes automated\nevaluation of the quality of nodes and edges in the scene\ngraph challenging. We instead evaluate the scene graph by\nengaging human evaluators on Amazon Mechanical Turk\n(AMT). For each node, we compute precision as the fraction\n5For large scenes where the description length of the scene graph exceeds\nthe context length of the LLM, one can easily substitute in alternative\n(concurrent) LLM planners [52].\n6We discard the (often noisy) tags produced by the image tagging model,\nrelying instead on our node captions.\nscene\nnode prec.\nvalid objects\nduplicates\nedge prec.\nCG\nroom0\n0.78\n54\n3\n0.91\nroom1\n0.77\n43\n4\n0.93\nroom2\n0.66\n47\n4\n1.0\noffice0\n0.65\n44\n2\n0.88\noffice1\n0.65\n23\n0\n0.9\noffice2\n0.75\n44\n3\n0.82\noffice3\n0.68\n60\n5\n0.79\nAverage\n0.71\n-\n-\n0.88\nCG-D\nroom0\n0.56\n60\n4\n0.87\nroom1\n0.70\n40\n3\n0.93\nroom2\n0.54\n49\n2\n0.93\noffice0\n0.59\n35\n0\n1.0\noffice1\n0.49\n24\n2\n0.9\noffice2\n0.67\n47\n3\n0.88\noffice3\n0.71\n59\n1\n0.83\nAverage\n0.61\n-\n-\n0.91\nTABLE I: Accuracy of constructed scene graphs: node precision\nindicates the accuracy of the label for each node (as measured\nby a human evaluator); valid objects is the number of human-\nrecognizable objects (mturkers used) discovered by our system;\nduplicates are the number of redundant detections; edge precision\nindicates the accuracy of each estimated spatial relationship (again,\nas evaluated by an mturker)\nof nodes for which at least 2 of 3 human evaluators deem\nthe node caption correct. We also report the number of\nvalid objects retrieved by each variant by asking evaluators\nwhether they deem each node a valid object. Both CG and\nCG-D identify a number of valid objects in each scene, and\nincur only a small number (0-5) of duplicate detections. The\nnode labels are accurate about 70% of the time; most of\nthe errors are incurred due to errors made by the LVLM\nemployed (LLaVA [55]). The edges (spatial relationships)\nare labeled with a high degree of accuracy (90% on average).\nB. 3D Semantic Segmentation\nConceptGraphs focuses on the construction of the open-\nvocabulary 3D scene graphs for scene understanding and\nplanning. For completeness, in this section, we also use\nan open-vocabulary 3D semantic segmentation task to eval-\nuate the quality of the obtained 3D maps. To generate\nthe semantic segmentation, given a set of class names, we\ncompute the similarity between the fused semantic feature\nof each object node and the CLIP text embeddings of the\nphrase an image of {class}. Then the points asso-\nciated with each object are assigned to the class with the\nhighest similarity, which gives a point cloud with dense\nclass labels. In Table II, we report the semantic segmentation\nresults on the Replica [56] dataset, following the evaluation\nprotocol used in ConceptFusion [17]. We also provide an\nadditional baseline, ConceptFusion+SAM, by replacing the\nMask2Former used in ConceptFusion with the more perfor-\nmant SAM [33] model. As shown in Table II, the proposed\nConceptGraphs performs comparably with or better than\nConceptFusion, which has a much larger memory footprint.\nC. Object Retrieval based on Text Queries\nWe assess the capability of ConceptGraphs to handle\ncomplex semantic queries, focusing on three key types.\n\u2022 Descriptive: E.g., A potted plant.\nMethod\nmAcc\nF-mIoU\nPrivileged\nCLIPSeg (rd64-uni) [57]\n28.21\n39.84\nLSeg [58]\n33.39\n51.54\nOpenSeg [59]\n41.19\n53.74\nZero-shot\nMaskCLIP [60]\n4.53\n0.94\nMask2former + Global CLIP feat\n10.42\n13.11\nConceptFusion [17]\n24.16\n31.31\nConceptFusion [17] + SAM [33]\n31.53\n38.70\nConceptGraphs (Ours)\n40.63\n35.95\nConceptGraphs-Detector (Ours)\n38.72\n35.82\nTABLE\nII:\nOpen-vocabulary\nsemantic\nsegmentation\non\nthe\nReplica [56] dataset. Privileged methods specifically finetune the\npretrained models for semantic segmentation. Zero-shot approaches\ndo not need any finetuning and are evaluated off the shelf.\n\u2022 Affordance: E.g., Something to use for temporarily\nsecuring a broken zipper.\n\u2022 Negation: E.g., Something to drink other than soda.\nWe evaluate on the Replica dataset [56] and a real-\nworld scan of the REAL Lab, where we staged a number\nof items including clothes, tools, and toys. For Replica,\nhuman evaluators on AMT annotate captions for SAM mask\nproposals, which serve as both ground truth labels and\ndescriptive queries. We created 5 affordance and negation\nqueries for each scene type (office & room) in Replica and 10\nqueries of each type for the lab scan, ensuring that each query\ncorresponds to at least one relevant object. We manually\nselect relevant objects as ground truth for each query.\nWe use two object retrieval strategies: CLIP-based and\nLLM-based. CLIP selects the object with the highest similar-\nity to the query\u2019s embedding, while the LLM goes through\nthe scenegraph nodes to identify the object with the most\nrelevant caption. Table III shows that CLIP excels with\ndescriptive queries but struggles with complex affordance\nand negation queries [61]. For example, CLIP inaccurately\nretrieves a backpack for the broken zipper query, whereas the\nLLM correctly identifies a roll of tape. The LLM performs\nwell across the board, but is limited by the accuracy of the\nnode captions, as discussed in Section III-A. Since the lab\nhas a larger variety of objects to choose from, the LLM finds\ncompatible objects for complex queries more reliably there.\nD. Complex Visual-Language Queries\nTo assess the performance of ConceptGraphs in a real-\nworld environment, we carry out navigation experiments in\nthe REAL Lab scene with a Clearpath Jackal UGV. The robot\nis equipped with a VLP-16 LiDAR and a forward-facing\nRealsense D435i camera.\nThe Jackal needs to respond to abstract user queries and\nnavigate to the most relevant object (Figure 1). By using\nan LVLM [55] to add a description of the current camera\nimage to the text prompt, the robot can also answer visual\nqueries. For example, when shown a picture of Michael Jor-\ndan and prompted with Something this guy would\nplay with, the robot finds a basketball.\nE. Object Search and Traversability Estimation\nIn this section, we showcase how the interaction between\nthe ConceptGraphs representation and an LLM can enable\nDataset\nQuery Type\nModel\nR@1\nR@2\nR@3\n# Queries\nReplica\nDescriptive\nCLIP\n0.59\n0.82\n0.86\n20\nLLM\n0.61\n0.64\n0.64\nAffordance\nCLIP\n0.43\n0.57\n0.63\n5\nLLM\n0.57\n0.63\n0.66\nNegation\nCLIP\n0.26\n0.60\n0.71\n5\nLLM\n0.80\n0.89\n0.97\nLab\nDescriptive\nCLIP\n1.00\n\u2013\n\u2013\n10\nLLM\n1.00\n\u2013\n\u2013\nAffordance\nCLIP\n0.40\n0.60\n0.60\n10\nLLM\n1.00\n\u2013\n\u2013\nNegation\nCLIP\n0.00\n\u2013\n\u2013\n10\nLLM\n1.00\n\u2013\n\u2013\nTABLE III: Object retrieval from text queries on the Replica and\nREAL Lab scenes. We measure the top-1, top-2, and top-3 recall.\nCLIP refers to object retrieval using cosine similarity, whereas LLM\nrefers to having an LLM parse the scene graph and return the most\nrelevant object.\na mobile robot to access a vast knowledge base of everyday\nobjects. Specifically, we prompt an LLM to infer two addi-\ntional object properties from ConceptGraphs captions: i) the\nlocation where a given object is typically found, and ii) if\nthe object can be safely pushed or traversed by the Jackal\nrobot. We design two tasks around the LLM predictions.\nObject search: The robot receives an abstract user query\nand must navigate to the most relevant object in the Concept-\nGraphs map. Using an LVLM [55], the robot then checks if\nthe object is at the expected location. If not, it queries an\nLLM to find a new plausible location given the captions of\nthe other objects in the representation. In our prompt, we\nnudge the LLM to consider typical containers or storage\nlocations. We illustrate two such queries where the target\nobject is moved in Figure 3.\nTraversability estimation: As shown in Fig. 4, we design\na real-world scenario where the robot finds itself enclaved\nby objects. In this scenario, the robot must push around\nmultiple objects and create a path to the goal state. While\ntraversability can be learned through experience [62], we\nshow that grounding LLM knowledge in a 3D map can grant\nsimilar capabilities to robotic agents.\nF. Open-Vocabulary Pick and Place\nTo illustrate how ConceptGraphs can act as the percep-\ntion backbone for open-vocabulary mobile manipulation, we\nconducted a series of experiments with a Boston Dynamics\nSpot Arm robot. Using an onboard RGBD camera and\na ConceptGraphs representation of the scene, the Spot robot\nresponds to the query cuddly quacker by grabbing a\nduck plush toy and placing it in a nearby box (Figure 1). In\nthe supplementary video, Spot completes a similar grasping\nmaneuver with a mango when prompted with the query\nsomething healthy to eat.\nG. Localization and Map Updates\nConceptGraphs can also be used for object-based lo-\ncalization and map updates. We showcase this with a 3-\nDoF (x, y and yaw) localization and remapping task in\nthe AI2Thor [63], [64] simulation environment, where a\nFig. 3: A Jackal robot answering user queries using the ConceptGraphs representation of a lab environment. We first query an LLM to\nidentify the most relevant object given the user query, then validate with an LVLM if the target object if is at the expected location. If\nnot, we query the LLM again to find a likely location or container for the missing object. (Blue) When prompted with something\nto wear for a space party, the Jackal attempts to find a grey shirt with a NASA logo. After failing to detect the shirt at the\nexpected location, the LLM reasons that it could likely be in the laundry bag. (Red) The Jackal searches for red and white sneakers after\nreceiving the user query footwear for a Ronald McDonald outfit. The LLM redirects the robot to a shoe rack after failing\nto detect the sneakers where they initially appeared on the map.\nFig. 4: The Jackal robot solving a traversability challenge. All\npaths to the goal are obstructed by objects. We query an LLM to\nidentify which objects can be safely pushed or traversed by the robot\n(green) and which objects would be too heavy or hinder the robot\u2019s\nmovement (red). The LLM relies on the ConceptGraphs node\ncaptions to make traversability predictions and we add the non-\ntraversable objects to the Jackal costmap for path planning. The\nJackal successfully reaches the goal by going through a curtain and\npushing a basketball, while also avoiding contact with bricks, an\niron dumbbell, and a flower pot.\nmobile robot uses a particle filter to localize in a pre-\nbuilt ConceptGraphs map of the environment. During the\nobservation update step of particle filtering, the robot\u2019s\ndetections are matched against the objects in the map based\non the hypothesized pose, in a similar way as described in\nSection II-A. The matching results are aggregated into a\nsingle observation score for weighting the pose hypothesis.\nDuring this process, previously observed objects are removed\nif they are not observed by the robot and new objects can also\nbe added. We provide a demonstration of this localization and\nmap updating approach in the supplementary video material.\nH. Limitations\nDespite its impressive performance, ConceptGraphs has\nfailure modes that remain to be addressed in future work.\nFirst, node captioning incurs errors due to the current limi-\ntations of LVLMs like LLaVA [55]. Second, our 3D scene\ngraph occasionally misses small or thin objects and makes\nduplicate detections. This impacts downstream planning, par-\nticularly when the incorrect detection is crucial to planning\nsuccess. Additionally, the computational and economic costs\nof our system include multiple LVLM (LLaVA [55]) and\none or more proprietary LLM inference(s) when building\nand querying the scenegraph, which may be significant.\nIV. CONCURRENT WORK\nWe briefly review recent and unpublished pre-prints that\nare exploring themes related to open-vocabulary object-based\nfactorization of 3D scenes. Concurrently to us, [65], [66]\nhave explored open-vocabulary object-based factorization of\n3D scenes. Where [65] assumes a pre-built point cloud map\nof the scene, [66] builds a map on the go. Both approaches\nassociate CLIP descriptors to the reconstruction, resulting\nin performance comparable to our system\u2019s CLIP variant,\nwhich struggles with queries involving complex affordances\nand negation, as shown in Table III. OGSV [67] is closer\nto our setting, building an open-vocabulary 3D scene graph\nfrom RGB-D images. However, [67] relies on a (closed-set)\ngraph neural network to predict object relationships; whereas\nConceptGraphs relies on the capabilities of modern LLMs,\neliminating the need to train an object relation model.\nV. CONCLUSION\nIn this paper, we introduced ConceptGraphs, a novel ap-\nproach to open-vocab object-centric 3D scene representation\nthat addresses key limitations in the existing landscape of\ndense and implicit representations. Through effective integra-\ntion of foundational 2D models, ConceptGraphs significantly\nmitigates memory constraints, provides relational informa-\ntion among objects, and allows for dynamic updates to the\nscene\u2014three pervasive challenges in current methods. Ex-\nperimental evidence underscores ConceptGraphs\u2019 robustness\nand extensibility, highlighting its superiority over existing\nbaselines for a variety of real-world tasks including manipu-\nlation and navigation. The versatility of our framework also\naccommodates a broad range of downstream applications,\nthereby opening new avenues for innovation in robot per-\nception and planning. Future work may delve into integrating\ntemporal dynamics into the model and assessing its perfor-\nmance in less structured, more challenging environments.\nACKNOWLEDGMENTS\nThis project was supported in part (KM, AT, JBT)\nby grants from the Army Research Laboratory (grant\nW911NF1820218), office of naval research (MURI). FS\nand LP thank NSERC for funding support. LP acknowl-\nedges support from the Canada CIFAR AI Chairs program.\nAll findings, opinions, and conclusions expressed in this\nmanuscript solely reflect the views of the authors; and not\nthat of our research sponsors.\nAPPENDIX\nA1. CONTRIBUTION STATEMENT\nQiao Gu, Ali Kuwajerwala, Sacha Morin, and Krishna\nMurthy were instrumental in the development, integration,\nand deployment of ConceptGraphs. These authors were\nresponsible for writing the majority of the manuscript.\nQiao spearheaded the implementation of the object-based\nmapping, localization and map update system, prototyped the\nobject captioning module, and conducted the segmentation\nexperiments.\nAli wrote initial prototypes of the mapping pipeline,\ncoordinated the real-robot experiments at the REAL Lab,\nimplemented the integration with the LLM planners and\nobject retrieval experiments, and performed a significant\namount of the hardware setup.\nSacha was at the forefront of deploying the system on\nrobot navigation, object search, and led the traversability\nexperiment, implementing a variety of crucial robot-side\nfunctionalities.\nKrishna crafted initial prototypes of the detector-free\nmapping system, scene graph construction, and vision/lan-\nguage model interfaces, also coordinating the project.\nBipasha Sen and Aditya Agarwal integrated Concept-\nGraphs with a robotic manipulation platform for open-\nvocabulary pick and place demonstrations, with Kirsty Ellis\nassisting them in setting up the manipulation platform.\nCorban Rivera and William Paul deployed Concept-\nGraphs on-board a Spot mini robot, showcasing the mobile\nmanipulation capabilities enabled by our system.\nChuang Gan helped qualitatively evaluate our system\nagainst end-to-end learned approaches like 3D-LLM.\nRama Chellappa and Celso de Melo provided adivce on\nreal-world demonstrations involving mobile manipulation.\nJosh Tenenbaum and Antonio Torralba contributed cog-\nnitive science and computer vision perspectives respectively,\nwhich shaped the experiments evaluating scene-graph con-\nstruction accuracy.\nFlorian Shkurti and Liam Paull provided equal advisory\nsupport on this project, contributing to brainstorming and\ncritical review processes throughout, and writing/proofread-\ning significant sections of the paper.\nA2. 3D SCENE GRAPH: GENERATING NODE CAPTIONS\nOnce we build an object-level map of the scene using\nthe methodology described in Sec. II-A, we extract and\nsummarize captions for each object. We first extract upto the\n10 most-informative views for each object, by tracking the\nnumber of (noise-free) 3D points that each image segment\ncontributes to an object in the map7. Intuitively, these views\noffer the best image views for the object. We run each view\nthrough an LVLM, here LLaVA-7B [55], to generate an\nimage caption. We use the same prompt across all images:\ndescribe the central object in this image.\nWe found the captions generated by LLaVA-7B to be\nincoherent or unreliable across all viewpoints. To alleviate\nthis, we employed GPT-4 as a caption summarizer, to map\nall of the LLaVA-7B captions to a coherent object tag (or\noptionally, declare the object as an invalid detection). We use\nthe following GPT-4 system prompt:\nIdentify and describe objects in scenes. Input\nand output must be in JSON format. The input\nfield \u2019captions\u2019 contains a list of image captions\naiming to identify objects. Output \u2019summary\u2019 as a\nconcise description of the identified object(s).\nAn object mentioned multiple times is likely\naccurate. If various objects are repeated and\na container/surface is noted such as a shelf or\ntable, assume the (repeated) objects are on that\ncontainer/surface. For unrelated, non-repeating\n(or empty) captions, summarize as \u2019conflicting\n(or empty) captions about [objects]\u2019 and set\n\u2019object tag\u2019 to \u2019invalid\u2019. Output \u2019possible tags\u2019\nlisting potential object categories. Set\n\u2019object tag\u2019 as the conclusive identification.\nFocus on indoor object types, as the input captions\nare from indoor scans.\nListing 1: GPT-4 system prompt used for caption summarization\nA3. LLM PLANNER: IMPLEMENTATION DETAILS\nFor task planning over 3D scene graphs, we use GPT-\n4 (gpt-4-0613) with a context length of 8K tokens8. We\nfirst convert each node in the 3D scene graph into a structured\ntext format (here, a JSON string). Each entry in the JSON\nlist corresponds to one object in the scene, and contains the\nfollowing attributes:\n1) object id: a unique (numerical) object identifier\n2) bounding box extents: dimensions of each side of the\nbounding cuboid\n3) bounding box center: centroid of the object bounding\ncuboid\n4) object tag: a brief tag describing the object\n5) caption: a one-sentence caption (possibly encoding\nmode details than present in the object tag\nHere is a sample snippet from the scene graph for the\nroom0 scene of the Replica [56] dataset.\n[\n{\nid: 2,\nbbox_extent: [2.0, 0.7, 0.6],\nbbox_center: [-0.6, 1.1, -1.2],\nobject_tag: wooden dresser or chest of drawers,\ncaption: A wooden dresser or chest of drawers\n},\n7We track these statistics throughout the mapping lifecycle; meaning that\nwe do not impose any additional computational overhead to determine the\n10 best views per object\n8We also prototyped variants of this approach on off-the-shelf LLMs with\nlarger context lengths, such as Claude-2 with a context length of 32K tokens,\nand found it to work reliably.\n{\nid: 3,\nbbox_extent: [0.6, 0.5, 0.4],\nbbox_center: [2.8, -0.4, -0.8],\nobject_tag: vase,\ncaption: a white, floral-patterned vase (or\npossibly a ceramic bowl)\n},\n...\n...\n{\nid: 110,\nbbox_extent: [1.2, 0.6, 0.0],\nbbox_center: [2.2, 2.1, 1.2],\nobject_tag: light fixture,\ncaption: a light fixture hanging from the\nceiling\n}\n]\nListing 2: Sample text entries in the 3D scene graph\nFor planning using the above text description of the scene,\nwe instantiate the system prompt for GPT-4 as follows:\nThe input to the model is a 3D scene described in a\nJSON format. Each entry in the JSON describes\none object in the scene, with the following\nfive fields:\n1. \"id\": a unique object id\n2. \"bbox_extent\": extents of the 3D bounding box\nfor the object\n3. \"bbox_center\": centroid of the 3D bounding box\nfor the object\n4. \"object_tag\": a brief (but sometimes inaccurate)\ntag categorizing the object\n5. \"caption\": a brief caption for the object\nOnce you have parsed the JSON and are ready to\nanswer questions about the scene, say \"I'm\nready\".\nThe user will then begin to ask questions, and the\ntask is to answer various user queries about\nthe 3D scene. For each user question, respond\nwith a JSON dictionary with the following\nfields:\n1. \"inferred_query\": your interpretaion of the user\nquery in a succinct form\n2. \"relevant_objects\": list of relevant object ids\nfor the user query (if applicable)\n3. \"query_achievable\": whether or not the user-\nspecified query is achievable using the objects\nand descriptions provided in the 3D scene.\n4. \"final_relevant_objects\": A final list of\nobjects relevant to the user-specified task. As\nmuch as possible, sort all objects in this\nlist such that the most relevant object is\nlisted first, followed by the second most\nrelevant, and so on.\n5. \"explanation\": A brief explanation of what the\nmost relevant object(s) is(are), and how they\nachieve the user-specified task.\nListing 3: System prompt for the LLM Planner interface\nHere are some example GPT-4 responses on our queries\nUser: I need a quick gift. Help!\nLLM-Planner: {\ninferred_query: Find suitable object for a gift.,\nrelevant_objects: [0, 6, 7, 23, 25, 31],\nquery_achievable: true,\nfinal_relevant_objects: [6],\nexplanation: The most suitable object for a gift\ncould be the ceramic vase (id 6). Vases are\ncommonly gifted items and this one could\npotentially be filled with a plant or flower\narrangement, making a lovely present.\n}\nUser: Need something to use as a paperweight\nLLM-Planner: {\ninferred_query: Find object to use as a paperweight\n.,\nrelevant_objects: [0, 6, 7, 8, 15, 23, 29, 31],\nquery_achievable: true,\nfinal_relevant_objects: [6],\nexplanation: The ceramic vase (id 6) could\npotentially be used as a paperweight due to its\nsmall size and assumed weight.\n}\nListing 4: Sample queries and raw outputs from the LLM Planner\nA4. EXAMPLE TEXT QUERIES\nHere we provide some of the text queries used in our\nexperiments.\nReplica Dataset Descriptive Queries: For each room, we\nrandomly selected 20 ground truth annotations collected via\nAmazon Mechanical Turk (AMT). Here is a sample from\nroom0 and office0.\noffice0 Descriptive Queries:\n1) This is a trash can against the wall next to a sofa.\n2) A chaise lounge right next to a small table.\n3) This is a television.\n4) This is a dropped, tiled ceiling in what appears to be a\nclassroom for children.\n5) This is a plant and it is next to the screens.\n6) This is the back of a chair in front of a screen.\n7) A small table in front of a large gray sectional couch.\n8) This is an armless chair and it\u2019s opposite a coffee table\nby the sofa.\n9) This is a plug-in and it is on the floor.\n10) These are table legs and they are underneath the table.\n11) These are chairs and they are next to a table.\n12) A diner style table in front of two chairs.\n13) These are rocks and they are on the wall.\n14) This is the right panel of a lighted display screen.\n15) This is a planet and it is on the wall.\n16) This is an electronic display screen showing a map, on\nthe wall.\n17) This is a couch and it is between a table and the wall.\n18) This is a garbage can and it is in front of the wall.\n19) This is a rug and it is on the floor.\n20) This is a table that is above the floor.\nroom0 Descriptive Queries:\n1) This is a pillow and this is on top of a couch.\n2) A pillow on top of a white couch.\n3) This is a couch and it is under a window.\n4) This is a stool and it is on top of a rug.\n5) This is a side table under a lamp.\n6) This is a ceiling light next to the window.\n7) This is an end table and it is below a lamp.\n8) These are books and they are on the table.\n9) This is a couch and it is in front of the wall.\n10) White horizontal blinds in a well lit room.\n11) This is a striped throw pillow on the loveseat.\n12) The pillow is on top of the chair.\n13) This is a window and it is next to a door.\n14) This is a hurricane candle and it is on top of a cabinet.\n15) This is a vase and it is on top of the table.\n16) This is a vent in the ceiling.\n17) This is a fish and it is on top of a cabinet.\n18) This is a window behind a chair.\n19) This is a trash can against a wall.\n20) Two cream colored cushioned chairs with blue pillows\nadjacent to each other.\nReplica Dataset Affordance Queries for Office Scenes:\n1) Something to watch the news on\n2) Something to tell the time\n3) Something comfortable to sit on\n4) Something to dispose of wastepaper in\n5) Something to add light into the room\nReplica Dataset Affordance Queries for Room Scenes:\n1) Somewhere to store decorative cups\n2) Something to add light into the room\n3) Somewhere to set food for dinner\n4) Something I can open with my keys\n5) Somewhere to sit upright for a work call\nReplica Dataset Negation Queries for Office Scenes:\n1) Something to sit on other than a chair\n2) Something very heavy, unlike a clock\n3) Something rigid, unlike a cushion\n4) Something small, unlike a couch\n5) Something light, unlike a table\nReplica Dataset Negation Queries for Room Scenes:\n1) Something small, unlike a cabinet\n2) Something light, unlike a table\n3) Something soft, unlike a table\n4) Something not transparent, unlike a window\n5) Something rigid, unlike a rug\nREAL Lab Scan Descriptive queries:\n1) A pair of red and white sneakers\n2) A NASA t-shirt\n3) A Rubik\u2019s cube\n4) A basketball\n5) A toy car\n6) A backpack\n7) An office chair\n8) A pair of headphones\n9) A yellow jacket\n10) A laundry bag\nREAL Lab Affordance Queries:\n1) Something to use to disassemble or take apart a laptop\n2) Something to use for cooling a CPU\n3) Something to use for carrying books day to day\n4) Something to use for temporarily securing a broken\nzipper\n5) Something to use to help a student understand how a\ncomputer works\n6) An object that is used in a sport involving rims and nets\n7) Something to keep myself from getting distracted by loud\nnoises\n8) Something to help explain math proofs to a student\n9) Something I can use to protect myself from the harsh\nwinter in Canada\n10) Something fun to pass the time with\nREAL Lab Negation Queries:\n1) A toy for someone who dislikes basketball\n2) Shoes that you wouldn\u2019t wear to something formal\n3) Something to protect me from the rain that\u2019s not an\numbrella\n4) Shoes that are not red and white\n5) Something to make a cape with that\u2019s not green\n6) Something to drink other than soda\n7) Something to use for exercise other than weights\n8) Something to wear unrelated to space or science\n9) Something light to store belongings, not a backpack\n10) Something to play with that\u2019s not a puzzle or colorful\nA5. NAVIGATION EXPERIMENTS\nFor our navigation experiments with the Jackal robot. Our\nrobot is equipped with a VLP-16 lidar and a foward-facing\nRealsense D435i camera. We begin by building a pointcloud\nof the REAL Lab using the onboard VLP-16 and Open3d\nSLAM [68]. The initial Jackal pointcloud does not include\ntask-relevant objects and is downprojected to a 2D costmap\nfor navigation using the base Jackal ROS stack.\nWe then stage two separate scenes with different objects:\none for object search and another for traversability estima-\ntion. In both cases, we map the scene with an Azure Kinect\nCamera and rely on RTAB-Map [69] to obtain camera poses\nand the scene point cloud. We proceed to build a Concept-\nGraphs representation and register the scene point cloud with\nthe initial Jackal map. For our navigation experiments, we\nonly use the objects OT .\nFor object search queries, we use the LLM Planner de-\nscribed in Section A3 as part of a simple state machine. The\nrobot first attempts to go look at the 3D coordinates of the\nmost relevant object identified in OT by the LLM Planner.\nWe then pass the onboard camera image to LLaVA [55] and\nask if the target object is in view. If not, we remove the\ntarget object from the scene graph and ask the LLM Planner\nto provide a new likely location for the object in the scene\nwith the following GPT-4 system prompt:\nThe object described as \u2018description\u2019 is not in the\nscene. Perhaps someone has moved it, or put it\naway. Let\u2019s try to find the object by visiting the\nlikely places, storage or containers that are\nappropriate for the missing object (eg: a a cabinet\nfor a wineglass, or closet for a broom). The new\nquery is: find a likely container or storage space\nwhere someone typically would have moved the object\ndescribed as \u2018description\u2019?\nListing 5: GPT system prompt for object localization.\nFor traversability estimation, we task GPT to classify a\ngiven object as traversable or non-traversable based on its\ndescription and possible tags. The system prompt is:\nYou are a wheeled robot that can push a maximum of\n5 pounds or 2.27 kg. Can you traverse through or\npush an object identified as \u2018description\u2019 with\npossible tags \u2018possible tags\u2019? Specifically, is it\npossible for you to push the object out of its path\nwithout damaging yourself?\nListing 6: GPT-4 system prompt for traversability estimation.\nWe then take the pointclouds of each non-traversable\nobjects and downproject them in the Jackal costmap before\nlaunching the navigation episode. The goal is provided in\nthis case as a specific pose in the room.\nFor all experiments in this section, we run a local instance\nof LLaVA offboard on a desktop when needed and otherwise\nuse the GPT-4 API for LLM queries.\nA6. LIMITATIONS\nAs indicated in Sec. III-H, there are a few failure modes\nof ConceptGraphs that remain to be addressed in subsequent\nwork. In particular, the LLaVA-7B [55] model used for node\ncaptioning misclassifies a non-negligible number of small\nobjects as toothbrushes or pairs of scissors. We believe that\nusing more performant vision-language models, including\ninstruction-finetuned variants of LLaVA [70] can alleviate\nthis issue to a large extent. This will, in turn, improve the\nnode and edge precisions of 3D scene graphs beyond what\nwe report in Table I.\nIn this work, we do not explicitly focus on improving\nLLM-based planning over 3D scene graphs. We refer the\ninterested reader to concurrent work, SayPlan [52], for\ninsights into how one might leverage the hierarchy inherent\nin 3D scene graphs, for efficient planning.\nREFERENCES\n[1] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim,\nA. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon,\n\u201cKinectfusion: Real-time dense surface mapping and tracking,\u201d in\nIEEE international symposium on mixed and augmented reality.\nIEEE, 2011, pp. 127\u2013136. 2\n[2] T. Whelan, S. Leutenegger, R. Salas-Moreno, B. Glocker, and A. Davi-\nson, \u201cElasticfusion: Dense slam without a pose graph,\u201d in Robotics\nScience and Systems, 2015. 2\n[3] B. Wen, J. Tremblay, V. Blukis, S. Tyree, T. M\u00a8uller, A. Evans, D. Fox,\nJ. Kautz, and S. Birchfield, \u201cBundleSDF: Neural 6-dof tracking and 3d\nreconstruction of unknown objects,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n606\u2013617. 2\n[4] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, \u201ciMAP: Implicit map-\nping and positioning in real-time,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 6229\u20136238.\n2\n[5] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and\nM. Pollefeys, \u201cNice-slam: Neural implicit scalable encoding for slam,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 12 786\u201312 796. 2\n[6] J. L. Schonberger and J.-M. Frahm, \u201cStructure-from-motion revisited,\u201d\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 4104\u20134113. 2\n[7] J. L. Sch\u00a8onberger, E. Zheng, J.-M. Frahm, and M. Pollefeys, \u201cPixel-\nwise view selection for unstructured multi-view stereo,\u201d in European\nConference on Computer Vision.\nSpringer, 2016, pp. 501\u2013518. 2\n[8] J. McCormac, A. Handa, A. Davison, and S. Leutenegger, \u201cSeman-\nticfusion: Dense 3d semantic mapping with convolutional neural net-\nworks,\u201d in IEEE International Conference on Robotics and automation\n(ICRA).\nIEEE, 2017, pp. 4628\u20134635. 2\n[9] M. Runz, M. Buffier, and L. Agapito, \u201cMaskfusion: Real-time recogni-\ntion, tracking and reconstruction of multiple moving objects,\u201d in IEEE\nInternational Symposium on Mixed and Augmented Reality (ISMAR).\nIEEE, 2018, pp. 10\u201320. 2\n[10] J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger,\n\u201cFusion++: Volumetric object-level slam,\u201d in international conference\non 3D vision (3DV).\nIEEE, 2018, pp. 32\u201341. 2\n[11] G. Narita, T. Seno, T. Ishikawa, and Y. Kaji, \u201cPanopticfusion: Online\nvolumetric semantic mapping at the level of stuff and things,\u201d in 2019\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS).\nIEEE, 2019, pp. 4205\u20134212. 2\n[12] J. Qian, V. Chatrath, J. Yang, J. Servos, A. P. Schoellig, and S. L.\nWaslander, \u201cPOCD: probabilistic object-level change detection and\nvolumetric mapping in semi-static scenes,\u201d in Robotics Science and\nSystems, K. Hauser, D. A. Shell, and S. Huang, Eds., 2022. 2\n[13] J. Qian, V. Chatrath, J. Servos, A. Mavrinac, W. Burgard, S. L.\nWaslander, and A. P. Schoellig, \u201cPOV-SLAM: probabilistic object-\naware variational SLAM in semi-static environments,\u201d in Robotics\nScience and Systems, K. E. Bekris, K. Hauser, S. L. Herbert, and\nJ. Yu, Eds., 2023. 2\n[14] K. Li, D. DeTone, Y. F. S. Chen, M. Vo, I. Reid, H. Rezatofighi,\nC. Sweeney, J. Straub, and R. Newcombe, \u201cODAM: Object detection,\nassociation, and mapping using posed rgb video,\u201d in Proceedings of\nInternational Conference on Computer Vision, 2021. 2\n[15] M. Zins, G. Simon, and M.-O. Berger, \u201cOA-SLAM: Leveraging\nobjects for camera relocalization in visual slam,\u201d in IEEE International\nSymposium on Mixed and Augmented Reality (ISMAR).\nIEEE, 2022,\npp. 720\u2013728. 2\n[16] K. Liu, F. Zhan, J. Zhang, M. Xu, Y. Yu, A. E. Saddik, C. Theobalt,\nE. Xing, and S. Lu, \u201c3d open-vocabulary segmentation with foundation\nmodels,\u201d arXiv preprint arXiv:2305.14093, 2023. 2\n[17] K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, G. Iyer,\nS. Saryazdi, T. Chen, A. Maalouf, S. Li, N. V. Keetha, A. Tewari,\nJ. B. Tenenbaum, C. M. de Melo, K. M. Krishna, L. Paull, F. Shkurti,\nand A. Torralba, \u201cConceptFusion: Open-set multimodal 3d mapping,\u201d\nin Robotics: Science and Systems, 2023. 2, 4, 5\n[18] S. Peng, K. Genova, C. Jiang, A. Tagliasacchi, M. Pollefeys,\nT. Funkhouser et al., \u201cOpenscene: 3d scene understanding with\nopen vocabularies,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 815\u2013824. 2\n[19] R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, and X. Qi, \u201cLowis3d:\nLanguage-driven open-world instance-level 3d scene understanding,\u201d\narXiv preprint arXiv:2308.00353, 2023. 2\n[20] R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, and X. Qi, \u201cPLA:\nLanguage-driven open-vocabulary 3d scene understanding,\u201d in Pro-\nceedings of Computer Vision and Pattern Recognition, 2023. 2\n[21] J. Zhang, R. Dong, and K. Ma, \u201cCLIP-FO3D: Learning free open-\nworld 3d scene representations from 2d dense clip,\u201d arXiv preprint\narXiv:2303.04748, 2023. 2\n[22] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi, \u201cNeural feature\nfusion fields: 3d distillation of self-supervised 2d image representa-\ntions,\u201d in International Conference on 3D Vision (3DV).\nIEEE, 2022.\n2\n[23] S. Kobayashi, E. Matsumoto, and V. Sitzmann, \u201cDecomposing nerf for\nediting via feature field distillation,\u201d Neural Information Processing\nSystems, vol. 35, pp. 23 311\u201323 330, 2022. 2\n[24] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam,\n\u201cClip-fields: Weakly supervised semantic fields for robotic memory,\u201d\nin Robotics: Science and Systems, K. E. Bekris, K. Hauser, S. L.\nHerbert, and J. Yu, Eds., 2023. 2\n[25] N. Tsagkas, O. Mac Aodha, and C. X. Lu, \u201cVl-fields: Towards\nlanguage-grounded neural implicit spatial representations,\u201d arXiv\npreprint arXiv:2305.12427, 2023. 2\n[26] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik,\n\u201cLERF: Language embedded radiance fields,\u201d in International Con-\nference on Computer Vision (ICCV), 2023. 2\n[27] C. Huang, O. Mees, A. Zeng, and W. Burgard, \u201cAudio visual language\nmaps for robot navigation,\u201d arXiv preprint arXiv:2303.07522, 2023. 2\n[28] W.\nShen,\nG.\nYang,\nA.\nYu,\nJ.\nWong,\nL.\nP.\nKaelbling,\nand\nP. Isola, \u201cDistilled feature fields enable few-shot manipulation,\u201d\nin International Conference on Robot Learning, 2023. [Online].\nAvailable: https://openreview.net/forum?id=Rb0nGIt kh5 2\n[29] F. Engelmann, F. Manhardt, M. Niemeyer, K. Tateno, M. Pollefeys,\nand F. Tombari, \u201cOpen-set 3d scene segmentation with rendered novel\nviews,\u201d 2023. 2\n[30] K. Mazur, E. Sucar, and A. J. Davison, \u201cFeature-realistic neural fusion\nfor real-time, open set scene understanding,\u201d in IEEE International\nConference on Robotics and Automation (ICRA).\nIEEE, 2023. 2\n[31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable\nvisual models from natural language supervision,\u201d in International\nConference on Machine Learning.\nPMLR, 2021. 2, 3, 4\n[32] OpenAI, \u201cGpt-4 technical report,\u201d arXiv preprint arXiv:2303.08774,\n2023. 2, 4\n[33] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\nT. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., \u201cSegment any-\nthing,\u201d Proceedings of International Conference on Computer Vision,\n2023. 2, 4, 5\n[34] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li,\nJ. Yang, H. Su, J. Zhu et al., \u201cGrounding dino: Marrying dino with\ngrounded pre-training for open-set object detection,\u201d arXiv preprint\narXiv:2303.05499, 2023. 2, 4\n[35] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d in\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022, pp. 10 684\u201310 695. 2\n[36] Y. Hong, Y. Du, C. Lin, J. Tenenbaum, and C. Gan, \u201c3d concept\ngrounding on neural fields,\u201d Neural Information Processing Systems,\n2022. 2\n[37] Y. Hong, C. Lin, Y. Du, Z. Chen, J. B. Tenenbaum, and C. Gan,\n\u201c3d concept learning and reasoning from multi-view images,\u201d in\nProceedings of Computer Vision and Pattern Recognition, 2023. 2\n[38] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan,\n\u201c3d-llm: Injecting the 3d world into large language models,\u201d Neural\nInformation Processing Systems, 2023. 2\n[39] M. Shridhar, L. Manuelli, and D. Fox, \u201cCLIPort: What and where\npathways for robotic manipulation,\u201d in Conference on Robot Learning,\nvol. 164.\nPMLR, 2021, pp. 894\u2013906. 2\n[40] S. Sharma, A. Rashid, C. M. Kim, J. Kerr, L. Y. Chen, A. Kanazawa,\nand K. Goldberg, \u201cLanguage embedded radiance fields for zero-\nshot task-oriented grasping,\u201d in International Conference on Robot\nLearning, 2023. [Online]. Available: https://openreview.net/forum?id=\nk-Fg8JDQmc 2\n[41] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song,\n\u201cClip on wheels: Zero-shot object navigation as object localization\nand exploration,\u201d arXiv preprint arXiv:2203.10421, 2022. 2\n[42] D. Shah, B. Osi\u00b4nski, S. Levine et al., \u201cLm-nav: Robotic navigation\nwith large pre-trained models of language, vision, and action,\u201d in\nInternational Conference on Robot Learning.\nPMLR, 2023. 2\n[43] M. Fisher, M. Savva, and P. Hanrahan, \u201cCharacterizing structural\nrelationships in scenes using graph kernels,\u201d ACM Trans. Graph.,\nvol. 30, no. 4, p. 34, 2011. 2\n[44] P. Gay, J. Stuart, and A. Del Bue, \u201cVisual graphs from motion\n(vgfm): Scene understanding with object geometry reasoning,\u201d in\nAsian Conference on Computer Vision.\nSpringer, 2019. 2\n[45] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, and\nS. Savarese, \u201c3d scene graph: A structure for unified semantics, 3d\nspace, and camera,\u201d in Proceedings of International Conference on\nComputer Vision, October 2019. 2\n[46] U.-H. Kim, J.-M. Park, T.-J. Song, and J.-H. Kim, \u201c3-d scene graph:\nA sparse and semantic representation of physical environments for\nintelligent agents,\u201d IEEE transactions on cybernetics, vol. 50, no. 12,\npp. 4921\u20134933, 2019. 2\n[47] J. Wald, H. Dhamo, N. Navab, and F. Tombari, \u201cLearning 3d semantic\nscene graphs from 3d indoor reconstructions,\u201d in Proceedings of\nComputer Vision and Pattern Recognition, 2020. 2\n[48] A. Rosinol, A. Violette, M. Abate, N. Hughes, Y. Chang, J. Shi,\nA. Gupta, and L. Carlone, \u201cKimera: From slam to spatial perception\nwith 3d dynamic scene graphs,\u201d The International Journal of Robotics\nResearch, vol. 40, no. 12-14, pp. 1510\u20131546, 2021. 2\n[49] N. Hughes, Y. Chang, and L. Carlone, \u201cHydra: A real-time spatial\nperception system for 3d scene graph construction and optimization,\u201d\narXiv preprint arXiv:2201.13360, 2022. 2\n[50] S.-C. Wu, J. Wald, K. Tateno, N. Navab, and F. Tombari, \u201cScenegraph-\nfusion: Incremental 3d scene graph prediction from rgb-d sequences,\u201d\nin Proceedings of Computer Vision and Pattern Recognition, 2021. 2\n[51] C. Agia, K. M. Jatavallabhula, M. Khodeir, O. Miksik, V. Vineet,\nM. Mukadam, L. Paull, and F. Shkurti, \u201cTaskography: Evaluating robot\ntask planning over large 3d scene graphs,\u201d in International Conference\non Robot Learning.\nPMLR, 2022. 2\n[52] K. Rana, J. Abou-Chakra, S. Garg, J. Haviland, I. Reid, and\nN. Suenderhauf, \u201cSayplan: Grounding large language models using\n3d scene graphs for scalable task planning,\u201d in International\nConference on Robot Learning, 2023. [Online]. Available: https:\n//openreview.net/forum?id=wMpOMO0Ss7a 2, 4, 10\n[53] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khali-\ndov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., \u201cDinov2:\nLearning robust visual features without supervision,\u201d arXiv preprint\narXiv:2304.07193, 2023. 2, 3\n[54] Y. Zhang, X. Huang, J. Ma, Z. Li, Z. Luo, Y. Xie, Y. Qin, T. Luo,\nY. Li, S. Liu et al., \u201cRecognize anything: A strong image tagging\nmodel,\u201d arXiv preprint arXiv:2306.03514, 2023. 2, 4\n[55] H. Liu, C. Li, Q. Wu, and Y. J. Lee, \u201cVisual instruction tuning,\u201d arXiv\npreprint arXiv:2304.08485, 2023. 2, 4, 5, 6, 7, 9, 10\n[56] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green,\nJ. J. Engel, R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan,\nB. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter,\nJ. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva,\nD. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove,\nand R. Newcombe, \u201cThe Replica dataset: A digital replica of indoor\nspaces,\u201d arXiv preprint arXiv:1906.05797, 2019. 4, 5, 7\n[57] T. L\u00a8uddecke and A. Ecker, \u201cImage segmentation using text and image\nprompts,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 7086\u20137096. 5\n[58] B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl,\n\u201cLanguage-driven semantic segmentation,\u201d in International Confer-\nence on Learning Representations, 2022. 5\n[59] G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin, \u201cScaling open-vocabulary\nimage segmentation with image-level labels,\u201d in European Conference\non Computer Vision.\nSpringer, 2022, pp. 540\u2013557. 5\n[60] C. Zhou, C. C. Loy, and B. Dai, \u201cExtract free dense labels from clip,\u201d\nin European Conference on Computer Vision (ECCV), 2022. 5\n[61] Y. Du, S. Li, and I. Mordatch, \u201cCompositional visual generation with\nenergy based models,\u201d in Neural Information Processing Systems,\n2020. 5\n[62] S. Levine and D. Shah, \u201cLearning robotic navigation from experience:\nprinciples, methods and recent results,\u201d Philosophical Transactions of\nthe Royal Society B, vol. 378, no. 1869, p. 20210447, 2023. 5\n[63] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti,\nM. Deitke, K. Ehsani, D. Gordon, Y. Zhu et al., \u201cAi2-thor: An interac-\ntive 3d environment for visual ai,\u201d arXiv preprint arXiv:1712.05474,\n2017. 5\n[64] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, K. Ehsani, J. Salvador,\nW. Han, E. Kolve, A. Kembhavi, and R. Mottaghi, \u201cProcthor: Large-\nscale embodied ai using procedural generation,\u201d Advances in Neural\nInformation Processing Systems, vol. 35, pp. 5982\u20135994, 2022. 5\n[65] A. Takmaz, E. Fedele, R. W. Sumner, M. Pollefeys, F. Tombari, and\nF. Engelmann, \u201cOpenmask3d: Open-vocabulary 3d instance segmen-\ntation,\u201d arXiv preprint arXiv:2306.13631, 2023. 6\n[66] S. Lu, H. Chang, E. P. Jing, A. Boularias, and K. Bekris, \u201cOVIR-3d:\nOpen-vocabulary 3d instance retrieval without training on 3d data,\u201d\nin International Conference on Robot Learning, 2023. [Online].\nAvailable: https://openreview.net/forum?id=gVBvtRqU1\n6\n[67] H. Chang, K. Boyalakuntla, S. Lu, S. Cai, E. P. Jing, S. Keskar,\nS. Geng, A. Abbas, L. Zhou, K. Bekris, and A. Boularious,\n\u201cContext-aware entity grounding with open-vocabulary 3d scene\ngraphs,\u201d in International Conference on Robot Learning, 2023.\n[Online]. Available: https://openreview.net/forum?id=cjEI5qXoT0 6\n[68] E. Jelavic, J. Nubert, and M. Hutter, \u201cOpen3d slam: Point cloud based\nmapping and localization for education,\u201d in Robotic Perception and\nMapping: Emerging Techniques, ICRA 2022 Workshop.\nETH Zurich,\nRobotic Systems Lab, 2022, p. 24. 9\n[69] M. Labb\u00b4e and F. Michaud, \u201cRtab-map as an open-source lidar and\nvisual simultaneous localization and mapping library for large-scale\nand long-term online operation,\u201d Journal of Field Robotics, vol. 36,\nno. 2, pp. 416\u2013446, 2019. 9\n[70] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui,\nY.-X. Wang, Y. Yang, K. Keutzer, and T. Darrell, \u201cAligning large\nmultimodal models with factually augmented rlhf,\u201d 2023. 10\n"
  }
]