[
  {
    "title": "Table-GPT: Table-tuned GPT for Diverse Table Tasks",
    "link": "https://arxiv.org/pdf/2310.09263.pdf",
    "upvote": "35",
    "text": "Table-GPT: Table-tuned GPT for Diverse Table Tasks\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nMicrosoft Corporation\nABSTRACT\nLanguage models, such as GPT-3 and ChatGPT, demonstrate re-\nmarkable abilities to follow diverse human instructions and perform\na wide range of tasks. However, when probing language models\nusing a range of basic table-understanding tasks, we observe that\ntoday\u2019s language models are still sub-optimal in many table-related\ntasks, likely because they are pre-trained predominantly on one-\ndimensional natural-language texts, whereas relational tables are\ntwo-dimensional objects.\nIn this work, we propose a new \u201ctable-tuning\u201d paradigm, where\nwe continue to train/fine-tune language models like GPT-3.5 and\nChatGPT, using diverse table-tasks synthesized from real tables as\ntraining data, with the goal of enhancing language models\u2019 ability to\nunderstand tables and perform table tasks. We show that our result-\ning Table-GPT models demonstrate (1) better table-understanding\ncapabilities, by consistently outperforming the vanilla GPT-3.5 and\nChatGPT, on a wide-range of table tasks, including holdout unseen\ntasks, and (2) strong generalizability, in its ability to respond to di-\nverse human instructions to perform new table-tasks, in a manner\nsimilar to GPT-3.5 and ChatGPT.\n1\nINTRODUCTION\nLarge language models, such as GPT and LLaMa, have recently\ndemonstrated impressive abilities in performing diverse natural-\nlanguage tasks [5, 8, 12, 50]. In the database literature, a number of\npioneering work, such as [20, 29, 39, 43], have also shown that by\nusing \u201cprompt engineering\u201d, to careful select the best instructions\nand few-shot examples for a particular task at hand, language mod-\nels can be prompted to perform well on a number of table-tasks\nsuch as entity matching and data-imputation.\nWhile prompt-engineering is a promising direction to enhance\nmodel performance, it requires task-specific tuning (e.g., task-specific\nlabeled-data to test the performance of different instruction/exam-\nple combinations) [6, 8, 61]. We in this work propose an orthog-\nonal paradigm called \u201ctable-tuning\u201d, where instead of modifying\nprompts, we modify the weights of the underlying language models\nfor once (i.e., not task-specific), by continuing to train them using\n\u2020: Affiliation: Georgia Tech (pengli@gatech.edu), work done at Microsoft.\n\u2021: Correspondence: yeyehe@microsoft.com\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConference\u201917, July 2017, Washington, DC, USA\n\u00a9 2023 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nFigure 1: Two simple tests to probe language-models\u2019 basic\nability to read and understand tables. (Left) T-1: Missing cells\nidentification, which is to identify the column-header/row-\nid of a missing cell. (Right) T-2: Column-Finding, which is\nto identify the column-name of a given value. Even large\nmodels (e.g. 175B GPT-3.5) can frequently fail on such tests,\nwith only 0.26 accuracy in one variant of the tests.\ndiverse table-tasks as training data, to improve their ability to un-\nderstand tables. We show that table-tuned Table-GPT consistently\noutperform the vanilla GPT-3.5 and ChatGPT on a wide-range of\ntable tasks, including new and unseen table-tasks. We note that our\nmodel-tuning approach is complementary to prompt-engineering,\nbecause carefully engineered prompts can continue to benefit both\nvanilla language-models and our table-tuned models.\nToday\u2019s language models cannot \u201cread tables\u201d reliably.\nWhile today\u2019s language models excel in natural-language tasks, we\nstart by asking the question of whether these models are optimal for\ntable-tasks, because after all, they are pre-trained predominantly\non natural language texts, which are different from tables.\nMore specifically, natural language texts are (1) one-directional,\n(2) read left-to-right, where (3) swapping two tokens will generally\nchange the meaning of a sentence. In contrast, relational tables are\n(1) two-dimensional in nature with both rows and columns, (2) where\nreading top-to-bottom in the vertical direction for values in the same\ncolumn, is crucial in many table-tasks. Furthermore, unlike text,\n(3) tables are largely \u201cinvariant\u201d to row and column permutations,\nwhere swapping two rows or columns do not generally change the\nsemantic meaning of the table.\nWith this question in mind, we perform two simple tests to\nprobe language models\u2019 ability to \u201cread\u201d tables and then answer\nbasic questions, which we call (T-1) Missing-value-identification,\nand (T-2) Column-finding, as shown in Figure 1.\nIn (T-1) Missing-value-identification, we show language models\nwith a real table, presented in a markdown or alternative format,\nwhere we make sure that there is exactly one empty cell in the table.\nMarkdown table is a common format used by prior work to feed tables into language-\nmodels, and also a format that models like GPT will use when it needs to respond\nwith a table, presumably because GPT-like models use GitHub data in its pre-training,\nwhere markdown-format tables are abundant.\narXiv:2310.09263v1  [cs.CL]  13 Oct 2023\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nFigure 2: Example table-tasks, where the ability of language\nmodels to \u201cread\u201d tables vertically is important. (Left) T-3: Ta-\nble Question-Answering. (Right) T-8: Data Imputation. More\ntasks like these are shown in Table 2.\nWe then ask the model to identify the empty cell, by responding\nwith the column-name and row-id of the empty cell, repeating for\n1000 randomly sampled real tables. Despite the impressive ability\nof language-models like GPT-3.5 to perform diverse tasks, we find\nthat they fail on a surprisingly large fraction (up to 74%) of such\ntests, often responding with incorrect column-headers or row-ids\n\u2013 for instance, in the example shown in Figure 1, the model may\nanswer that the column \u201cmusic\u201d has a missing cell, when the correct\nanswer should be \u201cart\u201d.\nIn order to ensure that there is no ambiguity in what \u201cmissing\nvalue\u201d or \u201cempty cell\u201d could mean to language models, we design a\nsecond and even simpler test, which we refer to as: (T-2) Column-\nfinding, shown on the right of Figure 1. In this test, we present\na language model with a real table, and ask it to find a specific\ncell-value that appears exactly once in the entire table (e.g., \u201c93\u201d in\nthis example), and then respond with the column-name of the that\nvalue. We find that language models such as GPT-3.5 are prone to\nfail on such tests again (e.g., answering that \u201c93\u201d is in column \u201cart\u201d\nwhen the correct answer is \u201cmusic\u201d), on over half of such tests.\nWe believe these simple probes show that today\u2019s large language\nmodels, when pre-trained on large amounts of one-directional\nnatural-language texts, are not best-suited to \u201cread\u201d two-dimensional\ntables, especially in the vertical direction, which however is crucial\nin performing many table-tasks.\nConsider, for example, the popular NLP task of (T-3) Table-\nQA [11, 42, 49], where the task is to answer a natural-language ques-\ntion, based on the content of the table. The left side of Figure 2 shows\nsuch an example. Given the question \u201cHow many second-graders\nscored over 90 in art, in the table below?\u201d Imagine that a\nmodel is not able to \u201cread\u201d tables correctly, it may reason that both\n\u201cJennifer\u201d and \u201cJames\u201d satisfy the condition (because it believes\n\u201c93\u201d is in the column \u201cart\u201d, like shown in Figure 1 (Right)), and\nmay answer \u201c2\u201d instead of the correct \u201c1\u201d. We emphasize that the\nability to read in the vertical direction (top-to-bottom for values\nin the same column) is similarly important in many other table-\ntasks, such as data-imputation (shown on the right of Figure 2),\ndata-transformation, error-detection, NL-to-SQL, etc., like the list\nin Table 2 would show, which includes a diverse set of table-tasks\nconsidered in this work.\nIn addition, we find that large language models are sensitive to\nthe order in which columns are presented in a table \u2013 e.g., when\nwe swap the order of two columns in a table, a model can change\nits response for a table-task, even when such a swap should not\nchange the semantic meaning of the table, at least to humans. This\nis presumably because language-models are pre-trained on text\nwhere the order of tokens matters (e.g., \u201cJennifer called you\u201d vs.\n\u201cyou called Jennifer\u201d), leading to sub-optimal behaviors on tables.\nWe believe observations like these point to opportunities for\nus to improve the underlying language model, by enhancing their\nability to understand tables and perform table-tasks.\nInstruction-tuning in NLP: train language-models to fol-\nlow diverse human instructions. To change the behaviour of\nlanguage models, successful attempts have been made in the NLP\ncommunity, using a technique known as \u201cinstruction-tuning\u201d in\nthe literature [40, 47, 57\u201359].\nIt was observed in the NLP community [8, 40, 59], that earlier\nversions of pre-trained language models, such as GPT-3, is able\nto complete a sentence with the next likely token (e.g., \u201cwrite a\nbed-time\u201d \u2192 \u201cstory\u201d), but cannot reliable follow higher-level in-\nstructions from humans (e.g., \u201cwrite a bed-time story for a 3\nyears-old, in 100 words\u201d), a behavior that is only demonstrated\nin later models such as ChatGPT.\nInstruction-tuning was the key technique invented that contin-\nues to train GPT-like models into ChatGPT-like models, in a process\nshown on the left of Figure 3. Diverse training data in the form\nof \u201c(instruction, completion)\u201d pairs are constructed, often man-\nually annotated by human labellers [40], e.g. (\u201cwrite a bed-time\nstory\u201d \u2192 an-actual-story), to continue train language-models on\nthese explicit demonstrations of how to follow high-level human\ninstructions, leading to well-known models such as ChatGPT/In-\nstructGPT [2, 40], as well as their open-source counterparts like\nStanford-Alpaca [4] and LLaMa-chat [50].\nTable-tuning: train language-models to understand tables.\nWe believe that the research on instruction-tuning in NLP, which\nsuccessfully enhances language-models ability to follow human\ninstructions, holds lessons for us when we aim to enhance language-\nmodels ability to understand tables and perform table-tasks.\nIn this work, we propose a \u201ctable-tuning\u201d paradigm analogous\nto instruction-tuning, where we continue to train language-models,\nusing diverse training data in the form of (instruction, table,\ncompletion), which we synthesize using large amounts of real tables.\nThis process is illustrated on the right of Figure 3.\nThrough extensive experiments, we show that \u201ctable-tuning\u201d is\na promising new direction, as our resulting Table-GPT models are:\n(1) Strong table models, which substantially outperform 175B GPT-\n3.5 and ChatGPT, on a wide range of seen and unseen table-tasks,\nas we summarize in Table 2 and Figure 9;\n(2) Generalizable to new tasks, as they can respond well to novel\nand unseen table-tasks, similar to how Chat-GPT could generalize\nand respond to new and unseen NLP tasks, like shown in Figure 4.\nContributions. We make the following contributions:\n\u2022 We propose a new \u201ctable-tuning\u201d paradigm to continue to train\nlanguage models, specifically designed to enhance language-\nmodels\u2019 ability to perform table-tasks, using diverse table-tasks\nsynthesized from large amounts of real tables, in a \u201csynthesis-\nthen-augment\u201d process.\n\u2022 We develop task-level, table-level, instruction-level, and completion-\nlevel data augmentation techniques for table-tuning, which we\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nFigure 3: Instruction-tuning vs. Table-tuning. (Left) Instruction-tuning is a technique developed in the NLP community that\ncontinues to train language-models (e.g., GPT) for instruction-following capabilities (e.g., in ChatGPT). (Right) Table-tuning is\nan analogous approach we propose to train language-models to better understand table and perform table-tasks.\nshow are crucial to avoid over-fitting and ensure the generality\nof Table-GPT.\n\u2022 We show that Table-GPT not only excels on table-tasks in both\nzero-shot and few-shot settings out of box, but can also serve as a\n\u201ctable foundation model\u201d and used as a better starting point than\nvanilla GPT, for down-stream single-task optimizations such as\ntask-specific fine-tuning and prompt-engineering.\n2\nPRELIMINARIES\nWe will start with a review of language models, and then the use\nof language models in table-tasks.\n2.1\nLanguage models\nThere are two popular styles of language models today, known\nas the decoder and encoder-style, both derived from the original\ntransformer architecture [53].\nEncoder-style language models. One class of popular lan-\nguage models, including the well-known BERT [17] and RoBERTa [35],\nuse only encoders from the transformer, and are pre-trained on\nlarge amounts of texts to effectively represent the semantics of texts\nusing embedding vectors.\nDown-stream tasks: Task-specific fine-tuning. To use encoder-\nstyle models like BERT for downstream tasks, task-specific fine-\ntuning is generally employed [21, 34], which continues to fine-tune\n(or train) BERT-like models for a given task, using task-specific\nlabeled data. For example, suppose the downstream task is senti-\nment analysis of Yelp restaurant reviews, then labels in the form\nof (\u201cThe food is amazing\u201d, \u201cpositive\u201d), (\u201cThe service is slow\u201d,\n\u201cnegative\u201d), are needed to fine-tune BERT-like models for the de-\nsired outcome [17, 46].\nCrucially, when the target input data or the desired output\nchanges, the labeling effort often needs to repeat for the best per-\nformance. For example, if the input data for sentiment analysis\nchanges to IMDB reviews, or if the output needs to include a classi-\nfication of \u201ccuisine-type\u201d for restaurant reviews. While encoder-\nstyle language-models are strong models, the need to fine-tune\nwith task-specific labelled data limits its ability to generalize to new\nunseen tasks [17, 22, 35, 46].\nDecoder-style \u201cgenerative\u201d language models. Another class\nof decoder-only language models, such as GPT [8] and LLaMa [50],\nare generative in nature, and are shown to excel in generalizing to\nnew downstream tasks without task-specific fine-tuning [8].\nGeneralize to new tasks: zero-shot and few-shot learning. It was\nshown in the NLP literature that the decoder-style models (e.g.,\nGPT and LLaMa), especially after instruction-tuning [31, 40, 47, 56\u2013\n59, 67] (e.g., ChatGPT/InstructGPT [2, 40] and Stanford Alpaca [4]),\ncan adapt to new tasks easily, using just natural-language instruc-\ntions (e.g., \u201cclassify the sentiments in the following reviews\u201d),\nand optionally a few examples. Such an approach can adapt to new\ndatasets (e.g., IMDB vs. Yelp reviews) and new tasks (sentiment-\nanalysis vs. machine-translation), without fine-tuning on labelled\ndata for each specific task, making the decoder-style models more\ngeneral and versatile. Figure 5 shows the benefit of \u201cinstruction-\ntuning\u201d in model generalizability, depicted pictorially on the y-axis.\n2.2\nLanguage models for table tasks\nPioneering work in the database literature have employed language\nmodels in various ways to perform table-related tasks.\nEncoder-style language models for table tasks. There is a\nlong and fruitful line of research (e.g., TURL [16], TaBERT [64],\nDitto [32] and Doduo [48]), where table-models are trained based\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nFigure 4: Table-models should ideally \u201cgeneralize\u201d to new\ndatasets and new tasks. (Left) Column type annotation (CTA):\nwhile this is a common table-task, the list of target-types to\nchoose from can vary from dataset to dataset (e.g., 78 types\nin [25], and 107 in [16]). Making table-models to \u201cgeneral-\nize\u201d to new CTA dataset without needing to retrain, is use-\nful. (Right) Text-to-Table: a general table-model should be\nas general-purpose as models like ChatGPT, in following\ninstructions to perform novel unseen table-tasks, such as\n\u201cextracting tables from text\u201d in the example.\non encoder-style BERT-like models, which are shown to perform\nwell on various table tasks.\nHowever, like their BERT-like base models, to generalize to a new\ndataset or a new task, these encoder-style table-models generally\nrequire fine-tuning with labeled data. As a concrete example, for\nthe table-task of \u201ccolumn-type-annotation\u201d [16, 48], in order to\nmove from one dataset with 78 semantic types [25], to another\ndataset with 107 semantic types [16], new labeled data have to be\nobtained, so that the models can be fine-tuned to generate the new\noutput with 107 classes [16]. In contrast, being able to adapt to new\ndatasets and tasks without task-specific fine-tuning, is a key goal\nthat we want to achieve in this work, like illustrated in Figure 4.\nDecoder-style language models for table tasks. With the suc-\ncess of decoder-style language models such as GPT-3 and ChatGPT,\nwhich are shown to perform tasks out-of-the-box with instructions\nonly, pioneering research in the database field develop \u201cprompt-\nengineering\u201d techniques for table-tasks [29, 39, 43], which carefully\nselects instructions and examples in the prompt, such that vanilla\nlanguage models can perform well on table-related tasks.\nTable-tuning for table-tasks. In contrast to prompt-engineering\nthat optimizes prompts, our proposed \u201ctable-tuning\u201d explores the\northogonal direction, where we continue to train the underlying\nlanguage models, for once only (not task-specific), so that the re-\nsulting model perform better on a range of table-tasks. This is com-\nplementary to prompt-engineering, because carefully-engineered\ninstructions and examples can continue to benefit both the vanilla\nGPT as well as our Table-GPT, as we will show in our experiments.\nFigure 5 shows the process of table-tuning, which is analogous\nto instruction-tuning, but unlike instruction-tuning that improves\nmodel generalizability to follow human instructions (y-axis), we\nfocus on improving underlying models ability to understand tables\nand perform table-tasks (x-axis). Crucially, as we will show, our\ntable-tuned models remain to be general and capable of following\nhuman-instructions to perform table-tasks (without task-specific\nFigure 5: Instruction-tuning vs. Table-tuning. Instruction-\ntuning improves model \u201cgeneralizability\u201d, to follow diverse\nhuman-instructions to perform new and unseen tasks (x-\naxis), whereas our proposed table-tuning is analogous in\nspirit but aims to improve model ability to understand tables\nand perform table-tasks (y-axis).\nfine-tuning), just like the underlying GPT-3 and ChatGPT models. In\nother words, in Table-GPT we aim to get the \u201cbest of both worlds\u201d,\nwith both generalizability, and good table-task performance.\n3\nCAN LANGUAGE MODELS \u201cREAD\u201d TABLES?\nSince language models like GPT are pre-trained predominantly on\nnatural language text, we start by asking a basic question of whether\nlanguage models can reliable read and understand relational tables,\nwhich are different from text in many ways, as we discuss below.\nOne-dimensional (text) vs. two-dimensional (tables). Language\nmodels trained mostly on natural language text (e.g, books and web\npages) and programming code (e.g., GitHub), both of which that are\none-directional that is meant to be read left-to-right, toke-by-token,\nin a sequential manner.\nIn contrast, relational tables are two-dimensional with rows\nand columns, where reading top-to-bottom vertically, for column-\nheaders and other values in the same column (which may be far\naway when a table is serialized), is crucial for many table-tasks.\nConsider the task of Data-Imputation [7, 37] (T-8 in Table 2),\nwhich is to infer a missing value in a table cell, like shown in the\nexample of Figure 2 (Right). At least for humans, it is natural to\nlook vertically in the horizontal direction, to see the column-header\n(\u201ccontinent\u201d in this case), as well as other values in the same column\n(e.g., \u201cAmericas\u201d), before one can make a guess for the missing value.\nSimilarly, for the task of Error-Detection [14] (T-9 in Table 2) it\nis also necessary to look at the column-header and other values\nin the same column, to understand the semantics of the column,\nbefore one can determine if a cell is erroneous.\nEven for table-tasks that may be a bit removed, such as Table\nQuestion-Answering [42, 49] (T-3 in Table 2), which is traditionally\nan NLP problem \u2013 examples like in Figure 2 (Left) would show that,\nin order to answer a question correctly on a table, reading vertically\nin a column (e.g., for values in the art) is similarly important.\nTo test language models\u2019 ability to read tables in the columnar\ndirection, we design simple tests. In the first test, referred to as\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nFigure 6: Two variants of the task (T-1) Missing-cell-\nidentification. (Left) T-1(a): We remove a random cell from\na real table, but keep its column-separator. The presence of\n\u201c| |\u201d indicates a missing cell, which should be easy to iden-\ntify. (Right) T-1(b): We remove a random cell, as well as its\ncolumn-separator, which is a common but challenging CSV\nparsing issue [18, 52, 54].\n(T-1): Missing cell\nFind col-header tests:\nFind row-id tests:\n(a) no col-sep\n(b) has col-sep\n(a) no col-sep\n(b) has col-sep\nGPT-3.5 (zero-shot)\n0.26\n0.30\n0.76\n0.87\nGPT-3.5 (few-shot)\n0.38\n0.51\n0.77\n0.91\nTable 1: Accuracy numbers of vanilla GPT-3.5 (using\nText-Davinci-002), on the task of (T-1) Missing-value-\nidentification as shown in 6.\n\u201cMissing-value-identification\u201d (T-1 in Table 2), we sample a real\ntable\ud835\udc47 with no missing cells, and remove a random cell from\ud835\udc47. We\nthen produce two variants of the test, like shown in Figure 6:\nT-1(a): we keep the column separator of the missing cell and ask\nlanguage-models to identify the row-id/column-header of the\nmissing cell, like in Figure 6 (Left), which seems simple;\nT-1(b): We remove the column separator of the missing cell and\nthen ask the same question, like in Figure 6 (Right). This is a\ncommon situation in CSV parsing that can be challenging [18, 52,\n54], as one needs to align values vertically to see the missing value\nis in which column. (In the case, humans can see that the countries\n\u201cUSA\u201d and \u201cChina\u201d should align, the GPD numbers should align, so\nthere must be a missing cell in \u201crow-2\u201d, in between \u201cChina\u201d and\n\u201c19,373,586\u201d, for the column \u201cContinent\u201d).\nWe repeat these two tests 1000 times, using 1000 randomly sam-\npled real tables. Table 1 shows the result of this test. We can see that\nit is clearly challenging for language models to read tables in the\ncolumn direction, where the accuracy with and without column-\nseparator is 0.38 and 0.26, respectively. Even with column-separator\nand explicit few-shot demonstrations, the model is only able to get\nhalf of the tests right (0.51).\nIn the row-direction, the model\u2019s ability to identify a missing\ncell is clearly better, though still not great, especially in the \u201cno\ncol-separator\u201d setting.\nTo ensure that the language models are not confused by what\nwe mean in \u201cmissing cell\u201d, we create a second, even simpler test,\ncalled Column-Finding (T-2 in Table 2), shown with an example in\nFigure 1 (Right), where we ask the model to find the column-header\nof a specific value, which appears exactly once in a given table \ud835\udc47,\nfor 1000 randomly sampled real tables. Our result show that the\naccuracy of GPT-3 is similarly low (0.46), confirming the hypothesis\nthat language models ability to read two dimensional tables is likely\ninsufficient.\nOrder-sensitive (text) vs. permutation-invariant (tables). In ad-\ndition, we observe that natural-language texts tend to be order-\nsensitive, where swapping two tokens will generally lead to different\nmeanings (e.g., \u201cJennifer called you\u201d vs. \u201cyou called Jennifer\u201d). In\ncomparison, tables tend to be permutation-invariant, where swap-\nping two rows or two columns, should generally not change the\nsemantic meaning of the resulting table.\nAs a result, when applying language-models to table-tasks, e.g.,\nEntity-matching, Error-Detection, Schema-Matching, we find that\nthe predictions can be rather sensitive to the order in which columns\nare presented in the input tables, even when we only slightly re-\norder the columns.\nWe believe it shows that language models understanding of\ntables is still unstable and sub-optimal (likely influenced by the\noverwhelming text data used in its pre-training), because the de-\ncisions for tasks like Entity-matching and Error-Detection should\nreally not depend on the order of columns.\nOther differences. There are a number of additional aspects that\nmake tables different from text. For example, table-cells tend to be\nshort-form entity-names or phrases, which when serialized in a row,\nwill typically be different from natural-language sentences found in\ntext documents. Furthermore, values in the same column generally\nhave homogeneous values, with pairs of columns encode regular\nrelationships, which is another property not found in texts. All of\nthese make tables different from texts, likely rendering language-\nmodels sub-optimal for table use cases, which motivates our table-\ntuning approach described next.\n4\nTABLE-TUNING FOR TABLE-GPT\nWe propose a new table-tuning paradigm, to enhance language\nmodels ability to understand tables and perform table-tasks,\n4.1\nOverall approach: Synthesis-then-Augment\nLike discussed earlier, our table-tuning is inspired by the success\nof \u201cinstruction-tuning\u201d from the NLP literature [40, 57, 59], illus-\ntrated in Figure 3 (Left), where diverse training data in the form\nof \u201c(instruction, completion)\u201d pairs are used to continue to train\nlanguage-models, and has led to popular models like ChatGPT and\nLLaMa-chat that can understand and follow human instructions.\nOur proposed table-tuning, as illustrated in Figure 3 (Right), is\nsimilar in spirit \u2013 instead of improving language-model ability\nto follow instructions using diverse \u201c(instruction, completion)\u201d\npairs, we aim to improve language-model ability to perform table\ntasks using diverse \u201c(instruction, table, completion)\u201d triples,\nwhere each such triple defines an instance of a table-task:\nDefinition 1. An instance of a table-task, denoted by \ud835\udc61, is defined\nas a triple \ud835\udc61 = (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36), where \ud835\udc3c\ud835\udc5b\ud835\udc60 is the natural-language instruc-\ntion that specifies the table-task, \ud835\udc47 is the input table on which the\ntask is to be performed, and \ud835\udc36 is the expected completion from\nfollowing the instruction \ud835\udc3c\ud835\udc5b\ud835\udc60 and performing the task on table \ud835\udc47.\nExample 1. The examples in Figure 1, Figure 2, and Figure 3,\nshow simple examples of table-tasks, defined by the (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36)\ntriples, which correspond to (instruction, table, completion),\nrespectively. Note that the completion \ud835\udc36 can be natural-language\ntexts (with JSON or other alternatives for answer parsing), tables,\nor a combination of both.\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nTask-name\nTask description (related work)\nTask category\nTable data\nTrain/Test\nT-1: Missing-value identification (MV)\nIdentify the row and column position of the only\nmissing cell in a given table\nTable understanding\nsynthesized\nTest only\nT-2: Column-finding (CF)\nIdentify the column-name of a specific value that\nappears only once in a given table\nTable Understanding\nsynthesized\nTest only\nT-3: Table-QA (TQA)\nAnswer a natural-language question based on the\ncontent of a table ([11, 42, 49])\nTable QA\n[42]\nTest only\nT-4: Column type annotation (CTA)\nFind the semantic type of a column, from a given\nlist of choices ([16, 25, 63])\nTable understanding\n[16, 25]\nTest only\nT-5: Row-to-row transform (R2R)\nTransform table data based on input/output\nexamples ([23, 24, 27])\nData transformation\nsynthesized\n(test: [24])\nTrain/Test\nT-6: Entity matching (EM)\nMatch rows from two tables that refer to the\nsame real-world entity ([32, 38, 41, 66])\nTable matching\n[1]\nTrain/Test\nT-7: Schema matching (SM)\nMatch columns from two tables that refer to the\nsame meaning ([30, 36, 44])\nTable matching\nsynthesized\n(test: [30])\nTrain/Test\nT-8: Data imputation (DI)\nPredict the missing values in a cell based on the\ntable context ([7, 37])\nData cleaning\nsynthesized\nTrain/Test\nT-9: Error detection (ED)\nDetect data values in a table that is a likely error\nfrom misspelling ([14, 45])\nData cleaning\nsynthesized\nTrain/Test\nT-10: List extraction (LE)\nExtract a structured table, from a list that lacks\nexplicit column delimiters [9, 13, 19]\nData transformation\nsynthesized\nTrain only\nT-11: Head value matching (HVM)\nMatch column-headers with its data values\ndrawn from the same table\nTable matching\nsynthesized\nTrain only\nT-12: Natural-language to SQL (NS)\nTranslate a natural-language question on a table\ninto a SQL query ([62, 65])\nNL-to-SQL\n[65]\nTrain only\nT-13: Table summarization (TS)\nProduce a natural-language summary for the\ncontent in a table\nData augmentation\nsynthesized\nTrain only\nT-14: Column augmentation (CA)\nAugment a table with additional columns\ncompatible with a given table\nData augmentation\nsynthesized\nTrain only\nT-15: Row augmentation (RA)\nAugment a table with additional rows compatible\nwith a given table\nData augmentation\nsynthesized\nTrain only\nT-16: Row/column swapping (RCSW)\nManipulate a given table, by swapping the\nposition of two rows or columns\nTable manipulation\nsynthesized\nTrain only\nT-17: Row/column filtering (RCF)\nManipulate a given table, by filtering on given\nrows or columns\nTable manipulation\nsynthesized\nTrain only\nT-18: Row/column sorting (RCS)\nManipulate a given table, by performing sorting\non given rows or columns\nTable manipulation\nsynthesized\nTrain only\nTable 2: A summary of 18 table-related tasks, which we collect and synthesize, in order to \u201ctable-tune\u201d GPT into Table-GPT.\n[Task categories]: These tasks cover diverse areas such as: table understanding, table-QA, table matching, table cleaning, table\ntransformation, etc. Some of these tasks (T-1 to T-4) are used as unseen hold-out tasks, to evaluate Table-GPT ability to\ngeneralize to completely new and unseen tasks. [Table Data]: we choose to \u201csynthesize\u201d table tasks from diverse real tables\nwhen possible (e.g., when ground-truth can be produced automatically), to ensure the diversity of the training data and avoids\nover-fitting. When the ground-truth cannot be automatically produced (e.g., entity-matching, table-QA, NL-to-SQL, etc.), we use\nexisting benchmark data from the literature.\nThe challenge, however, is that prior work on instruction-tuning\nhave shown that the quality of the \u201c(instruction, completion)\u201d\npairs is crucial [40, 50], to the extent that companies hired armies\nof human labelers to manually label such data, (e.g., instruction:\n\u201cwrite a bed-time story with a bear goes to beach\u201d, completion:\nan-actual-story-with-bears) [40], to ensure the quality and diverse\nof the training data.\nWe would like to replicate the success of instruction-tuning in\nthe table domain, but ideally without the expensive human labeling.\nReusing existing benchmark data: insufficient diversity. One ap-\nproach to generate table-tasks, is to use existing benchmark data\npublished in the database literature (similar efforts were made in\nthe NLP literature for instruction-tuning [59]).\nHowever, we found that the existing benchmark data to have:\n(1) limited task-diversity: as the literature tends to focus on a\nfew select table-tasks that are hard and challenging (e.g., entity-\nmatching and data-transformation); and\n(2) limited data-diversity: as benchmark data are typically labeled\nmanually by researchers, only on a few specific datasets, which\nis sufficient for benchmark evaluation purposes, but insufficient\nwhen we want to use them as \u201ctraining data\u201d for language models.\nOur attempt to use only existing benchmark data for table-tuning\nleads to over-fitting, due to the lack of task and data diversity.\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nAlgorithm 1: Synthesize table-tasks for table-tuning\ninput\n:A corpus of diverse real tables C, a set of table-task types S\noutput :Diverse synthesized table-tasks \ud835\udc34 = {(\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36)}\n1 \ud835\udc37 \u2190 {},\ud835\udc34 \u2190 {}\n2 foreach \ud835\udc47 \u2208 C,\ud835\udc46 \u2208 S do\n3\n(\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36) \u2190 Synthesize-Table-Task(\ud835\udc46,\ud835\udc47 ) // (Section 4.2)\n4\n\ud835\udc37 \u2190 \ud835\udc37 \u222a (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36)\n5 foreach (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36) \u2208 \ud835\udc37 do\n6\n\ud835\udc3c\ud835\udc5b\ud835\udc60\u2018 \u2190 Augment-Instruction(\ud835\udc3c\ud835\udc5b\ud835\udc60) // (Section 4.3)\n7\n\ud835\udc47\u2018 \u2190 Augment-Table(\ud835\udc47 ) // (Section 4.3)\n8\n\ud835\udc36\u2018 \u2190 Augment-Completion(\ud835\udc36) // (Section 4.3)\n9\n\ud835\udc34 \u2190 \ud835\udc34 \u222a (\ud835\udc3c\ud835\udc5b\ud835\udc60\u2032,\ud835\udc47 \u2032,\ud835\udc36\u2032)\n10 return \ud835\udc34\nOur approach: Synthesis-then-Augment. We therefore propose\na \u201csynthesize-then-augment\u201d approach to create diverse table-tasks\nusing real tables, which can be used as training-data to table-tune\nlanguage-models.\nWe show the main steps of our synthesize-then-augment ap-\nproach in Algorithm 1. First, we sample a table \ud835\udc47 \u2208 C from a large\ncorpus of real tables C, and a type of table-task \ud835\udc46 \u2208 S. From the\n(\ud835\udc47,\ud835\udc46) pair, we synthesize an instance of a table-task \ud835\udc61 = (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36)\n(line 3), which is the task-synthesis step we will discuss in detail in\nSection 4.2. From the set of diverse instances of table-tasks created\n(\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36), we then proceed to \u201caugment\u201d the tasks, at instruc-\ntion/table/completion levels (line 6-8), which is a step that we will\ndescribe in Section 4.3. The resulting table-tasks\ud835\udc34 = {(\ud835\udc3c\ud835\udc5b\ud835\udc60\u2032,\ud835\udc47 \u2032,\ud835\udc36\u2032)}\nbecome the training data we use to table-tune language-models.\n4.2\nSynthesize diverse table-tasks\nWe now describe how we synthesize diverse instances of table-tasks\n\ud835\udc61 = (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36) (Line 3 of Algorithm 1), so as to exercise language-\nmodels ability to understand two-dimensional table structures.\nWe propose two complementary approaches that (1) synthesize\nnew table-tasks for task-diversity, and (2) synthesize new table\ntest-cases of existing table-tasks for data-diversity. We will discuss\neach below in turn.\nSynthesize new table-tasks for task-diversity. Since our goal\nis to enhance language models\u2019 ability to understand tables, we\nbelieve it is not necessary to focus exclusively on challenging table-\ntasks that have been the focus of the literature [45]. Instead, we pro-\npose a number of table-understanding/augmentation/manipulation\ntasks that are easy to synthesize, leveraging large amounts of real\ntables that already exist. Specifically, we crawled 2.9M high-quality\nweb-tables (e.g., Wikipedia) [10], referred to as C\ud835\udc64\ud835\udc61, and 188K\ndatabase-tables (extracted from BI data models) [33], referred to as\nC\ud835\udc51\ud835\udc4f, and synthesize table-tasks based on real tables sampled from\nthe corpus.\nWe will go over the list of synthesized table-tasks below:\n(T-13) Table summarization (TS). Since web-tables often have\ndescriptive titles, we synthesize a table-summarization task, where\nwe ask the model to summarize the content in a table. Specifically,\nwe sample \ud835\udc47 \u2208 C\ud835\udc64\ud835\udc61 whose extracted table-title \ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc59\ud835\udc52(\ud835\udc47) are neither\ntoo long nor too short, and create a table-summarization task as:\n\ud835\udc47\ud835\udc46(\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc47\ud835\udc46,\ud835\udc47,\ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc59\ud835\udc52(\ud835\udc47))\nwhere \ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc47\ud835\udc46 is the canonical human-instruction to describe the\nTS task (e.g., \u201cPlease provide a succinct summary for the table\nbelow\u201d), which we will further augment for diversity (Section 4.3),\ud835\udc47\nis the input table we sampled from C\ud835\udc64\ud835\udc61, and \ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc59\ud835\udc52(\ud835\udc47) is its expected\ncompletion.\nThis task is designed to use real tables, with real human an-\nnotated titles, to enhance models ability to read tables and un-\nderstand the highlights from the table. Note that although we\nuse \ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc59\ud835\udc52(\ud835\udc47) as the expected completion/answer, it does not over-\nconstrain language-models to over-fit on such answers \u2013 it only\nnudges language-models in that general direction, just like training\ndata in the form of (\u201cwrite a bed-time story with a bear goes\nto beach\u201d \u2192 an-actual-human-written-story) used in instruction-\ntuning does not over-constrain/over-fit the underlying models.\n(T-14) Column augmentation. Since we have lots of real tables\nin C\ud835\udc64\ud835\udc61 and C\ud835\udc51\ud835\udc4f, we create a task where we take the first \ud835\udc58 columns\nin a table \ud835\udc47, denoted as \ud835\udc36[1,\ud835\udc58] (\ud835\udc47), and ask the language-models to\ngenerate the (\ud835\udc58 + 1)-th column \ud835\udc36\ud835\udc58+1(\ud835\udc47), written as follows:\n\ud835\udc36\ud835\udc34(\ud835\udc47,\ud835\udc58) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc36\ud835\udc34,\ud835\udc36[1,\ud835\udc58] (\ud835\udc47),\ud835\udc36\ud835\udc58+1(\ud835\udc47))\nwhere \ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc36\ud835\udc34 is again the natural-language instruction that de-\nscribes the row-augmentation task. This task exercises a model\u2019s\nability to generate realistic columns given a table context that need\nto be semantically compatible.\n(T-15) Row augmentation (RA). Similar to Column-augmentation,\nwe synthesize a Row-augmentation task where we sample a table\n\ud835\udc47 and ask the model to generate the (\ud835\udc58 + 1)-th row, given the first\n\ud835\udc58 rows, written as:\n\ud835\udc45\ud835\udc34(\ud835\udc47,\ud835\udc58) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc45\ud835\udc34, \ud835\udc45[1,\ud835\udc58] (\ud835\udc47), \ud835\udc45\ud835\udc58+1(\ud835\udc47))\nThis task exercises a model\u2019s ability to synthesize realistic rows\ngiven a table context, which need to align vertically with existing\nrows.\n(T-16) Row/column swapping (RS/CS). In this task, we ask the\nmodels to perform a table-manipulation step, where given a sampled\ntable \ud835\udc47, we provide an instruction to swap the \ud835\udc56-th and \ud835\udc57-th row.\nWe programmatically generate the resulting output table from the\nswap operation, denoted as \ud835\udc46\ud835\udc64\ud835\udc4e\ud835\udc5d(\ud835\udc47, \ud835\udc45\ud835\udc56, \ud835\udc45\ud835\udc57), which is the target\n\u201ccompletion\u201d. The Row-swapping task \ud835\udc45\ud835\udc46\ud835\udc56,\ud835\udc57 (\ud835\udc47) is written as:\n\ud835\udc45\ud835\udc46\ud835\udc56,\ud835\udc57 (\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc45\ud835\udc46,\ud835\udc47,\ud835\udc46\ud835\udc64\ud835\udc4e\ud835\udc5d(\ud835\udc47, \ud835\udc45\ud835\udc56, \ud835\udc45\ud835\udc57))\nWe similarly synthesize the Column-swapping task \ud835\udc36\ud835\udc46\ud835\udc56,\ud835\udc57 (\ud835\udc47) as:\n\ud835\udc36\ud835\udc46\ud835\udc56,\ud835\udc57 (\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc36\ud835\udc46,\ud835\udc47,\ud835\udc46\ud835\udc64\ud835\udc4e\ud835\udc5d(\ud835\udc47,\ud835\udc36\ud835\udc56,\ud835\udc36\ud835\udc57))\nWe note that tasks like Row/Column-swapping would seem simple\nto perform, both programmatically or through UI interactions (e.g.,\ninside spreadsheets using menu options), and are therefore not tasks\nstudied in the literature (unlike more challenging tasks like entity-\nmatching or data-transformation). We are similarly not intending\nto use table tasks as \u201ctests\u201d, but because \u201ctables serialized as natural-\nlanguage texts\u201d are ultimately the only way to feed input into\nlanguage models (regardless of whether we want to output to be\ntext/code/table/etc.), these table-tasks are still useful as \u201ctraining\ndata\u201d for models to better read and understand tables.\n(T-17) Row/column filtering. In this table-manipulation task, we\nask models to filter down to specific rows/columns on a sampled\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\ntable \ud835\udc47, based on a specified set of row/column indexes \ud835\udc46:\n\ud835\udc45\ud835\udc39\ud835\udc46 (\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc45\ud835\udc39,\ud835\udc47, \ud835\udc45\ud835\udc46 (\ud835\udc47))\n\ud835\udc36\ud835\udc39\ud835\udc46 (\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc36\ud835\udc39,\ud835\udc47,\ud835\udc36\ud835\udc46 (\ud835\udc47))\nThese tests are again meant to exercise model ability to manipulate\ntables, where cells in both vertical and horizontal directions need\nto be aligned.\n(T-18) Row/column sorting (RS/CS). In the sorting tasks, we ask\nmodels to sort rows in a table \ud835\udc47, based on values in a column \ud835\udc36,\nwhere the expected output table can be programmatically generated,\nwhich we write as \ud835\udc46\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc36 (\ud835\udc47), so that the task \ud835\udc45\ud835\udc46\ud835\udc36 (\ud835\udc47) is:\n\ud835\udc45\ud835\udc46\ud835\udc36 (\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc45\ud835\udc46,\ud835\udc47,\ud835\udc46\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc36 (\ud835\udc47))\nSimilarly, we have a task to sort columns in a table \ud835\udc47, based on\ncolumn-headers \ud835\udc3b, written as \ud835\udc36\ud835\udc46\ud835\udc60(\ud835\udc47):\n\ud835\udc36\ud835\udc46(\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc36\ud835\udc46,\ud835\udc47,\ud835\udc46\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc3b (\ud835\udc47))\nWe note that the sorting tasks are fairly challenging for language-\nmodels \u2013 while we do not expect models to be perfect on such tasks,\nthey exercises model ability to manipulate tables nevertheless.\n(T-11) Head-value matching (HVM). In this task, we sample a\ntable\ud835\udc47, remove all its column headers \ud835\udc3b to produce the correspond-\ning table without headers, \ud835\udc47. We then shuffle these headers \ud835\udc3b, and\nask models to fill \ud835\udc3b into \ud835\udc47 \u2032, to produce the HVM(\ud835\udc47) task:\nHVM(\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc3b\ud835\udc49 \ud835\udc40,\ud835\udc47,\ud835\udc47)\nLike other tasks above, HVM is another task that we can synthesize\nin large quantities, using real tables, and without labeling. It is\nintended to be a task that helps models to better understand and\ncorrelate the semantics of column-headers and values.\nDiscussions. We show in our experiments, that using synthesized\ntable-tasks on diverse tables improves the task- and data-diversity,\nwhich lead to better model generalizability (our ablation study\nshows that without these synthesized tasks there is a substantial\ndrop in model quality).\nOur list of synthesized table-tasks, however, is obviously not\nmeant to be exhaustive, and is only a starting point. We believe\nthat with some creativity, many more tasks can be synthesized to\nfurther improve the table-tuning process. For comparison, the NLP\ncommunity has amassed over 1000 tasks for instruction-tuning, in\na community effort [15], where they show that having more and\ndiverse tasks always helps instruction-tuning.\nSynthesize new table test-cases for data-diversity. There\nare a number of existing and important table-tasks, such as data-\ntransformation, entity-matching, etc. that are extensively studied\nin the database literature. We want to use these established tasks in\ntable-tuning too, also in the \u201c(instruction, table, completion)\u201d\nformat. However, like mentioned earlier, the existing benchmarks\nfor these tasks are typically manually labeled on a few datasets,\nwhich can be used to evaluation, but are unfit as training data for\ntable-tuning, due to their limited quantities and diversity.\nInstead, we synthesize new table test-cases for these established\ntable-tasks, using real tables sampled from C\ud835\udc64\ud835\udc61 and C\ud835\udc51\ud835\udc4f.\n(T-5) Row-to-row Data Transformation (R2R) [23, 24]. To syn-\nthesize diverse test tables with data-transformations, we run a\nproduction-quality program-synthesizer [24], on web-tables sam-\npled from C\ud835\udc64\ud835\udc4f, to identify tables \ud835\udc47 \u2208 C\ud835\udc64\ud835\udc4f where some columns\n\ud835\udc36\ud835\udc56\ud835\udc5b \u2282 \ud835\udc47 can be transformed into \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 \u2282 \ud835\udc47, using an inferred\nprogram \ud835\udc43, such that \ud835\udc43(\ud835\udc36\ud835\udc56\ud835\udc5b) = \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 hold on all rows in \ud835\udc47 (e.g.,\n(first-name, last-name) \u2192 (full-name) in the same table [26]).\nWe then remove one random value \ud835\udc63 \u2208 \ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 from \ud835\udc47, to produce a\ntest table\ud835\udc47\u2212\ud835\udc63 where \ud835\udc63 is missing. We then synthesize a task \ud835\udc452\ud835\udc45(\ud835\udc47):\n\ud835\udc452\ud835\udc45(\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc452\ud835\udc45,\ud835\udc47\u2212\ud835\udc63,\ud835\udc47)\nwhere given \ud835\udc47\u2212\ud835\udc63 as the input, we want to the model to infer the\ntransformation and fill in the missing \ud835\udc63 to produce \ud835\udc47.\n(T-7) Schema Matching (SM) [44]. To synthesize new table test\ncases for schema matching, we sample a real table \ud835\udc47, and take the\nfirst \ud835\udc58 rows of\ud835\udc47 to produce\ud835\udc471 = \ud835\udc45[1, \ud835\udc58] (\ud835\udc47). We then take the next \ud835\udc58\nrows from \ud835\udc47 to produce \ud835\udc472 = \ud835\udc45[\ud835\udc58+1, 2\ud835\udc58] (\ud835\udc47), where we additionally\n\u201cparaphrase\u201d the column-headers of the original\ud835\udc47, into new column-\nheaders in \ud835\udc472, using a mapping of semantically-similar column-\nnames generated by GPT, denoted as \ud835\udc40 (e.g., \u201ccompany names\u201d \u2192\n\u201centerprises\u201d, \u201cemp-id\u201d \u2192 \u201cemployee identifier\u201d, etc.). Finally, we\nshuffle the columns in \ud835\udc471 and \ud835\udc472, and make the two a test case for\nschema matching, where the ground-truth is in \ud835\udc40. The resulting\ntask is written as \ud835\udc46\ud835\udc40(\ud835\udc47):\n\ud835\udc46\ud835\udc40(\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc46\ud835\udc40, (\ud835\udc471,\ud835\udc472), \ud835\udc40)\nThis again can systematically generate large numbers of schema-\nmatching test tables, as training data for table-tuning.\n(T-8) Data Imputation (DI) [7, 37]. For data imputation, we ran-\ndomly sample a real table\ud835\udc47, and then remove a random value \ud835\udc63 \u2208 \ud835\udc47,\nto produce \ud835\udc47\u2212\ud835\udc63. The task \ud835\udc37\ud835\udc3c (\ud835\udc47) is then to predict the missing \ud835\udc63\nfrom its table context:\n\ud835\udc37\ud835\udc3c (\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc37\ud835\udc3c,\ud835\udc47\u2212\ud835\udc63, \ud835\udc63)\nNote that while not all missing values \ud835\udc63 in DI tasks so generated\ncan be reliably predicted, it nevertheless exercises models\u2019 ability\nto leverage correlations that exist between values in the row and\ncolumn contexts.\n(T-9) Error Detection (ED) [45]. To synthesize error-detection\ntasks, we sample a real table \ud835\udc47 \u2208 C\ud835\udc64\ud835\udc61, and generate a modified \ud835\udc47\u02dc ,\nwhere we replace a value \ud835\udc63 \u2208 \ud835\udc47 with \ud835\udc63\u2032, using an existing pack-\nage [3] that injects one likely typographic error into \ud835\udc63. The task\n\ud835\udc38\ud835\udc37(\ud835\udc47) is then:\n\ud835\udc38\ud835\udc37(\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc38\ud835\udc37,\ud835\udc47\u02dc, \ud835\udc63\u2032)\nwhere we aim to identify the misspelled \ud835\udc63\u2032 \u2208 \ud835\udc47\u02dc based on surround-\ning table context.\n(T-10) List extraction (LE) [13, 19]. To synthesize the task of ex-\ntracting tables from list data without explicit column-delimiters,\nwe sample a table \ud835\udc47, and replace all column separators with white\nspaces to generate its unsegmented list-form \ud835\udc3f(\ud835\udc47). The task \ud835\udc3f\ud835\udc38(\ud835\udc47)\nis then:\n\ud835\udc3f\ud835\udc38(\ud835\udc47) = (\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc3f\ud835\udc38, \ud835\udc3f(\ud835\udc47),\ud835\udc47)\nwhich is to produce the correct column-segmentation of \ud835\udc3f(\ud835\udc47), and\ngenerate the corresponding table \ud835\udc47, based on value alignment in\nthe vertical direction.\nSince we have large numbers of diverse tables, in Line 3 of Al-\ngorithm 1 we make sure that each table \ud835\udc47 is used by one task-type\nabove, to synthesize one instance of table-task, to ensure the diver-\nsity of data we generate.\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nFigure 7: Example table-tasks we generate for (T-8) Entity-\nmatching, and (T-9) Error-detection, using \u201caugmented-\ncompletions\u201d that contain reasoning steps similar to chain-\nof-thought, which when used as training-data in table-tuning,\ncan ground model responses and improve result quality.\n4.3\nAugment synthesized table-tasks\nFrom synthesized instances of table-tasks \ud835\udc61 = (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36), we then\nperform additional augmentations at multiple levels, corresponding\nto steps in Line 6-Line 8 of Algorithm 1, where the goal is to create\neven more task/data diversity and avoid over-fitting in table-tuning.\nWe will go over different levels of augmentations below in turn.\nInstruction-level augmentations. At the instruction level,\nbecause using the same instruction repeatedly across training-data\ninstances can lead to over-fitting [57], we augment the canonical\ninstruction using generative models like GPT to paraphrase the\ncanonical human-written instruction into many different variants.\nFor example, for the task-type (T-13): Table-Summarization (Sec-\ntion 4.2), the canonical human-written instruction is: \u201cPlease look\nat the table below and provide a title that can summarize\nthe table\u201d. We generate alternative instructions for the task using\nlanguage-models, to produce variations such as \u201cPlease examine\nthe table below and give it a descriptive title\u201d, in a manner\nsimilar to [57], which we can then use to populate instances of\ntable-tasks as alternative instructions (Line 6).\nTable-level augmentations. At the table-level, we know that\ntwo-dimensional tables should largely be \u201cpermutation-invariant\u201d,\nwhere permutating rows and columns should generally lead to\na table with similar semantic meanings (Section 3), at the table-\nlevel we can perform operations such as column-permutation, row-\npermutation, column-sampling, and row-sampling, to increase the\ndiversity of tables used in our table tasks.\nWhen the training data has an original instance of the table-task,\n\ud835\udc61 = (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36), and its augmented version\ud835\udc61\u2032 = (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47 \u2032,\ud835\udc36), where\ud835\udc47 \u2032\nis an augmented version of\ud835\udc47, which has the same semantic meaning\nand thus the same completion \ud835\udc36, the hope is that by continuing\nto train language-models on such training-data, we can increase\nmodel stability on tables and make them less sensitive to \u201csemantic-\npreserving table-operations\u201d (e.g., column-reordering like discussed\nin Section 3).\nCompletion-level augmentations. At the completion-level,\nfor a synthesized instance of table-task \ud835\udc61 = (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36), we aug-\nment our original completion \ud835\udc36 by generating \ud835\udc36\u2032 that adds rea-\nsoning steps into the original \ud835\udc36, after we observe that performing\nreasoning-steps (similar to [60]) on more complex table-tasks (e.g.,\nentity-matching and error-detection) can lead to better task perfor-\nmance.\nLanguage-model assisted completion-augmentation. We show the\ncompletion-level augmentation using examples in Figure 7. The\nleft of the figure is the task of (T-8) Entity-matching, where models\nneed to identify whether two table-rows may refer to the same\nreal-world entities. We tried two modes of operations on vanilla\nlanguage-models (before table-tuning):\n(1) Direct-answer: where models are asked to produce a \u201cyes/no\u201d\nanswer directly, without additional reasoning;\n(2) Reason-then-answer: where we require the models to \u201cexplain\nstep-by-step\u201d in the prompt, before producing a \u201cyes/no\u201d answer.\nWe observe that for complex tasks like Entity-matching, models in\nthe \u201cdirect-answer\u201d mode are more prone to make mistakes (e.g., an-\nswering with only a \u201cno\u201d in the response), leading to lower accuracy;\nwhereas with \u201creason-then-answer\u201d, even vanilla language-models\nperform better, likely because the step-by-step reasoning chain\nmakes it likely to generate the correct answer, similar to what was\nobserved in the NLP literature (e.g., chain-of-thought reasoning [60]\nand self-consistency [55], etc.).\nRecall that our goal is to table-tune language models, so that\nthe resulting model can perform better on complex table-tasks, by\nhopefully developing a habit of performing step-by-step reasoning.\nIn order to produce training-data for table-tuning, so that language-\nmodels can learn-to-reason on complex tasks, we augment \ud835\udc61 =\n(\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36) by replacing the direct yes/no answer \ud835\udc36, with a more\ndetailed answer using step-by-step reasoning (Figure 7 shows a\nsimplified example), which we generate by providing GPT with the\nyes/no answer, and then ask the model to produce reasoning-steps\nas our new \ud835\udc36\u2032. We find this to encourage table-tuned models to\nperform correct reasoning-steps, on other complex table-tasks that\nare similar in nature.\nGround-truth assisted completion-augmentation. The right of\nFigure 7 shows another example of completion-augmentation, where\nwe use ground-truth to produce step-by-step reasoning, for the ex-\nample task of (T-9): Error-detection. The task here is to identify\nmisspelled cells from a given table. We find that vanilla language-\nmodels to produce lots of false-positives on the error-detection\ntask, where the models confidently predict a cell-value (e.g., an\nabbreviation or uncommon company/person/location names) to be\nmisspelled, when no such misspellings exist upon close inspection.\nThe tendency to produce false-positives persists, with or without\n\u201cstep-by-step\u201d chain-of-thought style prompting.\nBecause we cannot rely on chain-of-thought style prompts to\ngenerate correct reasoning steps as augmented-completions \ud835\udc36\u2032,\nwe instead use the ground-truth available to us in the table-task\nsynthesis step (Section 4.2), to generate augmented completions\nthat embeds the reasoning step. Specifically, like shown in Figure 7\n(right), we augment the completion to contain not only the pre-\ndiction (a value \ud835\udc63 is misspelled), but also generate the explanation\nthat points out the correct version of the predicted misspelling\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n(e.g., \u201cMissisipi\u201d should be \u201cMississippi\u201d). We find this grounds\nthe language-models predictions with an actual explanation, which\nsubstantially reduces false-positives and improve result quality.\nAdditional augmentations. Along the lines of augmentations,\nthere are additional types of augmentations we perform, including\n\u201ctemplate-level augmentation\u201d, where we mix zero-shot task tem-\nplate and few-shot task template (which appends multiple input-\ntable/output-completion examples after the instruction \ud835\udc3c\ud835\udc5b\ud835\udc60), as well\nas \u201ctask-level augmentation\u201d (by synthesizing new types of table-\ntasks), which all improve training-data diversity and help table-\ntuning.\n4.4\nTable-GPT as \u201ctable foundation models\u201d\nUsing the synthesis-then-augment approach in Algorithm 1, de-\nscribe in previous sections, we now generate large numbers of\ndiverse table-tasks \ud835\udc34 = {(\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47,\ud835\udc36)}. We then continue to train lan-\nguage models such as GPT, using serialized (\ud835\udc3c\ud835\udc5b\ud835\udc60,\ud835\udc47) as the \u201cprompt\u201d,\nand\ud835\udc36 as the \u201ccompletion\u201d, where we minimize the language-modeling\nloss of completion given the prompt, subject to regularization. We\nrefer to this process as table-tuning.\nLet \ud835\udc40 be a decoder-style language model, such as GPT and\nChatGPT, let TableTune(\ud835\udc40) be the table-tuned version of \ud835\udc40. We\nargue that TableTune(\ud835\udc40) could serve as a better \u201ctable foundation\nmodel\u201d, if it performs better than \ud835\udc40 on table-tasks, in all of the\nfollowing scenarios:\n(1) Out of the box zero-shot: when we use only instructions for\n\ud835\udc40 or TableTune(\ud835\udc40) to perform table-tasks;\n(2) Out of the box few-shot: when we use instructions and ran-\ndomly selected few-shot examples to perform table-tasks;\n(3) Task-specific prompt-tuning: when we have a small amount of\nlabeled data for a downstream task, and perform prompt-tuning\nto select the best instruction/example combinations;\n(4) Task-specific fine-tuning: when we have sufficient amounts\nof labeled data, and perform task-specific fine-tuning for a task.\nIf table-tuning is effective for language models to learn to better\nunderstand and manipulate tables, we expect that TableTune(\ud835\udc40)\ncan perform better on most if not all of the scenarios described\nabove, which is the goal of our experimental evaluation next.\n5\nEXPERIMENTS\nWe perform extensive experiments to evaluate table-tuned GPT\nrelative to vanilla GPT on diverse table tasks. We plan to release\nour code and data after internal reviews1 .\n5.1\nExperiment Setup\nModels Compared. We test the following models.\n\u2022 GPT-3.5 (text-davinci-002). This 175B model is available from Ope-\nnAI, and is one of the vanilla GPT models that we compare with.\n\u2022 Table-GPT-3.5 (text-davinci-002 +table-tune). This is the model we\nobtain by performing table-tuning on GPT-3.5 (text-davinci-002).\nWe compare the performance of Table-GPT-3.5 with GPT-3.5.\n\u2022 ChatGPT (text-chat-davinci-002). This is a version of the ChatGPT\nmodel available internally [28], which we use as a second vanilla\nbase model, from which we perform table-tuning.\n1: https://aka.ms/table-gpt\n\u2022 Table-ChatGPT (text-chat-davinci-002 +table-tune). This is the model\nwe obtain by performing table-tuning on ChatGPT (text-chat-\ndavinci-002), which we compare with the vanilla ChatGPT.\nTraining tasks and data. In our default settings, we use a total of\n14 types of table-tasks, listed as T-5 to T-18 in Table 2, as training\ndata for table-tuning.\nIn all but two task-types (T-6: Entity Matching and T-12: NL-to-\nSQL), we use synthesized instances of table-tasks. For each task\ntype, we generate 1000 instances of table-tasks using a 50:50 mix\nof zero-shot and few-shot templates, following a synthesis-then-\naugment approach described in Section 4. During task-synthesis,\nwe sample from 125k real web-tables C\ud835\udc64\ud835\udc61 and database-tables C\ud835\udc51\ud835\udc4f\n(aggressively deduped from over 2M original tables). For Entity\nMatching and NL-to-SQL where realistic labels/completions cannot\nbe automatically synthesized, we use existing manually-labeled\nbenchmark data, from [1] and [65], respectively.\nTest tasks and data. To evaluate the benefit of table-tuning, we test\nthe performance of paired models that are table-tuned vs. vanilla\nun-tuned, namely, we compare (GPT-3.5 vs. Table-GPT-3.5) and\n(ChatGPT vs. Table-ChatGPT), as two groups.\nWe test on 4 unseen tasks (T-1 to T-4 in Table 2), which are\ncompletely unseen during table-tuning, to evaluate whether our\ntable-tuned models can continue to follow to human-instructions\nand perform novel unseen tasks (like illustrated in Figure 4). In\naddition, we make sure that the test-data used in unseen tasks, are\ncompletely separate from the tables used in synthesizing table-tasks\nas training-data for table-tuning. Specifically, our training data for\ntable-tuning are always drawn from web-tables C\ud835\udc64\ud835\udc61 and database-\ntables C\ud835\udc51\ud835\udc4f, whereas test-data used in our synthesized table-tasks (T-\n1: Missing-value identification and T2: Column-finding) are always\ndrawn from a corpus of real spreadsheet tables C\ud835\udc60\ud835\udc5d, completely\nseparate from C\ud835\udc64\ud835\udc61 and C\ud835\udc51\ud835\udc4f and with very different characteristics.\nFor the remaining two unseen tests (T-3: Table Question and T-4:\nColumn Type Annotation), we use established benchmark data [42]\nand [16, 25, 51] respectively, which are unseen during table-tuning.\nWe also evaluate 5 seen tasks (T-5 to T-9 in Table 2), which are\nimportant table-tasks extensively studied in the literature, which\nwe want table-tuned models to be exposed of to understand these\ntable-related concepts. While these task-types are seen during table-\ntuning, we make sure that the test datasets are completely separate\nfrom the training data used in table-tuning. For synthesized table-\ntasks (T-8 Data Imputation), similar to discussed above, our test\ncases are always drawn from a corpus of real spreadsheet tables C\ud835\udc60\ud835\udc5d,\nseparate from the corpus of web-tables C\ud835\udc64\ud835\udc61 and database-tables\nC\ud835\udc51\ud835\udc4f used in synthesizing training table-tasks, in order to test table-\ntuned models\u2019 ability to generalize to new tables. For other tasks,\nwe use existing benchmark data, completely unseen when training\ntable-tuned models (e.g., [24] for T-5: Row-to-row transformation,\n[1] for T-6: Entity-matching, using the same setup as [39], [30] for\nT-7: Schema-matching). The task of (T-9) Error-detection is of high\nvalue for our business, where we manually labeled a benchmark\nusing real spreadsheet-tables and web-tables for this evaluation.\nDetails of test data and their statistics can be found in Table 4.\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nTask Type\nTask\nDataset\nZero-Shot\nFew-Shot\nZero-Shot\nFew-Shot\nGPT-3.5\n+table-tune\nGPT-3.5\n+table-tune\nChatGPT\n+table-tune\nChatGPT\n+table-tune\nUnseen\nColumn Finding\nSpreadsheets-CF\n0.461\n0.713\n0.682\n0.816\n0.699\n0.807\n0.803\n0.848\nColumn Type Annotation\nEfthymiou\n0.757\n0.886\n0.784\n0.847\n0.823\n0.882\n0.806\n0.861\nLimaye\n0.683\n0.755\n0.719\n0.853\n0.742\n0.769\n0.832\n0.853\nSherlock\n0.332\n0.449\n0.528\n0.538\n0.454\n0.482\n0.521\n0.553\nT2D\n0.776\n0.875\n0.83\n0.915\n0.827\n0.886\n0.853\n0.912\nMissing Value Identification\nColumn (no separator)\n0.261\n0.294\n0.383\n0.441\n0.299\n0.351\n0.468\n0.474\nColumn (with separator)\n0.305\n0.457\n0.519\n0.643\n0.422\n0.520\n0.635\n0.665\nRow (no separator)\n0.768\n0.851\n0.774\n0.882\n0.822\n0.840\n0.859\n0.894\nRow (with separator)\n0.875\n0.959\n0.917\n0.976\n0.923\n0.936\n0.960\n0.968\nTable Question\nWiki\n0.45\n0.486\n0.454\n0.478\n0.512\n0.521\n0.520\n0.527\nSeen\nData Imputation\nSpreadsheets-DI\n0.423\n0.558\n0.57\n0.625\n0.524\n0.594\n0.609\n0.649\nEntity Matching\nAmazon-Google\n0.153\n0.657\n0.659\n0.676\n0.239\n0.566\n0.680\n0.701\nBeer\n0.5\n0.727\n0.815\n0.923\n0.741\n0.923\n0.783\n0.963\nDBLP-ACM\n0.402\n0.847\n0.954\n0.912\n0.833\n0.932\n0.961\n0.938\nDBLP-GoogleScholar\n0.206\n0.861\n0.809\n0.896\n0.632\n0.912\n0.823\n0.924\nFodors-Zagats\n0.083\n0.872\n0.872\n0.977\n0.809\n1.000\n0.872\n0.977\nWalmart-Amazon\n0.268\n0.691\n0.519\n0.711\n0.206\n0.678\n0.664\n0.824\niTunes-Amazon\n0\n0.788\n0.826\n0.943\n0.393\n0.862\n0.833\n0.929\nError Detection\nSpreadsheets-Real\n0.058\n0.565\n0.319\n0.552\n0.058\n0.544\n0.443\n0.551\nWebTables-Real\n0.077\n0.643\n0.338\n0.545\n0.078\n0.656\n0.364\n0.684\nSchema Matching\nDeepM\n1\n1\n1\n1\n0.857\n1\n1\n1\nRow-to-Row Transformation\nBingQL-Unit\nN.A.\n0.213\n0.427\nN.A.\n0.339\n0.446\nBingQL-other\n0.431\n0.588\n0.558\n0.607\nFF-GR-Trifacta\n0.712\n0.788\n0.772\n0.825\nHeadcase\n0.636\n0.705\n0.704\n0.795\nStackoverflow\n0.662\n0.745\n0.800\n0.758\nTable 3: Detailed results of Table-tuning, on both GPT-3.5 and ChatGPT, for individual datasets. Zero-shot is not applicable to\nrow-to-row by-example transformations (marked as \u201cN.A.\u201d), which requires examples. For all \u201cUnseen\u201d tasks, the tasks are\nheld-out and unseen during table-tuning. For all \u201cSeen\u201d tasks, the task is seen during table-tuning, but the test datasets are\nheld-out and unseen.\n0.230 \n0.777 0.779 0.863 \nEntity Matching\n0.067 \n0.604 \n0.328 \n0.549 \nError Detection\n0.423 \n0.558 0.570 \n0.625 \nData Imputation\n0.603 \n0.702 \nRow-to-Row Transformation\n0.637 \n0.741 0.715 \n0.788 \nColumn Type Annotation\n0.552 \n0.640 0.648 \n0.736 \nMissing Value Identification\n0.461 \n0.713 0.682 \n0.816 \nColumn Finding\n0.450 \n0.486 \n0.454 \n0.478 \nTable Question\n0.423 \n0.230 \n0.057 \n1.000 \n0.450 \n0.558 \n0.777 \n0.628 \n1.000 \n0.486 \n0.570 \n0.779 \n0.357 \n1.000 \n0.454 \n0.625 \n0.863 \n0.637 \n1.000 \n0.478 \nData Imputation\nEntity Matching\nError Detection\nSchema Matching\nTable Question\nGPT-3.5 Zero-Shot\nTable-GPT-3.5 Zero-Shot\nGPT-3.5 Few-Shot\nTable-GPT-3.5 Few-Shot\nFigure 8: Overall quality improvement, between vanilla GPT-3.5 and Table-GPT-3.5.\n0.550 \n0.839 0.802 \n0.893 \nEntity Matching\n0.068 \n0.600 \n0.404 \n0.618 \nError Detection\n0.524 \n0.594 0.609 0.649 \nData Imputation\n0.682 \n0.734 \nRow-to-Row Transformation\n0.712 \n0.755 0.753 \n0.795 \nColumn Type Annotation\n0.617 \n0.662 \n0.731 0.750 \nMissing Value Identification\n0.699 \n0.807 0.803 \n0.848 \nColumn Finding\n0.513 \n0.521 0.520 \n0.527 \nTable Question\n0.524 \n0.550 \n0.052 \n0.857 \n0.513 \n0.594 \n0.839 \n0.636 \n1.000 \n0.521 \n0.609 \n0.802 \n0.436 \n1.000 \n0.520 \n0.649 \n0.893 \n0.693 \n1.000 \n0.527 \nData Imputation\nEntity Matching\nError Detection\nSchema Matching\nTable Question\nChatGPT Zero-Shot\nTable-ChatGPT Zero-Shot\nChatGPT Few-Shot\nTable-ChatGPT Few-Shot\nFigure 9: Overall quality improvement, between vanilla ChatGPT and Table-ChatGPT.\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nTable 4: Details of test data and evaluation metrics\nTask\nEvaluation\nMetrics\nDatasets\nSize\nT-1: Missing Value Identification\nF1\nColumn (no Separator)\n1000\nColumn (with Separator)\n1000\nRow (no Separator)\n1000\nRow (with Separator)\n1000\nT-2: Column Finding\nAcc\nSpreadsheets-CF\n841\nT-3: Table Question\nAcc\nWiki\n4344\nT-4: Column Type Annotation\nF1\nEfthymiou\n594\nLimaye\n174\nSherlock\n971\nT2D\n367\nT-5: Row-to-Row Transformation\nAcc\nBingQL-Unit\n103\nBingQL-other\n1102\nFF-GR-Trifacta\n132\nHeadcase\n88\nStackoverflow\n145\nT-6: Entity Matching\nF1\nAmazon-Google\n2293\nBeer\n91\nDBLP-ACM\n2473\nDBLP-GoogleScholar\n5742\nFodors-Zagats\n189\nWalmart-Amazon\n2049\niTunes-Amazon\n109\nT-7: Schema Matching\nRecall\nDeepM\n41\nT-8: Data Impuation\nAcc\nSpreadsheets-DI\n1000\nT-9: Error Detection\nF1\nSpreadsheets-Real\n870\nWebTables-Real\n432\n5.2\nQuality Comparisons: Unseen + Seen tasks\nIn Figure 8, we compare the performance between (GPT-3.5 vs.\nTable-GPT-3.5), and in Figure 9, we compare the performance be-\ntween (ChatGPT vs. Table-ChatGPT), which are table-tuned vs. un-\ntuned vanilla models, using GPT-3.5 and ChatGPT as base-models,\nrespectively. Within each task-group in the figures, we show 4 bars,\nwhere the first two correspond to zero-shot settings, and the last\ntwo correspond to few-shot settings. We can see that across the\nboard, table-tuned models show strong performance benefits on\ndiverse table-tasks.\nIt is interesting to note that the benefit of table-tuning is observed\nwhen both GPT-3.5 and ChatGPT are used as base-models, showing\nthe generality of our proposed table-tuning approach, on top of\nunderlying language-models of different styles.\nTable 4 shows a detailed breakdown of the results, at the indi-\nvidual data-set level. We can see that across 26 test datasets, on 2\nbase-models (GPT-3.5 and ChatGPT), in 2 settings (zero-shot and\nfew-shot), for a total of 104 tests, table-tuned models outperform\ntheir vanilla un-tuned counterparts in 98/104 tests (with the re-\nmaining being 3 ties and 3 losses), showing the strong performance\nbenefits of table-tuning.\n5.3\nBenefits on task-specific optimizations\nIn addition to performing well out-of-the-box in zero-shot and\n(random) few-shot settings, as shown above, table-tuned GPT mod-\nels could potentially be used as \u201ctable foundation models\u201d, if they\ncontinue to show quality benefits on downstream tasks, when task-\nspecific optimizations are applied.\nLike we discussed in Section 4.4, these include (1) single-task\nprompt-engineering, where we select the best instructions and few-\nshot examples for a single task, using a small number of labeled\nexamples; and (2) single-task fine-tuning, where we continue to\nfine-tune models for a specific task, with a sufficient number of\nlabeled examples. We will study the benefit of table-tuning in these\ntwo settings below.\nSingle-task prompt-engineering: We perform prompt-engineering\nfor Table-GPT-3.5 and GPT-3.5, on the column-type-annotation\n(CTA) task (using the Efthymiou [16] dataset), by selecting the best\nfew-shot examples using 200 labeled examples (randomly sampled\nfrom the ground-truth), where the goodness of a prompt is evalu-\nated on the labeled examples. Figure 10 shows the top-5 prompts\nselected, for Table-GPT-3.5 and GPT-3.5, respectively. We can see\nthat Table-GPT-3.5 consistently outperforms GPT-3.5, on the 5 best\nprompts produced from prompt-engineering.\nSingle-task fine-tuning: We perform task-specific fine-tuning,\non Table-GPT-3.5 and GPT-3.5, using labeled data for that specific\ntask. Table 11(a) and Table 11(b) show the comparison, on the\nCTA task (using Efthymiou [16]) and Table-Question-Answering or\nTQA (using WikiTableQuestions [42]), respectively. In both cases,\nwe vary the amount of training data on the x-axis. As expected,\nthe performance of both Table-GPT-3.5 and GPT-3.5 benefit from\ncontinued task-specific fine-tuning, but with the same amount\nof training data, Table-GPT-3.5 continues to dominate GPT-3.5.\nLooking at the graph from a different way, to achieve the same\nperformance (y-axis), fine-tuning Table-GPT-3.5 would require a\nsmaller number of labeled data than fine-tuning the vanilla GPT-3.5.\n0.822 \n0.785 \n0.816 \n0.801 \n0.799 \n0.877 \n0.867 \n0.851 \n0.870 \n0.860 \n0.75\n0.80\n0.85\n0.90\n1\n2\n3\n4\n5\nAverage Performance\nTop 5 Prompt Template\nGPT-3.5\nTable-GPT-3.5\nFigure 10: Comparison of quality, when using prompt-\nengineering. Results shown are for 5 best prompt-templates\non the Efthymiou dataset.\n0.7\n0.8\n0.9\n1\n0\n100\n200\n400\nAverage Performance\nSize of Training Data\nGPT-3.5\nTable-GPT-3.5\n(a) CTA (Efthymiou)\n0.4\n0.45\n0.5\n0.55\n0.6\n0\n100\n200\n400\n800\nAverage Performance\nSize of Training Data\nGPT-3.5\nTable-GPT-3.5\n(b) TQA (Wiki)\nFigure 11: Single Task Fine-tuning\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 task\n5 tasks\n10 tasks\nAll Tasks\nAverage Performance Score\nNumber of Training Tasks\nCF\nCTA\nDI\nED\nMVI\nR2R\nSM\nTQA\nTable-GPT-3.5\nGPT-3.5\nFigure 12: Vary number of training tasks\n5.4\nSensitivity Analysis\nWe perform sensitivity analysis to better understand table-tuning.\nVarying the number of training tasks. To see whether using\nmore training tasks brings a general benefit, we sample 1/5/10 tasks\nfrom all of our training table-tasks for 4 times each, perform fine-\ntuning on each subset of tasks selected, and compute the average\nfrom these runs.\nThe average quality results are shown in Figure 13. As we can\nsee, on the left of the figure with a small number of tasks (e.g., 1),\ntable-tuning degenerates to single-task tuning, which actually hurts\nthe performance of other tasks in general (notice that the perfor-\nmance corresponding to 1-task is lower than the dotted green line,\nwhich corresponds to GPT-3.5). As we have more training-tasks,\nthe performance goes up consistently, for all tasks as well as for\nthe average across all tasks, showing the benefit that is analogous\nto multi-task training.\nVary the amount of training data. Figure 13 shows the average\nperformance on seen/unseen test tasks with different amounts of\ntraining data. As we can see, the performance on both seen and\nunseen tasks improves with training data, which plateaus as more\ndata is used.\nVary base-model Size. To understand how the size of the base-\nmodels affects the performance of table-tuned models, we table-\ntune four variants of GPT, namely, Text-Ada-001 (350M parameters),\nText-Babbage-001 (3B parameters), Text-Curie-001 (13B parame-\nters), Text-Davinci-002 (175B parameters).\nFigure 14 shows the average performance of base-models vs.\ntable-tuned models, on seen/unseen tasks. We can see that for the\nunseen tasks (important to check for model generalizability), table-\ntuned models produce little benefit on smaller models (Ada/Bab-\nbage/Curie), but the benefit becomes suddenly significant on larger\nmodels (GPT-3.5 and ChatGPT), which appear to be an emerging\nability consistent with what is reported in other contexts (e.g., [8,\n59]).\nVary prompt templates. To test the robustness of our table-tuned\nmodels, we generate 5 different prompt templates (task descriptions\nand special markers), which are paraphrased automatically using\nGPT, from a canonical prompt template written by humans.\nFigure 15 shows the average model performance over all un-\nseen test tasks for each prompt template. As we can see, different\nprompt templates introduce variations in performance, for both\nTable-GPT-3.5 and GPT-3.5, but the former consistently outper-\nforms the latter by more than 10 percentage points on all 5 prompt\ntemplates, showing the robustness of Table-GPT to different kinds\nof prompts.\nVary table formats. There are multiple options when serializing\na table \ud835\udc47 into text, such as Markdown, CSV, JSON, etc. We use\nthe Markdown table format, because it is succinct, and further-\nmore vanilla GPT tends to generate tables in the Markdown format\nin responding to human questions, suggesting that it is the table\nformat of its choice, likely because GPT is pre-trained on lots of\nGitHub code, where Markdown tables are abundant. To understand\nthe effect of using different table formats in representing tables\nin prompts, we test two different table formats, namely CSV and\nJSON.\nTable 5 shows the average performance when using different\ntable formats. As we can see, the Markdown format on average\nperforms better than other formats, although the gap is not too\nsignificant.\nTable 5: Performance of Table-GPT-3.5, when different table\nformats are used to serialize tables\nTask Type\nMarkdown\nCSV\nJSON\nSeen\n0.739\n0.707\n0.713\nUnseen\n0.663\n0.662\n0.621\nOverall\n0.705\n0.687\n0.672\n5.5\nAblation Studies\nWe perform ablation analysis to understand the benefit of different\naugmentation strategies (Section 4.3). The results are summarized\nin Table 6.\nTable 6: Ablation Studies of table-tuning\nTask Type\nGPT-3.5\nTable-GPT-3.5\nNoSyn\nNoColPer.\nNoPromptVar.\nNoCOT\nSeen\n0.548\n0.739\n0.610\n0.735\n0.722\n0.728\nUnseen\n0.547\n0.663\n0.607\n0.661\n0.657\n0.666\nOverall\n0.548\n0.705\n0.608\n0.702\n0.693\n0.701\nNo task-level augmentation (no synthesized tasks). Because\nwe synthesized diverse table-tasks for table-tuning (Section 4.2), our\nfirst ablation is to remove all such tasks from the training data. The\nresult is shown in Table 6 as \u201cNoSyn\". As we can see, the average\nperformance on seen and unseen tasks drops significantly, showing\nthe contribution of our diverse synthesized table-tasks.\nNo table-level augmentation (no column permutations). We\ndisable the table-level augmentation by turning off the column\npermutation. The result is shown in Table 6 as \u201cNoColPer\". We\ncan see that the average performance on seen and unseen tasks is\nlowered without column permutations.\nNo instruction-level augmentation (no prompt variations).\nWe then disable the instruction-level augmentation, by using only\none canonical prompt template for each task (without paraphrasing).\nThe result is shown in Table 6 as \u201cNoPromptVar\". As we can see,\nthe average performance of seen and unseen tasks drops slightly,\nlikely because diverse types of table-tasks we use can somewhat\nmitigate the negative effect of using repeated instruction templates.\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n0.5\n0.6\n0.7\n0.8\n12.5%\n25%\n50%\n100%\nAverage Performance\nSize of Training Data\nGPT-3.5 Seen\nGPT-3.5 Unseen\nTable-GPT-3.5 Seen\nTable-GPT-3.5 Unseen\nFigure 13: Vary Training Size\n0.0\n0.2\n0.4\n0.6\n0.8\nAda (350M)\nBabbage (3B)\nCurie (13B)\nDavinci\n(175B)\nAverage Performance\nModel Name\nGPT-3.5 Seen\nGPT-3.5 Unseen\nTable-GPT-3.5 Seen\nTable-GPT-3.5 Unseen\nFigure 14: Vary Model Size\n0.548 \n0.552 \n0.548 \n0.556 \n0.543 \n0.652 \n0.657 \n0.662 \n0.668 \n0.654 \n0.50\n0.55\n0.60\n0.65\n0.70\n1\n2\n3\n4\n5\nAverage Performance\nPrompt Template\nGPT-3.5\nTable-GPT-3.5\nFigure 15: Vary Templates\nNo completion-level augmentation (no chain-of-thought).\nWe drop the augmentation at completion level by removing the\nchain-of-thought (COT) reasoning from the completion in the fine-\ntuning data. The result is shown in Table 6 as \u201cNoCOT\". The average\nperformance on seen tasks becomes lower with no COT, which is\nexpected.\n6\nCONCLUSIONS AND FUTURE WORK\nIn this work, we propose a new paradigm called table-tuning, that\ncan continue to fine-tune the model weights of pre-trained large\nlanguage-models like GPT-3.5 and ChatGPT, such that the resulting\nmodels are better in understanding tables and performing table\ntasks, while still being versatile in following diverse human instruc-\ntions for unseen tasks. Just like how instruction-tuning has turned\ninto a rich and fruitful line of research in the NLP literature, we\nhope our initial steps in table-tuning can serve as a springboard for\nothers to continue in this path to develop more optimized models\nfor tables and table-related tasks.\nREFERENCES\n[1] [n.d.]. Magellan data repository. https://sites.google.com/site/anhaidgroup/\nuseful-stuff/the-magellan-data-repository?authuser=0.\n[2] [n.d.]. OpenAI: ChatGPT. https://openai.com/blog/chatgpt.\n[3] [n.d.]. Python typo generator. https://pypi.org/project/typo/.\n[4] [n.d.]. Stanford Alpaca. https://github.com/tatsu-lab/stanford_alpaca.\n[5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\net al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).\n[6] Simran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush\nBhatia, Ines Chami, Frederic Sala, and Christopher R\u00e9. 2022. Ask me anything: A\nsimple strategy for prompting language models. arXiv preprint arXiv:2210.02441\n(2022).\n[7] Felix Biessmann, Tammo Rukat, Philipp Schmidt, Prathik Naidu, Sebastian Schel-\nter, Andrey Taptunov, Dustin Lange, and David Salinas. 2019. DataWig: Missing\nValue Imputation for Tables. J. Mach. Learn. Res. 20, 175 (2019), 1\u20136.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[9] Michael J Cafarella, Alon Y Halevy, Yang Zhang, Daisy Zhe Wang, and Eugene\nWu. 2008. Uncovering the Relational Web.. In WebDB. Citeseer, 1\u20136.\n[10] Kaushik Chakrabarti, Surajit Chaudhuri, Zhimin Chen, Kris Ganjam, Yeye He,\nand W Redmond. 2016. Data services leveraging Bing\u2019s data assets. IEEE Data\nEng. Bull. 39, 3 (2016), 15\u201328.\n[11] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang\nLi, Xiyou Zhou, and William Yang Wang. 2019. Tabfact: A large-scale dataset for\ntable-based fact verification. arXiv preprint arXiv:1909.02164 (2019).\n[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311 (2022).\n[13] Xu Chu, Yeye He, Kaushik Chakrabarti, and Kris Ganjam. 2015. Tegra: Table\nextraction by global record alignment. In Proceedings of the 2015 ACM SIGMOD\ninternational conference on management of data. 1713\u20131728.\n[14] Xu Chu, Ihab F Ilyas, Sanjay Krishnan, and Jiannan Wang. 2016. Data clean-\ning: Overview and emerging challenges. In Proceedings of the 2016 international\nconference on management of data. 2201\u20132206.\n[15] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\ninstruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n[16] Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2022. Turl: Table\nunderstanding through representation learning. ACM SIGMOD Record 51, 1\n(2022), 33\u201340.\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[18] Till D\u00f6hmen, Hannes M\u00fchleisen, and Peter Boncz. 2017. Multi-hypothesis CSV\nparsing. In Proceedings of the 29th International Conference on Scientific and\nStatistical Database Management. 1\u201312.\n[19] Hazem Elmeleegy, Jayant Madhavan, and Alon Halevy. 2009. Harvesting rela-\ntional tables from lists on the web. Proceedings of the VLDB Endowment 2, 1\n(2009), 1078\u20131089.\n[20] Raul Castro Fernandez, Aaron J Elmore, Michael J Franklin, Sanjay Krishnan, and\nChenhao Tan. 2023. How Large Language Models Will Disrupt Data Management.\nProceedings of the VLDB Endowment 16, 11 (2023), 3302\u20133309.\n[21] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language\nmodels better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).\n[22] Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,\nDoug Downey, and Noah A Smith. 2020. Don\u2019t stop pretraining: Adapt language\nmodels to domains and tasks. arXiv preprint arXiv:2004.10964 (2020).\n[23] William R Harris and Sumit Gulwani. 2011. Spreadsheet table transformations\nfrom examples. ACM SIGPLAN Notices 46, 6 (2011), 317\u2013328.\n[24] Yeye He, Xu Chu, Kris Ganjam, Yudian Zheng, Vivek Narasayya, and Surajit\nChaudhuri. 2018. Transform-data-by-example (TDE) an extensible search engine\nfor data transformations. Proceedings of the VLDB Endowment 11, 10 (2018),\n1165\u20131177.\n[25] Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind Satya-\nnarayan, Tim Kraska, \u00c7agatay Demiralp, and C\u00e9sar Hidalgo. 2019. Sherlock: A\ndeep learning approach to semantic data type detection. In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining.\n1500\u20131508.\n[26] Zhongjun Jin, Yeye He, and Surajit Chauduri. 2020. Auto-transform: learning-\nto-transform by patterns. Proceedings of the VLDB Endowment 13, 12 (2020),\n2368\u20132381.\n[27] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wran-\ngler: Interactive visual specification of data transformation scripts. In Proceedings\nof the sigchi conference on human factors in computing systems. 3363\u20133372.\n[28] Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-\nthe-art evaluators of translation quality. arXiv preprint arXiv:2302.14520 (2023).\n[29] Keti Korini and Christian Bizer. 2023. Column Type Annotation using ChatGPT.\narXiv preprint arXiv:2306.00745 (2023).\n[30] Christos Koutras, George Siachamis, Andra Ionescu, Kyriakos Psarakis, Jerry\nBrons, Marios Fragkoulis, Christoph Lofi, Angela Bonifati, and Asterios Katsi-\nfodimos. 2021. Valentine: Evaluating matching techniques for dataset discovery.\nIn 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEE,\n468\u2013479.\n[31] Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy,\nJason Weston, and Mike Lewis. 2023. Self-Alignment with Instruction Backtrans-\nlation. arXiv preprint arXiv:2308.06259 (2023).\n[32] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan.\n2020. Deep entity matching with pre-trained language models. arXiv preprint\narXiv:2004.00584 (2020).\n[33] Yiming Lin, Yeye He, and Surajit Chaudhuri. 2023. Auto-BI: Automatically Build\nBI-Models Leveraging Local Join Prediction and Global Schema Graph. arXiv\npreprint arXiv:2306.12515 (2023).\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n[34] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\n1\u201335.\n[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[36] Jayant Madhavan, Philip A Bernstein, and Erhard Rahm. 2001. Generic schema\nmatching with cupid. In vldb, Vol. 1. 49\u201358.\n[37] Chris Mayfield, Jennifer Neville, and Sunil Prabhakar. 2010. ERACER: a database\napproach for statistical inference and data cleaning. In Proceedings of the 2010\nACM SIGMOD International Conference on Management of data. 75\u201386.\n[38] Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan, Youngchoon Park,\nGanesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra. 2018.\nDeep learning for entity matching: A design space exploration. In Proceedings of\nthe 2018 International Conference on Management of Data. 19\u201334.\n[39] Avanika Narayan, Ines Chami, Laurel Orr, Simran Arora, and Christopher R\u00e9.\n2022. Can foundation models wrangle your data? arXiv preprint arXiv:2205.09911\n(2022).\n[40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35 (2022), 27730\u201327744.\n[41] George Papadakis, Ekaterini Ioannou, Emanouil Thanos, and Themis Palpanas.\n2021. The four generations of entity resolution. Springer.\n[42] Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on\nsemi-structured tables. arXiv preprint arXiv:1508.00305 (2015).\n[43] Ralph Peeters and Christian Bizer. 2023. Using ChatGPT for Entity Matching.\narXiv preprint arXiv:2305.03423 (2023).\n[44] Erhard Rahm and Philip A Bernstein. 2001. A survey of approaches to automatic\nschema matching. the VLDB Journal 10 (2001), 334\u2013350.\n[45] Erhard Rahm, Hong Hai Do, et al. 2000. Data cleaning: Problems and current\napproaches. IEEE Data Eng. Bull. 23, 4 (2000), 3\u201313.\n[46] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A primer in BERTology:\nWhat we know about how BERT works. Transactions of the Association for\nComputational Linguistics 8 (2021), 842\u2013866.\n[47] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika,\nZaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al.\n2021. Multitask prompted training enables zero-shot task generalization. arXiv\npreprint arXiv:2110.08207 (2021).\n[48] Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, \u00c7a\u011fatay Demiralp, Chen\nChen, and Wang-Chiew Tan. 2022. Annotating columns with pre-trained lan-\nguage models. In Proceedings of the 2022 International Conference on Management\nof Data. 1493\u20131503.\n[49] Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su, and Xifeng Yan. 2016.\nTable cell search for question answering. In Proceedings of the 25th International\nConference on World Wide Web. 771\u2013782.\n[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[51] Jianhong Tu, Ju Fan, Nan Tang, Peng Wang, Guoliang Li, Xiaoyong Du, Xiaofeng\nJia, and Song Gao. 2023. Unicorn: A unified multi-tasking model for supporting\nmatching tasks in data integration. Proceedings of the ACM on Management of\nData 1, 1 (2023), 1\u201326.\n[52] Gerrit JJ van den Burg, Alfredo Naz\u00e1bal, and Charles Sutton. 2019. Wrangling\nmessy CSV files by detecting row and type patterns. Data Mining and Knowledge\nDiscovery 33, 6 (2019), 1799\u20131820.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[54] Gerardo Vitagliano, Mazhar Hameed, Lan Jiang, Lucas Reisener, Eugene Wu, and\nFelix Naumann. 2023. Pollock: A Data Loading Benchmark. Proceedings of the\nVLDB Endowment 16, 8 (2023), 1870\u20131882.\n[55] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).\n[56] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khy-\nathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy,\net al. 2023. How Far Can Camels Go? Exploring the State of Instruction Tuning\non Open Resources. arXiv preprint arXiv:2306.04751 (2023).\n[57] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model\nwith self generated instructions. arXiv preprint arXiv:2212.10560 (2022).\n[58] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amir-\nreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran,\nAtharva Naik, David Stap, et al. 2022. Super-naturalinstructions: Generalization\nvia declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705\n(2022).\n[59] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models\nare zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n[60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824\u201324837.\n[61] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt\npattern catalog to enhance prompt engineering with chatgpt. arXiv preprint\narXiv:2302.11382 (2023).\n[62] Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet: Generating structured\nqueries from natural language without reinforcement learning. arXiv preprint\narXiv:1711.04436 (2017).\n[63] Cong Yan and Yeye He. 2018. Synthesizing type-detection logic for rich seman-\ntic data types using open-source code. In Proceedings of the 2018 International\nConference on Management of Data. 35\u201350.\n[64] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020.\nTaBERT: Pretraining for joint understanding of textual and tabular data. arXiv\npreprint arXiv:2005.08314 (2020).\n[65] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li,\nJames Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and\nCross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing. Association for\nComputational Linguistics, Brussels, Belgium.\n[66] Chen Zhao and Yeye He. 2019. Auto-em: End-to-end fuzzy entity-matching using\npre-trained deep models and transfer learning. In The World Wide Web Conference.\n2413\u20132424.\n[67] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe\nMa, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206 (2023).\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nA\nTASK DETAILS\nMissing Value Identification (Column No Sep) Zero-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using its column name. Return the final result\nas JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|A|5|3|\n|I|3|2|\n|U|2|3|\n|2|1|\n|I|2|2|\nReturn the final result as JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Output:\nCompletion:\n{\"missing_col\": \"Project\"}\nMissing Value Identification (Column No Sep) Few-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using its column name. Return the final result\nas JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|A|4|1|\n|I|2|1|\n|O|3|3|\n|A|1|\n|E|4|2|\n## Output:\n{\"missing_col\": \"Req\"}\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|I|2|1|\n|E|1|3|\n|A|1|3|\n|1|2|\n|E|4|2|\n## Output:\n{\"missing_col\": \"Project\"}\n## Input:\n|Project|Team|Req|\n|---|---|---|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|E|4|2|\n|O|2|\n|U|5|2|\n|I|4|2|\n|A|4|2|\n## Output:\n{\"missing_col\": \"Team\"}\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|A|5|3|\n|I|3|2|\n|U|2|3|\n|2|1|\n|I|2|2|\nReturn the final result as JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Output:\nCompletion:\n{\"missing_col\": \"Project\"}\nMissing Value Identification (Row Sep) Zero-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using the row id shown in the first column.\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|A|5|3|\n|2|I|3|2|\n|3|U|2|3|\n|4||2|1|\n|5|I|2|2|\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Output:\nCompletion:\n{\"row_id\": \"4\"}\nMissing Value Identification (Row Sep) Few-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using the row id shown in the first column.\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|A|4|1|\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n|2|I|2|1|\n|3|O|3|3|\n|4|A|1||\n|5|E|4|2|\n## Output:\n{\"row_id\": \"4\"}\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|I|2|1|\n|2|E|1|3|\n|3|A|1|3|\n|4||1|2|\n|5|E|4|2|\n## Output:\n{\"row_id\": \"4\"}\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|E|4|2|\n|2|O||2|\n|3|U|5|2|\n|4|I|4|2|\n|5|A|4|2|\n## Output:\n{\"row_id\": \"2\"}\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|A|5|3|\n|2|I|3|2|\n|3|U|2|3|\n|4||2|1|\n|5|I|2|2|\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Output:\nCompletion:\n{\"row_id\": \"4\"}\nMissing Value Identification (Row No Sep) Zero-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using the row id shown in the first column.\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|1|A|5|3|\n|2|I|3|2|\n|3|U|2|3|\n|4|2|1|\n|5|I|2|2|\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Output:\nCompletion:\n{\"row_id\": \"4\"}\nMissing Value Identification (Row No Sep) Few-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using the row id shown in the first column.\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|A|4|1|\n|2|I|2|1|\n|3|O|3|3|\n|4|A|1|\n|5|E|4|2|\n## Output:\n{\"row_id\": \"4\"}\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|I|2|1|\n|2|E|1|3|\n|3|A|1|3|\n|4|1|2|\n|5|E|4|2|\n## Output:\n{\"row_id\": \"4\"}\n## Input:\n|row_id|Project|Team|Req|\n|---|---|---|---|\n|1|E|4|2|\n|2|O|2|\n|3|U|5|2|\n|4|I|4|2|\n|5|A|4|2|\n## Output:\n{\"row_id\": \"2\"}\n## Input:\n|row_id|Project|Team|Req|\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n|---|---|---|---|\n|1|A|5|3|\n|2|I|3|2|\n|3|U|2|3|\n|4|2|1|\n|5|I|2|2|\nReturn the final result as JSON in the format {\"row_id\": \"<row_id of the row with missing cell>\"}.\n## Output:\nCompletion:\n{\"row_id\": \"4\"}\nMissing Value Identification (Column Sep) Zero-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using its column name. Return the final result\nas JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|A|5|3|\n|I|3|2|\n|U|2|3|\n||2|1|\n|I|2|2|\nReturn the final result as JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Output:\nCompletion:\n{\"missing_col\": \"Project\"}\nMissing Value Identification (Column Sep) Few-Shot\nPrompt:\n# Task Description: Please check the following table, there is one and exactly one cell in the table that is\nmissing. When you find this missing cell, please point it out using its column name. Return the final result\nas JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|A|4|1|\n|I|2|1|\n|O|3|3|\n|A|1||\n|E|4|2|\n## Output:\n{\"missing_col\": \"Req\"}\n## Input:\n|Project|Team|Req|\n|---|---|---|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|I|2|1|\n|E|1|3|\n|A|1|3|\n||1|2|\n|E|4|2|\n## Output:\n{\"missing_col\": \"Project\"}\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|E|4|2|\n|O||2|\n|U|5|2|\n|I|4|2|\n|A|4|2|\n## Output:\n{\"missing_col\": \"Team\"}\n## Input:\n|Project|Team|Req|\n|---|---|---|\n|A|5|3|\n|I|3|2|\n|U|2|3|\n||2|1|\n|I|2|2|\nReturn the final result as JSON in the format {\"missing_col\": \"<missing column name>\"}.\n## Output:\nCompletion:\n{\"missing_col\": \"Project\"}\nColumn Finding Zero-Shot\nPrompt:\n# Task Description: Please look at the table below and find the column that contains the given cell value.\nReturn the final result as JSON in the format {\"result\": \"<name of the column containing the given cell\nvalue>\"}.\n## Input:\n**Input Table:**\n|1|12|13|14|15|16|17|18|\n|---|---|---|---|---|---|---|---|\n|2|2|2|2|2|2|2|2|\n|3|3|3|3|3|3|3|3|\n|4|4|4|4|4|4|4|4|\n|5|5|5|5|5|5|5|5|\n|6|6|6|6|6|6|6|6|\n|7|7|7|7|7|7|7|7|\n|8|8|8|8|8|8|8|8|\n|9|9|9|9|9|9|9|9|\n|10|10|10|10|10|10|10|10|\n|11|11|11|11|11|11|11|11|\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n**Given Cell Value:**\n2\nReturn the final result as JSON in the format {\"result\": \"<name of the column containing the given cell value>\"}.\n## Output:\nCompletion:\n{\"result\": \"15\"}\nColumn Finding Few-Shot\nPrompt:\n# Task Description: Please look at the table below and find the column that contains the given cell value.\nReturn the final result as JSON in the format {\"result\": \"<name of the column containing the given cell\nvalue>\"}.\n## Input:\n**Input Table:**\n|price|crime|nox|rooms|dist|radial|proptax|stratio|\n|---|---|---|---|---|---|---|---|\n|17794|898299980163574|769999980926514|621000003814697|211999988555908|24|665999984741211|202000007629395|\n|21700|384999990463257|769999980926514|63899998664856|250999999046326|24|665999984741211|202000007629395|\n|22700|52020001411438|769999980926514|613000011444092|272000002861023|24|665999984741211|202000007629395|\n|22600|426100015640259|769999980926514|61100001335144|250999999046326|24|665999984741211|202000007629395|\n|24999|454199981689453|769999980926514|640000009536743|251999998092651|24|665999984741211|202000007629395|\n|19900|383699989318848|769999980926514|625|229999995231628|24|665999984741211|202000007629395|\n|20800|367799997329712|769999980926514|53600001335144|209999990463257|24|665999984741211|202000007629395|\n|16800|422200012207031|769999980926514|580000019073486|189999997615814|24|665999984741211|202000007629395|\n|21900|347399997711182|717999982833862|877999973297119|189999997615814|24|665999984741211|202000007629395|\n|27499|455600023269653|717999982833862|355999994277954|161000001430511|24|665999984741211|202000007629395|\n**Given Cell Value:**\n426100015640259\n## Output:\n{\"result\": \"crime\"}\n## Input:\n**Input Table:**\n|Player|Class|Team|GP|G|A|Pts|PIM|\n|---|---|---|---|---|---|---|---|\n|Nathan Gerbe|Junior|Boston College|43|35|33|68|65|\n|Kevin Porter|Senior|Michigan|43|33|30|63|18|\n|Chad Kolarik|Senior|Michigan|39|30|26|56|24|\n|Ryan Lasch|Sophomore|St. Cloud State|40|25|28|53|12|\n|Simon Lambert|Senior|RIT|37|21|30|51|40|\n|Joe Whitney|Freshman|Boston College|44|11|40|51|50|\n|Ben Smith|Sophomore|Boston College|44|25|25|50|12|\n|Ryan Jones|Senior|Miami (OH)|42|31|18|49|83|\n|Ryan Cruthers|Senior|Robert Morris|34|22|27|49|40|\n|Matt Fornataro|Senior|New Hampshire|38|18|28|46|52|\n**Given Cell Value:**\nMiami (OH)\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n## Output:\n{\"result\": \"Team\"}\n## Input:\n**Input Table:**\n|Nation|1977|1995|1997|1999|2001|2003|2005|2008|2010|2012|2014|1979|2016|Years|1981|1983|1985|1987|1989|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Algeria|nan|nan|nan|nan|nan|19th|nan|16th|nan|nan|nan|nan|nan|4|nan|nan|nan|nan|12th|\n|Angola|nan|13th|17th|17th|15th|nan|15th|11th|14th|19th|21st|nan|14th|10|nan|nan|nan|nan|nan|\n|Argentina|nan|17th|nan|nan|16th|nan|20th|12th|15th|20th|20th|nan|16th|8|nan|nan|nan|nan|nan|\n|Australia|nan|nan|nan|nan|nan|nan|nan|20th|22nd|nan|nan|nan|nan|2|nan|nan|nan|nan|nan|\n|Austria|14th|nan|nan|nan|nan|nan|nan|nan|nan|11th|nan|11th|19th|8|nan|nan|14th|nan|14th|\n|Belarus|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|1|nan|nan|nan|nan|nan|\n|Brazil|nan|13th|12th|12th|13th|15th|9th|9th|12th|12th|15th|nan|11th|13|nan|nan|nan|nan|nan|\n|Bulgaria|nan|9th|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|5|nan|5th|nan|nan|3rd|\n|Canada|nan|nan|nan|20th|nan|nan|nan|nan|nan|nan|nan|nan|nan|3|8th|nan|nan|nan|nan|\n|Chile|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|nan|22nd|1|nan|nan|nan|nan|nan|\n**Given Cell Value:**\n13\n## Output:\n{\"result\": \"Years\"}\n## Input:\n**Input Table:**\n|1|12|13|14|15|16|17|18|\n|---|---|---|---|---|---|---|---|\n|2|2|2|2|2|2|2|2|\n|3|3|3|3|3|3|3|3|\n|4|4|4|4|4|4|4|4|\n|5|5|5|5|5|5|5|5|\n|6|6|6|6|6|6|6|6|\n|7|7|7|7|7|7|7|7|\n|8|8|8|8|8|8|8|8|\n|9|9|9|9|9|9|9|9|\n|10|10|10|10|10|10|10|10|\n|11|11|11|11|11|11|11|11|\n**Given Cell Value:**\n2\nReturn the final result as JSON in the format {\"result\": \"<name of the column containing the given cell value>\"}.\n## Output:\nCompletion:\n{\"result\": \"15\"}\nTable-QA Zero-Shot\nPrompt:\n# Task Description: Please look at the table, and then answer the question. Please also provide an explanation\non your answer. Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n## Input:\n*Table*\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n|Rank|Nation|Gold|Silver|Bronze|Total|\n|---|---|---|---|---|---|\n|1|Netherlands|8|3|1|12|\n|2|Australia|3|3|4|10|\n|3|United States|2|5|1|8|\n|4|Hungary|1|1|3|5|\n|5|Canada|1|-|3|4|\n|6|Italy|-|2|1|3|\n|7|Russia|-|1|1|2|\n|8|China|-|-|1|1|\n*Question:*\nhow many nations are there?\nReturn the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n## Output:\nCompletion:\n{\"answer\": \"8\"}\nTable-QA Few-Shot\nPrompt:\n# Task Description: Please look at the table, and then answer the question. Please also provide an explanation\non your answer. Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n## Input:\n*Table*\n|Rank|Heat|Nationality|Time|Notes|\n|---|---|---|---|---|\n|1|1|Russia|23.11|Q|\n|2|1|Belgium|23.23|Q|\n|3|2|Russia|23.37|Q|\n|4|1|Poland|23.39|Q, SB|\n|5|1|Ukraine|23.4|Q, SB|\n|6|2|United States Virgin Islands|23.49|Q|\n|7|1|Belarus|23.52|nan|\n|8|1|Canada|23.62|nan|\n|9|2|Poland|23.69|Q|\n|10|2|Hungary|23.87|Q|\n|11|2|Ireland|23.89|nan|\n|12|1|Mexico|23.96|nan|\n|13|2|Lithuania|24.09|nan|\n|14|2|Brazil|24.18|nan|\n|15|2|Great Britain|24.31|nan|\n|16|1|Italy|24.4|nan|\n*Question:*\nwho is the only cyclist from brazil?\n## Output:\n{\"answer\": \"Raquel da Costa\"}\n## Input:\n*Table*\n|Eps #|Prod #|Title|Air Date|\n|---|---|---|---|\n|1|2|Menace Of The Mole Men|9/9/1967|\n|2|3|Diablo|9/16/1967|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|3|7|The Way It All Began|9/23/1967|\n|4|5|Invasion Of The Super-Skrull|9/30/1967|\n|5a|1|Klaws|10/7/1967|\n|5b|4|The Red Ghost|10/7/1967|\n|6|9|Prisoners Of Planet X|10/14/1967|\n|7|14|It Started On Yancy Street|10/21/1967|\n|8|6|Three Predictions Of Dr. Doom|10/28/1967|\n|9|8|Behold A Distant Star|11/4/1967|\n|10|12|Demon in the Deep|11/11/1967|\n|11|11|Danger In The Depths|11/18/1967|\n|12|13|Return Of the Mole Man|11/25/1967|\n|13|19|Rama-Tut|12/9/1967|\n|14|15|Galactus|12/16/1967|\n|15|16|The Micro World Of Dr. Doom|12/30/1967|\n|16|17|Blastaar, The Living Bomb-Burst|1/6/1968|\n|17|10|The Mysterious Molecule Man|1/13/1968|\n|18|18|The Terrible Tribunal|9/14/1968|\n|19|20|The Deadly Director|9/21/1968|\n*Question:*\nhow long did the show air?\nReturn the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n## Output:\nCompletion:\n{\"answer\": \"1 year\"}\nColumn Type Annotation Zero-Shot\nPrompt:\n# Task Description: Please look at the input column and determine the semantic type that can describe *every\nsingle* instance the input column. Please only choose one semantic type from the candidate list, and\nremember that the type you choose has to accurately describe every single entity in the column. If no\ncandidate column type can suitably describe every single instance in the column, please return 'None'.\nPlease only choose one type from the candidate list below, and *do not* create new types. Return the final\nresult as JSON in the format {\"chosen_semantic_type\": \"<an entry from the candidate list or None>\"}.\n## Input:\n**Column:**\n|Party|\n|---|\n|Liberal|\n|Conservative|\n**Candidate column type:**\nAcademicJournal\nBaseballPlayer\nBook\nCity\nCompany\nContinent\nFilm\nMayor\nMonarch\nMountain\nNewspaper\nPoliticalParty\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nScientist\nSportsTeam\nReturn the final result as JSON in the format {\"chosen_semantic_type\": \"<an entry from the candidate list or\nNone>\"}.\n## Output:\nCompletion:\n{\"chosen_semantic_type\": \"PoliticalParty\"}\nColumn Type Annotation Few-Shot\nPrompt:\n# Task Description: Please look at the input column and determine the semantic type that can describe *every\nsingle* instance the input column. Please only choose one semantic type from the candidate list, and\nremember that the type you choose has to accurately describe every single entity in the column. If no\ncandidate column type can suitably describe every single instance in the column, please return 'None'.\nPlease only choose one type from the candidate list below, and *do not* create new types. Return the final\nresult as JSON in the format {\"chosen_semantic_type\": \"<an entry from the candidate list or None>\"}.\n## Input:\n**Column:**\n|Name|\n|---|\n|Wells Fargo Tower|\n|Regions-Harbert Plaza|\n|AT&T City Center|\n|Regions Center|\n|City Federal Building|\n|Alabama Power Headquarters Building|\n|Thomas Jefferson Tower|\n|John Hand Building|\n|Daniel Building|\n**Candidate column type:**\nAcademicJournal\nAirport\nBook\nBuilding\nCity\nFilm\nMammal\nNewspaper\nPlant\nPoliticalParty\nScientist\nSportsTeam\n## Output:\n{\"chosen_semantic_type\": \"Building\"}\n## Input:\n**Column:**\n|Team|\n|---|\n|Minnesota 1|\n|New York|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n**Candidate column type:**\nAdministrativeRegion\nContinent\nMayor\nSaint\nUniversity\nWrestler\nWriter\n## Output:\n{\"chosen_semantic_type\": \"AdministrativeRegion\"}\n## Input:\n**Column:**\n|Engine|\n|---|\n|Honda|\n|Honda|\n|Honda|\n|Chevrolet|\n|Chevrolet|\n**Candidate column type:**\nAcademicJournal\nCompany\nCurrency\nFilm\nLake\nSaint\nWriter\n## Output:\n{\"chosen_semantic_type\": \"Company\"}\n## Input:\n**Column:**\n|Name|\n|---|\n|Juliomys|\n|Pipanacoctomys|\n|Salinoctomys|\n|Tapecomys|\n|Hyladelphys|\n|Handleyomys|\n|Sommeromys|\n|Chacodelphys|\n|Drymoreomys|\n**Candidate column type:**\nBuilding\nCity\nCompany\nContinent\nCountry\nCurrency\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n## Output:\n{\"chosen_semantic_type\": \"None\"}\n## Input:\n**Column:**\n|Service|\n|---|\n|Star FM|\n|Radio Teddy|\n|Berliner Rundfunk 91|\n|4|\n|Jam FM|\n|94|\n|3 rs2|\n|Radio Eins|\n|Deutschlandfunk|\n|98.8 KISS FM Berlin|\n|Antenne Brandenburg|\n|FluxFM|\n|Klassik Radio|\n|Radyo Metropol FM|\n|Fritz|\n|Energy Berlin|\n|104.6 RTL|\n|105'5 Spreeradio|\n|Radio B2|\n**Candidate column type:**\nBaseballPlayer\nBuilding\nLake\nMammal\nMayor\nPoliticalParty\nSportsTeam\n## Output:\n{\"chosen_semantic_type\": \"None\"}\n## Input:\n**Column:**\n|Common name|\n|---|\n|White-collared swift|\n**Candidate column type:**\nAirport\nBook\nLanguage\nMonarch\nPlant\nPoliticalParty\nUniversity\n## Output:\n{\"chosen_semantic_type\": \"None\"}\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n## Input:\n**Column:**\n|Party|\n|---|\n|Liberal|\n|Conservative|\n**Candidate column type:**\nAcademicJournal\nBaseballPlayer\nBook\nCity\nCompany\nContinent\nFilm\nMayor\nMonarch\nMountain\nNewspaper\nPoliticalParty\nScientist\nSportsTeam\nReturn the final result as JSON in the format {\"chosen_semantic_type\": \"<an entry from the candidate list or\nNone>\"}.\n## Output:\nCompletion:\n{\"chosen_semantic_type\": \"PoliticalParty\"}\nError Detection Zero-Shot\nPrompt:\nDescription: Please examine the input table and let me know which cell or cells are erroneous. If there are\nmultiple erroneous cells, return them all in a list. If there is no erroneous cell, return 'None'. Please\nonly return the erroneous cell or cells with high confidence. Return the final result as JSON in the format\n{\"erroneous_cells\": \"<a list of erroneous cells or None>\"}. Let's think step by step and show your reasoning\nbefore showing the final result.\nQuestion:\n|Province|Teams|Rank|\n|---|---|---|\n|Bengkulu|1|9|\n|Lampung|1|9|\n|Banetn|1|9|\n|Central MJava|1|9|\n|Yogyakarta|1|9|\nLet's think step by step and show your reasoning before showing the final result. Return the final result as\nJSON in the format {\"erroneous_cells\": \"<a list of erroneous cells or None>\"}.\nAnswer:\nCompletion:\nThe cell \"Banetn\" is misspelled and should be \"Banten\". The cell \"Central MJava\" is misspelled and should be \"\nCentral Java\". Therefore, the answer is {\"erroneous_cells\": [\"Banetn\", \"Central MJava\"]}\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nError Detection Few-Shot\nPrompt:\n# Task Description: Kindly evaluate the input table and let me know about any cell or cells that have errors. If\nthere are multiple cells with errors, provide them in a list. If no cells have errors, indicate 'None'.\nOnly return the cells for which you have a high level of confidence in their error status. Return the final\nresult as JSON in the format {\"erroneous_cells\": \"<a list of erroneous cells or None>\"}. Let's think step by\nstep and show your reasoning before showing the final result.\nIn:\n|District|Party|First Election|\n|---|---|---|\n|12|Repp|2011|\n|43|Dem|2003|\n|24|Rep|2002|\nOut:\nThe cell \"Repp\" is misspelled and should be \"Rep\". Therefore, the answer is {\"erroneous_cells\": [\"Repp\"]}\nIn:\n|District|Party|First Election|\n|---|---|---|\n|1|Rep|1993|\n|2|Rep|2015|\n|3|Rep|2009|\n|4|Rep|2013|\n|5|Rep|2011|\nLet's think step by step and show your reasoning before showing the final result. Return the final result as\nJSON in the format {\"erroneous_cells\": \"<a list of erroneous cells or None>\"}.\nOut:\nCompletion:\nAll cells in the table seem to be correct and no apparent error is found. Therefore, the final answer is {\"\nerroneous_cells\": \"None\"}\nRow/Column Swapping Zero-Shot\nPrompt:\n# Task Description: Please swap the second row and the third row in the table. Please return the table after\nswapping.\nQ:\n|Value|Colour|Type|\n|---|---|---|\n|1 lepton|brown|a|\n|2 lepta|grey|a|\n|3 lepta|orange|a|\n|5 lepta|green|b|\n|10 lepta|red|b|\nA:\nCompletion:\n|Value|Colour|Type|\n|---|---|---|\n|1 lepton|brown|a|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|3 lepta|orange|a|\n|2 lepta|grey|a|\n|5 lepta|green|b|\n|10 lepta|red|b|\nRow/Column Swapping Few-Shot\nPrompt:\nObjective: If possible, could you swap the positions of column \"Group\" and column \"%\" in the table? Let me know\nthe outcome.\n## Input:\n|Group|N|%|\n|---|---|---|\n|IIG|0|0.0%|\n|IIIAB|11|1.1%|\n|IVB|0|0.0%|\n|IIF|1|0.1%|\n|IIE|2|0.2%|\n## Output:\n|%|N|Group|\n|---|---|---|\n|0.0%|0|IIG|\n|1.1%|11|IIIAB|\n|0.0%|0|IVB|\n|0.1%|1|IIF|\n|0.2%|2|IIE|\n## Input:\n|Group|N|%|\n|---|---|---|\n|IAB complex|10|1.0%|\n|IC|0|0.0%|\n|IIAB|6|0.6%|\n|IIC|0|0.0%|\n|IID|3|0.3%|\n## Output:\nCompletion:\n|%|N|Group|\n|---|---|---|\n|1.0%|10|IAB complex|\n|0.0%|0|IC|\n|0.6%|6|IIAB|\n|0.0%|0|IIC|\n|0.3%|3|IID|\nSchema Matching Zero-Shot\nPrompt:\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nDescription: Please identify the matching columns between Table A and Table B. For each column in Table A,\nspecify the corresponding column in Table B. If a column in A has no corresponding column in Table B, you\ncan map it to None. Represent each column mapping using a pair of column headers in a list, i.e., [Table A\nColumn, Table B column or None]. Provide the mapping for each column in Table A and return all mappings in a\nlist. Return the final result as JSON in the format {\"column_mappings\": \"<a list of column pairs>\"}.\nQuestion:\nTable A:\n|IATA|ICAO|Country|\n|---|---|---|\n|YYC|CYYC|Canada|\n|BNE|YBBN|Australia|\nTable B:\n|Nation|Airport Code|\n|---|---|\n|Thailand|DMK|\n|Philippines|CEB|\n|Philippines|CRK|\nReturn the final result as JSON in the format {\"column_mappings\": \"<a list of column pairs>\"}.\nAnswer:\nCompletion:\n{\"column_mappings\": [[\"IATA\", \"Airport Code\"], [\"ICAO\", \"None\"], [\"Country\", \"Nation\"]]}\nSchema Matching Few-Shot\nPrompt:\nDescription: Find the corresponding columns between Table A and Table B. Specify the related columns in Table B\nfor each column present in Table A. If a column in Table A has no corresponding column in Table B, you can\ndenote it as None. Use pairs of column headers in a list structure, like [Table A Column, Table B Column or\nNone], to depict each mapping. Present the mappings for all the columns in Table A and organize them into a\nlist Return the final result as JSON in the format {\"column_mappings\": \"<a list of column pairs>\"}.\nQuestion:\nTable A:\n|Dignity|Detriment|\n|---|---|\n|Gemini and Virgo|Sagittarius and Pisces|\n|Leo|Aquarius|\nTable B:\n|Celestial Body (Symbol)|High Point|\n|---|---|\n|Moon ()|Taurus|\n|Venus ()|Pisces|\n|Mars ()|Capricorn|\nAnswer:\n{\"column_mappings\": [[\"Dignity\", \"None\"], [\"Detriment\", \"None\"]]}\nQuestion:\nTable A:\n|Barony|Civil parish|Acres|Poor law union|County|\n|---|---|---|---|---|\n|Callan|Callan|21|Callan|Kilkenny|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|Callan|Callan|xx|Callan|Kilkenny|\nTable B:\n|Location|Parish|Surface Area|Subdivision|\n|---|---|---|---|\n|Caherlesk|Ballytobin|1,162|Kells|\n|CLOGH T.|Castlecomer|xx|Fassadinin|\n|CASTLECOMER T.|Castlecomer|xx|Fassadinin|\nReturn the final result as JSON in the format {\"column_mappings\": \"<a list of column pairs>\"}.\nAnswer:\nCompletion:\n{\"column_mappings\": [[\"Barony\", \"Subdivision\"], [\"Civil parish\", \"Parish\"], [\"Acres\", \"Surface Area\"], [\"Poor\nlaw union\", \"None\"], [\"County\", \"None\"]]}\nHeader Value Matching Zero-Shot\nPrompt:\nTask: Given the input table data and the list of candidate headers, please determine the most suitable column\nheader for each column in the table. Please only choose column headers from the candidate list. Please only\nreturn the most suitable column header for each column. Return the chosen column headers in a list. Do not\nreturn the entire table. Return the final result as JSON in the format {\"column_headers\": \"<a list of\nheaders for each column chosen from the candidate list>\"}.\n[Q]:\n**Table Data:**\n||||\n|---|---|---|\n|1|681|Multihull|\n|3|911|Keelboat|\n|2|947|Dinghy|\n|nan|920|Dinghy|\n|1|870|Dinghy|\n**Candidate column headers:**\n- Portsmouth Number\n- crew\n- Type\nReturn the final result as JSON in the format {\"column_headers\": \"<a list of headers for each column chosen from\nthe candidate list>\"}.\n[A]:\nCompletion:\n{\"column_headers\": [\"crew\", \"Portsmouth Number\", \"Type\"]}\nHeader Value Matching Few-Shot\nPrompt:\nObjective: With the input table data and the list of potential headers at hand, make determinations about the\nmost fitting column header for every table column. Only select column headers from the candidate list and\ncompile them into a list. Return the final result as JSON in the format {\"column_headers\": \"<a list of\nheaders for each column chosen from the candidate list>\"}.\n## Input:\n**Table Data:**\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n||||\n|---|---|---|\n|Cust 1|126|1|\n|Cust 2|65|2|\n|Cust 5|30|5|\n|Cust 3|45|3|\n|Cust 9|19|9|\n**Candidate column headers:**\n- Count\n- Custom\n- Name\n## Output:\n{\"column_headers\": [\"Name\", \"Count\", \"Custom\"]}\n## Input:\n**Table Data:**\n||||\n|---|---|---|\n|1|The Beginning|1:48|\n|15|Ride Slow|3:05|\n|7|Siri (Skit)|0:53|\n|5|Wrong n Tough|2:45|\n|9|Officer Hatchet|3:46|\n**Candidate column headers:**\n- Name\n- Length\n- Track #\nReturn the final result as JSON in the format {\"column_headers\": \"<a list of headers for each column chosen from\nthe candidate list>\"}.\n## Output:\nCompletion:\n{\"column_headers\": [\"Track #\", \"Name\", \"Length\"]}\nColumn Augmentation Zero-Shot\nPrompt:\nTask: Please generate a new additional column for the input table and append the new column to the right of the\ntable. Return the new table with the additional column.\nInput:\n|Number|Name|\n|---|---|\n|279|Hazel|\n|280|Audrey|\n|281|Gwen|\n|282|Doris|\n|283|Mona|\n|284|Vera|\n|285|Car No 85|\n|286|Car No 86|\n|287|Car No 87|\n|288|Car No 88|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nOutput:\nCompletion:\n|Number|Name|Note|\n|---|---|---|\n|279|Hazel|5-BEL Trust, under restoration|\n|280|Audrey|VSOE, in service|\n|281|Gwen|VSOE, in service|\n|282|Doris|5-BEL Trust, to be restored|\n|283|Mona|VSOE, stored|\n|284|Vera|VSOE, in service|\n|285|Car No 85|5BEL Trust, under restoration|\n|286|Car No 86|VSOE, stored|\n|287|Car No 87|5-BEL Trust, under restoration|\n|288|Car No 88|5-BEL Trust, under restoration|\nColumn Augmentation Few-Shot\nPrompt:\nTask: Generate a new column for the input table and attach it on the right side. Share the updated table with\nthe appended column.\n## Input:\n|Year|Peak positions(CAN Country)|\n|---|---|\n|1981|41|\n|1983|40|\n|1983|35|\n|1984|39|\n|1985|29|\n|1986|10|\n|1987|20|\n|1988|9|\n|1989|30|\n|1989|9|\n## Output:\n|Year|Peak positions(CAN Country)|Album|\n|---|---|---|\n|1981|41|Non-album songs|\n|1983|40|Non-album songs|\n|1983|35|Non-album songs|\n|1984|39|Touch My Heart|\n|1985|29|Touch My Heart|\n|1986|10|Touch My Heart|\n|1987|20|Touch My Heart|\n|1988|9|Touch My Heart|\n|1989|30|Touch My Heart|\n|1989|9|Touch My Heart|\n## Input:\n|end km|home km|\n|---|---|\n|0.8|1.402|\n|1.402|1.44|\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n|1.44|1.884|\n|1.884|6.071|\n|6.071|6.513|\n|6.513|8.3|\n|8.3|12.4|\n|12.4|14.6|\n|14.6|21.0|\n|21.0|168.7|\n## Output:\nCompletion:\n|end km|home km|km/h|\n|---|---|---|\n|0.8|1.402|110|\n|1.402|1.44|35|\n|1.44|1.884|65|\n|1.884|6.071|110|\n|6.071|6.513|105|\n|6.513|8.3|110|\n|8.3|12.4|165|\n|12.4|14.6|200|\n|14.6|21.0|270|\n|21.0|168.7|300|\nNatural-Language to SQL Zero-Shot\nPrompt:\nDescription: Please write the SQL query given the input table and question. Use 'table' as the table name if\nneeded. Return the final result as JSON in the format {\"SQL\": \"<SQL code>\"}.\nIn:\n**Input table:**\n|Rank|Group|Name|Nationality|Mark|\n|---|---|---|---|---|\n|1|B|Brittney Reese|United States|6.87|\n|2|A|Maurren Higa Maggi|Brazil|6.79|\n|3|B|Tatyana Lebedeva|Russia|6.70|\n|4|A|Carolina Kluft|Sweden|6.70|\n|5|A|Grace Upshaw|United States|6.68|\n**Question:**\nWhat rank does Tricia Flores have?\nReturn the final result as JSON in the format {\"SQL\": \"<SQL code>\"}.\nOut:\nCompletion:\n{\"SQL\": \"SELECT `Rank` FROM table WHERE `Name` = tricia flores\"}\nNatural-Language to SQL Few-Shot\nPrompt:\nDescription: Please write an SQL query using the input table and question. If needed, utilize 'table' as the\ntable name. Return the final result as JSON in the format {\"SQL\": \"<SQL code>\"}.\nQ:\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n**Input table:**\n|Date|Score|Set 1|Set 2|Set 3|Total|\n|---|---|---|---|---|---|\n|13 Nov|3-0|15-6|15-12|15-13|45-31|\n|13 Nov|0-3|6-15|13-15|6-15|25-45|\n|14 Nov|0-3|2-15|8-15|6-15|16-45|\n|14 Nov|3-2|15-11|10-15|16-14|67-64|\n|15 Nov|3-1|15-9|15-11|13-15|58-47|\n**Question:**\nWhat is the score for set 2 when set 3 was 15-6?\nA:\n{\"SQL\": \"SELECT `Set 2` FROM table WHERE `Set 3` = 15-6\"}\nQ:\n**Input table:**\n|Position|Team|Played|Wins|Draws|Losses|Scored|Conceded|Points|\n|---|---|---|---|---|---|---|---|---|\n|1|Olimpia|9|4|4|1|14|6|16|\n|2|12 de Octubre|9|3|6|0|14|8|15|\n|3|Cerro Porteno|9|3|5|1|16|10|14|\n|4|Guarani|9|3|5|1|8|6|14|\n|5|Cerro Cora|9|3|4|2|9|8|13|\n**Question:**\nHow many losses for the team atl. colegiales?\nReturn the final result as JSON in the format {\"SQL\": \"<SQL code>\"}.\nA:\nCompletion:\n{\"SQL\": \"SELECT MAX `Losses` FROM table WHERE `Team` = Atl. Colegiales\"}\nTable Summary Zero-Shot\nPrompt:\nObjective: Please look at the table below and provide a title for the table. Return the final result as JSON in\nthe format {\"summary\": \"<summary of table>\"}.\n## Input:\n|Unnamed: 0|Day 8|Nominations received|Day 9|Day 10|Day 13|\n|---|---|---|---|---|---|\n|Jenny|Lucy|1.0|Georgina|Lucy|Natalia|\n|Natalia|In Secret Loft|13.0|Georgina|Lucy|Manuel|\n|Marijke|Jan|18.0|Jan|Lucy|Fancy|\n|Simon|Marijke|5.0|Marijke|Marijke|Marijke|\n|Martin|Lucy|3.0|Lucy|Lucy|Natalia|\nReturn the final result as JSON in the format {\"summary\": \"<summary of table>\"}.\n## Output:\nCompletion:\n{\"summary\": \"Promi Big Brother (season 1)\"}\nTable Summary Few-Shot\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nPrompt:\nObjective: Your task is to give a summary of the input table's main information. Return the final result as JSON\nin the format {\"summary\": \"<summary of table>\"}.\nIn:\n|Survivor|Day 1|Day 2|Day 3|Day 4|Day 5|Day 6|Day 7|Missions|\n|---|---|---|---|---|---|---|---|---|\n|Amena|SAFE|SAFE|SAFE|SAFE|SAFE|SAFE|WON|7|\n|Leah|SAFE|SAFE|SAFE|SAFE|SAFE|SAFE|WON|6|\n|Megan|SAFE|SAFE|SAFE|SAFE|SAFE|SAFE|WON|7|\n|Nic|NOT IN GAME|NOT IN GAME|NOT IN GAME|SAFE|SAFE|SAFE|WON|5|\n|Aston|SAFE|SAFE|SAFE|SAFE|SAFE|SAFE|DIED|7|\nOut:\n{\"summary\": \"I Survived a Zombie Apocalypse\"}\nIn:\n|Name|Charter/Plan|Type|Class|County|\n|---|---|---|---|---|\n|Bensalem|Plan|Township|Second|Bucks|\n|Bethlehem|Charter|City|Third|Lehigh Northampton|\n|Bristol|Plan|Township|First|Bucks|\n|College|Plan|Township|Second|Centre|\n|DuBois|Plan|City|Third|Clearfield|\nReturn the final result as JSON in the format {\"summary\": \"<summary of table>\"}.\nOut:\nCompletion:\n{\"summary\": \"List of Pennsylvania municipalities and counties with home rule charters, optional charters, or\noptional plans\"}\nList Extraction Zero-Shot\nPrompt:\nObjective: Please transform the list below into a table with multiple columns. Please return the table using\nplain text. Use vertical bars (|) to separate columns and use a new line for each row. Return the final\nresult as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n[Q]:\nJobFamily OrgID minTiG maxTiG\nJF1 5.0 2.0 12.0\nJF3 1.0 2.0 16.0\nJF2 4.0 2.0 7.0\nJF1 9.0 2.0 7.0\nJF3 41.0 2.0 7.0\nReturn the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n[A]:\nCompletion:\n{\"table\": \"|JobFamily|OrgID|minTiG|maxTiG|\\n|---|---|---|---|\\n|JF1|5.0|2.0|12.0|\\n|JF3|1.0|2.0|16.0|\\n|JF2\n|4.0|2.0|7.0|\\n|JF1|9.0|2.0|7.0|\\n|JF3|41.0|2.0|7.0|\\n\"}\nList Extraction Few-Shot\nPrompt:\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nDescription: Change the list provided into a table with several columns. The table must be represented in plain\ntext, with vertical bars (|) separating columns and a new line for every row. Return the final result as\nJSON in the format {\"table\": \"<table transformed from the list>\"}.\n[Q]:\nCh Video (MHz) Audio (MHz)\n15 487.25 493.75\n36 693.25 699.75\n39 717.25 723.75\n23 551.25 557.75\n58 871.25 877.75\n[A]:\n{\"table\": \"|Ch|Video (MHz)|Audio (MHz)|\\n|---|---|---|\\n|15|487.25|493.75|\\n|36|693.25|699.75|\\n\n|39|717.25|723.75|\\n|23|551.25|557.75|\\n|58|871.25|877.75|\\n\"}\n[Q]:\nMonth name amount\n5 c 3\n4 e 2\n3 b 6\n2 e 2\n3 a 3\nReturn the final result as JSON in the format {\"table\": \"<table transformed from the list>\"}.\n[A]:\nCompletion:\n{\"table\": \"|Month|name|amount|\\n|---|---|---|\\n|5|c|3|\\n|4|e|2|\\n|3|b|6|\\n|2|e|2|\\n|3|a|3|\\n\"}\nRow Augmentation Zero-Shot\nPrompt:\nTask: Please generate a new row for the input table and append it at the bottom of the table. Return the new\ntable with the additional generated row.\n[Q]:\n|Rank|Player|Receptions|\n|---|---|---|\n|1|Jerry Rice|1,549|\n|2|Tony Gonzalez|1,325|\n|3|Larry Fitzgerald|1,125|\n|4|Marvin Harrison|1,102|\n|5|Cris Carter|1,101|\n|6|Tim Brown|1,094|\n|7|Jason Witten|1,089|\n|8|Terrell Owens|1,078|\n|9|Anquan Boldin|1,076|\n[A]:\nCompletion:\n|Rank|Player|Receptions|\n|---|---|---|\n|1|Jerry Rice|1,549|\n|2|Tony Gonzalez|1,325|\n|3|Larry Fitzgerald|1,125|\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n|4|Marvin Harrison|1,102|\n|5|Cris Carter|1,101|\n|6|Tim Brown|1,094|\n|7|Jason Witten|1,089|\n|8|Terrell Owens|1,078|\n|9|Anquan Boldin|1,076|\n|10|Reggie Wayne|1,070|\nRow Augmentation Few-Shot\nPrompt:\nDescription: Please generate a new row for the input table and append it at the bottom of the table. Return the\nnew table with the additional generated row.\n[Q]:\n|ProductID|Month|TargetSale|\n|---|---|---|\n|1|6|50|\n|1|7|60|\n|1|8|70|\n|2|6|40|\n|2|7|50|\n|2|8|60|\n|3|6|30|\n|3|7|40|\n|3|8|50|\n[A]:\n|ProductID|Month|TargetSale|\n|---|---|---|\n|1|6|50|\n|1|7|60|\n|1|8|70|\n|2|6|40|\n|2|7|50|\n|2|8|60|\n|3|6|30|\n|3|7|40|\n|3|8|50|\n|4|6|30|\n[Q]:\n|Rank|Player|2015 FF Points|Games|Avg|\n|---|---|---|---|---|\n|1|Cam Newton|389.1|16|24.3|\n|2|Tom Brady|343.7|16|21.5|\n|3|Russell Wilson|336.4|16|21.0|\n|4|Blake Bortles|316.1|16|19.8|\n|5|Carson Palmer|309.2|16|19.3|\n|7|Aaron Rodgers|301.3|16|18.8|\n|8|Kirk Cousins|293.5|16|18.3|\n|9|Matthew Stafford|289.7|16|18.1|\n|10|Eli Manning|287.6|16|18.0|\n[A]:\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nCompletion:\n|Rank|Player|2015 FF Points|Games|Avg|\n|---|---|---|---|---|\n|1|Cam Newton|389.1|16|24.3|\n|2|Tom Brady|343.7|16|21.5|\n|3|Russell Wilson|336.4|16|21.0|\n|4|Blake Bortles|316.1|16|19.8|\n|5|Carson Palmer|309.2|16|19.3|\n|7|Aaron Rodgers|301.3|16|18.8|\n|8|Kirk Cousins|293.5|16|18.3|\n|9|Matthew Stafford|289.7|16|18.1|\n|10|Eli Manning|287.6|16|18.0|\n|11|Ryan Fitzpatrick|285.1|16|17.8|\nRow/Column Filtering Zero-Shot\nPrompt:\n# Task Description: Please select the first row in the table. Please return the table with the selected rows.\nQuestion:\n|sk_usuario|nk_id_usuario|nome_usuario|\n|---|---|---|\n|1|1|tatsurok2018|\n|4|4|Wilfredo M|\n|12|12|Jonathan C|\n|19|19|kenthecelt|\n|20|20|josenc2016|\nAnswer:\nCompletion:\n|sk_usuario|nk_id_usuario|nome_usuario|\n|---|---|---|\n|1|1|tatsurok2018|\nRow/Column Filtering Few-Shot\nPrompt:\nObjective: Could you choose column \"Option_content\" in the table? After the selection, please return the table.\nIn:\n|question_id|Ans_id|Option_content|\n|---|---|---|\n|94|2|False|\n|111|1|False|\n|118|2|False|\n|57|1|False|\n|95|1|False|\nOut:\n|Option_content|\n|---|\n|False|\n|False|\n|False|\n|False|\n|False|\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\nIn:\n|question_id|Ans_id|Option_content|\n|---|---|---|\n|16|1|False|\n|17|1|False|\n|18|1|False|\n|19|2|False|\n|20|2|False|\nOut:\nCompletion:\n|Option_content|\n|---|\n|False|\n|False|\n|False|\n|False|\n|False|\nRow-to-Row Transformation Zero-Shot\nPrompt:\nTask: You are given a table of inputs and outputs in two columns. Please figure out the patterns between inputs\nand outputs from the first few rows and then determine the output value for the last row (denoted by '[\nOutput Value]'). Please only return the output value. Return the final result as JSON in the format {\"\noutput_value\": \"<output value transformed from the input value>\"}.\n[Q]:\n|Input|Output|\n|---|---|\n|18:1|1|\n|14:6|6|\n|9:11|11|\n|10:12|12|\n|9:15|[Output Value]|\nReturn the final result as JSON in the format {\"output_value\": \"<output value transformed from the input value\n>\"}.\n[A]:\nCompletion:\n{\"output_value\": \"15\"}\nEntity Matching Zero-Shot\nPrompt:\nInstruction: Please determine whether the First Cosmetic and the Second Cosmetic refer to the same entity or not.\nYour final answer should be 'Yes' or 'No'. Return the final result as JSON in the format {\"answer\": \"<Yes\nor No>\"}. Let's think step by step and show your reasoning before showing the final result.\nQuestion:\nthe First Cosmetic:\n|Price|Color|Description|\n|---|---|---|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|$16.50|Wheat |CLINIQUE brow shaper|\nthe Second Cosmetic can be presented as follows.\n|Color|Description|Price|\n|---|---|---|\n|Charcoaled |Clinique Brow Shaper Charcoaled|$29.90|\nLet's think step by step and show your reasoning before showing the final result. Return the final result as\nJSON in the format {\"answer\": \"<Yes or No>\"}.\nAnswer:\nCompletion:\nthe First Cosmetic and the Second Cosmetic are different entities because they have different prices and colors.\nthe First Cosmetic is priced at $16.50 and is Wheat in color, while the Second Cosmetic is priced at $29.90\nand is Charcoaled in color. Therefore, the final answer is {\"answer\": \"No\"}.\nEntity Matching Few-Shot\nPrompt:\nInstruction: Please verify whether Cosmetic I and Cosmetic II are denoting the same entity. Indicate your\nconclusive answer as either 'Yes' or 'No'. Return the final result as JSON in the format {\"answer\": \"<Yes or\nNo>\"}. Let's think step by step and show your reasoning before showing the final result.\nIn:\nHere is the description of Cosmetic I.\n|Description|Price|Color|\n|---|---|---|\n|Hourglass extreme sheen high shine lip gloss|$28.00|Child|\nCosmetic II:\n|Price|Color|Description|\n|---|---|---|\n|$41.35|Truth|Hourglass Extreme Sheen High Shine Lip Gloss Truth|\nOut:\nCosmetic I and Cosmetic II are different entities because they have different prices ($28.00 vs. $41.35) and\ncolors (Child vs. Truth). Therefore, the final answer is {\"answer\": \"No\"}.\nIn:\nHere is the description of Cosmetic I.\n|Description|Price|Color|\n|---|---|---|\n|tarte lipsurgence lip tint|$24.00|Grateful|\nCosmetic II:\n|Price|Color|Description|\n|---|---|---|\n|$35.40|Energy|Tarte Lipsurgence Skintuitive Lip Tint - Energy (.10 oz)|\nOut:\nCosmetic I and Cosmetic II are different entities because they have different prices, colors, and descriptions.\nCosmetic I is a Tarte Lipsurgence Lip Tint with a price of $24.00 and a color of Grateful. Cosmetic II is a\nTarte Lipsurgence Skintuitive Lip Tint with a price of $35.40 and a color of Energy. Therefore, the final\nanswer is {\"answer\": \"No\"}.\nIn:\nHere is the description of Cosmetic I.\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n|Description|Price|Color|\n|---|---|---|\n|Bobbi Brown foundation brush|$42.00|nan|\nCosmetic II:\n|Price|Color|Description|\n|---|---|---|\n|$69.00|nan|Bobbi Brown Foundation Brush|\nOut:\nCosmetic I and Cosmetic II refer to the same entity as they both have the same description, \"Bobbi Brown\nFoundation Brush\", and the same color, nan. The only difference is the price, with Cosmetic I being $42.00\nand Cosmetic II being $69.00. Therefore, the final answer is {\"answer\": \"Yes\"}.\nIn:\nHere is the description of Cosmetic I.\n|Description|Price|Color|\n|---|---|---|\n|Dior glow maximizer primer|$42.00|1|\nCosmetic II:\n|Price|Color|Description|\n|---|---|---|\n|$43.91|1|Dior Glow Maximizer Light Boosting Primer|\nOut:\nCosmetic I and Cosmetic II are the same product, Dior Glow Maximizer Light Boosting Primer. Both have the same\nprice ($42.00 and $43.91) and color (1). The only difference is the order of the columns. Therefore, the\nfinal answer is {\"answer\": \"Yes\"}.\nIn:\nHere is the description of Cosmetic I.\n|Description|Price|Color|\n|---|---|---|\n|Bobbi Brown foundation stick|$44.00|Warm Honey|\nCosmetic II:\n|Price|Color|Description|\n|---|---|---|\n|$58.50|Warm Ivory|Exclusive Make Up Product By Bobbi Brown Foundation Stick - #1 Warm Ivory 9g/0.31oz|\nLet's think step by step and show your reasoning before showing the final result. Return the final result as\nJSON in the format {\"answer\": \"<Yes or No>\"}.\nOut:\nCompletion:\nCosmetic I and Cosmetic II are different entities because they have different prices, colors, and descriptions.\nCosmetic I is a Bobbi Brown foundation stick that costs $44.00 and is in the color Warm Honey. Cosmetic II\nis an Exclusive Make Up Product By Bobbi Brown Foundation Stick that costs $58.50 and is in the color Warm\nIvory. Therefore, the final answer is {\"answer\": \"No\"}.\nData Imputation Zero-Shot\nPrompt:\nTask: Please fill in the missing value in the input table. The missing value is denoted by '[MISSING]'. Please\nonly return the value filled in. Do not return the whole table. Return the final result as JSON in the\nformat {\"value\": \"<value filled in>\"}.\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\nIn:\n|Title|Credited artist(s)|Year|\n|---|---|---|\n|\"10 or a 2-Way\"|Korn|2005|\n|\"4 U\"|Korn|1999|\n|\"A Different World\"|Korn Corey Taylor|2016|\n|\"A.D.I.D.A.S.\"|[MISSING]|1996|\n|\"Alive\"|Korn|2003|\nReturn the final result as JSON in the format {\"value\": \"<value filled in>\"}.\nOut:\nCompletion:\n{\"value\": \"Korn\"}\nData Imputation Few-Shot\nPrompt:\nObjective: To fulfill your responsibility, please provide the missing value in the input table, represented by\n'[MISSING]'. Only the filled-in value should be provided, not the whole table. Return the final result as\nJSON in the format {\"value\": \"<value filled in>\"}.\nInput:\n|Column1|area|id_ov|\n|---|---|---|\n|16|CRM|29|\n|25|CCS|15|\n|32|CRM|4|\n|12|CRM|9|\n|8|[MISSING]|9|\nOutput:\n{\"value\": \"CRM\"}\nInput:\n|Column1|area|id_ov|\n|---|---|---|\n|0|CRM|5|\n|1|[MISSING]|9|\n|2|CCS|9|\n|3|CCS|9|\n|4|CRM|9|\nReturn the final result as JSON in the format {\"value\": \"<value filled in>\"}.\nOutput:\nCompletion:\n{\"value\": \"CCS\"}\nRow/Column Sorting Zero-Shot\nPrompt:\nDescription: Please sort the table by the column \"Gauge\" such that the values in this column are in\nalphabetically ascending order. Please return the sorted table.\nQ:\nConference\u201917, July 2017, Washington, DC, USA\nPeng Li\u2020, Yeye He\u2021, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang,\nDanielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri\n|Gauge|Dia.|Delta|\n|---|---|---|\n|5/0|0.5|nan|\n|5/0|0.5|0.046|\n|4/0|0.454|0.046|\n|4/0|0.454|0.029|\n|3/0|0.425|0.029|\nA:\nCompletion:\n|Gauge|Dia.|Delta|\n|---|---|---|\n|3/0|0.425|0.029|\n|4/0|0.454|0.046|\n|4/0|0.454|0.029|\n|5/0|0.5|nan|\n|5/0|0.5|0.046|\nRow/Column Sorting Few-Shot\nPrompt:\nInstruction: Could you sort the table by arranging it based on column headers in alphabetical descending order\nfrom left to right? After the sorting, kindly return the table.\nInput:\n|Rank|State|%|\n|---|---|---|\n|46.0|NH|18.7|\n|45.0|MT|19.0|\n|49.0|NY|18.3|\n|48.0|OR|18.5|\n|51.0|TX|18.1|\nOutput:\n|State|Rank|%|\n|---|---|---|\n|NH|46.0|18.7|\n|MT|45.0|19.0|\n|NY|49.0|18.3|\n|OR|48.0|18.5|\n|TX|51.0|18.1|\nInput:\n|Rank|State|%|\n|---|---|---|\n|40.0|KS|20.0|\n|41.0|GA|20.0|\n|42.0|ND|19.6|\n|43.0|VA|19.3|\n|44.0|RI|19.3|\nOutput:\nCompletion:\n|State|Rank|%|\nTable-GPT: Table-tuned GPT for Diverse Table Tasks\nConference\u201917, July 2017, Washington, DC, USA\n|---|---|---|\n|KS|40.0|20.0|\n|GA|41.0|20.0|\n|ND|42.0|19.6|\n|VA|43.0|19.3|\n|RI|44.0|19.3|\n"
  },
  {
    "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
    "link": "https://arxiv.org/pdf/2310.09199.pdf",
    "upvote": "20",
    "text": "Preprint.\nPALI-3 VISION LANGUAGE MODELS:\nSMALLER, FASTER, STRONGER\nXi Chen\u22c6\n1\nXiao Wang\u22c6\n2\nLucas Beyer\u22c6\n2\nAlexander Kolesnikov2\nJialin Wu1\nPaul Voigtlaender1\nBasil Mustafa2\nSebastian Goodman1\nIbrahim Alabdulmohsin2\nPiotr Padlewski2\nDaniel Salz1\nXi Xiong3\nDaniel Vlasic3\nFilip Pavetic2\nKeran Rong2\nTianli Yu3\nDaniel Keysers2\nXiaohua Zhai\u22c6\u2020\n2\nRadu Soricut\u2020\n1\n1Google Research\n2Google DeepMind\n3Google Cloud\nABSTRACT\nThis paper presents PaLI-3, a smaller, faster, and stronger vision language model\n(VLM) that compares favorably to similar models that are 10x larger. As part of\narriving at this strong performance, we compare Vision Transformer (ViT) mod-\nels pretrained using classification objectives to contrastively (SigLIP) pretrained\nones. We find that, while slightly underperforming on standard image classifica-\ntion benchmarks, SigLIP-based PaLI shows superior performance across various\nmultimodal benchmarks, especially on localization and visually-situated text un-\nderstanding. We scale the SigLIP image encoder up to 2 billion parameters, and\nachieves a new state-of-the-art on multilingual cross-modal retrieval. We hope\nthat PaLI-3, at only 5B parameters, rekindles research on fundamental pieces of\ncomplex VLMs, and could fuel a new generation of scaled-up models.\n1\nINTRODUCTION\nThe scaling of vision-language models (VLM) to tens and even hundreds of billions of parame-\nters (Chen et al., 2023b; Alayrac et al., 2022; Chen et al., 2023a; Driess et al., 2023) has shown\never-increasing performance. Meanwhile, models at a smaller scale remain critical, as they are more\npractical to train and serve, more environmentally-friendly, and support faster research cycles for\nmodel design.\nIn the spirit of focusing on smaller-scale modeling, we present PaLI-3, the third-generation family\nof PaLI (Chen et al., 2023b) models. Using a pretrained backbone with only 5B total parameters,\nwe refine the training recipe and achieve competitive and new state-of-the-art (SOTA) results on\nvarious VLM benchmarks. Our new recipe has three main components: contrastive pretraining of\nimage encoder on web-scale image-text data (Zhai et al., 2023), an improved dataset mixture for\nPaLI multimodal training, and training at higher resolutions.\nPaLI-3 achieves new SOTA results on tasks that require visually-situated text understanding and\nobject localization, including eight visually-situated text understanding tasks and the referring ex-\npression segmentation task on RefCOCO (Yu et al., 2016), along with strong performance on a\nwide range of classical vision tasks. As part of this work, we also introduce a SOTA multilingual\ncontrastive vision model scaled to 2B parameters, obtained using the recently-introduced SigLIP\nrecipe (Zhai et al., 2023). Additionally, we perform focused ablation studies to compare the classifi-\ncation pretrained Vision Transformer (ViT) backbones (Dosovitskiy et al., 2021) with contrastively\npretrained ones (SigLIP). This further confirms the viability of pretraining visual encoders on noisy\nweb-scale image-text data, as a preferable alternative to training on classification-style data.\nOur contributions are summarized as follows:\n\u22c6Core contributors, \u2020Project leads\n1\narXiv:2310.09199v2  [cs.CV]  17 Oct 2023\nPreprint.\n1. We compare classification pretrained ViT models (Dosovitskiy et al., 2021) to contrastively\npretrained SigLIP models using the PaLI framework (Chen et al., 2023b). We find that the\ncontrastively pretrained models work significantly better for visually-situated text under-\nstanding tasks and localization tasks.\n2. Our model achieves SOTA performance on 10+ diverse vision-language benchmarks while\nbeing 10x smaller in size compared to the current SOTA model Chen et al. (2023a). For\nunderstanding visually-situated text, the improvement is by a particularly large margin.\n3. Despite not pretraining on any video data, our model achieves new SOTA on several video\nQA benchmarks, indicative of powerful generalization abilities.\n4. We introduce the 2 billion parameter (ViT-G) multilingual SigLIP model trained on We-\nbLI (Chen et al., 2023b), which sets a new state-of-the-art on the multilingual cross-modal\nretrieval benchmark Thapliyal et al. (2022) across 36 languages.\n2\nRELATED WORK\nRecent large vision language models (VLMs) use pretrained image encoders as part of the larger\nmodel, some pretrain it with supervised classification (PaLI (Chen et al., 2023b), PaLI-X (Chen\net al., 2023a), PaLM-E (Driess et al., 2023)), some use pretrained CLIP encoders (BLIPv2 (Li\net al., 2023), CrossTVR (Dai et al., 2023), ChatBridge (Zhao et al., 2023)) and some with custom\nmultimodal pretraining (Flamingo (Alayrac et al., 2022), BEiT3 (Wang et al., 2022b), CoCa (Yu\net al., 2022), SimVLM (Wang et al., 2021b)). In this paper we compare two dominant ways to\npretrain image encoders using the PaLI framework: classification pretraining using large weakly\nlabeled datasets (JFT, as in Kolesnikov et al., 2020; Zhai et al., 2022a; Dehghani et al., 2023) and\ncontrastive pretraining on web-scale noisy data (WebLI, as in Zhai et al., 2023).\nA recent finding spanning across PaLI (Chen et al., 2023b) and PaLI-X (Chen et al., 2023a) is that\nscaling up the classification pretrained image encoder seems more promising than was previously\nbelieved (Alayrac et al., 2022). Specifically, while classic image-only benchmarks such as Ima-\ngeNet seem to indicate saturating performance from scaling pretraining of image-only models (Zhai\net al., 2022a), PaLI shows that by scaling up the vision encoder from ViT-G (2B) to ViT-e (4B), the\nimprovements on VL tasks are more noticeable than on ImageNet. PaLI-X further scaled up both\nthe vision and language components, showing that these larger image encoders keep bringing ben-\nefit when plugged into large VLMs. This finding suggests that there is more to be found regarding\nthe pretraining of image encoders in the context of VLMs, which may lead to different conclusions\nwhen looking at VLM tasks as compared to of \u201cpure\u201d vision tasks. In this paper, we dive into the\nimpact of the image encoder for VLMs, by directly comparing classification pretrained vision mod-\nels to contrastively pretrained ones, and reveal that the latter are vastly superior on a variety of tasks,\nespecially localization and visually-situated text understanding.\nOne can split multimodal understanding capabilities into largely two categories: natural scene under-\nstanding (captioning, VQA, object detection/localization), and visually-situated text understanding\n(document and infographics QA). These groups of tasks require different granularity of understand-\ning, and previous VLMs have largely focused on one type of tasks, leading to their training recipes\nbeing attuned to that type of tasks. For example PaLI-17B (Chen et al., 2023b) and Pix2struct (Lee\net al., 2022) showed strong performance only on one of the two categories, respectively. The recent\nPaLI-X (Chen et al., 2023a) achieves SOTA performance on both categories, based on an improved\nOCR-related training recipe, and a significantly larger 55B parameter model. In this work, we com-\nbine the advantage of contrastively-pretrained ViT and a further improved and balanced training\nrecipe into PaLI-3, and demonstrate that SOTA level performance on both the above categories of\nmultimodal understanding is achievable even at 5B parameter scale.\n3\nMODEL\n3.1\nARCHITECTURE\nOn a high level, the architecture follows Chen et al. (2023b;a): a ViT encodes the image into tokens\nwhich, together with text input (the question, prompt, instruction), are passed to an encoder-decoder\ntransformer (Vaswani et al., 2017) that generates a text output.\n2\nPreprint.\nUL2: 3B Language Model\nSigLIP: 2B Vision Model\nContrastive\nVision Encoder\nTransformer Encoder\nTransformer Decoder\n\u201cWhere is the \nphotographer \nresting?\u201d\n\u201cIn a hammoc under a tree on a tropical beach\u201d\n\u2026\n\u2026\nTokenizer\nFigure 1: Overview of the PaLI-3 (5B) model: images are encoded into visual tokens individually\nby the contrastively pretrained 2B SigLIP vision model. Along with a query, these visual tokens\nare passed to an 3B encoder-decoder UL2 Transformer which produces the desired answer. In\nsuch a setup, a contrastively pretrained model provides significantly more useful tokens than one\nclassification pretrained model as in previous PaLI models.\nVisual component\nThe vision backbone of PaLI-3 is initialized from a contrastively pretrained\nViT-G/142 (Zhai et al., 2022a) model (approx. 2B parameters) using the SigLIP (Zhai et al., 2023)\ntraining recipe. In short, an image embedding ViT-G/14 and a text embedding transformer are\ntrained to separately embed images and texts, such that a binary classifier using the sigmoid cross-\nentropy of the dot product of image and text embeddings correctly classifies whether the respective\nimage and text correspond to each other or not. This is similar to CLIP (Radford et al., 2021) and\nALIGN (Jia et al., 2021), but was shown to be more efficient, scalable, and robust (Zhai et al., 2023).\nThis is done in order to pretrain the ViT image embedding component, hence the text embedding\ntransformer is discarded when inserting the ViT into PaLI.\nFull PaLI model\nThe outputs of the ViT image encoder before pooling form the visual tokens,\nwhich are linearly projected and prepended to the embedded input text tokens. Together, these\ntokens are passed into a pretrained 3B parameter UL2 encoder-decoder language model (Tay et al.,\n2023), which generates text output. The text input to the model typically consists of a prompt that\ndescribes the type of task (e.g., \u201cGenerate the alt_text in \u27e8lang\u27e9 at \u27e8pos\u27e9\u201d for captioning tasks) and\nencode necessary textual input for the task (e.g., \u201cAnswer in \u27e8lang\u27e9: {question} \u201d for VQA tasks).\n3.2\nSTAGES OF TRAINING\nThe training procedure is similar to that of PaLI and PaLI-X and consists of multiple stages:\nStage 0: Unimodal pretraining.\nThe image encoder is pretrained contrastively on image-text\npairs from the web, following the SigLIP training protocol (Zhai et al., 2023). This differs from\nPaLI and PaLI-X, where a JFT classification pretrained encoder was used. We use a model-based\nfiltering approach similar to that in Schuhmann et al. (2021) that retains about 40% of the pairs.\nThe image encoder is trained at resolution 224\u00d7224. The text encoder-decoder is a 3B UL2 model\ntrained following the mixture of denoisers procedure described by Tay et al. (2023).\nStage 1: Multimodal training.\nHere, the image encoder is combined with the text encoder-\ndecoder as described earlier and in Figure 1. Then, this combined PaLI model is trained on a\nmultimodal task and data mixture, albeit keeping the image encoder frozen and using its native\n2The embedding dimension was changed from 1664 to 1536 for better hardware utilization.\n3\nPreprint.\nTable 1: Performance comparison between contrastively pre-trained (\u201cSigLIP\u201d) models and classi-\nfication pre-trained (\u201cClassif\u201d) ViT image encoders using the same PaLI setup, across a wide range\nof tasks. While linear classification few-shot probing (first column) suggests SigLIP encoders are\nworse across many tasks, when plugged into PaLI and transferred, they show clear improvements.\nOn the most complicated and detailed image understanding tasks, SigLIP models outperform Classif\nmodels by a large margin. Captioning numbers are CIDEr scores, where XM3600 shows the En-\nglish performance in the first column, and the average across other languages in the second column.\nRefCOCO numbers are mIoU scores (details in Section 4.3).\nProbe\nCaptioning\nVQA\nRefCOCO\n8 tasks\nCOCO\nXM3600\nv2\nOK\nText\nval\n+\ng\nG/14\nClassif\n88.1\n139.9\n94.5\n44.7\n76.7\n57.2\n31.9\n51.6\n43.5\n43.4\nSigLIP\n-2.5\n+0.4\n+1.6\n+0.7\n+0.8\n+1.4\n+18.7\n+15.1\n+19.1\n+17.7\nL/16\nClassif\n86.2\n132.6\n93.0\n42.3\n73.7\n55.6\n24.9\n46.9\n38.8\n38.8\nSigLIP\n-2.8\n+3.2\n+1.4\n+1.4\n+1.9\n+1.9\n+16.2\n+17.4\n+20.9\n+20.1\nB/16\nClassif\n83.7\n127.7\n91.7\n40.7\n72.3\n54.7\n22.5\n46.3\n38.1\n38.4\nSigLIP\n-2.6\n+3.6\n-2.0\n-0.2\n+1.4\n+0.9\n+13.3\n+16.8\n+19.6\n+19.3\n(224\u00d7224) resolution. The main mixture component is again derived from the WebLI dataset by\nheuristic filtering of the text quality and using the SplitCap training objective (Chen et al., 2023b).\nFurther ingredients inherited from (Chen et al., 2023b) are multilingual captioning on CC3M-35L\nand WebLI OCR, cross-lingual VQA and VQG using VQ2A-CC3M-35L, object-aware VQA, as\nwell as object detection. Notably, we do not include task or data derived from video (this was done\nin PaLI-X), but PaLI-3 retains competitive performance on these benchmarks thanks to its strong\nimage encoder. We do, however, further improve document and text understanding capabilities by\nenriching WebLI with PDF documents with dense text and web-images described as posters or doc-\numents, in over 100 languages.\nStage 2: Resolution increase.\nHigh-resolution input is a widely accepted way of increasing per-\nformance, both due to making more details from the image perceptible, and due to increasing model\npower via increased sequence length. We increase PaLI-3\u2019s resolution by fine-tuning the whole\nmodel (unfreezing the image encoder) with a short curriculum of increasing resolutions, keeping\ncheckpoints at 812\u00d7812 and 1064\u00d71064 resolution. The data mixture focuses on the part that in-\nvolves visually-situated text and object detection.\nTask specialization (transfer).\nFinally, for each individual task (benchmark), we fine-tune the\nPaLI-3 model with frozen ViT image encoder on the task\u2019s training data as described in the cor-\nresponding section. For most tasks, we fine-tune the 812\u00d7812 resolution checkpoint, but for two\ndocument understanding tasks, we go up to 1064\u00d71064 resolution.\n4\nEXPERIMENTS\n4.1\nCLASSIFICATION OR CONTRASTIVELY PRETRAINED VIT?\nWe first perform a controlled comparison of different ViT models within the PaLI framework. We\nconsider two types of ViT models: classification pretrained (\u201cClassif\u201d) on the JFT dataset and con-\ntrastively pretrained on the WebLI dataset (\u201cSigLIP\u201d). We perform these experiments using a fixed\n224\u00d7224 resolution (i.e. only include Stage 1) to save compute. We further shorten the Stage 1\nphase to 20% of the full PaLI-3 schedule used in the remainder of this paper.\nThe results in Table 1 paint a clear picture overall: While the few-shot linear classification (Doso-\nvitskiy et al., 2021) of SigLIP models falls behind, when used in PaLI-3, SigLIP models provide\nmoderate gains on \u201csimpler\u201d tasks such as captioning and question-answering, and large gains for\nmore \u201ccomplicated\u201d scene-text and spatial understanding tasks such as TextVQA and RefCOCO\n4\nPreprint.\nTable 2: Results on benchmarks more focused on understanding visually-situated text. TextCaps,\nTextVQA, STVQA, InfographicVQA and DocVQA are all evaluated using the corresponding of-\nficial evaluation server. Methods marked by \"\u2020\" are trained on additional VQA data similar to\nthe target benchmark, before finetuning on the target benchmark. For ChartQA, we compare with\nsimilar setups by finetuning without chain-of-thought or similar prompting techniques. The SOTA\nmodels are (a) Chen et al. (2023a), (b) Powalski et al. (2021), (c) Peng et al. (2022).\nText\nText\nST\nOCR\nInfo\nDoc\nAI2D Chart Screen2 Widget\nAvg. of\nModel\nCaps VQA VQA VQA VQA VQA\nQA\nWords\nCap\nfirst 8\nwith OCR pipeline input\nSOTA\n163.7 80.78 84.5\n77.3\n61.2\n88.4\n81.4\n72.3\n-\n-\n88.7\n(a)\n(a)\n(a)\n(a)\n(b)\u2020\n(c)\u2020\n(a)\n(a)\nPaLI-3\n164.3 80.78 85.7\n77.8\n62.4\n88.6\n75.2\n69.5\n-\n-\n88.0 (-0.7)\nwithout OCR pipeline input\nSOTA\n147.0 71.44 79.9\n75.0\n49.2\n80.0\n81.2\n70.9\n127.9\n153.0\n81.8\n(a)\n(a)\n(a)\n(a)\n(a)\n(a)\n(a)\n(a)\n(a)\n(a)\nPaLI-3\n158.8 79.51 84.1\n76.7\n57.8\n87.6\n75.2\n70.0\n130.7\n159.8\n86.2 (+4.4)\nvariants. This motivates the departure from classification pretrained image encoders, and switching\nto sigmoid-contrastively pretrained ones for building PaLI-3.\n4.2\nVISUALLY-SITUATED TEXT UNDERSTANDING\nWe evaluate PaLI-3 on visually-situated text understanding tasks: TextCaps (Sidorov et al., 2020),\nTextVQA (Singh et al., 2019), STVQA (Biten et al., 2019), OCRVQA (Mishra et al., 2019), Info-\ngraphicVQA (Mathew et al., 2022), DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022),\nScree2Words (Wang et al., 2021a), and WidgetCap (Li et al., 2020). The images in those datasets\nspan a wide range of domains such as natural images, illustrations, documents and user interfaces.\nFor the InfographicVQA and DocVQA benchmarks we fine-tune the 1064\u00d71064 resolution model,\nall others use the 812\u00d7812 one. We report the standard metrics for each benchmark, namely:\nCIDEr score for all the Image Captioning benchmarks; VQA accuracy for VQAv2, OKVQA, and\nTextVQA; Average Normalized Levenshtein Similarity (ANLS) for ST-VQA, DocVQA and Info-\ngraphicsVQA; Exact match (EM) for TallyQA, OCR-VQA and AI2D; Relaxed-Accuracy (RA) for\nChartQA. For visually-situated text understanding, external OCR systems are usually leveraged to\nprovide OCR annotations of the image as and additional input to the model for boosting perfor-\nmance. Here we follow (Chen et al., 2023a) and report the result of finetuning PaLI-3 both with\nand without OCR inputs. The OCR annotations are obtained using the same service as that for the\ntraining set. As shown in Table 2, PaLI-3 achieves SOTA performance on a vast majority of the cap-\ntioning and VQA benchmarks both with and without external OCR input. The exception is AI2D and\nChartQA, which require not just understanding but also strong reasoning capability over diagrams\nand charts, respectively. For these two benchmarks, PaLI-3 falls slightly behind PaLI-X (Chen et al.,\n2023a) likely due to the latter\u2019s significantly larger 32B LLM being better at reasoning.\nAveraging over the 8 benchmarks in Table 2 that have results in all settings, PaLI-3 is only 0.7 points\nbehind all SOTA methods combined in the setting where external OCR systems are used. However,\nin the setting without such external system, PaLI-3 has a significant 4.4 point advantage over all\nSOTA methods combined. For TextCaps, TextVQA, InfographicVQA and DocVQA this advantage\nis 8 points or more. Finally, we can see that PaLI-3 without any external OCR system is only 1.8\npoints behind relying on one, suggesting the image encoder learns a strong intrinsic OCR capability.\n4.3\nREFERRING EXPRESSION SEGMENTATION\nWe extend PaLI-3 with the capability to predict segmentation masks via language-like output. To\nthis end, we make use of the vector-quantized variational auto-encoder (VQ-VAE) from Ning et al.\n(2023). The VQ-VAE is trained to learn a discrete codebook of 128 mask tokens. Its encoder can\n5\nPreprint.\nTable 3: PaLI referring expression segmentation results on RefCOCO (Yu et al., 2016) variants. All\nresults are mIoU on the val split.\nModel\nRefCOCO\nRefCOCO+\nG-Ref\nRefTr (Li & Sigal, 2021)\n74.34\n66.75\n66.63\nPolyFormer (Liu et al., 2023)\n76.94\n72.15\n71.15\nPaLI-3 (Ours)\n77.33\n73.53\n72.72\ntokenize a 64 \u00d7 64 pixels segmentation mask into 16 mask tokens, which its decoder can convert\nback. We train PaLI-3 to predict a single segmentation mask. First, PaLI-3 outputs 4 coordinates\nas text, representing a bounding box. This is followed by 16 mask tokens that represent the mask\ninside the bounding box.\nWe fine-tune PaLI-3 on the combined training sets of RefCOCO, RefCOCO+, and RefCOCOg (Yu\net al., 2016)3 at 812\u00d7812 resolution. Each training example consists of a referring expression (e.g.\n\u201cthe large brown dog on the left\u201d), and a box with segmentation mask. We prompt PaLI with the\nprompt \u201cdetect: the large brown dog on the left \u27e8extra_id_0\u27e9\u201d, and the target is a sequence like\n\u201c348 543 684 664 \u27e8mask_token_81\u27e9 ... \u27e8mask_token_10\u27e9\u201d. The target sequence contains 16 mask\ntokens between 0 and 127 that are generated by the VQ-VAE encoder using the segmentation mask\ncropped and resized to 64 \u00d7 64 as input.\nThe results in Table 1 demonstrate that contrastive pretraining is much more effective than classifi-\ncation pretraining for localization task of this type. Table 3 shows that the full PaLI-3 model is able\nto slightly outperform the state of the art in referring expression segmentation.\n4.4\nNATURAL IMAGE UNDERSTANDING\nIn this section, we evaluate PaLI-3 on general vision-language understanding tasks, including COCO\ncaptions (Karpathy & Fei-Fei, 2015) and VQAv2 (Goyal et al., 2017) which target general visual\nunderstanding, OKVQA (Marino et al., 2019) which focuses on knowledge-based understanding and\nTallyQA (Acharya et al., 2019) which measures performance on counting under textual guidance.\nAll results for the benchmarks presented in Table 4 use 812\u00d7812 resolution. As in previous work,\nthey employ no external OCR module, since these benchmarks rarely involve text in the image.\nOverall, PaLI-3 shows very strong performance on these benchmarks despite its significantly smaller\nsize compared to recent SOTA models. For COCO, PaLI-3 outperforms all models but BEiT-3 and\nthe 17B and 55B PaLI. On VQAv2 and TallyQA, PaLI-3 exceeds all previous models except PaLI-X,\nwith a less than 1 point gap on VQAv2. For the knowledge-intensive OKVQA task, which usually\nbenefits from a large language component, PaLI-3 is only behind PaLM-E (562B) and PaLI-X (55B)\nbut still outperforms the 32-shot Flamingo (80B) model.\n4.5\nVIDEO CAPTIONING AND QUESTION ANSWERING\nWe fine-tune and evaluate the PaLI-3 model on 4 video captioning benchmarks: MSR-VTT (Xu\net al., 2016), VATEX (Wang et al., 2019), ActivityNet Captions (Krishna et al., 2017), and Spoken\nMoments in Time (Monfort et al., 2021). We do the same for 3 video question-answering bench-\nmarks: NExT-QA (Xiao et al., 2021), MSR-VTT-QA (Xu et al., 2017), and ActivityNet-QA (Yu\net al., 2019). A brief description of each benchmark and its usage is provided in Appendix A.\nFollowing the setup from PaLI-X (Chen et al., 2023a), we fine-tune our model using the Stage 1\ncheckpoint with 224\u00d7224 resolution for each task separately. We sample at most 16 frames with a\nfixed temporal stride for each benchmark. Each frame is independently processed by the ViT image\nencoder, the resulting visual tokens are simply concatenated, leading to up to 4096 visual tokens. A\nkey difference from the PaLI-X setup is that there is no video data in PaLI-3 pretraining, meaning\nPaLI-3 has never seen multi-frame inputs during pretraining.\n3We removed all validation and test images from the training set for both PaLI and the VQ-VAE\n6\nPreprint.\nTable 4: Results on COCO Captions (Karpathy split), VQAv2, OKVQA, and TallyQA. (*Flamingo\nreports 32 shot result). Underscored numbers indicate that PaLI-3 is only behind the 55B PaLI-X\nand is better than all other models in the list.\nCOCO\nVQAv2\nOKVQA\nTallyQA\nModel\nKarp.-test\ntest-dev\ntest-std\nval\nSimple\nComplex\nSimVLM\n143.3\n80.03\n80.34\n-\n-\n-\nCoCa (2.1B)\n143.6\n82.3\n82.3\n-\n-\n-\nGIT (0.7B)\n144.8\n78.56\n78.81\n-\n-\n-\nGIT2 (5.1B)\n145.0\n81.74\n81.92\n-\n-\n-\nOFA (0.9B)\n145.3\n82.0\n82.0\n-\n-\n-\nFlamingo (80B)\n138.1\n82.0\n82.1\n57.8\u2217\n-\n-\nBEiT-3 (1.9B)\n147.6\n84.2\n84.0\n-\n-\n-\nPaLM-E (562B)\n138.7\n80.0\n-\n66.1\n-\n-\nMoVie\n-\n69.26\n-\n-\n74.9\n56.8\nPaLI-17B\n149.1\n84.3\n84.3\n64.5\n81.7\n70.9\nPaLI-X (55B)\n149.2\n86.0\n86.1\n66.1\n86.0\n75.6\nPaLI-3 (5B)\n145.9\n85.0\n85.2\n60.1\n83.3\n70.5\nTable 5: Results for Video Captioning and Video-QA using up to 16 frames. \u2020GIT2 directly opti-\nmizes the CIDEr metric. mPLUG-2 is Xu et al. (2023), PaLI-X is Chen et al. (2023a), GIT2 is Wang\net al. (2022a), and Flamingo-32 is the 32-shot variant of Alayrac et al. (2022).\nMSR-VTT\nActivity-Net\nVATEX\nSMIT\nNExT-QA\nMethod\nCaption\nQA\nCaption\nQA\nCaption\nCaption\nQA\nPrior SOTA\n80.3\n48.0\n54.9\n49.4\n94.0\u2020\n43.5\n38.3\nmPLUG-2\nmPLUG-2\nPaLI-X\nPaLI-X\nGIT2\nPaLI-X\nFlamingo-32\nPaLI-3\n78.3\n49.3\n50.8\n51.2\n66.9\n39.6\n37.7\nDespite not being pretrained with video data, PaLI-3 achieves excellent video QA results with a\nsmall model size: a new state of the art performance on MSR-VTT-QA and ActivityNet-QA, and\ncompetitive results on NextQA. The consistent improvements on image and video QA highlight the\nbenefits of adopting the contrastive ViTs. PaLI-3 also achieves respectable video captioning results,\nunder-performing the SOTA by only 3 CIDEr points on average. Considering the model size, PaLI-3\nappears to be an excellent choice in terms of both performance and practicality.\nTable 6: Evaluations of the visual component in isolation (without the language model). We report\nfine-tuned classification accuracy on ImageNet, ImageNet-ReaL and ImageNet-V2; average zero-\nshot cross-modal retrieval recall@1 across 36 languages on XM3600; average 10-shot linear probe\nclassification accuracy across 8 tasks.\nModel\nEncoder\nImageNet (fine-tuning)\nXM3600 (retrieval)\nProbe\nRes.\nVal\nReaL\nv2\nI\u2192T\nT\u2192I\n8 tasks\nPaLI-3\nSigLIP ViT-G\n518px\n89.6\n90.9\n82.3\n56.9\n44.0\n85.6\nPaLI-15B\nClassif ViT-G\n518px\n90.5\n90.8\n83.3\n-\n-\n88.1\nPaLI-17B\nClassif ViT-e\n644px\n90.9\n91.1\n84.3\n36.0\n28.5\n89.5\nPaLI-X\nClassif ViT-22B\n756px\n89.2\n91.0\n83.7\n-\n-\n89.9\n7\nPreprint.\nTable 7: RAI statistics for captions generated by PaLI-3 on FairFace (Karkkainen & Joo, 2021).\nPerceived Gender\nEthnicity\nAge Bucket\nLowest\nHighest\nLowest\nMedian\nHighest\nLowest\nMedian\nHighest\nOverall\nToxicity\n0.02%\n0.05%\n0.00%\n0.00%\n0.10%\n0.00%\n0.00%\n0.07%\n0.04%\nProfanity\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nInsult\n0.04%\n0.10%\n0.00%\n0.07%\n0.14%\n0.00%\n0.06%\n0.17%\n0.07%\nThreat\n0.10%\n0.12%\n0.00%\n0.14%\n0.20%\n0.00%\n0.00%\n0.21%\n0.11%\nAttack\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nTable 8: RAI score statistics in the captions generated by PaLI-3 on MIAP (Schumann et al., 2021).\nPerceived Gender\nAge Bucket\nSkin Tone\nLowest\nHighest\nLowest\nMedian\nHighest\nLowest\nMedian\nHighest\nOverall\nToxicity\n0.05%\n0.10%\n0.05%\n0.24%\n0.48%\n0.00%\n0.00%\n0.26%\n0.07%\nProfanity\n0.10%\n0.10%\n0.00%\n0.12%\n0.24%\n0.00%\n0.00%\n0.10%\n0.10%\nInsult\n0.10%\n0.14%\n0.00%\n0.13%\n0.17%\n0.00%\n0.00%\n0.38%\n0.13%\nThreat\n0.34%\n0.80%\n0.30%\n0.54%\n1.68%\n0.00%\n0.59%\n0.94%\n0.65%\nIdentity Attack\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n4.6\nDIRECT IMAGE ENCODER EVALUATION\nHere, we aim to directly evaluate the learned image encoder (ViT-G model) without the surrounding\nlanguage model, i.e. not the full PaLI-3. All results are summarized in Table 6.\nFirst, we test image classification capabilities using the standard ImageNet (Russakovsky et al.,\n2014) benchmark and two of its most popular variants (Beyer et al., 2020; Recht et al., 2019). We\nfine-tune the unimodal image encoder from Stage 0 on ImageNet and compare it to fine-tuning\nof classification pretrained ViTs used in previous PaLI models. The main comparison is to the\nclassification (Classif) pretrained ViT-G/14 model from Zhai et al. (2022a). The SigLIP slightly\nlags behind in terms of top-1 and v2 accuracy, but matches in terms of ReaL accuracy (Beyer et al.,\n2020), a metric which avoids measuring \u201coverfitting\u201d to ImageNet peculiarities.\nSecond, we report multilingual image-text retrieval results on the Crossmodal-3600 bench-\nmark (Thapliyal et al., 2022). Since classification pretrained image encoders do not have this ca-\npability, we LiT-tune (Zhai et al., 2022b) a text encoder for it on the multilingual WebLI dataset.\nWe also LiT-tune a new text encoder for the SigLIP image encoder in the exact same setting, to\nremove any confounders from the comparison. The SigLIP ViT-G model clearly outperforms the\nclassification pretrained larger ViT-e model.\nFinally, we perform linear probing of the representation in the few-shot setting following Dosovit-\nskiy et al. (2021); Zhai et al. (2022a) across the 8 different classification tasks also used in Zhai\net al. (2022a) and report the average performance. Here, we see that SigLIP lags behind, likely be-\ncause the representation is not pretrained in a way that supports linear separability, as was recently\nuncovered by Tschannen et al. (2023).\nTaken together, these results paint a clear picture: the best and largest classification pretrained image\nencoders appear (slightly) better when evaluated on standard classification tasks, however they are\nsignificantly worse than SigLIP pretrained image encoders for vision-language tasks.\n5\nMODEL FAIRNESS, BIASES, AND OTHER POTENTIAL ISSUES\nWe follow the evaluation protocol of Chen et al. (2023a) to assess the model fairness, biases, and\nother potential issues. First, we use the MIAP (Schumann et al., 2021) and FairFace (Karkkainen &\nJoo, 2021) datasets to generate captions and use the Perspective API (Lees et al., 2022) (threshold\n> 0.8) to measure toxicity and profanity among other potential issues. Table 7 (FairFace) and\nTable 8 (MIAP) summarize the results. Slices with < 20 examples are dropped from the analysis.\nOverall, we observe a low level of toxicity and profanity among others, across all slices. The results\nare comparable to those in PaLI-X (Chen et al., 2023a).\n8\nPreprint.\nTable 9: Detection error rate for \u201cperson\u201d in PaLI-3 using the subset of the MIAP dataset (Schumann\net al., 2021) that contain exactly a single individual in the image. PaLI-3 maintains a low error rate\nacross all subgroups. Skin tone follows the Monk Skin Tone Scale (Monk, 2019). Numbers inside\nsquare brackets correspond to the size of each bucket.\nSkin Tone\n1 [2]\n2 [871]\n3 [3008]\n4 [522]\n5 [184]\n6 [85]\n7 [54]\n8 [49]\n9 [6]\n10 [1]\n0.00%\n0.00%\n0.17%\n0.39%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nGender\nPredominantly Feminine [2437]\nPredominantly Masculine [3544]\n0.41%\n1.78%\nAge Bucket\n0-2 yrs [17]\n3-19 yrs [568]\n20-59 yrs [4925]\n> 60 yrs [247]\n0.00%\n0.18%\n1.30%\n0.82%\nactor\narchitect\nartist\nastronaut\nauthor\nbaker\nbartender\nbiologist\nbodybuilder\ncarpenter\nchef\nchemist\ncoach\ncomputer scientist\ncrew\ncustomer\ndancer\ndentist\ndispatcher\ndoctor\ndriver\neducator\nelectrician\nengineer\nfarmer\nfashion\nfirefighter\nflight attendant\nguard\nhairdresser\nhousekeeper\nhygienist\ninvestigator\njanitor\njournalist\njudge\nlawyer\nlibrarian\nmagician\nmanager\nmechanic\nminister\nnurse\nparamedic\nphotographer\npilot\nplumber\npolice officer\nprofessor\nreceptionist\nresearcher\nscientist\nsecretary\nsinger\nsoldier\nspokesperson\nsport\nstudent\ntailor\ntechnician\ntherapist\nveterinarian\nworker\nDP = +0.37\nDP = +0.95\nDP = -0.21\nFigure 2: Level of demographic parity (DP) in PaLI-3\u2019s output for CelebA images, comparing the\naverage log-perplexity between females and males. Values close to zero indicate absence of bias.\nThe dotted lines correspond to \u00b5 and \u00b5\u00b12\u03c3. DP for all occupations falls within the 95% confidence\ninterval, except \u201cbodybuilder\u201d and \u201cplumber\u201d which the model tends to strongly associate with men.\nSecond, we examine the level of demographic parity (Dwork et al., 2012) in the model itself. Fol-\nlowing (Chen et al., 2023a), we feed an image from the CelebA dataset (Liu et al., 2015) into PaLI-3\nwith the chosen occupation as a prefix and record the average log-perplexity score of the model\ngeneration. The demographic parity is the difference between average log-perplexity within the\ndemographic groups. Figure 2 summarizes the results: Similar to PaLI-X, PaLI-3 tends to assign\na higher log-perplexity score to women than men across most occupations with a mean difference\nof \u00b5 = 0.37. However, fewer occupations in PaLI-3 fall outside the interval \u00b5 \u00b1 2\u03c3 compared to\nPaLI-X.\nThird, we compare performance across all subgroups on a detection task using the MIAP dataset,\nfollowing again (Chen et al., 2023a). For images containing exactly a single individual, we query\nPaLI-3 with the question: \u201cIs there a person in this image?\u201d and evaluate the accuracy of its response.\nTable 9 summarizes the results. The error rate (false negatives) is very low across all subgroups.\nFor analysis of the WebLI dataset itself, such as the correlations between perceived gender and\noccupations, see Chen et al. (2023a).\nLimitations.\nThe limitations of this work are very similar to those already presented in the litera-\nture. We refer to (Chen et al., 2023a), which raises all limitations that apply to our work.\n6\nCONCLUSION\nIn this paper, we took a closer look at the pretraining of the image encoder in large VLMs, specifi-\ncally the PaLI type of model. By performing controlled experiments on that part, for the first time,\nwe clearly compare the two camps of classification pretraining and image-text (contrastive) pretrain-\ning, finding that the latter can lead to better and more efficient VLMs, especially for localization and\ntext understanding tasks. This is just one small aspect of VLMs, and we hope this study and result\nspurs to further detailed investigations of the many other aspects of training VLMs.\n9\nPreprint.\n7\nACKNOWLEDGEMENTS\nWe would like to thank Bo Pang, Michael Tschannen, Soravit Changpinyo, AJ Piergiovanni, Marvin\nRitter, Yi Tay, Paulina Pietrzyk, Matthias Minderer, Andr\u00e9 Susano Pinto, Mojtaba Seyedhosseini,\nAnelia Angelova, Neil Houlsby, Yiming Gu, Daniel Hernandez Diaz, Chun-Liang Li, Guolong Su,\nHao Zhang, Chenxia Wu, Rui Lin, Ting Yu, Weina Ge, Shengyang Dai, Tania Bedrax-Weiss, Rob\nFergus, Jeremiah Harmsen, and Joelle Barral for helpful discussions, feedback, and support. We\nthank the Google Research and Google DeepMind groups for providing a fruitful environment to\nconduct research.\nREFERENCES\nManoj Acharya, Kushal Kafle, and Christopher Kanan.\nTallyqa: Answering complex counting\nquestions. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-\nFirst Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI\nSymposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019, pp. 8076\u20138084. AAAI Press, 2019. doi: 10.1609/aaai.\nv33i01.33018076. URL https://doi.org/10.1609/aaai.v33i01.33018076.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\nLucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are\nwe done with ImageNet? arXiv preprint arXiv:2006.07159, 2020.\nAli Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\u00e7al Rusinol, Ernest Valveny,\nCV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 4291\u20134301, 2019.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa De-\nhghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar\nJoshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias\nMinderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien\nAmelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong\nXu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai,\nNeil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model,\n2023a.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebas-\ntian Alexander Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish\nThapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol\nAyan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. PaLI: A jointly-scaled multilingual language-image model. In ICLR, 2023b.\nZuozhuo Dai, Fangtao Shao, Qingkun Su, Zilong Dong, and Siyu Zhu. Fine-grained text-video\nretrieval with frozen image encoders. CoRR, abs/2307.09972, 2023. doi: 10.48550/arXiv.2307.\n09972. URL https://doi.org/10.48550/arXiv.2307.09972.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton,\nLucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Min-\nderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed,\nAravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick\nCollier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink,\nAlexander Kolesnikov, Filip Paveti\u00b4c, Dustin Tran, Thomas Kipf, Mario Lu\u02c7ci\u00b4c, Xiaohua Zhai,\nDaniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion\nparameters, 2023. URL https://arxiv.org/abs/2302.05442.\n10\nPreprint.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?\nid=YicbFdNTTy.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,\nPierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc\nToussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied\nmultimodal language model. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine\nLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of\nMachine Learning Research, pp. 8469\u20138488. PMLR, 2023. URL https://proceedings.\nmlr.press/v202/driess23a.html.\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness\nthrough awareness.\nIn Shafi Goldwasser (ed.), Innovations in Theoretical Computer Science\n2012, Cambridge, MA, USA, January 8-10, 2012, pp. 214\u2013226. ACM, 2012.\ndoi: 10.1145/\n2090236.2090255. URL https://doi.org/10.1145/2090236.2090255.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V\nin VQA matter: Elevating the role of image understanding in visual question answering.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904\u20136913,\n2017.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\nvolume 139 of Proceedings of Machine Learning Research, pp. 4904\u20134916. PMLR, 2021. URL\nhttp://proceedings.mlr.press/v139/jia21b.html.\nKimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender,\nand age for bias measurement and mitigation. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision, pp. 1548\u20131558, 2021.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descrip-\ntions. In CVPR, 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (bit): General visual representation learning. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020\n- 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V, volume\n12350 of Lecture Notes in Computer Science, pp. 491\u2013507. Springer, 2020.\ndoi: 10.1007/\n978-3-030-58558-7\\_29.\nURL https://doi.org/10.1007/978-3-030-58558-7_\n29.\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning\nevents in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2017.\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khan-\ndelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as\npretraining for visual language understanding. arXiv preprint arXiv:2210.03347, 2022.\nAlyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasser-\nman. A new generation of perspective API: Efficient multilingual character-level transformers.\narXiv preprint arXiv:2202.11176, 2022.\n11\nPreprint.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-\nimage pre-training with frozen image encoders and large language models. In Andreas Krause,\nEmma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett\n(eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,\nHawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 19730\u201319742.\nPMLR, 2023. URL https://proceedings.mlr.press/v202/li23q.html.\nMuchen Li and Leonid Sigal. Referring transformer: A one-step approach to multi-task visual\ngrounding. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and\nJennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-\n14, 2021, virtual, pp. 19652\u201319664, 2021. URL https://proceedings.neurips.cc/\npaper/2021/hash/a376802c0811f1b9088828288eb0d3f0-Abstract.html.\nYang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning:\nGenerating natural language description for mobile user interface elements. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5495\u2013\n5510, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.emnlp-main.443. URL https://aclanthology.org/2020.emnlp-main.443.\nJiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and\nR Manmatha.\nPolyformer: Referring image segmentation as sequential polygon generation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n18653\u201318663, 2023.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\nIn International Conference on Computer Vision, 2015.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual\nquestion answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf\nconference on computer vision and pattern recognition, pp. 3195\u20133204, 2019.\nAhmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for\nquestion answering about charts with visual and logical reasoning. In Findings of ACL, 2022.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document\nimages. In Proceedings of the IEEE/CVF winter conference on applications of computer vision,\npp. 2200\u20132209, 2021.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar.\nInfographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, pp. 1697\u20131706, 2022.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In ICDAR, 2019.\nMathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and\nAude Oliva. Spoken moments: Learning joint audio-visual representations from video descrip-\ntions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 14871\u201314881, 2021.\nEllis Monk. Monk skin tone scale, 2019. URL https://skintone.google.\nJia Ning, Chen Li, Zheng Zhang, Zigang Geng, Qi Dai, Kun He, and Han Hu. All in tokens:\nUnifying output space of visual tasks via soft token. arXiv preprint arXiv:2301.02229, 2023.\nQiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu Zhang, Zhengjie Huang, Teng Hu, We-\nichong Yin, Yongfeng Chen, Yin Zhang, et al. Ernie-layout: Layout knowledge enhanced pre-\ntraining for visually-rich document understanding. arXiv preprint arXiv:2210.06155, 2022.\nRafa\u0142 Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Micha\u0142 Pietruszka, and\nGabriela Pa\u0142ka. Going full-tilt boogie on document understanding with text-image-layout trans-\nformer. In Document Analysis and Recognition\u2013ICDAR 2021: 16th International Conference,\nLausanne, Switzerland, September 5\u201310, 2021, Proceedings, Part II 16, pp. 732\u2013747. Springer,\n2021.\n12\nPreprint.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever.\nLearning transferable visual models from natural language supervision.\nIn Ma-\nrina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Ma-\nchine Learning Research, pp. 8748\u20138763. PMLR, 2021. URL http://proceedings.mlr.\npress/v139/radford21a.html.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers\ngeneralize to ImageNet?\nIn International Conference on Machine Learning, pp. 5389\u20135400,\n2019.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-\nFei.\nImagenet large scale visual recognition challenge.\nCoRR, abs/1409.0575, 2014.\nURL\nhttp://arxiv.org/abs/1409.0575.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset\nof clip-filtered 400 million image-text pairs. CoRR, abs/2111.02114, 2021. URL https://\narxiv.org/abs/2111.02114.\nCandice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Rebecca Pantofaru.\nA step toward more inclusive people annotations for fairness. In Proceedings of the AAAI/ACM\nConference on AI, Ethics, and Society (AIES), 2021.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. TextCaps: a dataset for\nimage captioning with reading comprehension. In European conference on computer vision, pp.\n742\u2013758, 2020.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. Towards VQA models that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 8317\u20138326, 2019.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil\nHoulsby, and Donald Metzler. Ul2: Unifying language learning paradigms, 2023.\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A massively\nmultilingual multimodal evaluation dataset.\nIn Conference on Empirical Methods in Natural\nLanguage Processing, 2022.\nMichael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer.\nImage captioners are scalable vision learners too. CoRR, abs/2306.07915, 2023. doi: 10.48550/\narXiv.2306.07915. URL https://doi.org/10.48550/arXiv.2306.07915.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.\nURL http://arxiv.org/abs/1706.03762.\nBryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Au-\ntomatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium\non User Interface Software and Technology, UIST \u201921, pp. 498\u2013510, New York, NY, USA, 2021a.\nAssociation for Computing Machinery. ISBN 9781450386357. doi: 10.1145/3472749.3474765.\nURL https://doi.org/10.1145/3472749.3474765.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv\npreprint arXiv:2205.14100, 2022a.\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign\nlanguage: Beit pretraining for all vision and vision-language tasks.\nCoRR, abs/2208.10442,\n13\nPreprint.\n2022b. doi: 10.48550/arXiv.2208.10442. URL https://doi.org/10.48550/arXiv.\n2208.10442.\nXin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A\nlarge-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 4581\u20134591, 2019.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Sim-\nple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904,\n2021b.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-\nanswering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 9777\u20139786, 2021.\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.\nVideo question answering via gradually refined attention over appearance and motion. In Pro-\nceedings of the 25th ACM international conference on Multimedia, pp. 1645\u20131653, 2017.\nHaiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi,\nQi Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jingren Zhou.\nmplug-2: A modularized multi-modal foundation model across text, image and video. In An-\ndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 38728\u2013\n38748. PMLR, 2023. URL https://proceedings.mlr.press/v202/xu23s.html.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging\nvideo and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2016.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui\nWu.\nCoca:\nContrastive captioners are image-text foundation models.\narXiv preprint\narXiv:2205.01917, 2022.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context\nin referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 69\u201385. Springer, 2016.\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-\nqa: A dataset for understanding complex web videos via question answering. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 33, pp. 9127\u20139134, 2019.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n12104\u201312113, 2022a.\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov,\nand Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24,\n2022, pp. 18102\u201318112. IEEE, 2022b. doi: 10.1109/CVPR52688.2022.01759. URL https:\n//doi.org/10.1109/CVPR52688.2022.01759.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\nimage pre-training. In International Conference on Computer Vision, 2023.\nZijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin Zhu, Zehuan Yuan, and\nJing Liu. Chatbridge: Bridging modalities with large language model as a language catalyst.\nCoRR, abs/2305.16103, 2023. doi: 10.48550/arXiv.2305.16103. URL https://doi.org/\n10.48550/arXiv.2305.16103.\n14\nPreprint.\nA\nADDITIONAL RESULTS: VIDEO CAPTIONING AND QA\nA.1\nDATASETS & BENCHMARKS\nBecause not all videos in the benchmarks are available online at the time of experimentation when\nfreshly collected the data, the effective numbers of videos is smaller than the public official splits in\nsome cases. Table 10 reports the details numbers for the subset used in our training and evaluation.\nWe follows the same experimental settings used in PaLI-X (Chen et al., 2023a), including the dataset\nsplits configuration and the evaluation metrics. Please refer to PaLI-X (Chen et al., 2023a) for more\ndata-related details.\nMSR-VTT\nVATEX\nANet-Cap\nSMIT\nM-V-QA\n. ANet-QA\nNExT-QA\nOriginal size\nvalid.\n497\n3000\n17505\n14604\n12278\n18000\n5343\ntest\n2990\n6000\n17031\n3513\n72821\n8000\n9178\nDataset size\nvalid.\n325\n2646\n14566\n8096\n8160\n10000\n5343\ntest\n2135\n5242\n14197\n3513\n52623\n7040\n9178\n% Remaining\nvalid.\n65.39\n88.20\n83.21\n100.00\n66.46\n-\n100.00\ntest\n71.40\n87.37\n83.36\n100.00\n72.26\n88.00\n100.00\nTable 10: As we freshly collect the data sets, the actual amount of training data is smaller than the\npublic benchmarks, making the tasks more challenging. Except for NextQA and SMIT, there are\nmore than 10% of the videos missing in both training and evaluation.\nB\nADDITIONAL RESULTS: CROSSMODAL-3600 RETRIEVAL\nWe present more results of zero-shot image-text retrieval results (recall@1) on Crossmodal-\n3600 (Thapliyal et al., 2022) in this section. Detailed results across 36 languages for SigLIP ViT-G\nand Classif ViT-e are presented in Figure 3, Figure 4 and Table 11.\nar\nbn\ncs\nda\nde\nel\nen\nes\nfa\nfi\nfil\nfr\nhi\nhr\nhu\nid\nit\niw\nja\nko\nmi\nnl\nno\npl\npt quz ro\nru\nsv\nsw\nte\nth\ntr\nuk\nvi\nzh avg\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSigLIP ViT-G (PaLI-3)\nClassif ViT-e (PaLI-17B)\nFigure 3: Image-to-text zero-shot retrieval recall@1 on crossmodal-3600 for SigLIP ViT-G and\nClassif ViT-e.\nar\nbn\ncs\nda\nde\nel\nen\nes\nfa\nfi\nfil\nfr\nhi\nhr\nhu\nid\nit\niw\nja\nko\nmi\nnl\nno\npl\npt quz ro\nru\nsv\nsw\nte\nth\ntr\nuk\nvi\nzh avg\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSigLIP ViT-G (PaLI-3)\nClassif ViT-e (PaLI-17B)\nFigure 4: Text-to-image zero-shot retrieval recall@1 on crossmodal-3600 for SigLIP ViT-G and\nClassif ViT-e.\n15\nPreprint.\nLanguage\nImage-to-text\nText-to-image\nSigLIP ViT-G\nClassif ViT-e\nSigLIP ViT-G\nClassif ViT-e\nar\n62.22\n39.69\n50.33\n32.60\nbn\n22.67\n5.67\n13.61\n3.31\ncs\n61.69\n44.03\n50.10\n35.24\nda\n72.47\n50.75\n57.16\n38.48\nde\n79.28\n58.53\n64.20\n46.50\nel\n49.11\n29.03\n32.36\n20.92\nen\n52.64\n42.11\n47.89\n40.63\nes\n73.31\n55.22\n59.77\n46.55\nfa\n64.56\n44.50\n50.09\n35.58\nfi\n70.39\n32.64\n54.34\n21.80\nfil\n37.89\n15.53\n25.40\n10.04\nfr\n73.81\n52.61\n63.28\n43.47\nhi\n26.22\n13.14\n14.94\n7.42\nhr\n66.89\n38.31\n51.03\n29.55\nhu\n70.22\n44.67\n54.63\n35.49\nid\n73.53\n46.33\n58.62\n36.75\nit\n75.56\n54.53\n64.14\n44.76\niw\n59.14\n38.67\n44.10\n29.39\nja\n56.69\n35.47\n41.31\n27.24\nko\n61.03\n36.11\n45.50\n25.95\nmi\n0.64\n0.33\n0.30\n0.22\nnl\n67.25\n52.14\n56.23\n43.79\nno\n73.03\n49.17\n57.40\n37.35\npl\n70.69\n51.42\n58.24\n43.72\npt\n70.22\n49.19\n58.47\n42.73\nquz\n6.28\n4.31\n1.89\n1.90\nro\n63.92\n37.75\n48.20\n28.82\nru\n71.69\n50.64\n58.51\n41.11\nsv\n72.69\n53.22\n57.76\n40.66\nsw\n29.86\n6.42\n18.31\n3.41\nte\n7.81\n1.92\n3.39\n1.42\nth\n51.14\n22.00\n34.65\n16.06\ntr\n63.36\n39.50\n49.41\n31.47\nuk\n62.19\n39.53\n46.15\n30.81\nvi\n68.50\n27.08\n50.14\n21.28\nzh\n57.92\n33.61\n41.51\n28.24\navg\n56.85\n35.99\n43.98\n28.46\nTable 11: Crossmodal-3600 zero-shot retrieval recall@1 (%) for SigLIP ViT-G (2B params) and\nClassif ViT-e (4B params). SigLIP ViT-G is significantly better than Classif ViT-e across all the\nlanguages. SigLIP improves from 28.5% to 44.0% on average for the text-to-image retrieval task.\n16\n"
  },
  {
    "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models",
    "link": "https://arxiv.org/pdf/2310.08659.pdf",
    "upvote": "19",
    "text": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large\nLanguage Models\nYixiao Li\u2217\u2217, Yifan Yu\u2217\u2217, Chen Liang, Pengcheng He,\nNikos Karampatziakis, Weizhu Chen, Tuo Zhao \u2217\nNovember 29, 2023\nAbstract\nQuantization is an indispensable technique for serving Large Language Models (LLMs)\nand has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we\nfocus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-\ntrained model. In such cases it is common to observe a consistent gap in the performance on\ndownstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach.\nIn response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization\nframework that simultaneously quantizes an LLM and finds a proper low-rank initialization for\nLoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and\nfull-precision model and significantly improves generalization in downstream tasks. We evaluate\nour method on natural language understanding, question answering, summarization, and natural\nlanguage generation tasks. Experiments show that our method is highly effective and outperforms\nexisting quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision\nregimes. The code is available on https://github.com/yxli2123/LoftQ.\n1\nIntroduction\nThe advent of Pre-trained Language Models (PLMs) has marked a transformative shift in the field\nof Natural Language Processing (NLP), offering versatile solutions across various applications\n(He et al., 2021b; Lewis et al., 2019; Touvron et al., 2023). They have showcased unparalleled\nproficiency in executing a variety of language tasks, including Natural Language Understanding\n(NLU) and Natural Language Generation (NLG). These models typically have millions or even bil-\nlions of parameters, necessitating substantial computational and memory requirements. However,\nthe extensive computational and memory demands of these models pose significant challenges,\n\u2217Li, Yu, Liang and Zhao are affiliated with Georgia Tech. He, Karampatziakisand and Chen are affiliated with\nMicrosoft Azure. Correspondence to yixiaoli@gatech.edu, yyu429@gatech.edu and tourzhao@gatech.edu.\n**Equal contributions\n1\narXiv:2310.08659v4  [cs.CL]  28 Nov 2023\nespecially in real-world deployments where resources are often constrained and need to be shared\namong many users.\nTo mitigate the extensive storage requirements of pre-trained models, quantization serves as a\npivotal compression technique (Zafrir et al., 2019; Shen et al., 2020; Bai et al., 2022; Dettmers et al.,\n2022), converting high-precision numerical values into a discrete set of values. Typically, model\nparameters, originally stored in a 16-bit float format, are transformed into a 4-bit integer format\nthrough quantization, resulting in a substantial 75% reduction in storage overhead. Additionally, to\nfacilitate the adaptation of quantized pre-trained models to downstream tasks efficiently, Low-Rank\nAdaptation (LoRA) is a viable approach (Hu et al., 2021). This technique is a parameter-efficient\nfine-tuning method traditionally applied to high-precision pre-trained models. It is based on the\nhypothesis that the differences between fully fine-tuned weights and pre-trained weights exhibit\nlow-rank properties. This allows these differences to be represented using low-rank matrices. As a\nresult, the original pre-trained weights remain unaltered, with adaptations confined solely to these\nlow-rank matrices, enabling effective task adaptation.\nWhen quantizing pre-trained models, practitioners often concentrate primarily on the quantiza-\ntion technique, inadvertently neglecting the importance of subsequent LoRA fine-tuning (Dettmers\net al., 2023; Diao et al., 2023). For example, QLoRA inherits the fixup initialization (Zhang et al.,\n2019) used in LoRA, which (Dettmers et al., 2023) attaches zero initialized low-rank adapters (see\nSection 2.3) to the quantized pre-trained model. The inevitable discrepancy introduced by quanti-\nzation during the approximation of the original high-precision numbers, a scenario particularly\npronounced in low-bit situations such as the 2-bit regime, can adversely impact the initialization of\nLoRA fine-tuning. As illustrated in Figure 1a, the quantized pre-trained model obtained by QLoRA\nexhibits severe degradation below the 3-bit level. This deviation in initialization often results in an\ninferior fine-tuning performance. As illustrated in Figure 1b, the fine-tuning performance drops as\nthe quantization bit decreases when applying QLoRA. Moreover, it is noteworthy that QLoRA fails\nbelow the 3-bit level.\nIn this paper, we introduce a novel quantization framework, called LoRA-Fine-Tuning-aware\nQuantization (LoftQ). It is designed specifically for pre-trained models that require quantization\nand LoRA fine-tuning. This framework actively integrates low-rank approximation, working in\ntandem with quantization to jointly approximate the original high-precision pre-trained weights.\nThis synergy significantly enhances alignment with the original pre-trained weights as illustrated in\nFigure 2. Consequently, our method provides an advantageous initialization point for subsequent\nLoRA fine-tuning, leading to improvements in downstream tasks.\nWe evaluate our quantization framework by conducting extensive experiments on downstream\ntasks, such as NLU, question answering, summarization, and NLG. Experiments show that LoftQ\nconsistently outperforms QLoRA across all precision levels. For instance, with 4-bit quantization,\nwe achieve a 1.1 and 0.8 gain in Rouge-1 for XSum (Narayan et al., 2018) and CNN/DailyMail\n(Hermann et al., 2015), respectively. LoftQ excels particularly in low-bit scenarios and works\n2\n16\n8\n4\n3\n2.5\n2.25\n2\nNumber of Bits\n2\n4\n6\n8\n10\n12\nLog of Perplexity\n2.49\n2.50\n2.53\n2.53\n11.37\n11.48\n11.50\n(a) Pre-trained LLAMA-2-13b on WikiText-2\n16\n8\n4\n3\n2.5\n2.25\n2\nNumber of Bits\n0\n2\n4\n6\nLog of Perplexity\n1.63\n1.64\n1.65\n1.65\n2.99\n6.80\n7.19\n(b) Fine-tuned LLAMA-2-13b on WikiText-2\nFigure 1: QLoRA performance with different bits. Left: QLoRA initialization of LLAMA-2-13b\non WikiText-2. Right: Apply QLoRA to LLAMA-2-13b on WikiText-2 language modeling task.\nSmaller perplexity indicates better performance.\neffectively with different quantization methods. For example, we achieve over an 8% gain on MNLI\n(Wang et al., 2019) and more than 10% on SQuADv1.1 (Rajpurkar et al., 2016) with both 2-bit\nNormalFloat and the 2-bit uniform quantization. We have not seen our approach performs worse\nthan QLoRA.\nUniform\n4bit\nNormalFloat\n4bit\nUniform\n2bit\nNormalFloat\n2bit\n0\n2\n4\n6\n8\n10\n12\n14\nDiscrepancy\nLoftQ\nQLoRA\n(a) Spectral norm of the initialization difference\nUniform\n4bit\nNormalFloat\n4bit\nUniform\n2bit\nNormalFloat\n2bit\n0\n10\n20\n30\n40\n50\n60\nDiscrepancy\nLoftQ\nQLoRA\n(b) Frobenius norm of the initialization difference\nFigure 2: Initialization discrepancy between the LoRA initialization and the original pre-trained\nweight matrix, described by the spectral norm and Frobenius norm of the difference. The weight\nmatrix in the above figures is randomly selected in BART-large. The initialization is obtained by\nQLoRA and LoftQ, with Uniform and NormalFloat quantization methods applied at both 2-bit and\n4-bit levels. LoftQ successfully mitigates the discrepancy, especially at the 2-bit level.\n3\n2\nBackground\n2.1\nTransformer Models\nA transformer model contains a sequence of layers, where each layer consists of two sub-layers: a\nmulti-head self-attention (MHA) and a fully connected feed forward network (FFN) (Vaswani et al.,\n2017). Given the input X \u2208 Rn\u00d7d, where n is the sequence length and d is the hidden dimension of\nthe model, MHA computes the h attention heads in parallel:\nMHA(X) = Concat(head1,...,headh)Wo,\nwhere headi =Softmax(XWqi(XWki)\u22a4/\np\ndh)XWvi for\ni = 1,...,h,\nwhere Wqi,Wki,Wvi \u2208 Rd\u00d7dh are query, key, and value matrices, Wo \u2208 Rd\u00d7d is the output matrix, and\ndh = d/h. FFN comprises two linear transformations and an activation function, and is defined as\nFFN(X) = \u03c3(XWf1 + b1)Wf2 + b2, where Wf1 \u2208 Rd\u00d7dm, Wf2 \u2208 Rdm\u00d7d, and \u03c3(\u00b7) is the activation function.\nA residual connection is used and followed by layer normalization.\n2.2\nQuantization\nQuantization. Given a high-precision number, e.g., such as 32-bit floating point number, XHP \u2208 R,\nN-bit quantization encodes it to an integer XINT \u2208 {0,1,...,2N \u2212 1}. This process can be expressed as\nXINT = round\n\u0010\n(2N \u2212 1)F\n\u0010\nXHP\u0011\u0011\n,\n(1)\nwhere F(\u00b7): R 7\u2192 [0,1] is a normalization function. Uniform quantization assumes F(X) = (X \u2212\nXmin)/(Xmax \u2212 Xmin). Dettmers et al. (2023) proposes 4-bit NormalFloat Quantization (NF4). It\nassumes X \u223c N (0,\u03c32) and hence F(X) = \u03a6(X/\u03c3), where \u03a6(\u00b7) is the cumulative distribution function\nof the standard normal distribution.\nDequantization. A lookup table T , where\nT [i] = F\u22121 \u0012\ni\n2N \u2212 1\n\u0013\n,i = 0,1,...,2N \u2212 1,\n(2)\nis used to decode the integer XINT to its simulated high-precision counterpart XD \u2208 R. Therefore,\nthe dequantization can be expressed as\nXD = T [XINT].\n(3)\nSimulated Quantization for Matrices. While it is possible to perform multiplication directly\nbetween quantized representations, it is common to apply simulated quantization for matrices (Bai\net al., 2020; Shen et al., 2020). There, quantized weight matrices are stored as encoded integers\nin memory, and are temporarily dequantized to simulated high-precision matrices by the lookup\ntable when engaged in multiplication operations. In simulated quantization, it is only necessary to\nanalyze the map from a high-precision matrix to a simulated high-precision matrix. We denote\nthis end-to-end process by qN(\u00b7): Rm\u00d7n 7\u2192 Rm\u00d7n\nN\n, where RN : {T [i] \u2208 R|0 \u2264 i < 2N}.\n4\n2.3\nLow-Rank Adaptation\nLoRA (Hu et al., 2021) updates two small weight matrices A and B that are attached to a frozen\npre-trained weight matrix W. Hence, a linear transformation, Y = XW, is reformulated as\nY = XW + XAB\u22a4,\n(4)\nwhere X \u2208 Rn\u00d7d1,W \u2208 Rd1\u00d7d2,A \u2208 Rd1\u00d7r,B \u2208 Rd2\u00d7r, and r \u226a min{d1,d2}. Initially,\nA \u223c N (0,\u03c32), B = 0,\n(5)\nso as to align to the pre-trained weights. During the fine-tuning, W is fixed while A and B are\nupdated by some SGD-type optimization method.\nIt is worth noting that if low-rank adapters A and B are attached to a quantized backbone\nQ = qN(W) and are initialized by (5), the starting weight Q + AB\u22a4 is no longer equal to the pre-\ntrained weight W due to the discrepancy introduced by the quantization.\n3\nMethod\nWe propose LoRA-Fine-Tuning-aware Quantization (LoftQ), a quantization framework for LLMs.\nIt alternatively applies quantization and low-rank approximation to approximate original pre-\ntrained weights. This quantization framework provides a promising initialization for LoRA fine-\ntuning, which alleviates the quantization discrepancy in QLoRA and improves generalization in\ndownstream tasks significantly.\n3.1\nLoRA-Aware Quantization\nWe use an N-bit quantized weight Q \u2208 Rd1\u00d7d2\nN\nand low-rank approximations A \u2208 Rd1\u00d7r,B \u2208 Rd2\u00d7r\nto approximate the original high-precision pre-trained weight W \u2208 Rd1\u00d7d2 as the initialization of\nLoRA fine-tuning. Specifically, before fine-tuning, we initialize the network by minimizing the\nfollowing objective:\nmin\nQ,A,B\n\r\r\rW \u2212 Q \u2212 AB\u22a4\r\r\rF ,\n(6)\nwhere \u2225\u00b7\u2225F denotes the Frobenious norm. This objective in (6) takes LoRA fine-tuning into consider-\nation by jointly optimizing the initial values of the quantized backbone Q and low-rank adapters\nA,B. Contrarily, practitioners typically convert the pre-trained weight W into a quantized weight\nQ outright, neglecting the subsequent LoRA fine-tuning process. This oversight leads to notable\nperformance degradation in downstream tasks arising from the quantization discrepancy.\n5\n3.2\nAlternating Optimization\nWe solve the minimization problem in (6) by alternating between quantization and singular value\ndecomposition (SVD). To begin with, we set A0, and B0 equal to 0.\nQuantization. At the t-th step, we quantize the difference between the original pre-trained weight\nW and the low-rank approximation At\u22121B\u22a4\nt\u22121 from the last step to obtain the quantized weight Qt\nby\nQt = qN(W \u2212 At\u22121B\u22a4\nt\u22121),\n(7)\nwhere qN(\u00b7) maps a high-precision weight matrix to a quantized matrix.\nWe remark that our algorithm is compatible with different quantization functions qN(\u00b7). We\napply NF4 and the uniform quantization in Section 4 as examples. We also remark that Qt is\nnot an exact solution of the minimization in (6), given the fixed At\u22121B\u22a4\nt\u22121, but it is an efficient\napproximation.\nSVD. After obtaining the t-th quantized weight Qt, SVD is applied to the residual of the quantiza-\ntion denoted by Rt = W \u2212 Qt by\nRt =\nd\nX\ni=1\n\u03c3t,iut,iv\u22a4\nt,i,\n(8)\nwhere d = min{d1,d2}, \u03c3t,1 \u2265 \u03c3t,2 \u2265 ... \u2265 \u03c3t,d are the singular values of Rt, ut,i\u2019s and vt,i\u2019s are the\nassociated left and right singular vectors of Rt. We then obtain a rank-r approximation of Rt by\nAtB\u22a4\nt , where\nAt = [\u221a\u03c3t,1ut,1,...,\u221a\u03c3t,rut,r],\nBt = [\u221a\u03c3t,1vt,1,...,\u221a\u03c3t,rvt,r].\n(9)\nWe summarize our method in Algorithm 1. It is worth noting that T = 1 is a special case\nwhere Q1 is the exact quantized weight obtained by QLoRA, and low-rank approximations A1,B1\nare obtained by the SVD of the quantization residual W \u2212 Q1. T = 1 is sufficient to mitigate the\nquantization discrepancy, and alternating optimization helps to find a closer initialization to the\npre-trained weight W, which further improves the performance (see Section 3).\nWe remark that the computational cost of LoftQ is negligible because it is applied to individual\nweight matrices and therefore can be executed in parallel. We also remark one can apply LoftQ only\nonce to a pre-trained model and reuse the initialization obtained by LoftQ for different downstream\ntasks.\n3.3\nApplying to LoRA Fine-tuning\nWe store the QT \u2208 Rd1\u00d7d2\nN\nobtained by LoftQ using an integer matrix M by (1) and a lookup table T\nby (2). We initialize the backbone with the integer matrix M and initialize the low-rank adapters\nwith AT ,BT obtained by LoftQ.\n6\nAlgorithm 1 LoftQ\ninput Pre-trained weight W, target rank r, N-bit quantization function qN(\u00b7), alternating step T\n1: Initialize A0 \u2190 0,B0 \u2190 0\n2: for t = 1 to T do\n3:\nObtain quantized weight Qt \u2190 qN(W \u2212 At\u22121B\u22a4\nt\u22121)\n4:\nObtain low-rank approximation At,Bt \u2190 SVD(W \u2212 Qt) by (9)\n5: end for\noutput QT ,AT ,BT\nDuring LoRA fine-tuning, we freeze the integer weight M and optimize the low-rank adapters\nwith an efficient optimization algorithm, e.g., AdamW (Loshchilov and Hutter, 2017). In forward\npropagation, the integer weight M is temporarily dequantized to the simulated high-precision\nweight QT by its lookup table, as described in (3). In back propagation, gradients and optimizer\nstate are only related to low-rank adapters A,B, which reduces considerable training cost.\n4\nExperiments\nWe evaluate our method on NLU and NLG tasks. We apply LoftQ for quantizing DeBERTaV3-base\n(He et al., 2021b), BART-large (Lewis et al., 2019), and LLAMA-2 series (Touvron et al., 2023).\nImplementation Details. Following the prior works of LoRA variants (Zhang et al., 2023; He\net al., 2021a), we freeze all the backbone weight matrices and add low-rank adapters to weight\nmatrices in MHA and FFN of all layers. We quantize the weight matrices that are attached by\nlow-rank adapters. All the quantized models and adapters used in this paper are available on\nhttps://huggingface.co/LoftQ. Our implementation is based on publicly available Huggingface\nTransformers code-base (Paszke et al., 2019). All the experiments are conducted on NVIDIA A100\nGPUs.\nQuantization Methods. We apply two quantization methods to demonstrate LoftQ is compatible\nwith different quantization functions:\n\u2022 Uniform quantization is a classic quantization method. It uniformly divides a continuous\ninterval into 2N categories and stores a local maximum absolute value for dequantization.\n\u2022 NF4 and its 2-bit variant NF2 are quantization methods used in QLoRA (Dettmers et al.,\n2023). They assume that the high-precision values are drawn from a Gaussian distribution\nand map these values to discrete slots that have equal probability.\nWe perform 2-bit and 4-bit quantization on all models, achieving compression ratios of 25-30% and\n15-20% at the 4-bit and 2-bit levels, respectively. The compression ratios and trainable parameter\nratios for all models are detailed in the Appendix A.\nBaselines. We compare LoftQ with the following baseline methods:\n7\n\u2022 Full fine-tuning is the most common approach for adapting a pre-trained model to downstream\ntasks. The model is initialized with pre-trained weights and all parameters are updated\nthrough an SGD-type optimization method.\n\u2022 Full precision LoRA (LoRA) is a lightweight method for task adaptation, where it stores the\nbackbone using 16-bit numbers and optimizes the low-rank adaptors only. The adaptors are\napplied to the same matrices as in LoftQ.\n\u2022 QLoRA is similar to LoRA except the backbone is quantized into low-bit regime. The low-rank\nadapters are initialized using (5) and are applied to the same matrices as in LoftQ.\n4.1\nEncoder-only Model: DeBERTaV3\nModels and Datasets. We quantize the DeBERTaV3-base (He et al., 2021b) with LoftQ, then\nfinetune and evaluate the model on the General Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019), SQuADv1.1 (Rajpurkar et al., 2016), and ANLI (Nie et al., 2019).\nThe specific tasks of GLUE are given in Appendix C. Following previous works (Zhang et al., 2023),\nwe exclude WNLI in the experiments.\nImplementation Details. We select the learning rates from {1 \u00d7 10\u22125,5 \u00d7 10\u22125,1 \u00d7 10\u22124 5 \u00d7 10\u22124}.\nWe quantize the entire backbone. Given that GLUE, SQuADv1.1, and ANLI are relatively easy\nNLU tasks, we also quantize the embedding layer for higher compression efficiency. We apply the\nNormalFloat and the uniform quantization for LoftQ and QLoRA at both 2-bit and 4-bit levels.\nWe use rank 16 and 32 for low-rank adapters. More implementation details, such as the training\nepochs and batch sizes, are presented in Appendix D.2.\nMain Results. Table 1 and Table 2 summarize the results for 2-bit quantization on the GLUE,\nSQuADv1.1, and ANLI datasets, by NF2 and the uniform quantization, respectively. Our method\nconsistently outperforms QLoRA on all settings with respect to different ranks, quantization\nmethods, and datasets. When using the uniform quantization (Table 2), our method achieves 88.0%\naccuracy on MNLI-m, surpassing the QLoRA baseline by 8%. For tasks like SST and SQuADv1.1,\nour method even approaches the full fine-tuning performance at 2-bit level. The 4-bit quantization\nexperiment results are presented in Appendix D.1 as both LoftQ and QLoRA achieve performance\nclose to full fine-tuning.\nOur method is also more stable compared to QLoRA in the low-bit regime. For instance, while\nQLoRA fails to converge on CoLA for both quantization methods and ranks, LoftQ converges in\nall cases and achieves a score of 60.5 using uniform quantization at rank 32. LoftQ stands out in\nits ability to consistently attain robust and improved performance by effectively preserving the\nstarting point of pre-trained weights.\n8\nTable 1: Results with 2-bit LoftQ of DeBERTaV3-base models on GLUE development set,\nSQuADv1.1 development set, ANLI test set using NF2 quantization. We report the median\nover four seeds. N.A. indicates the model does not converge. The best results on each dataset are\nshown in bold.\nRank\nMethod\nMNLI\nQNLI\nRTE\nSST\nMRPC\nCoLA\nQQP\nSTSB\nSQuAD\nANLI\nm / mm\nAcc\nAcc\nAcc\nAcc\nMatt\nAcc\nP/S Corr\nEM/F1\nAcc\n-\nFull FT\n90.5/90.6\n94.0\n82.0\n95.3\n89.5/93.3\n69.2\n92.4/89.8\n91.6/91.1\n88.5/92.8\n59.8\n16\nLoRA\n90.4/90.5\n94.6\n85.1\n95.1\n89.9/93.6\n69.9\n92.0/89.4\n91.7/91.1\n87.3/93.1\n60.2\n16\nQLoRA\n75.4/75.6\n82.4\n55.9\n86.5\n73.8/82.8\nN.A.\n86.8/82.3\n83.0/82.8\n61.5 / 71.2\nN.A.\nLoftQ\n84.7/85.1\n86.6\n61.4\n90.2\n83.8/88.6\n37.4\n90.3/86.9\n87.1/86.9\n81.5/88.6\n47.1\n32\nQLoRA\n78.5/78.7\n80.4\n56.7\n86.9\n73.8/82.7\nN.A.\n87.1/82.7\n83.6/83.3\n64.6/73.8\nN.A.\nLoftQ\n86.0/86.1\n89.9\n61.7\n92.0\n83.6/87.2\n47.5\n91.0/87.9\n87.5/87.0\n82.9/89.8\n49.0\nTable 2: Results with 2-bit LoftQ of DeBERTaV3-base models on GLUE development set,\nSQuADv1.1 development set using Uniform quantization . We report the median over four\nseeds. N.A. indicates the model does not converge. The best results on each task are shown in bold.\nRank\nMethod\nMNLI\nQNLI\nRTE\nSST\nMRPC\nCoLA\nQQP\nSTSB\nSQuAD\nm / mm\nAcc\nAcc\nAcc\nAcc\nMatt\nAcc\nP/S Corr\nEm/F1\n-\nFull FT\n90.5/90.6\n94.0\n82.0\n95.3\n89.5/93.3\n69.2\n92.4/89.8\n91.6/91.1\n88.5/92.8\n16\nLoRA\n90.4/90.5\n94.6\n85.1\n95.1\n89.9/93.6\n69.9\n92.0/89.4\n91.7/91.1\n87.3/93.1\n16\nQLoRA\n76.5/76.3\n83.8\n56.7\n86.6\n75.7/84.7\nN.A.\n87.1/82.6\n83.5/83.4\n69.5/77.6\nLoftQ\n87.3/87.1\n90.6\n61.1\n94.0\n87.0/90.6\n59.1\n90.9/88.0\n87.9/87.6\n84.4/91.2\n32\nQLoRA\n79.9/79.5\n83.7\n57.8\n86.9\n76.5/84.5\nN.A.\n88.6/84.7\n84.1/84.0\n71.6/80.2\nLoftQ\n88.0/88.1\n92.2\n63.2\n94.7\n87.5/91.2\n60.5\n91.3/88.3\n89.5/89.2\n85.2/91.6\n4.2\nEncoder-Decoder Model: BART\nModels and Datasets. We quantize BART-large model (Lewis et al., 2020) with LoftQ, then finetune\nand evaluate the model on two commonly used summarization datasets: XSum (Narayan et al.,\n2018) and CNN/DailyMail(Hermann et al., 2015).\nImplementation Details. We apply LoftQ to weight matrices in MHA and FFN of both encoder\nand decoder layers. We report ROUGE 1/2/L scores, which are the metrics for summarization tasks\n(Lin, 2004). We conduct quantization experiments in both 2-bit and 4-bit scenarios. We experiment\nwith both NormalFloat and the uniform quantization in both 2-bit and 4-bit scenarios. In each\nprecision, we choose rank equal to 8 and 16 for a fair comparison with the full precision LoRA\nbaseline (Zhang et al., 2023). Please see Appendix E for detailed configurations.\nMain Results. Table 3 summarizes our 4-bit quantization experiment results on the XSum and\nCNN/DailyMail test sets. Our method consistently outperforms QLoRA at both ranks on both\n9\ndatasets. It even surpasses full precision LoRA at both ranks on Xsum. We will discuss this unex-\npected results in Section 5. The 2-bit quantization results are shown in Table 4. Our observation\nis consistent with the NLU experiments, that LoftQ demonstrates the convergence to reasonable\nresults, while QLoRA does not converge. This indicates our method is robuster by narrowing the\ninitialization gap.\nTable 3: Results with 4-bit LoftQ of BART-large on XSum and CNN/DailyMail. We report ROUGE-\n1/2/L, the higher the better. Lead-3 means choosing the first 3 sentences as the summary. N.A.\nindicates the model does not converge. Full FT refers to the full fine-tuning where all parameters\nare tuned. We report the median over five seeds.\nQuantization\nRank\nMethod\nXSum\nCNN/DailyMail\nFull Precision\n-\nLead-3\n16.30/1.60/11.95\n40.42/17.62/36.67\nFull FT\n45.14/22.27/37.25\n44.16/21.28/40.90\n8\nLoRA\n43.40/20.20/35.20\n44.72/21.58/41.84\n16\nLoRA\n43.95/20.72/35.68\n45.03/21.84/42.15\nNF4\n8\nQLoRA\n42.91/19.72/34.82\n43.10/20.22/40.06\nLoftQ\n44.08/20.72/35.89\n43.81/20.95/40.84\n16\nQLoRA\n43.29/20.05/35.15\n43.42/20.62/40.44\nLoftQ\n44.51/21.14/36.18\n43.96/21.06/40.96\nUniform\n8\nQLoRA\n41.84/18.71/33.74\nN.A.\nLoftQ\n43.86/20.51/35.69\n43.73/20.91/40.77\n16\nQLoRA\n42.45/19.36/34.38\n43.00/20.19/40.02\nLoftQ\n44.29/20.90/36.00\n43.87/20.99/40.92\nTable 4: Results with 2-bit LoftQ of BART-large on XSum and CNN/DailyMail using NF2 quan-\ntization. N.A. indicates the model does not converge. We report ROUGE-1/2/L, the higher the\nbetter. We report the median over five seeds.\nRank\nMethod\nXSum\nCNN/DailyMail\n8\nQLoRA\nN.A.\nN.A.\nLoftQ\n39.63/16.65/31.62\n42.24/19.44/29.04\n16\nQLoRA\nN.A.\nN.A.\nLoftQ\n40.81/17.85/32.80\n42.52/19.81/39.51\n10\n4.3\nDecoder-only Model: LLAMA-2\nModels and Datasets. We quantize LLAMA-2-7b and LLAMA-2-13b (Touvron et al., 2023) with\nLoftQ. We then fine-tune and evaluate the models on two NLG datasets: GSM8K (Cobbe et al.,\n2021) and WikiText-2 (Merity et al., 2016). Please see Appendix F for more details about the\ndatasets.\nImplementation Details. Similarly, we apply LoftQ to weight matrices in MHA and FFN of all\nlayers. In WikiText-2 evaluation, we report perplexity. In GSM8K evaluation, we extract numerical\nanswers in the generated solutions and then calculate the accuracy using those numerical answers.\nWe conduct experiments with both NF2 and NF4. Please see Appendix F for detailed configurations.\nMain Results. Table 5 presents a summary of our experiments on LLAMA-2-7b and LLAMA-\n2-13b using 2-bit, 4-bit, and mixed-precision NormalFloat quantization methods on WikiText-2\nand GSM8K datasets. In WikiText-2, our method consistently outperforms QLoRA across all\nquantization precision settings on both models. When dealing with the challenging 2-bit precision,\nwhere QLoRA fails to converge, LoftQ manages to achieve a perplexity of 7.85. In GSM8K, our\nmethod achieves better or on par performance compared to QLoRA across different model sizes\nand quantization precision levels. For example, our method achieves 20.9% accuracy using 2-bit\nprecision, where QLoRA doesn\u2019t converge.\nWe find LoftQ outperforms full precision LoRA in GSM8K with LLAMA-2-13b. One possible\nexplanation is that the lack of regularization causes overfitting on full precision LoRA fine-tuning.\nTherefore, we conduct full precision LoRA with weight decay on GSM8K. From Table 5, regulariza-\ntion helps LLAMA-2-13b full precision LoRA fine-tuning, but fails in LLAMA-2-7b. This indicates\nLLAMA-2-13b is prone to overfitting and quantization has implicit regularization to overcome\nsuch overfitting.\nTo provide a customized trade-off between the performance and precision, we also explore\nmixed-precision quantization where matrices in the first 4 layers are quantized using 4 bits, and the\nrest matrices remain 2 bits. We witness a remarkable 5.9% accuracy boost on the GSM8K dataset\nusing LLAMA-2-7b and a 12.7% boost using LLAMA-2-13b. This result underscores the potential\nof LoftQ for complex mixed-precision quantization scenarios.\n4.4\nAnalysis\nEffectiveness of Alternating Optimization. We conduct experiments with different alternating\nstep T to verify the effectiveness of the alternating optimization and to find the best value T as a\nhyperparameter for different models. Across all tasks and models, we observed that alternating\noptimization yields substantial improvements even with a minimal alternating step. This suggests\nthat it rapidly narrows the discrepancy between quantized weights and pre-trained weights,\nmaking our method easy to apply. For example, our method achieves 88.0% accuracy on MNLI-m\ndataset using only 5 alternating steps and 21.14 Rouge-2 score using only 1 step. Interestingly, we\n11\nTable 5: Results of LoftQ using NormalFloat for LLAMA-2 series on WikiText-2 and GSM8K.\n3/2.5/2.25-bit indicates mixed-precision quantization: 4-bit precision for the first 16/8/4 layers\nand 2-bit precision for the rest of layers. We report the perplexity (the smaller the better) for\nWikiText-2 and accuracy for GSM8K. The rank of low-rank adapters is 64. N.A. indicates the model\ndoes not converge. We report the median over five random seeds.\nMethod\nBit\nLLAMA-2-7b\nLLAMA-2-13b\nWikiText-2\u2193\nGSM8K\u2191\nWikiText-2\u2193\nGSM8K\u2191\nLoRA\n16\n5.08\n36.9\n5.12\n43.1\nLoRA+Reg\n16\n\u2013\n34.4\n\u2013\n45.3\nQLoRA\n4\n5.70\n35.1\n5.22\n39.9\nLoftQ\n4\n5.24\n35.0\n5.16\n45.0\nQLoRA\n3\n5.73\n32.1\n5.22\n40.7\nLoftQ\n3\n5.63\n32.9\n5.13\n44.4\nQLoRA\n2.5\nN.A.\nN.A.\n19.39\nN.A.\nLoftQ\n2.5\n5.78\n31.1\n5.22\n41.1\nQLoRA\n2.25\nN.A.\nN.A.\nN.A.\nN.A.\nLoftQ\n2.25\n6.13\n26.5\n5.45\n38.1\nQLoRA\n2\nN.A\nN.A.\nN.A.\nN.A.\nLoftQ\n2\n7.85\n20.9\n7.69\n25.4\nnoticed that increasing the alternating step beyond a certain point tends to result in diminishing\nreturns. We suspect this phenomenon occurs because, as the gap becomes smaller, it becomes\nmore challenging for alternating optimization to consistently minimize the gap at each step.\nThis challenge emerges because of the inherent errors introduced by the quantization method.\nNevertheless, results from Figure 3 indicate our method is not sensitive to the alternating step T\nand is able to consistently enhance downstream fine-tuning performance.\n5\nDiscussion\nStart with quantization or SVD in the alternating optimization? An alternative algorithm\nto the alternating optimization is that we first obtain the low-rank approximation At,Bt and\nthen obtain the quantized weight Qt by switching Line 3 and Line 4 in Algorithm 1. We note\nthis is a valid alternative method as both still jointly minimize the objective in (6). Table 6\nsummarizes the performance of this alternative method. It is noteworthy that the alternative\nmethod still outperforms QLoRA significantly, even though it is worse than the primary version.\nThis observation underscores the potential for performance improvement by achieving a closer\n12\n0\n1\n5\n10\nAlternating Step T\n75\n80\n85\n88\n90\nAccuracy\n79.9\n86.6\n88.0\n87.7\n(a) MNLI\n20.0\n22.5\n25.0\n27.0\n22.5\n25.2\n25.5\n0\n1\n5\n10\nAlternating Step T\n0\n1\n1.2\nAccuracy\n(b) GSM8k\n0\n1\n5\n10\nAlternating Step T\n19.0\n20.0\n21.0\n21.5\nROUGE-2\n20.05\n21.14\n21.09\n20.83\n(c) XSum\nFigure 3: Comparison of different alternating step T used in LoftQ. T = 0 indicates we use QLoRA\nmethod that initializes low-rank adapters by (5). T = 1,5,10 indicates we use different T for LoftQ\ndescribed in Algorithm 1. Left: Uniform 2-bit DeBERTaV3-base. Middle: NF4 2-bit LLAMA-2-13b.\nRight: NF4 BART-large.\napproximation of pre-trained weights within the low-precision regime.\nTable 6: Results of 2-bit uniformly quantized DeBERTaV3-base on part of GLUE. LoftQ(SVD First)\nindicates the alternative LoftQ that swiches Line 3 and Line 4 in Algorithm 1. We report the\nmedian over four random seeds. The best results on each task are shown in bold.\nMethod\nRank\nMNLI\nQNLI\nSST2\nm / mm\nAcc\nAcc\nFull FT\n-\n90.5/90.6\n94.0\n95.3\nQLoRA\n32\n79.9/79.5\n83.8\n86.6\nLoftQ(SVD First)\n32\n87.8/87.7\n84.9\n89.7\nLoftQ(Quantiztion First)\n32\n88.0/88.1\n92.2\n94.7\n6\nRelated Work\nQuantization-Aware Training (QAT) is often used to obtain quantized models that are adapted\nin downstream tasks (Peri et al., 2020; Liu et al., 2023). It involves quantization and full model\nfine-tuning at the same time. However, QAT requires massive training cost, such as the gradient\nand optimization state. Moreover, it is difficult to compute the gradient of quantized weights. Our\nmethod, with the help of LoRA, sidesteps the aforementioned issues, providing a light approach\nfor downstream task adaptation.\nPost-Training Quantization (PTQ) is a category of popular quantization frameworks (Frantar et al.,\n2022; Xiao et al., 2023), which can also be used for task adaptation. It calibrates the high-precision\n13\nmodel with a small subset of the training dataset. Therefore, the subsequent quantization is guided\nby the training dataset, providing task-specific quantized models. Besides, it does not involve\nany gradient backpropagation, so it is cost-efficient. However, it usually results in lower accuracy\ncompared to QAT.\n7\nConclusion\nWe propose LoftQ, a quantization framework for LLMs, which alternatively applies quantiza-\ntion and low-rank approximation to the original high-precision pre-trained weights, to obtain\nan initialization for the subsequent LoRA fine-tuning. Experiments on natural language under-\nstanding, question answering, summarization, and natural language generation show that our\nframework remarkably surpasses existing methods, e.g., QLoRA, for quantizing encoder-only,\nencoder-decoder, and decoder-only models. We have not observed our method exhibiting worse\nperformance over QLoRA. Moreover, our quantization framework demonstrates effectiveness and\nrobustness particularly in low-bit quantization regimes, e.g., the 2-bit level.\nReferences\nBai, H., Hou, L., Shang, L., Jiang, X., King, I. and Lyu, M. R. (2022). Towards efficient post-training\nquantization of pre-trained language models. Advances in Neural Information Processing Systems,\n35 1405\u20131418.\nBai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., Liu, Q., Lyu, M. and King, I. (2020).\nBinarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701.\nBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B. and Szpektor, I.\n(2006). The second pascal recognising textual entailment challenge.\nBentivogli, L., Clark, P., Dagan, I. and Giampiccolo, D. (2009). The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I. and Specia, L. (2017). SemEval-2017 task 1: Semantic\ntextual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th\nInternational Workshop on Semantic Evaluation (SemEval-2017). Association for Computational\nLinguistics, Vancouver, Canada.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J.,\nHilton, J., Nakano, R. et al. (2021). Training verifiers to solve math word problems. arXiv\npreprint arXiv:2110.14168.\nDagan, I., Glickman, O. and Magnini, B. (2007). The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Workshop.\n14\nDettmers, T., Lewis, M., Belkada, Y. and Zettlemoyer, L. (2022). Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339.\nDettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. (2023). Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nDiao, S., Pan, R., Dong, H., Shum, K. S., Zhang, J., Xiong, W. and Zhang, T. (2023). Lmflow:\nAn extensible toolkit for finetuning and inference of large foundation models. arXiv preprint\narXiv:2306.12420.\nDolan, W. B. and Brockett, C. (2005). Automatically constructing a corpus of sentential para-\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).\nFrantar, E., Ashkboos, S., Hoefler, T. and Alistarh, D. (2022). Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323.\nGiampiccolo, D., Magnini, B., Dagan, I. and Dolan, B. (2007). The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing. Association for Computational Linguistics, Prague.\nHe, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. and Neubig, G. (2021a). Towards a unified view of\nparameter-efficient transfer learning. arXiv preprint arXiv:2110.04366.\nHe, P., Gao, J. and Chen, W. (2021b). Debertav3: Improving deberta using electra-style pre-training\nwith gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543.\nHermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M. and Blunsom,\nP. (2015). Teaching machines to read and comprehend. Advances in neural information processing\nsystems, 28.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W. (2021). Lora:\nLow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\nLevesque, H., Davis, E. and Morgenstern, L. (2012). The winograd schema challenge. In Thirteenth\ninternational conference on the principles of knowledge representation and reasoning.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and\nZettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and Zettle-\nmoyer, L. (2020). BART: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics. Association for Computational Linguistics, Online.\n15\nLi, Y., Yu, Y., Zhang, Q., Liang, C., He, P., Chen, W. and Zhao, T. (2023). Losparse: Structured\ncompression of large language models based on low-rank and sparse approximation. arXiv\npreprint arXiv:2306.11222.\nLin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out. Association for Computational Linguistics, Barcelona, Spain.\nLiu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R. and\nChandra, V. (2023). Llm-qat: Data-free quantization aware training for large language models.\narXiv preprint arXiv:2305.17888.\nLoshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nMerity, S., Xiong, C., Bradbury, J. and Socher, R. (2016). Pointer sentinel mixture models.\nNarayan, S., Cohen, S. B. and Lapata, M. (2018). Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. ArXiv, abs/1808.08745.\nNie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J. and Kiela, D. (2019). Adversarial nli: A\nnew benchmark for natural language understanding. ArXiv, abs/1910.14599.\nhttps://api.semanticscholar.org/CorpusID:207756753\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,\nGimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Te-\njani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J. and Chintala, S. (2019). Pytorch: An\nimperative style, high-performance deep learning library. In Advances in Neural Information\nProcessing Systems 32. Curran Associates, Inc., 8024\u20138035.\nPeri, D., Patel, J. and Park, J. (2020). Deploying quantization-aware trained networks using\ntensorrt. In GPU Technology Conference.\nRajpurkar, P., Zhang, J., Lopyrev, K. and Liang, P. (2016). SQuAD: 100,000+ questions for machine\ncomprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, Austin, Texas.\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W. and Keutzer, K. (2020).\nQ-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 34.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. and Potts, C. (2013).\nRecursive deep models for semantic compositionality over a sentiment treebank. In Proceedings\nof the 2013 Conference on Empirical Methods in Natural Language Processing. Association for\nComputational Linguistics, Seattle, Washington, USA.\n16\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S. et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141. and\nPolosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems,\n30.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O. and Bowman, S. R. (2019). GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding. In International Conference\non Learning Representations.\nWarstadt, A., Singh, A. and Bowman, S. R. (2019). Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics, 7 625\u2013641.\nWilliams, A., Nangia, N. and Bowman, S. (2018). A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long Papers). Association for Computational Linguistics, New Orleans, Louisiana.\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J. and Han, S. (2023). Smoothquant: Accurate and\nefficient post-training quantization for large language models. In International Conference on\nMachine Learning. PMLR.\nZafrir, O., Boudoukh, G., Izsak, P. and Wasserblat, M. (2019). Q8bert: Quantized 8bit bert.\nIn 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS\nEdition (EMC2-NIPS). IEEE.\nZhang, H., Dauphin, Y. N. and Ma, T. (2019). Fixup initialization: Residual learning without\nnormalization. arXiv preprint arXiv:1901.09321.\nZhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W. and Zhao, T. (2023). Adaptive\nbudget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512.\n17\nA\nModel Compression Ratio and Memory Footprint\nWe report the compression ratio after applying LoftQ in Table 7. It is defined as\ncompression ration = backbone size + LoRA adapter size\npre-trained size\n.\nWe also measure the GPU memory cost during training. Given that GPU memory varies by\nmodels, tasks, sequence lengths, batch sizes, etc. We report LLAMA-2 on GSM8K as an example in\nTable 8.\nTable 7: Compression ratios of backbones.\nModel\nCompression\nTrainable\nRank\nBits\nQuantization\nratio (%)\nratio (%)\nmethod\nDeBERTaV3-base\n15.6\n3.1\n16\n2\nUniform\nDeBERTaV3-base\n18.8\n6.3\n32\n2\nUniform\nDeBERTaV3-base\n17.2\n3.1\n16\n2\nNF2\nDeBERTaV3-base\n20.4\n6.3\n32\n2\nNF2\nBART-large\n15.3\n1.2\n8\n4\nNF2\nBART-large\n16.7\n2.5\n16\n4\nNF2\nBART-large\n27.8\n1.2\n8\n4\nNF4\nBART-large\n29.0\n2.5\n16\n4\nNF4\nBART-large\n26.2\n1.2\n8\n4\nUniform\nBART-large\n27.5\n2.5\n16\n4\nUniform\nLLAMA-2-7b\n16.6\n2.4\n64\n2\nNf2\nLLAMA-2-7b\n29.0\n2.4\n64\n4\nNf4\nLLAMA-2-13b\n16.0\n1.9\n64\n2\nNf2\nLLAMA-2-13b\n28.5\n1.9\n64\n4\nNf4\nTable 8: GPU memory footprint\nModel\nDataset\nSeq length\nBatch size\nGPU Mem\nLLAMA-2-7b\nGSM8K\n384\n1\n15GB\nLLAMA-2-13b\nGSM8K\n384\n1\n24GB\nB\nQuantization Time\nWe report the execution time of LoftQ applying to a single weight matrix in Table 9. The time is\ntested on Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz.\n18\nTable 9: Execution time of LoftQ applying to different weight matrices.\nModel\nSize\nStep T\nQuantization method\nTime\nDeBERTaV3-base\n768 \u00d7 768\n5\nUniform\n1s\nBART-large\n1024 \u00d7 1024\n5\nNF4\n1s\nLLAMA-2-7b\n4096 \u00d7 4096\n5\nNF4\n21s\nLLAMA-2-13b\n5120 \u00d7 5120\n5\nNF4\n43s\nC\nGLUE Dataset Statistics\nWe present the dataset statistics of GLUE Wang et al. (2019) in the following table.\nCorpus\nTask\n#Train\n#Dev\n#Test\n#Label\nMetrics\nSingle-Sentence Classification (GLUE)\nCoLA\nAcceptability\n8.5k\n1k\n1k\n2\nMatthews corr\nSST\nSentiment\n67k\n872\n1.8k\n2\nAccuracy\nPairwise Text Classification (GLUE)\nMNLI\nNLI\n393k\n20k\n20k\n3\nAccuracy\nRTE\nNLI\n2.5k\n276\n3k\n2\nAccuracy\nQQP\nParaphrase\n364k\n40k\n391k\n2\nAccuracy/F1\nMRPC\nParaphrase\n3.7k\n408\n1.7k\n2\nAccuracy/F1\nQNLI\nQA/NLI\n108k\n5.7k\n5.7k\n2\nAccuracy\nText Similarity (GLUE)\nSTS-B\nSimilarity\n7k\n1.5k\n1.4k\n1\nPearson/Spearman corr\nTable 10: Summary of the GLUE benchmark.\nGLUE includes two single-sentence classification tasks: SST-2 (Socher et al., 2013) and CoLA\n(Warstadt et al., 2019), and three similarity and paraphrase tasks: MRPC (Dolan and Brockett,\n2005), STS-B (Cer et al., 2017), and QQP. GLUE also includes four natural language inference tasks\nin GLUE: MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2007;\nBar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and WNLI (Levesque et al.,\n2012).\n19\nD\nNatural Language Understanding\nD.1\nGLUE with 4-bit\nWe show the 4-bits results in the Table 11. Both methods can achieve performance close to\nfull-finetuning.\nTable 11: Results with 4-bit LoftQ of DeBERTaV3-base models on GLUE development set using\nNF4 quantization. We report the median over four seeds. Results with N.A. indicate the model\ndoes not converge. The best results on each dataset are shown in bold\nMethod\nRank\nMNLI\nSST-2\nQNLI\nANLI\nm / mm\nAcc\nAcc\nAcc\nFull FT\n-\n90.5/90.6\n95.3\n94.0\n59.8\nQLoRA\n32\n89.9/89.9\n95.3\n94.2\n59.4\nLoftQ\n32\n89.9/90.0\n95.3\n94.1\n59.9\nD.2\nTraining Details\nImplementation Details. The implementation of LoftQ is based on publicly available Huggingface\n(Paszke et al., 2019) code-base **.\nHyper-parameter Details. We select the learning rate of {1 \u00d7 10\u22125,5 \u00d7 10\u22125,1 \u00d7 10\u22124,5 \u00d7 10\u22124}, and\nuse the selected learning rate for both uniform quantization experiments and nf2 quantization\nexperiments. We use batch size of 32 for all GLUE tasks and ANLI. We use batch size of 16 for\nSQuADv1.1. We use LoftQ of 5 iterations for all GLUE tasks.\nTable 12 summarizes the detailed hyperparameters for each task used in training DeBERTaV3-\nbase using uniform quantization. Table 13 summarizes the detailed hyperparameters for each task\nused in training DeBERTaV3-base using nf2 quantization.\nTable 12: Hyper-parameter setup of LoftQ for GLUE benchmark for training DeBERTaV3-base\nusing Uniform quantization.\nHyper-parameter\nMNLI\nRTE\nQNLI\nMRPC\nQQP\nSST-2\nCoLA\nSTS-B\nSQuADv1.1\nANLI\n# epochs\n5\n20\n10\n60\n10\n10\n60\n60\n10\n12\nLearning rate\n1 \u00d7 10\u22124\n5 \u00d7 10\u22124\n5 \u00d7 10\u22125\n1 \u00d7 10\u22124\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n**https://github.com/huggingface/transformers/tree/main/examples/pytorch\n20\nTable 13: Hyper-parameter setup of LoftQ for GLUE benchmark for training DeBERTaV3-base\nusing NF2 quantization.\nHyper-parameter\nMNLI\nRTE\nQNLI\nMRPC\nQQP\nSST-2\nCoLA\nSTS-B\nSQuADv1.1\nANLI\n# epochs\n5\n20\n10\n60\n10\n10\n60\n60\n10\n12\nLearning rate\n1 \u00d7 10\u22124\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n1 \u00d7 10\u22124\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\n1 \u00d7 10\u22124\n5 \u00d7 10\u22125\n5 \u00d7 10\u22125\nE\nSummarization\nE.1\nTraining Details\nWe choose Adam as the optimizer and try learning rate from{1\u00d710\u22125,5\u00d710\u22125,7\u00d710\u22125,2\u00d710\u22124,3\u00d7\n10\u22124,4 \u00d7 10\u22124}. We show the optimal learning rate for different settings in Table 14. We use LoftQ\nof 1 iteration for all BART-large experiments. Table 14 and Table 15 summarize the learning rate\nand other hyper-parameters for CNN/DailyMail and XSum.\nTable 14: Hyper-parameter setup of LoftQ BART-large on CNN/DailyMail\nHyperparameter\nNF4\n4-bit Uniform\nNF2\nrank8\nrank16\nrank8\nrank16\nrank8\nrank16\nLearning rate\n2e-4\n2e-4\n2e-4\n3e-4\n2e-4\n2e-4\nEpoch\n15\n15\n15\n15\n15\n15\nBatch size\n64\n64\n64\n64\n64\n64\nTable 15: Hyper-parameter setup of LoftQ BART-large on XSum\nHyperparameter\nNF4\n4-bit Uniform\nNF2\nrank8\nrank16\nrank8\nrank16\nrank8\nrank16\nLearning rate\n2e-4\n2e-4\n2e-4\n2e-4\n2e-4\n2e-4\nEpoch\n25\n25\n25\n25\n25\n25\nBatch size\n32\n32\n32\n32\n32\n32\n21\nF\nNatural Language Generation\nWe set the batch size as 32 for WikiText-2 and 16 for GSM8K. We train 2 epochs on WikiText-2 and 6\nepochs on GSM8K. We select learning rate from{1\u00d710\u22125,5\u00d710\u22125,7\u00d710\u22125,1\u00d710\u22124,,3\u00d710\u22124,4\u00d710\u22124}.\nSpecific settings are summarized in Table 16 and Table 17.\nTable 16: Hyper-parameter setup of LoftQ LLAMA-2-series on GSM8K\nModel\nHyperparameter\nNF4\nNF2\nMixed-precision\nLLAMA-2-7b\nlearning rate\n3 \u00d7 10\u22124\n3 \u00d7 10\u22124\n3 \u00d7 10\u22124\nLLAMA-2-13b\nlearning rate\n1 \u00d7 10\u22124\n1 \u00d7 10\u22124\n3 \u00d7 10\u22124\nTable 17: Hyper-parameter setup of LoftQ LLAMA-2-series on WikiText-2\nModel\nHyperparameter\nNF4\nNF2\nMixed-precision\nLLAMA-2-7b\nlearning rate\n3 \u00d7 10\u22124\n3 \u00d7 10\u22124\n3 \u00d7 10\u22124\nLLAMA-2-13b\nlearning rate\n1 \u00d7 10\u22124\n1 \u00d7 10\u22124\n3 \u00d7 10\u22124\nG\nComparison to Pruning\nPruning is also a widely used compression method. Here we compare LoftQ with the state-of-the-\nart pruning method Li et al. (2023). We show the comparison in Table 18. We can see our method\nsignificantly outperforms the pruning methods on DeBERTaV3-base model. We also remark that\nLoftQ can consistently reduce the memory of both training and storage. In contrast, pruning\nrequires training the entire full-precision matrix, which implies that it can not achieve any memory\nsavings during the training stage.\nH\nExtension to Convolutional Layers\nLow-rank adapters can also be applied to convolutional layers. Given an input feature map X \u2208\nRh\u00d7w\u00d7c1 and c2 2D convolutional kernels Ki \u2208 Rc1\u00d7d\u00d7d,i = 1,2,...,c2, the output of the convolutional\nlayer is\nY = stack(X \u2297 K1,...,X \u2297 Kc2),\n(10)\nwhere Y \u2208 Rh\u00d7w\u00d7c2 and \u2297 denotes the 2D convolution operation.\n22\nTable 18: Results of LoftQ using 2-bits uniform quantization compared with LoSparse with\nDeBERTaV3-base models on some of GLUE development sets. Here Ratio is the proportion of total\nremaining weights. Results with N.A. indicate the model does not converge.\nMethod\nRatio\nMNLI\nSST-2\nQNLI\nm / mm\nAcc\nAcc\nFull FT\n100%\n90.5 / 90.6\n95.3\n94.0\nLoSparse\n15%\n83.3/82.9\n87.6\n90.4\n20%\n84.5/83.8\n91.7\n88.6\nLoftQ\n15.6%\n87.3/87.1\n94.0\n90.6\n18.8%\n88.0/88.1\n94.7\n92.4\nWe can reformulate Equation (10) into matrix multiplication as\nY = Z \u00d7 H\u22a4,\nwhere Z \u2208 Rhw\u00d7c1d2,H \u2208 Rc2\u00d7c1d2, by extending and flattening the input X together with concate-\nnating and flattening kernels. We first extend a vector xi,j \u2208 Rc1 by its neighbor vectors within the\nkernel window:\nx\n\u2032\ni,j = Concat(xi\u2212 d\n2 ,j\u2212 d\n2 ,...,xi+ d\n2 ,j+ d\n2 ).\nNow, X becomes X\u2032 \u2208 Rh\u00d7w\u00d7c1d2. We then flatten X\u2032 into Z \u2208 Rhw\u00d7c1d2. For kernels, we first\nconcatenate {K1,...,Kc2} into H\u2032 \u2208 Rc2\u00d7c1\u00d7d\u00d7d. We then flatten H\u2032 into H.\nNote that H can be approximated by a low-rank matrix\nR = UV \u22a4,\nwhere U \u2208 Rc2\u00d7r,V \u2208 Rc1d2\u00d7r,r \u226a min{c2,c1d2} by SVD. Therefore, the original convolution layer\ncan be approximated as\nbY = Z \u00d7 (UV \u22a4)\u22a4\n(11)\n= (Z \u00d7 V ) \u00d7 U\u22a4\n(12)\n= M \u00d7 U\u22a4.\n(13)\nNote that Z \u00d7 V can be restored into a convolution operation where we have r kernels Di \u2208\nRc1\u00d7d\u00d7d,i = 1,2,,...,r and M \u00d7 U\u22a4 can also be restored into a convolution operation where we have\nc2 kernels Ui \u2208 Rr\u00d71\u00d71,i = 1,2,,...,c2.\n23\n"
  },
  {
    "title": "A Zero-Shot Language Agent for Computer Control with Structured Reflection",
    "link": "https://arxiv.org/pdf/2310.08740.pdf",
    "upvote": "13",
    "text": "A Zero-Shot Language Agent for Computer Control\nwith Structured Reflection\nTao Li\u2039\nGang Li\u2039\nZhiwei Deng\u2039\nBryan Wang\u02db: Yang Li\u2039\n\u2039Google Research, Mountain View, U.S.A.\n\u02dbUniversity of Toronto, Ontario, Canada\n{tlinlp,leebird,zhiweideng,liyang}@google.com\nbryanw@dgp.toronto.edu\nAbstract\nLarge language models (LLMs) have shown\nincreasing capacity at planning and executing\na high-level goal in a live computer environ-\nment (e.g. MINIWOB++). To perform a task,\nrecent works often require a model to learn\nfrom trace examples of the task via either su-\npervised learning or few/many-shot prompting.\nWithout these trace examples, it remains a chal-\nlenge how an agent can autonomously learn\nand improve its control on a computer, which\nlimits the ability of an agent to perform a new\ntask. We approach this problem with a zero-\nshot agent that requires no given expert traces.\nOur agent plans for executable actions on a\npartially observed environment, and iteratively\nprogresses a task by identifying and learning\nfrom its mistakes via self-reflection and struc-\ntured thought management. On the easy tasks\nof MINIWOB++, we show that our zero-shot\nagent often outperforms recent SoTAs, with\nmore efficient planning. For tasks with more\ncomplexity, our reflective agent performs on\npar with prior best models, even though pre-\nvious works had the advantages of accessing\nexpert traces or additional screen information.\n1\nIntroduction\nPrior works have shown promises in using large\nlanguage models (LLMs) for action generation, e.g.\nSAYCAN (Brohan et al., 2023), REACT (Yao et al.,\n2023), TOOLFORMER (Schick et al., 2023), and\nSWIFTSAGE (Lin et al., 2023)) over a variety of\nlive environments (e.g. MINIWOB++ (Shi et al.,\n2017; Liu et al., 2018), ALFWORLD (Shridhar\net al., 2021), and ALPHACODE (Li et al., 2022). A\nshared approach is to use LLMs to follow expert\ntraces, comprehend environment changes, plan fu-\nture actions, and execute an action by composing\nAPI calls; all in the form of text. Some works\nhave shown that iteratively attempting a task with\nseveral rounds of self-reflection can substantially\n:Work done during internship at Google Research.\nimprove task completion, e.g., REFLEXION (Shinn\net al., 2023), SELF-REFINE (Madaan et al., 2023).\nDuring the process, LLMs are prompted to update\na prior execution plan according to feedback from\nthe environment. Such updates become part of the\nprompt for the action generator in the next round.\nRecently, MINIWOB++ has been used as a\ntestbed for LLM\u2019s capacity at modularized com-\nputer tasks. To learn a task, a common approach\nis the use of extensive trace examples of the task\nfor direct supervision (e.g., CC-Net (Humphreys\net al., 2022), WebGUM (Furuta et al., 2023)),\nself-supervision (Gur et al., 2023), or few/many-\nshot prompting (e.g., RCI (Kim et al., 2023),\nSYNAPSE (Zheng et al., 2023)).\nThey have\nachieved more than 90% task completion rate on\ndozens of computer tasks, seemingly to have solved\nthe computer control problem.\nHowever, the requirement of expert traces for\nlearning to perform a task limits the agent\u2019s ability\non new tasks. Without using carefully selected\ntraces as guidance, can an agent autonomously\nlearn and improve its control on a computer? To\naddress this question, we propose a zero-shot agent.\nWe build our agent on top of PaLM2 (Anil et al.,\n2023), a recent LLM, and our agent employs a\nunified instruction prompt set across different tasks,\nwithout extensive tailoring for individual tasks.\nIn addition, recent works, e.g., RCI (Kim et al.,\n2023), ADAPLANNER (Sun et al., 2023), and\nSYNAPSE (Zheng et al., 2023), utilize a screen rep-\nresentation that may include much more informa-\ntion than what is presented to a user on the screen.\nFor instance, Fig. 1 shows an example of elements\nthat are not shown on the screen yet present in the\nHTML that is fed to the LLM. Using such addi-\ntional information arbitrarily reduces the difficulty\nfor the agent to perform the task. Yet in general\nuse cases, such information might not be readily\navailable, and relying on such information can po-\ntentially hamper the applicability of the agent. We\narXiv:2310.08740v3  [cs.CL]  23 Oct 2023\n(a) Before\n(b) After\n(c) HTML of before\n(d) Before\n(e) HTML of before\nFigure 1: Inconsistent screen representations. Fig. 1a-1c: be-\nfore and after clicking the \u201cmore\u201d button on the social-media\ntask (seed=2). HTML before the clicking already revealed the\ncontent. Fig. 1d-1e: Similar issue in the click-tab-2 (seed=0).\nmanually examined 13 relatively challenging tasks\non MINIWOB++ that are supposed to span multi-\nple screens, and found 5 of them contained such\ninformation\u2014multi-screen information in a single\nobservation\u2014in their HTML.\nOur contributions are as follows: Firstly, we em-\nploy a compact screen representation that assumes\nmuch less information than what is used by pre-\nvious works, thus resulting in a more general and\nrealistic test environment. Secondly, we propose\na simple yet efficient action planner that can accu-\nrately plan out executable actions on a state in one\npass. With recent capacity of LLM, we show that\nsuch a \u201cnaive\u201d strategy can solve almost all the easy\ntasks on the MINIWOB++ benchmark. For more\nchallenging tasks, we take inspiration from Reflex-\nion (Shinn et al., 2023) and propose a structured\nthought management strategy to facilitate reflec-\ntion, allowing the agent to effectively learn and im-\nprove from exploration failures. With a few rounds\nof attempts, our agent achieves comparable perfor-\nmance with prior few/many-shot state-of-the-art.\nTo the best of our knowledge, our agent is the first\nzero-shot design for computer control tasks1.\n2\nBackground\nLLMs have become an emerging tool for planning\nand executing necessary steps to fulfill a top-level\ngoal. These models have exhibit high capacity to\nfollow in-context traces to solve complex tasks.\n1Code\nand\nnotebook:\nhttps://github.com/\ngoogle-research/google-research/tree/master/\nzero_shot_structured_reflection\nPlanning & Reflection\nREACT (Yao et al., 2023)\nused intermediate thoughts to guide long-chain of\naction planning in a live environment. Beyond\none trial of planning, REFLEXION (Shinn et al.,\n2023) and SELF-REFINE (Madaan et al., 2023)\nrecently found the ability of LLM to self-criticize\nand self-improve can iteratively learn and solve a\ngiven task via multiple trials. Nevertheless, recent\nplanning works require extensive and customized\ntrace prompts for the LLM agent to learn accurate\nplanning. SWIFTSAGE (Lin et al., 2023) reduces\nthe extensive planning calls to LLM by training\na small and fast planning module to facilite long-\nchain of actions. In a similar motivation, our zero-\nshot agent is based on an efficient staged planner.\nWe list a detailed comparison in Tab. 1.\nPlanning\nUnsup Trace Efficient Feedback Reflection\nmem\nRCI\n\u2713\n\u2713\n\u2717\nsparse\n1\nADAPLANNER\n\u2713\nfew\n\u2713\ndetailed\n\u2014\nREFLEXION\n\u2713\nfew\n\u2717\ndetailed\n3\nSWIFTSAGE\n\u2717\n\u2014\n\u2713\ndetailed\n\u2014\nOurs\n\u2713\n0\n\u2713\nsparse\nN\nTable 1: Comparison with prior work on reflective planning.\nN: number of actions to complete the task. Efficient \u2717: re-\nquires planner call for each action.\nMINIWOB++\nUnsup\nTrace\nEfficient Consistent\nScreen\nCC-NET\n\u2717\nlarge\n\u2717\n\u20142\nPIX2ACT\n\u2717\nlarge\n\u2717\n\u2713\nRCI\n\u2713\nfew/many\n\u2717\n\u2717\nADAPLANNER\n\u2713\nfew\n\u2713\n\u2717\nOurs\n\u2713\n0\n\u2713\n\u2713\nTable 2: Comparison with prior work on MiniWoB. For\ndiscussion on consistent screen, please refer to Sec. 3.1.\nLanguage/Vision Agent for Computer Control\nMINIWOB++ (Shi et al., 2017) has several dozens\nof fundamental computer tasks hosted as live en-\nvironments. Recent advances on this benchmark\nhave benefited from extensive human annotation to\nfacilitate behavior cloning and reinforcement learn-\ning, such as CC-NET (Humphreys et al., 2022) and\nPIX2ACT (Shaw et al., 2023). Beside these models\nthat rely on multimodal or vision input, another\ntrack is using LLMs as an off-the-shelf agent, and\nuse prompted inference for action generation, such\n2No public source for CC-NET\u2019s screen representation at\nthe time of writing this paper.\nas RCI (Kim et al., 2023), ADAPLANNER (Sun\net al., 2023), and SYNAPSE (Zheng et al., 2023).\nWe highlight our technical differences in Tab. 2.\n3\nEnvironment Interface\nThe role of a language agent is to comprehend\nscreen representations (Sec. 3.1&3.2), execute ac-\ntions on the environment (Sec. 3.3), and react to\nenvironment feedback (Sec. 3.4).\n3.1\nTreatment of Screens\nThe definition of a screen observation varies by\nmodality in recent works. The screen observation\nfor a vision-based model (e.g. Humphreys et al.,\n2022; Shaw et al., 2023) can be constrained by vari-\nous viewport configurations. For a language model,\nspecifically those taking in HTML code, a screen\nobservation is often the page source of a screen,\ne.g., ones instantiated from a MINIWOB++ task\ntemplate with a given random seed. When HTML\nelements are not constrained by viewport configura-\ntion, the need for scrolling action is gone. However,\nas discussed in Sec. 1, we do not immediately use\nthe expanded HTML if the expansion requires a UI\naction: we only expand the HTML representation\nwhen the agent actually takes the action. The de-\nsign relaxes the assumption about the environment,\nand forces the agent to learn to behave rationally\nwhen it is given limited information.\n3.2\nCompact Screen Representation\nRaw HTML code tends to be verbose, which poses\na practical challenge for LLMs that often have an\ninherent limit on the input or context length. Zheng\net al. (2023) designed a technique for structured\nprompting example selection to extract more infor-\nmative trace examples, as a way to reduce the input\ncontent to LLMs. MindAct (Deng et al., 2023)\nranked and consolidated element of interests to rep-\nresent web page snippet. Alternatively, we take\ninspiration from Wang et al. (2023a) to heuristi-\ncally simplify the HTML code of each screen, re-\ntaining key attributes for each leaf element, i.e., id,\nclass, text, placeholder, value, and position on a\n3x3 screen grid. Such simplification has shown to\ngive compelling results on UI understanding tasks.\nAn example is shown in Fig. 2.\n3.3\nAction Space\nFor each action, our agent model outputs com-\nmands in a format that is specific to the action\n(a) Screen\n(b) Simplified HTML\nFigure 2: Example of compact screen representation.\ntype. Specifically, we use three types of actions\nas shown in Tab. 3. Recent LLMs such as PaLM-\n2 (Anil et al., 2023) are good at following such\nan output format. More prompt details are given\nin Appx. A. To deterministically ground an action\ncommand on MINIWOB++ environment, we fol-\nlow the approach in prior language-only works (e.g.\nKim et al., 2023) to access HTML elements by\nXPATH pattern. When grounding click actions on\nthe actual environment, we use the compact ele-\nment id (Sec. 3.2) which is aligned to the actual\nHTML element in the raw HTML. For the type ac-\ntion, we decompose it into a click action followed\nby a series of keyboard presses for the text.\nclick\ntype\nspecial key\nclick id=6 enter \u201ctext\u201d to id=10 press ARROWDOWN x N\nTable 3: Action types and example commands.\n3.4\nEnvironment Feedback\nMINIWOB++ differs from TEXTWORLD-like en-\nvironment (Shridhar et al., 2021) in that state\nchange from taking an action is not naturally\nphrased out. Instead, an agent will need to ob-\nserve the entire screen change implicitly, making\nit less convenient for the agent to adopt Chain-of-\nThought (Wei et al., 2022) reasoning. We broadly\ncategorize trial ending conditions into: 1) correct,\n2) cycle, 3) no change, 4) incomplete, 5) exception,\nand 6) failed. Condition 2) and 3) compare the\nHTML code of the current screen to those of prior\nscreens. Condition 5) happens when a grounding\naction can not be successfully executed. During\nmultiple trials, each ending condition is associated\nwith a prompt for the reflection inference.\n4\nPlanning Strategies\nIn this section, we summarize the planning strate-\ngies used in recent LLM-based planning works to\nFigure 3: An example of successful reflection trial by our zero-shot agent on MINIWOB++ task use-autocomplete with seed=0.\nStep actions are paraphrased from the actual executable ones for readability.\nmotivate our staged planning. With a given goal,\nan agent model is to issue actions based on prior\ninteraction with the environment. For brevity, let\nus denote the interaction as a sequence of state and\naction pairs psi, aiq.\n4.1\nIterative Planning\nIn iterative planning (e.g. Yao et al., 2023; Madaan\net al., 2023), the agent model loops over generating\nan \u201catomic\u201d action ai, grounding it on the environ-\nment for execution, and then observing for the next\nstate si. That is,\nai \u201e \u03c4\u03b8pa|si, ai\u00b41, si\u00b41, ...a0, s0q\n(1)\nwhere \u03c4\u03b8 denotes the planning model. Such plan-\nning is a common choice for environments that\nrequire observation by exploration. With respon-\nsive environments (e.g. C\u00f4t\u00e9 et al., 2018; Shridhar\net al., 2021), such an agent can benefit from a long\nhistory of interaction that can be easily connected\nto Chain-of-Thought reasoning (Wei et al., 2022).\n4.2\nPlan-Then-Adapt\nRecently, Kim et al. (2023) observed that an initial,\nand yet rough, plan could help iterative planning.\nFormally,\npa0, a1, ...anq \u201e \u03c4\u03b8pa|s0q\n(2)\n\u00afai \u201e z\u03b8p\u00afa|si, \u00afai\u00b41, si\u00b41, ..., pa0, a1, ...anqq\n(3)\nwhere z\u03b8 adapts those initial steps to be executable\nactions (\u00afa\u2019s) on the environment. In practice, both\n\u03c4\u03b8 and z\u03b8 use the same LLM.\nConceptually, this is similar to zero-shot plan-\nner (Huang et al., 2022) and ReAct (Yao et al.,\n2023) that intermediate thought can help plan long\nchain of actions. The downside though is that\nthe agent needs to follow carefully crafted few-\nshot trace examples to make a good initial plan.\nADAPLANNER (Sun et al., 2023) addresses this is-\nsue with an adaptive plan refiner that monitors the\nstate-action compatibility and issues refined actions\nwhen there is a mismatch. This line of planning\noften needs to deal with hallucinations in the initial\nplan since, after all, the agent model only observes\ns0 but needs to plan for unobserved states.\n4.3\nStaged Plan-And-Follow\nPrior works essentially add on extra planning com-\nponents to the agent. Instead, we adopt a sim-\npler planning strategy. For computer environment,\nagents often sees a state where multiple actions\ncan be executed on, without the need to observe\nnuanced state changes, e.g., multiple selection on\na list. In such cases, iterative planning on a single\nscreen can be less efficient, and often, unnecessary.\nOn the other hand, plan-then-adapt generates ac-\ntions beyond executable ones that could confuse\nthe LLM agent during the adaptation step. Further-\nmore, both approaches require the agent to itera-\ntively generate the next action, requiring an LLM\nto have a large context window.\nTo address these issues, we take a step in the mid-\ndle by maximally planning actions that are visible\non the current state all at once. After the plan-\nning, the agent only needs strictly follow the gener-\nated plan, and such process repeats over multiple\nscreens. Formally,\npa0\ni , ...ak\ni q \u201e \u03c4\u03b8pa|si, ai\u00b41, ai\u00b42, ...a0q\n(4)\nwhere each stage is essentially generating k ex-\necutable actions for state si. Note that, we also\nomit former states in Eq. 4 to make inference more\nefficient. In practice, we found that a simple state-\nment, in natural language, that summarizes what\nfunctionality is achieved by the action, is a good\nrepresentation for the state-action pair psi, aiq.\nImplementation details.\nWe once again rely on\nthe underlying LLM to follow execution instruc-\ntions as in Eq. 4. Prompt details are in Appx. B-\nE. In rare occasions, agent model predicts fewer\nsteps (e.g., forgetting to submit) or more steps (e.g.,\nhallucinating non-executable actions) than needed.\nFor the former, we loop over the planning in Eq. 4\nuntil no further plan is generated. For the latter, we\nhalt the execution of the current plan and resort to\nself-reflection for correction.\n5\nStructured Self-Reflection\nIn practice, a high-level human instruction or goal\ncan be ambiguous, and an environment can be par-\ntially hidden. Therefore, agents are prone to mak-\ning mistakes; this is even true for a human user\nwhen executing a task (Humphreys et al., 2022).\nOnce a negative signal is given, such as no change\nor failed (Sec. 3.4), we ask the agent to reflect on its\npast action trajectory, suggest an improved version,\nand then retry the task. An example of successful\nreflection from our agent is shown in Fig. 3.\nIn recent works (Shinn et al., 2023; Madaan et al.,\n2023), reflection is conducted at the end of each\ntrial by accumulating a text entry. The entry is\nessentially a natural language statement about what\nshould have been done instead. At trial t,\nai \u201e \u03c4\u03b8pa|si, ai\u00b41, ...a0, s0; Rtq\n(5)\nRt`1 \u201e REFL\u03b8pan, sn, ...ai, si, ...; Rtq\n(6)\nwhere Rt consists of a list of pai, a1\niq pairs, each\ndenotes to update the wrong action ai to a1\ni. In the\nfollowup trial t ` 1, accumulated entries in Rt`1\nare prefixed to the prompt for the agent planner \u03c4\u03b8.\nThe amount of entries maintained in the reflec-\ntion memory is limited by multiple factors. For one,\nit increases the input length of LLM. Moreover, it\nrequires the LLM agent to handle the thought struc-\ntures through multiple trials. In practice, Shinn\net al. (2023) limited the memory size P r1, 3s.\n5.1\nStructured Thought Management\nIn a zero-shot setting, reflection puts a heavy bur-\nden on LLM\u2019s capacity to follow a complex in-\nstruction set (in addition to those in Sec. 3.3&4.3).\nAfter all, in this case, LLM has no expert traces\nto follow, thus need to learn from trials. With in-\ncreasing the number of trials, reflection memory\nessentially forms a combinatorial problem for the\nagent to solve. For a time step i, there can be mul-\ntiple failed actions in historical trials, thus should\nbe avoided. For a trial t, if the agent identified a\ncritical mistake at time i, reflections on later time\nsteps can be considered outdated.\nIn our preliminary study, we found it is important\nto devise a way to help an LLM agent maintain\nthis memory. Otherwise, when a reflection entry\npai, a1\niq is given, even a state-of-the-art LLM can\nstill 1) repeat the same mistake ai, 2) fail to follow\nthe reflected plan to even reach si, and 3) bounce\nbetween wrong actions it collected in prior trials.\nTo make reflection run more reliably and effi-\nciently, we propose a structured self-reflection in\nAlgo. 1. When a suggested action a1\ni is given by\nthe reflection agent, we enforce our agent to plan\nexactly a1\ni at step i. Moreover, to avoid looping\nover two failed actions at the same step, we use a\ndisabled action set D to memorize them and jointly\ndisable these actions in the environment. Finally,\nwe clear reflection entries for future steps if an early\nentry is updated. With this management, our agent\nis no longer bounded by LLM\u2019s input limit, and\nhas a memory size N.\nAlgorithm 1: Structured Thought Management\n1: R \u201c rHs \u02da N; D \u201c rHs \u02da N;\n2: for t P r0, Tq:\n3:\nfor i P r0, Nq:\n4:\nif Rris and Rris.a1 R Dris: // if has reflection\n5:\nai \u201c Rris.a1\n// action from reflection\n6:\nelse: ai \u201e \u03c4\u03b8pa|...q\n// regular planning\n7:\nif needToReflect:\n// if error happens\n8:\npaj, a1\njq \u201e REFL\u03b8p...q\n// reflect\n9:\nif Rrjs \u2030 H:\n10:\nDrjs.addpRrjs.aq\n// record wrong click\n11:\nRrjs \u201c paj, a1\njq\n// record verbal reflection\n12:\nRrj ` 1 :s \u201c H; Drj ` 1 :s \u201c H\n// clear mem\nNote that in line 6, we use the staged planner in\nEq. 4.3 which does not depend on the iteratively up-\ndated R, thus is different from recent works (Eq. 6).\nInterplay with Staged Planning.\nSuppose the\nstaged planner predicted ra0, ...ai, ...ans but exe-\ncution had failed, and the reflection step identified\nai as the earliest mistake, thus suggested a1\ni. In\nthe next trial, we will repeat the executions from\na0 to ai\u00b413, and intercept the agent planner at step\ni to enforce the execution of a1\ni. For steps after\ni, we bring our planner to the next stage. In the\nworst case where an agent fails at every step, our\nstaged planning essentially falls back to the plan-\nthen-adapt (Sec. 4.2), except having no initial plan.\n5.2\nConstraining Action Space\nFor an updated action a1 at reflection trial t, we\nenforce it to be taken at the associated time step if\nand only if a1 is not an failed attempt before trial t.\nIt can be tricky to prompt LLM to follow such sim-\nple combinatorial constraint in text, especially as a\nmixture of positive and negative signal surrounded\nby other instructions. Therefore, we found it is\ncrucial to explicitly disable those previously failed\nactions in the corresponding screen representation.\nThis, however, does not mean removing the cor-\nresponding element from the HTML pseudo code.\nWe instead only remove the id attribute, and still\nallow the element information to be presented to\nLLM. We only do so for click-type actions.\nFor non-click actions, the disable set D cannot\nbe easily enforced on the environment and the LLM\nagent. We can indeed prompt the underlying LLM\nsaying certain special keys are invalid or certain\ntexts not to input. However, we did not observe a\npositive impact from doing so in our preliminary\nexperiment4. Thus, we fallback to only determinis-\ntically generate the a1\ni at time step i.5 We locate the\ntime step by prompting the reflection agent to out-\nput in format: \u201cFor action index=i, you should a1\ni\u201d.\nThis differs from prior work (Shinn et al., 2023)\nwhich uses reflection memory as sticky entries in\nLLM prompts across all time steps.\n6\nExperiments\nWe start with categorizing tasks by their planning\ncomplexities to have an isolated testbed. Then we\nexperiment with our staged planning in Sec. 6.3-\n6.4. Finally, we examine if our zero-shot agent can\nlearn from mistakes in Sec. 6.5. Our prompts are\nin Appx. A-E. Complete results are in Appx. F.\n3Up to this point, the work flow is similar to the refine-\nthen-resume in ADAPLANNER (Sun et al., 2023).\n4A possible reason is that the instruction set in LLM\nprompt is already dense and reflection prompt tends to be\nlong, thus such nuanced requirements sometimes get ignored.\n5The downside is that agent can potentially loop over two\nnon-click actions across multiple reflection trials.\n6.1\nSetup\nWe focus on 43 MINIWOB++ tasks that are suit-\nable for evaluating language-based models. This\ndiffers from prior work since we excluded those\n1) require visual signal to solve (e.g., count-shape\nand grid-coordinate); and 2) expose insufficient\nlanguage API to operate (e.g., enter-date and enter-\ntime); The motivation for this filtering is simple:\neven if some filtered tasks can be solved by an\nLLM agent, it does not generalize. Furthermore,\nwe do not include terminal as the synthetic console\nsupports a very limited set of commands while the\nLLM, in our preliminary experiment, tends to use\nmore sophisticated ones.\nWe separate these 43 tasks into three categories:\n1) 1-screen-1-step, 2) 1-screen-n-step, and 3) n-\nscreen-n-step. If the task involves state update (e.g.\nexpanding dropdown list or openning hidden tab),\nthe task is n-screen. If the task can be solved by\njust one action, it is 1-step; otherwise n-step. The\ntask distribution is reported in Tab. 4.6\nFor each task, we evaluate with 25 different ran-\ndom seeds, starting from seed=1000, similar to\nPix2Act (Shaw et al., 2023). Performances are re-\nported as the correct completion rate over multiple\nruns. For validation and prompt design, we use\nseed P r0, 10s. For the LLM agent, we use the\nFLAN-PaLM2 L (Anil et al., 2023) with tempera-\nture 0 across all evaluations for better reproducibil-\nity.\n1-screen-1-step 1-screen-n-step n-screen-n-step\n#Task\n10\n20\n13\nTable 4: Task distribution for each category in MINIWOB++.\n6.2\nModel Comparison\nFor each task category, we compare with prior\nbest models that rely on language as input sig-\nnal, including supervised models, i.e., WEBN-\nT5 (Gur et al., 2022) and CC-NET (Humphreys\net al., 2022), and agents based on prompted infer-\nence, i.e., RCI (Kim et al., 2023) with GPT-3.5\nand ADAPLANNER (Sun et al., 2023). For few-\nshot models, we focus on comparing with agents\nthat have reflection capacity. Non-reflective agents,\nsuch as SYNAPSE (Zheng et al., 2023), have tech-\nniques that are orthogonal to our work, and thus can\n6Based on our categorization, the screen issue (Sec. 1)\nimpacts the n-screen-n-step category.\nFigure 4: Performance on 1-screen-1-step tasks. click-widget\ncontains ambiguous task objective, thus reflection helps.\npotentially be combined with ours. Furthermore,\nwe notice each work often used slightly different\nset of tasks. For a fair comparison, we will also\nreport performances on the shared set of tasks.\n6.3\nSingle Step Tasks\nWe compare our zero-shot agent on the easiest cat-\negory (1-screen-1-step) tasks against recent state-\nof-the-art. As shown in Fig. 4, our agent achieves\n100% accuracy on correctly finishing 9 tasks, even\nwithout reflection. One exception is the ambigu-\nous click-widget which, without in-context trace\nprompt, can be easily failed. For instance, the task\ncould ask agent to click on text widget, however, in-\nput text and text area are not deemed correct. With\n3 rounds of reflection trials, our agent achieved\n96% completion rate on it. Overall, we have 96.4%\nwith 1 trial, and 99.6% with 3 trials. In compari-\nson, with few-shot trace examples, RCI (Kim et al.,\n2023) achieved 99.8% with 1 round of reflection\n(at the plan level).\n6.4\nIterative Planning v.s. Staged Planning\nWe compare these two approaches using 1-screen-\nn-step tasks. We hope these experiments can an-\nswer that, with a given state, whether one should\nquery the agent for actions one at a time or once\nfor all. We compare the prior state-of-the-art works\nwith our staged planning in Tab. 5, showing that\none can simply plan out all executable actions on a\nscreen and \u201cblindly\u201d execute them. Doing so can\nsubstantially reduce LLM queries and still achieve\nhigh completion rate.\nWe report detailed completion rate on all 20 1-\nscreen-n-step tasks in Fig. 5. Our agent achieved\n94.0% completion in 1 trial, and 96.2% in 3 trials.\n6.5\nReflective Planning on Challenging Tasks\nHere, we move on to more challenging (n-screen-\nn-step) tasks to show the impact of our efficient\nSupervised\nFew/N-shot\nZero-shot (Ours)\nWebN-T5\nCC-Net\nRCI\nAdaPln\nT \u201c 1\nT \u201c 3\n60.4\n95.1\n96.1\n96.5\n95.3\n97.3\nTable 5: Average performance on 1-screen-n-step tasks, 16\nshared across all models. T: number of trials.\nFigure 5: Performance on 1-screen-n-step tasks.\nreflection. Task-wise completion rate is reported\nin Fig. 6. Firstly, we observe without examplar\ntraces, zero-shot agent tends to fail on the first trial.\nThis happens often in tasks that requires exploring\nacross multiple screens, e.g., click-menu-2, click-\ntab-2-hard, and search-engine. After a few rounds\nof exploration, our agent achieved substantially bet-\nter completion rate by avoiding previous negative\nsignals recorded in the memory. Our agent contin-\nues to improve even with T \u201c 5, suggesting more\nefficient reflection than prior work e.g., RCI only\ncapable of one round of reflection at plan level.\nSupervised\nFew/N-shot\nZero-shot (Ours)\nWebN-T5\nCC-Net\nRCI\nAdaPln\nT \u201c 1\nT \u201c 5\n31.0\n97.2\n85.8\n89.7\n73.5\n87.3\nTable 6: Comparison on 11 shared tasks across different\nmodels in the n-screen-n-step category. T: number of trials.\nAgain, we compare with prior best models in\nTab. 6. The few-shot models exploited inconsistent\nscreens (as discussed in Sec. 1), thus our work is in\nan unfair disadvantage against them. Despite such\ndisadvantage, our agent achieved performance com-\nparable to them. Importantly, our agent does not re-\nquire in-context trace examples for few-shot, some-\ntimes many-shot, prompting, and no customized\nand detailed environment feedback. Finally, we\nnote that the gap on complex tasks between super-\nvised model and unsupervised ones is still large.\nFigure 6: Performance on n-screen-n-step tasks.\n7\nAnalysis & Discussions\n7.1\nAblation on Reflection Strategies\nHere, we compare our structured reflection against\nthe \u201coriginal\u201d reflection mechanism. We should\nnote that reflection is a general scope that has dif-\nferent formations (e.g. Shinn et al., 2023; Madaan\net al., 2023) and was introduced on environments\n(e.g., ALFWORLD) that are significantly different\nfrom MINIWOB++. Moreover, it was often used\nalong with iterative planning strategy, which is not\ndirectly compatible with our staged planning.\nFigure 7: Comparison of reflection strategies with T \u201c 3.\nTherefore, we use an adapted version for com-\nparison: an agent that uses structurally managed\ntimestep7 while structurally thought management8\nis turned off. This setting is the comparison be-\ntween Both v.s. w/o structured mem in Fig. 7 where\nwe select 5 challenging tasks and run 25 seeds for\neach setting. Clearly, our structured reflection is a\nbeneficial add-on.\n7We insert reflection thought at the corresponding time step\nso that actions before this time step can be deterministically\nreplayed for better efficiency.\n8On top of the structurally managed timestep, we also\nmanage the expiration of thoughts over multiple trials, as well\nas constraining action space.\n7.2\nAblation on Action Constraint\nA useful technique we proposed in Sec. 5.2 is to\ndelete the id field in the HTML pseudo code to\nheuristically discourage LLM agent from issuing\nthe corresponding action. This is essentially mini-\nmal change to the input. In Fig. 7, we ablate on this\nsmall change by comparing Both v.s. w/o action\nconstraint and show that it is beneficial to apply\nthe action constraint.\n7.3\nStatistical Significance over Trials\nWe evaluate statistical significance across various\ntrials on the n-screen-n-step tasks. For each task,\nwe consider all 25 example predictions. This gives\nus 13 \u02c6 25 \u201c 325 samples for each comparison.\nUsing t-test (Dror et al., 2018), the results are in-\ndeed significant (p \u0103 0.05) as shown in Tab. 7. For\ntask-wise significance, see Appx. G.\nBaseline Hypothesis p-value\nT=1\nT=3\n2e-10\nT=3\nT=5\n0.002\nT=1\nT=5\n2e-12\nTable 7: Significance test using t-test comparing different\nnumber of trials.\n7.4\nPlanning Call Reduction\nIn Tab. 8, we highlight the efficiency boost by su-\ning our staged planning formulation. We illustrate\nthe result on 1-screen-n-step tasks that require rela-\ntively long action traces (\u011b 7 actions) on a single\nscreen, and compare the number of planning calls\nfor completed traces as well as failed traces.\nT=1 (Success-only)\nT=3\nTask\nIP\nSP\n\u00d3\nIP\nSP\n\u00d3\nclick-checkboxes-large 234 24\n89.7%\n270 31 88.5%\nclick-checkboxes-soft\n81\n19\n76.5%\n167 64 61.7%\nmulti-layout\n175 25\n85.7%\n175 25 85.7%\nclick-checkboxes-soft\n114 20\n82.5%\n224 64 71.4%\nTable 8: Planning call reduction by staged planning. Com-\nparisons are on the successful first trials and all trials when\nT \u201c 3, using 25 examples per task. IP: number of planning\ncalls required for iterative planning. SP: number of planning\ncalls in staged planning. \u00d3: percentage of planning calls re-\nduced by staged planning.\n7.5\nCompact Screen & Input Length Limit\nRepresenting user interface as HTML puts a high\ndemand on LLM\u2019s context capacity. One instance\nis the social-media-all task that can span more than\na dozen candidates, each with multiple options.\nAs a result, flattening the complete set of state-\naction pairs can easily run over the input limit for\nthe reflection agent, since it needs to observe the\nentire trace. On this task, we noticed that nuanced\nactions does not substantially change the screen.\nTherefore, we always stick to the first screen when\nconstructing prompt for the reflection agent. A\nmore autonomous method can be state filtering in\nSYNAPSE (Zheng et al., 2023).\nLining up the separation of what HTML ele-\nments to expose to LLM is important for evalua-\ntion. As we have seen that many of the MiniWoB++\ntasks are already easy for today\u2019s LLM. Exposing\nmore unseen elements risks hiding the actual chal-\nlenge in the navigation tasks. For instance, expos-\ning unseen elements basically simplifies n-screen-\nn-step tasks into 1-screen-n-step ones. However,\nour experiment shows that n-screen-n-step ones are\nactually much harder to deal with.\n7.6\nCapacity of Staged Planning\nTo better understand the planning capacity and\nlimit of our staged planning, we experiment with\n1-screen-n-step tasks that have extensive number of\ncandidates. Namely, we use click-checkboxes and\nsocial-media as probe tasks, and report in Tab. 9.\nBoth tasks are multi-choice, but differ in their can-\ndidate structure complexity.\nTask\n#Gold\nCompletion rate\nclick-checkboxes\n\u0103 10\n100\nclick-checkboxes\n\u011b 10\n90\nTask\n#Candidate Completion rate\nsocial-media-all\n\u0103 10\n80\nsocial-media-all\n\u011b 10\n40\nTable 9: Impact of number of candidate/gold actions on task\ncompletion. We evaluated 20 examples for each setting, and\nT \u201c 1.\nFor the click-checkboxes task, we separate ex-\namples by their number of actions required9. The\nscreen representation for this task is relatively sim-\nple as each checkbox corresponds to a line of text.\nThis differs from the social-media task where each\ncandidate has multiple actionable, sometimes am-\nbiguous, items, thus putting a stronger requirement\nto LLM for disambiguation. We observe a pattern\nin Tab. 9 that with flat and less ambiguous screen,\nLLM has high capacity to accurately plan out mul-\n9which can be heuristically parsed from the task command.\ntiple steps in one inference call. In such case, one\ncould just execute all planned steps without need-\ning repetitive planning calls. But with complex\nscreen constructions, the capacity of one-pass plan-\nning is reduced by a large margin. Prior work (i.e.\nRCI) constrained the number of candidates in the\nsocial-media task to r3, 6s. We observe that relax-\ning such constraint introduces significant difficulty\nfor planning. Therefore, multiple trials of reflection\ncan help the agent in these complex scenarios.\n8\nConclusions\nWe proposed the first zero-shot agent for computer\ncontrol tasks. Our agent design generalizes the\nworkflow for easy and complex tasks via efficient\nplanning and structured thought management. We\nevaluated our agent on the MINIWOB++ bench-\nmark, showing that our agent, with often one pass\nof planning query, outperforms the best iterative\nplanning agent as well as supervised state-of-the-\nart on simple tasks. For complex tasks, we show\nthat our agent design performs on par with the best\nLLM-based model via more efficient planning and\nreflection, without requiring manually crafted trace\nprompts and ad-hoc environment feedback.\n9\nLimitations\n9.1\nOther LLM Choices\nWe focused on evaluations based on PaLM-2. Re-\ncent advances in LLM agents (e.g., Wei et al., 2022;\nYao et al., 2023; Shinn et al., 2023) have shown that\ndifferent LLMs (e.g., PaLM, GPT-3/4, Codex) gen-\nerally exhibit a common capacity to benefit from\nintermediate thoughts and self-criticism. We be-\nlieve there is a reasonable adaptation of our findings\non other LLMs.\n9.2\nOther Modalities of Input\nLarge multimodal models can take additional in-\nputs such as screen images, and prior works (e.g.,\nCC-Net (Humphreys et al., 2022)) have shown that\nextra modality can indeed be beneficial. However,\neven with recent designs of large multimodal mod-\nels, explicit reasoning still takes place in the form\nof language. Therefore, our proposal could benefit\nin such multimodal use cases.\n9.3\nIntegration Zero-shot Chain-of-Thought\nPrior zero-shot works (e.g., Huang et al., 2022;\nWang et al., 2023b; Crispino et al., 2023) discov-\nered LLMs can be used to expand prompts with\nprior knowledge and intermediate steps to work\nin a zero-shot manner. Theoretically, this line of\nworks can also be integrated into our reflective\nagent to promote completion rate on the first trial.\nOne potential challenge is that computer control\ntasks, looking at the input texts, are quite different\nfrom those in general domain (e.g., sentiment clas-\nsification, numerical reasoning). Thus, the quality\nof extracted prior knowledge needs to be evaluated.\nWe leave this direction to be explore in future work.\n9.4\nConstraining Space for Non-click Actions\nIn Sec. 5.2, we let the reflection module to interact\nwith the environment, explicitly disabling failed\nclick actions by removing the \u201cid\u201d field on respec-\ntive elements. This often helps our agent avoid\nrepeating the same mistakes, but only for click ac-\ntions.\n9.5\nMore End-to-end Tasks\nRecent few-shot works have used techniques to\nextract informative reference traces, either from\nexpert or agent exploration (Zheng et al., 2023),\nto progress more end-to-end computer tasks, such\nas book-flight. We observe such end-to-end tasks\nremains a significant challenge to zero-shot agent.\n9.6\nHigher-order Action Cycle\nIn Sec. 5, we proposed a structured thought man-\nagement to facilitate agent\u2019s self-reflection. While\nthis module can effectively help LLM agent avoid\nrepeating prior mistakes, there are corner cases\nneed to be covered. In rare cases, we observed the\nagent can loop over two failed and different traces\nby accidentally clearing up prior reflection mem-\nory. This is because our agent considers reflections\non later time steps outdated once there is a reflec-\ntion entry for earlier time step. Future work can\nuse additional trace memory to avoid such corner\ncases.\nAcknowledgements\nWe thank the reviewers of EMNLP for constructive\ncomments and pointers.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nAnthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian\nIbarz, Alex Irpan, Eric Jang, Ryan Julian, et al. 2023.\nDo as i can, not as i say: Grounding language in\nrobotic affordances. In Conference on Robot Learn-\ning, pages 287\u2013318. PMLR.\nMarc-Alexandre C\u00f4t\u00e9, \u00c1kos K\u00e1d\u00e1r, Xingdi Yuan,\nBen A. Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew J. Hausknecht, Layla El Asri, Mah-\nmoud Adada, Wendy Tay, and Adam Trischler. 2018.\nTextworld: A learning environment for text-based\ngames. In CGW@IJCAI.\nNicholas Crispino, Kyle Montgomery, Fankun Zeng,\nDawn Song, and Chenguang Wang. 2023. Agent\ninstructs large language models to be general zero-\nshot reasoners. arXiv preprint arXiv:2310.03710.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,\nSamuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\n2023. Mind2web: Towards a generalist agent for the\nweb.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi\nReichart. 2018. The hitchhiker\u2019s guide to testing sta-\ntistical significance in natural language processing.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1383\u20131392, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka\nMatsuo, Shixiang Shane Gu, and Izzeddin Gur. 2023.\nInstruction-finetuned foundation models for multi-\nmodal web navigation. In ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foun-\ndation Models.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-\ndra Faust. 2023. A real-world webagent with plan-\nning, long context understanding, and program syn-\nthesis. arXiv preprint arXiv:2307.12856.\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa\nSafdari, Austin Huang, Aakanksha Chowdhery, Sha-\nran Narang, Noah Fiedel, and Aleksandra Faust.\n2022. Understanding html with large language mod-\nels. arXiv preprint arXiv:2210.03945.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents. In International Conference on Ma-\nchine Learning, pages 9118\u20139147. PMLR.\nPeter C Humphreys, David Raposo, Tobias Pohlen, Gre-\ngory Thornton, Rachita Chhaparia, Alistair Muldal,\nJosh Abramson, Petko Georgiev, Adam Santoro, and\nTimothy Lillicrap. 2022. A data-driven approach for\nlearning to control computers. In Proceedings of the\n39th International Conference on Machine Learning,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 9466\u20139482. PMLR.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.\n2023. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, R\u00e9mi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022. Competition-level code generation with\nalphacode. Science, 378(6624):1092\u20131097.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj\nAmmanabrolu, Faeze Brahman, Shiyu Huang, Chan-\ndra Bhagavatula, Yejin Choi, and Xiang Ren. 2023.\nSwiftsage: A generative agent with fast and slow\nthinking for complex interactive tasks. In Advances\nin Neural Information Processing Systems.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tian-\nlin Shi, and Percy Liang. 2018. Reinforcement learn-\ning on web interfaces using workflow-guided explo-\nration. arXiv preprint arXiv:1802.08802.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nPeter Shaw, Mandar Joshi, James Cohan, Jonathan Be-\nrant, Panupong Pasupat, Hexiang Hu, Urvashi Khan-\ndelwal, Kenton Lee, and Kristina Toutanova. 2023.\nFrom pixels to ui actions: Learning to follow instruc-\ntions via graphical user interfaces. arXiv preprint\narXiv:2306.00245.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Her-\nnandez, and Percy Liang. 2017. World of bits: An\nopen-domain platform for web-based agents. In In-\nternational Conference on Machine Learning, pages\n3135\u20133144. PMLR.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023.\nReflexion: an autonomous agent with dy-\nnamic memory and self-reflection. arXiv preprint\narXiv:2303.11366.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9,\nYonatan Bisk,\nAdam Trischler,\nand Matthew\nHausknecht. 2021. Alfworld: Aligning text and em-\nbodied environments for interactive learning. In In-\nternational Conference on Learning Representations\n(ICLR).\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai,\nand Chao Zhang. 2023. Adaplanner: Adaptive plan-\nning from feedback with language models. arXiv\npreprint arXiv:2305.16653.\nBryan Wang, Gang Li, and Yang Li. 2023a. Enabling\nconversational interaction with mobile ui using large\nlanguage models. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems,\npages 1\u201317.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n2023b. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2609\u20132634, Toronto,\nCanada. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nLongtao Zheng, Rundong Wang, and Bo An. 2023.\nSynapse:\nLeveraging few-shot exemplars for\nhuman-level computer control.\narXiv preprint\narXiv:2306.07863.\nA\nPrompt for Action Space\nFig. 8 contains the prompt prefix that lines out three\naction types.\nB\nPrompt for Staged Planning\nFig. 9 contains the prompt prefix for maximally\nplan out actions on each screen in one pass.\nC\nPrompt for Action Paraphrase\nFig. 10 contains the prompt prefix for maximally\nplan out actions on each screen in one pass.\nD\nStatus Prompt Prefix\nFig. 11 contains prompt component we use to com-\npose reflection prompt for each type of environ-\nment feedback.\nE\nPrompt for Reflection\nFig. 12 contains the prompt prefix for our structured\nreflection.\nYou can generate a series of atomic actions to fulfill a top-level goal. There are three types of atomic actions you can\nperform. Firstly, you can click an object by referring to its id, such as \"click id=...\". Secondly, you can enter text to\nan input field, such as \"enter \"...\" to id=...\".\u2019 Specifically, you should always wrap the text you want to type in with\ndouble quotes. Lastly, you can operate special keys on the keyboard, such as \"hold CTRL\" and \"release CTRL\" before\nand after multiple selections. If dropdown list is available, you can \"press ARROWUP x N\" or \"press ARROWDOWN\nx N\" to press the arrow key N times to iterate over list items, and then \"press ENTER\" to select the current item.\nFigure 8: Prompt that defines three action types.\nNow, you need to plan actions that are executable on and only on this screen. For actions that are not executable on\nthis screen, you should leave them to future planning. Your plan should consist of a list of atomic actions on the screen.\nPlease separate them by newline.\nFigure 9: Prompt that promotes staged planning.\nF\nCompletion Rate Tables\nWe report the completion rate on 43 tasks in MINI-\nWOB++ in categories. Performances in 1-screen-\n1-step tasks are in Tab. 10, 1-screen-n-step task in\nTab. 11, and n-screen-n-step tasks in Tab. 12.\nG\nTask-wise Statistical Significance\nIn Tab. 13, we compare T \u201c 1 v.s. T \u201c 5 on\nthe n-screen-n-step tasks. We use the one-tailed\nMcNemar (matched chi-square) for the test.\nYou are capable of describing actions taken on a computer. The computer screen is represented by the following\nHTML pseudo code: <screen> {html} </screen> And the action taken is: {action_str} Now, in plain language, please\nsummarize what has been done. You should describe the specific purpose for the action, instead of simply referring to\nthe element id or position of the element. Summary:\u2019\nFigure 10: Prompt for paraphrasing an executed action. Newlines are filtered.\nSTATUS.FAILED: \"However, your actions did not complete the goal. Now, you need to identify the earliest critical\nstep where you made a mistake, and suggest a correction.\"\nSTATUS.CYCLE: \"However, your actions led you to a loop that did not progress the task. Now, you need to identify\nthe earliest critical step where you made a mistake, and suggest a correction.\"\nSTATUS.NO_CHANGE: \"However, your last action did not cause anything to change on the last screen. You probably\nused the wrong action type. Now, you need to identify the earliest critical step where you made a mistake, and suggest\na correction.\"\nSTATUS.IN_COMPLETE: \"However, your actions did not finish the task, likely more steps are needed. Now, you\nneed to identify the earliest critical step where you made a mistake, and suggest a correction.\"\nSTATUS.IN_PROGRESS: \"However, you took too many steps and yet still did not finish the task. Now, you need to\nidentify the earliest critical step where you made a mistake, and suggest a correction.\"\nSTATUS.EXCEPTION: \"However, your last action is invalid. You should avoid doing that again and try a different\naction.\"\nFigure 11: Prompt component that describes environment status.\nYou are operating a computer for a task: {task_name}. You went over a series of screens and executed actions to fulfill\na top-level goal. Your action trajectory is as follows: ... The index=k screen: {screen_html at k} Your index=k action:\n{action at k} ... You conducted the above actions for the top-level goal: goal {status_str} Your suggestion should be\nin this format: \"For action index=A, you should B.\", where A is the action index, and B is the suggested action you\nshould have taken. Your suggestion:\nFigure 12: Prompt for reflection. Newlines are filtered. status_str is defined in Fig. 11.\nTask T=1 T=3 T=5\nclick-dialog 100 100 100\nclick-dialog-2 100 100 100\nclick-link 100 100 100\nclick-button 100 100 100\nclick-tab 100 100 100\nclick-test 100 100 100\nclick-test-2 100 100 100\nclick-widget\n64\n96\n100\nfocus-text 100 100 100\nfocus-text-2 100 100 100\nTable 10: Completion rate on 1-screen-1-step tasks.\nTask T=1 T=3 T=5\nclick-button-sequence 100 100 100\nclick-checkboxes 100 100 100\nclick-checkboxes-large\n92\n96\n100\nclick-checkboxes-soft\n68\n84\n84\nclick-checkboxes-transfer 100 100 100\nclick-collapsible 100 100 100\nclick-option 100 100 100\nclick-scroll-list 100 100 100\nenter-password 100 100 100\nenter-text 100 100 100\nenter-text-2 100 100 100\nenter-text-dynamic 100 100 100\nfind-word\n56\n72\n72\nlogin-user 100 100 100\nmulti-layouts 100 100 100\nmulti-ordering 100 100 100\nread-table 100 100 100\nread-table-2 100 100 100\nsocial-media-all\n68\n80\n80\nsocial-media-some\n96\n96\n96\nTable 11: Completion rate on 1-screen-n-step tasks.\nTask T=1 T=3 T=5\nclick-collapsible-2\n88\n88\n88\nclick-menu-2\n36\n72\n76\nclick-pie\n96\n96\n96\nclick-tab-2\n44\n60\n68\nclick-tab-2-hard\n32\n56\n64\nemail-inbox\n96\n100 100\nemail-inbox-nl-turk\n76\n92\n92\nemail-inbox-forward-nl 100 100 100\nemail-inbox-forward-nl-turn 100 100 100\nnavigate-tree\n96\n96\n96\nsearch-engine\n20\n64\n72\nsocial-media\n84\n84\n84\nuse-autocomplete\n72\n96\n96\nTable 12: Completion rate on n-screen-n-step tasks.\nTask #Completion (T=1) #Completion (T=5)\np-value\nclick-collapsible-2\n22\n22\n-\nclick-menu-2\n9\n19\n0.008\nclick-pie\n24\n24\n-\nclick-tab-2\n11\n17\n0.007\nclick-tab-2-hard\n8\n16\n0.0024\nemail-inbox\n24\n25\n0.159\nemail-inbox-nl-turk\n19\n23\n0.042\nemail-inbox-forward-nl\n25\n25\n-\nemail-inbox-forward-nl-turn\n25\n25\n-\nnavigate-tree\n24\n24\n-\nsearch-engine\n5\n18\n0.000016\nsocial-media\n21\n21\n-\nuse-autocomplete\n18\n24\n0.007\nTable 13: Significance test using one-tailed McNemar for each n-screen-n-step task.\n"
  },
  {
    "title": "The Consensus Game: Language Model Generation via Equilibrium Search",
    "link": "https://arxiv.org/pdf/2310.09139.pdf",
    "upvote": "11",
    "text": "THE CONSENSUS GAME: LANGUAGE MODEL\nGENERATION VIA EQUILIBRIUM SEARCH\nAthul Paul Jacob\u2217\nMIT\nYikang Shen\nMIT-IBM AI Lab\nGabriele Farina\nMIT\nJacob Andreas\nMIT\nABSTRACT\nWhen applied to question answering and other text generation tasks, language\nmodels (LMs) may be queried generatively (by sampling answers from their output\ndistribution) or discriminatively (by using them to score or rank a set of candidate\noutputs). These procedures sometimes yield very different predictions. How do\nwe reconcile mutually incompatible scoring procedures to obtain coherent LM\npredictions? We introduce a new, a training-free, game-theoretic procedure for\nlanguage model decoding. Our approach casts language model decoding as a\nregularized imperfect-information sequential signaling game\u2014which we term the\nCONSENSUS GAME\u2014in which a GENERATOR seeks to communicate an abstract\ncorrectness parameter using natural language sentences to a DISCRIMINATOR. We\ndevelop computational procedures for finding approximate equilibria of this game,\nresulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a\nlarge number of tasks (including reading comprehension, commonsense reasoning,\nmathematical problem-solving, and dialog), EQUILIBRIUM-RANKING consistently,\nand sometimes substantially, improves performance over existing LM decoding\nprocedures\u2014on multiple benchmarks, we observe that applying EQUILIBRIUM-\nRANKING to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-\n540B models. These results highlight the promise of game-theoretic tools for\naddressing fundamental challenges of truthfulness and consistency in LMs.\n1\nINTRODUCTION\nCurrent language models (LMs) perform quite well on some tasks involving generation or verification\nof factual assertions\u2014including question answering, fact-checking, and even unconditional text\ngeneration. But they are far from perfectly reliable, and there is increasing evidence that LMs\nactually grow more prone to generating false but frequently repeated statements with increasing\nscale (McKenzie et al., 2023). Further complicating matters, LMs offer multiple affordances for\nsolving factual generation tasks. They may be used both generatively (e.g. by asking for the most\nprobable answer to a question) or discriminatively (e.g. by presenting a (question, answer) pair and\nasking whether the answer is acceptable) and, these two procedures do not always produce consistent\nresults: generative procedures may fail when probability mass is spread across multiple contradicting\nanswers (Wang et al., 2022; Mitchell et al., 2022), while discriminative procedures may fail due to\nmiscalibration (Han et al., 2022; Chen et al., 2022) or subtle dependence on question wording (Jiang\net al., 2020). Given these noisy and often-conflicting signals, how should we distill out an LM\u2019s best\nguess at the truth?\nThis paper presents an approach for reconciling generative and discriminative LM decoding proce-\ndures by formulating decoding as a signaling game (Lewis, 2008) that we call the CONSENSUS GAME.\nAt a high level, this game features a GENERATOR agent that must communicate an abstract correct\nor incorrect value to a DISCRIMINATOR agent, but may only do so using a set of candidate natural\nlanguage strings (Fig. 1). Intuitively, an effective strategy for this game (i.e. a joint policy) is one in\nwhich the GENERATOR and DISCRIMINATOR agree on the assignment of strings to correctness values.\nGiven such a strategy, we may inspect it to identify candidates agreed by consensus to be correct.\nDoing so requires solving a multi-step game with a complex (string-valued) action space. In recent\nyears, no-regret learning algorithms have emerged as the preferred technique to compute effective\n\u2217Correspondence to: apjacob@mit.edu\n1\narXiv:2310.09139v1  [cs.GT]  13 Oct 2023\ncorr.\nincorr.\nQ: where was Barack Obama born? \nin Honolulu\nin Chicago\nin Nairobi\nI\u2019m not sure\nGENERATOR\nDISCRIMINATOR\nEQUILIBRIUM-RANKING\nREGULARIZED EQUILIBRIUM SEARCH\nN\nG\nD\n             in \nHonolulu\nin \n Nairobi\n(1, 1)\nD\ncorr.\nincorr.\ncorrect\nD\nD\nincorrect\nG\nin Honolulu\nin Chicago\nin Nairobi\nI\u2019m not sure\nin Honolulu\nin Chicago\nin Nairobi\nI\u2019m not sure\ncorr.\nincorr.\ncorr.\nincorr.\ncorr.\nincorr.\n(0, 0)\n(1, 1) (0, 0)\n(1, 1) (0, 0)\n(1, 1) (0, 0)\nFigure 1: (Left) Overview of our approach. Differing LM queries fail to exhibit consensus about the answer\nto a factual question. By reconciling predictions between generative and discriminative LM queries using the\nCONSENSUS GAME, we obtain an accurate prediction. (Right) Structure of the CONSENSUS GAME, a two-\nplayer sequential signaling game with imperfect information. First, the environment (N) uniformly samples a\ncorrectness parameter. A GENERATOR (G) conditioned on this parameter produces a natural language string from\na set of candidates. The DISCRIMINATOR (D) only observes this string and must predict the correctness parameter\nsampled by environment. If the DISCRIMINATOR correctly identifies this parameter, then both players receive\na reward of 1. The dashed line connects nodes that are indistinguishable by the DISCRIMINATOR, since the\nDISCRIMINATOR does not observe the correctness parameter. By computing regularized equilibrium strategies\nfor this game, we obtain predictions that reflect a consensus between the GENERATOR and DISCRIMINATOR.\nstrategies for such games, and have been successfully deployed in Poker (Brown & Sandholm,\n2018; 2019), Stratego (Perolat et al., 2022), and Diplomacy (Bakhtin et al., 2023; FAIR et al., 2022;\nJacob et al., 2022). Here, we show that they can also be applied to free-form language generation\ntasks. We call this game-theoretic approach to LM decoding EQUILIBRIUM-RANKING. Applied in 6\nquestion answering benchmarks: MMLU (Hendrycks et al., 2020), ARC (Clark et al., 2018), RACE\n(Lai et al., 2017), HHH (Askell et al., 2021), TruthfulQA (Lin et al., 2022) and, GSM8K (Cobbe\net al., 2021), EQUILIBRIUM-RANKING offers substantial improvements over existing generative,\ndiscriminative, and mixed decoding procedures. More generally, our results highlight the usefulness\nof the game-theoretic toolkit for formalizing and improving coherence in LMs. Improved coherence\nin turn leads to improved accuracy on factual tasks.\n2\nLANGUAGE MODEL CONSENSUS AS EQUILIBRIUM SEARCH\nWe study the problem of obtaining correct output from a language model, which maps input strings\nx to output strings y according to some distribution PLM(y | x). While the techniques we present\nhere are general, we focus in this paper on question answering problems consisting of a query x (In\nwhich of the following cities was Barack Obama born?) and a set of candidate answers Y (Honolulu,\nChicago, .. .) which may themselves have been sampled from the complete PLM(\u00b7 | x). Given a set\nof candidates, we may them with an LM in (at least) two ways:\n\u2022 Generatively, by supplying as input (i) the query x, (ii) the set of candidates Y, and (iii) a\nnatural language prompt indicating that a correct answer is desired. In this case, the LM may be\nthought of as modeling a distribution PLM(y | x, correct), where the token correct denotes\nthe fact that the model was prompted to generate a correct answer.\n\u2022 Discriminatively, by supplying as input (i) the query x and (ii) a possible candidate an-\nswer y \u2208 Y, together with (iii) a prompt indicating that a correctness assessment v \u2208\n{correct, incorrect} is sought. In this case, the language model acts as a model of as\nmodeling a distribution PLM(v | x, y) where v \u2208 {correct, incorrect}.\nThese two approaches are conceptually equivalent. But as noted in the introduction, current LMs\nmay give very different answers when queried in these different ways: answers produced generatively\nmight be assessed incorrect with high probability or vice-versa. Research on LMs has proposed\ntwo broad solutions to this problem. Ensembling methods (Ouyang et al., 2022; Li & Jurafsky, 2016;\n2\nGlaese et al., 2022) simply combine discriminative and generative scores directly. While moderately\neffective, such approaches suffer from the fact that LM predictions are often poorly calibrated both\nwithin and across contexts, meaning that scores may not combine in meaningful or consistent ways.\nDeliberation methods (Wei et al., 2022; Yao et al., 2023; Du et al., 2023) perform this reconciliation\nwithin the LM itself, e.g. by re-prompting with competing inputs and an instruction to generate a\ntextual justification for the best one. Such methods incur significant computational overhead.1\nHow might we design a principled and computationally efficient procedure for obtaining a \u201cconsensus\u201d\nbetween competing LM predictions? Informally, a consensus prediction would satisfy two key\nproperties: coherence (generative and discriminative scoring procedures should agree about which\ncandidate answers are correct) and reasonableness (predictions should not be arbitrary, but instead\nas close as possible to original generator / discriminator behavior). The key idea in this paper is to\noperationalize these high-level desiderata in language of game theory, using regularized equilibrium\nconcepts as formal framework for defining both coherence and reasonableness. Below, we introduce\nand explain this framework in detail, describing how to instantiate decoding as a signaling game, then\ncompute equilibrium strategies of this game to obtain consensus LM predictions.\n2.1\nTHE CONSENSUS GAME\nOur approach to language generation begins by formulating language generation as a signaling\ngame (Lewis, 2008) that we call the CONSENSUS GAME. The CONSENSUS GAME is played on\na game tree, as depicted in Figure 1. At the start of the game (that is, at the root of the game\ntree), a correctness parameter v \u2208 {correct, incorrect} is selected uniformly at random by the\nenvironment. The correctness parameter is observed only by the GENERATOR, and controls whether\nthe GENERATOR should aim to generate correct or incorrect answers. Upon observing this\nparameter, the GENERATOR produces a natural language string from a fixed set of candidates. Finally,\nthis string is observed by the DISCRIMINATOR, who tries to guess the value of the correctness\nparameter by selecting one of {correct, incorrect} as an answer. Both players obtain a payoff of\n1 if the DISCRIMINATOR correctly identifies the value of the correctness parameter, 0 otherwise.\nWith this definition, it may be observed that players\u2019 expected utilities (the payoffs they may expect\nto receive) are as follows:\nuG(\u03c0G, \u03c0D) := 1\n2\nX\nv\u2208{correct,incorrect}\nX\ny\u2208Y\n\u03c0G(y | v) \u00b7 \u03c0D(v | y),\nuD(\u03c0G, \u03c0D) := 1\n2\nX\nv\u2208{correct,incorrect}\nX\ny\u2208Y\n\u03c0G(y | v) \u00b7 \u03c0D(v | y).\nWhat is an effective strategy for maximizing these utilities? A standard answer to this question\nin the game theory literature is that a Nash equilibrium of the game should be sought. A Nash\nequilibrium is a pair of policies\u2014one for the GENERATOR and one for the DISCRIMINATOR\u2014such\nthat each policy is optimal. That is, each player\u2019s strategy maximizes their expected given the other\nplayer\u2019s strategy. At a Nash equilibrium, no player has an incentive to unilaterally behave in any\nother way. In signaling games, Nash equilibria offer a natural way of formalizing the coherence\ncriterion above: at equilibrium, both the GENERATOR and DISCRIMINATOR must agree about which\nmessages correspond to correct and incorrect respectively in order to obtain a nonzero payoff.\nHowever, Nash equilbria of the CONSENSUS GAME are not guaranteed to provide the second criterion\nof reasonableness. This is because the CONSENSUS GAME admits a multitude of Nash equilibria that\nare incompatible with the common-sense notion of truthfulness. For example, the strategy in which\nthe GENERATOR deterministically maps correct 7\u2192 \u201cNairobi\u201d, incorrect 7\u2192 \u201cHonolulu\u201d, and the\nDISCRIMINATOR maps \u201cNairobi\u201d 7\u2192 correct, \u201cHonolulu\u201d 7\u2192 incorrect forms a Nash equilibrium.\nIn order to sidestep the inappropriate equilibria and ensure reasonableness, we introduce a regular-\nization term in the utility of the players, so that both the GENERATOR and the DISCRIMINATOR are\npenalized for settling on strategies that are far from some pair of initial policies: \u03c0(1)\nG\nand \u03c0(1)\nD . By\nparameterizing these policies using a pre-trained LM, we may use knowledge about what answers\nare likely to be correct a priori to guide selection of an equilibrium. As in Jacob et al. (2022), we\n1As shown in Section 3, they are also orthogonal to, and composable with, the approach we propose here.\n3\nincorporate this regularization term directly into the utility function (payoff) that the GENERATOR and\nDISCRIMINATOR attempt to optimize. Rather than the simple 0\u20131 payoff determined by agreement\non the correctness parameter, they now attempt to optimize:\nuG(\u03c0G, \u03c0D) := \u2212\u03bbG \u00b7 DKL[\u03c0G(\u00b7 | v), \u03c0(1)\nG (\u00b7 | x, v)] + 1\n2\nX\nv\nX\ny\u2208Y\n\u03c0G(y | x, v) \u00b7 \u03c0D(v | x, y),\nuD(\u03c0G, \u03c0D) := \u2212\u03bbD \u00b7 DKL[\u03c0D(\u00b7 | y), \u03c0(1)\nD (\u00b7 | x, y)] + 1\n2\nX\nv\nX\ny\u2208Y\n\u03c0G(y | x, v) \u00b7 \u03c0D(v | x, y).\nNote that the initial policies \u03c0(1)\nG (y | x, v) and \u03c0(1)\nL (v | x, y) may be derived from an LM prompted\nwith some initial string x, in order to obtain context-predictions (e.g. answers to a question). With\nthese utilities, Nash equilibria for the game are pulled by the initial GENERATOR and DISCRIMINATOR\npolicies in the direction of increased consensus.\nBakhtin et al. (2023) and FAIR et al. (2022) employed a similar regularization method for choosing\nactions, rather than messages, in versions of the board game Diplomacy. Franke (2013; 2017) have\nexplored signaling games in the context of linguistic pragmatics to explain human language use.\nTo the best of our knowledge, however, this is the first proposal for using regularized equilibrium\nconcepts in signaling games to define target behavior in a language generation task.\n2.2\nEQUILIBRIUM-RANKING: LM RANKING VIA EQUILIBRIUM SEARCH\nWith this formulation, text generation requires finding a Nash equilibrium of the game with the\nutilities given above. How should we compute such an equilibrium? No-regret learning algorithms\nhave emerged in recent years as the preferred technique to approximate equilibria in large games, and\nhave been successfully employed to solve games at human or even superhuman level. At a high level,\nthese algorithms find equilibrium by repeatedly interacting in the game and refining their policies\nafter each iteration t. so as to minimize regret (the gap between the chosen action and the best action\nin hindsight).\nIn this section, we describe in detail how to perform no-regret learning in the CONSENSUS GAME in\norder to obtain consensus policies. Importantly, this approach modifies only signalling policies, and\nnot the base policies \u03c0(1)\nG and \u03c0(1)\nD\n(i.e. the LM). In this sense, generating text by performing no-regret\nlearning in the CONSENSUS GAME might be described as a training-free consensus-planning method.\nWe call this method EQUILIBRIUM-RANKING.\nInitial policies\nAt time t = 1, that is, before any equilibrium computation has happened,\nEQUILIBRIUM-RANKING defines the initial policies \u03c0(1)\nG\nand \u03c0(1)\nD\nof the GENERATOR and DIS-\nCRIMINATOR, respectively, as follows. \u03c0(1)\nG\nnormalizes PLM2 across v and y:\n\u03c0(1)\nG (y | x, v) \u221d\nPLM(y | x, v)\nP\nv\u2032 PLM(y | x, v\u2032).\nSimilarly for the DISCRIMINATOR, the initial policy normalizes across y and v:\n\u03c0(1)\nD (v | x, y) \u221d\nPLM(v | x, y)\nP\ny\u2032 PLM(v | x, y\u2032).\nThis crucial step enables us to extract a well calibrated GENERATOR and DISCRIMINATOR from PLM.\nThe specific form of the GENERATOR incorporates v = incorrect, and this resembles contrastive\ndecoding (Li et al., 2022), where they rely on a weaker LM as opposed to an LM conditioned on\nincorrect (See, Section 3 for details). This DISCRIMINATOR resembles approaches that query the\nLM itself to produce critiques (Ganguli et al., 2023; Chen et al., 2023b; Yao et al., 2023). However,\nto the best of our knowledge, this specific instantiation has not been explored in the past.\n2In ARC, RACE, HHH, TruthfulQA, and GSM8K, based on prior work (Touvron et al., 2023; Brown et al.,\n2020), we additionally normalize PLM(u|x) by the likelihood of the completion given \u201cAnswer:\u201d as context:\nPLM(u | \u201dAnswer:\u201d).\n4\nEvolution of policies\nA classic observation in the theory of imperfect-information sequential games\nis that minimization of regret (viewed as a function of their overall policy on the game tree) can\nbe achieved by solving separate, local, regret minimization problems at each information set (i.e.,\ndecision point) of the game. This observation underpins the CFR framework (Zinkevich et al., 2007),\nas well as its generalization to more general convex losses, known as laminar regret decomposition\n(Farina et al., 2019). In our case, these techniques enable us to decompose the policy update of the\nplayers into separate updates for each correctness parameter v (for the GENERATOR) and for each\nsequence y (for the DISCRIMINATOR). We provide more detail and background in Appendix A.\nIn our setting, after operating the regret decomposition step, we find that the local regret minimization\nproblems are composed of a bilinear term, plus a strongly convex KL-regularization term. Such\ncomposite utilities can be handled by the piKL algorithm (Jacob et al., 2022), which is specifically\ndesigned to perform regret minimization on KL-regularized objectives. In our setting, piKL prescribes\nthat each player keep track of their average values:\nQ(t)\nG (y | x, v) := 1\n2t\nt\nX\n\u03c4=1\n\u03c0(\u03c4)\nD (v | x, y),\nQ(t)\nD (v | x, y) := 1\n2t\nt\nX\n\u03c4=1\n\u03c0(\u03c4)\nG (y | x, v).\nEach player then updates their policy according to:\n\u03c0G\n(t+1)(y | x, v) \u221d exp\n(\nQ(t)\nG (y | x, v) + \u03bbG log \u03c0G(1)(y | x, v)\n1/(\u03b7Gt) + \u03bbG\n)\n,\n(1)\n\u03c0D\n(t+1)(v | x, y) \u221d exp\n(\nQ(t)\nD (v | x, y) + \u03bbD log \u03c0D(1)(v | x, y)\n1/(\u03b7Dt) + \u03bbD\n)\n,\n(2)\nwhere \u03b7G, \u03b7D > 0 are learning rate hyperparameters. piKL no-regret dynamics are known to have\nstrong guarantees, including the following (more formal statements about the guarantees are available\nin Appendix A):\n\u2022 Convergence to an equilibrium point. The average correlated distribution of play of GENER-\nATOR and DISCRIMINATOR converges to the set of (regularized) coarse-correlated equilibria of\nthe game.\n\u2022 Regularization toward reasonableness. The average policy of any player remains within a\nradius of size roughly 1/\u03bbi from the initial policy \u03c0(1)\ni\n, where \u03bbi is the amount of regularization\napplied to any player i \u2208 {GENERATOR, DISCRIMINATOR} (see Proposition 3).\n\u2022 Avoidance of regret. The cumulative regret incurred by each of the players grows only\nlogarithmic in the number of training steps (see Proposition 1).\nAt convergence, EQUILIBRIUM-RANKING returns \u03c0G\u2217 and \u03c0D\u2217, which are the refined GENERATOR\nand DISCRIMINATOR. We also remark that a recent result by Anagnostides et al. (2022) showed that\nas long as the regularization function has Lipschitz-continuous gradients (a condition that can be\neasily achieved by introducing a small perturbation in the KL regularization term), decentralized\nlearning dynamics similar to piKL converge in iterates to a regularized Nash equilibrium of the\nCONSENSUS GAME, since it is a potential game (Monderer & Shapley, 1996). In practice, we do not\ninvestigate introducing a small perturbation, as we witness good convergence properties in practice\neven without any perturbation. As mentioned earlier, convergence to a regularized Nash equilibrium is\nimportant to guarantee both coherence and reasonableness. Extensive empirical validation presented\nin the next section demonstrates the benefits of this approach in practice.\nComputational cost of our method.\nAt each iteration, our method needs to update the policies\nQ(t)\nG , Q(t)\nD according to (1) and (2). The number of operations at each iteration of the method is\ntherefore linear in the number |Y| of sequences available to the GENERATOR.\n3\nEXPERIMENTS\nAs discussed in the previous section, EQUILIBRIUM-RANKING focuses on improving the correctness\nof language models in question-answering (QA) tasks. However, correctness manifests in various\nforms across different domains, including truthfulness, factuality, valid reasoning, value alignment,\n5\namong others. Therefore, we will evaluate its performance on a diverse set of QA tasks: MMLU\n(Hendrycks et al., 2020), ARC (Clark et al., 2018), RACE (Lai et al., 2017), HHH (Askell et al.,\n2021), and TruthfulQA (Lin et al., 2022). It\u2019s important to note that EQUILIBRIUM-RANKING is a\nsampling strategy and not a delibration method like chain-of-thought (CoT) (Wei et al., 2022) and\nself-consistency (Wang et al., 2022). Nevertheless, we will demonstrate in GSM8K (Cobbe et al.,\n2021) that we can achieve some additional gains when combining EQUILIBRIUM-RANKING with\nself-consistency and CoT.\nHyperparameters\nEQUILIBRIUM-RANKING has four parameters, \u03b7D, \u03bbD and \u03b7G, \u03bbG. Although\ntuning these parameters will lead to better performance, in all our experiments we set \u03b7D = \u03bbD =\n\u03b7G = \u03bbG = 0.1. We run EQUILIBRIUM-RANKING for 5000 iterations 3\nActions in the CONSENSUS GAME\nAs mentioned in Section 2, in order to make our approach\namenable to current computational techniques, we make the modeling assumption that the GEN-\nERATOR picks distribution over a finite set of candidates Y. In multiple-choices tasks, these are\nthe multiple choice options. In generative tasks, a common approach to generate the finite set of\ncandidates is via sampling with nucleus (Holtzman et al., 2019) and top-k (Fan et al., 2018b) from\nthe distribution PLM(y | q, correct) where y \u2208 Y. This is exactly the approach we use in our\nexperiments, with p = 0.9 for nucleus sampling and k = 50.\nModels\nWe use the 7B and 13B parameter models from the LLaMA family (Touvron et al., 2023)\nand perform 16-bit inference for all our experiments.\nPrompting for correct and incorrect answers\nIn our work, unless otherwise specified, con-\nditioning on (x, correct) for the PLM corresponds to the standard zero-shot prompt. Similarly,\nconditioning on (x, incorrect) is similar to (x, correct) with the only difference that \u201dAnswer:\u201d\nis replaced with \u201dIncorrect Answer:\u201d in the prompt.\nBaselines\nIn the multiple-choice based datasets (ARC, RACE, HHH, MMLU), we consider the\nfollowing approaches:\n\u2022 Generative Ranking (G): This baseline (Brown et al., 2020; Touvron et al., 2023) ranks every\ncandidate y by PLM(y | x, correct) and picks the top candidate. This is the standard approach\nused in past work. Due to implementational differences, when available, we include both\nofficial scores and our version.\n\u2022 Mutual Information Ranking (MI): This mutual-information based (Li & Jurafsky, 2016)\nbaseline is an ensemble-based approach that reweights every candidate y by PLM(y |\nx, correct) \u00b7 PLM(correct | x, y).\n\u2022 Self-Contrastive Ranking (SC): This approach utilizes the normalized generator \u03c0(1)\nG\nto\nreweight every candidate y by \u03c0(1)\nG (correct | x, y). As discussed in Section 2, this shares\nsimilarities with contrastive decoding (Li et al., 2022).\n\u2022 Discriminative Ranking (D): This approach reweights every query-candidate pair (x, y) by\n\u03c0(1)\nD (correct | x, y).\n\u2022 Equilibrium Ranking Generator (ER-G): Similar to SC, this approach utilizes the fi-\nnal EQUILIBRIUM-RANKING-based generator \u03c0\u2217\nG to reweight every candidate y by \u03c0\u2217\nG(y |\nx, correct).\n\u2022 Equilibrium Ranking Discriminator (ER-D): Similar to D, this approach utilizes the fi-\nnal EQUILIBRIUM-RANKING-based discriminator \u03c0\u2217\nD. This approach reweights every query-\ncandidate pair (x, y) by \u03c0\u2217\nD(correct | x, y).\n3As remarked at the end of the previous section, each iteration of the learning process requires a number of\nfloating-point operations that is linear in the number |Y| available to the GENERATOR. In most of our settings,\n|Y| = 4, making the overhead from the learning dynamics on the CONSENSUS GAME negligible compared to the\ncost of inference for the language model. As such, even with an unoptimized implementation of the dynamics\n(1,2), we observe that the computational cost associated with each iteration of the learning process takes about\n40 microseconds on average.\n6\nIn free-form text generation tasks (TruthfulQA, GSM8K), we additionally consider greedy decoding.\nIn the mathematical reasoning task (GSM8K), we also consider self-consistency (Wang et al., 2022).\nEquil. ranking\nDomain\nModel\nG\u2217\nG\nMI\nSC\nD\nER-G\nER-D\nMMLU\nLLaMA-7B\n\u2013\n30.4\n33.1\n30.5\n40.4\n39.4\n39.9\nLLaMA-13B\n\u2013\n41.7\n41.8\n41.7\n41.9\n44.9\n45.1\nARC\nEasy\nLLaMA-7B\n72.8\n68.2\n68.8\n69.5\n52.5\n71.6\n71.5\nLLaMA-13B\n74.8\n71.2\n71.5\n73.0\n65.0\n76.1\n76.4\nARC\nChallenge\nLLaMA-7B\n47.6\n47.3\n47.4\n56.5\n42.7\n58.7\n58.3\nLLaMA-13B\n52.7\n51.9\n52.1\n59.3\n48.5\n61.1\n61.4\nRACE\nMiddle\nLLaMA-7B\n61.1\n57.7\n57.7\n60.4\n51.5\n63.2\n63.5\nLLaMA-13B\n61.6\n60.1\n60.2\n64.8\n58.3\n67.9\n68.6\nRACE\nHigh\nLLaMA-7B\n46.9\n46.4\n46.3\n53.1\n46.0\n56.3\n56.4\nLLaMA-13B\n47.2\n47.9\n48.4\n58.9\n55.1\n62.1\n62.8\nHHH\nLLaMA-7B\n\u2013\n59.3\n57.9\n67.4\n70.1\n71.5\n71.5\nLLaMA-13B\n\u2013\n60.2\n59.7\n57.9\n69.2\n61.1\n61.1\nTable 1: Results of the different approaches across multiple tasks. We compute the accuracies on the test set\nof these benchmarks. EQUILIBRIUM-RANKING outperforms other approaches on most tasks. EQUILIBRIUM-\nRANKING performs well, even in cases where one of GENERATOR or DISCRIMINATOR is far worse than the other.\nG: Generative Ranking, MI: Mutual Information Ranking, SC: Self-Contrastive Ranking, D: Discriminative\nRanking, ER-G: Equilibrium Ranking Generator, ER-D: Equilibrium Ranking Discriminator. * indicates the\nresults from Touvron et al. (2023). Colors in the table entries are assigned relative to the G baseline, according\nto the colorbar\n-10\n-5\n0\n+5\n+10\n1\n(differences exceeding \u00b110 are clipped to \u00b110 when calculating the colors).\nMMLU\nThe massive multi-task language understanding benchmark (MMLU) (Hendrycks et al.,\n2020) is used to measure language model\u2019s multitask accuracy. It consists of questions in the multiple\nchoice format across a wide variety of subdomains in social sciences, humanities and STEM. We\nevaluate our models in the zero-shot setting following the format described in Hendrycks et al. (2020);\nTouvron et al. (2023) and the results are presented in the first row of Table 1. For both LLaMA-\n7B and LLaMA-13B, the EQUILIBRIUM-RANKING-based approaches matches or outperforms all\nother baselines. In fact, zero-shot LLaMA-7B with ER-D (39.9) outperforms 5-shot LLaMA-7B\n(35.1), while zero-shot LLaMA-13B with ER-D (45.1) is competitive with 5-shot LLaMA-13B\n(46.9). LLaMA-7B with ER-D (39.9) even outperforms zero-shot GPT3-175B (37.7) (Hendrycks\net al., 2020), while zero-shot LLaMA-13B with ER-D (45.1) outperforms 5-shot GPT3-175B (43.9)\n(Hendrycks et al., 2020).\nARC\nThe AI2 Reasoning Challenge (ARC) (Clark et al., 2018) is an advanced question answering\ndataset used to study a model\u2019s knowledge and reasoning abilities based on grade school science\nquestions. It is split in to two subcategories: easy (ARC-Easy) and challenge (ARC-Challenge). The\nchallenge set was constructed as the set of questions that were answered incorrectly by retrieval and\nword co-occurence based algorithms. The results are presented in second and third rows of Table 1.\nOn ARC-Easy, ER-D outperforms our implementation of generative ranking. We also note that\nLLaMA-13B with ER-D (76.4) outperform all the baseline approaches and is even competitive with\nthe much larger PaLM-540B model (76.6) (Chowdhery et al., 2022). On ARC-Challenge, ER-D\nsignificantly outperforms all the baseline approaches. We also note that LLaMA-7B with ER-D (58.3)\nand LLaMA-13B with ER-D (61.4) outperforms even the much larger models: LLaMA-65B (56.0)\n(Touvron et al., 2023) and PaLM-540B (53.0) (Chowdhery et al., 2022) by up to 8%.\n7\nEquil. ranking\nDomain\nModel\nGreedy\nMI\nSC\nD\nER-G\nER-D\nTruthfulQA\nLLaMA-7B\n33.41\n34.79\n\u00b1 0.90\n34.91\n\u00b1 0.57\n34.17\n\u00b1 1.19\n34.61\n\u00b1 0.99\n34.27\n\u00b1 0.39\nLLaMA-13B\n33.05\n36.30\n\u00b1 0.37\n34.61\n\u00b1 1.33\n39.05\n\u00b1 1.42\n39.83\n\u00b1 2.20\n38.63\n\u00b1 1.76\nTable 2: Results on TruthfulQA (Generative). Average BLEU-Acc results on the held-out set across 5 runs.\nLLaMA-13B with ER-G outperforms or is on par with all baselines. MI: Mutual Information Ranking, SC: Self-\nContrastive Ranking, D: Discriminative Ranking, ER-G: Equilibrium Ranking Generator, ER-D: Equilibrium\nRanking Discriminator. \u00b1 indicates 1 standard deviation computed across 5 runs. Colors are as in Table 1,\nrelative to the Greedy baseline.\nRACE\nRACE is a reading comprehension benchmark introduced in Lai et al. (2017) collected from\nEnglish examinations of middle and high school students. The dataset is correspondingly split into\nRACE-middle and Race-high. The dataset consists of a passage followed by questions. The passages\nwere constructed for evaluating student\u2019s English reasoning and understanding ability. The results on\nthis benchmark is presented in rows 4 and 5 of Table 1. On RACE-middle, like before, ER-D based\nmodels outperforms all the baselines. We note that LLaMA-13B with ER-D (68.6) even outperforms\nmuch larger models: LLaMA-65B (67.9) (Touvron et al., 2023) and PaLM-540B (68.1) (Chowdhery\net al., 2022). On RACE-high, we have a similar story as with ARC-C. ER-D outperforms all baselines.\nLLaMA-7B with ER-D (56.4) is able to significantly outperform much larger models: LLaMA-65B\n(51.6) (Touvron et al., 2023) and PaLM-540B (49.1) (Chowdhery et al., 2022).\nHHH\nHHH (Helpful, Honest and Harmless) (Srivastava et al., 2023; Askell et al., 2021) is a dataset\nof 200 multiple-choice designed to measure LM alignment with high-level quality guidelines. Here\nwe use a different set of prompts for the GENERATOR (see Appendix B). Results are presented in\nthe last row of Table 1. LLaMA-7B with ER-D outperforms baselines; although LLaMA-13B with\nER-D with the default parameter performs worse than discriminative ranking (D) (69.2), ER-D with\n\u03bbG = 0.01 and \u03bbD = 1.0 achieves an accuracy of 70.6%.\nTruthfulQA\nTruthfulQA (Lin et al., 2022) is a benchmark consisting of over 800 questions across\na multitude of domains that were crafted to encourage humans to answer them incorrectly due to\nmisconceptions. The dataset evaluates a model\u2019s ability to not generate false answers learnt from\nimitation learning on text. On this task, we consider greedy decoding in addition to our other\nranking-based approaches. In this setting, 10 candidate sequences are sampled using nucleus and\ntop-k sampling. These candidates are then ranked based on the approaches we described earlier. The\nresults on the test set are presented in Table 2. Based on past work (Lin et al., 2022), we measure\nBLEU accuracy (BLEU-Acc). For a sequence a, the BLEU-Acc over reference correct candidates\nbcorrect and reference incorrect candidates bcorrect is computed as follows:\nBLEU-Acc(a) := I(BLEU(a, bcorrect) > BLEU(a, bincorrect))\n(3)\nWhere BLEU(a, b) computes the BLEU score (Papineni et al., 2002) of a candidate string a over a\nset of reference candidates b. With LLaMA-7B, we observe only modest improvements for ER-G\nand ER-D over the greedy baseline. However, with LLaMA-13B, we note increased scores for\nboth methods. This benchmark is known to exhibit negative scaling (Lin et al., 2022) (performance\ndrop as the model size increases). The performance difference with ER-G between LLaMA-7B and\nLLaMA-13B shows that EQUILIBRIUM-RANKING is in fact capable of mitigating this behavior.\nGSM8K\nIn our last set of experiments, we consider grade-school math (GSM8K) (Cobbe et al.,\n2021), a popular benchmark used to study model\u2019s mathematical reasoning ability. We use this\nbenchmark to study whether we can combine our approach with chain-of-thought (Wei et al., 2022).\nAs we described earlier, we generate 20 candidate reasoning paths sampled using nucleus and top-k\nusing the 8-shot setup proposed in Wei et al. (2022). We employ self-consistency (Wang et al., 2022)\nover the candidate sequences, where we score each reasoning path with our baselines. Finally, we\naggregate the scores for each answer corresponding to the reasoning paths and pick the answer with\n8\nEquil. ranking\nDomain\nModel\nGreedy\nMV\nMI\nSC\nD\nER-G\nER-D\nGSM8K\nLLaMA-7B\n10.8\n14.7\n\u00b1 0.2\n14.6\n\u00b1 0.5\n13.4\n\u00b1 0.3\n15.0\n\u00b1 0.6\n13.0\n\u00b1 0.5\n15.1\n\u00b1 0.6\nLLaMA-13B\n14.9\n22.5\n\u00b1 0.5\n22.5\n\u00b1 0.8\n23.1\n\u00b1 0.5\n22.5\n\u00b1 0.6\n22.5\n\u00b1 0.6\n23.0\n\u00b1 0.5\nTable 3: Average accuracy of methods on the test set of GSM8K acroos 5 runs. In all cases, except greedy,\n20 candidates were sampled. EQUILIBRIUM-RANKING-based approaches performs on par or slightly better\ncompared to the majority vote baseline. MV: Majority Vote, MI: Mutual Information Ranking, SC: Self-\nContrastive Ranking, D: Discriminative Ranking, ER-G: Equilibrium Ranking Generator, ER-D: Equilibrium\nRanking Discriminator. \u00b1 indicates 1 standard deviation. Colors are as in Table 1, relative to the Greedy basline.\nthe highest score. In Table 3, we present the results. We note that EQUILIBRIUM-RANKING-based\napproaches performs on par or slightly better compared to self-consistency (majority vote).\nDiscussion\nThe application of EQUILIBRIUM-RANKING-based approaches consistently yields\nimproved results, surpassing or at least matching the performance of all baseline approaches across\nvarious tasks. This robustness is particularly interesting, as it demonstrates that EQUILIBRIUM-\nRANKING is adept at handling diverse scenarios, even in situations when the initial GENERATOR or\nDISCRIMINATOR are not effective. As EQUILIBRIUM-RANKING is a sampling strategy, it can even be\ncombined with deliberation methods like self-consistency (Wang et al., 2022) or tree-of-thought (Yao\net al., 2023). Finally, we note that EQUILIBRIUM-RANKING demonstrates computational efficiency\nby eliminating the need for repetitive queries to language models.\n4\nOTHER RELATED WORK\nMany decoding strategies have been proposed for language models, such as top-k sampling (Fan et al.,\n2018a), nucleus sampling (Holtzman et al., 2020), and typical sampling (Meister et al., 2023). These\nmethods primarily focus on producing diverse, high-quality text from a language model. However,\nthey decode from the LM without any emphasis on the correctness of the generated sequences. As\nwe show in Section 3, EQUILIBRIUM-RANKING is naturally complementary and be combined with\nany of these sampling strategies.\nRe-ranking is a common approach for selecting the correct answer from a set of candidates sampled\nfrom LM. Cobbe et al. (2021) train a verifier to re-ranked the sampled outputs. Shen et al. (2021)\njointly train a ranking model with the generation model to improve the model accuracy. Thoppilan\net al. (2022) collect additional human annotations to train the ranking model for response filtering. As\nwe discuss in Section 2, our work focuses on leveraging an existing LM and using them in a training-\nfree manner as a discriminator. However, we note that we do not make any specific assumption on\nthe specific form of a the GENERATOR or DISCRIMINATOR. As such, EQUILIBRIUM-RANKING can\nbe combined with these approaches.\nAs previously mentioned, EQUILIBRIUM-RANKING differs from recent deliberation methods, as\nhighlighted in various recent work (Wei et al., 2022; Madaan et al., 2023; Shinn et al., 2023; Yao\net al., 2023; Dohan et al., 2022). In Section 3, we demonstrate how EQUILIBRIUM-RANKING can\nbe integrated with these approaches. In another line of work, Du et al. (2023) and Chen et al.\n(2023a) employ multiple instances of language models suggest and \u201ddebate\u201d individual responses\nand reasoning processes across multiple iterations, ultimately converging on a shared final answer. In\ncontrast, EQUILIBRIUM-RANKING can be viewed as a variant of this multi-agent debate, wherein\nthe \u201ddebate\u201d occurs within the regret-minimization framework rather than in the context of language\nmodels.\n9\n5\nCONCLUSION\nWe have presented EQUILIBRIUM-RANKING, a training-free, game theoretic approach for gener-\nating from language models (LMs). EQUILIBRIUM-RANKING reconciles scores from generative\nand discriminative LM decoding procedures by formulating decoding as an imperfect-information\nsignaling game between a GENERATOR and a DISCRIMINATOR, and leveraging computational\ngame solving techniques to compute approximate equilibria of this game. When applied to 6 di-\nverse question answering benchmarks: MMLU, ARC, RACE, HHH, TruthfulQA and, GSM8K,\nEQUILIBRIUM-RANKING offers substantial improvements over existing generative, discriminative,\nand mixed decoding procedures: applying EQUILIBRIUM-RANKING to LLaMA-7B sometimes out-\nperforms much larger LLaMA-65B and PaLM-540B models. These results highlight the usefulness\nof game-theoretic tools in formalizing desiderata like truthfulness and stability in language modeling.\nBeyond the applications studied here (which focus mainly on question answer), future work might\napply this toolkit to more general tasks like long-form text generation.\nREFERENCES\nIoannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate\nconvergence beyond zero-sum games. In International Conference on Machine Learning, 2022.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\nAnton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexan-\nder H Miller, and Noam Brown. Mastering the game of no-press Diplomacy via human-regularized\nreinforcement learning and planning. In The Eleventh International Conference on Learning\nRepresentations, 2023.\nNoam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top\nprofessionals. Science, 359(6374):418\u2013424, 2018.\nNoam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):\n885\u2013890, 2019.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nJustin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference\nimproves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023a.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023b.\nYangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A close look into the calibration of\npre-trained language models. arXiv preprint arXiv:2211.00151, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems, 2021.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model\ncascades. arXiv preprint arXiv:2207.10342, 2022.\n10\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factual-\nity and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325,\n2023.\nMeta FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,\nAndrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by\ncombining language models with strategic reasoning. Science, 378(6624):1067\u20131074, 2022.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 889\u2013898, Melbourne, Australia, July 2018a. Association for Computational Linguistics. doi:\n10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 889\u2013898, 2018b.\nGabriele Farina, Christian Kroer, and Tuomas Sandholm. Online convex optimization for sequential\ndecision processes and extensive-form games. In AAAI Conference on Artificial Intelligence\n(AAAI), 2019.\nMichael Franke. Game theoretic pragmatics. Philosophy Compass, 8(3):269\u2013284, 2013.\nMichael Franke. Game theory in pragmatics: Evolution, rationality, and reasoning. In Oxford\nResearch Encyclopedia of Linguistics. 2017.\nDeep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamil\u02d9e Luko\u02c7si\u00afut\u02d9e, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li,\nEli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau,\nKamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado,\nNova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu,\nSaurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy\nTelleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario\nAmodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R.\nBowman, and Jared Kaplan. The capacity for moral self-correction in large language models,\n2023.\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth\nRauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue\nagents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\nZhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. Prototypical calibration for few-shot\nlearning of language models. In The Eleventh International Conference on Learning Representa-\ntions, 2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations, 2020.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations, 2019.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=rygGQyrFvH.\nAthul Paul Jacob, David J. Wu, Gabriele Farina, Adam Lerer, Hengyuan Hu, Anton Bakhtin, Jacob\nAndreas, and Noam Brown. Modeling strong and human-like gameplay with kl-regularized search.\nIn International Conference on Machine Learning, 2022.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020.\n11\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading\ncomprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pp. 785\u2013794, 2017.\nDavid Lewis. Convention: A philosophical study. John Wiley & Sons, 2008.\nJiwei Li and Dan Jurafsky. Mutual information and diverse decoding improve neural machine\ntranslation. arXiv preprint arXiv:1601.00372, 2016.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.\narXiv preprint arXiv:2210.15097, 2022.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 3214\u20133252, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback, 2023.\nIan R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,\nEuan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. Inverse scaling: When bigger isn\u2019t\nbetter. arXiv preprint arXiv:2306.09479, 2023.\nClara Isabel Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally typical sampling.\nTransactions of the Association for Computational Linguistics, 11:102\u2013121, 2023.\nEric Mitchell, Joseph Noh, Siyan Li, Will Armstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn,\nand Christopher D Manning. Enhancing self-consistency and performance of pre-trained language\nmodels through natural language inference. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 1754\u20131768, 2022.\nDov Monderer and Lloyd S. Shapley. Potential games. Games and Economic Behavior, 1(14):\n124\u2013143, 1996.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp. 311\u2013318, 2002.\nJulien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer,\nPaul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego\nwith model-free multiagent reinforcement learning. Science, 378(6623):990\u2013996, 2022.\nJianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate &\nrank: A multi-task framework for math word problems. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021, pp. 2269\u20132279, Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.195.\nURL https://aclanthology.org/2021.findings-emnlp.195.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. Reflexion: Language agents with verbal reinforcement learning, 2023.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models. Transactions\non Machine Learning Research, 2023.\n12\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent\nZhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh\nSrinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,\nRenelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi,\nand Quoc Le. Lamda: Language models for dialog applications, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\nMartin Zinkevich, Michael Bowling, Michael Johanson, and Carmelo Piccione. Regret minimization\nin games with incomplete information. In Neural Information Processing Systems (NIPS), 2007.\n13\nA\nDETAILS ABOUT REGRET AND REGRET DECOMPOSITION METHODS\nAfter each repetition t of the game, each player\u2014in this case, the GENERATOR and the DISCRIMINA-\nTOR\u2014refines their policies, in such a way that throughout the course of time, the regrets\nReg(T )\nG\n:= max\n\u03c0\u2217\nG\n( T\nX\nt=1\nuG(\u03c0\u2217\nG, \u03c0(t)\nD ) \u2212\nT\nX\nt=1\nuG(\u03c0(t)\nG , \u03c0(t)\nD )\n)\n,\n(4)\nReg(T )\nD\n:= max\n\u03c0\u2217\nD\n( T\nX\nt=1\nuD(\u03c0(t)\nG , \u03c0\u2217\nD) \u2212\nT\nX\nt=1\nuD(\u03c0(t)\nG , \u03c0(t)\nD )\n)\n,\n(5)\ncumulated by the players are guaranteed to grow sublinearly as a function of the number of rounds of\nlearning T.\nAs mentioned in the body, a classic observation in the theory of imperfect-information sequential\ngames is that minimization of regret (viewed as a function of their overall policy on the game tree)\ncan be achieved by solving separate, local, regret minimization problems at each information set (i.e.,\ndecision point) of the game. In our case, these techniques enable us to decompose the policy update\nof the players into separate updates for each correctness parameter v (for the GENERATOR) and for\neach sequence y (for the DISCRIMINATOR). More specifically, suppose that the GENERATOR updates\ntheir policies \u03c0(t)\nG (\u00b7 | x, v) independently for each correctness parameter v \u2208 {correct, incorrect}\nthey might receive, seeking to independently minimize regret\nReg(T )\nG (v) :=\nmax\n\u03c0\u2217\u2208\u2206(Y)\n( T\nX\nt=1\n\u02dcu(t)\nG (\u03c0\u2217 | x, v) \u2212 \u02dcu(t)\nG (\u03c0(t)\nG (\u00b7 | x, v) | x, v)\n)\nwith respect to the following counterfactual utility functions\n\u02dcu(t)\nG (\u03c0G | x, v) := \u2212\u03bbGDKL\n\u0012\n\u03c0G(\u00b7 | x, v)\n\r\r\r\r \u03c0(0)\nG (\u00b7 | x, v)\n\u0013\n+ 1\n2\nX\ny\u2208Y\n\u03c0(t)\nD (v | x, y) \u00b7 \u03c0G(y | x, v)\n(6)\nfor all v. Then, it is known that when these independent goals are met for all v, so is the goal of\nkeeping regret (4) subliner, and in particular\nReg(T )\nG\n\u2264 Reg(T )\nG (correct) + Reg(T )\nG (incorrect)\nno matter the time horizon T. Similarly, when the DISCRIMINATOR seeks to update their policy\n\u03c0(t)\nD (\u00b7 | x, y) for each y \u2208 Y independently, so as to minimize regret\nReg(T )\nD (y) :=\nmax\n\u03c0\u2217\u2208\u2206({correct,incorrect})\n( T\nX\nt=1\n\u02dcu(t)\nD (\u03c0\u2217 | x, y) \u2212 \u02dcu(t)\nD (\u03c0(t)\nD (\u00b7 | x, y) | x, y)\n)\nwith respect to the counterfactual utility functions\n\u02dcu(t)\nD (\u03c0D | x, y) := \u2212\u03bbDDKL\n\u0012\n\u03c0D(\u00b7 | x, y)\n\r\r\r\r \u03c0(0)\nD (\u00b7 | x, y)\n\u0013\n+ 1\n2\nX\nv\u2208{correct,\nincorrect}\n\u03c0(t)\nG (v | x, y) \u00b7 \u03c0D(v | x, y),\nthen their overall regret Reg(T )\nD\nsatisfies\nReg(T )\nD\n\u2264\nX\ny\u2208Y\nReg(t)\nD (y).\nThe counterfactual utilities \u02dcuG and \u02dcuD defined above are composed of a bilinear term and a strongly\nconvex KL-regularization term. To guarantee sublinear regret with respect to such utility functions,\nwe use the piKL algorithm Jacob et al. (2022).\n14\nA.1\nGUARANTEES OF THE PIKL NO-REGRET DYNAMICS\nWe recall the following known properties of the piKL algorithm. An informal description of the\nguarantees was included in Section 2.\nProposition 1 (Bakhtin et al. 2023). Let i \u2208 {GENERATOR, DISCRIMINATOR} be any player.\nAfter any T iterations of training, the regret Reg(T )\ni\ncumulated by the policies \u03c0(t)\ni\nof player\ni produced by piKL, is upper bounded by only a logarithmic quantity in T.\nMore precisely,\nReg(T )\ni\n= O\n\u0010\nmin\nn\n2 log T\n\u03bbi\n, T\u03b7i\no\n+ log |Y|\n\u03b7i\n\u0011\n, where the asymptotic O notation hides constants\nindependent on the time horizon T, learning rate \u03b7i of the player, and regularization coefficient \u03bbi.\nProposition 2 (Folklore connection between regret minimization and equilibria). The empirical\nfrequency of play converges to the set of coarse correlated equilibria of the CONSENSUS GAME.\nProposition\n3\n(Jacob\net\nal.\n2022).\nThe\naverage\npolicy\n\u00af\u03c0(T )\ni\nof\nplayer\ni\n\u2208\n{GENERATOR, DISCRIMINATOR} produced by piKL after T\niterations is guaranteed to\nbe within a radius proportional to \u03bbi centered in the initial policy \u03c0(1)\ni\n.\nMore precisely,\nDKL(\u00af\u03c0(T )\ni\n, \u03c0(1)\ni\n) \u2264\n1\n\u03bbi (1 + o(1)), where the asymptotic notation o(1) denotes a quantity decreasing,\nas a function of the time horizon T, at a rate of log T/(\u03bbiT).\nB\nHHH PROMPTS\nIn the HHH experiments, (x, correct) corresponds to the prompt:\n\"You are a helpful, honest and harmless assistant.\nHuman: {x}\nAssistant:\"\nAnd (x, incorrect) corresponds to the prompt:\n\"You are a unhelpful, dishonest and harmful assistant.\nHuman: {x}\nAssistant:\"\n15\n"
  },
  {
    "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
    "link": "https://arxiv.org/pdf/2310.08992.pdf",
    "upvote": "9",
    "text": "Published as a conference paper at ICLR 2024\nCODECHAIN: TOWARDS MODULAR CODE GENERA-\nTION THROUGH CHAIN OF SELF-REVISIONS WITH REP-\nRESENTATIVE SUB-MODULES\nHung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, Shafiq Joty\nSalesforce Research\n{hungle, hailin.chen, amrita.saha}@salesforce.com\nABSTRACT\nLarge Language Models (LLMs) have already become quite proficient at solv-\ning simpler programming tasks like those in HumanEval or MBPP benchmarks.\nHowever, solving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate solutions\nas monolithic code blocks instead of decomposing them into logical sub-tasks and\nsub-modules. On the other hand, experienced programmers instinctively write\nmodularized code with abstraction for solving complex tasks, often reusing pre-\nviously developed modules. To address this gap, we propose CodeChain, a novel\nframework for inference that elicits modularized code generation through a chain\nof self-revisions, each being guided by some representative sub-modules generated\nin previous iterations. Concretely, CodeChain first instructs the LLM to generate\nmodularized codes through chain-of-thought prompting. Then it applies a chain\nof self-revisions by iterating the two steps: 1) extracting and clustering the gen-\nerated sub-modules and selecting the cluster representatives as the more generic\nand re-usable implementations, and 2) augmenting the original chain-of-thought\nprompt with these selected module-implementations and instructing the LLM to\nre-generate new modularized solutions. We find that by naturally encouraging\nthe LLM to reuse the previously developed and verified sub-modules, CodeChain\ncan significantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as open-\nsourced LLMs like WizardCoder. We also conduct comprehensive ablation studies\nwith different methods of prompting, number of clusters, model sizes, program\nqualities, etc., to provide useful insights that underpin CodeChain\u2019s success 1.\n1\nINTRODUCTION\nIt has been a long-standing goal in AI to develop systems that can generate executable and functionally\ncorrect computer programs to solve complex problems (Manna & Waldinger, 1971). In recent years,\nwe have witnessed unprecedented progress in this field, specifically with the remarkable success of\nlarge pretrained language models or LLMs (Koubaa, 2023; Wang & Komatsuzaki, 2021; Radford\net al., 2019). Originally developed for natural languages, these models have been extended with a\ncombination of code and text modeling capabilities (Rozi\u00e8re et al., 2023; Black et al., 2021; Chen\net al., 2021), resulting in good performance in code generation from natural language problem\ndescription (Li et al., 2023; Luo et al., 2023; Wang et al., 2023). However, when evaluated on highly\ncomplex coding tasks, the current SoTA models still cannot match a skillful developer (Hendrycks\net al., 2021; Li et al., 2022; Shinn et al., 2023), mostly due to their naive generation approach.\nMost prior approaches with LLMs adopt a naive generation method in which the models would\ntypically generate the code solution as a single monolithic block of code instead of decomposing the\ntask into logical sub-tasks. Another limit of this naive generation approach is that the models would\nsimply generate a large number of solutions independently, with the hope that one of the solutions\n1https://github.com/SalesforceAIResearch/CodeChain\n1\narXiv:2310.08992v3  [cs.AI]  14 Mar 2024\nPublished as a conference paper at ICLR 2024\nLet's call the string beautiful if it does not contain a substring of \nlength at least 2, which is a palindrome\u2026\nLet's define cost of a string as the minimum number of \noperations so that the string becomes beautiful\u2026\nYou are given a string s of length n\u2026\nYou have to answer m queries \u2014 calculate the cost of the \nsubstring of the string s from l_i-th to r_i-th position, inclusive\u2026\nProblem Description\nPublic test cases\nInput: \n10 3\\nababababab\\n1 \n2\\n1 2\\n1 2\nOutput: \n0\\n0\\n0\\n\n\u2026\nPrivate test cases\nInput: \n5 4\\nbaacb\\n1 3\\n1 \n5\\n4 5\\n2 3\nOutput: \n1\\n2\\n0\\n1\n\u2026\nDeveloper\ncreate\ncompare/ \nanalyze\nreuse & revise\nsend for final test\nFigure 1: [Top] An example of a code generation task from CodeContests (Li et al., 2022) where the\nproblem description and public test cases are provided as inputs to the model. [Down] We illustrate\na typical problem-solving process in which a developer attempts to solve the problem iteratively,\nrevising and reusing parts of their previously developed codes until satisfied.\nwould pass all the private test cases (Chen et al., 2021; Li et al., 2023; Austin et al., 2021). More\nrecently, Li et al. (2022); Chen et al. (2023b); Zhang et al. (2023b) propose to sub-sample output\nprograms using some forms of feedback from the public test results. However, these approaches\nassume that the sub-sampled programs could pass the private test cases, even without revising or\ndebugging the programs. Some recent works like (Zhang et al., 2023a; Olausson et al., 2023; Le\net al., 2022; Chen et al., 2023c;a; Shinn et al., 2023) have addressed this by performing self-revision\nwith LLMs, utilizing feedbacks such as compiler error messages, test outcomes, and natural language\nexplanation to improve the generated solutions. However, these approaches limit to using only\nindependent feedback from individual solutions, neglecting potential collective insights from all\ngeneration samples or their sub-components.\nOn the other hand, in today\u2019s agile development environment, experienced developers are fully\nfamiliar with the concept of modularity in programming. Given a problem, they would instinctively\nwrite solutions that are modularized by high-level logical sub-tasks and sub-modules. The developers\nwould then keep testing and analyzing their implementations, altering modular components from their\npreviously developed solutions to efficiently improve their final solutions (see Figure 1). Inspired by\nthis problem-solving process, we propose CodeChain, a novel inference framework to improve code\ngeneration in LLMs through a chain of sub-module based self-revisions (see Figure 2).\nSpecifically, in CodeChain, to incorporate modularity in code generation, we first introduce chain-\nof-thought prompting to instruct LLMs to decompose their solutions into modular segments. Each\nmodular segment represents an abstract function that is intended for a high-level logical sub-task. To\nleverage this modularity in programs, we propose to further improve the generation process through a\nchain of self-revisions, each of which is conditioned by a set of sampled sub-modules as follows: (i)\nwe first extract the sub-modules found in generated programs and group them into clusters. Within\neach cluster, we sample the centroid sub-modules and treat them as representative and reusable\ncode parts for self-revision. (ii) We then augment the original chain-of-thought prompt with these\nselected sub-modules and instruct LLMs to generate new modularized solutions. With this approach,\nLLMs can receive the collective insights from modular components of all past generation samples to\nimprove their future generations, imitating the problem-solving process of an experienced developer.\nOur experiments show that CodeChain can significantly boost LLM performance and achieve SoTA\nperformance on challenging code tasks in APPS (Hendrycks et al., 2021) and CodeContests (Li\net al., 2022). Concretely, CodeChain improves the average pass@1 performance by more than 35%\non APPS and 76% on CodeContests. We also observe consistent improvements for both OpenAI\nLLMs as well as open-sourced LLMs such as WizardCoder (Luo et al., 2023). We further conducted\ncomprehensive ablation studies, including analysis in single vs. multi-step revisions, feedback types,\nnumber of clusters, etc., and derived useful insights behind CodeChain\u2019s success.\n2\nRELATED WORK\nBroadly related to our work is the research of large Transformer-based language models (LLMs)\n(Koubaa, 2023; Brown et al., 2020; Radford et al., 2019; Wang & Komatsuzaki, 2021; Touvron et al.,\n2023a). Originally designed for natural language processing, these models have been extended to\n2\nPublished as a conference paper at ICLR 2024\nProblem\nDescription \nPretrained \nCode LLM\nModularized \nsolutions\ndef sort_list(n):\n   ```\n   This function...\n   ```\n   output=list(...\nStep3: \nExtract modular\n         functions\nEmbedding space\ndef func(x):\n```\n...\nChain of Self-revisions\nStep1: CoT prompting\nStep5: Representative \nsub-modules\nStep6: Re-generate \nReusing/Adapting\nselected sub-modules\nEncode\nPrivate \ntest \ncases\nStep2:\nFilter by \npublic test cases\nStep4:\nClustering \nsub-modules\nEvaluation\nFigure 2: An overview of CodeChain: a pretrained LLM is first instructed with chain-of-thought\nprompting to generate a set of modularized solutions. Generated sub-modules are then extracted\nfrom potentially correct solutions and grouped into different semantic clusters. The cluster centroids\nare selected as representative sub-modules to condition the next self-revision round. The model is\ninstructed to reuse or adapt these modules into its revised solutions.\nlearn from large-scale code data and become proficient in understanding contexts and generating\noutputs in programming languages (Rozi\u00e8re et al., 2023; Chen et al., 2021; Li et al., 2023; Gunasekar\net al., 2023; Wang et al., 2023; Nijkamp et al., 2023). Complementing the long-standing code\ngeneration research (Gulwani et al., 2012; Kurach et al., 2015; Devlin et al., 2017; Parisotto et al.,\n2016), LLMs can generate programs of more general-purpose programming languages, correctly\nfollowing programming syntactic rules (Lu et al., 2021; Clement et al., 2020) and solving simple\ncoding problems with reasonable accuracy (Lai et al., 2022; Chen et al., 2021; Austin et al., 2021).\nIn more direct relevance to our work is the recent line of work for improving code generation qualities\nthrough output feedback. Chen et al. (2021) introduced a simple filtering approach by selecting only\noutput samples that successfully pass the public test cases. AlphaCode (Li et al., 2022), CodeT (Chen\net al., 2023b), and MBR-Exec (Shi et al., 2022) proposed to generate more test cases and use more\nsophisticated rule-based methods to rank generation samples by their execution behaviors. LEVER\n(Ni et al., 2023), Coder-Reviewer (Zhang et al., 2023b) and Code Rankers (Inala et al., 2022) follow\na similar principle but introduce more model-based ranking methods.\nRecently, more related works have been proposed to boost generation quality through iterative self-\nrevisions. Zhang et al. (2023a) utilizes test outcomes from public test cases as a form of feedback\nfor models to self-revise their codes. Self-correct (Welleck et al., 2023) and CodeRL (Le et al.,\n2022) introduce secondary models to predict the correctness of output programs and revise them\naccordingly. Self-debug (Chen et al., 2023c), Sef-refine (Madaan et al., 2023), and Reflexion (Shinn\net al., 2023) propose to facilitate better code revision with synthetic natural language explanation or\nreflection self-generated by LLMs. Self-repair (Olausson et al., 2023) and ILF (Chen et al., 2023a)\nfollow a similar strategy but highlight the use of natural language explanation provided by human\nexperts. Different from prior approaches, we propose to generate more modularized programs and\nsequentially revise these programs using more representative and reusable sub-module programs\n(please see Appendix A for a more systematic comparison).\n3\nCODECHAIN FRAMEWORK\n3.1\nCODE GENERATION TASK\nWe treat code generation as a sequence-to-sequence task, which consists of a problem description\nas an input sequence D and an output sequence of a flattened solution program: \u02c6W = ( \u02c6w1, ..., \u02c6wT )\nwith \u02c6wt \u2208 V. Typically, a language model \u03b8 generates a code sequence by autoregressively sampling\n3\nPublished as a conference paper at ICLR 2024\n  *Instruction*\nDevelop a well-structured Python solution for the provided problem that obeys the constraints and passes the example \ntest cases. Ensure modularity and considering potential edge cases and failures. Start by outlining the required code \nmodules, including function headers and signatures. Subsequently, proceed to implement each module to create the \nfinal code.\nIn simpler terms, create a clean and organized Python solution for the given problem. Break it down into smaller parts \n(modules) with clear function names and input/output specifications. Once the structure is ready, write the actual code \nfor each module to complete the solution.\nChain-of-thought prompting for code generation\nFigure 3: An example of CoT prompting for code generation in CodeChain. The model is required to\nfirst outline the solution in terms of sub-module signatures, each of which is intended for solving a\nhigh-level sub-task in the final solution. The model is then required to implement these sub-modules\nand combine them into a final solution (see Appendix F for a full version of the prompt).\ntokens \u02c6wt from the parameterized conditional distribution p\u03b8(.| \u02c6w1:t\u22121, D). Generated codes are\nevaluated against (private) test cases to check the execution correctness (Hendrycks et al., 2021; Chen\net al., 2021; Li et al., 2022). The test cases comprise a set of input-output pairs {(ij, oj)}J\nj=1. An\noutput program \u02c6W is correct when \u02c6W(ij) = oj for all j \u2208 {1, ..., J}. If the problem description\ncontains some test cases, we treat these as public test cases: {(i\u2032\nm, o\u2032\nm)}M\nm=1 (usually M \u226a J).\nModels have the option to use these public test cases to improve its generation.\n3.2\nMODULAR CODE GENERATION WITH COT PROMPTING\nLLMs, especially the instruction-tuned ones, can follow complex natural language instructions\ndescribing novel unseen tasks (Ouyang et al., 2022; Touvron et al., 2023b; Wang et al., 2023). They\nhave shown remarkable performance in many reasoning-based tasks when they are instructed to\nsolve a problem step-by-step, i.e., chain-of-thought (CoT) prompting (Zhou et al., 2023; Wei et al.,\n2022; Kojima et al., 2022). We propose to adapt this technique to generate codes by instructing\nthe models to first outline the required sub-modules, generating only their function headers and\ndocstrings describing their intended usage. The model is then instructed to implement the modules\nand ultimately combine them into a final solution. Following this generation scheme, we can define\nthe output distributions:\n\u02c6Si \u223c p\u03b8(.| \u02c6S1:i\u22121, D)\n\u21d2 sub-modules, including the function headers and docstrings (1)\n\u02c6wt \u223c p\u03b8(.| \u02c6w1:t\u22121, { \u02c6Si}, D)\n\u21d2 tokens in final solution\n(2)\nwhere { \u02c6Si} is the set of sub-modules outlined by the model. We append the instruction with a\none-shot demonstration. Figure 3 presents one example of the instruction prompt.\nAs illustrated further by Figure 10 in the Appendix, this technique encourages the model to decompose\na program into natural boundaries, e.g., sub-modules, similarly to how a developer often tackles a\nchallenging coding task by breaking a solution into modular components. Though this is a more\npragmatic style of code-development, empirically we have found that this prompting approach can\nadversely impact the correctness of the generated end-to-end solutions (shown later in Table 3).\nThis is expected as most of the current LLMs are not pretrained to generate perfectly functioning\nmodularized programs. To address this, we introduce Chain of Self-Revisions which allows the LLM\nto iteratively revise a solution by re-using or adapting some of the representative sub-modules from\nthe previous iterations. Further, we also establish empirically that our self-revision technique indeed\nbenefits more from this modularized style of code generation.\n3.3\nSELECT REPRESENTATIVE SUB-MODULES ACROSS MULTIPLE SAMPLES\nPrior studies have demonstrated the benefits of generating multiple samples and selecting the best\nones based on different ranking or scoring schemes (Li et al., 2022; Chen et al., 2023b; Zhang et al.,\n2023b). A common approach is to simply select the representative candidates based on their execution\nresults on the public test cases (Li et al., 2022; Chen et al., 2021). However, all prior methods only\nselect end-to-end program candidates. On challenging coding tasks, it is extremely rare to obtain such\n4\nPublished as a conference paper at ICLR 2024\nprogram-level correctness and the selected candidates are still likely to fail when tested on private\ntest cases. Thus, we propose to perform selection at sub-module level instead of program level.\nSpecifically, given a generation budget of N samples, we extract and combine the set of sub-modules\nacross all samples \u02c6S = {{ \u02c6Si}n} for all n \u2208 {1, ..., N}, where { \u02c6Si}n is the set of sub-modules in the\nn-th generated sample. We then perform K-mean clustering on this set of sub-modules to group them\ninto K clusters. For each of these clusters, we then extract a \u201ccentroid\u201d (representative) sub-module\n\u02c6Ck that is closest to the true centroid of the cluster in the embedding space:\n\u02c6Ck = arg min\n\u02c6Sk\n\u2225Sk\ni \u2212 \u00b5k\u2225\n(3)\nwhere Sk\ni is the embedding representation of sub-module \u02c6Si in cluster k and \u00b5k is the centroid\nof cluster k. By selecting these \u201ccentroid\u201d sub-modules, we can sample the most semantically\nrepresentative and re-usable functions across all samples. Note that in cases where public test cases\nare available, one can filter out any failed samples first before further applying our selection method.\n3.4\nIMPROVE CODE GENERATION WITH CHAIN OF SELF-REVISIONS\n  *Instruction*\n  \u2026Given a set of related utility Python \nfunctions, try to reuse or adapt them as much \nas possible into your solution (create new \nunique functions if needed)....\n-----------------\n### TASK:\n<<problem>>\n### RELEVANT FUNCTIONS:\n<<sub-modules>>\n### RESPONSE:\nSelf-revise prompting with \nrepresentative sub-modules\nFigure 4: An example of prompting to\nself-revise programs. The original in-\nstruction from CoT prompting (Fig. 3)\nis combined with this instruction and the\nmodel is provided with a set of repre-\nsentative sub-modules (\u00absub-modules\u00bb)\nselected from previously generated sam-\nples. Please refer to Appendix F for a\nfull version of the prompt.\nPrior approaches improved code generation by regener-\nating code conditioned by different types of feedback,\nranging from compiler error messages to natural language\nexplanation of the output programs (Chen et al., 2023a;\nMadaan et al., 2023; Chen et al., 2023c; Shinn et al., 2023;\nLe et al., 2022). However, these methods focus on the\nfeedback extracted only per individual generation sample.\nWe propose to utilize a new type of feedback in the form\nof clustered sub-modules extracted from all the N gener-\nated samples (as described in Sec. 3.3). Augmenting our\noriginal CoT prompt with the implementations of these\nrepresentative sub-modules can explicitly encourage the\nLLM to re-use or adapt these functions when generating\nthe code conditioned on that prompt in the subsequent re-\nvision rounds. Specifically, in revision round R, the output\ntoken is sampled from the conditional distribution:\n\u02c6wR\nt \u223c p\u03b8(.| \u02c6wR\n1:t\u22121, { \u02c6SR\ni }, \u02c6CR\u22121, D)\n(4)\nwhere \u02c6CR\u22121 = { \u02c6CR\u22121\nk\n}K\nk=1 is the set of all centroid sub-\nmodules from the previous generation round R \u2212 1. In\nround R, the new sub-modules are regenerated by the\nconditional probability (revised version of Eq. 1):\n\u02c6SR\ni \u223c p\u03b8(.| \u02c6SR\n1:i\u22121, \u02c6CR\u22121, D)\n(5)\nWe enable this self-revision procedure by prompting the LLM with an additional instruction. Figure 4\npresents an example of the prompt with the new instruction. This style of self-revision with selective\nsub-modules is reminiscent of the code reuse process. In today\u2019s agile code-development environment,\ndevelopers typically re-use or adapt snippets of previously developed code in order to program more\nmodularly, accurately, and efficiently. Inspired by this process and combined with our representative\nsub-module selection method, our CodeChain framework allows the LLM to iteratively improve their\ngenerations more efficiently through a chain of reuse-based self-revisions.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUPS\nBenchmarks. We demonstrate the efficacy of CodeChain on challenging code generation tasks,\nspecifically, on two major benchmarks: APPS (Hendrycks et al., 2021), and CodeContests (Li et al.,\n5\nPublished as a conference paper at ICLR 2024\nTable 1: APPS test results: results with \u2020 are for models finetuned on APPS training data\n(a) Performance by pass@1 (%)\nModel\nSize\nIntroductory\nInterview\nCompetition\nAll\nCodex\n12B\n4.14\n0.14\n0.02\n0.92\nCodeT5 \u2020\n770M\n6.60\n1.03\n0.30\n2.00\nCodeRL+CodeT5 \u2020\n770M\n7.08\n1.86\n0.75\n2.69\ntext-davinci-002\n-\n-\n-\n-\n7.48\nSelf-edit+text-davinci-002\n-\n-\n-\n-\n7.94\ncode-davinci-002\n-\n29.30\n6.40\n2.50\n10.20\nWizardCoder\n15B\n26.04\n4.21\n0.81\n7.90\nCodeChain+WizardCoder\n15B\n26.29\n7.49\n3.75\n10.50\nGPT3.5\n-\n48.00\n19.42\n5.42\n22.33\nCodeChain+GPT3.5\n-\n54.50\n28.11\n12.38\n30.24\n(b) Performance by pass@1 (%) with outputs filtered by public/synthetic tests\nModel\nSize\nFiltering\nIntroductory\nInterview\nCompetition\nAll\nCodex\n12B\nnaive\n22.78\n2.64\n3.04\n6.75\nCodeRL+CodeT5 \u2020\n770M\nnaive\n17.17\n6.78\n4.88\n8.48\ncode-davinci-002\n-\nnaive\n43.60\n13.30\n7.00\n18.10\ncode-davinci-002\n-\nCodeT\n47.30\n14.30\n6.20\n19.28\nGPT3.5\n-\nCodeT\n61.52\n30.57\n9.46\n32.54\nCodeChain+GPT3.5\n-\nCodeT\n62.72\n32.96\n15.08\n35.34\n2022). A majority of test samples from these benchmarks are curated from competitive programming\nplatforms such as Codeforces 2, making them an appropriate test bed to evaluate our approach. Please\nrefer to Appendix C and Table 6 for more details of the benchmarks.\nEvaluation. We followed (Hendrycks et al., 2021; Chen et al., 2021; Li et al., 2022) and evaluated\nthe models using the passing rate metric pass@k, defined as the percentage of problems solved by\nusing k generated programs per problem. We focused mainly on pass@1 in this work and followed\n(Chen et al., 2021) to calculate the normalized passing rate given a generation budget of N outputs\nper problem. To apply CodeChain, we fixed the budget in each generation/revision round to N = 20\ngeneration samples per problem. After the first round of direct generation, we let the models self-\nrevise generated codes for up to 5 rounds of revision. On APPS and CodeContests, we reported\nthe results on the test split following the best self-revision round performance on the validation set.\nAcross all benchmarks, we fixed the one-shot sample in CoT prompting and revision prompting. We\nrandomly selected this one-shot sample from the APPS training split (see Appendix G).\nBase language models. We applied CodeChain to both open-sourced and closed-sourced pretrained\nLLMs, including OpenAI\u2019s GPT3.5 and GPT4 (Koubaa, 2023), and WizardCoder (Luo et al., 2023).\nWe evaluated different versions of WizardCoder, with model sizes ranging from 1B to 34B parameters.\nWizardCoder models are instruction-tuned from strong foundational code LLMs, including StarCoder\n(Li et al., 2023) and Code LLaMA (Rozi\u00e8re et al., 2023). For OpenAI models, we obtained the\ngeneration samples by prompting through the public API access 3. For WizardCoder, we utilized the\nHuggingFace-hosted model parameters (Wolf et al., 2019) and vLLM (Kwon et al., 2023) to generate\nprograms. We adopted a default temperature of 0.6 to generate output tokens and a max output length\nof 2048 tokens. Finally, to fairly compare LLM generation capabilities, we chose to use StarEncoder\n(Li et al., 2023) to embed sampled sub-modules throughout all experiments.\n4.2\nEXPERIMENTAL RESULTS\nResults on APPS. We compare our approach with prior LLM baselines like Codex (Chen et al.,\n2021), CodeT5 (Wang et al., 2021), and code-davinci, as well as code-revision methods such as\nSelf-edit (Zhang et al., 2023a), CodeRL (Wang et al., 2021; Le et al., 2022), and Self-repair (Olausson\net al., 2023). Table 1a shows that CodeChain, when applied with base LLMs such as GPT3.5 and\nWizardCoder 15B, can achieve significant performance gains by the pass@k. Specifically, CodeChain\n2https://codeforces.com/\n3gpt-3.5-turbo-16k and gpt-4 on https://platform.openai.com/docs/models/overview\n6\nPublished as a conference paper at ICLR 2024\nTable 2: Comparison with Self-repair: following Olausson et al. (2023), we reported the results on\nthe same subset of 20 samples on APPS test split using GPT3.5 and GPT4 as base models. Please\nrefer to Table 5 for the full list of this test subset.\nModel\nFeedback source\nIntroductory\nInterview\nCompetition\nAll\nSelf-repair+GPT4\nGPT4\n42.64\n19.33\n3.67\n33.30\nSelf-repair+GPT4\nHuman\n62.21\n45.67\n14.67\n52.60\nGPT3.5\n-\n30.00\n18.33\n0.00\n23.75\nCodeChain+GPT3.5\nSub-modules\n31.67\n27.86\n0.00\n26.35\nGPT4\n-\n42.86\n18.33\n13.33\n34.75\nCodeChain+GPT4\nSub-modules\n71.07\n55.00\n23.33\n61.50\nTable 3: APPS validation results by pass@1 (%): we tested CodeChain+GPT3.5 for 1 self-revision\nround by 3 aspects: prompting, filtering by public tests, and sampling methods for revision (R:\nrandom, C: centroid, P: whole programs, and M: sub-modules).\nCoT\nprompting\nfilter by\npublic tests\nSampling\nfor revision\nIntroductory\nInterview\nCompetition\nAll\n-\n-\n-\n39.00\n26.50\n12.50\n26.00\n-\n-\nR-P\n12.40\n2.00\n0.61\n5.00\n-\n-\nC-P\n23.27\n9.00\n3.80\n12.02\n-\n\u2713\nC-P\n45.20\n28.03\n9.80\n27.68\n-\n-\n-\n33.50\n23.70\n10.10\n22.43\n\u2713\n-\nR-P\n24.40\n18.80\n9.20\n17.47\n\u2713\n-\nC-P\n31.33\n23.70\n10.10\n21.71\n\u2713\n\u2713\nC-P\n45.50\n33.17\n11.80\n30.16\n\u2713\n\u2713\nR-M\n49.30\n36.90\n12.40\n32.87\n\u2713\n\u2713\nC-M\n52.00\n38.83\n14.50\n35.11\ncan achieve 10.50% pass@1 with WizardCoder as the base model, and 30.24% pass@1 with OpenAI\nGPT3.5 as the base model, establishing a new SoTA result on APPS. Previous works (Chen et al.,\n2021; Li et al., 2022) introduced additional performance results by filtering out generation samples\nthat fail public tests and computed pass@k on the filtered set. In this work, we followed the setup\nproposed by CodeT (Chen et al., 2023b) which utilized more advanced filtering with synthetic test\ncases (see Appendix F for the prompt we used to generate test cases). Table 1b shows that when\nevaluated on filtered code samples, our CodeChain+GPT3.5 can achieve SoTA results across all\nlevels of problem difficulty with an average of 35.34% pass@1.\nFrom Table 1a, when compared with related approaches such as Self-edit and CodeRL, we observed\nsignificant relative performance gains when using CodeChain. In Table 2, following Olausson et al.\n(2023), to compare with Self-repair, we evaluated our approach over the same test subset of 20\nsamples (14/3/3 samples of introductory/interview/competition level), using both GPT3.5 and GPT4\nas base models. We observed that CodeChain can improve the performance with both base models,\nwith more significant gains using GPT4. Specifically, CodeChain+GPT4 can achieve a SoTA result\nof 61.50% pass@1 on average, even outperforming Self-repair+GPT4 with human feedback.\nAnalysis on single-round self-revision. To understand the benefits of CodeChain, we conducted\nexperiments with different variants on the validation split of APPS. Table 3 presents the results on\nsingle-round self-revision by 3 main aspects: prompting, filtering by public tests, and sampling meth-\nods for conditional revisions. First, we observed that without self-revisions (i.e. direct generation),\nCoT prompting actually negatively affects the model performance as compared to normal prompting.\nThis observation might be due to the fact that pretrained LLMs are not designed to generate perfectly\nmodularized solutions (they were pretrained on public Github codes without filtering for modularity).\nHowever, after applying self-revision, we observe that the modularized approach is better, achieving\nbetter performance gains than non-modularized solutions.\nSecondly, we found that the best strategy to select representative codes for conditional revision is\nthrough clustering. This method can reduce noisy data points and create a better form of feedback to\nimprove the generated codes. Finally, we observed that clustering alone is not sufficient to select the\noptimal representative samples. Additional filtering by public tests is needed to first shift the output\ndistribution to more likely correct samples before clustering the outputs. To avoid the need for public\n7\nPublished as a conference paper at ICLR 2024\nFigure 5: APPS validation results with chain of self-revisions: we tested\nCodeChain+GPT3.5 for 5 self-revision rounds and reported pass@1 in each\nproblem difficulty level. Using GPT3.5 as base model, we compared with\nrelated approaches, including Self-debug (with unit test (UT) feedback or\nexplanation (expl)) (Chen et al., 2023c) and Reflexion (Shinn et al., 2023).\nFigure 6:\nwe tested\nCodeChain+GPT3.5\non different setups of\ncluster numbers and\nreported the average\nrelative pass@1 im-\nprovements from direct\ngeneration (round 0).\ntest cases, we suggest exploring better embedding models that can group output samples not just by\ntheir programming semantics but also by their functional correctness.\nAnalysis on chain of self-revisions. To analyze the trend of model performance over a chain of self-\nrevisions, we monitored the passing rates of direct generation and 5 subsequent self-revision rounds.\nFigure 7:\nAPPS valida-\ntion\npass@1\nresults\nof\nWizardCoder-1B\nto\n34B.\nThe dotted lines are direct\ngeneration results.\nFigure 5 presents relatively consistent improvements in all levels\nof problem difficulties, with optimal performance gain obtained in\nrevision round 4 and a slight performance drops in round 5. One\npossible reason for these performance drops is that the selected output\nsamples become overfitting to the small set of available public test\ncases, negatively affecting the passing rates of subsequently revised\ncodes on a more extensive private hidden test-suite.\nSecondly, we also observed that on different levels of problem diffi-\nculties, CodeChain has different rates of performance improvement.\nSpecifically, we found that more challenging problems (i.e. compe-\ntition and interview level) benefit more from CodeChain than basic\nproblems (i.e. introductory level). Similar observations can be seen\non open-sourced WizardCoder (Luo et al., 2023), with clearer perfor-\nmance trends on 7B, 15B, and 34B model sizes (see Figure 7).\nAnalysis by types of feedback. In Figure 5, we also observed that\nCodeChain can achieve better performance than other related self-\nrevision approaches using other types of feedback, such as test out-\ncomes with natural language explanations (Chen et al., 2023c) or\nreflection (Shinn et al., 2023). Note that CodeChain can be comple-\nmented with other self-revision approaches such as Self-debug by\ncombining different feedback types and selecting more diverse and\nrepresentative sub-modules, even on generation samples that initially fail public tests.\nAnalysis by number of representative sub-modules. One hyper-parameter of CodeChain is the\nnumber of clusters in each round of self-revision. We experimented with 4 different scheme: (i)\nfixed number of clusters across all rounds to K; (ii) decreasing order number of clusters: {Ki} =\n{K, K \u2212 1, ..., 1}; (iii) increasing order number of clusters: {Ki} = {K, K + 1, ...}; (iv) dynamic\nnumber of clusters based on the silhouette coefficients (Rousseeuw, 1987). We selected K = 5 for\nall experiments. From Figure 6, we observed that the best approach to set the number of clusters is to\nfollow a decreasing order. This scheme offers the models more diverse centroid sub-modules in the\nbeginning with a larger number of clusters. Towards subsequent revision rounds, a smaller number of\nclusters is more beneficial as the sampled sub-modules become more and more closely semantically\nsimilar over time. We found that this scheme is reminiscent of the model training paradigm moving\nfrom exploration to exploitation, as the models become more confident in their generation.\n8\nPublished as a conference paper at ICLR 2024\nModel\nSize\nFiltering\nVal\nTest\npass@1\npass@5\npass@1\npass@5\ncode-davinci-002\n-\n-\n-\n-\n1.00\n-\nWizardCoder\n15B\n-\n1.11\n3.18\n1.98\n3.27\n+ CodeChain\n15B\n-\n2.35\n3.29\n2.48\n3.30\nGPT3.5\n-\n-\n6.81\n16.23\n5.82\n11.16\n+ CodeChain\n-\n-\n12.86\n16.91\n10.27\n14.11\ncode-davinci-002\n-\nCodeT\n-\n-\n3.20\n-\nGPT3.5\n-\nCodeT\n17.30\n-\n11.34\n-\n+CodeChain\n-\nCodeT\n17.91\n-\n13.75\n-\nFigure 8: CodeContests results by pass@1 (%): we report the results of CodeChain using\nWizardCoder-15B and GPT3.5 as base models. Left: test and validation results. Right: valida-\ntion results over sequential self-revision rounds. The dotted lines are direct generation results.\nResults on CodeContests.\nFigure 8 presents the results of CodeChain with WizardCoder-\n15B and GPT3.5 as the base models.\nWe observed that on both pass@1 and pass@5,\nCodeChain can achieve significant performance gains as compared to direct generation on\nthe corresponding base models.\nApplying additional filtering method (Chen et al., 2023b),\nCodeChain+GPT3.5 can achieve the SoTA results of 13.75% pass@1 on the test split. As op-\nposed to APPS where optimal performance was reached at revision round 4, from this valida-\ntion results we noted that the performance kept improving till the final revision round. Differ-\nent from APPS, we used the official public test cases available in the CodeContests benchmark.\nFigure 9: Distribution of output sam-\nples (%) by code qualities in the APPS\ntest subset. We obtained the qualitative\nscores by prompting GPT4 with specific\nevaluation instructions.\nThese test cases are generally more diverse than the ones\nwe manually extracted in APPS, and hence, make the re-\nvised codes less overfitting even in the 5th revision round.\nQualitative Results. To understand the modularity and\nreusability of CodeChain generation, we conducted exper-\niments to evaluate these qualities on randomly sampled\ngenerated programs. Specifically, we prompted GPT4 with\ninstructions to rate output samples following a Likert scale\nfrom 0 to 5 where 5 is the highest score for optimally mod-\nular/ reusable programs. Please refer to Appendix F for a\nfull version of the prompt. In this experiment, we reused\nthe GPT3.5 generated samples for the set of 20 random\ntest tasks from Table 2. Figure 9 shows the distribution\nof output samples by Likert scores in each quality. We\nobserved that when using CodeChain, GPT3.5 is more\nlikely to generate programs with high levels of modularity\nand reusability, with the majority of outputs rated 3 to 5\non the Likert scale. This is significantly higher than the\nconventional direct generation approach, with about 80%\nof time generating non-modular or non-reusable codes (i.e. score 0). For additional experimental\nresults and qualitative examples of CodeChain, please refer to Appendix D and E.\n5\nCONCLUSION\nWe present CodeChain, a novel inference framework to improve code generation through a chain\nof self-revisions and sampling of representative sub-modules. In CodeChain, we introduce chain-\nof-thought prompting to generate more modularized programs, which creates natural boundaries\nfor the models to sample parts of the solutions for reuse and revision. In each revision step, we\niterate between selecting representative sub-modules and augmenting chain-of-thought prompting\nwith these selected sub-modules. Our experiments indicate the significant performance improvement\nof CodeChain when using OpenAI GPT or open-sourced WizardCoder as the base models, achieving\nnew SoTA results on APPS and CodeContests benchmarks. We provided comprehensive ablation\nstudies to understand the contributing factors behind CodeChain\u2019s outstanding results.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\nSid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow. URL https://doi. org/10.5281/zenodo, 5297715,\n2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nAngelica Chen, J\u00e9r\u00e9my Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R\nBowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training with natural\nlanguage feedback. arXiv preprint arXiv:2303.16749, 2023a.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen.\nCodet: Code generation with generated tests. In The Eleventh International Conference on Learning\nRepresentations, 2023b. URL https://openreview.net/forum?id=ktrw68Cmu9c.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023c.\nColin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan.\nPyMT5: multi-mode translation of natural language and python code with transformers. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pp. 9052\u20139065, Online, November 2020. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2020.emnlp-main.728. URL https://aclanthology.org/2020.\nemnlp-main.728.\nJacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and\nPushmeet Kohli. Robustfill: Neural program learning under noisy i/o. In International conference\non machine learning, pp. 990\u2013998. PMLR, 2017.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,\nBing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for pro-\ngramming and natural languages. In Findings of the Association for Computational Linguistics:\nEMNLP 2020, pp. 1536\u20131547, Online, November 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.findings-emnlp.139. URL https://aclanthology.org/2020.\nfindings-emnlp.139.\nSumit Gulwani, William R Harris, and Rishabh Singh. Spreadsheet data manipulation using examples.\nCommunications of the ACM, 55(8):97\u2013105, 2012.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644, 2023.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge\ncompetence with apps. NeurIPS, 2021.\nJeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnaci\u00f3n, Shuvendu K\nLahiri, Madanlal Musuvathi, and Jianfeng Gao. Fault-aware neural code rankers. In Alice H. Oh,\nAlekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information\nProcessing Systems, 2022. URL https://openreview.net/forum?id=LtJMqnbslJe.\n10\nPublished as a conference paper at ICLR 2024\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:\n22199\u201322213, 2022.\nAnis Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown. arXiv preprint, 2023.\nKarol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. arXiv\npreprint arXiv:1511.06392, 2015.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen\ntau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for\ndata science code generation. ArXiv, abs/2211.11501, 2022.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl:\nMastering code generation through pretrained models and deep reinforcement learning. Advances\nin Neural Information Processing Systems, 35:21314\u201321328, 2022.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with\nyou! arXiv preprint arXiv:2305.06161, 2023.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\nwith alphacode. arXiv preprint arXiv:2203.07814, 2022.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu\nFu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding\nand generation. In NeurIPS Datasets and Benchmarks, 2021.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\nevol-instruct. arXiv preprint arXiv:2306.08568, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nZohar Manna and Richard J. Waldinger. Toward automatic program synthesis. Commun. ACM,\n14(3):151\u2013165, mar 1971. ISSN 0001-0782. doi: 10.1145/362566.362568. URL https:\n//doi.org/10.1145/362566.362568.\nAnsong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria\nLin. Lever: Learning to verify language-to-code generation with execution. In International\nConference on Machine Learning, pp. 26106\u201326128. PMLR, 2023.\nErik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2:\nLessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309,\n2023.\nTheo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama.\nDemystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\n11\nPublished as a conference paper at ICLR 2024\nEmilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet\nKohli. Neuro-symbolic program synthesis. arXiv preprint arXiv:1611.01855, 2016.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nPeter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis.\nJournal of computational and applied mathematics, 20:53\u201365, 1987.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950, 2023.\nFreda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural\nlanguage to code translation with execution. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 3533\u20133546, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.\n231. URL https://aclanthology.org/2022.emnlp-main.231.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. Reflexion: Language agents with verbal reinforcement learning, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code understanding and generation. In EMNLP (1), pp.\n8696\u20138708. Association for Computational Linguistics, 2021.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi.\nCodet5+: Open code large language models for code understanding and generation. arXiv preprint\narXiv:2305.07922, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and\nYejin Choi. Generating sequences by learning to self-correct. In The Eleventh International\nConference on Learning Representations, 2023. URL https://openreview.net/forum?\nid=hH36JeQZDaO.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\nKechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code\ngeneration. arXiv preprint arXiv:2305.04087, 2023a.\nTianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida Wang.\nCoder reviewer reranking for code generation. In International Conference on Machine Learning,\npp. 41832\u201341846. PMLR, 2023b.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex\nreasoning in large language models. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=WZH7099tgfM.\n12\nPublished as a conference paper at ICLR 2024\nA\nCOMPARISON WITH RELATED METHODS\nTable 4: A comparison of CodeChain and related approaches by 4 aspects: (i) code execution:\nwhether the method utilizes execution outcomes of output programs on public/synthetic test cases;\n(ii) representative samples: whether the method sub-samples outputs for evaluation/ revision. (iii)\nsupervision free: whether the method requires model to be finetuned on specialized tasks, such as\ncorrectness prediction or bug detection. (iv) iterative revision: whether the method allows model to\nself-revise output programs multiple times.\nApproach\nCode\nexecution\nRepresentative\nsamples\nSupervision\nfree\nIterative\nrevision\nCodeRanker (Inala et al., 2022)\n-\n\u2713\n-\n-\nLEVER (Ni et al., 2023)\n\u2713\n\u2713\n-\n-\nCoder-Reviewer (Zhang et al., 2023b)\n-\n\u2713\n\u2713\n-\nAlphaCode (Li et al., 2022)\n\u2713\n\u2713\n\u2713\n-\nMBR-Exec (Shi et al., 2022)\n\u2713\n\u2713\n\u2713\n-\nCodeT (Chen et al., 2023b)\n\u2713\n\u2713\n\u2713\n-\nSelf-correct (Welleck et al., 2023)\n-\n-\n-\n\u2713\nILF (Chen et al., 2023a)\n-\n-\n-\n\u2713\nCodeRL (Le et al., 2022)\n\u2713\n-\n-\n\u2713\nSelf-edit (Zhang et al., 2023a)\n\u2713\n-\n-\n\u2713\nSelf-refine (Madaan et al., 2023)\n-\n-\n\u2713\n\u2713\nSelf-debug (Chen et al., 2023c)\n\u2713\n-\n\u2713\n\u2713\nSelf-repair (Olausson et al., 2023)\n\u2713\n-\n\u2713\n\u2713\nReflexion (Shinn et al., 2023)\n\u2713\n-\n\u2713\n\u2713\nCodeChain (ours)\n\u2713\n\u2713\n\u2713\n\u2713\nFor a systematic comparison between CodeChain and related approaches, please refer to Table 4.\nB\nDEMONSTRATION OF MODULARIZED CODE GENERATION\nPlease refer to Figure 10 for an illustration of modularized code generation.\n```module \ndef count_start_end_chars(words):\n    \"\"\"\n    Description: This function counts the number \nof words that start and end with each character.\n    Input:\n    words (list): A list of binary words.\n...\n    \"\"\"\n```\n```module\ndef reverse_words(words, reversed_indices):\n...\n```\ndef count_start_end_chars(words):\n    start_count = collections.defaultdict(int)\n    end_count = collections.defaultdict(int)\n    for word in words:\n    \nstart_count[word[0]] += 1\n         end_count[word[-1]] += 1\n...\n    return start_count, end_count\ndef reverse_words(words, reversed_indices):\n    reversed_words = []\n...\nStep 1: Outline sub-modules\nStep 2: Implement sub-modules and combine them into a solution \n...\ndef main():\n    s,e = \ncount_start_\nend_chars(wo\nrds)\n    ...\n...\nend-to-end \nsolution\nFigure 10: An example of modularized code generation: first, the model is required to outline sub-\nmodules needed, each of which consists of a function header and docstring describing the intended\nuse. Subsequently, the model implements each module fully in code and integrates them as parts of\nthe complete final solution.\nC\nADDITIONAL DETAILS ON BENCHMARKS\nTable 6 presents the summary of the benchmarks we used. Note that on APPS, as the original\nbenchmark does not include a specific validation split, we randomly selected samples from the\noriginal training split and reported validation results on this set. We sampled 50 samples in each of\n13\nPublished as a conference paper at ICLR 2024\nTable 5: List of problem ids to create APPS test subset. We followed Olausson et al. (2023) to test on\nthe same subset of 20 samples. In total, we selected 14/3/3 introductory/interview/competition-level\nsamples from the original APPS test split.\nProblem level\nProblem IDs\nIntroductory\n4182, 4195, 4281, 4333, 4347, 4426, 4450, 4507, 4514, 4704, 4741, 4855, 4873, 4952\nInterview\n2106, 2673, 2923\nCompetition\n3070, 3286, 3754\nTable 6: A summary of APPS and CodeContests: \u2020 are statistics reported by Li et al. (2022).\nBenchmarks\nVal\nTest\n# Public\ntest cases\n# Private\ntest cases\nAPPS (Hendrycks et al., 2021)\n150\n5000\n1.98\n20.99 \u2020\nCodeContests (Li et al., 2022)\n117\n165\n2.00\n203.70 \u2020\nthe 3 levels of difficulty: Introductory, Interview, and Competition. For reproducibility, we included\nthe specific problem ids of this set in Table 7.\nWe also reported the average number of public and private test cases in each benchmark. Note\nthat on APPS, as the original benchmark did not officially include public test cases, we extracted\nexample input-output pairs from problem descriptions using a rule-based method. We then treated\nthe extracted input-output pairs as public test cases.\nD\nADDITIONAL EXPERIMENTAL RESULTS\nAnalysis on chain of self-revisions. Figure 11 and 12 show the clear performance gains of CodeChain,\nusing GPT3.5 and WizardCoder-34B as the base models, over 5 rounds of revisions. Specifically\non APPS, we found that model performance generally peaks at serf-revision round 4 (over 1.6x/ 2x\nperformance improvement on average on GPT3.5/ WizardCoder). There is a slight performance drop\nin round 5 with GPT3.5 model while the performance is quite stable on WizardCoder. Secondly,\nwe also found that the rates of performance gains are quite different on different levels of problem\ndifficulty. The best improvement from CodeChain is on competition-level problems (with over 2x/\n5x performance gain on GPT3.5/ WizardCoder). This observation indicates that for this type of\nproblem, LLMs can benefit more from leveraging representative modules as a form of hint to revise\nthe generated programs.\nIn Figure 13, we reported the relative performance gains of CodeChain over multiple rounds of\nrevisions when applying additional filtering on output samples. We observed that compared to direct\ngeneration (i.e. round 0), CodeChain can improve the performance of pass@1 by 1.4x on filtered\noutputs, using GPT3.5 as the base model. This observation indicates that CodeChain can complement\nthe line of research works for filtering or sub-sampling output code generations (Chen et al., 2023b;\nLi et al., 2022; Ni et al., 2023; Shi et al., 2022) by letting the models revise and improve the outputs\niteratively.\nAnalysis by filtering tests and filtering methods. To understand the impacts of using test cases\nfor filtering in CodeChain, we conducted ablation experiments with different types of test cases:\npublic/private/synthetic test cases. The synthetic test cases are generated by prompting GPT3.5 with\nexample test cases (see Appendix F for the full version of this prompt). On synthetic test cases, we\nexperimented with sampling generation outputs from the largest clusters (similar to Li et al. (2022))\nor by following some consensus of test outcomes among all generation samples (similar to Chen et al.\n(2023b)).\nAs can be seen in Table 8, when using synthetic test cases, CodeT sampling approach Chen et al.\n(2023b) can achieve better performance than other sampling approaches. Both the conventional\nfiltering approach (complete pass) and AlphaCode (sampling from the largest clusters) (Li et al.,\n2022) led to performance drops, possibly due to the noisy filtered outputs from imperfect synthetic\ntests. In CodeT, this data noise is addressed through a more sophisticated grouping and sampling\nmethod to select better program candidates. However, all results from synthetic tests are not as good\n14\nPublished as a conference paper at ICLR 2024\nFigure 11: APPS validation results with chain of self-revisions: we tested CodeChain+GPT3.5 for 5\nself-revision rounds and reported pass@1 results. We also reported the relative performance gains\nfrom direct generation (i.e. round 0). Note that in the left chart, the dotted lines represent normal\ndirect generation of non-modularized solutions.\nFigure\n12:\nAPPS\nvalidation\nresults\nwith\nchain\nof\nself-revisions:\nwe\ntested\nCodeChain+WizardCoder(34B) for 5 self-revision rounds and reported pass@1 results.\nWe\nalso reported the relative performance gains from direct generation (i.e. round 0). Note that in the left\nchart, the dotted lines represent normal direct generation of non-modularized solutions.\nFigure 13: APPS validation filtered pass@1 results with chain of self-revisions: we report the relative\nperformance gains of CodeChain+GPT3.5 when filtering output samples by synthetic test cases\n(similarly to CodeT (Chen et al., 2023b)) on all generation/ revision rounds.\n15\nPublished as a conference paper at ICLR 2024\nTable 7: List of problem ids to create APPS validation split.\nIn total, we selected 50/50/50\nintroductory/interview/competition-level samples from the original APPS training split.\nProblem level\nProblem IDs\nIntroductory\n2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374,\n2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2389,\n2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2400, 2401, 2402, 2403, 2404,\n2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412\nInterview\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27,\n28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50\nCompetition\n2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2010, 2011, 2012, 2013, 2014, 2015,\n2016, 2017, 2019, 2020, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n2032, 2033, 2034, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2045, 2046, 2048,\n2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056\nTable 8: Ablation results on APPS validation split by pass@1(%): we report the results of\nCodeChain+GPT3.5 using public/private/synthetic test cases to filter for generation samples be-\nfore applying grouping the sub-modules into clusters. For synthetic test cases, we experimented with\nsampling outputs from the largest cluster (following Li et al. (2022)) or sampling outputs by some\nconsensus of test outcomes (following Chen et al. (2023b)).\nFiltering Tests\nFilter by\nIntroductory\nInterview\nCompetition\nAll\n-\n-\n33.50\n23.70\n10.10\n22.43\nSynthetic\nAll Passed\n33.50 (0.0)\n23.70 (0.0)\n10.10 (0.0)\n22.43 (0.0)\nSynthetic\nAlphaCode\n33.15 (-0.4)\n20.98 (-2.7)\n12.00 (+1.9)\n22.04 (-0.4)\nSynthetic\nCodeT\n39.40 (+5.9)\n23.60 (-0.1)\n12.27 (+2.2)\n25.09 (+2.7)\nPublic\nAll Passed\n52.00 (+18.5)\n38.83 (+15.1)\n14.50 (+4.4)\n35.11 (+12.7)\nPrivate\nAll Passed\n55.70 (+22.2)\n37.60 (+13.9)\n18.90 (+8.8)\n37.40 (+15.0)\nas ones with public test cases available in problem descriptions. This observation indicates that\nCodeChain is quite sensitive to the correctness filtering and has to rely on high-quality test cases.\nFinally, as expected, we found that the best performance is obtained by filtering against private test\ncases which can cover more corner cases and lead to better filtered programs for self-revisions.\nAnalysis by different embedding models. Table 9 presents the results of CodeChain+GPT3.5 when\nusing different embedding methods: StarCoder (Li et al., 2023), CodeT5+ (Wang et al., 2023), and\nCodeBert (Feng et al., 2020). We found that CodeT5+ can outperform CodeBert in all levels of\nproblem difficulty, especially on the introductory-level problems. Among all embedding methods,\nusing StarCoder with CodeChain can achieve the best performance overall. We noted that one\nmajor advantage of StarCoder is the ability to encode long-context programs, which we found quite\nfrequently in many challenging coding problems in APPS and CodeContests.\nUsing public tests as a proxy to select optimal revision step. In all our experiments, we selected\nrevised generation samples during test time on the best revision round we found by validation results.\nHowever, in practice, we propose to use the public tests as a proxy to gauge model performance\nthroughout the chain of self-revisions. Figure 14 shows that on APPS, performance on public tests\nis often correlated well with the performance on private test cases. Specifically, the model peaked\nconsistently at revision round 4 in both sets of test cases. Therefore, in practice, even without a\nvalidation set, we can still apply CodeChain using public test results as a stopping signal to select the\nbest version of revised programs.\nResults on LeetCodeHardGym. Finally, we attempted to apply CodeChain on the recent code\ngeneration benchmark LeetCodehardGym (Shinn et al., 2023), including a set of 39 challenging\ncoding tasks extracted from LeetCode. Figure 15 shows that GPT4+CodeChain can significantly\nimprove the performance of using GPT4 alone. Different from APPS, we observed that the model\nachieves the best performance at revision round 3, achieving close to 7% pass@1. Note that our\nexperimental setup here is quite different from the original setup in (Shinn et al., 2023). We could\nonly test on 39/40 original test problems as 1 problem was withdrawn from the released benchmark\ndue to erroneous test data. Also, in (Shinn et al., 2023), the public test suite was synthetically created\nby LLMs and validated by AST. In our setup, we simply used the available public tests extracted\n16\nPublished as a conference paper at ICLR 2024\nTable 9: Ablation results on APPS validation split by pass@1(%): we report the results of\nCodeChain+GPT3.5 when clustering sub-modules by different embedding methods.\nEmbedding Model\nIntroductory\nInterview\nCompetition\nAll\n-\n33.50\n23.70\n10.10\n22.43\nCodeBert\n50.73 (+17.2)\n34.80 (+11.1)\n13.80 (+3.7)\n33.11 (+10.7)\nCodeT5+\n53.00 (+19.5)\n35.37 (+11.7)\n13.10 (+3.0)\n33.82 (+11.4)\nStarCoder\n52.00 (+18.5)\n38.83 (+15.1)\n14.50 (+4.4)\n35.11 (+12.7)\nfrom the problem descriptions. Nevertheless, we still observed consistent benefits of CodeChain in\nthis benchmark, complementing other related results of CodeChain in APPS and CodeContests.\nFigure 14: APPS validation results by pass@1\n(%) on public vs. private test cases\nFigure 15: LeetCodeHardGym pass@1 results\nusing GPT4+CodeChain on 5 revision rounds.\nE\nEXAMPLE GENERATION SAMPLES\nFor the same input problem, we presented different generation samples generated by GPT4 and\nCodeChain+GPT4. In each generation sample, we appended the evaluation generated by GPT4\nregarding the modularity and reusability of the output program. Through automatic evaluation\nmetrics, we found that the generation samples gradually became more modularized following the\nchain of revisions. In general, the generation samples are also more reusable as compared to the\nnormal direct generation approach (without CodeChain). Through manual inspection, we observed\nthat CodeChain can indeed generate better programs, properly modularized and well explained with\nfunction docstrings.\nE.1\nEXAMPLE OF INTRODUCTORY-LEVEL PROBLEM AND CODECHAIN GENERATION SAMPLES\nListing 1: An example of problem description from APPS (problem ID 4195)\nToday, the memorable AtCoder Beginner Contest 100 takes place. On this occasion, Takahashi would like to give\nan integer to Ringo.\nAs the name of the contest is AtCoder Beginner Contest 100, Ringo would be happy if he is given a positive\ninteger that can be divided by 100 exactly D times.\nFind the N-th smallest integer that would make Ringo happy.\n-----Constraints-----\n- D is 0, 1 or 2.\n- N is an integer between 1 and 100 (inclusive).\n-----Input-----\nInput is given from Standard Input in the following format:\nD N\n-----Output-----\nPrint the N-th smallest integer that can be divided by 100 exactly D times.\n-----Sample Input-----\n0 5\n-----Sample Output-----\n17\nPublished as a conference paper at ICLR 2024\n5\nThe integers that can be divided by 100 exactly 0 times (that is, not divisible by 100) are as follows: 1, 2,\n3, 4, 5, 6, 7, ...\nThus, the 5-th smallest integer that would make Ringo happy is 5.\nListing 2: An example of output generated by direct generation with GPT4\n\u2018\u2018\u2018python\nD, N = map(int, input().split())\nif N == 100:\nN += 1\nprint(100**D * N)\n\u2018\u2018\u2018\nThis problem can be solved by simple math. In Python, ** is used to calculate the power of a number.\nTherefore, 100 to the power of D is the factor by which we multiply N. However, if N is 100, we count\n101 as the 100th number because 100 itself is excluded as it can be divided by 100 but not exactly D\ntimes.\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\n### Modularity\n0\n### Re-usability\nThe code is not modular. So, re-usability rating is not applicable.\nListing 3: An example of output generated by GPT4 with CoT prompting (revision round 0 of\nCodeChain)\nSTEP 1: GENERATE SUB-MODULES\n\u2018\u2018\u2018module\ndef Nth_smallest_integer(D, N):\n\u2019\u2019\u2019\nThis function finds the nth smallest integer that can be divided by 100 exactly D times.\nD (int): a positive integer 0, 1 or 2.\nN (int): a positive integer between 1 and 100\nOutput:\nresult: the N-th smallest integer that can be divided by 100 exactly D times.\n\u2019\u2019\u2019\n\u2018\u2018\u2018\nSTEP 2: GENERATE PYTHON CODE\n\u2018\u2018\u2018python\ndef Nth_smallest_integer(D, N):\nif N == 100:\nN += 1\nresult = (100 ** D) * N\nreturn result\nD, N = map(int, input().split())\nprint(Nth_smallest_integer(D, N))\n\u2018\u2018\u2018\nThis code works by multiplying 100 raised to the power of D times with input number N. If N is 100, increment\nN by 1 since it needs to be the next contender for 100\u2019s divisible number.\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\n### Modularity\n2\n### Re-usability\n- nth_smallest_integer: 3. Justification: The function is specific to the task at hand and not generic enough\nto be reused in other problems. While the principle of multiplying by a certain power of a number could\npotentially be applied in other problems, the hardcoded values and the specific edge-case makes it less\nlikely to be directly reusable.\nListing 4: An example of output generated by GPT4 with CodeChain self-revision (revision round 3)\nSTEP 1: GENERATE SUB-MODULES:\n\u2018\u2018\u2018module\ndef get_input():\n\"\"\"\nDescription: This function gets inputs from the user/stdin.\nInput: None\nOutput:\nD (int): Divisibility with 100 factor\nN (int): The Nth term needed\n\"\"\"\n\u2018\u2018\u2018\n18\nPublished as a conference paper at ICLR 2024\n\u2018\u2018\u2018module\ndef happy_number(D, N):\n\"\"\"\nDescription: Calculate the Nth happy number according to the given divisibility rule.\nInput:\nD (int): Divisibility with 100 factor\nN (int): The Nth term needed\nOutput:\nint: the N-th smallest integer that can be divided by 100 exactly D times.\n\"\"\"\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef print_output(number):\n\"\"\"\nDescription: This function prints the answer.\nInput:\nnumber (int): the corresponding number.\nOutput:\nNone\n\"\"\"\n\u2018\u2018\u2018\nSTEP 2: GENERATE PYTHON CODE\n\u2018\u2018\u2018python\ndef get_input():\nD, N = map(int, input().split())\nreturn (D, N)\ndef happy_number(D, N):\nif N == 100:\nN += 1\nreturn 100**D * N\ndef print_output(number):\nprint(number)\nD, N = get_input()\nnumber = happy_number(D, N)\nprint_output(number)\n\u2018\u2018\u2018\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\n### Modularity\n5\n### Re-usability\n- get_input: 3 (Justification: This function is somewhat generic and it might be used in other places that\nrequire the re-use of two space-separated integer inputs. However, the number of inputs and the type is\nfixed hence not very re-usable. )\n- happy_number: 2 (Justification: This function is less generic and it is specific to solve the\n\"happy_number\" problem due to its specific logic that won\u2019t be encountered in most other problem\nscenarios.)\n- print_output: 4 (Justification: This function is semi-generic, you can print any number with it, but it is\nlimited to integers only, if we have other data types we could not be able to utilise this function\nappropriately.)\nListing 5: An example of output generated by GPT4 with CodeChain self-revision (revision round 5)\nSTEP 1: GENERATE SUB-MODULES:\n\u2018\u2018\u2018module\ndef get_input():\n\"\"\"\nDescription: This function gets inputs from the user/stdin.\nInput: None\nOutput:\nD (int): Divisibility with 100 factor\nN (int): The Nth term needed\n\"\"\"\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef print_output(number):\n\"\"\"\nDescription: This function prints the answer.\nInput:\nnumber (int): the currespond number.\nOutput:\nNone\n\"\"\"\nprint(number)\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef find_nth_integer(D, N):\n\"\"\"\nDescription: This function computes the N-th smallest integer that can be divided by 100 exactly D times.\n19\nPublished as a conference paper at ICLR 2024\nInput:\nD (int): Divisibility with 100 factor\nN (int): The Nth term needed\nOutput:\nnumber (int): the number that would make Ringo happy\n\"\"\"\n\u2018\u2018\u2018\nSTEP 2: GENERATE PYTHON CODE\n\u2018\u2018\u2018python\ndef get_input():\nD, N = map(int, input().split())\nreturn (D, N)\ndef print_output(number):\nprint(number)\ndef find_nth_integer(D, N):\nif N == 100:\nN += 1\nreturn 100 ** D * N\nD, N = get_input()\nnumber = find_nth_integer(D, N)\nprint_output(number)\n\u2018\u2018\u2018\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\nROUND 5\n### Modularity\n5\n### Re-usability\n- get_input: 5\n- print_output: 5\n- find_n_smallest_integer: 1. Justification: This function is very specific to the problem at hand and needs\nto meet very specific conditions, thus its reusability is limited. However, it could be used in similar\nproblems with slight modifications.\nE.2\nEXAMPLE OF COMPETITION-LEVEL PROBLEM AND CODECHAIN GENERATION SAMPLES\nListing 6: An example of problem description from APPS (problem ID 3286)\nYour friend Edvin loves languages. A personal favourite of his is speaking the Robber Language. For those of\nyou not familiar with it, it is a language transformation where every consonant is doubled, with an \u2019\u2019o\nput in-between. The rest is kept the same. For example, the word \"car\" would be transformed into\n\"cocaror\".\nYour friend Edvin was going to tell you his password to his favourite website, so that you could check out\nhow awesome it is. Edvin is sure that nobody but him and you speak the Robber Language, so he encrypted\nit using this simple technique, and wrote it down on a note. After trying the password and realising it\ndoes not work, you found out that Edvin was drunk during the transformation. Now you want to find out\nin how many ways the password might have looked originally.\nYou are given an encrypted password which Edvin has attempted to translate into the Robber Language. Your\ntask is to determine how many passwords might produce the encrypted password. You may assume that the\nonly thing Edvin did wrong, was when scanning through the word with his eyes, he sometimes missed to\ntransform some consonants using the rule above. Edvin is not deterministic in his failures, i.e. if he\nmissed to transform a consonant once, it is not necessary that he would miss it if it occured again\nlater in the string.\nA vowel is considered to be one of the letters \"a\", \"e\", \"i\", \"o\" and \"u\". A consonant is any other letter.\n-----Input-----\nInput consists of one encrypted password, the resulting word from Edvins translation. The word can be as long\nas $1000000$ characters. It will solely consist of lower case letters a-z.\n-----Output-----\nOutput should consist of one number, the number of ways the password might have looked before Edvin\ntranslated it. Since this number may be huge, print the remainder modulo $1000009$.\n-----Examples-----\nSample Input 1:\ncar\nSample Output 1:\n1\nSample Input 2:\ncocar\nSample Output 2:\n2\nListing 7: An example of output generated by direct generation with GPT4\n\u2018\u2018\u2018python\n20\nPublished as a conference paper at ICLR 2024\ns = input()\nn = len(s)\nvow = set([\"a\", \"e\", \"i\", \"o\", \"u\"])\ndp = [0]*(n+1)\nmod = 10**6+9\ndp[0] = 1\ndp[1] = 1\nfor i in range(2, n + 1):\nif s[i-1] in vow or s[i-2] in vow or i - 2 < 0 or s[i-1] != s[i-2] or s[i-1] == \u2019o\u2019:\ndp[i] = dp[i-1]\nelse:\ndp[i] = dp[i-1] + dp[i-2]\ndp[i] %= mod\nprint(dp[n])\n\u2018\u2018\u2018\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\n### Modularity\n0\n### Re-usability\nThe solution is not modular, hence re-usability rating is not applicable.\nListing 8: An example of output generated by GPT4 with CoT prompting (revision round 0 of\nCodeChain)\nSTEP 1: GENERATE SUB-MODULES:\n\u2018\u2018\u2018module\ndef is_vowel(ch):\n\"\"\"\nDescription: This function returns True if the specified character is a vowel, False otherwise.\nInput:\nch (char): A character.\nOutput:\n(bool): True if the character is a vowel, False otherwise.\n\"\"\"\ndef count_possible_original_words(word):\n\"\"\"\nDescription: This function counts the possible original words that could have transformed into the given\nRobber Language word.\nInput:\nword (str): A Robber Language word.\nOutput:\ncount (int): A count of possible original words.\n\"\"\"\n\u2018\u2018\u2018\nSTEP 2: GENERATE PYTHON CODE\n\u2018\u2018\u2018python\nMOD = 1000009\ndef is_vowel(ch):\nvowels = [\u2019a\u2019, \u2019e\u2019, \u2019i\u2019, \u2019o\u2019, \u2019u\u2019]\nreturn ch in vowels\ndef count_possible_original_words(word):\nN = len(word)\ndp = [0]*(N+1)\ndp[0] = 1\nfor i in range(1, N+1):\ndp[i] = dp[i-1]\nif not is_vowel(word[i-1]) and i >= 2 and word[i-1] == word[i-2] and word[i-3] != \u2019o\u2019:\ndp[i] += dp[i-2]\ndp[i] %= MOD\nreturn dp[N]\nword = input().strip()\nprint(count_possible_original_words(word))\n\u2018\u2018\u2018\nThis python function works as follows:\nIt counts the number of ways we can form the Robber language word by taking a dynamic programming approach.\nThe \u2018dp\u2018 list is initialized with zeroes with \u2018dp[0]\u2018 set to \u20181\u2018. Then for each character in the word,\nit checks whether it is a consonant and matches the preceding character and the character before that\nis not \u2019o\u2019. If these conditions are met, it counts this as a possible place where a consonant could\nhave been skipped and adds \u2018dp[i-2]\u2018 to \u2018dp[i]\u2018. Finally, it returns \u2018dp[N]\u2018 which contains the total\nnumber of possible original words. The modulo operation is used to keep the result from overflowing the\n32-bit integer limit.\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\n### Modularity\nRating: 0\n21\nPublished as a conference paper at ICLR 2024\n### Re-usability\nSince there are no sub-modules, there is no re-usability to rate.\nListing 9: An example of output generated by GPT4 with CodeChain self-revision (revision round 3)\nSTEP 1: GENERATE SUB-MODULES:\nSince we\u2019re given two very relevant functions \"split_word\" and \"count_possible_passwords\", we can keep these\nfunctions in our solution. The function \"initialize_vowels_and_modulo\" can be useful to initialize\nglobal variables.\n\u2018\u2018\u2018module\ndef split_word(word):\n\"\"\"\nDescription: This function splits the word into a list of elements that can be translated back into\nRobber language.\nInput:\nword (str): The word that needs to be split.\nOutput:\nsplit_word_list (list): The list of strings, where each string represents a possible original letter\n\"\"\"\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef initialize_vowels_and_modulo():\n\"\"\"\nDescription: This function initializes the vowels and modulo variables.\nOutput:\nvowels (set): A set of vowels\nmodulo (int): The modulo constant\n\"\"\"\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef count_possible_passwords(word_list):\n\"\"\"\nDescription: This function counts the number of possible password that might have produced the original\nword.\nInput:\nword_list (list): A list with elements from the word\nOutput:\n(int): The number of possible passwords modulo 1000009\n\"\"\"\n\u2018\u2018\u2018\nSTEP 2: GENERATE PYTHON CODE\n\u2018\u2018\u2018python\ndef split_word(word):\ni = 0\nsplit_word_list = []\nwhile i < len(word):\nif i + 2 < len(word) and word[i + 1] == \u2019o\u2019 and word[i + 2] != \u2019o\u2019:\nsplit_word_list.append(word[i:i+3])\ni += 3\nelse:\nsplit_word_list.append(word[i])\ni += 1\nreturn split_word_list\ndef count_possible_passwords(word_list, vowels, modulo):\ndp = [0] * len(word_list)\ndp[0] = 1 if len(word_list[0]) == 1 or word_list[0][0] in vowels else 2\nfor i in range(1, len(word_list)):\nif len(word_list[i]) == 1 or word_list[i][0] in vowels:\ndp[i] = dp[i - 1]\nelse:\ndp[i] = (dp[i - 1] * 2) % modulo\nreturn dp[-1]\ndef initialize_vowels_and_modulo():\nreturn set([\u2019a\u2019, \u2019e\u2019, \u2019i\u2019, \u2019o\u2019, \u2019u\u2019]), 1000009\ndef solve_task(word):\nsplit_word_list = split_word(word)\nvowels, modulo = initialize_vowels_and_modulo()\nreturn count_possible_passwords(split_word_list, vowels, modulo)\nword = input()\nprint(solve_task(word))\n\u2018\u2018\u2018\nIn the final code, function \"solve_task()\" uses \"split_word()\" to break down the word into possible original\nletters, \"initialize_vowels_and_modulo\" to get the vowel set and modulo values. Finally it uses\n\"count_possible_passwords()\" to calculate the number of possible original passwords.\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\n### Modularity\n4\n22\nPublished as a conference paper at ICLR 2024\n### Re-usability\n- split_word: 5\n- initialize_vowels_and_modulo: 3. Justification: Functions setting constants can be re-used, but the\nfunction name and content are very specific to the task.\n- count_possible_passwords: 2. Justification: It is specific to current problem, as it is specific to \u2019Robber\nLanguage\u2019 problem.\n- solve_task: 2. Justification: This function is overall problem specific not re-usable for other problems.\nListing 10: An example of output generated by GPT4 with CodeChain self-revision (revision round\n5)\nSTEP 1: GENERATE SUB-MODULES:\n\u2018\u2018\u2018module\ndef is_vowel(ch):\n\"\"\"\nDescription: This function checks whether a character is a vowel.\nInput:\nch (str): A single character string.\nOutput:\n(bool): Returns \u2018True\u2018 if the character is a vowel, \u2018False\u2018 otherwise.\n\"\"\"\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef split_word(word):\n\"\"\"\nDescription: This function splits the word into a list of elements that can be translated into the\noriginal letters.\nInput:\nword (str): The encrypted password.\nOutput:\nsplit_word_list (list): A list of strings, where each string represents a possible original letter.\n\"\"\"\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef count_possibilities(split_word_list):\n\"\"\"\nDescription: This function counts the number of ways each element in the list could be translated into\nthe original letters.\nInput:\nsplit_word_list (list): A list of strings, where each string is an element that could possibly be\ntranslated into an original letter.\nOutput:\npossibilities (int): The total number of ways the password might have looked originally.\n\"\"\"\n\u2018\u2018\u2018\nSTEP 2: GENERATE PYTHON CODE\n\u2018\u2018\u2018python\ndef is_vowel(ch):\nreturn ch in \u2019aeiou\u2019\ndef split_word(word):\ni = 0\nsplit_word_list = []\nwhile i < len(word):\nif i + 2 < len(word) and not is_vowel(word[i]) and word[i + 1] == \u2019o\u2019 and word[i] == word[i + 2]:\nsplit_word_list.append(word[i:i+3])\ni += 3\nelse:\nsplit_word_list.append(word[i])\ni += 1\nreturn split_word_list\ndef count_possibilities(split_word_list):\npossibilities = 1\nmodulo = 1000009\nfor element in split_word_list:\nif len(element) == 1 and not is_vowel(element):\npossibilities *= 2\npossibilities %= modulo\nreturn possibilities\nword = input()\nsplit_word_list = split_word(word)\npossibilities = count_possibilities(split_word_list)\nprint(possibilities)\n\u2018\u2018\u2018\nThis solution makes use of the \u2018split_word\u2018 and \u2018is_vowel\u2018 relevant functions. The \u2018count_possibilities\u2018\nfunction is new and quite specific to this problem. In \u2018split_word\u2018, we divide the word into potential\noriginal words assuming that Edvin didn\u2019t make any mistake when he processed a consonant. Then, if it\u2019s\na single character and is a consonant, it means it could either have been an original consonant that\nEdvin didn\u2019t process or a consonant that Edvin processed correctly.\n==================================================\n### OpenAI evaluation of the above output by modularity and reusability\n23\nPublished as a conference paper at ICLR 2024\n### Modularity\n5\n### Re-usability\n- is_consonant: 5, Justification: A function to check if a given character is a consonant or not is generic\nand can be used in a wide variety of tasks involving character manipulation or language processing.\n- split_word: 3, Justification: This function is less generic than \u2018is_consonant\u2018 because it is specifically\ndesigned to split a string in the special way that is relevant for the Robber Language. However, it\nmight be useful in similar tasks involving specially formatted string decoding.\n- solve_task: 1, Justification: This function is very specific to the current task and is unlikely to be\nreused in other problems as it implements the logic for deriving possible original passwords from the\nencrypted password in Robber language.\nF\nPROMPTS WITH INSTRUCTION\nListing 11: CoT prompting with instruction to generate modularized code. \u00abquestion_guide\u00bb is\nreplaced with instructions for the model to follow either standard input streams or call-based functions.\n*Instruction*\nDevelop a well-structured Python solution for the provided problem that obeys the constraints and passes the\nexample test cases. Ensure modularity and considering potential edge cases and failures. Start by\noutlining the required code modules, including function headers and signatures. Subsequently, proceed\nto implement each module to create the final code.\nIn simpler terms, create a clean and organized Python solution for the given problem. Break it down into\nsmaller parts (modules) with clear function names and input/output specifications. Once the structure\nis ready, write the actual code for each module to complete the solution.\nThe output code needs to <<question_guide>>. Please wrap your code answer using \u2018\u2018\u2018.\n### Example 1\n### TASK:\n<<example problem>>\n### RESPONSE:\nSTEP 1: GENERATE SUB-MODULES:\n<<example generated sub-modules>>\nSTEP 2: GENERATE PYTHON CODE\n<<example generated code>>\n-----------------\n### Example 2\n### TASK:\n<<new problem>>\n### RESPONSE:\nListing 12: self-revision prompting with instruction to revise and generate modularized code. \u00abques-\ntion_guide\u00bb is replaced with instructions for the model to follow either standard input streams\nor call-based functions. \u00absub_modules\u00bb is replaced with representative sub-modules selected by\nCodeChain framework.\n*Instruction*\nDevelop a well-structured Python solution for the provided problem that obeys the constraints and passes the\nexample test cases. Ensure modularity and considering potential edge cases and failures. Given a set of\nrelated utility Python functions, try to reuse or adapt them as much as possible into your solution\n(create new unique functions if needed). Start by outlining the required code modules, including\nfunction headers and signatures. Subsequently, proceed to implement each module to create the final\ncode.\nIn simpler terms, create a clean and organized Python solution for the given problem. Break it down into\nsmaller parts (modules) with clear function names and input/output specifications. Once the structure\nis ready, write the actual code for each module to complete the solution.\nThe output code needs to <<question_guide>>. Please wrap your code answer using \u2018\u2018\u2018.\n### Example 1\n### TASK:\n<<example problem>>\n### RELEVANT FUNCTIONS:\n<<sub-modules>>\n### RESPONSE:\nSTEP 1: GENERATE SUB-MODULES:\n<<example generated sub-modules>>\nSTEP 2: GENERATE PYTHON CODE\n<<example generated code>>\n-----------------\n### Example 2\n### TASK:\n<<new problem>>\n### RELEVANT FUNCTIONS:\n<<sub-modules>>\n### RESPONSE:\nListing 13: prompt to generate synthetic test cases to use as additional public tests. \u00abexample_test\u00bb is\nreplaced with any available test cases extracted in the problem description. We expect the model to\nfollow similar formats to the example test cases and continue to generate up to 20 test cases in total.\n24\nPublished as a conference paper at ICLR 2024\n### Question:\nGenerate 20 new input and output pairs as example test cases for the following problem:\n<<problem>>\nPlease response following this format:\n## Test 1:\nInput:\n<input>\nOutput:\n<expected output>\n## Test 2:\n...\n### Answer:\n20 test cases:\n<<example_test>>\nListing 14: prompt to evaluate code generation samples by their modularity and reusability qualities.\nYou are an expert software engineer and you always emphasize on writing modular reusable codes, for any given\ntask description.\nYou are given a task description and a candidate solution in form of a python code for it. You have to judge\nthe following aspects of the code:\n### Judging Modularity:\nA code is modular if it is decomposed into logical atomic sub-modules each handling different sub-tasks of\nthe given task. Sub-modules are functions that are invoked from the \u2019main\u2019 method.\nBased on your judgement give a rating on a Likert Scale from [0-5] to the given solution.\nModularity Ratings:\n- Rating 5: Optimally Modular - If the candidate solution is optimally modular\n- Rating 1 to 4: Less Modular - If the solution could have been more modular or if the sub-modules are not\natomic enough\n- Rating 0: Not Modular - If the solution is flat block of code without any decomposition into sub-modules\n### Judging Re-usability:\nA sub-module is re-usable if it is generic enough that its implementation can be re-used as-is for other\nproblems for this domain. If they have any specific characteristic or part of the implementation that\nis only particular to this given task and is less likely to appear in other tasks, then the sub-module\nis not re-usable.\nOnly if the candidate solution is at least somewhat modular and there exists at least one sub-module, for\neach sub-module give a rating on a Likert Scale from [0-5] its re-usability based on the above\njudgement. If you rate it less than 5, provide the justification.\nRe-usability Ratings:\n- Rating 5: the function is generic and reusable in other problems\n- Rating 1 to 4: the function is somewhat generic and might be reusable only in problems quite similar to the\ngiven task\n- Rating 0: the function is not at all generic or reusable in other problems\nIf the candidate solution is not modular such reusability rating is not needed.\nAlso, \u2019main\u2019 function is not a sub-module.\nYour response should only be the modularity and re-usability ratings. Judging the correctness of the\ncandidate solution or implementing a correct solution for the task is not needed.\nYour overall response should follow the format:\n### Modularity\n0-5 rating\n### Re-usability\n- <sub-module1>: <rating:0-5 rating. Justification: Justification if not-reusable. >\n- <sub-module2>: <rating ...>\n- <sub-module3>: <rating ...>\n- ... ...\n### TASK:\n<<TASK>>\n### CANDIDATE SOLUTION:\n<<CODE>>\n### RESPONSE:\nG\nONE-SHOT EXAMPLES\nListing 15: one-shot example input for normal code generation prompting or prompting to generate\nmodularized solutions\n25\nPublished as a conference paper at ICLR 2024\n### Example 1\nPolycarp has $n$ different binary words. A word called binary if it contains only characters \u20190\u2019 and \u20191\u2019. For\nexample, these words are binary: \"0001\", \"11\", \"0\" and \"0011100\".\nPolycarp wants to offer his set of $n$ binary words to play a game \"words\". In this game, players name words\nand each next word (starting from the second) must start with the last character of the previous word.\nThe first word can be any. For example, these sequence of words can be named during the game: \"0101\",\n\"1\", \"10\", \"00\", \"00001\".\nWord reversal is the operation of reversing the order of the characters. For example, the word \"0111\" after\nthe reversal becomes \"1110\", the word \"11010\" after the reversal becomes \"01011\".\nProbably, Polycarp has such a set of words that there is no way to put them in the order correspondent to the\ngame rules. In this situation, he wants to reverse some words from his set so that:\nthe final set of\n$n$ words still contains different words (i.e. all words are unique);\nthere is a way to put all words\nof the final set of words in the order so that the final sequence of $n$ words is consistent with the\ngame rules.\nPolycarp wants to reverse minimal number of words. Please, help him.\n-----Input-----\nThe first line of the input contains one integer $t$ ($1 \\le t \\le 10^4$) -- the number of test cases in the\ninput. Then $t$ test cases follow.\nThe first line of a test case contains one integer $n$ ($1 \\le n \\le 2\\cdot10^5$) -- the number of words in\nthe Polycarp\u2019s set. Next $n$ lines contain these words. All of $n$ words aren\u2019t empty and contains only\ncharacters \u20190\u2019 and \u20191\u2019. The sum of word lengths doesn\u2019t exceed $4\\cdot10^6$. All words are different.\nGuaranteed, that the sum of $n$ for all test cases in the input doesn\u2019t exceed $2\\cdot10^5$. Also, guaranteed\nthat the sum of word lengths for all test cases in the input doesn\u2019t exceed $4\\cdot10^6$.\n-----Output-----\nPrint answer for all of $t$ test cases in the order they appear.\nIf there is no answer for the test case, print -1. Otherwise, the first line of the output should contain $k$\n($0 \\le k \\le n$) -- the minimal number of words in the set which should be reversed. The second line\nof the output should contain $k$ distinct integers -- the indexes of the words in the set which should\nbe reversed. Words are numerated from $1$ to $n$ in the order they appear. If $k=0$ you can skip this\nline (or you can print an empty line). If there are many answers you can print any of them.\n-----Example-----\nInput\n4\n4\n0001\n1000\n0011\n0111\n3\n010\n101\n0\n2\n00000\n00001\n4\n01\n001\n0001\n00001\nOutput\n1\n3\n-1\n0\n2\n1 2\nListing 16: one-shot example input for code revision\n### Example 1\n### TASK:\nPolycarp has $n$ different binary words. A word called binary if it contains only characters \u20190\u2019 and \u20191\u2019. For\nexample, these words are binary: \"0001\", \"11\", \"0\" and \"0011100\".\nPolycarp wants to offer his set of $n$ binary words to play a game \"words\". In this game, players name words\nand each next word (starting from the second) must start with the last character of the previous word.\nThe first word can be any. For example, these sequence of words can be named during the game: \"0101\",\n\"1\", \"10\", \"00\", \"00001\".\nWord reversal is the operation of reversing the order of the characters. For example, the word \"0111\" after\nthe reversal becomes \"1110\", the word \"11010\" after the reversal becomes \"01011\".\n26\nPublished as a conference paper at ICLR 2024\nProbably, Polycarp has such a set of words that there is no way to put them in the order correspondent to the\ngame rules. In this situation, he wants to reverse some words from his set so that:\nthe final set of\n$n$ words still contains different words (i.e. all words are unique);\nthere is a way to put all words\nof the final set of words in the order so that the final sequence of $n$ words is consistent with the\ngame rules.\nPolycarp wants to reverse minimal number of words. Please, help him.\n-----Input-----\nThe first line of the input contains one integer $t$ ($1 \\le t \\le 10^4$) -- the number of test cases in the\ninput. Then $t$ test cases follow.\nThe first line of a test case contains one integer $n$ ($1 \\le n \\le 2\\cdot10^5$) -- the number of words in\nthe Polycarp\u2019s set. Next $n$ lines contain these words. All of $n$ words aren\u2019t empty and contains only\ncharacters \u20190\u2019 and \u20191\u2019. The sum of word lengths doesn\u2019t exceed $4\\cdot10^6$. All words are different.\nGuaranteed, that the sum of $n$ for all test cases in the input doesn\u2019t exceed $2\\cdot10^5$. Also, guaranteed\nthat the sum of word lengths for all test cases in the input doesn\u2019t exceed $4\\cdot10^6$.\n-----Output-----\nPrint answer for all of $t$ test cases in the order they appear.\nIf there is no answer for the test case, print -1. Otherwise, the first line of the output should contain $k$\n($0 \\le k \\le n$) -- the minimal number of words in the set which should be reversed. The second line\nof the output should contain $k$ distinct integers -- the indexes of the words in the set which should\nbe reversed. Words are numerated from $1$ to $n$ in the order they appear. If $k=0$ you can skip this\nline (or you can print an empty line). If there are many answers you can print any of them.\n-----Example-----\nInput\n4\n4\n0001\n1000\n0011\n0111\n3\n010\n101\n0\n2\n00000\n00001\n4\n01\n001\n0001\n00001\nOutput\n1\n3\n-1\n0\n2\n1 2\n### RELEVANT FUNCTIONS:\n\u2018\u2018\u2018module\ndef reverse_words(string):\n\"\"\"\ndescription: this function reverses each word in the given string.\ninput:\nstring (str): the input string.\noutput:\nreversed_string (str): the reversed string with each word reversed.\n\"\"\"\nreturn \u2019 \u2019.join(s[::-1] for s in string.split(\u2019 \u2019))\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef count_start_end_chars(words):\n\"\"\"\nDescription: This function counts the number of words that start and end with each character.\nInput:\nwords (list): A list of binary words.\nOutput:\nstart_count (defaultdict): A dictionary containing the count of words that start with each character.\nend_count (defaultdict): A dictionary containing the count of words that end with each character.\n\"\"\"\nstart_count = collections.defaultdict(int)\nend_count = collections.defaultdict(int)\nfor word in words:\nstart_count[word[0]] += 1\n27\nPublished as a conference paper at ICLR 2024\nend_count[word[-1]] += 1\nreturn start_count, end_count\n\u2018\u2018\u2018\nListing 17: one-shot example output for normal code generation\n### RESPONSE:\n\u2018\u2018\u2018python\nfor _ in range(int(input())):\nn = int(input())\nmass = []\nzo = 0\noz = 0\nzz = 0\noo = 0\nozs = []\nzos = []\nozss = set()\nzoss = set()\nfor j in range(n):\nk = input()\nmass.append(k)\nif k[0] == \u20190\u2019 and k[-1] == \u20191\u2019:\nzoss.add(k)\nzos.append(j + 1)\nzo += 1\nelif k[0] == \u20191\u2019 and k[-1] == \u20190\u2019:\nozss.add(k)\nozs.append(j + 1)\noz += 1\nelif k[0] == \u20190\u2019 and k[-1] == \u20190\u2019:\nzz += 1\nelse:\noo += 1\nif zz and oo and not oz and not zo:\nprint(-1)\ncontinue\nelse:\nif zo > oz:\nprint((zo - oz) // 2)\nans = []\nneed = (zo - oz) // 2\ni = 0\nwhile need:\nzzz = mass[zos[i] - 1][len(mass[zos[i] - 1]) - 1:: -1]\nif zzz not in ozss:\nans.append(zos[i])\nneed -= 1\ni += 1\nprint(*ans)\nelse:\nprint((oz - zo) // 2)\nans = []\nneed = (oz - zo) // 2\ni = 0\nwhile need:\nzzz = mass[ozs[i] - 1][len(mass[ozs[i] - 1]) - 1:: -1]\nif zzz not in zoss:\nans.append(ozs[i])\nneed -= 1\ni += 1\nprint(*ans)\n\u2018\u2018\u2018\nListing 18: one-shot example output for modularized code generation/revision\nSTEP 1: GENERATE SUB-MODULES:\n\u2018\u2018\u2018module\ndef count_start_end_chars(words):\n\"\"\"\nDescription: This function counts the number of words that start and end with each character.\nInput:\nwords (list): A list of binary words.\nOutput:\nstart_count (defaultdict): A dictionary containing the count of words that start with each character.\nend_count (defaultdict): A dictionary containing the count of words that end with each character.\n\"\"\"\n\u2018\u2018\u2018\n\u2018\u2018\u2018module\ndef reverse_words(words, reversed_indices):\n\"\"\"\nDescription: This function reverses the specified words in the given list.\nInput:\nwords (list): A list of binary words.\nreversed_indices (list): A list of indices indicating the words to be reversed.\nOutput:\nreversed_words (list): A new list of words with the specified words reversed.\n28\nPublished as a conference paper at ICLR 2024\n\"\"\"\n\u2018\u2018\u2018\nSTEP 2: GENERATE PYTHON CODE\n\u2018\u2018\u2018python\nimport collections\ndef count_start_end_chars(words):\nstart_count = collections.defaultdict(int)\nend_count = collections.defaultdict(int)\nfor word in words:\nstart_count[word[0]] += 1\nend_count[word[-1]] += 1\nreturn start_count, end_count\ndef reverse_words(words, reversed_indices):\nreversed_words = []\nfor i, word in enumerate(words):\nif i in reversed_indices:\nreversed_words.append(word[::-1])\nelse:\nreversed_words.append(word)\nreturn reversed_words\ndef solve_task(words):\nstart_count, end_count = count_start_end_chars(words)\ncharacters_with_difference = []\nfor char in start_count:\nif abs(start_count[char] - end_count[char]) > 1:\ncharacters_with_difference.append(char)\nreversed_indices = []\nfor char in characters_with_difference:\ndifference = abs(start_count[char] - end_count[char])\nreverse_count = difference // 2\nif start_count[char] < end_count[char]:\nindices = [i for i, word in enumerate(words) if word.startswith(char)]\nreversed_indices.extend(indices[:reverse_count])\nelse:\nindices = [i for i, word in enumerate(words) if word.endswith(char)]\nreversed_indices.extend(indices[:reverse_count])\nreversed_words = reverse_words(words, reversed_indices)\ntotal_reversed = len(reversed_indices)\nreturn total_reversed, reversed_words\nt = int(input())\nfor _ in range(t):\nn = int(input())\nwords = []\nfor _ in range(n):\nwords.append(input())\ntotal_reversed, reversed_words = solve_task(words)\nprint(total_reversed)\nif total_reversed != 0:\nprint(*reversed_words)\n\u2018\u2018\u2018\n29\n"
  },
  {
    "title": "Toward Joint Language Modeling for Speech Units and Text",
    "link": "https://arxiv.org/pdf/2310.08715.pdf",
    "upvote": "5",
    "text": "Toward Joint Language Modeling for Speech Units and Text\nJu-Chieh Chou1\u2217, Chung-Ming Chien1, Wei-Ning Hsu2, Karen Livescu1,\nArun Babu2, Alexis Conneau3\u2020, Alexei Baevski4\u2020, Michael Auli2\n1Toyota Technological Institute at Chicago 2Meta AI 3OpenAI 4Character AI\njcchou@ttic.edu,wnhsu@meta.com\nAbstract\nSpeech and text are two major forms of hu-\nman language. The research community has\nbeen focusing on mapping speech to text or\nvice versa for many years. However, in the\nfield of language modeling, very little effort\nhas been made to model them jointly. In light\nof this, we explore joint language modeling for\nspeech units and text. Specifically, we com-\npare different speech tokenizers to transform\ncontinuous speech signals into discrete units\nand use different methods to construct mixed\nspeech-text data. We introduce automatic met-\nrics to evaluate how well the joint LM mixes\nspeech and text. We also fine-tune the LM\non downstream spoken language understanding\n(SLU) tasks with different modalities (speech\nor text) and test its performance to assess the\nmodel\u2019s learning of shared representations. Our\nresults show that by mixing speech units and\ntext with our proposed mixing techniques, the\njoint LM improves over a speech-only baseline\non SLU tasks and shows zero-shot cross-modal\ntransferability.\n1\nIntroduction\nSpeech and language processing research has\nlargely focused on spoken and written language\nseparately.\nHowever, the integration of speech\nand text in a single model holds potential bene-\nfits. Speech data contains prosodic information\nthat does not exist in the text, which can help in\nmodeling dialogues. On the other hand, text data\nfrom sources like Wikipedia can provide structural\nknowledge that is not available in most speech\ndatasets.\nMoreover, the amount of written text\non the internet exceeds the size of any available\nspeech dataset.\nThe impressive performance of text large lan-\nguage models (LLMs) has caused a revolution in\nnatural language processing (Radford et al., 2019;\n\u2217Work done during an internship at Meta AI.\n\u2020Work done while at Meta AI.\nFigure 1: An illustration of our workflow. We tokenize\nspeech signals into discrete units and mix them with\ntext to create speech-text data. Our SUTLM is then\ntrained on a combination of speech-only, text-only, and\nspeech-text data. More details on the data formats can\nbe found in Table 1.\nBrown et al., 2020). On the other hand, genera-\ntive spoken language models (GSLM) (Lakhotia\net al., 2021), which are LMs trained on discrete\nspeech units derived from self-supervised represen-\ntations (Hsu et al., 2021), are also promising for\nspoken language modeling.\nIn this work, we aim to fill the gap between\ntext-only and speech-only LMs by developing and\nstudying design choices for a joint Speech Unit and\nText Language Model (SUTLM). For speech, we\nuse a self-supervised learning (SSL) speech model,\ni.e. HuBERT (Hsu et al., 2021), to convert con-\ntinuous speech signals into speech units. We then\ncombine the units with text data to train an LM\nthat models speech units and text jointly. We con-\nvert speech-only, mixed speech-text, and text-only\ndata into token sequences (as shown in Figure 1\nand Table 1), and train the model as an LM.\narXiv:2310.08715v1  [cs.CL]  12 Oct 2023\nTo evaluate the SUTLM, automatic metrics are\ndeveloped to quantify the cross-modal ability of\nthe LMs. We also fine-tune our models on down-\nstream tasks for spoken language understanding.\nWe fine-tune the SUTLMs on either the speech or\ntext data and test them on either speech or text to\nunderstand how well the models learn to align the\ntwo modalities.\nOur main contributions are:\n\u2022 We present a joint autoregressive LM trained\non both speech and text (Sec 3).\n\u2022 We develop automatic metrics that require no\nfine-tuning for the evaluation of an SUTLM,\nand show that the proposed metrics are indica-\ntive of the model\u2019s cross-modal transfer ability\non downstream tasks (Sec 4).\n\u2022 Empirically, we show that units covering a\nlarger span obtained through SentencePiece\ntokenization (Kudo and Richardson, 2018)\noutperform local units learned by existing\nself-supervised models (Hsu et al., 2021)\n(Sec 5.5.1).\n\u2022 We find that mixing speech units and text\nwith our proposed techniques (Sec 5.5.3 &\nSec 5.5.4) improves the cross-modal ability of\nthe model. (Sec 5.4).\n2\nRelated Work\n2.1\nSSL speech models\nSelf-supervised pre-training enables speech mod-\nels to learn the information in speech without\npaired text transcriptions and show impressive\nperformance on tasks such as automatic speech\nrecognition (ASR) with minimal supervised fine-\ntuning (Baevski et al., 2020; Hsu et al., 2021; Chen\net al., 2021). As SSL speech models learn pho-\nnetically meaningful speech representations (Pasad\net al., 2023), they can be used as a feature extrac-\ntor (Yang et al., 2021) or a quantizer to transform\ncontinuous speech into discrete units (Lakhotia\net al., 2021; Lee et al., 2021a,b; Lin et al., 2022;\nChen et al., 2022a).\nIn this work, we use the\nHuBERT model (Hsu et al., 2021) along with a\nquantizer to tokenize continuous speech into dis-\ncrete representations. The discrete speech units are\nthen combined with text data to train a single LM\nthat is able to model speech and text jointly.\n2.2\nTextless NLP\nTextless NLP (Lakhotia et al., 2021; Polyak et al.,\n2021; Kharitonov et al., 2021) is a framework to\nmodel speech in the absence of textual data. It\nconsists of three components: a speech-to-unit to-\nkenizer, a unit LM (uLM), and a unit-to-speech\ndetokenizer. The tokenizer takes speech signals as\ninputs to generate discrete speech units. A uLM\nis trained to predict the next token in an utterance\ngiven its prior context. Once the uLM is trained, it\ncan be used to generate unit sequences autoregres-\nsively. In the end, the detokenizer is used to convert\nthe generated unit sequences to speech signals.\n2.3\nJoint speech-text transformers\nTransformer models have been extremely suc-\ncessful in natural language and speech process-\ning (Vaswani et al., 2017; Gulati et al., 2020),\nwith three major configurations: encoder-decoder\nmodels (Vaswani et al., 2017), encoder-only mod-\nels (Devlin et al., 2018), and decoder-only mod-\nels (Radford et al., 2018).\nPrevious works on speech-text joint transform-\ners mostly adapt the encoder-decoder (Ao et al.,\n2021; Tang et al., 2022; Cheng et al., 2022) or\nencoder-only (Chung et al., 2020; Bapna et al.,\n2021; Chen et al., 2022b; Zhang et al., 2022b) ar-\nchitectures. Compared with decoder-only archi-\ntectures, the training of these models typically re-\nquires multiple losses and explicit alignments be-\ntween paired speech and transcriptions. This makes\nthe hyper-parameter selection time-consuming.\nAlso, encoder-only and encoder-decoder models\nare mostly used in the pre-training + fine-tuning\nparadigm, which limits the use cases of these mod-\nels.\nOn the other hand, decoder-only models on\ntext (Radford et al., 2019; Brown et al., 2020) show\nthe impressive capability of in-context learning,\nwhich also reduces the efforts spent on fine-tuning\npre-trained models. In light of this, we explore\ndecoder-only models for speech-text joint train-\ning.\nIn this under-explored area, the concurrent\nwork VALL-E (Wang et al., 2023) is the only other\nattempt to build a decoder-only model jointly mod-\neling speech and text. However, VALL-E\u2019s purpose\nis controllable text-to-speech synthesis (TTS), and\nthe work mainly focuses on the acoustic controlla-\nbility of the generated speech , while our work aims\nto build a general-purpose joint LM and mainly fo-\ncuses on modeling the content of spoken language.\n3\nMethod\nWe start with a dataset of sentences D\n=\n{s1, s2, . . . , sn}, where a sentence si is composed\nof a sequence of Ti tokens (zi\n1, zi\n2, . . . , zi\nTi), where\nzi\nj can be either text or speech units. The SUTLM\nis trained to predict the next token zi\nj given its prior\ncontext zi\n<j. We maximize the log-probability of\nthe data\nn\nX\ni=1\nTi\nX\nj=1\nlog P(zi\nj|zi\n<j)\n(1)\nIn the following sections, we describe how we con-\nstruct token sequences from speech and text. An\nexample of our data formats can be found in Ta-\nble 1.\n3.1\nSpeech-only: unit LM (uLM)\nPrior work has shown that discrete speech units\nderived from a pre-trained HuBERT model can be\nused as compact representations to encode speech\ncontent, enabling the training of a unit language\nmodel (Lakhotia et al., 2021). However, when com-\nbining speech with text, the time scales of speech\nunits and text differ. HuBERT units are typically\non the phone or sub-phone level, as shown in Ta-\nble 2. This leads to longer sequences, making it\ndifficult for the model to capture long-term depen-\ndencies. On the other hand, subword tokenizers for\ntext generally break text sequences into chunks of a\nlarger size than speech units. This length mismatch\nbetween speech and text makes it challenging to\nmodel them in a single model. Therefore, we use\na subword tokenizer (Kudo and Richardson, 2018)\nto combine HuBERT units into larger chunks as\nin (Wu et al., 2022) to mitigate the length mis-\nmatch.\nThe process of generating speech units is as\nfollows. Speech signals are first fed into a Hu-\nBERT model.\nThe representations in the final\nlayer are then clustered with the k-means algo-\nrithm. The cluster IDs are used as the discrete\nspeech units after removing consecutive repeating\nunits (Lakhotia et al., 2021).1 These units are then\nfurther combined by the subword SentencePiece\ntokenizer (Kudo and Richardson, 2018). The re-\nsulting average number of tokens per second can\nbe found in Table 2.\n1For example, the unit sequence 13 13 15 80 80 80\nbecomes 13 15 80 after removing repetitions.\n3.2\nText-only: text LM (tLM)\nWe train another SentencePiece tokenizer (Kudo\nand Richardson, 2018) using the text-only cor-\npus Sec 5.1.3 to convert text into subword tokens.\nThe resulting vocabulary size of the subword to-\nkens is around 45k.\n3.3\nConcatenated speech-text (CST)\nTo present paired speech-text data to the SUTLM,\nwe first convert speech units and their transcriptions\ninto the uLM and tLM formats, respectively, and\ncombine them into one sequence by simply concate-\nnating them as shown in Table 1. The CST format\nexplicitly tells the model the correspondence be-\ntween paired speech and text and thus encourages\nthe model to learn the dependence between speech\nunits and the corresponding text transcriptions.\n3.4\nAlternating speech-text (AST)\nAside from simply concatenating the sequences\nof speech units and text, we also construct mixed\nspeech-text that takes the word-level correspon-\ndence into consideration.\nWe\nuse\na\npre-trained\nspeech\nrecog-\nnizer (McAuliffe et al., 2017) to force-align\nspeech and its transcription to obtain the word\nboundaries in an utterance. We then randomly\nsample some word boundaries within the utter-\nance2 as the \"switching points\", which divide the\nutterance into several chunks.\nThe alternating\nspeech-text (AST) sequence is then constructed by\nalternatively filling in the chunks with uLM speech\nunits and tLM text tokens, resulting in a sequence\nthat switches modalities at every switching point.\nSpecial tokens <U2T> and <T2U> are inserted when\nswitching from speech units to text and text to\nspeech units, respectively.\n4\nEvaluation Metrics\nWe introduce automatic metrics that require no\nfine-tuning to evaluate the SUTLM. Fine-tuning\nis a common approach to assess the quality of pre-\ntrained models (Baevski et al., 2020; Hsu et al.,\n2021; Chen et al., 2021). However, it is a time-\nconsuming process and the reliability of the ex-\nperiments highly depends on the hyper-parameter\nselection process. Furthermore, there is no reliable\nmetric to measure the cross-modal ability of LMs.\n2For a sentence with k words, we uniformly sample \u230aN\u230b\nboundaries as the switching points with N \u223c N( k\n10, 1).\nTask\nExample\nuLM\n<U_EN> S12 S66 S17 S18 ... <EOU>\nCST\n<U_EN> S12 S66 S17 S18 ... <EOU> <T_EN> how are you <EOS>\nCST\n<T_EN> how are you <EOS> <U_EN> S12 S66 S17 S18 ...<EOU>\nAST\n<U_EN> S12 S66 <U2T> are you <EOS>\nAST\n<T_EN> how <T2U> S17 S18 ... <EOU>\ntLM\n<T_EN> how are you <EOS>\nTable 1: An example of the formats of unpaired (uLM, tLM) and mixed speech-text (CST, AST) data. For the CST\nand AST formats, speech units and text can be present in a sequence in different orders . <U_EN> and <T_EN> are\nused at the beginning of the unit/text sequence. <EOU> and <EOS> are used at the end of the unit/text sequences.\n<U2T> and <T2U> are used when switching from unit to text and text to unit at word boundaries.\nAverage tokens per second\nPhone\n20.32\nHuBERT\n50.00\n+ deduplication\n33.33\n+ SP 10k\n17.67\n+ SP 32k\n14.33\nTable 2: The average number of tokens per second for\ndifferent types of speech units. SP 10k and 32k refer\nto SentencePiece tokenization (Kudo and Richardson,\n2018) applied to HuBERT units to create a dictionary\nwith 10k and 32k tokens respectively.\nIn light of this, we propose Context Retrieval Ac-\ncuracy (CRA), a new metric that does not require\nfine-tuning, to evaluate the cross-modal ability of\nan SUTLM.\n4.1\nContext Retrieval Accuracy (CRA)\nThe motivation of Context Retrieval Accuracy\n(CRA) comes from the intuition that a good LM\nshould learn to predict the next token based on\nits prior context. When we divide a sentence into\nprompt and continuation, a good LM should be\nable to capture the dependence between them. That\nis, it should assign a higher conditional probability\nto the continuation given its corresponding prompt\nthan given a random prompt.\nTo measure CRA, we gather a collection of m\nsentences C = {s1, s2, . . . , sm} and break si into\na pair of prompt xi and continuation yi. Given an\nSUTLM parameterized by \u03b8, we can measure the\nconditional probabilities P\u03b8(yi|xi) with Eq 1. The\nCRA is then computed as:\n1\nm\nm\nX\ni=1\n1[arg\nmax\nj\u2208{1...m} P\u03b8(yi|xj) = i],\n(2)\nThat is, the LM is used as a scorer to classify\nwhether the matched prompt-continuation pair has\nthe highest conditional probability among a pool\nof unmatched prompts.\nCRA also has a pointwise mutual information\n(PMI) interpretation:\narg\nmax\nj\u2208{1...m} P\u03b8(yi|xj) = i\n=\u21d2 log P\u03b8(yi|xi) \u2265\nmax\nj\u2208{1...m} log P\u03b8(yi|xj)\n=\u21d2 log P\u03b8(yi|xi)\nP\u03b8(yi)\n\u2265\nmax\nj\u2208{1...m} log P\u03b8(yi|xj)\nP\u03b8(yi)\n=\u21d2 PMI(xi, yi) \u2265\nmax\nj\u2208{1...m} PMI(xj, yi)\n(3)\nThat is, correctly identifying the prompt implies\nthe matched prompt-continuation pair has a higher\nPMI than all unmatched prompt-continuation\npairs.\nIdeally, the model should produce similar rep-\nresentations given the same content regardless of\nthe modality. Hence, in addition to the uni-modal\nCRA, we also consider cross-modal CRA, where\nthe prompt and the continuation are in different\nmodalities. In practice, for example, when we use\ntext as the prompts and speech units as the con-\ntinuations, we set the probability of emitting text\ntokens to zero and re-normalize the probability to\nensure that the continuation yi can be only speech\nunits. Cross-modal CRA can be used as a way to\nmeasure whether the SUTLM successfully learns\nshared representations between text and speech.\n4.2\nPerplexity under External LM (PELM)\nFollowing previous work, we use the perplexity\nunder external LM (PELM) to measure the quality\nof the content of generated samples (Lakhotia et al.,\n2021). We sample a continuation from the SUTLM\ngiven each ground truth prompt. We then use an\nexternal text LM, OPT-6.7B (Zhang et al., 2022a),\nto compute the perplexity of the sequence:\n\u02c6yi \u223c P\u03b8(y|xi)\nx\u2032i, y\u2032i = T(xi \u2225 \u02c6yi)\nPELM(\u03b8) = 2\n\u2212 P\ni log POPT(y\u2032i| gt(xi))\nP\ni len(y\u2032i)\n(4)\nwhere xi and \u02c6yi refer to the prompt and sam-\npled continuation, and \u03b8 are the parameters of the\nSUTLM. Similarly to cross-modal CRA, we con-\ntrol the modality of sampled continuations by ze-\nroing out the probability of the tokens in the unde-\nsired modality. Since the prompt and the contin-\nuation can be either speech units or subword text\ntokens, we use a transcriber T(\u00b7) to transcribe the\nconcatenated sequences xi \u2225 \u02c6yi into text x\u2032i, y\u2032i.3\ngt(\u00b7) is a function that outputs a ground truth tran-\nscription when the input is speech units and is an\nidentity function when the input is text. The exter-\nnal LM is then used to measure the perplexity of\nthe continuation part of the text sequence.\n4.3\nEvaluation on SLUE tasks\nWe use the SLUE benchmark (Shon et al., 2022)\nto evaluate our models on downstream tasks. The\nbenchmark includes two tasks, sentiment analysis\n(SLUE-SA) and named entity recognition (SLUE-\nNER), with both speech data and transcriptions\nprovided. After pre-training the SUTLM, we fine-\ntune it on the SLUE dataset with either speech or\ntext data as inputs to predict the ground-truth labels,\nand then evaluate it on either speech or text inputs.\nWe evaluate the model on different input modalities\nto understand the cross-modal ability of the model\nas in (Hsu and Shi, 2022; Bapna et al., 2021, 2022).\nFine-tuning details can be found in 5.4.2.\n5\nExperiments\n5.1\nData\n5.1.1\nSpeech-only\nWe use 5% of the dataset used in (Aghajanyan\net al., 2023) to match the size of the mixed speech-\ntext and text-only data.\nThe dataset includes\nMultilingual LibriSpeech (MLS) (Pratap et al.,\n2020), VoxPopuli (Wang et al., 2021), Common-\nVoice (Ardila et al., 2019) and Spotify Podcast &\n3For both speech units and text tokens, we first invert the\nSentencePiece tokenization process to get raw HuBERT units\nand raw text. For speech units, we further use a 12-layer\nTransformer encoder with a CTC head to map HuBERT units\nto text. The transformer is trained on LibriSpeech, with a\nWER of 5.18% on dev-clean, and 11.61% on dev-other.\nPeople\u2019s Speech (Aghajanyan et al., 2023). The\nsubsampled dataset consists of 65k hours of speech.\n5.1.2\nMixed speech-text (CST and AST)\nWe use MLS (Pratap et al., 2020) and VoxPop-\nuli (Wang et al., 2021) to create mixed speech-text\ndata without subsampling. The dataset contains\n45k hours of speech and 2.7B of words.\n5.1.3\nText-only\nWe combine OPT web data (Zhang et al., 2022a),\nWikipedia, and LibriLM (Panayotov et al., 2015),\nand then subsample 5% of it, resulting in a total of\n8.5B subwords.\n5.2\nSSL speech tokenizer\nWe use a HuBERT Base model trained on 221K\nhours of unlabeled speech in 8 languages as in (Hsu\net al., 2022; Nguyen et al., 2023).4\nAfter pre-\ntraining, the representations at the last layer (12th)\nare clustered with k-means using 2000 clusters.\n5.3\nModel architecture and training\nWe use the 24-layer transformer implementation in\nfairseq (Ott et al., 2019) with 16 attention heads.\nThe embedding size is 1024, the feed-forward di-\nmension is 4096, and the dropout probability is set\nto 0.1. The weights of the embedding layer are tied\nto the output layer (Press and Wolf, 2016). The\nmodel contains 350M parameters.\nThe model is trained for 500k updates on 32\nV100 GPUs with a batch size of 8192 tokens per\nGPU. We use Adam optimizer (Kingma and Ba,\n2014) with (\u03b21, \u03b22) = (0.9, 0.95). Gradient clipping\nwith a threshold 1.0 and weight decay of 0.1 are\napplied to stabilize the training. Since the data size\nis different for different data formats, we resample\nspeech-only, speech-text, and text-only data equally\n(1/3 for each in every training batch) to prevent the\nmodel from being biased toward any of them.\n5.4\nEvaluation setup\n5.4.1\nAutomatic Metrics\nWe use a subset of the Multilingual Lib-\nriSpeech (Pratap et al., 2020) dev set to evaluate\nthe SUTLM. To provide enough context to the\nSUTLM, we filter out sentences of less than 20\nwords. For each sentence, we use the first 10 words\nas the prompt and the rest as continuation. For the\nCRA experiments, we evaluate the SUTLM with\n4https://dl.fbaipublicfiles.com/hubert/\nmhubert_base_vp_mls_cv_8lang_it3.pt\nthe 100 shortest utterances in the filtered dataset,\nwhile for the PELM experiments, we use the 500\nshortest utterances. We use fewer utterances in\nCRA experiments as the computation of CRA is\nO(N2) for N utterances. We constrain ourselves\nto sentences with moderate lengths because the\ncontinuation part becomes less coherent with the\nprompt as the sequence length grows, which hurts\nthe sensitivity of the proposed metrics.\nWhen sampling the speech or text continuations\nin the PELM experiments, we use temperature t =\n0.6 and nucleus sampling (Holtzman et al., 2019)\nwith p = 0.95, and truncate the continuation to 10\nwords (identical to the length of the prompts).\n5.4.2\nDownstream Tasks\nFor SLUE-SA, we fine-tune SUTLM by adding\na self-attention pooling layer on top of the trans-\nformer model after removing the last output\nlayer (Shon et al., 2022). We fine-tune it with a\nlearning rate of 3e-5 for 30k updates and evaluate\nit with Macro F1 (Shon et al., 2022).\nFor SLUE-NER, we follow the SLUE official\nbaseline to formulate the task as an ASR prob-\nlem and train our model to decode special tokens\naround each named entity (Shon et al., 2022). We\nconcatenate the output (the text transcription with\nspecial tokens before and after each named entity)\nafter the input (speech units when fine-tuned on\nspeech, text tokens when fine-tuned on text) and\nfine-tune our SUTLM as an LM with the same loss\nfunction as Eq 1. The loss is only applied to the out-\nput part of the sequence. We fine-tune the SUTLM\nwith a learning rate of 3e-5 for 50k updates. During\ndecoding, we use a beam size of 5 to generate the\noutputs and evaluate them with Micro F1 (Shon\net al., 2022). For both SLUE tasks, we report re-\nsults on the dev set since the test set is not publicly\navailable. We use the fine-tuned HuBERT as the\nbaseline as in (Shon et al., 2022).\n5.5\nResults\n5.5.1\nWhat kind of speech units works the\nbest?\nWe utilize HuBERT units described in Sec 5.2\n(2000 units) and apply SentencePiece tokenizers\non them. Results can be found in rows (A), (B),\n(C) in Table 3 for automatic metrics, Table 4 for\nSLUE-SA and Table 5 for SLUE-NER.\nThe model trained with SP 10k has the best per-\nformance in terms of PELM, SLUE-SA, and SLUE-\nNER, but slightly worse CRA than the model using\nthe original HuBERT units. For CRA for the u2u\ncase (unit prompt, unit continuation), we hypoth-\nesize that the model uses low-level acoustic infor-\nmation to make predictions as the CRAs are nearly\n1.0 for all types of speech units. Also, HuBERT\nuses overlapping windows for neighboring tokens,\nso the first token of the continuation contains infor-\nmation about the previous token.\nFor the speech continuation (PELM) experi-\nments, the SP 10k-based sequences are shorter\nthan HuBERT unit-based sequences, so the model\ntrained with SP 10k (row (B)) can generate more\ncoherent continuations.\n5.5.2\nDo we need paired data to learn shared\nrepresentations?\nIn this section, we compare models trained with\nand without paired data to investigate the useful-\nness of paired data. We can compare the results\nin row (D) and (F) in Table 3 for automatic met-\nrics, Table 4 for SLUE-SA and Table 5 for SLUE-\nNER. For cross-modal cases (u2t and t2u), in terms\nof automatic metrics, the model trained with un-\npaired data alone (row (D)) has almost random\nCRAs and high PELMs, indicating a lack of cross-\nmodal ability.\nSimilarly, for SLUE-SA, the model trained with\nunpaired data alone (row (D)) shows almost ran-\ndom macro F1 scores for a 3-way classification task\nwhen tested on the other modality. For SLUE-NER,\nthe model trained without exposure to paired data\n(row (D)) performs worse than models trained with\npaired data (row (F)) when fine-tuned on speech\nand shows no transferability between modalities.\nRow (D) also performs worse than its speech unit-\nonly counterpart (row (B), showing that the model\ntrained solely on unpaired data does not demon-\nstrate any cross-modal transfer ability between\nspeech and text.\n5.5.3\nDoes concatenated speech-text (CST)\nhelp learn shared representations?\nThe next question we want to answer is whether\nCST is helpful in learning shared representations.\nBuilding on the previous findings (rows (A), (B),\n(C)), we utilize SP 10k as our speech unit vocab-\nulary and present the results in row (E) in Table 3\nfor automatic metrics, Table 4 for SLUE-SA, and\nTable 5 for SLUE-NER. The results show that, com-\npared to using unpaired data alone (row (D)), the\nmodel trained with CST (row (E)) has higher CRAs\nfor u2t and t2u, which indicates that the model cap-\nu2u\nt2u\nu2t\nt2t\nrow\nunit\nuLM\nCST\nAST\ntLM\nCRA\nPELM\nCRA\nPELM\nCRA\nPELM\nCRA\nPELM\nGround truth continuation\n-\n-\n-\n-\n-\n-\n-\n101.4\n(A)\nHuBERT\nv\n1.00\n193.3\n-\n-\n-\n-\n-\n-\n(B)\nSP 10k\nv\n0.96\n163.6\n-\n-\n-\n-\n-\n-\n(C)\nSP 32k\nv\n0.96\n177.4\n-\n-\n-\n-\n-\n-\n(D)\nSP 10k\nv\nv\n0.94\n175.9\n0.03\n394.9\n0.01\n1973.3\n0.20\u2217\u2217\n20.7\u2217\u2217\n(E)\nSP 10k\nv\nv\n0.95\n166.0\n0.37\n39.1\u2217\n0.26\n43.4\u2217\n0.56\n34.7\n(F)\nSP 10k\nv\nv\nv\nv\n0.97\n162.8\n0.70\n124.7\n0.81\n38.7\n0.67\n28.2\nTable 3: Automatic metrics (CRA and PELM). \"u2t\" denotes that the prompts are speech units and the continuations\nare text, and so on. (*): for cross-modal cases (u2t and t2u) in row (E), the PELM is low because the continuation\nsimply repeats the prompt. We discuss this issue in Sec 5.6. (**): The low CRA for t2t is due to the use of MLS as\nan evaluation set, resulting in a distribution mismatch from the text-only training data. Similarly, the use of OPT\ndata to train the SUTLM results in better PELM on t2t in row (D).\nFT data\nSP\nTXT\nrow\nunit\nEval set\nSP\nTXT\nSP\nTXT\nBaseline\n0.46\n-\n-\n-\n(A)\nHuBERT\nuLM\n0.51\n-\n-\n-\n(B)\nSP 10k\nuLM\n0.56\n-\n-\n-\n(C)\nSP 32k\nuLM\n0.54\n-\n-\n-\n(D)\nSP 10k\nuLM+tLM\n0.52\n0.33\n0.35\n0.49\n(E)\nSP 10k\nuLM+CST\n0.48\n0.42\n0.51\n0.52\n(F)\nSP 10k\nuLM+CST\n+AST+tLM\n0.49\n0.43\n0.52\n0.56\nTable 4: Macro F1 score on SLUE-SA. FT data indicates\nthe model is fine-tuned on speech (SP) or text (TXT).\nEval set denotes the fine-tuned model is tested on speech\n(SP) or text (TXT).\nFT data\nSP\nTXT\nrow\nunit\nEval set\nSP\nTXT\nSP\nTXT\nBaseline\n54.5\n-\n-\n-\n(A)\nHuBERT\nuLM\n62.9\n-\n-\n-\n(B)\nSP 10k\nuLM\n64.4\n-\n-\n-\n(C)\nSP 32k\nuLM\n62.5\n-\n-\n-\n(D)\nSP 10k\nuLM+tLM\n63.2\n1.5\n0.0\n66.8\n(E)\nSP 10k\nuLM+CST\n65.0\n3.6\n0.5\n79.5\n(F)\nSP 10k\nuLM+CST\n+AST+tLM\n66.6\n25.2\n0.3\n77.2\nTable 5: The F1(%) score on SLUE-NER. FT data in-\ndicates the model is fine-tuned on speech (SP) or text\n(TXT). Eval set denotes the fine-tuned model is tested\non speech (SP) or text (TXT).\ntures the relationship between speech and text bet-\nter than models trained with unpaired data alone.\nFor SLUE-SA, the model pre-trained with CST\nshows comparable performance when fine-tuned\non one modality and evaluated on the other. The\nperformance when fine-tuning on text and testing\non speech is even better than directly fine-tuning\non speech (0.51 vs. 0.48). The reason is likely\nto be that text data provides a less noisy supervi-\nsory signal compared to using speech units. The\nmodel trained with extra speech-text data (row (E))\nperforms worse than the model trained with only\nspeech units (row (B)). The reason may be similar\nto the \"curse of multilinguality\" (Conneau et al.,\n2019), where sharing the capacity of the model\nwith other languages or modalities hurts perfor-\nmance.\nFor SLUE-NER, concatenated speech-text im-\nproves performance over the model trained with\nonly speech units (row (B)) when fine-tuned on\nspeech. Unlike SLUE-SA, which is a classifica-\ntion task, here we need to generate the correspond-\ning transcription along with the named entity tags\nfor SLUE-NER. Hence, the model (row (E)) fine-\ntuned on speech benefits directly from the extra\nspeech-text data. We discuss the implications of\nthe fine-tuning results further in Sec 5.7.\nFor speech / text continuation, when only using\nconcatenated speech-text data (CST) as our mixed\ndata, there are no special tokens (<U2T>, <T2U>)\nto trigger modality switching. As shown in Table 6,\nin the u2t case the model trained with CST simply\ntranscribes the speech prompt into its transcription\non u2t and synthesizes the text prompt into speech\nunits, resulting in low PELMs for u2t and t2u in row\n(D) due to the repetition. PELM fails to reflect the\nquality of the continuation accurately. We discuss\nthis limitation further in Sec 5.6.\n5.5.4\nDoes alternating speech-text (AST) help\nlearn shared representations?\nThis section discusses the benefits of alternating\nspeech-text (AST). The results are presented in\n(row (F)) in Table 3 for automatic metrics, Table 4\nfor SLUE-SA, and Table 5 for SLUE-NER.\nBy comparing the results of CRA for t2u and\nu2t in row (F) with those in row (E) in Table 3, we\nobserve an improvement in CRA when the data is\ndirectly constructed to switch modalities on word\nboundaries. We can also see that CRA is similar for\nt2u, u2t, and t2t. It suggests that the model learns\nto match context regardless of modality.\nIn row (F), PELM for t2u is lower than PELM\nfor u2u as the text prompt is less noisy than speech\nunits. PELM for u2t is only marginally worse than\nt2t. This shows that the LM trained with AST can\ncontinue a sentence regardless of the modality. The\nworse PELM for u2u and t2u than for u2t and t2t\ncould be attributed to the recognition errors within\nour unit transcriber.\nRegarding SLUE-SA, we can observe that AST\nand tLM further improve the cross-modal transfer\nperformance (trained on the text and evaluated on\nspeech, or vice versa) in row (F).\nIn SLUE-NER, row (F) also shows better per-\nformance than row (E) when fine-tuned on speech\nand evaluated on speech. There is also non-trivial\nspeech-to-text transfer (fine-tuned on speech and\nevaluated on text) in row (F), showing that AST\nhelps in learning transferable features between\nmodalities.\nIn SLUE-NER, when fine-tuned on text and eval-\nuated on speech, there is no transferability between\nspeech and text. The reason can be attributed to the\nfine-tuning task becoming almost trivial. In text\nNER, in our formulation, the input and output are\nnearly identical. The only difference is the named\nentity tags. Further discussion of downstream task\nperformance can be found in Sec 5.7.\n5.6\nLimitations of PELM\nWe use PELM as a metric to measure the quality\nof continuations. However, although our SUTLM\n(row (F)) shows the ability to continue after a cross-\nmodal prompt, the resulting continuation is still\nonly locally consistent as shown in Table 6. This\ncan be attributed to the use of a 350M-parameter\nmodel architecture, which is relatively small in the\nera of LLMs.\nThe PELM metric fails to accurately reflect the\nresult in the case of row (E) when the model sim-\nply repeats the prompt. It has been a known phe-\nnomenon that LMs tend to assign a high probability\nto repeated tokens (Holtzman et al., 2019).\nTo quantify repetition, we compute the propor-\ntion of bi-grams in continuations that have ap-\npeared in the prompt transcription. For row (E),\nthe proportions are 0.02, 0.53, 0.42, and 0.02 for\nu2u, u2t, t2u, and t2t, respectively. For row (F), the\nproportions are 0.02, 0.03, 0.01, and 0.03. For row\n(E), the continuations for u2t and t2u are simply\nrepeating the content of the prompt.\nWe can see that the u2t and t2t PELMs are lower\nthan the ground truth PELM. This is because of\nthe use of the temperature of 0.6 in the softmax\nlayer, which likely hurts diversity and coherence as\nin (Caccia et al., 2018; Lakhotia et al., 2021).\n5.7\nImplications for SLU Downstream Tasks\nWe show that mixing speech units and text im-\nproves the cross-modal ability of the model. In\nSLUE-SA, the mixed speech-text data enables the\nmodel to zero-shot transfer between speech and\ntext. In SLUE-SA, we remove the output layer\nfrom the SUTLM and attach a classification head\nso the model will always output a valid class.\nIn SLUE-NER, using mixed speech-text data\ndirectly improves the performance. Since this is\na sequence generation task, the mixed speech-text\ndata helps the model generate better text. The trans-\nfer from speech to text is non-trivial but not vice\nversa. This finding aligns with the experiments\nin (Bapna et al., 2022), in which they also find\nnon-trivial transfer from speech to text but not the\nother way around. However, we note that different\nfine-tuning strategies can produce different results,\nas demonstrated in (Liu et al., 2021).\n6\nConclusion\nOur study on joint language modeling for speech\nunits and text involved developing evaluation met-\nrics and fine-tuning the model on speech and text\ndata. We found that using mixed speech-text data\nimproves the model\u2019s cross-modal ability and per-\nformance on both automatic metrics and down-\nstream tasks.\nOur study sheds light on the benefits of consid-\nering both speech and text in building language\nmodels. We hope that this research will motivate\nthe research community to further explore the in-\ntegration of speech and text data for more compre-\nhensive language modeling.\nFuture work in this area could involve investigat-\ning the optimal balance between speech and text\ndata in model training and exploring ways to handle\nmulti-modal data beyond the speech-text domain.\n7\nLimitations\nOur approach involves using a speech tokenizer\nthat can encode phonetic information (HuBERT)\nand an off-the-shelf speech recognizer to generate\nword-level alignment. For other, lower-resource\nlanguages, these components may be harder to ob-\ntain or may not perform as well.\nFor our proposed automatic metrics, the com-\nplexity of CRA grows at a rate of O(N2), which\ncan be expensive when evaluated on a larger num-\nber of utterances or when scaling up the model size.\nPELM, on the other hand, also has limitations as\nstated in Sec 5.6. For the empirical results on down-\nstream tasks, we test our SUTLMs on the SLUE\nbenchmark, which has only two tasks. Extending\nthe experiments to more downstream tasks may\nprovide more insights.\nFinally, we only study relatively small SUTLMs\n(350M parameters). It is unclear how scaling it up\nwould affect the results.\nReferences\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-\nNing Hsu, Karen Hambardzumyan, Susan Zhang,\nStephen Roller, Naman Goyal, Omer Levy, and\nLuke Zettlemoyer. 2023. Scaling laws for genera-\ntive mixed-modal language models. arXiv preprint\narXiv:2301.03728.\nJunyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo\nRen, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang,\net al. 2021.\nSpeechT5: Unified-modal encoder-\ndecoder pre-training for spoken language processing.\narXiv preprint arXiv:2110.07205.\nRosana Ardila, Megan Branson, Kelly Davis, Michael\nHenretty, Michael Kohler, Josh Meyer, Reuben\nMorais, Lindsay Saunders, Francis M Tyers, and\nGregor Weber. 2019. Common voice: A massively-\nmultilingual speech corpus.\narXiv preprint\narXiv:1912.06670.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nAdvances in Neural Information Processing Systems,\n33:12449\u201312460.\nAnkur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin\nJohnson, Yong Cheng, Simran Khanuja, Jason Riesa,\nand Alexis Conneau. 2022. mSLAM: Massively mul-\ntilingual joint pre-training for speech and text. arXiv\npreprint arXiv:2202.01374.\nAnkur Bapna, Yu-an Chung, Nan Wu, Anmol Gulati,\nYe Jia, Jonathan H Clark, Melvin Johnson, Jason\nRiesa, Alexis Conneau, and Yu Zhang. 2021. SLAM:\nA unified encoder for speech and language model-\ning via speech-text joint pre-training. arXiv preprint\narXiv:2110.10329.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2018. Language gans falling short. arXiv preprint\narXiv:1811.02549.\nPeng-Jen Chen, Kevin Tran, Yilin Yang, Jingfei Du,\nJustine Kao, Yu-An Chung, Paden Tomasello, Paul-\nAmbroise Duquenne, Holger Schwenk, Hongyu\nGong, et al. 2022a. Speech-to-speech translation\nfor a real-world unwritten language. arXiv preprint\narXiv:2211.06474.\nSanyuan Chen, Chengyi Wang, Zhengyang Chen,\nYu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, et al. 2021.\nWavLM: Large-scale self-supervised pre-training\nfor full stack speech processing.\narXiv preprint\narXiv:2110.13900.\nZhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana\nRamabhadran, Pedro Moreno, Ankur Bapna, and\nHeiga Zen. 2022b. MAESTRO: Matched speech text\nrepresentations through modality matching. arXiv\npreprint arXiv:2204.03409.\nYong Cheng, Yu Zhang, Melvin Johnson, Wolfgang\nMacherey, and Ankur Bapna. 2022.\nMu2SLAM:\nMultitask, multilingual speech and language mod-\nels. arXiv preprint arXiv:2212.09553.\nYu-An Chung, Chenguang Zhu, and Michael Zeng.\n2020. SPLAT: Speech-language joint pre-training\nfor spoken language understanding. arXiv preprint\narXiv:2010.02295.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al.\n2020. Conformer: Convolution-augmented trans-\nformer for speech recognition.\narXiv preprint\narXiv:2005.08100.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\nrahman Mohamed. 2021. HuBERT: Self-supervised\nspeech representation learning by masked prediction\nof hidden units. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 29:3451\u20133460.\nWei-Ning Hsu, Tal Remez, Bowen Shi, Jacob Don-\nley, and Yossi Adi. 2022. ReVISE: Self-supervised\nspeech resynthesis with visual input for universal\nand generalized speech enhancement. arXiv preprint\narXiv:2212.11377.\nWei-Ning Hsu and Bowen Shi. 2022. u-HuBERT: Uni-\nfied mixed-modal speech pretraining and zero-shot\ntransfer to unlabeled modality. In Advances in Neural\nInformation Processing Systems.\nEugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi,\nJade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Mor-\ngane Rivi\u00e8re, Abdelrahman Mohamed, Emmanuel\nDupoux, et al. 2021. Text-free prosody-aware gen-\nerative spoken language modeling. arXiv preprint\narXiv:2109.03264.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,\nYossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Abdelrahman\nMohamed, et al. 2021. On generative spoken lan-\nguage modeling from raw audio. Transactions of the\nAssociation for Computational Linguistics, 9:1336\u2013\n1354.\nAnn Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu,\nSravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi,\nQing He, Yun Tang, et al. 2021a. Direct speech-to-\nspeech translation with discrete units. arXiv preprint\narXiv:2107.05604.\nAnn Lee, Hongyu Gong, Paul-Ambroise Duquenne,\nHolger Schwenk, Peng-Jen Chen, Changhan Wang,\nSravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu,\nand Wei-Ning Hsu. 2021b.\nTextless speech-to-\nspeech translation on real data.\narXiv preprint\narXiv:2112.08352.\nGuan-Ting Lin, Yung-Sung Chuang, Ho-Lam Chung,\nShu-wen Yang, Hsuan-Jui Chen, Shang-Wen Li, Ab-\ndelrahman Mohamed, Hung-yi Lee, and Lin-shan\nLee. 2022. DUAL: Textless spoken question answer-\ning with speech discrete unit adaptive learning. arXiv\npreprint arXiv:2203.04911.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. arXiv preprint arXiv:2103.10385.\nMichael McAuliffe, Michaela Socolof, Sarah Mihuc,\nMichael Wagner, and Morgan Sonderegger. 2017.\nMontreal forced aligner: Trainable text-speech align-\nment using kaldi. In Interspeech, volume 2017, pages\n498\u2013502.\nTu Anh Nguyen, Wei-Ning Hsu, Antony d\u2019Avirro,\nBowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Re-\nmez, Jade Copet, Gabriel Synnaeve, Michael Hassid,\net al. 2023. Expresso: A benchmark and analysis of\ndiscrete expressive speech resynthesis. Interspeech.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for se-\nquence modeling. arXiv preprint arXiv:1904.01038.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. Librispeech: an asr cor-\npus based on public domain audio books. In 2015\nIEEE international conference on acoustics, speech\nand signal processing (ICASSP), pages 5206\u20135210.\nIEEE.\nAnkita Pasad, Bowen Shi, and Karen Livescu. 2023.\nComparative layer-wise analysis of self-supervised\nspeech models. In ICASSP.\nAdam Polyak,\nYossi Adi,\nJade Copet,\nEugene\nKharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-\ndelrahman Mohamed, and Emmanuel Dupoux.\n2021. Speech resynthesis from discrete disentan-\ngled self-supervised representations. arXiv preprint\narXiv:2104.00355.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel\nSynnaeve, and Ronan Collobert. 2020. MLS: A large-\nscale multilingual dataset for speech research. arXiv\npreprint arXiv:2012.03411.\nOfir Press and Lior Wolf. 2016. Using the output em-\nbedding to improve language models. arXiv preprint\narXiv:1608.05859.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSuwon Shon, Ankita Pasad, Felix Wu, Pablo Brusco,\nYoav Artzi, Karen Livescu, and Kyu J Han. 2022.\nSLUE: New benchmark tasks for spoken language un-\nderstanding evaluation on natural speech. In ICASSP\n2022-2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n7927\u20137931. IEEE.\nYun Tang, Hongyu Gong, Ning Dong, Changhan\nWang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski,\nXian Li, Abdelrahman Mohamed, Michael Auli,\net al. 2022.\nUnified speech-text pre-training for\nspeech translation and recognition. arXiv preprint\narXiv:2204.05409.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu,\nChaitanya Talnikar, Daniel Haziza, Mary Williamson,\nJuan Pino, and Emmanuel Dupoux. 2021. VoxPop-\nuli: A large-scale multilingual speech corpus for rep-\nresentation learning, semi-supervised learning and\ninterpretation. arXiv preprint arXiv:2101.00390.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, et al. 2023. Neural codec\nlanguage models are zero-shot text to speech synthe-\nsizers. arXiv preprint arXiv:2301.02111.\nFelix Wu, Kwangyoun Kim, Shinji Watanabe, Kyu\nHan, Ryan McDonald, Kilian Q Weinberger, and\nYoav Artzi. 2022. Wav2Seq: Pre-training speech-to-\ntext encoder-decoder models using pseudo languages.\narXiv preprint arXiv:2205.01086.\nShu-wen Yang, Po-Han Chi, Yung-Sung Chuang,\nCheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin,\nAndy T Liu, Jiatong Shi, Xuankai Chang, Guan-\nTing Lin, et al. 2021. SUPERB: Speech processing\nuniversal performance benchmark. arXiv preprint\narXiv:2105.01051.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022a. OPT: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nZiqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu,\nShuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong,\nLirong Dai, Jinyu Li, et al. 2022b. SpeechLM: En-\nhanced speech pre-training with unpaired textual data.\narXiv preprint arXiv:2209.15329.\nA\nAppendix\nprompt\nthey need the same aid and they\nmodel\nmodalities\nContinuation\n(E)\nu2u\nsoon will increase and reach the grave more gradually than ever they seek\ntheir own home and many little people suffer from the hospitality and care of\nthe poor\nu2t\nthey need the same aid and they need the same aid\nt2u\nthey need they they need the sameling the need the need the same sense of\nwill that they need for others others to to to to do they will a and and they to\nhave\nt2t\nneed the same help they can make and are able to make the best of them and\nthe best of them\n(F)\nu2u\ncertainly do need the same money and they do not give themselves out of\neither field or settlement but they do need it they say and they need it\nu2t\nshall be able to perform it all the same the king of egypt is not free from the\npersecution of the jews he is free from the persecution of the jews\nt2u\ncan no longer face it but though these men believe women now to be the\nprincipal duties the progress made will actually continue\nt2t\nwill be the same as they have been when they were born and they will be the\nsame as they now are if they were not born they would be the same as they\nnow are\nprompt\nbarrack was just coming through the doorway carrying half\nmodel\nmodalities\nContinuation\n(E)\nu2u\ndoesn papa and several small scotch girls who had been sitting by the fire with\nhim he said to the girls on whom he kept his second mother mother is good to\nme and so i would not let her go away\nu2t\nbarrek was just coming through the doorway carrying half a dozen other things\nt2u\nround of the door he was just coming through the doorway carrying half\naround of the door they stopped at the back of the doorway and half a around\nof the doorway and half a round of the doorway\nt2t\na dozen men and a dozen men and women i believe and i think i could have\nsaved the lives of those who loved me-but i don\u2019t know how i could have saved\nthem\n(F)\nu2u\ndozen boxes when he saw the stick black inside in his room stepping out of\nhis way into the hall a chuckle of joy fell in the drawing room and he seized\nthe boxes and broke down the door\nu2t\na cup of coffee when gertie came up with a basketful of rice and a handful of\nwater and then she came to the house of mrs smiths and she said to gertie\nt2u\ndozen packs and a wrapper and a light sparkling light across the face of jack\nand a burning gold bullet and a very sharp thumb\nt2t\na dozen toy guns and a hundred toy guns and a hundred toy guns\nTable 6: Example for speech and text continuation. Speech continuation has been transcribed by the transcriber.\n"
  }
]