[
  {
    "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents",
    "link": "https://arxiv.org/pdf/2306.16527.pdf",
    "upvote": "39",
    "text": "OBELICS: An Open Web-Scale Filtered\nDataset of Interleaved Image-Text Documents\nHugo Lauren\u00e7on\u2217,1,2\nLucile Saulnier\u2217,1\nL\u00e9o Tronchon\u2217,1\nStas Bekman\u2217,1\nAmanpreet Singh\u2217,1\nAnton Lozhkov1\nThomas Wang1\nSiddharth Karamcheti1,3\nAlexander M. Rush\u2020,1\nDouwe Kiela\u2020,1,3\nMatthieu Cord\u2020,2\nVictor Sanh\u2217,\u2020,1\n\u2217Equal contributions, \u2020Senior contributions\nhugo@huggingface.co\n1Hugging Face 2Sorbonne Universit\u00e9 3Stanford University\nAbstract\nLarge multimodal models trained on natural documents, which interleave\nimages and text, outperform models trained on image-text pairs on various\nmultimodal benchmarks. However, the datasets used to train these models\nhave not been released, and the collection process has not been fully specified.\nWe introduce the OBELICS dataset, an open web-scale filtered dataset of in-\nterleaved image-text documents comprising 141 million web pages extracted\nfrom Common Crawl, 353 million associated images, and 115 billion text\ntokens. We describe the dataset creation process, present comprehensive\nfiltering rules, and provide an analysis of the dataset\u2019s content. To show\nthe viability of OBELICS, we train vision and language models of 9 and 80\nbillion parameters named IDEFICS, and obtain competitive performance\non different multimodal benchmarks. We release our dataset, models and\ncode.1.\n1\nIntroduction\nRecent systems demonstrate the effectiveness of training large multimodal models such as\nFlamingo on naturally occurring multimodal documents (Alayrac et al., 2022; Aghajanyan\net al., 2022; Huang et al., 2023). A multimodal document is a succession of text paragraphs\ninterleaved by images, such as web pages that contain images. Models trained on these web\ndocuments outperform vision and language models trained solely on image-text pairs on\nvarious benchmarks (Alayrac et al., 2022). They can also generate long and coherent text\nabout a set of multiple images.\nWhile these results are compelling, they have not been replicable. The datasets used in\nthese works are not publicly available, and relatively little information is known about their\ncreation process and composition. This state motivates the creation of large-scale collections\nof high-quality multimodal web documents to support the creation of the next generation of\nmodels.\nWe take inspiration from existing large open image-text datasets such as LAION (Schuhmann\net al., 2022) and COYO (Byeon et al., 2022), comprised of hundreds of millions of image-text\n1OBELICS: https://huggingface.co/datasets/HuggingFaceM4/OBELICS\nOBELICS reproduction code: https://github.com/huggingface/OBELICS\nIDEFICS models: https://huggingface.co/HuggingFaceM4/idefics-80b\nPreprint. Under review.\narXiv:2306.16527v2  [cs.IR]  21 Aug 2023\nTottenham vs Chelsea Live \nStreaming\nTottenham Spurs vs Chelsea \nLive Streaming\nThe match between Tottenham Spurs vs Chelsea will kick \noff from 16:30 at Tottenham Hotspur Stadium, London.\nThe derby had been played 54 times and the Blues have \ndominated the Spurs. Out of 54 matches played, Chelsea has \nwon 28 times and Spurs had only won 7 times. The remaining \n19 matches had ended in draw.\nHowever, in recent 5 meetings, Spurs had won 3 times where \nChelsea had won the other two times. \u2026\nImage-Text Pairs\nMultimodal Document\nFigure 1: A comparison of extraction from the same web document. For image-text pairs, the\nalt-text of images is often short or non-grammatical. For OBELICS, the extracted multimodal\nweb document interleaves long-form text with the images on the page.\npairs obtained through web crawling. These datasets have been critical to developing and\nreplicating numerous recent multimodal models (Radford et al., 2021; Wang et al., 2022; Yu\net al., 2022; Wang et al., 2022; Liu et al., 2023). While this approach allows for building\nextremely large and diverse training datasets, we note several limitations to using only\nimage-text pairs. From a language perspective, these datasets rely primarily on alt-text,\nmeaning the text given is brief, captures an approximate snapshot of the image\u2019s content, and\noften lacks grammatical correctness. From a document perspective, image-text pairs remove\nan image from its natural context on a page and its relationship with other documents.\nIn this work, we introduce OBELICS2, an openly-accessible curated web-scale dataset consisting\nof 141 million multimodal English web documents which contain 353 million associated\nimages and 115 billion tokens. OBELICS collects full multimodal documents interleaving\ntext and images as shown in Figure 1. We describe the dataset creation process, outline\nthe filtering and curation steps and shed light on the dataset\u2019s content and limitations. To\ndemonstrate the viability of OBELICS, we train IDEFICS, an 80 billion parameter multimodal\nmodel and show competitive performance against large-scale multimodal models such as\nFlamingo (Alayrac et al., 2022).\n2\nRelated Works\nImage-text pairs datasets\nThe largest multimodal datasets, such as LAION (Schuhmann\net al., 2021, 2022), Conceptual Captions (Sharma et al., 2018; Changpinyo et al., 2021),\nALIGN (Jia et al., 2021), COYO (Byeon et al., 2022), and DataComp (Gadre et al., 2023),\ncontain billions of image-text pairs and are usually obtained through web-crawling and\nalt-text extraction. A variety of multimodal models have been trained on this type of dataset:\nmultimodal encoder models which use a contrastive objective (Radford et al., 2021; Wang\net al., 2022), image generation based on Transformers or diffusion processes (Nichol et al.,\n2022; Ramesh et al., 2022; Rombach et al., 2021; Saharia et al., 2022). While the scale of\nthese datasets makes them attractive candidates for training, our work focuses on extracting\nimages and the textual context in which they appear instead of extracting the associated\nalternative text.\nWeb document datasets\nInsights from scaling language models (Kaplan et al., 2020;\nHoffmann et al., 2022) emphasize the need for increasingly bigger datasets. For instance,\n2Open Bimodal Examples from Large fIltered Commoncrawl Snapshots\n2\nLLaMA (Touvron et al., 2023) was trained on a dataset of 1.4T tokens created exclusively\nfrom openly accessible English web content. The authors noticed that an even bigger dataset\nwould have benefited the model. To address that need, multiple web-scale datasets have\nbeen introduced and made available: c4 (Raffel et al., 2019), ROOTS (Lauren\u00e7on et al.,\n2022), Pile (Gao et al., 2020), OSCAR (Ortiz Su\u00e1rez et al., 2020). Although OBELICS falls\nin the same category of making accessible large collections of curated web documents, the\nadditional extraction of images changes the nature of the resulting dataset. It allows training\nmodels with additional vision capabilities.\nMultimodal web document datasets\nThe recent most performant vision and language\nmodels are trained on large sets of multimodal web documents. For instance, Flamingo\n(Alayrac et al., 2022), an 80 billion multimodal model, was trained on a mix of 2.1 billion\nimage-text pairs, 27 million video-text pairs, and 43 million multimodal web documents.\nThe latter called M3W, includes 185 million images. Similarly, KOSMOS-1 (Huang et al.,\n2023) was trained on a mixture containing 71 million multimodal web documents. However,\nin both cases, the dataset is not publicly available, and little information is accessible as\nto the dataset\u2019s content, the strategies employed to create that dataset (including filtering\nstrategies), and the quality of the resulting web documents, which ultimately hinders further\nresearch.\nConcurrently to our work, the Multimodal C4 (mmc4) dataset (Zhu et al., 2023) was recently\nmade accessible. It consists of 103 million multimodal web documents that include 585\nmillion images. Although there are similarities between our datasets, it is important to\nhighlight particular distinctions. First, our dataset is based on more recent documents from\nFebruary 2020 to February 2023, whereas mmc4 uses documents from April 2019. Additionally,\nour filtering heuristics appear to be more comprehensive: we leverage the HTML DOM trees\nto filter out undesirable texts and images, whereas mmc4 uses the HTML to find images in\norder to merge them with the original C4 dataset by solving a bipartite assignment problem\nbased on a CLIP model similarities. Last, we implement additional deduplication steps at\nthe image, document, and paragraph levels.\n3\nCreation of the Multimodal Web Document Dataset\nCollecting a large number\nof HTML files\n\u2022 Selection of English content\n\u2022 Early text deduplication\n\u2022 Quality classification\nFiltering multimodal web\ndocuments\n\u2022 Node level image filtering\n\u2022 Paragraph-level text filtering\n\u2022 Document-level filtering\n10x smaller HTML files\n365M docs\n1.4B images\nOBELICS\nDeduplicating\n\u2022 Image deduplication\n\u2022 Document deduplication\n\u2022 Paragraph deduplication\nResponsible filtering\n\u2022 Exclusion of opted-out images\n\u2022 NSFW images removal\nExtracting multimodal\nweb documents\n\u2022 Preservation of the original\nstructure of the web pages\n\u2022 Image downloading\n141M docs\n353M images\n1.1B docs\n2B images\nSimplifying HTML\nfiles\n\u2022 DOM tree cleaning strategies\n\u2022 Tag unwrapping\n\u2022 Node removal\n\u2022 Modification of specific nodes\nCommon\nCrawl\ndata\n41.2B\ndocs\n1.1B\ndocs\nFigure 2: Overview of the steps involved in creating OBELICS.\nThis section provides an overview of the critical choices of the creation and filtering process.\nFigure 2 gives a high-level summary of the main steps involved. Many details are omitted\nfrom this section, and we invite the reader to refer to the appendix A.1 for completeness.\n3\n3.1\nCollecting a Large Number of HTML Files\nFirst, we collect a vast amount of raw web documents by considering the 25 most recent\nCommon Crawl dumps at the time of the creation, spanning from February 2020 to Jan-\nuary/February 20233. We extract the main text from the documents while discarding\ndocuments with text of insufficient quality. This process results in 41.2 billion documents.\nTo filter out non-English content, we apply the FastText classifier (Joulin et al., 2017) to the\nextracted text, which removes 63.6% of the documents. We perform a MinHash (Broder,\n1997) deduplication to remove duplicate content. Additionally, we filter out documents with\nsignificant proportions of repeated paragraphs and n-grams, following the methodology used\nin MassiveText (Rae et al., 2022). Previous studies (Lee et al., 2022; Abbas et al., 2023)\nhave demonstrated the prevalence of duplication in crawled data and the benefits of training\non deduplicated data.\nSimilar to Brown et al. (2020), we employ a logistic regression classifier with hashed token\nfrequencies to ensure high-quality text. This classifier, trained using curated datasets like\nWikipedia or OpenWebText (Gokaslan and Cohen, 2019) as positive examples and documents\nsampled from Common Crawl as negative ones, is fast and effective at detecting human-\nwritten text. After these steps, we are left with 1.1 billion documents and their HTML\nsources from the associated Common Crawl WARC files.\n3.2\nSimplifying HTML Files\nThe original HTML content of a document contains a wealth of valuable information that\nproves highly beneficial in the process of filtering out undesirable text and images. Therefore,\nwe prioritize pre-processing the raw HTML into simplified HTML, making the subsequent\nextraction of textual and visual elements more efficient.\nTo this aim, we devise multiple pre-processing strategies for an HTML DOM tree. By\nmanually inspecting instances of all HTML nodes, we differentiate nodes likely to contain\nrelevant texts or images from those that should be discarded, and we formulate specific rules\nfor each type of node. After these pre-processing steps, the resulting simplified HTML files\nare more than ten times smaller and have been stripped of a large proportion of generic text\n(spam, ads, boilerplate template, etc.) and generic images, such as logos, while retaining the\nrelevant content.\n3.3\nExtracting Multimodal Web Documents\nIn this step, we transform the simplified HTML files previously obtained into a structured\nweb multimodal web document format. This format consists of interleaved texts and images.\nWe meticulously preserve the original structure of the web pages from the simplified HTML\nfiles by extracting the texts and image links while maintaining their rendering defined by the\nDOM tree. Given that each HTML tag denotes a distinct separation between the preceding\nand subsequent nodes, we leverage that information to retain line breaks and line feeds on\nthe original page, preserving the formatting and visual rendering of the content.\nWe obtain 3.6 billion image links and successfully download 55% of them (approximately 2\nbillion images).\n3.4\nFiltering Multimodal Web Documents\nThe filtering process comprises two distinct steps operating at different granularity levels.\nIn the first step, filtering occurs at the node level for images and the paragraph level for\ntext. This step guarantees that only high-quality and relevant images and paragraphs are\nretained. Each paragraph or image is evaluated based on specific criteria and may undergo\nmodifications or be eliminated if necessary. The second step, conducted at the document\nlevel, involves deciding whether to retain or discard the output documents obtained from the\n3https://commoncrawl.org/\n4\nfirst step. Most text filters used in both steps are primarily derived from Lauren\u00e7on et al.\n(2022).\nNode-level image filtering\nWe discard images that are too small, excessively large or\nhave disproportionate dimensions. We observe that these images are often indicative of\nlow-quality or irrelevant content. To eliminate some logos and generic images, we remove\nimages whose URLs contain one of the banned sub-strings, like logo.\nParagraph-level text filtering\nWe apply multiple filters to text paragraphs to remove\nundesirable content. Specifically, paragraphs that contain an insufficient number of words are\ndiscarded. Additionally, we filter out paragraphs with high repetition ratios, excessive ratios\nof special characters, low ratios of stop words, low punctuation ratios, high proportions of\nflagged words associated with adult or inappropriate content, or excessively high perplexity\nscores (as measured by an n-gram language model trained on Wikipedia (Heafield, 2011)).\nTo identify boilerplate sentences or invitations to share articles on social networks, we create\na list of frequently used words associated with these paragraphs and remove paragraphs\ncontaining an excessive proportion of words from this list. To further identify machine-\ngenerated content, we extract words from web-crawled documents to form a list of common\nwords and discard documents with a low ratio of common words.\nDocument-level filtering\nAt the document level, we remove all documents with no or\nexcessively high number of images. For text filters, the same filters used at the paragraph\nlevel are applied, with sometimes stricter cutoff values.\nAfter these filtering steps, we are left with 365 million web documents and 1.4 billion images.\nAt this step, images can be duplicated across documents.\n3.5\nResponsible Filtering and Deduplication\nWe take measures to minimize the amount of inappropriate content in the dataset. In\nparticular, based on manual inspections and tool availability, we implement filters to respect\ndata consent and remove images with pornographic content. Additionally, we also heavily\ndeduplicate content.\nExclusion of opted-out images\nTo respect the preferences of content creators, we remove\nall images for which creators explicitly opted out of AI model training. We used the Spawning\nAPI4 to verify that the images in the dataset respect the original copyright owners\u2019 choices.\nImage deduplication based on URL\nSome images could be present across different\ndocuments. We observe that it is particularly true for browser-specific icons or common\nadvertisements encountered during the crawling process. To address this issue, we remove\nall images that appear more than ten times across the entire dataset. We intentionally do\nnot perform strict deduplication, as we notice that when an image is duplicated only a few\ntimes across different documents, the surrounding text and contextual information tend to\nbe different. We also deduplicate images within the same document.\nNSFW image filtering\nTo reduce explicit adult content, we use an open-source NSFW\nclassifier to remove entire documents containing pornographically classified images. We also\nfilter out images with URLs containing banned sub-strings.\nDocument deduplication based on URL and set of images\nWe complete the initial\ndeduplication step by forming clusters of documents with the same URLs and retaining the\nmost recent document within each cluster. We repeat this operation by forming clusters of\ndocuments containing identical sets of images.\nParagraph deduplication across documents of the same domain names\nTo remove\ngeneric spam phrases commonly found at the end of documents, we perform paragraph-level\n4https://api.spawning.ai/spawning-api\n5\nexact deduplication within documents sharing the same domain name, resulting in the\nelimination of approximately 15% of the text.\nFollowing these filtering and deduplication steps, the final dataset contains 141 million\ndocuments and 353 million images, of which 298 million are unique. We observe that using\nstricter values for the filtering steps yields fewer multimodal documents, although not of\nhigher quality. As such, we invite users who are interested in manipulating a smaller subset\nof OBELICS to start with a random subset.\n4\nAnalysis of OBELICS\nFigure 1 provides an example showcasing an original webpage alongside the resulting\nmultimodal web document. Extracting and filtering the multimodal document is non-trivial\nas it requires carefully removing undesirable information on the left, top, and bottom\nof the page, such as menus and navigation bars. We provide other examples at https:\n//huggingface.co/spaces/HuggingFaceM4/obelics_visualization and in Figures 7, 8\nand 9.\nGiven the scale of OBELICS, it would be prohibitive to describe its content exhaustively.\nInstead, we provide high-level statistics and analyses that shed light on the dataset\u2019s\nproperties.\n4.1\nGeneral Statistics\nDataset\nImages\n%\nunique\nimages\nDocs\nTokens\nOpen\nKOSMOS-1\n-\n-\n71M\n-\n\u2717\nM3W\n185M\n-\n43M\n-\n\u2717\nmmc4-ff\n385M\n60.6%\n79M\n34B\n\u2713\nmmc4\n585M\n-\n103M\n43B\n\u2713\nOBELICS\n353M\n84.3%\n141M\n115B\n\u2713\nTable 1: General statistics of OBELICS and the current largest\nalternatives.\n0\n20 40 60 80 100\n0\n20\n40\n60\n80\n100\nmax # of images in doc\ny = % of images\nbelonging to a doc\nwith at most x images\nmmc4\nOBELICS\nFigure 3: Distribution of images.\nTable 1 compares OBELICS against the largest existing alternatives. mmc4-ff is the mmc4\ndataset with fewer faces. Our dataset has the highest number of unique documents and total\ntokens while containing a huge number of images.\nIt is worth mentioning that we have fewer images than mmc4 (Zhu et al., 2023).\nThis\ndiscrepancy can be attributed to two reasons. First, our analysis reveals that mmc4 contains\nmany duplicated images, with only 60.6% being unique compared to 84.3% for OBELICS. We\nfound that images duplicated multiple times often indicate spam or unrelated generic content.\nSecond, mmc4 does not limit the number of images within a document. As a result, the\ndistribution of images across documents is highly uneven, with a substantial portion of them\nconcentrated in documents with excessive image counts (see Figure 3). The images in these\ndocuments are often unrelated to each other and exhibit spam or advertisement content.\nMoreover, these documents often have little text, making them unsuitable for learning the\nalignment between text and images (see an example in Figure 10).\nFigure 4 shows the joint distribution of a number of tokens and a number of images in\nOBELICS. Although we limit the number of images in a document to 30, we cut the plot at 6\nimages for clarity. The documents of OBELICS contain a median number of images of 1 and\na median number of tokens of 677.\nPerplexity analysis\nTo assess the quality of our text in comparison to reference datasets\nused for training large language models, we leverage an n-gram language model trained on\nWikipedia (Heafield, 2011; Lauren\u00e7on et al., 2022). This allows us to compute perplexity\n6\nFigure 4: Heatmap displaying the\njoint distribution of the number of\ntokens and the number of images\nin OBELICS documents, accompa-\nnied by their respective marginal\ndistributions.\n0\n200\n400\n600\n800\n1,000 1,200 1,400\n0\n1\n2\n3\n4 \u00b710\u22123\nPerplexity score\nProbability\nOSCAR\nc4\nThe Pile\nmmc4\nOBELICS\nFigure 5: Kernel density estimations representing the distri-\nbution of perplexity scores for OBELICS compared to reference\ndatasets. The lower the perplexity for a document, the more\nit resembles a Wikipedia article.\nscores for 100,000 documents from each dataset. Lower perplexity scores indicate a higher\nresemblance to Wikipedia documents. Figure 5 displays the distributions of these scores. Our\nresults demonstrate that the texts in OBELICS have a significantly lower average perplexity\ncompared to the texts in c4 (Raffel et al., 2019), mmc4 (Zhu et al., 2023), and OSCAR\n(Ortiz Su\u00e1rez et al., 2020). Furthermore, our distribution aligns closely with the one from\nThe Pile (Gao et al., 2020), which was thoughtfully curated from diverse, high-quality sources.\n4.2\nTopic Modeling\nSimilar to Zhu et al. (2023), we employ a Latent Dirichlet Allocation (LDA) (Blei et al., 2003)\nto understand the diversity of the dataset. The LDA gives us insights into the distribution\nof topics in the dataset, along with estimated proportions and frequently associated words.\nTable 5 and 6 present the results of the LDA with respectively 20 and 200 topics, offering\nboth a high-level and a more granular analysis of the dataset\u2019s content. We observe that\nthe dataset covers topics ranging from Politics to Health by way of Music. Additionally, we\ncompute the most frequent domains and show that news sites are systematically the most\nrepresented (Table 4).\n4.3\nQualitative Assessment of Dataset Samples\nWe manually inspect 250 documents from OBELICS to verify the dataset\u2019s quality and asses\nthe risks contained in the dataset. We focus on the images\u2019 content in relation to the text\nsince it\u2019s the core addition compared to a language modeling dataset.\n80% of documents have photo images, while 29% have graphic images (drawings, cartoons,\netc.). 90% of the documents have all images clearly related to the text content. 30% of\ndocuments have images containing at least one written word, and 5% of documents have\nimages that are structured text (slides, tables, scanned documents, etc.), which can help\nmodels learn OCR capabilities. 7% of documents have content (images or text) that hasn\u2019t\nbeen captured by cleaning filters (non-English text, spam or advertisement, etc.). 46% of\ndocuments contain images with faces (portraits or group photos). No obvious Personally\nIdentifiable Information (PII) texts were found, except for public personalities and people\nmentioned in news articles. No NSFW images were found. Only 3% of documents contain\nimages with watermarks, and 2% have images with logos.\n5\nValidating the Viability of OBELICS\nTo confirm the viability of our dataset, we first show that vision and language models trained\non our multimodal web documents outperform the same models trained on image-text pairs\non various multimodal benchmarks. Following that, we demonstrate the effectiveness of\n7\nOBELICS as an alternative to closed datasets by training models of different sizes on par with\nclosed-source models.\nModel details\nWe follow the Flamingo (Alayrac et al., 2022) architecture closely: we\ncombine two frozen unimodal backbones - LLaMA (Touvron et al., 2023) for the language\nmodel, and OpenClip 5 for the vision encoder - add learnable cross-attention Transformer\nblocks to connect the language and vision blocks. For multimodal web documents, we feed the\nmodel sequences corresponding to the succession of text paragraphs and images. For image-\ntext pairs, we form the training sequences by packing images with their captions. The images\nare encoded with the vision encoder and vision hidden states are pooled with Transformer\nPerceiver blocks and then fused into the text sequence through the cross-attention blocks.\nThe training objective is the standard next token prediction. For more details, we refer to\nthe original paper.\nFollowing Alayrac et al. (2022), we evaluate our models on a series of multimodal benchmarks\nspanning visual question answering (VQAv2 (Antol et al., 2015), OKVQA (Marino et al.,\n2019), TextVQA (Singh et al., 2019), VizWiz (Gurari et al., 2018)), visual dialogs (VisDial\n(Das et al., 2017)), hateful speech detection (HatefulMeme (Kiela et al., 2020)), image\ncaptioning (COCO (Lin et al., 2014), Flickr30k (Young et al., 2014)), and OCR (IIIT5k\n(Mishra et al., 2012)).\nAdditional details about the architecture, the training, the compute and the evaluation are\npresent in Appendix A.4.\n108\n109\n1010\n0.3\n0.4\n0.5\n0.6\n# of training tokens\nAggregated perf.\n106\n107\n108\n109\n0.3\n0.4\n0.5\n0.6\n# of training images\nLAION only\nOBELICS only\nOBELICS + LAION\nFigure 6: Aggregated 4-shot performance through the training using LAION only, OBELICS only and\na mixture of both. The training sequences from multimodal documents and the packed sequences\nobtained from image-text pairs have different numbers of images but the same number of tokens.\nThus, we plot the performance over two log x-axes. The initial uptick of the model trained on\nimage-text pairs is attributed to the fact the performance on VQA tasks starts by increasing and\nthen slowly degrades.\nTraining on different mixture of data\nFigure 6 shows the result of the first experiment,\nwhich consists in training 9B-parameter models on different mixture of data. Training\non multimodal web documents allows reaching the same performance using an order of\nmagnitude fewer images than training on image-text pairs, even though the images from\nthe two datasets come from Common Crawl. This underlines the benefit of having longer\ntext contexts for training multimodal models. Moreover, the model trained on multimodal\nweb documents performs better on average. This is particularly striking on visual question-\nanswering benchmarks on which the model trained on image-text pairs slowly degrades\nthrough the training. We note, however, that the model trained on image-text pairs has a\nslight advantage performance-wise in captioning, classification, and OCR tasks (see more\ndetails in Appendix A.4.5). We hypothesize that this is due to the nature of image-text\npairs: captions can be seen as fuzzy class labels. Last, similarly to Alayrac et al. (2022), we\nobserve that combining the two types of datasets leads to increased performance for a given\nnumber of images, tokens, or training compute.\nModels trained on OBELICS achieve competitive performance at different scales\nFollowing these insights, we show that OBELICS is a viable open alternative to other datasets.\n5https://laion.ai/blog/large-openclip/\n8\nShot\nCOCO\nFlickr30k\nVQAv2\nOKVQA\nTextVQA\nVizWiz\nVisDial\nHatefulMemes\nFlamingo-9B\n0\n79.4\n61.5\n51.8\n44.7\n31.8\n22.8\n48.0\n57.0\nOpenFlamingo-9B\n79.5\n59.5\n52.7\n37.8\n24.2\n27.5\n-\n51.6\nIDEFICS-9B\n46.0\n27.3\n50.9\n38.4\n25.9\n35.5\n48.7\n51.8\nFlamingo-9B\n4\n93.1\n72.6\n56.3\n49.3\n33.6\n34.9\n50.4\n62.7\nOpenFlamingo-9B\n89.0\n65.8\n54.8\n40.1\n28.2\n34.1\n-\n54.0\nIDEFICS-9B\n93.0\n59.7\n55.4\n45.4\n27.6\n36.9\n47.9\n50.7\nFlamingo-9B\n8\n99.0\n73.4\n58.0\n50.0\n33.6\n39.4\n51.2\n63.9\nOpenFlamingo-9B\n96.3\n62.9\n54.8\n41.1\n29.1\n38.5\n-\n54.7\nIDEFICS-9B\n97.0\n61.9\n56.4\n47.7\n27.5\n40.4\n47.6\n51.1\nFlamingo-9B\n16\n102.2\n72.7\n59.4\n50.8\n33.5\n43.0\n51.3\n64.5\nOpenFlamingo-9B\n98.8\n62.8\n54.3\n42.7\n27.3\n42.5\n-\n53.9\nIDEFICS-9B\n99.7\n64.5\n57.0\n48.4\n27.9\n42.6\n-\n50.1\nFlamingo-9B\n32\n106.3\n72.8\n60.4\n51.0\n32.6\n44.0\n50.4\n63.5\nOpenFlamingo-9B\n99.5\n61.3\n53.3\n42.4\n23.8\n44.0\n-\n53.8\nIDEFICS-9B\n98.0\n64.3\n57.9\n49.6\n28.3\n43.7\n-\n49.8\nFlamingo\n0\n84.3\n67.2\n56.3\n50.6\n35.0\n31.6\n52.0\n46.4\nIDEFICS\n91.8\n53.7\n60.0\n45.2\n30.9\n36.0\n48.9\n60.6\nFlamingo\n4\n103.2\n75.1\n63.1\n57.4\n36.5\n39.6\n55.6\n68.6\nIDEFICS\n110.3\n73.7\n63.6\n52.4\n34.4\n40.4\n48.4\n57.8\nFlamingo\n8\n108.8\n78.2\n65.6\n57.5\n37.3\n44.8\n56.4\n70.0\nIDEFICS\n114.3\n76.6\n64.8\n55.1\n35.7\n46.1\n47.9\n58.2\nFlamingo\n16\n110.5\n78.9\n66.8\n57.8\n37.6\n48.4\n56.8\n70.0\nIDEFICS\n116.6\n80.1\n65.4\n56.8\n36.3\n48.3\n-\n57.8\nFlamingo\n32\n113.8\n75.4\n67.6\n57.8\n37.9\n49.8\n55.6\n70.0\nIDEFICS\n116.6\n81.1\n65.9\n57.8\n36.7\n50.0\n-\n52.5\nTable 2: Performance of IDEFICS against OpenFlamingo and Flamingo. The evaluations\nwere done with random in-context examples, and in an open-ended setting for VQA tasks.\n(Task, Metric, Query split): (COCO, CIDEr, test), (Flickr30k, CIDEr, test (Karpathy)),\n(VQAv2, VQA acc., testdev), (OKVQA, VQA acc., val), (TextVQA, VQA acc., val), (VizWiz,\nVQA acc., testdev), (VisDial, NDCG, val), (HatefulMemes, ROC-AUC, test seen).\n9\nWe train IDEFICS, an 80 billion parameters Flamingo-like model on a mixture of image-\ntext pairs from LAION (Schuhmann et al., 2022), openly accessible captioning datasets\n(Singh et al., 2022), OBELICS and multimodal web documents obtained from Wikipedia\nusing a similar extraction strategy. We also train a smaller version of 9 billion parameters,\nIDEFICS-9B. We compare these models against OpenFlamingo v2 (Awadalla et al., 2023) and\nFlamingo of the same sizes and trained on a similar mixture of multimodal web documents\nand image-text pairs. We report the results in Table 2.\nIDEFICS is often on par with Flamingo on various multimodal benchmarks. Out of the\n8 evaluation tasks, with 32 in-context examples, it either performs better or obtain the\nsame result as Flamingo on 4 of them. At the 9 billion parameter scale, we are still behind\nFlamingo-9B. However, it is important to highlight that we outperform OpenFlamingo-9B,\nwhich was trained on mmc4, in terms of aggregated performance. We achieved a score of\n56.5, compared to their score of 55.8, by selecting the best performance across all numbers\nof in-context examples for each task. This highlights the advantages of OBELICS as an open\nalternative to a multimodal web document dataset.\n6\nConclusion\nWith the goal of supporting open-source large multimodal models, we introduce OBELICS, an\nopen web-scale collection of filtered interleaved multimodal web documents based on Common\nCrawl snapshots. We document a collection and filtering process that balances the scale\nand removal of undesirable texts and images while addressing some of the well-documented\nethical concerns of large-scale multimodal datasets, notably data consent and pornographic\ncontent. To demonstrate the usefulness of models trained on multimodal documents, we\ntrain IDEFICS on OBELICS and show that it is a viable alternative to closed datasets. Open\ndatasets of multimodal documents with scale, quality, and diversity of sources can help\nsupport the ability to train competitive open models.\n10\nAcknowledgments and Disclosure of Funding\nThe authors were granted access to the HPC resources of the Institut du d\u00e9veloppement\net des ressources en informatique scientifique (IDRIS) du Centre national de la recherche\nscientifique (CNRS) under the allocation 2022-A0121013450 made by Grand \u00e9quipement\nnational de calcul intensif (GENCI). The initial development of the dataset was done on\nJean-Zay cluster of IDRIS, and we thank the IDRIS team for their responsive support\nthroughout the project, in particular R\u00e9mi Lacroix. We thank Guillaume Salou for setting\nup the virtual machines used to download the images of our dataset, and Sebastian Nagel\nfor his valuable assistance in providing insights on Common Crawl. We thank Yacine Jernite\nand Daniel van Strien for conducting a bias analysis of the models trained on OBELICS.\nReferences\nAbbas, A., K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos (2023).\nSemdedup:\nData-efficient learning at web-scale through semantic deduplication.\nAghajanyan, A., B. Huang, C. Ross, V. Karpukhin, H. Xu, N. Goyal, D. Okhonko, M. Joshi,\nG. Ghosh, M. Lewis, and L. Zettlemoyer (2022). Cm3: A causal masked multimodal model\nof the internet. ArXiv abs/2201.07520.\nAlayrac, J.-B., J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch,\nK. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei,\nM. Monteiro, J. L. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a.\nBi\u0144kowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan (2022). Flamingo:\na visual language model for few-shot learning. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing\nSystems, Volume 35, pp. 23716\u201323736. Curran Associates, Inc.\nAntol, S., A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh (2015).\nVQA: Visual Question Answering. In International Conference on Computer Vision\n(ICCV).\nAwadalla, A., I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton,\nS. Gadre, S. Sagawa, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman,\nand L. Schmidt (2023). Openflamingo: An open-source framework for training large\nautoregressive vision-language models.\nBai, Y., A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort,\nD. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk,\nN. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt,\nN. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann,\nand J. Kaplan (2022). Training a helpful and harmless assistant with reinforcement\nlearning from human feedback.\nBeaumont, R. (2021). img2dataset: Easily turn large sets of image urls to an image dataset.\nhttps://github.com/rom1504/img2dataset.\nBender, E. M., T. Gebru, A. McMillan-Major, and S. Shmitchell (2021). On the dangers\nof stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Transparency.\nBiderman, S. and W. J. Scheirer (2020, 12 Dec). Pitfalls in machine learning research:\nReexamining the development cycle. In J. Zosa Forde, F. Ruiz, M. F. Pradier, and\nA. Schein (Eds.), Proceedings on \"I Can\u2019t Believe It\u2019s Not Better!\" at NeurIPS Workshops,\nVolume 137 of Proceedings of Machine Learning Research, pp. 106\u2013117. PMLR.\nBiderman, S., H. Schoelkopf, Q. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan,\nS. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal (2023).\nPythia: A suite for analyzing large language models across training and scaling.\n11\nBirhane, A., V. U. Prabhu, and E. Kahembwe (2021). Multimodal datasets: misogyny,\npornography, and malignant stereotypes. ArXiv abs/2110.01963.\nBlei, D. M., A. Y. Ng, and M. I. Jordan (2003, mar). Latent dirichlet allocation. J. Mach.\nLearn. Res. 3(null), 993\u20131022.\nBroder, A. (1997). On the resemblance and containment of documents. In Proceedings.\nCompression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pp. 21\u201329.\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei (2020). Language models are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information Processing\nSystems, Volume 33, pp. 1877\u20131901. Curran Associates, Inc.\nByeon, M., B. Park, H. Kim, S. Lee, W. Baek, and S. Kim (2022). Coyo-700m: Image-text\npair dataset. https://github.com/kakaobrain/coyo-dataset.\nCaswell, I., T. Breiner, D. van Esch, and A. Bapna (2020).\nLanguage id in the\nwild:\nUnexpected challenges on the path to a thousand-language web text corpus.\nArXiv abs/2010.14571.\nChangpinyo, S., P. Sharma, N. Ding, and R. Soricut (2021). Conceptual 12M: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In CVPR.\nChowdhery, A., S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat,\nS. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,\nD. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,\nA. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,\nZ. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern,\nD. Eck, J. Dean, S. Petrov, and N. Fiedel (2022). Palm: Scaling language modeling with\npathways.\nDas, A., S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. F. Moura, D. Parikh, and D. Batra\n(2017, July). Visual dialog. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nDehghani, M., J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner,\nM. Caron, R. Geirhos, I. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab,\nX. Wang, C. Riquelme, M. Minderer, J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste,\nG. F. Elsayed, A. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. P. Collier,\nA. Gritsenko, V. Birodkar, C. Vasconcelos, Y. Tay, T. Mensink, A. Kolesnikov, F. Paveti\u0107,\nD. Tran, T. Kipf, M. Lu\u010di\u0107, X. Zhai, D. Keysers, J. Harmsen, and N. Houlsby (2023).\nScaling vision transformers to 22 billion parameters.\nDeng, X., P. Shiralkar, C. Lockard, B. Huang, and H. Sun (2022). Dom-lm: Learning\ngeneralizable representations for html documents. ArXiv abs/2201.10608.\nDesai, K., G. Kaul, Z. Aysola, and J. Johnson (2021). Redcaps: Web-curated image-text data\ncreated by the people, for the people. In J. Vanschoren and S. Yeung (Eds.), Proceedings of\nthe Neural Information Processing Systems Track on Datasets and Benchmarks, Volume 1.\nCurran.\nDodge, J., A. Marasovi\u0107, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner (2021).\nDocumenting large webtext corpora: A case study on the colossal clean crawled corpus.\nIn Conference on Empirical Methods in Natural Language Processing.\n12\nGadre, S. Y., G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Worts-\nman, D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. Pratt, V. Ramanujan,\nY. Bitton, K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. W. Koh,\nO. Saukh, A. Ratner, S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. Dimakis,\nJ. Jitsev, Y. Carmon, V. Shankar, and L. Schmidt (2023). Datacomp: In search of the\nnext generation of multimodal datasets. arXiv preprint arXiv:2304.14108.\nGao, L., S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\nN. Nabeshima, S. Presser, and C. Leahy (2020). The Pile: An 800gb dataset of diverse\ntext for language modeling. arXiv preprint arXiv:2101.00027.\nGokaslan, A. and V. Cohen (2019). Openwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nGu, J., X. Meng, G. Lu, L. Hou, N. Minzhe, X. Liang, L. Yao, R. Huang, W. Zhang,\nX. Jiang, C. XU, and H. Xu (2022). Wukong: A 100 million large-scale chinese cross-modal\npre-training benchmark. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\nand A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35, pp.\n26418\u201326431. Curran Associates, Inc.\nGurari, D., Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham\n(2018). Vizwiz grand challenge: Answering visual questions from blind people.\nHeafield, K. (2011, July). KenLM: Faster and smaller language model queries. In Proceedings\nof the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, pp. 187\u2013\n197. Association for Computational Linguistics.\nHoffmann, J., S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,\nD. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican,\nG. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae,\nO. Vinyals, and L. Sifre (2022). Training compute-optimal large language models.\nHuang, S., L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed,\nB. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and\nF. Wei (2023). Language is not all you need: Aligning perception with language models.\nJaegle, A., F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira (2021). Perceiver:\nGeneral perception with iterative attention.\nJia, C., Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y.-H. Sung, Z. Li, and\nT. Duerig (2021). Scaling up visual and vision-language representation learning with noisy\ntext supervision. In International Conference on Machine Learning.\nJiang, A. Q., S. Welleck, J. P. Zhou, T. Lacroix, J. Liu, W. Li, M. Jamnik, G. Lample, and\nY. Wu (2023). Draft, sketch, and prove: Guiding formal theorem provers with informal\nproofs. In The Eleventh International Conference on Learning Representations.\nJoulin, A., E. Grave, P. Bojanowski, and T. Mikolov (2017, April). Bag of tricks for efficient\ntext classification. In Proceedings of the 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume 2, Short Papers, Valencia, Spain, pp.\n427\u2013431. Association for Computational Linguistics.\nKaplan, J., S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray,\nA. Radford, J. Wu, and D. Amodei (2020). Scaling laws for neural language models.\nK\u00e4rkk\u00e4inen, K. and J. Joo (2021). Fairface: Face attribute dataset for balanced race,\ngender, and age for bias measurement and mitigation. 2021 IEEE Winter Conference on\nApplications of Computer Vision (WACV), 1547\u20131557.\nKiela, D., H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine\n(2020). The hateful memes challenge: Detecting hate speech in multimodal memes. In\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural\nInformation Processing Systems, Volume 33, pp. 2611\u20132624. Curran Associates, Inc.\n13\nKoh, J. Y., R. Salakhutdinov, and D. Fried (2023). Grounding language models to images\nfor multimodal generation.\nLaborde, G. Deep nn for nsfw detection.\nLauren\u00e7on, H., L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao,\nL. Von Werra, C. Mou, E. Gonz\u00e1lez Ponferrada, H. Nguyen, J. Frohberg, M. \u0160a\u0161ko,\nQ. Lhoest, A. McMillan-Major, G. Dupont, S. Biderman, A. Rogers, L. Ben allal, F. De Toni,\nG. Pistilli, O. Nguyen, S. Nikpoor, M. Masoud, P. Colombo, J. de la Rosa, P. Villegas,\nT. Thrush, S. Longpre, S. Nagel, L. Weber, M. Mu\u00f1oz, J. Zhu, D. Van Strien, Z. Alyafeai,\nK. Almubarak, M. C. Vu, I. Gonzalez-Dios, A. Soroa, K. Lo, M. Dey, P. Ortiz Suarez,\nA. Gokaslan, S. Bose, D. Adelani, L. Phan, H. Tran, I. Yu, S. Pai, J. Chim, V. Lepercq,\nS. Ilic, M. Mitchell, S. A. Luccioni, and Y. Jernite (2022). The bigscience roots corpus: A\n1.6tb composite multilingual dataset. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,\nK. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35,\npp. 31809\u201331826. Curran Associates, Inc.\nLee, K., D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini\n(2022). Deduplicating training data makes language models better. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics. Association for\nComputational Linguistics.\nLi, J., D. Li, S. Savarese, and S. Hoi (2023). Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models.\nLi, J., D. Li, C. Xiong, and S. Hoi (2022). Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In ICML.\nLi, R., L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li,\nJ. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj,\nJ. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee,\nL. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. Stillerman, S. S.\nPatel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu,\nS. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor,\nJ. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson,\nC. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau,\nY. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries\n(2023). Starcoder: may the source be with you!\nLin, T.-Y., M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan,\nC. L. Zitnick, and P. Doll\u00e1r (2014). Microsoft coco: Common objects in context. cite\narxiv:1405.0312Comment: 1) updated annotation pipeline description and figures; 2) added\nnew section describing datasets splits; 3) updated author list.\nLiu, S., L. Fan, E. Johns, Z. Yu, C. Xiao, and A. Anandkumar (2023).\nPrismer: A\nvision-language model with an ensemble of experts. arXiv preprint arXiv:2303.02506.\nLiu, Y., G. Zhu, B. Zhu, Q. Song, G. Ge, H. Chen, G. Qiao, R. Peng, L. Wu, and J. Wang\n(2022). Taisu: A 166m large-scale high-quality dataset for chinese vision-language pre-\ntraining. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.),\nAdvances in Neural Information Processing Systems, Volume 35, pp. 16705\u201316717. Curran\nAssociates, Inc.\nLoshchilov, I. and F. Hutter (2017).\nFixing weight decay regularization in adam.\nCoRR abs/1711.05101.\nLuccioni, A. S., C. Akiki, M. Mitchell, and Y. Jernite (2023). Stable bias: Analyzing societal\nrepresentations in diffusion models.\nMarino, K., M. Rastegari, A. Farhadi, and R. Mottaghi (2019). Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In Conference on Computer Vision\nand Pattern Recognition (CVPR).\n14\nMishra, A., K. Alahari, and C. V. Jawahar (2012). Scene text recognition using higher order\nlanguage priors. In BMVC.\nNichol, A., P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever,\nand M. Chen (2022). Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models.\nOrtiz Su\u00e1rez, P. J., L. Romary, and B. Sagot (2020, July). A monolingual approach to\ncontextualized word embeddings for mid-resource languages. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, Online, pp. 1703\u20131714.\nAssociation for Computational Linguistics.\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,\nP. Welinder, P. F. Christiano, J. Leike, and R. Lowe (2022). Training language models\nto follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing\nSystems, Volume 35, pp. 27730\u201327744. Curran Associates, Inc.\nPiktus, A., C. Akiki, P. Villegas, H. Lauren\u00e7on, G. Dupont, A. S. Luccioni, Y. Jernite, and\nA. Rogers (2023). The roots search tool: Data transparency for llms.\nRadenovic, F., A. Dubey, A. Kadian, T. Mihaylov, S. Vandenhende, Y. Patel, Y. Wen,\nV. Ramanathan, and D. Mahajan (2023). Filtering, distillation, and hard negatives for\nvision-language pre-training.\nRadford, A., J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, G. Krueger, and I. Sutskever (2021). Learning transferable visual\nmodels from natural language supervision.\nIn International Conference on Machine\nLearning.\nRae, J. W., S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson,\nR. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell,\nG. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl,\nS. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese,\nA. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan,\nM. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya,\nD. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz,\nT. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d\u2019Autume,\nY. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones,\nJ. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart,\nS. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis,\nK. Kavukcuoglu, and G. Irving (2022). Scaling language models: Methods, analysis &\ninsights from training gopher.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu (2019). Exploring the limits of transfer learning with a unified text-to-text transformer.\narXiv e-prints.\nRamesh, A., P. Dhariwal, A. Nichol, C. Chu, and M. Chen (2022).\nHierarchical text-\nconditional image generation with clip latents.\nRombach, R., A. Blattmann, D. Lorenz, P. Esser, and B. Ommer (2021). High-resolution\nimage synthesis with latent diffusion models.\nSaharia, C., W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K.\nAyan, S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi (2022).\nPhotorealistic text-to-image diffusion models with deep language understanding.\nSchuhmann, C., R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\nA. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt,\nR. Kaczmarczyk, and J. Jitsev (2022). Laion-5b: An open large-scale dataset for training\nnext generation image-text models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,\n15\nK. Cho, and A. Oh (Eds.), Advances in Neural Information Processing Systems, Volume 35,\npp. 25278\u201325294. Curran Associates, Inc.\nSchuhmann, C., R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes,\nJ. Jitsev, and A. Komatsuzaki (2021). Laion-400m: Open dataset of clip-filtered 400\nmillion image-text pairs.\nSharma, P., N. Ding, S. Goodman, and R. Soricut (2018). Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of\nACL.\nSingh, A., R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela (2022).\nFLAVA: A foundational language and vision alignment model. In CVPR.\nSingh, A., V. Natarjan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach (2019).\nTowards vqa models that can read. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 8317\u20138326.\nSorscher, B., R. Geirhos, S. Shekhar, S. Ganguli, and A. Morcos (2022). Beyond neural\nscaling laws: beating power law scaling via data pruning. In S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information\nProcessing Systems, Volume 35, pp. 19523\u201319536. Curran Associates, Inc.\nSrinivasan, K., K. Raman, J. Chen, M. Bendersky, and M. Najork (2021). Wit: Wikipedia-\nbased image text dataset for multimodal multilingual machine learning. In Proceedings\nof the 44th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR \u201921, New York, NY, USA, pp. 2443\u20132449. Association for\nComputing Machinery.\nTeam, M. N. (2023). Introducing mpt-7b: A new standard for open-source, commercially\nusable llms.\nTouvron, H., T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re,\nN. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample (2023).\nLlama: Open and efficient foundation language models.\nWang, P., A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang (2022,\n17\u201323 Jul). OFA: Unifying architectures, tasks, and modalities through a simple sequence-\nto-sequence learning framework. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\nG. Niu, and S. Sabato (Eds.), Proceedings of the 39th International Conference on Machine\nLearning, Volume 162 of Proceedings of Machine Learning Research, pp. 23318\u201323340.\nPMLR.\nWang, Q., Y. Fang, A. Ravula, F. Feng, X. Quan, and D. Liu (2022). Webformer: The\nweb-page transformer for structure information extraction.\nWang, W., H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed,\nS. Singhal, S. Som, and F. Wei (2022). Image as a foreign language: Beit pretraining for\nall vision and vision-language tasks.\nWebster, R., J. Rabin, L. Simon, and F. Jurie (2023). On the de-duplication of laion-2b.\nWorkshop, B., :, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u0107, D. Hesslow, R. Castagn\u00e9,\nA. S. Luccioni, F. Yvon, M. Gall\u00e9, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S.\nAmmanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Baw-\nden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O.\nSuarez, V. Sanh, H. Lauren\u00e7on, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan,\nA. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou,\nC. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada,\nE. Levkovizh, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli,\nH. Elsahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmumin, I. Johnson, I. Gonzalez-Dios,\nJ. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee,\nK. Almubarak, K. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan, L. B. allal, L. Tanguy,\n16\nM. Dey, M. R. Mu\u00f1oz, M. Masoud, M. Grandury, M. \u0160a\u0161ko, M. Huang, M. Coavoux,\nM. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner,\nN. Khamis, O. Nguyen, O. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo,\nP. Amuok, Q. Lhoest, R. Harliman, R. Bommasani, R. L. L\u00f3pez, R. Ribeiro, S. Osei,\nS. Pyysalo, S. Nagel, S. Bose, S. H. Muhammad, S. Sharma, S. Longpre, S. Nikpoor, S. Sil-\nberberg, S. Pai, S. Zink, T. T. Torrent, T. Schick, T. Thrush, V. Danchev, V. Nikoulina,\nV. Laippala, V. Lepercq, V. Prabhu, Z. Alyafeai, Z. Talat, A. Raja, B. Heinzerling, C. Si,\nD. E. Ta\u015far, E. Salesky, S. J. Mielke, W. Y. Lee, A. Sharma, A. Santilli, A. Chaffin,\nA. Stiegler, D. Datta, E. Szczechla, G. Chhablani, H. Wang, H. Pandey, H. Strobelt,\nJ. A. Fries, J. Rozen, L. Gao, L. Sutawika, M. S. Bari, M. S. Al-shaibani, M. Manica,\nN. Nayak, R. Teehan, S. Albanie, S. Shen, S. Ben-David, S. H. Bach, T. Kim, T. Bers,\nT. Fevry, T. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong, Z. Sun, S. Brody, Y. Uri,\nH. Tojarieh, A. Roberts, H. W. Chung, J. Tae, J. Phang, O. Press, C. Li, D. Narayanan,\nH. Bourfoune, J. Casper, J. Rasley, M. Ryabinin, M. Mishra, M. Zhang, M. Shoeybi,\nM. Peyrounette, N. Patry, N. Tazi, O. Sanseviero, P. von Platen, P. Cornette, P. F. Laval-\nl\u00e9e, R. Lacroix, S. Rajbhandari, S. Gandhi, S. Smith, S. Requena, S. Patil, T. Dettmers,\nA. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat, A. Subramonian, A. N\u00e9v\u00e9ol, C. Lovering,\nD. Garrette, D. Tunuguntla, E. Reiter, E. Taktasheva, E. Voloshina, E. Bogdanov, G. I.\nWinata, H. Schoelkopf, J.-C. Kalo, J. Novikova, J. Z. Forde, J. Clive, J. Kasai, K. Kawa-\nmura, L. Hazan, M. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov, O. Antverg,\nO. van der Wal, R. Zhang, R. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina,\nT. Scialom, T. Yun, T. Limisiewicz, V. Rieser, V. Protasov, V. Mikhailov, Y. Pruk-\nsachatkun, Y. Belinkov, Z. Bamberger, Z. Kasner, A. Rueda, A. Pestana, A. Feizpour,\nA. Khan, A. Faranak, A. Santos, A. Hevia, A. Unldreaj, A. Aghagol, A. Abdollahi,\nA. Tammour, A. HajiHosseini, B. Behroozi, B. Ajibade, B. Saxena, C. M. Ferrandis,\nD. Contractor, D. Lansky, D. David, D. Kiela, D. A. Nguyen, E. Tan, E. Baylor, E. Ozoani,\nF. Mirza, F. Ononiwu, H. Rezanejad, H. Jones, I. Bhattacharya, I. Solaiman, I. Sedenko,\nI. Nejadgholi, J. Passmore, J. Seltzer, J. B. Sanz, L. Dutra, M. Samagaio, M. Elbadri,\nM. Mieskes, M. Gerchick, M. Akinlolu, M. McKenna, M. Qiu, M. Ghauri, M. Burynok,\nN. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kromann, R. Hao,\nS. Alizadeh, S. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang,\nZ. Nguyen, A. R. Kashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada,\nA. Singh, B. Beilharz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L.\nPeri\u00f1\u00e1n, D. Molano, D. Yu, E. Manjavacas, F. Barth, F. Fuhrimann, G. Altay, G. Bayrak,\nG. Burns, H. U. Vrabec, I. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada,\nK. R. Sivaraman, L. Bulchandani, L. Liu, L. Shinzato, M. H. de Bykhovetz, M. Takeuchi,\nM. P\u00e0mies, M. A. Castillo, M. Nezhurina, M. S\u00e4nger, M. Samwald, M. Cullan, M. Wein-\nberg, M. D. Wolf, M. Mihaljcic, M. Liu, M. Freidank, M. Kang, N. Seelam, N. Dahlberg,\nN. M. Broad, N. Muellner, P. Fung, P. Haller, R. Chandrasekhar, R. Eisenberg, R. Martin,\nR. Canalli, R. Su, R. Su, S. Cahyawijaya, S. Garda, S. S. Deshmukh, S. Mishra, S. Ki-\nblawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter, S. Bharati, T. Laud, T. Gigant,\nT. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman, Y. Xu, Y. Xu, Y. Xu,\nZ. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf (2023). Bloom: A 176b-parameter\nopen-access multilingual language model.\nXie, S. M., H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. Liang, Q. V. Le, T. Ma, and A. W.\nYu (2023). Doremi: Optimizing data mixtures speeds up language model pretraining.\nYang, Z., Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, and L. Wang (2022). An empirical study\nof gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on\nArtificial Intelligence, Volume 36, pp. 3081\u20133089.\nYoung, P., A. Lai, M. Hodosh, and J. Hockenmaier (2014). From image descriptions to\nvisual denotations: New similarity metrics for semantic inference over event descriptions.\nTransactions of the Association for Computational Linguistics 2, 67\u201378.\nYu, J., Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu (2022). Coca:\nContrastive captioners are image-text foundation models.\nTransactions on Machine\nLearning Research.\n17\nYuan, S., S. Zhao, J. Leng, Z. Xue, H. Zhao, P. Liu, Z. Gong, W. X. Zhao, J. Li, and J. Tang\n(2022). Wudaomm: A large-scale multi-modal dataset for pre-training models.\nYuksekgonul, M., F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou (2023). When and why vision-\nlanguage models behave like bags-of-words, and what to do about it? In International\nConference on Learning Representations.\nZhang, B. and R. Sennrich (2019). Root Mean Square Layer Normalization. In Advances in\nNeural Information Processing Systems 32, Vancouver, Canada.\nZhang, J., Y. Zhao, M. Saleh, and P. J. Liu (2019). Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization.\nZhang, R., J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao (2023).\nLlama-adapter: Efficient fine-tuning of language models with zero-init attention.\nZhang, S., S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li,\nX. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar,\nT. Wang, and L. Zettlemoyer (2022). Opt: Open pre-trained transformer language models.\nZhou, Y., Y. Sheng, N. H. Vo, N. Edmonds, and S. Tata (2021). Simplified dom trees for\ntransferable attribute extraction from the web. ArXiv abs/2101.02415.\nZhu, W., J. Hessel, A. Awadalla, S. Y. Gadre, J. Dodge, A. Fang, Y. Yu, L. Schmidt, W. Y.\nWang, and Y. Choi (2023). Multimodal C4: An open, billion-scale corpus of images\ninterleaved with text. arXiv preprint arXiv:2304.06939.\n18\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect\nthe paper\u2019s contributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See Section 4.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes]\nWe think that the release of such a dataset strikes a constructive trade-off\nbetween the risks associated with datasets built on top of crawled web pages\n(for instance, the presence of images with faces, the potential of PII in texts,\noffensive, insulting or threatening, etc.) with the future works that a dataset of\nsuch scale, quality and thoughtful filtering can enable. We further discuss these\npoints in A.3.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms\nto them? [Yes] We read the ethics review guidelines and tried our best to match\nthe expectations. Our content is extracted from publicly available websites at\nthe time of the web crawl. Given the size of our dataset, it would be prohibitive\nto get the explicit consent of the authors of these websites. Instead, we respect\nthe choice of content creators by removing opted-out images. Such a strategy\ncannot be exhaustive and we remain available for content creators to opt-out of\nthe dataset.\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main\nexperimental results (either in the supplemental material or as a URL)? [Yes]\nWe will release the code used for the creation of the model and its training,\nalong with the model itself.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how\nthey were chosen)? [Yes] See Appendix A.4.\n(c) Did you report error bars (e.g., with respect to the random seed after running\nexperiments multiple times)? [N/A]\n(d) Did you include the total amount of compute and the type of resources used\n(e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix\nA.4.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new\nassets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] We mentioned\nthe libraries we used.\n(b) Did you mention the license of the assets? [Yes] We only used open-source\nlibraries.\n(c) Did you include any new assets either in the supplemental material or as a\nURL? [N/A]\n(d) Did you discuss whether and how consent was obtained from people whose data\nyou\u2019re using/curating? [Yes] See the ethics review guidelines part.\n(e) Did you discuss whether the data you are using/curating contains personally\nidentifiable information or offensive content? [Yes] The dataset we are releasing\nis built from publicly accessible websites. As such, there is no content in our\ndataset that hasn\u2019t been publicly visible on the web at some point. Similarly, the\ndataset might contain texts or images that can be considered offensive, insulting,\nor threatening, as such data is prevalent on the web. We took measures to\nremove pornographic content and low-quality texts as much as possible. We\ndid not take additional intentional measures to remove personal information.\nA manual inspection of 250 random samples reveals that there isn\u2019t obvious\n19\npersonally identifiable information (excluding celebrities and people mentioned\nin news articles), although it is likely that the dataset contains some.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots,\nif applicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional\nReview Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total\namount spent on participant compensation? [N/A]\n20\nA\nAppendix\nA.1\nCreation of the Multimodal Web Document Dataset\nA.1.1\nCollecting of a Large Number of HTML Files\nOur data collection process begins by considering the 25 most recent Common Crawl6 dumps\navailable at the time of dataset creation. It contains webpages spanning from February 2020\nto January/February 2023. We use a modified version of readability-lxml7 to extract\nthe main text from the pages, discarding any pages that contain text of excessively high\nperplexity. This process yields a total of 41.2 billion documents.\nSelection of English content\nTo identify non-English content, we apply the FastText\nclassifier (Joulin et al., 2017) to the extracted text, effectively filtering out 63.6% of the\ndocuments.\nEarly text deduplication\nOften, a set of URLs is crawled repeatedly across different\nCommon Crawl snapshots. However, the content of these websites may vary as web ad-\nministrators make changes over time. Hence, at this stage, we refrain from deduplicating\ndocuments based on their URLs. Instead, we perform MinHash (Broder, 1997) deduplication\nwith 16 hashes calculated over 5-grams. To further refine the data, we eliminate documents\ncontaining substantial proportions of repeated paragraphs and n-grams, employing the\nmethodology described in MassiveText (Rae et al., 2022). (Lee et al., 2022; Abbas et al.,\n2023) show that crawled data often contains a significant amount of duplication, and training\non deduplicated data can improve performance.\nQuality classification\nWe employ a logistic regression classifier with hashed token fre-\nquencies to only retain pages containing human-written text, similar to Brown et al. (2020).\nThe classifier is trained using documents from curated datasets, such as Wikipedia and\nOpenWebText (Gokaslan and Cohen, 2019), as positive examples, and documents sampled\nfrom Common Crawl as negative examples. For simplicity, we use a threshold of 0.5 for the\nprobability that a document comes from a curated corpus, which acts as an indicator that a\ndocument is human-written.\nFollowing these steps, we obtain 1.1 billion documents and their HTML sources from the\nassociated Common Crawl WARC files.\nA.1.2\nSimplifying HTML Files\nThe original HTML content of a document contains a wealth of valuable information that\nproves highly beneficial in the process of filtering out undesirable text and images. Therefore,\nwe prioritize pre-processing the raw HTML into simplified HTML, making the subsequent\nextraction of textual and visual elements more efficient. For this purpose, we use the library\nselectolax8 that facilitates efficient parsing of HTML files and creates corresponding DOM\ntrees.\nDOM Tree cleaning strategies\nTo simplify the DOM trees, we employ several cleaning\nstrategies. Firstly, we convert tags that indicate line breaks (such as <br>) into actual line\nbreaks. Multiple consecutive line breaks and spaces are condensed into a single instance.\nAdditionally, HTML comments are removed from the DOM trees. Furthermore, we implement\nrecursive processes to eliminate empty leaves and unnest nodes. When a parent node lacks\nattached text and has only one child, the child node replaces the parent node in the DOM\nhierarchy. We repeat these operations after removing some nodes, and describe this process\nin the following paragraphs.\n6https://commoncrawl.org/\n7https://github.com/buriy/python-readability\n8https://github.com/rushter/selectolax\n21\nTag unwrapping\nThis operation involves removing unnecessary styling applied to dis-\nplayed text by unwrapping a predefined set of tags given below. By applying this procedure,\ntags such as <i>example</i> are transformed into example, eliminating the associated\nstyling elements.\nThe following tags are unwrapped during the processing of HTML files: a, abbr, acronym,\nb, bdi, bdo, big, cite, code, data, dfn, em, font, i, ins, kbd, mark, q, s, samp, shadow,\nsmall, span, strike, strong, sub, sup, time, tt, u, var, wbr.\nNode removal\nFollowing the previous step, we conduct a manual inspection of practical\nexamples encompassing all existing HTML tags. Based on our findings, we establish a\ncurated list that outlines the tags we intend to retain. Any nodes within the HTML DOM\ntree with tags not included in this list are subsequently removed. We specifically retain tags\nthat define the document structure (e.g., p or h) and tags associated with media elements\n(e.g., img). However, we opt to remove tags that typically consist of logos, generic content,\nor spam (e.g., header), as well as tags that often contain noisy text related to website\nnavigation (e.g., li), or text that poses challenges in terms of linearization (e.g., table).\nWe retain the following tags during the processing of HTML files, as they define the\ndocument\u2019s structure: address, article, aside, blink, blockquote, body, br, caption,\ncenter, dd, dl, dt, div, figcaption, h, h1, h2, h3, h4, h5, h6, hgroup, html, legend, main,\nmarquee, ol, p, section, summary, title, ul. Additionally, we also preserve the following\ntags that define media elements: audio, embed, figure, iframe, img, object, picture,\nvideo. Furthermore, we keep the source tag as it may contain an interesting attribute.\nModification of specific nodes\nWe then specifically target some <div> nodes that\ncontain footer, header, navigation, nav, navbar, or menu as ID or date as attribute, as\nwell as CSS rules that possess footer or site-info as class. These nodes typically contain\nwebsite navigation content or article dates and are therefore removed. Additionally, we\nobserve that the presence of a CSS rule with the class more-link often indicates a distinct\nshift in topic within the webpage, resembling the start of a new document. To account\nfor this, we replace these nodes with the text END_OF_DOCUMENT_TOKEN_TO_BE_REPLACED,\nwhich we replace by an end-of-sentence (EOS) token during training.\nWith these processing steps, we reduce the size of the HTML files by more than 10 on average\nwhile preserving the interesting content.\nA.1.3\nExtracting Multimodal Web Documents\nIn this section, we begin with the simplified HTML files obtained from the previous section.\nOur objective is to transform these files into a structured web document format, which is a\nsequence of interleaved texts and images.\nPreservation of the original structure of the web pages\nDuring the extraction\nprocess, we meticulously preserve the original structure of the web pages from the simplified\nHTML files. We extract the texts and image links while maintaining their order of appearance\nin the DOM tree. Each HTML tag denotes a distinct separation between the preceding and\nsubsequent nodes and we retain any line breaks and line feeds that are present in the original\npage, preserving the formatting and visual rendering of the content.\nImage downloading\nTo download the images, we use the img2dataset (Beaumont, 2021)\nlibrary. We attempt to download a massive collection of 3.6 billion images, of which 55%\n(approximately 2 billion images) were successfully downloaded. For that, we employ 20\nvirtual machines. This distributed approach allow us to complete the operation within a few\ndays.\nA.1.4\nFiltering Multimodal Web Documents\nThe filtering process consists of two steps, targeting different levels of granularity. In the\nfirst step, filtering occurs at the node level for images and at the paragraph level (separated\nby line breaks) for text. We evaluate each paragraph or image and we potentially modify or\n22\nremove these based on specific criteria. The second step, conducted at the document level,\ninvolves deciding whether to retain or discard the output documents from the first step. The\nmajority of the filters for text we use for both steps were adapted from Lauren\u00e7on et al.\n(2022).\nNode-level image filtering\nWe discard images with formats other than jpg, png or\nwebp, with a side length below 150 pixels or exceeding 20,000 pixels, as well as those with an\naspect ratio greater than 2 or less than 1/2. These criteria help exclude images that are too\nsmall, excessively large, or have disproportionate dimensions, which are often indicative of\nlow-quality or irrelevant content. To eliminate some logos and generic images, as in (Zhu\net al., 2023), we remove images whose URL contains one of the sub-strings logo, button, icon,\nplugin or widget.\nParagraph-level text filtering\nRegarding text paragraphs, we apply a series of filters to\nremove undesirable or irrelevant content. We discard paragraphs with fewer than 4 words, as\nthey typically contain insufficient information to be considered meaningful. Additionally, we\nremove paragraphs with a high repetition ratio, indicating potential spam content, and those\nwith an excessive ratio of special characters, often associated with irrelevant or low-quality\ntext.\nFurthermore, we filter out paragraphs with a low ratio of stop words, as it is often indicative\nof machine-generated or nonsensical content. Similarly, we exclude paragraphs with a low\npunctuation ratio, as they typically indicate poor-quality texts. We also consider the flagged\nword ratio, removing paragraphs with a high proportion of flagged words associated with\nadult or inappropriate content. We also use KenLM (Heafield, 2011) models trained on\nWikipedia to filter out paragraphs with excessively high perplexity scores.\nTo minimize spam, one approach is to identify generic sentences or invitations to share\narticles on social networks commonly found at the end of documents. We create a list of\nfrequently used words associated with these paragraphs and then filter out paragraphs that\ncontain an excessive proportion of words from this list.\nTo augment our ability to identify non-human-generated content, we consider a subset of\n10 million documents from OSCAR (Ortiz Su\u00e1rez et al., 2020), a web-crawled corpus. We\nextract the words from these documents, removed punctuations, converted them to lowercase,\nand retain only the words occurring at least twice, which we refer to as common words. We\nfilter out paragraphs with a too low common word ratio.\nThe detail of the cutoff values for all text filters at the paragraph level is present in Table 3.\nBy applying these node-level and paragraph-level filters, we ensure that only high-quality\nand relevant images and paragraphs are retained for further processing and analysis.\nDocument-level filtering\nFor document-level filtering, we start by removing all docu-\nments with no images or with more than 30 images. We have found that when there are too\nmany images in a document, they are often not related to each other, and are more likely to\nbe considered as spam.\nFor text filters, we use the same filters as for filtering at paragraph level. Since we are at the\ndocument level, the filter metrics are more precise, and we can typically set stricter cutoff\nvalues while limiting the number of false positives. The cutoff values used are also present in\nTable 3.\nAfter these filtering steps, we obtained 365 million web documents and 1.4 billion images\n(potentially duplicated in different documents at this stage).\nA.1.5\nAdditional Filtering and Deduplication Steps\nExclusion of opted-out images\nTo respect the preferences of content creators, we remove\nall images for which creators explicitly opted out of AI model training. We used the Spawning\nAPI9 to verify that the images in the dataset respect the original copyright owners\u2019 choices.\nThis step had a small impact on the overall dataset, by removing only 0.047% of the images.\n9https://api.spawning.ai/spawning-api\n23\nMetric\nCutoff\ntype\nCutoff\nvalue\n(paragraph-\nlevel)\nCutoff\nvalue\n(document-\nlevel)\nNumber of words\nmin\n4\n10\nNumber of words\nmax\n1,000\n2,000\nCharacter repetition ratio\nmax\n0.1\n0.1\nWord repetition ratio\nmax\n0.1\n0.2\nSpecial character ratio\nmax\n0.3\n0.275\nStop word ratio\nmin\n0.3\n0.35\nFlagged word ratio\nmax\n0.01\n0.01\nPunctuation ratio\nmin\n0.001\n0.03\nSpam word ratio\nmax\n0.12\n0.12\nCommon word ratio\nmin\n0.8\n0.9\nLanguage identification prediction\nscore\nmin\n0.8\n0.8\nPerplexity score\nmax\n1500\n1500\nTable 3: Cutoff values for text filters at paragraph and document levels. A \u2019min\u2019 (or \u2019max\u2019)\ncutoff indicates that any paragraph or document, depending on the level, with a value for\nthe considered metric strictly below (or above) the cutoff value is removed.\nImage deduplication based on URL\nPrior to this step, it is possible for the same image\nto be present in multiple documents under the same URL. However, we observe that the\ndistribution of image occurrences was highly skewed, with the majority of images appearing\nonly once, while a small subset of images appeared hundreds of thousands of times. Upon\ncloser examination, we notice that these frequently occurring images are predominantly\ncomprised of common advertisements encountered during the crawling process, browser-\nspecific icons, and similar elements. To address this issue, we remove all images that appear\nmore than 10 times across the entire dataset. This approach significantly reduces the presence\nof unwanted images. We intentionally do not perform strict deduplication, as we observe that\nwhen an image is duplicated only a few times across different documents, the surrounding\ntext and contextual information tend to vary. These diverse contexts associated with the\nduplicated image could be beneficial for the training of a model. We also deduplicate images\nwithin the same document.\nNSFW image removal\nWe use an open-source NSFW classifier10 to reduce the proportion\nof explicit adult content within our dataset. We carefully choose a cutoff that reduces as\nmuch as possible the proportion of false positives. Indeed, if favoring precision to recall\nmay seem to be a good idea to remove as much undesirable content as possible, it hurts\ndiversity. An analysis of false positives shows that in many cases, simple portrait photos of\nwomen are classified as pornographic, which is not the case for men. People of color are also\nmore often misclassified. We remove the entire document when a pornographically classified\nimage is found in the document. In addition, we also remove all images whose URLs contain\nthe sub-strings porn, sex or xxx. We remove approximately 1% of the documents with this\nfilter. Note that many pornographic documents have been previously removed by the filter\non flagged words.\nDocument deduplication based on URL\nSince we consider many Common Crawl\ndumps, it is possible that several documents may be associated with the same URL, despite\nthe initial deduplication efforts. Recognizing the inherent similarity among these documents,\nwe opt to retain only the most recent document for each common URL.\nDocument deduplication based on set of images\nIt is possible that documents with\ndifferent URLs and domain names are very similar and have not been removed by the first\n10https://github.com/GantMan/nsfw_model\n24\ndeduplication, for instance, news articles copied and pasted multiple times across various\nsources. To mitigate this, we form groups of documents with an identical set of images, and\nwe keep only the most recent document for each group.\nParagraph deduplication across documents of the same domain names\nTo elim-\ninate generic spam phrases commonly found at the end of documents, such as \"Share on\nFacebook,\" \"Post a comment,\" or \"Accept the cookies,\" we implement a paragraph-level\ndeduplication process within documents sharing the same domain name. This approach aims\nto enhance the quality of the text by removing redundant and repetitive content. For each\ndomain name, we identify paragraphs that appear at least three times in an identical manner\nacross associated documents. These repetitive paragraphs are subsequently removed from\nthe documents, resulting in the elimination of approximately 15% of the text present in the\nweb documents.\nAfter all these steps, the final dataset contains 141 million documents and 353 million images,\nof which 298 million are unique.\nWe observe that using stricter values for the filtering steps yields fewer multimodal documents,\nalthough not of higher quality. As such, we invite users who are interested in manipulating\na smaller subset of OBELICS to start with a random subset.\n25\nA.2\nAnalysis of OBELICS\nA.2.1\nExamples of Multimodal Web Documents\nFigure 7: Example of a document in OBELICS.\nFrom http://birdingcraft.com/wordpress/2018/01/23/what-happens-with-birding-in-costa-rica-when-the-rain-stops/\n26\nFigure 8: Example of a document in OBELICS.\nFrom https://www.halt.org/can-i-expect-compensation-for-my-injuries/\n27\nFigure 9: Example of a document in OBELICS.\nFrom https://www.quirkybyte.com/blog/2021/11/how-dane-whitman-will-become-black-knight-kit-harringtons-character-explained/\n28\nA.2.2\nUnwanted Document Containing Many Images\nFigure 10: Undesirable document containing many images. Text is only present in small\nproportions, and the relation between the images is not always clear.\n29\nA.2.3\nTop 100 Domains\nRank\nDomain name\nNumber of\ndocuments\n1\nwww.dailymail.co.uk\n434,498\n2\nen.wikipedia.org\n155,258\n3\nnypost.com\n141,494\n4\nwww.thestar.com\n138,224\n5\nsputniknews.com\n133,695\n6\nwww.rediff.com\n133,233\n7\nwww.theepochtimes.com\n132,539\n8\nwww.fool.com\n125,220\n9\nwww.businessinsider.com.au\n123,841\n10\nwww.bustle.com\n122,581\n11\nwww.dailysabah.com\n120,029\n12\nwww.firstpost.com\n119,642\n13\nwww.irishtimes.com\n118,329\n14\ntheathletic.com\n101,982\n15\nwww.news.com.au\n98,339\n16\nwww.indiatimes.com\n98,197\n17\nwww.theglobeandmail.com\n92,805\n18\ntvtropes.org\n92,104\n19\nwww.dailydot.com\n91,034\n20\nmashable.com\n88,310\n21\nobserver.com\n87,336\n22\nwww.cbsnews.com\n86,759\n23\nwww.rappler.com\n86,554\n24\nwww.tmz.com\n84,472\n25\nwww.salon.com\n84,420\n26\nwww.modernghana.com\n83,918\n27\nwww.foxnews.com\n83,002\n28\nwww.huffpost.com\n81,701\n29\nwww.ndtv.com\n81,549\n30\nwww.thisismoney.co.uk\n80,930\n31\nwww.famousbirthdays.com\n78,931\n32\nwww.engadget.com\n76,817\n33\nwww.rnz.co.nz\n76,327\n34\nwww.metro.us\n75,627\n35\nwww.patheos.com\n75,003\n36\nwww.news24.com\n73,883\n37\nwww.thestar.com.my\n73,265\n38\nwww.dw.com\n72,774\n39\nwww.npr.org\n71,939\n40\nkoreajoongangdaily.joins.com\n71,091\n41\npeoplesdaily.pdnews.cn\n71,048\n42\npagesix.com\n70,602\n43\nwww.thenigerianvoice.com\n70,470\n44\nwikimili.com\n69,928\n45\nwww.indiebound.org\n67,986\n46\nwww.cricketcountry.com\n66,605\n47\nexpressdigest.com\n64,250\n48\nwww.capitalfm.co.ke\n64,163\n49\nwww.bizpacreview.com\n64,157\n50\nwww.wionews.com\n63,797\n51\nprofootballtalk.nbcsports.com\n63,532\n52\njamaica-gleaner.com\n63,137\n53\nwww.rte.ie\n63,074\n30\n54\nwww.aspentimes.com\n62,552\n55\nkids.kiddle.co\n62,419\n56\nenglish.alarabiya.net\n60,368\n57\nwww.jellypages.com\n59,381\n58\npeople.com\n59,293\n59\nmuse.jhu.edu\n59,061\n60\nwww.geeky-gadgets.com\n58,975\n61\nwww.khaleejtimes.com\n58,851\n62\nwww.nbcsports.com\n57,922\n63\nen.topwar.ru\n56,723\n64\nwww.thewrap.com\n56,146\n65\nwww.outlookindia.com\n55,752\n66\nwww.celebdirtylaundry.com\n55,618\n67\ntime.com\n55,527\n68\nwww.dailystar.co.uk\n55,503\n69\nwww.legit.ng\n55,395\n70\nwww.thehansindia.com\n55,109\n71\nwww.bbc.co.uk\n55,015\n72\nnewsinfo.inquirer.net\n54,927\n73\nnesn.com\n54,756\n74\nwww.tellerreport.com\n53,939\n75\nwww.rawstory.com\n53,676\n76\nwww.thestatesman.com\n53,286\n77\nwccftech.com\n52,510\n78\nforward.com\n51,969\n79\nnationalinterest.org\n51,851\n80\nwww.pearltrees.com\n50,933\n81\nwww.contactmusic.com\n50,284\n82\nwww.tweaktown.com\n50,138\n83\nwww.destructoid.com\n50,081\n84\nwww.publishersweekly.com\n49,735\n85\nwww.cbs58.com\n49,680\n86\nwww.markedbyteachers.com\n48,994\n87\nwww.caughtoffside.com\n48,857\n88\nwww.islamicinvitationturkey.com\n48,721\n89\ndailyhive.com\n48,447\n90\nwww.aljazeera.com\n47,393\n91\nwww.bbc.com\n47,349\n92\nworldbulletin.dunyabulteni.net\n47,300\n93\nwww.romper.com\n47,115\n94\nwww.catchnews.com\n47,025\n95\nwww.odt.co.nz\n46,712\n96\nwww.jewishpress.com\n46,688\n97\nwww.irishcentral.com\n46,629\n98\ntechcrunch.com\n46,539\n99\nwww.nhl.com\n46,247\n100\nwww.tuko.co.ke\n46,106\nTable 4: Ranking of the 100 domains with the highest number of associated documents in\nOBELICS.\nA.2.4\nTopic Modeling with 20 Topics\nConcept\nRatio\nRelated words\n31\nJustice\n5.16%\nsaid, police, people, year, according, court, case, told, news,\nman, two, death, also, one, old, investigation, found, fire,\nofficers\nPolitics\n6.35%\nsaid, state, government, would, president, trump, law, court,\nparty, public, new, election, states, political, federal, house,\npeople, also, bill\nFamily\n5.24%\nfamily, one, day, back, life, time, home, would, old, said,\nyears, like, two, love, mother, children, first, man, went\nMusic\n5.23%\nmusic, album, band, song, new, songs, show, also, first,\nsound, rock, one, musical, year, released, live, festival, record,\ntrack\nClimate\n3.46%\nwater, energy, climate, species, also, earth, space, one, used,\ngas, use, solar, natural, power, carbon, years, change, system,\nmay\nBusiness\n7.12%\nyear, company, million, market, said, new, business, com-\npanies, per, also, billion, percent, price, financial, money,\nindustry, years, growth, according\nSports\n3.75%\ngame, season, team, first, year, two, said, three, play, last,\ngames, one, win, second, points, coach, back, players, four\nSports (2nd)\n5.67%\nteam, first, year, season, league, last, two, club, world, race,\none, game, win, time, back, players, match, second, final\nAutomotive\n4.18%\nnew, car, also, design, one, power, cars, two, model, use,\nused, system, camera, first, speed, engine, high, vehicle,\nbattery\nCinema\n7.36%\nfilm, story, series, movie, book, new, show, one, also, char-\nacters, character, first, world, star, films, love, best, life,\nman\nWar\n4.26%\nwar, country, said, military, countries, russia, world, russian,\ngovernment, united, international, people, states, president,\nalso, security, israel, army, forces\nGaming\n5.77%\ngame, use, also, new, games, data, one, users, app, online,\nusing, video, google, players, play, time, used, information,\ncontent\nHealth\n3.0%\nhealth, also, may, medical, patients, disease, study, people,\ntreatment, cancer, body, use, drug, research, risk, brain,\ncare, virus, cases\nFood\n2.08%\nfood, also, one, beer, like, eat, made, wine, restaurant, make,\ncoffee, meat, well, used, tea, sugar, use, water, taste\nUrban\n4.62%\ncity, area, new, park, one, building, town, road, also, north,\nday, around, river, island, south, place, along, local, two\nExistence\n5.23%\none, people, god, life, world, women, many, even, human,\nmay, like, way, men, often, would, man, also, social, power,\nmust\nAsia\n1.61%\nindia, indian, also, china, said, chinese, government, minister,\npakistan, country, delhi, kong, hong, people, singh, two,\nkhan, sri, asia\nHistory\n4.24%\nbook, art, first, history, years, new, century, work, one, books,\nalso, church, american, world, time, museum, english, known\nEducation\n5.11%\nschool, said, students, work, university, new, community,\nalso, people, years, year, education, program, women, work-\ning, support, college, children, project\nOther\n10.56%\nlike, one, get, would, time, people, really, know, even, think,\nmuch, good, going, way, see, could, make, want, things,\nsomething\nTable 5: LDA with 20 topics, trained on 100,000 random web documents. A concept for\neach topic is derived from the related words.\n32\nA.2.5\nTopic Modeling with 200 Topics\nConcept\nRatio\nRelated words\nCelebrity Relationships\n0.52%\nstar, fans, show, love, instagram, couple, together,\nshared, relationship, revealed, year, kim, charlie,\ntold, actress, pete, new, former, old, lisa\nMusic Industry\n1.47%\nband, music, song, album, songs, rock, tour, live,\nsinger, show, record, country, bands, released, stage,\none, love, played, pop\nRacial Diversity\n0.26%\nblack, white, people, race, african, american, racial,\ncommunity, racism, gay, racist, americans, diversity,\nlgbtq, justice, color, lgbt, gender, discrimination,\nqueer\nLanguage Usage\n0.17%\nlanguage, english, word, words, name, languages,\nuse, used, text, names, letter, letters, meaning,\ntranslation, writing, spoken, speech, speaking,\nspeak, term\nTeam Spirit\n0.38%\nsaid, get, team, good, really, going, lot, year, think,\ngot, great, like, last, back, well, play, time, guys,\nbig, hard\nNews Media\n0.28%\nnews, media, radio, fox, press, magazine, journal-\nists, television, journalism, story, newspaper, editor,\njournalist, coverage, times, broadcast, interview,\ndaily, podcast, show\nEuropean Culture\n0.04%\nvan, dutch, netherlands, tattoo, amsterdam, bel-\ngium, portugal, belgian, der, tattoos, portuguese,\nbulgaria, sofia, holland, bulgarian, lisbon, santos,\neurope, tulip, brussels\nEuropean Nations\n0.19%\neuropean, germany, german, europe, berlin, swe-\nden, poland, greece, also, countries, swedish, polish,\nczech, denmark, norway, austria, greek, hungary,\nfinland\nFilm Industry\n1.29%\nfilm, movie, films, director, movies, best, actor,\nhollywood, documentary, cinema, role, screen, story,\ndirected, production, actors, also, oscar, award\nAustralian\nAchieve-\nments\n0.12%\naustralia, australian, new, zealand, sydney, award,\nmelbourne, awards, year, victoria, queensland,\nsouth, nsw, brisbane, australians, best, won, auck-\nland, prize\nCulinary Delights\n0.88%\ncream, recipe, cheese, make, chocolate, made,\nbread, add, taste, ice, butter, sauce, cake, sugar,\ncook, food, salt, milk, sweet\nLife and Death\n0.4%\ndeath, one, people, life, world, dead, even, lives,\nmany, die, died, lost, killed, still, never, man, end,\nleft, day, hope\nSpiritual Philosophy\n0.2%\nphilosophy, spiritual, buddhist, religion, religious,\nyoga, buddha, meditation, buddhism, tibetan, guru,\nbook, practice, knowledge, thought, mind, life, mod-\nern, texts, tradition\nCultural Histories\n0.13%\njewish, jews, indigenous, native, holocaust, rabbi,\ntribe, people, indian, community, peoples, tribal,\nisrael, tribes, anti, culture, land, camp, history,\ntorah\nPersonal Development\n0.07%\nsays, people, explains, like, new, adds, get, work,\nwant, also, tells, lot, say, year, years, really, working,\npart, wants, help\n33\nRoyal Families\n0.23%\nking, prince, royal, queen, princess, charles, henry,\nelizabeth, duke, harry, palace, meghan, family,\nwilliam, anne, castle, kate, lady, diana, edward\nDaily News\n0.19%\nsaid, week, friday, monday, wednesday, according,\ntuesday, thursday, news, last, day, told, sunday, sat-\nurday, reported, statement, days, morning, hours\nCreative Projects\n0.19%\nproject, design, work, working, projects, creative,\ncreate, idea, team, process, also, ideas, new, make,\ndesigner, created, started, concept, worked, wanted\nLegal Investigations\n0.6%\ninvestigation, information, former, report, fbi, de-\npartment, office, according, documents, evidence,\npublic, intelligence, government, claims, allegations,\ncorruption, fraud, alleged, officials, federal\nMedical Procedures\n0.19%\nsurgery, skin, pain, treatment, cancer, procedure,\npatients, teeth, bone, patient, surgical, injury, eye,\nhair, tissue, surgeon, tooth, breast, honey, medical\nAthletic Competitions\n0.46%\nolympic, sports, world, athletes, games, sport,\nolympics, gold, team, medal, NUMm, event, won,\nyear, championships, competition, athlete, time,\nfirst\nHistorical Artifacts\n0.62%\nancient, century, NUMth, history, temple, stone,\nroman, years, one, city, also, greek, found, known,\nbuilt, old, site, time, today\nLiterary Works\n0.87%\nbook, books, read, story, author, novel, writing,\nreading, series, stories, first, written, fiction, pub-\nlished, readers, characters, world, one, write, new\nTime Progression\n0.73%\none, year, years, last, still, could, even, time, big,\nnew, two, much, like, back, next, would, since,\nanother, well, already\nEveryday Life\n0.2%\nday, time, sleep, night, home, hours, room, water,\nhouse, bed, days, morning, work, get, every, food,\nhour, two, camp, minutes\nColorful Nature\n0.16%\ncolor, tea, dark, white, green, flowers, skin, like,\nblack, flower, colors, blue, rose, leaves, light, pink,\nalso, red, used, golden\nAutomotive Industry\n1.21%\ncar, cars, engine, vehicle, new, vehicles, model,\nelectric, ford, drive, also, wheel, rear, speed, driving,\ntoyota, motor, front, power\nAmerican Cities\n0.11%\nnew, york, california, city, san, los, angeles, fran-\ncisco, chicago, jersey, state, times, diego, brooklyn,\ncenter, santa, bay, seattle, county\nPolitical Movements\n0.57%\npolitical, people, power, party, government, right,\namerica, politics, anti, war, state, world, left, free,\nnation, democracy, american, country, media, sys-\ntem\nMythical Creatures\n0.12%\nbear, wolf, dragon, snake, bears, lion, like, tiger,\nmonster, wild, human, wolves, animals, snakes,\ncave, creatures, giant, humans, hunter, dragons\nAsian Cultures\n0.09%\nnorth, korea, harry, kim, korean, potter, south, jon,\nthrones, jong, pyongyang, stewart, nuclear, ron,\nwarner, hogwarts, house, game, colbert, peninsula\nData Modeling\n0.31%\ndata, model, number, value, using, numbers, func-\ntion, used, models, values, two, example, method,\nfigure, one, set, problem, object, line\nRomantic Stories\n1.34%\nstory, love, life, girl, one, new, woman, find, young,\nman, finds, characters, father, friend, two, charac-\nter, family, romance, secret, series\n34\nMedical Research\n0.41%\ncancer, cells, cell, dna, disease, gene, human, pa-\ntients, genetic, immune, protein, treatment, genes,\nbacteria, researchers, diseases, research, proteins,\nstudy, clinical\nFitness and Training\n0.21%\nrunning, race, run, training, marathon, fitness,\nmiles, exercise, bike, mile, runners, NUMk, course,\ngym, finish, cycling, yoga, half, runner\nPersonal Perspectives\n1.43%\nlike, people, think, really, would, know, going, get,\nsee, one, lot, things, something, time, want, way,\nmuch, thing, say, could\nGastronomy Scene\n0.44%\nfood, restaurant, coffee, bar, restaurants, menu,\nchef, chicken, pizza, meal, kitchen, dishes, dinner,\neat, dining, burger, table, meals, served, like\nLabor Rights\n0.29%\nworkers, work, employees, job, jobs, union, pay,\nlabor, working, employment, insurance, employers,\nwage, employee, company, paid, worker, labour,\nstaff, business\nCompetitive Sports\n0.75%\ngame, second, goal, first, ball, half, back, minutes,\nwin, lead, two, points, score, minute, final, match,\nside, three, time\nPublic Events\n0.71%\nyear, event, festival, christmas, day, events, NUMth,\nshow, night, tickets, special, holiday, party, live,\ncelebrate, held, also, place, saturday\nDigital Marketing\n0.37%\ndigital, content, marketing, media, brand, adver-\ntising, platform, online, campaign, ads, business,\nindustry, social, new, users, platforms, brands, com-\npanies, internet, consumers\nPublic Safety\n0.24%\nsafety, report, action, letter, statement, said, inci-\ndent, ban, made, public, actions, claims, reported,\naccording, response, taken, complaints, following,\ntake, serious\nFrench Heritage\n0.1%\nfrench, france, paris, jean, saint, les, des, pierre,\ndame, marie, europe, macron, notre, louis, euro-\npean, michel, jamaica, jacques, emmanuel\nEastern European Poli-\ntics\n0.38%\nrussian, russia, ukraine, ukrainian, moscow, putin,\nsoviet, state, vladimir, war, azerbaijan, country, ar-\nmenian, armenia, president, russians, union, sanc-\ntions, region\nHorror Entertainment\n0.58%\nmovie, story, horror, characters, character, film,\naction, one, plot, ghost, scene, evil, movies, like,\nseries, original, genre, dark, scenes, first\nPolitical Campaigns\n1.25%\ntrump, president, election, vote, campaign, obama,\nparty, biden, house, donald, political, republican,\npresidential, voters, democratic, democrats, candi-\ndate, clinton, candidates, white\nIndian Cinema\n0.64%\nfilm, khan, actor, also, movie, bollywood, films,\nkapoor, indian, actress, seen, role, singh, india,\nrelease, hindi, kumar, directed, hai, salman\nCorporate Leadership\n0.82%\nyears, board, director, president, team, business,\nleadership, work, executive, also, chief, role, mem-\nber, management, service, experience, served, staff,\nworking\nLaw Enforcement\n1.94%\npolice, said, officers, man, officer, arrested, year,\nold, incident, two, found, according, investigation,\nkilled, department, shot, scene, vehicle, suspect\nFootball Clubs\n1.26%\nclub, league, season, united, premier, players, city,\nfootball, chelsea, team, arsenal, player, manchester,\nliverpool, game, side, back, last, games\n35\nEssential Skills\n0.84%\nget, make, need, one, also, time, best, want, many,\nuse, may, take, find, like, even, help, way, good,\npeople, much\nArtistic Expression\n0.75%\nart, museum, artist, work, artists, exhibition, paint-\ning, works, gallery, arts, paintings, collection, artis-\ntic, drawing, new, show, contemporary, painted,\nartwork\nAmerican Regions\n0.22%\nstate, county, texas, florida, north, south, michigan,\nohio, carolina, states, virginia, west, georgia, center,\nuniversity, washington, colorado, iowa, arizona\nIndustrial Production\n0.28%\nproduction, company, industry, mining, manufac-\nturing, gold, mine, port, supply, project, companies,\nfactory, industrial, plant, steel, products, equip-\nment, coal, goods\nGlobal Affairs\n0.36%\nworld, countries, international, united, trade, china,\nstates, global, country, foreign, europe, region, asia,\neconomic, european, nations, south, india, east\nGovernment Affairs\n1.26%\nminister, government, said, meeting, party, presi-\ndent, prime, would, members, committee, council,\nparliament, also, general, decision, agreement, po-\nlitical, secretary, national, commission\nSoftware Development\n0.67%\ncode, use, file, using, software, version, files, win-\ndows, run, server, application, web, source, open,\nuser, system, new, linux, install\nUK Happenings\n0.22%\nlondon, british, england, britain, centre, brexit, bbc,\nwales, labour, west, manchester, johnson, north,\nprogramme, south, across, may, year, east\nReal Estate Market\n0.16%\nproperty, housing, estate, home, real, homes, house,\nrent, properties, market, land, mortgage, rental,\nsale, houses, price, owner, buyers, sales, units\nFashion Trends\n0.43%\nfashion, hair, wearing, dress, wear, look, style, cloth-\ning, clothes, black, wore, designer, beauty, shirt,\nwomen, also, made, show, costume, new\nGaming Culture\n0.38%\ngame, cards, card, games, play, players, poker,\nplayer, casino, online, gambling, win, deck, playing,\nbetting, lottery, bet, slot, chess, played\nFamous Personalities\n0.04%\nbond, kelly, martin, daniel, peter, doctor, tony,\njohnny, parker, sean, evans, frank, andy, ian, lucas,\ndave, reynolds, spy, emily, amber\nWildlife Conservation\n0.61%\nspecies, birds, bird, animals, fish, found, animal,\nalso, wild, wildlife, eggs, habitat, large, food, like,\nsmall, humans, insects, many, endangered\nPandemic Responses\n0.94%\ncovid, pandemic, health, people, virus, coronavirus,\nvaccine, cases, said, spread, outbreak, public, lock-\ndown, vaccines, government, new, disease, vaccina-\ntion, deaths\nPopular Names\n0.11%\njohn, michael, david, paul, jones, james, johnson,\nmike, jim, steve, robert, two, bob, davis, moore,\nallen, brian, mark, one\nChristian Theology\n0.45%\ngod, jesus, christ, bible, christian, church, faith,\nlord, people, gospel, paul, christians, john, prayer,\nword, biblical, kingdom, pastor, moses\nSports\n0.77%\nseason, team, game, nba, games, basketball, players,\nplayer, play, coach, league, hockey, points, teams,\nnhl, played, first, star, year\nCybersecurity\n0.63%\ndata, security, network, internet, cloud, informa-\ntion, access, technology, services, service, NUMg,\nsoftware, computer, systems, networks, cyber, de-\nvices, users, attacks, use\n36\nBusiness/Finance\n0.78%\ncompany, business, companies, market, industry, in-\nvestment, investors, capital, tech, firm, ceo, based,\ntechnology, billion, businesses, group, million, fi-\nnancial, growth\nProfessional Wrestling\n0.18%\nwwe, ring, wrestling, match, rick, randy, champion,\ntitle, wrestler, vince, show, fans, wrestlers, owens,\ntag, baker, triple, shane, raw, cody\nJapanese Culture/Tech\n0.15%\nanime, musk, japanese, tesla, manga, series, elon,\njapan, ninja, episode, samurai, kai, characters, de-\nmon, karate, character, also, dragon, arc, tokyo\nScottish Personalities\n0.03%\nbrown, scotland, scottish, gordon, glasgow, celtic,\nperry, walker, murray, graham, letter, edinburgh,\ncover, campbell, watson, thomas, also, well, neil,\nhenderson\nStreaming Media\n0.12%\nvideo, youtube, videos, live, watch, channel, stream-\ning, audio, content, stream, channels, footage,\nshows, online, also, NUMk, recording, watching,\nclip, one\nChristianity\n0.36%\nchurch, catholic, pope, religious, christian, churches,\nbishop, francis, faith, holy, priest, saint, mass, vati-\ncan, religion, pastor, christ, parish, christians\nSmartphone\nTechnol-\nogy\n0.83%\nphone, apple, samsung, iphone, pro, smartphone,\ndevice, galaxy, camera, also, display, battery, new,\nsNUM, screen, NUMgb, phones, NUMg, android\nUrban Development\n0.78%\ncity, project, area, council, residents, community,\npark, town, street, public, local, cities, new, de-\nvelopment, mayor, urban, construction, district,\nbuilding\nSociocultural Issues\n0.39%\nsocial, culture, society, cultural, people, political,\ndifferent, moral, identity, important, values, issues,\noften, public, role, many, way, community, under-\nstanding, view\nCommon Male Names\n0.03%\nsmith, jack, tom, ben, adam, alex, kevin, richard, si-\nmon, holmes, billy, bell, oliver, harvey, jake, collins,\nburke, baldwin, joel, aaron\nCombat Sports\n0.49%\nfight, title, tennis, champion, ufc, round, world,\nboxing, fighter, one, win, open, martial, first, match,\nmma, fighters, fighting, career\nIndian Politics\n0.64%\nindia, indian, state, delhi, government, also, min-\nister, bjp, said, modi, singh, chief, congress, crore,\npradesh, mumbai, gandhi, lakh, hindu\nMilitary History\n0.25%\nwar, world, battle, empire, british, army, history,\ngerman, peace, great, military, wars, end, conflict,\npower, two, land, forces, soldiers, fight\nInternet Cartography\n0.04%\nwww, map, sri, http, https, maps, lanka, com,\natlas, derby, tamil, lankan, html, maria, angelo,\ntara, colombo, org, mapping, easter\nEuropean Football\n0.46%\nleague, champions, team, goals, world, season, foot-\nball, club, cup, madrid, barcelona, player, real,\nplayers, match, messi, ronaldo, liverpool, final\nMobile Applications\n0.73%\napp, google, apple, android, users, mobile, apps,\nphone, new, devices, device, ios, iphone, microsoft,\nuse, also, features, user, screen, windows\nKorean Entertainment\n0.11%\nlee, korean, korea, kim, south, park, seoul, drama,\ngroup, bts, jin, jung, first, also, members, won, woo,\nhyun, young, min\nEconomics\n1.01%\nmarket, price, prices, markets, growth, inflation,\neconomy, stock, economic, rate, rates, investors,\nhigher, year, demand, stocks, trading, dollar, gold\n37\nVideo Games\n0.49%\ngames, game, xbox, gaming, nintendo, video, play,\nconsole, playstation, mario, psNUM, one, sony,\nplayers, steam, gamers, switch, playing, titles\nTime Indicators\n0.3%\nfirst, years, since, time, two, NUMth, three, total,\nday, year, may, second, september, june, january,\nnovember, four, NUM/NUM, april\nScience\nFic-\ntion/Fantasy\n0.14%\nstar, wars, trek, lego, luke, figures, force, series,\njedi, kirk, toy, universe, figure, new, ship, galaxy,\ncrew, fans, space, disney\nMusic Production\n1.09%\nalbum, sound, music, band, track, song, guitar,\nmetal, sounds, tracks, songs, record, bass, vocals,\nnew, release, rock, like, released, drums\nTransportation\n0.42%\ndocument, token, road, end, replaced, bike, traf-\nfic, driving, drivers, bus, train, driver, bridge, car,\nstation, ride, roads, route, transport, rail\nPersonal Life\n1.14%\nlife, people, love, world, many, time, one, always,\nyears, great, every, like, way, friends, never, day,\nwork, first, hope, best\nAmerican History\n0.6%\namerican, history, NUMs, new, first, years, century,\namerica, early, states, united, NUMth, became,\nworld, many, one, today, time, war\nGlobal Policy\n0.96%\nchange, climate, development, economic, govern-\nment, global, policy, need, sector, world, public,\nnew, support, economy, national, social, future,\nhealth, impact, crisis\nSouth Asian Affairs\n0.2%\npakistan,\nafghanistan,\ntaliban,\nkashmir,\nbangladesh,\nkhan,\nindia,\npakistani,\nafghan,\nalso, nepal, country, indian, kabul, jammu, singh,\nislamabad, ali, lahore, karachi\nSports Scores\n0.83%\ngame, points, first, season, two, three, win, second,\nfour, team, lead, run, third, one, five, scored, home,\ngames, point\nTravel/Daily Life\n1.03%\nday, time, back, get, last, one, got, good, night,\nnext, morning, went, first, trip, week, see, around,\nway, little\nAnnouncements\n0.83%\nnew, year, first, last, time, next, NUMth, month,\nalso, release, announced, two, months, march, since,\noctober, september, week, may\nOnline Dating\n0.13%\ndating, gay, online, sites, date, site, tinder, free,\nmen, best, matchmaking, meet, guy, hookup, guys,\napp, apps, relationship, singles, dates\nSuperhero Comics\n0.42%\ncomic, marvel, comics, man, batman, spider, super-\nhero, character, avengers, superman, universe, hero,\ncaptain, new, heroes, fans, issue, super, characters,\nalso\nSpace Exploration\n0.31%\nspace, nasa, mission, mars, drone, launch, rocket,\nsatellite, robot, earth, robots, drones, moon, first,\nstation, orbit, satellites, spacecraft, technology\nMusical Performance\n0.57%\nmusic, jazz, musical, concert, piano, orchestra, com-\nposer, musicians, classical, symphony, played, per-\nformance, playing, performed, piece, work, instru-\nments, also, festival, instrument\nPersonal Finance\n0.17%\nmoney, pay, card, credit, bank, cash, vegas, pay-\nment, paid, account, las, payments, fees, cost, cards,\namount, buy, service, fee\nTelevision Shows\n0.74%\nshow,\nseries,\nseason,\nepisode,\nnetflix,\nshows,\nepisodes, television, comedy, watch, cast, fans, also,\nnew, seasons, character, drama, viewers, first\n38\nCelebrity Culture\n0.11%\ntaylor, jackson, justin, swift, star, jennifer, singer,\njay, tyler, cohen, nicole, spencer, also, eddie, cole,\ncarrie, amy, lopez, bieber, casey\nEnvironmental Conser-\nvation\n0.32%\nwater, river, land, environmental, forest, wildlife,\nconservation, area, natural, lake, areas, project, en-\nvironment, rivers, dam, resources, forests, national,\nmanagement\nPhysical/Quantum Sci-\nences\n0.35%\nwater, air, chemical, used, process, material, sur-\nface, materials, quantum, temperature, high, oxy-\ngen, carbon, radiation, particles, liquid, salt, energy,\npollution, chemicals\nAstronomy\n0.37%\nearth, sun, moon, planet, sky, stars, solar, star,\nspace, light, universe, planets, telescope, years, sci-\nentists, system, galaxy, eclipse, dark\nIslamic/Middle\nEast-\nern Culture\n0.19%\nmuslim, saudi, muslims, islam, islamic, arabia,\negypt, arab, dubai, allah, uae, ali, middle, abu,\nprophet, religious, muhammad, mosque, iran, egyp-\ntian\nGender Issues\n0.14%\nwomen, men, woman, female, girls, gender, male,\nabortion, sexual, girl, young, sex, life, equality,\nfeminist, man, violence, ladies, rights, boys\nFantasy/Mythology\n0.03%\nsam, lewis, max, rings, twin, troy, monkey, toy,\nstephen, palmer, doll, hobbit, tolkien, zeus, lord,\nmonkeys, seth, horse, toys, witch\nVideo Game Mechanics\n0.36%\nattack, damage, enemy, pokemon, use, weapon,\nenemies, level, also, fight, battle, attacks, players,\npower, weapons, ability, magic, hero, character,\narmor\nMMORPG Gaming\n1.16%\ngame, games, players, play, new, player, world, play-\ning, characters, gameplay, mode, character, also,\nstory, battle, fun, experience, free, fantasy\nEnergy and Environ-\nment\n0.65%\nenergy, oil, gas, power, carbon, solar, fuel, emis-\nsions, electricity, climate, wind, renewable, coal,\nnatural, green, production, industry, fossil, environ-\nmental\nFinancial Regulations\n0.57%\ntax, financial, bank, government, debt, income,\nbanks, money, taxes, budget, economy, finance,\nloan, pay, billion, loans, credit, economic, fund\nUS Legislation\n0.75%\nstate, bill, would, federal, house, senate, congress,\nlaw, legislation, act, states, governor, government,\npassed, public, committee, lawmakers, plan, fund-\ning\nSubjective Experience\n0.91%\nlike, good, really, one, well, much, great, bit, even,\nlittle, quite, also, though, still, pretty, lot, see, get,\nbetter, would\nParenthood\n0.16%\nchildren, child, kids, parents, baby, age, young,\nbirth, parent, pregnancy, pregnant, family, families,\nbabies, adults, mother, old, early, mothers\nPersonal Experiences\n1.93%\nlike, get, one, know, got, really, good, little, even,\nthink, guy, thing, going, love, pretty, right, let,\nmuch, never, back\nEducation\n0.55%\nschool, students, education, schools, college, stu-\ndent, high, university, class, teachers, year, teacher,\ncampus, program, learning, teaching, classes, chil-\ndren, grade, parents\nLatin American Cul-\ntures\n0.17%\nmexico, spanish, italian, spain, italy, san, mexi-\ncan, latin, puerto, del, cuba, rico, colombia, costa,\namerica, cuban, venezuela, juan, country\n39\nTechnological Systems\n0.68%\nsystem, new, technology, systems, development,\nalso, use, time, process, high, based, performance,\nwork, used, well, using, provide, quality, level, de-\nveloped\nSocial Movements\n0.6%\nrights,\npeople,\ngovernment,\nhuman,\nviolence,\nprotest, freedom, police, country, protests, law,\ncivil, political, protesters, movement, state, justice,\nactivists, right, groups\nSurfing/Beach Culture\n0.02%\nscott, ryan, wilson, joe, anderson, wave, josh, sarah,\nphil, surf, jackie, waves, robinson, logan, beach, ken,\nsurfing, phoenix, duncan, gibson\nBrazilian Culture\n0.03%\nbrazil, brazilian, miller, rio, phillips, paulo, por-\ntuguese, peterson, grande, s\u00e3o, janeiro, ivy, bol-\nsonaro, herman, silva, state, amazon, sao, spike,\nhernandez\nLiterature/Poetry\n0.32%\npoetry, writing, essay, writer, poem, poems, literary,\nliterature, work, poet, book, published, writers,\nwrote, write, english, works, collection, written, life\nFamily Life\n0.58%\nfamily, years, wife, home, mary, born, school, life,\nfuneral, friends, died, church, death, service, many,\nmember, may, mrs, passed\nCricket\n0.47%\ncricket, india, test, match, runs, team, england,\nseries, first, wickets, ipl, overs, game, tNUM, played,\nindian, ball, innings, captain\nCanadian/Irish Affairs\n0.09%\ncanada, canadian, ireland, irish, toronto, ontario,\nvancouver, dublin, province, alberta, northern,\ncanadians, ottawa, montreal, provincial, centre,\nquebec, north, trudeau\nMusic Industry\n1.01%\nmusic, album, song, artists, artist, hip, single, hop,\nreleased, new, songs, rapper, track, video, rap, pop,\nrelease, hit, singer\nCriminal Justice\n0.6%\nprison, crime, criminal, court, charges, sexual, trial,\ncase, jail, years, crimes, guilty, victims, murder,\nabuse, accused, sentence, justice, convicted\nAcademic Research\n0.66%\nuniversity, research, science, professor, institute,\nstudies, college, scientific, school, work, study, en-\ngineering, national, international, department, stu-\ndents, degree, academic, center\nNames and Dates\n0.02%\nwilliams, hill, ross, carter, kennedy, clark, jan, nel-\nson, jordan, stanley, rated, murphy, arthur, mar-\nshall, hudson, feb, nov, oct, mar\nWeather Conditions\n0.49%\nweather, ice, snow, mountain, winter, north, tem-\nperatures, cold, climate, south, high, lake, rain,\ntemperature, east, west, summer, conditions, ski\nHealth and Medicine\n0.54%\nblood, brain, disease, symptoms, may, heart, pa-\ntients, body, treatment, also, cause, risk, pain, con-\ndition, effects, common, severe, doctor, pressure\nCryptocurrency\n0.47%\nbitcoin, blockchain, crypto, cryptocurrency, digital,\nmining, ethereum, cryptocurrencies, currency, ex-\nchange, btc, market, network, tokens, users, price,\nnft, trading, transactions, token\nDiet and Nutrition\n0.38%\nfood, diet, weight, health, body, fat, eating, foods,\neat, sugar, healthy, also, high, diabetes, people,\nmeat, protein, obesity, levels\nActions\nand\nMove-\nments\n0.12%\nback, get, time, take, right, move, way, next, see,\nstart, around, keep, make, end, away, going, one,\nleft, another, turn\n40\nHistoric Landmarks\n0.36%\nNUMth, town, village, name, william, george, cen-\ntury, hall, john, family, built, castle, early, house,\nmill, street, history, became, morris\nElectronic Devices\n0.41%\npower, light, battery, use, control, device, used,\nsystem, led, also, using, devices, high, signal, air,\nelectrical, switch, low, sensor\nPerforming Arts\n0.43%\ntheatre, show, dance, stage, play, theater, perfor-\nmance, production, audience, musical, opera, arts,\nbroadway, dancing, cast, performances, performing,\ncompany, ballet, shakespeare\nMental Health\n0.26%\nmental, people, health, disorder, depression, help,\nself, anxiety, stress, emotional, person, life, physical,\nmay, often, brain, also, social, autism, feel\nOnline Interaction\n0.35%\npost, blog, read, comments, posted, like, would,\none, see, com, please, know, article, share, site,\nemail, comment, posts, link, page\nSubstance Usage\n0.27%\ndrug, drugs, cannabis, marijuana, use, cbd, medical,\neffects, addiction, fda, used, alcohol, cocaine, sub-\nstance, prescription, heroin, treatment, products,\nthc, also\nOutdoor Landscapes\n0.46%\ntree, trees, trail, water, road, river, along, forest,\narea, around, small, park, one, near, old, wood,\nway, hill, across, ground\nColors\n0.06%\nred, blue, white, green, black, yellow, color, light,\nflag, orange, grey, colors, gray, logo, one, pearl, hat,\nlook, colour, two\nIsrael and Fishing\n0.19%\nisrael, israeli, fish, palestinian, jerusalem, fishing,\ngaza, palestinians, netanyahu, hamas, jewish, bank,\nwest, palestine, state, arab, israelis, trout, salmon\nAir Travel\n0.4%\nairport, flight, aircraft, air, airlines, plane, flights,\ntravel, airline, passengers, aviation, flying, fly, inter-\nnational, airports, pilot, passenger, boeing, service\nWaste and Recycling\n0.16%\nplastic, waste, made, used, use, bags, make, bag,\npaper, items, nike, fabric, shoes, cola, using, coca,\ntrash, recycling, also, shoe\nPhilosophical\nDis-\ncourse\n0.34%\nwould, even, one, could, however, much, fact, yet,\nrather, far, though, many, well, might, perhaps,\nless, long, despite, may, time\nProblems and Issues\n0.16%\ncould, problem, many, may, problems, due, however,\nissues, issue, would, even, also, cause, result, still,\ntime, situation, damage, impact, without\nFirearms and Malaysia\n0.17%\ngun, shooting, guns, malaysia, hunting, rifle,\nfirearms, shot, deer, weapons, shoot, weapon,\nmalaysian, pistol, firearm, ammunition, rmNUM,\nhunt, buck\nDisney and Animation\n0.12%\ndisney, magic, world, ray, animation, alice, walt,\npark, animated, fairy, ride, parks, disneyland,\ntheme, magical, pixar, jungle, studios, orlando,\ncharacters\nMiddle Eastern Con-\nflict\n0.81%\nsyria, turkey, forces, iraq, military, security, attacks,\nattack, killed, syrian, terrorist, turkish, war, people,\nstate, group, isis, terrorism, terrorists, government\nPhysical Descriptions\n0.48%\neyes, like, face, could, head, hand, back, little,\nlooked, hands, said, around, look, body, would,\nvoice, see, away, hair, felt\nArchitecture\n0.62%\nbuilding, house, room, space, built, floor, construc-\ntion, wall, buildings, new, home, design, tower, two,\nwalls, architecture, roof, rooms, designed\n41\nTravel Destinations\n0.94%\ncity, hotel, park, one, visit, tour, world, town, place,\ntravel, area, many, also, trip, beautiful, places, visi-\ntors, located, island\nComputer Hardware\n0.41%\nintel, performance, computer, memory, amd, core,\ngraphics, usb, windows, laptop, drive, cpu, card,\npower, nvidia, hardware, gpu, processor, gaming\nAfrican Nations\n0.17%\nafrica, south, african, kenya, country, cape, uganda,\nrNUM, zimbabwe, continent, national, congo,\nafricans, west, tanzania, president, town, johan-\nnesburg, rwanda, nairobi\nMilitary Operations\n0.37%\nmilitary, army, war, soldiers, forces, troops, general,\nservice, battle, soldier, commander, men, armed,\ncorps, force, command, training, unit, guard, com-\nbat\nTobacco and Cookies\n0.15%\ncookies, website, smoking, use, tobacco, cigarettes,\nbuy, smoke, experience, cigar, cookie, necessary,\nused, ivermectin, cigarette, consent, online, may,\nvaping, also\nNigerian Politics\n0.67%\nstate, nigeria, said, government, nigerian, gover-\nnor, president, ghana, lagos, buhari, also, nNUM,\nnigerians, country, national, federal, people, apc,\nsecurity, abuja\nFamily Dynamics\n0.54%\nfamily, father, mother, son, old, daughter, home,\nchildren, years, year, parents, wife, young, brother,\nlife, dad, two, house, sister\nFarming and Agricul-\nture\n0.4%\nplant, farmers, farm, food, plants, agriculture, gar-\nden, soil, agricultural, seeds, grow, growing, seed,\ncrop, crops, production, farming, farms, fruit, har-\nvest\nRetail Industry\n0.27%\nstore, market, products, sales, amazon, stores, cus-\ntomers, price, company, business, retail, product,\nbuy, shop, online, consumers, brand, shopping, sell,\nselling\nOnline Resources\n0.32%\ndownload, information, free, page, available, online,\nbook, edition, website, pdf, article, site, published,\nlibrary, content, please, text, may, read\nPersonal Experiences\n2.07%\nwould, time, could, one, didn, first, back, got, went,\nyears, came, wanted, made, started, took, never,\nday, wasn, thought, even\nTheology and Morality\n0.45%\ngod, man, one, lord, world, life, earth, upon, power,\nmay, spirit, human, evil, love, heaven, gods, soul,\nmust, every, shall\nSports and Games\n1.29%\nseason, game, team, football, nfl, yards, baseball,\ngames, players, league, coach, field, play, year,\nplayer, bowl, quarterback, teams, first\nAsia and Pacific\n0.07%\njapan, japanese, tokyo, vietnam, indonesia, pa-\ncific, hawaii, island, vietnamese, indonesian, islands,\nasian, also, asia, west, rice, jakarta, abe, hawaiian\nHealthcare\n0.27%\nhealth, care, medical, hospital, patients, doctors,\nhealthcare, patient, treatment, services, medicine,\ndoctor, hospitals, hiv, nursing, nurses, emergency,\ninsurance, nurse, staff\nCommemorations\n0.21%\nday, memorial, anniversary, national, NUMth, cere-\nmony, veterans, flag, honor, statue, cemetery, peo-\nple, nation, war, country, president, service, years,\nmonument\nCollectibles and Auc-\ntions\n0.32%\ngold, collection, silver, watch, auction, box, original,\nsold, coin, coins, one, made, sale, watches, design,\nset, edition, also, rare\n42\nEast Asia\n0.18%\nchina, chinese, kong, hong, singapore, philippines,\nbeijing, taiwan, thailand, shanghai, asia, also, thai,\nprovince, asian, country, philippine, city, manila\nMaritime Exploration\n0.4%\nsea, island, ship, boat, ocean, water, coast, beach,\nbay, ships, marine, islands, boats, cruise, port, wa-\nters, crew, fishing, sailing\nNatural Disasters\n0.39%\nfire, people, storm, hurricane, disaster, emergency,\nfires, damage, flood, earthquake, rescue, smoke,\nflooding, firefighters, homes, residents, burning, hit,\narea\nLegal Matters\n0.69%\ncourt, law, case, judge, legal, supreme, justice, de-\ncision, attorney, filed, trial, cases, courts, lawyer,\nlawyers, lawsuit, appeal, ruling, judges\nDimensions and Posi-\ntioning\n0.47%\ntwo, side, one, top, right, back, cut, line, use,\nsmall, used, hand, like, left, body, front, size, using,\naround\nRelationships and Mar-\nriage\n0.18%\nmarriage, sex, relationship, married, wedding, love,\ncouple, sexual, divorce, man, husband, wife, cou-\nples, together, woman, partner, men, one, relation-\nships, bride\nCommunity Projects\n0.84%\ncommunity, support, group, people, members, pro-\ngram, help, local, foundation, event, also, work,\norganization, part, project, together, youth, young,\nyear\nPhotography\n0.26%\nimage, camera, images, photo, photos, NUMd, pho-\ntography, pictures, cameras, picture, light, lens,\nphotographer, capture, photographs, taken, shot,\nlook, using, shoot\nCompetitive Sports\n0.88%\nteam, players, teams, cup, tournament, world, foot-\nball, competition, final, round, golf, play, club,\nmatch, first, won, league, win, sports\nInnovation and Science\n0.57%\nworld, human, new, reality, create, like, time, life,\nfuture, nature, work, experience, way, process,\nspace, ideas, different, form, idea, science\nPersonal Opinions\n1.87%\npeople, know, like, think, say, even, want, make,\none, something, things, someone, way, doesn, would,\ngood, need, person, feel, never\nStatistics\n0.99%\npercent, per, year, number, according, cent, av-\nerage, report, increase, years, rate, million, data,\npopulation, last, people, increased, growth, higher\nPersonal Communica-\ntion\n0.15%\nsaid, would, told, people, added, could, asked, also,\ngoing, think, want, year, last, say, saying, one,\ninterview, make, come, according\nAnimal Companions\n0.3%\ndog, dogs, cat, animals, animal, cats, horse, pet,\nbreed, horses, pets, also, owner, bull, owners, pig,\nrescue, puppy, pigs, humans\nScientific Research\n0.41%\nstudy, research, data, researchers, found, results,\nstudies, risk, analysis, evidence, group, published,\ntest, findings, based, university, likely, may, could\nMystery\nand\nAdven-\nture\n0.43%\nman, back, one, left, door, street, front, around,\naway, saw, car, went, two, night, told, heard, took,\nlater, behind, another\nMotor Racing\n0.85%\nrace, racing, team, season, track, car, races, sec-\nond, first, win, championship, lap, two, driver, top,\nseries, year, drivers, fNUM\nInternational Politics\n0.56%\nunited, states, iran, border, trump, nuclear, pres-\nident, immigration, security, country, administra-\ntion, foreign, american, countries, migrants, policy,\nrefugees, immigrants, government, washington\n43\nAir Defense\n0.34%\nair, aircraft, force, military, navy, defense, defence,\nwing, fighter, missile, flying, base, naval, command,\npilot, pilots, flight, forces, jet\nAdditional Information\n0.62%\nwithin, however, additionally, stated, mentioned,\none, extra, password, might, individuals, simply,\ntime, present, actually, get, place, may, together,\ndifferent\nFinancial Performance\n0.62%\nmillion, year, billion, company, quarter, sales, rev-\nenue, per, said, share, total, according, last, first,\nNUMm, percent, expected, growth, reported\nAlcohol and Beverages\n0.38%\nbeer, wine, drink, alcohol, brewery, drinking, wines,\nbottle, brewing, beers, craft, taste, brew, drinks,\nwhisky, ale, tasting, bar, whiskey, bottles\nCelebrity Profiles\n0.66%\nalso, career, born, known, years, worth, age, net,\nlife, famous, american, became, name, first, million,\nstarted, year, appeared, actress\nStorytelling and Narra-\ntives\n1.26%\nlike, life, story, world, one, time, sense, way, yet,\nmuch, work, makes, narrative, every, often, takes,\nmoments, something, stories, piece\nLegislation\n0.78%\nlaw, act, rules, may, legal, laws, government, public,\nmust, state, regulations, would, information, rule,\ncommission, states, required, order, authority\nSocial Media\n0.45%\ntwitter, facebook, social, media, instagram, post,\npeople, account, also, pic, tweet, share, news, online,\nposted, video, users, page, wrote, shared\nComparative Analysis\n0.42%\none, also, however, two, may, different, many, used,\nexample, well, often, first, part, although, another,\ntime, known, fact, various, number\nTable 6: LDA with 200 topics, trained on 100,000 random web documents. A concept for\neach topic is derived from the related words.\n44\nA.3\nEthical discussion\nAt the beginning of the project, we reflected on ethical principles11 guiding the project,\nincluding the creation of the dataset, in order to incorporate ethical values we agreed on.\nThese values motivated the careful crafting of the content filters. For instance, we used the\nSpawning API to respect as much as possible the consent decisions of content creators or\niterated significantly on filters around pornographic content.\nExploring large-scale corpora is often a tedious process which contributes to the lack of\ntransparency and lack of documentation around these artifacts. With that in mind, we\nbuilt an interactive visualization12 of OBELICS which allows browsing through a subset\n(11M documents) of the dataset and navigate the different topics covered. Yet, we note that\ndespite our efforts, OBELICS contains a small proportion of documents that are not suitable\nfor all audiences. For instance, one might find the cluster named \u201cSex\u201d which predominantly\ncontains descriptions of pornographic movies along with pornographic images. Other clusters\nwould contain advertising for sex workers, or reports of violent shootings. In our experience,\nthese documents represent a small proportion of all the documents.\nDue to the nature of our dataset (multimodal documents extracted from the web), OBELICS\ninherits the same ethical concerns of unlabeled text corpora crawled from the web: difficulty\nto document/inspect, presence of unintended biases, under-representation of certain demo-\ngraphics, etc. These concerns have been well documented for text corpora (Biderman and\nScheirer, 2020; Bender et al., 2021). Data audits have shed light on the some limitations\nand unintended biases contained in these text corpora (Caswell et al., 2020; Dodge et al.,\n2021). The augmentation of text corpora with interleaved images is a recent development\nof multimodal machine learning. We hope that our dataset along with exploration tools\nwill serve as a solid ground for endeavors such as data audits. Existing works auditing\nlarge-scale multimodal datasets have focused on image-text pairs datasets (Birhane et al.,\n2021) and highlight how curation and filtering decisions lead to biases (including racism\nand misogyny) in the resulting pairs. We believe that interleaved image-text datasets will\nplay a significant role in the development of increasingly more capable multimodal models,\nand having large-scale versions of these datasets that are transparent, maintained and in\nopen-access is critical.\nWe also have evaluated the trained models as part of a red-teaming effort and a systematic\nevaluation of the generations produced by the model compared across the axis of gender\nand race. More specifically, the model was separately prompted to write a resume, a dating\nprofile, and a headline about a person\u2019s recent arrest based on their appearance. We studied\nthe generations and analyzed the trends for each protected characteristic using FairFace\n(K\u00e4rkk\u00e4inen and Joo, 2021) and StableBias (Luccioni et al., 2023). The details of these\nevaluations and insights are made public as part of the model release. As an example, the\nmodel trained on OBELICS associates men more frequently than women with terms like\n\u201cfinancial\u201d, \u201cdevelopment\u201d, \u201cproduct\u201d, and \u201csoftware\u201d.\nA.4\nBuilding the Model\nA.4.1\nArchitecture Details\nWe closely follow the Flamingo architecture introduced in Alayrac et al. (2022). To form\nthe model, we combine a pre-trained image encoder, a pre-trained language model, and\nadd newly initialized parameters of the form of Perceiver blocks (Jaegle et al., 2021) and\nTransformer-based cross-attentions blocks inserted within the language model every 4 layers.\nThe pre-trained backbones are frozen during the training, and only the new parameters are\nupdated along with the embeddings of additional tokens.\nFollowing Dehghani et al. (2023), we apply a layer normalization on the projected queries\nand keys of both the Perceiver and cross-attention blocks, which improved training stability\n11https://huggingface.co/blog/ethical-charter-multimodal\n12https://atlas.nomic.ai/map/f2fba2aa-3647-4f49-a0f3-9347daeee499/\nee4a84bd-f125-4bcc-a683-1b4e231cb10f\n45\nin our early experiments. We use the RMSNorm implementation (Zhang and Sennrich, 2019)\nfor the layer normalization.\nTotal\nTrainable\nLanguage Model\nVision Model\nPerceiver\nCross-Attentions\n9B\n1.5B\n7B\n630M\n126M\n1.4B\n80B\n14B\n65B\n630M\n126M\n13.9B\nTable 7: Breakdown of model parameters. We use LLaMA (Touvron et al., 2023) for the\nlanguage backbone and OpenCLIP (https://laion.ai/blog/large-openclip/) for the\nvision backbone.\nA.4.2\nTraining Details\nWe roughly use the same set hyper-parameters for all the runs presented in Figure 6 and\nTable 2, as detailed in Table 8. The training of IDEFICS uses a larger batch size and\nexamples of longer sequence length.\nIn all experimental runs, we employ the AdamW\noptimizer (Loshchilov and Hutter, 2017) and incorporate an auxiliary loss, denoted as\nz_loss = 10\u22123 \u00d7 log2(Z), to encourage the softmax normalizer log(Z) to get closer to 0\n(Chowdhery et al., 2022). We use gradient clipping of 1.0.\nDuring the training, two models \u2013 IDEFICS and the 9B-parameter model trained on LAION\n+ OBELICS \u2013 encountered unrecoverable loss spikes. As a remedial measure, we restarted the\ntraining from a checkpoint before the spike, shuffled the data and optionally reduced the\nlearning rate. Both models underwent exactly three restarts within the training duration.\nThe four runs conducted have distinct data mixtures as detailed in Table 10, and Tabel 9\ngives the number of tokens and images in the different datasets. Each run involves training\non a mixture of web documents and image-text pairs. A sampling probability p determines\nthe mixture of these two data sources, which influences the frequency of batches originating\nfrom web documents versus those from image-text pairs.\nFor IDEFICS and IDEFICS-9B, the web-document dataset includes both OBELICS and\nWikipedia, and the image-text pair dataset included LAION and Public Multimodal Dataset\n(PMD) (Singh et al., 2022). Given Wikipedia and PMD\u2019s higher quality but lower number\nof examples, we repeat PMD three times and Wikipedia three times.\nWe used a deduplicated version of LAION (Webster et al., 2023) for all the runs where this\ndataset was used.\nA.4.3\nCompute Details\nWe train the 9B-parameter models on OBELICS-only and LAION-only on 32 80GB A100\nGPUs, and on OBELICS + LAION on 64 80GB A100s, for approximately 6 days. These 3\ntrainings have the same effective batch size. We train IDEFICS on 512 80GB A100 GPUs and\nIDEFICS-9B on 128 80GB A100 GPUs for about 14 days each. The compute infrastructure\nis hosted on an AWS cluster located in Oregon.\nA.4.4\nEvaluation\nTo ensure fair comparisons against Flamingo (Alayrac et al., 2022), we make sure that we\nare using the same evaluation splits for each benchmark. We evaluate the models using\nan in-context learning approach (Brown et al., 2020), with random in-context examples.\nFor the 0-shot evaluations, as in Alayrac et al. (2022), we use 2 random priming in-context\nexamples but without passing the associated images. We systematically use different data\nsplits to select the best-performing prompt (which involves creating validation sets from the\ntraining sets, following the methodology proposed by Alayrac et al. (2022)). Table 11 lists\nthe prompts used for each model and task.\nFor the classification tasks (HatefulMeme (Kiela et al., 2020), IIIT-5k (Mishra et al., 2012)),\nwe use rank classification, i.e. we compute the log probability of the prompt followed by\n46\nParameters\nIDEFICS-80B\nIDEFICS-9B\nPerceiver\nResampler\nNumber of Layers\n6\n6\nNumber of Latents\n64\n64\nNumber of Heads\n16\n16\nResampler Head Dimension\n96\n96\nModel\nLanguage Model Backbone\nLlama-65b\nLlama-7b\nVision Model Backbone\nlaion/CLIP-ViT\n-H-14-laion2B\n-s32B-b79K\nlaion/CLIP-ViT\n-H-14-laion2B\n-s32B-b79K\nCross-Layer Interval\n4\n4\nTraining\nSequence Length\n1024\n1024\nEffective Batch Size (# of\ntokens)\n3.67M\n1.31M\nMax Training Steps\n200K\n200K\nWeight Decay\n0.1\n0.1\nOptimizer\nAdam(0.9,\n0.999)\nAdam(0.9,\n0.999)\nGradient Clipping\n1.0\n1.0\nZ-loss weight\n1e-3\n1e-3\nLearning\nRate\nInitial Max\n5e-5\n1e-5\nInitial Final\n3e-5\n6e-6\nDecay Schedule\nLinear\nLinear\nLinear warmup Steps\n2K\n2K\nLarge-scale\nOptim.\nGradient Checkpointing\nTrue\nTrue\nPrecision\nMixed-pres bf16\nMixed-pres bf16\nZeRO Optimization\nStage 3\nStage 3\nTable 8: Training Hyper-Parameters\nData\nSource\nData Type\n# Tokens in\nSource\n# Images in\nSource\nEpochs\nOBELICS\nUnstructured Multimodal\nWeb Documents\n114.9B\n353M\n1\nWikipedia\nUnstructured Multimodal\nWeb Documents\n3.192B\n39M\n3\nLAION\nImage-Text Pairs\n29.9B\n1.120B\n1\nPMD\nImage-Text Pairs\n1.6B\n70M\n3\nTable 9: Number of tokens and images in the different datasets used for the training of\nIDEFICS.\neach of the labels individually, and select as the predicted label the one with the highest\nprobability.\n47\nModel\nOBELICS\nWikipedia\nLAION\nPMD\n9B-parameter model, OBELICS + LAION\n50%\n0%\n50%\n0%\n9B-parameter model, OBELICS only\n100%\n0%\n0%\n0%\n9B-parameter model, LAION only\n0%\n0%\n100%\n0%\nIDEFICS-9B\n73.85%\n6.15%\n17.18%\n2.82%\nIDEFICS\n73.85%\n6.15%\n17.18%\n2.82%\nTable 10: Breakdown of the dataset mixtures used. Percentages correspond to the effective\nnumber of tokens seen from each dataset.\nFor the image captioning (COCO (Lin et al., 2014), Flickr30k (Young et al., 2014)) and\nvisual question answering tasks (VQAv2 (Antol et al., 2015), OKVQA (Marino et al., 2019),\nTextVQA (Singh et al., 2019), VizWiz (Gurari et al., 2018)), we report evaluation in the\nopen-ended setup. We use the greedy decoding as we found that it increased the performance.\nHowever, we observe that the models tend to generate long answers.\nTo truncate the\ngenerated caption or answer, unless specified otherwise, we use a list of manually selected\nstop words. For VisDial, since the evaluation metric is NDCG, we instead rank the possible\ncandidates for each question.\nThe VQA tasks comporting a high proportion of questions with a single-word answer, it was\nbeneficial for the 9B-parameter model trained on LAION only to keep the first word of the\ngenerated answer as the prediction to boost its performance.\nTask\nModel\nPrefix prompt\nExample prompt\nStop words\nVQAv2\nOKVQA\nTextVQA\nIDEFICS\nIDEFICS-9B\n9B LAION only\n9B OBELICS only\n9B LAION + OBELICS\n{bos_token}Instruction:\npro-\nvide an answer to the question.\nUse the image to answer.\\n\nImage:{token_around_\nimage}{image_token}{token_\naround_image}Question:\n{question} Answer: {answer}\\n\n\"Question\",\n\"User\",\n\"Image\",\n\"task\",\n\"What\",\n\"Who\",\n\"When\",\n\"Where\",\n\"Why\",\n\"How\"\nCOCO\nFlickr30k\nIDEFICS\nIDEFICS-9B\n9B OBELICS only\n9B LAION + OBELICS\n{bos_token}\nImage:{token_around_\nimage}{image_token}{token_\naround_image}Caption: {cap-\ntion}\\n\n\"Caption\",\n\"De-\nscription\", \"User\",\n\"Image\", \"task\"\nCOCO\nFlickr30k\n9B LAION only\n{bos_token}Instruction:\npro-\nvide a short caption of the input\nimage.\\n\nImage:{token_around_\nimage}{image_token}{token_\naround_image}Image descrip-\ntion: {caption}\\n\n\"Caption\",\n\"De-\nscription\", \"User\",\n\"Image\", \"task\"\nHateful-\nMemes\nIDEFICS\nIDEFICS-9B\n9B LAION only\n9B OBELICS only\n9B LAION + OBELICS\nIt\u2019s a conversation between a\nhuman, the user, and an intel-\nligent visual AI, Bot. The user\nsends memes with text written\non them, and Bot has to say\nwhether the meme is hateful or\nnot.\n{token_around_\nimage}{image_token}{token_\naround_image}is\nan\nimage\nwith\nwritten\n\"{context}\"\non\nit.\nIs it hateful?\nAnswer:\n{class_name}\n\u2717\nIIIT5k\n9B LAION only\n9B OBELICS only\n9B LAION + OBELICS\n\u2717\n{token_around_\nimage}{image_token}{token_\naround_image}\"{class_\nname}\"\nis\nwritten\non\nthe\npicture.\n\u2717\nVizWiz\nIDEFICS\nIDEFICS-9B\n{bos_token}Task: Answer the\nquestions based on the image\nwhen possible, otherwise say\nunanswerable.\\n\nImage:{token_around_\nimage}{image_token}{token_\naround_image}Question:\n{question} Answer: {answer}\\n\n\"Question\",\n\"User\",\n\"Image\",\n\"task\",\n\"What\",\n\"Who\",\n\"When\",\n\"Where\",\n\"Why\",\n\"How\"\nVisDial\nIDEFICS\nIDEFICS-9B\n\u2717\n{token_around_\nimage}{image_token}{token_\naround_image}{caption}.\n{context}{class_name}.\n\u2717\nTable 11: We select the prompts from a pool of candidates by evaluating 5 intermediate\ncheckpoints on the query and support validation task sets. To form the prompt with N\npriming examples, we concatenate the prefix prompt, followed by N example prompts filled\nwith data from the priming examples, and finally the example prompt filled with data from\nthe example to be evaluated. The data to be replaced is between curly brackets.\n48\nA.4.5\nAdditional Experimental Results\nIn Figure 11, we plot the performance per benchmark for the 9B-parameter models trained\non LAION only, OBELICS only, and a mixture of OBELICS and LAION. We notice that, even\nif the training on LAION only is smooth and the loss keeps decreasing (there are no spikes\nnor instabilities), performance starts to decrease after a certain point on visual question\nanswering benchmarks. We hypothesize that training on image-text pairs can allow a fast\nassociation of concepts between images and texts, but fails to teach the model more complex\nreasoning skills required to solve visual question answering. We tried many different prompt\ncandidates in order to boost the performance of the model trained on LAION only for the\nVQA tasks, without much success.\nOn the other hand, we note that training on image-text pairs yield stronger performance on\nimage captioning tasks than on multimodal documents only. This is expected since training\nand evaluation correspond to the exact same task.\n49\n0.4\n0.5\nVQAv2\n0.4\n0.5\n0.3\n0.4\nOK-VQA\n0.3\n0.4\n0.1\n0.2\n0.3\nTextVQA\n0.1\n0.2\n0.3\n0.6\n0.8\nCOCO\n0.6\n0.8\n0.2\n0.4\n0.6\nFlickr30k\n0.2\n0.4\n0.6\n0.4\n0.6\n0.8\n1\nIIIT5K\n0.4\n0.6\n0.8\n1\n108\n109\n1010\n0.5\n# of training tokens\nHateful Memes\n106\n107\n108\n0.5\n# of training images\nLAION only\nOBELICS only\nOBELICS + LAION\nFigure 11: 4-shot performance through the training using LAION only, OBELICS only and a mixture\nof both. The training sequences from multimodal documents and the packed sequences obtained\nfrom image-text pairs have different numbers of images but the same number of tokens. Thus, we\nplot the performance over two log x-axes.\n50\nA.5\nLicense and Author Statement\nWe release the dataset under a CC-BY license and Terms of Use that require disclosure of\nwhen the dataset is used for the purpose of training models. This license is not intended to\nreplace the licenses of the source content, and any use of content included in the dataset\nmust comply with the original licenses and applicable rights of its data subjects.\nThe purpose of this statement is to clarify the responsibilities and liabilities associated with\nthe use of this dataset. While we have made every effort to ensure the accuracy and legality\nof the data contained within this dataset, we cannot guarantee its absolute completeness or\ncorrectness.\nTherefore, if any rights, legal or otherwise, are violated through this dataset, including but\nnot limited to copyright infringement, privacy violations, or misuse of sensitive information,\nwe, the authors, assume no liability for such violations.\nBy utilizing this dataset, you agree that any consequences, legal or otherwise, arising from\nusing this dataset will be the user\u2019s sole responsibility. You acknowledge that you will\nexercise due diligence and adhere to all applicable laws, regulations, and ethical guidelines\nwhen using the dataset.\nBy accessing, downloading, or using this dataset, you signify your acceptance of this statement\nand your commitment to abide by the terms and conditions of the CC-BY license.\nIf you disagree with the terms of this statement or the CC-BY license, you are not authorized\nto use this dataset.\nThe dataset will be hosted and maintained on the Hugging Face Hub.\n51\n"
  },
  {
    "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization",
    "link": "https://arxiv.org/pdf/2306.16928.pdf",
    "upvote": "35",
    "text": "One-2-3-45: Any Single Image to 3D Mesh in 45\nSeconds without Per-Shape Optimization\nMinghua Liu1\u2217\nChao Xu2\u2217\nHaian Jin3,4\u2217\nLinghao Chen1,4\u2217\nMukund Varma T5\nZexiang Xu6\nHao Su1\n1 UC San Diego 2 UCLA 3 Cornell University 4 Zhejiang University 5 IIT Madras 6 Adobe\nProject Website: http://one-2-3-45.com\nAbstract\nSingle image 3D reconstruction is an important but challenging task that requires\nextensive knowledge of our natural world. Many existing methods solve this\nproblem by optimizing a neural radiance field under the guidance of 2D diffusion\nmodels but suffer from lengthy optimization time, 3D inconsistency results, and\npoor geometry. In this work, we propose a novel method that takes a single image\nof any object as input and generates a full 360-degree 3D textured mesh in a single\nfeed-forward pass. Given a single image, we first use a view-conditioned 2D\ndiffusion model, Zero123, to generate multi-view images for the input view, and\nthen aim to lift them up to 3D space. Since traditional reconstruction methods\nstruggle with inconsistent multi-view predictions, we build our 3D reconstruction\nmodule upon an SDF-based generalizable neural surface reconstruction method\nand propose several critical training strategies to enable the reconstruction of 360-\ndegree meshes. Without costly optimizations, our method reconstructs 3D shapes\nin significantly less time than existing methods. Moreover, our method favors\nbetter geometry, generates more 3D consistent results, and adheres more closely to\nthe input image. We evaluate our approach on both synthetic data and in-the-wild\nimages and demonstrate its superiority in terms of both mesh quality and runtime.\nIn addition, our approach can seamlessly support the text-to-3D task by integrating\nwith off-the-shelf text-to-image diffusion models.\n1\nIntroduction\nSingle image 3D reconstruction, the task of reconstructing a 3D model of an object from a single\n2D image, is a long-standing problem in the computer vision community and is crucial for a wide\nrange of applications, such as robotic object manipulation and navigation, 3D content creation, as\nwell as AR/VR [42, 9, 86]. The problem is challenging as it requires not only the reconstruction\nof visible parts but also the hallucination of invisible regions. Consequently, this problem is often\nill-posed and corresponds to multiple plausible solutions because of insufficient evidence from\na single image. On the other hand, humans can adeptly infer unseen 3D content based on our\nextensive knowledge of the 3D world. To endow intelligent agents with this ability, many existing\nmethods [30, 18, 24, 10, 81, 85, 15, 77] exploit class-specific priors by training 3D generative\nnetworks on 3D shape datasets [4]. However, these methods often fail to generalize to unseen\ncategories, and their reconstruction quality is constrained by the limited size of public 3D datasets.\nIn this work, we pursue a generic solution to turn an image of any object, regardless of its category,\ninto a high-quality 3D textured mesh. To achieve this, we propose a novel approach that can effectively\nutilize the strong priors learned by 2D diffusion models for 3D reconstruction. Compared to 3D\ndata, 2D images are more readily available and scalable. Recent 2D generative models (e.g., DALL-\nE [59, 58], Imagen [65], and Stable Diffusion [64]) and visual-language models (e.g., CLIP [56])\n\u2217Equal Contribution\nPreprint. Under review.\narXiv:2306.16928v1  [cs.CV]  29 Jun 2023\nFigure 1: One-2-3-45 reconstructs a full 360\u25e6 mesh of any object in 45 seconds given a single image\nof it. In each example, we showcase the input image in the left column, alongside the generated\ntextured and textureless meshes from three different views.\nhave made significant strides by pre-training on Internet-scale image datasets. Since they learn a\nwide range of visual concepts and possess strong priors about our 3D world, it is natural to marry 3D\ntasks with them. Consequently, an emerging body of research [25, 22, 47, 55, 35], as exemplified\nby DreamField [25], DreamFusion [55], and Magic3D [35], employs 2D diffusion models or vision\nlanguage models to assist 3D generative tasks. The common paradigm of them is to perform per-shape\noptimization with differentiable rendering and the guidance of the CLIP model or 2D diffusion models.\nWhile many other 3D representations have been explored, neural fields are the most commonly used\nrepresentation during optimization.\nAlthough these optimization-based methods have achieved impressive results on both text-to-3D [55,\n25, 35] and image-to-3D tasks [43, 68], they face some common dilemmas: (a) time-consuming. Per-\nshape optimization typically involves tens of thousands of iterations of full-image volume rendering\nand prior model inferences, resulting in typically tens of minutes per shape. (b) memory intensive.\nSince the full image is required for the 2D prior model, the volume rendering can be memory-intensive\nwhen the image resolution goes up. (c) 3D inconsistent. Since the 2D prior model only sees a\nsingle view at each iteration and tries to make every view look like the input, they often generate 3D\ninconsistent shapes (e.g., with two faces, or the Janus problem [43, 55]). (d) poor geometry. Many\nmethods utilize the density field as the representation in volume rendering. It is common that they\nproduce good RGB renderings but extracting high-quality mesh tends to be difficult.\nIn this paper, instead of following the common optimization-based paradigm, we propose a novel\napproach to utilize 2D prior models for 3D modeling. At the heart of our approach is the combi-\nnation of a 2D diffusion model with a cost-volume-based 3D reconstruction technique, enabling\nthe reconstruction of a high-quality 360\u25e6 textured mesh from a single image in a feed-forward pass\nwithout per-scene optimization. Specifically, we leverage a recent 2D diffusion model, Zero123 [36],\nwhich is fine-tuned on Stable Diffusion [64] to predict novel views of the input image given the\ncamera transformation. We utilize it to generate multi-view predictions of the input single image so\nthat we can leverage multi-view 3D reconstruction techniques to obtain a 3D mesh. There are two\nchallenges associated with reconstruction from synthesized multi-view predictions: (a) the inherent\nlack of perfect consistency within the multi-view predictions, which can lead to severe failures in\n2\noptimization-based methods such as NeRF methods [48, 5]. (b) the camera pose of the input image is\nrequired but unknown. To tackle them, we build our reconstruction module upon a cost volume-based\nneural surface reconstruction approach, SparseNeuS [40], which is a variant of MVSNeRF [6].\nAdditionally, we introduce a series of essential training strategies that enable the reconstruction of\n360-degree meshes from inherently inconsistent multi-view predictions. We also propose an elevation\nestimation module that estimates the elevation of the input shape in Zero123\u2019s canonical coordinate\nsystem, which is used to compute the camera poses required by the reconstruction module.\nBy integrating the three modules of multi-view synthesis, elevation estimation, and 3D reconstruction,\nour method can reconstruct 3D meshes of any object from a single image in a feed-forward manner.\nWithout costly optimizations, our method reconstructs 3D shapes in significantly less time, e.g., in just\n45 seconds. Our method favors better geometry due to the use of SDF representations, and generates\nmore consistent 3D meshes, thanks to the camera-conditioned multi-view predictions. Moreover, our\nreconstruction adheres more closely to the input image compared to existing methods. See Figure 1\nfor some of our example results. We evaluate our method on both synthetic data and real images and\ndemonstrate that our method outperforms existing methods in terms of both quality and efficiency.\n2\nRelated Work\n2.1\n3D Generation Guided by 2D Prior Models\nRecently, 2D generative models (e.g., DALL-E [59, 58], Imagen [65], and Stable Diffusion [64])\nand vision-language models (e.g., CLIP [56]) have learned a wide range of visual concepts by pre-\ntraining on Internet-scale image datasets. They possess powerful priors about our 3D world and have\ninspired a growing body of research to employ 2D prior models for assisting 3D generative tasks.\nExemplified by DreamField [25], DreamFusion [55], and Magic3D [35], a line of works follows the\nparadigm of per-shape optimization. They typically optimize a 3D representation (i.e., NeRF, mesh,\nSMPL human model) and utilize differentiable rendering to generate 2D images from various views.\nThe images are then fed to the CLIP model [22, 25, 47, 34, 3, 31, 2, 27, 83, 38] or 2D diffusion\nmodel [55, 35, 68, 43, 12, 72, 82, 46, 93, 57] for calculating the loss functions, which are used to\nguide the 3D shape optimization. In addition to optimization-based 3D shape generation, some works\ntrain a 3D generative model but leverage the embedding space of CLIP [8, 39, 67], and some works\nfocus on generating textures or materials for input meshes using 2D models\u2019 prior [47, 76, 7, 46, 63].\n2.2\nSingle Image to 3D\nBefore the emergence of CLIP and large-scale 2D diffusion models, people often learn 3D priors from\n3D synthetic data [4] or real scans [60]. Unlike 2D images, 3D data can be represented in various\nformats and numerous representation-specific 3D generative models have been proposed. By combing\n2D image encoder and 3D generators, they generates 3D data in various representations, including 3D\nvoxels [18, 79, 10, 81, 80, 85], point clouds [15, 88, 19, 1, 44, 90], polygon meshes [30, 73, 77, 51],\nand parametric models [54, 94, 95]. Recently, there has been an increasing number of work on learning\nto generate a 3D implicit field from a single image [84, 45, 66, 24, 53, 17, 20, 26, 50, 78, 49].\nAs previously mentioned, several recent works leverage 2D diffusion models to perform per-shape\noptimization, allowing for the text-to-3D task [55, 35, 25] given that diffusion models are typically\nconditioned on text. To enable the generation of 3D models from a single image, some works [43,\n12, 46] utilize textual inversion [16], to find the best-matching text embedding for the input image,\nwhich is then fed into a diffusion model. NeuralLift-360 [23] adds a CLIP loss to enforce similarity\nbetween the rendered image and the input image. 3DFuse [68] finetunes the Stable Diffusion model\nwith LoRA layers [23] and a sparse depth injector to ensure greater 3D consistency. A recent work\nZero123 [36] finetunes the Stable Diffusion model [65] to generate a novel view of the input image\nbased on relative camera pose. In addition to these methods, OpenAI trains a 3D native diffusion\nmodel Point-E [52], which uses several million internal 3D models to generate point clouds. Very\nrecently, they published another model Shap-E [29] which is trained to generate parameters of implicit\nfunctions that can be used for producing textured meshes or neural radiance fields.\n2.3\nGeneralizable Neural Reconstruction\nTraditional NeRF-like methods [48, 74] use a neural network to represent a single scene and require\nper-scene optimization. However, some approaches aim to learn priors across scenes and generalize\nto novel scenes. These methods typically take a few source views as input and leverage 2D networks\nfor extracting 2D features. The pixel features are then unprojected into 3D space, and a NeRF-based\nrendering pipeline is applied on top of them. In this way, they can generate a 3D implicit field given a\n3\nFigure 2: Our method consists of three primary components: (a) Multi-view synthesis: we use a\nview-conditioned 2D diffusion model, Zero123 [36], to generate multi-view images in a two-stage\nmanner. The input of Zero123 includes a single image and a relative camera transformation, which is\nparameterized by the relative spherical coordinates (\u2206\u03b8, \u2206\u03d5, \u2206r). (b) Pose estimation: we estimate\nthe elevation angle \u03b8 of the input image based on four nearby views generated by Zero123. We\nthen obtain the poses of the multi-view images by combining the specified relative poses with the\nestimated pose of the input view. (c) 3D reconstruction: We feed the multi-view posed images to an\nSDF-based generalizable neural surface reconstruction module for 360\u25e6 mesh reconstruction.\nfew source views in a single feed-forward pass. Among the methods, some [75, 61, 21, 89, 87, 37, 33,\n70, 71] directly aggregate 2D features with MLPs or transformers, while others explicitly construct\nthe 3D feature/cost volume [6, 28, 92, 40], and utilize the voxel feature for decoding density and\ncolor. In addition to the density field representation, some methods such as SparseNeuS [40] and\nVolRecon [62] utilize SDF representations for geometry reconstruction.\n3\nMethod\nOur overall pipeline is illustrated in Figure 2. In Section 3.1, we introduce a view-conditioned 2D\ndiffusion model, Zero123 [36], which is used to generate multi-view images. In Section 3.2, we\nshow that traditional NeRF-based and SDF-based methods fail to reconstruct high-quality meshes\nfrom inconsistent multi-view predictions even given ground truth camera poses. Therefore, in\nSection 3.3, we propose a cost volume-based neural surface reconstruction module that can be trained\nto handle inconsistent multi-view predictions and reconstruct a 3D mesh in a single feed-forward pass.\nSpecifically, we build upon the SparseNeuS [40] and introduce several critical training strategies\nto support 360\u25e6 mesh reconstruction. Additionally, in Section 3.4, we demonstrate the necessity of\nestimating the pose of the input view in Zero123\u2019s canonical space for 3D reconstruction. While the\nazimuth and radius can be arbitrarily specified, we propose a novel module that utilizes four nearby\nviews generated by Zero123 to estimate the elevation of the input view.\n3.1\nZero123: View-Conditioned 2D Diffusion\nRecent 2D diffusion models [59, 65, 64] have demonstrated the ability to learn a wide range of\nvisual concepts and strong priors by training on internet-scale data. While the original diffusion\nmodels mainly focused on the task of text-to-image, recent work [91, 23] has shown that fine-tuning\npretrained models allows us to add various conditional controls to the diffusion models and generate\nimages based on specific conditions. Several conditions, such as canny edges, user scribbles, depth,\nand normal maps, have already proven effective [91].\nThe recent work Zero123 [36] shares a similar spirit and aims to add viewpoint condition control\nfor the Stable Diffusion model [64]. Specifically, given a single RGB image of an object and a\nrelative camera transformation, Zero123 aims to control the diffusion model to synthesize a new\nimage under this transformed camera view. To achieve this, Zero123 fine-tunes the Stable Diffusion\non paired images with their relative camera transformations, synthesized from a large-scale 3D\ndataset [11]. During the creation of the fine-tuning dataset, Zero123 assumes that the object is\ncentered at the origin of the coordinate system and uses a spherical camera, i.e., the camera is\nplaced on the sphere\u2019s surface and always looks at the origin. For two camera poses (\u03b81, \u03d51, r1) and\n(\u03b82, \u03d52, r2), where \u03b8i, \u03d5i, and ri denote the polar angle, azimuth angle, and radius, their relative\ncamera transformation is parameterized as (\u03b82 \u2212 \u03b81, \u03d52 \u2212 \u03d51, r2 \u2212 r1). They aim to learn a model\nf, such that f(x1, \u03b82 \u2212 \u03b81, \u03d52 \u2212 \u03d51, r2 \u2212 r1) is perceptually similar to x2, where x1 and x2 are two\nimages of an object captured from different views. Zero123 finds that such fine-tuning enables the\n4\nFigure 3: NeRF-based method [48] and SDF-based method [74] fail to reconstruct high-quality\nmeshes given multi-view images predicted by Zero123. See Figure 1 for our reconstruction results.\nFigure 4: We analyze the prediction quality of Zero123 by comparing its predictions to ground truth\nrenderings across various view transformations. For each view transformation, we report the average\nPSNR, mask IoU, and CLIP similarity of 100 shapes from the Objaverse [11] dataset. The prediction\nmask is calculated by considering foreground objects (i.e., non-white regions). Zero123 provides\nmore accurate predictions when the view transformation is small.\nStable Diffusion model to learn a generic mechanism for controlling the camera viewpoints, which\nextrapolates outside of the objects seen in the fine-tuning dataset.\n3.2\nCan NeRF Optimization Lift Multi-View Predictions to 3D?\nGiven a single image of an object, we can utilize Zero123 [36] to generate multi-view images, but\ncan we use traditional NeRF-based or SDF-based methods [5, 74] to reconstruct high-quality 3D\nmeshes from these predictions? We conduct a small experiment to test this hypothesis. Given a single\nimage, we first generate 32 multi-view images using Zero123, with camera poses uniformly sampled\nfrom the sphere surface. We then feed the predictions to a NeRF-based method (TensoRF [48]) and\nan SDF-based method (NeuS [74]), which optimize density and SDF fields, respectively. However, as\nshown in Figure 3, both methods fail to produce satisfactory results, generating numerous distortions\nand floaters. This is primarily due to the inconsistency of Zero123\u2019s predictions. In Figure 4, we\ncompare Zero123\u2019s predictions with ground-truth renderings. We can see that the overall PSNR is not\nvery high, particularly when the input relative pose is large or the target pose is at unusual locations\n(e.g., from the bottom or the top). However, the mask IoU (most regions are greater than 0.95) and\nCLIP similarity are relatively good. This suggests that Zero123 tends to generate predictions that are\nperceptually similar to the ground truth and have similar contours or boundaries, but the pixel-level\nappearance may not be exactly the same. Nevertheless, such inconsistencies between the source views\nare already fatal to traditional optimization-based methods. Although the original Zero123 paper\nproposes another method for lifting its multi-view predictions, we will demonstrate in experiments\nthat it also fails to yield perfect results and entails time-consuming optimization.\n3.3\nNeural Surface Reconstruction from Imperfect Multi-View Predictions\nInstead of using optimization-based approaches, we base our reconstruction module on a generalizable\nSDF reconstruction method SparseNeuS [40], which is essentially a variant of the MVSNeRF [6]\npipeline that combines multi-view stereo, neural scene representation, and volume rendering. As\nillustrated in Figure 2, our reconstruction module takes multiple source images with corresponding\ncamera poses as input and generates a textured mesh in a single feed-forward pass. In this section,\nwe will first briefly describe the network pipeline of the module and then explain how we train the\nmodule, select the source images, and generate textured meshes. Additionally, in Section 3.4, we will\ndiscuss how we generate the camera poses for the source images.\nAs shown in Figure 2, our reconstruction module takes m posed source images as input. The module\nbegins by extracting m 2D feature maps using a 2D feature network. Next, the module builds a 3D\ncost volume whose contents are computed by first projecting each 3D voxel to m 2D feature planes\nand then fetching the variance of the features across the m projected 2D locations. The cost volume\nis then processed using a sparse 3D CNN to obtain a geometry volume that encodes the underlying\ngeometry of the input shape. To predict the SDF at an arbitrary 3D point, an MLP network takes the\n3D coordinate and its corresponding interpolated features from the geometry encoding volume as\ninput. To predict the color of a 3D point, another MLP network takes as input the 2D features at\nthe projected locations, interpolated features from the geometry volume, and the viewing direction\n5\nof the query ray relative to the viewing direction of the source images. The network predicts the\nblending weights for each source view, and the color of the 3D point is predicted as the weighted sum\nof its projected colors. Finally, an SDF-based rendering technique is applied on top of the two MLP\nnetworks for RGB and depth rendering [74].\n2-Stage Source View Selection and Groundtruth-Prediction Mixed Training. Although the\noriginal SparseNeuS [40] paper only demonstrated frontal view reconstruction, we have extended it to\nreconstruct 360-degree meshes in a single feed-forward pass by selecting source views in a particular\nway and adding depth supervision during training. Specifically, our reconstruction model is trained on\na 3D object dataset while freezing Zero123. We follow Zero123 to normalize the training shapes and\nuse a spherical camera model. For each shape, we first render n ground-truth RGB and depth images\nfrom n camera poses uniformly placed on the sphere. For each of the n views, we use Zero123 to\npredict four nearby views. During training, we feed all 4 \u00d7 n predictions with ground-truth poses\ninto the reconstruction module and randomly choose one of the n ground-truth RGB images views as\nthe target view. We call this view selection strategy as 2-stage source view selection. We supervise\nthe training with both the ground-truth RGB and depth values. In this way, the module can learn\nto handle the inconsistent predictions from Zero123 and reconstruct a consistent 360\u25e6 mesh. We\nargue that our two-stage source view selection strategy is critical since uniformly choosing n \u00d7 4\nsource views from the sphere surface would result in larger distances between the camera poses.\nHowever, cost volume-based methods [40, 28, 6] typically rely on very close source views to find\nlocal correspondences. Furthermore, as shown in Figure 4, when the relative pose is small (e.g., 10\ndegrees apart), Zero123 can provide very accurate and consistent predictions and thus can be used to\nfind local correspondences and infer the geometry.\nDuring training, we use n ground-truth renderings in the first stage to enable depth loss for better\nsupervision. However, during inference, we can replace the n ground-truth renderings with Zero123\npredictions, as shown in Figure 2, and no depth input is needed. We will show in the experiments that\nthis groundtruth-prediction mixed training strategy is also important. To export the textured mesh, we\nuse marching cubes [41] to extract the mesh from the predicted SDF field and query the color of the\nmesh vertices as described in [74]. Although our reconstruction module is trained on a 3D dataset,\nwe find that it mainly relies on local correspondences and can generalize to unseen shapes very well.\n3.4\nCamera Pose Estimation\nOur reconstruction module requires camera poses for the 4\u00d7n source view images. Note that we adopt\nZero123 for image synthesis, which parameterizes cameras in a canonical spherical coordinate frame,\n(\u03b8, \u03d5, r), where \u03b8, \u03d5 and r represent the elevation, azimuth, and radius. While we can arbitrarily\nadjust the azimuth angle \u03d5 and the radius r of all source view images simultaneously, resulting in the\nrotation and scaling of the reconstructed object accordingly, this parameterization requires knowing\nthe absolute elevation angle \u03b8 of one camera to determine the relative poses of all cameras in a\nstandard XYZ frame. More specifically, the relative poses between camera (\u03b80, \u03d50, r0) and camera\n(\u03b80 + \u2206\u03b8, \u03d50 + \u2206\u03d5, r0) vary for different \u03b80 even when \u2206\u03b8 and \u2206\u03d5 are the same. Because of this,\nchanging the elevation angles of all source images together (e.g., by 30 degrees up or 30 degrees\ndown) will lead to the distortion of the reconstructed shape (see Figure 10 for examples).\nTherefore, we propose an elevation estimation module to infer the elevation angle of the input\nimage. First, we use Zero123 to predict four nearby views of the input image. Then we enumerate\nall possible elevation angles in a coarse-to-fine manner. For each elevation candidate angle, we\ncompute the corresponding camera poses for the four images and calculate a reprojection error for\nthis set of camera poses to measure the consistency between the images and the camera poses. The\nelevation angle with the smallest reprojection error is used to generate the camera poses for all 4 \u00d7 n\nsource views by combining the pose of the input view and the relative poses. Please refer to the\nsupplementary for details on how we calculate the reprojection error for a set of posed images.\n4\nExperiments\n4.1\nImplementation Details\nFor each input image, we generate n = 8 images by choosing camera poses uniformly placed on the\nsphere surface and then generate 4 local images (10\u25e6 apart) for each of the 8 views, resulting in 32\nsource-view images for reconstruction. During training, we freeze the Zero123 [36] model and train\nour reconstruction module on Objaverse-LVIS [11] dataset, which contains 46k 3D models in 1,156\ncategories. We use BlenderProc [13] to render ground-truth RGB and depth images. For images with\n6\nFigure 5: Qualitative examples of One-2-3-45 for both synthetic and real images. Each triplet\nshowcases an input image, a textured mesh, and a textureless mesh.\nFigure 6: We compare One-2-3-45 with Point-E [52], Shap-E [29], Zero123 (Stable Dreamfusion\nversion) [36], 3DFuse [68], and RealFusion [43]. In each example, we present both the textured\nand textureless meshes. As 3DFuse [68] and RealFusion [43] do not natively support the export of\ntextured meshes, we showcase the results of volume rendering instead.\nbackground, we utilize an off-the-shelf segmentation network SAM [32] with bounding-box prompts\nfor background removal. Please refer to the supplementary for more details.\n4.2\nSingle Image to 3D Mesh\nWe present qualitative examples of our method in Figures 1 and 5, illustrating its effectiveness in\nhandling both synthetic images and real images. We also compare One-2-3-45 with existing zero-shot\nsingle image 3D reconstruction approaches, including Point-E [52], Shap-E [29], Zero123 (Stable\nDreamfusion version) [36], 3DFuse [68], and RealFusion [43]. Among them, Point-E and Shap-E are\ntwo 3D native diffusion models released by OpenAI, which are trained on several million internal 3D\ndata, while others are optimization-based approaches leveraging priors from Stable Diffusion [64].\n7\nTable 1: Quantitative Comparison on GSO [14] and\nObjaverse [11] datasets.\nPrior\nF-Score\nCLIP Similarity\nTime\nSource\nGSO\nObj.\navg. GSO Obj. avg.\nPoint-E [52]\ninternal\n81.0\n81.0\n81.0 74.3 78.5 76.4\n78s\nShap-E [29]\n3D data\n83.4\n81.2\n82.3 79.6 82.1 80.9\n27s\nZero123+SD [36]\n2D\ndiffusion\nmodels\n75.1\n69.9\n72.5 71.0 72.7 71.9 \u223c15min\nRealFusion [43]\n66.7\n59.3\n63.0 69.3 69.5 69.4 \u223c90min\n3DFuse [68]\n60.7\n60.2\n60.4 71.4 74.0 72.7 \u223c30min\nOurs\n84.0\n83.1\n83.5 76.4 79.7 78.1\n45s\nFigure 7: Error distribution of predicted\nelevations. The median and average are\n5.4 and 9.7 degrees.\nFigure 8: Ablations on training strategies of the reconstruction module and the number of views.\nFigure 6 presents the qualitative comparison. While most methods can generate plausible 3D meshes\nfrom a single image, notable differences exist among them in terms of geometry quality, adherence to\nthe input, and overall 3D consistency. In terms of geometry quality, approaches like RealFusion [43]\nand 3DFuse [68], which optimize a neural radiance field, face challenges in extracting high-quality\nmeshes. Likewise, Point-E [52] produces a sparse point cloud as its output, resulting in numerous\nholes on the reconstructed meshes. In contrast, our approach utilizes an SDF presentation and favors\nbetter geometry. Regarding adherence to the input, we observe that most baseline methods struggle to\npreserve the similarity to the input image. Although Shap-E performs slightly better, it still produces\nlots of failure cases (see the backpack without shoulder straps, distorted\nshoe, and stool with three legs). In contrast, our approach leverages\na powerful 2D diffusion model to directly produce high-quality multi-\nview images, rather than relying on 3D space hallucination. This\nstrategy provides better adherence to the input views, alleviates the\nburden of the 3D reconstruction module, and yields results that are\nmore finely attuned to the input. Furthermore, many approaches en-\ncounter challenges in achieving consistent 3D results (also known as the Janus problem [43, 55]), as\nhighlighted in the right figure (two-handle mug, multi-face Mario, and two-face backpack). One of\nthe contributing factors to this issue is that several methods optimize each view independently, striving\nto make each view resemble the input. In contrast, our method capitalizes on the view-conditioned\n2D diffusion model, inherently enhancing 3D consistency.\nWe also quantitatively compare the approaches on Objaverse [11] and GoogleScannedObjects\n(GSO) [14] datasets. For each dataset, we randomly choose 20 shapes and render a single im-\nage per shape for evaluation. To align the predictions with the ground-truth mesh, we linearly search\nthe scaling factor and the rotation angle, apply Iterative Closest Point (ICP) for sampled point clouds,\nand select the one with the most number of inliers. We follow RealFusion [43] to report F-score (with\na threshold of 0.05) and CLIP similarity, and the runtime on an A100 GPU. As shown in Table 1, our\nmethod outperforms all baseline approaches in terms of F-Score. As for CLIP similarity, we surpass\nall methods except a concurrent work Shap-E [29]. We find that CLIP similarity is very sensitive to\nthe color distribution and less discriminative in local geometry variations (i.e., the number of legs of a\nstool, the number of handles of a mug). Regarding running time, our method demonstrates a notable\nadvantage over optimization-based approaches and performs on par with 3D native diffusion models,\nsuch as Point-E [52] and Shap-E [29]. Specifically, our 3D reconstruction module reconstructs a 3D\n8\nFigure 9: 360\u25e6 reconstruction vs. multi-\nview fusion. Meshes from different views\nare in different colors.\nFigure 10: Incorrect elevations lead to distorted re-\nconstruction. Our elevation estimation module can\npredict an accurate elevation of the input view.\nFigure 11: Text to 3D. First row: \u201ca bear in cowboy suit.\u201d Second row: \u201ca kungfu cat.\u201d We utilize\nDALL-E 2 [58] to generate an image conditioned on the text and then lift it to 3D. We compare our\nmethod with Stable Dreamfusion [55] and 3DFuse [68]. For baselines, volume renderings are shown.\nmesh in approximately 5 seconds, with the remaining time primarily spent on Zero123 predictions,\nwhich take roughly 1 second per image on an A100 GPU.\n4.3\nAblation Study\nTraining strategies. We ablate our training strategies in Figure 8. We found that without our 2-stage\nsource view selection strategy, a network trained to consume 32 uniformly posed Zero123 predictions\n(first column) suffers from severe inconsistency among source views, causing the reconstruction\nmodule to fail completely. If we feed only 8 source views (second column) without the four nearby\nviews, the reconstruction fails to capture local correspondence and cannot reconstruct fine-grained\ngeometry. Similarly, when we do not apply the depth loss during training (third column), the network\nfails to learn how to reconstruct fine-grained geometries. During training, we first render n ground-\ntruth renderings and then use Zero123 to predict four nearby views for each of them. If we train\ndirectly on 8 \u00d7 4 ground-truth renderings without Zero123 prediction during training (fourth column),\nit fails to generalize well to Zero123 predictions during inference, with many missing regions. Instead,\nif we replace the n ground-truth renderings with n Zero123 predictions during training (fifth column),\nthe network also breaks due to the incorrect depth supervision.\nElevation estimation. Our reconstruction module relies on accurate elevation angles of the input view.\nIn Figure 10, we demonstrate the impact of providing incorrect elevation angles (e.g., altering the\nelevation angles of source views by \u00b130\u25e6), which results in distorted reconstruction results. Instead,\nutilizing our predicted elevation angles can perfectly match results with ground truth elevations. We\nalso quantitatively test our elevation estimation module by rendering 1,700 images from random\ncamera poses. As shown in Figure 7, our elevation estimation module predicts accurate elevations.\nNumber of source views. In Figure 8, we also investigate the impact of varying the number of source\nviews on 3D reconstruction. We observe that our method is not very sensitive to the number of views\nas long as the reconstruction module is retrained with the corresponding setting.\n360\u25e6 reconstruction vs. multi-view fusion. While our method reconstructs a 360\u25e6 mesh in a single\npass, most existing generalizable neural reconstruction approaches [40, 28, 6] primarily focus on\nfrontal view reconstruction. An alternative approach is to independently infer the geometry for\neach view and subsequently fuse them together. However, we have observed that this strategy often\nstruggles with multi-view fusion due to inconsistent Zero123 predictions, as illustrated in Figure 9.\n4.4\nText to 3D Mesh\nAs shown in Figure 11, by integrating with off-the-shelf text-to-image 2D diffusion models [64, 58],\nour method can be naturally extended to support text-to-image-3D tasks and generate high-quality\ntextured meshes in a short time. See supplementary for more examples.\n9\n5\nConclusion\nIn this paper, we present a novel method for reconstructing a high-quality 360\u25e6 mesh of any object\nfrom a single image of it. In comparison to existing zero-shot approaches, our results exhibit superior\ngeometry, enhanced 3D consistency, and a remarkable adherence to the input image. Notably,\nour approach reconstructs meshes in a single forward pass without the need for time-consuming\noptimization, resulting in significantly reduced processing time. Furthermore, our method can be\neffortlessly extended to support the text-to-3D task.\n6\nAppendix\nWe first show more qualitative comparison in Section 6.1, which is followed by a demonstration of\nadditional examples on real-world images and the text-to-3D task in Sections 6.2 and 6.3 respectively.\nFurthermore, we present the details of our elevation estimation module in Section 6.4, training and\nevaluation details in Section 6.5. We finally show the failure cases and discuss the limitations in\nSection 6.6.\n6.1\nMore Qualitative Comparison\nFigure 12: We compare One-2-3-45 with Point-E [52], Shap-E [29], Zero123 (Stable Dreamfusion\nversion) [36], 3DFuse [68], and RealFusion [43]. In each example, we present both the textured\nand textureless meshes. As 3DFuse [68] and RealFusion [43] do not natively support the export of\ntextured meshes, we showcase the results of volume rendering instead.\nIn Figure 12, we demonstrate more qualitative comparison on Objaverse [11] and GoogleScannedOb-\njects (GSO) [14] datasets. Note that all test shapes are not seen during the training of our 3D\nreconstruction module.\n6.2\nMore Examples on Real-World Images\nIn Figure 13, we showcase more examples on real-world images and compare our method with the\nconcurrent method Shap-E [29]. The input images are from unsplash.com or captured by ourselves.\nNote that our results exhibit a closer adherence to the input image.\n10\nFigure 13: We compare One-2-3-45 with Shap-E [29] on real-world images. In each example, we\npresent the input image, generated textured and textureless meshes.\n6.3\nMore Examples on Text-to-3D\nIn Figure 14, we present additional examples for the text-to-3D task. It is evident that existing\napproaches struggle to capture fine-grained details, such as a tree hollow, or achieve compositionality,\nas seen in examples like an orange stool with green legs, a pineapple-shaped Havana hat, or a\nrocking horse chair. In contrast, our method produces superior results that adhere more closely to\nthe input text. We hypothesize that controlling such fine-grained attributes in the 3D space using\nexisting optimization strategies is inherently challenging. However, by leveraging established 2D\n11\nFigure 14: Text-to-3D: We compare our method against two native text-to-3D approaches Stable\nDreamFusion [55] and 3DFuse [68]. To enable text-to-3D, our method first uses a pretrained text-to-\nimage model DALL-E 2 [58] to generate an image from input text (prompted with \u201c3d model, long\nshot\u201d), and then uplifts the image to a 3D textured mesh.\ntext-to-image diffusion models, our method becomes more effective in lifting a single 2D image to a\ncorresponding 3D textured mesh.\n6.4\nDetails of Elevation Estimation\nTo estimate the elevation angle \u03b8 of the input image, we first utilize Zero123 [36] to predict four\nnearby views (10 degrees apart) of the input view. With these predicted views, we proceed to\n12\nenumerate all possible elevation angles and compute the re-projection error for each candidate angle.\nThe re-projection error assesses the consistency between camera poses and image observations, akin\nto the bundle adjustment module employed in the Structure-from-Motion (SfM) pipeline.\nSpecifically, we enumerate all candidate elevation angles in a coarse-to-fine manner. In the coarse\nstage, we enumerate elevation angles with a 10-degree interval. Once we have determined the\nelevation angle e\u2217 associated with the smallest re-projection error, we proceed to the fine stage. In this\nstage, we enumerate elevation angle candidates ranging from e\u2217 \u2212 10\u25e6 to e\u2217 + 10\u25e6 with a 1-degree\ninterval. This coarse-to-fine design facilitates rapid estimation, completing the elevation estimation\nmodule in under 1 second for each shape.\nGiven a set of four predicted nearby views, we perform feature matching to identify corresponding\nkeypoints across each pair of images (a total of six pairs) using an off-the-shelf module LoFTR [69].\nFor each elevation angle candidate, we calculate the camera pose for the input image by employing\nthe spherical coordinate system with a radius of 1.2 and an azimuth angle of 0. Note that the azimuth\nangle \u03d5 and the radius r can be arbitrarily adjusted, resulting in the rotation and scaling of the\nreconstructed object accordingly. Subsequently, we obtain the camera poses for the four predicted\nviews by incorporating the specified delta poses.\nOnce we have the four posed images, we compute the re-projection error by enumerating triplet\nimages. For each triplet of images (a, b, c) sharing a set of keypoints P, we consider each point\np \u2208 P. Utilizing images a and b, we perform triangulation to determine the 3D location of p. We then\nproject the 3D point onto the third image c and calculate the reprojection error, which is defined as the\nl1 distance between the reprojected 2D pixel and the estimated keypoint in image c. By enumerating\nall image triplets and their corresponding shared keypoints, we obtain the mean projection error for\neach elevation angle candidate.\n6.5\nDetails of Training and Evaluation\nTraining\nWe train the reconstruction module using the following loss function:\nL = Lrgb + \u03bb0Ldepth + \u03bb1Leikonal + \u03bb2Lsparsity\n(1)\nwhere Lrgb represents the l1 loss between the rendered and ground truth color, weighted by the sum of\naccumulated weights; Ldepth corresponds to the l1 loss between the rendered and ground truth depth;\nLeikonal and Lsparsity are the Eikonal and sparsity terms, respectively, following SparseNeuS [40].\nWe empirically set the weights as \u03bb0 = 1, \u03bb1 = 0.1, and \u03bb2 = 0.02. For \u03bb2, we adopt a linear\nwarm-up strategy following SparseNeuS [40]. To train our reconstruction module, we utilize the\nLVIS subset of the Objaverse [11] dataset, which consists of 46k 3D models across 1,156 categories.\nThe reconstruction module is trained for 300k iterations using two A10 GPUs, with the training\nprocess lasting approximately 6 days. It is important to note that our reconstruction module does not\nheavily rely on large-scale training data, as it primarily leverages local correspondence to infer the\ngeometry, which is relatively easier to learn and generalize.\nEvaluation\nWe evaluate all baseline approaches using their official codebase. Since the approaches\ntake only a single image as input, the predicted mesh may not have the same scale and transformation\nas the ground-truth mesh. To ensure a fair comparison, we employ the following process to align the\npredicted mesh with the ground-truth mesh. First, we align the up direction for the results generated\nby each approach. Next, for each generated mesh, we perform a linear search over scales and rotation\nangles along the up direction. After applying each pair of scale and z-rotation, we utilize the Iterative\nClosest Point (ICP) algorithm to align the transformed mesh to the ground-truth mesh. Finally, we\nselect the mesh with the largest number of inliers as the final alignment. This alignment process\nhelps us establish a consistent reference frame for evaluating the predicted meshes across different\napproaches.\n6.6\nFailure Cases and Limitations\nOur method relies on Zero123 for generating multi-view images, which introduces challenges due\nto its occasional production of inconsistent results. In Figure 15, we present two typical cases that\nexemplify such inconsistencies. The first case involves an input view that lacks sufficient information,\nsuch as the back view of a fox. In this scenario, Zero123 struggles to generate consistent predictions\n13\nFigure 15: Failure cases. Our method relies on Zero123 to generate multi-view images, and we\nencounter challenges when Zero123 generates inconsistent results. (a) The input view lacks sufficient\ninformation. (b) The input view contains ambiguous or complicated structures.\nfor the invisible regions, such as the face of the fox. As a consequence, our method may encounter\ndifficulties in accurately inferring the geometry for those regions. The second case involves an input\nview with ambiguous or complex structures, such as the pulp and peel of a banana. In such situations,\nZero123\u2019s ability to accurately infer the underlying geometry becomes limited. As a result, our\nmethod may be affected by the inconsistent predictions generated by Zero123. It is important to\nacknowledge that these limitations arise from the occasional scenarios, and they can impact the\nperformance of our method in certain cases. Addressing these challenges and refining the reliability\nof Zero123\u2019s predictions remain areas for further investigation and improvement.\nWe have also noticed slight artifacts on the back side of our generated results. As one of the first works\nin combining view-conditioned 2D diffusion models with generalizable multi-view reconstruction,\nwe believe that there is still ample room for exploring more advanced reconstruction techniques and\nincorporating additional regularizations. By doing so, we expect to significantly mitigate the minor\nartifacts and further enhance results in the future.\n6.7\nAcknowledgements\nWe would like to thank the following sketchfab users for the models used for the demo images in this\npaper: dimaponomar2019 (backpack), danielpeng (bag), pmlzbt233 (wooden barrel), felixyadomi\n(cactus), avianinda (burger), shedmon (robocat), ie-niels (stool), phucn (armchair), techCIR (mug),\nsabriny (fox). All models are CC-By licensed.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning repre-\nsentations and generative models for 3d point clouds. In International conference on machine\nlearning, pages 40\u201349. PMLR, 2018.\n[2] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Nie\u00dfner. Clipface: Text-guided editing\nof textured 3d morphable models. arXiv preprint arXiv:2212.01406, 2022.\n[3] Zehranaz Canfes, M Furkan Atasoy, Alara Dirik, and Pinar Yanardag. Text and image guided\n3d avatar generation and manipulation. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 4421\u20134431, 2023.\n[4] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. arXiv preprint arXiv:1512.03012, 2015.\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance\nfields. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XXXII, pages 333\u2013350. Springer, 2022.\n[6] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao\nSu. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14124\u2013\n14133, 2021.\n[7] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nie\u00dfner.\nText2tex: Text-driven texture synthesis via diffusion models. arXiv preprint arXiv:2303.11396,\n2023.\n[8] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liangyan Gui.\nSdfusion: Multimodal 3d shape completion, reconstruction, and generation. arXiv preprint\narXiv:2212.04493, 2022.\n14\n[9] Han-Pang Chiu, Leslie Pack Kaelbling, and Tom\u00b4as Lozano-P\u00b4erez. Automatic class-specific 3d\nreconstruction from a single image. CSAIL, pages 1\u20139, 2009.\n[10] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2:\nA unified approach for single and multi-view 3d object reconstruction. In Computer Vision\u2013\nECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part VIII 14, pages 628\u2013644. Springer, 2016.\n[11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt,\nLudwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe\nof annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022.\n[12] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir\nAnguelov, et al. Nerdi: Single-view nerf synthesis with language-guided diffusion as general\nimage priors. arXiv preprint arXiv:2212.03267, 2022.\n[13] Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus\nKnauer, Klaus H. Strobl, Matthias Humt, and Rudolph Triebel. Blenderproc2: A procedural\npipeline for photorealistic rendering. Journal of Open Source Software, 8(82):4901, 2023.\n[14] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Rey-\nmann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality\ndataset of 3d scanned household items. In 2022 International Conference on Robotics and\nAutomation (ICRA), pages 2553\u20132560. IEEE, 2022.\n[15] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object\nreconstruction from a single image. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 605\u2013613, 2017.\n[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022.\n[17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany,\nZan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes\nlearned from images. Advances In Neural Information Processing Systems, 35:31841\u201331854,\n2022.\n[18] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable\nand generative vector representation for objects. In Computer Vision\u2013ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part\nVI 14, pages 484\u2013499. Springer, 2016.\n[19] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. A\npapier-m\u02c6ach\u00b4e approach to learning 3d surface generation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 216\u2013224, 2018.\n[20] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O\u02d8guz. 3dgen: Triplane latent\ndiffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023.\n[21] Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel,\nAndrea Vedaldi, and David Novotny. Unsupervised learning of 3d object categories from videos\nin the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4700\u20134709, 2021.\n[22] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu.\nAvatarclip: Zero-shot text-driven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022.\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu\nWang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021.\n[24] Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, and James M Rehg. Planes vs.\nchairs: Category-guided 3d shape learning without any 3d cues. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part I,\npages 727\u2013744. Springer, 2022.\n[25] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot\ntext-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 867\u2013876, 2022.\n[26] Wonbong Jang and Lourdes Agapito. Codenerf: Disentangled neural radiance fields for object\ncategories. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 12949\u201312958, 2021.\n[27] Nikolay Jetchev. Clipmatrix: Text-controlled creation of 3d textured meshes. arXiv preprint\narXiv:2109.12922, 2021.\n15\n[28] Mohammad Mahdi Johari, Yann Lepoittevin, and Franc\u00b8ois Fleuret. Geonerf: Generalizing nerf\nwith geometry priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18365\u201318375, 2022.\n[29] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv\npreprint arXiv:2305.02463, 2023.\n[30] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Learning category-\nspecific mesh reconstruction from image collectionsgirdhar2016learning. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pages 371\u2013386, 2018.\n[31] Nasir Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Text to mesh without 3d\nsupervision using limit subdivision. arXiv preprint arXiv:2203.13333, 2022.\n[32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n[33] Jon\u00b4a\u02c7s Kulh\u00b4anek, Erik Derner, Torsten Sattler, and Robert Babu\u02c7ska. Viewformer: Nerf-free\nneural rendering from few images using transformers. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XV, pages\n198\u2013216. Springer, 2022.\n[34] Han-Hung Lee and Angel X Chang. Understanding pure clip guidance for voxel grid nerf\nmodels. arXiv preprint arXiv:2209.15172, 2022.\n[35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d\ncontent creation. arXiv preprint arXiv:2211.10440, 2022.\n[36] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl\nVondrick. Zero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328,\n2023.\n[37] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei\nZhou, and Wenping Wang. Neural rays for occlusion-aware image-based rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n7824\u20137833, 2022.\n[38] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-Wing Fu. Iss: Image as stetting stone\nfor text-guided 3d shape generation. arXiv preprint arXiv:2209.04145, 2022.\n[39] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-Wing Fu. Iss++: Image as stepping\nstone for text-guided 3d shape generation. arXiv preprint arXiv:2303.15181, 2023.\n[40] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. Sparseneus: Fast\ngeneralizable neural surface reconstruction from sparse views. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXII,\npages 210\u2013227. Springer, 2022.\n[41] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface\nconstruction algorithm. ACM siggraph computer graphics, 21(4):163\u2013169, 1987.\n[42] Oier Mees, Maxim Tatarchenko, Thomas Brox, and Wolfram Burgard. Self-supervised 3d shape\nand viewpoint estimation from single images for robotics. In 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages 6083\u20136089. IEEE, 2019.\n[43] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Realfusion: 360\n{\\deg} reconstruction of any object from a single image. arXiv preprint arXiv:2302.10663,\n2023.\n[44] Luke Melas-Kyriazi, Christian Rupprecht, and Andrea Vedaldi. pc2: Projection\u2014conditioned\npoint cloud diffusion for single-image 3d reconstruction. arXiv preprint arXiv:2302.10668,\n2023.\n[45] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.\nOccupancy networks: Learning 3d reconstruction in function space. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 4460\u20134470, 2019.\n[46] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022.\n[47] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-\ndriven neural stylization for meshes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13492\u201313502, 2022.\n[48] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoor-\nthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis.\nCommunications of the ACM, 65(1):99\u2013106, 2021.\n[49] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors\nfor 3d completion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference\n16\non Computer Vision and Pattern Recognition, pages 306\u2013315, 2022.\n[50] Norman M\u00a8uller, Andrea Simonelli, Lorenzo Porzi, Samuel Rota Bul`o, Matthias Nie\u00dfner, and\nPeter Kontschieder. Autorf: Learning 3d object radiance fields from single view observations.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 3971\u20133980, 2022.\n[51] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive\ngenerative model of 3d meshes. In International conference on machine learning, pages\n7220\u20137229. PMLR, 2020.\n[52] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A\nsystem for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751,\n2022.\n[53] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.\nDeepsdf: Learning continuous signed distance functions for shape representation. In Proceed-\nings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165\u2013174,\n2019.\n[54] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman,\nDimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body\nfrom a single image. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 10975\u201310985, 2019.\n[55] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using\n2d diffusion. arXiv preprint arXiv:2209.14988, 2022.\n[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021.\n[57] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran\nZada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven\ntext-to-3d generation. arXiv preprint arXiv:2303.13508, 2023.\n[58] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[59] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, pages 8821\u20138831. PMLR, 2021.\n[60] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and\nDavid Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d\ncategory reconstruction. In International Conference on Computer Vision, 2021.\n[61] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut,\nand David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life\n3d category reconstruction. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10901\u201310911, 2021.\n[62] Yufan Ren, Fangjinhua Wang, Tong Zhang, Marc Pollefeys, and Sabine S\u00a8usstrunk. Volrecon:\nVolume rendering of signed ray distance functions for generalizable multi-view reconstruction.\narXiv preprint arXiv:2212.08067, 2022.\n[63] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture:\nText-guided texturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023.\n[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022.\n[66] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao\nLi. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In\nProceedings of the IEEE/CVF international conference on computer vision, pages 2304\u20132314,\n2019.\n[67] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero,\nand Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n18603\u201318613, 2022.\n17\n[68] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-\nHwa Kim, Jiyoung Lee, and Seungryong Kim. Let 2d diffusion model know 3d-consistency for\nrobust text-to-3d generation. arXiv preprint arXiv:2303.07937, 2023.\n[69] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free\nlocal feature matching with transformers. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8922\u20138931, 2021.\n[70] Alex Trevithick and Bo Yang. Grf: Learning a general radiance field for 3d representation and\nrendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 15182\u201315192, 2021.\n[71] Mukund Varma, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, and\nZhangyang Wang. Is attention all that nerf needs? In The Eleventh International Conference on\nLearning Representations, 2022.\n[72] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score\njacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. arXiv preprint\narXiv:2212.00774, 2022.\n[73] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh:\nGenerating 3d mesh models from single rgb images. In Proceedings of the European conference\non computer vision (ECCV), pages 52\u201367, 2018.\n[74] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang.\nNeus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021.\n[75] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T\nBarron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning\nmulti-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4690\u20134699, 2021.\n[76] Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, and Kim-Hui Yap. Taps3d: Text-guided\n3d textured shape generation from pseudo supervision, 2023.\n[77] Chao Wen, Yinda Zhang, Zhuwen Li, and Yanwei Fu. Pixel2mesh++: Multi-view 3d mesh\ngeneration via deformation. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 1042\u20131051, 2019.\n[78] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari.\nMultiview compressive coding for 3d reconstruction. arXiv preprint arXiv:2301.08247, 2023.\n[79] Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill Freeman, and Josh Tenenbaum.\nMarrnet: 3d shape reconstruction via 2.5 d sketches. Advances in neural information processing\nsystems, 30, 2017.\n[80] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. Pix2vox:\nContext-aware 3d reconstruction from single and multi-view images. In Proceedings of the\nIEEE/CVF international conference on computer vision, pages 2690\u20132698, 2019.\n[81] Haozhe Xie, Hongxun Yao, Shengping Zhang, Shangchen Zhou, and Wenxiu Sun. Pix2vox++:\nMulti-scale context-aware 3d object reconstruction from single and multiple images. Interna-\ntional Journal of Computer Vision, 128(12):2919\u20132935, 2020.\n[82] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-\n360: Lifting an in-the-wild 2d photo to a 3d object with 360 {\\deg} views. arXiv preprint\narXiv:2211.16431, 2022.\n[83] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua\nGao. Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion\nmodels. arXiv preprint arXiv:2212.14704, 2022.\n[84] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep\nimplicit surface network for high-quality single-view 3d reconstruction. Advances in neural\ninformation processing systems, 32, 2019.\n[85] Farid Yagubbayli, Yida Wang, Alessio Tonioni, and Federico Tombari. Legoformer: Trans-\nformers for block-by-block multi-view 3d reconstruction. arXiv preprint arXiv:2106.12102,\n2021.\n[86] Daniel Yang, Tarik Tosun, Benjamin Eisner, Volkan Isler, and Daniel Lee. Robotic grasping\nthrough combined image-based grasp proposal and 3d reconstruction. In 2021 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA), pages 6350\u20136356. IEEE, 2021.\n[87] Hao Yang, Lanqing Hong, Aoxue Li, Tianyang Hu, Zhenguo Li, Gim Hee Lee, and Liwei Wang.\nContranerf: Generalizable neural radiance fields for synthetic-to-real novel view synthesis via\ncontrastive learning. arXiv preprint arXiv:2303.11052, 2023.\n[88] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Foldingnet: Point cloud auto-encoder via\ndeep grid deformation. In Proceedings of the IEEE conference on computer vision and pattern\n18\nrecognition, pages 206\u2013215, 2018.\n[89] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields\nfrom one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 4578\u20134587, 2021.\n[90] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and\nKarsten Kreis. Lion: Latent point diffusion models for 3d shape generation. arXiv preprint\narXiv:2210.06978, 2022.\n[91] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels, 2023.\n[92] Xiaoshuai Zhang, Sai Bi, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Nerfusion: Fusing\nradiance fields for large-scale scene reconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5449\u20135458, 2022.\n[93] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for\n3d reconstruction. In CVPR, 2023.\n[94] Silvia Zuffi, Angjoo Kanazawa, and Michael J Black. Lions and tigers and bears: Capturing non-\nrigid, 3d, articulated shape from images. In Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pages 3955\u20133963, 2018.\n[95] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 3d menagerie: Modeling\nthe 3d shape and pose of animals. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 6365\u20136373, 2017.\n19\n"
  },
  {
    "title": "DreamDiffusion: Generating High-Quality Images from Brain EEG Signals",
    "link": "https://arxiv.org/pdf/2306.16934.pdf",
    "upvote": "29",
    "text": "DreamDiffusion: Generating High-Quality Images from Brain EEG Signals\nYunpeng Bai1, Xintao Wang2, Yan-Pei Cao2, Yixiao Ge2, Chun Yuan1,3, Ying Shan2\n1 Tsinghua Shenzhen International Graduate School,\n2Tencent AI Lab, 3Peng Cheng Laboratory\nGenerated Images\nEEG Signals\nDream\nDiffusion\nFigure 1. Our proposed DreamDiffusion is capable of generating high-quality images directly from brain electroencephalogram (EEG)\nsignals, without the need to translate thoughts into text.\nAbstract\nThis paper introduces DreamDiffusion, a novel method\nfor generating high-quality images directly from brain elec-\ntroencephalogram (EEG) signals, without the need to trans-\nlate thoughts into text.\nDreamDiffusion leverages pre-\ntrained text-to-image models and employs temporal masked\nsignal modeling to pre-train the EEG encoder for effective\nand robust EEG representations. Additionally, the method\nfurther leverages the CLIP image encoder to provide ex-\ntra supervision to better align EEG, text, and image em-\nbeddings with limited EEG-image pairs. Overall, the pro-\nposed method overcomes the challenges of using EEG sig-\nnals for image generation, such as noise, limited informa-\ntion, and individual differences, and achieves promising\nresults.\nQuantitative and qualitative results demonstrate\nthe effectiveness of the proposed method as a significant\nstep towards portable and low-cost \u201cthoughts-to-image\u201d,\nwith potential applications in neuroscience and computer\nvision. The code is available here https://github.\ncom/bbaaii/DreamDiffusion.\n1. Introduction\nImage generation [16, 22, 4] has made great strides in\nrecent years, especially after breakthroughs in text-to-image\ngeneration [31, 12, 30, 34, 1]. The recent text-to-image gen-\neration not only dramatically improves the quality of gener-\nated images, but also enables the creation of people\u2019s ideas\ninto exquisite paintings and artworks controlled by text. We\nare very curious whether we could control image creation\ndirectly from brain activities (such as electroencephalogram\n(EEG) recordings), without translating our thoughts into\ntext before creation. This kind of \u201cthoughts-to-images\u201d has\nbroad prospects and could broaden people\u2019s imagination.\nFor example, it can greatly improve the efficiency of artistic\ncreation and help capture those fleeting inspirations. It also\nhas the potential to help us visualize our dreams at night,\n(which inspires the name DreamDiffusion). Moreover, it\nmay even aid in psychotherapy, having the potential to help\nchildren with autism and those with language disabilities.\nSome recent works, such as MinD-Vis [7] and [40],\nattempt to reconstruct visual information based on fMRI\n(functional Magnetic Resonance Imaging) signals, which is\nanother way to measure brain activities. They have demon-\nstrated the feasibility of reconstructing high-quality results\nfrom brain activities. However, they are still far away from\n1\narXiv:2306.16934v2  [cs.CV]  30 Jun 2023\nour goal of using brain signals to create conveniently and\nefficiently. 1) Since fMRI equipment is not portable and\nneeds to be operated by professionals, it is difficult to cap-\nture fMRI signals. 2) The cost of fMRI acquisition is high.\nThey greatly hinder the widespread use of this method in\nthe practical artistic generation.\nIn contrast, EEG (elec-\ntroencephalogram) is a non-invasive and low-cost method\nof recording electrical activity in the brain. Portable com-\nmercial products are now available for the convenient ac-\nquisition of EEG signals, showing great potential for future\nart generation.\nIn this work, we aim to leverage the powerful generative\ncapabilities of pre-trained text-to-image models (i.e., Sta-\nble Diffusion [32]) to generate high-quality images directly\nfrom brain EEG signals. However, this is non-trivial and has\ntwo challenges. 1) EEG signals are captured non-invasively\nand thus are inherently noisy. In addition, EEG data are lim-\nited and individual differences cannot be ignored. How to\nobtain effective and robust semantic representations from\nEEG signals with so many constraints? 2) Thanks to the\nuse of CLIP [28] and the training on a large number of text-\nimage pairs, the text and image spaces in Stable Diffusion\nare well aligned. However, the EEG signal has its own char-\nacteristics, and its space is quite different from that of text\nand image. How to align EEG, text and image spaces with\nlimited and noisy EEG-image pairs?\nTo address the first challenge, we propose to train EEG\nrepresentations using large amounts of EEG data instead of\nonly rare EEG-image pairs. Specifically, we adopt masked\nsignal modeling to predict the missing tokens based on con-\ntextual cues. Different from MAE [18] and MinD-Vis [7],\nwhich treat inputs as two-dimensional images and mask the\nspatial information, we consider the temporal characteris-\ntics of EEG signals, and dig deep into the semantics behind\ntemporal changes in people\u2019s brains. We randomly mask\na proportion of tokens and then reconstruct those masked\nones in the time domain. In this way, the pre-trained en-\ncoder learns a deep understanding of EEG data across dif-\nferent people and various brain activities.\nAs for the second challenge, previous methods [40, 7]\nusually directly fine-tune Stable Diffusion (SD) models us-\ning a small number of noisy data pairs.\nHowever, it is\ndifficult to learn accurate alignment between brain signals\n(e.g., EEG and fMRI) and text spaces by end-to-end fine-\ntuning SD only using the final image reconstruction loss.\nWe thus propose to employ additional CLIP [28] super-\nvision to assist in the alignment of EEG, text, and image\nspaces. Specifically, SD itself uses CLIP\u2019s text encoder to\ngenerate text embeddings, which are quite different from\nthe masked pre-trained EEG embeddings in the previous\nstage. We leverage CLIP\u2019s image encoder to extract rich\nimage embeddings that align well with CLIP text embed-\ndings. Those CLIP image embeddings are then used to fur-\nther optimize EEG embedding representations. Therefore,\nthe refined EEG feature embeddings can be well aligned\nwith the CLIP image and text embeddings, and are more\nsuitable for SD image generation, which in turn improves\nthe quality of generated images.\nEquipped with the above two delicate designs, our pro-\nposed method, namely, DreamDiffusion, can generate high-\nquality and realistic images from EEG signals. Our con-\ntributions can be summarized as follows. 1) We propose\nDreamDiffusion, which leverages the powerful pre-trained\ntext-to-image diffusion models to generate realistic images\nfrom EEG signals only. It is a further step towards portable\nand low-cost \u201cthoughts-to-images\u201d. 2) A temporal masked\nsignal modeling is employed to pre-train EEG encoder for\neffective and robust EEG representations. 3) We further\nleverage the CLIP image encoder to provide extra super-\nvision to better align the EEG, text, and image embeddings\nwith limited EEG-image pairs. 4) Quantitative and qualita-\ntive results have shown the effectiveness of our DreamDif-\nfusion.\n2. Related Works\n2.1. Generating images from brain activity\nThe use of brain signals, including fMRI and EEG, to\ngenerate images has been an active area of research. For the\nuse of fMRI, traditional methods rely on fMRI-image paired\ndata to train the model to predict image features from fMRI.\nThese image features will be fed into GANs [36] for stim-\nulus reconstruction during testing. However, recent stud-\nies [3] have proposed unsupervised approaches, such as a\nreconfigurable autoencoder design, to learn from unpaired\nfMRI and images, and utilize regression models [25, 27] to\nextract a latent fMRI representation that can be used to fine-\ntune a pre-trained conditional BigGAN [5] for decoding.\nThe recent work MinD-Vis [8] integrates SC-MBM and\nDC-LDM to generate more plausible images with better-\npreserved semantic information.\nSimilarly, generating images from EEG signals has\nalso\nbeen\nexplored\nusing\ndeep\nlearning\ntechniques.\nBrain2image [23] have developed using LSTM and genera-\ntive methods to learn a more compact representation of EEG\ndata for generating visual stimuli that evoke specific brain\nresponses. ThoughtViz [41] takes encoded EEG signals as\ninput to generate corresponding images, even with limited\ntraining data. [9] uses EEG as a supervision signal for learn-\ning semantic feature representations and achieving compa-\nrable performance to semantic image editing. Overall, these\napproaches demonstrate the potential of using brain signals\nto generate images and advance the field of brain-computer\ninterfaces.\nFigure 2. Overview of DreamDiffusion. Our method comprises three main components: 1) masked signal pre-training for an effective\nand robust EEG encoder, 2) fine-tuning with limited EEG-image pairs with pre-trained Stable Diffusion, and 3) aligning the EEG, text, and\nimage spaces using CLIP encoders.\n2.2. Model pre-training\nPre-training models have become increasingly popular\nin the field of computer vision, with various self-supervised\nlearning approaches focusing on different pretext tasks [13,\n43, 26].\nThese methods often utilize pretext tasks such\nas contrastive learning [2, 17], which models image simi-\nlarity and dissimilarity, or autoencoding [6], which recov-\ners the original data from a masked portion. In particu-\nlar, masked signal modeling (MSM) has been successful\nin learning useful context knowledge for downstream tasks\nby recovering the original data from a high mask ratio for\nvisual signals [18, 44] and a low mask ratio for natural\nlanguages [10, 29]. Another recent approach, CLIP [28],\nbuilds a multi-modal embedding space by pre-training on\n400 million text-image pairs collected from various sources\non the Internet. The learned representations by CLIP are ex-\ntremely powerful, enabling state-of-the-art zero-shot image\nclassification on multiple datasets, and providing a method\nto estimate the semantic similarity between text and images.\n2.3. Diffusion models\nDiffusion models have become increasingly popular as\ngenerative models for producing high-quality content [37].\nThe basic form of diffusion models is a probabilistic model\ndefined by a bi-directional Markov Chain of states [19].\nThese models [11, 19, 33, 39] exhibit strong generative\npower due to their natural fit with the inductive biases of\nimage-like data.\nThe best synthesis quality is typically\nachieved when using a reweighted objective during train-\ning [19], allowing for a trade-off between image quality and\ncompression capabilities. However, evaluating and optimiz-\ning these models in pixel space is computationally expen-\nsive and time-consuming [24, 35, 38, 20, 42].\nTo address these challenges, some diffusion models\nwork on a compressed latent space of lower dimensional-\nity, such as the proposed LDMs [32]. By compressing im-\nages into lower-dimensional latent features using a Vector\nQuantization (VQ) [15] regularized autoencoder and then\nreconstructing them using the same latent space features,\nthe LDM reduces computational costs while maintaining\nsynthesis quality.\nAdditionally, a UNet-based denoising\nmodel with attention modules offers the flexibility to condi-\ntion image generation through key/value/query vectors dur-\ning Markov Chain transitions. This approach has several\nadvantages, including reduced computational costs and bet-\nter quality of image synthesis.\n3. Proposed Method\nOur method comprises three main components:\n1)\nmasked signal pre-training for an effective and robust EEG\nencoder, 2) fine-tuning with limited EEG-image pairs with\npre-trained Stable Diffusion, and 3) aligning the EEG, text,\nand image spaces using CLIP encoders. Firstly, we lever-\nage masked signal modeling with lots of noisy EEG data to\ntrain an EEG encoder to extract contextual knowledge. The\nresulting EEG encoder is then employed to provide condi-\nFigure 3. Masked signals modeling with large-scale noisy EEG\ndata. We visualize the reconstruction results of one channel from\nthe EEG data. We can observe that the overall trend is accurate,\nbut the details are influenced by the dataset, as the EEG signals in\nthese datasets are relatively noisy.\ntional features for Stable Diffusion via the cross-attention\nmechanism. In order to enhance the compatibility of EEG\nfeatures with Stable Diffusion, we further align the EEG,\ntext, and image embedding spaces by reducing the distance\nbetween EEG embeddings and CLIP image embeddings\nduring fine-tuning. As a result, we can obtain DreamDif-\nfusion, which is capable of generating high-quality images\nfrom EEG signals only.\n3.1. Masked signal pre-training for effective and\nrobust EEG representations\nEEG (Electroencephalogram) data is a recording of elec-\ntrical activity generated by the human brain, measured us-\ning electrodes placed on the scalp. It is a non-invasive and\nlow-cost method of measuring brain activity. EEG data has\nseveral characteristics. Firstly, the data is two-dimensional,\nwith one dimension representing the channels or electrodes\nplaced on the scalp, and the other dimension representing\ntime. The temporal resolution of EEG is high, meaning that\nit can capture rapid changes in brain activity that occur on\nthe order of milliseconds. However, the spatial resolution\nof EEG is low, meaning that it is difficult to precisely lo-\ncalize the source of the activity within the brain. Secondly,\nEEG signals are highly variable, influenced by factors such\nas age, sleep, and cognitive state. Finally, EEG data is of-\nten noisy, and requires careful processing and analysis to\nextract meaningful information.\nDue to the inherent variability and noise in EEG data,\nconventional modeling methods often struggle to extract\nmeaningful information from EEG signals. Consequently,\nadopting masked signal modeling techniques, which have\nbeen proven effective in capturing contextual information\nfrom noisy and variable data [18, 7], represents a promis-\ning avenue for deriving meaningful contextual knowledge\nfrom large-scale noisy EEG data. Different from MAE [18]\nand MinD-Vis [7], which treat inputs as two-dimensional\nimages and mask the spatial information, we consider the\ntemporal characteristics of EEG signals, and dig deep into\nthe semantics behind temporal changes in people\u2019s brains.\nGiven the high temporal resolution of EEG signals, we\nfirst divide them into tokens in the time domain, and ran-\ndomly mask a certain percentage of tokens. Subsequently,\nthese tokens will be transformed into embeddings by us-\ning a one-dimensional convolutional layer. Then, we use\nan asymmetric architecture such as MAE [18] to predict the\nmissing tokens based on contextual cues from the surround-\ning tokens. Through reconstructing the masked signals, the\npre-trained EEG encoder learns a deep understanding of\nEEG data across different people and various brain activ-\nities.\n3.2. Fine-tuning with Stable Diffusion on limited\nEEG-image pairs\nAfter obtaining an effective representation of EEG sig-\nnals from masked signal pre-training, we utilize it to gen-\nerate images by leveraging a pre-trained Stable Diffusion\n(SD) model. Stable Diffusion involves gradually denois-\ning a normally distributed variable to learn a data distribu-\ntion. SD is augmented with a cross-attention mechanism\nfor more flexible conditional image generation and the most\ncommon condition is the text prompt. Stable Diffusion has\nshown great generative power in generating high-quality\nimages from various types of signals, such as labels, text,\nand semantic maps.\nStable Diffusion operates on the latent space. Given an\nimage x in pixel space, x is encoded by a VQ encoder\nE(\u00b7) to obtain the corresponding latent z = E(x). Condi-\ntional signals are introduced by the cross-attention mecha-\nnism in the UNet. This cross-attention can also incorporate\nconditional information from the EEG data. Specifically,\nthe output of EEG encoder y is further projected with a\nprojector \u03c4\u03b8 into an embedding \u03c4\u03b8(y) \u2208 RM\u00d7d\u03c4 . Then,\nthis EEG representation is incorporated into U-Net by a\ncross-attention layer implementing Attention(Q, K, V ) =\nsoftmax\n\u0010\nQKT\n\u221a\nd\n\u0011\n\u00b7 V .\nQ = W (i)\nQ \u00b7 \u03c6i (zt) , K = W (i)\nK \u00b7 \u03c4\u03b8(y), V = W (i)\nV\n\u00b7 \u03c4\u03b8(y),\n(1)\nwhere \u03c6i (zt) \u2208 RN\u00d7di\ne denotes intermediate values of\nthe U-Net. W (i)\nV\n\u2208 Rd\u00d7di\n\u03f5, W (i)\nQ\n\u2208 Rd\u00d7d\u03c4 and W (i)\nK\n\u2208\nRd\u00d7d\u03c4 are projection matrices with learnable parameters.\nFigure 4. Main results. The images on the left depict paired image data, while the three images on the right represent the sampling results.\nIt can be observed that our model generates images of high quality from the EEG data, and these images match the EEG data accurately.\nDuring the fine-tuning process, we optimize the EEG en-\ncoder and cross-attention heads of the U-Net together. We\nkeep the remaining parts of Stable Diffusion fixed. We use\nthe following SD loss function for fine-tuning.\nLSD = Ex,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (xt, t, \u03c4\u03b8(y))\u22252\n2\ni\n,\n(2)\nwhere \u03f5\u03b8 is the denoising function implemented as UNet.\n3.3. Aligning the EEG, text, and image spaces with\nCLIP encoders\nNext, we will fine-tune the EEG representation obtained\nfrom pre-training to make it more suitable for generating\nimages. The pre-trained Stable Diffusion model is specif-\nically trained for text-to-image generation; however, the\nEEG signal has its own characteristics, and its latent space\nis quite different from that of text and image. Therefore,\ndirectly fine-tuning the Stable Diffusion model end-to-end\nusing limited EEG-image paired data is unlikely to accu-\nrately align the EEG features with existing text embedding\nin pre-trained SD.\nThanks to the use of CLIP [28] and the training on a\nlarge number of text- image pairs, the text and image spaces\nin Stable Diffusion are well aligned. Therefore, we pro-\npose to employ additional CLIP [28] supervision to assist in\nthe alignment of EEG, text, and image space. Specifically,\nthe EEG features obtained from the pre-trained encoder are\nFigure 5. Comparison with Brain2Image. The quality of the generated images produced by DreamDiffusion is significantly higher than\nthose generated by Brain2Image.\ntransformed into embeddings with the same dimension as\nthose of CLIP through a projection layer. We then use a\nloss function to minimize the distance between the EEG\nembeddings and the image embeddings obtained from the\nCLIP image encoder. The CLIP model is fixed during the\nfine-tuning process. The loss function is defined as follows:\nLclip = 1 \u2212 EI(I) \u00b7 h(\u03c4\u03b8(y))\n|EI(I)||h(\u03c4\u03b8(y))|,\n(3)\nwhere h is a projection layer and EI is the CLIP image en-\ncoder. This loss function can encourage the EEG features\nto become more closely aligned with the image and thus\nmore similar to text features. In this way, we can align the\nEEG signal, text and image in one unified space. The opti-\nmized EEG embedding representation is more suitable for\nSD image generation, which in turn improves the quality of\ngenerated images.\n4. Experiments and Analyses\n4.1. Implementation details\nData for EEG representation pre-training. We have col-\nlected approximately 120,000 EEG data samples from over\n400 subjects with channel ranges from 30 to 128 on the\nMOABB [21] platform for the EEG pre-training. MOABB\nis a software package designed to facilitate the develop-\nment of brain-computer interface (BCI) algorithms by pro-\nviding a collection of publicly available EEG datasets in a\ncommon format, along with a suite of state-of-the-art algo-\nrithms. This platform enables researchers to easily validate\nnew algorithms using automated statistical analysis, elimi-\nnating the need for time-consuming and unreliable data pre-\nprocessing. These data contain a wide variety of EEG data,\nincluding tasks such as looking at an object, motor imagery,\nand watching videos. Our goal is to learn universal repre-\nsentations from diverse EEG data, without specific require-\nments on the types of EEG data. Due to variations in the\nequipment used for data acquisition, the channel counts of\nModel\nMSM Pretraining\nCLIP Finetuning\nMask Ratio\nE + A\nParams\nAcc (%)\nFull\n!\n!\n0.75\nE + A\n297M\n45.8\n1\n%\n%\n-\nE + A\n297M\n4.2\n2\n%\n%\n-\nE + A\n18.3M\n3.7\n3\n%\n!\n-\nE + A\n297M\n32.3\n4\n%\n!\n-\nE + A\n18.3M\n24.5\n5\n!\n!\n0.25\nE + A\n297M\n19.7\n6\n!\n!\n0.5\nE + A\n297M\n38.3\n7\n!\n!\n0.85\nE + A\n297M\n33.4\n8\n!\n!\n0.75\nE + A\n458M\n38.5\n9\n!\n!\n0.75\nE + A\n162M\n36.6\n10\n!\n!\n0.75\nE + A\n74M\n29.8\n11\n!\n!\n0.75\nE + A\n18.3M\n28.7\n12\n!\n!\n0.75\nE only\n297M\n22.4\n13\n!\n%\n0.75\nE + A\n297M\n28.3\n14\n!\n%\n0.75\nA only\n297M\n20.9\nTable 1. Quantitative results of ablation studies. E and A represent fine-tuning of the encoder and cross-attention heads, respectively.\nthese EEG data samples differ significantly. To facilitate\npre-training, we have uniformly padded all the data to 128\nchannels by filling missing channels with replicated values.\nDuring the pre-training process, every 4 adjacent time steps\nare grouped into a token and each token is transformed into\na 1024-dimensional embedding through a projection layer\nfor subsequent masked signal modeling. The loss function\ncalculates the MSE between the reconstructed and original\nEEG signals. The loss is only computed on masked patches.\nThe reconstruction is performed on the entire set of 128\nchannels as a whole, rather than on a per-channel basis. The\ndecoder is discarded after pretraining.\nPaired EEG-image data.\nWe adopt the ImageNet-\nEEG [23] dataset for our \u201cthoughts-to-image\u201d experiments,\nwhich is a collection of EEG recordings obtained from 6\nsubjects while they were shown 2000 images belonging\nto 40 different categories of objects from the ImageNet\ndataset. Each category consisted of 50 images, and each im-\nage was presented for 0.5 seconds, followed by a 10-second\npause for every 50 images. The EEG data were recorded\nusing a 128-channel Brainvision EEG system, resulting in\na total of 12000 128-channel EEG sequences. The dataset\nincludes images of various objects, such as animals (dogs,\ncats, elephants, etc.), vehicles (airliners, bikes, cars, etc.),\nand everyday objects (computers, chairs, mugs, etc.). More\ndetails can be found in the related reference [23].\nOther implementation details. We use version 1.5 of Sta-\nble Diffusion for image generation. The mask ratio for EEG\nsignals is set to 75%. All EEG signals are filtered within\nthe frequency range of 5-95 Hz before pretraining. Sub-\nsequently, the signals are truncated to a common length\nof 512.\nThe encoder is pre-trained for 500 epochs and\nfinetuned with Stable Diffusion for another 300. The pre-\ntraining model for EEG is similar to ViT-Large in [14]. The\ntraining and testing were conducted on the same subject,\nand all results presented in the paper were generated using\ndata from Subject 4.\n4.2. Comparison with Brain2Image [23]\nIn this section, we present a comparison of our pro-\nposed approach with Brain2Image [23], a recent work that\nemploys conventional generative models, i.e., variational\nautoencoders (VAE) and generative adversarial networks\n(GAN), to achieve EEG-to-images. Brain2Image, however,\npresents results for only a few categories and does not pro-\nvide a reference implementation. In light of this, we con-\nducted a qualitative comparison of the results on a few cate-\ngories (namely, Airliner, Jack-o-Lantern, and Panda) that\nwere showcased in the Brain2Image paper.\nTo ensure a\nfair comparison, we followed the same subjective evalua-\ntion strategy as outlined by Brain2Image and presented gen-\nerated instances of different methods in Figure 5. The top\nrows depict the results generated by Brain2Image, whereas\nthe bottom rows were generated by our proposed method,\nDreamDiffusion. We observed that the quality of the gen-\nerated images produced by DreamDiffusion is significantly\nhigher than those generated by Brain2Image, thus validat-\ning the efficacy of our proposed method.\n4.3. Ablation studies\nIn this section, we conduct several ablation studies on\nthe proposed framework using various cases. We evaluate\nthe effectiveness of different methods by employing a 50-\nway top-1 accuracy classification task. We use a pre-trained\nImageNet1K classifier [14] to determine the semantic cor-\nrectness of the generated images. Both the ground-truth and\ngenerated images will be inputted into the classifier. Then,\nFigure 6. Qualitative results of ablation studies.\nwe will verify whether the top-1 classification of the gen-\nerated image matches the ground-truth classification in 50\nselected classes. A generated image will be deemed correct\nas long as the semantic classification results of the gener-\nated image and the ground-truth are consistent.\nRole of pre-training. To demonstrate the effectiveness of\nthe pretraining with large-scale EEG data, we conduct a val-\nidation by training several models with untrained encoders.\nOne of the models is identical to the full model, while the\nother model has a shallow EEG encoding layer with only\ntwo layers to avoid overfitting the data. During the training\nprocess, the two models were trained with and without clip\nsupervision, and the results are shown in Table 1, Model 1-\n4. It can be observed that the accuracy of the model without\npre-training decreased.\nMask ratios. We investigate to determine the optimal mask\nratio for MSM pretraining with EEG data. As shown in\nModel 5-7 of Table 1, excessively high or low mask ratios\ncan have a detrimental effect on the model\u2019s performance.\nThe highest overall accuracy was achieved at a mask ratio\nof 0.75. This finding is significant as it suggests that, un-\nlike natural language processing where low mask ratios are\ncommonly used, a high mask ratio is also a preferable op-\ntion when performing MSM on EEG.\nCLIP aligning. One of the keys of our method is to align\nthe EEG representation with the image through the CLIP\nencoder. To validate the effectiveness of this approach, we\nconducted experiments 13-14 as shown in Table 1. It can\nbe observed that the performance of the model significantly\ndecreases when CLIP supervision is not used. In fact, as\nshown in the bottom right corner of Figure 6, even in the\nabsence of pre-training, using CLIP to align EEG features\ncan still yield reasonable results, which highlights the im-\nportance of CLIP supervision in our method.\nFigure 7. Failure cases of DreamDiffusion.\n5. Conclusion\nThis paper proposes a novel method, DreamDiffusion,\nfor generating high-quality images from EEG signals,\nwhich is a non-invasive and easily obtainable source of\nbrain activity. The proposed method addresses the chal-\nlenges associated with EEG-based image generation by\nutilizing the knowledge learned from large EEG datasets\nand the powerful generative capabilities of image diffusion\nmodels.\nThrough a pre-training and fine-tuning scheme,\nEEG data can be encoded to the representation suitable for\nimage generation using Stable Diffusion. Our method rep-\nresents a significant advancement in the field of image gen-\neration from brain activity.\nLimitations.\nCurrently, EEG data only provide coarse-\ngrained information at the category level in experimental\nresults. Figure 7 shows some failure cases, where some cat-\negories are mapped to other categories with similar shapes\nor colors. We assume this may be due to the fact that the\nhuman brain considers shape and color as two important\nfactors when recognizing objects. Nevertheless, DreamD-\niffusion has the potential to be used in a wide range of ap-\nplications, such as neuroscience, psychology, and human-\ncomputer interaction.\nReferences\n[1] Yunpeng Bai, Cairong Wang, Shuzhao Xie, Chao Dong,\nChun Yuan, and Zhi Wang.\nTextir: A simple framework\nfor text-based editable image restoration.\narXiv preprint\narXiv:2302.14736, 2023.\n[2] Suzanna Becker and Geoffrey E Hinton.\nSelf-organizing\nneural network that discovers surfaces in random-dot stere-\nograms. Nature, 355(6356):161\u2013163, 1992.\n[3] Chris M Bird, Samuel C Berens, Aidan J Horner, and\nAnna Franklin.\nCategorical encoding of color in the\nbrain.\nProceedings of the National Academy of Sciences,\n111(12):4590\u20134595, 2014.\n[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018.\n[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale GAN training for high fidelity natural image synthe-\nsis. In 7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020.\n[7] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and\nJuan Helen Zhou. Seeing beyond the brain: Conditional dif-\nfusion model with sparse masked modeling for vision decod-\ning. arXiv preprint arXiv:2211.06956, 2022.\n[8] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and\nJuan Helen Zhou. Seeing beyond the brain: Masked model-\ning conditioned diffusion model for human vision decoding.\nIn arXiv, November 2022.\n[9] Keith M Davis, Carlos de la Torre-Ortiz, and Tuukka Ruot-\nsalo.\nBrain-supervised image editing.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18480\u201318489, 2022.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018.\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Informa-\ntion Processing Systems, 34:8780\u20138794, 2021.\n[12] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via hi-\nerarchical transformers. arXiv preprint arXiv:2204.14217,\n2022.\n[13] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 1422\u20131430, 2015.\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873\u201312883, 2021.\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020.\n[17] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensional-\nity reduction by learning an invariant mapping. In 2006 IEEE\nComputer Society Conference on Computer Vision and Pat-\ntern Recognition (CVPR\u201906), volume 2, pages 1735\u20131742.\nIEEE, 2006.\n[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022.\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020.\n[20] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. J. Mach. Learn.\nRes., 23:47:1\u201347:33, 2022.\n[21] Vinay Jayaram and Alexandre Barachant. Moabb: trustwor-\nthy algorithm benchmarking for bcis. Journal of neural en-\ngineering, 15(6):066011, 2018.\n[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019.\n[23] Isaak Kavasidis, Simone Palazzo, Concetto Spampinato,\nDaniela Giordano, and Mubarak Shah. Brain2Image: Con-\nverting brain signals into images. In Proceedings of the 2017\nACM on Multimedia Conference, MM 2017, Mountain View,\nCA, USA, October 23-27, 2017, pages 1809\u20131817. ACM,\n2017.\n[24] Zhifeng Kong and Wei Ping. On fast sampling of diffusion\nprobabilistic models. CoRR, abs/2106.00132, 2021.\n[25] Milad Mozafari, Leila Reddy, and Rufin VanRullen. Recon-\nstructing natural scenes from fmri patterns using bigbigan.\nIn 2020 International joint conference on neural networks\n(IJCNN), pages 1\u20138. IEEE, 2020.\n[26] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In Com-\nputer Vision\u2013ECCV 2016: 14th European Conference, Am-\nsterdam, The Netherlands, October 11-14, 2016, Proceed-\nings, Part VI, pages 69\u201384. Springer, 2016.\n[27] Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila\nReddy, and Rufin VanRullen. Reconstruction of perceived\nimages from fmri patterns and semantic brain exploration us-\ning instance-conditioned gans. In 2022 International Joint\nConference on Neural Networks (IJCNN), pages 1\u20138. IEEE,\n2022.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015.\n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022.\n[35] Robin San-Roman,\nEliya Nachmani,\nand Lior Wolf.\nNoise estimation for generative diffusion models.\nCoRR,\nabs/2104.02600, 2021.\n[36] Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu\nHorikawa, and Yukiyasu Kamitani. End-to-end deep image\nreconstruction from human brain activity. Frontiers Comput.\nNeurosci., 13:21, 2019.\n[37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models.\nIn 9th International Con-\nference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[39] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions.\nIn 9th International Conference on Learning Rep-\nresentations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net, 2021.\n[40] Yu Takagi and Shinji Nishimoto. High-resolution image re-\nconstruction with latent diffusion models from human brain\nactivity. bioRxiv, pages 2022\u201311, 2022.\n[41] Praveen Tirupattur, Yogesh Singh Rawat, Concetto Spamp-\ninato, and Mubarak Shah.\nThoughtviz: Visualizing hu-\nman thoughts using generative adversarial network. In 2018\nACM Multimedia Conference on Multimedia Conference,\nMM 2018, Seoul, Republic of Korea, October 22-26, 2018,\npages 950\u2013958. ACM, 2018.\n[42] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based\ngenerative modeling in latent space. In Marc\u2019Aurelio Ran-\nzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang,\nand Jennifer Wortman Vaughan, editors, Advances in Neu-\nral Information Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pages 11287\u201311302,\n2021.\n[43] Xiaolong Wang and Abhinav Gupta. Unsupervised learn-\ning of visual representations using videos. In Proceedings of\nthe IEEE international conference on computer vision, pages\n2794\u20132802, 2015.\n[44] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9653\u20139663, 2022.\n"
  },
  {
    "title": "Generate Anything Anywhere in Any Scene",
    "link": "https://arxiv.org/pdf/2306.17154.pdf",
    "upvote": "20",
    "text": "Generate Anything Anywhere in Any Scene\nYuheng Li, Haotian Liu, Yangming Wen, Yong Jae Lee\nUniversity of Wisconsin\u2013Madison\nhttps://yuheng-li.github.io/PACGen/\nsks \nstuffed \nanimal\n\u201cA sks stuffed animal \nin watercolor style\u201d  \n\u201cA sks stuffed animal with \nautumn leaves\u201d\n\u201cA sks stuffed animal \nin a cyberpunk city\u201d\n\u201cA sks stuffed animal on branch \non the beach\u201d\nuser images\nsks \nstuffed \nanimal\nsks \nstuffed animal\nsks \nstuffed \nanimal\nsks \nstuffed \nanimal\n\u201cA sks stuffed animal \nin a drink bar\u201d\nsks \nstuffed \nanimal\n\u201cA sks stuffed animal in \npointillism style\u201d\nFigure 1: Given just a handful of user images (left), our model, PACGen, can generate the personalized\nconcept with both high fidelity and localization controllability in novel contexts (right).\nAbstract\nText-to-image diffusion models have attracted considerable interest due to their\nwide applicability across diverse fields. However, challenges persist in creating\ncontrollable models for personalized object generation. In this paper, we first\nidentify the entanglement issues in existing personalized generative models, and\nthen propose a straightforward and efficient data augmentation training strategy that\nguides the diffusion model to focus solely on object identity. By inserting the plug-\nand-play adapter layers from a pre-trained controllable diffusion model, our model\nobtains the ability to control the location and size of each generated personalized\nobject. During inference, we propose a regionally-guided sampling technique to\nmaintain the quality and fidelity of the generated images. Our method achieves\ncomparable or superior fidelity for personalized objects, yielding a robust, versatile,\nand controllable text-to-image diffusion model that is capable of generating realistic\nand personalized images. Our approach demonstrates significant potential for\nvarious applications, such as those in art, entertainment, and advertising design.\n1\nIntroduction\nThe task of generating and manipulating photo-realistic images from textual descriptions has garnered\nsignificant attention in recent years. Solutions to this problem offer a wide range of potential\napplications in art, advertising, entertainment, virtual reality, and more. Recently, diffusion models\n[27, 8, 28, 18, 21, 23, 25], together with cross-modal conditioning and large language models (e.g.,\nCLIP [19] and T5-XXL [20]), have contributed significant advancements to this area.\nWhile text-to-image diffusion models have significantly advanced both image realism and broadened\nthe conceptual scope of what can be generated, there are still ongoing challenges. In many practical\napplications, controllable models that can provide more fine-grained control beyond textual descrip-\ntions are highly sought after. For example, the freedom to generate any object in any location within\nPreprint. Under review.\narXiv:2306.17154v1  [cs.CV]  29 Jun 2023\nany scene is a desirable property. Such a model would enable a new degree of creative possibilities.\nFor instance, advertisement designers could seamlessly position their products in any desired spot in\nan ad; likewise, an ordinary person could exhibit their most treasured toy in any location within the\nmost prestigious showroom in Paris.\nResearchers have been working diligently to create such controllable diffusion models. One approach\nentails personalization, as seen in projects like Textual-Inversion [5], DreamBooth [24], and Custom\nDiffusion [12]. In these works, a pre-trained diffusion model is optimized or fine-tuned with a new\npersonalized visual concept based on a handful of images provided by a user, to connect the visual\nconcept to a unique word identifier. While these models can generate personalized items in any\nscene, they fall short in controlling the size and location of the generated objects. Another very\nrecent research direction investigates localization-controllable diffusion models [14, 33, 37, 17, 9, 1].\nIn particular, GLIGEN [14] integrates a new adapter layer into each Transformer block of a pre-\ntrained diffusion model to achieve size and location control. Although GLIGEN can generate objects\naccording to a user-specified size and location information (e.g., provided by a bounding box or\nkeypoints) and semantic category information provided via text, it cannot control the specific identity\nof the generated objects, rendering the model ineffective in dealing with personalized objects.\nTo tackle these challenges, in this paper, we propose a method, PACGen (Personalized and Con-\ntrollable Text-to-Image Generation), that combines the key advantages of both research directions,\nleading to a more robust, versatile, and controllable text-to-image diffusion model capable of generat-\ning realistic and personalized images. We first show that personalization efforts like DreamBooth [24],\nin the process of learning object identity, undesirably also entangle object identity with object location\nand size due to limited variability and quantity in the provided training data (e.g., the provided users\u2019\nimages are few and tend to be object-centric). This fundamental issue presents difficulties for location\nand size control in personalized generative models.\nTo address this issue, we propose a straightforward data augmentation technique. Specifically, when\nfinetuning the pretrained diffusion model for personalization, we randomly resize and reposition\nthe user provided training images in order to disentangle object identity from its size and location\nfactors. During inference, we can simply plug in a pre-trained GLIGEN\u2019s adapter layers to control\nthe object\u2019s size and location. This strategy enables PACGen to associate the word identifier solely\nwith the object\u2019s identity and allows it to control its size and location without any further finetuning.\nHowever, we find that some undesirable artifacts (like collaging effects) can be introduced from data\naugmentation. We thus also introduce a regionally-guided sampling technique to ensure the overall\nquality and fidelity of the generated images.\nIn sum, our main contributions are: 1) we identify and propose a solution to the entanglement\nissues in existing personalization efforts; 2) a novel text-to-image diffusion model that provides both\npersonalization and controllability; and 3) our quantitative and qualitative results demonstrate that\nPACGen matches or exceeds the high fidelity of existing personalized models, while also providing\nlocalization controllability which is lacking in existing personalized models.\n2\nRelated Work\nText-to-Image Generation Models. Text-to-image generation has been an active research area for\nyears, with initial focus on GAN [6]-based techniques [35, 36, 32, 39]. However, these methods faced\nchallenges when applied to general-domain datasets or scenes, in part to the training instabilities\nof GANs. Subsequent approaches like VQGAN [34], DALL-E [22], and CogView [4] integrated\nTransformers [29] to produce images with greater fidelity. The recent emergence of diffusion\nmodels [27, 8, 28, 23], capitalized on the power of cross-modal language-image representations such\nas CLIP [19] and large language models like T5-XXL [20], has facilitated significant improvements\nin text-driven image generation and manipulation. Notable examples including GLIDE [18], DALL-\nE2 [21], Imagen [25], and Stable Diffusion [25]. Our work advances this line of work by providing\nboth personalization and localization controllability to text-to-image generation models.\nConditional Text-to-Image Diffusion Models. Observing that text-to-image methods lack precise\ncontrol over the generated content, recent work has explored a variety of additional conditioning\nfactors beyond text in text-to-image diffusion models. For example, GLIGEN [14], ControlNet [37],\nT2I-Adapter [17], and Composer [9] can condition the diffusion model on text as well as bounding\nboxes, poses, depth maps, edge maps, normal maps, and semantic masks. These works demonstrate a\nsimple and effective approach to injecting the additional controlling information via a trainable plug-\n2\nand-play module without affecting the pre-trained diffusion model\u2019s original capabilities. However,\nthese methods lack the ability to control the subject identity for personalized image generation.\nPersonalized and Controllable Image Generation. A key objective in image generation is to enable\npersonalized and controllable manipulation. GAN Inversion [31] accomplishes this by converting\na given image into a corresponding latent representation of a pretrained GAN [10, 3]. In the realm\nof diffusion-based text-to-image models, DALL-E2 [21] has shown promise by mapping images\ninto CLIP-based codes, though it struggles to capture unique details of personalized objects. Textual\nInversion [5] inverts a user-provided image into a word vector for personalized image generation,\ndemonstrating impressive flexibility of the textual embedding space. However, it falls short when\ntasked with rendering the same subject in new contexts [24]. In contrast to optimizing a word vector,\nDreamBooth [24] embeds and finetunes the given subject instance into the entire text-to-image\ndiffusion model. Further, Custom Diffusion [12] composes multiple new concepts by optimizing\nonly a small subset of weights in the diffusion model\u2019s cross-attention layers. Despite these advances,\nmaintaining semantic coherence while exerting fine-grained control over the synthesized images\nstill remains a complex challenge. Our work tackles this by introducing fine-grained localization\ncontrol from GLIGEN [14] into the customized fine-tuning process of DreamBooth [24] to enable\nboth personalized and controllable capabilities in text-to-image generation.\n3\nBackground on DreamBooth and GLIGEN\nIn this section, we briefly introduce two key components of our model, DreamBooth [24] and\nGLIGEN [14], and discuss their limitations, to motivate our work on combining their strengths for a\nmore powerful text-to-image diffusion model that provides both personalization and controllability.\n3.1\nDreamBooth\nPersonalization of text-to-image diffusion models has emerged as a hot topic in recent research.\nDreamBooth [24] is a notable contribution. It works by establishing a link between a personalized\nvisual concept with a few (3-5) user images and an identifier in the text vocabulary. To achieve this,\nDreamBooth employs a combination of a rare token V \u2217 (e.g., sks) together with the coarse category\nname (e.g., \u201ctoy\u201d), to represent the personalized concept. To incorporate the novel concept into a\npretrained diffusion model, the model is finetuned with a prompt containing the concept (e.g., a\nphoto of a [sks] [class name]) using the regular denoising diffusion objective:\nmin\n\u03b8 L = Ex,\u03f5\u223cN (0,I),t\n\u0002\n\u2225\u03f5 \u2212 f\u03b8(xt, t, c)\u22252\n2\n\u0003\n,\n(1)\nwhere t is sampled from time steps {1, \u00b7 \u00b7 \u00b7 , T}, xt is the step-t noisy variant of image x, and\nf\u03b8(\u2217, \u2217, c) is the denoising network conditioned on text feature c.\nTo mitigate language drift [13, 16] and maintain output diversity, DreamBooth is trained to reconstruct\nother (generated) images of the same class as the personalized item for half of the training time. After\nadapting the pretrained model\u2019s weights, DreamBooth can combine the personalized concept with\nthe identifier V \u2217 to generate new images, e.g., placing the personalized concept into new contexts.\n3.2\nGLIGEN\nAnother line of topical research is investigating controllable generation in pretrained diffusion models.\nIn particular, GLIGEN [14] enhances the functionality of text-to-image diffusion models by providing\nadditional control through bounding boxes, keypoints, and other conditioning features. It does this\nby freezing all of the original weights of a pretrained diffusion model and inserting a new trainable\nadaptor, a self-attention layer that takes in the additional conditions, into each Transformer block:\nv = v + SelfAttn(v)\n(2)\nv = v + tanh(\u03b3) \u00b7 TS(SelfAttn([v, g]))\n(3)\nv = v + CrossAttn(v, c)\n(4)\nEqs 2 and 4 represent the original diffusion model layers, where v and c denote the visual and\ntext prompt tokens, respectively. Eq. 3 is the new trainable layer introduced by GLIGEN, with g\nrepresenting the grounding tokens that contain additional controlling information, such as bounding\nboxes. The token selection operation, TS(\u00b7), considers only visual tokens, and \u03b3 is a learnable scalar\nthat acts like a gate (i.e., how much of the original pretrained model\u2019s capabilities should be retained).\n3\n\u201cA sks toy on the beach \u201d\nTraining data: \n\u201cA sks toy on the beach \u201d\nsks toy\nbeach\nsks toy\nbeach\nFigure 2: A naive combination of DreamBooth and GLIGEN. (Left) The model generates accurate\nidentities when the bounding box size and location roughly match those of the training distribution.\n(Right) However, it fails when the size and location fall outside the training distribution.\nAfter training with a relatively small amount of new data, GLIGEN (with box conditioning) can\ndetermine the location and size of noun entities in the prompt. Specifically, the prompt feature c\n(e.g., a photo of a cat and dog) is fed into the cross-attention layer in Eq. 4. The grounding\ntokens g (e.g., one for dog and one for cat) are used in Eq. 3 to control each object\u2019s location. At a\nhigh-level, each grounding token is derived from:\ng = f(e, l),\n(5)\nwhere e is the feature of the grounded noun text (e.g., cat) using the same text encoder as c, l is\nits bounding box, and f(\u00b7) is a simple MLP. Importantly, GLIGEN exhibits open-set generation\ncapabilities, even when finetuned on a limited number of categories (e.g., COCO [15]). For example,\nit can control the location and size of entities like hello kitty even though it is not a category in\nthe COCO dataset. It does this by learning a feature correspondence between c and g so that it can\nuse the grounding condition in a category-agnostic way. More details can be found in [14].\n4\nApproach\nWhile both DreamBooth and GLIGEN have brought significant advances to text-to-image diffusion\nmodels, they possess certain limitations. Specifically, DreamBooth is capable of generating images\nthat incorporate personalized concepts, but lacks the ability to control their precise location and size.\nOn the other hand, GLIGEN offers greater control over object size and location, but cannot handle\npersonalized objects. These limitations call for an approach which combines their strengths, enabling\npersonalized object generation with controlled placement.\nIn the ensuing, we first identify an issue with DreamBooth, namely that it entangles object identity\nwith other factors like location and size. We then propose a simple remedy using data augmentation.\nFinally, to improve image quality and fidelity, we propose a regionally-guided sampling technique.\n4.1\nDreamBooth Incorrectly Entangles Object Identity and Spatial Information\nTo enable spatial control over personalized concepts, a straightforward but naive solution would\nbe to plug in GLIGEN\u2019s controllable layers into a pretrained DreamBooth model (this is possible\nwhen both GLIGEN and DreamBooth build upon the same diffusion model architecture, e.g., Stable\nDiffusion). For example, suppose DreamBooth is trained with the toy cat images in Fig. 2 with its\nlearned unique identifier V \u2217 = sks. During inference, the corresponding grounding token g can be\ncomputed by combining the text feature of sks toy cat and its location l into Eq. 5 to control its\nlocation using the pre-trained GLIGEN layers.\nFig. 2 shows that when the specified bounding box size and location roughly follow the size and\nlocation of the toy cat in the four user-provided training images, the generated images correctly\ncontain the object identity (left). However, if the bounding box size and location fall outside the\ntraining distribution, the generated object\u2019s identity can deviate significantly from the original object\n(right). This suggests that DreamBooth not only learns the object\u2019s identity but also its location\nand size within the training data; i.e., it overfits to the limited training data and incorrectly learns to\nentangle identity and spatial information.\n4\nSelfAttn\nCrossAttn\n\u201cA photo of a sks toy\u201d\nResNet\nDiffusion Model\nSelfAttn\nSelfAttn\nResNet\nOurs\nCrossAttn\n\u201cA photo of a sks toy on the beach\u201d\nsks toy \nbeach\n\ud835\udc65!\n\ud835\udc65!\"#\n\ud835\udc65!\n\u2026\n\u2026\n\u2026\n\u2026\nTraining \nInference \nloss\n\ud835\udc65!\"#\nFigure 3: By incorporating a data augmentation technique that involves aggressive random resizing\nand repositioning of training images, PACGen effectively disentangles object identity and spatial\ninformation in personalized image generation.\nTo further substantiate this hypothesis, we create a toy dataset comprising four screwdriver images,\nall positioned on the left side of the image, as depicted in Fig. 4. Upon training DreamBooth on\nthis dataset, we discover that the model can accurately generate the object identity only when the\ngenerated objects are situated in the same location as the training data. In other locations, DreamBooth\ngenerates a screwdriver that is different from that in the training data.\nCaption: \u201ca sks screwdriver on the grass\u201d\nTraining data\nDreamBooth samples\nFigure 4: DreamBooth incorrectly learns to entangle object identity with spatial information during\ntraining. It generates the correct identity only when the location matches the training distribution.\n4.2\nDisentangling Identity and Spatial Information in Personalized Image Generation\nA possible remedy to disentangle object identity and spatial information would be to ask the user to\nprovide many more photos of their personalized concept in various locations and sizes. However,\nin many cases, this would be highly impractical and undesirable. We thus present a simple training\nstrategy which enables disentangled control over object identity and spatial information, even when\nthere are only a handful of training images. Rather than directly use the handful of user-provided\ntraining images as-is to train the diffusion model, we apply aggressive data augmentation to transform\nthe size and location of the personalized concept. More formally, denote a training image as x and\nthe transformed input to the diffusion model as y. We obtain y using the following transformation:\ny = aug(s, p, g(x)),\n(6)\nwhere g(\u00b7) represents the combined rescale and center crop operation, ensuring that the user image\nconforms to the expected input size of the pretrained diffusion model (e.g., 512\u00d7512) \u2013 this is needed\nsince a user image can be of any arbitrary resolution. aug(s, p, \u00b7) is a random resize and position\noperation, which randomly resizes (with resizing factor randomly sampled from [s, 1], 0 < s < 1)\nand inserts the user provided image g(x) into a clear gray image at sampled location p. The training\nobjective is still the denoising loss (Eq. 1) but only applied to the relevant image region. Fig. 3 (left)\nillustrates this training methodology, where the loss is applied only to the pixels within the red box.\n4.3\nImproving Image Quality and Fidelity during Inference\nWith the above data augmentation strategy, PACGen, which integrates DreamBooth with GLIGEN\u2019s\ncontrollable layers, can now effectively generate objects with accurate identity and position control.\nHowever, as illustrated in Fig. 5, we sometimes observe three types of artifacts introduced by the\naugmentation technique: collaging (examples 1, 2), multiple objects where the extra object(s) appears\n5\nCaption: \u201cA photo of sks toy cat with Eiffel tower in the background \u201d\ncollaging artifact \nEiffel tower\nsks toy\nmulti-objects artifact \ndullness artifact \nFigure 5: Data augmentation sometimes introduces collaging, multi-object, and dullness artifacts.\nin unspecified locations (examples 3 to 4), and a gray dullness effect in which the generated image has\nan overall dull and plain gray color (examples 5 and 6). We hypothesize that these model behaviors\nstem from the fact that the loss function is applied exclusively to the image region (pixels within\nthe red box in Fig. 3), resulting in an undefined loss for areas outside this region. The collaging and\ndullness effects may further be attributed to the model\u2019s ability to \u2018see\u2019 a sharp boundary and uniform\nvalue in the external region during training due to the receptive field of convolution and self-attention.\nWe introduce a regionally guided sampling technique to address these issues, without any further\ntraining, inspired by negative prompting. Specifically, in standard text-to-image diffusion sampling,\nwe can denote the input text as a positive prompt ppostive. Classifier-free guidance [7] was introduced\nto improve image fidelity and quality by utilizing a null text. More recently, the research community\nhas discovered that replacing the null text with a negative prompt pnegative (i.e., undesirable properties\nin the output, such as bad quality, low resolution) can be more effective:\n\u03f5aggregate = \u03f5negative + s \u2217 (\u03f5positive \u2212 \u03f5negative)\n(7)\nwhere \u03f5positive and \u03f5negative are noise predicted by the corresponding prompts, and s is a scalar\nguidance scale (typically s > 1). The final noise \u03f5aggregate is used in the denoising sample step.\nWe introduce two additional prompts: a multi-object suppression prompt psuppress and a diversity\nencouraging prompt pdiverse. We define psuppress as the combination of the identifier and class name\n(e.g., sks toy) used during training. It aims to suppress the learned concept outside the user-provided\nlayout (bounding box) region. For pdiverse, we choose a high quality, colorful image as the\ndefault prompt. Since we do not want to suppress the object inside the layout region, and we also\nfind that applying the diversity prompt pdiverse outside the layout region is sufficient to encourage the\nwhole image to be high quality and colorful, we define our regional negative prompt as:\n\u03f5negative = m \u2217 \u03f5negative + (1 \u2212 m) \u2217 (\u03f5negative + \u03f5suppress \u2212 \u03f5diverse)\n(8)\nwhere m is a mask whose value is 1 inside the V \u2217 object region, which can be derived from the\ninput layout; 1 is an all one mask. Note that we do not introduce any additional prompt engineering\nwork for the user, since pdiverse is fixed for all personalized concepts and psuppress can be computed\nautomatically based on the user-provided layout information. Alg. 1 shows the inference procedure.\nAlgorithm 1 Inference Algorithm\n1: Input: Personal-GLIGEN \u03d5\u03b8, layout l, sampler S(xt, \u03f5), guidance scale s\n2:\nPrompts: ppositive, pnegative, psuppress, pdiverse\n3: Sample a noise xT\n4: Extract regional mask m from l\n5: for t = T, \u00b7 \u00b7 \u00b7 , 1 do\n6:\n\u03f5positive = \u03d5\u03b8(xt, ppositive, t, l);\n\u03f5negative = \u03d5\u03b8(xt, pnegative, t, NULL)\n7:\n\u03f5suppress = \u03d5\u03b8(xt, psuppress, t, NULL);\n\u03f5diverse = \u03d5\u03b8(xt, pdiverse, t, NULL)\n8:\n\u03f5negative = m \u2217 \u03f5negative + (1 \u2212 m) \u2217 (\u03f5negative + \u03f5suppress \u2212 \u03f5diverse)\n9:\n\u03f5aggregate = \u03f5negative + s \u2217 (\u03f5positive \u2212 \u03f5negative)\n10:\nxt\u22121 = S(xt, \u03f5aggregate)\n11: end for\n12: Output: x0\n5\nExperiments\nIn this section, we evaluate PACGen\u2019s personalized controllable text-to-image generation capabilities\nboth quantitatively and qualitatively, and ablate our design choices.\n6\nText\nImage\nObject\nMethod\nalignment (\u2191)\nalignment (\u2191)\nalignment (\u2191)\nKID (\u2193)\nIOU (\u2191)\nSingle-\nConcept\nDreamBooth [24]\n0.779\n0.740\n0.825\n22.580\n0.395\nTextual Inversion [5]\n0.643\n0.741\n0.766\n17.037\n-\nCustom Diffusion [12]\n0.785\n0.776\n0.818\n19.301\n-\nGLIGEN (image) [14]\n0.691\n0.775\n0.799\n-\n0.787\nPACGen (Ours)\n0.794\n0.776\n0.828\n19.582\n0.795\nMulti-\nConcept\nCustom Diffusion [12]\n0.841\n0.690\n0.732\n-\n-\nPACGen (Ours)\n0.818\n0.724\n0.744\n-\n0.642\nTable 1: (Top rows) Single-concept evaluation averaged across 32 datasets. Our method consistently\nachieves the best semantic and identity alignment, while also enabling position control. Our method\nachieves similar KID score as Custom Diffusion, and is better than DreamBooth. Textual Inversion\nhas the lowest KID as it does not update the model. (Bottom rows) Multi-concept evaluation averaged\nacross the 3 composition pairs. Our method performs worse/better/better than Custom Diffusion [12]\nfor text/image/object alignment. Importantly, ours provides position control, which [12] lacks.\nDatasets. We use the same dataset as DreamBooth [24], which contains 30 subjects, toy-cat example\nfrom [5], and screwdriver example we created in Fig. 4. Together we have 32 objects covering diverse\ncommon categories such as backpacks, stuffed animals, dogs, cats, sunglasses, cartoons, etc.\nBaselines. We compare to four baselines: (1) DreamBooth [24] and (2) Textual Inversion [5] are\npioneering approaches in personalized item generation with optimization-based techniques; (3)\nCustom Diffusion [12], a computationally efficient technique that optimizes only a small subset of\ncross-attention weights; and (4) GLIGEN [14], which enables additional controllability in text-to-\nimage diffusion models. It has multiple modalities and we use GLIGEN (image) which embeds a\nreference image using an image encoder to control its location using bounding box information. The\noriginal DreamBooth code is unavailable and has been studied using Imagen [25]. Therefore, we\nutilize the code from diffusers [30] and Stable Diffusion [23] as the pre-trained model. For the other\nthree baselines, we use their official implementations, all of which are built upon Stable Diffusion.\nMetrics. Following [12], we use three evaluation criteria: (1) Image alignment, which refers to the\nvisual resemblance between the generated image and target concept, measured using similarity in\nCLIP image feature space [19]; (2) Text alignment, which measures the correspondence between\nthe generated image and given prompt (we use prompts from the DreamBooth dataset), computed\nusing text-image similarity in CLIP feature space; and (3) KID [2], which is used to evaluate the\nforgetting of existing related concepts when compared to the distribution of real images retrieved from\nLAION-400M [26]. In addition, we also compute (4) Object alignment, similar to image alignment,\nbut after cropping out the object region using an object detector [11] and (5) IOU, which measures\nthe spatial overlap between the generated object\u2019s location and the provided bounding box.\n5.1\nSingle Object Results\nWe first evaluate our method\u2019s ability to generate single object images. Following [12], we use 20\nprompts and generate 50 images per prompt. To create the bounding box inputs that our method\nneeds for generation in an automatic and fair manner, we first generate results using DreamBooth and\nthen fire an object detector [11]. Since all baselines except GLIGEN cannot take in a bounding box\nas input, we only report IOU for GLIGEN and DreamBooth (the latter is simply for a lower-bound\nperformance reference). Since GLIGEN takes in image CLIP features from one inference image, we\naverage the features of all (3\u223c5) reference images and provide it the resulting average feature.\nTable 1 shows that our model, PACGen, can produce results that are comparable or slightly better in\nimage fidelity than alternative state-of-the-art baselines. The IOU metric demonstrates that our model\neffectively adheres to the input bounding boxes with the same controllability as GLIGEN, a capability\nnot present in previous personalized diffusion models. Figure 6 presents a qualitative comparison. It\ncan be observed that our method and DreamBooth are superior in fidelity than the other baselines. In\ngeneral, we notice that as the number of parameters used for finetuning increases, the fidelity of the\nlearned model improves (Ours \u2248 Dreambooth > Custom Diffusion > GLIGEN). We hypothesize that\nthe CLIP image encoder, employed for calculating the alignment scores, might not effectively capture\nfine-grained details that are crucial for assessing personalized objects, which is why the quantitative\nresults do not sufficiently reflect the larger discrepancies revealed in the qualitative results.\n7\n\u201ca photo of a sks dog in the snow\u201d\n\u201ca photo of a sks stuffed animal on the wooden floor\u201d\nTextual Inversion\nDreamBooth\nGLIGEN\nCustom Diffusion\nOurs\nUser Images\nFigure 6: Our method exhibits fidelity comparable to DreamBooth, and outperforms other baselines.\n5.2\nMulti Object Results\nWe next evaluate multi-object generation under three configurations: (1) man-made object + man-\nmade object (toy cat + clock); (2) man-made object + living pet (backpack_dog + cat); and\n(3) living pet + living pet (cat2 + dog2). Following [12] we use 8 prompts, where each prompt is\nused to generate 50 images for evaluation. Table 1 shows a comparison with Custom Diffusion. Our\nmethod attains slightly inferior text alignment, which could be due to our fine-tuning the entire model,\nwhile the baseline fine-tunes only a portion of it, resulting in reduced overfitting. Nevertheless, our\nmethod demonstrates improved fidelity and offers location control, a capability absent in [12].\n5.3\nAblation Study and Applications\nFinetuning GLIGEN\u2019s controllable layers. In the training phase, our method finetunes a pretrained\ntext-to-image diffusion model by utilizing data augmentation. In the inference phase, we plug-and-\nplay new layers from a pre-trained GLIGEN model (refer to Eq. 3) without further finetuning. If we\nalso finetune the GLIGEN layers, we obtain 0.786, 0.778, and 0.834 for the text, image, and object\nalignment scores, respectively. These values are comparable to the numbers in Table 1. This suggests\nthat the original diffusion model possesses sufficient capacity for visual concept representation, and\nthe new layers introduced by GLIGEN only serve the purpose of spatially adjusting the visual features\naccording to the inserted controllable location and size information (e.g., bounding boxes).\nFidelity to identity at different scales and locations. The bounding boxes utilized for our evaluation\nin Table 1 were automatically derived from DreamBooth results, and can be considered as having\n\u2018typical\u2019 object size and location. To investigate the impact of varying the size and position of\nbounding boxes on the fidelity of the generated object identity, we assess our method by scaling the\nboxes across scales [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2, 1.5, 2.0] and random positions. We evaluate the\nobject alignment score, as text and image alignment are not appropriate measures due to the fact that\nthe object occupies only a small portion of images at smaller scales. The results are [0.78, 0.80, 0.81,\n0.82, 0.83, 0.83, 0.82, 0.81, 0.78], which demonstrate that our method can maintain object fidelity\nacross a wide range of bounding box scales and locations.\nRegional Guided Sampling. It is introduced to address artifacts leaked by data augmentation. We\nablate the effect of each component. (1) Removing regular negative prompt pnegative, which is set as\n\u201ccollaging effect, low quality, assembled image\u201d. We train a binary classifier (see supp\nfor details) to classify if collaging effects exist in an image. The percentage of collaging examples\nare 12.7% and 2.4% before and after adding this prompt. (2) Removing multi-object suppression\nprompt psuppress. We again use [11] to detect objects and use mIOU for evaluation. The result is\ndropped from 0.795 to 0.702 due to multiple generated objects (vs only one GT box). (3) Removing\ndiversity prompt pdiversity. We calculate LPIPS [38] distance between pairs of images sharing the\nsame prompt. The distance drops from 0.6583 to 0.5489 (i.e., the generated images become less\ndiverse). We show random qualitative samples for these three ablations in the supp.\n8\nsks \nsneaker\nsks \ndog\nsks cat\nsks \nvase\nmbc \nstuffed\nanimal\nEiffel\nTower\nImage composition \njohannes vermeer\npierre-auguste\nrembrandt\nvan gogh\nArt style\niron man\npolo shirt \nsuperman\nsuit\nAccessorization\nAttribute\nblue\ngreen\nsliver\nyellow\nLocation control\nsks toy\nFigure 7: Our method offers versatile applications. It can generate objects at specified locations in\ndiverse scenes and artistic styles, and accessorize objects or modify their attributes with precision.\nApplications. Fig. 7 showcases various intriguing applications of PACGen. The Composition feature\nallows for the creation of multiple objects from user images with high fidelity and the generation of\nadditional objects (e.g., Eiffel Tower) with location control. The Art Style aspect enables art style\nchanges for personalized objects while maintaining location control. Lastly, Accessorization and\nAttribute features facilitate the addition of accessories or the modification of attributes (e.g., color)\nfor personalized objects, allowing them to be freely positioned at any location within a scene.\n6\nConclusion and Broader Impact\nWe introduced a straightforward yet effective approach for addressing the entanglement issue in\nexisting personalization diffusion models, and incorporated a inference-time sampling technique to\nensure high-quality generation with location control. Nevertheless, our work has some limitations.\nWe occasionally observed some attribute mixture in multi-concept cases; for example, the color of one\nobject can blend into another object. Our method takes more time for inference due to two additional\nprompts (psuppress and pdiversity) in the diffusion process, with the inference time increasing from 5\nto 8 seconds per image on an A6000 GPU.\nThis project aims to provide users with a tool for controllable synthesis of personalized subjects in\nvarious contexts, offering improved location control. However, the potential misuse of generated\nimages by bad actors remains a concern. Future research should continue to address these issues in\ngenerative modeling and personalized generative priors.\n9\nAppendix\nTraining details. We implement our training based on the DreamBooth code from the diffusers [30].\nWe finetune Stable Diffusion v1.4 [23] with batch size as 2 for 1000 iterations. We set random\nresizing scalar s (Eq. 6) as 0.3 for all experiments. All other hyperparamters are the same as the\ndefault ones in diffusers [30]. For multi concepts, we double the batch size to 4 following [12].\nInference details. In the inference phase, we integrate location control adapters (Eq 3) from GLIGEN\ninto the finetuned Stable Diffusion model. The classifier-free guidance scale is set to 5 (Eq 7), and we\nemploy schedule sampling with a value of 0.3 for \u03c4, which is a technique proposed by GLIGEN [14]\nthat helps strike a balance between image quality and layout correspondence.\nCollaging classifer. In order to evaluate the effects of collaging in our ablation study, we train a\nclassifier. Specifically, we utilize the CLIP [19] ViT-L-14 variant to extract features from images\nand subsequently train a linear classifier using these features while keeping the backbone fixed. Our\ndataset consists of ADE20K images. Each regular image is treated as a non-collaged class, while\ntwo randomly selected images are combined via collaging (by randomly resizing and positioning one\nimage onto the other) to create a collaged class. We train the linear classification head using a batch\nsize of 128 for 50 epochs.\nA\nAdditional qualitative results\nApart from conducting a quantitative ablation study as in the main paper, we also present a visual\nablation study for our designs. The crucial designs during the inference stage involve regional\nguidance, which comprises three prompts. We systematically ablate each prompt, and Figure 8\ndisplays random samples obtained by removing each of them. The results clearly demonstrate the\nnecessity of each prompt. When pnegative is absent, the results exhibit a collaging effect, especially\nwhen the object only occupies a relatively small part of the image, as illustrated in the provided case.\nFurthermore, the results appear less colorful and less interesting in the absence of pnegative. Lastly,\nwithout psuppress, the model tends to generate additional instances in undesired locations.\nFinally, Figure 9, Figure 10, and Figure 11 show additional results on single/multi-object location\ncontrol and various applications by our model PACGen.\n10\n\ud835\udc5b\ud835\udc5c \ud835\udc5d!\"#$%&'\"\n\ud835\udc5b\ud835\udc5c \ud835\udc5d(&'\")*\"\n\ud835\udc5b\ud835\udc5c \ud835\udc5d*+,,)\"**\no\ud835\udc62\ud835\udc5f\ud835\udc60\nFigure 8: Qualitative ablation. Random samples. The caption and layout is the same as Fig. 5 in the\nmain paper.\n11\nLocation control\nA sks dog on the road\nA sks toy is surrounded by sunflowers\nFigure 9: Location control. For the same concept, our model can position it in any arbitrary location.\nsks dog\nmbc cat\nboat\nImage composition \nA sks dog and a mbc cat on a boat\nA sks toy and a mbc clock under a rainbow\nsks toy\nmbc\nclock\nFigure 10: Multi object composition. Our model can combine multiple instances provided by a user.\nsks\nteapot\nsks dog\nsks\nbackpack\njohannes vermeer\npierre-auguste\nrembrandt\nvan gogh\nArt style\ncheetah\nfox\npanda\nrabbit\nHybrid\nAttribute\nblue\ngray\ngreen\nyellow\nFigure 11: Applications. We enable various applications while controlling the location and size of\nobjects.\n12\nReferences\n[1] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum,\nJonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. arXiv preprint\narXiv:2302.07121, 2023.\n[2] Mikolaj Binkowski, Danica J. Sutherland, Michal Arbel, and Arthur Gretton. Demystifying\nmmd gans. arXiv, abs/1801.01401, 2018.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity\nnatural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n[4] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin,\nXu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation\nvia transformers. NeurIPS, 2021.\n[5] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022.\n[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, and Yoshua Bengio. Generative adversarial networks. NeurIPS, 2014.\n[7] Jonathan Ho. Classifier-free diffusion guidance. arXiv, abs/2207.12598, 2022.\n[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,\n2020.\n[9] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou.\nCom-\nposer: Creative and controllable image synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023.\n[10] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In CVPR, 2019.\n[11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick.\nSegment anything. arXiv:2304.02643, 2023.\n[12] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-\nconcept customization of text-to-image diffusion. CVPR, 2023.\n[13] Jason Lee, Kyunghyun Cho, and Douwe Kiela. Countering language drift via visual grounding.\nIn EMNLP, 2019.\n[14] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan\nLi, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. CVPR, 2023.\n[15] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n[16] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering\nlanguage drift with seeded iterated learning. In ICML, 2020.\n[17] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\n[18] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. ICML, 2022.\n13\n[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In ICML,\n2021.\n[20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[21] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[22] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n[23] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. CVPR, 2022.\n[24] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arxiv:2208.12242, 2022.\n[25] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David\nFleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 2022.\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. ArXiv, abs/2111.02114, 2021.\n[27] Jascha Narain Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep\nunsupervised learning using nonequilibrium thermodynamics. In ICML, 2015.\n[28] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\n[30] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https:\n//github.com/huggingface/diffusers, 2022.\n[31] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan\ninversion: A survey. TPAMI, 2022.\n[32] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\nHe. Attngan: Fine-grained text to image generation with attentional generative adversarial\nnetworks. In CVPR, 2018.\n[33] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Reco: Region-controlled text-to-image\ngeneration. In CVPR, 2023.\n[34] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku,\nYuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021.\n[35] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and\nDimitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative\nadversarial networks. In ICCV, 2017.\n14\n[36] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and\nDimitris N Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial\nnetworks. TPAMI, 41(8):1947\u20131962, 2018.\n[37] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[38] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreason-\nable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\n[39] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative\nadversarial networks for text-to-image synthesis. In CVPR, 2019.\n15\n"
  },
  {
    "title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
    "link": "https://arxiv.org/pdf/2306.17156.pdf",
    "upvote": "20",
    "text": "Generative AI for Programming Education:\nBenchmarking ChatGPT, GPT-4, and Human Tutors\u2217\nTung Phung\nMPI-SWS\nmphung@mpi-sws.org\nVictor-Alexandru P\u02d8adurean\nMPI-SWS\nvpadurea@mpi-sws.org\nJos\u00e9 Cambronero\u2020\nMicrosoft\njcambronero@microsoft.com\nSumit Gulwani\u2020\nMicrosoft\nsumitg@microsoft.com\nTobias Kohn\u2020\nTU Wien\ntobias.kohn@tuwien.ac.at\nRupak Majumdar\u2020\nMPI-SWS\nrupak@mpi-sws.org\nAdish Singla\u2020\nMPI-SWS\nadishs@mpi-sws.org\nGustavo Soares\u2020\nMicrosoft\ngsoares@microsoft.com\nAbstract\nGenerative AI and large language models hold great promise in enhancing com-\nputing education by powering next-generation educational technologies for in-\ntroductory programming. Recent works have studied these models for different\nscenarios relevant to programming education; however, these works are limited\nfor several reasons, as they typically consider already outdated models or only\nspecific scenario(s). Consequently, there is a lack of a systematic study that bench-\nmarks state-of-the-art models for a comprehensive set of programming education\nscenarios. In our work, we systematically evaluate two models, ChatGPT (based\non GPT-3.5) and GPT-4, and compare their performance with human tutors for\na variety of scenarios. We evaluate using five introductory Python programming\nproblems and real-world buggy programs from an online platform, and assess\nperformance using expert-based annotations. Our results show that GPT-4 drasti-\ncally outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors\u2019\nperformance for several scenarios. These results also highlight settings where\nGPT-4 still struggles, providing exciting future directions on developing techniques\nto improve the performance of these models.\n1\nIntroduction\nGenerative AI and large language models (LLMs) have the potential to power next-generation AI-\ndriven educational technologies and drastically improve the landscape of computing education. We\nfocus in this paper on the role of LLMs in enhancing introductory programming education. State-of-\nthe-art models like OpenAI\u2019s ChatGPT [2] and GPT-4 [3] could enhance programming education in\nvarious roles, e.g., by acting as a personalized digital tutor for a student, as a digital assistant for an\neducator, or as a digital peer for collaborative learning [4\u20136]. In our work, we seek to comprehensively\nevaluate and benchmark state-of-the-art LLMs for various scenarios in programming education.\nRecent works have studied several LLMs for different scenarios relevant to programming education [7\u2013\n11]. However, these works are limited for several reasons: they considered models that are already\n\u2217This article is a full version of the poster (extended abstract) from ICER\u201923 [1].\n\u2020These authors are listed in alphabetical order. Correspondence to: Adish Singla <adishs@mpi-sws.org>.\narXiv:2306.17156v3  [cs.CY]  1 Aug 2023\noutdated (e.g., OpenAI\u2019s Codex [12] is no longer publicly available since March 2023), or they\ntypically considered only specific scenario(s) (e.g., generating explanations). Consequently, there\nis a lack of systematic study that benchmarks state-of-the-art models for a comprehensive set of\nprogramming education scenarios.\nIn our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and\ncompare their performance with human tutors for a variety of programming education scenarios.\nThese scenarios are designed to capture distinct roles these models could play, namely digital tutors,\nassistants, and peers, as discussed above. More concretely, we consider the following six scenarios: (i)\nprogram repair; (ii) hint generation; (iii) grading feedback; (iv) pair programming; (v) contextualized\nexplanation; and (vi) task synthesis.\nWe evaluate the performance of different methods (i.e., LLMs and human tutors) using expert-based\nannotations involving a mix of quantitative and qualitative assessments. We conduct our evaluation\nusing five introductory Python programming problems with a diverse set of input/output specifications.\nFor each of these problems, we consider real-world buggy programs based on publicly accessible sub-\nmissions from geeksforgeeks.org platform [13]; these buggy programs are picked to capture different\ntypes of bugs for each problem. Our results show that GPT-4 drastically outperforms ChatGPT (based\non GPT-3.5) and comes close to human tutors\u2019 performance for several scenarios. These results also\nhighlight that GPT-4 still struggles with more challenging scenarios, e.g., grading feedback and task\nsynthesis, where the performance of GPT-4 is quite low compared to that of human tutors.\nThe rest of this paper is organized as follows. Section 2 provides an overview of our evaluation setup\nand introduces the data used for evaluation. Sections 3\u20138 provide results for the above-mentioned six\nscenarios. Section 9 discusses some limitations of our current work and directions for future work.\n2\nEvaluation Setup\nThis section provides an overview of our evaluation setup, including the programming education\nscenarios, Python problems along with buggy programs, and the overall process used for evaluation.\nProgramming education scenarios.\nIn our work, we consider the following six scenarios in\nprogramming education, capturing different roles that AI-based educational agents could play in the\nform of digital tutors, assistants, and peers:\n(i) Program repair, i.e., fixing a student\u2019s buggy program [8, 11, 14].\n(ii) Hint generation, i.e., providing hints to a student to help resolve current issues [10, 11].\n(iii) Grading feedback, i.e., grading a student\u2019s buggy program w.r.t. a given rubric [15, 16].\n(iv) Pair programming, i.e., completing an incomplete/partial program written by a student [17\u201320].\n(v) Contextualized explanation, i.e., explaining specific parts in the context of a given program [9, 21].\n(vi) Task synthesis, i.e., generating new tasks that exercise specific types of concepts/bugs [9, 22\u201324].\nIntroductory Python problems.\nWe conduct our evaluation using five introductory Python pro-\ngramming problems summarized in Figure 1. These are typical programming problems, and variants\nof these problems appear in several online programming websites, courses, and textbooks [13, 25\u201327].\nWe picked these specific problems as they capture a diverse set of programming and algorithmic\nconcepts required to solve and vary in their input-output specifications; moreover, solution programs\nfor these problems are short and comprise up to 10 lines of code.\nReal-world buggy programs.\nFor these five problems, we consider buggy programs based on\npublicly accessible submissions from geeksforgeeks.org platform [13]. These problems are available\non https://practice.geeksforgeeks.org/ at the following links: (a) problems/gcd-of-two-numbers3459/1;\n(b) problems/fibonacci-to-n0811/1; (c) problems/number-of-divisors1631/1; (d) problems/palindrome-\nstring0817/1; (e) problems/merge-two-strings2736/1.3 For each of these five problems, we picked\n3We use detailed problem descriptions available at these links when designing prompts. Moreover, we use the\nproblem specifications and automated test suites available at these links to check the correctness of a program.\n2\nProblem ID\nShort Text\nInput\nOutput\nGCD\nGiven two positive integers n1 and n2, find GCD of n1 and n2.\nTwo integers\nInteger\nFIBONACCI\nGiven a positive integer n, calculate the Fibonacci series until the number n.\nInteger\nList of integers\nDIVISORSDIV3 Given a positive integer n, find its number of divisors that are divisible by 3.\nInteger\nInteger\nPALINDROME\nGiven a string S, check if it is palindrome or not.\nString\nBoolean\nMERGESTRS\nGiven two strings S1 and S2, merge them alternatively.\nTwo strings\nString\nFigure 1: Five introductory Python programming problems used in our work.\nProblem ID\nProgram ID\nDescription of Bug(s)\n#Lines\nGCD\nBP-01\n\u2018range\u2019 function is misused. Time complexity is violated.\n04\nGCD\nBP-02\n\u2018range\u2019 function is misused. Time complexity and space complexity are violated.\n17\nGCD\nBP-03\nImplementation of the Euclidean algorithm is incorrect.\n07\nGCD\nBP-04\nWrong arguments are passed to the recursive function call.\n04\nGCD\nBP-05\nA wrong variable is returned.\n06\nFIBONACCI\nBP-06\nProblem description misunderstood.\n09\nFIBONACCI\nBP-07\nTime complexity is violated.\n11\nFIBONACCI\nBP-08\nNumbers at the end of series are missing.\n07\nFIBONACCI\nBP-09\nNumbers at the beginning of series are missing. Problem description misunderstood.\n15\nFIBONACCI\nBP-10\nNumbers at the beginning of series are missing.\n15\nDIVISORSDIV3\nBP-11\nTime complexity is violated.\n06\nDIVISORSDIV3\nBP-12\nNon-divisors are also counted.\n09\nDIVISORSDIV3\nBP-13\nThere is an off-by-one error.\n13\nDIVISORSDIV3\nBP-14\nSome valid divisors are not counted.\n10\nDIVISORSDIV3\nBP-15\nDivisors larger than \u221an are not considered.\n08\nPALINDROME\nBP-16\nThere is an issue with string indexing. Return type is incorrect.\n09\nPALINDROME\nBP-17\nAll strings with odd lengths are regarded as non-palindrome.\n06\nPALINDROME\nBP-18\nThere is a mistake in the algorithm.\n04\nPALINDROME\nBP-19\n\u2018return\u2019 keyword is missing. Space complexity is violated.\n07\nPALINDROME\nBP-20\nThere is a misconception regarding mutability of lists.\n15\nMERGESTRS\nBP-21\nIndentation of a statement is incorrect.\n14\nMERGESTRS\nBP-22\nThere is a mistake regarding lexicographical ordering of strings.\n17\nMERGESTRS\nBP-23\nThere is a mistake regarding ordering of the merging strings.\n31\nMERGESTRS\nBP-24\nAn if-elif-else statement is misused.\n16\nMERGESTRS\nBP-25\nThere is a issue regarding the slicing of strings.\n08\nFigure 2: Real-world buggy programs used in our work. These programs are based on publicly\naccessible submissions from geeksforgeeks.org platform [13] and are picked to capture different\ntypes of bugs for each problem. The last column, titled \u201c#Lines\u201d, indicates the number of lines in\nthe buggy program (not counting the lines that are part of the given template).\nfive buggy programs, i.e., a total of 25 buggy programs as summarized in Figure 2. We picked these\nbuggy programs to capture different types of bugs for each problem and ensure that these programs\nare associated with submissions by different users; moreover, these programs vary in size, ranging\nfrom 4 to 31 lines of code.4\nMethods evaluated.\nWe evaluate three methods in our work: (a) ChatGPT that uses OpenAI\u2019s\nChatGPT (based on GPT-3.5) as its LLM via web platform [2, 28]; (b) GPT-4 that uses OpenAI\u2019s\n4We note that the geeksforgeeks.org platform doesn\u2019t provide URL links to specific submissions for a problem.\nFor this reason, we have only provided URL links for five problems. The description of bug(s) and the number\nof lines provided in Figure 2 give useful insights into how one could curate similar data sets from different\nresources to conduct future studies.\n3\n1 #User\nfunction\nTemplate\nfor\npython3\n2\nclass\nSolution:\n3\ndef\nnFibonacci (self ,N):\n4\n# base\ncases\n5\nif N == 0: return\n[0]\n6\nif N == 1: return [0, 1, 1]\n7\n# create\nthe\nfibonacci\nsequence\n8\nfib_sequence = [0, 1]\n9\nwhile\nTrue:\n10\nnext_value = fib_sequence [-1] + fib_sequence [-2]\n11\nif\nnext_value\n> N:\n12\nbreak\n13\nfib_sequence .append( next_value )\n14\nreturn\nfib_sequence\n15\n16 # Driver\nCode\nStarts\n17 #Initial\nTemplate\nfor\nPython 3\n18 if\n__name__ ==\u2019__main__ \u2019:\n19\nt=int(input ())\n20\nfor _ in range(t):\n21\nN=int(input ())\n22\nob=Solution ()\n23\nans=ob.nFibonacci (N)\n24\nfor i in ans:\n25\nprint(i,end=\" \")\n26\nprint ()\n27 # } Driver\nCode\nEnds\nFigure 3: A solution program generated by GPT-4 for FIBONACCI. The highlighted lines are generated\nby GPT-4; the rest, including the Driver Code, is part of the solution template on geeksforgeeks.org\nplatform [13]. This template, along with the problem description, is given in the prompt.\nGPT-4 as its LLM via web platform [3, 29]; (c) Tutor that corresponds to human experts with\nexperience in Python programming and tutoring introductory programming classes. Prompts used to\ninteract with LLMs are provided in the subsequent sections for different scenarios. The information in\nthese prompts also serves as instructions for human experts that are part of the Tutor method; these\nexperts can naturally draw on their own experiences and use additional resources\u2014e.g., debugger,\nweb, or course materials\u2014similar to how a typical human tutor/educator would work on these\nscenarios in real-world settings. Next, we describe the interaction process with these models and\noutputs for evaluation. For a given method and scenario, we have 25 total instances for evaluation,\ncomprising a problem and program (5\u00d75 instances). For each instance, ChatGPT and GPT-4 perform\nnChatGPT = nGPT-4 = 1 query to their corresponding LLMs through web platforms to generate one\noutput per instance; Tutor has nTutor = 2 human experts that independently generate two outputs\nper instance.5 We describe further scenario-specific details in the subsequent sections.\nMetrics and evaluation process.\nWe will introduce scenario-specific performance metrics in the\nsubsequent sections. We have nevals = 2 human evaluators who provide annotations to assess\nthe quality of generated output for each instance w.r.t. corresponding performance metrics. In our\nevaluation, this set of nevals = 2 human evaluators is same as nTutor = 2 human experts that are\npart of the Tutor method. More concretely, each of the nevals human evaluators independently\nannotates the quality of generated outputs for ChatGPT, GPT-4, and Tutor (only for the nTutor\u22121\nhuman experts by excluding the evaluator themselves). Then, for each method, results are first\naggregated across 25 instances or across 5 instances when reporting problem-specific results. Finally,\nwe aggregate results across nevals human evaluators and report averaged results as mean (stderr).\nWe provide scenario-specific details in the subsequent sections.\nRemark.\nSince we want LLMs to play the role of experienced digital tutors and assistants, a natural\nquestion is whether they can solve five problems used in the evaluation, i.e., generate correct solution\nprograms. Before evaluating ChatGPT and GPT-4 on different programming education scenarios, we\nchecked their problem-solving capabilities by querying them with suitable prompts consisting of a\nproblem description along with a solution template as input and instruction to generate a solution\nprogram as output. GPT-4 was able to solve all five problems; Figure 3 above and Figures 16\u201319 in\nAppendix A.1 show solution programs generated by GPT-4 for these problems. ChatGPT was able to\nsolve four out of five problems; it consistently failed on FIBONACCI across multiple queries.\n5We note that GPT-4 currently has a cap of 25 messages every 3 hours [29]. In future studies, it would be\nuseful to scale up the evaluation process by increasing nChatGPT, nGPT-4, and nTutor.\n4\nPrompt: Program Repair\nI\u2019m working on a Python programming problem. The current program below is not working well.\nCan you help in fixing this program with as few changes as possible? Below I first provide the\nproblem description and then the current buggy program.\n{problem_description}\nBuggy Program:\n```\n{buggy_program}\n```\nCan you fix the above buggy program?\nThe code marked as #Driver Code is correct and\nshould not be modified. Make sure that you make minimal possible changes needed to fix the\nprogram.\nFigure 4: Prompt for the program repair scenario. This prompt has two placeholders for the problem\ndescription and the buggy program.\n3\nProgram Repair Scenario\nThis section is dedicated to the programming education scenario of program repair [8, 11, 14]. This\nscenario is motivated by an AI-based educational agent acting as a digital tutor for a student and\nproviding help by fixing the student\u2019s buggy program. Next, we provide details of this scenario\u2019s\nprompt, input-output formats, performance metrics, and results.\nPrompt and output generation.\nWe begin by describing the content provided as input to a method\nand the desired output content we seek to generate. The input consists of a detailed problem\ndescription and a student\u2019s buggy program; the desired output consists of a fixed program. Figure 4\nshows the prompt\u2014with placeholders for the inputs\u2014used to interact with LLMs for ChatGPT and\nGPT-4 methods. The prompt starts with an overview text about the scenario, followed by a detailed\nproblem description and a student\u2019s buggy program, and then summarizes the desired output. When\ninteracting with LLMs, we first generate content using this prompt and then manually extract the\ngenerated program as the final output for evaluation.\nOutput quality and performance metrics.\nWe assess the generated output along several quality\nattributes and use aggregated results over these quality attributes as performance metrics in our\nevaluation. Correct (binary, 1 value being better) captures whether the generated program is correct\nw.r.t. the problem specification; we use automated test suites to check the correctness of a program as\nmentioned in Footnote 3. EditTokens (non-negative number, lower value being better) captures the\ntoken-based edit distance between the generated program and the buggy program.6 Human evaluators\nannotate the quality of generated output for each of the 25 instances; in this particular scenario, human\nevaluators computed these attributes using automated scripts without requiring manual annotation.\nResults.\nFigure 5a provide results for various metrics aggregated across all problems, and Figure 5b\nfor the metric Correct separately on five problems. These aggregated results for the metric Correct\nare reported in terms of %. Next, we summarize some of the key findings. First, results in Figure 5a\nfor the metric Correct highlight that GPT-4 (88.0) substantially improves up on ChatGPT (68.0)\nand comes close to the performance of Tutor (100.0). However, in terms of the metric EditTokens,\nGPT-4 (36.6) does a lot more edits when fixing buggy programs in contrast to that made by Tutor\n(19.0). Second, results in Figure 5b highlight that these findings are generally consistent across all\nfive problems for the metric Correct; the gap in the performance of GPT-4 vs. Tutor is worst on\nFIBONACCI for this scenario. In Appendix A.2, we provide an illustrative example showing the\noutputs of different methods.\n6Edit-distance between two programs is measured by first tokenizing programs using Pygments library [30]\nand then computing Levenshtein edit-distance over token strings.\n5\nMethod\n(Fixed Program, Buggy Program)\nCorrect\nEditTokens\nChatGPT\n68.0 (0.0)\n43.0 (0.0)\nGPT-4\n88.0 (0.0)\n36.6 (0.0)\nTutor\n100.0 (0.0)\n19.0 (1.2)\n(a)\nAll Problems\nGCD\nFIBONACCI\nDIVISORSDIV3\nPALINDROME\nMERGESTRS\n0\n20\n40\n60\n80\n100\nCorrect\nChatGPT\nGPT-4\nTutor\n(b)\nFigure 5: Results for the program repair scenario. (a) Results for various metrics aggregated across\nall problems. (b) Results for the metric Correct separately on five problems. For the metric Correct,\nthese aggregated results are reported in terms of %. Details are in Section 3.\n4\nHint Generation Scenario\nThis section is dedicated to the programming education scenario of hint generation [10, 11]. This\nscenario is motivated by an AI-based educational agent acting as a digital tutor for a student and\nproviding help via hints to resolve current issues in the student\u2019s buggy program. Next, we provide\ndetails of this scenario\u2019s prompt, input-output formats, performance metrics, and results.\nPrompt and output generation.\nWe begin by describing the content provided as input to a method\nand the desired output content we seek to generate. The input consists of a detailed problem\ndescription and a student\u2019s buggy program; the desired output consists of a hint and an explanation\nthat provides the reasoning behind the generated hint. Figure 6 shows the prompt\u2014with placeholders\nfor the inputs\u2014used to interact with LLMs for ChatGPT and GPT-4 methods. When interacting with\nLLMs, we first generate content using this prompt and then manually extract the generated hint and\nexplanation as the final output used for evaluation.\nOutput quality and performance metrics.\nWe assess the generated output along several quality\nattributes and use aggregated results over these quality attributes as performance metrics in our\nevaluation. All attributes for this scenario are binary, with a value of 1 being better. HCorrect\n(binary) captures whether the generated hint provides correct information for resolving issues in the\nstudent\u2019s buggy program. HInformative (binary) captures whether the generated hint provides useful\ninformation to help the student resolve bug(s); this attribute is set to 0 by default when the hint is\nincorrect. HConceal (binary) captures that the information in the generated hint is not too detailed, so\nthe student would also have to reason about implementing the fixes; this attribute is set to 0 by default\nwhen the hint is incorrect. HComprehensible (binary) captures whether the generated hint is easy to\nunderstand, presented in a readable format, and doesn\u2019t contain redundant information. HOverall\n(binary) is 1, i.e., good quality, only if the generated hint satisfies all the four quality attributes\nmentioned above. ECorrect (binary) captures whether the generated explanation contains correct\nreasoning behind the generated hint; this attribute is set to 0 by default when the hint is incorrect.\nOverall (binary) is 1 only when both the HOverall and ECorrect attributes are 1. Human evaluators\nannotate the quality of generated output for each of the 25 instances; in this scenario, these attributes\nrequire manual annotation (in contrast to automated annotation for the program repair scenario).\nResults.\nFigure 7a provide results for various metrics aggregated across all problems, and Figure 7b\nfor the metric Overall separately on five problems. These aggregated results for various metrics are\nreported in terms of %. Next, we summarize some of the key findings. First, results in Figure 7a for\nthe metric Overall highlight that GPT-4 (66.0) substantially improves up on ChatGPT (18.0), though\n6\nPrompt: Hint Generation\nI\u2019m working on a Python programming problem. The current program below is not working well.\nCan you help by giving a hint? Below I first provide the problem description and then the current\nbuggy program.\n{problem_description}\nBuggy Program:\n```\n{buggy_program}\n```\n(1) Can you describe the bug(s) in this program and the required fixes?\n(2) Can you provide a concise single-sentence hint about one bug in this program?\nThe\nhint should not be too detailed as I want to think about the fixes by myself. However, the hint should\nnot be too abstract, as I need some help.\nFigure 6: Prompt for the hint generation scenario. This prompt has two placeholders for the problem\ndescription and the buggy program.\nMethod\n(Hint, Explanation)\nHint\nExplanation\nOverall\nHOverall\nHCorrect\nHInformative\nHConceal\nHComprehensible\nECorrect\nChatGPT\n18.0 ( 6.0)\n22.0 ( 6.0)\n50.0 (10.0)\n38.0 ( 6.0)\n38.0 (10.0)\n94.0 ( 2.0)\n40.0 ( 8.0)\nGPT-4\n66.0 (10.0)\n70.0 (10.0)\n74.0 (10.0)\n74.0 (10.0)\n72.0 ( 8.0)\n96.0 ( 4.0)\n70.0 (10.0)\nTutor\n92.0 ( 4.0)\n92.0 ( 4.0)\n94.0 ( 6.0)\n94.0 ( 6.0)\n94.0 ( 6.0)\n98.0 ( 2.0)\n94.0 ( 6.0)\n(a)\nAll Problems\nGCD\nFIBONACCI\nDIVISORSDIV3\nPALINDROME\nMERGESTRS\n0\n20\n40\n60\n80\n100\nOverall\nChatGPT\nGPT-4\nTutor\n(b)\nFigure 7: Results for the hint generation scenario. (a) Results for various metrics aggregated across\nall problems. (b) Results for the metric Overall separately on five problems. For all metrics, these\naggregated results are reported in terms of %. Details are in Section 4.\nthere is still a large gap in comparison to the performance of Tutor (92.0). Combining this with\nresults on metrics HOverall and ECorrect, we can see that GPT-4\u2019s detailed reasoning is usually\ncorrect whenever it manages to generate a good quality hint. Second, results in Figure 7b highlight\nthat these findings are generally consistent across all five problems for the metric Overall; the gap\nin the performance of GPT-4 vs. Tutor is worst on PALINDROME for this scenario. Interestingly,\nGPT-4\u2019s performance on GCD is slightly better than that of Tutor. In Appendix A.3, we provide an\nillustrative example to qualitatively show the outputs of different methods.\n5\nGrading Feedback Scenario\nThis section is dedicated to the programming education scenario of grading feedback [15, 16]. This\nscenario is motivated by an AI-based educational agent acting as a digital assistant for an educator\nand providing assistance by grading students\u2019 programs w.r.t. a given rubric. Next, we provide details\nof this scenario\u2019s prompt, input-output formats, performance metrics, and results.\n7\nPrompt: Grading Feedback\nI have to grade a student\u2019s program for a Python programming problem. Can you help in grading\nthis student\u2019s program according to a given rubric? Below I first provide the problem description,\nthe student\u2019s program, and then the grading rubric.\n{problem_description}\nStudent\u2019s Program:\n```\n{student_program}\n```\nGrading Rubric:\nThe grading rubric is divided into five dimensions to assess different aspects of the pro-\ngram. The maximum possible points for a program is 100.\n1. Program format (10 points)\n- The program implements the {function_name} function with the correct input parameters and\nreturn type as specified in the problem description. The possible points along this rubric dimension\nare 10 or 0.\n2. Time complexity (15 points)\n- The program meets the expected time complexity of {time_complexity}. The possible points\nalong this rubric dimension are 15 or 0.\n3. Space complexity (15 points)\n- The program meets the expected auxiliary space complexity of {space_complexity}. The\npossible points along this rubric dimension are 15 or 0.\n4. Correctness for general inputs (30 points)\n- The program outputs the correct results for most of the inputs. We give full points as long as the\nprogram works correctly for general inputs, i.e., excluding specific inputs that are edge cases or\ncause issues with time/space complexity. The possible points along this rubric dimension are 30 or\n0.\n5. Correctness for edge cases (30 points)\n- The program outputs the correct results for specific inputs that are edge cases. We subtract 10\npoints for each failing type of edge case. We give 0 points when there are more than three types of\nfailing edge cases. The possible points along this rubric dimension are 30, 20, 10 or 0.\nCan you grade the student\u2019s program according to the above five-dimensional rubric and\nalso provide total points scored by the student\u2019s program? The code marked as #Driver Code is\ncorrect and should not be considered for grading.\nFigure 8: Prompt for the grading feedback scenario. This prompt has two placeholders for the problem\ndescription and the student\u2019s program, and three placeholders for problem-specific details in the rubric.\nPrompt and output generation.\nWe begin by describing the content provided as input to a method\nand the desired output content we seek to generate. The input consists of a detailed problem\ndescription, a student\u2019s program with bugs, and a grading rubric; the desired output consists of\ngrading points w.r.t. the grading rubric. Figure 8 shows the prompt\u2014with placeholders for the\ninputs\u2014used to interact with LLMs for ChatGPT and GPT-4 methods. When interacting with LLMs,\nwe first generate content using this prompt and then manually extract the grading points w.r.t. the\nrubric as the final output for evaluation.7\nOutput quality and performance metrics.\nWe assess the generated output along several quality\nattributes and use aggregated results over these quality attributes as performance metrics in our\n7LLMs typically also generate explanations along with grading points; we do not evaluate the quality of these\nexplanations. In Appendix A.4, we will provide an illustrative example to show some of these explanations.\n8\nMethod\nMatch and Difference (D-) in Points Across Grading Rubric\nMatch\nD-Total\nD-ProgramFormat D-TimeComplexity D-SpaceComplexity D-CorrectGeneral D-CorrectEdge\nChatGPT 10.0 (6.0) 36.7 (1.3)\n0.8 (0.4)\n0.6 (0.6)\n2.7 (0.9)\n18.0 (1.2)\n18.2 (0.6)\nGPT-4\n16.0 (4.0) 23.5 (2.9)\n1.2 (0.4)\n1.8 (0.6)\n3.3 (0.9)\n8.4 (1.2)\n11.0 (0.6)\nTutor\n52.0 (0.0)\n9.0 (0.0)\n0.8 (0.0)\n1.2 (0.0)\n1.8 (0.0)\n2.4 (0.0)\n3.6 (0.0)\n(a)\nAll Problems\nGCD\nFIBONACCI\nDIVISORSDIV3\nPALINDROME\nMERGESTRS\n0\n20\n40\n60\n80\n100\nMatch\nChatGPT\nGPT-4\nTutor\n(b)\nFigure 9: Results for the grading feedback scenario. (a) Results for various metrics aggregated\nacross all problems. (b) Results for the metric Match separately on five problems. For the metric\nMatch, these aggregated results are reported in terms of %, and higher values correspond to better\nperformance. For the metrics related to difference (D-), lower differences in points correspond to\nbetter performance. Details are in Section 5.\nevaluation. Match (binary, 1 value being better) captures the exact match between the points w.r.t.\ngrading rubric provided by the method and that provided by the human evaluator during annotation.\nD-Total (non-negative number, lower value being better) captures the absolute difference between\nthe total points provided by the method and that provided by the human evaluator during annotation.\nMoreover, we consider attributes corresponding to the grading rubric specified in the prompt; here, we\nhave used a five-dimensional rubric with rubric dimensions of program format, time complexity, space\ncomplexity, correctness for general inputs, and correctness for edge cases. For this rubric, we have\nfive additional attributes, namely, D-ProgramFormat, D-TimeComplexity, D-SpaceComplexity, D-\nCorrectGeneral, and D-CorrectEdge, that are variants of D-Total for computing absolute differences\nalong specific rubric dimensions.\nResults.\nFigure 9a provide results for various metrics aggregated across all problems, and Figure 9b\nfor the metric Match separately on five problems. These aggregated results for the metric Match are\nreported in terms of %, and higher values correspond to better performance. Next, we summarize\nsome of the key findings. First, results in Figure 9a for the metric Match highlight that even though\nGPT-4 (16.0) improves up on ChatGPT (10.0), it still performs substantially worse in comparison\nto the performance of Tutor (52.0). Further, results on various metrics suggest that the gap in the\nperformance of GPT-4 and Tutor is worse on the metric D-CorrectEdge that requires more in-depth\nreasoning about bugs in terms of failing edge cases. Second, results in Figure 9b highlight that\nthese findings are generally consistent across all five problems for the metric Match; the gap in the\nperformance of GPT-4 vs. Tutor is worst on FIBONACCI and PALINDROME. In Appendix A.4, we\nprovide an illustrative example to qualitatively show the outputs of different methods.\n6\nPair Programming Scenario\nThis section is dedicated to the programming education scenario of pair programming [17\u201320]. This\nscenario is motivated by an AI-based educational agent acting as a digital peer for a student and col-\nlaborating via completing an incomplete/partial program written by the student. In contrast to the pro-\ngram repair scenario where the input is a complete student\u2019s program with bugs, here the input is an in-\ncomplete student\u2019s program (e.g., half-done program) that the AI agent is expected to complete. Next,\nwe provide details of this scenario\u2019s prompt, input-output formats, performance metrics, and results.\nPrompt and output generation.\nWe begin by describing the content provided as input to a method\nand the desired output content we seek to generate. The input consists of a detailed problem\n9\nPrompt: Pair Programming\nI\u2019m working on a Python programming problem. I have written a part of the program. Can you help\nin completing this partial program by adding new lines of code? You should make as few changes as\npossible to already written lines in the partial program. Below I first provide the problem description\nand then my partial program.\n{problem_description}\nPartial Program:\n```\n{partial_program}\n```\nCan you complete the above partial program?\nThe code marked as #Driver Code is cor-\nrect and should not be modified. Make sure that you make minimal possible changes to already\nwritten lines in my partial program.\nFigure 10: Prompt for the pair programming scenario. This prompt has two placeholders for the\nproblem description and the partial program.\nMethod\n(Completed Program, Partial Program)\nOverall\nCorrect\nContextKept\nEditLines\nChatGPT\n38.0 ( 2.0)\n52.0 (0.0)\n86.0 ( 2.0)\n6.3 (0.0)\nGPT-4\n64.0 ( 0.0)\n92.0 (0.0)\n72.0 ( 0.0)\n7.7 (0.0)\nTutor\n82.0 (10.0)\n100.0 (0.0)\n82.0 (10.0)\n6.0 (0.1)\n(a)\nAll Problems\nGCD\nFIBONACCI\nDIVISORSDIV3\nPALINDROME\nMERGESTRS\n0\n20\n40\n60\n80\n100\nOverall\nChatGPT\nGPT-4\nTutor\n(b)\nFigure 11: Results for the pair programming scenario. (a) Results for various metrics aggregated\nacross all problems. (b) Results for the metric Overall separately on five problems. For metrics Cor-\nrect, ContextKept, and Overall, aggregated results are reported in terms of %. Details are in Section 6.\ndescription and a student\u2019s partial program; the desired output consists of a completed program.8\nFigure 10 shows the prompt used to interact with LLMs for ChatGPT and GPT-4 methods. When\ninteracting with LLMs, we first generate content using this prompt and then manually extract the\ncompleted program as the final output for evaluation.\nOutput quality and performance metrics.\nWe assess the generated output along several quality\nattributes and use aggregated results over these quality attributes as performance metrics in our\nevaluation. Correct (binary, 1 value being better) captures whether the completed program is correct\nw.r.t. the problem specification; we use automated test suites to check the correctness of a program as\nmentioned in Footnote 3. ContextKept (binary, 1 value being better) captures whether the completed\nprogram keeps the context of the partial program, e.g., variable names. EditLines (non-negative\nnumber, lower value being better) captures the line-based edit distance between the completed\n8In our evaluation, we obtain these partial programs from 25 buggy programs by removing the second half\nof the program in terms of the number of lines (only considering lines that are not part of the given template).\nImportantly, note that the partial program could have bugs or could be using some wrong algorithmic procedure.\n10\nprogram and the partial program.9 Overall (binary, 1 value being better) is 1 only when both the\nCorrect and ContextKept attributes are 1. Human evaluators annotate the quality of generated output\nfor each of the 25 instances. In this scenario, human evaluators computed attributes Correct and\nEditLines using automated scripts without requiring manual annotation, and computed the attribute\nContextKept using manual annotation.\nResults.\nFigure 11a provide results for various metrics aggregated across all problems, and Fig-\nure 11b for the metric Overall separately on five problems. These aggregated results for metrics\nCorrect, ContextKept, and Overall are reported in terms of %. Next, we summarize some of the key\nfindings. First, results in Figure 11a for the metric Overall highlight that GPT-4 (64.0) substantially\nimproves up on ChatGPT (38.0) and closed about half the gap in comparison to the performance of\nTutor (82.0). However, the results on metrics ContextKept and EditLines indicate that GPT-4 has\nthe tendency to make more edits and thereby not keep the context in the partial program provided\nas input. Second, results in Figure 11b highlight that these findings are generally consistent across\nall five problems for the metric Overall; the gap in the performance of GPT-4 vs. Tutor is worst\non FIBONACCI and PALINDROME for this scenario. Interestingly, GPT-4\u2019s performance on GCD is\nslightly better than that of Tutor. These results on specific problems of FIBONACCI, PALINDROME,\nand GCD are aligned with what we observed for the scenarios of program repair (Figure 5b) and hint\ngeneration (Figure 7b). In Appendix A.5, we provide an illustrative example to qualitatively show the\noutputs of different methods.\n7\nContextualized Explanation Scenario\nThis section is dedicated to the programming education scenario of contextualized explanation [9, 21].\nThis scenario is motivated by an AI-based educational agent acting as a digital tutor for a student\nand providing help by explaining a specific part of a given program that the student is trying to\nunderstand. In contrast to the hint generation scenario in Section 4 where the input is a student\u2019s\nbuggy program, here the input is a correct program along with a specific part (e.g., a line) that the\nAI agent is expected to explain to the student. Next, we provide details of this scenario\u2019s prompt,\ninput-output formats, performance metrics, and results.\nPrompt and output generation.\nWe begin by describing the content provided as input to a method\nand the desired output content we seek to generate. The input consists of a detailed problem\ndescription, a given program without bugs, and a specific part of the program that a student is trying\nto understand; the desired output consists of an explanation that describes this specific part in the\ncontext of the whole program.10 Figure 12 shows the prompt\u2014with placeholders for the inputs\u2014used\nto interact with LLMs for ChatGPT and GPT-4 methods. When interacting with LLMs, we first\ngenerate content using this prompt and then manually extract the contextualized explanation as the\nfinal output used for evaluation.\nOutput quality and performance metrics.\nWe assess the generated output along several quality\nattributes and use aggregated results over these quality attributes as performance metrics in our\nevaluation. All attributes for this scenario are binary, with a value of 1 being better. Correct (binary)\ncaptures whether the generated explanation contains correct information about the specific part in the\ncontext of the whole program. Complete (binary) captures whether the generated explanation contains\ncomplete information in the context of the whole program. Comprehensible (binary) captures whether\nthe generated explanation is easy to understand, presented in a readable format, and doesn\u2019t contain\nredundant information. Overall (binary) is 1 only when if the generated explanation satisfies all the\nthree attributes mentioned above. Human evaluators annotate the quality of generated output for each\nof the 25 instances; in this scenario, all the above attributes require manual annotation (similar to the\nhint generation scenario in Section 4).\n9Edit-distance between two programs is measured based on computing line-diff between them and counting\nthe number of lines that differ.\n10In our evaluation, we obtain these input programs from 25 buggy programs by fixing all bugs; importantly,\nprograms used as input for this scenario are correct. For a given program, we select a specific part as the program\nline with the most depth in the Abstract Syntax Tree representation of the program (in the case of ties, we select\na line with the higher line number in the program).\n11\nPrompt: Contextualized Explanation\nI\u2019m trying to understand a given program for a Python programming problem. Can you help by\nexplaining a specific part of this program? Below I first provide the problem description, then the\nprogram, and then a specific part of this program.\n{problem_description}\nProgram:\n```\n{program}\n```\nSpecific Part:\n```\n{program_part_to_explain}\n```\nCan you provide a detailed explanation about the specific part above in the context of the\nwhole program?\nFigure 12: Prompt for the contextualized explanation scenario. This prompt has two placeholders\nfor the problem description and the program, and one placeholder for providing specific part of the\nprogram to be explained.\nMethod\nExplanation\nOverall\nCorrect\nComplete\nComprehensible\nChatGPT\n72.0 (12.0)\n76.0 (12.0)\n100.0 (0.0)\n94.0 (2.0)\nGPT-4\n84.0 ( 4.0)\n88.0 ( 8.0)\n98.0 (2.0)\n96.0 (0.0)\nTutor\n92.0 ( 4.0)\n92.0 ( 4.0)\n100.0 (0.0)\n100.0 (0.0)\n(a)\nAll Problems\nGCD\nFIBONACCI\nDIVISORSDIV3\nPALINDROME\nMERGESTRS\n0\n20\n40\n60\n80\n100\nOverall\nChatGPT\nGPT-4\nTutor\n(b)\nFigure 13: Results for the contextualized explanation scenario. (a) Results for various metrics\naggregated across all problems. (b) Results for the metric Overall separately on five problems. For\nall metrics, these aggregated results are reported in terms of %. Details are in Section 7.\nResults.\nFigure 13a provide results for various metrics aggregated across all problems, and Fig-\nure 13b for the metric Overall separately on five problems. These aggregated results for various\nmetrics are reported in terms of %. Next, we summarize some of the key findings. First, results\nin Figure 13a for the metric Overall highlight that GPT-4 (84.0) and ChatGPT (72.0) have high\nperformance, close to that of the performance of Tutor (92.0). One of the main reasons for this\nhigh performance in this scenario is that the methods take bug-free programs as input; moreover, the\ninput programs are short and correspond to solutions to popular problems, which makes it somewhat\neasy to provide correct contextualized explanations. Second, results in Figure 13b highlight that\nthese findings are generally consistent across all five problems for the metric Overall; the gap in the\nperformance of GPT-4 vs. Tutor is worst on MERGESTRS for this scenario. In Appendix A.6, we\nprovide an illustrative example to qualitatively show the outputs of different methods.\n12\nPrompt: Task Synthesis\nI\u2019m helping a novice student on a Python programming problem. The student\u2019s program below\nhas bug(s). Can you help by creating a new simpler problem along with a minimal buggy program\nthat highlights a bug in the student\u2019s program? Below I first provide the problem description, the\nstudent\u2019s buggy program, and then a fix to the student\u2019s buggy program.\n{problem_description}\nStudent\u2019s Buggy Program:\n```\n{buggy_program}\n```\nFix to the Student\u2019s Buggy Program:\n```\n{line_diffs_with_fixed_program}\n```\nBased on the above, can you create a new simpler problem along with a buggy program\nthat has the same type of bug(s)? If the student\u2019s buggy program has multiple bugs, it is ok to focus\non only one of those bugs. Make sure that the new problem is simpler than the original problem.\nFigure 14: Prompt for the task synthesis scenario. This prompt has two placeholders for the problem\ndescription and the student\u2019s buggy program, and one placeholder for providing fix to the student\u2019s\nbuggy program in the form of line differences.\n8\nTask Synthesis Scenario\nThis section is dedicated to the programming education scenario of task synthesis [9, 22\u201324]. This\nscenario is motivated by an AI-based educational agent acting as a digital assistant for an educator\nor digital tutor for a student \u2013 the agent provides assistance/help by generating new tasks (in the\nform of debugging quizzes) that exercise specific types of bugs the student is encountering. Next, we\nprovide details of this scenario\u2019s prompt, input-output formats, performance metrics, and results.\nPrompt and output generation.\nWe begin by describing the content provided as input to a method\nand the desired output content we seek to generate. The input consists of a detailed problem\ndescription, a student\u2019s buggy program, and fixes to the buggy program as line-diffs with fixed\nprogram; the desired output consists of a new debugging task comprising (new problem, new buggy\nprogram).11 Figure 14 shows the prompt used to interact with LLMs for ChatGPT and GPT-4 methods.\nWhen interacting with LLMs, we first generate content using this prompt and then manually extract\nthe new debugging task, i.e., (new problem, new buggy program) as the final output for evaluation.\nOutput quality and performance metrics.\nWe assess the generated output along several quality\nattributes and use aggregated results over these quality attributes as performance metrics in our\nevaluation. All attributes for this scenario are binary, with a value of 1 being better. Correct (binary)\ncaptures whether the generated new problem is correct in terms of its description and specification,\nand can be solved. Simpler (binary) captures whether the generated new problem is simpler than the\ninput problem. SimilarBugs (binary) captures whether the generated new buggy program has bug(s)\nsimilar to bug(s) in the student\u2019s buggy program. MinimalBugs (binary) captures that the generated\nnew buggy program doesn\u2019t contain any other bugs. Overall (binary) is 1 only if the generated new\nproblem and new buggy program jointly satisfy all the four attributes mentioned earlier. Human\nevaluators annotate the quality of generated output for each of the 25 instances; in this scenario, all\nthe above attributes require manual annotation.\n11In our evaluation, we obtain these fixed programs from 25 buggy programs by fixing all bugs with a minimal\nnumber of edits required.\n13\nMethod\n(New Problem, New Buggy Program)\nNew Problem\nNew Buggy Program\nOverall\nCorrect\nSimpler\nSimilarBugs MinimalBugs\nChatGPT\n10.0 (2.0)\n78.0 (10.0) 66.0 (2.0)\n36.0 (0.0)\n76.0 (8.0)\nGPT-4\n22.0 (2.0)\n94.0 ( 6.0) 88.0 (4.0)\n40.0 (0.0)\n76.0 (8.0)\nTutor\n74.0 (2.0)\n98.0 ( 2.0) 98.0 (2.0)\n92.0 (4.0)\n82.0 (6.0)\n(a)\nAll Problems\nGCD\nFIBONACCI\nDIVISORSDIV3\nPALINDROME\nMERGESTRS\n0\n20\n40\n60\n80\n100\nOverall\nChatGPT\nGPT-4\nTutor\n(b)\nFigure 15: Results for the task synthesis scenario. (a) Results for various metrics aggregated across\nall problems. (b) Results for the metric Overall separately on five problems. For all metrics, these\naggregated results are reported in terms of %. Details are in Section 8.\nResults.\nFigure 15a provide results for various metrics aggregated across all problems, and Fig-\nure 15b for the metric Overall separately on five problems. These aggregated results for various\nmetrics are reported in terms of %. Next, we summarize some of the key findings. First, results in\nFigure 15a for the metric Overall highlight that even though GPT-4 (22.0) improves up on ChatGPT\n(10.0), it still performs substantially worse in comparison to the performance of Tutor (74.0). Fur-\nther, results on various metrics suggest that the gap in the performance of GPT-4 and Tutor is worse\non the metric SimilarBugs that requires a more in-depth understanding of bugs in the input program\nand then transferring them to a newly generated program. Second, results in Figure 15b highlight that\nthese findings are generally consistent across all five problems for the metric Overall; the gap in the\nperformance of GPT-4 vs. Tutor is worst on DIVISORSDIV3 and PALINDROME. In Appendix A.7,\nwe provide an illustrative example to qualitatively show the outputs of different methods.\n9\nConcluding Discussions\nWe conducted a study to benchmark state-of-the-art generative AI and large language models for a\ncomprehensive set of programming education scenarios. Our results show that GPT-4 drastically\noutperforms ChatGPT (based on GPT-3.5) and comes close to human tutors\u2019 performance for several\nscenarios. These results also highlight scenarios and specific problems where GPT-4 still struggles,\nin particular, for the scenarios of grading feedback and task synthesis that have a substantial gap in\nthe performance of GPT-4 compared to that of human tutors.\nNext, we discuss some limitations of our current work and ideas to tackle them in the future. First,\nour work involved only two human experts acting as tutors and evaluators; it would be useful to scale\nup the study. Second, we focused only on introductory Python programming education; it would be\ninteresting to conduct a similar study for other programming languages and other domains beyond pro-\ngramming. Third, we considered English as the primary mode of language, and it would be interesting\nto evaluate these models in multilingual settings. Fourth, our evaluation only considered expert-based\nassessments and didn\u2019t involve students; it would be useful to consider student-based assessments.\nApart from the above extensions, there are many exciting directions for future work, including but\nnot limited to: (a) curating larger-scale benchmarks that the research community can use to evaluate\nnew versions of these models; (b) evaluating alternate generative models, in particular, open-source\nvariants; (c) developing techniques to improve the performance of generative AI and large language\nmodels, e.g., by leveraging symbolic methods, fine-tuning, or automated prompting; (d) conducting\nstudies in classrooms with students.\n14\nAcknowledgments and Disclosure of Funding\nFunded/Co-funded by the European Union (ERC, TOPS, 101039090). Views and opinions expressed\nare however those of the author(s) only and do not necessarily reflect those of the European Union or\nthe European Research Council. Neither the European Union nor the granting authority can be held\nresponsible for them.\nReferences\n[1] Tung Phung, Victor-Alexandru P\u02d8adurean, Jos\u00e9 Cambronero, Sumit Gulwani, Tobias Kohn,\nRupak Majumdar, Adish Singla, and Gustavo Soares. Generative AI for Programming Education:\nBenchmarking ChatGPT, GPT-4, and Human Tutors. In ICER V.2, 2023.\n[2] OpenAI. ChatGPT. https://openai.com/blog/chatgpt, 2023.\n[3] OpenAI. GPT-4 Technical Report. CoRR, abs/2303.08774, 2023.\n[4] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi,\nMarco T\u00falio Ribeiro, and Yi Zhang. Sparks of Artificial General Intelligence: Early Experiments\nwith GPT-4. CoRR, abs/2303.12712, 2023.\n[5] David Baidoo-Anu and Leticia Owusu Ansah. Education in the Era of Generative Artificial\nIntelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and\nLearning. Available at SSRN 4337484, 2023.\n[6] Weng Marc Lim, Asanka Gunasekara, Jessica Leigh Pallant, Jason Ian Pallant, and Ekaterina\nPechenkina.\nGenerative AI and the Future of Education: Ragnar\u00f6k or Reformation? A\nParadoxical Perspective from Management Educators. The International Journal of Management\nEducation, 21(2):100790, 2023.\n[7] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather.\nThe Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Pro-\ngramming. In ACE, 2022.\n[8] Jialu Zhang, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust\nVerbruggen. Repairing Bugs in Python Assignments Using Large Language Models. CoRR,\nabs/2209.14876, 2022.\n[9] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic Generation of Program-\nming Exercises and Code Explanations Using Large Language Models. In ICER, 2022.\n[10] Juho Leinonen, Arto Hellas, Sami Sarsa, Brent N. Reeves, Paul Denny, James Prather, and\nBrett A. Becker. Using Large Language Models to Enhance Programming Error Messages. In\nSIGCSE, 2023.\n[11] Tung Phung, Jos\u00e9 Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla,\nand Gustavo Soares. Generating High-Precision Feedback for Programming Syntax Errors\nusing Large Language Models. In EDM, 2023.\n[12] Mark Chen et al. Evaluating Large Language Models Trained on Code. CoRR, abs/2107.03374,\n2021.\n[13] geeksforgeeks.org. GeeksforGeeks: A Computer Science Portal for Geeks. https://www.\ngeeksforgeeks.org/, 2009.\n[14] Jooyong Yi, Umair Z. Ahmed, Amey Karkare, Shin Hwei Tan, and Abhik Roychoudhury. A Fea-\nsibility Study of Using Automated Program Repair for Introductory Programming Assignments.\nIn ESEC/FSE, 2017.\n[15] Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil T. Heffernan, Xintao Wu, and Dongwon\nLee. MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics\nEducation. CoRR, abs/2106.07340, 2021.\n[16] Hiroaki Funayama, Tasuku Sato, Yuichiroh Matsubayashi, Tomoya Mizumoto, Jun Suzuki, and\nKentaro Inui. Balancing Cost and Quality: An Exploration of Human-in-the-Loop Frameworks\nfor Automated Short Answer Scoring. In AIED, 2022.\n[17] GitHub. GitHub Copilot: Your AI Pair Programmer. https://github.com/features/\ncopilot, 2022.\n15\n[18] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. Reading Between the\nLines: Modeling User Behavior and Costs in AI-Assisted Programming. CoRR, abs/2210.14306,\n2022.\n[19] Saki Imai. Is Github Copilot a Substitute for Human Pair-programming? An Empirical Study.\nIn ICSE Companion Proceedings, 2022.\n[20] Qianou Ma, Tongshuang Wu, and Kenneth R. Koedinger. Is AI the Better Programming Partner?\nHuman-Human Pair Programming vs. Human-AI pAIr Programming. CoRR, abs/2306.05153,\n2023.\n[21] Hannah Potter, Ardi Madadi, Ren\u00e9 Just, and Cyrus Omar. Contextualized Programming\nLanguage Documentation. In SIGPLAN Onward!, 2022.\n[22] Umair Z. Ahmed, Maria Christakis, Aleksandr Efremov, Nigel Fernandez, Ahana Ghosh, Abhik\nRoychoudhury, and Adish Singla. Synthesizing Tasks for Block-based Programming. In\nNeurIPS, 2020.\n[23] Ahana Ghosh, Sebastian Tschiatschek, Sam Devlin, and Adish Singla. Adaptive Scaffolding in\nBlock-Based Programming via Synthesizing New Tasks as Pop Quizzes. In AIED, 2022.\n[24] Victor-Alexandru P\u02d8adurean, Georgios Tzannetos, and Adish Singla. Neural Task Synthesis for\nVisual Programming. CoRR, abs/2305.18342, 2023.\n[25] Mikhail Mirzayanov. Codeforces. https://codeforces.com/.\n[26] Y Daniel Liang. Introduction to Programming using Python. 2013.\n[27] Ana\nBell,\nEric\nGrimson,\nand\nJohn\nGuttag.\nIntroduction\nto\nComputer\nScience\nand\nProgramming\nin\nPython.\nhttps://ocw.mit.edu/courses/\n6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/\npages/lecture-slides-code/, 2016.\n[28] OpenAI.\nChatGPT model=text-davinci-002.\nhttps://chat.openai.com/?model=\ntext-davinci-002-render-sha, 2023.\n[29] OpenAI. GPT-4 model=gpt-4. https://chat.openai.com/?model=gpt-4, 2023.\n[30] Georg Brandl, Matth\u00e4us Chajdas, and Jean Abou-Samra. Pygments. https://pygments.\norg/.\n16\nAppendix A\nAppendix\nThis appendix provides illustrative examples for six programming education scenarios discussed in\nSections 3\u20138. For each scenario, we have picked one illustrative example to highlight settings where\nGPT-4 still struggles. These examples provide further insights and potential ideas for future work on\ndeveloping techniques to improve the performance of these models. For each example, we first show\nspecific inputs provided in the prompt, followed by the outputs from ChatGPT, GPT-4, and Tutor.\nThe detailed problem descriptions used in prompts are mentioned in Footnote 3.\nWhen presenting these illustrative examples, we obfuscate the input programs by changing variable\nnames and formatting styles while keeping the original bugs exactly the same. Accordingly, we make\nthe same modifications in the generated output to align it with these changes.\nNext, we provide a brief description of the content in the rest of this appendix.\n\u2022 Appendix A.1 provides solution programs generated by GPT-4 for all problems.\n\u2022 Appendix A.2 provides an example for the program repair scenario on FIBONACCI.\n\u2022 Appendix A.3 provides an example for the hint generation scenario on PALINDROME.\n\u2022 Appendix A.4 provides an example for the grading feedback scenario on FIBONACCI.\n\u2022 Appendix A.5 provides an example for the pair programming scenario on DIVISORSDIV3.\n\u2022 Appendix A.6 provides an example for the contextualized explanation scenario on MERGESTRS.\n\u2022 Appendix A.7 provides an example for the task synthesis scenario on GCD.\nA.1\nSolution Programs for Five Problems\nFigure 3 in Section 2 provided a solution program generated by GPT-4 for FIBONACCI. Fig-\nures 16, 17, 18, and 19 below provide solution programs generated by GPT-4 for the remaining four\nproblems. GPT-4 is able to correctly solve these five problems, i.e., these generated programs pass\nautomated test suites (see Footnote 3).\n1 #User\nfunction\nTemplate\nfor\npython3\n2\nclass\nSolution:\n3\ndef\nmerge(self , S1 , S2):\n4\n# initializing\nthe\nresultant\nstring\n5\nresult = \"\"\n6\n# getting\nthe\nlength of both\nstrings\n7\nlen1 , len2 = len(S1), len(S2)\n8\n# getting\nthe\nminimum\nlength\n9\nmin_len = min(len1 , len2)\n10\n# adding\ncharacters\nalternatively\nfrom\nboth\nstrings\nto the\nresultant\nstring\n11\nfor i in range(min_len):\n12\nresult += S1[i] + S2[i]\n13\n# if S1 is longer , adding\nthe\nremaining\ncharacters\nof S1 to the\nresultant\nstring\n14\nif len1 > len2:\n15\nresult += S1[min_len :]\n16\n# if S2 is longer , adding\nthe\nremaining\ncharacters\nof S2 to the\nresultant\nstring\n17\nelif\nlen1 < len2:\n18\nresult += S2[min_len :]\n19\n# returning\nthe\nresultant\nstring\n20\nreturn\nresult\n21\n22 #{\n23 # Driver\nCode\nStarts\n24 #Initial\nTemplate\nfor\nPython 3\n25 if\n__name__\n== \u2019__main__ \u2019:\n26\nt = int(input ())\n27\nfor _ in range(t):\n28\nS1 ,S2 = map(str ,input ().strip ().split ())\n29\nob = Solution ()\n30\nprint(ob.merge(S1 , S2))\n31 # } Driver\nCode\nEnds\nFigure 16: A solution program generated by GPT-4 for MERGESTRS.\n17\n1 #User\nfunction\nTemplate\nfor\npython3\n2\nclass\nSolution:\n3\ndef gcd(self , A, B):\n4\n# If B is 0 then A is the GCD\n5\nif B == 0:\n6\nreturn A\n7\n# Else we\nrecursively\ncall\nthe gcd\nfunction\n8\n# with B and the\nremainder\nwhen A is\ndivided\nby B\n9\nelse:\n10\nreturn\nself.gcd(B, A % B)\n11\n12 # {\n13 # Driver\nCode\nStarts\n14 #Initial\nTemplate\nfor\nPython 3\n15\n16 if\n__name__\n== \u2019__main__ \u2019:\n17\nt = int(input ())\n18\nfor _ in range(t):\n19\nA,B = list(map(int , input ().strip ().split ()))\n20\nob = Solution ()\n21\nprint(ob.gcd(A,B))\n22 # } Driver\nCode\nEnds\nFigure 17: A solution program generated by GPT-4 for GCD.\n1 #User\nfunction\nTemplate\nfor\npython3\n2\nimport\nmath\n3\n4\nclass\nSolution:\n5\ndef\ncount_divisors (self , N):\n6\ncount = 0\n7\ni = 1\n8\nwhile i * i <= N:\n9\nif N % i == 0:\n10\n# Check if\ndivisors\nare\ndivisible\nby 3\n11\nif i % 3 == 0:\n12\ncount += 1\n13\nif i != N // i and (N // i) % 3 == 0:\n14\ncount += 1\n15\ni += 1\n16\nreturn\ncount\n17\n18 # Driver\nCode\nStarts\n19 #Initial\nTemplate\nfor\nPython 3\n20 if\n__name__\n== \u2019__main__ \u2019:\n21\nt = int (input ())\n22\nfor _ in range (t):\n23\nN = int(input ())\n24\nob = Solution ()\n25\nprint(ob. count_divisors (N))\n26 # } Driver\nCode\nEnds\nFigure 18: A solution program generated by GPT-4 for DIVISORSDIV3.\n1 #User\nfunction\nTemplate\nfor\npython3\n2\nclass\nSolution:\n3\ndef\nisPalindrome (self , S):\n4\n# Start\nfrom\nthe\nends\nand\nwork\ninwards\n5\nleft , right = 0, len(S) - 1\n6\nwhile\nleft < right:\n7\n# If the\ncharacters\ndon \u2019t match , it\u2019s not a palindrome\n8\nif S[left] != S[right ]:\n9\nreturn 0\n10\nleft , right = left + 1, right - 1\n11\n# All\ncharacters\nmatched , it\u2019s a palindrome\n12\nreturn 1\n13\n14 #{\n15 # Driver\nCode\nStarts\n16 #Initial\nTemplate\nfor\nPython 3\n17 if\n__name__\n== \u2019__main__ \u2019:\n18\nT=int(input ())\n19\nfor i in range(T):\n20\nS = input ()\n21\nob = Solution ()\n22\nanswer = ob. isPalindrome (S)\n23\nprint(answer)\n24 # } Driver\nCode\nEnds\nFigure 19: A solution program generated by GPT-4 for PALINDROME.\n18\nA.2\nIllustrative Example for the Program Repair Scenario\nFigure 20 shows an example for the program repair scenario on FIBONACCI problem. In this example,\nthe student\u2019s buggy program shown in Figure 20a misses some numbers at the end of the series for a\nfew test cases. In the generated programs shown in Figures 20b, 20c, and 20d, we have highlighted\nthe lines where the input buggy program has been modified. ChatGPT generates an incorrect program\n(i.e., the attribute Correct is 0). GPT-4 generates a correct program (i.e., the attribute Correct is 1);\nhowever, it makes large number of edits (EditTokens is 33). Tutor generates a correct program with\na small number of edits (i.e., EditTokens is 1).\n1 class\nSolution:\n2\ndef\nnFibonacci (self , N):\n3\nres = [0, 1]\n4\nfor i in range (2, N + 1):\n5\nif(res[i - 1] + res[i - 2]\n<= N):\n6\nres.append(res[i - 1] + res[i - 2])\n7\nelse:\n8\nbreak\n9\nreturn(res)\n(a) Input: Buggy program\n1 class\nSolution:\n2\ndef\nnFibonacci (self , N):\n3\nres = [0, 1]\n4\nfor i in range (2, N):\n5\nif res[i - 1] + res[i - 2]\n<= N:\n6\nres.append(res[i - 1] + res[i - 2])\n7\nelse:\n8\nbreak\n9\nreturn\nres\n(b) Output by ChatGPT\n1 class\nSolution:\n2\ndef\nnFibonacci (self , N):\n3\nres = [0, 1]\n4\nwhile\nTrue:\n5\nnext_val = res [-1] + res [-2]\n6\nif\nnext_val\n<= N:\n7\nres.append(next_val)\n8\nelse:\n9\nbreak\n10\nreturn(res)\n(c) Output by GPT-4\n1 class\nSolution:\n2\ndef\nnFibonacci (self , N):\n3\nres = [0, 1]\n4\nfor i in range (2, N + 2):\n5\nif res[i - 1] + res[i - 2]\n<= N:\n6\nres.append(res[i - 1] + res[i - 2])\n7\nelse:\n8\nbreak\n9\nreturn(res)\n(d) Output by Tutor\nFigure 20: An illustrative example for the program repair scenario on FIBONACCI problem.\n19\nA.3\nIllustrative Example for Hint Generation Scenario\nFigure 21 shows an example for the hint generation scenario on PALINDROME problem. In this exam-\nple, the student\u2019s buggy program shown in Figure 21a wrongly considers all strings with odd lengths\nas not palindrome. In the generated outputs shown in Figures 21b, 21c, and 21d, we have highlighted\nthe text where the hint/explanation is wrong. Both ChatGPT and GPT-4 generate an incorrect hint\n(i.e., the attribute HintCorrect is 0). Tutor\u2019s output meets all the quality criteria (i.e., Overall is 1).\n1 class\nSolution:\n2\ndef\nisPalindrome (self , S):\n3\nif len(S) % 2 != 0:\n4\nreturn 0\n5\nfor i in range(len(S)):\n6\nif S[i] != S[-i - 1]:\n7\nreturn 0\n8\nreturn 1\n(a) Input: Buggy program\nHint\nThe condition that checks if the length of the string is even is incorrect.\nExplanation\n[...] The program has two bugs:\n- The condition \u2018if len(S) % 2 != 0\u2019 incorrectly checks if the length of the string is odd, but it\nshould check if it is even.\n- The loop \u2018for i in range(len(S))\u2019 iterates through [...]\n(b) Output by ChatGPT\nHint\nThink about how a palindrome string behaves with respect to the symmetry of its characters,\nregardless of whether its length is odd or even. Additionally, reconsider how you\u2019re comparing\nmirrored indices in the string.\nExplanation\nThere are two main bugs in your code:\n- First, your assumption that the string cannot be a palindrome if its length is odd is incorrect. [...]\n- Secondly, you are comparing characters at mirrored indices incorrectly. Python allows for\nnegative indexing, with \u2018-1\u2019 being the last element, \u2018-2\u2019 the second last, and so forth. In your code,\nyou\u2019re comparing \u2018S[I]\u2019 with \u2018S[-i-1]\u2019, which will actually compare the first character with the last,\nsecond with second last and so on, but when \u2018i\u2019 reaches the midpoint of the string, \u2018-i-1\u2019 becomes\npositive and starts from the start of the string, which is incorrect.\n(c) Output by GPT-4\nHint\nA string with an odd number of characters can also be palindrome (e.g., \"aba\" is palindrome).\nExplanation\nFor Input: pjxcxjp\nYour Code\u2019s output is: 0\nIt\u2019s Correct output is: 1\nLook at line 3-4. Note that a string with an odd number of characters can also be palindrome.\n(d) Output by Tutor\nFigure 21: An illustrative example for the hint generation scenario on PALINDROME problem.\n20\nA.4\nIllustrative Example for Grading Feedback Scenario\nFigure 22 shows an example for the grading feedback scenario on FIBONACCI problem. In this\nexample, the student\u2019s buggy program shown in Figure 22a misses some numbers at the beginning\nof the series for most test cases. Both ChatGPT and ChatGPT fail to recognize this issue in the\nstudent\u2019s program and wrongly assign high points for the rubric dimensions of CorrectGeneral (i.e.,\nCorrectness for general inputs) and CorrectEdge (i.e., Correctness for edge cases). Interestingly,\nGPT-4 also writes a detailed textual description where it wrongly reasons about its grading strategy:\n\u201c[...] Correctness for general inputs (30 points): The program seems to handle gen-\neral inputs well, but it does have an issue. The program does not return the correct\nresult for N = 0. However, as per the problem statement, N is always greater than or\nequal to 1. So, this doesn\u2019t impact the grading in this category. Score: 30/30 [...]\u201d.\nTutor recognizes this issue in the program and correctly assigns 0 points for these two dimensions.\n1 class\nSolution:\n2\ndef\nnFibonacci (self , N):\n3\nres = []\n4\nif N == 0:\n5\nres.append (0)\n6\nif N == 1:\n7\nres.append (0)\n8\nres.append (1)\n9\nres.append (1)\n10\nx = 1\n11\ny = 1\n12\nz = x + y\n13\nwhile z <= N:\n14\nres.append(z)\n15\nx, y = y, z\n16\nz = x + y\n17\nreturn\nres\n(a) Input: Buggy program\nMethod\nPoints Across Grading Rubric\nTotal\nProgramFormat\nTimeComplexity\nSpaceComplexity\nCorrectGeneral\nCorrectEdge\nChatGPT\n90\n10\n15\n15\n30\n20\n(b) Output by ChatGPT\nMethod\nPoints Across Grading Rubric\nTotal\nProgramFormat\nTimeComplexity\nSpaceComplexity\nCorrectGeneral\nCorrectEdge\nGPT-4\n90\n10\n15\n15\n30\n20\n(c) Output by GPT-4\nMethod\nPoints Across Grading Rubric\nTotal\nProgramFormat\nTimeComplexity\nSpaceComplexity\nCorrectGeneral\nCorrectEdge\nTutor\n40\n10\n15\n15\n0\n0\n(d) Output by Tutor\nFigure 22: An illustrative example for the grading feedback scenario on FIBONACCI problem.\n21\nA.5\nIllustrative Example for Pair Programming Scenario\nFigure 23 shows an example for the pair programming scenario on DIVISORSDIV3 problem. In this\nexample, the incomplete/partial program is shown in Figure 23a. In the generated programs shown in\nFigures 23b, 23c, and 23d, we have highlighted the lines where the input partial program has been\nupdated. ChatGPT generates an incorrect program (i.e., the attribute Correct is 0). GPT-4 generates\na correct program and keeps the context (i.e., attributes Correct and ContextKept are 1); however,\nit makes a large number of edits (EditLines is 10). Tutor generates a correct program, keeps the\ncontext, and makes a small number of edits (i.e., EditLines is 4).\n1 from\nmath\nimport\nsqrt\n2\nclass\nSolution:\n3\ndef\ncount_divisors (self , N):\n4\nsq = int(sqrt(N))\n5\ncount = 0\n6\nfor i in range (1, sq + 1):\n7\nif N % i == 0 and i % 3 == 0:\n(a) Input: Partial program\n1 from\nmath\nimport\nsqrt\n2\nclass\nSolution:\n3\ndef\ncount_divisors (self , N):\n4\nsq = int(sqrt(N))\n5\ncount = 0\n6\nfor i in range (1, sq + 1):\n7\nif N % i == 0 and i % 3 == 0:\n8\ncount += 1\n9\nif N // i != i and (N // i) % 3 == 0:\n10\ncount += 1\n11\nreturn\ncount\n(b) Output by ChatGPT\n1 from\nmath\nimport\nsqrt\n2\nclass\nSolution:\n3\ndef\ncount_divisors (self , N):\n4\nsq = int(sqrt(N))\n5\ncount = 0\n6\nfor i in range (1, sq + 1):\n7\nif N % i == 0:\n8\n# If\ndivisors\nare equal , increment\ncount by one\n9\nif N // i == i:\n10\nif i % 3 == 0:\n11\ncount += 1\n12\nelse:\n# Else\nincrement\ncount\ntwice as we count\nboth i and N/i\n13\nif i % 3 == 0:\n14\ncount += 1\n15\nif N // i % 3 == 0:\n16\ncount += 1\n17\nreturn\ncount\n(c) Output by GPT-4\n1 from\nmath\nimport\nsqrt\n2\nclass\nSolution:\n3\ndef\ncount_divisors (self , N):\n4\nsq = int(sqrt(N))\n5\ncount = 0\n6\nfor i in range (1, sq + 1):\n7\nif N % i == 0 and i % 3 == 0:\n8\ncount += 1\n9\nif N % i == 0 and i != (N / i) and (N / i) % 3 == 0:\n10\ncount += 1\n11\nreturn\ncount\n(d) Output by Tutor\nFigure 23: An illustrative example for the pair programming scenario on DIVISORSDIV3 problem.\n22\nA.6\nIllustrative Example for Contextualized Explanation Scenario\nFigure 24 shows an example for the contextualized explanation scenario on MERGESTRS problem.\nIn this example, the input program and a specific part of the program (line 11) to be explained are\nshown in Figure 24a. Note that the input program has no bugs; see Footnote 10. In the generated\nexplanations shown in Figures 24b, 24c, and 24d, we have highlighted the text where the explanation\nis wrong. Both ChatGPT and GPT-4 generate an incorrect explanation (i.e., the attribute Correct is\n0). Tutor\u2019s output meets all the quality criteria (i.e., Overall is 1).\n1 class\nSolution:\n2\ndef\nmerge(self , S1 , S2):\n3\nx = \u2019\u2019\n4\nz = S1 + S2 [:: -1]\n5\ni = 0\n6\nj = len(z) - 1\n7\nn = min ([ len(S1), len(S2)])\n8\nfor k in range (0, n):\n9\nif i != j:\n10\nx += z[i]\n11\nx += z[j]\n12\n13\ni += 1\n14\nj -= 1\n15\nif len(S1) > len(S2):\n16\nx = x + S1[n:]\n17\nelse:\n18\nx = x + S2[n:]\n19\nreturn x\n(a) Input: Program and a specific part (line 11)\nIn the provided program, the specific part \u2018x += z[j]\u2019 is used to append a character to\nthe string \u2018x\u2019. [...] Now, focusing on the specific part you mentioned (\u2018x += z[j]\u2019), it is\nresponsible for appending the character at index \u2018j\u2019 of the string \u2018z\u2019 to the result string \u2018x\u2019.\nThis line ensures that the characters from \u2018S2\u2019 (which were concatenated in reverse order\nwith \u2018S1\u2019) are merged alternatively with the characters from \u2018S1\u2019. In the loop, when \u2018i\u2019 is\nnot equal to \u2018j\u2019, it means that the loop is not at the middle character of \u2018z\u2019 (when the lengths\nof \u2018S1\u2019 and \u2018S2\u2019 are odd). Therefore, both the characters at index \u2018i\u2019 and \u2018j\u2019 are appended\nto a one after the other, ensuring the alternative merging of characters. [...]\n(b) Output by ChatGPT\nThe line of code \u2018x += z[j]\u2019 is part of a larger program to merge two strings S1 and S2\nalternatively. [...] Now let\u2019s get to the part of the program with the specific line you\u2019re\nasking about. The for loop iterates \u2018k\u2019 from 0 to \u2018n - 1\u2019. Inside the loop: The condition\n\u2018if i != j\u2019: is checking if the indices are different. This prevents adding the same character\ntwice when the lengths of the input strings are different. [...]\n(c) Output by by GPT-4\n\u2018x += z[j]\u2019 adds the j-th character of \u2018z\u2019 to the resulting string \u2018x\u2019. The algorithm used in\nthis program is quite complex. \u2018z\u2019 is obtained by concatenating S1 and the reverse of S2.\nThen, the function iteratively takes one character from the start of \u2018z\u2019 and one character\nfrom the end of \u2018z\u2019 to the resulting string \u2018x\u2019. In particular, \u2018x += z[i]\u2019 adds one character\nfrom the start of \u2018z\u2019 to \u2018x\u2019. \u2018x += z[j]\u2019 adds one character from the end of \u2018z\u2019 to \u2018x\u2019.\n(d) Output by Tutor\nFigure 24: An illustrative example for the contextual explanation scenario on MERGESTRS problem.\n23\nA.7\nIllustrative Example for Task Synthesis Scenario\nFigure 25 shows an example for the task synthesis scenario on GCD problem. In this example, the\nstudent\u2019s buggy program shown in Figure 25a has a mistake in passing the arguments to the recursive\nfunction call. ChatGPT\u2019s output is of bad quality w.r.t. the attributes SimilarBugs and MinimalBugs.\nGPT-4\u2019s output is of bad quality w.r.t. the attribute SimilarBugs as there is no bug in the generated\nprogram as required. Interestingly, GPT-4 also writes a detailed textual description\u2014not shown\nhere\u2014where it wrongly reasons about how it has generated new problem and new buggy program\nthat matches the prompt requirements. Tutor\u2019s output meets all the quality criteria (i.e., Overall is 1).\n1 class\nSolution:\n2\ndef gcd(self , A, B):\n3\nif B == 0:\n4\nreturn A\n5\nelse:\n6\nreturn\nself.gcd(A, B % A)\n(a) Input: Buggy program\nNew Problem\nSum of Two Numbers: Given two positive integers A and B, find the sum of A and B. You don\u2019t need\nto read input or print anything. Complete the function \u2018sum_of_two_numbers()\u2019 which takes two\npositive integers as input parameters and returns an integer.\nExample: Input: A = 4, B = 5. Output: 9. Explanation: The sum of 4 and 5 is 9.\nNew Buggy Program\n1 class\nSolution:\n2\ndef\nsum_of_two_numbers (self , A, B):\n3\nif A == 0:\n4\nreturn B\n5\nelse:\n6\nreturn\nself. sum_of_two_numbers (B, A - 1)\n(b) Output by ChatGPT\nNew Problem\nMultiplication of Two Positive Numbers: Given two positive integers A and B, return the result of A\nmultiplied by B. You don\u2019t need to read input or print anything. Complete the function multiply()\nwhich takes two positive integers as input parameters and returns an integer.\nExample 1: Input: A = 3, B = 6. Output: 18. Explanation: 3 multiplied by 6 is 18.\nExample 2: Input: A = 2, B = 5. Output: 10. Explanation: 2 multiplied by 5 is 10.\nNew Buggy Program\n1 class\nSolution:\n2\ndef\nmultiply(self , A, B):\n3\nif B == 1:\n4\nreturn A\n5\nelse:\n6\nreturn A + self.multiply(A, B -1)\n(c) Output by GPT-4\nNew Problem\nEuclidean Division: Find the reminder of A divided by B, using successive subtractions. A and B are\npositive integers.\nExample 1: Input: A = 5, B = 3. Output: 2\nExample 2: Input: A = 10, B = 3. Output: 1\nExample 3: Input: A = 10, B = 5. Output: 0\nNew Buggy Program\n1 class\nSolution:\n2\ndef\nreminder(self , A, B):\n3\nif A < B:\n4\nreturn A\n5\nreturn\nself.reminder(B, A-B)\n(d) Output by Tutor\nFigure 25: An illustrative example for the task synthesis scenario on GCD problem.\n24\n"
  },
  {
    "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
    "link": "https://arxiv.org/pdf/2306.17107.pdf",
    "upvote": "11",
    "text": "LLaVAR: Enhanced Visual Instruction Tuning\nfor Text-Rich Image Understanding\nYanzhe Zhang1\u2217, Ruiyi Zhang2, Jiuxiang Gu2, Yufan Zhou2, Nedim Lipka2,\nDiyi Yang3, Tong Sun2\n1Georgia Tech, 2Adobe Research, 3Stanford University\nAbstract\nInstruction tuning enhances the capability of Large Language Models (LLMs) to\ninteract with humans. Furthermore, recent instruction-following datasets include\nimages as visual input, collecting responses for image-based instructions. However,\ncurrent visual instruction-tuned models cannot comprehend textual details within\nimages well. This work enhances the current visual instruction tuning pipeline with\ntext-rich images (e.g., movie posters, book covers, etc.). Specifically, we first used\npublicly available OCR tools to collect results on 422K text-rich images from the\nLAION dataset. Furthermore, we prompt text-only GPT-4 with recognized text and\nimage captions to generate 16K conversations, each containing question-answer\npairs for text-rich images. By combining our collected data with previous multi-\nmodal instruction-following data, our model, LLaVAR, substantially improves the\ncapability of the LLaVA model on text-based VQA datasets (up to 20% accuracy\nimprovement). The GPT-4-based instruction-following evaluation also demon-\nstrates the improvement of our model on both natural images and text-rich images.\nThrough qualitative analysis, LLaVAR shows promising interaction skills ( e.g.,\nreasoning, writing, and elaboration) with humans based on the latest real-world\nonline content that combines text and images. We make our code/data/models\npublicly available.\n1\nIntroduction\nInstruction tuning [1, 2] improves generalization to unseen tasks by formulating various tasks into\ninstructions. Such open-ended question-answering capability fosters the recent chatbot boom since\nChatGPT. Recently, visual instruction-tuned models [3\u20135] further augment conversation agents\nwith visual encoders such as CLIP-ViT [6, 7], enabling human-agent interaction based on images.\nHowever, possibly due to the dominance of natural images in training data (e.g., Conceptual Captions\n[8] and COCO [9]), they struggle with understanding texts within images [10]. However, textual\nunderstanding is integral to visual perception in everyday life.\nFortunately, tools such as Optical Character Recognition (OCR, 11) allow us to recognize text in\nimages. One naive way to utilize this is to add recognized texts to the input of visual instruction-tuned\nmodels [12]. However, such approach significantly increases the computation (longer context lengths),\nand might not fully leverage the encoding capability of visual encoders. To this end, we propose to\nenhance the end-to-end visual instruction-tuned model by collecting instruction-following data that\nrequire understanding texts within images.\n\u2217Collaborations through Adobe University Gift Program.\nPreprint. Work in progress.\narXiv:2306.17107v2  [cs.CV]  2 Feb 2024\nFigure 1: The process of collecting high-quality instruction-following data.\nSpecifically, we first collect 422K noisy instruction-following data using text-rich2 images by com-\nbining manually written instructions (e.g., \u201cIdentify any text visible in the provided image.\u201d) and\nthe OCR results. Such large-scale noisy-aligned data effectively improve feature alignment be-\ntween visual features and the language decoder. Furthermore, we prompt text-only GPT-4 [13] with\nOCR results and image captions to generate 16K conversations, where each conversation can be\nmultiple turns of question & answer pairs, as high-quality instruction-following examples. This\nprocess requires GPT-4 to de-noise the OCR results and develop specific questions to create complex\ninstructions based on the input (Figure 1).\nTo evaluate the effectiveness of the collected data, we use noisy and high-quality examples to augment\nthe pretraining and fine-tuning stages of LLaVA [3] accordingly. We name our model LLaVAR,\nsignifying the LLaVA (Large Language and Vision Assistant) that can Read. Compared to the\noriginal LLaVA, we also conducted experiments scaling the input resolution from 2242 to 3362\nto better encode small textual details. Empirically, we report the results on four text-based VQA\ndatasets following the evaluation protocol from Liu et al. [10]. Moreover, we apply GPT-4-based\ninstruction-following evaluation to 30 natural images from COCO [9, 3] and 50 text-rich images from\nLAION [14]. We also provide qualitative analysis (e.g., on posters, website screenshots, and tweets)\nto test more complex instruction-following skills.\nTo sum up, our contributions are as follows:\n\u2022 We collect 422K noisy instruction-following data and 16K high-quality instruction-following\ndata. Both are shown to be effective in augmenting visual instruction tuning.\n\u2022 Our model, LLaVAR, significantly enhances text understanding within images while slightly\nimproving the model\u2019s performance on natural images.\n\u2022 The enhanced capability enables our model to provide end-to-end interactions based on\nvarious forms of online content that combine text and images.\n\u2022 We open source the training and evaluation data together with the model checkpoints.\n2\nRelated Work\nInstruction Tuning\nFollowing natural language instructions is the key capability for an agent to\ninteract with real-world users. Instruction tuning starts from collecting human-preferred feedback for\nhuman written instructions [1] or formulating multi-task training in a multi-task instruction-following\nmanner [2, 15]. However, large, capable instruction-tuned models are usually closed-sourced and\nserve as commercial APIs only. Recently, Alpaca [16, 17], Vicuna [18], and Baize [19] start the trend\nof generating high-quality instruction-following data based on LLMs such as GPT-3.5 / ChatGPT /\n2In this work, we use the phrase \u201ctext-rich images\u201d to describe images with text in them, such as posters and\nbook covers. In contrast, we refer to images without text as \u201cnatural images\u201d.\n2\nGPT-4 and finetuning the open source LLaMA model [20]. However, evaluating the ability to follow\ninstructions remains a challenge. While GPT-4 has demonstrated superior evaluation capabilities [21],\nthere are still a number of concerns, such as bias toward response length [19] and lack of robustness\nto the order of examples [22]. Following Chiang et al. [18], Liu et al. [3], Dubois et al. [23], we use\nGPT-4-based instruction-following evaluation in this work.\nMultimodal Instruction Tuning\nRecently, instruction tuning has been expanded to the multi-\nmodal setting, including image, video [24, 25], and audio [26, 27]. For image-based instruction\ntuning, MiniGPT-4 [28] employs ChatGPT to curate and improve detailed captions for high-quality\ninstruction-following data. LLaVA [3] generates multimodal instruction-following data by prompt-\ning text-only GPT-4 with captions and object\u2019s bounding boxes. LLaMA-Adapter [29, 12] uses\nCOCO data for text-image feature alignment and utilizes textual data only for instruction tuning.\nmPLUG-owl [30] combines more than 1000M image-text pairs for pretraining and a 400K mixture\nof text-only/multimodal instruction-following data for finetuning. However, according to Liu et al.\n[10], most of these models struggle to accomplish tasks requiring OCR capability. InstructBLIP\n[31] transforms 13 vision-language tasks (including OCR-VQA [32]) into the instruction-following\nformat for instruction tuning. Cream [33] applies multi-task learning that includes predicting masked\ntexts in images. A more comprehensive survey can be found in Li et al. [34]. In this work, we select\nLLaVA as our baseline, which is the most data-efficient and powerful model, and demonstrate the\neffectiveness of our proposed pipeline.\n3\nData Collection\nStarting from the LAION-5B [14] dataset 3, our goal is only to keep images that are text-rich.\nConsidering that documents usually contain plenty of text, we first obtained a binary classifica-\ntion dataset by combining natural images and document data. Subsequently, we trained an im-\nage classifier using a DiT [35]-base backbone, which was fine-tuned on the RVL-CDIP dataset\n[36]. Hopefully, such a classifier can predict whether an image contains text or not. We first\nbuild a subset by selecting images with a predicted probability greater than 0.8 while also sat-\nisfying p(watermark) < 0.8 and p(unsafe) < 0.5 4.\nThe derived subset is noisy due to\nthe limitation of the classifier. To further clean up the data and incorporate human judgment,\nFigure 2: CLIP-based categorization of our collected im-\nages. The left refers to images used to collect noisy data,\nand the right refers to images used in the GPT-4 prompting.\nBoth pie charts are based on 10K sampled images from the\ncorresponding datasets.\nwe randomly sampled 50K images\nand clustered them into 100 clusters\nbased on CLIP-ViT-B/32 visual fea-\ntures. After inspecting the clustering\nresults, we carefully select 14 clusters\n(see Figure 10 in the Appendix for ex-\namples) containing diverse text-rich im-\nages ranging from posters, covers, ad-\nvertisements, infographics, educational\nmaterials, and logos. The cluster model\nis then used as the filter to collect im-\nages for constructing our instruction-\nfollowing examples. As a reference,\nwe provide a CLIP [7]-based catego-\nrization (see Appendix A for details.)\nto illustrate the distribution of images\nfor both two types of data we collected\nin Figure 2. We summarize our col-\nlected data and LLaVA\u2019s data in Table\n1.\nNoisy Instruction-following Data\nUsing the clustering model as a filter, we collect 422K dedu-\nplicated images that belong to the 14 preferred clusters. To balance the examples from different\ncategories, we keep at most 52K examples for one cluster. We run all images through PaddleOCR 5.\n3https://huggingface.co/datasets/laion/laion-high-resolution\n4Both probabilities are from the LAION dataset\u2019s metadata.\n5https://github.com/PaddlePaddle/PaddleOCR\n3\nData\nImage\nInstruction\n# Conv\nAvg Ins Len\nAvg Res Len\nLLaVA pretraining\nCC3M\nCC3M\n595K\n15.9\n15.4\nRpretraining (Ours)\nLAION\nPaddleOCR\n422K\n17.2\n48.8\nLLaVA finetuning\nCOCO\nGPT-4\n158K\n15.9\n93.1\nRfinetuning (Ours)\nLAION\nGPT-4\n16K\n15.1\n40.5\nTable 1: Summary of data statistics. Rpretraining and Rfinetuning denote the additional pre-training /\nfinetuning data we collected. The average instruction and response length are calculated after LLaMA\ntokenization.\nNote that running OCR at the original resolution (e.g.,10242) might recognize small fonts that are\nnot visible by visual encoders like CLIP ViT (6, 7, resolution up to 3362). To ensure the recognition\nof visible fonts while maintaining OCR accuracy, we perform OCR on the image after downsampling\n(the short edge is resized to 384 pixels if longer than that.) to extract the text. Then, based on\nthe geometric relationships between the recognized words, we merge them into paragraphs before\nconcatenating them. As a robust instruction-following model should react similarly to instructions\nwith similar meanings, we reword \u201cIdentify any text visible in the provided image.\u201d into ten distinct\ninstructions (Table 6 in Appendix). We then create a single-turn conversation for a given image by\n(i) randomly sampling an input instruction and (ii) using recognized texts as the desired output\nresponse. Such instruction-following data is noisy because of the relatively limited performance of\nOCR tools on diverse fonts and colorful backgrounds.\nGPT-4-based Instruction-following Data\nCompared to high-quality instruction-following data,\nthere are mainly two issues for the noisy data collected above. (i) Responses should contain organized\nsentences instead of raw OCR results with missing words and grammar errors. (ii) Instructions should\nbe diverse, suitable and specific to the given image instead of monotonously asking for all visible\ntexts. To address these issues, we follow Liu et al. [3] to generate instruction-following data by\nprompting text-only GPT-4 [13] with OCR results and captions.\nIt is challenging to prompt GPT-4 with fragmented OCR results in a few words to generate non-trivial\ninstructions. To this end, we carefully select 4 of the 14 previously mentioned clusters (the 3rd, 4th,\n6th and 9th clusters in Figure 10) to collect images with enough visible and coherent sentences. As\nshown in Figure 2, such filtering dramatically increases the percentage of book covers and quote\nimages. We randomly selected 4K examples from each cluster (no overlap with images used for noisy\ninstruction-following data), yielding a total of 16K images. Following prior work [16, 17, 3], we\nprovide the visualization of verb-noun pairs for instructions generated by GPT-4 in Appendix Figure\n11. For those instructions without a verb-noun pair, we demonstrate the frequency of objects being\nasked in Appendix Figure 12.\nFurthermore, based on the system message and two in-context few-shot examples (shown in Appendix\nB), we ask GPT-4 to generate conversational data based on OCR results and image captions (Figure\n1). The generated questions are used as input instructions, and answers are used as output responses.\nConcretely, for a given image, we first provide two OCR results from EasyOCR and PaddleOCR,\nwhich can complement each other. To illustrate visual elements other than texts within the image, we\nalso provide the result of BLIP-2 image captioning [37]. To prevent the caption from focusing on the\ntext, we use OCR bounding boxes to mask the text and then use the inpainting [38] to refill the mask\nbefore using generation captions. Note that captioning models might suffer from hallucinations [39].\nWe mention this unreliability in our system message and ask GPT-4 only to generate questions with\nsure answers. We leave the generation of more detailed captions [40, 41] for future work.\n4\nModel Architecture and Training\nArchitecture\nIn most of our study, we use the same model architecture as LLaVA. For the visual\nencoder V , we use CLIP-ViT-L/14 for 2242 resolution and CLIP-ViT-L/14-336 for 3362 resolu-\ntion. The grid features before the last transformer layer are then transformed into the word embedding\nspace of the language decoder through a trainable projection matrix W. We use Vicuna-13B [18], a\n4\nFigure 3: The model training process for the visual encoder V , projection matrix W, and language\ndecoder D. Blue blocks denote frozen modules and yellow blocks denote trainable modules. The\ntraining input is image tokens (<img>) and instruction tokens (<ins>), while the target is response\ntokens (<res>).\nLLaMA-based [20] instruction-tuned language model, as the language decoder D except the ablation\nstudy in Table 4.\nIn Section 5.1 and Appendix H, we extend the current architecture by adding an extra high-resolution\n(high-res) visual encoder. Such a high-res encoder outputs thousands of patch features, which means\nthat the transformed features and instruction tokens cannot fit in the context length of the language\ndecoder. To this end, we propose to add cross-attention modules to the decoder, which attend to\nkey-value pairs transformed from the high-res patch features.\nTraining\nWe follow the two-stage training design of LLaVA (Figure 3). The training objectives of\nboth stages are the same: generate output responses (<res>) for the input instructions (<ins>). The\ntransformed image tokens (<img>) are added before or after the first input instruction. (i) During\nthe first pre-training stage, only the projection matrix W is trained for feature alignment. Since\nthe decoder D is frozen, training tolerates noisy data. In the pre-training stage, we combine the\n595K pre-training data from LLaVA with our 422K noisy instruction-following data. (ii) Both the\nprojection matrix W and the language decoder D are trained during the finetuning stage, where we\nmerge our 16K instruction-following data into the 158K instruction-following data from LLaVA as\nthe training set. Note that the visual encoder is frozen throughout the training period, which might\nrestrict text recognition performance, as CLIP is trained for general-purpose text-image alignment.\nBetter choices of the visual encoder [42] or CLIP-ViT finetuning [30] may further benefit the visual\nunderstanding capability, which we leave for future work.\n5\nExperiments\nWe use the same training hyperparameters as LLaVA6, except that (i) We set the maximum sequence\nlength to 1024 during pre-training. (ii) We first pad any given image to a square shape before resizing\nit to the desired input size, preventing some image content from cropping during preprocessing. For\nboth resolutions (2242, 3362), we reproduce the original LLaVA for a fair comparison. The GPT-4\nmodel used in this work refers to the gpt-4-0314 version, while the cost to collect finetuning data is\naround $300. The temperature used to sample GPT-4 is set to 1.0 for the generation of training data,\n0.7 for the generation of evaluation data, and 0.2 for the evaluation based on GPT-4. All experiments\nare run on NVIDIA A100 80GB GPUs. During the evaluation, the temperature used to sample from\nour model is set at 0.9 for text-based VQA, 0.7 for GPT-4-based instruction-following evaluation,\nand 0.2 for other qualitative demonstrations.\n5.1\nQuantitative Analysis\nText-based VQA\nFollowing the evaluation protocol in Liu et al. [10], we evaluate the performance\nof LLaVAR on four text-based VQA datasets: ST-VQA [45], OCR-VQA [32], TextVQA [46], and\nDocVQA [47], representing various domains (see Appendix C for more details and Appendix E for\n6https://github.com/haotian-liu/LLaVA\n5\nRes.\nST-VQA\nOCR-VQA\nTextVQA\nDocVQA\nBLIP-2 [2023] \u2020\n2242\n21.7\n30.7\n32.2\n4.9\nOpenFlamingo [2023] \u2020\n19.3\n27.8\n29.1\n5.1\nMiniGPT4 [2023] \u2020\n14.0\n11.5\n18.7\n3.0\nLLaVA [2023] \u2020\n22.1\n11.4\n28.9\n4.5\nmPLUG-Owl [2023] \u2020\n29.3\n28.6\n40.3\n6.9\nLLaVA \u2021\n2242\n24.3\n10.8\n31.0\n5.2\nLLaVAR\n30.2 (+5.9)\n23.4 (+12.6)\n39.5 (+8.5)\n6.2 (+1.0)\nLLaVA \u2021\n3362\n28.9\n11.0\n36.7\n6.9\nLLaVAR\n39.2 (+10.3)\n23.8 (+12.8)\n48.5 (+11.8)\n11.6 (+4.7)\nTable 2: Results (accuracy %) on text-based VQA. We use \u2020 to refer to the results obtained from\nLiu et al. [10] and \u2021 to refer to our reproduced results. The accuracy metric used by Liu et al. [10]\nonly counts for whether the ground truth appears in the response. For more metrics, please refer to\nAppendix D.\nST-VQA\nOCR-VQA\nTextVQA\nDocVQA\n(1) LLaVA\n28.9\n11.0\n36.7\n6.9\n(2) LLaVA + Rpretraining\n36.7\n26.1\n46.5\n9.6\n(3) LLaVA + Rfinetuning\n34.1\n21.6\n43.6\n9.5\n(4) LLaVA + Cpretraining\n35.4\n27.0\n45.6\n9.2\n(5) LLaVA + Nfinetuning\n34.1\n25.9\n43.3\n10.2\n(6) LLaVAR\n39.2\n23.8\n48.5\n11.6\nTable 3: Ablation Study on pretraining/finetuning data. All results are from 3362-based models.\nRpretraining and Rfinetuning denote the extra pretraining/finetuning data we collected. Cpretraining refers\nto using captions instead of OCR results as responses during pretraining. Nfinetuning refers to using\nwritten questions + raw OCR results instead of GPT-generated QA for finetuning.\nmore datasets). We present the results of the baseline models and our models in Table 2. Note that\nInstructBLIP includes OCR-VQA in its training sets, making it incomparable with our settings. In\nboth resolution settings and all four datasets, LLaVAR substantially improves the LLaVA baseline,\ndemonstrating that our collected data can bring about a robust improvement. Furthermore, the\nimprovement is more significant in 3362 resolution compared to 2242, indicating that the collected\ndata might bring a greater improvement at even higher resolutions. Our best model, 3362-based\nLLaVAR, performs best in 3 out of 4 evaluated datasets. Note that this is not a fair comparison. Some\nkey factors include different language decoders, resolutions, and magnitudes of text-image training\ndata. We provide more discussions on the comparison with mPLUG-Owl and the result of finetuning\nmPLUG-Owl using our data in Appendix F.\nAblation Study on pretraining/finetuning data\nWe report the result in Table 3 and Figure 4.\n(i) Based on variants (2) and (3), we find that the collected data can benefit the pretraining stage\n(Rpretraining) and finetuning stage (Rfinetuning) separately while being complementary to each other in\nmost cases 7. More importantly, enhancing the pretraining stage alone achieves the second-best overall\nperformance, indicating the potential to boost textual detail understanding without dependence on\nGPT-4-generated high-quality data. (ii) Using pretraining images, we obtain Cpretraining by replacing\nthe pretraining instructions with questions & captions, the same pattern as LLaVA. As variant (4) is\nnot as good as (2), we can conclude that OCR is more advantageous than captions. (iii) We further\nvalidate the value of GPT-4 generated data by generating noisy finetuning data (Nfinetuning), similar\nto pretraining data. Variant (5) achieves comparable accuracy as variant (3). However, as shown in\nFigure 4, such noisy finetuning data hurts the instruction-following capability: (5) responds with all\nrecognized texts while ignoring the questions.\n7Since the metric only consider the recall, it might favor variant (2)(4)(5) due to their longer outputs.\n6\nCLIP Res.\nExtra Enc.\nRpretraining\nST-VQA\nOCR-VQA\nTextVQA\nDocVQA\n(a)\n2242\n\u2717\nLow\n28.9\n25.6\n37.8\n6.2\n(b)\n3362\n\u2717\nLow\n37.4\n31.0\n45.7\n11.4\n(c)\n2242\n\u2717\nHigh\n28.9\n24.9\n35.8\n6.2\n(d)\n3362\n\u2717\nHigh\n36.9\n30.6\n46.5\n11.7\n(e)\n2242\n\u2713\nLow\n31.8\n30.9\n41.2\n8.1\n(f)\n2242\n\u2713\nHigh\n33.5\n29.8\n42.2\n10.2\n(g)\n2242\n\u2713\nHigh (2\u00d7)\n35.8\n30.7\n45.6\n15.3\nTable 4: Ablation Study on encoders and image resolution. \u201cExtra Enc.\u201d refers to whether we use\nPix2Struct-base [44] as an extra high-res visual encoder. \u201cRpretraining\u201d refers to images used for\npretraining (before preprocessing) and the collection of OCR results, where \u201cLow\u201d refers to the\nresized images (the same as the rest of this paper), \u201cHigh\u201d refers to the original high-res images, and\n\u201c2\u00d7\u201d means doubling the number of examples using the same criteria. All results from this table use\nVicuna-7B, as adding the extra encoder requires more computation and memory.\nQuestion: Who wrote this book?\nGT\nSandra Boynton\n(1)\nThe book was written by Sandra Boynton.\n(2)\nThe book was written by Sandra Boyton. It is a children\u2019s book\nthat features a family calendar with an accompanying activity\nguide. The book provides information (...) The cover of the book\ndisplays a picture of the family from the calendar, which helps\nto visually represent the topic of the book.\n(3)\nThe book is written by Sandra Byington.\n(4)\nThe book was written by Sandra Boynton.\n(5)\nWith 500+ Nity StickerS &aSwEllMagnetic Phone List! Moms\n291573016171800 FAMII (Pocket) Calendar (...) 2016 SAN-\nDRA BOYNTON\n(6)\nThe book was written by Sandra Boynton.\nFigure 4: Ablation study based an example from OCR-VQA. GT refers to ground truth, and (1) - (6)\nare different model variants from Table 3. We replace the excessively long response with (...).\nAblation Study on encoders/image resolution\nWhile keeping finetuning data the same, we report\nthe quantitative results of adding an extra visual encoder and varying the pretraining data in Table\n4. (i) Take Pix2Struct-base as an example, we find that adding an extra high-res visual encoder\nwith cross-attention indeed improves the performance ((g) vs. (a)), especially achieving the best\nzero-shot performance on DocVQA (15.3% accuracy). The performance gain on other datasets is\nrelatively limited, probably due to the extra encoder we use being pretrained on web pages instead of\nnatural images. On the other hand, the performance of (e) and (f) remains poor, without doubling\nthe number of high-res examples in Rpretraining. Given the larger number of parameters initialized in\nthe cross-attention module, they may be underfitting when trained on the same data as the projection\nmatrix W (e.g., (e) vs. (b)), similar to the finding in Zeng et al. [48]. (ii) Considering (c) vs. (a) and\n(d) vs. (b), while the images are resized to the same size after preprocessing, high-res OCR results\nturn out to be not necessarily better than the low-resolution version, suggesting the capability of the\nvisual encoder is almost saturated in (a) and (b). For more details and results on the extra high-res\nencoder, please refer to Appendix H.\nGPT-4-based instruction-following evaluation\nWe also report the GPT-4 evaluation results on\ninstruction-following questions in Table 5. (i) Natural Images: 90 questions based on 30 COCO\nvalidation images from Liu et al. [3], including three aspects: conversation, detail description, and\ncomplex reasoning. This aims to test whether our collected data will hurt, maintain, or improve the\nmodel\u2019s performance on natural images. First of all, using a higher resolution brings improvement\n(+2.9) in the performance of detail description, which is intuitive. Furthermore, LLaVAR achieves a\nbetter trade-off and increases the performance of all three aspects (+1.6 on average). More details\nare in Appendix J. (ii) Text-Rich Images: Similar to collecting the finetuning data, we leverage\n7\nRes\nConversation\nDetail\nComplex\nRead\nLLaVA (Original)\n2242\n83.1\n75.3\n96.5\n-\nLLaVA\n3362\n83.9\n78.2\n95.3\n87.9\nLLaVAR\n3362\n84.5\n78.9\n96.5\n91.7\nTable 5: Relative scores (w.r.t. text-only GPT-4) for instruction-following questions, where the first\nthree dimensions are based on natural images, the last dimension (\u201cRead\u201d) is based on text-rich\nimages. In the first row, we show the original results (2242-based) fetched from Liu et al. [3]. We\nreport our reproduced LLaVA on 3362 resolution for a fair comparison.\nH: When will this movie be released?\nL: The movie \"The Call of the Ottoman\" starring Daniel Craig is\nexpected to be released on February 14th, 2023.\nR: The movie will be released on January 13.\nH: Who starred in the movie?\nL: The movie stars Tom Cruise and Cobie Smulders.\nR: Tom Hanks starred in the movie.\nH: What is this movie about?\nL: The movie \"A Man Called Ove\" is a comedy-drama film directed\nby Hannes Holm. (...)\nR: The movie is about Tom Hanks, who is called Ottol, and it is an\ninternational bestseller based on a true story. (...)\nH: Can you describe the given image?\nL: The image is a movie poster featuring a man and a cat. The man,\nwho appears to be a character from the movie \u201cThe Ugly Truth,\u201d (...)\nR: The image features a man standing in front of a house with a cat\nbeside him.\nH: Who directed the film?\nL: The film was directed by Florian Henckel von Donnersmarck.\nR: The film was directed by Tom Hanks, who made his directorial\ndebut with this movie.\nFigure 5: Qualitative Comparison. We color the human inputs (H) in blue, LLaVA\u2019s responses (L) in\npink, and LLaVAR\u2019s responses (R) in green. Both are 3362-based models. We replace the excessively\nlong response with (...).\n50 text-rich images from LAION to collect instruction-following questions based on OCR results\nand human-annotated captions. We then collect responses from our trained model and use GPT-4\nto calculate the relative score w.r.t GPT-4 responses. We add this as an extra dimension \u201cRead\u201d to\nTable 5, where our model demonstrates a significant (+3.8) improvement. The Appendix provides an\nexample in Table 17.\n5.2\nQualitative Analysis\nWe use a recent movie poster 8 to demonstrate the difference between LLaVA and LLaVAR when\ninteracting with humans based on text-rich images. LLaVA, without augmenting textual understanding\nwithin images, suffers from hallucination when answering these questions. Some mentioned movies,\nlike \u201cA Man Called Ove\u201d and \u201cThe Ugly Truth\u201d, are real movies, suggesting that the language\ndecoder is hallucinating its internal knowledge, while the visual encoder cannot encode helpful\ninformation. Alternatively, LLaVAR can correctly answer many of the provided questions with\nfaithful information, which is clearly grounded in the image. However, some limitations remain, such\nas the spelling error \u201cottol\u201d (We provide more statistics related to such spelling errors in Appendix I).\nAlso, the final question asks for information that is not observable from the given poster, where an\nexpected response should express such uncertainty instead of giving concrete answers. Nevertheless,\nneither model correctly answers the question.\n8https://www.imdb.com/title/tt7405458/\n8\nFigure 6: Case study of the recognizable font size, in which the x-axis refers to the height of ground\ntruth answers in the image and the y-axis stands for the answer accuracy of models. We plot the\nresults for both 2242-based models and 3362-based models.\n5.3\nCase Study: Recognizable Font Size\nWe first collect 825 examples from OCR-VQA, which have answers directly presented in the image\nand are detectable by the OCR tool. By rescaling the images, we test the model\u2019s performance in\nanswering these questions while the vertical heights of answers range from 3 pixels to 19 pixels. We\nreport the result in Fig 6. (i) For the baseline model LLaVA, it struggles to provide correct answers\nin all scenarios, for both 2242-based and 3362-based versions. (ii) Our model LLaVAR achieves\nsignificantly better results in all scales. We observe a threshold for recognizable texts for both\n2242-based and 3362-based versions as the accuracy sharply decreases when the height is smaller\nthan 7 pixels. More interestingly, the 2242-based version achieves better performance on small texts\nwith 3 pixels height while the 3362-based achieves better performance on large texts with more than\n7 pixels height. We assume the extra training stage of CLIP 3362 makes it better on the larger scale\nbut worse on the smaller scale.\n5.4\nTransferred Instruction-following Capability\nAccording to the dataset statistics (Table 1) and the visualization (Figure 11), our collected instruction-\nfollowing data is not as diverse and substantial as LLaVA. This can be attributed to the relatively\nlimited information given GPT-4 compared to five different human-written captions used in LLaVA.\nThe content of text-rich images is also less diverse than that of natural images. While using more\ncomplex in-context examples can definitely stimulate generating more complicated instruction-\nfollowing examples, it can also multiply the cost. In Appendix Figure 9, we demonstrate the\ntransferred instruction-following capability of LLaVA, potentially from both the LLaVA data and\nthe Vicuna backbone. While the extra data we add mainly focuses on understanding the visible texts\nwithin images, LLaVAR manages to build its reasoning, writing, and elaboration skills based on\nthe top of its text recognition capability in an end-to-end manner. This allows users to interact with\nvarious online content based on simple screenshots.\n6\nConclusion\nIn this work, we enhance visual instruction-tuned models in terms of their capability to read texts\nin images. Using text-rich images from the LAION dataset, we collect 422K noisy instruction-\nfollowing examples using OCR results only and 16K high-quality instruction-following data based\non text-only GPT-4. These two sets of data are leveraged to augment the pretraining stage and\nfinetuning stage of LLaVA accordingly. Our model, LLaVAR, demonstrates superior performance\nin understanding texts within images and following human instructions on both prior benchmarks\nand real-world online content. Moreover, our analysis shows that the same augmented data is more\neffective with higher resolution. Additionally, using noisy instruction-following examples to augment\npretraining essentially boosts the model performance without prompting GPT-4. For future work, we\nencourage exploration of (i) better image selection criteria or domain reweighting strategy [49] and\n(ii) more data-efficient and computation-efficient ways to augment instruction-following models with\nmultimodal capability, especially in the high-res scenario.\n9\nReferences\n[1] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback, 2022.\n[2] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie\nPellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent\nZhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\nDevlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned\nlanguage models, 2022.\n[3] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n[4] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\n[5] Chunyuan Li. Large multimodal models: Notes on cvpr 2023 tutorial. ArXiv, abs/2306.14895,\n2023.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale, 2020.\n[7] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021.\n[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts, 2021.\n[9] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays,\nPietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. Microsoft coco: Common\nobjects in context, 2015.\n[10] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu\nLiu, Mingrui Chen, Chunyuan Li, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the hidden\nmystery of ocr in large multimodal models, 2023.\n[11] S. Mori, C.Y. Suen, and K. Yamamoto. Historical review of ocr research and development.\nProceedings of the IEEE, 80(7):1029\u20131058, 1992. doi: 10.1109/5.156468.\n[12] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,\nConghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient\nvisual instruction model, 2023.\n[13] OpenAI. Gpt-4 technical report, 2023.\n[14] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[15] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir\nParmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh\nPuri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy,\n10\nSumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh\nHajishirzi, and Daniel Khashabi. Super-naturalinstructions: Generalization via declarative\ninstructions on 1600+ nlp tasks, 2022.\n[16] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instruc-\ntions, 2022.\n[17] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\n[18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL\nhttps://lmsys.org/blog/2023-03-30-vicuna/.\n[19] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model\nwith parameter-efficient tuning on self-chat data, 2023.\n[20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023.\n[21] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval:\nNlg evaluation using gpt-4 with better human alignment, 2023.\n[22] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu,\nand Zhifang Sui. Large language models are not fair evaluators, 2023.\n[23] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback, 2023.\n[24] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\nmodel for video understanding. arXiv preprint arXiv:2306.02858, 2023.\n[25] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:\nTowards detailed video understanding via large vision and language models, 2023.\n[26] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning\nWu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, and Shinji Watanabe.\nAudiogpt: Understanding and generating speech, music, sound, and talking head. ArXiv,\nabs/2304.12995, 2023.\n[27] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng\nQiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational\nabilities, 2023.\n[28] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhanc-\ning vision-language understanding with advanced large language models, 2023.\n[29] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan\nLu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with\nzero-init attention, 2023.\n[30] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng\nTian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language\nmodels with multimodality, 2023.\n[31] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning, 2023.\n11\n[32] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa:\nVisual question answering by reading text in images. In 2019 international conference on\ndocument analysis and recognition (ICDAR), pages 947\u2013952. IEEE, 2019.\n[33] Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim, Sangdoo\nYun, Taeho Kil, Bado Lee, and Seunghyun Park. Cream: Visually-situated natural language\nunderstanding with contrastive reading model and frozen large language models, 2023.\n[34] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants, 2023.\n[35] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. Dit: Self-supervised\npre-training for document image transformer. Proceedings of the 30th ACM International\nConference on Multimedia, Oct 2022. doi: 10.1145/3503161.3547911. URL http://dx.doi.\norg/10.1145/3503161.3547911.\n[36] Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis. Evaluation of deep convolutional\nnets for document image classification and retrieval, 2015.\n[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models, 2023.\n[38] Alexandru Telea. An image inpainting technique based on the fast marching method. Journal\nof graphics tools, 9(1):23\u201334, 2004.\n[39] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object\nhallucination in image captioning, 2018.\n[40] Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, and Ron Kimmel. Fusecap: Leverag-\ning large language models to fuse visual data into enriched image captions, 2023.\n[41] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, and Jiebo Luo. Promptcap:\nPrompt-guided task-aware image captioning, 2022.\n[42] Michael Tschannen, Basil Mustafa, and Neil Houlsby. Clippo: Image-and-language understand-\ning from pixels only, 2022.\n[43] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. URL https:\n//doi.org/10.5281/zenodo.7733589.\n[44] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi\nKhandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot\nparsing as pretraining for visual language understanding, 2022.\n[45] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Minesh Mathew,\nC.V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Icdar 2019 competition on scene\ntext visual question answering. 2019 International Conference on Document Analysis and\nRecognition (ICDAR), Sep 2019. doi: 10.1109/icdar.2019.00251. URL http://dx.doi.org/\n10.1109/ICDAR.2019.00251.\n[46] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi\nParikh, and Marcus Rohrbach. Towards vqa models that can read. 2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), Jun 2019. doi: 10.1109/cvpr.2019.00851.\nURL http://dx.doi.org/10.1109/CVPR.2019.00851.\n[47] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: A dataset for vqa on\ndocument images, 2020.\n[48] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang,\nand Tao Kong. What matters in training a gpt4-style language model with multimodal inputs?\narXiv preprint arXiv:2307.02469, 2023.\n12\n[49] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang,\nQuoc V. Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up\nlanguage model pretraining, 2023.\n[50] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\nchains for science question answering, 2022.\n[51] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multi-\nmodal chain-of-thought reasoning in language models, 2023.\n[52] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun\nZhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language\nmodels. ArXiv, abs/2304.09842, 2023.\n[53] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li.\nVisual genome: Connecting language and vision using crowdsourced dense image annotations,\n2016.\n[54] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\n[55] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova,\nHassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, Serge Belongie, Victor Gomes,\nAbhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and\nKevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image\nclassification. Dataset available from https://github.com/openimages, 2017.\n[56] Brian Kenji Iwana, Syed Tahseen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, and Seiichi\nUchida. Judging a book by its cover, 2016.\n[57] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng Chan, and Chew Lim Tan. A robust\narbitrary text detection system for natural scene images. Expert Systems with Applications, 41\n(18):8027\u20138048, 2014.\n[58] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A\nbenchmark for question answering about charts with visual and logical reasoning, 2022.\n[59] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence\nZitnick, and Devi Parikh. VQA: Visual Question Answering. In International Conference on\nComputer Vision (ICCV), 2015.\n[60] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with\nimproved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and\nClare Voss, editors, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation\nMeasures for Machine Translation and/or Summarization, pages 65\u201372, Ann Arbor, Michigan,\nJune 2005. Association for Computational Linguistics. URL https://aclanthology.org/\nW05-0909.\n[61] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational\nLinguistics. URL https://aclanthology.org/W04-1013.\n[62] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation, 2014.\n[63] Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang\nBai. Visual information extraction in the wild: Practical dataset and end-to-end solution, 2023.\n13\nInstructions\nIdentify any text visible in the image provided.\nList all the text you can see in the given image.\nEnumerate the words or sentences visible in the picture.\nDescribe any readable text present in the image.\nReport any discernible text you see in the image.\nShare any legible words or sentences visible in the picture.\nProvide a list of texts observed in the provided image.\nNote down any readable words or phrases shown in the photo.\nReport on any text that can be clearly read in the image.\nMention any discernable and legible text present in the given picture.\nTable 6: Ten instructions asking for OCR results.\nA\nCLIP-based categorization\nBased on the observation of selected clusters, we divide the images\nused into 8 categories. For each category, we use one or multiple words as labels.\n\u2022 Quote & Meme: \u201cquote\u201d, \u201cinternet meme\u201d.\n\u2022 Poster: \u201cmovie poster\u201d, \u201cpodcast poster\u201d, \u201cTV show poster\u201d, \u201cevent poster\u201d, \u201cposter\u201d,\n\u2022 Book Cover: \u201cbook cover\u201d, \u201cmagazine cover\u201d.\n\u2022 Game Cover: \u201cgame cover\u201d.\n\u2022 Ad & Product Packaging: \u201cad\u201d, \u201cadvertisement\u201d, \u201cfood packaging\u201d, \u201cproduct packaging\u201d.\n\u2022 Infographic: \u201cchart\u201d, \u201cbar chart\u201d, \u201cpie chart\u201d, \u201cscatter plot\u201d.\n\u2022 Educational Material: \u201cexam paper\u201d, \u201cquiz\u201d, \u201ccertificate\u201d, \u201cbook page\u201d.\n\u2022 Logo: \u201clogo\u201d.\nFor each word, we use the following templates to achieve embedding-space ensembling [7]:\n\u2022 \u201ca photo of a {}.\u201d\n\u2022 \u201ca blurry photo of a {}.\u201d\n\u2022 \u201ca black and white photo of a {}.\u201d\n\u2022 \u201ca low contrast photo of a {}.\u201d\n\u2022 \u201ca high contrast photo of a {}.\u201d\n\u2022 \u201ca bad photo of a {}.\u201d\n\u2022 \u201ca good photo of a {}.\u201d\n\u2022 \u201ca photo of a small {}.\u201d\n\u2022 \u201ca photo of a big {}.\u201d\nFor each image, we calculate the similarity between the image and all words mentioned above using\nCLIP-ViT-L/14. If the highest similarity is less than 0.15, we then classify the image into Other,\notherwise we classify into the \u201csuper class\u201d (e.g., Poster) of the word (e.g., \u201cmovie poster\u201d) with the\nhighest similarity.\nB\nSystem Message\nadapted from Liu et al. [3].\n14\nYou are an AI visual assistant, and you are seeing a single image. What you see is\nprovided with two OCR results and one image caption describing the information within\nthe same image you are looking at. Image captions might include hallucinations, while\nOCR results are more accurate. Answer all questions with definite answers as you are\nseeing the image.\nDesign a conversation between you and a person asking about this photo. The answers\nshould be in a tone that a visual AI assistant is seeing the image and answering the\nquestion. Ask diverse questions and give corresponding answers.\nInclude questions asking about the visual content of the image (e.g., the man, the sunset,\nthe ocean.) and the texts contained in the image. Only include questions that have\ndefinite answers:\n(1) one can see the content in the image that the question asks about and can answer\nconfidently;\n(2) one can determine confidently from the image that it is not in the image. Do not ask\nany questions that cannot be answered confidently;\n(3) DO NOT mention OCR or image caption in your questions and answers;\n(4) DO NOT ask about information from captions while it looks unrelated to or contra-\ndicts OCR results.\nAlso include complex questions that are relevant to the content in the image, for example,\nasking about background knowledge of the texts in the image, asking to discuss about\nthe design of the image, etc. Again, do not ask about uncertain details. Provide detailed\nanswers when answering complex questions. For example, give detailed examples or\nreasoning steps to make the content more convincing and well-organized. You can\ninclude multiple paragraphs if necessary.\nFew-shot Examples\nHuman:\nWHAT F YOUR BLESSINGS COME THROUGH RAINDROPS PDf Induded LAURAS-\nTOR Y DEVOHONAL BASED ON THE GRAAAMY VARD WIN ISONG mAT HAS\nINSEIRED MILLION\nWHATIFYOUR\nBLESSINGS\nCOMETHROUGHRAINDROPS\nPDF\nIncluded\nLAURASTORY A DEVOTIONAL BASEDON THE GRAMMY AWARD WINNI\nPISONGTHATHASINSPIREDMILLIONSE\na girl is standing in a field with a rainbow\nGPT:\nQuestion: What is the name of the devotional mentioned in the image?\nAnswer: The devotional is called \u201cWhat If Your Blessings Come Through Raindrops\u201d\nby Laura Story.\nQuestion: What is special about the song mentioned in the image?\nAnswer: The song is special because it\u2019s a Grammy Award-winning song that has\ninspired millions of people.\"\nHuman:\nOne of the hardest things in life to accept is a called third strike Robert Frost te\nOne of the hardest things in life to accept is a called third strike Robert Frost quotefancy\na close up of a baseball glove\nGPT:\nQuestion: Why is the third strike hard to accept? Explain the quote to me.\nAnswer: The quote \u00d6ne of the hardest things in life to accept is a called third strike\u00e4t-\ntributed to Robert Frost is a metaphor for life situations. In baseball, a called third\nstrike is when the umpire determines that a pitch was a strike, but the batter did not\nswing. This can be frustrating for the batter because they missed an opportunity to hit\nthe ball due to either a lack of confidence or misjudgment.\nIn life, this metaphor refers to difficult moments where we might miss opportunities or\nface setbacks because of our own inaction or hesitation. The quote suggests that it is\n15\nhard to accept these missed opportunities or challenges, especially when they result\nfrom our own choices or lack of action.\nMethod\nSubject\nContext Modality\nGrade\nAverage\nNAT\nSOC\nLAN\nTXT\nIMG\nNO\nG1-6\nG7-12\nHuman [2022]\n90.23\n84.97\n87.48\n89.60\n87.50\n88.10\n91.59\n82.42\n88.40\nGPT-3.5 [2022]\n74.64\n69.74\n76.00\n74.44\n67.28\n77.42\n76.80\n68.89\n73.97\nGPT-3.5 w/ CoT [2022]\n75.44\n70.87\n78.09\n74.68\n67.43\n79.93\n78.23\n69.68\n75.17\nLLaMA-Adapter [2023]\n84.37\n88.30\n84.36\n83.72\n80.32\n86.90\n85.83\n84.05\n85.19\nMM-CoTBase [2023]\n87.52\n77.17\n85.82\n87.88\n82.90\n86.83\n84.65\n85.37\n84.91\nMM-CoTLarge [2023]\n95.91\n82.00\n90.82\n95.26\n88.80\n92.89\n92.44\n90.31\n91.68\nLLaVA [2023]\n90.36\n95.95\n88.00\n89.49\n88.00\n90.66\n90.93\n90.90\n90.92\nLLaVA+GPT-4 [2023]\n91.56\n96.74\n91.09\n90.62\n88.99\n93.52\n92.73\n92.16\n92.53\nChameleon (GPT-4) [2023]\n89.83\n74.13\n89.82\n88.27\n77.64\n92.13\n88.03\n83.72\n86.54\nLLaVAR\n91.79\n93.81\n88.73\n90.57\n88.70\n91.57\n91.30\n91.63\n91.42\nTable 7: Results (accuracy %) on Science QA dataset. All baseline results are from Liu et al. [3], Lu\net al. [52]. The categories are denoted as NAT: natural science, SOC: social science, LAN: language\nscience, TXT: text context, IMG: image context, NO: no context, G1-6: grades 1-6, G7-12: grades\n7-12.\nC\nDetails of evaluation datasets used in the main paper:\n\u2022 ST-VQA [45] contains 31791 questions that require understanding the scene text, based on\nimages from COCO [9], Visual Genome [53], ImageNet [54], etc.\n\u2022 TextVQA [46] contains 45,336 questions that need reading and reasoning about the text in\nimages to answer, based on images from OpenImages [55].\n\u2022 OCR-VQA [32] contains more than 1 million questions asking about information from book\ncover images [56].\n\u2022 DocVQA [47] contains 50000 questions based on document images.\nDetails of extra datasets in Appendix:\n\u2022 CT80 [57] contains 80 images for curved text OCR evaluation. The formats of questions\nare: (1) \u201cWhat is written in the image?\" for English words. (2) \u201cWhat is the number in the\nimage?\" for digit string.\n\u2022 POIE [46] contains 3000 camera images collected from the Nutrition Facts label of products,\ntogether with 111,155 text instances. The format of questions is \u201cWhat is {entity name} in\nthe image?\".\n\u2022 ChartQA [58] includes 4,804 charts with 9608 human-written questions.\nD\nRes.\nMETEOR\nROUGE-L\nCIDEr\nLLaVA\n2242\n7.0\n8.2\n15.3\nLLaVAR\n10.0\n11.4\n24.5\nLLaVA\n3362\n8.4\n9.9\n19.1\nLLaVAR\n12.8\n14.3\n30.9\nTable 8: Results on ST-VQA using text-matching metrics.\n16\nRes.\nMETEOR\nROUGE-L\nCIDEr\nLLaVA\n2242\n8.7\n10.5\n12.2\nLLaVAR\n12.5\n14.9\n21.4\nLLaVA\n3362\n9.9\n12.1\n15.3\nLLaVAR\n14.8\n17.4\n27.0\nTable 9: Results on textVQA using text-matching metrics.\nRes.\nMETEOR\nROUGE-L\nCIDEr\nLLaVA\n2242\n0.2\n0.1\n0.0\nLLaVAR\n0.3\n0.1\n0.0\nLLaVA\n3362\n0.3\n0.1\n0.0\nLLaVAR\n0.2\n0.1\n0.0\nTable 10: Results on OCR-VQA using text-matching metrics.\nResults of other metrics\nThe metric used for text-based VQA in the main paper is the standard\npractice in VQA benchmarks [59]. For STVQA and DocVQA, previous works use ANLS (Average\nNormalized Levenshtein Similarity) as the metric [45, 47], which calculates the average normalized\nedit distance and only works for supervised models trained to output short and precise answers. It\nworks badly for instruction-following models that usually output long sequences instead of brief\nanswers. For reference, we provide more text-matching metrics (METEOR, 60, ROUGE-L, 61,\nCIDEr, 62) to demonstrate the improvement of our model (Table 8, 9, 10, 11), which works well\nexcept for OCR-VQA. We assume these metrics are not valuable for OCR-VQA since the ground\ntruth answers are usually too short.\nE\nResults on extra datasets\nIn Table 12, we provide results on three extra datasets: CT80 (OCR, 57),\nPOIE (Information Extraction, 63), and ChartQA [58]. We use the same VQA metric as other text-\nbased VQA datasets. We observe similar trends as the main paper results: LLaVAR data significantly\nimproves over the LLaVA baseline, usually more considerably in a higher resolution.\nF\nComparison with mPLUG-Owl\nWe find that LLaVAR usually performs similarly well with\nmPLUG-Owl in the same 2242 resolution.We further clarify the setting differences between mPLUG-\nOwl and ours: mPLUG-Owl is trained on 1000M+ text-image pairs, while the original LLaVA is\ntrained on about 0.6M text-image pairs. Our model, LLaVAR, is trained on about 1M text-image pairs.\nWithin the same resolution, LLaVAR demonstrates a good performance with decent data efficiency.\nWe presume that training on large-scale non-OCR data improves OCR performance, as many of the\ncaptions in LAION datasets are equivalent to incomplete OCR results (Texts in an online image will\nsometimes appear in the captions). In the scale of our experiment, we observe similar improvement\nthat just training on captions of text-rich images can help with text recognition capability: In Table 3,\nvariant (4) is better than variant (1). However, training on captions only (variant (4)) is not as good as\ntraining on OCR-based data (variant (2)(6)), at least in the scale of our experiments.\nResults of finetuning mPLUG-Owl\nTo further validate the effectiveness of our collected data, we\nprovide the results of finetuning mPLUG-Owl using our 16K GPT-4-based instruction-following data\nin Table 13. Though the mPLUG-Owl checkpoint is extensively trained on 1000M+ text-image pairs,\nwe find that our data can boost performance in most cases, demonstrating the effectiveness of our\ndata.\n17\nRes.\nMETEOR\nROUGE-L\nCIDEr\nLLaVA\n2242\n3.8\n4.8\n6.3\nLLaVAR\n5.6\n6.9\n12.7\nLLaVA\n3362\n4.6\n5.6\n8.7\nLLaVAR\n8.6\n10.0\n21.5\nTable 11: Results on DocVQA using text-matching metrics.\nRes.\nCT80\nPOIE\nChartQA\nBLIP-2 [2023] \u2020\n2242\n80.9\n2.5\n7.2\nOpenFlamingo [2023] \u2020\n67.7\n2.1\n9.1\nMiniGPT4 [2023] \u2020\n57.3\n1.3\n4.3\nLLaVA [2023] \u2020\n61.1\n2.1\n7.3\nmPLUG-Owl [2023] \u2020\n81.9\n3.3\n9.5\nLLaVA \u2021\n2242\n61.5\n1.9\n9.2\nLLaVAR\n81.6 (+20.1)\n5.7(+3.8)\n10.2 (+1.0)\nLLaVA \u2021\n3362\n64.9\n2.5\n10.2\nLLaVAR\n83.0 (+18.1)\n8.7(+6.2)\n13.5 (+3.3)\nTable 12: Results (accuracy %) on three extra datasets: OCR, Information Extraction, and Chart\nQuestion Answering. We use \u2020 to refer to the results obtained from Liu et al. [10] and \u2021 to refer to\nour reproduced results.\nG\nScienceQA Results\nStarting from our pretrained LLaVAR (3362-based, without finetuning), we\nalso report the results of further finetuning on the ScienceQA dataset [50] in Table 7, which is a\nmultimodal multi-choice QA dataset covering diverse domains. Our motivation is that some images\nin this dataset contain text descriptions and tables that require textual understanding within images.\nThe LLaVAR model finetuned on ScienceQA achieves an average accuracy of 91.42%, better than\nLLaVA (90.92%), while the most considerable improvement comes from natural science questions\n(+1.43%).\nH\nThe original version of LLaVAR only supports up to 3362 resolution, while our case study has\nalso shown the threshold for the recognizable font size. Both suggest the difficulty of processing\nreal-world high-res images without scaling and cutting. To this end, we test a dual visual encoder\nsystem for the high-res variant of LLaVAR, where a high-res visual encoder is added to work with\nthe standard one. Ideally, the standard visual encoder extracts general, high-level information, while\nthe high-res one specifically helps with detailed information.\nArchitecture\nA high-res visual encoder usually outputs thousands of visual features. Simply\nfollowing LLaVA to feed the transformed visual features into the context of the language decoder\nis impractical, as the maximum sequence length of the language decoder is usually 2048/4096. To\nthis end, we propose to handle high-res visual features by cross-attention module and standard visual\nfeatures by feature transformation. We depict the proposed system in Figure 7.\nSpecifically, given a standard visual encoder V1, the extracted features are transformed into the\nword embedding space of the language decoder through a trainable projection matrix W. These\ntransformed features are then concatenated with the word embeddings to build the input embeddings\nof the language decoder D.\nemb(\u27e8img1\u27e9), \u00b7 \u00b7 \u00b7 , emb(\u27e8imgm\u27e9) = WV1(I)\ninput_emb = concat([emb(\u27e8img1\u27e9), \u00b7 \u00b7 \u00b7 , emb(\u27e8imgm\u27e9), emb(\u27e8ins1\u27e9), \u00b7 \u00b7 \u00b7 , emb(\u27e8insn\u27e9)]) (1)\n18\nST-VQA OCR-VQA\nTextVQA\nDocVQA\nCT80\nPOIE\nChartQA\nmPLUG-Owl\n29.3\n28.6\n40.3\n6.9\n81.9\n3.3\n9.5\nmPLUG-Owlours\n29.6\n31.2\n40.8\n7.0\n84.7\n3.7\n10.2\nTable 13: Results (accuracy %) of finetuning mPLUG-Owl. mPLUG-Owlours denotes mPLUG-Owl\nfinetuned on our 16K GPT-4-based instruction-following data.\nFigure 7: Illustration of the dual visual encoder system. Given an image, it is simultaneously\nprocessed by visual encoders V1 and V2. V1 features are transformed by transformation matrix W and\ndirectly used as input embeddings to the language model. For V2 features, they are transformed by\ntransformation matrix K and V and used as keys and values to calculate the cross attention in every\ntransformer layer (assume there are N layers), which uses the transformed hidden states (through Q)\nfrom the self-attention module as queries. For the language decoder D, the input is image tokens\n(<img>) and instruction tokens (<ins>), while the target is response tokens (<res>).\nwhere I is the input image, V1 denotes extracting the grid features before the last transformer layer.\nAt the same time, we use the high-res visual encoder V2 to extract high-res visual features, which are\nthen transformed into keys/values as the inputs of the cross-attention module in transformer layers.\nGiven hj as the hidden state before the cross-attention module in layer j,\nCrossAttention(h, V2, I) = softmax(Qjhj(KjV2(I))T\n\u221a\nd\n)V jV2(I)\n(2)\nwhere Qj, Kj, V j denotes the query/key/value projection matrix in the j-th transformers layer. In\npractice, there is a pre-attention LayerNorm before calculating the attention and another output\nprojection matrix Oj to project the aggregated values back to the hidden space.\nAs the pretrained language decoder D might only have self-attention modules, we manually add\nanother cross-attention module after the original self-attention module in every transformer layer.\nConsidering the random initialization of cross-attention modules might hurt the original language\ngeneration capability, we initialize the value projection matrix V j as a zero matrix and the output\nprojection matrix Oj as an identity matrix.\nImplementation\nWe use CLIP-ViT-L/14 as the standard visual encoder. For the high-resolution\nencoder, we test two models: (i) Pix2Struct-base [44] is a visual encoder trained on screenshot\nto HTML transformation. It supports up to 2048 patches with size 162, equivalent to 1024 \u2217 512.\n(ii) ConcatCLIP refers to using 16 CLIP-ViT-L/14 models to encode the 4 \u2217 4 grids of images\nseparately and then concatenate the extracted features together. In other words, it supports 8962\nresolution. We use Vicuna-7B as the language decoder for the high-res version of LLaVAR.\n19\nTraining\nOnly cross-attention modules and the projection matrix W are trained during pretraining,\nwhile visual encoders and the language decoder are frozen. Cross-attention modules, the projection\nmatrix W, and the language decoder D are trained during finetuning.\nData\nTo fully unlock the potential of the augmented visual encoder, we also double the number of\npretraining examples using the same criteria mentioned in Section 3. This corresponds to the variant\n(g) in Table 4.\nST-VQA\nOCR-VQA\nTextVQA\nDocVQA\nPix2Struct + LLaVA\n21.9\n11.8\n28.7\n4.4\nPix2Struct + LLaVAR\n35.8 (+13.9)\n30.7 (+18.9)\n45.6 (+16.9)\n15.3 (+10.9)\nConcatCLIP + LLaVA\n23.1\n14.2\n30.5\n5.1\nConcatCLIP + LLaVAR\n42.1 (+19.0)\n30.8 (+16.8)\n52.1 (+21.6)\n18.5 (+13.4)\nTable 14: Additional results on the dual visual encoder system.\nDiscussion\nWe report the performance of augmented architecture, using either LLaVA or LLaVAR\ndata in Table 14. By comparing the relative improvement in Table 2 and 14, we find that higher-\nresolution models benefit more from our collected data, suggesting our data is underutilized in the\noriginal LLaVA architecture.\nI\nRes.\nCorrect %\nPartially Correct%\nLLaVA\n2242\n1.6%\n8.7%\nLLaVAR\n6.8%\n22.8%\nLLaVA\n3362\n2.2%\n11.2%\nLLaVAR\n9.0%\n26.8%\nTable 15: Statistics of correct answers and partially correct answers on OCR-VQA.\nThe impact of OCR spelling errors\nWe study such OCR errors by studying 1673 examples from\nOCR-VQA, which have ground truth answers with more than ten characters. We (i) define \u201ccorrect\u201d\nas the ground truth answers that are exactly in the predictions, and (ii) define \u201cpartially correct\u201d as\nthere exists a substring in the prediction that has high enough similarity with the ground truth but\nnot the same. Specifically, we look at all substrings with the same length of the ground truth in the\nprediction to calculate ANLS (Average Normalized Levenshtein Similarity) and regard the prediction\nas \u201cpartially correct\u201d if the highest ANLS is greater or equal to 0.5 but smaller than 1.\nWe find that many predictions can be considered partially correct, indicating the actual performance\nof tested models is better than the reported accuracy numbers. However, the percentage of partially\ncorrect predictions is highly correlated with the percentage of correct predictions. Therefore, we\nbelieve that the current metrics can effectively compare the performance of different models.\nJ\nAblation Study on Instruction-following Evaluation\nTo provide a more detailed understanding\nof LLaVAR\u2019s data, we provide the ablation study results on 90 instruction-following questions from\nLLaVA [3]. We find that including pretraining data improves the conversation capability, probably\nbecause longer training data leads to generating longer responses (Table 1), which is favored by\nGPT-4 [19]. On the other hand, including only finetuning data hurts the conversation capability but\nincreases complex reasoning. Combining pretraining and finetuning data improves the trade-off\nbetween conversation and complex reasoning. Such results prove that incorporating our data will at\nleast not harm the performance of interacting with natural images.\n20\nConversation\nDetail\nComplex\nLLaVA\n83.6\n78.1\n95.2\nLLaVA + Rpretrain\n86.7\n79.3\n95.1\nLLaVA + Rfinetune\n79.0\n79.4\n98.0\nLLaVAR\n84.4\n78.9\n96.6\nTable 16: Ablation Study on GPT-4-based Instruction-following Evaluation from LLaVA [3].\nQuestion: Based on the title and the image on the cover, what can be inferred about\nthe content of \"Bo\u2019s Lasting Lessons\" and its potential target audience?\nRelative\nScore\nGPT-4\n(Text-Only)\nFrom the title and image, it can be inferred that \"Bo\u2019s Lasting Lessons\"\nis a book about leadership lessons taught by a legendary coach, likely\nin the context of sports. The image of men running suggests that the\nbook might be focused on a specific sport, such as football or track and\nfield. The target audience could be individuals interested in coaching,\nsports, or leadership in general, looking to learn from the experiences\nand teachings of a successful coach.\n-\nLLaVA\nThe book \"Bo\u2019s Lasting Lessons: The Timeless Teachings of Bo Schem-\nbechler\" is written by Bo Schembechler and Jim Harshaw. Given the\ntitle and the image on the cover, which shows a football coach running\nacross the field with his players in pursuit, it can be inferred that the\ncontent of the book is likely focused on football and the strategies, tech-\nniques, or principles that can help players improve their performance\nand achieve success in the sport. The potential target audience for this\nbook could be football players, coaches, and fans who are interested\nin learning more about the game and the experiences and insights of a\nseasoned coach like Bo Schembechler.\n75%\nLLaVAR\n\"Bo\u2019s Lasting Lessons\" appears to be a book that shares the legacy\ncoach Bo Schembechler\u2019s teachings on leadership and timeless funda-\nmentals. The image on the cover, featuring a football coach leading\nhis team, further supports this inference. The book might be targeted\ntowards sports enthusiasts, particularly fans of football, and people\ninterested in learning about leadership principles and fundamentals\nfrom a renowned coach\u2019s perspective. The audiobook, read by John H.\nMayer and featuring a foreword and conversation with John U. Bacon,\nsuggests that it could also appeal to audiences who prefer listening to\nbooks or want to gain more insights from the author and other experts.\n129%\nTable 17: An example for our collected evaluation data, corresponding to the \u201cRead\u201d dimension of\nGPT-4-based instruction-following evaluation. The corresponding image is Figure 8 in Appendix.\n21\nFigure 8: An example for the Read dimension of GPT-4-based instruction-following evaluation.\n22\nFigure 9: Transferred instruction-following capability of LLaVAR.\n23\nFigure 10: All 14 clusters we selected as text-rich images. Each row corresponds to one cluster,\nwhere we show ten randomly sampled examples before de-duplication.\n24\nFigure 11: Visualization of collected instructions.\n25\nFigure 12: Visualization of collected instructions.\n26\n"
  },
  {
    "title": "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation",
    "link": "https://arxiv.org/pdf/2306.17115.pdf",
    "upvote": "10",
    "text": "Michelangelo: Conditional 3D Shape Generation based\non Shape-Image-Text Aligned Latent Representation\nZibo Zhao1,2\u2217\nWen Liu2\u2217\nXin Chen2\nXianfang Zeng2\nRui Wang2\nPei Cheng2\nBin Fu2\nTao Chen3\nGang Yu2\nShenghua Gao1,4,5\u2020\n1ShanghaiTech University\n2Tencent PCG, China\n3School of Information Science and Technology, Fudan University, China\n4Shanghai Engineering Research Center of Intelligent Vision and Imaging\n5Shanghai Engineering Research Center of Energy Efficient and Custom AI IC\nhttps://github.com/NeuralCarver/Michelangelo\nFigure 1: Visualization of the 3D shape produced by our framework, which splits into triplets with\na conditional input on the left, a normal map in the middle, and a triangle mesh on the right. The\ngenerated 3D shapes semantically conform to the visual or textural conditional inputs.\nAbstract\nWe present a novel alignment-before-generation approach to tackle the challenging\ntask of generating general 3D shapes based on 2D images or texts. Directly\nlearning a conditional generative model from images or texts to 3D shapes is prone\nto producing inconsistent results with the conditions because 3D shapes have an\nadditional dimension whose distribution significantly differs from that of 2D images\nand texts. To bridge the domain gap among the three modalities and facilitate multi-\nmodal-conditioned 3D shape generation, we explore representing 3D shapes in a\nshape-image-text-aligned space. Our framework comprises two models: a Shape-\nImage-Text-Aligned Variational Auto-Encoder (SITA-VAE) and a conditional\nAligned Shape Latent Diffusion Model (ASLDM). The former model encodes the\n3D shapes into the shape latent space aligned to the image and text and reconstructs\n\u2217Contributed equally and work done while Zibo Zhao was a Research Intern with Tencent PCG.\n\u2020Corresponding author.\nPreprint\narXiv:2306.17115v2  [cs.CV]  3 Jul 2023\nthe fine-grained 3D neural fields corresponding to given shape embeddings via\nthe transformer-based decoder. The latter model learns a probabilistic mapping\nfunction from the image or text space to the latent shape space. Our extensive\nexperiments demonstrate that our proposed approach can generate higher-quality\nand more diverse 3D shapes that better semantically conform to the visual or\ntextural conditional inputs, validating the effectiveness of the shape-image-text-\naligned space for cross-modality 3D shape generation.\n1\nIntroduction\nConditional generative model-based 3D shaping generations, such as GAN [8, 30, 58], VAE [7, 31, 5],\nAuto-Regressive model [62, 34, 64], and Diffusion-based model [63, 36, 13, 11, 27, 37, 24], have\ngreat potential to increase productivity in the asset design of games, AR/VR, film production, the\nfurniture industry, manufacturing, and architecture construction. However, two obstacles limit their\nability to produce high-quality and diverse 3D shapes conforming to the conditional inputs: 1) diverse\n3D shape topologies are complicated to be processed into a neural network-friendly representation;\n2) since generating a high-quality 3D shape from a 2D image or textual description is an ill-pose\nproblem, and also the distribution between the 3D shape space and image or text space is quite\ndifferent, it is hard to learn a probabilistic mapping function from the image or text to 3D shape.\nRecently, the neural fields in terms of occupancy [32, 40], Signed Distance Function (SDF) [38],\nand radiance field [33] have been driving the 3D shape representation in the computer vision and\ngraphics community because their topology-free data structure, such as global latent [38], regular\ngrid latent [40, 12], and point latent [63, 64], are easier to process for neural networks in an implicit\nfunctional manner. Once arrive at a compatible space to represent different topological 3D shapes, in\nlight of the great success of auto-regressive and diffusion-based models in audio [25, 26], image [45,\n46, 44, 48, 3], video [57, 53, 18, 6], and 3D human motions [66, 55, 60], a conditional auto-regressive\nor diffusion-based generative model [13, 63, 64] is learned to sample a 3D shape in latent from an\nimage or text. However, generating a high-quality 3D shape from a 2D image or textual description\nis an ill-posed problem, and it usually requires more prior information for 3D shapes. In contrast,\nthe distribution of the 3D shape space is significantly different from the 2D image or text space, and\ndirectly learning a probabilistic mapping function from the image or text to the 3D shape might reduce\nthe quality, diversity, and semantic consistency with the conditional inputs. Prior research [63, 37]\nhas endeavored to mitigate this concern through a coarse-to-fine approach, whereby the initial step\ninvolves generating coarse point clouds as an intermediary representation, followed by the regression\nof a neural field based on the point cloud.\nUnlike the previous 3D shape representation, where the neural fields only characterize the geometric\ninformation of each 3D shape and capture the shape distribution by regularizing the shape latent\nwith KL-divergence via VAE [11, 27, 65] or VQ-VAE [34, 64], we investigate a novel 3D shape\nrepresentation that further brings the semantic information into the neural fields and designs a Shape-\nImage-Text-Aligned Variational Auto-Encoder (SITA-VAE). Specifically, it uses a perceiver-based\ntransformer [56, 22] to encode each 3D shape into the shape embeddings and utilizes a contrastive\nlearning loss to align the 3D shape embeddings with pre-aligned CLIP [43] image/text feature space\nfrom large-scale image-text pairs. After that, a transformer-based neural implicit decoder reconstructs\nthe shape of latent embeddings to a neural occupancy or SDF field with a high-quality 3D shape.\nWith the help of the aligned 3D shape, image, and text space which closes the domain gap between\nthe shape latent space and the image/text space, we propose an Aligned Shape Latent Diffusion\nModel (ASLDM) with a UNet-like skip connection-based transformer architecture [47, 4] to learn\na better probabilistic mapping from the image or text to the aligned shape latent space and thereby\ngenerate a higher-quality and more diverse 3D shape with more semantic consistency conforming to\nthe conditional image or text inputs.\nTo summarize, we explore bringing the semantic information into 3D shape representation via aligning\nthe 3D shape, 2D image, and text into a compatible space. The encoded shape latent embeddings\ncould also be decoded to a neural field that preserves high-quality details of a 3D shape. Based on\nthe powerful aligned 3D shape, image, and text space, we propose an aligned shape latent diffusion\nmodel to generate a higher-quality 3D shape with more diversity when given the image or text input.\nWe perform extensive experiments on a standard 3D shape generation benchmark, ShapeNet [10], and\n2\na further collected 3D Cartoon Monster dataset with geometric details to validate the effectiveness of\nour proposed method. All codes will be publicly available.\n2\nRelated Work\n2.1\nNeural 3D Shape Representation\nNeural Fields have dominated the research of recent 3D shape representation, which predict the\noccupancy [32, 40], Sign Distance Function (SDF), density [38, 51], or feature vectors [9] of each\n3D coordinate in the field via a neural network to preserve the high-fidelity of a specific 3D shape\nin a topology-free way. However, the vanilla neural field can only model a single 3D shape and\ncannot be generalized to other shapes. To this end, the researchers usually take additional latent\ncodes, such as a global latent [38], a regular latent grid [40, 12], a set of latent points [63, 64], or\nlatent embeddings [65, 24], which describe a particular shape along with each 3D coordinate to\nmake the neural fields generalizable to other 3D shapes or scenes. Though current neural fields\u2019\n3D representation can characterize the low-level shape geometry information and preserve the high-\nfidelity shape details, bringing the high-level semantic information into the neural fields is still a\nrelatively poorly studied problem. However, semantic neural fields are significant to downstream\ntasks, such as conditional 3D shape generations and 3D perception [21, 52].\n2.2\nConditional 3D Shape Generation\nOptimization-based approaches which employ a text-image matching loss function to optimize a\n3D representation of the neural radiance field (NeRF). Dreamfields and AvatarCLIP [23, 20] adopt a\npre-trained CLIP [43] model to measure the similarity between the rendering image and input text as\nthe matching objective. On the other hand, DreamFusion [41] and Magic3D [28] utilize a powerful\npre-trained diffusion-based text-to-image model as the optimization guidance and produce more\ncomplex and view-consistent results. However, per-scene optimization-based methods suffer from a\nlow success rate and a long optimization time in hours to generate a high-quality 3D shape. However,\nthey only require a pre-trained CLIP or text-to-image model and do not require any 3D data.\nOptimization-free methods are an alternative approach to conditional 3D shape generation that\nleverages paired texts/3D shapes or images/3D shapes to directly learn a conditional generative model\nfrom the text or image to the 3D shape representations. CLIP-Forge [50] employs an invertible\nnormalizing flow model to learn a distribution transformation from the CLIP image/text embedding to\nthe shape embedding. AutoSDF [34], ShapeFormer [62], and 3DILG [64] explore an auto-regressive\nmodel to learn a marginal distribution of the 3D shapes conditioned on images or texts and then\nsample a regular grid latent or irregular point latent shape embeddings from the conditions. In\nrecent years, diffusion-based generative models have achieved tremendous success in text-to-image,\nvideo, and human motion generation. Several contemporaneous works, including SDFusion [11],\nDiffusion-SDF [27, 13], 3D-LDM [36], 3DShape2VecSet [65], and Shap-E [24], propose to learn a\nprobabilistic mapping from the textual or visual inputs to the shape latent embeddings via a diffusion\nmodel. Since these approaches learn the prior information of the 3D shape data, they could improve\nthe yield rate of high-quality shape generation. Moreover, there is no long-time optimization process,\nand the inference time is orders of magnitude faster than the optimization-based approaches. However,\ndirectly learning a conditional generative model to sample the 3D shape from the conditions might\nproduce low-quality with less-diverse results due to the significant distribution gap between the shape\nspace and the image/text space.\n2.3\nContrastive Learning in 3D\nContrastive Language-Image Pre-training (CLIP) [43] has emerged as a fundamental model in 2D\nvisual recognition tasks and cross-modal image synthesis by building the representation connection\nbetween vision and language within an aligned space. Recent works have extended the multi-modal\ncontrastive learning paradigm to 3D. CrossPoint [1] learns the 3D-2D alignment to enhance the 3D\npoint cloud understanding. PointCLIP [68] takes full advantage of the CLIP model pre-trained on\nlarge-scale image-text pairs and performs alignment between CLIP-encoded point cloud and 3D\ncategory texts to generalize the ability of 3D zero-shot and few-shot classification. ULIP [61] and\nCLIP-goes-3D [15] further learn a unified and aligned representation of images, texts, and 3D point\n3\nPoint\nCloud\n3D Shape\nEncoder\nLearnable Query Tokens\nShape Latent \nEmbeddings \ud835\udc38!\n\ud835\udc3f\"#\n3D Shape\nDecoder\nVolume Points\n(x,y,z)\nReconstructed\nMesh\n\ud835\udc3f(!%&'(,*+&,()\n\ud835\udc3f(!%&'(,.(/.)\n\ud835\udc52!\nCLIP Image\nEncoder\nImage\na 3d model of guitar, \nESP KH-Ouija V2,  \ninstrument. \nCLIP Text\nEncoder\n\ud835\udf16!\n\ud835\udc38*\n\ud835\udc38.\n\ud835\udc61\n\ud835\udc38!\n(0)\n\ud835\udc38!\n(1)\n\ud835\udc52*\n\ud835\udc52.\n\ud835\udc3f2\na): Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE)\nb): Aligned Shape Latent Diffusion Model (ASLDM)\noccupancy\nContrastive\nContrastive\nFigure 2: Alignment-before-generation pipeline. Our method contains two models: the Shape-\nImage-Text-Aligned Variational Auto-Encoder (SITA-VAE) and the Aligned Shape Latent Diffusion\nModel (ASLDM). The SITA-VAE consists of four modules: an image encoder, a text encoder, a 3D\nshape encoder, and a 3D shape decoder. Encoders encode inputs pair into an aligned space, and the\n3D shape decoder reconstructs 3D shapes given embeddings from the aligned space. The ASLDM\nmaps the image or text condition to the aligned shape latent space for sampling a high-quality 3D\nshape embedding, which latterly reconstructed to high-fidelity 3D shapes by the 3D shape decoder.\nclouds by pre-training with object triplets from the three modalities to improve 3D understanding.\nWhile most of these works focus on 3D recognition tasks, establishing the connection between 3D\nrecognition and generation tasks remains an under-explored problem.\n3\nOur Approach\nThe direct learning of a probabilistic mapping from images or texts to 3D shapes is prone to produce\ninconsistent results due to the significant distribution gap between the 3D shapes and the 2D images\nand texts. To address this issue, we propose an alignment-before-generation solution for cross-modal\n3D shape generation, as illustrated in Figure 2. Our approach involves two models: the Shape-Image-\nText-Aligned Variational Auto-Encoder (SITA-VAE)(Section 3.1) and the Aligned Shape Latent\nDiffusion Model (ASLDM) (Section 3.2). The former model learns an alignment among the 3D\nshapes, images, and texts via contrastive learning and then reconstructs the shape embeddings back\nto the neural field. The latter model is based on the aligned space and is designed to learn a better\nconditional generative model from the images or texts to shape latent embeddings. By adopting this\nalignment-before-generation approach, we aim to overcome the challenges posed by the distribution\ngap and produce more consistent and high-quality results in cross-modal 3D shape generation.\n3.1\nShape-Image-Text Aligned Variational Auto-Encoder\nOur SITA-VAE contains four components, a pre-trained and fixed CLIP image encoder Ei and CLIP\ntext encoder Et, a trainable 3D shape encoder Es and neural field decoder Ds. The CLIP image\nencoder and text encoder take 2D images I \u2208 RH\u00d7W \u00d73 and tokenized texts T \u2208 RLt\u00d7dt as input,\nand generate image tokens Ei \u2208 R(1+Li)\u00d7d and text tokens Et \u2208 RLt\u00d7d, where (1 + Li) and Lt are\nthe sequence length of image tokens Ei and text tokens Et. We take advantage of the pre-trained\nimage encoder and text encoder from CLIP. These two encoders are trained on large-scale image-text\npairs and robust enough to capture a well-aligned vision-language space, which will enrich the\nsemantics of the 3D shape representation after multi-modal alignment via contrastive learning.\n4\n3D shape encoder aims to extract powerful feature representations to effectively characterize each 3D\nshape. To achieve this, we first sample point clouds P \u2208 RN\u00d7(3+C) from the surface of 3D shapes,\nwhere N represents the number of points, and C denotes additional point features such as normal\nor color. Next, we use a linear layer to project the concatenation of the Fourier positional encoded\npoint clouds P to the 3D shape encoder input X \u2208 RN\u00d7d. Drawing inspiration from previous\ntransformer-based architectures for point cloud understanding [22], we build our 3D shape encoder\non a perceiver-based transformer. Specifically, we use a cross-attention layer to inject the 3D shape\ninformation from the input X into a series of learnable query tokens Q \u2208 R(1+Ls)\u00d7d, where 1 + Ls\nis the length of query tokens Q, consisting of one global head token Qg \u2208 R1\u00d7d with high-level\nsemantics and Ls local tokens Ql \u2208 RL\u00d7d containing low-level geometric structure information.\nThen, several self-attention blocks are used to iteratively improve the feature representation and\nobtain the final shape embeddings, Es \u2208 R(1+Ls)\u00d7d.\nAlignment among 3D shapes, images, and texts plays a crucial role in SITA-VAE and the conditional\ngenerative models. Since the 3D data is the order of magnitudes smaller than the images and texts\ndata, to learn a better-aligned shape among 3D shapes, images, and texts, we enforce the 3D shape\nencoder close to a pre-aligned vision-language space which is pre-trained on a large-scale image-text\npair with rich image and text representations by leveraging the contrastive learning strategy. Consider\nan input pair of 3D shapes X, images I and tokenized texts T. The triplet encoders generate the\ncorresponding shape embedding es, image embedding ei and text-embedding et by projecting the\nextracted shape tokens Es, image tokens Ei and text tokens Et as three vectors with the same\ndimension, which is expressed as: es = Fs(Es), ei = Fi(Ei), and et = Ft(Et), where Fs is a\nlearnable shape embedding projector, while image embedding projector Fi and text embedding\nprojector Ft are pre-trained and frozen during training and inference. The contrastive loss is:\nL(shape,image) = \u22121\n2\nX\n(j,k)\n(log\nexp(ej\nsek\ni )\nP\nl\nexp(ej\nsel\ni)\n+ log\nexp(ej\nsek\ni )\nP\nl\nexp(elsek\ni )),\nL(shape,text) = \u22121\n2\nX\n(j,k)\n(log\nexp(ej\nsek\nt )\nP\nl\nexp(ej\nsel\nt)\n+ log\nexp(ej\nsek\nt )\nP\nl\nexp(elsek\nt )),\n(1)\nwhere (j, k) indicates the positive pair in training batches, and since we utilize pre-trained encoders\nfrom CLIP, the model is free from constraint L(image,text).\n3D shape decoder, Ds, takes the shape embeddings Es as inputs to reconstruct the 3D neural field\nin a high quality. We use the KL divergence loss LKL to facilitate the generative process to maintain\nthe latent space as a continuous distribution. Besides, we leverage a projection layer to compress\nthe latent from dimension d to lower dimensions d0 for a compact representation. Then, another\nprojection layer is used to transform the sampled latent from dimension d0 back to high dimension d\nfor reconstructing neural fields of 3D shapes. Like the encoder, our decoder model also builds on\na transformer with the cross-attention mechanism. Given a query 3D point x \u2208 R3 in the field and\nits corresponding shape latent embeddings Es, the decoder computes cross attention iterative for\npredicting the occupancy of the query point O(x). The training loss expresses as:\nLr = Ex\u2208R3[BCE(D(x|Es), O(x))],\n(2)\nwhere BCE is binary cross-entropy loss, and the total loss for training Shape-Image-Text Aligned\nVariational Auto-Encoder (SITA) is written as:\nLSIT A = \u03bbc(L(shape,image) + L(shape,text)) + Lr + \u03bbKLLKL.\n(3)\n3.2\nAligned Shape Latent Diffusion Model\nAfter training the SITA-VAE, we obtain an alignment space among 3D shapes, images, and texts, as\nwell as a 3D shape encoder and decoder that compress the 3D shape into low-dimensional shape latent\nembeddings and reconstruct shape latent embeddings to a neural field with high quality. Building on\nthe success of the Latent Diffusion Model (LDM) [46] in the text-to-image generation, which strikes a\nbalance between computational overhead and generation quality, we propose a shape latent diffusion\nmodel on the aligned space to learn a better probabilistic mapping from 2D images or texts to 3D\nshape latent embeddings. By leveraging the alignment space and the shape latent diffusion model, we\ncan generate high-quality 3D shapes that better conform to the visual or textural conditional inputs.\n5\nOur Aligned Shape Latent Diffusion Model (ASLDM) builds on a UNet-like transformer [47, 56, 4],\naim to fit a distribution of the shape latent embeddings, accompanied by an auto-encoder for encoding\ndata samples into the latent space and reconstructing the data samples given the sampled latent. By\nlearning in the latent space, the latent diffusion model is computationally efficient, and leveraging\nsuch a compact representation enables the model to fit the target distribution faster. Specifically, the\nmodel \u03f5\u03b8 focuses on generating shape latent embeddings Es conditioned on C, which is represented\nby the CLIP image or text encoder. Following LDM [46], the objective is\nL = EEs,\u03f5\u223cN(0,1),t[\u2225\u03f5 \u2212 \u03f5\u03b8(E(t)\ns , C, t)\u22252\n2],\n(4)\nwhere t is uniformaly samppled from {1, ..., T} and E(t)\ns\nis a noisy version of E(0)\ns . During inference,\nsampling a Gaussian noise, the model gradually denoises the signal until reaching E(0)\ns . Followed with\nclassifier-free guidance (CFG) [19], we train our conditional latent diffusion model with classifier-free\nguidance. In the training phase, the condition C randomly converts to an empty set \u2205 with a fixed\nprobability 10%. Then, we perform the sampling with the linear combination of conditional and\nunconditional samples:\n\u03f5\u03b8(E(t)\ns , C, t) = \u03f5\u03b8(E(t)\ns , \u2205, t) + \u03bb(\u03f5\u03b8(E(t)\ns , C, t) \u2212 \u03f5\u03b8(E(t)\ns , \u2205, t)),\n(5)\nwhere \u03bb is the guidance scale for trading off the sampling fidelity and diversity.\n4\nExperiments\nTo validate the effectiveness of our proposed framework, we conducted extensive experiments. In\nthis section, we provide implementation details of our model in Section A. We also describe the data\npreparation process, including comparisons with baselines and metrics used in our evaluation, in\nSection 4.2. Of particular importance, we present quantitative comparison results to validate our\nmodel\u2019s generation ability. Additionally, we provide visual comparison results to illustrate the quality\nof the generative outputs in Section 4.3. Also, we conduct ablation studies in Section 4.4 to validate\nthe effectiveness of training the generative model in the aligned space, the effectiveness of pre-trained\nvision-language models (VLM) on the SITA-VAE and the impact of learnable query embeddings.\n4.1\nImplementations\nWe implement our Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) based on\nperceiver-based transformer architecture [22], where the 3D shape encoder consists of 1 cross-\nattention block and eight self-attention blocks. At the same time, the neural field decoder has 16\nsefl-attention blocks with a final cross-attention block for the implicit neural field modeling. All\nattention modules are the transformer [56] style with multi-head attention mechanism (with 12 heads\nand 64 dimensions of each head), Layer Normalization (Pre-Norm) [2], Feed-Forward Network (with\n3072 dimensions) [56] and GELU activation [16]. The learnable query embeddings are E \u2208 R513\u00d7768\nwith one head-class token for multi-modal contrastive learning and left 512 shape tokens with a linear\nprojection layer to the VAE space \u2208 R512\u00d764 for the 3D shape reconstruction. Moreover, we employ\npre-train encoders in the CLIP (ViT-L-14) [43] as our visual encoder and text encoder and freeze them\nduring training and sampling. Besides, our aligned shape latent diffusion model (ASLDM) builds on\na UNet-like transformer [47, 56, 4] consisting of 13 self-attention blocks with skip-connection by\ndefault. It contains 12 heads with 64 dimensions for each, and 3076 dimensions in the Feed-Forward\nNetwork. Both models use an AdamW-based gradient decent optimizer [29] with a 1e-4 learning rate.\nOur framework is implemented with PyTorch [39], and we both train the SITA-VAE and ASLDM\nmodels with 8 Tesla V100 GPUs for around 5 days. We use DDIM sampling scheduler [54] with 50\nsteps, which generates a high-quality 3D mesh within 10 seconds.\n4.2\nDatasets and Evaluation Metrics\nDataset. We use a standard benchmark, ShapeNet [10], to evaluate our model, which provides about\n50K manufactured meshes in 55 categories. Each mesh has a category tag and corresponding texts,\nlike fine-grained categories or brief descriptions given by the creator. We follow the train/val/test\nprotocol with 3DILG [64]. We further collect 811 Cartoon Monster 3D shapes with detailed structures,\nwith 615 shapes for training, 71 shapes for validation, and 125 for testing, to evaluate the models\u2019\nability to generate complex 3D shapes. To prepare the triplet data (3D shape, image, text), we first\n6\naugment the provided texts in two ways. First, we string the shape tag and corresponding description\nin the format \"a 3D model of (shape tag), in the style of (description)\" or \"a 3D model of (shape\ntag), (description)\". Then, inspired by ULIP [61], we also leverage multiple templates containing\n65 predefined phrases to provide more text information during training. As for the image data, we\nrender each mesh under four camera poses, augmenting and improving the rendering diversity via the\ndepth-condition-based ControlNet [67].\nMetrics. We use the Intersection of Union (IoU) to reflect the accuracy of reconstructions. Then, we\npropose two new metrics for evaluating 3D shape generation methods. The first is a shape-image score\n(SI-S). We use a 3D shape encoder and image encoder to extract corresponding shape embedding and\nimage embedding and compute the Cosine Similarity of these two modalities. Another is a shape-text\nscore (ST-S), which computes the similarity between the generated 3D shape and the conditional text\ninput in the aligned shape embedding and text embedding space. Both metrics evaluate the similarity\nbetween results and their corresponding conditions. Moreover, we use both the pre-trained ULIP [61]\nand our SITA to compute SI-S and ST-S, in terms of SI-S (ULIP), ST-S (ULIP), SI-S (SITA) and ST-S\n(SITA), respectively. Besides, we follow the metrics of P-IS and P-FID as introduced in Point-E [37]\nand use a pre-trained PointNet++ [42] to compute the point cloud analogous Inception Score [49]\nand FID [17] to evaluate the diversity and quality of the generated 3D shapes.\n4.3\nExperimental Comparision\nBaselines. In the representation stage, we compare our method with Occ [32], ConvOcc [40], IF-\nNet [12], 3DILG [64], and 3DS2V [65] on reconstruction tasks to valid the ability of the model to\nrecover a neural field given shape embeddings on the ShapeNet dataset [10]. For the conditional\ngeneration stage, we choose the baselines of two recent powerful 3D generation methods, 3DILG and\n3DS2V. We first finetune their shape representation module on a mixture dataset of the ShapeNet and\nthe 3D Cartoon Monster. Then we both retrain the text and image conditional generative models of\n3DILG and 3DS2V with all the same protocols as ours.\nOverall\nSelected\nTable\nChair\nAirplane\nCar\nRifle\nLamp\nOccNet [32]\n0.825\n0.81\n0.823\n0.803\n0.835\n0.911\n0.755\n0.735\nConvOccNet [40]\n0.888\n0.873\n0.847\n0.856\n0.881\n0.921\n0.871\n0.859\nIF-Net [12]\n0.934\n0.924\n0.901\n0.927\n0.937\n0.952\n0.914\n0.914\n3DILG [64]\n0.950\n0.948\n0.963\n0.95\n0.952\n0.961\n0.938\n0.926\n3DS2V [65]\n0.955\n0.955\n0.965\n0.957\n0.962\n0.966\n0.947\n0.931\nOurs\n0.966\n0.964\n0.965\n0.966\n0.966\n0.969\n0.967\n0.95\nTable 1: Numerical results for reconstruction comparison on IoU(\u2191, a larger value is better).\nThe results show that our model has the best performance in 55 overall categories. The results of\nselected categories further prove that our model could reconstruct each category faithfully.\nImage-Conditioned\nText-Conditioned\nSI-S (ULIP)\u2191\nSI-S (SITA)\u2191\nP-FID\u2193\nP-IS\u2191\nST-S (ULIP)\u2191\nST-S (SITA)\u2191\nP-FID\u2193\nP-IS\u2191\n3DILG\n9.134\n11.703\n4.592\n12.247\n10.293\n6.878\n10.283\n12.921\n3DS2V\n13.289\n15.156\n2.921\n12.92\n12.934\n9.833\n5.704\n13.149\nOurs\n13.818\n15.206\n1.586\n13.233\n16.647\n13.128\n2.075\n13.558\nTable 2: Numerical results for conditional generation comparison. The results show that our\nmodel achieves the best generative performance. The SI-S and ST-S indicate that our model generates\nhigh-fidelity results by well-mapping the condition information to its related 3D shapes. Moreover,\nP-FID reflects that our model generates the most realistic 3D shapes, and P-IS indicates that the\ngenerated samples are diverse. \u2191 means a larger value is better, and \u2193 otherwise.\nNumerical Comparison. We report the numerical results in Table 1 and Table 2. Table 1 shows\nthat our model achieves the best reconstruction performance on 55 overall categories. Results of the\nselected category further proves that our model could faithfully reconstruct 3D shapes in each of 55\ncategories. Table 2 reports the numerical results for conditional 3D shape generation. Our model\nachieves the best on all the SI-S and ST-S, indicating that it could map the information from the image\nor text to its corresponding 3D shape information for generating high-fidelity results. Moreover,\nthe P-FID proves that our model could produce high-quality shape-tokens for generating realistic\n7\n3D shapes, and P-IS indicates the diversity of the samples. Specifically, the four left columns show\nthat our model surpasses the baselines on image-conditioned generation, proving that our model can\nbetter map visual information to 3D shapes. The four right columns validate the generative quality of\ntext-conditioned generation. Since natural language, compared to the 2D image, usually provides\nlimited and abstract information, and thus learning a model to map text information to the 3D shape\nis challenging. However, benefiting from training on the aligned latent space, our model significantly\nimproves text-conditioned generation, shown in the right of Table 2, which reflects that our model\nwell-maps natural language information to 3D shapes and generates diverse and high-quality results.\nImage\nCondition\n3DILG\n3DS2V\nOurs\nImage\nCondition\n3DILG\n3DS2V\nOurs\nImage\nCondition\n3DILG\n3DS2V\nOurs\nFigure 3: Visual results for image-conditioned generation comparison. The figure shows that\n3DILG [64] generates over-smooth surfaces and lacks details of shapes, whereas 3DS2V [65]\ngenerates few details with noisy and discontinuous surfaces of shapes. In contrast to baselines, our\nmethod produces smooth surfaces and portrays shape details. Please zoom in for more visual details.\nText\nCondition\n3DILG\n3DS2V\nOurs\nText\nCondition\n3DILG\n3DS2V\nOurs\nText\nCondition\n3DILG\n3DS2V\nOurs\nairplane, Flying \nAircraft Carrier* \naeroplane, \nspace ship* \nvessel, StarLite \n25' Sailboat* \nchair, Banqueta* \nsuitcase, Leather\nSuitcase* \ntrash barrel, \nThomas Steele \nLitter CRTR-32* \nflying monster, \nCute monster in \nthe form of flying \nbirds, with 2 feet * \nsandshrew monster, \nCute monster in the \nform of mountain \narmor, with 2 feet \nand 2 hands* \npsi monster, The \nstanding humanoid \nshape of the \nmonster, with 2 \nfeet and 2 hands* \nstanding monster, \nCute four -legged \nmonster, with 4 feet* \nfire monster, Cute \nflying monster \nin the shape of a \ndragon, with 2 feet \nand 2 hands* \nbug monster, \nflying monster in \nthe form of \nmosquitoes with 4 \nfeet* \n*denotes \u201ca 3D model of \u2026\u201d\nFigure 4: Visual results for text-conditioned generation comparison. In the first two rows, we test\nthe model with abstract texts, and the result shows that only our model could generate a 3D shape\nthat conforms to the target text with a smooth surface and fine details. The last two rows show the\nresult given texts containing detailed descriptions, which further shows that our model could capture\nthe global conditional information and the local information for generating high-fidelity 3D shapes.\nKeywords are highlighted in red; please zoom in for more visual details.\nVisual Comparison. The visual comparisons of the image- and text-conditional 3D shape generations\nillustrates in Figure 8 and Figure 9. Figure 8 shows that 3DILG [64] pays more attention to the global\n8\nshape in the auto-regressive generation process, where its results lack depictions of details of 3D\nshapes. While 3DS2V [65] generates more details of 3D shapes and discontinuous surfaces and noisy\nresults. Besides, both methods are unstable to generate a complete shape when the given conditions\nmaps to a complex object, fine machine, or rare monster. Figure 9 shows the visual comparison of\ntext-conditional generation. In the upper-half rows, we show the results given simple and abstract\nconcepts, while in the lower-half rows, we show the results given detailed texts like descriptions\nfor deterministic parts of the target shape. Similar to the observation above, 3DILG [64] generates\nan over-smooth shape surface with fewer details, and 3DS2V [65] produces fewer details on the\ndiscontinuous object surface. Therefore, only our model produces correct shapes that conform to the\ngiven concepts or detailed descriptions with delicate details on smooth surfaces.\n4.4\nAblation Studies and Analysis\nWe ablation study our model from three perspectives, the effectiveness of training generative model\nin the aligned space, the effectiveness of vision-language models (VLMs) on the SITA-VAE, and the\nimpact of learnable query embeddings.\nThe effectiveness of training generative model in the aligned space. We perform a visual com-\nparison for ablation study the effectiveness of training the generative model in the aligned space,\nas illustrated in the Figure 5. The uppers are sampled from the generative model that trains in the\naligned space, while the lowers are sampled from the generative model that trains without aligned\nspace. It proves that the uppers conform to the given text and the lower does not, which indicates that\ntraining the generative model in the aligned space leads to high-fidelity samples.\na 3d model of table, regulation table, tennis table, bench.\na 3d model of vintage car, 1938 Alfa Romeo 8C 2900B.\na) With Aligned Space\nb) Without Aligned Space\nFigure 5: Ablation study the effectiveness of training generative model in the aligned space.\nThis figure illustrates visual comparisons for ablation studies on the effectiveness of training the\ngenerative model in the aligned space. Compared with the lower samples based on the conditional\ntexts, the upper samples are closer to the conditions semantically, which indicates the effectiveness of\nthe training generative model in the aligned space.\nThe effectiveness of vision-language models. Except for the well-known vision-language model\n(VLM) CLIP [43], we introduce another vision-language model (VLM) SLIP [35] for training the\nSITA-VAE for a comprehensive comparison. First, we evaluate the impact of the vision-language\nmodel on SITA-VAE\u2019s reconstruction ability, and the results are shown in Figure 6. It shows that\nour model composed with CLIP achieves the best performance. Then, we evaluate the vision-\nlanguage model\u2019s impact on the ability to align multi-modal space. We select standard and zero-shot\nclassification tasks to reflect the impact of the vision-language model. Note that the classification is\nperformed by a feature matching operation, where we provide multiple 3D shapes and phrases to the\nSITA-VAE; it returns the similarity between 3D shapes to each phrase as classification results, which\nindicates that the more the multi-modal space is aligned, leading the higher classification accuracy.\nThe results show that our model composed with CLIP achieves the best performance.\nThe impact of the learnable query embeddings. We ablation study learnable query embeddings with\nthe same experiments as the above, and the results show that using 512 learnable query embeddings\nleads to the best performance on reconstructions and classifications.\n9\nFigure 6: Ablation study the effectiveness of vision-language models and the impact of learnable\nquery embeddings. This figure shows the ablation study on the effectiveness of the vision-language\nmodel and the impact of learnable query embeddings. According to the table, our model composed\nwith CLIP and 512 learnable query embeddings achieves the best reconstruction and classification\nperformance, indicating its ability to recover 3D shapes and align multi-modal space.\n5\nDisscusion and Conclusion\nThough our method has achieved excellent results, it still has some limitations. First, our method\nneeds the ground truth 3D shapes from training, while 3D data is usually an order of magnitude small\nthan the 2D data. Learning the shape representation with a 3D shape-image-text aligned space from\nonly 2D (multi-view) images via differentiable rendering is a promising direction. Furthermore, since\nwe represent each 3D shape as an occupancy field, it needs to convert the 3D mesh into a watertight\none, which will inevitably degrade the original quality of the 3D mesh.\nIn conclusion, we propose a novel framework for cross-modal 3D shape generation that involves\naligning 3D shapes with 2D images and text. We introduce a new 3D shape representation that can\nreconstruct high-quality 3D shapes from latent embeddings and incorporate semantic information by\naligning 3D shapes, 2D images, and text in a compatible space. This aligned space effectively closes\nthe domain gap between the shape latent space and the image/text space, making it easier to learn\na better probabilistic mapping from the image or text to the aligned shape latent space. As a result,\nour proposed method generates higher-quality and more diverse 3D shapes with greater semantic\nconsistency that conform to the conditional image or text inputs.\nReferences\n[1] Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake, Amaya Dharmasiri, Kanchana Thilakarathna,\nand Ranga Rodrigo. Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud\nunderstanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 9902\u20139912, June 2022.\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble\nof expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[4] Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based\ndiffusion models. arXiv preprint arXiv:2209.12152, 2022.\n[5] Hugo Bertiche, Meysam Madadi, and Sergio Escalera. Cloth3d: clothed 3d humans. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XX 16,\npages 344\u2013359. Springer, 2020.\n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. arXiv\npreprint arXiv:2304.08818, 2023.\n[7] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Generative and discriminative voxel\nmodeling with convolutional neural networks. arXiv preprint arXiv:1608.04236, 2016.\n[8] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath\nHariharan. Learning gradient fields for shape generation. In Proceedings of the European Conference on\nComputer Vision (ECCV), 2020.\n[9] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative\nadversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16123\u201316133, 2022.\n10\n[10] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio\nSavarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An\nInformation-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University\n\u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.\n[11] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tuyakov, Alex Schwing, and Liangyan Gui. SDFusion: Multimodal\n3d shape completion, reconstruction, and generation. arXiv, 2022.\n[12] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for 3d shape\nreconstruction and completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nIEEE, jun 2020.\n[13] Gene Chou, Yuval Bahat, and Felix Heide. Diffusionsdf: Conditional generative modeling of signed\ndistance functions. arXiv preprint arXiv:2211.13757, 2022.\n[14] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and\nlanguage knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.\n[15] Deepti Hegde, Jeya Maria Jose Valanarasu, and Vishal M Patel. Clip goes 3d: Leveraging prompt tuning\nfor language grounded 3d recognition. arXiv preprint arXiv:2303.11313, 2023.\n[16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information\nprocessing systems, 30, 2017.\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video\ngeneration with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\n[20] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip:\nZero-shot text-driven generation and animation of 3d avatars. ACM Transactions on Graphics (TOG),\n41(4):1\u201319, 2022.\n[21] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for\nvision-based 3d semantic occupancy prediction. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2023.\n[22] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver:\nGeneral perception with iterative attention. In International conference on machine learning, pages 4651\u2013\n4664. PMLR, 2021.\n[23] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object\ngeneration with dream fields. 2022.\n[24] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023.\n[25] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion\nmodel for audio synthesis. In 9th International Conference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[26] Yichong Leng, Zehua Chen, Junliang Guo, Haohe Liu, Jiawei Chen, Xu Tan, Danilo Mandic, Lei He,\nXiangyang Li, Tao Qin, et al. Binauralgrad: A two-stage conditional diffusion probabilistic model for\nbinaural audio synthesis. Advances in Neural Information Processing Systems, 35:23689\u201323700, 2022.\n[27] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-sdf: Text-to-shape via voxelized diffusion. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n11\n[29] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[30] Andrew Luo, Tianqin Li, Wen-Hao Zhang, and Tai Sing Lee. Surfgen: Adversarial 3d shape synthesis with\nexplicit surface discriminators. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 16238\u201316248, 2021.\n[31] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J\nBlack. Learning to dress 3d people in generative clothing. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 6469\u20136478, 2020.\n[32] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy\nnetworks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 4460\u20134470, 2019.\n[33] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren\nNg. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM,\n65(1):99\u2013106, 2021.\n[34] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for 3d\ncompletion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 306\u2013315, 2022.\n[35] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-\nimage pre-training. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XXVI, pages 529\u2013544. Springer, 2022.\n[36] Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero. 3d-ldm:\nNeural implicit 3d shape generation with latent diffusion models. arXiv preprint arXiv:2212.00842, 2022.\n[37] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for\ngenerating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\n[38] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf:\nLearning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 165\u2013174, 2019.\n[39] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n[40] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional\noccupancy networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August\n23\u201328, 2020, Proceedings, Part III 16, pages 523\u2013540. Springer, 2020.\n[41] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv, 2022.\n[42] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature\nlearning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021.\n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[45] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,\npages 8821\u20138831. PMLR, 2021.\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10684\u201310695, 2022.\n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages\n234\u2013241. Springer, 2015.\n12\n[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022.\n[49] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training gans. Advances in neural information processing systems, 29, 2016.\n[50] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Ka-\nmal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18603\u201318613, 2022.\n[51] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a\nhybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021.\n[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. In Proceedings of the 6th Conference on Robot Learning (CoRL), 2022.\n[53] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv\npreprint arXiv:2209.14792, 2022.\n[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[55] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, and Daniel Cohen-Or. Human\nmotion diffusion model. In International Conference on Learning Representations (ICLR), 2023.\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n[57] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video\ngeneration from open domain textual description. arXiv preprint arXiv:2210.02399, 2022.\n[58] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic\nlatent space of object shapes via 3d generative-adversarial modeling. Advances in neural information\nprocessing systems, 29, 2016.\n[59] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1912\u20131920, 2015.\n[60] Chen Xin, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your\ncommands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2023.\n[61] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos\nNiebles, and Silvio Savarese. Ulip: Learning unified representation of language, image and point cloud for\n3d understanding. arXiv preprint arXiv:2212.05171, 2022.\n[62] Xingguang Yan, Liqiang Lin, Niloy J Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Shape-\nformer: Transformer-based shape completion via sparse representation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 6239\u20136249, 2022.\n[63] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent point diffusion models for 3d shape generation. In Advances in Neural Information Processing\nSystems (NeurIPS), 2022.\n[64] Biao Zhang, Matthias Nie\u00dfner, and Peter Wonka. 3dilg: Irregular latent grids for 3d generative modeling.\nIn NeurIPS, 2022.\n[65] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation\nfor neural fields and generative diffusion models. arXiv preprint arXiv:2301.11445, 2023.\n[66] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu,\nand Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n13\n[67] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.\n[68] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and\nHongsheng Li. Pointclip: Point cloud understanding by clip. In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 8542\u20138552, 2022.\nThis appendix serves as a supplementary extension, enriching and expanding upon the core content\npresented in the main body. We first describe the training details of the shape-image-text aligned auto-\nencoder (SITA-VAE) and aligned shape latent diffusion model (ASLDM) in section A. In section B,\nwe describe more details for the zero-shot classification experiments in Figure 5 in the main text.\nFurthermore, in section C, we provide the predefined phrases for augmenting the shape-image-text\ndata pair. Benefiting from the alignment among 3D shapes, images, and texts via contrastive learning,\nour model can retrieve 3D shapes given a query image, and we show the visual result in section D.\nWe also show more visual comparisons in section E. Moreover, we test our model with conditioning\ninput from the internet and show results in section F. Note that HTML files in the zip file accompany\nall visual results in browsers with interactive 3D viewing.\nA\nTraining Details\nStage 1: SITA-VAE. The encoder takes N = 4096 point clouds with normal features as the\ninputs. Equation (3) is the training loss for SITA-VAE. We set \u03bbc as 0.1 and \u03bbKL as 0.001. For the\nreconstruction term Lr, we follow the training strategies with 3DILG [64], which first normalize all\nmesh into [\u22121, 1], and then separately samples 1024 volumetric points and 1024 near-surface points\nwith ground-truth inside/outside labels from the watertight mesh. The mini-batch size is 40, and we\ntrain this model around 200,000 steps.\nStage 2: ASLDM. We the training diffusion scheduler with LDM [46] whose training diffusion\nsteps are 1000, \u03b2 \u2208 [0.00085, 0.012] with scaled linear \u03b2 scheduler. The mini-batch size is 64, and\nwe train the model around 500,000 steps. In the inference phase, we follow with the classifier-free\nguidance (CFG) [19] as shown in Equation (5), and we set the guidance scale \u03bb as 7.5.\nB\nDetails in zero-shot classification experiments\nDataset. We conduct zero-shot classification experiments on ModelNet40 [59], which provides\n12311 synthetic 3D CAD models in 40 categories. The dataset splits into two parts for training and\ntesting, respectively, where the training set contains 9843 models and the testing set contains 2468\nmodels.\nSettings. We first train our shape-image-text aligned variational auto-encoder (SITA-VAE) on\nshapent [10]. Then, we utilize the trained encoders of SITA-VAE for classification on the testing set\nof ModelNet40 directly. Specifically, for a query 3D shape, we compute the cosine similarity between\nthe shape and each category, where the category reformulates by the phrase \"a 3D model of {}\".\nBesides, we report top-1 accuracy and top-5 accuracy, where top-1 accuracy indicates that the ground-\ntruth category achieves the highest similarity, and top-5 accuracy indicates that the ground-truth\ncategory achieves similarity in the top 5.\n14\nC\nTemplate in building shape-image-text data pair\nWe list the phrase in the predefined template in Table 3. Except for the template introduced in previous\nwork [14, 61], we add one more phrase, \"a 3D model of {}\" in the template, and while training the\nmodel, we replace \"{}\" with the tag of 3D shapes.\nPhrases\n\"a 3D model of {}.\",\n\"a point cloud model of {}.\",\n\"There is a {} in the scene.\",\n\"There is the {} in the scene.\",\n\"a photo of a {} in the scene.\",\n\"a photo of the {} in the scene.\",\n\"a photo of one {} in the scene.\",\n\"itap of a {}.\",\n\"itap of my {}.\",\n\"itap of the {}.\",\n\"a photo of a {}.\",\n\"a photo of my {}.\",\n\"a photo of the {}.\",\n\"a photo of one {}.\",\n\"a photo of many {}.\",\n\"a good photo of a {}.\",\n\"a good photo of the {}.\",\n\"a bad photo of a {}.\",\n\"a bad photo of the {}.\",\n\"a photo of a nice {}.\",\n\"a photo of the nice {}.\",\n\"a photo of a cool {}.\",\n\"a photo of the cool {}.\",\n\"a photo of a weird {}.\",\n\"a photo of the weird {}.\",\n\"a photo of a small {}.\",\n\"a photo of the small {}.\",\n\"a photo of a large {}.\",\n\"a photo of the large {}.\",\n\"a photo of a clean {}.\",\n\"a photo of the clean {}.\",\n\"a photo of a dirty {}.\",\n\"a photo of the dirty {}.\",\n\"a bright photo of a {}.\",\n\"a bright photo of the {}.\",\n\"a dark photo of a {}.\",\n\"a dark photo of the {}.\",\n\"a photo of a hard to see {}.\",\n\"a photo of the hard to see {}.\",\n\"a low resolution photo of a {}.\",\n\"a low resolution photo of the {}.\",\n\"a cropped photo of a {}.\",\n\"a cropped photo of the {}.\",\n\"a close-up photo of a {}.\",\n\"a close-up photo of the {}.\",\n\"a jpeg corrupted photo of a {}.\",\n\"a jpeg corrupted photo of the {}.\",\n\"a blurry photo of a {}.\",\n\"a blurry photo of the {}.\",\n\"a pixelated photo of a {}.\",\n\"a pixelated photo of the {}.\",\n\"a black and white photo of the {}.\",\n\"a black and white photo of a {}\",\n\"a plastic {}.\",\n\"the plastic {}.\",\n\"a toy {}.\",\n\"the toy {}.\",\n\"a plushie {}.\",\n\"the plushie {}.\",\n\"a cartoon {}.\",\n\"the cartoon {}.\",\n\"an embroidered {}.\",\n\"the embroidered {}.\",\n\"a painting of the {}.\",\n\"a painting of a {}.\"\nTable 3: Predefined templates for building shape-image-text pairs. Note that \"{}\" will be replaced by\ntags of the 3D shape during training.\n15\nD\nVisualization for image/shape retrieval\nBenefiting from the alignment among 3D shapes, images, and texts via contrastive learning, our\nmodel can measure the similarity between 3D shapes and images. Therefore, our model could retrieve\n3D shapes from the database given a query image. Specifically, given a query image, our model\ntravels through the database and computes the similarity between the image and each 3D shape,\nwhere the similarity reflects the visual alignment between the image and the 3D shape. We show\nvisual results in Figure 7, where the golden model is the 3D shape most similar to the query image.\nFigure 7: 3D shapes retrieval. Given a query image, our model could retrieve similar 3D shapes from\nthe database. Results show that the visual information is close, which proves our model could capture\n3D shape information aligned with image information. (Please refer to the \u2032supp_retrieve/ \u2217 .html\u2032\nfiles in the supplementary materials for the interactive 3D viewing visualization.)\n16\nE\nMore visual comparison\nImage-conditioned generation. We illustrate more image-conditioned 3D shape generation examples\nin Figure 8. Furthermore, the result proves that our model could capture details in the image and\nfurther generate 3D shapes faithfully. Since images only propose single-view information of 3D\nmodels, our model could also imagine plausible solutions for generating complete 3D shapes.\nFigure 8: Image-conditioned generation comparison: Ours, 3DS2V [65], and 3DILG [64]. (Please\nrefer to the \u2032supp_image_cond/ \u2217 .html\u2032 files in the supplementary materials for the interactive 3D\nviewing visualization.)\n17\nText-conditioned generation. We show more text-conditioned 3D shape generation results in\nFigure 9. According to the result, our model could understand the language correctly and map the\nkeyword to corresponding parts in 3D shapes. The result further shows that training the model on the\nshape-image-text aligned space boosts the model\u2019s generative ability.\nFigure 9: Text-conditioned generation comparison: Ours, 3DS2V [65], and 3DILG [64]. (Please\nrefer to the \u2032supp_text_cond/ \u2217 .html\u2032 files in the supplementary materials for the interactive 3D\nviewing visualization.)\n18\nF\nTest in the wild\nWe also test the model with data in the wild, including images from the internet and manually design\ntext.\nConditional 3D shape generation on images from the Internet. We select some images from the\nInternet as conditions for the model. Results are shown in Figure 10. According to the generated\n3D shapes, the model could map the visual information to 3D shapes, proving that our model could\nrobustly handle some out-of-domain images.\nFigure 10: Conditional 3D shape generation on images from the Internet. (Please refer to the\n\u2032supp_wild/image/ \u2217 .html\u2032 files in the supplementary materials for the interactive 3D viewing\nvisualization.)\n19\nConditional 3D shape generation on manual input text. Moreover, we manually design input texts\nas conditions for the model, and the results are shown in Figure 11. The generated 3D shapes prove\nthat our model could capture keyword information and produce results that conform to the text.\nFigure 11: Conditional 3D shape generation on manually design text. (Please refer to the\n\u2032supp_wild/text/ \u2217 .html\u2032 files in the supplementary materials for the interactive 3D viewing\nvisualization.)\n20\n"
  },
  {
    "title": "Benchmarking Large Language Model Capabilities for Conditional Generation",
    "link": "https://arxiv.org/pdf/2306.16793.pdf",
    "upvote": "7",
    "text": "Benchmarking Large Language Model Capabilities for Conditional\nGeneration\nJoshua Maynez\nGoogle DeepMind\njoshuahm@google.com\nPriyanka Agrawal\nGoogle DeepMind\npriyankagr@google.com\nSebastian Gehrmann\nGoogle Research\ngehrmann@google.com\nAbstract\nPre-trained large language models (PLMs) un-\nderlie most new developments in natural lan-\nguage processing. They have shifted the field\nfrom application-specific model pipelines to a\nsingle model that is adapted to a wide range\nof tasks. Autoregressive PLMs like GPT-3\nor PaLM, alongside techniques like few-shot\nlearning, have additionally shifted the output\nmodality to generation instead of classification\nor regression. Despite their ubiquitous use,\nthe generation quality of language models is\nrarely evaluated when these models are intro-\nduced. Additionally, it is unclear how existing\ngeneration tasks\u2014-while they can be used to\ncompare systems at a high level\u2014-relate to\nthe real world use cases for which people have\nbeen adopting them. In this work, we discuss\nhow to adapt existing application-specific gen-\neration benchmarks to PLMs and provide an\nin-depth, empirical study of the limitations and\ncapabilities of PLMs in natural language gen-\neration tasks along dimensions such as scale,\narchitecture, input and output language. Our\nresults show that PLMs differ in their applica-\nbility to different data regimes and their gen-\neralization to multiple languages and inform\nwhich PLMs to use for a given generation task\nsetup. We share best practices to be taken into\nconsideration when benchmarking generation\ncapabilities during the development of upcom-\ning PLMs.\n1\nIntroduction\nNatural language generation tasks require gener-\nating understandable text given textual or non-\nlinguistic information as input, such as documents,\ntables, or other structured forms. These texts seek\nto achieve a communicative goal (e.g., summarize\na document). The standard approach to tackle\nthese problems over the last years has been to\nstart with a pretrained encoder-decoder model like\nT5 (Raffel et al., 2020a) or BART (Lewis et al.,\n2020a) and finetune it on a corpus that captures\nthe downstream task.\nThe recent much larger\npretrained language models use a decoder-only\narchitecture and upended this paradigm. These\nmodels enabled few-shot or in-context learning ap-\nproaches in which a model is presented with one or\nmore examples and tasked to continue generating\nwithout any finetuning. We refer to both kinds of\npretrained models as PLMs.\nDue to the lack of grounding in the specific task\nsetup, few-shot learning in generation settings\nleads to a model approaching the communicative\ngoal from very different angles. These diverse\nrange of outputs make the typical reference-based\nautomatic evaluation strategies largely incompati-\nble. While human evaluation can be used to over-\ncome this shortcoming, it is infeasible to monitor\nthe performance of an actively training model this\nway or to re-run all evaluations every time a new\nmodel is introduced. This leads to the question\nhow one should reliably monitor generation ca-\npabilities, a question that is only growing in im-\nportance as more tasks are approached by casting\nthem into generation setups.\nIn this work, we evaluate 8 models in few-shot\nand finetuning settings on 27 generation tasks cov-\nering 14 languages via automatic evaluation, pre-\nsenting the first large-scale benchmark of PLMs\nin conditional NLG settings. We discuss design\nchoices and challenges to ensure a fair comparison\nbetween the different systems, including suitable\nmethods, tasks, and metrics. Based on our em-\npirical results, we derive recommendations that\ncould be used for future benchmarks during the\ndevelopment of PLMs. To combat the need for\nrepeating computationally expensive explorations,\nwe investigate how many evaluation examples are\nnecessary to identify differences between models\nand find that, in many cases, fewer than 500 exam-\nples are sufficient, which opens the path for future\nevaluation-only task developments.\narXiv:2306.16793v1  [cs.CL]  29 Jun 2023\n2\nBackground and Related Work\nThe shift from specialized pipelines toward pre-\ntrained language models has led to significant\nchanges in how models are evaluated. We now fo-\ncus more on questions such as \u201chow good are the\nlearned representations?\u201d instead of user-facing\nmeasures of utility. The changes manifested in\nleaderboards and standard benchmarks that aim\nto characterize a wide range of model capabili-\nties (Ethayarajh and Jurafsky, 2020).\nAn additional recent shift is that from finetuning\ntoward few-shot learning. Models like T5 (Raffel\net al., 2020a), BART (Lewis et al., 2020a), and\nmT5 (Xue et al., 2021) were finetuned on super-\nvised datasets covering tasks including translation\nand summarization, and their outputs are com-\npared to \u201cground truth\u201d outputs via widely used\nmetrics like ROUGE (Lin, 2004) which provide a\nnoisy indication of the \u201cquality\u201d of the output and\nwhich can be used to determine whether a model\nis better than others.1 In contrast, large PLMs\nwith autoregressive language modeling pretrain-\ning objectives are more capable to produce results\nwithout explicit finetuning and are thus typically\nevaluated via few-shot and in-context approaches,\nwhere the model is given the task description and\nexemplars showing how the task should be com-\npleted. GPT-3 (Brown et al., 2020) and models that\nfollowed such as GLaM (Du et al., 2022), Gopher\n(Rae et al., 2021), and LaMDA (Thoppilan et al.,\n2022), have achieved few-shot state-of-the-art re-\nsults on a large number of tasks at their time of\npublication. However, few-shot approaches work\nbest for tasks with a clear answer such as classifi-\ncation or span-based question-answering.2\nGeneration metrics penalize systems when their\nwriting style differs from how the references are\nwritten (Mathur et al., 2020; Freitag et al., 2020;\nMille et al., 2021). Without finetuning, there is no\nguarantee that PLMs produce outputs that look like\nthe ground truth, both in style and content. Recent\nwork found that these differences leads to sharp dif-\nferences in how humans and automatic metrics rate\nthe generation quality (Goyal et al., 2022). Due\nto this uncertainty, most evaluations of new PLMs\n1For an in-depth review of the usefulness of automatic\nmetrics, we refer to Gehrmann et al. (2022b) and point to\nSection 4 for a discussion of the application of metrics to\nbenchmarks.\n2We refer to the two task types as NLU and NLG tasks but\nnote that this distinction becomes fuzzy with autoregressive\nmodels since technically all answers are \u201cgenerated\u201d.\nare limited to NLU benchmarks such as Super-\nGLUE (Wang et al., 2019). For example, LaMDA\n(Thoppilan et al., 2022) did not evaluate on NLG\ntasks, GLaM (Du et al., 2022) limited its genera-\ntion evaluation to short span question answering\ntasks, and GPT-3 (Brown et al., 2020) evaluated\nonly on machine translation. A first autoregressive\nPLM with broad NLG evaluation, PaLM (Chowdh-\nery et al., 2022), benchmarked summarization and\ndata-to-text tasks in multiple languages.\nThe recent Holistic Evaluation of Language Mod-\nels project (HELM, Liang et al., 2022) aims to\nstandardize evaluation of language models. With\nthe explicit goal to broaden the task and metric\ncoverage, HELM has established an impressive\nfew-shot benchmark for many natural language\ntasks. Corroborating the prior findings, they also\nconclude that human evaluation is necessary for\nNLG. This distinction means that the reference-\nbased approach for generated text that the field\nhas used since the advent of deep learning may no\nlonger sufficient and that we need clear evaluation\nprotocols that continue to allow us to answer broad\nquestions about \u201cgeneration quality\u201d of a model.\nComplementing this work, we take a deeper look\nat a wider set of NLG tasks and explore LLMs in\nfinetuning and few-shot setups to identify whether\nreference-based automatic evaluation can still be\nused to produce system rankings.\nResearch Questions\nWe aim to define a method-\nology that allows us to answer the question \u201cHow\ngood are learned representations of a model for\ngenerating natural language?\u201d via few-shot and\nfinetuning approaches. To develop and apply this\nmethodology we seek to answer the following\nthree research questions:\nR1 How do different model architectures compare\nin terms of automatic metrics?\nWe aim to identify patterns that emerge in eval-\nuations and to uncover aspects inherent to the\ntasks, e.g. have metrics on specific tasks satu-\nrated?, and to the models\u2019 architectural choices,\ne.g., are encoder-decoders better suited for par-\nticular task formulations? (Section 4)\nR2 What set of tasks, methods, and metrics is best\nsuited for the monitoring of improvements in\nlanguage generation capabilities?\nUsing the results of R1, we aim to select a sub-\nset of tasks, methods, and metrics that robustly\nproduce reliable model rankings. (Section 5)\nLength\nSize\nDataset\nLanguages\nInput\nOutput\nTraining\nTest\nE2E\nen\n146\n135\n35k\n4.7k\nWebNLG\nen,ru\n169.5\n157\n14k\u201335k\n1.1k-1.8k\nToTTo\nen\n357\n120k\n7.7k\nCzech Rest.\ncs\n70\n80\n3.5k\n842\nXSum\nen\n1845\n153\n23k\n1.2k\nWikiLingua\nen,es,ru,tr,vi\n1k\u20135k\n159\u2013489\n5k\u20133.8M\n900-29k\nMLSum\nes,de\n4152\n147\n220k\u2013250k\n10k-13k\nXL-Sum\nar,bn,ja,id,sw,\n1k\u201310k\n137\u2013614\n1.3k\u2013300k\n500-9k\nko,ru,te,th,tr,\nes,vi,hi\nTable 1: Details of the datasets evaluated in this paper:\nlanguages, lengths in number of tokens according to\nthe mT5 tokenizer (Xue et al., 2021), and size of the\ntraining and test splits.\nR3 What are the broader implications for how the\nquality of newly developed models should be\nmonitored?\nRobustly ranking systems is particularly impor-\ntant when monitoring a system during training\nand when comparing across many tasks. In\nline with the \u201creality check\u201d theme track at\nACL 2023, we discuss the implications of our\nfindings on how evaluation results should be\nproduced and interpreted. (Section 6)\n3\nMethod\n3.1\nData\nWe select a combination of data-to-text and text-\nto-text datasets as different input modalities. The\nselected datasets capture different input and output\nlengths, domains, languages, and communicative\ngoals. The text-to-text task with most available\nmultilingual datasets is summarization which we\npick for this paper.3 We pick the following tasks:4\n\u2022 MLSum (Scialom et al., 2020) \u2013 Summarize a\nnews article in multiple sentences.\n\u2022 WikiLingua (Ladhak et al., 2020) \u2013 Gener-\nate section headers for step-by-step instructions\nfrom WikiHow.\n\u2022 XSum (Narayan et al., 2018) \u2013 Generate the first\nsentence of a news article.\n\u2022 Clean E2E NLG (Novikova et al., 2017; Du\u0161ek\net al., 2019) \u2013 Given a set of key-value attribute\n3Since benchmarks for machine translation are well-\nestablished (e.g., Akhbardeh et al., 2021) we exclude it from\nour scope. However, any methodological outcomes of our\nwork can be applied to translation or similar tasks.\n4All datasets were retrieved via the Generation Evaluation\nand Metrics benchmark (Gehrmann et al., 2021, 2022a). We\nuse these datasets for research purposes only in line with their\nintended use.\npairs, describe a restaurant in one or two sen-\ntences.\n\u2022 Czech Restaurant response generation (Dusek\nand Jurvc\u2019ivcek, 2019) \u2013 Given a dialog context\nand a dialog act representation, generate a one\nsentence long response.\n\u2022 WebNLG 2020 (Gardent et al., 2017; Ferreira\net al., 2020) \u2013 Verbalize subject-predicate-object\ntriples in one or more sentences.\n\u2022 ToTTo (Parikh et al., 2020) \u2013 Describe high-\nlighted cells in a table in a single sentence.\n\u2022 XL-Sum (Hasan et al., 2021) \u2013 Summarize a\nnews article, in the same language, in a single\nsentence.\nTable 1 provides an overview of these datasets in\nterms of languages, the lengths of input and output\nand split sizes. For highly multilingual datasets,\nwe evaluate on a subset of typologically diverse\nlanguages following the selection by Clark et al.\n(2020). To this selection, we add languages that\nappear bothin WikiLingua and XL-Sum.\n3.2\nModels\nPrior results for the benchmarked tasks primarily\ncome from finetuning T5 (Raffel et al., 2020b),\nmT5 (Xue et al., 2021), or BART (Lewis et al.,\n2020b), which are encoder-decoder models pre-\ntrained with an infilling objectives. These models\nare significantly smaller than newer models like\nGPT-3, with sizes ranging from 130M to 13B pa-\nrameters. Encoder-decoder models trained for in-\nfilling often outperform larger decoder-only LMs\nin the finetuning setting (Tay et al., 2022), while\nthe latter work better for few-shot setting. There\nhas also been recent work on reducing the com-\nputational cost of large models by \u223c10x by using\na mixture of experts (Zoph et al., 2022). It is im-\nportant to compare these diverse set of models to\nunderstand how scale plays a role with the model\u2019s\narchitecture and its pretraining. We benchmark the\nfollowing models:5\n\u2022 PaLM PaLM is a pretrained decoder-only\ntransformer-based model trained with standard\nleft-to-right language modeling objective. It is\npretrained on a range of multilingual corpora in-\ncluding Wikipedia, news, and code. In this work,\nwe use two models scales: 8B parameters and\n540B parameters.\n5Model names omitted for anonymity.\n\u2022 GPT-3.5 (Ouyang et al., 2022b) GPT-3.5 is\na 175B parameter decoder-only transformer-\nmodel of the GPT-3 family (Brown et al., 2020)\nbut trained on a blend of text and code from\nbefore Q4 2021.\nThis model, named code-\ndavinci-002, was introduced as the base model\nfor InstructGPT-3 (Ouyang et al., 2022b) with-\nout the supervision on human-written demon-\nstrations and human-vetted model samples.6\n\u2022 ST-MoE (Zoph et al., 2022) ST-MoE is a 269B\nsparse pretrained variant of a dense encoder-\ndecoder transformer-based model.\n\u2022 LaMDA (Thoppilan et al., 2022) LaMDA (137B\nparameters) is a decoder-only transformer-based\nlanguage model specialized for dialog applica-\ntions. It is pretrained on dialog data as well as\nweb text data followed by rank-based tuning.\n\u2022 T5 (Raffel et al., 2020a) T5-XXL (11B parame-\nters) is a pretrained encoder-decoder transformer-\nbased model trained on a span corruption objec-\ntive with a novel unified text-to-text format. It\nis pretrained on Common Crawl data, mostly\ncontaining English-only documents.\n\u2022 mT5 (Xue et al., 2021) mT5-XXL (11B param-\neters) is a multilingual variant of T5 that was\npretrained on a multilingual corpus, mC4, cover-\ning 101 languages.\n\u2022 LongT5 (Guo et al., 2021) LongT5 (3B param-\neters) a similar architecture as T5, where the\nencoder is extended to have global-local atten-\ntion sparsity patterns to handle long inputs.\n3.3\nFew-shot evaluation methodology\nTo evaluate the models for few-shot inference, we\nconcatenate a task-specific prompt7 to the input\nand prepend an output prompt to the output. To\nhandle the oftentimes very long inputs or outputs\nfor tasks such as summarization, inputs were trun-\ncated to 2048 tokens and inference was done pro-\nviding only one exemplar at a time, referred to as 1-\nshot. These simple prompts are analogous to those\nused in related work (Chowdhery et al., 2022; Scao\net al., 2022). We do not tune the prompts or use\nmore complex strategies to keep fair comparisons\nbetween multiple systems, as prompt selection can\nlead to overfitting. The exemplars are separated\nthrough double linebreaks, which are also used\n6More\ndetails\ncan\nbe\nfound\nat\nhttps://beta.openai.com/docs/model-index-for-researchers\n7For Summarization, this prompt was \u201cSummarize the\nfollowing article:\u201d, and for Data-to-Text it was \u201cVerbalize:\u201d.\nThis was translated to the appropriate language.\nto truncate output predictions for evaluation. All\nfew-shot exemplars are randomly sampled from\nthe training corpus. From early experimentation,\nwe found this particularly important since it avoids\noverfitting to exemplars that work well for one\nmodel but not another.\n3.4\nFinetuning methodology\nTo use the decoder-only architectures during fine-\ntuning, inputs and targets are concatenated. The\nconcatenated sequences are truncated to 2048 to-\nkens, the training context used during pretraining,\nwith 512 tokens reserved for the target. Only sum-\nmarization tasks required input truncation. We\nfinetuned models with standard hyperparameters;\nrefer to Appendix-B for thorough details. The best\nmodel checkpoint for each dataset was selected by\nthe best performing geometric mean of ROUGE-1,\nROUGE-2 and ROUGE-L scores on the valida-\ntion set. Decoding was done with beam-search\nwith a beam size of 4 for encoder-decoder models,\nwhile inference in decoder-only PLMs (LaMDA,\nPaLM, ST-MoE) was performed using top-k sam-\npling with k=10, due to issues with scaling beam\nsearch at the time of publication.\n3.5\nMetrics\nFollowing the suggestions by Gehrmann et al.\n(2022b), we report a combination of lexical and\nlearned metrics, starting with ROUGE-2 and\nROUGE-L (Lin, 2004). Since the default ROUGE\nimplementation uses English-specific tokenization,\nstemming and punctuation normalization, it is in-\ncompatible with other languages.\nHasan et al.\n(2021) extended ROUGE by integrating additional\nstemmers and tokenizers to cover up to the 45 lan-\nguages. To support more languages, and avoid\ndependency on varying implementations, we use\na SentencePiece tokenizer (Kudo and Richardson,\n2018) which, provided a vocabulary distribution\nfile, is self-contained and has sensible fall-backs\nto unexpected words. Specifically, we used mT5\u2019s\nSentencePiece vocabulary.\nFor the same reason, we also evaluate with\nChrF (Popovi\u00b4c, 2015), which is a character-level\nn-gram overlap metrics and thus independent from\ntokenizers. BLEURT (Sellam et al., 2020; Pu et al.,\n2021) is a multilingual model-based evaluation\nmetric for generation designed to compute the sim-\nilarity between a pair of sentences i.e. a reference\nand a candidate. It finetunes RemBERT (Chung\net al., 2021) on synthetic sentence pairs and gold\nratings. In contrast to the lexical metrics, BLEURT\nis meant to capture the non-trivial semantic simi-\nlarities between two texts.\nFor brevity, the main text of this section focuses\non the F-measure of ROUGE-L for English and\nSentencePiece-ROUGE-L for all other languages\nwhile the remaining results are in Appendix A.\nWe additionally investigate the agreement between\nmetrics in Section 5.8\n4\nEmpirical Observations\nFew-shot learning falls behind finetuning\nFor\nmany generation tasks, including multilingual sum-\nmarization tasks, we observe a large gap between\nfinetuning and few-shot results, indicating that fine-\ntuning will play an important role when it comes to\nmaximizing automatic scores. On data-to-text, the\nfew-shot results follow a similar trend as in sum-\nmarization, but the gap to the best finetuned results\nshrinks drastically. Moreover, the finetuning result\ndo not always follow a trend according to scale or\narchitecture. We hypothesize that multiple tasks\nhave saturated to the metrics. If this is the case, ap-\nproaching them as few-shot generation tasks may\nstill yield insights but it is no longer productive to\nuse them to benchmark finetuned models.\nFinetuned decoder-only PLMs can match\nencoder-decoder performance with scale\nIn\nsummarization, finetuned decoder-only PLMs,\nsuch as PaLM-540B, closely match or exceeds\nthe best reported prior results on all English gen-\neration tasks. This demonstrates that PLMs can\nmake up their architectural disadvantage through\nits vastly increased scale. While finetuning PLMs\nis computationally expensive, it serves as an im-\nportant upper bound for few-shot predictions.\nMultilingual generation capabilities are highly\ndependent on pretraining data\nThe PLMs eval-\nuated are mostly pretrained on English corpora:\n99+% for T5, LongT5, ST-MoE; 90% for PaLM,\nLaMDA; contrarily mT5 is explicitly pretrained\n8For\nROUGE,\nwe\nused\nthe\npython\nimplementa-\ntion at https://github.com/google-research/\ngoogle-research/tree/master/rouge at commit\nf935042 and whitespace-tokenized references and predic-\ntions before calling the library.\nFor BLEURT, we used\nBLEURT-20 checkpoint from the library at https://\ngithub.com/google-research/bleurt and com-\nmit c6f2375.\nin a multilingual corpus.9 PaLM achieves best re-\nsults in 3 out of 4 English generation tasks which\ngenerate English text, even when the input is non-\nEnglish. However, the much smaller mT5 bests\nthe other models in 10 out of 14 non-English sum-\nmarization tasks, and the relative difference be-\ntween few-shot and finetuning is larger for non-\nEnglish generation. This suggests that English-\ncentric PLMs are better at processing non-English\ninput than generating non-English output.\nAnalyzing the effects of input context length\nTasks with long inputs suffer from models\u2019 limita-\ntion to process said inputs. Inputs are thus usually\ntransformed (e.g. cropped, re-ranked, etc) to fit\ninto the model. We found that a several of the\nevaluated tasks, such as WikiLingua and MLSum\nbenefit from a longer input context in models even\nif the long-context model is smaller (i.e., LongT5\nvs T5). In contrast, the performance is comparable\nfor the rest of short-context tasks.\nFigure 1: General recommendations when monitoring\nor benchmarking PLMs.\n5\nDeriving Evaluation Practices\nFigure 1 summarizes the recommendations we\ndeveloped from challenges we faced and our ob-\nserved empirical results. These recommendations\nare best understood in the context of monitoring\n9The language breakdown for GPT-3.5 is unknown.\nOne-shot\nFinetuning\nTask\nPaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en)\n37.7\n46.6\n7.1\n46.6\n52.9\n52.3\n51.5\n52.9\n52.2\n53.1\nWebNLG (en)\n45.3\n54.7\n8.4\n54.6\n56.8\n58.0\n56.4\n50.8\n47.7\n58.0\nToTTo (en)\n40.2\n50.7\n5.6\n51.9\n65.8\n67.5\n67.0\n66.1\n65.5\n66.3\nCzech Restaurant (cs)\n16.9\n34.1\n3.3\n38.5\n45.5\n45.5\n40.7\n45.4\n39.4\n44.8\nWebNLG (ru)\n16.8\n33.7\n4.5\n33.3\n40.9\n40.5\n28.2\n41.2\n41.1\n41.6\nEnglish Generation\nXSum (en)\n19.9\n28.6\n10.0\n34.0\n31.4\n36.5\n38.3\n36.5\n33.2\n36.0\nXLSum (en)\n16.8\n22.7\n8.4\n27.9\n34.6\n44.3\n45.4\n43.1\n41.8\n42.6\nWikiLingua (en)\n6.5\n6.4\n5.9\n7.7\n8.0\n7.5\n7.8\n7.9\n7.9\n7.8\nCrosslingual Generation\nWikiLingua (es \u2192 en)\n6.5\n6.1\n5.0\n7.7\n7.7\n7.6\n7.3\n7.8\n7.6\n7.9\nWikiLingua (ru \u2192 en)\n10.2\n17.5\n0.7\n18.9\n29.9\n35.7\n25.1\n27.9\n31.7\n30.8\nWikiLingua (tr \u2192 en)\n10.1\n20.0\n7.7\n21.2\n31.1\n38.8\n31.5\n26.8\n36.7\n28.2\nWikiLingua (vi \u2192 en)\n7.7\n14.5\n2.2\n16.2\n28.9\n32.9\n22.9\n22.7\n31.0\n28.5\nMultilingual Generation [SentencePiece-ROUGE-2]\nMLSum (es)\n12.8\n14.3\n5.2\n13.0\n23.0\n24.5\n25.0\n24.3\n25.7\n25.6\nMLSum (de)\n13.6\n21.3\n3.9\n22.6\n35.2\n41.4\n44.1\n43.5\n43.3\n43.7\nXLSum (ar)\n12.2\n19.0\n10.8\n18.0\n36.2\n39.9\n15.7\n15.2\n42.3\n6.2\nXLSum (bn)\n5.8\n6.9\n6.1\n11.7\n26.4\n31.1\n11.1\n10.2\n36.5\n11.0\nXLSum (ja)\n11.3\n15.1\n5.4\n18.3\n38.7\n42.5\n4.5\n4.5\n43.7\n4.6\nXLSum (id)\n16.8\n20.4\n9.0\n20.1\n35.5\n43.5\n41.1\n41.6\n43.5\n40.8\nXLSum (sw)\n16.7\n24.5\n11.5\n15.4\n32.7\n36.4\n37.0\n37.4\n40.7\n36.3\nXLSum (ko)\n16.1\n18.2\n7.9\n17.6\n33.8\n37.3\n20.3\n19.5\n45.0\n19.9\nXLSum (ru)\n12.6\n16.1\n10.8\n19.1\n30.3\n38.3\n18.1\n17.8\n38.6\n17.7\nXLSum (te)\n6.5\n7.7\n6.2\n13.1\n20.5\n30.0\n15.1\n15.1\n33.5\n14.8\nXLSum (th)\n6.7\n8.6\n5.2\n13.3\n23.4\n29.5\n13.5\n13.7\n34.3\n13.1\nXLSum (tr)\n15.2\n17.7\n8.0\n16.8\n33.3\n42.4\n30.3\n30.4\n42.3\n29.7\nXLSum (es)\n15.7\n17.4\n8.3\n16.9\n25.2\n34.3\n31.9\n32.5\n33.9\n32.3\nXLSum (vi)\n13.2\n14.9\n6.9\n15.4\n25.9\n41.5\n27.7\n27.3\n41.0\n26.7\nXLSum (hi)\n10.0\n12.1\n9.3\n15.2\n37.7\n43.6\n13.7\n2.3\n43.5\n2.3\nTable 2: ROUGE-L and SentencePiece-ROUGE-L results on data-to-text and compression datasets. Best results in bold.\nFew-shot results lag behind finetuned results and the gap increases as tasks become more complex. The non-English performance\nmostly follows the trend that higher percentages of non-English pretraining data leads to better performance. Despite their much\nsmaller size, encoder-decoder model frequently much larger decoder-only models after finetuning.\nand benchmarking PLMs during training or infer-\nence.\nComparable few-shot learning evaluation\nAs\nmentioned in Section 3, our design choices were\nmade to ensure that results are comparable across\nPLMs. Primarily, prompts were deliberately kept\nextremely simple and all few-shot exemplars were\nrandomly sampled. While highly curated prompts\nor methods like chain-of-thought prompting can\nincrease the performance considerably (Wei et al.,\n2022b), it can also lead to overfitting to the partic-\nular model the prompt was developed on, in turn\nmaking a comparison to other models unfair and\nproducing unrealistic expectations when people\nhave single interactions with it.\nOverlap-based metrics are not calibrated to\nevaluate few-shot learning\nFew-shot genera-\ntion suffers from not being able to predict out-\nput length properly given the few exemplars pro-\nvided. While encoder-decoder models utilize end-\nof-string tokens, these are not always learned dur-\ning decoder-only pretraining. To circumvent this\nissue, researchers rely on PLMs match to the few-\nshot format provided e.g. line-breaks that separate\nexemplars. We observed PLMs fail to follow the\nformat a significant number of times, producing\nthe largest allowed length on occasion. In our\nexperiments, we tried to avoid very long outputs\nby trimming outputs to the 95-percentile length\nseen in the targets.10 Still, few-shot output lengths\n10This simple method avoids discrepancies across PLMs\nwhich might have different maximum decoding lengths.\nare on average 2-3 times the average target length\nwhile finetuned model\u2019s output average 80% the\naverage target length, across all tasks. Overlap\nmetrics used in generation are sensitive to length\n(Sun et al., 2019) making a natural disadvantage\nfor few-shot learners. We do not recommend using\noverlap-based metrics to compare few-shot results\nwithout length normalization.\nComputational costs can be decreased without\nsacrificing relative model performance\nThe\ncomputational cost of evaluating large datasets,\nsome with more than 10K examples, are pro-\nhibitive and perhaps unnecessary. To that end,\nwe investigate if a model ranking can be produced,\nwith a high degree of certainty, while only con-\nsidering a random subset of the test set, saving\ncompute cost to possibly evaluate on more tasks\ninstead.\nTo investigate this effect, we ran the\nfollowing experiment: (1) Sample n datapoints\nfrom a dataset and all corresponding model scores.\n(2) Following Kocmi et al. (2021) and Graham\net al. (2014), we perform Wilcoxon Rank Sum\ntest (Wilcoxon, 1946) to assess the stability of the\nranking. (3) Repeat steps 1&2 k times and record\nthe fraction of runs in which models scores from\nany two models were not distinguishable from\neach other (those with a p-value of > 0.05). Since\nwe are considering 10 model settings in this work,\nthis experiment considers all 45 possible pairs.\nThe result shown in Figure 2 provides insight into\nthe required number of data points to produce rank-\nings. For most datasets, we can produce stable\nmodel rankings with only 500 examples, some\nwith as little as 100. Tasks where models achieve\nvery similar scores tend to require more test ex-\namples, since smaller score differences require\nmore examples to be distinguishable from each\nother (Wei and Jia, 2021).11\nAnalyzing metrics utility\nWe use different au-\ntomated metrics to evaluate the generation quality\nof the models. These metrics attempt to capture\nthe similarity between system generated output\nand the reference text. While ROUGE and chrF\naccount for the lexical overlap, BLEURT is meant\nto compute the semantic similarity. It is important\nto understand the agreement between these met-\nrics. We compute the the system-level agreement\nvia Spearman correlation coefficient (Spearman,\n1987) between the scores given by the metrics to\n11Full results available in Appendix A.\nFigure 2: Empirical probability of p-value of Wilcoxon\nRank Sum test < 0.05 for any combination between\n1-shot and finetuned models.\nFigure 3: Spearman correlation coefficients between\nmetrics: (SP)ROUGE-L, BLEURT and ChrF.\nthe fine-tuned set of models. Figure 3 shows the\ncorrelation between ROUGE-L (RL), BLEURT\nand ChrF. We observe that the metrics are highly\ncorrelated for most datasets. Similar to Figure 2,\non the tasks where the models have similar per-\nformance, we notice less correlation among the\nmetrics. Such tasks are may have either saturated\nperformance, e.g., ToTTo (en) or all models per-\nform poorly, e.g., Wikilingua (es-> en). Due to the\nsmall differences between models, metrics fail to\nproduce the same rankings.\n6\nDiscussion and Reality Check\nIn line with our goal to provide a \u201creality check\u201d\nvia empirical and theoretical research, and to re-\nflect on the ways in which reported performance\nimprovements are meaningful, we want to situate\nour findings in the context of the broader NLP\ncommunity. Openly accessible APIs and publicly\navailable large models have led to increased at-\ntention on large pretrained models, but they have\nalso led to a \u201crelease-then-test\u201d philosophy where\nmodels are released without extensive evaluations.\nWhile the findings we present in this paper do not\nsolve this issue, agreeing on a shared evaluation\nprocess could lead to more realistic claims about\nmodel performance (and shortcomings), and allow\nfor a more accurate monitoring of models during\ntraining.\nWhat claims can we not make?\nEmpirical find-\nings demonstrate that incorporating generation into\nNLU tasks via Chain-of-Thought leads to better\nmodel performance (Wei et al., 2022b; Suzgun\net al., 2022). Providing additional grounding via\nfinetuning on instructions and aligning a model to\nhuman feedback leads to better task-specific per-\nformance without supervision (Wei et al., 2022a;\nOuyang et al., 2022a). However, we lack the sci-\nentific methods to quantify these advances. While\nbenchmarks provide an indication whether a model\nis performing better than a previous iteration, and\nprojects like BIG-bench (Srivastava et al., 2022)\nand HELM (Liang et al., 2022) enable evaluation\non a very wide range of possible tasks, they are\nalso inherently limited.\nWhen benchmarking models in few-shot settings,\nespecially models for which little information\nabout their training data is available, it is hard\nto disambiguate model performance from mem-\norization, i.e. if the examples were seen during\npretraining. Instruction tuning further blur the line\nbetween finetuning and few-shot, which can lead\nto very different outputs and are not fully compa-\nrable. It is thus near impossible to make claims\nabout why a model is succeeding at one particular\ntask without having access to its training data.\nAs mentioned earlier, the target of this work is to\nderive best practices for comparing models in gen-\neration settings with constrained computational\nbudgets, for example when monitoring a training\nmodel or when trying to compare on many dif-\nferent tasks. Our findings are grounded in much\nprior work that finds that metrics have a very high\nagreement with human judgments on the system-\nlevel (e.g., Kocmi et al., 2021), but are essentially\nmeaningless on the segment-level. For that reason,\nwe cannot derive claims beyond these rankings\nabout utility of a model or whether a particular\nmodel would actually produce useful outputs for a\ntask. To derive such insights, we point to work on\nextrinsic evaluation which requires comprehensive\nhuman evaluations (e.g., Lee et al., 2022).\nHow can our findings be applied to improve the\nstatus quo?\nSince the generation capabilities of\nPLMs are currently not extensively monitored or\nevaluated, we set out to derive best practices for\nhow these evaluations can look. We found that\nmany of the \u201ceasy\u201d tasks, on which finetuned mod-\nels saturate the metrics, still yield insights for few-\nshot approaches. We further identified the tension\nbetween doing a computationally expensive full\nevaluation on a dataset and adding more evaluation\nsets for different tasks or languages. Our findings\nsuggest that evaluation on small subsets of more\ntasks can be beneficial to the overall results.\nTo further motivate this suggestion, consider the\nfollowing thought experiment: We have two tasks,\nA and B. At 500 examples, they have a risk of\nproducing a \u201cwrong\u201d ranking of 10%. At 1,000\nexamples, they have a risk of producing a wrong\nranking of 5%. These risks are not correlated, i.e.,\ntheir covariance is 0. Given a computational bud-\nget of evaluating on 1,000 examples, the risk of\nonly evaluating on one dataset is 5%, and the risk\nof producing two wrong ratings after evaluating\non A and B is only 1%. While additional datasets\nintroduce a larger risk of one individual dataset pro-\nducing misleading results (18% in this case), one\ncan easily expand this argument to a whole port-\nfolio of tasks to hedge against individual dataset\nrisk (Stuart and Markowitz, 1959). Many exist-\ning NLU benchmarks like BIG bench (Srivastava\net al., 2022) already follow such a strategy and we\nbelieve that generation evaluation, especially con-\nsidering the additional risk due to metrics, should\nfollow this approach for the use cases discussed\nin this work. To further minimize the individual\ndataset risk, they can be switched out once they\nsaturate or their sample sizes increased.\n7\nConclusion\nIn this work, we produced an extensive evaluation\nof a diverse set of state-of-the-art pre-trained lan-\nguage models (PLMs) for 27 different multilingual\ngeneration tasks under few-shot learning and fine-\ntuning settings. We discuss empirical results that\nhelp inform practitioners which tasks, methods\nand metrics are suitable. We provide recommenda-\ntions on how best to monitor conditional genera-\ntion capabilities of PLMs, including how to fairly\nbenchmark few-shot learning, automated metrics\nand their utility, and how to efficiently utilize com-\nputational resources. We hope that such findings\nand recommendations could positively influence\nnatural language evaluation in future work.\n8\nLimitations\nIn this work, we have presented results that help in-\nform us what tasks, methods and metrics are best\nsuited for monitoring as well as methodologies\nand empirical information about the current set of\nmodels. We provide detailed information of how\nthese results can be reproduced, to the extend that\nresearch have access to the PLMs in question, but\nthese results have limitations, in order to reduce\ncosts, many languages were not evaluated which\nmight have left unforeseen patterns not discussed\nin this work. Moreover, few-shot learning, in par-\nticular, could exhibit large variance if different\nprompts were chosen, or a different set of exem-\nplars chosen. Because of the high costs involved\nour work does not explore the performance differ-\nence when multiple sets of hyper-parameters were\nchosen.\nOn the conceptual level, we make the assump-\ntion that system-level improvements on our tasks\ntranslate to downstream usefulness. While prior\nwork suggests that this is the case, tools like chat-\nGPT have significantly expanded the possible ap-\nplication space beyond the realm of \u201ctypical\u201d NLP\ntasks, and we don\u2019t know how well our findings\ngeneralize to this space of tasks.\n9\nEthics Statement\nThis paper focuses on conditional generation tasks\nwhere models are free to generate long text se-\nquences. Typical issues associated with text gen-\neration such as hallucinations, memorization of\nprivate information publicly available, toxic and\ndiscriminatory language, or sensitive generated\ncontent could and are likely to arise. measuring\nthe extent to which these issues occur is a neces-\nsary and crucial additional dimension of model\nevaluation which we do not include in this work,\nwhich should be seen as supplemental.\nReferences\nFarhad Akhbardeh, Arkady Arkhangorodsky, Mag-\ndalena Biesialska, Ond\u02c7rej Bojar, Rajen Chatter-\njee,\nVishrav Chaudhary,\nMarta R. Costa-jussa,\nCristina Espa\u00f1a-Bonet, Angela Fan, Christian Fe-\ndermann, Markus Freitag, Yvette Graham, Roman\nGrundkiewicz, Barry Haddow, Leonie Harter, Ken-\nneth Heafield, Christopher Homan, Matthias Huck,\nKwabena Amponsah-Kaakyire, Jungo Kasai, Daniel\nKhashabi, Kevin Knight, Tom Kocmi, Philipp Koehn,\nNicholas Lourie, Christof Monz, Makoto Morishita,\nMasaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa,\nMatteo Negri, Santanu Pal, Allahsera Auguste Tapo,\nMarco Turchi, Valentin Vydrin, and Marcos Zampieri.\n2021. Findings of the 2021 conference on machine\ntranslation (WMT21). In Proceedings of the Sixth Con-\nference on Machine Translation, pages 1\u201388, Online.\nAssociation for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, T. J. Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeff Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models are few-\nshot learners. ArXiv, abs/2005.14165.\nAakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Benton C. Hutchin-\nson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju\nDuke, Anselm Levskaya, Sanjay Ghemawat, Sunipa\nDev, Henryk Michalewski, Xavier Garc\u00eda, Vedant\nMisra, Kevin Robinson, Liam Fedus, Denny Zhou,\nDaphne Ippolito, David Luan, Hyeontaek Lim, Bar-\nret Zoph, Alexander Spiridonov, Ryan Sepassi, David\nDohan, Shivani Agrawal, Mark Omernick, Andrew M.\nDai, Thanumalayan Sankaranarayana Pillai, Marie Pel-\nlat, Aitor Lewkowycz, Erica Moreira, Rewon Child,\nOleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Fi-\nrat, Michele Catasta, Jason Wei, Kathleen S. Meier-\nHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and\nNoah Fiedel. 2022. Palm: Scaling language modeling\nwith pathways. ArXiv, abs/2204.02311.\nHyung Won Chung, Thibault Fevry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2021. Rethink-\ning embedding coupling in pre-trained language mod-\nels. In International Conference on Learning Represen-\ntations.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jen-\nnimaria Palomaki. 2020. TyDi QA: A benchmark for\ninformation-seeking question answering in typologi-\ncally diverse languages. Transactions of the Associa-\ntion for Computational Linguistics, 8:454\u2013470.\nNan Du, Yanping Huang, Andrew M. Dai, Simon\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph,\nLiam Fedus, Maarten Bosma, Zongwei Zhou, Tao\nWang, Yu Emma Wang, Kellie Webster, Marie Pel-\nlat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju\nDuke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui\nWu, Z. Chen, and Claire Cui. 2022. Glam: Efficient\nscaling of language models with mixture-of-experts. In\nInternational Conference on Machine Learning.\nOndrej Dusek and Filip Jurvc\u2019ivcek. 2019.\nNeural\ngeneration for czech: Data and baselines.\nOnd\u02c7rej Du\u0161ek, David M Howcroft, and Verena Rieser.\n2019. Semantic Noise Matters for Neural Natural Lan-\nguage Generation. In Proceedings of the 12th Inter-\nnational Conference on Natural Language Generation\n(INLG 2019), pages 421\u2013426, Tokyo, Japan.\nKawin Ethayarajh and Dan Jurafsky. 2020. Utility is\nin the eye of the user: A critique of NLP leaderboards.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4846\u20134853, Online. Association for Computa-\ntional Linguistics.\nThiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh,\nChris van Der Lee, Simon Mille, Diego Moussallem,\nand Anastasia Shimorina. 2020. The 2020 Bilingual,\nBi-Directional WebNLG+ Shared Task Overview and\nEvaluation Results (WebNLG+ 2020).\nIn Proceed-\nings of the 3rd International Workshop on Natural Lan-\nguage Generation from the Semantic Web (WebNLG+),\nDublin/Virtual, Ireland.\nMarkus Freitag, George Foster, David Grangier, and\nColin Cherry. 2020. Human-paraphrased references\nimprove neural machine translation. In Proceedings\nof the Fifth Conference on Machine Translation, pages\n1183\u20131192, Online. Association for Computational Lin-\nguistics.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. Creating training\ncorpora for nlg micro-planners. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n179\u2013188. Association for Computational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya Ag-\ngarwal, Pawan Sasanka Ammanamanchi, Aremu An-\nuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu,\nMiruna Clinciu, Dipanjan Das, Kaustubh D. Dhole,\nWanyu Du, Esin Durmus, Ond\u02c7rej Du\u0161ek, Chris Emezue,\nVarun Gangal, Cristina Garbacea, Tatsunori Hashimoto,\nYufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng\nJi, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Lad-\nhak, Aman Madaan, Mounica Maddela, Khyati Maha-\njan, Saad Mahamood, Bodhisattwa Prasad Majumder,\nPedro Henrique Martins, Angelina McMillan-Major,\nSimon Mille, Emiel van Miltenburg, Moin Nadeem,\nShashi Narayan, Vitaly Nikolaev, Rubungo Andre Niy-\nongabo, Salomey Osei, Ankur Parikh, Laura Perez-\nBeltrachini, Niranjan Ramesh Rao, Vikas Raunak,\nJuan Diego Rodriguez, Sashank Santhanam, Jo\u00e3o Se-\ndoc, Thibault Sellam, Samira Shaikh, Anastasia Shi-\nmorina, Marco Antonio Sobrevilla Cabezudo, Hen-\ndrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang,\nAkhila Yerukola, and Jiawei Zhou. 2021. The gem\nbenchmark: Natural language generation, its evaluation\nand metrics.\nSebastian Gehrmann, Abhik Bhattacharjee, Abinaya\nMahendiran, Alex Wang, Alexandros Papangelis,\nAman Madaan, Angelina McMillan-Major, Anna\nShvets, Ashish Upadhyay, Bingsheng Yao, et al. 2022a.\nGemv2: Multilingual nlg benchmarking in a single line\nof code. arXiv preprint arXiv:2206.11249.\nSebastian Gehrmann, Elizabeth Clark, and Thibault\nSellam. 2022b. Repairing the cracked foundation: A\nsurvey of obstacles in evaluation practices for generated\ntext. ArXiv, abs/2202.06935.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of GPT-3.\nCoRR, abs/2209.12356.\nYvette Graham, Timothy Baldwin, Alistair Moffat, and\nJustin Zobel. 2014. Is machine translation getting bet-\nter over time? In Proceedings of the 14th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 443\u2013451, Gothenburg,\nSweden. Association for Computational Linguistics.\nMandy Guo, Joshua Ainslie, David C. Uthus, Santi-\nago Onta\u00f1\u00f3n, Jianmo Ni, Yun-Hsuan Sung, and Yinfei\nYang. 2021. LongT5: Efficient text-to-text transformer\nfor long sequences. CoRR, abs/2112.07916.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-sum:\nLarge-scale multilingual abstractive summarization for\n44 languages. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages 4693\u2013\n4703, Online. Association for Computational Linguis-\ntics.\nTom Kocmi, Christian Federmann, Roman Grund-\nkiewicz, Marcin Junczys-Dowmunt, Hitokazu Mat-\nsushita, and Arul Menezes. 2021. To ship or not to\nship: An extensive evaluation of automatic metrics\nfor machine translation. In Proceedings of the Sixth\nConference on Machine Translation, pages 478\u2013494,\nOnline. Association for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tokenizer\nand detokenizer for neural text processing. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing: System Demonstra-\ntions, pages 66\u201371, Brussels, Belgium. Association for\nComputational Linguistics.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new benchmark\ndataset for cross-lingual abstractive summarization. In\nFindings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 4034\u20134048, Online. Associ-\nation for Computational Linguistics.\nMina Lee, Megha Srivastava, Amelia Hardy, John\nThickstun, Esin Durmus, Ashwin Paranjape, Ines\nGerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda\nRong, Rose E. Wang, Minae Kwon, Joon Sung\nPark, Hancheng Cao, Tony Lee, Rishi Bommasani,\nMichael S. Bernstein, and Percy Liang. 2022. Eval-\nuating human-language model interaction.\nCoRR,\nabs/2212.09746.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Annual Meeting of the Association for Compu-\ntational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020b.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pages\n7871\u20137880, Online. Association for Computational Lin-\nguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, Ben-\njamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang,\nChristian Cosgrove, Christopher D. Manning, Christo-\npher R\u2019e, Diana Acosta-Navas, Drew A. Hudson, E. Ze-\nlikman, Esin Durmus, Faisal Ladhak, Frieda Rong,\nHongyu Ren, Huaxiu Yao, Jue Wang, Keshav San-\nthanam, Laurel J. Orr, Lucia Zheng, Mert Yuksek-\ngonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Ni-\nladri S. Chatterji, O. Khattab, Peter Henderson, Qian\nHuang, Ryan Chi, Sang Michael Xie, Shibani San-\nturkar, Surya Ganguli, Tatsunori Hashimoto, Thomas F.\nIcard, Tianyi Zhang, Vishrav Chaudhary, William\nWang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta\nKoreeda. 2022. Holistic evaluation of language models.\nArXiv, abs/2211.09110.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summarization\nBranches Out, pages 74\u201381, Barcelona, Spain. Associ-\nation for Computational Linguistics.\nNitika Mathur, Johnny Wei, Markus Freitag, Qingsong\nMa, and Ond\u02c7rej Bojar. 2020. Results of the WMT20\nmetrics shared task. In Proceedings of the Fifth Confer-\nence on Machine Translation, pages 688\u2013725, Online.\nAssociation for Computational Linguistics.\nSimon Mille, Kaustubh D. Dhole, Saad Mahamood,\nLaura Perez-Beltrachini, Varun Gangal, Mihir Sanjay\nKale, Emiel van Miltenburg, and Sebastian Gehrmann.\n2021. Automatic construction of evaluation suites for\nnatural language generation datasets. In Proceedings\nof the Neural Information Processing Systems Track\non Datasets and Benchmarks 1, NeurIPS Datasets and\nBenchmarks 2021, December 2021, virtual.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme\nsummarization. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing,\npages 1797\u20131807, Brussels, Belgium. Association for\nComputational Linguistics.\nJekaterina Novikova, Ond\u02c7rej Du\u0161ek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-to-end\ngeneration. In Proceedings of the 18th Annual SIGdial\nMeeting on Discourse and Dialogue, pages 201\u2013206,\nSaarbr\u00fccken, Germany. Association for Computational\nLinguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul F.\nChristiano, Jan Leike, and Ryan Lowe. 2022a. Train-\ning language models to follow instructions with human\nfeedback. CoRR, abs/2203.02155.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke E. Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Francis Christiano, Jan Leike, and Ryan J. Lowe.\n2022b. Training language models to follow instructions\nwith human feedback. ArXiv, abs/2203.02155.\nAnkur P. Parikh, Xuezhi Wang, Sebastian Gehrmann,\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-\npanjan Das. 2020. Totto: A controlled table-to-text\ngeneration dataset. ArXiv, abs/2004.14373.\nMaja Popovi\u00b4c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392\u2013395, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nAmy Pu, Hyung Won Chung, Ankur Parikh, Sebastian\nGehrmann, and Thibault Sellam. 2021. Learning com-\npact metrics for MT. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 751\u2013762, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah\nYoung, Eliza Rutherford, Tom Hennigan, Jacob Menick,\nAlbin Cassirer, Richard Powell, George van den Driess-\nche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen\nHuang, Amelia Glaese, Johannes Welbl, Sumanth\nDathathri, Saffron Huang, Jonathan Uesato, John\nF. J. Mellor, Irina Higgins, Antonia Creswell, Nathan\nMcAleese, Amy Wu, Erich Elsen, Siddhant M. Jayaku-\nmar, Elena Buchatskaya, David Budden, Esme Suther-\nland, Karen Simonyan, Michela Paganini, L. Sifre,\nLena Martens, Xiang Lorraine Li, Adhiguna Kun-\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\nDonato, Angeliki Lazaridou, Arthur Mensch, Jean-\nBaptiste Lespiau, Maria Tsimpoukelli, N. K. Grig-\norev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nTobias Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones, James\nBradbury, Matthew G. Johnson, Blake A. Hechtman,\nLaura Weidinger, Iason Gabriel, William S. Isaac, Ed-\nward Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway,\nL. L. Bennett, Demis Hassabis, Koray Kavukcuoglu,\nand Geoffrey Irving. 2021.\nScaling language mod-\nels: Methods, analysis & insights from training gopher.\nArXiv, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020a. Exploring the limits of\ntransfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1\u201367.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, Peter J Liu, et al. 2020b. Exploring the limits of\ntransfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1\u201367.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9,\nAlexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\nGall\u00e9, et al. 2022. Bloom: A 176b-parameter open-\naccess multilingual language model. arXiv preprint\narXiv:2211.05100.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, and Jacopo Staiano. 2020. ML-\nSUM: the multilingual summarization corpus. CoRR,\nabs/2004.14900.\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. Bleurt: Learning robust metrics for text genera-\ntion. In Annual Meeting of the Association for Compu-\ntational Linguistics.\nC. Spearman. 1987. The proof and measurement of\nassociation between two things. by c. spearman, 1904.\nThe American journal of psychology, 100 3-4:441\u201371.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0\nGarriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz,\nAkshat Agarwal, Alethea Power, Alex Ray, Alex\nWarstadt, Alexander W. Kocurek, Ali Safaya, Ali\nTazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman\nHussain, Amanda Askell, Amanda Dsouza, Ameet Ra-\nhane, Anantharaman S. Iyer, Anders Andreassen, An-\ndrea Santilli, Andreas Stuhlm\u00fcller, Andrew M. Dai,\nAndrew La, Andrew K. Lampinen, Andy Zou, Angela\nJiang, Angelica Chen, Anh Vuong, Animesh Gupta,\nAnna Gottardi, Antonio Norelli, Anu Venkatesh, Arash\nGholamidavoodi, Arfa Tabassum, Arul Menezes, Arun\nKirubarajan, Asher Mullokandov, Ashish Sabharwal,\nAustin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas,\nand et al. 2022. Beyond the imitation game: Quanti-\nfying and extrapolating the capabilities of language\nmodels. CoRR, abs/2206.04615.\nAlan L. Stuart and Harry M. Markowitz. 1959. Portfo-\nlio selection: Efficient diversification of investments. A\nQuarterly Journal of Operations Research, 10:253.\nSimeng Sun, Ori Shapira, Ido Dagan, and Ani Nenkova.\n2019. How to compare summarizers without target\nlength? pitfalls, solutions and re-examination of the\nneural summarization literature. In Proceedings of the\nWorkshop on Methods for Optimizing and Evaluating\nNeural Language Generation, pages 21\u201329, Minneapo-\nlis, Minnesota. Association for Computational Linguis-\ntics.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli,\nSebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny\nZhou, and Jason Wei. 2022. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\nCoRR, abs/2210.09261.\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won\nChung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q Tran, Dani Yogatama, and Donald Metzler.\n2022.\nScaling laws vs model architectures: How\ndoes inductive bias influence scaling? arXiv preprint\narXiv:2207.10551.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaguang Li, Hongrae Lee, Huaixiu Zheng, Amin\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim\nKrikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten\nBosma, Yanqi Zhou, Chung-Ching Chang, I. A. Kri-\nvokon, Willard James Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,\nRenelito Delos Santos, Toju Duke, Johnny Hartz\nS\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark D\u00edaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravindran Rajakumar, Alena Butryna, Matthew\nLamm, V. O. Kuzmina, Joseph Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. Lamda: Language models for dialog applica-\ntions. ArXiv, abs/2201.08239.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. ArXiv, abs/1905.00537.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V. Le. 2022a. Finetuned language mod-\nels are zero-shot learners. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou.\n2022b. Chain of thought prompting elicits reasoning in\nlarge language models. ArXiv, abs/2201.11903.\nJohnny Wei and Robin Jia. 2021. The statistical ad-\nvantage of automatic NLG metrics at the system level.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 6840\u20136854,\nOnline. Association for Computational Linguistics.\nFrank Wilcoxon. 1946.\nIndividual comparisons of\ngrouped data by ranking methods. Journal of economic\nentomology, 39(2):269\u2013270.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,\nand Colin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 483\u2013498, Online.\nAssociation for Computational Linguistics.\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du,\nYanping Huang, Jeff Dean, Noam Shazeer, and William\nFedus. 2022. St-moe: Designing stable and transferable\nsparse expert models.\nA\nAdditional empirical results\nTable 3, Table 4 and Table 5 report ROUGE-2\nand BLEURT and ChrF results respectively for all\ntasks. These results are in line with the discussed\nresults in 4\nB\nTechnical details\nFinetuning and inference was done in the t5x\nframework for public and closed access models.\nFew-shot learning task methodology is well de-\nscribed in 3, for public access models inference\nwas done via their respective public API, whilst all\nother models were loaded from the standard check-\npoint into TPU accelerators and inference was\ndone on batches of 64. Finetuning was carried out\nin TPU accelerators, for PaLM we used a constant\nlearning rate of 5\u00d710\u22125, 20x smaller than during\npretraining and reset the optimizer\u2019s (Adafactor)\naccumulators, for T5, mT5 and LongT5 we used a\nconstant learning rate of 1\u00d710\u22124.\nC\nComputational Cost and\nEnvironmental Impact\nIn our work we benchmark 27 generation tasks\nwhich require a substantial amount of computa-\ntional resources. Inference on PLMs is exceed-\ningly more efficient than finetuning. We report\nthe number of test examples across all tasks to be\n194,844. Inference over this number of examples\ntimes 10 models evaluated amounts to 2 million\ninference examples. Finetuning on the other hand,\nrequires all parameters to be trained and training\ndataset sizes are considerably larger. We estimate\nthe compute usage for finetuning to be 256 TPU\nv3 chips for 200 hours. One of the goals of this\nwork is to encourage benchmarking in the future\nto avoid these costs by more efficiently selecting\nsmaller test size and persuade researchers to only\nevaluate finetuning approaches when suitable.\nFigure 4: Empirical probability of p-value of Wilcoxon Rank Sum test < 0.05 for any combination between 1-shot\nand finetuned models.\nOne shot\nFinetuning\nTask\nPaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en)\n26.7\n37.3\n4.2\n37.9\n45.7\n45.3\n44.2\n45.2\n45.5\n46.3\nWebNLG (en)\n33.8\n45.8\n5.4\n46.0\n47.7\n49.2\n47.6\n39.6\n35.8\n48.8\nToTTo (en)\n26.4\n37.8\n3.2\n38.1\n53.9\n55.9\n55.2\n54.1\n53.3\n54.5\nCzech Restaurant (cs)\n7.9\n18.1\n0.9\n22.3\n30.2\n30.6\n25.4\n28.8\n25.0\n29.9\nWebNLG (ru)\n4.9\n16.5\n1.2\n16.8\n22.4\n23.4\n13.0\n23.1\n23.2\n24.2\nEnglish Generation\nXSum (en)\n8.0\n14.4\n3.4\n19.9\n16.3\n21.2\n22.8\n21.0\n17.5\n20.8\nXLSum (en)\n6.5\n11.7\n2.7\n17.0\n19.6\n29.5\n30.6\n28.0\n26.5\n27.4\nWikiLingua (en)\n0.7\n0.4\n0.7\n0.9\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\nCrosslingual Generation\nWikiLingua (es \u2192 en)\n0.7\n0.5\n0.4\n0.6\n0.4\n0.4\n0.3\n0.4\n0.4\n0.4\nWikiLingua (ru \u2192 en)\n3.1\n6.8\n0.1\n7.8\n14.0\n18.7\n12.0\n12.7\n15.1\n14.4\nWikiLingua (tr \u2192 en)\n3.1\n8.7\n2.1\n10.1\n16.6\n23.0\n17.7\n13.8\n20.8\n14.3\nWikiLingua (vi \u2192 en)\n2.4\n5.5\n0.4\n6.8\n13.4\n0.4\n10.3\n9.7\n14.8\n13.2\nMultilingual Generation [SentencePiece-ROUGE-2]\nMLSum (es)\n3.7\n4.5\n0.7\n4.9\n10.7\n0.7\n13.1\n12.1\n13.5\n13.6\nMLSum (de)\n8.8\n16.8\n1.2\n16.9\n26.9\n33.4\n36.5\n36.1\n35.9\n36.3\nXLSum (ar)\n4.5\n11.7\n2.4\n9.6\n25.8\n30.0\n1.9\n2.0\n32.1\n0.6\nXLSum (bn)\n1.0\n1.8\n0.5\n2.7\n18.5\n23.5\n0.2\n0.1\n29.4\n0.1\nXLSum (ja)\n4.0\n6.7\n0.3\n9.6\n27.1\n31.8\n0.1\n0.1\n31.5\n0.1\nXLSum (id)\n7.2\n11.3\n3.7\n12.6\n25.0\n33.0\n30.5\n30.7\n32.7\n30.3\nXLSum (sw)\n7.9\n16.2\n6.5\n6.6\n22.7\n26.8\n27.8\n27.6\n31.3\n26.8\nXLSum (ko)\n6.9\n9.6\n1.6\n9.4\n23.0\n26.7\n4.1\n3.7\n34.9\n4.0\nXLSum (ru)\n6.0\n9.2\n3.7\n10.9\n20.8\n29.4\n4.5\n6.1\n29.6\n6.1\nXLSum (te)\n2.4\n3.3\n1.1\n4.8\n13.3\n22.7\n3.2\n3.2\n26.5\n3.3\nXLSum (th)\n2.9\n4.0\n0.3\n6.2\n16.4\n22.5\n2.4\n2.5\n26.9\n2.4\nXLSum (tr)\n7.5\n10.5\n3.2\n10.7\n23.7\n32.7\n17.6\n17.8\n32.2\n17.7\nXLSum (es)\n5.8\n9.0\n3.1\n9.6\n14.2\n23.7\n20.1\n20.6\n23.0\n20.6\nXLSum (vi)\n4.8\n6.8\n1.5\n7.5\n20.2\n35.9\n11.9\n13.2\n35.5\n13.1\nXLSum (hi)\n4.4\n6.4\n1.8\n7.0\n29.0\n35.7\n1.8\n0.0\n35.4\n0.0\nTable 3: ROUGE-2 and SentencePiece-ROUGE-2 results in data-to-text, English and multilingual generation\ndatasets.\nOne shot\nFinetuning\nTask\nPaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en)\n46.1\n57.8\n15.1\n57.8\n62.5\n61.9\n61.1\n62.1\n60.9\n61.8\nWebNLG (en)\n47.5\n61.8\n17.0\n61.8\n63.6\n65.2\n64.1\n59.4\n55.8\n65.4\nToTTo (en)\n43.5\n55.8\n12.6\n55.2\n67.5\n69.4\n68.3\n67.3\n66.8\n67.7\nCzech Restaurant (cs)\n17.6\n35.6\n7.9\n41.5\n48.0\n48.1\n36.6\n38.2\n44.1\n40.1\nWebNLG (ru)\n21.8\n45.5\n15.3\n45.3\n62.7\n62.6\n24.4\n31.8\n63.5\n32.1\nEnglish Generation\nXSum (en)\n24.6\n32.7\n18.5\n37.6\n34.4\n38.9\n41.3\n39.3\n36.8\n39.0\nXLSum (en)\n23.8\n29.9\n13.9\n33.5\n35.8\n45.8\n47.0\n46.0\n43.9\n44.4\nWikiLingua (en)\n14.7\n14.3\n15.4\n15.8\n15.1\n14.6\n15.2\n15.8\n15.8\n15.3\nCrosslingual Generation\nWikiLingua (es \u2192 en)\n16.8\n13.2\n10.2\n14.6\n14.2\n14.9\n13.0\n15.1\n14.8\n15.7\nWikiLingua (ru \u2192 en)\n19.1\n21.5\n1.5\n22.5\n30.6\n35.9\n24.0\n28.8\n34.3\n33.3\nWikiLingua (tr \u2192 en)\n19.3\n24.6\n12.3\n26.7\n34.4\n39.4\n32.6\n30.8\n39.0\n32.7\nWikiLingua (vi \u2192 en)\n16.4\n19.9\n4.4\n21.3\n31.8\n14.2\n23.2\n27.6\n32.3\n32.5\nMultilingual Generation\nMLSum (es)\n21.3\n22.9\n5.3\n20.6\n28.0\n18.4\n29.1\n28.7\n30.7\n30.3\nMLSum (de)\n28.9\n37.0\n5.7\n34.4\n41.9\n48.8\n50.9\n50.5\n50.1\n51.5\nXLSum (ar)\n14.2\n22.7\n11.4\n24.4\n35.0\n39.6\n0.2\n0.2\n41.6\n0.1\nXLSum (bn)\n10.3\n12.7\n4.5\n17.5\n28.6\n34.0\n0.0\n0.0\n37.8\n0.0\nXLSum (ja)\n8.8\n11.6\n1.2\n13.8\n26.0\n31.3\n0.8\n0.9\n30.6\n0.9\nXLSum (id)\n21.0\n26.0\n9.0\n26.2\n36.8\n45.3\n43.2\n42.3\n43.0\n43.4\nXLSum (sw)\n24.0\n33.0\n15.0\n21.5\n36.2\n42.0\n40.1\n41.1\n44.4\n41.6\nXLSum (ko)\n6.9\n9.4\n1.6\n10.0\n18.0\n21.8\n1.4\n1.2\n27.5\n1.4\nXLSum (ru)\n15.0\n19.8\n9.4\n26.5\n29.1\n38.6\n14.4\n20.1\n40.3\n19.9\nXLSum (te)\n11.3\n13.6\n4.7\n16.8\n18.0\n29.8\n0.3\n0.2\n30.4\n0.3\nXLSum (th)\n14.7\n16.8\n4.4\n21.5\n27.1\n33.4\n0.3\n0.3\n33.9\n0.3\nXLSum (tr)\n20.3\n24.9\n6.2\n24.5\n32.7\n43.1\n31.2\n33.1\n42.6\n33.8\nXLSum (es)\n19.0\n22.9\n7.3\n22.0\n24.5\n33.4\n31.5\n31.9\n32.6\n32.8\nXLSum (vi)\n10.9\n13.1\n2.4\n14.2\n21.8\n37.1\n16.9\n20.2\n34.3\n21.1\nXLSum (hi)\n12.2\n15.1\n6.6\n18.8\n33.2\n39.6\n0.2\n0.0\n39.1\n0.0\nTable 4: ChrF results in data-to-text, English and multilingual generation datasets.\nOne shot\nFinetuning\nTask\nPaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en)\n60.0\n71.8\n44.2\n72.3\n76.5\n75.8\n75.1\n76.4\n75.9\n76.2\nWebNLG (en)\n62.3\n74.5\n43.5\n74.7\n75.8\n76.8\n75.6\n71.2\n67.8\n76.3\nToTTo (en)\n56.9\n69.4\n33.1\n69.5\n76.8\n77.9\n77.0\n76.6\n76.8\n76.7\nCzech Restaurant (cs)\n34.7\n66.8\n32.2\n72.2\n75.8\n74.4\n48.8\n51.8\n72.9\n48.8\nWebNLG (ru)\n39.2\n67.8\n19.7\n66.9\n77.5\n78.0\n25.9\n29.8\n78.4\n29.9\nEnglish Generation\nXSum (en)\n43.0\n46.9\n28.2\n53.4\n51.0\n55.5\n58.5\n56.4\n53.2\n56.4\nXLSum (en)\n32.7\n41.1\n22.0\n51.1\n52.6\n61.9\n63.0\n61.8\n60.3\n60.8\nWikiLingua (en)\n33.3\n34.0\n27.9\n34.3\n32.2\n32.2\n31.3\n32.4\n32.6\n32.1\nCrosslingual Generation\nWikiLingua (es \u2192 en)\n32.9\n33.7\n16.8\n33.4\n32.3\n32.6\n31.0\n32.5\n33.0\n32.5\nWikiLingua (ru \u2192 en)\n38.8\n43.3\n6.4\n45.9\n50.6\n54.4\n46.4\n49.1\n52.3\n51.5\nWikiLingua (tr \u2192 en)\n39.3\n44.0\n19.3\n46.4\n49.2\n54.4\n48.6\n45.4\n52.8\n46.8\nWikiLingua (vi \u2192 en)\n35.6\n38.2\n5.7\n40.8\n50.6\n32.8\n45.4\n45.5\n51.4\n50.0\nMultilingual Generation\nMLSum (es)\n21.0\n21.5\n-1.3\n26.7\n30.6\n6.3\n25.9\n24.2\n32.0\n27.0\nMLSum (de)\n39.4\n50.1\n4.6\n49.3\n57.8\n63.4\n62.5\n61.6\n61.5\n61.4\nXLSum (ar)\n19.8\n28.5\n2.5\n27.7\n44.6\n50.1\n2.8\n3.5\n53.0\n4.3\nXLSum (bn)\n31.8\n41.8\n0.2\n27.4\n46.6\n57.1\n2.9\n3.6\n62.7\n2.8\nXLSum (ja)\n28.1\n31.4\n-1.2\n34.7\n47.0\n52.2\n-0.3\n-0.3\n53.0\n-0.3\nXLSum (id)\n41.2\n47.4\n9.5\n53.8\n58.7\n68.0\n61.4\n65.5\n66.8\n65.6\nXLSum (sw)\n25.5\n36.3\n14.3\n24.0\n45.8\n52.9\n48.6\n50.2\n59.1\n48.9\nXLSum (ko)\n25.6\n31.6\n-0.3\n33.0\n40.5\n47.1\n0.8\n1.6\n54.6\n1.4\nXLSum (ru)\n30.1\n37.3\n3.2\n33.0\n47.9\n59.6\n14.2\n16.7\n58.0\n17.0\nXLSum (te)\n29.6\n35.0\n6.5\n22.7\n32.0\n49.1\n10.9\n11.5\n51.6\n12.4\nXLSum (th)\n22.0\n27.2\n-0.1\n16.3\n31.9\n43.6\n-1.1\n-0.9\n46.0\n-1.1\nXLSum (tr)\n30.8\n34.5\n3.3\n40.0\n49.8\n63.8\n21.4\n26.4\n62.5\n26.3\nXLSum (es)\n21.2\n26.3\n0.0\n30.6\n31.5\n46.2\n33.3\n36.1\n45.2\n35.7\nXLSum (vi)\n14.5\n14.5\n-1.6\n16.4\n24.7\n46.5\n-4.0\n-4.6\n45.0\n-4.5\nXLSum (hi)\n33.9\n40.4\n7.0\n33.7\n50.7\n57.5\n5.7\n4.6\n57.3\n4.6\nTable 5: BLEURT results in data-to-text, English and multilingual generation datasets.\n"
  },
  {
    "title": "ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch",
    "link": "https://arxiv.org/pdf/2306.16857.pdf",
    "upvote": "5",
    "text": "ArrayBot: Reinforcement Learning for Generalizable\nDistributed Manipulation through Touch\nZhengrong Xue* 1,2,3,4, Han Zhang* 1,2, Jingwen Cheng1, Zhengmao He2,\nYuanchen Ju1,5, Changyi Lin2, Gu Zhang2,4, Huazhe Xu\u2020 1,2,3\n1Tsinghua University,\n2Shanghai Qi Zhi Institute,\n3Shanghai AI Lab,\n4Shanghai Jiao Tong University,\n5Southwest University\nhttps://steven-xzr.github.io/ArrayBot\nAbstract: We present ArrayBot, a distributed manipulation system consisting of a\n16\u00d716 array of vertically sliding pillars integrated with tactile sensors, which can\nsimultaneously support, perceive, and manipulate the tabletop objects. Towards\ngeneralizable distributed manipulation, we leverage reinforcement learning (RL)\nalgorithms for the automatic discovery of control policies. In the face of the mas-\nsively redundant actions, we propose to reshape the action space by considering\nthe spatially local action patch and the low-frequency actions in the frequency do-\nmain. With this reshaped action space, we train RL agents that can relocate diverse\nobjects through tactile observations only. Surprisingly, we find that the discovered\npolicy can not only generalize to unseen object shapes in the simulator but also\ntransfer to the physical robot without any domain randomization. Leveraging the\ndeployed policy, we present abundant real-world manipulation tasks, illustrating\nthe vast potential of RL on ArrayBot for distributed manipulation.\nKeywords: Distributed Manipulation, Reinforcement Learning, Tactile Sensing\nFigure 1: We design and manufacture the hardware of ArrayBot, a distributed manipulation system.\nTowards generalizable distributed manipulation, we train reinforcement learning (RL) agents on the\nsimulated ArrayBot where the only accessible observation is the tactile information. We deploy the\ncontrol policy to the physical robot and showcase the bird\u2019s-eye view of the trajectories for real-\nworld manipulation tasks: relocating objects of novel shapes, manipulation in parallel, trajectory\nfollowing, and manipulating transparent objects under severe visual disturbance.\n*Equal Contribution. \u2020Corresponding Author. Contact: huazhe_xu@mail.tsinghua.edu.cn\narXiv:2306.16857v1  [cs.RO]  29 Jun 2023\n1\nIntroduction\nThe notion of robotic manipulation [1, 2] easily invokes the image of a biomimetic robot arm or\nhand trying to grasp tabletop objects and then rearrange them into desired configurations inferred\nby exteroceptive sensors such as RGBD cameras. To facilitate this manipulation pipeline, the robot\nlearning community has made tremendous efforts in either how to determine steadier grasping poses\nin demanding scenarios [3, 4, 5, 6, 7] or how to understand the exteroceptive inputs in a more robust\nand generalizable way [8, 9, 10, 11, 12, 13]. Acknowledging these progresses, this paper attempts to\nbypass the challenges in the prevailing pipeline by advocating ArrayBot, a reinforcement learning\ndriven system for distributed manipulation [14], where the objects are manipulated through a great\nnumber of actuators with only proprioceptive tactile sensing [15, 16, 17, 18].\nConceptually, the hardware of ArrayBot is a 16\u00d716 array of vertically sliding pillars, each of which\ncan be independently actuated, leading to a 16\u00d716 action space. Functionally, the actuators beneath\na tabletop object can support its weight and at the same time cooperate to lift, tilt, and even translate\nit through proper motion policies. To equip ArrayBot with proprioceptive sensing, we integrate each\nactuator with a slim and low-cost Force Sensing Resistor (FSR) sensor, allowing the robot to \u201cfeel\u201d\nthe object when lack of external visual inputs. Thanks to its distributed nature, ArrayBot is flexible\nin size, inherently supports manipulation in parallel, and has the potential to manipulate objects\ntimes larger than the size of its end-effector.\nPrevious works for distributed manipulation show up in the names of actuator array [19, 20, 21, 22],\nsmart surface [23, 24], or auxiliary functions of tangible user interface [25, 26, 27]. Despite their\npromises to manipulate tabletop objects, they heavily depend on pre-defined motion primitives to\nfit the specific designs of the actuators and could only manipulate fixed types of objects in a pre-\nprogrammed manner, which hinders their usefulness in actual applications. Towards distributed\nmanipulation enjoying better generalizability and versatility, we explore the feasibility of applying\nmodel-free reinforcement learning (RL) [28, 29, 30] to the automatic discovery of control policies.\nHowever, compared with popular manipulators such as arms or hands, controlling ArrayBot in its\n2D-array action space can be extremely challenging because the massive redundancy of the actions\nmakes the trial-and-error process hopelessly inefficient.\nIn awareness of its redundancy, we propose to reshape the action space with the objective to\nstrengthen its inductive bias towards more favorable actions for distributed manipulation. To start\nwith, we explicitly restrict the extend of the valid action space to the local action patch centered\naround the object since far away actuators could hardly exert any physical effect. Meanwhile, we\npropose the idea of considering actions in the frequency domain via 2D Discrete Cosine Transform\n(DCT) [31]. The rationale is that for distributed manipulation, the impacts of collective behaviors\nare more stable and predictable than individual collisions [32, 33]. As each channel in the frequency\ndomain processes a spatially global horizon, an action space in the frequency domain may enjoy\nan implicit bias toward collaborations. On top of the frequency transform, we further perform high\nfrequency truncation on the action channels, which is inspired by image compression methods such\nas JPEG [34] that discard high-frequency patterns. Likewise, we argue the same preference applies\nto the 2D-array action space because lower-frequency actions are likely to produce emergent mo-\ntion primitives with semantics, e.g., the DC channel implies lifting, and the base frequency implies\ntilting. In contrast, the detailed texture information determined by higher-frequency channels makes\nlittle influence on the manipulation tasks.\nWith the reshaped action space, we first demonstrate that the policies discovered by model-free RL\non ArrayBot are as capable as pre-defined primitives by setting up the ArrayBot environment in\nthe Isaac Gym simulator [35] and then training RL agents [30] that lift and flip a cube respectively.\nGoing beyond the reach of motion primitives, we showcase one single policy on ArrayBot learned\nby RL that is agnostic of both object shapes and visual observations, but could relocate hundreds of\npreviously unseen objects of various shapes from and to arbitrary positions via touch sensing alone.\nSurprisingly, we find it possible to directly deploy the policy trained in the simulator to the real-\nworld machine without any sim-to-real transfer techniques including domain randomization [36].\n2\nLeveraging the general relocate-via-touch policy deployed to the real world, we present some more\nconcrete applications of ArrayBot showing its ability to manipulate objects of diverse shapes and\ndrive objects according to specified trajectories, its robustness to dramatically varying environment\nappearances and transparent objects, and finally its potential for manipulation in parallel. We also\npresent some hard-coded policies for very static but more complicated tasks to demonstrate the vast\npotential of our hardware. For visual impression, please refer to the project website.\nOur contributions are summarized as: (i) we design and manufacture the hardware of ArrayBot that\ncould simultaneously support, perceive, and manipulate objects; (ii) we propose methods of action\nspace reshaping for ArrayBot that validate the automatic policy discovery via model-free RL on\na distributed manipulation system; (iii) we train one relocate-via-touch policy suitable for various\nshapes in the simulator which can be instantly deployed to the real world; (iv) we provide concrete\napplications empowered by the RL-derived policy to better illustrate the advantages of ArrayBot.\n2\nAction Space Reshaping for Distributed Manipulation\nThe central challenge against the employment of RL for distributed manipulation comes from its un-\nconventional action space with massive redundancy. In this section, we present a series of techniques\nto reshape the action space of ArrayBot so that it is more favored for distributed manipulation.\nLocal action patch.\nThe action space of ArrayBot is in the shape of a 16 \u00d7 16 array. Given the\nfact that the actuators far away from the object could not make any impact on manipulation, we only\nconsider a 5 \u00d7 5 local action patch (LAP) centered around the object. So far, an untouched detail\nis how to determine the center of the local patch. If the ground-truth positions of the objects are\nassumed to be accessible in the simulator, we simply select the actuator that is closest to the center\nof the object as the center of the LAP. However, the observation space in a real-world scenario does\nnot include such privileged state information. In ArrayBot, we estimate the object position through\ntouch. More specifically, we post-process the noisy readings of the 16\u00d716 FSR sensor array, giving\na binary tactile map that indicates contact conditions. Afterward, the mean of the contact points is\ncalculated as the estimated center position of the manipulated object.\nHigh\nLow\nFigure 2: We consider the 6 lowest-\nfrequency channels marked in green\non a 5 \u00d7 5 2D DCT map.\nActions in the frequency domain.\nRather than figure out\nthe impact of every individual contact in a contact-rich en-\nvironment which is virtually insolvable [32, 33], we instead\nconsider the impacts of collective behaviors for distributed\nmanipulation [26, 21, 22]. We propose to learn actions in\nthe frequency domain in which each component may have\na global impact in the spatial domain. Hence, rather than\ndirectly predict a flattened 25-dimensional variance of posi-\ntions, the policy network outputs a 25-dimensional variance\nof frequencies. Subsequently, the 25-dimensional output is\nunflattened to the shape of 5\u00d75 and then post-processed by a\n2D inverse Discrete Cosine Transform (iDCT) [31] operator\nto produce the ultimate 5 \u00d7 5 action in the spatial domain.\nHigh frequency truncation.\nBesides a latent inductive bias\ntowards collaborations, the frequency domain also provides\na valuable point of view to re-inspect the redundancy of the\nactions. Intuitively, low-frequency channels lead to smooth planar surfaces with semantics that\nare likely to correspond to emergent motion primitives. Based on these observations, we propose\nto truncate the high-frequency channels of the actions. The policy network is now designed to\noutput a 6-dimensional prediction, which is used to fill the 6 lowest-frequency channels of the entire\npredicted action in the frequency domain. To acquire a full 25-dimensional action ready for the\ninverse frequency transform, we simply zero-pad the rest 19 channels of higher frequencies.\n3\nObject Position (3,)\nTactile Map (16x16)\nGoal Direction (3,)\nTarget Position (3,)\nRobot State\nDescriptor (6,)\nFrequency\nTruncation\n2D DCT\nRobot State (16x16)\n5 x 5\nObject Position \nEstimate\n2D \niDCT\n5 x 5\nPolicy Net\nState (15,)\nTarget\nPosition\nInfer\nPID Control\nAction\nFigure 3: An overview of the RL framework on ArrayBot for general relocation. The state is the\ncombination of the estimated object position, the specified target position, the residual goal direction,\nand the robot state in the frequency domain. Exempt from any visual inputs, the states are inferred\nfrom purely proprioceptive observations of the robot joint configuration and the tactile sensor array.\n3\nLearning Control Policies in the Reshaped Action Space\n3.1\nSystem Setup\nThe hardware.\nWe design and manufacture the hardware of ArrayBot, which can be perceived\nas a 16 \u00d7 16 array of vertically sliding units. Each unit from down to up consists of a motor that\ndrives the unit to move in the vertical direction, a rectangular pillar whose length of each dimension\nis 16 \u00d7 16 \u00d7 200 mm respectively, a slim and low-cost Force Sensing Resistor (FSR) sensor that\nmeasures the vertical pressure, and a silicone hemispheric end-effector that protects the tactile sensor\nand increases the frictions. The effective range of each vertically prismatic joint is 55 mm, whose\nmaximum speed of motion is 53 mm/s. Actions are executed by ArrayBot via positional PID control.\nThe simulator.\nThe physical simulation of contact-rich interactions could be time-consuming. To\nproduce sufficient samples in an efficient way so as to feed data-hungry RL algorithms, we build the\nsimulated environment of ArrayBot in the Isaac Gym [35] simulator. The frequency of the physical\nsimulation steps is 50 Hz. Due to the mechanical speed limit, we set the frequency of RL control to\nbe 5 Hz. Since the RL algorithm considers the binarized outcome of the tactile sensor, we retrieve\nthe information from the contact buffer of the simulator as the simulation of tactile sensors.\n3.2\nEnvironments\nWe consider three manipulation tasks: lifting, flipping, and general relocation. In lifting, we ask\nArrayBot to raise a cubic block as high as possible. In flipping, we ask ArrayBot to flip the same\nblock by 90 degrees along a certain axis. In general relocation, we ask ArrayBot to relocate objects\nof various shapes from and to any arbitrary positions within its reach via only tactile sensing.\nStates.\nThe tasks of lifting and flipping directly make use of the privileged position and orientation\ninformation provided by the simulator. In general relocation, we consider a more realistic scenario\nwhere the only observation is the binarized tactile information. The estimated states for general\nrelocation are visualized in Figure 3. More details can be found in Table 1 in Appendix B.\nRewards.\nThe tasks of lifting and flipping simply consider a dense reward of object height and\norientation respectively. In general relocation, apart from the dense reward of object position, we\n4\nadd one more sparse bonus reward when reaching the goal that would encourage the robot to timely\nstop the object at the goal position. More details can be found in Table 2 in Appendix B.\nActions.\nAt each step, the policy outputs a 6-dimensional action in the frequency domain, which\nis post-processed to produce the relative joint configuration on the 5 \u00d7 5 LAP.\nResets.\nWe reset the episode if the tabletop object moves out of the border or the episode length\nreaches 100 steps. With the existence of the 5 \u00d7 5 LAP, we request that the center of the object\nshould locate on the central 11 \u00d7 11 patch.\nManipulated objects.\nIn lifting and flipping, we manipulate an 8\u00d78\u00d78 cm cubic block which can\nbe roughly supported by a 4 \u00d7 4 array of actuators. In general relocation, we train an RL agent that\nis generalizable to shape variance by sampling 128 different shapes from the EGAD [37] training\nset and then re-scaling them. At test time, we evaluate the performance of the generalizable agent on\nthe EGAD test set with a total of 49 unseen object shapes. For fast and accurate collision detection\nin the simulator, we perform V-HACD [38] convex decomposition to all of the object shapes.\n3.3\nTraining RL agents\nFor all the tasks, we train proximal policy optimization (PPO) [30] agents on 128 parallel Isaac\nGym environments for the automatic discovery of control policies. More details in RL training can\nbe found in Appendix B. Notably, all of the 128 parallel environments for general relocation involve\nmutually different object shapes. With a state space agnostic of the object shapes at all, the agent\nreceives mixed types of dynamics. This forces the agent to discover a policy that is as universal as\npossible for all the shapes in the dataset, thus enhancing its generalizability to the shape variance.\n3.4\nExperiments for Lifting and Flipping\nMetrics.\nIn lifting and flipping, we compare the averaged accumulated returns and the survival\nsteps of each episode. The survival step refers to the steps an object could stay on the robot without\nfalling. The survival step has a maximum of 100.\nCompared methods.\nTo study the necessity of our reshaped action space, we train the same PPO\nalgorithm in the following action spaces: (i) LAP only in the spatial domain; (ii) LAP+DCT-25 that\npreserves all of the 25 channels in the frequency domain; and (iii) LAP+DCT-6 that considers only\nthe 6 lowest-frequency channels.\nTraining steps\nReturns\nSurvival Steps\nTraining steps\nTraining steps\nTraining steps\nLAP only\nLAP + DCT-25\nLAP + DCT-6\nLifting\nLifting\nFlipping\nFlipping\nReturns\nSurvival Steps\n(a)\n(b)\n(c)\nFigure 4: (a) The training curves in terms of episode returns and survival steps. The results are aver-\naged on 5 seeds. The shaded area stands for the standard deviation. (b)(c) The example trajectories\nof the policies learned by (b) LAP+DCT-6 and (c) LAP only for flipping.\n5\nResults.\nThe learning curves of both tasks are shown in Figure 4(a). In both tasks, the DCT-based\napproaches have a significant advantage over the one trained in the spatial action space in terms\nof both total returns and survival steps. Further, the DCT-6 method survives longer and behaves\nbetter than DCT-25, especially in the more challenging task of flipping, echoing the intuition that\nlow-frequency patterns lead to more steady actions. By visualizing the flipping trajectories in Fig-\nure 4(b)(c), we find that LAP only hacks the environment and learns to gain rewards by throwing the\nblock off the robot in a rolling way. In comparison, the actions of LAP+DCT-6 are more gentle and\nreasonable, which explains its better performance and longer survival time.\n3.5\nExperiments for General Relocation\nMetrics.\nWe report the success rate of relocation on the EGAD [37] test set containing 49 unseen\nobjects. An episode is judged to succeed if the object reaches the target position and insists for at\nleast 1 second. The results are averaged over 200 trails with random initial and target positions.\nFigure 5: The success rate of the gen-\neral relocation policy on the previously\nunseen EGAD [37] test set.\nResults.\nThe success rates shown in the right figure fol-\nlow the same taxonomy in EGAD where the grasping\ndifficulty and shape complexity of objects is alphabeti-\ncally and numerically sorted. We observe that ArrayBot\ncan achieve relatively high success rates on most of the\nlow-difficulty objects even when the shape complexity is\nhigh. Considering none of the objects is shown to the RL\nagent at training time, this indicates remarkable general-\nizability. Meanwhile, we find that specific objects that are\nconsidered as hard to grasp (e.g., F3 & F4) are relatively\neasy for ArrayBot. This implies that ArrayBot might have\ncomplementary ability to the existing general manipula-\ntors. In the course of the evaluation, we find the control\npolicy discovered by RL on ArrayBot is a mixture of slid-\ning and rolling. This explains why slender planar objects\n(e.g., G6 is in the shape of a piece of snow), which pose significant difficulties for conventional\ngrasping methods, also present challenges for distributed manipulation techniques due to their lim-\nited ability to undergo rolling motion.\n4\nDeploying Control Policies to the Physical Robot\nZero-shot sim-to-real transfer.\nTo our surprise, we find that the general relocation policy trained\non the EGAD dataset in the simulator can be effortlessly deployed to the physical robot for multi-\nple relocation-based tasks. Note that the instant sim-to-real transfer even exempts the involvement\nof popular sim-to-real techniques such as domain randomization [36]. Intuitively, there are two\nsources of sim-to-real gaps: perception and dynamics. Our selection of proprioceptive observations\nand binarized tactile measurements keeps the discrepancies in perception at a low level. Mean-\nwhile, both the diverse shapes introduced at training time and the massive redundancy in the action\nspace contribute to the resilience towards the shift in dynamics. These factors combined explain the\nextraordinary convenience that ArrayBot brings in the sim-to-real procedure.\nReal-world manipulation tasks.\nLeveraging the general relocation policy successfully deployed\nto the real world, we manage to further demonstrate the merits of our ArrayBot by presenting the\nfollowing manipulation tasks on the physical robot:\n\u2022 Manipulating objects of diverse shapes. ArrayBot is particularly good at natural objects such as\nmelons and dragon fruits. It could also manipulate man-made objects such as rubber duck and\nrugby-shaped artware. Note that the objects are times larger than the size of the end-effector,\nwhich is beyond the reach of arms or hands since they are too large to grasp or hold.\n6\n(a)\n(b)\nFigure 6: Real-world trajectories of the manipulation tasks showing the robustness of our system to\nthe impacts of (a) unexpected external forces (b) severe visual degradations.\n\u2022 Trajectory following. Trajectory following can be easily achieved by iterative calls of the general\nrelocation policy. Note that ArrayBot is friendly to incremental operations since its efforts are in\nproportion to the traveled distance. In comparison, slight operations are as troublesome for arms\nor hands since the costs in \u201cpick\u201d and \u201cplace\u201d are constants whatever the scale of the operations.\n\u2022 Manipulation in parallel. Consisting of a large number of actuators, ArrayBot inherently supports\nparallel manipulation. Thanks to the spatial self-similarity in the mechanical structure, the same\ncontrol policy automatically adapts to all local patches. Assuming no collision occurs, manipulat-\ning two objects in parallel is as easy as initializing two independent processes for manipulation.\nWe leave more complicated collision-involved parallel manipulation for future work.\n\u2022 Manipulation under visual degradations. Relying on no visual observations at all, ArrayBot is no\ndoubt robust to visual degradations. We exhibit the ability of ArrayBot to confidently manipulate\ntransparent objects under severe visual disturbance from a projector.\nBesides the power of RL-derived control policies, we also showcase some hard-coded policies for\nstatic but more complicated manipulation settings. For visual illustration of the aforementioned\ntasks, please find Figure 1&6, Figure 10&11 in Appendix C, and the project website.\n5\nA Sketch for the Hardware Design\nArrayBot consists of a robotic frame and densely arranged controllable pillars. The robotic frame is\ncomposed of supporting aluminum alloy columns, acrylic plates, and a stainless steel base. Inside\nthe frame, there are 256 rectangular prism pillars that can move vertically powered by motor drives\nunder the constraints of sliding rail brackets. Each pillar has a cross-sectional area of 16 \u00d7 16 mm2\nand needs to be compactly arranged with gaps of 4 mm for smooth manipulation. The pillar move-\nments are controlled by STM32 microcontrollers. In this section, we provide important hardware\ndesign choices of ArrayBot. We leave the full-fledged design details to Appendix A.\nEnd-effector.\nThe right side of Figure 7(a) is the end-effector, which has a silicone semi-spheric\ncap and an FSR tactile sensor. When an object is placed on the end effector, the supporting force is\ntransmitted through the silicone cap to the plate-shaped FSR sensor. For successful signal transmis-\nsion in limited physical space, we design a conductive sliding rail that can avoid the use of wires.\nMotor\nEncoder\nCoupling\nLead Screw\nSliding Rail\nFSR Sensor\nPillar\nSilicone Cap\nSTM 32 \nBoard\nCircuit of Sliding Rail\n(a)\n(b)\nFigure 7: (a) The exploded view of a single pillar. (b) The assembled modular unit of a 1 \u00d7 2 array.\n7\nActuators.\nThe left side of Figure 7(a) is the actuator, which is a direct current (DC) gear motor.\nIt converts the rotational motion of the motor into translational motion through a screw structure. A\nmagnetic encoder is installed on the rotating shaft of the motor to calculate the angle and angular\nvelocity and ultimately map them to the position and speed of the pillar.\nPillar module.\nAs shown in Figure 7(b), in order to facilitate assembly and maximize the utiliza-\ntion of the microcontroller\u2019s performance, we combine every two pillars into a module controlled by\nan STM32 board. Hence, the entire robot contains 128 modules that are controlled by 4 Controller\nArea Network (CAN) signals. During operation, the STM32 board receives the CAN commands\nfrom a desktop, processes them, and outputs two pulse width modulation (PWM) signals respec-\ntively to control the two motors.\n6\nRelated Works\nDistributed manipulation\ncontrols the motion of the target object through numerous points of\ncontact [14]. Composed of an array of stationary unit cells, a distributed manipulation system is\nable to be scaled in size and inherently support manipulation in parallel. Reviewing the literature, the\nmajority of distributed manipulation systems consist of an array of special-purpose actuators such as\nvibrating plates [19], air jets [39], roller wheels [20], electromagnets [25] and delta robots [21, 22].\nWhile they are designed to be skilled in specific types of manipulation tasks, they are typically\nnot versatile enough and demand elaborately pre-defined motion primitives. Compared with the\nprototypes in robotics research, the hardware of ArrayBot is more related to the branch of works\nknown as the \u201cshape-changing tangible user interfaces\u201d [26, 27, 40] in the human-computer interface\ncommunity, where the actuators are vertically prismatic pillars. The simplicity in design not only\nmakes it easier to manufacture; its organized action space also helps open the door to learning motion\npolicies with model-free RL.\nLearning in the frequency domain\nis a widely discussed concept in a variety of scopes in the\nmachine learning community. In computer vision studies, frequency transformations are adopted to\nbridge the gap between high-quality images and down-sampled ones [41], and help achieve more\naccurate gradient approximations in Binary CNNs [42]. In the topics of reinforcement learning, the\nconcept of frequency is utilized to boost the efficiency of search-control in model-based architec-\ntures [43], and represent the characteristic function of returns for distributional RL [44]. In contrast\nto all the existing works that we have found, frequency transformation is leveraged in ArrayBot for\naction space reshaping, aiming at reformative sample efficiency in model-free RL.\n7\nConclusion and Limitations\nWe present ArrayBot, an RL-driven distributed manipulation system with tactile sensing. We train a\ngeneralizable relocate-via-touch policy in the proposed reshaped action space, which can be instantly\ntransferred to the physical robot. Leveraging the deployed policy, we showcase the distinctive merits\nof our ArrayBot through a variety of real-world manipulation tasks.\nGiven all our findings, we would like to call for the attention of the robot learning community to\ndistributed manipulation. We believe the redundancy in its unconventional action space is a two-\nedged sword. As long as we find ways to alleviate its demon face of sample inefficiency, its angel\nface of fault tolerance might offer shortcuts for solving well-known challenges in the community\nsuch as generalizability to object shapes and bridging the sim-to-real gaps.\nAdmittedly, the prototype of ArrayBot is not yet perfect. For instance, its 16 \u00d7 16 size is still not\nlarge enough for large-scale multi-object manipulation in parallel. The speed limit of the joints\nmay hinder the RL agent from exploring more highly dynamical control policies. We also find that\nthe tactile sensors are sometimes not sensitive enough given that the pressure exerted by an object\nis shared by multiple sensors. In the future, we will continually upgrade both the hardware and\nsoftware of ArrayBot so as to unearth the even greater potential of RL for distributed manipulation.\n8\nAcknowledgments\nWe would like to give special thanks to Mr. Yuanhong Wang and his group of colleagues from Kun-\nshan Qiaotuo ModelTech Co., Ltd. for their contributions to the manufacture of the hardware. We\nalso thank Yitong Wang for hardware maintenance, Tianyu Huang and Yao Lu for elegant graphic\nrendering of the hardware design, and Yuzhe Qin for his advice on the Isaac Gym simulator. Huazhe\nXu is supported by National Key R&D Program of China (2022ZD0161700).\nReferences\n[1] A. Billard and D. Kragic. Trends and challenges in robot manipulation. Science, 364(6446):\neaat8414, 2019.\n[2] O. Kroemer, S. Niekum, and G. Konidaris.\nA review of robot learning for manipulation:\nChallenges, representations, and algorithms. The Journal of Machine Learning Research, 22\n(1):1395\u20131476, 2021.\n[3] J. Kerr, L. Fu, H. Huang, Y. Avigal, M. Tancik, J. Ichnowski, A. Kanazawa, and K. Goldberg.\nEvo-nerf: Evolving nerf for sequential robot grasping of transparent objects. In 6th Annual\nConference on Robot Learning.\n[4] W. Zhou and D. Held. Learning to grasp the ungraspable with emergent extrinsic dexterity. In\nConference on Robot Learning, pages 150\u2013160. PMLR, 2023.\n[5] X. Zhu, D. Wang, O. Biza, G. Su, R. Walters, and R. Platt. Sample efficient grasp learning\nusing equivariant models. arXiv preprint arXiv:2202.09468, 2022.\n[6] L. Shao, F. Ferreira, M. Jorda, V. Nambiar, J. Luo, E. Solowjow, J. A. Ojea, O. Khatib, and\nJ. Bohg. Unigrasp: Learning a unified model to grasp with multifingered robotic hands. IEEE\nRobotics and Automation Letters, 5(2):2286\u20132293, 2020.\n[7] J. W. James and N. F. Lepora. Slip detection for grasp stabilization with a multifingered tactile\nrobot hand. IEEE Transactions on Robotics, 37(2):506\u2013519, 2020.\n[8] R. Shi, Z. Xue, Y. You, and C. Lu. Skeleton merger: an unsupervised aligned keypoint detector.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n43\u201352, 2021.\n[9] W. Yuan, C. Paxton, K. Desingh, and D. Fox. Sornet: Spatial object-centric representations for\nsequential manipulation. In Conference on Robot Learning, pages 148\u2013157. PMLR, 2022.\n[10] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitz-\nmann. Neural descriptor fields: Se (3)-equivariant object representations for manipulation. In\n2022 International Conference on Robotics and Automation (ICRA), pages 6394\u20136400. IEEE,\n2022.\n[11] Z. Xue, Z. Yuan, J. Wang, X. Wang, Y. Gao, and H. Xu.\nUseek: Unsupervised se (3)-\nequivariant 3d keypoints for generalizable manipulation. arXiv preprint arXiv:2209.13864,\n2022.\n[12] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell.\nReal-world robot\nlearning with masked visual pre-training. In Conference on Robot Learning, pages 416\u2013426.\nPMLR, 2023.\n[13] B. Wen, W. Lian, K. Bekris, and S. Schaal. You only demonstrate once: Category-level ma-\nnipulation from single visual demonstration. In Robotics: Science and Systems (RSS), 2022.\n[14] K. F. B\u00a8ohringer, H. Choset, and H. Choset. Distributed manipulation. Springer Science &\nBusiness Media, 2000.\n9\n[15] G. Khandate, M. Haas-Heger, and M. Ciocarlie. On the feasibility of learning finger-gaiting\nin-hand manipulation with intrinsic sensing. In 2022 International Conference on Robotics\nand Automation (ICRA), pages 2752\u20132758. IEEE, 2022.\n[16] S. Pai, T. Chen, M. Tippur, E. Adelson, A. Gupta, and P. Agrawal. Tactofind: A tactile only\nsystem for object retrieval. arXiv preprint arXiv:2303.13482, 2023.\n[17] I. Guzey, B. Evans, S. Chintala, and L. Pinto. Dexterity from touch: Self-supervised pre-\ntraining of tactile representations with robotic play. arXiv preprint arXiv:2303.12076, 2023.\n[18] Z.-H. Yin, B. Huang, Y. Qin, Q. Chen, and X. Wang. Rotating without seeing: Towards in-hand\ndexterity through touch. arXiv preprint arXiv:2303.10880, 2023.\n[19] K.-F. Bohringer, V. Bhatt, and K. Y. Goldberg. Sensorless manipulation using transverse vi-\nbrations of a plate. In Proceedings of 1995 IEEE International Conference on Robotics and\nAutomation, volume 2, pages 1989\u20131996. IEEE, 1995.\n[20] J. E. Luntz, W. Messner, and H. Choset. Distributed manipulation using discrete actuator\narrays. The International Journal of Robotics Research, 20(7):553\u2013583, 2001.\n[21] S. Thompson, P. Mannam, Z. Temel, and O. Kroemer. Towards robust planar translations using\ndelta-manipulator arrays. In 2021 IEEE International Conference on Robotics and Automation\n(ICRA), pages 6563\u20136569. IEEE, 2021.\n[22] S. Patil, T. Tao, T. Hellebrekers, O. Kroemer, and F. Z. Temel. Linear delta arrays for dexterous\ndistributed manipulation. arXiv preprint arXiv:2206.04596, 2022.\n[23] D. R. Barr, D. Walsh, and P. Dudek. A smart surface simulation environment. In 2013 IEEE\nInternational Conference on Systems, Man, and Cybernetics, pages 4456\u20134461. IEEE, 2013.\n[24] T. A. T. Dang, M. Bosch-Mauchand, N. Arora, C. Prelle, and J. Daaboul. Electromagnetic\nmodular smart surface architecture and control in a microfactory context. Computers in Indus-\ntry, 81:152\u2013170, 2016.\n[25] G. Pangaro, D. Maynes-Aminzade, and H. Ishii. The actuated workbench: computer-controlled\nactuation in tabletop tangible interfaces. In Proceedings of the 15th annual ACM symposium\non User interface software and technology, pages 181\u2013190, 2002.\n[26] S. Follmer, D. Leithinger, A. Olwal, A. Hogge, and H. Ishii. inform: dynamic physical affor-\ndances and constraints through shape and object actuation. In Uist, volume 13, pages 2501\u2013\n988. Citeseer, 2013.\n[27] D. Leithinger, S. Follmer, A. Olwal, and H. Ishii. Shape displays: Spatial interaction with\ndynamic physical form. IEEE computer graphics and applications, 35(5):5\u201311, 2015.\n[28] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.\nContinuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n[29] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. In International conference on machine\nlearning, pages 1861\u20131870. PMLR, 2018.\n[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[31] N. Ahmed, T. Natarajan, and K. R. Rao. Discrete cosine transform. IEEE transactions on\nComputers, 100(1):90\u201393, 1974.\n10\n[32] A. Ajay, J. Wu, N. Fazeli, M. Bauza, L. P. Kaelbling, J. B. Tenenbaum, and A. Rodriguez.\nAugmenting physical simulators with stochastic neural networks: Case study of planar pushing\nand bouncing. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 3066\u20133073. IEEE, 2018.\n[33] N. Fazeli, S. Zapolsky, E. Drumwright, and A. Rodriguez. Fundamental limitations in perfor-\nmance and interpretability of common planar rigid-body contact models. In Robotics Research:\nThe 18th International Symposium ISRR, pages 555\u2013571. Springer, 2020.\n[34] G. K. Wallace. The jpeg still picture compression standard. Communications of the ACM, 34\n(4):30\u201344, 1991.\n[35] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,\nA. Allshire, A. Handa, et al. Isaac gym: High performance gpu-based physics simulation for\nrobot learning. arXiv preprint arXiv:2108.10470, 2021.\n[36] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization\nfor transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ\ninternational conference on intelligent robots and systems (IROS), pages 23\u201330. IEEE, 2017.\n[37] D. Morrison, P. Corke, and J. Leitner. Egad! an evolved grasping analysis dataset for diversity\nand reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5(3):\n4368\u20134375, 2020.\n[38] K. Mamou, E. Lengyel, and A. Peters. Volumetric hierarchical approximate convex decompo-\nsition. In Game Engine Gems 3, pages 141\u2013158. AK Peters, 2016.\n[39] J. Luntz and H. Moon. Distributed manipulation with passive air flow. In Proceedings 2001\nIEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal\nRole of Robotics in the the Next Millennium (Cat. No. 01CH37180), volume 1, pages 195\u2013201.\nIEEE, 2001.\n[40] A. F. Siu, E. J. Gonzalez, S. Yuan, J. B. Ginsberg, and S. Follmer. Shapeshift: 2d spatial\nmanipulation and self-actuation of tabletop shape displays for tangible and haptic interaction.\nIn Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages\n1\u201313, 2018.\n[41] K. Xu, M. Qin, F. Sun, Y. Wang, Y.-K. Chen, and F. Ren. Learning in the frequency domain. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n1740\u20131749, 2020.\n[42] Y. Xu, K. Han, C. Xu, Y. Tang, C. Xu, and Y. Wang. Learning frequency domain approximation\nfor binary neural networks. Advances in Neural Information Processing Systems, 34:25553\u2013\n25565, 2021.\n[43] Y. Pan, J. Mei, and A.-m. Farahmand. Frequency-based search-control in dyna. arXiv preprint\narXiv:2002.05822, 2020.\n[44] A.-m. Farahmand. Value function in frequency domain and the characteristic value iteration\nalgorithm. Advances in Neural Information Processing Systems, 32, 2019.\n11\nA\nMore Details in Hardware Design\nA.1\nRobotic Frame Design\nThe robotic frame is shown in Figure 8. The grid cover on top is made of Acrylonitrile Butadiene\nStyrene plastic (ABS), using Computerized Numerical Control (CNC) for fabrication. The four\nsurrounding sides of the shell are made of acrylic. The motor brackets and column supports are\nmade of aluminum alloy, using CNC for production. The base is made of iron, which is welded and\npainted.\nFigure 8: The robotic frame with a grid cover, a transparent shell, and an iron base.\nA.2\nDesign and Fabrication of Pillars\nFigure 9: A module of two pillars controlled by a STM32 board.\nSilicone cap. To improve the transmission of force to the sensor, we develop a silicone cap with\na 30A hardness. This cap is produced through vacuuming and heat setting processes. The CNC\ntechnique is also employed for producing the mold of the silicone cap.\nTactile sensor. We employ the force-sensitive resistor (FSR) sensor model RXD1016, which has\na measuring range of 200 g, a resistance range of 0.5k-10k ohms, and an effective detection area\nof a circular shape with a diameter of 10 mm. The FSR sensor measures the normal force, and we\nconvert the sensor resistance to voltage through a voltage divider in series. We utilize the micro-\ncontroller\u2019s Analog-to-Digital Converter (ADC) port to read the values, with the perceived pressure\n12\nbeing directly proportional to the voltage. As the STM32F042 board\u2019s ADC can discern up to 0.8\nmV voltage, the theoretical trigger value of the sensor is 10.36 g. In order to mitigate the impact\nof components such as the silicone cap on assessing the contact condition, we conduct a calibration\nprior to usage. This process uses the no-load voltage as the new reference zero point to evaluate the\ncontact status.\nActuator. The DC gear motor is CHR-GM16-050ABHL, with a rated voltage of 12 V, a maximum\npower of 3.3 W, and a no-load speed of 800 rpm. The screw has a pitch of 1 mm and a lead of 4\nmm. When the motor completes one rotation, a single column can move up by 4 mm.\nMicrocontroller. We select the STM32F042 microcontroller due to its inclusion of an ADC inter-\nface, a CAN interface, and an ample number of pins. Given its acceptable level of performance and\nrelatively affordable price, this microcontroller is satisfactory for controlling ArrayBot.\nPower. The robot is powered by an adjustable DC voltage source, and the power supply outputs 12\nV and 5 V respectively to control the motor and the microcontroller.\nB\nMore details in RL training\nRL.\nOur policy and value functions are separate neural networks with hidden layers of sizes [256,\n256, 128]. All the inputs are normalized between 0 to 1 before feeding to the networks. We use\nReLU as the activation function. We also list the hyper-parameters of PPO [30] in Table 3. The\nstates and rewards are summarized in Table 1 and Table 2.\nTraining.\nThroughout the training phase, reinforcement learning is conducted on GPU, and the\nsimulation is performed on CPU. Acceleration of the simulation is facilitated by using a multi-core\nparallel method across 128 environments. The frequency for simulation is 50 Hz, while the control\nfrequency is 5 Hz. Training on an A40 graphics card, convergence is achieved after 10 minutes.\nTable 1: A summary of all the states used in all the tasks.\nParameter\nDefinition\nDimension\nlifting\np\nobject position in World Frame\n3\nflipping\n\u03b8\nobject orientation with axis-angle representation in world frame\n3\ngeneral relocation\npt\nobject position in world frame\n3\npg\nobject target position in world frame\n3\npd\nposition difference\n3\nq\nrobot joint configuration in the frequency domain\n6\nTable 2: A summary of reward functions used this various tasks.\nExpression\nDefinition\nCoefficient\nlifting\nht \u2212 ht\u22121\nobject height difference\n100\nflipping\n\u2212\n\r\r\u03b8t \u00b7 \u03b8\u22121\ng\n\r\r +\n\r\r\u03b8t\u22121 \u00b7 \u03b8\u22121\ng\n\r\r\nobject-to-goal orientation difference\n10\ngeneral relocation\n\u2212 \u2225pt \u2212 pg\u2225 + \u2225pt\u22121 \u2212 pg\u2225\nobject position difference\n100\n1{\u2225pt \u2212 pg\u2225 < \u03f5}, \u03f5 = 2cm\nGoal Reaching Indicator\n1\n13\nTable 3: Training Hyper-parameters\nHyperparameter\nValue\nDiscount factor\n0.99\nGAE parameter\n0.95\nLearning rate\n3e-4\nEnvironments\n128\nOptimizer\nAdam\nNormalize input\nTrue\nNormalize value\nTrue\nNormalize advantage\nTrue\nC\nMore Details in Real-world tasks\nWe show more tasks that are developed using RL on ArrayBot in Figure 10. We observe that our\nmethod can generalize well to various objects, even under heavy human perturbation or visual dis-\nturbance. Additionally, we also achieve several intriguing applications by using pre-coded motion\nprimitives on ArrayBot, as shown in Figure 11. The achieved behaviors show the potential of Ar-\nrayBot to accomplish diverse and harder tasks.\nFigure 10: Applications empowered by RL-derived policies.\n14\nFigure 11: Applications empowered by hard-coding policies.\n15\n"
  },
  {
    "title": "NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes",
    "link": "https://arxiv.org/pdf/2306.16869.pdf",
    "upvote": "5",
    "text": "NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural\nNetwork Inference in Low-Voltage Regimes\nHao-Lun Sun 1 Lei Hsiung 2 Nandhini Chandramoorthy 3 Pin-Yu Chen 3 Tsung-Yi Ho 4\nAbstract\nDeep neural networks (DNNs) have become\nubiquitous in machine learning, but their energy\nconsumption remains a notable issue. Lowering\nthe supply voltage is an effective strategy for\nreducing energy consumption. However, aggres-\nsively scaling down the supply voltage can lead\nto accuracy degradation due to random bit flips\nin static random access memory (SRAM) where\nmodel parameters are stored. To address this\nchallenge, we introduce NeuralFuse, a novel add-\non module that addresses the accuracy-energy\ntradeoff in low-voltage regimes by learning input\ntransformations to generate error-resistant data\nrepresentations. NeuralFuse protects DNN accu-\nracy in both nominal and low-voltage scenarios.\nMoreover, NeuralFuse is easy to implement and\ncan be readily applied to DNNs with limited\naccess, such as non-configurable hardware or\nremote access to cloud-based APIs. Experimental\nresults demonstrate that, at a 1% bit error rate,\nNeuralFuse can reduce SRAM memory access\nenergy by up to 24% while recovering accuracy\nby up to 57%. To the best of our knowledge,\nthis is the first model-agnostic approach (i.e.,\nno model retraining) to address low-voltage-\ninduced bit errors. The source code is available at\nhttps://github.com/IBM/NeuralFuse.\n1. Introduction\nEnergy-efficient computing is a primary consideration for\nthe deployment of Deep Neural Networks (DNNs), partic-\nularly on edge devices and on-chip AI systems. Increas-\ning energy efficiency and lowering the carbon footprint of\nDNN computation involves iterative efforts from both chip\ndesigners and algorithm developers. Processors with spe-\ncialized hardware accelerators for AI computing are now\n1Department of Computer Science, National Tsing Hua Uni-\nversity, Taiwan 2Dartmouth College, USA 3IBM Research, New\nYork, USA 4The Chinese University of Hong Kong, Hong Kong.\nPreprint. Copyright 2024 by the author(s).\nubiquitous, capable of providing orders of magnitude more\nperformance and energy efficiency for AI computation. In\naddition to reduced precision/quantization and architectural\noptimizations, low voltage operation is a powerful knob that\nimpacts power consumption. There is ample evidence in\ncomputer engineering literature that studies the effects of\nundervolting and low-voltage operation on accelerator mem-\nories that store weights and activations during computation.\nAggressively scaling down the SRAM (Static Random Ac-\ncess Memory) supply voltage below the rated value leads\nto an exponential increase in bit failures, but saves power\non account of the quadratic dependence of dynamic power\non voltage (Chandramoorthy et al., 2019; Ganapathy et al.,\n2017). Such memory bit flips in the stored weight and\nactivation values can cause catastrophic accuracy loss.\nA recent spate of works advocates low voltage operation of\nDNN accelerators using numerous techniques to preserve\naccuracy ranging from hardware-based error mitigation tech-\nniques (Chandramoorthy et al., 2019; Reagen et al., 2016)\nto error-aware robust training of DNN models (Kim et al.,\n2018; Koppula et al., 2019; Stutz et al., 2021). On-chip error\nmitigation methods have significant performance and power\noverheads. On the other hand, some have also proposed\nto generate models that are more robust to bit errors via a\nspecific learning algorithm (Kim et al., 2018; Koppula et al.,\n2019; Stutz et al., 2021), thereby eliminating the need for\non-chip error mitigation. However, error-aware training to\nfind the optimal set of robust parameters for each model is\ntime and energy-intensive and may not be possible in all\naccess-limited settings.\nIn this paper, we propose a novel model-agnostic approach:\nNeuralFuse. NeuralFuse is a machine learning module that\nallows for mitigating bit errors caused by very low voltage\noperation, through a trainable input transformation parame-\nterized by a relatively small DNN, to enhance the robustness\nof the original input and provide accurate inference. The\npipeline of NeuralFuse is illustrated in Figure 1. NeuralFuse\naccepts the scenarios under access-limited neural networks\n(e.g., non-configurable hardware or remote access to cloud-\nbased APIs) to protect the deployed models from making\nwrong predictions under low power. Specifically, we con-\nsider two practical access-limited scenarios: (a) Relaxed\n1\narXiv:2306.16869v2  [cs.LG]  21 Feb 2024\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nNeuralFuse\nNeuralFuse Disabled\nNominal Volt\n92.6%\n89.8%\n38.9%\n87.8%\nLow Volt\nBit Errors\nInput data\n\ud835\udc7f = {\ud835\udc651,\u2026,\ud835\udc65\ud835\udc5b}\nOn-chip Model Inference\n(a) The pipeline of the NeuralFuse framework at inference.\n18\n22\n26\n30\nEnergy Saving Percentage (%)\n50\n70\n90\nPerturbed Accuracy (%)\nAcc. 20.6%\nSav. 29.1%\nAcc. 48.8%\nSav. 19.0%\nAcc. 29.9%\nSav. 27.5%\nAcc. 49.6%\nSav. 21.2%\nAcc. 29.8%\nSav. 28.9%\nAcc. 45.6%\nSav. 24.0%\nResNet18\n(Base Model)\n1% B.E.R.\nEnergy-Accuracy Tradeoff of CIFAR-10 Pre-trained ResNet18\nNeuralFuse Architecture\nConvS\nConvL\nDeConvS\nDeConvL\nUNetS\nUNetL\nNeuralFuse Architecture\nConvS\nConvL\nDeConvS\nDeConvL\nUNetS\nUNetL\n(b) Energy-accuracy tradeoff example.\nFigure 1. (a) At inference, NerualFuse transforms input samples x into robust data representations. The nominal voltage allows models to\nwork as expected; however, the low-voltage one would encounter bit errors (e.g., 1%) and cause incorrect inference. The percentage\nillustrates the accuracy of a CIFAR-10 pre-trained ResNet18 with and without equipping NeuralFuse in both cases. (b) On the same\nbase model (ResNet18), we illustrate the energy-accuracy tradeoff of different NeuralFuse implementations. The X-axis represents the\npercentage reduction in dynamic memory access energy at low-voltage settings (base model protected by NeuralFuse), compared to the\nbit-error-free (nominal) voltage; the Y-axis represents the perturbed accuracy (evaluated at low voltage) with a 1% bit error rate.\nAccess, where the model details are unknown but backprop-\nagation through the black-box models is possible, and (b)\nRestricted Access, where models are unknown, and back-\npropagation is disallowed. For relaxed access, we train\nNeuralFuse via backpropagation, and for restricted access,\nwe train NeuralFuse on a white-box surrogate model and\ntransfer it to the restricted access models. To the best of our\nknowledge, this is the first study that leverages a learning-\nbased method to address random bit errors for recovering\naccuracy in low-voltage and access-limited settings.\nWe summarize our main contributions as follows:\n\u2022 We propose NeuralFuse, a novel learning-based input\ntransformation framework to enhance the accuracy of\nDNNs subject to random bit errors caused by undervolt-\ning. NeuralFuse is model-agnostic because it operates\nin a plug-and-play manner at the data input and does not\nrequire re-training the deployed DNN model.\n\u2022 We consider two practical access-limited scenarios for\nneural network inference: Relaxed Access and Restricted\nAccess. In the former setting, we use gradient-based meth-\nods to train the NeuralFuse module. In the latter setting,\nwe use a white-box surrogate model to train NeuralFuse\nand show its high transferability to other types of DNN\narchitectures.\n\u2022 We conduct extensive experiments on various combina-\ntions of DNN models (ResNet18, ResNet50, VGG11,\nVGG16, and VGG19), datasets (CIFAR-10, CIFAR-100,\nGTSRB, and ImageNet-10), and NeuralFuse implemen-\ntations with different architectures and sizes. The results\nshow that NeuralFuse can consistently increase the per-\nturbed accuracy (accuracy evaluated under random bit\nerrors in weights) by up to 57%, while simultaneously\nsaving the overall SRAM memory access energy by up\nto 24%, based on the realistic characterization of bit cell\nfailures for a given memory array in a low-voltage regime\ninducing a 0.5%/1% of bit error rate.\n\u2022 We demonstrate NeuralFuse\u2019s advantages in various sce-\nnarios. The experimental results show that NeuralFuse has\nhigh transferability (adaptability to unseen base models),\nversatility (capable of recovering low-precision quanti-\nzation loss, as well as bit errors), and competitiveness\n(achieving state-of-the-art performance).\n2. Related Work and Background\nSoftware-based Energy Saving Strategies.\nRecent stud-\nies have proposed reducing energy consumption from a\nsoftware perspective. For instance, quantization techniques\nhave been proposed to reduce the precision of storing model\nweights and decrease total memory storage (Gong et al.,\n2014; Rastegari et al., 2016; Wu et al., 2016). On the other\nhand, Yang et al. (2017) proposed energy-aware pruning on\neach layer and finetunes the weights to maximize the final\naccuracy. Yang et al. have also proposed several ways to\nreduce the energy consumption of DNN. For example, they\nproposed ECC, a DNN compression framework, that com-\npresses a DNN model to meet the given energy constraint\n(Yang et al., 2019a); they have also proposed to compress\nDNN models via joint pruning and quantization (Yang et al.,\n2020). Besides, it is also feasible to treat energy constraint\nas an optimization problem during DNN training to reduce\nenergy consumption and maximize training accuracy (Yang\net al., 2019b). However, these methods focus on changing\neither the model architectures or model weights to reduce\nenergy consumption, which is orthogonal to our approach\nwith NeuralFuse, which serves as an add-on module for any\ngiven model.\n2\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nFigure 2. Left: Bit error rate. Right: Dynamic energy per memory\naccess versus voltage for SRAM arrays in Chandramoorthy et al.\n(2019). The x-axis shows voltages normalized with respect to the\nminimum bit error-free voltage (Vmin).\nHardware-based Energy Saving Strategies.\nExisting\nworks have studied improving energy efficiency by design-\ning specific hardware. Several works have studied the un-\ndervolting of DNN accelerators and proposed methods to\nmaintain accuracy in the presence of bit errors. For instance,\nReagen et al. (2016) proposed an SRAM fault mitigation\ntechnique that rounds the faulty weights into zeros to avoid\nthe degradation of the prediction accuracy. Srinivasan et al.\n(2016) proposed to store the sensitive MSBs (most signifi-\ncant bits) in robust SRAM cells to preserve accuracy. Chan-\ndramoorthy et al. (2019) proposed dynamic supply voltage\nboosting to a higher voltage to improve the resilience of the\nmemory access operation. On the other hand, Stutz et al.\n(2021) considered a learning-based approach that tries to\nfind models that are robust to bit errors. The paper discusses\nseveral techniques to improve the robustness, such as quan-\ntization methods, weight clipping, random bit error training,\nand adversarial bit error training. The authors observe that\nthe combination of quantization, weight clipping, and ad-\nversarial bit error training achieves excellent performance\nin their experiments. However, they also admitted that the\ntraining process for that is sensitive to hyperparameter set-\ntings, and hence, it might come with a challenging training\nprocedure.\nWe argue that the methods mentioned above are not easy to\nimplement or not suitable for real-world scenarios in access-\nlimited settings. For example, the weights of DNN models\npacked on embedded systems may not be configurable or\nupdatable. Therefore, model retraining (e.g., Stutz et al.\n(2021)) is not a viable option in this scenario. Moreover, the\ntraining of DNNs is already a tedious and time-consuming\ntask. Adding error-aware training during training may fur-\nther increase the training complexity and introduce chal-\nlenges in hyperparameter search, as identified in previous\nliterature. \u00a8Ozdenizci & Legenstein (2022) also note that\nthis error-aware training was also found ineffective for large\nDNNs with millions of bits. Our proposed NerualFuse\nspares the need for model retraining by attaching a trainable\ninput transformation function parameterized by a relatively\nsmall DNN as an add-on module to any DNN model as is.\nSRAM Bit Errors in DNNs.\nLow-voltage-induced mem-\nory bit cell failures can cause bit flips from 0 to 1 and vice\nversa. In practice, SRAM memory bit errors increase ex-\nponentially when the supply voltage is scaled below Vmin,\nwhich is the minimum voltage required to avoid bit errors.\nThis phenomenon has been well studied in previous liter-\nature, such as the work by Chandramoorthy et al. (2019)\nand Ganapathy et al. (2017). As shown in Figure 2, for\nan SRAM array of size 512\u00d764 bits and 14nm technol-\nogy node, the number of bit errors increases as the voltage\nscales down. The corresponding dynamic energy per read\naccess of SRAM is shown on the right, measured at each\nvoltage at a constant frequency. In this example, accessing\nthe SRAM at 0.83Vmin leads to a 1% bit error rate, and at\nthe same time, the dynamic energy per access is reduced by\napproximately 30%. This can lead to inaccurate inferences\nin DNNs, particularly when bit flips occur at the MSBs.\nHowever, improving the robustness to bit errors allows us to\nlower the voltage and exploit the resulting energy savings.\nIt has been observed that bit cell failures for a given mem-\nory array are randomly distributed and independent of each\nother. That is, the spatial distribution of bit flips could be\nassumed random, as it is generally different between ar-\nrays, even within a chip and between different chips. In\nthis paper, we follow the methodology in Chandramoorthy\net al. (2019) and model bit errors in a memory array of a\ngiven size by generating a random distribution of bit errors\nwith equal likelihood of 0-to-1 and 1-to-0 bit flip. Specif-\nically, we assume that the model weights are quantized to\n8-bit precision (i.e., from 32-bit floats to 8-bit integers),\nand randomly distributed bit errors are injected into the\nquantized 2\u2019s complement representation of weights to gen-\nerate perturbed models. Please refer to Section 4.1 for more\nimplementation details.\n3. NeuralFuse: Framework and Algorithms\n3.1. Error-Resistant Input Transformation\nAs illustrated in Figure 1, to overcome the drawback of per-\nformance degradation in low-voltage regimes, we propose a\nnovel trainable input transformation module parametrized\nby a relatively small DNN, NeuralFuse, to mitigate the\naccuracy-energy tradeoff for model inference. The ratio-\nnale is to use a specially designed loss function and training\nscheme to derive the NeuralFuse and apply it to the input\ndata such that the transformed inputs will become robust to\nlow-voltage-induced bit errors.\nConsider the input x sampled from the data distribution\nX and a model Mp with random bit errors on its weights\n(which is called a perturbed model). When there are no\nbit errors (i.e., the normal-voltage settings), the perturbed\nmodel reduces to a nominal deterministic model denoted by\nM0. NeuralFuse aims to ensure the perturbed model Mp can\nmake correct inferences on the transformed inputs as well as\nretain consistent results of M0 in regular (normal-voltage)\n3\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nsettings. In order to adapt to different data characteristics,\nNeuralFuse (F) is designed to be input-aware, which can\nbe formally defined as:\nF(x) = clip[\u22121,1]\n\u0000x + G(x)\n\u0001\n,\n(1)\nwhere G(x) is a \u201cgenerator\u201d (i.e., an input transformation\nfunction) that can generate a perturbation based on the input\nx. The NeuralFuse transformed data F(x) will be passed to\nthe deployed model (M0 or Mp) for final inference. Without\nloss of generality, we assume the transformed input lies\nwithin a scaled input range F(\u00b7) \u2208 [\u22121, 1]d, where d is the\n(flattened) dimension of x.\n3.2. Training Objective and Optimizer\nTo train the generator G(\u00b7), which should ensure the correct-\nness of both the perturbed model Mp and the clean model\nM0, we parameterize G(\u00b7) by a neural network and design\nthe following training objective function:\narg max\nWG log PM0(y|F(x; WG))+\n\u03bb \u00b7 EMp\u223cMp[log PMp(y|F(x; WG))],\n(2)\nwhere WG is the set of trainable parameters for G, y is the\nground-truth label of x, PM denotes the likelihood of y com-\nputed by a model M given a transformed input F(x; WG),\nMp is the distribution of the perturbed models inherited\nfrom the clean model M0 under a p% random bit error rate,\nand \u03bb is a hyperparameter that balances the importance\nbetween the nominal and perturbed models.\nThe training objective function can be readily converted\nto a loss function (loss) that evaluates the cross-entropy\nbetween the ground-truth label y and the prediction\nPM(y|F(x; WG). That is, the total loss function becomes\nLossT otal = lossM0 + \u03bb \u00b7 lossMp.\n(3)\nTo optimize the loss function entailing the evaluation of\nthe loss term lossMp on randomly perturbed models, our\ntraining process is inspired by the EOT (Expectation Over\nTransformation) attacks (Athalye et al., 2018). EOT attacks\naim to find a robust adversarial example against a variety\nof image transformations. Based on the idea, we propose a\nnew optimizer for solving (3), which we call Expectation\nOver Perturbed Models (EOPM). EOPM-trained generators\ncan generate error-resistant input transformations and miti-\ngate the inherent bit errors. However, it is computationally\nimpossible to enumerate all possible perturbed models with\nrandom bit errors, and the number of realizations for per-\nturbed models is limited by the memory constraint of GPUs\nused for training. In practice, we only take N perturbed\nmodels for each iteration to calculate the empirical average\nloss, i.e.,\nLossN =\nlossMp1 + ... + lossMpN\nN\n,\n(4)\nwhere N is the number of simulated perturbed models\n{Mp1, . . . , MpN } under random bit errors to calculate the\nloss. Therefore, the gradient used to update the generator\ncan be calculated as follows:\n\u2202LossT otal\n\u2202WG\n=\n\u2202lossM0\n\u2202WG\n+ \u03bb\nN\n\u0010 \u2202lossMp1\n\u2202WG\n+ \u00b7 \u00b7 \u00b7 +\n\u2202lossMpN\n\u2202WG\n\u0011\n.\n(5)\nIn our implementation, we find that N = 10 can already\ndeliver stable performance, and there is little gain in using\na larger value. The ablation study for different N can be\nfound in Appendix E.\n3.3. Training Algorithm\nAlgorithm 1 in Appendix A summarizes the training steps\nof NeuralFuse. We split the training data X into B mini-\nbatches for training the generator in each epoch. For each\nmini-batch, we first feed these data into F(\u00b7) to get the\ntransformed inputs. Also, we simulate N perturbed models\nusing a p% random bit error rate, denoted by Mp1, ..., MpN ,\nfrom Mp. Then, the transformed inputs are fed into these\nN perturbed models and the clean model M0 to calculate\ntheir losses and gradients. Finally, NeuralFuse parameters\nWG are updated based on the gradient obtained by EOPM.\n4. Experiments\nIn this section, we present the experimental setup and results\nof NeuralFuse on different datasets and various architectures.\nIn addition, we also provide the visualization results, de-\ntailed analysis, and ablation studies to better understand the\nproperties of NeuralFuse.\n4.1. Experiment Setups\nDatasets.\nWe evaluate NeuralFuse on four different\ndatasets: CIFAR-10 (Krizhevsky & Hinton, 2009), CIFAR-\n100 (Krizhevsky & Hinton, 2009), GTSRB (Stallkamp et al.,\n2012), and ImageNet-10 (Deng et al., 2009). CIFAR-10 con-\nsists of ten classes, with 50,000 training images and 10,000\nfor testing. Similarly, CIFAR-100 consists of 100 classes,\nwith 500 training images and 100 testing images in each\nclass. GTSRB (German Traffic Sign Recognition Bench-\nmark) is a dataset that contains 43 classes with 39,209 train-\ning images and 12,630 testing images. Similar to CIFAR-10\nand CIFAR-100, we resize GTSRB into 32\u00d732\u00d73 in our\nexperiment. For ImageNet-10, we chose the same ten cat-\negories as Huang et al. (2022), and there are 13,000 train-\ning images and 500 test images, which are cropped into\n224\u00d7224\u00d73. Due to the space limit, the CIFAR-100 results\nare given in Appendix F and G.\nBase Models.\nWe select several common architectures for\nour base models: ResNet18, ResNet50 (He et al., 2016),\nVGG11, VGG16, and VGG19 (Simonyan & Zisserman,\n4\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\n2015). In order to meet the setting of deploying models on\na chip, all of our based models are trained by quantization-\naware training following Stutz et al. (2021).\nNeuralFuse Generators.\nThe architecture of the Neu-\nralFuse generator (G) is based on the Encoder-Decoder\nstructure.\nWe design and compare three types of gen-\nerators, namely Convolution-based, Deconvolution-based,\nand UNet-based. We also consider large(L)/small(S) net-\nwork sizes for each type. Both Convolution-based and\nDeconvolution-based variants follow a similar architecture\nfor ease of comparison. Also, the generators were trained\nwith quantization-aware training. More details are given in\nAppendix B.\n\u2022 Convolution-based (Conv).\nConv uses Convolution\nwith MaxPool layers for the encoder and Convolution\nwith UpSample layers for the decoder. This architecture\nhas shown its efficiency and effectiveness for generating\ninput-aware backdoor triggers (Nguyen & Tran, 2020).\n\u2022 Deconvolution-based (DeConv).\nDeConv uses Con-\nvolution with MaxPool layers for the encoder and De-\nconvolution layers for the decoder. This modification is\nexpected to enhance its performance and reduce energy\nconsumption.\n\u2022 UNet-based (UNet). UNet uses Convolution with Max-\nPool layers for the encoder and Deconvolution layers for\nthe decoder. UNet is known for its robust performance in\nimage segmentation (Ronneberger et al., 2015), and thus,\nwe incorporated it as one of our implementations.\nEnergy Consumption Calculation.\nIn this paper, we fo-\ncus on the resilience to low-voltage-induced bit errors in\nmodel weights, and our reported energy consumption in Fig-\nure 1 is based on the product of the total number of SRAM\nmemory accesses in a systolic-array-based CNN accelerator\nand the dynamic energy per read access at a given voltage.\nResearch by Chen et al. (2016) also shows that the energy\nconsumption of the SRAM part (buffer and array) accounts\nfor a large part of the total system energy consumption. We\nassume that there is no bit error on NeuralFuse, as we con-\nsider it as an add-on data preprocessing module that could\nalso be performed by the general-purpose core. Therefore,\nwhen implemented in the accelerator, NeuralFuse compu-\ntation is performed at a higher error-free voltage with a dy-\nnamically scaled voltage supply to the memories. We report\nthe reduction in overall weight memory energy consumption\n(i.e., NeuralFuse + Base Model under p% bit error rate) with\nrespect to the unprotected base model in the regular voltage\nmode (i.e., 0% bit error rate and without NeuralFuse).\nTo obtain the number of memory accesses, we used the\nSCALE-SIM simulator (Samajdar et al., 2020), and our cho-\nsen configuration simulates an output-stationary dataflow\nand a 32\u00d732 systolic array with 256KB of weight mem-\nory. We obtained the dynamic energy per read access of the\nSRAM at the minimum voltage (Vmin) and at the voltage\ncorresponding to a 1% bit error rate (Vber \u2248 0.83Vmin)\nfrom Cadence ADE Spectre simulations, both at the same\nclock frequency.\nRelaxed and Restricted Access Settings.\nOur experi-\nments consider two scenarios (relaxed/restricted access).\nFor relaxed access, the information of the base model is not\nentirely transparent but it allows obtaining gradients from\nthe black-box model through backpropagation. Therefore,\nthis setting allows direct training of NeuralFuse with the\nbase model using EOPM. On the other hand, for restricted\naccess, only the inference function is allowed for the base\nmodel. Therefore, we train NeuralFuse by using a white-box\nsurrogate base model and then transfer the generator to the\naccess-restricted model.\nComputing Resources.\nOur experiments are performed\nusing 8 Nvidia Tesla V100 GPUs and are implemented\nwith PyTorch. NeuralFuse generally takes 150 epochs to\nconverge, and its training time is similar for the same base\nmodel. On both CIFAR-10 and CIFAR-100 datasets, the\naverage training times were 17 hours (ResNet18), 50 hours\n(ResNet50), 9 hours (VGG11), 13 hours (VGG16), and 15\nhours (VGG19). For GTSRB, the average training times\nwere 9 hours (ResNet18), 27 hours (ResNet50), 5 hours\n(VGG11), 7 hours (VGG16), and 8 hours (VGG19). For\nImageNet-10, the average training times were 32 hours\n(ResNet18), 54 hours (ResNet50), 50 hours (VGG11), 90\nhours (VGG16), and 102 hours (VGG19).\n4.2. Performance Evaluation on Relaxed Access Setting\nThe experimental results of the Relaxed Access setting are\nshown in Figure 3. We train and test NeuralFuse with vari-\nous base models (ResNet18, ResNet50, VGG11, VGG16,\nand VGG19). Two power settings have been considered:\nnominal voltage (no bit error) and low voltage (random\nbit errors), and the corresponding bit error rate (B.E.R.)\ndue to low voltage is 1% (CIFAR-10, GTSRB) and 0.5%\n(ImageNet-10). The B.E.R. of ImageNet-10 is lower be-\ncause the pre-trained models have more parameters than\nCIFAR-10 and GTSRB. For each experiment, we sample\nN = 10 perturbed models (independent from training) for\nevaluation and report the mean and standard deviation of\nthe test accuracy. In the following, clean accuracy (C.A.)\nmeans that the model is measured at nominal voltage, and\nperturbed accuracy (P.A.) means that it is measured at low\nvoltage.\nFor CIFAR-10 and GTSRB, we observe that large gener-\nators like ConvL and UNetL can significantly recover the\nperturbed accuracy in the range of 41% to 63% on ResNet18,\n5\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nNominal (w/o NeuralFuse)\nLow voltage (w/o NeuralFuse)\nNominal (w/ NeuralFuse)\nLow voltage (w/ NeuralFuse)\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.6\nAccuracy (%)\n38  \n     .9\n89  \n88  \n89  \n82  \n86  \n84  \n     .8\n     .2\n     .6\n     .9\n     .6\n     .4\n87  \n59  \n88  \n68  \n84  \n68  \n     .8\n     .5\n     .5\n     .8\n     .6\n     .8\nCIFAR-10 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.6\n26  \n     .1\n85  \n85  \n87  \n82  \n86  \n77  \n     .5\n     .2\n     .4\n     .4\n     .2\n     .3\n53  \n34  \n63  \n42  \n75  \n56  \n     .2\n     .6\n     .3\n     .2\n     .5\n     .2\nCIFAR-10 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n88.4\n42  \n     .2\n89  \n84  \n89  \n85  \n87  \n85  \n     .6\n     .9\n     .3\n     .6\n     .1\n     .5\n87  \n66  \n87  \n68  \n83  \n72  \n     .2\n     .3\n     .2\n     .2\n     .6\n     .7\nCIFAR-10 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n90.3\n35  \n     .7\n90  \n87  \n89  \n86  \n87  \n87  \n     .1\n     .4\n     .7\n     .8\n     .4\n     .4\n86  \n59  \n85  \n66  \n83  \n71  \n     .0\n     .6\n     .5\n     .5\n     .4\n     .2\nCIFAR-10 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n90.5\n36  \n     .0\n89  \n87  \n86  \n86  \n86  \n86  \n     .8\n     .3\n     .3\n     .5\n     .3\n     .3\n77  \n52  \n78  \n58  \n82  \n66  \n     .7\n     .7\n     .4\n     .2\n     .1\n     .4\nCIFAR-10 pre-trained VGG19\n(a) CIFAR-10, 1% Bit Error Rate\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.5\nAccuracy (%)\n36  \n     .9\n95  \n94  \n95  \n95  \n96  \n95  \n     .7\n     .4\n     .6\n     .7\n     .2\n     .9\n91  \n68  \n91  \n78  \n93  \n85  \n     .1\n     .6\n     .3\n     .1\n     .8\n     .1\nGTSRB pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.0\n29  \n     .5\n95  \n94  \n94  \n93  \n94  \n94  \n     .6\n     .8\n     .9\n     .0\n     .5\n     .7\n71  \n50  \n71  \n56  \n80  \n64  \n     .6\n     .5\n     .6\n     .4\n     .6\n     .7\nGTSRB pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n91.9\n34  \n     .9\n94  \n91  \n95  \n92  \n92  \n94  \n     .8\n     .1\n     .0\n     .4\n     .2\n     .7\n85  \n62  \n84  \n67  \n83  \n73  \n     .7\n     .2\n     .6\n     .5\n     .2\n     .4\nGTSRB pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.2\n15  \n     .1\n96  \n94  \n96  \n93  \n95  \n94  \n     .3\n     .1\n     .4\n     .8\n     .8\n     .3\n72  \n39  \n72  \n50  \n78  \n63  \n     .4\n     .8\n     .0\n     .9\n     .6\n     .3\nGTSRB pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.5\n36  \n     .6\n96  \n93  \n95  \n94  \n95  \n94  \n     .0\n     .8\n     .4\n     .5\n     .4\n     .6\n88  \n69  \n87  \n73  \n88  \n80  \n     .3\n     .0\n     .2\n     .1\n     .2\n     .6\nGTSRB pre-trained VGG19\n(b) GTSRB, 1% Bit Error Rate\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.2\nAccuracy (%)\n72  \n     .3\n94  \n91  \n94  \n92  \n94  \n93  \n     .0\n     .8\n     .0\n     .8\n     .0\n     .2\n88  \n83  \n89  \n87  \n88  \n86  \n     .0\n     .6\n     .2\n     .5\n     .1\n     .4\nImageNet-10 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n89.8\n39  \n     .4\n92  \n91  \n93  \n93  \n92  \n92  \n     .2\n     .8\n     .0\n     .2\n     .2\n     .4\n80  \n65  \n79  \n70  \n80  \n73  \n     .0\n     .0\n     .4\n     .9\n     .5\n     .6\nImageNet-10 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n91.6\n47  \n     .8\n92  \n89  \n91  \n89  \n92  \n86  \n     .0\n     .4\n     .0\n     .0\n     .4\n     .2\n86  \n66  \n86  \n72  \n83  \n73  \n     .1\n     .4\n     .0\n     .5\n     .0\n     .5\nImageNet-10 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n94.6\n38  \n     .4\n90  \n90  \n91  \n90  \n90  \n86  \n     .8\n     .2\n     .2\n     .0\n     .6\n     .4\n77  \n60  \n77  \n62  \n81  \n72  \n     .1\n     .2\n     .2\n     .3\n     .1\n     .3\nImageNet-10 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.4\n37  \n     .2\n91  \n88  \n91  \n88  \n89  \n87  \n     .4\n     .8\n     .0\n     .8\n     .4\n     .6\n75  \n56  \n75  \n64  \n77  \n65  \n     .5\n     .5\n     .9\n     .0\n     .9\n     .9\nImageNet-10 pre-trained VGG19\n(c) ImageNet-10, 0.5% Bit Error Rate\nFigure 3. Relaxed Access setting: Test accuracy (%) of different pre-trained models with or without NeuralFuse, compared at nominal\nvoltage (0% bit error rate) or low voltage (with specified bit error rates). The results demonstrate that NeuralFuse consistently recovers\nperturbation accuracy.\nVGG11, VGG16, and VGG19. For ResNet50, the recover\npercentage is slightly worse than other base models, but it\ncan attain up to 51% recover percentage on GTSRB. On the\nother hand, the recover percentage based on small genera-\ntors like DeConvS are worse than larger generators. This can\nbe explained by the better ability to learn error-resistant gen-\nerators for larger-sized networks (though they may consume\nmore energy). For ImageNet-10, using larger generators can\nalso gain better performance recovery on perturbed accu-\nracy. This also demonstrates that NeuralFuse can still work\nwell even with large input sizes and is applicable to different\ndatasets.\n4.3. Transferability for Restricted Access Setting\nIn the Restricted Access scenario, we train NeuralFuse gen-\nerators on a white-box surrogate base model and transfer it\nto other black-box base models. The experimental results\nare shown in Table 1. We adopt ResNet18 and VGG19 as\nthe white-box surrogate (source) models for training the gen-\nerators under a 1.5% bit error rate. For the generators, we\nchoose ConvL and UNetL as they obtain better performance\nin Figure 3.\nFrom Table 1, we can find that transferring from a larger\nB.E.R. like 1.5% can give strong resilience to a smaller\nB.E.R. like 1% or 0.5%. We also find that using VGG19 as\na surrogate model with UNet-based generators like UNetL\ncan give better recovery than other combinations. On the\nother hand, in some cases, we observe that if we transfer\nbetween the same source and target models (but with differ-\nent B.E.R. for training and testing), the performance may\noutperform the original relaxed-access results. For instance,\nwhen transferring VGG19 with UNetL under a 1.5% B.E.R.\nto VGG19 or VGG11 under a 0.5% B.E.R., the accuracy is\n85.86% v.s. 84.99% for VGG19 (original), and 84.81% v.s.\n82.42% for VGG11 (original), respectively. We conjecture\nthat the generators trained on a larger B.E.R. can actually\ncover the error patterns of a smaller B.E.R., and even help\nimprove generalization under a smaller B.E.R. These find-\nings show great promise for recovering the accuracy of\naccess-limited base models in low-voltage settings.\n4.4. Energy-Accuracy Tradeoff\nWe report the total dynamic energy consumption as the total\nnumber of SRAM accesses times the dynamic energy of a\nsingle SRAM access. Specifically, we used SCALE-SIM\nto calculate the total weight memory access (T.W.M.A.),\nwhich can be found in Table 6 in Appendix C. In Table 2,\nwe report the percentage of energy savings at voltages that\n6\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nTable 1. Restricted Access setting: Transfer results trained with 1.5% bit error rate on CIFAR-10.\nS.M.\nT.M.\nB.E.R.\nC.A.\nP.A.\nConvL (1.5%)\nUNetL (1.5%)\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nResNet18\n1%\n92.6\n38.9 \u00b1 12.4\n89.8\n89.0 \u00b1 0.5\n50.1\n85.8\n85.2 \u00b1 0.5\n46.3\n0.5%\n70.1 \u00b1 11.6\n89.6 \u00b1 0.2\n19.5\n85.7 \u00b1 0.2\n15.6\nResNet50\n1%\n92.6\n26.1 \u00b1 9.4\n89.2\n36.1 \u00b1 18\n10.0\n84.4\n38.9 \u00b1 16\n12.8\n0.5%\n61.0 \u00b1 10.3\n74.1 \u00b1 10\n13.1\n72.7 \u00b1 4.6\n11.7\nVGG11\n1%\n88.4\n42.2 \u00b1 11.6\n86.3\n59.2 \u00b1 10\n17.0\n82.3\n69.8 \u00b1 7.5\n27.6\n0.5%\n63.6 \u00b1 9.3\n78.9 \u00b1 4.9\n15.3\n77.0 \u00b1 4.0\n13.4\nVGG16\n1%\n90.3\n35.7 \u00b1 7.9\n89.4\n62.2 \u00b1 18\n26.5\n84.7\n68.9 \u00b1 14\n33.2\n0.5%\n66.6 \u00b1 8.1\n83.4 \u00b1 5.5\n16.8\n80.5 \u00b1 5.9\n13.9\nVGG19\n1%\n90.5\n36.0 \u00b1 12.0\n89.8\n49.9 \u00b1 23\n13.9\n85.0\n55.1 \u00b1 17\n19.1\n0.5%\n64.2 \u00b1 12.4\n81.8 \u00b1 8.5\n17.6\n78.5 \u00b1 6.8\n14.3\nVGG19\nResNet18\n1%\n92.6\n38.9 \u00b1 12.4\n88.9\n62.6 \u00b1 13\n23.7\n85.0\n72.3 \u00b1 11\n33.4\n0.5%\n70.1 \u00b1 11.6\n84.2 \u00b1 7.2\n14.1\n82.1 \u00b1 2.2\n12.0\nResNet50\n1%\n92.6\n26.1 \u00b1 9.4\n88.8\n37.9 \u00b1 18\n11.8\n85.2\n46.7 \u00b1 17\n20.6\n0.5%\n61.0 \u00b1 10.3\n76.6 \u00b1 7.8\n15.6\n78.3 \u00b1 3.7\n17.3\nVGG11\n1%\n88.4\n42.2 \u00b1 11.6\n88.9\n76.0 \u00b1 6.1\n33.8\n85.5\n81.9 \u00b1 3.9\n39.7\n0.5%\n63.6 \u00b1 9.3\n85.9 \u00b1 2.6\n22.3\n84.8 \u00b1 0.5\n21.2\nVGG16\n1%\n90.3\n35.7 \u00b1 7.9\n89.0\n76.5 \u00b1 9.0\n40.8\n85.9\n79.2 \u00b1 7.5\n43.5\n0.5%\n66.6 \u00b1 8.1\n87.7 \u00b1 0.7\n21.1\n84.7 \u00b1 0.9\n18.1\nVGG19\n1%\n90.5\n36.0 \u00b1 12.0\n89.1\n80.2 \u00b1 12\n44.2\n86.3\n84.3 \u00b1 1.2\n48.3\n0.5%\n64.2 \u00b1 12.4\n88.8 \u00b1 0.4\n24.6\n85.9 \u00b1 0.3\n21.7\n[Note] S.M.: source model, used for training generators, T.M.: target model, used for testing generators, B.E.R.: the bit error rate of the target\nmodel, C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nTable 2. The energy saving percentage (%) for different combina-\ntions of base models and NeuralFuse.\nBase Model ConvL ConvS DeConvL DeConvS UNetL UNetS\nResNet18\n19.0\n29.1\n21.2\n27.5\n24.0\n28.9\nResNet50\n25.4\n29.9\n26.4\n29.2\n27.7\n29.9\nVGG11\n6.6\n27.5\n11.2\n24.1\n17.1\n27.2\nVGG16\n17.1\n28.9\n19.7\n27.0\n23.0\n28.7\nVGG19\n20.3\n29.7\n22.3\n27.8\n24.8\n29.1\nwould yield a 1% bit error rate for various base model and\ngenerator combinations. The formula of the energy saving\npercentage (ES, %) is defined as:\nES =\nEnergyNV\u2212\n\u0000EnergyLV+EnergyNeuralFuse at NV\n\u0001\nEnergyNV\n\u00d7 100%,\n(6)\nwhere NV denotes nominal voltage, and LV denotes low\nvoltage regime.\nThe results demonstrate that when utilizing ResNet18 as\na base model, NeuralFuse can recover model accuracy by\n20% \u223c 49% while saving 19% \u223c 29% energy. In Appendix\nC, we provided more results on the tradeoff between energy\nand accuracy of different NeuralFuse and Base Model com-\nbinations. Overall, our results show promising results that\nusing NeuralFuse can effectively restore model accuracy\nwhen the SRAM encounters low-voltage-induced random\nbit errors.\nRuntime and Latency.\nOn the other hand, runtime and its\ncorresponding energy consumption may also affect the over-\nall energy saving. For instance, previous research has shown\nthat Multiply-and-accumulate (MAC) operations account for\nover 99% of total operations in state-of-the-art CNNs, dom-\ninating both processing runtime and energy consumption\n(Yang et al., 2017). Therefore, we also report the results of\nMACs-based energy consumption estimation in Appendix\nC, and latency report in Appendix D. We want to note that\nthe additional latency overhead is an inevitable tradeoff for\nreducing energy consumption in our setting. Although run-\ntime and latency are not the major focus of this paper, we\nenvision future research designing a more lightweight ver-\nsion of the NeuralFuse module or using model compression\ntechniques on NeuralFuse to reduce latency and runtime.\n4.5. Model Size and Efficiency of NeuralFuse\nTo provide a full performance characterization of Neural-\nFuse, we analyze the relationship between the final recovery\nof each base model and generators of varying parameter\ncounts. The efficiency ratio is defined as the recover per-\ncentage in perturbed accuracy divided by the parameter\ncount of NeuralFuse. We compare the efficiency ratio for\nall NeuralFuse generators (trained on CIFAR-10) in Table\n3. The results show that UNet-based generators have better\nefficiency than both Convolution-based and Deconvolution-\nbased ones per million parameters.\n7\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nTable 3. The efficiency ratio (%) for all NeuralFuse generators.\nBase\nModel\nB.E.R.\nNeuralFuse\nConvL ConvS DeConvL DeConvS UNetL UNetS\nResNet18\n1%\n67.5\n182\n76.6\n190.7\n94.5\n245.9\n0.5%\n24.7\n73.3\n30.7\n62.5\n33.6\n88.3\nResNet50\n1%\n37.4\n75.1\n57.4\n102.7\n102.3\n248.4\n0.5%\n35.2\n108.7\n40.4\n92.5\n47.4\n124.6\nVGG11\n1%\n62.3\n212.9\n69.5\n165.8\n92.0\n251.7\n0.5%\n32.3\n96.3\n35.8\n77.2\n38.9\n100.7\nVGG16\n1%\n69.6\n211.2\n76.9\n196.5\n98.8\n292.9\n0.5%\n30.3\n98.1\n33.0\n75.3\n40.6\n113\nVGG19\n1%\n57.6\n147.5\n65.5\n141.6\n95.4\n250.8\n0.5%\n33.0\n91.0\n37.5\n70.2\n43.1\n106.4\nOriginal Accuracy\nw/o NeuralFuse\nNeuralFuse (ConvL)\nNeuralFuse (UNetL)\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\n95.5\nAccuracy (%)\n 95   \n 95   \n 95   \n 95   \n 92   \n 67   \n     .5\n     .5\n     .4\n     .4\n     .6\n     .7\n 93   \n 93   \n 93   \n 93   \n 93   \n 92   \n     .4\n     .3\n     .3\n     .3\n     .3\n     .0\n 96   \n 96   \n 96   \n 96   \n 96   \n 95   \n     .2\n     .2\n     .2\n     .2\n     .1\n     .7\n3  \n   .8 3  \n   .8 3  \n   .8\nGTSRB pre-trained ResNet18 (Nominal voltage, No Bit Error)\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\nAccuracy (%)\n 75   \n 69   \n 67   \n 48   \n 24   \n5  \n     .2\n     .5\n     .2\n     .6\n     .6\n   .3\n 89   \n 89   \n 88   \n 79   \n 52   \n9  \n     .5\n     .9\n     .9\n     .3\n     .1\n   .7\n 93   \n 94   \n 93   \n 87   \n 62   \n 12   \n     .5\n     .5\n     .5\n     .5\n     .2\n     .6\n3  \n   .8 3  \n   .8 3  \n   .8\nGTSRB pre-trained ResNet18 (Low voltage, 0.5% B.E.R.)\nFigure 4. Reduced-precision accuracy\n4.6. NeuralFuse on Reduced-precision Quantization\nHere we explore the robustness of NeuralFuse to low-\nprecision quantization on model weights. Uniform quanti-\nzation is the de-facto method for quantizing model weights\n(Gholami et al., 2022). However, it is possible to cause an\naccuracy drop due to precision loss. As a bit-error-oriented\nprotector, we seek to understand whether NeuralFuse could\nalso make a recovery to mitigate the model\u2019s accuracy drop\nin this scope. We conducted an experiment that uniformly\nquantized the model weights to a lower bit precision and\nreported the resulting accuracy. Specifically, we apply sym-\nmetric uniform quantization on our base models with differ-\nent numbers of bits to induce precision loss. Therefore, the\nquantized weight Wq (integer) is defined as Wq = \u230a W\ns \u2309,\nwhere W denotes the original model weight (full precision),\ns = max |W|\n2b\u22121\u22121 denotes the quantization scale parameter, and\nb is the precision (number of bits) that used to quantize\nthe models. In addition, we considered the possibility of\nrandom bit errors (the low-voltage setup) in low-precision\nregimes.\nWe use GTSRB pre-trained ResNet18 as an example to\nevaluate two NeuralFuse generators (ConvL and UNetL,\ntrained with 0.5% B.E.R.), and vary the precision b from\n8 bits to 2 bits (integer). The result is shown in Figure\n4. We find that when b > 3 bits, NeuralFuse can take\neffect to recover the accuracy in both scenarios. When\nb = 3, while NeuralFuse can still handle the bit-error-free\nmodel (top panel), it exhibits a limited ability to recover the\nrandom bit-error case (bottom panel). We find the result\nencouraging because the observed robustness to reduced-\nprecision inference is an emergent ability of NeuralFuse.\nThat is, NeuralFuse was only trained with random bit errors,\nbut it demonstrated high accuracy to unseen bit quantization\nerrors. This experiment showcases NeuralFuse\u2019s potential\nin protecting against accuracy drops caused by different\nsources of bit errors. More experimental results of different\nbase models and datasets can be found in Appendix H.\n4.7. Extended Analysis\nWe highlight some key findings from the additional results\nin Appendix. In Appendix E, we compare NeuralFuse to the\nsimple baseline of learning a universal input perturbation.\nWe find that the baseline is much worse than NeuralFuse,\nwhich validates the necessity of adopting input-aware trans-\nformation for learning error-resistant data representations\nin low-voltage scenarios. In Appendix G, we find that en-\nsemble training of white-box surrogate base models can\nfurther improve the transferability of NeuralFuse in the\nrestricted-access setting. In Appendix K and Appendix L,\nwe present visualization results of data embeddings and\ntransformed inputs via NeuralFuse. In Appendix J, we show\nthat NeuralFuse can further recover the accuracy of a base\nmodel trained with adversarial weight perturbation in the\nlow-voltage setting.\n5. Conclusion\nIn this paper, we propose NeuralFuse, the first non-intrusive\npost-hoc protection module for model inference against bit\nerrors induced by low voltage. NeuralFuse is particularly\nsuited for practical machine deployment settings where ac-\ncess to the base model is limited or relaxed. The design\nof NeuralFuse includes a novel loss function and a new\noptimizer named EOPM to handle simulated randomness\nin perturbed models. Our comprehensive experimental re-\nsults and analysis show that NeuralFuse can significantly\nrecover test accuracy (by up to 57%) while simultaneously\nenjoying up to 24% reduction in memory access energy.\nFurthermore, NeuralFuse demonstrates high transferability\n(to access-constrained models), and versatility (e.g., robust-\nness to low-precision quantization). Our results show that\nNeuralFuse provides significant improvements in mitigating\nthe energy-accuracy tradeoff of neural network inference\nin low-voltage regimes and sheds new insights on green AI\ntechnology. Our future work includes extending our study\nto other neural network architectures and modalities, such\nas transformer-based language models.\n8\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nImpact Statements\nWe see no immediate ethical or negative societal conse-\nquences of our work, and it holds the potential for positive\nsocial impacts. Previous works have highlighted scenarios\naffected by energy concerns, necessitating strategies to re-\nduce energy consumption (Yang et al., 2017; 2019a). By\nenhancing the accuracy of models in low-voltage regimes,\nNeuralFuse can function as an effective protector and an ef-\nficient energy saver. This technology should be particularly\nvaluable for extending the battery life of devices reliant on\nbatteries.\nReferences\nAthalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Syn-\nthesizing robust adversarial examples. In International\nConference on Machine Learning (ICML), pp. 284\u2013293.\nPMLR, 2018.\nBejnordi, B. E., Blankevoort, T., and Welling, M. Batch-\nshaping for learning conditional channel gated networks.\nIn International Conference on Learning Representations\n(ICLR), 2020.\nChandramoorthy, N., Swaminathan, K., Cochet, M., Paidi-\nmarri, A., Eldridge, S., Joshi, R. V., Ziegler, M. M.,\nBuyuktosunoglu, A., and Bose, P. Resilient low volt-\nage accelerators for high energy efficiency.\nIn 2019\nIEEE International Symposium on High Performance\nComputer Architecture (HPCA), pp. 147\u2013158, 2019. doi:\n10.1109/HPCA.2019.00034.\nChen, Y.-H., Emer, J., and Sze, V. Eyeriss: A spatial ar-\nchitecture for energy-efficient dataflow for convolutional\nneural networks. ACM SIGARCH computer architecture\nnews, 44(3):367\u2013379, 2016.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\nFei-Fei, L. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 248\u2013255, 2009. doi:\n10.1109/CVPR.2009.5206848.\nGanapathy, S., Kalamatianos, J., Kasprak, K., and Raasch,\nS.\nOn characterizing near-threshold sram failures in\nfinfet technology. In 2017 54th ACM/EDAC/IEEE De-\nsign Automation Conference (DAC), pp. 1\u20136, 2017. doi:\n10.1145/3061639.3062292.\nGholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,\nand Keutzer, K. A survey of quantization methods for effi-\ncient neural network inference. In Low-Power Computer\nVision, pp. 291\u2013326. Chapman and Hall/CRC, 2022.\nGong, Y., Liu, L., Yang, M., and Bourdev, L. Compressing\ndeep convolutional networks using vector quantization.\narXiv preprint arXiv:1412.6115, 2014.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp.\n770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.\nHuang, Z., Chen, J., Zhang, J., and Shan, H. Learning\nrepresentation for clustering via prototype scattering and\npositive sampling. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, pp. 1\u201316, 2022. doi: 10.1109/\nTPAMI.2022.3216454.\nKim, S., Howe, P., Moreau, T., Alaghi, A., Ceze, L., and\nSathe, V. Matic: Learning around errors for efficient\nlow-voltage neural network accelerators. In 2018 Design,\nAutomation & Test in Europe Conference & Exhibition\n(DATE), pp. 1\u20136. IEEE, 2018.\nKoppula, S., Orosa, L., Ya\u02d8gl\u0131kc\u00b8\u0131, A. G., Azizi, R., Shahroodi,\nT., Kanellopoulos, K., and Mutlu, O. Eden: Enabling\nenergy-efficient, high-performance deep neural network\ninference using approximate dram. In Proceedings of\nthe 52nd Annual IEEE/ACM International Symposium on\nMicroarchitecture, pp. 166\u2013181, 2019.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. Technical report, University\nof Toronto, Toronto, Ontario, 2009.\nMoosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and\nFrossard, P. Universal adversarial perturbations. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 1765\u20131773, 2017.\nNguyen, T. A. and Tran, A. Input-aware dynamic back-\ndoor attack. Advances in Neural Information Processing\nSystems (NeurIPS), 33:3454\u20133464, 2020.\n\u00a8Ozdenizci, O. and Legenstein, R. Improving robustness\nagainst stealthy weight bit-flip attacks by output code\nmatching. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 13388\u2013\n13397, 2022.\nRastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.\nXnor-net: Imagenet classification using binary convo-\nlutional neural networks. In European Conference on\nComputer Vision (ECCV), pp. 525\u2013542. Springer, 2016.\nReagen, B., Whatmough, P., Adolf, R., Rama, S., Lee,\nH., Lee, S. K., Hern\u00b4andez-Lobato, J. M., Wei, G.-Y.,\nand Brooks, D. Minerva: Enabling low-power, highly-\naccurate deep neural network accelerators.\nIn 2016\nACM/IEEE 43rd Annual International Symposium on\nComputer Architecture (ISCA), pp. 267\u2013278, 2016. doi:\n10.1109/ISCA.2016.32.\n9\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nRonneberger, O., Fischer, P., and Brox, T. U-net: Con-\nvolutional networks for biomedical image segmentation.\nIn International Conference on Medical Image Comput-\ning and Computer-Assisted Intervention, pp. 234\u2013241.\nSpringer, 2015.\nSamajdar, A., Joseph, J. M., Zhu, Y., Whatmough, P., Mat-\ntina, M., and Krishna, T. A systematic methodology\nfor characterizing scalability of dnn accelerators using\nscale-sim. In 2020 IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS),\npp. 58\u201368, 2020.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition. In Interna-\ntional Conference on Learning Representations (ICLR),\n2015.\nSovrasov, V.\nptflops:\na flops counting tool for neu-\nral\nnetworks\nin\npytorch\nframework,\n2018-2023.\nURL\nhttps://github.com/sovrasov/\nflops-counter.pytorch. Github Repository.\nSrinivasan, G., Wijesinghe, P., Sarwar, S. S., Jaiswal, A., and\nRoy, K. Significance driven hybrid 8t-6t sram for energy-\nefficient synaptic storage in artificial neural networks. In\n2016 Design, Automation & Test in Europe Conference\n& Exhibition (DATE), pp. 151\u2013156. IEEE, 2016.\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. Man\nvs. computer: Benchmarking machine learning algo-\nrithms for traffic sign recognition. Neural Networks, 32:\n323\u2013332, 2012.\nStutz, D., Chandramoorthy, N., Hein, M., and Schiele, B.\nBit error robustness for energy-efficient dnn accelerators.\nIn Smola, A., Dimakis, A., and Stoica, I. (eds.), Pro-\nceedings of Machine Learning and Systems, volume 3,\npp. 569\u2013598, 2021. URL https://proceedings.\nmlsys.org/paper/2021/file/\na684eceee76fc522773286a895bc8436-Paper.\npdf.\nvan der Maaten, L. and Hinton, G. Visualizing data using\nt-sne. Journal of Machine Learning Research, 9(86):\n2579\u20132605, 2008.\nWu, D., Xia, S.-T., and Wang, Y. Adversarial weight per-\nturbation helps robust generalization. In Larochelle, H.,\nRanzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.),\nAdvances in Neural Information Processing Systems,\nvolume 33, pp. 2958\u20132969. Curran Associates, Inc.,\n2020.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2020/file/\n1ef91c212e30e14bf125e9374262401f-Paper.\npdf.\nWu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized\nconvolutional neural networks for mobile devices. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 4820\u20134828, 2016.\nYang, H., Zhu, Y., and Liu, J. Ecc: Platform-independent\nenergy-constrained deep neural network compression\nvia a bilinear regression model. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 11206\u201311215, 2019a.\nYang, H., Zhu, Y., and Liu, J. Energy-constrained com-\npression for deep neural networks via weighted sparse\nprojection and layer input masking. In International Con-\nference on Learning Representations (ICLR), 2019b.\nYang, H., Gui, S., Zhu, Y., and Liu, J. Automatic neural net-\nwork compression by sparsity-quantization joint learning:\nA constrained optimization-based approach. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020.\nYang, T.-J., Chen, Y.-H., and Sze, V. Designing energy-\nefficient convolutional neural networks using energy-\naware pruning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp.\n5687\u20135695, 2017.\nZhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., and\nJordan, M. Theoretically principled trade-off between\nrobustness and accuracy. In International Conference on\nMachine Learning (ICML), pp. 7472\u20137482. PMLR, 2019.\n10\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nAppendix\nThe appendix provides more implementation details for our method, experimental results on more datasets and settings,\nablation studies, and qualitative analysis. The appendices cover the following:\n\u2022 Implementation Details: NeuralFuse Training Algorithm (Sec. A), NeuralFuse Generator (Sec. B), Energy-Accuracy\nTradeoff (Sec. C), Latency Reports (Sec. D)\n\u2022 Experimental Results: Ablation Studies (Sec. E), Relaxed Access (Sec. F), Restricted Access (Sec. G), Reduced-\nprecision Quantization (Sec. H), Adversarial Training (Sec. I), Adversarial Weight Perturbation (Sec. J)\n\u2022 Qualitative Studies: Data Embeddings Visualization (Sec. K), Transformed Inputs Visualization (Sec. L)\nA. Training Algorithm of NeuralFuse\nAlgorithm 1 Training steps for NeuralFuse\nInput: Base model M0; Generator G; Training data samples X; Distribution of the perturbed models Mp; Number of\nperturbed models N; Total training iterations T\nOutput: Optimized parameters WG for the Generator G\n1: for t = 0, ..., T \u2212 1 do\n2:\nfor all mini-batches {x, y}B\nb=1 \u223c X do\n3:\nCreate transformed inputs xt = F(x) = clip[\u22121,1]\n\u0000x + G(x)\n\u0001\n.\n4:\nSample N perturbed models {Mp1, ..., MpN } from Mp under p% random bit error rate.\n5:\nfor all Mpi \u223c {Mp1, ..., MpN } do\n6:\nCalculate the loss losspi based on the output of the perturbed model Mpi. Then calculate the gradients gpi for\nWG based on losspi.\n7:\nend for\n8:\nCalculate the loss loss0 based on the output of the clean model M0. Then calculate the gradients g0 for WG based\non loss0.\n9:\nCalculate the final gradient gfinal using (5) based on g0 and gp1, ..., gpN .\n10:\nUpdate WG using gfinal.\n11:\nend for\n12: end for\nB. Implementation Details of NeuralFuse Generator\nWe consider two main goals in designing the NeuralFuse Generator: 1) efficiency (so the overall energy overhead is\ndecreased) and 2) robustness (so that it can generate robust patterns on the input image and overcome the random bit flipping\nin subsequent models). Accordingly, we choose to utilize an encode-decoder architecture in implementing the generator.\nThe design of ConvL is inspired by Nguyen & Tran (2020), in which the authors utilize a similar architecture to design an\ninput-aware trigger generator, and have demonstrated its efficiency and effectiveness. Furthermore, we attempted to enhance\nit by replacing the Upsampling layer with a Deconvolution layer, leading to the creation of DeConvL. The UNetL-based\nNeuralFuse draws inspiration from Ronneberger et al. (2015), known for its robust performance in image segmentation,\nand thus, we incorporated it as one of our architectures. Lastly, ConvS, DeConvS, and UNetS are scaled-down versions\nof the model designed to reduce computational costs and total parameters. The architectures of Convolutional-based and\nDeconvolutional-based are shown in Table 4, and the architecture of UNet-based generators is in Table 5. For the abbreviation\nused in the table, ConvBlock means the Convolution block, Conv means a single Convolution layer, DeConvBlock means\nthe Deconvolution block, DeConv means a single Deconvolution layer, and BN means a Batch Normalization layer. We use\nlearning rate = 0.001, \u03bb = 5, and Adam optimizer. For CIFAR-10, GTSRB, and CIFAR-100, we set batch size b = 25 for\neach base model. For ImageNet-10, we set b = 64 for ResNet18, ResNet50 and VGG11, and b = 32 for both VGG16 and\nVGG19.\n11\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nTable 4. Model architecture for both Convolution-based and Deconvolution-based generators. Each ConvBlock consists of a Convolution\n(kernel = 3 \u00d7 3, padding = 1, stride = 1), a Batch Normalization, and a ReLU layer. Each DeConvBlock consists of a Deconvolution\n(kernel = 4 \u00d7 4, padding = 1, stride = 2), a Batch Normalization, and a ReLU layer.\nConvL\nConvS\nDeConvL\nDeConvS\nLayers\n#CHs\nLayers\n#CHs\nLayers\n#CHs\nLayers\n#CHs\n(ConvBlock)\u00d72, MaxPool\n32\nConvBlock, Maxpool\n32\n(ConvBlock)\u00d72, MaxPool\n32\nConvBlock, Maxpool\n32\n(ConvBlock)\u00d72, MaxPool\n64\nConvBlock, Maxpool\n64\n(ConvBlock)\u00d72, MaxPool\n64\nConvBlock, Maxpool\n64\n(ConvBlock)\u00d72, MaxPool\n128\nConvBlock, Maxpool\n64\n(ConvBlock)\u00d72, MaxPool, 128\nConvBlock, Maxpool\n64\nConvBlock, UpSample, ConvBlock 128\nConvBlock, UpSample\n64\nConvBlock\n128\nDeConvBlock\n64\nConvBlock, UpSample, ConvBlock\n64\nConvBlock, UpSample\n32\nDeConvBlock, ConvBlock\n64\nDeConvBlock\n32\nConvBlock, UpSample, ConvBlock\n32\nConvBlock, UpSample\n3\nDeConvBlock, ConvBlock\n32\nDeConv, BN, Tanh\n3\nConv, BN, Tanh\n32\nConv, BN, Tanh\n3\nConv, BN, Tanh\n3\n[Note] #CHs: number of channels.\nTable 5. Model architecture for UNet-based generators. Each ConvBlock consists of a Convolution (kernel = 3 \u00d7 3, padding = 1,\nstride = 1), a Batch Normalization, and a ReLU layer. Other layers, such as the Deconvolutional layer (kernel = 2 \u00d7 2, padding = 1,\nstride = 2), are used in UNet-based models. For the final Convolution layer, the kernel size is set to 1.\nUNetL\nUNetS\nLayers\n#Channels\nLayers\n#Channels\nL1: (ConvBlock)\u00d72\n16\nL1: (ConvBlock)\u00d72\n8\nL2: Maxpool, (ConvBlock)\u00d72\n32\nL2: Maxpool, (ConvBlock)\u00d72\n16\nL3: Maxpool, (ConvBlock)\u00d72\n64\nL3: Maxpool, (ConvBlock)\u00d72\n32\nL4: Maxpool, (ConvBlock)\u00d72\n128\nL4: Maxpool, (ConvBlock)\u00d72\n64\nL5: DeConv\n64\nL5: DeConv\n32\nL6: Concat[L3, L5]\n128\nL6: Concat[L3, L5]\n64\nL7: (ConvBlock)\u00d72\n64\nL7: (ConvBlock)\u00d72\n32\nL8: DeConv\n32\nL8: DeConv\n16\nL9: Concat[L2, L8]\n64\nL9: Concat[L2, L8]\n32\nL10: (ConvBlock)\u00d72\n32\nL10: (ConvBlock)\u00d72\n16\nL11: DeConv\n16\nL11: DeConv\n8\nL12: Concat[L1, L11]\n32\nL12: Concat[L1, L11]\n16\nL13: (ConvBlock)\u00d72\n16\nL13: (ConvBlock)\u00d72\n8\nL14: Conv\n3\nL14: Conv\n3\nC. NeuralFuse\u2019s Energy-Accuracy Tradeoff\nSCALE-SIM.\nSCALE-SIM (Samajdar et al., 2020) is a systolic array-based CNN simulator that can calculate the\nnumber of memory accesses and the total time in execution cycles by giving the specific model architecture and accelerator\narchitectural configuration as inputs. In this paper, we use SCALE-SIM to calculate the weights memory access of 5 based\nmodels (ResNet18, ResNet50, VGG11, VGG16, VGG19), and 6 generators (ConvL, ConvS, DeConvL, DeConvS, UNetL,\nUNetS). While SCALE-SIM supports both Convolutional and Linear layers, it does not yet support Deconvolution layers.\nInstead, we try to approximate the memory costs of Deconvolution layers by Convolution layers. We change the input and\noutput from Deconvolution into the output and input of the Convolution layers. Besides, we also change the stride into 1\nwhen we approximate it. We also add padding for the convolution layers while generating input files for SCALE-SIM. In\nthis paper, we only consider the energy saving on weights accesses, so we only take the value \u201cSRAM Filter Reads\u201d from\nthe output of SCALE-SIM as the total weights memory accesses (T.W.M.A.) for further energy calculation.\nIn Table 6, we report the total weight memory access (T.W.M.A.) using SCALE-SIM. We then showed the energy-accuracy\ntradeoff between all of the combinations of NeuralFuse and base models under a 1% of bit error rate in Figure 5.\n12\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nTable 6. The total weights memory access calculated by SCALE-SIM.\nBase Model\nResNet18\nResNet50\nVGG11\nVGG16\nVGG19\n-\nT.W.M.A.\n2,755,968\n6,182,144\n1,334,656\n2,366,848\n3,104,128\n-\nNeuralFuse\nConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\nT.W.M.A.\n320,256\n41,508\n259,264\n86,208\n180,894\n45,711\n[Note] T.W.M.A.: total weight memory access.\nFigure 5. The energy-accuracy tradeoff of different NeuralFuse implementations with all CIFAR-10 pre-trained based models. The X-axis\nrepresents the percentage reduction in dynamic memory access energy at low-voltage settings (base model protected by NeuralFuse)\ncompared to the bit-error-free (nominal) voltage; the Y-axis represents the perturbed accuracy (evaluated at low voltage) with a 1% bit\nerror rate.\nParameters and MACs Calculation.\nIn addition to T.W.M.A., the number of parameters and MACs (multiply\u2013accumulate\noperations) are also common metrics in measuring the energy consumption of machine learning models. Yang et al. (2017)\nhave shown that the energy consumption of computation and memory accesses are both proportional to MACs, allowing us\nto take computation energy consumption into account.\nHere, we use the open-source package ptflops (Sovrasov, 2018-2023) to calculate the parameters and MAC values of all\nthe base models and the NeuralFuse generators, in the same units as Bejnordi et al. (2020) used. The results are shown in\nTable 7. Note that we modified the base model architectures for ImageNet-10, as it has larger input sizes. For example,\nwe use a larger kernel size = 7 instead of 3 in the first Convolution layer in ResNet-based models to enhance the learning\nabilities. Therefore, the parameters of base models are different between different datasets. For NeuralFuse generators, we\nutilize the same architectures for implementation (including ImageNet-10). As a result, our proposed NeuralFuse generators\nare generally smaller than base models, either on total model parameters or MAC values.\nMACs-based Energy Consumption.\nWe can then use the MAC values to further approximate the end-to-end energy\nconsumption of the whole model. Assume that all values are stored on SRAM and that a MAC represents single memory\naccess. The corresponding MACs-based energy saving percentage (MAC-ES, %) can be derived from Eq. 7 (c.f. Sec. 4.4),\nand results can be found in Table 8. We can observe that most combinations can save a large amount of energy, except that\nVGG11 with two larger NeuralFuse (ConvL and DeConvL) may increase the total energy. These results are consistent with\nthe results reported in Table 2. In addition, we also showed the MACs-based energy-accuracy tradeoff between all of the\ncombinations of NeuralFuse and base models under a 1% of bit error rate in Figure 6.\nMAC-ES =\nMACsbase model\u00b7Energynominal voltage\u2212\n\u0000MACsbase model\u00b7Energylow-voltage-regime+MACsNeuralFuse\u00b7EnergyNeuralFuse at nominal voltage\n\u0001\nMACsbase model\u00b7Energynominal voltage\n\u00d7 100%\n(7)\n13\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nTable 7. Parameter counts and MACs for all base models and generators in this paper.\nBase Model\nResNet18\nResNet50\nVGG11\nVGG16\nVGG19\n-\nParameter\nCIFAR-10\n11,173,962\n23,520,842\n9,231,114\n14,728,266\n20,040,522\n-\nImageNet-10\n11,181,642\n23,528,522\n128,812,810\n134,309,962\n139,622,218\nMACs\nCIFAR-10\n557.14M\n1.31G\n153.5M\n314.43M\n399.47M\n-\nImageNet-10\n1.82G\n4.12G\n7.64G\n15.53G\n19.69G\nNeuralFuse\nConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\nParameter\nCIFAR-10\nImageNet-10\n723,273\n113,187\n647,785\n156,777\n482,771\n121,195\nMACs\nCIFAR-10\n80.5M\n10.34M\n64.69M\n22.44M\n41.41M\n10.58M\nImageNet-10\n3.94G\n506.78M\n3.17G\n1.1G\n2.03G\n518.47M\nTable 8. The MACs-Based energy saving percentage (%) for different combinations of base models and NeuralFuse.\nBase Model\nConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\nResNet18\n16.2\n28.7\n19.0\n26.6\n23.2\n28.7\nResNet50\n24.5\n29.8\n25.7\n28.9\n27.4\n29.8\nVGG11\n-21.8\n23.9\n-11.5\n16.0\n3.6\n23.7\nVGG16\n5.0\n27.3\n10.0\n23.5\n17.4\n27.2\nVGG19\n10.4\n28.0\n14.4\n25.0\n20.2\n28.0\nFigure 6. The Mac-based energy-accuracy tradeoff of different NeuralFuse implementations with all CIFAR-10 pre-trained based models.\nThe X-axis represents the percentage reduction in dynamic memory access energy at low-voltage settings (base model protected by\nNeuralFuse), compared to the bit-error-free (nominal) voltage; the Y-axis represents the perturbed accuracy (evaluated at low voltage)\nwith a 1% bit error rate.\nAlthough using ConvL or DeConvL along with base model VGG11 for CIFAR-10 implies an increase in energy consumption,\nusing other smaller-scale generators, we can still save the overall energy and recover the base model\u2019s accuracy. That\nsaid, developers can always choose smaller generators (with orders of magnitude fewer MAC operations than the original\nnetwork) to restore model accuracy, further demonstrating the flexibility of choosing NeuralFuse generators of different\nsizes.\n14\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nD. Inference Latency of NeuralFuse\nIn Table 9, we report the latency (batch size=1, CIFAR-10/ImageNet-10 testing dataset) of utilizing the different NeuralFuse\ngenerators with two different base models, ResNet18 and VGG19. We can see that although NeuralFuse brings a certain\ndegree of extra latency, we deemed it an inevitable tradeoff for reducing energy consumption in our setting. Although latency\nis not the main focus of this paper, we envision future research designing a more lightweight version of the NeuralFuse\nmodule or using model compression techniques on NeuralFuse to reduce latency.\nTable 9. The Inference Latency of base model and base model with NeuralFuse.\nResNet18 (CIFAR-10)\nVGG19 (CIFAR-10)\nResNet18 (ImageNet-10)\nVGG19 (ImageNet-10)\nBase Model\n5.84 ms\n5.32 ms\n6.21 ms\n14.34 ms\n+ ConvL\n9.37 ms (+3.53)\n8.96 ms (+3.64)\n10.51 ms (+4.3)\n17.66 ms (+3.32)\n+ ConvS\n7.86 ms (+2.02)\n7.40 ms (+2.08)\n8.28 ms (+2.07)\n16.72 ms (+2.38)\n+ DeConvL\n9.18 ms (+3.34)\n8.59 ms (+3.27)\n10.07 ms (+3.86)\n17.24 ms (+2.90)\n+ DeConvS\n7.49 ms (+1.65)\n7.04 ms (+1.72)\n7.79 ms (+1.58)\n15.67 ms (+1.33)\n+ UNetL\n10.69 ms (+4.85)\n10.06 ms (+4.74)\n11.14 ms (+4.93)\n18.54 ms (+4.20)\n+ UNetS\n10.63 ms (+4.79)\n10.13 ms (+4.81)\n11.36 ms (+5.15)\n18.60 ms (+4.26)\nE. Ablation Studies\nStudy for N in EOPM.\nHere, we study the effect of N used in EOPM (Eq. 5). In Figure 7, we report the results for\nConvL and ConvS on CIFAR-10 pre-trained ResNet18, under a 1% bit error rate (B.E.R.). The results demonstrate that if\nwe apply larger N, the performance increases until convergence. Specifically, for ConvL (Figure 7a), larger N empirically\nhas a smaller standard deviation; this means larger N gives better stability but at the cost of time-consuming training. In\ncontrast, for the small generator ConvS (Figure 7b), we can find that the standard deviation is still large even trained by\nlarger N; the reason might be that small generators are not as capable of learning representations as larger ones. Therefore,\nthere exists a tradeoff between the stability of the generator performance and the total training time. In our implementation,\nchoosing N = 5 or 10 is a good balance.\n(a) Using ConvL\n(b) Using ConvS\nFigure 7. The experimental results on using different sizes of N for EOPM.\nTradeoff Between Clean Accuracy (C.A.) and Perturbed Accuracy (P.A.).\nWe conducted an experiment to study the\neffect of different \u03bb values, which balance the ratio of clean accuracy and perturbed accuracy. In Table 10, the experimental\nresults showed that a smaller \u03bb can preserve clean accuracy, but result in poor perturbed accuracy. On the contrary, larger \u03bb\ncan deliver higher perturbed accuracy, but with more clean accuracy drop. This phenomenon has also been observed in\nadversarial training (Zhang et al., 2019).\nComparison to Universal Input Perturbation (UIP).\nMoosavi-Dezfooli et al. (2017) has shown that there exists a\nuniversal adversarial perturbation to the input data such that the model will make wrong predictions on a majority of the\n15\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nTable 10. Experimental results based on \u03bb value choosing. The results show that \u03bb = 5\ncan balance the tradeoff between clean accuracy and perturbed accuracy.\nBase\nModel\n\u03bb\nC.A.\nP.A.\nConvL\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\n10\n92.6\n38.9 \u00b1 12.4\n90.1\n88.0 \u00b1 1.7\n49.1\n5\n89.8\n87.8 \u00b1 1.7\n48.8\n1\n90.0\n84.2 \u00b1 3.8\n45.3\n0.1\n91.6\n65.7 \u00b1 9.3\n26.8\n0.01\n92.2\n43.6 \u00b1 13\n4.7\nVGG19\n10\n90.5\n36.0 \u00b1 12.0\n89.6\n77.9 \u00b1 19\n41.9\n5\n89.8\n77.7 \u00b1 19\n41.7\n1\n89.9\n73.1 \u00b1 19\n37.1\n0.1\n89.1\n51.2 \u00b1 16\n15.2\n0.01\n90.2\n36.8 \u00b1 12\n0.8\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and\nR.P.: total recover percentage of P.A. (NF) v.s. P.A.\nperturbed images. In our NeuralFuse framework, the universal perturbation is a special case when we set G(x) = tanh (UIP)\nfor any data sample x. The transformed data sample then becomes xt = clip[\u22121,1]\n\u0000x + tanh (UIP)\n\u0001\n, where xt \u2208 [\u22121, 1]d\nand UIP is a trainable universal input perturbation that has the same size as the input data. The experimental results with\nthe universal input perturbation are shown in Table 11. We observe that its performance is much worse than our proposed\nNeuralFuse. This result validates the necessity of adopting input-aware transformation for learning error-resistant data\nrepresentations in low-voltage scenarios.\nTable 11. Performance of the universal input perturbation (UIP) trained by EOPM on CIFAR-10 pre-trained ResNet18.\nBase Model\nB.E.R.\nC.A.\nP.A.\nC.A. (UIP)\nP.A. (UIP)\nR.P.\nResNet18\n1%\n92.6\n38.9 \u00b1 12.4\n91.8\n37.9 \u00b1 11\n-1.0\n0.5%\n70.1 \u00b1 11.6\n92.5\n70.6 \u00b1 11\n0.5\nResNet50\n1%\n92.6\n26.1 \u00b1 9.4\n80.7\n21.0 \u00b1 5.9\n-5.1\n0.5%\n61.0 \u00b1 10.3\n91.9\n62.4 \u00b1 12\n1.4\nVGG11\n1%\n88.4\n42.2 \u00b1 11.6\n86.9\n43.0 \u00b1 11\n0.8\n0.5%\n63.6 \u00b1 9.3\n88.2\n64.2 \u00b1 8.8\n0.6\nVGG16\n1%\n90.3\n35.7 \u00b1 7.9\n90.1\n37.1 \u00b1 8.5\n1.4\n0.5%\n66.6 \u00b1 8.1\n90.4\n67.3 \u00b1 8.1\n0.7\nVGG19\n1%\n90.5\n36.0 \u00b1 12.0\n89.9\n35.3 \u00b1 12\n-0.7\n0.5%\n64.2 \u00b1 12.4\n90.1\n64.4 \u00b1 12\n0.2\n[Note] B.E.R.: the bit error rate of the base model, C.A. (%): clean accuracy, UIP: universal input transformation\nparameter, P.A.(%): perturbed accuracy, and R.P.: total recover percentage of P.A. (UIP) v.s. P.A.\n16\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nF. Additional Experimental Results on Relaxed Access Settings\nWe conducted more experiments on Relaxed Access settings to show that our NeuralFuse can protect the models under\ndifferent B.E.R. The results can be found in Sec. F.1 (CIFAR-10), Sec. F.2 (GTSRB), Sec. F.3 (ImageNet-10), and Sec. F.4\n(CIFAR-100). For comparison, we also visualize the experimental results in the figures below each table.\nF.1. CIFAR-10\nTable 12. Testing accuracy (%) under 1% and 0.5% of random bit error rate on CIFAR-10.\nBase\nModel\nNF\nC.A.\n1% B.E.R.\n0.5% B.E.R.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nConvL\n92.6\n89.8\n87.8 \u00b1 1.7\n48.8\n90.4\n87.9 \u00b1 2.2\n17.8\nConvS\n88.2\n59.5 \u00b1 11\n20.6\n91.7\n78.4 \u00b1 8.3\n8.3\nDeConvL\n38.9\n89.6\n88.5 \u00b1 0.8\n49.6\n70.1\n90.2\n90.0 \u00b1 0.2\n19.9\nDeConvS\n\u00b1 12.4\n82.9\n68.8 \u00b1 6.4\n29.9\n\u00b1 11.6\n84.1\n79.9 \u00b1 3.6\n9.8\nUNetL\n86.6\n84.6 \u00b1 0.8\n45.6\n89.7\n86.3 \u00b1 2.4\n16.2\nUNetS\n84.4\n68.8 \u00b1 6.0\n29.8\n90.9\n80.7 \u00b1 5.8\n10.7\nResNet50\nConvL\n92.6\n85.5\n53.2 \u00b1 22\n27.1\n90.3\n86.5 \u00b1 3.2\n25.5\nConvS\n85.2\n34.6 \u00b1 14\n8.5\n90.8\n73.3 \u00b1 8.7\n12.3\nDeConvL\n26.1\n87.4\n63.3 \u00b1 21\n37.2\n61.0\n89.5\n87.2 \u00b1 2.5\n26.2\nDeConvS\n\u00b1 9.4\n82.4\n42.2 \u00b1 17\n16.1\n\u00b1 10.3\n90.3\n75.5 \u00b1 8.1\n14.5\nUNetL\n86.2\n75.5 \u00b1 12\n49.4\n89.9\n83.9 \u00b1 3.6\n22.9\nUNetS\n77.3\n56.2 \u00b1 19\n30.1\n89.7\n76.1 \u00b1 7.2\n15.1\nVGG11\nConvL\n88.4\n89.6\n87.2 \u00b1 2.9\n45.1\n89.8\n87.0 \u00b1 1.3\n23.3\nConvS\n84.9\n66.3 \u00b1 7.5\n24.1\n88.2\n74.5 \u00b1 5.7\n10.9\nDeConvL\n42.2\n89.3\n87.2 \u00b1 2.6\n45.0\n63.6\n89.6\n86.9 \u00b1 1.1\n23.2\nDeConvS\n\u00b1 11.6\n85.6\n68.2 \u00b1 7.1\n26.0\n\u00b1 9.3\n88.3\n75.7 \u00b1 4.6\n12.1\nUNetL\n87.1\n83.6 \u00b1 1.3\n41.4\n88.0\n82.4 \u00b1 1.8\n18.8\nUNetS\n85.5\n72.7 \u00b1 4.6\n30.5\n88.1\n75.8 \u00b1 4.3\n12.2\nVGG16\nConvL\n90.3\n90.1\n86.0 \u00b1 6.2\n50.3\n90.2\n88.5 \u00b1 0.9\n21.9\nConvS\n87.4\n59.6 \u00b1 12\n23.9\n89.9\n77.8 \u00b1 4.8\n11.1\nDeConvL\n35.7\n89.7\n85.5 \u00b1 6.8\n49.8\n66.6\n89.7\n88.2 \u00b1 1.0\n21.4\nDeConvS\n\u00b1 7.9\n86.8\n66.5 \u00b1 11\n30.8\n\u00b1 8.1\n90.0\n78.4 \u00b1 4.7\n11.8\nUNetL\n87.4\n83.4 \u00b1 4.4\n47.7\n89.0\n86.2 \u00b1 1.5\n19.6\nUNetS\n87.4\n71.2 \u00b1 8.2\n35.5\n89.0\n80.2 \u00b1 3.5\n13.7\nVGG19\nConvL\n90.5\n89.8\n77.7 \u00b1 19\n41.7\n90.4\n88.1 \u00b1 1.8\n23.9\nConvS\n87.3\n52.7 \u00b1 17\n16.7\n89.6\n74.5 \u00b1 9.0\n10.3\nDeConvL\n36.0\n86.3\n78.4 \u00b1 18\n42.4\n64.2\n90.4\n88.5 \u00b1 1.4\n24.3\nDeConvS\n\u00b1 12.0\n86.5\n58.2 \u00b1 18\n22.2\n\u00b1 12.4\n89.7\n75.2 \u00b1 8.6\n11.0\nUNetL\n86.3\n82.1 \u00b1 4.8\n46.0\n89.1\n85.0 \u00b1 2.7\n20.8\nUNetS\n86.3\n66.4 \u00b1 13\n30.4\n89.2\n77.1 \u00b1 7.3\n12.9\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nNominal (w/o NeuralFuse)\nLow voltage (w/o NeuralFuse)\nNominal (w/ NeuralFuse)\nLow voltage (w/ NeuralFuse)\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.6\nAccuracy (%)\n38  \n     .9\n89  \n88  \n89  \n82  \n86  \n84  \n     .8\n     .2\n     .6\n     .9\n     .6\n     .4\n87  \n59  \n88  \n68  \n84  \n68  \n     .8\n     .5\n     .5\n     .8\n     .6\n     .8\nCIFAR-10 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.6\n26  \n     .1\n85  \n85  \n87  \n82  \n86  \n77  \n     .5\n     .2\n     .4\n     .4\n     .2\n     .3\n53  \n34  \n63  \n42  \n75  \n56  \n     .2\n     .6\n     .3\n     .2\n     .5\n     .2\nCIFAR-10 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n88.4\n42  \n     .2\n89  \n84  \n89  \n85  \n87  \n85  \n     .6\n     .9\n     .3\n     .6\n     .1\n     .5\n87  \n66  \n87  \n68  \n83  \n72  \n     .2\n     .3\n     .2\n     .2\n     .6\n     .7\nCIFAR-10 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n90.3\n35  \n     .7\n90  \n87  \n89  \n86  \n87  \n87  \n     .1\n     .4\n     .7\n     .8\n     .4\n     .4\n86  \n59  \n85  \n66  \n83  \n71  \n     .0\n     .6\n     .5\n     .5\n     .4\n     .2\nCIFAR-10 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n90.5\n36  \n     .0\n89  \n87  \n86  \n86  \n86  \n86  \n     .8\n     .3\n     .3\n     .5\n     .3\n     .3\n77  \n52  \n78  \n58  \n82  \n66  \n     .7\n     .7\n     .4\n     .2\n     .1\n     .4\nCIFAR-10 pre-trained VGG19\n(a) CIFAR-10, 1% Bit Error Rate\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.6\n70  \n     .1\n90  \n91  \n90  \n84  \n89  \n90  \n     .4\n     .7\n     .2\n     .1\n     .7\n     .9\n87  \n78  \n90  \n79  \n86  \n80  \n     .9\n     .4\n     .0\n     .9\n     .3\n     .7\nCIFAR-10 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.6\n61  \n     .0\n90  \n90  \n89  \n90  \n89  \n89  \n     .3\n     .8\n     .5\n     .3\n     .9\n     .7\n86  \n73  \n87  \n75  \n83  \n76  \n     .5\n     .3\n     .2\n     .5\n     .9\n     .1\nCIFAR-10 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n88.4\n63  \n     .6\n89  \n88  \n89  \n88  \n88  \n88  \n     .8\n     .2\n     .6\n     .3\n     .0\n     .1\n87  \n74  \n86  \n75  \n82  \n75  \n     .0\n     .5\n     .9\n     .7\n     .4\n     .8\nCIFAR-10 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n90.3\n66  \n     .6\n90  \n89  \n89  \n90  \n89  \n89  \n     .2\n     .9\n     .7\n     .0\n     .0\n     .0\n88  \n77  \n88  \n78  \n86  \n80  \n     .5\n     .8\n     .2\n     .4\n     .2\n     .2\nCIFAR-10 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n90.5\n64  \n     .2\n90  \n89  \n90  \n89  \n89  \n89  \n     .4\n     .6\n     .4\n     .7\n     .1\n     .2\n88  \n74  \n88  \n75  \n85  \n77  \n     .1\n     .5\n     .5\n     .2\n     .0\n     .1\nCIFAR-10 pre-trained VGG19\n(b) CIFAR-10, 0.5% Bit Error Rate\nFigure 8. Experimental results on CIFAR-10\n17\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nF.2. GTSRB\nTable 13. Testing accuracy (%) under 1% and 0.5% of random bit error rate on GTSRB.\nBase\nModel\nNF\nC.A.\n1% B.E.R.\n0.5% B.E.R.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nConvL\n95.5\n95.7\n91.1 \u00b1 4.7\n54.2\n93.4\n89.5 \u00b1 1.9\n14.3\nConvS\n94.4\n68.6 \u00b1 12\n31.7\n94.8\n87.7 \u00b1 4.2\n12.4\nDeConvL\n36.9\n95.6\n91.3 \u00b1 4.3\n54.4\n75.2\n95.4\n93.4 \u00b1 1.1\n18.1\nDeConvS\n\u00b1 16.0\n95.7\n78.1 \u00b1 9.1\n41.2\n\u00b1 12.7\n95.8\n90.1 \u00b1 3.3\n14.9\nUNetL\n96.2\n93.8 \u00b1 1.0\n56.9\n96.2\n93.5 \u00b1 1.6\n18.3\nUNetS\n95.9\n85.1 \u00b1 6.9\n48.2\n95.5\n91.4 \u00b1 2.8\n16.2\nResNet50\nConvL\n95.0\n95.6\n71.6 \u00b1 20\n42.1\n94.6\n90.6 \u00b1 3.7\n16.6\nConvS\n94.8\n50.5 \u00b1 22\n21.0\n95.4\n84.5 \u00b1 8.5\n10.5\nDeConvL\n29.5\n94.9\n71.6 \u00b1 21\n42.0\n74.0\n94.7\n91.6 \u00b1 2.9\n17.6\nDeConvS\n\u00b1 16.9\n93.0\n56.4 \u00b1 17\n26.9\n\u00b1 13.0\n94.6\n87.4 \u00b1 5.9\n13.5\nUNetL\n94.5\n80.6 \u00b1 15\n51.1\n96.5\n93.7 \u00b1 2.3\n19.7\nUNetS\n94.7\n64.7 \u00b1 22\n35.2\n95.9\n90.6 \u00b1 4.8\n16.7\nVGG11\nConvL\n91.9\n94.8\n85.7 \u00b1 7.2\n50.9\n93.9\n92.6 \u00b1 0.7\n27.7\nConvS\n91.1\n62.2 \u00b1 11\n27.3\n90.9\n80.5 \u00b1 3.5\n15.7\nDeConvL\n34.9\n95.0\n84.6 \u00b1 7.6\n49.7\n64.9\n93.6\n91.9 \u00b1 0.6\n27.1\nDeConvS\n\u00b1 12.4\n92.4\n67.5 \u00b1 11\n32.6\n\u00b1 10.8\n92.3\n83.1 \u00b1 3.7\n18.2\nUNetL\n92.2\n83.2 \u00b1 6.0\n48.3\n94.8\n90.6 \u00b1 1.7\n25.7\nUNetS\n94.7\n73.4 \u00b1 10\n38.5\n94.6\n88.9 \u00b1 2.2\n24.1\nVGG16\nConvL\n95.2\n96.3\n72.4 \u00b1 12\n57.3\n95.6\n93.2 \u00b1 1.8\n34.4\nConvS\n94.1\n39.8 \u00b1 13\n24.6\n94.3\n82.2 \u00b1 6.2\n23.4\nDeConvL\n15.1\n96.4\n72.0 \u00b1 12\n56.9\n58.8\n95.6\n93.1 \u00b1 2.0\n34.3\nDeConvS\n\u00b1 6.8\n93.8\n50.9 \u00b1 13\n35.8\n\u00b1 8.9\n95.1\n84.0 \u00b1 5.3\n25.2\nUNetL\n95.8\n78.6 \u00b1 11\n63.5\n96.0\n92.8 \u00b1 2.0\n34.0\nUNetS\n94.3\n63.3 \u00b1 14\n48.1\n95.4\n87.8 \u00b1 3.6\n29.0\nVGG19\nConvL\n95.5\n96.0\n88.3 \u00b1 7.2\n51.7\n95.6\n93.4 \u00b1 2.1\n24.2\nConvS\n93.8\n69.0 \u00b1 14\n32.4\n94.9\n87.0 \u00b1 4.4\n17.8\nDeConvL\n36.6\n95.4\n87.2 \u00b1 7.5\n50.6\n69.1\n95.5\n92.4 \u00b1 2.2\n23.3\nDeConvS\n\u00b1 6.8\n94.5\n73.1 \u00b1 12\n36.5\n\u00b1 11.1\n95.5\n88.8 \u00b1 3.7\n19.7\nUNetL\n95.4\n88.2 \u00b1 6.7\n51.7\n94.9\n91.7 \u00b1 2.5\n22.6\nUNetS\n94.6\n80.6 \u00b1 9.0\n44.1\n96.5\n90.8 \u00b1 3.4\n21.6\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nNominal (w/o NeuralFuse)\nLow voltage (w/o NeuralFuse)\nNominal (w/ NeuralFuse)\nLow voltage (w/ NeuralFuse)\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.5\nAccuracy (%)\n36  \n     .9\n95  \n94  \n95  \n95  \n96  \n95  \n     .7\n     .4\n     .6\n     .7\n     .2\n     .9\n91  \n68  \n91  \n78  \n93  \n85  \n     .1\n     .6\n     .3\n     .1\n     .8\n     .1\nGTSRB pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.0\n29  \n     .5\n95  \n94  \n94  \n93  \n94  \n94  \n     .6\n     .8\n     .9\n     .0\n     .5\n     .7\n71  \n50  \n71  \n56  \n80  \n64  \n     .6\n     .5\n     .6\n     .4\n     .6\n     .7\nGTSRB pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n91.9\n34  \n     .9\n94  \n91  \n95  \n92  \n92  \n94  \n     .8\n     .1\n     .0\n     .4\n     .2\n     .7\n85  \n62  \n84  \n67  \n83  \n73  \n     .7\n     .2\n     .6\n     .5\n     .2\n     .4\nGTSRB pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.2\n15  \n     .1\n96  \n94  \n96  \n93  \n95  \n94  \n     .3\n     .1\n     .4\n     .8\n     .8\n     .3\n72  \n39  \n72  \n50  \n78  \n63  \n     .4\n     .8\n     .0\n     .9\n     .6\n     .3\nGTSRB pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.5\n36  \n     .6\n96  \n93  \n95  \n94  \n95  \n94  \n     .0\n     .8\n     .4\n     .5\n     .4\n     .6\n88  \n69  \n87  \n73  \n88  \n80  \n     .3\n     .0\n     .2\n     .1\n     .2\n     .6\nGTSRB pre-trained VGG19\n(a) GTSRB, 1% Bit Error Rate\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.5\n75  \n     .2\n93  \n94  \n95  \n95  \n96  \n95  \n     .4\n     .8\n     .4\n     .8\n     .2\n     .5\n89  \n87  \n93  \n90  \n93  \n91  \n     .5\n     .7\n     .4\n     .1\n     .5\n     .4\nGTSRB pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.0\n74  \n     .0\n94  \n95  \n94  \n94  \n96  \n95  \n     .6\n     .4\n     .7\n     .6\n     .5\n     .9\n90  \n84  \n91  \n87  \n93  \n90  \n     .6\n     .5\n     .6\n     .4\n     .7\n     .6\nGTSRB pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n91.9\n64  \n     .9\n93  \n90  \n93  \n92  \n94  \n94  \n     .9\n     .9\n     .6\n     .3\n     .8\n     .6\n92  \n80  \n91  \n83  \n90  \n88  \n     .6\n     .5\n     .9\n     .1\n     .6\n     .9\nGTSRB pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.2\n58  \n     .8\n95  \n94  \n95  \n95  \n96  \n95  \n     .6\n     .3\n     .6\n     .1\n     .0\n     .4\n93  \n82  \n93  \n84  \n92  \n87  \n     .2\n     .2\n     .1\n     .0\n     .8\n     .8\nGTSRB pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n95.5\n69  \n     .1\n95  \n94  \n95  \n95  \n94  \n96  \n     .6\n     .9\n     .5\n     .5\n     .9\n     .5\n93  \n87  \n92  \n88  \n91  \n90  \n     .4\n     .0\n     .4\n     .8\n     .7\n     .8\nGTSRB pre-trained VGG19\n(b) GTSRB, 0.5% Bit Error Rate\nFigure 9. Experimental results on GTSRB.\n18\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nF.3. ImageNet-10\nTable 14. Testing accuracy under 0.5% of random bit error rate on ImageNet-10.\nBase\nModel\nNF\nC.A.\n0.5% B.E.R.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nConvL\n92.2\n72.3 \u00b1 7.0\n94.0\n88.0 \u00b1 2.0\n15.7\nConvS\n91.8\n83.6 \u00b1 4.1\n11.3\nDeConvL\n94.0\n89.2 \u00b1 1.3\n16.9\nDeConvS\n92.8\n87.5 \u00b1 2.3\n15.2\nUNetL\n94.0\n88.1 \u00b1 1.4\n15.8\nUNetS\n93.2\n86.4 \u00b1 2.2\n14.1\nResNet50\nConvL\n89.8\n39.4 \u00b1 11\n92.2\n80.0 \u00b1 5.8\n40.6\nConvS\n91.8\n65.0 \u00b1 11\n25.6\nDeConvL\n93.0\n79.4 \u00b1 5.9\n40.0\nDeConvS\n93.2\n70.9 \u00b1 9.1\n31.5\nUNetL\n92.2\n80.5 \u00b1 5.8\n41.1\nUNetS\n92.4\n73.6 \u00b1 8.9\n34.2\nVGG11\nConvL\n91.6\n47.8 \u00b1 13\n92.0\n86.1 \u00b1 3.7\n38.3\nConvS\n89.4\n66.4 \u00b1 7.1\n18.6\nDeConvL\n91.0\n86.0 \u00b1 3.0\n38.2\nDeConvS\n89.0\n72.5 \u00b1 7.8\n24.7\nUNetL\n92.4\n83.0 \u00b1 3.5\n35.2\nUNetS\n86.2\n73.5 \u00b1 6.0\n25.7\nVGG16\nConvL\n94.6\n38.4 \u00b1 17\n90.8\n77.1 \u00b1 11\n38.7\nConvS\n90.2\n60.2 \u00b1 14\n21.8\nDeConvL\n91.2\n77.2 \u00b1 11\n38.8\nDeConvS\n90.0\n62.3 \u00b1 14\n23.9\nUNetL\n90.6\n81.1 \u00b1 5.9\n42.7\nUNetS\n86.4\n72.3 \u00b1 8.8\n33.9\nVGG19\nConvL\n92.4\n37.2 \u00b1 11\n91.4\n75.5 \u00b1 8.8\n38.3\nConvS\n88.8\n56.5 \u00b1 13\n19.3\nDeConvL\n91.0\n75.9 \u00b1 8.9\n38.7\nDeConvS\n88.8\n64.0 \u00b1 11\n26.8\nUNetL\n89.4\n77.9 \u00b1 6.1\n40.7\nUNetS\n87.6\n65.9 \u00b1 10\n28.7\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.:\ntotal recover percentage of P.A. (NF) v.s. P.A.\nNominal (w/o NeuralFuse)\nLow voltage (w/o NeuralFuse)\nNominal (w/ NeuralFuse)\nLow voltage (w/ NeuralFuse)\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.2\nAccuracy (%)\n72  \n     .3\n94  \n91  \n94  \n92  \n94  \n93  \n     .0\n     .8\n     .0\n     .8\n     .0\n     .2\n88  \n83  \n89  \n87  \n88  \n86  \n     .0\n     .6\n     .2\n     .5\n     .1\n     .4\nImageNet-10 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n89.8\n39  \n     .4\n92  \n91  \n93  \n93  \n92  \n92  \n     .2\n     .8\n     .0\n     .2\n     .2\n     .4\n80  \n65  \n79  \n70  \n80  \n73  \n     .0\n     .0\n     .4\n     .9\n     .5\n     .6\nImageNet-10 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n91.6\n47  \n     .8\n92  \n89  \n91  \n89  \n92  \n86  \n     .0\n     .4\n     .0\n     .0\n     .4\n     .2\n86  \n66  \n86  \n72  \n83  \n73  \n     .1\n     .4\n     .0\n     .5\n     .0\n     .5\nImageNet-10 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n94.6\n38  \n     .4\n90  \n90  \n91  \n90  \n90  \n86  \n     .8\n     .2\n     .2\n     .0\n     .6\n     .4\n77  \n60  \n77  \n62  \n81  \n72  \n     .1\n     .2\n     .2\n     .3\n     .1\n     .3\nImageNet-10 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n92.4\n37  \n     .2\n91  \n88  \n91  \n88  \n89  \n87  \n     .4\n     .8\n     .0\n     .8\n     .4\n     .6\n75  \n56  \n75  \n64  \n77  \n65  \n     .5\n     .5\n     .9\n     .0\n     .9\n     .9\nImageNet-10 pre-trained VGG19\nFigure 10. Experimental results on ImageNet-10, 0.5% Bit Error Rate.\n19\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nF.4. CIFAR-100\nAs mentioned in the previous section, larger generators like ConvL, DeConvL, and UNetL have better performance than\nsmall generators. For CIFAR-100, we find that the gains of utilizing NeuralFuse are less compared to the other datasets. We\nbelieve this is because CIFAR-100 is a more challenging dataset (more classes) for the generators to learn to protect the base\nmodels. Nevertheless, NeuralFuse can still function to restore some degraded accuracy; these results also demonstrate that\nour NeuralFuse is applicable to different datasets. In addition, although the recover percentage is less obvious on CIFAR-100\n(the more difficult dataset), we can still conclude that our NeuralFuse is applicable to different datasets.\nTable 15. Testing accuracy (%) under 1%, 0.5% and 0.35% of random bit error rate on CIFAR-100.\nBase\nModel\nNF\nC.A.\n1% B.E.R.\n0.5% B.E.R.\n0.35% B.E.R.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nConvL\n73.7\n54.8\n11.0 \u00b1 7.7\n6.4\n65.2\n39.0 \u00b1 7.1\n18.1\n69.4\n42.9 \u00b1 6.2\n11.4\nConvS\n49.7\n4.2 \u00b1 2.2\n-0.4\n70.0\n24.5 \u00b1 7.6\n3.6\n72.1\n35.1 \u00b1 7.3\n3.7\nDeConvL\n4.6\n55.2\n11.9 \u00b1 8.2\n7.3\n20.9\n66.3\n38.2 \u00b1 6.9\n17.3\n31.4\n69.2\n42.9 \u00b1 5.5\n11.4\nDeConvS\n\u00b1 2.9\n32.7\n4.0 \u00b1 2.2\n-0.6\n\u00b1 7.4\n68.2\n25.9 \u00b1 6.8\n5\n\u00b1 7.6\n71.6\n35.8 \u00b1 5.5\n4.4\nUNetL\n50.6\n14.5 \u00b1 8.9\n10.0\n66.2\n40.1 \u00b1 6.4\n19.2\n70.3\n46.3 \u00b1 5.5\n14.9\nUNetS\n26.8\n4.6 \u00b1 2.5\n-0.0\n67.1\n28.8 \u00b1 6.8\n7.9\n70.9\n38.3 \u00b1 6.4\n6.9\nResNet50\nConvL\n73.5\n63.5\n3.2 \u00b1 1.7\n0.1\n68.4\n28.8 \u00b1 6.7\n7.6\n72.0\n40.8 \u00b1 7.5\n5.1\nConvS\n65.5\n3.2 \u00b1 1.6\n0.1\n71.9\n23.1 \u00b1 6.9\n1.9\n73.0\n37.4 \u00b1 8.0\n1.7\nDeConvL\n3.0\n59.6\n3.2 \u00b1 1.7\n0.2\n21.3\n68.1\n28.6 \u00b1 7.0\n7.4\n35.7\n71.7\n41.7 \u00b1 7.7\n6.1\nDeConvS\n\u00b1 1.8\n61.1\n3.2 \u00b1 1.7\n0.1\n\u00b1 7.0\n70.3\n25.0 \u00b1 6.7\n3.7\n\u00b1 8.6\n72.8\n38.9 \u00b1 7.9\n3.3\nUNetL\n39.0\n5.0 \u00b1 1.7\n1.9\n66.6\n36.5 \u00b1 6.2\n15.3\n70.8\n45.3 \u00b1 6.7\n9.6\nUNetS\n47.7\n3.4 \u00b1 1.8\n0.3\n69.1\n26.1 \u00b1 6.6\n4.8\n72.6\n39.6 \u00b1 7.8\n3.9\nVGG11\nConvL\n64.8\n58.3\n19.7 \u00b1 11\n11.5\n63.1\n38.8 \u00b1 9.3\n15.0\n63.9\n42.4 \u00b1 9.0\n11.1\nConvS\n56.6\n10.4 \u00b1 7.4\n2.2\n62.7\n27.9 \u00b1 10\n4.0\n63.9\n41.8 \u00b1 8.3\n10.5\nDeConvL\n8.2\n60.3\n21.2 \u00b1 11\n13.0\n23.9\n63.9\n40.0 \u00b1 9.0\n16.2\n31.3\n64.0\n42.8 \u00b1 9.1\n11.5\nDeConvS\n\u00b1 5.7\n58.3\n11.8 \u00b1 7.9\n3.5\n\u00b1 9.4\n61.9\n29.8 \u00b1 9.9\n5.9\n\u00b1 10\n63.5\n36.1 \u00b1 10\n4.8\nUNetL\n51.1\n22.1 \u00b1 8.2\n13.9\n61.8\n37.8 \u00b1 9.0\n13.9\n63.5\n40.9 \u00b1 9.3\n9.6\nUNetS\n51.9\n13.1 \u00b1 7.9\n4.9\n61.7\n29.8 \u00b1 9.7\n6.0\n63.8\n35.7 \u00b1 9.9\n4.5\nVGG16\nConvL\n67.8\n51.4\n19.2 \u00b1 6.0\n12.6\n61.8\n41.1 \u00b1 5.6\n18.7\n64.9\n44.9 \u00b1 5.3\n13.8\nConvS\n44.3\n6.7 \u00b1 2.3\n0.1\n63.8\n27.5 \u00b1 6.8\n5.1\n66.0\n36.3 \u00b1 6.1\n5.1\nDeConvL\n7.0\n53.1\n20.8 \u00b1 6.2\n14.2\n22.4\n62.8\n42.1 \u00b1 5.5\n19.8\n31.1\n65.0\n46.6 \u00b1 5.2\n15.5\nDeConvS\n\u00b1 3.5\n23.5\n4.8 \u00b1 1.7\n-1.8\n\u00b1 7.0\n62.1\n29.9 \u00b1 6.7\n7.5\n\u00b1 7.2\n64.9\n38.1 \u00b1 6.3\n7.0\nUNetL\n50.2\n25.3 \u00b1 1.7\n18.7\n61.7\n41.3 \u00b1 5.0\n18.9\n64.8\n46.8 \u00b1 4.6\n15.7\nUNetS\n27.7\n9.9 \u00b1 2.1\n3.3\n61.6\n31.3 \u00b1 6.3\n8.9\n65.0\n39.8 \u00b1 5.9\n8.7\nVGG19\nConvL\n67.8\n59.4\n29.2 \u00b1 8.1\n18.6\n65.6\n46.5 \u00b1 6.8\n12.5\n66.9\n49.2 \u00b1 7.4\n7.0\nConvS\n63.7\n14.4 \u00b1 5.1\n3.8\n66.6\n38.3 \u00b1 6.8\n4.2\n67.7\n45.3 \u00b1 8.5\n3.2\nDeConvL\n10.6\n60.1\n29.6 \u00b1 8.5\n19.0\n34.0\n65.7\n46.9 \u00b1 7.1\n12.9\n42.1\n67.3\n49.8 \u00b1 7.6\n7.6\nDeConvS\n\u00b1 4.3\n60.9\n16.1 \u00b1 6.0\n5.6\n\u00b1 9.6\n66.5\n39.0 \u00b1 3.7\n5.0\n\u00b1 9.4\n67.7\n45.7 \u00b1 8.4\n3.6\nUNetL\n58.7\n30.2 \u00b1 8.2\n19.6\n65.5\n46.9 \u00b1 6.5\n12.9\n67.4\n50.0 \u00b1 7.5\n7.9\nUNetS\n59.1\n18.0 \u00b1 6.2\n7.4\n66.3\n40.1 \u00b1 8.0\n6.1\n67.5\n46.6 \u00b1 8.4\n4.5\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nNominal (w/o NeuralFuse)\nLow voltage (w/o NeuralFuse)\nNominal (w/ NeuralFuse)\nLow voltage (w/ NeuralFuse)\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n73.7\nAccuracy (%)\n4  \n   .6\n54  \n49  \n55  \n32  \n50  \n26  \n     .8\n     .7\n     .2\n     .7\n     .6\n     .8\n11  \n4  \n11  \n4  \n14  \n4  \n     .0\n   .2\n     .9\n   .0\n     .5\n   .6\nCIFAR-100 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n73.5\n3  \n   .0\n63  \n65  \n59  \n61  \n39  \n47  \n     .5\n     .5\n     .6\n     .1\n     .0\n     .7\n3  \n3  \n3  \n3  \n5  \n3  \n   .2\n   .2\n   .2\n   .2\n   .0\n   .4\nCIFAR-100 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n64.8\n8  \n   .2\n58  \n56  \n60  \n58  \n51  \n51  \n     .3\n     .6\n     .3\n     .3\n     .1\n     .9\n19  \n10  \n21  \n11  \n22  \n13  \n     .7\n     .4\n     .2\n     .8\n     .1\n     .1\nCIFAR-100 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n67.8\n7  \n   .0\n51  \n44  \n53  \n23  \n50  \n27  \n     .4\n     .3\n     .1\n     .5\n     .2\n     .7\n19  \n6  \n20  \n4  \n25  \n9  \n     .2\n   .7\n     .8\n   .8\n     .3\n   .9\nCIFAR-100 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n67.8\n10  \n     .6\n59  \n63  \n60  \n60  \n58  \n59  \n     .4\n     .7\n     .1\n     .9\n     .7\n     .1\n29  \n14  \n29  \n16  \n30  \n18  \n     .2\n     .4\n     .6\n     .1\n     .2\n     .0\nCIFAR-100 pre-trained VGG19\n(a) CIFAR-100, 1% Bit Error Rate\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n73.7\n20  \n     .9\n65  \n70  \n66  \n68  \n66  \n67  \n     .2\n     .0\n     .3\n     .2\n     .2\n     .1\n39  \n24  \n38  \n25  \n40  \n28  \n     .0\n     .5\n     .2\n     .9\n     .1\n     .8\nCIFAR-100 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n73.5\n21  \n     .3\n68  \n71  \n68  \n70  \n66  \n69  \n     .4\n     .9\n     .1\n     .3\n     .6\n     .1\n28  \n23  \n28  \n25  \n36  \n26  \n     .8\n     .1\n     .6\n     .0\n     .5\n     .1\nCIFAR-100 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n64.8\n23  \n     .9\n63  \n62  \n63  \n61  \n61  \n61  \n     .1\n     .7\n     .9\n     .9\n     .8\n     .7\n38  \n27  \n40  \n29  \n37  \n29  \n     .8\n     .9\n     .0\n     .8\n     .8\n     .8\nCIFAR-100 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n67.8\n22  \n     .4\n61  \n63  \n62  \n62  \n61  \n61  \n     .8\n     .8\n     .8\n     .1\n     .7\n     .6\n41  \n27  \n42  \n29  \n41  \n31  \n     .1\n     .5\n     .1\n     .9\n     .3\n     .3\nCIFAR-100 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n67.8\n34  \n     .0\n65  \n66  \n65  \n66  \n65  \n66  \n     .6\n     .6\n     .7\n     .5\n     .5\n     .3\n46  \n38  \n46  \n39  \n46  \n40  \n     .5\n     .3\n     .9\n     .0\n     .9\n     .1\nCIFAR-100 pre-trained VGG19\n(b) CIFAR-100, 0.5% Bit Error Rate\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n73.7\n31  \n     .4\n69  \n72  \n69  \n71  \n70  \n70  \n     .4\n     .1\n     .2\n     .6\n     .3\n     .9\n42  \n35  \n42  \n35  \n46  \n38  \n     .9\n     .1\n     .9\n     .8\n     .3\n     .3\nCIFAR-100 pre-trained ResNet18\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n73.5\n35  \n     .7\n72  \n73  \n71  \n72  \n70  \n72  \n     .0\n     .0\n     .7\n     .8\n     .8\n     .6\n40  \n37  \n41  \n38  \n45  \n39  \n     .8\n     .4\n     .7\n     .9\n     .3\n     .6\nCIFAR-100 pre-trained ResNet50\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n64.8\n31  \n     .3\n63  \n63  \n64  \n63  \n63  \n63  \n     .9\n     .9\n     .0\n     .5\n     .5\n     .8\n42  \n41  \n42  \n36  \n40  \n35  \n     .4\n     .8\n     .8\n     .1\n     .9\n     .7\nCIFAR-100 pre-trained VGG11\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n67.8\n31  \n     .1\n64  \n66  \n65  \n64  \n64  \n65  \n     .9\n     .0\n     .0\n     .9\n     .8\n     .0\n44  \n36  \n46  \n38  \n46  \n39  \n     .9\n     .3\n     .6\n     .1\n     .8\n     .8\nCIFAR-100 pre-trained VGG16\nw/o NF  ConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n67.8\n42  \n     .1\n66  \n67  \n67  \n67  \n67  \n67  \n     .9\n     .7\n     .3\n     .7\n     .4\n     .5\n49  \n45  \n49  \n45  \n50  \n46  \n     .2\n     .3\n     .8\n     .7\n     .0\n     .6\nCIFAR-100 pre-trained VGG19\n(c) CIFAR-100, 0.35% Bit Error Rate\nFigure 11. Experimental results on CIFAR-100.\n20\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nG. Additional Experimental Results on Restricted Access Settings (Transferability)\nWe conduct more experiments with Restricted Access settings to show that our NeuralFuse can be transferred to protect\nvarious black-box models. The experimental results are shown in Sec. G.1 (CIFAR-10), Sec. G.2 (GTSRB), and Sec. G.3\n(CIFAR-100).\nWe find that using VGG19 as a white-box surrogate model has better transferability than ResNet18 for all datasets. In\naddition, we can observe that some NeuralFuse generators have downward applicability if base models have a similar\narchitecture. In other words, if we try to transfer a generator trained on a large B.E.R. (e.g., 1%) to a model with a small\nB.E.R. (e.g., 0.5%), the performance will be better than that of a generator trained with the original B.E.R. (e.g., 0.5%).\nFor example, in Table 16, we could find that if we use VGG19 as a source model to train the generator ConvL (1%), the\ngenerator could deliver better performance (in terms of P.A. (NF)) when applied to similar base models (e.g., VGG11,\nVGG16, or VGG19) under a 0.5% B.E.R., compared to using itself as a source model (shown in Table 12). We conjecture\nthat this is because the generators trained on a larger B.E.R. can also cover the error patterns of a smaller B.E.R., and thus\nthey have better generalizability across smaller B.E.Rs.\nTo further improve the transferability to cross-architecture target models, we also conduct an experiment in Sec. G.4 to show\nthat using ensemble-based training can help the generator to achieve this feature.\nG.1. CIFAR-10\nThe results of CIFAR-10 in which NeuralFuse is trained at 1% B.E.R. are shown in Table 16.\nTable 16. Transfer results on CIFAR-10: NeuralFuse trained on S.M. with 1% B.E.R.\nS.M.\nT.M.\nB.E.R.\nC.A.\nP.A.\nConvL (1%)\nUNetL (1%)\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nResNet18\n0.5%\n92.6\n70.1 \u00b1 11.6\n89.8\n89.5 \u00b1 0.2\n19.4\n86.6\n86.2 \u00b1 0.3\n16.1\nResNet50\n1%\n92.6\n26.1 \u00b1 9.4\n89.5\n36.0 \u00b1 19\n9.9\n85.2\n38.8 \u00b1 19\n12.7\n0.5%\n61.0 \u00b1 10.3\n75.1 \u00b1 10\n14.1\n77.1 \u00b1 5.0\n16.1\nVGG11\n1%\n88.4\n42.2 \u00b1 11.6\n88.4\n62.5 \u00b1 8.4\n20.3\n76.8\n61.1 \u00b1 8.5\n18.9\n0.5%\n63.6 \u00b1 9.3\n81.0 \u00b1 4.6\n17.4\n73.7 \u00b1 3.0\n10.1\nVGG16\n1%\n90.3\n35.7 \u00b1 7.9\n89.6\n63.3 \u00b1 18\n27.6\n85.2\n59.9 \u00b1 16\n24.2\n0.5%\n66.6 \u00b1 8.1\n85.0 \u00b1 3.4\n18.4\n80.2 \u00b1 4.5\n13.6\nVGG19\n1%\n90.5\n36.0 \u00b1 12.0\n89.6\n50.7 \u00b1 22\n14.7\n85.3\n51.1 \u00b1 16\n15.1\n0.5%\n64.2 \u00b1 12.4\n80.2 \u00b1 8.7\n16.0\n76.5 \u00b1 7.8\n12.3\nVGG19\nResNet18\n1%\n92.6\n38.9 \u00b1 12.4\n89.8\n61.0 \u00b1 17\n22.1\n87.0\n69.7 \u00b1 11\n30.8\n0.5%\n70.1 \u00b1 11.6\n86.1 \u00b1 6.9\n16.0\n84.2 \u00b1 3.0\n14.1\nResNet50\n1%\n92.6\n26.1 \u00b1 9.4\n89.9\n34.0 \u00b1 19\n7.9\n87.0\n44.2 \u00b1 17\n18.1\n0.5%\n61.0 \u00b1 10.3\n76.5 \u00b1 10\n15.5\n80.7 \u00b1 4.2\n19.7\nVGG11\n1%\n88.4\n42.2 \u00b1 11.6\n89.7\n76.5 \u00b1 7.0\n34.3\n87.1\n79.9 \u00b1 5.6\n37.7\n0.5%\n63.6 \u00b1 9.3\n88.0 \u00b1 2.1\n24.4\n85.4 \u00b1 0.8\n21.8\nVGG16\n1%\n90.3\n35.7 \u00b1 7.9\n89.6\n75.5 \u00b1 12\n39.8\n87.2\n78.9 \u00b1 7.8\n43.2\n0.5%\n66.6 \u00b1 8.1\n88.9 \u00b1 0.6\n22.3\n86.2 \u00b1 0.3\n19.6\nVGG19\n0.5%\n90.5\n64.2 \u00b1 12.4\n89.8\n89.6 \u00b1 8.7\n25.4\n87.4\n86.8 \u00b1 0.4\n22.6\n[Note] S.M.: source model, used for training generators, T.M.: target model, used for testing generators, B.E.R.: the bit error rate of the target\nmodel, C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\n21\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nG.2. GTSRB\nIn Tables 17 and 18, we show the results on GTSRB in which NeuralFuse is trained at 1.5% and 1% B.E.R., respectively.\nTable 17. Transfer results on GTSRB: NeuralFuse trained on S.M. with 1.5% B.E.R.\nS.M.\nT.M.\nB.E.R.\nC.A.\nP.A.\nConvL (1.5%)\nUNetL (1.5%)\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nResNet18\n1%\n95.5\n36.9 \u00b1 16.0\n95.7\n93.9 \u00b1 1.9\n57.0\n94.9\n94.4 \u00b1 0.4\n57.5\n0.5%\n75.2 \u00b1 12.7\n95.7 \u00b1 0.2\n20.5\n94.8 \u00b1 0.2\n19.6\nResNet50\n1%\n95.0\n29.5 \u00b1 16.9\n94.4\n37.0 \u00b1 22\n7.5\n94.4\n47.1 \u00b1 23\n17.6\n0.5%\n74.0 \u00b1 13.0\n77.5 \u00b1 13\n3.5\n84.8 \u00b1 9.5\n10.8\nVGG11\n1%\n91.9\n34.9 \u00b1 12.4\n92.8\n45.2 \u00b1 10\n10.3\n91.4\n50.5 \u00b1 13\n15.6\n0.5%\n64.9 \u00b1 10.8\n79.4 \u00b1 5.8\n14.5\n83.9 \u00b1 4.2\n19.0\nVGG16\n1%\n95.2\n15.1 \u00b1 6.8\n95.4\n31.1 \u00b1 13\n15.9\n94.6\n36.8 \u00b1 12\n21.7\n0.5%\n58.8 \u00b1 8.9\n84.5 \u00b1 8.3\n25.8\n86.0 \u00b1 8.6\n27.2\nVGG19\n1%\n95.5\n36.6 \u00b1 6.8\n95.0\n56.4 \u00b1 15\n19.8\n94.3\n60.8 \u00b1 15\n24.2\n0.5%\n69.1 \u00b1 11.1\n86.9 \u00b1 3.4\n17.8\n87.7 \u00b1 3.8\n18.6\nVGG19\nResNet18\n1%\n95.5\n36.9 \u00b1 16.0\n88.4\n50.3 \u00b1 12\n13.4\n92.8\n63.7 \u00b1 16\n26.8\n0.5%\n75.2 \u00b1 12.7\n77.9 \u00b1 7.4\n2.7\n87.5 \u00b1 3.9\n12.3\nResNet50\n1%\n95.0\n29.5 \u00b1 16.9\n87.5\n29.7 \u00b1 17\n0.2\n92.5\n40.4 \u00b1 21\n10.9\n0.5%\n74.0 \u00b1 13.0\n67.9 \u00b1 17\n-6.1\n77.5 \u00b1 15\n3.5\nVGG11\n1%\n91.9\n34.9 \u00b1 12.4\n89.7\n47.1 \u00b1 11\n12.2\n93.5\n60.0 \u00b1 12\n25.1\n0.5%\n64.9 \u00b1 10.8\n76.3 \u00b1 5.1\n11.4\n86.0 \u00b1 3.8\n21.1\nVGG16\n1%\n95.2\n15.1 \u00b1 6.8\n93.0\n29.2 \u00b1 15\n14.1\n93.0\n38.5 \u00b1 16\n23.4\n0.5%\n58.8 \u00b1 8.9\n75.7 \u00b1 12\n16.9\n79.9 \u00b1 8.3\n21.1\nVGG19\n1%\n95.5\n36.6 \u00b1 6.8\n95.1\n87.4 \u00b1 6.0\n50.8\n94.6\n88.7 \u00b1 5.0\n52.1\n0.5%\n69.1 \u00b1 11.1\n92.4 \u00b1 2.4\n23.3\n92.4 \u00b1 2.2\n23.3\n[Note] S.M.: source model, used for training generators, T.M.: target model, used for testing generators, B.E.R.: the bit error rate of the target\nmodel, C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nTable 18. Transfer results on GTSRB: NeuralFuse trained on S.M. with 1% B.E.R.\nS.M.\nT.M.\nB.E.R.\nC.A.\nP.A.\nConvL (1%)\nUNetL (1%)\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nResNet18\n0.5%\n95.5\n75.2 \u00b1 12.7\n95.7\n95.3 \u00b1 0.5\n20.1\n96.2\n95.7 \u00b1 0.3\n20.5\nResNet50\n1%\n95.0\n29.5 \u00b1 16.9\n94.5\n35.6 \u00b1 21\n6.1\n95.6\n42.6 \u00b1 23\n13.1\n0.5%\n74.0 \u00b1 13.0\n78.8 \u00b1 13\n4.8\n87.3 \u00b1 9.0\n13.3\nVGG11\n1%\n91.9\n34.9 \u00b1 12.4\n93.1\n45.8 \u00b1 11\n10.9\n94.0\n47.1 \u00b1 14\n12.2\n0.5%\n64.9 \u00b1 10.8\n81.8 \u00b1 5.0\n16.9\n84.2 \u00b1 4.8\n19.3\nVGG16\n1%\n95.2\n15.1 \u00b1 6.8\n95.5\n26.5 \u00b1 12\n11.4\n95.5\n32.4 \u00b1 11\n17.3\n0.5%\n58.8 \u00b1 8.9\n82.2 \u00b1 9.0\n23.4\n85.4 \u00b1 6.7\n26.6\nVGG19\n1%\n95.5\n36.6 \u00b1 6.8\n94.9\n53.2 \u00b1 14\n16.6\n95.6\n60.9 \u00b1 15\n24.3\n0.5%\n69.1 \u00b1 11.1\n85.4 \u00b1 4.5\n16.3\n87.5 \u00b1 3.7\n18.4\nVGG19\nResNet18\n1%\n95.5\n36.9 \u00b1 16.0\n93.7\n53.1 \u00b1 16\n16.2\n95.0\n63.4 \u00b1 18\n26.5\n0.5%\n75.2 \u00b1 12.7\n83.9 \u00b1 7.6\n8.7\n89.7 \u00b1 4.8\n14.5\nResNet50\n1%\n95.0\n29.5 \u00b1 16.9\n92.8\n30.6 \u00b1 18\n1.1\n95.4\n38.9 \u00b1 22\n9.4\n0.5%\n74.0 \u00b1 13.0\n74.7 \u00b1 18\n0.7\n81.5 \u00b1 16\n7.5\nVGG11\n1%\n91.9\n34.9 \u00b1 12.4\n93.7\n50.6 \u00b1 11\n15.7\n95.1\n58.9 \u00b1 15\n24.0\n0.5%\n64.9 \u00b1 10.8\n82.3 \u00b1 5.1\n17.4\n87.5 \u00b1 3.7\n22.6\nVGG16\n1%\n95.2\n15.1 \u00b1 6.8\n95.2\n27.8 \u00b1 15\n12.7\n95.2\n33.5 \u00b1 14\n18.4\n0.5%\n58.8 \u00b1 8.9\n79.0 \u00b1 12\n20.2\n81.8 \u00b1 7.8\n23.0\nVGG19\n0.5%\n95.5\n69.1 \u00b1 11.1\n96.0\n94.0 \u00b1 2.2\n24.9\n95.4\n93.9 \u00b1 2.1\n24.8\n[Note] S.M.: source model, used for training generators, T.M.: target model, used for testing generators, B.E.R.: the bit error rate of the target\nmodel, C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\n22\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nG.3. CIFAR-100\nIn Tables 19 and 20, we show results on CIFAR-100 with NeuralFuse trained at 1% and 0.5% B.E.R., respectively.\nTable 19. Transfer results on CIFAR-100: NeuralFuse trained on S.M. with 1% B.E.R.\nS.M.\nT.M.\nB.E.R.\nC.A.\nP.A.\nConvL (1%)\nUNetL (1%)\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nResNet18\n0.5%\n73.7\n20.9 \u00b1 7.4\n54.8\n35.8 \u00b1 5.2\n14.9\n50.6\n39.3 \u00b1 2.8\n18.4\n0.35%\n31.4 \u00b1 7.6\n41.7 \u00b1 3.7\n10.3\n43.3 \u00b1 1.4\n11.9\nResNet50\n1%\n73.5\n3.0 \u00b1 1.8\n44.9\n2.2 \u00b1 2.0\n-0.8\n41.5\n2.4 \u00b1 1.9\n-0.6\n0.5%\n21.3 \u00b1 7.0\n15.9 \u00b1 8.2\n-5.4\n17.1 \u00b1 7.1\n-4.2\n0.35%\n35.7 \u00b1 8.6\n23.7 \u00b1 7.1\n-12.0\n26.2 \u00b1 5.6\n-9.5\nVGG11\n1%\n64.8\n8.2 \u00b1 5.7\n41.2\n9.8 \u00b1 5.6\n1.6\n37.5\n10.2 \u00b1 5.1\n2.0\n0.5%\n23.9 \u00b1 9.4\n24.2 \u00b1 5.9\n0.3\n24.5 \u00b1 4.7\n0.6\n0.35%\n31.3 \u00b1 10.0\n29.0 \u00b1 5.4\n-2.3\n28.2 \u00b1 4.5\n-3.1\nVGG16\n1%\n67.8\n7.0 \u00b1 3.5\n44.0\n7.9 \u00b1 3.7\n0.9\n39.5\n10.1 \u00b1 4.5\n3.1\n0.5%\n22.4 \u00b1 7.0\n22.4 \u00b1 7.6\n0.0\n26.3 \u00b1 5.3\n3.9\n0.35%\n31.1 \u00b1 7.2\n28.1 \u00b1 5.9\n-3.0\n30.6 \u00b1 3.6\n-0.5\nVGG19\n1%\n67.8\n10.6 \u00b1 4.3\n44.2\n13.5 \u00b1 6.1\n2.9\n40.7\n15.6 \u00b1 6.2\n5.0\n0.5%\n34.0 \u00b1 9.6\n27.9 \u00b1 4.8\n-6.1\n29.3 \u00b1 4.6\n-4.7\n0.35%\n42.1 \u00b1 9.4\n33.2 \u00b1 48\n-8.9\n32.8 \u00b1 3.9\n-9.3\nVGG19\nResNet18\n1%\n73.7\n4.6 \u00b1 2.9\n55.5\n5.8 \u00b1 3.7\n1.2\n57.3\n6.8 \u00b1 4.4\n2.2\n0.5%\n20.9 \u00b1 7.4\n24.6 \u00b1 6.3\n3.7\n28.1 \u00b1 5.9\n7.2\n0.35%\n31.4 \u00b1 7.6\n31.1 \u00b1 5.0\n-0.3\n36.4 \u00b1 4.5\n5.0\nResNet50\n1%\n73.5\n3.0 \u00b1 1.8\n56.1\n2.8 \u00b1 2.1\n-0.2\n56.1\n3.7 \u00b1 2.4\n0.7\n0.5%\n21.3 \u00b1 7.0\n18.9 \u00b1 8.6\n-2.4\n22.8 \u00b1 8.5\n1.5\n0.35%\n35.7 \u00b1 8.6\n28.7 \u00b1 8.2\n-7.0\n33.7 \u00b1 7.0\n-2.0\nVGG11\n1%\n64.8\n8.2 \u00b1 5.7\n52.8\n12.3 \u00b1 8.4\n4.1\n53.9\n15.4 \u00b1 9.4\n7.2\n0.5%\n23.9 \u00b1 9.4\n30.0 \u00b1 9.3\n6.1\n33.3 \u00b1 7.2\n9.4\n0.35%\n31.3 \u00b1 10.0\n36.5 \u00b1 7.7\n5.2\n38.8 \u00b1 6.5\n7.5\nVGG16\n1%\n67.8\n7.0 \u00b1 3.5\n53.6\n11.2 \u00b1 4.4\n4.2\n55.2\n13.6 \u00b1 5.2\n6.6\n0.5%\n22.4 \u00b1 7.0\n32.4 \u00b1 7.3\n10.0\n35.9 \u00b1 6.2\n13.5\n0.35%\n31.1 \u00b1 7.2\n39.4 \u00b1 6.3\n8.3\n42.4 \u00b1 4.9\n11.3\nVGG19\n0.5%\n67.8\n34.0 \u00b1 9.6\n59.4\n50.2 \u00b1 3.1\n16.2\n58.7\n49.1 \u00b1 3.5\n15.1\n0.35%\n42.1 \u00b1 9.4\n53.1 \u00b1 2.3\n11.0\n52.0 \u00b1 3.1\n9.9\n[Note] S.M.: source model, used for training generators, T.M.: target model, used for testing generators, B.E.R.: the bit error rate of the target model, C.A.\n(%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nTable 20. Transfer results on CIFAR-100: NeuralFuse trained on S.M. with 0.5% B.E.R.\nS.M.\nT.M.\nB.E.R.\nC.A.\nP.A.\nConvL (0.5%)\nUNetL (0.5%)\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\nResNet18\n0.35%\n73.7\n31.4 \u00b1 7.6\n65.2\n47.7 \u00b1 4.9\n16.3\n66.2\n49.2 \u00b1 4.1\n17.8\nResNet50\n0.5%\n73.5\n21.3 \u00b1 7.0\n62.5\n24.0 \u00b1 9.9\n2.8\n63.5\n26.4 \u00b1 9.1\n5.1\n0.35%\n35.7 \u00b1 8.6\n36.3 \u00b1 8.9\n0.6\n39.4 \u00b1 8.1\n3.7\nVGG11\n0.5%\n64.8\n23.9 \u00b1 9.4\n59.2\n33.0 \u00b1 9.8\n9.2\n61.1\n34.2 \u00b1 9.8\n10.3\n0.35%\n31.3 \u00b1 10.0\n40.4 \u00b1 8.7\n9.1\n41.4 \u00b1 9.0\n10.1\nVGG16\n0.5%\n67.8\n22.4 \u00b1 7.0\n59.5\n34.7 \u00b1 8.0\n12.3\n61.4\n37.5 \u00b1 6.8\n15.2\n0.35%\n31.1 \u00b1 7.2\n42.9 \u00b1 6.0\n11.8\n45.3 \u00b1 4.9\n14.2\nVGG19\n0.5%\n67.8\n34.0 \u00b1 9.6\n61.6\n43.7 \u00b1 6.2\n9.6\n62.0\n45.0 \u00b1 6.3\n11.0\n0.35%\n42.1 \u00b1 9.4\n49.0 \u00b1 5.5\n6.8\n50.5 \u00b1 5.3\n8.3\nVGG19\nResNet18\n0.5%\n73.7\n20.9 \u00b1 7.4\n66.1\n24.9 \u00b1 6.7\n4.0\n67.8\n27.7 \u00b1 6.8\n6.8\n0.35%\n31.4 \u00b1 7.6\n34.4 \u00b1 5.4\n3.0\n38.1 \u00b1 5.6\n6.7\nResNet50\n0.5%\n73.5\n21.3 \u00b1 7.0\n66.2\n22.7 \u00b1 7.8\n1.4\n66.7\n25.4 \u00b1 8.0\n4.2\n0.35%\n35.7 \u00b1 8.6\n35.5 \u00b1 7.7\n-0.2\n38.8 \u00b1 7.5\n3.2\nVGG11\n0.5%\n64.8\n23.9 \u00b1 9.4\n59.9\n29.3 \u00b1 10\n5.4\n61.0\n31.2 \u00b1 9.8\n7.4\n0.35%\n31.3 \u00b1 10.0\n36.6 \u00b1 9.5\n5.3\n38.1 \u00b1 9.0\n6.8\nVGG16\n0.5%\n67.8\n22.4 \u00b1 7.0\n62.5\n30.8 \u00b1 7.3\n8.4\n62.6\n33.0 \u00b1 7.3\n10.7\n0.35%\n31.1 \u00b1 7.2\n40.0 \u00b1 6.5\n8.9\n42.5 \u00b1 5.9\n11.3\nVGG19\n0.35%\n67.8\n42.1 \u00b1 9.4\n65.6\n52.0 \u00b1 6.2\n9.8\n65.5\n52.6 \u00b1 6.1\n10.4\n[Note] S.M.: source model, used for training generators, T.M.: target model, used for testing generators, B.E.R.: the bit error rate of the target model, C.A.\n(%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\n23\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nG.4. Generator Ensembling\nTo improve the transferability performance on cross-architecture cases (e.g., using ResNet-based models as surrogate models\nto train NeuralFuse and then transfer NeuralFuse to VGG-based target models), we try to adopt ensemble surrogate models\nto train our NeuralFuse. The experimental results are shown in Table 21. We use the same experimental settings mentioned\nin Table 1 but change one source model (e.g., ResNet18 or VGG19) into two (ResNet18 with VGG19) for training. The\nresults show that the overall performance is better than the results shown in Table 1, which means ensemble-based training\ncan easily solve the performance degradation on cross-architecture target models.\nTable 21. Transfer results on CIFAR-10: NeuralFuse trained on two S.M. with 1.5% B.E.R.\nS.M.\nT.M.\nB.E.R.\nC.A.\nP.A.\nConvL (1.5%)\nUNetL (1.5%)\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\n+\nVGG19\nResNet18\n1%\n92.6\n38.9 \u00b1 12.4\n89.4\n88.1 \u00b1 1.0\n49.2\n86.3\n85.4 \u00b1 0.5\n46.5\n0.5%\n70.1 \u00b1 11.6\n89.2 \u00b1 0.2\n19.1\n86.1 \u00b1 0.2\n16.0\nResNet50\n1%\n92.6\n26.1 \u00b1 9.4\n89.3\n44.0 \u00b1 22\n17.9\n86.1\n50.9 \u00b1 20\n24.8\n0.5%\n61.0 \u00b1 10.3\n80.3 \u00b1 6.7\n19.3\n78.6 \u00b1 3.9\n17.6\nVGG11\n1%\n88.4\n42.2 \u00b1 11.6\n89.1\n77.0 \u00b1 5.6\n34.8\n85.9\n82.3 \u00b1 4.1\n40.1\n0.5%\n63.6 \u00b1 9.3\n87.5 \u00b1 1.6\n23.9\n85.0 \u00b1 0.6\n21.4\nVGG16\n1%\n90.3\n35.7 \u00b1 7.9\n89.1\n80.5 \u00b1 8.6\n44.8\n85.7\n81.4 \u00b1 5.5\n45.7\n0.5%\n66.6 \u00b1 8.1\n88.2 \u00b1 0.7\n21.6\n85.0 \u00b1 0.7\n18.4\nVGG19\n1%\n90.5\n36.0 \u00b1 12.0\n89.2\n75.1 \u00b1 17\n39.1\n86.1\n83.0 \u00b1 3.4\n47.0\n0.5%\n64.2 \u00b1 12.4\n89.0 \u00b1 0.2\n24.8\n85.9 \u00b1 0.4\n21.7\n[Note] S.M.: source model, used for training generators, T.M.: target model, used for testing generators, B.E.R.: the bit error rate of the target\nmodel, C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nH. NeuralFuse on Reduced-precision Quantization and Random Bit Errors\nAs mentioned in Sec. 4.6, we explore the robustness of NeuralFuse to low-precision quantization of model weights and\nconsider the case of random bit errors. Here, we demonstrate that NeuralFuse can recover not only the accuracy drop due\nto reduced precision, but also the drop caused by low-voltage-induced bit errors (0.5% B.E.R.) under low precision. We\nselected two NeuralFuse generators (ConvL and UNetL) for our experiments, and these generators were trained with the\ncorresponding base models (ResNet18 and VGG19) at 1% B.E.R. (CIFAR-10, GTSRB) and 0.5% B.E.R. (ImageNet-10).\nThe experimental results are shown as follows: CIFAR-10 (Sec. H.1), GTSRB (Sec. H.2), and ImageNet-10 (Sec. H.3).\nSimilarly, for ease of comparison, we visualize the experimental results in the figures below each table. Our results show\nthat NeuralFuse can consistently perform well in low-precision regimes as well as recover the low-voltage-induced accuracy\ndrop.\nH.1. CIFAR-10\nTable 22. Reduced-precision Quantization and with 0.5% B.E.R. on CIFAR-10 pre-trained models.\nBase\n#Bits\nC.A.\nP.A.\nConvL (1%)\nUNetL (1%)\nModel\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\n8\n92.6\n70.1 \u00b1 11.6\n89.8\n89.5 \u00b1 0.2\n19.4\n86.6\n86.2 \u00b1 0.3\n16.1\n7\n92.5\n68.8 \u00b1 10.4\n89.8\n89.5 \u00b1 1.7\n20.7\n86.5\n86.0 \u00b1 0.5\n17.2\n6\n92.6\n68.4 \u00b1 11.2\n89.7\n89.5 \u00b1 0.2\n21.1\n86.6\n85.9 \u00b1 0.3\n17.5\n5\n92.4\n52.7 \u00b1 14.1\n89.7\n90.0 \u00b1 0.7\n37.3\n86.5\n85.5 \u00b1 0.8\n32.8\n4\n91.8\n26.3 \u00b1 12.7\n89.8\n58.7 \u00b1 24.5\n32.4\n86.6\n64.9 \u00b1 22.5\n38.6\n3\n84.8\n11.3 \u00b1 1.8\n89.8\n12.8 \u00b1 5.8\n1.5\n86.0\n14.8 \u00b1 10.0\n3.5\n2\n10.0\n10.0 \u00b1 0.0\n10.0\n10.0 \u00b1 0.0\n0.0\n10.0\n10.0 \u00b1 0.0\n0.0\nVGG19\n8\n90.5\n64.2 \u00b1 12.4\n89.8\n89.6 \u00b1 8.7\n25.4\n87.4\n86.8 \u00b1 0.4\n22.6\n7\n90.3\n66.5 \u00b1 8.5\n89.8\n89.6 \u00b1 0.2\n23.1\n87.4\n86.7 \u00b1 0.3\n20.2\n6\n90.1\n59.8 \u00b1 13.2\n89.9\n89.4 \u00b1 3.8\n29.6\n87.4\n86.4 \u00b1 0.7\n26.6\n5\n90.2\n37.7 \u00b1 14.1\n89.8\n78.0 \u00b1 15.8\n40.3\n87.2\n79.8 \u00b1 0.8\n42.1\n4\n87.5\n14.7 \u00b1 6.0\n89.8\n27.8 \u00b1 18.9\n13.1\n87.2\n34.4 \u00b1 20.5\n19.7\n3\n78.3\n10.5 \u00b1 1.5\n89.7\n10.9 \u00b1 2.6\n0.4\n86.8\n11.0 \u00b1 2.9\n0.5\n2\n10.0\n10.0 \u00b1 0.0\n10.0\n10.0 \u00b1 0.0\n0.0\n10.0\n10.0 \u00b1 0.0\n0.0\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF)\nv.s. P.A.\n24\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nOriginal Accuracy\nw/o NeuralFuse\nNeuralFuse (ConvL)\nNeuralFuse (UNetL)\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\n92.6\nAccuracy (%)\n 92   \n 92   \n 92   \n 92   \n 91   \n 84   \n     .6\n     .5\n     .6\n     .4\n     .8\n     .8\n 90   \n 90   \n 90   \n 90   \n 90   \n 89   \n     .4\n     .4\n     .4\n     .4\n     .0\n     .9\n 89   \n 89   \n 89   \n 89   \n 89   \n 88   \n     .7\n     .6\n     .7\n     .7\n     .7\n     .9\n 10   \n     .0 10   \n     .0 10   \n     .0\nCIFAR-10 pre-trained ResNet18 (Nominal voltage, No Bit Error)\n(a) Base Model: ResNet18, no bit error.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\nAccuracy (%)\n 70   \n 68   \n 68   \n 52   \n 26   \n 11   \n     .1\n     .8\n     .4\n     .7\n     .3\n     .3\n 87   \n 87   \n 87   \n 78   \n 40   \n 11   \n     .9\n     .5\n     .0\n     .8\n     .0\n     .6\n 88   \n 85   \n 84   \n 77   \n 42   \n 12   \n     .1\n     .2\n     .9\n     .4\n     .4\n     .6  10   \n     .0 10   \n     .0 10   \n     .0\nCIFAR-10 pre-trained ResNet18 (Low voltage, 0.5% B.E.R.)\n(b) Base Model: ResNet18, 0.5% B.E.R.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\n90.5\nAccuracy (%)\n 90   \n 90   \n 90   \n 90   \n 87   \n 78   \n     .5\n     .3\n     .1\n     .2\n     .5\n     .3\n 90   \n 90   \n 90   \n 90   \n 90   \n 89   \n     .4\n     .3\n     .4\n     .3\n     .1\n     .4\n 89   \n 89   \n 89   \n 88   \n 88   \n 87   \n     .1\n     .1\n     .2\n     .9\n     .6\n     .3\n 10   \n     .0 10   \n     .0 10   \n     .0\nCIFAR-10 pre-trained VGG19 (Nominal voltage, No Bit Error)\n(c) Base Model: VGG19, no bit error.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\nAccuracy (%)\n 64   \n 66   \n 59   \n 37   \n 14   \n 10   \n     .2\n     .5\n     .8\n     .7\n     .7\n     .5\n 88   \n 88   \n 85   \n 60   \n 19   \n 10   \n     .1\n     .0\n     .5\n     .1\n     .5\n     .4\n 85   \n 84   \n 82   \n 60   \n 19   \n 10   \n     .0\n     .7\n     .0\n     .2\n     .8\n     .9  10   \n     .0 10   \n     .0 10   \n     .0\nCIFAR-10 pre-trained VGG19 (Low voltage, 0.5% B.E.R.)\n(d) Base Model: VGG19, 0.5% B.E.R.\nFigure 12. Results of Reduced-precision and bit errors (0.5%) on CIFAR-10 pre-trained base models.\nH.2. GTSRB\nTable 23. Reduced-precision Quantization and with 0.5% B.E.R. on GTSRB pre-trained models.\nBase\n#Bits\nC.A.\nP.A.\nConvL (1%)\nUNetL (1%)\nModel\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\n8\n95.5\n75.2 \u00b1 12.7\n95.7\n95.3 \u00b1 0.5\n20.1\n96.2\n95.7 \u00b1 0.3\n20.5\n7\n95.5\n69.5 \u00b1 10.6\n95.7\n95.3 \u00b1 0.3\n25.8\n96.2\n95.9 \u00b1 0.3\n26.4\n6\n95.4\n67.2 \u00b1 14.4\n95.7\n95.2 \u00b1 0.5\n28.0\n96.2\n95.7 \u00b1 0.5\n28.5\n5\n95.4\n48.6 \u00b1 18.2\n95.8\n92.6 \u00b1 5.1\n44.0\n96.2\n94.8 \u00b1 2.5\n46.2\n4\n92.6\n24.6 \u00b1 9.8\n95.9\n75.6 \u00b1 16.2\n51.0\n96.2\n86.6 \u00b1 9.5\n62.0\n3\n67.7\n5.3 \u00b1 3.5\n95.4\n18.4 \u00b1 15.3\n13.1\n96.2\n25.3 \u00b1 22.5\n20.0\n2\n3.8\n3.8 \u00b1 0.0\n4.1\n3.8 \u00b1 0.0\n0.0\n3.8\n3.8 \u00b1 0.0\n0.0\nVGG19\n8\n95.5\n69.1 \u00b1 11.1\n96.0\n94.0 \u00b1 2.2\n24.9\n95.4\n93.9 \u00b1 2.1\n24.8\n7\n95.6\n66.1 \u00b1 14.8\n96.0\n92.2 \u00b1 5.7\n26.1\n95.4\n92.6 \u00b1 3.7\n26.5\n6\n95.3\n64.2 \u00b1 8.4\n96.0\n92.2 \u00b1 5.7\n28.0\n95.4\n92.3 \u00b1 2.3\n28.1\n5\n95.2\n48.2 \u00b1 14.0\n96.0\n92.2 \u00b1 5.7\n44.0\n95.4\n86.2 \u00b1 8.4\n38.0\n4\n92.0\n18.2 \u00b1 14.3\n93.0\n92.2 \u00b1 5.7\n74.0\n95.0\n49.6 \u00b1 22.8\n31.4\n3\n60.0\n2.0 \u00b1 0.9\n87.3\n92.2 \u00b1 5.7\n90.2\n87.2\n1.7 \u00b1 0.9\n-0.3\n2\n5.9\n3.8 \u00b1 0.0\n5.9\n3.8 \u00b1 0.0\n0.0\n5.9\n3.8 \u00b1 0.0\n0.0\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nOriginal Accuracy\nw/o NeuralFuse\nNeuralFuse (ConvL)\nNeuralFuse (UNetL)\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\n95.5\nAccuracy (%)\n 95   \n 95   \n 95   \n 95   \n 92   \n 67   \n     .5\n     .5\n     .4\n     .4\n     .6\n     .7\n 93   \n 93   \n 93   \n 93   \n 93   \n 92   \n     .4\n     .3\n     .3\n     .3\n     .3\n     .0\n 96   \n 96   \n 96   \n 96   \n 96   \n 95   \n     .2\n     .2\n     .2\n     .2\n     .1\n     .7\n3  \n   .8 3  \n   .8 3  \n   .8\nGTSRB pre-trained ResNet18 (Nominal voltage, No Bit Error)\n(a) Base Model: ResNet18, no bit error.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\nAccuracy (%)\n 75   \n 69   \n 67   \n 48   \n 24   \n5  \n     .2\n     .5\n     .2\n     .6\n     .6\n   .3\n 89   \n 89   \n 88   \n 79   \n 52   \n9  \n     .5\n     .9\n     .9\n     .3\n     .1\n   .7\n 93   \n 94   \n 93   \n 87   \n 62   \n 12   \n     .5\n     .5\n     .5\n     .5\n     .2\n     .6\n3  \n   .8 3  \n   .8 3  \n   .8\nGTSRB pre-trained ResNet18 (Low voltage, 0.5% B.E.R.)\n(b) Base Model: ResNet18, 0.5% B.E.R.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\n95.5\nAccuracy (%)\n 95   \n 95   \n 95   \n 95   \n 92   \n 60   \n     .5\n     .6\n     .3\n     .2\n     .0\n     .0\n 95   \n 95   \n 95   \n 95   \n 95   \n 91   \n     .6\n     .6\n     .6\n     .5\n     .1\n     .0\n 94   \n 94   \n 95   \n 94   \n 94   \n 90   \n     .9\n     .9\n     .0\n     .7\n     .3\n     .8\n5  \n   .9 5  \n   .9 5  \n   .9\nGTSRB pre-trained VGG19 (Nominal voltage, No Bit Error)\n(c) Base Model: VGG19, no bit error.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\nAccuracy (%)\n 69   \n 66   \n 64   \n 48   \n 18   \n2  \n     .1\n     .1\n     .2\n     .2\n     .2\n   .0\n 93   \n 90   \n 91   \n 80   \n 37   \n1  \n     .4\n     .4\n     .3\n     .2\n     .1\n   .8\n 91   \n 90   \n 89   \n 77   \n 39   \n1  \n     .7\n     .0\n     .6\n     .6\n     .0\n   .8\n3  \n   .8 3  \n   .8 3  \n   .8\nGTSRB pre-trained VGG19 (Low voltage, 0.5% B.E.R.)\n(d) Base Model: VGG19, 0.5% B.E.R.\nFigure 13. Results of Reduced-precision and bit errors (0.5%) on GTSRB pre-trained base models.\n25\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nH.3. ImageNet-10\nTable 24. Reduced-precision Quantization and with 0.5% B.E.R. on ImageNet-10 pre-trained models.\nBase\n#Bits\nC.A.\nP.A.\nConvL (0.5%)\nUNetL (0.5%)\nModel\nC.A. (NF)\nP.A. (NF)\nR.P.\nC.A. (NF)\nP.A. (NF)\nR.P.\nResNet18\n8\n92.2\n72.3 \u00b1 7.0\n94.0\n88.0 \u00b1 2.0\n15.7\n94.0\n88.1 \u00b1 1.4\n15.8\n7\n92.4\n70.6 \u00b1 13.0\n94.2\n86.7 \u00b1 4.1\n16.1\n93.6\n87.8 \u00b1 3.5\n17.2\n6\n92.4\n68.9 \u00b1 9.9\n94.2\n85.1 \u00b1 4.8\n16.2\n93.6\n86.4 \u00b1 3.7\n17.5\n5\n91.0\n60.9 \u00b1 13.0\n94.2\n82.5 \u00b1 6.8\n21.6\n94.0\n83.2 \u00b1 5.9\n22.3\n4\n91.4\n47.4 \u00b1 9.8\n93.8\n68.6 \u00b1 9.8\n21.2\n92.6\n68.7 \u00b1 9.2\n21.3\n3\n85.2\n28.8 \u00b1 11.8\n89.2\n44.1 \u00b1 14.0\n15.3\n89.4\n42.7 \u00b1 14.2\n13.9\n2\n10.0\n10.0 \u00b1 0.0\n10.0\n10.0 \u00b1 0.0\n0.0\n10.0\n10.0 \u00b1 0.0\n0.0\nVGG19\n8\n92.4\n37.2 \u00b1 11.0\n91.4\n75.5 \u00b1 8.8\n38.3\n89.4\n77.9 \u00b1 6.1\n40.7\n7\n92.0\n27.3 \u00b1 6.6\n91.2\n59.3 \u00b1 13.0\n32.0\n89.4\n65.4 \u00b1 10.0\n38.1\n6\n92.4\n27.9 \u00b1 6.4\n91.0\n59.7 \u00b1 11.8\n31.8\n89.4\n64.9 \u00b1 9.9\n37.0\n5\n92.0\n15.1 \u00b1 4.4\n91.6\n23.1 \u00b1 0.7\n8.0\n89.0\n27.9 \u00b1 8.8\n12.8\n4\n89.4\n12.2 \u00b1 2.7\n90.8\n14.0 \u00b1 4.3\n1.8\n89.6\n14.6 \u00b1 4.9\n2.4\n3\n46.8\n9.9 \u00b1 0.5\n83.2\n10.4 \u00b1 0.6\n0.5\n84.2\n9.9 \u00b1 0.7\n0.0\n2\n10.0\n10.0 \u00b1 0.0\n10.0\n10.0 \u00b1 0.0\n0.0\n10.0\n10.0 \u00b1 0.0\n0.0\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: NeuralFuse, and R.P.: total recover percentage of P.A. (NF)\nv.s. P.A.\nOriginal Accuracy\nw/o NeuralFuse\nNeuralFuse (ConvL)\nNeuralFuse (UNetL)\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\n92.2\nAccuracy (%)\n 92   \n 92   \n 92   \n 91   \n 91   \n 85   \n     .2\n     .4\n     .4\n     .0\n     .4\n     .2\n 94   \n 94   \n 94   \n 94   \n 93   \n 89   \n     .0\n     .2\n     .2\n     .2\n     .8\n     .2\n 94   \n 93   \n 93   \n 94   \n 92   \n 89   \n     .0\n     .6\n     .6\n     .0\n     .6\n     .4\n 10   \n     .0 10   \n     .0 10   \n     .0\nImageNet-10 pre-trained ResNet18 (Nominal voltage, No Bit Error)\n(a) Base Model: ResNet18, no bit error.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\nAccuracy (%)\n 72   \n 70   \n 68   \n 60   \n 47   \n 28   \n     .3\n     .6\n     .9\n     .9\n     .4\n     .8\n 88   \n 86   \n 85   \n 82   \n 68   \n 44   \n     .0\n     .7\n     .1\n     .5\n     .6\n     .1\n 88   \n 87   \n 86   \n 83   \n 68   \n 42   \n     .1\n     .8\n     .4\n     .2\n     .7\n     .7\n 10   \n     .0 10   \n     .0 10   \n     .0\nImageNet-10 pre-trained ResNet18 (Low voltage, 0.5% B.E.R.)\n(b) Base Model: ResNet18, 0.5% B.E.R.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\n92.4\nAccuracy (%)\n 92   \n 92   \n 92   \n 92   \n 89   \n 46   \n     .4\n     .0\n     .4\n     .0\n     .4\n     .8\n 91   \n 91   \n 91   \n 91   \n 90   \n 83   \n     .4\n     .2\n     .0\n     .6\n     .8\n     .2\n 89   \n 89   \n 89   \n 89   \n 89   \n 84   \n     .4\n     .4\n     .4\n     .0\n     .6\n     .2\n 10   \n     .0 10   \n     .0 10   \n     .0\nImageNet-10 pre-trained VGG19 (Nominal voltage, No Bit Error)\n(c) Base Model: VGG19, no bit error.\n8\n7\n6\n5\n4\n3\n2\nBit Quantization Number\nAccuracy (%)\n 37   \n 27   \n 27   \n 15   \n 12   \n9  \n     .2\n     .3\n     .9\n     .1\n     .2\n   .9\n 75   \n 59   \n 59   \n 23   \n 14   \n 10   \n     .5\n     .3\n     .7\n     .1\n     .0\n     .4\n 77   \n 65   \n 64   \n 27   \n 14   \n9  \n     .9\n     .4\n     .9\n     .9\n     .6\n   .9  10   \n     .0 10   \n     .0 10   \n     .0\nImageNet-10 pre-trained VGG19 (Low voltage, 0.5% B.E.R.)\n(d) Base Model: VGG19, 0.5% B.E.R.\nFigure 14. Results of Reduced-precision and bit errors (0.5%) on ImageNet-10 pre-trained base models.\nI. Additional Experiments on Adversarial Training\nAdversarial training is a common strategy to derive a robust neural network against certain perturbations. By training the\ngenerator using adversarial training proposed in Stutz et al. (2021), we report its performance against low voltage-induced\nbit errors. We use ConvL as the generator and ResNet18 as the base model, trained on CIFAR-10. Furthermore, we explore\ndifferent K flip bits as the perturbation on weights of the base model during adversarial training, and then for evaluation, the\ntrained-generator will be applied against 1% of bit errors rate on the base model. The results are shown in Table 25. After\ncareful tuning of hyperparameters, we find that we are not able to obtain satisfactory recovery when adopting adversarial\ntraining. Empirically, we argue that adversarial training may not be suitable for training generator-based methods.\n26\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nTable 25. Performance of the generator trained by adversarial training under K flip bits on ResNet18 with CIFAR-10. The results show\nthat the generator trained by adversarial training cannot achieve high accuracy against bit errors under a 1% bit error rate.\nK-bits\nC.A.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\n100\n92.6\n38.9 \u00b1 12.4\n92.4\n38.3 \u00b1 12.1\n-0.6\n500\n92.1\n38.7 \u00b1 12.5\n-0.2\n5,000\n92.6\n38.9 \u00b1 12.5\n0\n20,000\n60.1\n23.0 \u00b1 8.1\n-16\n100,000\n71.1\n23.6 \u00b1 6.6\n-16\n[Note] C.A. (%): clean accuracy, P.A. (%): perturbed accuracy, NF: Neural-\nFuse, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nJ. Additional Experiments on Robust Model Trained with Adversarial Weight Perturbation with\nNeuralFuse\nPreviously, Wu et al. (2020) proposed that one could obtain a more robust model via adversarial weight perturbation. To\nseek whether such models could also be robust to random bit errors, we conducted an experiment on CIFAR-10 with the\nproposed adversarially trained PreAct ResNet18. The experimental results are shown in Table 26. We find that the average\nperturbed accuracy is 23% and 63.2% for PreAct ResNet18 under 1% and 0.5% B.E.R., respectively. This result is lower\nthan 38.9% and 70.1% from ResNet18 in Table 12, indicating their poor generalization ability against random bit errors.\nNevertheless, when equipped NeuralFuse on the perturbed model, we could still witness a significant recover percentage\nunder both 1% and 0.5% B.E.R. This result further demonstrates that NeuralFuse could be adapted to various models (i.e.,\ntrained in different learning algorithms).\nTable 26. Performance of NeuralFuse trained with rubust CIFAR-10 pre-trained PreAct ResNet18. The\nresults show that NeuralFuse can be used together with a robust model and further improve perturbed\naccuracy under both 1% and 0.5% B.E.R.\nBase Model\nB.E.R.\nNF\nC.A.\nP.A.\nC.A. (NF)\nP.A. (NF)\nR.P.\nPreAct\nResNet18\n1%\nConvL\n89.7\n23.0 \u00b1 9.3\n87.6\n53.7 \u00b1 26\n30.7\nConvS\n83.1\n34.6 \u00b1 15\n11.6\nDeConvL\n87.7\n55.4 \u00b1 27\n32.4\nDeConvS\n82.9\n32.4 \u00b1 14\n9.4\nUNetL\n86.1\n60.4 \u00b1 28\n37.4\nUNetS\n80.4\n51.9 \u00b1 24\n28.9\n0.5%\nConvL\n89.7\n63.2 \u00b1 8.7\n89.2\n87.8 \u00b1 1.1\n24.6\nConvS\n89.2\n74.0 \u00b1 6.5\n10.8\nDeConvL\n89.0\n87.4 \u00b1 1.1\n24.2\nDeConvS\n89.9\n74.4 \u00b1 7.0\n11.2\nUNetL\n87.5\n85.9 \u00b1 0.8\n22.7\nUNetS\n88.2\n80.4 \u00b1 3.9\n17.2\n[Note] B.E.R.: the bit error rate of the base model, NF: NeuralFuse, C.A. (%): clean accuracy, P.A. (%):\nperturbed accuracy, and R.P.: total recover percentage of P.A. (NF) v.s. P.A.\nK. Data Embeddings Visualization\nTo further understand how our proposed NeuralFuse works, we visualize the output distribution from the final linear layer of\nthe base models and project the results onto the 2D space using t-SNE (van der Maaten & Hinton, 2008). Figure 15 shows\nthe output distribution from ResNet18 (trained on CIFAR-10) under a 1% bit error rate. We chose two generators that have\nsimilar architecture: ConvL and ConvS, for this experiment. We can observe that: (a) The output distribution of the clean\nmodel without NeuralFuse can be grouped into 10 classes denoted by different colors. (b) The output distribution of the\nperturbed model under a 1% bit error rate without NeuralFuse shows mixed representations and therefore degraded accuracy.\n(c) The output distribution of the clean model with ConvL shows that applying NeuralFuse will not hurt the prediction of\n27\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nthe clean model too much (i.e., it retains high accuracy in the regular voltage setting). (d) The output distribution of the\nperturbed model with ConvL shows high separability (and therefore high perturbed accuracy) as opposed to (b). (e)/(f)\nshows the output distribution of the clean/perturbed model with ConvS. For both (e) and (f), we can see nosier clustering\nwhen compared to (c) and (d), which means the degraded performance of ConvS compared to ConvL. The visualization\nvalidates that NeuralFuse can help retain good data representations under random bit errors and that larger generators in\nNeuralFuse have better performance than smaller ones.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 15. t-SNE results for ResNet18 trained by CIFAR-10 under 1% of bit error rate. (a) Clean model. (b) Perturbed model. (c) Clean\nmodel with ConvL. (d) Perturbed model with ConvL. (e) Clean model with ConvS. (f) Perturbed model with ConvS.\nL. Qualitative Analysis of Transformed Inputs\nIn this section, we conduct a qualitative study to visualize the images transformed by NeuralFuse and present some properties\nand observations of these images. We utilize six different architectures of NeuralFuse generators trained with ResNet18\nunder a 1% bit error rate.\nFigure 16 (a) showcases several images from the truck class in CIFAR-10. Notably, images of the same class, when\ntransformed by the same NeuralFuse, exhibit similar patterns, such as circles symbolizing the wheels of the trucks.\nIn Figures 16 (b) and 16 (c), we observe analogous phenomena in the GTSRB and CIFAR-100 datasets. Transformed\nimages of the same class using the same generator consistently display patterns. On GTSRB, NeuralFuse-generated patterns\nhighlight the sign\u2019s shape with a green background, even if the original images have a dark background and are under\ndifferent lighting conditions. These results further underscore the efficacy and efficiency of NeuralFuse.\nFigure 17 presents more images from different classes in (a) CIFAR-10, (b) GTSRB, and (c) CIFAR-100. The transformed\nimages exhibit distinct patterns for each class, suggesting that NeuralFuse effectively transforms images into class-specific\npatterns, making associated features robust to random bit errors and easily recognizable by the base model in low-voltage\nsettings.\n28\nNeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes\nClean\nConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n(a) Truck class in CIFAR-10.\n(b) No Passing sign in GTSRB.\n(c) Apple class in CIFAR-100.\nFigure 16. Visualization of transformed images from different NeuralFuse generators trained with ResNet18 at 1% bit error rate.\nClean\nConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n(a) Ten different classes were sampled from CIFAR-10\n(b) Ten different traffic signs were sampled from GTSRB\nClean\nConvL\nConvS\nDeConvL\nDeConvS\nUNetL\nUNetS\n(c) 20 different classes were sampled from CIFAR-100\nFigure 17. Visualization of transformed images from different NeuralFuse generators trained by ResNet18 with 1% bit error rate.\n29\n"
  },
  {
    "title": "Dynamic-Resolution Model Learning for Object Pile Manipulation",
    "link": "https://arxiv.org/pdf/2306.16700.pdf",
    "upvote": "5",
    "text": "Dynamic-Resolution Model Learning for\nObject Pile Manipulation\nYixuan Wang2\u2217\nYunzhu Li1,2\u2217\nKatherine Driggs-Campbell2\nLi Fei-Fei1\nJiajun Wu1\n1Stanford University\n2University of Illinois Urbana-Champaign\n{yixuan22, yunzhuli, krdc}@illinois.edu, {feifeili, jiajunwu}@cs.stanford.edu\nReal scene (w/ goal mask)\nt\nParticle repr.\nParticle #: 18\nParticle #: 20\nParticle #: 27\nParticle #: 29\nModel-Predictive Control Steps\nPredicted Resolution\nParticle #: 18\nParticle #: 20\nParticle #: 27\nParticle #: 29\nFig. 1.\nDynamic-Resolution Model Learning for Object Pile Manipulation in the Real World. Depending on the progression of a\ntask, representations at different granularity levels may be needed at each model-predictive control (MPC) step to make the most effective\nprogress on the overall task. In this work, we construct dynamic-resolution particle representations of the environment and learn a unified\ndynamics model using graph neural networks (GNNs) that allows adaptive selection of the abstraction level. In this figure, we demonstrate\na real-world task of gathering the object pile into a target region. Figures on the left show the task execution process and the corresponding\nparticle representation. The plot on the right shows the predicted optimal resolution at each MPC step, where the red circles correspond to\nthe frames on the left. For video illustrations, we invite you to visit our project page\u2020.\nAbstract\u2014Dynamics models learned from visual observations\nhave shown to be effective in various robotic manipulation tasks.\nOne of the key questions for learning such dynamics models is\nwhat scene representation to use. Prior works typically assume\nrepresentation at a fixed dimension or resolution, which may be\ninefficient for simple tasks and ineffective for more complicated\ntasks. In this work, we investigate how to learn dynamic and\nadaptive representations at different levels of abstraction to\nachieve the optimal trade-off between efficiency and effectiveness.\nSpecifically, we construct dynamic-resolution particle represen-\ntations of the environment and learn a unified dynamics model\nusing graph neural networks (GNNs) that allows continuous\nselection of the abstraction level. During test time, the agent\ncan adaptively determine the optimal resolution at each model-\npredictive control (MPC) step. We evaluate our method in object\npile manipulation, a task we commonly encounter in cooking,\nagriculture, manufacturing, and pharmaceutical applications.\nThrough comprehensive evaluations both in the simulation and\nthe real world, we show that our method achieves significantly\nbetter performance than state-of-the-art fixed-resolution base-\nlines at the gathering, sorting, and redistribution of granular\nobject piles made with various instances like coffee beans,\n\u2217Denotes equal contribution.\n\u2020https://RoboPIL.github.io/dyn-res-pile-manip/\nalmonds, corn, etc.\nI. INTRODUCTION\nPredictive models have been one of the core components\nin various robotic systems, including navigation [26], locomo-\ntion [29], and manipulation [23, 73]. For robotic manipulation\nin particular, people have been learning dynamics models of\nthe environment from visual observations and demonstrated\nimpressive results in various manipulation tasks [17, 35, 69,\n55]. A learning-based dynamics model typically involves an\nencoder that maps the visual observation to a scene rep-\nresentation, and a predictive model predicts the representa-\ntion\u2019s evolution given an external action. Different choices\nof scene representations (e.g., latent vectors [21, 20, 34],\nobject-centric [71, 15] or keypoint representations [41, 38, 67])\nimply different expressiveness and generalization capabilities.\nTherefore, it is of critical importance to think carefully about\nwhat scene representation to use for a given task.\nPrior works typically use a fixed representation throughout\nthe entire task. However, to achieve the best trade-off between\nefficiency and effectiveness, the optimal representation may\narXiv:2306.16700v2  [cs.RO]  30 Jun 2023\nneed to be different depending on the object, the task, or\neven different stages of a task. An ideal representation should\nbe minimum in its capacity (i.e., efficiency) but sufficient to\naccomplish the downstream tasks (i.e., effectiveness) [62, 5].\nTake the object pile manipulation task as an example. When\nthe task objectives are different, the more complicated target\nconfiguration will require a more fine-grained model to capture\nall the details. On the other hand, if the targets are the same,\ndepending on the progression of the task, we might want\nrepresentations at different abstraction levels to come up with\nthe most effective action, as illustrated in Figure 1.\nIn this work, we focus on the robotic manipulation of object\npiles, a ubiquitous task critical for deploying robotic manipu-\nlators in cooking, agriculture, manufacturing, and pharmaceu-\ntical scenarios. Object pile manipulation is challenging in that\nthe environment has extremely high degrees of freedom [53].\nTherefore, developing methods that solve this complex task\nallows us to best demonstrate how we can learn dynamics\nmodels at different levels of abstraction to achieve the optimal\ntrade-off between efficiency and effectiveness.\nOur goal is to learn a unified dynamics model that can\nadaptively express the world at different granularity levels,\nfrom which the agent can automatically determine the optimal\nresolution given the task objective and the current observation.\nSpecifically, we introduce a resolution regressor that predicts\nthe optimal resolution conditioned on the current observation\nand the target configuration. The regressor is learned in a\nself-supervised fashion with labels coming from Bayesian\noptimization [19] that determines the most effective resolution\nfor minimizing the task objective under a given time budget.\nBesides the resolution regressor, our model also includes\nperception, dynamics, and planning modules (Figure 2).\nDuring task execution, we follow a model-predictive control\n(MPC) framework. At each MPC step, the resolution regressor\npredicts the resolution most effective for control optimization.\nThe perception module then samples particles from the RGBD\nvisual observation based on the predicted resolution. The\nderived particle-based scene representation, together with the\nrobot action, will be the input to the dynamics model to predict\nthe environment\u2019s evolution. The dynamics model can then be\nused for trajectory optimization to derive the action sequence.\nSpecifically, the dynamics model is instantiated as a graph\nneural network consisting of node and edge encoders. Such\ncompositional structures naturally generalize to particle sets of\ndifferent sizes and densities\u2014a unified graph-based dynamics\nmodel can support model-predictive control at various abstrac-\ntion levels, selected continuously by the resolution regressor.\nWe evaluate the model in various object pile manipulation\ntasks, including gathering spread-out pieces to a specific loca-\ntion, redistributing the pieces into complicated target shapes,\nand sorting multiple object piles. The tasks involve the manip-\nulation of piles consisting of different instances, including corn\nkernels, coffee beans, almonds, and candy pieces (Figure 3b).\nWe show that our model can automatically determine the\nresolution of the scene representation conditioned on the\ncurrent observation and the task goal, and make plans to\naccomplish these tasks.\nWe make three core contributions: (1) We introduce a\nframework that, at each planning step, can make continuous\npredictions to dynamically determine the scene representation\nat different abstraction levels. (2) We conduct comprehensive\nevaluations and suggested that our dynamic scene representa-\ntion selection performs much better than the fixed-resolution\nbaselines. (3) We develop a unified robotic manipulation\nsystem capable of various object pile manipulation tasks, in-\ncluding gathering, sorting, and redistributing into complicated\ntarget configurations.\nII. RELATED WORK\nA. Scene Representation at Different Abstraction Levels\nTo build multi-scale models of the dynamical systems, prior\nworks have adopted wavelet-based methods and windowed\nFourier Transforms to perform multi-resolution analysis [13,\n14, 30]. Kevrekidis et al. [27, 28] investigated equation-free,\nmulti-scale modeling methods via computer-aided analysis.\nKutz et al. [31] also combined multi-resolution analysis with\ndynamic mode decomposition for the decomposition of multi-\nscale dynamical data. Our method is different in that we\ndirectly learn from vision data for the modeling and planning\nof real-world manipulation systems.\nIn computer vision, Marr [39] laid the foundation by\nproposing a multi-level representational framework back in\n1982. Since then, people have investigated pyramid methods\nin image processing [1, 7] using Gaussian, Laplacian, and\nSteerable filters. Combined with deep neural networks, the\nmulti-resolution visual representation also showed stunning\nperformance in various visual recognition tasks [22, 72].\nIn the field of robotics, reinforcement learning researchers\nhave also studied task- or behavior-level abstractions and\ncome up with various hierarchical reinforcement learning\nalgorithms [46, 2, 6, 66, 45, 16, 48]. Our method instead\nfocuses on spatial abstractions from vision, where we learned\nstructured representations based on particles to model the\nobject interactions within the environment at different levels.\nB. Compositional Model Learning for Robotic Manipulation\nPhysics-based models have demonstrated their effectiveness\nin many robotic manipulation tasks (e.g., [23, 73, 47, 59]).\nHowever, they typically rely on complete information about\nthe environment, limiting their use in scenarios where full-\nstate estimation is hard or impossible to acquire (e.g., precise\nshape and pose estimation of each one of the object pieces in\nFigure 1). Learning-based approaches provide a way of build-\ning dynamics models directly from visual observations. Prior\nmethods have investigated various scene representations for\ndynamics modeling and manipulation of objects with compli-\ncated physical properties, including clothes [35, 25], ropes [10,\n42], fluids [34], softbodies [54], and plasticine [55]. Among\nthe methods, graph-structured neural networks (GNNs) have\nshown great promise by introducing explicit relational induc-\ntive biases [4]. Prior works have shown GNNs\u2019 effectiveness\nin modeling compositional dynamical systems involving the\ninteraction between multiple objects [3, 9, 33, 51, 18, 56],\nsystems represented using particles or meshes [44, 32, 65,\n52, 49], or for compositional video prediction [70, 24, 68,\n71, 50, 63, 74]. However, these works typically assume scene\nrepresentation at a fixed resolution, whereas our method learns\na unified graph dynamics model that can generalize to scene\nrepresentations at different levels of abstraction.\nC. Object Pile Manipulation\nRobotic manipulation of object piles and granular pieces has\nbeen one of the core capabilities if we want to deploy robot\nsystems for complicated tasks like cooking and manufacturing.\nSuh and Tedrake [58] proposed to learn visual dynamics based\non linear models for redistributing the object pieces. Along the\nlines of learning the dynamics of granular pieces, Tuomainen\net al. [64] and Schenck et al. [53] also proposed the use of\nGNNs or convolutional neural dynamics models for scooping\nand dumpling granular pieces. Other works introduced success\npredictors for excavation tasks [36], a self-supervised mass\npredictor for grasping granular foods [60], visual serving\nfor shaping deformable plastic materials [11], or data-driven\nmethods to calibrate the physics-based simulators for both\nmanipulation and locomotion tasks [75, 40]. Audio feedback\nhas also shown to be effective at estimating the amount and\nflow of granular materials [12]. Our work instead focuses on\nthree tasks (i.e., gather, redistribute, and sort object pieces)\nusing a unified dynamic-resolution graph dynamics to balance\nefficiency and effectiveness for real-world deployment.\nIII. METHOD\nIn this section, we first present the overall problem formula-\ntion. We then discuss the structure of our dynamic-resolution\ndynamics models, how we learn a resolution regressor to\nautomatically select the scene representation, and how we use\nthe model in a closed loop for the downstream planning tasks.\nA. Problem Formulation\nOur goal is to derive the resolution \u03c9 to represent the\nenvironment to achieve the best trade-off between efficiency\nand effectiveness for control optimization. We define the\nfollowing trajectory optimization problem over a horizon T:\nmin\n{ut}\nc(z\u03c9\nT , yg),\ns.t.\n\u03c9 = g(y0, yg),\nz\u03c9\n0 = h(y0, \u03c9),\nz\u03c9\nt+1 = f(z\u03c9\nt , ut, \u03c9),\n(1)\nwhere the resolution regressor g(\u00b7, \u00b7) takes the current obser-\nvation y0 and the goal configuration yg as input and predicts\nthe model resolution. h(\u00b7, \u00b7), the perception module, takes in\nthe current observation y0 and the predicted resolution \u03c9,\nthen derives the scene representation z\u03c9\n0 for the current time\nstep. The dynamics module f(\u00b7, \u00b7, \u00b7) takes the current scene\nrepresentation z\u03c9\nt , the input action ut, and the resolution \u03c9 as\ninputs, and then predicts the representation\u2019s evolution at the\nnext time step z\u03c9\nt+1. The optimization aims to find the action\nsequence {ut} to minimize the task objective c(z\u03c9\nT , yg).\nIn the following sections, we describe (1) the details of the\nperception module h(\u00b7, \u00b7) and the dynamics module f(\u00b7, \u00b7, \u00b7) in\nSection III-B, (2) how we obtain the self-supervision for the\nresolution regressor g(\u00b7, \u00b7) in Section III-C, and (3) how we\nsolve Equation 1 in a closed planning loop in Section III-D.\nB. Dynamic-Resolution Model Learning Using GNNs\nTo instantiate the optimization problem defined in Equa-\ntion 1, we use graphs of different sizes as the representation\nz\u03c9\nt = (Ot, Et), where \u03c9 indicates the number of vertices in\nthe graph. The vertices Ot = {oi\nt}i=1,...,|Ot| denote the particle\nset and oi\nt represents the 3D position of the ith particle. The\nedge set Et = {ej\nt}j=1,...,|Et| denotes the relations between the\nparticles, where ej\nt = (uj\nt, vj\nt ) denotes an edge pointing from\nparticle of index vj\nt to uj\nt.\nTo obtain the particle set Ot from the RGBD visual observa-\ntion yt, we first transform the RGBD image into a point cloud\nand then segment the point cloud to obtain the foreground\naccording to color and depth information \u00afyt \u2208 RN\u00d73. We\nthen deploy the farthest point sampling technique [43] to\nsubsample the foreground but ensure sufficient coverage of\n\u00afyt. Specifically, given already sampled particles o1,...,i\u22121\nt\n, we\napply\noi\nt = arg max\nyk\u2208\u00afyt\nmin\noj\nt\u2208o1,...,i\u22121\nt\n\u2225yk \u2212 oj\nt\u22252\n2\n(2)\nto find the ith particle oi\nt. We iteratively apply this process until\nwe reach \u03c9 particles. Different choices of \u03c9 indicate scene\nrepresentations at different abstraction levels, as illustrated\nin Figure 2a. The edge set is constructed dynamically over\ntime and connects particles within a predefined distance while\nlimiting the maximum number of edges a node can have.\nWe instantiate the dynamics model f(\u00b7, \u00b7, \u00b7) as graph neural\nnetworks (GNNs) that predict the evolution of the graph\nrepresentation z\u03c9\nt under external actions ut and the selected\nresolution \u03c9. f(\u00b7, \u00b7, \u00b7) consists of node and edge encoders\nf enc\nO (\u00b7, \u00b7, \u00b7), f enc\nE (\u00b7, \u00b7, \u00b7) to obtain node and edge representations:\npi\nt = f enc\nO (oi\nt, ut, \u03c9),\ni = 1, . . . , |Ot|,\nqj\nt = f enc\nE (ouj\nt\nt , ovj\nt\nt , \u03c9),\nj = 1, . . . , |Et|.\n(3)\nWe then have node and edge decoders f dec\nO (\u00b7, \u00b7), f dec\nE (\u00b7, \u00b7)\nto obtain the corresponding representations and predict the\nrepresentation at the next time step:\nrj\nt = f dec\nE (qj\nt , \u03c9),\nj = 1, . . . , |Et|,\n\u02c6oi\nt+1 = f dec\nO (pi\nt,\nX\nj\u2208Ni\nrj\nt),\ni = 1, . . . , |Ot|,\n(4)\nwhere Ni is the index set of the edges, in which particle i\nis the receiver. In practice, we follow Li et al. [33] and use\nmulti-step message passing over the graph to approximate the\ninstantaneous propagation of forces.\nTo train the dynamics model, we iteratively predict future\nparticle states over a time horizon of T and then optimize the\nneural network\u2019s parameters by minimizing the mean squared\n<latexit sha1_base64=\"Cn2m6DbPyN6NBpet4/up3iEn04Q=\">AB/HicbVDJSgNBEO1xjXHJqEcvjUHwIGFG3I5BLx4jZIPMOPR0epImvQzdPUIc4o94jSfx6\nqcI/o2d5aCJDwoe71VRVS9OGdXG876dldW19Y3NwlZxe2d3r+TuHzS1zBQmDSyZVO0YacKoIA1DSPtVBHEY0Za8eBu4reiNJUiroZpiTkqCdoQjEyVorcUhDz/Hn0GEhOeiqR27Zq3hTwGXiz0kZzFGL3K+gK3HGiTCYIa07vpeaMEfKUMzIqBhkmqQID1CPdCwViBMd5tPDR/DEKl2YSGVLGDhVf0/kiGs95LHt5Mj09aI3Ec9gzP\n/zO5lJbsKcijQzRODZsiRj0Eg4SQJ2qSLYsKElCtq74W4jxTCxuZVtEH4i28vk+Z5xb+qXD5clKu380gK4Agcg1Pg2tQBfegBhoAgwy8gjF4c16csfPufMxaV5z5zCH4A+fzB4yvlJE=</latexit>z!\nT\n<latexit sha1_base64=\"ma6kORg4aNXFrt/hzkofHOFmWcA=\">AB/HicbVDJ\nSgNBEO2JW4xLRj16aQyCBwkz4nYMevEYwSyQGYeTk/SpJehu0eIQ/wRr/EkXv0Uwb+xsxw08UHB470qurFKaPaeN63U1hZXVvfKG6WtrZ3dsvu3n5Ty0xh0sCSdW\nOkSaMCtIw1DSThVBPGakFQ9uJ37riShNpXgw5SEHPUETShGxkqRWw5inj+PHgPJSQ9FXuRWvKo3BVwm/pxUwBz1yP0KuhJnAiDGdK643upCXOkDMWMjEpBpkmK8A\nD1SMdSgTjRYT49fASPrdKFiVS2hIFT9fdEjrjWQx7bTo5MXy96E/EUxvw/v5OZ5DrMqUgzQwSeLUsyBo2EkyRglyqCDRtagrCi9l6I+0ghbGxeJRuEv/j2MmeVf3L6\nsX9eaV2M4+kCA7BETgBPrgCNXAH6qABMjAKxiDN+fFGTvzsesteDMZw7AHzifP1WPlG0=</latexit>z!\n0\n<latexit sha1_base64=\"wtZHWzDnI8976luZKYDMpN0DvcA=\">ACBHicbVDL\nSgMxFM3UV62vqgsXboJFqFDKjPhaFt24rNAXtHXIpJlpaDIZkoxQh9n6I27rStz6GYJ/Y9rOQlsPXDicy/3uNFjCpt29WbmV1bX0jv1nY2t7Z3SvuH7SUiCUmTSy\nYkB0PKcJoSJqakY6kSIe4y0vdHd1G8/EamoCBt6HJE+R0FIfYqRNpJbPMLlnseT5/SxJzgJkNuowLEbnLnFkl21Z4DLxMlICWSou8Wv3kDgmJNQY4aU6jp2pPsJkp\npiRtJCL1YkQniEAtI1NEScqH4yeyCFp0YZQF9IU6GM/X3RIK4UmPumU6O9FAtelOxAj3+n9+NtX/T2gYxZqEeL7MjxnUAk4TgQMqCdZsbAjCkp7IR4ibA2uRVME\nM7i28ukdV51rqXDxel2m0WSR4cgxNQBg64BjVwD+qgCTBIwSuYgDfrxZpY79bHvDVnZTOH4A+szx9a6pcg</latexit>c(z!\nT , yg)\n\u2026\nRGBD\nGoal\n<latexit sha1_base64=\"Ce9Zju/b4eJUuBWKNPiRJrjOiUs=\">AB8XicbVDJS\ngNBEK2JW4xb1KOXxiB4kDAjbsegF48RzALJEHo6PUmbXobuHiEM+Qev8SRe/R/Bv7GTzEGjDwoe71VRVS9KODPW97+8wsrq2vpGcbO0tb2zu1feP2galWpCG0RxpdsRN\npQzSRuWU7biaZYRJy2otHdzG89U2Yko92nNBQ4IFkMSPYOqnZVYIOcK9c8av+HOgvCXJSgRz1Xvmz21ckFVRawrExncBPbJhbRnhdFLqpoYmIzwgHYclVhQE2bza\nyfoxCl9FCvtSlo0V39OZFgYMxaR6xTYDs2yNxPUCT+8zupjW/CjMktVSxbI45cgqNHsf9ZmxPKxI5ho5u5FZIg1JtaFVHJBMtv/yXN82pwVb18uKjUbvNIinAEx\n3AKAVxDe6hDg0g8AQvMIVXz3hT7817X7QWvHzmEH7B+/gG/yOQWA=</latexit>!\n<latexit sha1_base64=\"MhfU5rnb1JAnMK9YhiQrKCoBt+w=\">AB7HicbVDLS\ngNBEOz1GeMr6tHLYBA8SNgVX8egF48JmAckS5id9CZDZmeXmVkhLPkCr/EkXv0kwb9xkuxBEwsaiqpuruCRHBtXPfbWVvf2NzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtg\nGoUXGLDcCOwnSikUSCwFYweZ37rBZXmsXw24wT9iA4kDzmjxkr1Qa9UdivuHGSVeDkpQ45ar/TV7csjVAaJqjWHc9NjJ9RZTgTOCl2U40JZSM6wI6lkao/Wx+6IScW\n6VPwljZkobM1d8TGY20HkeB7YyoGeplbyZekiD6z+kJrz3My6T1KBki2VhKoiJyexz0ucKmRFjSyhT3N5L2JAqyozNp2iD8JbfXiXNq4p3W7mpX5erD3kBTiFM7gAD\n+6gCk9QgwYwQHiFKbw50pk6787HonXNyWdO4A+czx8zao4o</latexit>g\n<latexit sha1_base64=\"7bw5dQcZP6vH/oColPA7YTERtls=\">AB7HicbVDLS\ngNBEOz1GeMr6tHLYBA8SNgVX8egF48JmAckS5id9CZDZmeXmVkhLPkCr/EkXv0kwb9xkuxBEwsaiqpuruCRHBtXPfbWVvf2NzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtg\nGoUXGLDcCOwnSikUSCwFYweZ37rBZXmsXw24wT9iA4kDzmjxkr1Ya9UdivuHGSVeDkpQ45ar/TV7csjVAaJqjWHc9NjJ9RZTgTOCl2U40JZSM6wI6lkao/Wx+6IScW\n6VPwljZkobM1d8TGY20HkeB7YyoGeplbyZekiD6z+kJrz3My6T1KBki2VhKoiJyexz0ucKmRFjSyhT3N5L2JAqyozNp2iD8JbfXiXNq4p3W7mpX5erD3kBTiFM7gAD\n+6gCk9QgwYwQHiFKbw50pk6787HonXNyWdO4A+czx808o4p</latexit>h\n<latexit sha1_base64=\"t5ECyL4znGloWD3B/JBtaie+YvI=\">AB7HicbVDLS\ngNBEOz1GeMr6tHLYBA8SNgVX8egF48JmAckS5id9CZDZmeXmVkhLPkCr/EkXv0kwb9xkuxBEwsaiqpuruCRHBtXPfbWVvf2NzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtg\nGoUXGLDcCOwnSikUSCwFYweZ37rBZXmsXw24wT9iA4kDzmjxkr1sFcquxV3DrJKvJyUIUetV/rq9mOWRigNE1Trjucmxs+oMpwJnBS7qcaEshEdYMdSPUfjY/dELOr\ndInYaxsSUPm6u+JjEZaj6PAdkbUDPWyNxMvSRD953dSE97GZdJalCyxbIwFcTEZPY56XOFzIixJZQpbu8lbEgVZcbmU7RBeMtvr5LmVcW7rdzUr8vVhzySApzCGVyAB\n3dQhSeoQMYILzCFN4c6Uyd+dj0brm5DMn8AfO5w8x4o4n</latexit>f\n\u2026\n<latexit sha1_base64=\"6abG8d7uw2h\nzZkf8ZengTdHdOM=\">AB7nicbVDLSgNBEOyNrxhfUY9eBoPgQcKuxMcx6MVjR\nPOAZAmzk9lkyMzsMjMrLEs+wWs8iVe/SPBvnCR70MSChqKqm+6uIOZMG9f9dgpr6\nxubW8Xt0s7u3v5B+fCopaNEdokEY9UJ8CaciZp0zDaSdWFIuA03Ywvp/57ReqN\nIvks0lj6gs8lCxkBsrPaV9t1+uFV3DrRKvJxUIEejX/7qDSKSCoN4VjrufGx\ns+wMoxwOin1Ek1jTMZ4SLuWSiyo9rP5qRN0ZpUBCiNlSxo0V39PZFhonYrAdgpsR\nnrZm4kXKBD/+d3EhLd+xmScGCrJYlmYcGQiNPsdDZixPDUEkwUs/ciMsIKE2MTK\ntkgvOW3V0nrsupdV68ea5X6XR5JEU7gFM7BgxuowM0oAkEhvAKU3hzYmfqvDsfi\n9aCk8cwx84nz91JI7d</latexit>y0\n<latexit sha1_base64=\"hiHrqYvq1z\njYx3lQq7J/6i+f3A=\">AB7nicbVDLSsNAFL3xWeur6tLNYBFcSEnE17LoxmVF+\n4A2lMl0kg6dmYSZiRBCP8FtXYlbv0jwb5y2WjrgQuHc+7l3nuChDNtXPfbWVldW\n9/YLG2Vt3d29/YrB4ctHaeK0CaJeaw6AdaUM0mbhlO4miWASctoPR/dRv1ClW\nSyfTZQX+BIspARbKz0lPWjfqXq1twZ0DLxClKFAo1+5as3iEkqDSEY627npsYP\n8fKMLpuNxLNU0wGeGIdi2VWFDt57NTx+jUKgMUxsqWNGim/p7IsdA6E4HtFNgM9\naI3Fc9RIP7zu6kJb/2cySQ1VJL5sjDlyMRo+jsaMEWJ4ZklmChm70VkiBUmxiZUt\nkF4i28vk9ZFzbuXT1eVut3RSQlOIYTOAMPbqAOD9CAJhCI4BUm8OYkzsR5dz7mr\nStOMXMEf+B8/gDJXI8U</latexit>yg\nRobot Workspace\nRGBD\n<latexit sha1_bas\ne64=\"7bw5dQcZP6vH/oColPA7YTERtls\n=\">AB7HicbVDLSgNBEOz1GeMr6tHLY\nBA8SNgVX8egF48JmAckS5id9CZDZmeXm\nVkhLPkCr/EkXv0kwb9xkuxBEwsaiqpu\nruCRHBtXPfbWVvf2NzaLuwUd/f2Dw5LR\n8dNHaeKYPFIlbtgGoUXGLDcCOwnSikU\nSCwFYweZ37rBZXmsXw24wT9iA4kDzmjx\nkr1Ya9UdivuHGSVeDkpQ45ar/TV7csj\nVAaJqjWHc9NjJ9RZTgTOCl2U40JZSM6w\nI6lkao/Wx+6IScW6VPwljZkobM1d8TG\nY20HkeB7YyoGeplbyZekiD6z+kJrz3M\ny6T1KBki2VhKoiJyexz0ucKmRFjSyhT3\nN5L2JAqyozNp2iD8JbfXiXNq4p3W7mpX\n5erD3kBTiFM7gAD+6gCk9QgwYwQHiFK\nbw50pk6787HonXNyWdO4A+czx808o4p<\n/latexit>h\n<latexit sha1_base64=\"7bw5dQcZP6v\nH/oColPA7YTERtls=\">AB7HicbVDLSgNBEOz1GeMr6tHLYBA8SNgVX8egF48Jm\nAckS5id9CZDZmeXmVkhLPkCr/EkXv0kwb9xkuxBEwsaiqpuruCRHBtXPfbWVvf2\nNzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtgGoUXGLDcCOwnSikUSCwFYweZ37rBZXms\nXw24wT9iA4kDzmjxkr1Ya9UdivuHGSVeDkpQ45ar/TV7csjVAaJqjWHc9NjJ9RZ\nTgTOCl2U40JZSM6wI6lkao/Wx+6IScW6VPwljZkobM1d8TGY20HkeB7YyoGeplb\nyZekiD6z+kJrz3My6T1KBki2VhKoiJyexz0ucKmRFjSyhT3N5L2JAqyozNp2iD8\nJbfXiXNq4p3W7mpX5erD3kBTiFM7gAD+6gCk9QgwYwQHiFKbw50pk6787HonXNy\nWdO4A+czx808o4p</latexit>h\n<latexit \nsha1_base64=\"MRz\nAe+ES6bq/45BXaJn\nv3prXc3s=\">AB8\n3icbVDJSgNBEK2JW\n4xb1KOXxiB4kDAjb\nsegF48RzALJEHo6P\nUmTXobuHiEM+Qmv8\nSRe/R3Bv7GTzETH\nxQ83quiql6UcGas7\n397hbX1jc2t4nZpZ\n3dv/6B8eNQ0KtWEN\nojiSrcjbChnkjYs\n5y2E02xiDhtRaOHm\nd96odowJZ/tOKGhw\nAPJYkawdVK7qwQd4\nJ7fK1f8qj8HWiVBT\niqQo94rf3X7iqSCS\nks4NqYT+IkNM6wtI\n5xOSt3U0ASTER7Qj\nqMSC2rCbH7vBJ05p\nY9ipV1Ji+bq74kMC\n2PGInKdAtuhWfZm4\ngWKxH9+J7XxXZgxm\naSWSrJYFqcWYVmA\naA+05RYPnYE83cv\nYgMscbEuphKLohg+\ne1V0rysBjfV6erS\nu0+j6QIJ3AK5xDAL\ndTgEerQAIcXmEKb\n17qTb1372PRWvDym\nWP4A+/zBylIkPs=<\n/latexit>!0\n<latexit sha1_bas\ne64=\"hS0pTf2XDUTPn12Ly2iT/MSfsU\n=\">AB83icbVDJSgNBEO2JW4xb1KOXx\niB4kDAjbsegF48RzALJEHo6NUmTXobuH\niEM+Qmv8SRe/R3Bv7GTzETHxQ83quiq\nl6UcGas7397hbX1jc2t4nZpZ3dv/6B8e\nNQ0KtUGlRxpdsRMcCZhIZlkM70UBEx\nKEVjR5mfusFtGFKPtxAqEgA8liRol1U\nrurBAxIL+iVK37VnwOvkiAnFZSj3it/d\nfuKpgKkpZwY0wn8xIYZ0ZRDpNSNzWQE\nDoiA+g4KokAE2bzeyf4zCl9HCvtSlo8V\n39PZEQYMxaR6xTEDs2yNxMvcCT+8zupj\ne/CjMktSDpYlmcmwVngWA+0wDtXzsC\nKGauXsxHRJNqHUxlVwQwfLbq6R5WQ1uq\ntdPV5XafR5JEZ2gU3SOAnSLaugR1VEDU\ncTRK5qiNy/1pt6797FoLXj5zDH6A+/zB\nyrQkPw=</latexit>!1\n<latexit sha1_base64=\"07oDQxgBmC2tmTbZ6LGdTowLqes=\">AB7nicbVDL\nSgMxFL3xWeur6tJNsAgupMyIr2XRjcuK9gHtUDJpg1NMkOSEcrQT3BbV+LWLxL8G9N2Ftp64MLhnHu5954wEdxYz/tGK6tr6xubha3i9s7u3n7p4LBh4lRTVqexiHU\nrJIYJrljdcitYK9GMyFCwZji8n/rNF6YNj9WzHSUskKSveMQpsU56Srtet1T2Kt4MeJn4OSlDjlq39NXpxTSVTFkqiDFt30tskBFtORVsXOykhiWEDkmftR1VRDITZLN\nTx/jUKT0cxdqVsnim/p7IiDRmJEPXKYkdmEVvKp7jUP7nt1Mb3QYZV0lqmaLzZVEqsI3x9Hfc45pRK0aOEKq5uxfTAdGEWpdQ0QXhL769TBoXFf+6cvV4Wa7e5ZEU4B\nhO4Ax8uIEqPEAN6kChD68wgTeUoAl6Rx/z1hWUzxzBH6DPH278jtk=</latexit>u0\n<latexit sha1_base64=\"8QJKdYWLP8TydNSmwkDxOeGwWk=\">AB8nicbVDL\nSgNBEOyNrxhfUY9eBoPgQcOu+DoGvXiMkBckIcxOZpMhs7PLTK8QlnyE13gSr36P4N84SfagiQUNRVU3V1+LIVB1/12cmvrG5tb+e3Czu7e/kHx8KhokQzXmeRjHT\nLp4ZLoXgdBUreijWnoS950x89zvzmC9dGRKqG45h3QzpQIhCMopWaS+tXqTXrHklt05yCrxMlKCDNVe8avTj1gScoVMUmPanhtjN6UaBZN8UugkhseUjeiAty1VNOS\nm87PnZAzq/RJEGlbCslc/T2R0tCYcejbzpDi0Cx7M/GC+OF/fjvB4L6bChUnyBVbLAsSTAis/9JX2jOUI4toUwLey9hQ6opQ5tSwQbhLb+9ShpXZe+2fPN8Xao8ZJ\nHk4QRO4Rw8uIMKPEV6sBgBK8whTcHnanz7nwsWnNONnMf+B8/gBMeJB7</latexit>uT \u22121\n<latexit sha1_base64=\"t5ECyL4znGloWD3B/JBtaie+YvI=\">AB7HicbVDLS\ngNBEOz1GeMr6tHLYBA8SNgVX8egF48JmAckS5id9CZDZmeXmVkhLPkCr/EkXv0kwb9xkuxBEwsaiqpuruCRHBtXPfbWVvf2NzaLuwUd/f2Dw5LR8dNHaeKYPFIlbtg\nGoUXGLDcCOwnSikUSCwFYweZ37rBZXmsXw24wT9iA4kDzmjxkr1sFcquxV3DrJKvJyUIUetV/rq9mOWRigNE1Trjucmxs+oMpwJnBS7qcaEshEdYMdSPUfjY/dELOr\ndInYaxsSUPm6u+JjEZaj6PAdkbUDPWyNxMvSRD953dSE97GZdJalCyxbIwFcTEZPY56XOFzIixJZQpbu8lbEgVZcbmU7RBeMtvr5LmVcW7rdzUr8vVhzySApzCGVyAB\n3dQhSeoQMYILzCFN4c6Uyd+dj0brm5DMn8AfO5w8x4o4n</latexit>f\nResolution \nRegressor\nPerception\nDynamics\nPrediction\n(a) Representations at different resolutions\n(b) Dynamics prediction and inverse planning\nTrajectory \nOptimization\nFig. 2.\nOverview of the proposed framework. (a) Our perception module h processes the input RGBD image and generates particle\nrepresentations at different levels of abstraction depending on the resolution \u03c9. (b) The resolution regressor g takes the current observation\ny0 and the goal yg as input. It then predicts the resolution \u03c9 we intend to represent the environment. The dynamics model f, conditioned\non the dynamically-selected resolution \u03c9 and the input action ut, predicts the temporal evolution of the scene representation z\u03c9\nt . During\nplanning time, we calculate the task objective c(z\u03c9\nT , yg) and backpropagate the gradients to optimize the action sequence {ut}.\nerror (MSE) between the predictions and the ground truth\nfuture states:\nL =\n1\nT \u00b7 |Ot|\nT\nX\nt\u2032=1\n|Ot|\nX\ni=1\n\u2225\u02c6oi\nt+t\u2032 \u2212 oi\nt+t\u2032\u22252\n2.\n(5)\nC. Adaptive Resolution Selection via Self-Supervised Learning\nThe previous sections discussed how to obtain the particle\nset and how we predict its evolution given a resolution \u03c9.\nIn this section, we present how we learn the resolution\nregressor g(\u00b7, \u00b7) in Equation 1 that can automatically determine\nthe resolution in a self-supervised manner. Specifically, we\nintend to find the resolution \u03c9 that is the most effective for\nminimizing the task objective given the current observation\ny0 and the goal yg. We reformulate the optimization problem\nin Equation 1 by considering \u03c9 as a variable of the objective\nfunction as the following:\nc\u2217(y0, yg, \u03c9) = min\n{ut}\nc(z\u03c9\nT , yg),\ns.t.\nz\u03c9\n0 = h(y0, \u03c9),\nz\u03c9\nt+1 = f(z\u03c9\nt , ut, \u03c9).\n(6)\nFor a given \u03c9, we solve the above optimization problem via a\ncombination of sampling and gradient descent using shooting\nmethods [61] under a given time budget\u2014the higher resolution\nrepresentation will go through fewer optimization iterations.\nFor simplicity, we denote the objective in Equation 6 as c\u2217(\u03c9)\nin the following part of this section.\nGiven the formulation, we are then interested in finding the\nparameter \u03c9 that can minimize the following objective:\nmin\n\u03c9\nc+(\u03c9) = c\u2217(\u03c9) + R(\u03c9),\ns.t.\n\u03c9 \u2208 (\u03c9min, \u03c9max),\n(7)\nwhere R(\u03c9) is a regularizer penalizing the choice of an\nexcessively large \u03c9 to encourage efficiency. Regularizer details\ncan be found in supplementary materials. We use Bayesian\noptimization [57] to find the optimal \u03c9 by iteratively sampling\n\u03c9 and approximating c+(\u03c9) using the Gaussian process. At\neach sampling stage, we sample one or more data points\n\u03c9i according to the expected improvement of the objective\nfunction and evaluate their value c+(\u03c9i). Then, at the approx-\nimation stage, we assume the distribution of c+(\u03c9) follows the\nGaussian distribution N(\u00b5(\u03c9), \u03c32); thus, the joint distribution\nof the evaluated points \u2126train = [\u03c91, . . . , \u03c9n] and the testing\npoints \u2126test = [\u03c9\u2032\n1, . . . , \u03c9\u2032\nm] can be expressed as the following:\nCtrain = [c+(\u03c91), . . . , c+(\u03c9n)],\nMtrain = [\u00b5(\u03c91), . . . , \u00b5(\u03c9n)],\nCtest = [c+(\u03c9\u2032\n1), . . . , c+(\u03c9\u2032\nm)],\nMtest = [\u00b5(\u03c9\u2032\n1), . . . , \u00b5(\u03c9\u2032\nm)],\n\u0014Ctrain\nCtest\n\u0015\n\u223c N\n\u0012\u0014Mtrain\nMtest\n\u0015\n,\n\u0014 K\nK\u2217\nK\u22a4\n\u2217\nK\u2217\u2217\n\u0015\u0013\n,\n(8)\nwhere K is a kernel function matrix derived via K =\nK(\u2126train, \u2126train). K(\u00b7, \u00b7) is the kernel function used to com-\npute the covariance. Similarly, K\u2217 = K(\u2126train, \u2126test) and\nK\u2217\u2217 = K(\u2126test, \u2126test).\nEquation 8 shows the joint probability of Ctrain and Ctest\nconditioned on \u2126train and \u2126test. Through marginalization, we\ncould fit c+(\u03c9) using the following conditional distribution:\nCtest|Ctrain, \u2126train, \u2126test \u223c\nN(K\u22a4\n\u2217 KCtrain, K\u2217\u2217 \u2212 K\u22a4\n\u2217 K\u22121K\u2217).\n(9)\nWe can then use the mean value of the Gaussian distribution\nin Equation 9 as the metric to minimize c+(\u03c9). Therefore, the\nsolution to Equation 7 is approximated as the following:\n\u03c9\u2217 = arg min\n\u03c9\nK\u22a4\n\u2217 KCtrain\ns.t.\n\u03c9 \u2208 (\u03c9min, \u03c9max).\n(10)\nTo train the resolution regressor, we randomly generate a\ndataset containing the observation and goal pairs (y0, yg).\nFor each pair, we follow the above optimization process to\ngenerate the optimal resolution label \u03c9\u2217. We then train the\nresolution regressor \u03c9 = g(y0, yg) to predict the resolution\nbased on the observation and the goal via supervised learning.\nTraining the \u03c9 regressor is a self-supervised learning process,\nas the labels are automatically generated via an optimization\nprocess without any human labeling.\nD. Closed-Loop Planning via Adaptive Repr. Selection\nNow that we have obtained the resolution regressor g, the\nperception module h, and the dynamics module f. We can\nwire things together to solve Equation 1 and use the optimized\naction sequence in a closed loop within a model-predictive\ncontrol (MPC) framework [8]. Specifically, for each MPC step,\nwe follow Algorithm 1, which first determines the resolution\nto represent the environment, then uses a combination of\nsampling and gradient descent to derive the action sequence\nthrough trajectory optimization using the shooting method. We\nthen execute the first action from the action sequence in the\nreal world, obtain new observations, and apply Algorithm 1\nagain. Such a process allows us to take feedback from the\nenvironment and adaptively select the most appropriate reso-\nlution at each step as the task progresses. Figure 2b also shows\nan overview of the future prediction and inverse planning\nprocess. Details including task objective definition and MPC\nhyperparameter are included in supplementary materials.\nAlgorithm 1 Trajectory optimization at each MPC step\nInput: Current observation y0, goal yg, time horizon T,\nthe resolution regressor g, the perception module h,\nthe dynamics module f, and gradient descent iteration N\nOutput: Actions u0:T \u22121\nPredict the resolution \u03c9 \u2190 g(y0, yg)\nObtain the current representation z\u03c9\n0 \u2190 h(y0, \u03c9)\nSample M action sequences \u02c6u1:M\n0:T \u22121\nfor m = 1, . . . , M do\nfor i = 1, . . . , N do\nfor t = 0, . . . , T \u2212 1 do\nPredict the next step z\u03c9\nt+1 \u2190 f(z\u03c9\nt , \u02c6um\nt , \u03c9)\nend for\nCalculate the task loss cm \u2190 c(z\u03c9\nT , yg)\nif i < N then\nUpdate \u02c6um\n0:T \u22121 using gradients \u2207\u02c6um\n0:T \u22121cm\nend if\nend for\nend for\nm\u2217 \u2190 arg minm cm\nReturn \u02c6um\u2217\n0:T \u22121\nIV. EXPERIMENTS\nIn this section, we evaluate the proposed framework in\nvarious object pile manipulation tasks. In particular, we aim to\nWorkspace\nPusher\nRGBD \nCamera\nObject Piles\n(a) Robot setup\n(b) Object piles considered in this work\nFig. 3.\nRobot setup and the testing object piles. (a) The dashed\nblack square shows the robot\u2019s workspace. The robotic manipulator,\nequipped with a pusher at the end effector, pushes the object piles\nwithin the workspace. A calibrated RGBD camera mounted at the\ntop provides visual observations of the environment. (b) We show\nthe object piles considered in this work, including M&M, almond,\ngranola, candy, carrot, rice, corn, and coffee beans.\nanswer the following three questions through the experiments.\n(1) Does a trade-off exist between efficiency and effectiveness\nas we navigate through representations at different abstraction\nlevels? (2) Is a fixed-resolution dynamics model sufficient,\nor do we need to dynamically select the resolution at each\nMPC step? (3) Can our dynamic-resolution model accomplish\nthree challenging object pile manipulation tasks: Gather,\nRedistribute, and Sort?\nA. Setup\nWe conduct experiments in both the simulation environment\nand the real world. The simulation environment is built using\nNVIDIA FleX [37, 32], a position-based simulator capable of\nsimulating the interactions between a large number of object\npieces. In the real world, we conducted experiments using the\nsetup shown in Figure 3a. We use RealSense D455 as the top-\nResolution\nNegative Loss Reduction\nResolution\nNegative Loss Reduction\nResolution\nNegative Loss Reduction\nResolution\nNegative Loss Reduction\n(a) Same initial but different goal configurations\n(b) Same goal but different initial configurations\nCurrent observation\nOverlay the target\nBayesian optimization\nCurrent observation\nOverlay the target\nBayesian optimization\nFig. 4.\nOptimal resolution differs depending on the initial and goal configurations. (a) We show two examples with the same initial but\ndifferent goal configurations. We apply Bayesian optimization to solve the problem discussed in Section III-C to find the optimal resolution\nfor both cases. The example with a more complicated target shape requires a higher-resolution representation to be the most effective at\nmaking task progress. (b) When the goal is to gather the pieces in the center of the workspace, a coarse representation is sufficient for\nexamples with spread-out pieces. The task progresses as long as the agent pushes any outlying pieces toward the goal region. In contrast, a\nhigher-resolution representation is needed to reveal the subtle difference between the initial and goal configurations when they are close.\ndown camera to capture the RGBD visual observations of the\nworkspace. We attach a flat pusher to the robotic manipulator\u2019s\nend effector to manipulate the object piles.\nB. Tasks\nWe evaluate our methods on three object pile manipulation\ntasks that are common in daily life.\n\u2022 Gather: The robot needs to push the object pile into a\ntarget blob with different locations and radii.\n\u2022 Redistribute: The robot is tasked to manipulate the object\npiles into many complex target shapes, such as letters.\n\u2022 Sort: The robot has to move two different object piles to\ntarget locations without mixing each other.\nWe use a unified dynamics model for all three tasks, which\ninvolve objects pieces of different granularities, appearances,\nand physical properties (Figure 3b).\nC. Trade-Off Between Efficiency and Effectiveness\nThe trade-off between efficiency and effectiveness can vary\ndepending on the tasks, the current, and the goal configura-\ntions. As we have discussed in Section III-C, given the reso-\nlution \u03c9, we set a fixed time budget to solve the optimization\nproblem defined in Equation 6. Intuitively, if the resolution is\ntoo low, the representation will not contain sufficiently detailed\ninformation about the environment to accomplish the task, the\noptimization of which is efficient but not effective enough to\nfinish the task. On the contrary, if we choose an excessively\nhigh resolution, the representation will carry redundant infor-\nmation not necessary for the task and can be inefficient in\noptimization. We thus conduct experiments evaluating whether\nthe trade-off exists (i.e., whether the optimal resolution \u03c9\ncalculated from Equation 10 is different for different initial\nand goal configurations).\nTask Score\nDistribution Distance Threshold\nFig. 5.\nModel-predictive control (MPC) results. We evaluated the\nMPC performance on different representation choices. We use the\ntask score as the evaluation metric. Task execution trial results in a\ndistribution distance lower than the threshold is considered a success.\nThe task score is the number of successful trials divided by the total\nnumber of task trials for both the Gather and Redistribute tasks. Our\nmethod automatically and adaptively selects the scene representation,\nwhich achieves the best overall performance compared with the scores\nof fixed-resolution baselines and a method that uses convolutional\nneural networks (CNN) as the dynamics model class.\nWe use Bayesian optimization and follow the algorithm\ndescribed in Section III-C to find the optimal trade-off on\nGather and Redistribute tasks in the simulation. As shown\nin Figure 4a, higher-resolution dynamics models do not nec-\nessarily lead to better performance due to their optimization\ninefficiency. Compared between goal configurations, even if\nthe current observation is the same, a more complicated goal\ntypically requires a higher resolution representation to make\nInitial\nResult\n2 particles\n25 particles\n50 particles\n75 particles\n100 particles\nAuto (Ours)\nOutlying \nPieces\nTask Loss\nModel-Predictive Control Steps\n(a) Qualitative comparison between fixed-resolution baselines and our automatic resolution selection method\n(b) Task loss reduction on the gathering trial shown in (a)\n(d) Redistribute to target configurations\n(e) Sort different object piles\n(c) Gather object piles of different materials\nInitial\nResult\nt\nt\nt\nCorn kernel\nRice\nGranola \nM&M \nCarrots\nAlmond\nAlmond \nCoffee & Carrot\nCoffee & Candy\nAlmond & Candy \nFig. 6.\nQualitative results in the real world. (a) Qualitative comparison of MPC performance between our method with fixed-resolution\nbaselines. Our method could gather corn pieces into the target region clearly, whereas the fixed-resolution baselines fail to reach the goal or\nleave outlying pieces. (b) Quantitative comparisons for the qualitative results in (a). Starting from similar initial configurations, our automatic\nresolution selection method performs the best throughout the MPC steps. (c) Evaluation of our method on the Gather task with different\nobjects. The objects vary in their scales and physical properties (e.g., while rice and granola are quasi-static during MPC steps, M&M can\nexhibit rolling motions after pushing). (d) Redistribute the object pieces into more complicated target configurations. Our method can push\nrandomly-spread object piles into the desired letter shapes with clear boundaries. (e) Our method can also be coupled with a simple high-level\nplanner to accomplish more complex tasks, such as sorting different object piles into target regions without mixing them.\nthe most effective task progression. More specifically, when\nthe target region is a plain circle, the coarse representation\ncaptures the rough shape of the object pile, sufficient for\nthe task objective, allowing more efficient optimization than\nthe higher-resolution counterparts. However, when the target\nregion has a more complicated shape, low-resolution represen-\ntation fails to inform downstream MPC of detailed object pile\nshapes. Therefore, high-resolution representation is necessary\nfor effective trajectory optimization.\nThe desired representation does not only depend on goal\nconfigurations. Even if the goal configurations are the same,\ndifferent initial configurations can also lead to different opti-\nmal resolutions, as illustrated in Figure 4b. When the initial\nconfiguration is more spread out, the most effective way\nof decreasing the loss is by pushing the outlying pieces to\nthe goal region. Our farthest sampling strategy, even with\njust a few particles, could capture outlying pieces and helps\nthe agent to make good progress. Therefore, when pieces\nare sufficiently spread out, higher particle resolution does\nnot necessarily contain more useful information for the task\nbut makes the optimization process inefficient. On the other\nhand, when the initial configuration concentrates on the goal\nregion, to effectively decrease the task objective, MPC needs\nmore detailed information about the object pile\u2019s geometry to\npinpoint the mismatching area. For example, the agent needs\nto know more precise contours of the goal region and the\noutlying part of object piles to decide how to improve the\nplanning results further. Low-resolution representations will be\nless effective in revealing the difference between the current\nobservation and the goal, thus less helpful in guiding the agent\nto make action decisions.\nD. Is a Single Resolution Dynamics Model Sufficient?\nAlthough there is a trade-off between representation reso-\nlution and task progression, can we benefit from this trade-\noff in trajectory optimization? We compared our dynamic-\nresolution dynamics model with fixed-resolution dynamics\nmodels on Gather and Redistribute tasks. Figure 1 shows\nhow our model changes its resolution prediction as MPC\nproceeds in the real world. Trained on the generated dataset\nof optimal \u03c9, our regressor learned that fixing a resolution\nthroughout the MPC process is not optimal. Instead, our\nregressor learns to adapt the resolution according to the current\nobservation feedback. In addition, for the example shown in\nFigure 1, we can see that the resolution increases as object\npiles approach the goal. This matches our expectation as\nexplained in Section IV-C.\nWe quantitatively evaluate different fixed-resolution base-\nlines and our adaptive representation learning algorithms in\nsimulation. We record the final step distribution distance\nbetween object piles and the goal. Specifically, given a distance\nthreshold \u03c4p, the number of tasks with a distance lower than\n\u03c4p is Np, and the total number of tasks is N. The task\nscore is then defined as Np/N (i.e., y-axis in Figure 5). Our\nadaptive resolution model almost always achieves the highest\ntask score, regardless of the threshold used.\nFigure 6a shows a qualitative comparison between the\nfixed-resolution baselines and our dynamic-resolution selec-\ntion method on the Gather task in the real world. All methods\nstart from a near-identical configuration. We can see from the\nqualitative results that our method manipulates the object pile\nto a configuration closest to the goal region, whereas the best-\nperforming fixed-resolution baseline still has some outlying\npieces far from the goal region. In addition, we could see\nfrom the quantitative evaluation curve in Figure 6b that our\nmodel is always the best throughout the whole MPC process.\nRepresentations with an excessively high resolution are un-\nlikely to converge to a decent solution within the time budget,\nas demonstrated by resolutions 75 and 100. Conversely, if the\nrepresentation is too low resolution, it will converge to a loss\nmuch higher than our model. A resolution of 25 reached a\ncomparable final loss to our method. However, because the\nsame resolution was ineffective for initial timesteps, its loss\ndoes not reduce as rapidly as our adaptive approach. Because\nour model could adapt to different resolutions in different\nscenes, making it more effective at control optimization.\nThat is why our model could reach the goal region faster\nthan all other fixed-resolution models and consistently per-\nforms better at all timestamps, highlighting the benefits of\nadaptive resolution selection.\nE. Can a Unified Dynamics Model Achieve All Three Tasks?\nWe further demonstrate that our method could work on\nall three tasks and diverse object piles. For the Gather task,\nwe test our method on different objects with different initial\nand goal configurations. From left to right in Figure 6c, our\nagent gathers different object piles made with almond, granola,\nor M&MTM. Different appearances and physical properties\nchallenge our method\u2019s generalization capability. For example,\nwhile almonds and granola are almost quasi-static during\nthe manipulation, M&MTM will roll around and have high\nuncertainties in its dynamics. In addition, unlike almonds and\nM&MTM, granola pieces are non-uniform. Our method has a\ngood performance for all these objects and configurations.\nFor the Redistribute task, we redistribute carrots and al-\nmonds into target letters \u2018J\u2019, \u2018T\u2019, and \u2018U\u2019 with spread-out\ninitial configurations. The final results match the desired letter\nshape. Please check our supplementary materials for video\nillustrations of the manipulation process.\nFor the Sort task, we use a high-level motion planner to\nfind the intermediate waypoints in the image space. Then\nwe use a similar method as Gather task to push the object\npile into the target location. For the three examples shown\nin Figure 6e, we require object piles to go to their own\ntarget locations while not mixing with each other. Objects\nwith different scales and shapes are present here. For example,\ncoffee beans have smaller granularity and round shapes, while\ncandies are relatively large and square. Here we demonstrate\nsuccess trials of manipulating the object piles to accomplish\nthe Sort task for different objects and goal configurations.\nPlease check our video for the manipulation process.\nV. CONCLUSION\nDynamics models play an important role in robotics. Prior\nworks developed dynamics models based on representations\nof various choices, yet they are typically fixed throughout\nthe entire task. In this work, we introduced a dynamic and\nadaptive scene representation learning framework that could\nautomatically find a trade-off between efficiency and effec-\ntiveness for different tasks and scenes. The resolution of the\nscene representation is predicted online at each time step.\nAnd a unified dynamics model, instantiated as GNNs, predicts\nthe evolution of the dynamically-selected representation. The\ndownstream MPC then plans the action sequence to minimize\nthe task objective. We evaluate our method on three chal-\nlenging object pile manipulation tasks with diverse initial and\ngoal configurations. We show that our model can dynamically\ndetermine the optimal resolution online and has better control\nperformance compared to fixed-resolution baselines.\nAcknowledgments: This work is in part supported by\nStanford Institute for Human-Centered Artificial Intelligence\n(HAI), Toyota Research Institute (TRI), NSF RI #2211258,\nONR MURI N00014-22-1-2740, and Amazon.\nREFERENCES\n[1] Edward H Adelson, Charles H Anderson, James R\nBergen, Peter J Burt, and Joan M Ogden.\nPyramid\nmethods in image processing. RCA engineer, 29(6):33\u2013\n41, 1984.\n[2] Andrew G Barto and Sridhar Mahadevan.\nRecent ad-\nvances in hierarchical reinforcement learning. Discrete\nevent dynamic systems, 13(1-2):41\u201377, 2003.\n[3] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo\nJimenez Rezende, et al. Interaction networks for learning\nabout objects, relations and physics. Advances in neural\ninformation processing systems, 29, 2016.\n[4] Peter W Battaglia, Jessica B Hamrick, Victor Bapst,\nAlvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz\nMalinowski, Andrea Tacchetti, David Raposo, Adam\nSantoro, Ryan Faulkner, et al.\nRelational inductive\nbiases, deep learning, and graph networks. arXiv preprint\narXiv:1806.01261, 2018.\n[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent.\nRepresentation learning: A review and new perspectives.\nIEEE transactions on pattern analysis and machine in-\ntelligence, 35(8):1798\u20131828, 2013.\n[6] Matthew Michael Botvinick. Hierarchical reinforcement\nlearning and decision making. Current opinion in neu-\nrobiology, 22(6):956\u2013962, 2012.\n[7] Peter J Burt and Edward H Adelson.\nThe laplacian\npyramid as a compact image code.\nIn Readings in\ncomputer vision, pages 671\u2013679. Elsevier, 1987.\n[8] Eduardo F Camacho and Carlos Bordons Alba. Model\npredictive control. Springer science & business media,\n2013.\n[9] Michael B Chang, Tomer Ullman, Antonio Torralba, and\nJoshua B Tenenbaum.\nA compositional object-based\napproach to learning physical dynamics. arXiv preprint\narXiv:1612.00341, 2016.\n[10] Peng Chang and Tas\u00b8k\u0131n Pad\u0131r. Model-based manipulation\nof linear flexible objects with visual curvature feed-\nback. In 2020 IEEE/ASME International Conference on\nAdvanced Intelligent Mechatronics (AIM), pages 1406\u2013\n1412. IEEE, 2020.\n[11] Andrea Cherubini, Valerio Ortenzi, Akansel Cosgun,\nRobert Lee, and Peter Corke. Model-free vision-based\nshaping of deformable plastic materials.\nThe Interna-\ntional Journal of Robotics Research, 39(14):1739\u20131759,\n2020.\n[12] Samuel Clarke, Travers Rhodes, Christopher G Atke-\nson, and Oliver Kroemer.\nLearning audio feedback\nfor estimating amount and flow of granular material.\nProceedings of Machine Learning Research, 87, 2018.\n[13] Ingrid Daubechies.\nTen lectures on wavelets.\nSIAM,\n1992.\n[14] Lokenath Debnath and Firdous Ahmad Shah. Wavelet\ntransforms and their applications. Springer, 2002.\n[15] Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake,\nand Marc Toussaint.\nLearning multi-object dynamics\nwith compositional neural radiance fields. arXiv preprint\narXiv:2202.11855, 2022.\n[16] Nima Fazeli, Miquel Oller, Jiajun Wu, Zheng Wu,\nJoshua B Tenenbaum, and Alberto Rodriguez.\nSee,\nfeel, act: Hierarchical learning for complex manipulation\nskills with multisensory fusion. Science Robotics, 4(26):\neaav3123, 2019.\n[17] Chelsea Finn and Sergey Levine. Deep visual foresight\nfor planning robot motion. In 2017 IEEE International\nConference on Robotics and Automation (ICRA), pages\n2786\u20132793. IEEE, 2017.\n[18] Niklas Funk, Georgia Chalvatzaki, Boris Belousov, and\nJan Peters. Learn2assemble with structured representa-\ntions and search for robotic architectural construction.\nIn Conference on Robot Learning, pages 1401\u20131411.\nPMLR, 2022.\n[19] Roman Garnett.\nBayesian optimization.\nCambridge\nUniversity Press, 2023.\n[20] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mo-\nhammad Norouzi. Dream to control: Learning behaviors\nby latent imagination. arXiv preprint arXiv:1912.01603,\n2019.\n[21] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben\nVillegas, David Ha, Honglak Lee, and James Davidson.\nLearning latent dynamics for planning from pixels. In\nInternational conference on machine learning, pages\n2555\u20132565. PMLR, 2019.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun.\nSpatial pyramid pooling in deep convolutional\nnetworks for visual recognition. IEEE transactions on\npattern analysis and machine intelligence, 37(9):1904\u2013\n1916, 2015.\n[23] Franc\u00b8ois Robert Hogan and Alberto Rodriguez. Feedback\ncontrol of the pusher-slider system: A story of hybrid\nand underactuated contact dynamics.\narXiv preprint\narXiv:1611.08268, 2016.\n[24] Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-\nFei, and Juan Carlos Niebles. Learning to decompose\nand disentangle representations for video prediction. Ad-\nvances in neural information processing systems, 31,\n2018.\n[25] Zixuan Huang, Xingyu Lin, and David Held. Mesh-based\ndynamics with occlusion reasoning for cloth manipula-\ntion. arXiv preprint arXiv:2206.02881, 2022.\n[26] Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien\nGaidon, and Marco Pavone.\nMats: An interpretable\ntrajectory forecasting representation for planning and\ncontrol. arXiv preprint arXiv:2009.07517, 2020.\n[27] Ioannis G Kevrekidis, C William Gear, James M Hyman,\nPanagiotis G Kevrekidis, Olof Runborg, Constantinos\nTheodoropoulos, et al.\nEquation-free, coarse-grained\nmultiscale computation: enabling microscopic simulators\nto perform system-level analysis. Commun. Math. Sci, 1\n(4):715\u2013762, 2003.\n[28] Ioannis G Kevrekidis, C William Gear, and Gerhard\nHummer.\nEquation-free: The computer-aided analysis\nof complex multiscale systems. AIChE Journal, 50(7):\n1346\u20131355, 2004.\n[29] Scott Kuindersma, Robin Deits, Maurice Fallon, Andr\u00b4es\nValenzuela,\nHongkai\nDai,\nFrank\nPermenter,\nTwan\nKoolen, Pat Marion, and Russ Tedrake. Optimization-\nbased locomotion planning, estimation, and control de-\nsign for the atlas humanoid robot. Autonomous robots,\n40:429\u2013455, 2016.\n[30] J Nathan Kutz.\nData-driven modeling & scientific\ncomputation: methods for complex systems & big data.\nOxford University Press, 2013.\n[31] J Nathan Kutz, Xing Fu, and Steven L Brunton. Mul-\ntiresolution dynamic mode decomposition. SIAM Journal\non Applied Dynamical Systems, 15(2):713\u2013735, 2016.\n[32] Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenen-\nbaum, and Antonio Torralba. Learning particle dynamics\nfor manipulating rigid bodies, deformable objects, and\nfluids. arXiv preprint arXiv:1810.01566, 2018.\n[33] Yunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B Tenen-\nbaum, Antonio Torralba, and Russ Tedrake. Propagation\nnetworks for model-based control under partial observa-\ntion. In 2019 International Conference on Robotics and\nAutomation (ICRA), pages 1205\u20131211. IEEE, 2019.\n[34] Yunzhu\nLi,\nShuang\nLi,\nVincent\nSitzmann,\nPulkit\nAgrawal, and Antonio Torralba.\n3d neural scene rep-\nresentations for visuomotor control. In Conference on\nRobot Learning, pages 112\u2013123. PMLR, 2022.\n[35] Xingyu Lin, Yufei Wang, Zixuan Huang, and David\nHeld. Learning visible connectivity dynamics for cloth\nsmoothing.\nIn Conference on Robot Learning, pages\n256\u2013266. PMLR, 2022.\n[36] Qingkai Lu and Liangjun Zhang. Excavation learning for\nrigid objects in clutter. IEEE Robotics and Automation\nLetters, 6(4):7373\u20137380, 2021.\n[37] Miles Macklin, Matthias M\u00a8uller, Nuttapong Chentanez,\nand Tae-Yong Kim. Unified particle physics for real-time\napplications. ACM Transactions on Graphics (TOG), 33\n(4):1\u201312, 2014.\n[38] Lucas Manuelli, Yunzhu Li, Pete Florence, and Russ\nTedrake.\nKeypoints into the future: Self-supervised\ncorrespondence in model-based reinforcement learning.\narXiv preprint arXiv:2009.05085, 2020.\n[39] David Marr. Vision: A computational investigation into\nthe human representation and processing of visual infor-\nmation. MIT press, 2010.\n[40] Carolyn Matl, Yashraj Narang, Ruzena Bajcsy, Fabio\nRamos, and Dieter Fox. Inferring the material properties\nof granular media for robotic tasks. In 2020 ieee inter-\nnational conference on robotics and automation (icra),\npages 2770\u20132777. IEEE, 2020.\n[41] Matthias Minderer, Chen Sun, Ruben Villegas, Forrester\nCole, Kevin P Murphy, and Honglak Lee. Unsupervised\nlearning of object structure and dynamics from videos.\nAdvances in Neural Information Processing Systems, 32,\n2019.\n[42] Peter Mitrano, Dale McConachie, and Dmitry Berenson.\nLearning where to trust unreliable models in an unstruc-\ntured world for deformable object manipulation. Science\nRobotics, 6(54):eabd8170, 2021.\n[43] Carsten Moenning and Neil A Dodgson. Fast marching\nfarthest point sampling. Technical report, University of\nCambridge, Computer Laboratory, 2003.\n[44] Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick\nHaber, Li F Fei-Fei, Josh Tenenbaum, and Daniel L\nYamins. Flexible neural representation for physics pre-\ndiction.\nAdvances in neural information processing\nsystems, 31, 2018.\n[45] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and\nSergey Levine. Data-efficient hierarchical reinforcement\nlearning.\nAdvances in neural information processing\nsystems, 31, 2018.\n[46] Monica N Nicolescu and Maja J Matari\u00b4c. A hierarchical\narchitecture for behavior-based robots. In Proceedings\nof the first international joint conference on Autonomous\nagents and multiagent systems: part 1, pages 227\u2013233,\n2002.\n[47] Tao Pang, HJ Suh, Lujie Yang, and Russ Tedrake. Global\nplanning for contact-rich manipulation via local smooth-\ning of quasi-dynamic contact models.\narXiv preprint\narXiv:2206.10787, 2022.\n[48] Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan,\nand Chai Quek.\nHierarchical reinforcement learning:\nA comprehensive survey.\nACM Computing Surveys\n(CSUR), 54(5):1\u201335, 2021.\n[49] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez,\nand Peter W Battaglia. Learning mesh-based simulation\nwith graph networks. arXiv preprint arXiv:2010.03409,\n2020.\n[50] Haozhi Qi, Xiaolong Wang, Deepak Pathak, Yi Ma, and\nJitendra Malik. Learning long-term visual dynamics with\nregion proposal interaction networks.\narXiv preprint\narXiv:2008.02265, 2020.\n[51] Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias\nSpringenberg, Josh Merel, Martin Riedmiller, Raia Had-\nsell, and Peter Battaglia. Graph networks as learnable\nphysics engines for inference and control. In Interna-\ntional Conference on Machine Learning, pages 4470\u2013\n4479. PMLR, 2018.\n[52] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias\nPfaff, Rex Ying, Jure Leskovec, and Peter Battaglia.\nLearning to simulate complex physics with graph net-\nworks. In International conference on machine learning,\npages 8459\u20138468. PMLR, 2020.\n[53] Connor Schenck, Jonathan Tompson, Sergey Levine, and\nDieter Fox. Learning robotic manipulation of granular\nmedia. In Conference on Robot Learning, pages 239\u2013\n248. PMLR, 2017.\n[54] Bokui Shen, Zhenyu Jiang, Christopher Choy, Leonidas J\nGuibas, Silvio Savarese, Anima Anandkumar, and Yuke\nZhu. Acid: Action-conditional implicit visual dynamics\nfor deformable object manipulation.\narXiv preprint\narXiv:2203.06856, 2022.\n[55] Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and\nJiajun Wu.\nRobocraft: Learning to see, simulate, and\nshape elasto-plastic objects with graph networks. arXiv\npreprint arXiv:2205.02909, 2022.\n[56] Tom Silver, Rohan Chitnis, Aidan Curtis, Joshua B\nTenenbaum, Tom\u00b4as Lozano-P\u00b4erez, and Leslie Pack Kael-\nbling. Planning with learned object importance in large\nproblem instances using graph neural networks. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 35, pages 11962\u201311971, 2021.\n[57] Jasper Snoek, Hugo Larochelle, and Ryan P Adams.\nPractical bayesian optimization of machine learning al-\ngorithms.\nAdvances in neural information processing\nsystems, 25, 2012.\n[58] HJ Suh and Russ Tedrake. The surprising effectiveness\nof linear models for visual foresight in object pile ma-\nnipulation. arXiv preprint arXiv:2002.09093, 2020.\n[59] Hyung Ju Terry Suh, Tao Pang, and Russ Tedrake. Bun-\ndled gradients through contact via randomized smooth-\ning. IEEE Robotics and Automation Letters, 7(2):4000\u2013\n4007, 2022.\n[60] Kuniyuki Takahashi, Wilson Ko, Avinash Ummadisingu,\nand Shin-ichi Maeda. Uncertainty-aware self-supervised\ntarget-mass grasping of granular foods. In 2021 IEEE\nInternational Conference on Robotics and Automation\n(ICRA), pages 2620\u20132626. IEEE, 2021.\n[61] Russ Tedrake. Underactuated robotics: Learning, plan-\nning, and control for efficient and agile machines course\nnotes for mit 6.832. Working draft edition, 3:4, 2009.\n[62] Naftali Tishby, Fernando C Pereira, and William Bialek.\nThe information bottleneck method.\narXiv preprint\nphysics/0004057, 2000.\n[63] Hsiao-Yu Fish Tung, Zhou Xian, Mihir Prabhude-\nsai, Shamit Lal, and Katerina Fragkiadaki.\n3d-oes:\nViewpoint-invariant object-factorized environment simu-\nlators. arXiv preprint arXiv:2011.06464, 2020.\n[64] Neea Tuomainen, David Blanco-Mulero, and Ville Kyrki.\nManipulation of granular materials by learning particle\ninteractions. IEEE Robotics and Automation Letters, 7\n(2):5663\u20135670, 2022.\n[65] Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and\nVladlen Koltun. Lagrangian fluid simulation with con-\ntinuous convolutions.\nIn International Conference on\nLearning Representations, 2020.\n[66] Alexander Sasha Vezhnevets, Simon Osindero, Tom\nSchaul, Nicolas Heess, Max Jaderberg, David Silver, and\nKoray Kavukcuoglu.\nFeudal networks for hierarchical\nreinforcement learning. In International Conference on\nMachine Learning, pages 3540\u20133549. PMLR, 2017.\n[67] Weiyao Wang, Andrew S Morgan, Aaron M Dollar,\nand Gregory D Hager. Dynamical scene representation\nand control with keypoint-conditioned neural radiance\nfield.\nIn 2022 IEEE 18th International Conference\non Automation Science and Engineering (CASE), pages\n1138\u20131143. IEEE, 2022.\n[68] Nicholas Watters, Daniel Zoran, Theophane Weber, Peter\nBattaglia, Razvan Pascanu, and Andrea Tacchetti. Visual\ninteraction networks: Learning a physics simulator from\nvideo. Advances in neural information processing sys-\ntems, 30, 2017.\n[69] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Ken\nGoldberg, and Pieter Abbeel.\nDaydreamer: World\nmodels for physical robot learning.\narXiv preprint\narXiv:2206.14176, 2022.\n[70] Yufei Ye, Maneesh Singh, Abhinav Gupta, and Shubham\nTulsiani. Compositional video prediction. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pages 10353\u201310362, 2019.\n[71] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli,\nJiajun Wu, Antonio Torralba, and Joshua B Tenenbaum.\nClevrer: Collision events for video representation and\nreasoning. arXiv preprint arXiv:1910.01442, 2019.\n[72] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2881\u20132890, 2017.\n[73] Jiaji Zhou, Yifan Hou, and Matthew T Mason. Pushing\nrevisited: Differential flatness, trajectory planning, and\nstabilization.\nThe International Journal of Robotics\nResearch, 38(12-13):1477\u20131489, 2019.\n[74] Guangxiang Zhu, Zhiao Huang, and Chongjie Zhang.\nObject-oriented dynamics predictor. Advances in Neural\nInformation Processing Systems, 31, 2018.\n[75] Yifan Zhu, Laith Abdulmajeid, and Kris Hauser. A data-\ndriven approach for fast simulation of robot locomotion\non granular media.\nIn 2019 international conference\non robotics and automation (ICRA), pages 7653\u20137659.\nIEEE, 2019.\n"
  },
  {
    "title": "BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion",
    "link": "https://arxiv.org/pdf/2306.16940.pdf",
    "upvote": "4",
    "text": "BEDLAM: A Synthetic Dataset of\nBodies Exhibiting Detailed Lifelike Animated Motion\nMichael J. Black1,*\nPriyanka Patel1,*\nJoachim Tesch1,*\nJinlong Yang2,*,\u2020\n1Max Planck Institute for Intelligent Systems, T\u00a8ubingen, Germany\n2Google\nFigure 1. BEDLAM is a large-scale synthetic video dataset designed to train and test algorithms on the task of 3D human pose and shape\nestimation (HPS). BEDLAM contains diverse body shapes, skin tones, and motions. Beyond previous datasets, BEDLAM has SMPL-X\nbodies with hair and realistic clothing animated using physics simulation. With BEDLAM\u2019s realism and scale, we find that synthetic data\nis sufficient to train regressors to achieve state-of-the-art HPS accuracy on real-image datasets without using any real training images.\nAbstract\nWe show, for the first time, that neural networks trained\nonly on synthetic data achieve state-of-the-art accuracy on\nthe problem of 3D human pose and shape (HPS) estima-\ntion from real images.\nPrevious synthetic datasets have\nbeen small, unrealistic, or lacked realistic clothing. Achiev-\ning sufficient realism is non-trivial and we show how to\ndo this for full bodies in motion. Specifically, our BED-\nLAM dataset contains monocular RGB videos with ground-\ntruth 3D bodies in SMPL-X format. It includes a diver-\nsity of body shapes, motions, skin tones, hair, and cloth-\ning. The clothing is realistically simulated on the moving\nbodies using commercial clothing physics simulation. We\nrender varying numbers of people in realistic scenes with\nvaried lighting and camera motions. We then train vari-\nous HPS regressors using BEDLAM and achieve state-of-\nthe-art accuracy on real-image benchmarks despite train-\ning with synthetic data. We use BEDLAM to gain insights\n*The authors contributed equally and are listed alphabetically.\n\u2020This work was performed when JY was at MPI-IS.\ninto what model design choices are important for accu-\nracy.\nWith good synthetic training data, we find that a\nbasic method like HMR approaches the accuracy of the\ncurrent SOTA method (CLIFF). BEDLAM is useful for a\nvariety of tasks and all images, ground truth bodies, 3D\nclothing, support code, and more are available for research\npurposes.\nAdditionally, we provide detailed information\nabout our synthetic data generation pipeline, enabling oth-\ners to generate their own datasets. See the project page:\nhttps://bedlam.is.tue.mpg.de/.\n1. Introduction\nThe estimation of 3D human pose and shape (HPS)\nfrom images has progressed rapidly since the introduc-\ntion of HMR [36], which uses a neural network to regress\nSMPL [49] pose and shape parameters from an image. A\nsteady stream of new methods have improved the accuracy\nof the estimated 3D bodies [25,37,39,42,45,83,106]. The\nprogress, however, entangles two things: improvements to\nthe architecture and improvements to the training data. This\nmakes it difficult to know which matters most. To answer\narXiv:2306.16940v1  [cs.CV]  29 Jun 2023\nthis, we need a dataset with real ground truth 3D bodies\nand not simply 2D joint locations or pseudo ground truth.\nTo that end, we introduce a new, realistic, synthetic dataset\ncalled BEDLAM (Bodies Exhibiting Detailed Lifelike An-\nimated Motion) and use it to analyze the current state of the\nart (SOTA). Fig. 1 shows example images from BEDLAM\nalong with the ground-truth SMPL-X [63] bodies.\nTheoretically, synthetic data has many benefits.\nThe\nground truth is \u201cperfect\u201d by construction, compared with\nexisting image datasets.\nWe can ensure diversity of the\ntraining data across skin tones, body shapes, ages, etc., so\nthat HPS methods are inclusive. The data can also be easily\nrepurposed to new cameras, scenes, and sensors. Conse-\nquently, there have been many attempts to create synthetic\ndatasets to train HPS methods. While prior work has shown\nsynthetic data is useful, it has not been sufficient so far. This\nis likely due to the lack of realism and diversity in existing\nsynthetic datasets.\nIn contrast, BEDLAM provides the realism necessary to\ntest whether \u201csynthetic data is all you need\u201d. Using BED-\nLAM, we evaluate different network architectures, back-\nbones, and training data and find that training only using\nsynthetic data produces methods that generalize to real im-\nage benchmarks, obtaining SOTA accuracy on both 3D hu-\nman pose and 3D body shape estimation. Surprisingly, we\nfind that even basic methods like HMR [36] achieve SOTA\nperformance on real images when trained on BEDLAM.\nDataset. BEDLAM contains monocular RGB videos to-\ngether with ground truth 3D bodies in SMPL-X format. To\ncreate diverse data, we use 271 body shapes (109 men and\n162 women), with 100 skin textures from Meshcapade [3]\ncovering a wide range of skin tones. In contrast to previous\nwork, we add 27 different types of hair (Reallusion [1]) to\nthe head of SMPL-X. To dress the body, we hired a profes-\nsional 3D clothing designer to make 111 outfits, which we\ndrape and simulate on the body using CLO3D [2]. We also\ntexture the clothing using 1691 artist-designed textures [6].\nThe bodies are animated using 2311 motions sampled from\nAMASS [51]. Because AMASS does not include hand mo-\ntions, we replace the static hands with hand motions sam-\npled from the GRAB dataset [84]. We render single people\nas well as groups of people (varying from 3-10) moving in a\nvariety of 3D scenes (8) and HDRI panoramas (95). We use\na simple method to place multiple people in the scenes so\nthat they do not collide and use simulated camera motions\nwith various focal lengths. The synthetic image sequences\nare rendered using Unreal Engine 5 [5] at 30 fps with mo-\ntion blur. In total, BEDLAM contains around 380K unique\nimage frames with 1-10 people per image, for a total of 1M\nunique bounding boxes with people.\nWe divide BEDLAM into training, validation, and test\nsets with 75%, 20% and 5% of the total bounding boxes\nrespectively. While we make all the image data available,\nwe withhold the SMPL-X ground truth from the test set and\nprovide an automated evaluation server. For the training and\nvalidation sets, we provide all the SMPL-X animations, the\n3D clothing, skin textures, and all freely available assets.\nWhere we have used commercial assets, we provide infor-\nmation about how to obtain the data and replicate our re-\nsults. We also provide the details necessary for researchers\nto create their own data.\nEvaluation. With sufficient high-quality training data,\nfairly simple neural-network architectures often produce\nSOTA results on many vision tasks. Is this true for HPS\nregression? To tackle this question, we train two different\nbaseline methods (HMR [36] and CLIFF [42]) on varying\namounts of data and with different backbones; HMR repre-\nsents the most basic method and CLIFF the recent SOTA.\nSince BEDLAM provides paired images with SMPL-X pa-\nrameters, we train methods to directly regress these parame-\nters; this simplifies the training compared with methods that\nuse 2D training data. We evaluate on natural-image datasets\nincluding 3DPW [89] and RICH [30], a laboratory dataset\n(Human3.6M [31]), as well as two datasets that evaluate\nbody shape accuracy (SSP-3D [76] and HBW [19]).\nSurprisingly, despite its age, we find that training HMR\non synthetic data produces results on 3DPW that are bet-\nter than many recently published results and are close to\nCLIFF. We find that the backbone has a large impact on\naccuracy, and pre-training on COCO is significantly better\nthan pre-training on ImageNet or from scratch. We perform\na large number of experiments in which we train with just\nsynthetic data, just real data, or synthetic data followed by\nfine tuning on real data. We find that there is a significant\nbenefit to training on synthetic data over real data and that\nfine tuning with real data offers only a small benefit.\nA key property of BEDLAM is that it contains realisti-\ncally dressed people with ground truth body shape. Con-\nsequently, we compare the performance of methods trained\non BEDLAM with two SOTA methods for body shape re-\ngression: SHAPY [19] and Sengupta et al. [77] using both\nthe HBW and SSP-3D datasets. CLIFF trained with BED-\nLAM does well on both datasets, achieving the best overall\nof all methods tested. This illustrates how methods trained\non BEDLAM generalize across tasks and datasets.\nSummary. We propose a large synthetic dataset of re-\nalistic moving 3D humans. We show that training on syn-\nthetic dataset alone, even with a basic network architecture,\nproduces accurate 3D human pose and shape estimates on\nreal data. BEDLAM enables us to perform an extensive\nmeta-ablation study that illuminates which design decisions\nare most important. While we focus on HPS, the dataset has\nmany other uses in learning 3D clothing models and action\nrecognition. BEDLAM is available for research purposes\ntogether with an evaluation server and the assets needed to\ngenerate new datasets.\n2. Related work\nThere are four main types of data used to train HPS re-\ngressors: (1) Real images from constrained scenarios with\nhigh-quality ground truth (lab environments with motion\ncapture). (2) Real images in-the-wild with 2D ground truth\n(2D keypoints, silhouettes, etc.). (3) Real images in-the-\nwild with 3D pseudo ground truth (estimated from 2D or\nusing additional sensors). (4) Synthetic images with perfect\nground truth. Each of these has played an important role in\nadvancing the field to its current state. The ideal training\ndata would have perfect ground truth 3D human shape and\npose information together with fully realistic and highly di-\nverse imagery. None of the above fully satisfy this goal. We\nbriefly review 1-3 while focusing our analysis on 4.\nReal Images.\nReal images are diverse, complex, and\nplentiful.\nMost methods that use them for training rely\non 2D keypoints, which are easy to manually label at\nscale [8, 32, 46, 52]. Such data relies on human annota-\ntors who may not be consistent, and only provides 2D con-\nstraints on human pose with no information about 3D body\nshape. In controlled environments, multiple cameras and\nmotion capture equipment provide accurate ground truth\n[11, 14, 16, 28, 30, 31, 35, 41, 58, 79, 87, 89, 100, 107]. In\ngeneral, the cost and complexity of such captures limits the\nnumber of subjects, the variety of clothing, the types of mo-\ntion, and the number of scenes.\nSeveral methods fit 3D body models to images to get\npseudo ground truth SMPL parameters [34, 39, 56]. Net-\nworks trained on such data inherit any biases of the meth-\nods used to compute the ground truth; e.g. a tendency to es-\ntimate bent knees, resulting from a biased pose prior. Syn-\nthetic data does not suffer such biases.\nMost image datasets are designed for 3D pose estimation\nand only a few have addressed body shape. SSP-3D [76]\ncontains 311 in-the-wild images of 62 people wearing tight\nsports clothing with pseudo ground truth body shape. Hu-\nman Bodies in the Wild (HBW) [19] uses 3D body scans\nof 35 subjects who are also photographed in the wild with\nvaried clothing. HBW includes 2543 photos with \u201cperfect\u201d\nground truth shape. Neither dataset is sufficiently large to\ntrain a general body shape regressor.\nIn summary, real data for training HPS involves a fun-\ndamental trade off. One can either have diverse and natural\nimages with low-quality ground truth or limited variability\nwith high-quality ground truth.\nSynthetic. Synthetic data promises to address the limita-\ntions of real imagery and there have been many previous\nattempts. While prior work has shown synthetic data to be\nuseful (e.g. for pre-training), no prior work has shown it to\nbe sufficient without additional real training data. We hy-\npothesize that this is due to the fact that prior datasets have\neither been too small or not sufficiently realistic. To date, no\nstate-of-the-art method is trained from synthetic data alone.\nRecently, Microsoft has shown that a synthetic dataset of\nfaces is sufficiently accurate to train high-quality 2D feature\ndetection [92]. While promising, human bodies are more\ncomplex. AGORA [62] provides realistic images of clothed\nbodies from static commercial scans with SMPL-X ground\ntruth. SPEC [38] extends AGORA to more varied camera\nviews. These datasets have limited avatar variation (e.g. few\nobese bodies) and lack motion.\nSynthetic from real. Since creating realistic people us-\ning graphics is challenging, several methods capture real\npeople and then render them synthetically in new scenes\n[26,53,54]. For example, MPI-INF-3DHP [53] captures 3D\npeople, augments their body shape, and swaps out clothing\nbefore compositing the people on images. Like real data,\nthese capture approaches are limited in size and variety. An-\nother direction takes real images of people plus information\nabout body pose and, using machine learning methods, syn-\nthesizes new images that look natural [71, 102]. This is a\npromising direction but, to date, no work has shown that\nthis is sufficient train HPS regressors.\nSynthetic data without clothing. Synthesizing images\nof 3D humans on image backgrounds has a long history\n[80]. We focus on more recent datasets for training HPS re-\ngressors for parametric 3D human body models like SCAPE\n[9] (e.g. Deep3DPose [18]) and SMPL [49] (e.g. SUR-\nREAL [88]). Both apply crude textures to the naked body\nand then render the bodies against random image back-\ngrounds.\nIn [18, 29], the authors use domain adaptation\nmethods to reduce the domain gap between synthetic and\nreal images. In [88] the authors use synthetic data largely\nfor pre-training, requiring fine tuning on real images.\nSince realistic clothes and textures are hard to gener-\nate, several methods render SMPL silhouettes or part seg-\nments and then learn to regress HPS from these [64,73,96].\nWhile one can generate an infinite amount of such data,\nthese methods rely on a separate process to compute sil-\nhouettes from images, which can be error prone. For exam-\nple, STRAPS [76] uses synthetic data to regress body shape\nfrom silhouettes.\nSynthetic data with rigged clothing. Another approach\nrenders commercial, rigged, body models for which the\nclothing deformations are not realistic. For example PSP-\nHDRI+ [23], 3DPeople [65], and JTA [24] use rigged char-\nacters but provide only 3D skeletons so they cannot be used\nfor body shape estimation. The Human3.6M dataset [31]\nincludes mixed-reality data with rigged characters inserted\ninto real videos. There are only 5 sequences, 7.5K frames,\nand a limited number of rigged models, making it too small\nfor training. Multi-Garment Net (MGN) [13] constructs a\nwardrobe from rigged 3D scans but renders them on im-\nages with no background.\nSynthetic data has also been\nused to estimate ego-motion from head-mounted cameras\nFigure 2. Dataset construction. Illustration of each step in the process, shown for a single character. Left to right: (a) sampled body\nshape. (b) skin texture. (c) clothing simulation. (d) cloth texture. (e) hair. (f) pose. (g) scene and illumination. (h) motion blur.\nFigure 3. Skin tone diversity. Example body textures from 50 male\nand 50 female textures, covering a wide range of skin tones.\n[7, 86, 95]. HSPACE [10] uses 100 rigged people with 100\nmotions and 100 3D scenes. To get more variety, they fit\nGHUM [94] to the scans and reshape them. They train an\nHPS method [103] on the data and note that \u201cmodels trained\non synthetic data alone do not perform the best, not even\nwhen tested on synthetic data.\u201d This statement is consistent\nwith the findings of other methods and points to the need\nfor increased diversity to achieve generalization.\nSimulated clothing with images. Physics-based cloth\nsimulation provides greater realism than rigged clothing and\nallows us to dress a wide range of bodies in varied clothing\nwith full control. The problem, however, is that physics\nsimulation is challenging and this limits the size and com-\nplexity of previous datasets. Liang and Lin [43] and Liu\net al. [48] simulate 3D clothing draped on SMPL bodies.\nThey render the people on image backgrounds with limited\nvisual realism. BCNet [33] uses both physics simulation\nand rigged avatars but the dataset is aimed at 3D clothing\nmodeling more than HPS regression. Other methods use a\nvery limited number of garments or body shapes [21,91].\nSimulated clothing without images. Several methods\ndrape clothing on the 3D body to create datasets for learning\n3D clothing deformations [12,27,61,75,85]. These datasets\nare limited in size and do not contain rendered images.\nSummary. The prior work is limited in one or more\nof these properties: body shapes, textures, poses, motions,\nbackgrounds, clothing types, physical realism, cameras, etc.\nAs a result, these datasets are not sufficient for training HPS\nmethods that work on real images.\n3. Dataset\nEach step in the process of creating BEDLAM is ex-\nplained below and illustrated in Fig. 2. Rendering is per-\nformed using Unreal Engine 5 (UE5) [5]. Additionally, the\nSup. Mat. provides details about the process and all the 3D\nassets. The Supplemental Video shows example sequences.\nFigure 4. Diversity of clothing and texture. Top: samples from\nBEDLAM\u2019s 111 outfits with real-world complexity. Bottom: each\noutfit has several clothing textures. Total: 1691.\n3.1. Dataset Creation\nBody shapes.\nWe want a diversity of body shapes, from\nslim to obese. We get 111 adult bodies in SMPL-X format\nfrom AGORA dataset. These bodies mostly correspond to\nmodels with low BMI. To increase diversity, we sample an\nadditional 80 male and 80 female bodies with BMI > 30\nfrom the CAESAR dataset [70].\nThus we sample body\nshapes from a diverse pool of 271 body shapes in total.\nThe ground truth body shapes are represented with 11 shape\ncomponents in the SMPL-X gender-neutral shape space.\nSee Sup. Mat. for more details about the body shapes.\nSkin tone diversity. HPS estimation will be used in a wide\nrange of applications, thus it is important that HPS solutions\nbe inclusive. Existing HPS datasets have not been designed\nto ensure diversity and this is a key advantage of synthetic\ndata. Specifically, we use 50 female and 50 male commer-\ncial skin albedo textures from Meshcapade [3] with mini-\nmal clothing and a resolution of 4096x4096. These artist-\ncreated textures represent a total of seven ethnic groups\n(African, Asian, Hispanic, Indian, Mideast, South East\nAsian and White) with multiple variations within each. A\nfew examples are shown in Fig. 3.\n3D Clothing and textures. A key limitation of previous\nsynthetic datasets is the lack of diverse and complex 3D\nclothing with realistic physics simulation of the clothing in\nmotion. To address this, we hired a 3D clothing designer\nto create 111 unique real-world outfits, including but not\nFigure 5. Clothing as texture maps for high-BMI bodies. Left:\nexample simulated clothing. Right: clothing texture mapped on\nbodies with BMIs of 30, 40, and 50.\nFigure 6. 10 examples of BEDLAM\u2019s 27 hairstyles.\nlimited to T-shirts, shirts, jeans, tank tops, sweaters, coats,\nduvet jackets, suits, gowns, bathrobes, vests, shorts, pants,\nand skirts. Unlike existing synthetic clothing datasets, our\nclothing designs have complex and realistic structure and\ndetails such as pleats, pockets, and buttons. Example out-\nfits are shown in Fig. 4. We use commercial simulation\nsoftware from CLO3D [2] to obtain realistic clothing defor-\nmations with various body motions for the bodies from the\nAGORA dataset (see Supplemental Video). This 3D dataset\nis a unique resource that we will make available to support\na wide range of research on learning models of 3D clothing.\nDiversity of clothing appearance is also important. For\neach outfit we design 5 to 27 clothing textures with different\ncolors and patterns using WowPatterns [6]. In total we have\n1691 unique clothing textures (see Fig. 4).\nFor high-BMI bodies, physics simulation of clothing\nfails frequently due to the difficulty of garment auto-\nresizing and interpenetration between body parts. For such\nsituations, we use clothing texture maps that look like cloth-\ning \u201cpainted\u201d on the body. Specifically, we auto-transfer the\ntextures of 1738 simulated garments onto the body UV-map\nusing Blender. We then render high-BMI body shapes using\nthese textures (see Fig. 5).\nHair. We use the Character Creator (CC) software from\nReallusion [1] and purchased hairstyles to generate 27\nhairstyles (Fig. 6). We auto-align our SMPL-X female and\nmale template mesh to the CC template mesh and then\ntransfer the SMPL-X deformations to it. We then apply the\nhairstyles in the CC software to match our custom head-\nshapes. We export the data to Blender to automatically pro-\ncess the hair mesh vertices so that their world vertex po-\nsitions are relative to the head node positioned at the ori-\ngin. Note that vendor-provided plugins take care of the ex-\ntensive shader setup needed for proper rendering of these\nhair-card-based meshes. Finally the \u201cvirtual toupees\u201d are\nimported into Unreal Engine where they are attached to the\nhead nodes of the target SMPL-X animation sequences. The\nworld-pose of each toupee is then automatically driven by\nthe Unreal Engine animation system.\nHuman motions.\nWe sample human motions from the\nAMASS dataset [51]. Due to the long-tail distribution of\nmotions in the dataset, a naive random sampling leads to a\nstrong bias towards a small number of frequent motions, re-\nsulting in low motion diversity. To avoid this, we make use\nof the motion labels provided by BABEL [66]. Specifically,\nwe sample different numbers of motion sequences for each\nmotion category according to their motion diversity (see\nSup. Mat. for details). This leads to 2311 unique motions.\nEach motion sequence lasts from 4 to 8 seconds. Naively\ntransferring these motions to new body shapes in the format\nof joint angle sequences may lead to self-interpenetration,\nespecially for high-BMI bodies. To avoid this, we follow\nthe approach in TUCH [57] to resolve collisions among\nbody parts for all the high-BMI bodies. While the released\ndataset is rendered at 30fps, we only use every 5th frame for\ntraining and evaluation to reduce pose redundancy. The full\nsequences will be useful for research on 3D human track-\ning, e.g. [67,82,98,101].\nUnfortunately, most motion sequences in AMASS con-\ntain no hand motion. To increase realism, diversity, and\nenable research on hand pose estimation, we add hand mo-\ntions sampled from the GRAB [84] dataset. While these\nhand motions do not semantically \u201cmatch\u201d the body motion,\nthe rendered sequences still look realistic, and are sufficient\nfor training full-body and hand regressors.\nScenes and lighting. We represent the environment either\nthrough 95 panoramic HDRI images [4] or through 8 3D\nscenes. We manually select HDRI panoramas that enable\nthe plausible placement of animated bodies on a flat ground\nplane up to a distance of 10m. We randomize the viewpoint\ninto the scenes and use the HDRI images for image-based\nlighting. For the 3D scenes we focus on indoor environ-\nments since the HDRI images already cover outdoor envi-\nronments well. To light the 3D scenes, we either use Light-\nmass precalculated global illumination or the new Lumen\nreal-time global illumination system introduced in UE5 [5].\nMultiple people in the scene. For each sequence we ran-\ndomly select between 1 and 10 subjects. For each subject\na random animation sequence is selected. We leverage bi-\nnary ground occupancy maps and randomly place the mov-\ning people into the scene such that they do not collide with\neach other or scene objects. See Sup. Mat. for details.\nCameras. For BEDLAM, we focus on cameras that one\nnaturally encounters in common computer vision datasets.\nFor most sequences we use a static camera with random-\nized camera extrinsics. The extrinsics correspond to typical\nground-level hand-held cameras in portrait and landscape\nmode. Some sequences use additional extrinsics augmenta-\ntion by simulating a cinematic orbit camera shot. Camera\nintrinsics are either fixed at HFOV of 52 and 65 or zoom in\nfrom 65 to 25 HFOV.\nRendering. We render the image sequences using the UE5\ngame engine rasterizer with the cinematic camera model\nsimulating a 16:9 DSLR camera with a 36x20.25mm sensor\nsize. The built-in movie render subsystem (Movie Render\nQueue) is used for deterministic and high-quality image se-\nquence generation. We simulate motion blur caused by the\ndefault camera shutter speed by generating 7 temporal im-\nage samples for each final output image. A single Windows\n11 PC using one NVIDIA RTX3090 GPU was used to ren-\nder all color images and store them as 1280x720 lossless\ncompressed PNG files with motion blur at an average rate\nof more than 5 images/s.\nDepth maps and segmentation. While our focus is on HPS\nregression, BEDLAM can support other uses. Since the\ndata is synthetic, we also render out depth maps and seg-\nmentation masks with semantic labels (hair, clothing, skin).\nThese are all available as part of the dataset release. See\nSup. Mat. for details.\n3.2. Dataset Statistics\nIn summary, BEDLAM is generated from a combination\nof 271 bodies, 27 hairstyles, 111 types of clothing, with\n1691 clothing textures, 2311 human motions, in 95 HDRI\nscenes and 8 3D scenes, with on average 1-10 person per\nscene, and a variety of camera poses. See Sup. Mat. for\ndetailed statistics. This results in 10K motion clips, from\nwhich we use 380K RGB frames in total. We compute the\nsize of the dataset in terms of the number of unique bound-\ning boxes containing individual people. BEDLAM contains\n1M such bounding boxes, which we divide into sets of about\n750K, 200K, and 50K examples for training, validation, and\ntest, respectively. See Sup. Mat. for a detailed comparison\nof BEDLAM\u2019s size and diversity relative to existing real\nand synthetic datasets.\n4. Experiments\n4.1. Implementation Details\nWe train both HMR and CLIFF on the synthetic data\n(BEDLAM+AGORA) using an HRNet-W48 [81] back-\nbone and refer to these as BEDLAM-HMR and BEDLAM-\nCLIFF respectively. We conduct different experiments with\nthe weights of the backbone initialized from scratch, using\nImageNet [22], or using a pose estimation network trained\non COCO [93]. We represent all ground truth bodies in a\ngender neutral shape space to supervise training; we do not\nuse gender labels. We remove the adversary from HMR\nand set the ground truth hand poses to neutral when training\nBEDLAM-HMR and BEDLAM-CLIFF. We apply a variety\nof data augmentations during training. We experiment with\na variety of losses; the final loss is a combination of MSE\nloss on model parameters, projected keypoints, 3D joints,\nand an L1 loss on 3D vertices.\nWe re-implement CLIFF (called CLIFF\u2020) and train it on\nonly real image data using the same settings as BEDLAM-\nCLIFF. Following [42], we train CLIFF\u2020 using Human3.6M\n[31], MPI-INF-3DHP [53], and 2D datasets COCO [47] and\nMPII [8] with pseudo-GT provided by the CLIFF annotator.\nTable 1 shows that, when trained on real images, and fine-\ntuned on 3DPW training data, CLIFF\u2020 matches the accuracy\nreported in [42] on 3DPW and is even more accurate on\nRICH. Thus our implementation can be used as a reference.\nWe also train a full body network, BEDLAM-CLIFF-X,\nto regress body and hand poses. To train the hand network,\nwe create a dataset of hand crops from BEDLAM training\nimages using the ground truth hand keypoints. Since hands\nare occluded by the body in many images, MediaPipe [50]\nis used to detect the hand in the crop. Only the crops where\nthe hand is detected with a confidence greater than 0.8 are\nused in the training. For details see Sup. Mat.\n4.2. Datasets and Evaluation Metrics\nDatasets.\nFor training we use around 750K crops\nfrom BEDLAM and 85K crops from AGORA [62]. We\nalso finetune BEDLAM-CLIFF and BEDLAM-HMR on\n3DPW training data; these are called BEDLAM-CLIFF*\nand BEDLAM-HMR*. To do so, we convert the 3DPW\n[89] GT labels in SMPL-X format. We use 3DPW for eval-\nuation but, since it has limited camera variation, we also\nuse RICH [30] which has more varied camera angles. Both\n3DPW and RICH have limited body shape variation, hence\nto evaluate body shape we use SSP-3D [76] and HBW [19].\nIn Sup. Mat. we also evaluate on Human3.6M [31] and ob-\nserve that, without fine-tuning on the dataset, training on\nBEDLAM produces more accurate results than training us-\ning real images; that is, BEDLAM generalizes better to the\nlab data. To evaluate the output from BEDLAM-CLIFF-X,\nwe use the AGORA and BEDLAM test sets.\nEvaluation metrics. We use standard metrics to evalu-\nate body pose and shape accuracy. PVE and MPJPE rep-\nresent the average error in vertices and joints positions,\nrespectively, after aligning the pelvis. PA-MPJPE further\naligns the rotation and scale before computing distance.\nPVE-T-SC is per-vertex error in a neutral pose (T-pose) after\nscale-correction [76]. P2P20k is per-vertex error in a neutral\npose, computed by evenly sampling 20K points on SMPL-\nX\u2019s surface [19]. All errors are in mm.\nFor evaluation on 3DPW and SSP-3D, we convert our\npredicted SMPL-X meshes to SMPL format by using a ver-\ntex mapping D \u2208 R10475\u00d76890 [63]. The RICH dataset\nhas ground truth in SMPL-X format but hand poses are less\nreliable than body pose due to noise in multi-view fitting.\nFigure 7. Example BEDLAM-CLIFF results from all test datasets. Left to right: SSP-3D \u00d7 2, HBW \u00d7 3, RICH, 3DPW.\nMethod\n3DPW (14)\nRICH (24)\nPA-MPJPE\nMPJPE\nPVE\nPA-MPJPE\nMPJPE\nPVE\nPARE* [37]\n46.5\n74.5\n88.6\n60.7\n109.2\n123.5\nMETRO* [44]\n47.9\n77.1\n88.2\n64.8\n114.3\n128.9\nCLIFF* [42]\n43.0\n69.0\n81.2\n56.6\n102.6\n115.0\nCLIFF\u2020*\n43.6\n68.8\n82.1\n55.7\n91.6\n104.4\nBEDLAM-HMR*\n43.3\n71.8\n83.6\n50.9\n88.2\n101.8\nBEDLAM-CLIFF*\n43.0\n66.9\n78.5\n50.2\n84.4\n95.6\nHMR [36]\n76.7\n130\nN/A\n90.0\n158.3\n186.0\nSPIN [39]\n59.2\n96.9\n116.4\n69.7\n122.9\n144.2\nSPEC [38]\n53.2\n96.5\n118.5\n72.5\n127.5\n146.5\nPARE [37]\n50.9\n82.0\n97.9\n64.9\n104.0\n119.7\nHybrIK [40]\n48.8\n80\n94.5\n56.4\n96.8\n110.4\nPang et. al. [60]\n47.3\n81.9\n96.5\n63.7\n117.6\n136.5\nCLIFF\u2020\n46.4\n73.9\n87.6\n55.7\n90.0\n102.0\nBEDLAM-HMR\n47.6\n79.0\n93.1\n53.2\n91.4\n106.0\nBEDLAM-CLIFF\n46.6\n72.0\n85.0\n51.2\n84.5\n96.6\nTable 1. Reconstruction error on 3DPW and RICH. *Trained with\n3DPW training set. \u2020Trained on real images with same setting as\nBEDLAM-CLIFF. Parenthesis: (#joints).\nHence, we use it only for evaluating body pose and shape.\nWe convert the ground truth SMPL-X vertices to SMPL for-\nmat using D after setting the hand and face pose to neutral.\nTo compute joint errors, we use 24 joints computed from\nthese vertices using the SMPL joint regressor. For evalua-\ntion on AGORA-test and BEDLAM-test, we use a similar\nevaluation protocol as described in [62].\n4.3. Comparison with the State-of-the-Art\nTable 1 summarizes the key results. (1) Pre-training on\nBEDLAM and fine-tuning with a mix of 3DPW and BED-\nLAM training data gives the most accurate results on 3DPW\nand RICH (i.e. BEDLAM-CLIFF* is more accurate than\nCLIFF\u2020* or [42]). (2) Using the same training, makes HMR\n(i.e. BEDLAM-HMR*) nearly as accurate on 3DPW and\nmore accurate than CLIFF\u2020* on RICH. This suggests that\neven simple methods can do well if trained on good data. (3)\nBEDLAM-CLIFF, with no 3DPW fine-tuning, does nearly\nas well as the fine-tuned version and generalizes better to\nRICH than CLIFF with, or without, 3DPW fine-tuning. (4)\nBoth CLIFF and HMR trained only on synthetic data out-\nperform the recent methods in the field. This suggests that\nmore effort should be put into obtaining high-quality data.\nSee Sup. Mat. for SMPL-X results.\nTable 2 shows that BEDLAM-CLIFF has learned to es-\ntimate body body shape under clothing.\nWhile SHAPY\n[104] performs best on HBW and Sengputa et al. [77] per-\nforms best on SSP-3D, both of them perform poorly on\nthe other dataset. Despite not seeing either of the train-\ning datasets, BEDLAM-CLIFF ranks 2nd on SSP-3D and\nHBW. BEDLAM-CLIFF has the best rank averaged across\nthe datasets, showing its generalization ability.\nQualitative results on all these benchmarks are shown in\nFig. 7. Note that, although we do not assign gender labels to\nany of the training data, we find that, on test data, methods\ntrained on BEDLAM predict appropriately gendered body\nshapes. That is, they have automatically learned the associ-\nation between image features and gendered body shape.\n4.4. Ablation Studies\nTable 3 shows the effect of varying datasets, backbone\nweights and percentage of data; see Sup. Mat. for the full\ntable with results for HMR. We train with synthetic data\nonly and measure the performance on 3DPW. Note that the\nbackbones are pre-trained on image data, which is standard\npractice. Training them from scratch on BEDLAM gives\nworse results. It is sufficient to train using simple 2D task\nfor which there is plentiful data. Similar to [60], we find that\ntraining the backbone on a 2D pose estimation task (COCO)\nis important. We also vary the percentage of BEDLAM\ncrops used in training. Interestingly, we find that uniformly\nsampling just 5% of the crops from BEDLAM produces\nreasonable performance on 3DPW. Performance monoton-\nically improves as we add more training data. Note that\n5% of BEDLAM, i.e. 38K crops, produces better results\nthan 85K crops from AGORA, suggesting that BEDLAM\nis more diverse. Still, these synthetic datasets are comple-\nmentary, with our best results coming from a combination\nof the two. We also found that realistic clothing simulation\nleads to significantly better results than training with tex-\ntured bodies. This effect is more pronounced when using a\nbackbone pre-trained on ImageNet rather than COCO. See\nSup. Mat. for details.\nMethod\nModel\nSSP-3D\nHBW\nAverage\nPVE-T-SC\nRank\nP2P20k\nRank\nRank\nHMR [36]\nSMPL\n22.9\n8\n-\n-\n-\nSPIN [39]\nSMPL\n22.2\n7\n29\n4\n5.5\nSHAPY [19]\nSMPL-X\n19.2\n6\n21\n1\n3.5\nSTRAPS [76]\nSMPL\n15.9\n4\n47\n6\n5\nSengupta et al. [78]\nSMPL\n15.2\n3\n-\n-\n-\nSengupta et al. [77]\nSMPL\n13.6\n1\n32\n5\n3\nCLIFF\u2020\nSMPL\n18.4\n5\n27\n3\n4\nBEDLAM-CLIFF\nSMPL-X\n14.2\n2\n22\n2\n2\nTable 2. Per-vertex 3D body shape error on the SSP-3D and HBW\ntest set in T-pose (T). SC refers to scale correction.\nMethod\nDataset\nBackbone\nCrops %\nPA-MPJPE\nMPJPE\nPVE\nCLIFF\nB+A\nscratch\n100\n61.8\n97.8\n115.9\nCLIFF\nB+A\nImageNet\n100\n51.8\n82.1\n96.9\nCLIFF\nB+A\nCOCO\n100\n47.4\n73.0\n86.6\nCLIFF\nB\nCOCO\n5\n54.0\n80.8\n96.8\nCLIFF\nB\nCOCO\n10\n53.8\n79.9\n95.7\nCLIFF\nB\nCOCO\n25\n52.2\n77.7\n93.6\nCLIFF\nB\nCOCO\n50\n51.0\n76.3\n91.1\nCLIFF\nA\nCOCO\n100\n54.0\n88.0\n101.8\nCLIFF\nB\nCOCO\n100\n50.5\n76.1\n90.6\nTable 3. Ablation experiments on 3DPW. B denotes BEDLAM\nand A denotes AGORA. Crop %\u2019s only apply to BEDLAM.\n5. Limitations and Future Work\nOur work demonstrates that synthetic human data can\nstand in for real image data. By providing tools to enable re-\nsearchers to create their own data, we hope the community\nwill create new and better synthetic datasets. To support\nthat effort, below we provide a rather lengthy discussion of\nlimitations and steps for improvement; more in Sup. Mat.\nOpen source assets. There are many high-quality com-\nmercial assets that we did not use in this project because\ntheir licences restrict their use in neural network training.\nThis is a significant impediment to research progress. More\nopen-source assets are needed.\nMotion and scenes. The human motions we use are\nrandomly sampled from AMASS. In real life, clothing and\nmotions are correlated, as are scenes and motions. Addi-\ntionally, people interact with each other and with objects in\nthe world. Methods are needed to automatically synthesize\nsuch interactions realistically [99]. Also, the current dataset\nhas relatively few sitting, lying, and complex sports poses,\nwhich are problematic for cloth simulation.\nHair. BEDLAM lacks hair physics, long hairstyles, and\nhair color diversity. Our solution, based on hair cards, is\nnot fully realistic and suffers from artifacts under certain\nlighting conditions.\nA strand-based hair groom solution\nwould allow long flowing hair with hair-body interaction\nand proper rendering with diverse lighting.\nBody shape diversity. Our distribution of body shapes\nis not uniform (see Sup. Mat.). Future work should use a\nmore even distribution and add children and people with di-\nverse body types (scoliosis, amputees, etc.). Note that drap-\ning high-BMI models in clothing is challenging because the\nmesh self-intersects, causing failures of the cloth simula-\ntion. Retargeting AMASS motions to high-BMI subjects is\nalso problematic. We describe solutions in Sup. Mat.\nMore realistic body textures.\nOur skin textures are\ndiverse but lack details and realistic reflectance proper-\nties.\nFinding high-quality textures with appropriate li-\ncences, however, is difficult.\nShoes. BEDLAM bodies are barefoot. Adding basic\nshoes is fairly straightforward but the general problem is\nactually complex because shoes, such as high heels, change\nbody posture and gait. Dealing with high heels requires re-\ntargeting, inverse kinematics, or new motion capture.\nHands and Faces. There is very little mocap data with\nthe full body and hands and even less with hands interacting\nwith objects. Here we ignored facial motion; there are cur-\nrently no datasets that evaluate full body and facial motion.\n6. Discussion and Conclusions\nBased on our experiments we can now try to answer\nthe question \u201cIs synthetic data all you need?\u201d Our results\nsuggest that BEDLAM is sufficiently realistic that meth-\nods trained on it generalize to real scenes that vary signif-\nicantly (SSP-3D, HBW, 3DPW, and RICH). If BEDLAM\ndoes not well represent a particular real-image domain\n(e.g. surveillance-camera footage), then one can re-purpose\nthe data by changing camera views, imaging model, mo-\ntions, etc. Synthetic data will only get more realistic, clos-\ning the domain gap further. Then, does architecture mat-\nter? The fact that BEDLAM-HMR outperforms many re-\ncent, more sophisticated, methods argues that it may be less\nimportant than commonly thought.\nThere is one caveat to the above, however.\nWe find\nthat HPS accuracy depends on backbone pre-training. Pre-\ntraining the backbone for 2D pose estimation on COCO ex-\nposes it to all the variability of real images and seems to\nhelp it generalize. We expect that pre-training will eventu-\nally be unnecessary as synthetic data improves in realism.\nWe believe that there is much more research that BED-\nLAM can support. None of the methods tested here estimate\nhumans in world coordinates [82, 98]. The best methods\nalso do not exploit temporal information or action seman-\ntics. BEDLAM can support new methods that push these\ndirections. BEDLAM can also be used to model 3D cloth-\ning and learn 3D avatars using implicit shape methods.\nAcknowledgments. We thank STUDIO LUPAS GbR\nfor creating the 3D clothing, Meshcapade GmbH for\nthe skin textures, Lea M\u00a8uller for help removing self-\nintersections in high-BMI bodies and Timo Bolkart for\naligning SMPL-X to the CC template mesh. We thank T.\nAlexiadis, L. S\u00b4anchez, C. Mendoza, M. Ekinci and Y. Fin-\ncan for help with clothing texture generation.\nDisclosure:\nhttps://files.is.tue.mpg.de/\nblack/CoI_CVPR_2023.txt\nReferences\n[1] Character Creator (CC), Reallusion.\nhttps://www.\nreallusion.com/character-creator, 2022. 2,\n5\n[2] CLO. https://www.clo3d.com, 2022. 2, 5\n[3] Meshcapade GmbH, T\u00a8ubingen, Germany.\nhttps://\nmeshcapade.com, 2022. 2, 4\n[4] Poly Haven.\nhttps://polyhaven.com/hdris,\n2022. 5\n[5] Unreal Engine 5.\nhttps://www.unrealengine.\ncom, 2022. 2, 4, 5\n[6] WowPatterns. https://www.wowpatterns.com/,\n2022. 2, 5\n[7] Hiroyasu Akada, Jian Wang, Soshi Shimada, Masaki Taka-\nhashi, Christian Theobalt, and Vladislav Golyanik.\nUn-\nrealEgo: A new dataset for robust egocentric 3D human\nmotion capture. In European Conference on Computer Vi-\nsion (ECCV), 2022. 4\n[8] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2D human pose estimation: New benchmark\nand state of the art analysis. In Computer Vision and Pattern\nRecognition (CVPR), 2014. 3, 6\n[9] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller,\nSebastian Thrun, Jim Rodgers, and James Davis. SCAPE:\nShape completion and animation of people. Transactions\non Graphics (TOG), 24(3):408\u2013416, 2005. 3\n[10] Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir,\nWilliam T. Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu. HSPACE: Synthetic parametric humans animated\nin complex environments. arXiv, 2112.12867, 2021. 4, 16,\n18\n[11] Yizhak Ben-Shabat, Xin Yu, Fatemeh Saleh, Dylan Camp-\nbell, Cristian Rodriguez-Opazo, Hongdong Li, and Stephen\nGould.\nThe IKEA ASM dataset: Understanding people\nassembling furniture through actions, objects and pose.\nIn Winter Conference on Applications of Computer Vision\n(WACV), 2021. 3\n[12] Hugo Bertiche, Meysam Madadi, and Sergio Escalera.\nCLOTH3D: Clothed 3D humans.\nIn European Conf. on\nComputer Vision (ECCV), pages 344\u2013359. Springer Inter-\nnational Publishing, 2020. 4\n[13] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,\nand Gerard Pons-Moll. Multi-Garment Net: Learning to\ndress 3D people from images. In IEEE International Con-\nference on Computer Vision (ICCV). IEEE, oct 2019. 3\n[14] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian\nSminchisescu, Christian Theobalt, and Gerard Pons-Moll.\nBEHAVE: Dataset and method for tracking human object\ninteractions. In Computer Vision and Pattern Recognition\n(CVPR), 2022. 3\n[15] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khved-\nchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A.\nKalinin. Albumentations: Fast and flexible image augmen-\ntations. Information, 11(2), 2020. 17\n[16] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin,\nTao Yu, Wenjia Wang, Xiangyu Fan, Yangmin Gao, Yi-\nfan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang,\nChen Change Loy, Lei Yang, and Ziwei Liu. HuMMan:\nMulti-modal 4D human dataset for versatile sensing and\nmodeling. In European Conference on Computer Vision,\n2022. 3\n[17] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei,\nDaxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, and Zi-\nwei Liu. Playing for 3d human recovery. arXiv preprint\narXiv:2110.07588, 2021. 18\n[18] Wenzheng Chen, Huan Wang, Yangyan Li, Hao Su, Zhen-\nhua Wang, Changhe Tu, Dani Lischinski, Daniel Cohen-\nOr, and Baoquan Chen. Synthesizing training images for\nboosting human 3D pose estimation. In 2016 Fourth Inter-\nnational Conference on 3D Vision (3DV), pages 479\u2013488.\nIEEE, 2016. 3\n[19] Vasileios Choutas, Lea M\u00a8uller, Chun-Hao P. Huang, Siyu\nTang, Dimitrios Tzionas, and Michael J. Black. Accurate\n3D body shape regression using metric and semantic at-\ntributes. In IEEE/CVF Conf. on Computer Vision and Pat-\ntern Recognition (CVPR), pages 2718\u20132728, June 2022. 2,\n3, 6, 8\n[20] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-\nitrios Tzionas, and Michael J. Black. Monocular expres-\nsive body regression through body-driven attention. In Eu-\nropean Conference on Computer Vision (ECCV), volume\n12355, pages 20\u201340, 2020. 20\n[21] R. Dan\u02c7e\u02c7cek, E. Dibra, C. \u00a8Oztireli, R. Ziegler, and M. Gross.\nDeepGarment: 3D garment shape estimation from a single\nimage. Comput. Graph. Forum, 36(2):269\u2013280, may 2017.\n4, 18\n[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical im-\nage database. In Computer Vision and Pattern Recognition\n(CVPR), 2009. 6\n[23] Salehe Erfanian Ebadi, Saurav Dhakad, Sanjay Vish-\nwakarma,\nChunpu Wang,\nYou-Cyuan Jhang,\nMaciek\nChociej, Adam Crespi, Alex Thaman, and Sujoy Ganguly.\nPSP-HDRI+: A synthetic dataset generator for pre-training\nof human-centric computer vision models. In First Work-\nshop on Pre-training: Perspectives, Pitfalls, and Paths For-\nward at ICML 2022, 2022. 3\n[24] Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea\nPalazzi, Roberto Vezzani, and Rita Cucchiara.\nLearning\nto detect and track visible and occluded body joints in a vir-\ntual world. In European Conference on Computer Vision\n(ECCV), 2018. 3\n[25] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\nTzionas, and Michael J. Black. Collaborative regression of\nexpressive bodies using moderation. In International Con-\nference on 3D Vision (3DV), pages 792\u2013804, 2021. 1, 20\n[26] Valentin Gabeur, Jean-Sebastien Franco, Xavier Martin,\nCordelia Schmid, and Gregory Rogez. Moulding humans:\nNon-parametric 3D human shape estimation from single\nimages. In 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 2232\u20132241, 2019. 3, 18\n[27] Peng Guan, Loretta Reiss, David Hirshberg, Alex Weiss,\nand Michael J. Black.\nDRAPE: DRessing Any PErson.\nACM Trans. on Graphics (Proc. SIGGRAPH), 31(4):35:1\u2013\n35:10, July 2012. 4\n[28] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas,\nand Michael J. Black. Resolving 3D human pose ambigui-\nties with 3D scene constraints. In International Conference\non Computer Vision (ICCV), pages 2282\u20132292, Oct. 2019.\n3\n[29] David T. Hoffmann, Dimitrios Tzionas, Michael J. Black,\nand Siyu Tang. Learning to train with synthetic humans. In\nGerman Conference on Pattern Recognition (GCPR), pages\n609\u2013623, 2019. 3\n[30] Chun-Hao P. Huang, Hongwei Yi, Markus H\u00a8oschle, Matvey\nSafroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel\nScharstein, and Michael J. Black. Capturing and inferring\ndense full-body human-scene contact. In Computer Vision\nand Pattern Recognition (CVPR), 2022. 2, 3, 6\n[31] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and pre-\ndictive methods for 3D human sensing in natural environ-\nments. Transactions on Pattern Analysis and Machine In-\ntelligence (TPAMI), 36(7):1325\u20131339, 2013. 2, 3, 6, 18,\n19\n[32] Umar Iqbal, Anton Milan, and Juergen Gall. PoseTrack:\nJoint multi-person pose estimation and tracking. In Com-\nputer Vision and Pattern Recognition (CVPR), pages 4654\u2013\n4663, 2017. 3\n[33] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Lig-\nang Liu, and Hujun Bao. BCNet: Learning body and cloth\nshape from a single image. In Computer Vision - ECCV\n2020 - 16th European Conference, Glasgow, UK, August\n23-28, 2020, Proceedings, Part XX, pages 18\u201335, 2020. 4,\n18\n[34] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-\nemplar fine-tuning for 3D human pose fitting towards in-\nthe-wild 3D human pose estimation. In International Con-\nference on 3D Vision (3DV), pages 42\u201352, 2020. 3\n[35] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan,\nLin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic Studio: A massively multiview system for\nsocial interaction capture. Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI), 41(1):190\u2013204, 2019. 3\n[36] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In Computer Vision and Pattern Recognition (CVPR),\npages 7122\u20137131, 2018. 1, 2, 7, 8\n[37] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,\nand Michael J. Black. PARE: Part attention regressor for\n3D human body estimation. In International Conference\non Computer Vision (ICCV), pages 11127\u201311137, 2021. 1,\n7, 21\n[38] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch,\nLea M\u00a8uller, Otmar Hilliges, and Michael J. Black. SPEC:\nSeeing people in the wild with an estimated camera.\nIn\nProceedings International Conference on Computer Vision\n(ICCV), pages 11035\u201311045. IEEE, Oct. 2021. 3, 7\n[39] Nikos Kolotouros, Georgios Pavlakos, Michael J. Black,\nand Kostas Daniilidis.\nLearning to reconstruct 3D hu-\nman pose and shape via model-fitting in the loop. In In-\nternational Conference on Computer Vision (ICCV), pages\n2252\u20132261, 2019. 1, 3, 7, 8\n[40] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin\nYang, and Cewu Lu. HybrIK: A hybrid analytical-neural\ninverse kinematics solution for 3D human pose and shape\nestimation. In Computer Vision and Pattern Recognition\n(CVPR), pages 3383\u20133393, 2021. 7\n[41] Ruilong Li, Shan Yang, David A Ross, and Angjoo\nKanazawa. AI choreographer: Music conditioned 3D dance\ngeneration with AIST++. In International Conference on\nComputer Vision (ICCV), 2021. 3\n[42] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,\nand Youliang Yan. CLIFF: Carrying location information\nin full frames into human pose and shape estimation. In\nEuropean Conference on Computer Vision, 2022. 1, 2, 6, 7,\n18, 19, 20, 21\n[43] Junbang Liang and Ming C Lin. Shape-aware human pose\nand shape reconstruction using multi-view images. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 4352\u20134362, 2019. 4, 18\n[44] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-\nman pose and mesh reconstruction with transformers. In\nComputer Vision and Pattern Recognition (CVPR), pages\n1954\u20131963. Computer Vision Foundation / IEEE, 2021. 7\n[45] Kevin Lin, Lijuan Wang, and Zicheng Liu.\nMesh\ngraphormer.\nIn Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 12939\u2013\n12948, 2021. 1\n[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft COCO: common objects\nin context. In European Conference on Computer Vision\n(ECCV), volume 8693, pages 740\u2013755, 2014. 3, 19\n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC. Lawrence Zitnick. Microsoft COCO: Common objects\nin context. In European Conference on Computer Vision\n(ECCV), 2014. 6\n[48] Jian Liu, Naveed Akhtar, and Ajmal Mian. Temporally co-\nherent full 3D mesh human pose recovery from monocular\nvideo. arXiv preprint arXiv:1906.00161, 2019. 4, 18\n[49] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\nmulti-person linear model.\nTransactions on Graphics\n(TOG), 34(6):248:1\u2013248:16, 2015. 1, 3, 16\n[50] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-\nClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-\nLing Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh\nChang, Wei Hua, Manfred Georg, and Matthias Grund-\nmann. Mediapipe: A framework for building perception\npipelines. CoRR, abs/1906.08172, 2019. 6, 20\n[51] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje,\nGerard Pons-Moll, and Michael J. Black. AMASS: Archive\nof motion capture as surface shapes. In International Con-\nference on Computer Vision (ICCV), pages 5442\u20135451,\n2019. 2, 5, 14\n[52] Roberto Martin-Martin, Mihir Patel, Hamid Rezatofighi,\nAbhijeet Shenoi, JunYoung Gwak, Eric Frankel, Amir\nSadeghian, and Silvio Savarese.\nJRDB: A dataset and\nbenchmark of egocentric robot visual perception of humans\nin built environments. Transactions on Pattern Analysis and\nMachine Intelligence (TPAMI), 2021. Early access. 3\n[53] Dushyant Mehta,\nHelge Rhodin,\nDan Casas,\nPascal\nFua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt.\nMonocular 3D human pose estimation in the\nwild using improved CNN supervision. In 3D Vision (3DV),\n2017 Fifth International Conference on. IEEE, 2017. 3, 6,\n18\n[54] Dushyant\nMehta,\nOleksandr\nSotnychenko,\nFranziska\nMueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll,\nand Christian Theobalt. Single-shot multi-person 3D pose\nestimation from monocular RGB. In 3DV, 2018. 3, 18\n[55] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee.\nAccurate 3d hand pose estimation for whole-body 3d hu-\nman mesh estimation.\nIn Computer Vision and Pattern\nRecognition Workshop (CVPRW), 2022. 20\n[56] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee.\nNeuralannot: Neural annotator for 3d human mesh training\nsets. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 2299\u20132307,\n2022. 3\n[57] Lea M\u00a8uller, Ahmed A. A. Osman, Siyu Tang, Chun-Hao P.\nHuang, and Michael J. Black. On self-contact and human\npose. In Computer Vision and Pattern Recognition (CVPR),\npages 9990\u20139999, 2021. 5, 15, 19\n[58] Aiden Nibali, Joshua Millward, Zhen He, and Stuart Mor-\ngan. ASPset: An outdoor sports pose video dataset with\n3D keypoint annotations.\nImage and Vision Computing,\n111:104196, 2021. 3\n[59] Ahmed A. A. Osman, Timo Bolkart, Dimitrios Tzionas,\nand Michael J. Black. SUPR: A sparse unified part-based\nhuman representation. In European Conference on Com-\nputer Vision (ECCV). Springer International Publishing,\nOct. 2022. 16\n[60] Hui En Pang, Zhongang Cai, Lei Yang, Tianwei Zhang,\nand Ziwei Liu.\nBenchmarking and analyzing 3d human\npose and shape estimation beyond algorithms. In Thirty-\nsixth Conference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track, 2022. 7, 19\n[61] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-\nMoll.\nTailornet: Predicting clothing in 3d as a function\nof human pose, shape and garment style. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR).\nIEEE, jun 2020. 4\n[62] Priyanka Patel, Chun-Hao Paul Huang, Joachim Tesch,\nDavid Hoffmann, Shashank Tripathi, and Michael J. Black.\nAGORA: Avatars in geography optimized for regression\nanalysis.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 13468\u201313478, 2021. 3, 6, 7, 14, 18, 20\n[63] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3D hands, face,\nand body from a single image. In Computer Vision and Pat-\ntern Recognition (CVPR), pages 10975\u201310985, 2019. 2, 6,\n14, 20\n[64] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas\nDaniilidis. Learning to estimate 3d human pose and shape\nfrom a single color image. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n459\u2013468, 2018. 3\n[65] Albert Pumarola, Jordi Sanchez, Gary Choi, Alberto San-\nfeliu, and Francesc Moreno-Noguer. 3DPeople: Modeling\nthe Geometry of Dressed Humans. In International Con-\nference in Computer Vision (ICCV), 2019. 3, 18\n[66] Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos\nAthanasiou, Alejandra Quiros-Ramirez, and Michael J.\nBlack. BABEL: Bodies, action and behavior with english\nlabels. In Proceedings IEEE/CVF Conf. on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 722\u2013731, June\n2021. 5, 15\n[67] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, and Jitendra Malik.\nTracking people by\npredicting 3D appearance, location & pose. In Computer\nVision and Pattern Recognition (CVPR), 2022. 5\n[68] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and\nMichael J Black. Generating 3d faces using convolutional\nmesh autoencoders. In Proceedings of the European con-\nference on computer vision (ECCV), pages 704\u2013720, 2018.\n18\n[69] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\nimprovement. arXiv preprint arXiv:1804.02767, 2018. 21\n[70] Kathleen M. Robinette, Sherri Blackwell, Hein Daanen,\nMark Boehmer, Scott Fleming, Tina Brill, David Hoefer-\nlin, and Dennis Burnsides. Civilian American and Euro-\npean Surface Anthropometry Resource (CAESAR) final re-\nport. Technical Report AFRL-HE-WP-TR-2002-0169, US\nAir Force Research Laboratory, 2002. 4, 14\n[71] Gr\u00b4egory Rogez and Cordelia Schmid. MoCap-guided data\naugmentation for 3D pose estimation in the wild. In Pro-\nceedings of the 30th International Conference on Neural In-\nformation Processing Systems, NIPS\u201916, page 3116\u20133124,\nRed Hook, NY, USA, 2016. Curran Associates Inc. 3\n[72] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\nEmbodied hands: Modeling and capturing hands and bod-\nies together. ACM Transactions on Graphics, (Proc. SIG-\nGRAPH Asia), 2017. 17\n[73] Yu Rong,\nZiwei Liu,\nCheng Li,\nKaidi Cao,\nand\nChen Change Loy. Delving deep into hybrid annotations\nfor 3d human recovery in the wild. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 5340\u20135348, 2019. 3\n[74] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmo-\ncap: A monocular 3d whole-body pose estimation system\nvia regression and integration. In IEEE International Con-\nference on Computer Vision Workshops, 2021. 20\n[75] Igor Santesteban, Miguel A Otaduy, and Dan Casas.\nSNUG: Self-Supervised Neural Dynamic Garments.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 4\n[76] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla.\nSynthetic training for accurate 3D human pose and shape\nestimation in the wild. In British Machine Vision Confer-\nence (BMVC), 2020. 2, 3, 6, 8, 19\n[77] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Hi-\nerarchical kinematic probability distributions for 3D human\nshape and pose estimation from images in the wild. In In-\nternational Conference on Computer Vision (ICCV), pages\n11219\u201311229, 2021. 2, 7\n[78] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla.\nProbabilistic 3D human shape and pose estimation from\nmultiple unconstrained images in the wild. In Computer Vi-\nsion and Pattern Recognition (CVPR), pages 16094\u201316104,\n2021. 8\n[79] Leonid Sigal, Alexandru Balan, and Michael J Black. Hu-\nmanEva: Synchronized video and motion capture dataset\nand baseline algorithm for evaluation of articulated human\nmotion. International Journal of Computer Vision (IJCV),\n87(1):4\u201327, 2010. 3\n[80] Cristian Sminchisescu,\nAmit Kanaujia,\nand Dimitris\nMetaxas. Learning joint top-down and bottom-up processes\nfor 3D visual inference. In Proc. IEEE Conf. on Computer\nVision and Pattern Recognition (CVPR), volume 2, pages\n1743 \u2013 1752, 02 2006. 3\n[81] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose es-\ntimation.\nIn Computer Vision and Pattern Recognition\n(CVPR), 2019. 6\n[82] Yu Sun, Qian Bao, Wu Liu, Tao Mei, and Michael J. Black.\nTRACE: 5D temporal regression of avatars with dynamic\ncameras in 3D environments. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2023.\n5, 8\n[83] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J\nBlack. Putting people in their place: Monocular regression\nof 3D people in depth. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13243\u201313252, 2022. 1\n[84] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-\nitrios Tzionas.\nGRAB: A dataset of whole-body human\ngrasping of objects. In European Conference on Computer\nVision (ECCV), 2020. 2, 5\n[85] Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Ger-\nard Pons-Moll. SIZER: A dataset and model for parsing\n3D clothing and learning size sensitive 3D clothing. In Eu-\nropean Conference on Computer Vision (ECCV). Springer,\nAugust 2020. 4\n[86] Denis Tome, Patrick Peluse, Lourdes Agapito, and Hernan\nBadino. xR-EgoPose: Egocentric 3D human pose from an\nHMD camera. In Proceedings of the IEEE International\nConference on Computer Vision, pages 7728\u20137738, 2019.\n4\n[87] Matt Trumble, Andrew Gilbert, Charles Malleson, Adrian\nHilton, and John Collomosse. Total capture: 3D human\npose estimation fusing video and inertial sensors. In British\nMachine Vision Conference (BMVC), 2017. 3\n[88] G\u00a8ul Varol, Javier Romero, Xavier Martin, Naureen Mah-\nmood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.\nLearning from synthetic humans. In Computer Vision and\nPattern Recognition (CVPR), pages 4627\u20134635, 2017. 3,\n16, 18\n[89] Timo von Marcard, Roberto Henschel, Michael Black,\nBodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-\ncurate 3D human pose in the wild using IMUs and a mov-\ning camera. In European Conference on Computer Vision\n(ECCV), volume 11214, pages 614\u2013631, 2018. 2, 3, 6\n[90] Timo von Marcard, Roberto Henschel, Michael Black,\nBodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-\ncurate 3D human pose in the wild using IMUs and a mov-\ning camera. In European Conference on Computer Vision\n(ECCV), 2018. 19\n[91] Tuanfeng Y. Wang, Duygu Ceylan, Jovan Popovi\u00b4c, and\nNiloy J. Mitra.\nLearning a shared shape space for mul-\ntimodal garment design. ACM Trans. Graph., 37(6), dec\n2018. 4\n[92] Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, Matthew\nJohnson, Jingjing Shen, Nikola Milosavljevic, Daniel\nWilde, Stephan Garbin, Toby Sharp, Ivan Stojiljkovic, Tom\nCashman, and Julien Valentin. 3D face reconstruction with\ndense landmarks. In European Conf. on Computer Vision\n(ECCV), 2022. 3\n[93] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines\nfor human pose estimation and tracking. In European Con-\nference on Computer Vision (ECCV), 2018. 6\n[94] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,\nWilliam T Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu.\nGHUM & GHUML: Generative 3D human\nshape and articulated pose models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6184\u20136193, 2020. 4, 16\n[95] Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer,\nHelge Rhodin, Pascal Fua, Hans-Peter Seidel, and Christian\nTheobalt. Mo2Cap2 : Real-time mobile 3D motion capture\nwith a cap-mounted fisheye camera. IEEE Transactions on\nVisualization and Computer Graphics, 25(5):2093\u20132101,\n2019. 4\n[96] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. DenseRaC:\nJoint 3D pose and shape estimation by dense render-and-\ncompare. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 7760\u20137770, 2019.\n3\n[97] Haonan Yan, Jiaqi Chen, Xujie Zhang, Shengkai Zhang,\nNianhong Jiao, Xiaodan Liang, and Tianxiang Zheng. Ul-\ntrapose: Synthesizing dense pose with 1 billion points by\nhuman-body decoupling 3d model. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 10891\u201310900, 2021. 18\n[98] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo\nKanazawa.\nDecoupling human and camera motion from\nvideos in the wild. In Computer Vision and Pattern Recog-\nnition (CVPR), 2023. 5, 8\n[99] Hongwei Yi, Chun-Hao P. Huang, Shashank Tripathi, Lea\nHering, Justus Thies, and Michael J. Black.\nMIME:\nHuman-aware 3D scene generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2023. 8, 16\n[100] Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth\nVenkatesh, Jaesik Park, Jihun Yu, and Hyun Soo Park.\nHUMBI: A large multiview dataset of human body ex-\npressions.\nIn Computer Vision and Pattern Recognition\n(CVPR), 2020. 3\n[101] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and\nJan Kautz. Glamr: Global occlusion-aware human mesh\nrecovery with dynamic cameras.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 5\n[102] Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Andrei\nZanfir, and Cristian Sminchisescu. Human synthesis and\nscene compositing. Proceedings of the AAAI Conference\non Artificial Intelligence, 34(07):12749\u201312756, Apr. 2020.\n3\n[103] Mihai Zanfir, Andrei Zanfir, Eduard Gabriel Bazavan,\nWilliam T Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu. THUNDR: Transformer-based 3D human recon-\nstruction with markers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2021. 4\n[104] Chao Zhang, Sergi Pujades, Michael Black, and Gerard\nPons-Moll.\nDetailed, accurate, human shape estimation\nfrom clothed 3D scan sequences. In Computer Vision and\nPattern Recognition (CVPR), pages 5484\u20135493, 2017. 7\n[105] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng\nLi, Liang An, Zhenan Sun, and Yebin Liu.\nPymaf-\nx: Towards well-aligned full-body model regression from\nmonocular images.\narXiv preprint arXiv:2207.06400,\n2022. 20\n[106] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D hu-\nman pose and shape regression with pyramidal mesh align-\nment feedback loop. In International Conference on Com-\nputer Vision (ICCV), pages 11446\u201311456, 2021. 1\n[107] Tianshu Zhang, Buzhen Huang, and Yangang Wang.\nObject-occluded human shape and pose estimation from a\nsingle color image. In Computer Vision and Pattern Recog-\nnition (CVPR), 2020. 3\n[108] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and\nHao Li.\nOn the continuity of rotation representations in\nneural networks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n5745\u20135753, 2019. 18\nSupplementary Material\nThis document supplements the main text with (1) More\ndetails about the creation of the dataset. (2) More statistics\nabout the dataset\u2019s contents. (3) More example images from\nthe dataset. (4) Experimental results referred to in the main\ntext. (5) Visual presentation of the qualitative results.\nIn addition to this document, please see the Supple-\nmental Video, where the motions in the dataset are pre-\nsented. The video, data, and related materials can be found\nat https://bedlam.is.tue.mpg.de/\nBEDLAM: Definition\nnoun\nA scene of uproar and confusion: there was bed-\nlam in the courtroom.\nThe name of the dataset refers to the fact that the synthetic\nhumans in the dataset are animated independently of each\nother and the scene. The resulting motions have a chaotic\nfeel; please see the video for examples.\nA. Dataset creation\nFigure 8. Body diversity in BEDLAM. Top: BMI distribution of\nthe 271 different body shapes uses in BEDLAM. Bottom: BMI\ndistribution in all rendered videos; 55009 in total. Blue bars repre-\nsent bodies from the AGORA dataset, while orange bars represents\nhigh-BMI bodies from CAESAR dataset. BEDLAM uses both to\ncover a wide range of BMIs.\nBody shape diversity.\nThe AGORA [62] dataset has 111\nadult bodies in SMPL-X format [63]. These bodies mostly\ncorrespond to models with low BMI. Why do we use the\nbodies from AGORA? To create synthetic clothing we fo-\ncused on creating synthetic versions of the clothed scans in\nAGORA. That is, we create \u201cdigital twins\u201d of the AGORA\nscans. Our hope is that having 3D scans paired with sim-\nulated digital clothing will be useful for research on 3D\nclothing. Thus our 3D clothing is designed around AGORA\nbodies. Note that we do not make use of this property in\nBEDLAM but did this to enable future use cases. To in-\ncrease diversity beyond AGORA, we sample an additional\n80 male and 80 female bodies with BMI > 30 from the\nCAESAR dataset [70].\nNote that the AGORA and CAESAR bodies are repre-\nsented in gendered shape spaces using 10 shape compo-\nnents. When we render the images, we use these gendered\nbodies. For BEDLAM we use a gender-neutral shape space,\nenabling networks to automatically learn the appropriate\nbody shape within this space, effectively learning to recog-\nnize gender. To make the ground truth shapes for BEDLAM\nin this gender-neutral space, we fit the gender-neutral model\nwith 11 SMPL-X shape components to the gendered bodies.\nThis is trivial since the meshes are in full correspondence.\nWe use 11 shape components because, in the gender neutral\nspace, the first component roughly captures the differences\nbetween male and female body shapes. Thus, adding one\nextra component means that the SMPL-X ground truth (GT)\napproximates the original gendered body shapes. There is\nsome loss of fidelity but it is minimal; the V2V error be-\ntween the rendered bodies and the GT bodies in neutral pose\nis 2.4mm.\nIdeally, we want a diversity of body shapes, from slim\nto obese. Figure 8 shows the distribution of body BMIs\nin the training set. Specifically, we show the distribution\nof AGORA and CAESAR bodies, from which we sample.\nWe also show the final distribution of BMIs in the training\nimages.\nNotice that the AGORA bodies are almost all slim. We\nadd the CAESAR bodies to increase diversity and enable\nthe network to predict high-BMI shapes. There is a dip\nin the distribution between 25-30 BMI. This happens to be\nprecisely where the peak of the real population lies. De-\nspite this lack of average BMIs, BEDLAM does a good job\nof predicting body shape, suggesting that it has learned to\ngeneralize.\nNote that is it not clear what the right distribution for\ntraining is \u2013 one could mimic the distribution of a specific\npopulation or uniformly sample across BMIs. We plan to\nevaluate this and increase the diversity of the dataset; please\ncheck the project page for updates. Future work should also\nexpand the types of bodies used to include children and peo-\nple with diverse body types (athletes, little people, scolio-\nsis, amputees, etc.). Note that draping high-BMI models\nin clothing is challenging because the mesh self-intersects,\ncausing failures of the cloth simulation. Future work could\naddress this by automatically removing such intersections.\nAdditionally, there is little motion capture data of obese\npeople. So we need to retarget AMASS motions [51] to\nhigh-BMI subjects. But this is also problematic. Naive re-\ntargeting of motion from low-BMI bodies to high-BMI bod-\nies results in interpenetration.\nFigure 9. Clothing deformation is well modeled by physics-based\nsimulation.\nFigure 10. Examples of animation ground trajectories. Top-view\npelvis trajectories, color coded by subject. These trajectories are\nautomatically placed so that the bodies do not collide. Here, 15\nsample sequences are shown with varying numbers of subjects.\nHere we use a simple solution to this problem. Given a\nmotion sequence from AMASS, we first replace the original\nbody shape with a high-BMI body. Then, we optimize the\npose for each frame to minimize the body-body intersec-\ntion using the code provided by TUCH [57]. Although this\nresolves interpenetration between body parts, it can create\njittery motion sequences. As a remedy, we then smooth the\njittery motion with a Gaussian kernel. Although this sim-\nple solution does not guarantee a natural motion without\nbody-body interpenetration, it is sufficient to create a good\namount of valid motion sequences for larger bodies. Future\nwork should address the capture or retargeting of motion for\nhigh-BMI body shapes.\nSkin tone diversity.\nOur skin tones were provided by\nMeshcapade GmbH and are categorized into several ethnic\nbackgrounds, with skin-tone variety within each category.\nTo generate BEDLAM subjects, we sample uniformly from\nthe Meshcapade skins. This means the final renders are\nsampled with the following representations\n\u2022 African 20%,\n\u2022 Asian 24%,\n\u2022 Hispanic 6%,\n\u2022 Indian 20%,\n\u2022 Mideast 6%,\n\u2022 South East Asian 10%,\n\u2022 White 14%.\nThe same proportions hold in the training, validation and\ntest sets.\nMotion sampling.\nDue to the imbalanced distribution of\nmotions in AMASS, we use the motion labels from BA-\nBEL [66] to sample the motions for a wide and even cov-\nerage of the motion space. After visualizing the motions in\neach labelled category, we manually assign the number of\nmotions sampled from each category. Specifically, we sam-\nple 64 sequences for motions such as \u201cturn\u201d, \u201ccartwheel\u201d,\n\u201cbend\u201d, \u201csit \u201d, \u201ctouch ground\u201d, etc. We sample 4 sequences\nfrom motion labels containing less pose variation, such as\n\u201cdraw\u201d, \u201csmell\u201d, \u201click\u201d, \u201clisten \u201d, \u201clook\u201d, etc. We do not\nsample any sequences from labels indicating static poses,\nfor example, \u201cstand\u201d, \u201ca pose\u201d, and \u201ct pose\u201d. For the re-\nmaining motion labels, we sample 16 random sequences\nfrom each. Each sampled motion sequence lasts from 4 to\n8 seconds.\nClothing.\nOur outfits are designed to reflect real-world\nclothing complexity.\nWe have layered garments and de-\ntailed structures such as pleats and pockets. We also have\nopen jackets and many wide skirts, which usually have large\ndeformation under different body motion. These deforma-\ntions can only be well modeled with a physics-based simu-\nlation. See Fig. 9 for examples.\nPutting multiple people in the scene.\nFor each sequence\nwe randomly select between 1 and 10 subjects. For each\nsubject a random animation sequence is selected.\nThe\nshortest animation sequence determines the image sequence\nlength to ensure that there are no \u201cfrozen\u201d body poses. We\nthen pick a random sub-motion of the desired sequence\nlength from each body motion in the sequence. Next the\nbody motions are placed in a desired target area of the scene\nat a randomized position with a randomized camera yaw.\nTo avoid overlapping body motions and collisions with the\n3D environment, we use 2D binary ground plane occupancy\nmasks of the pelvis location for each randomly placed mo-\ntion. The order of motion placement is determined by the\nground plane pelvis coverage bounding box. This ensures\nthat walking motions, which are challenging to place in a\nlimited space, have the maximum free ground space avail-\nable before more constrained motions fill the remaining\nspace; cf. [10]. Generated root trajectories can be seen in\nFig. 10. This is a simple strategy (cf. [10]) and future work\nshould explore the generation or placement of motions that\nmake more sense together and with respect to the scene.\nOne direction would use MIME [99] to take human motions\nand produce 3D scenes that are consistent with them.\nAdditional limitations: Hair and shadows.\nDesigning\nhigh-quality hair assets requires experienced artists. Here\nwe used a commercial hair solution based on \u201chair cards\u201d;\nthese are simpler than strand-based methods. The downside\nis that they require the use of temporal accumulation buffers\nin the deferred rendering system. This can introduce ghost-\ning artefacts when rendering fast motions at low frame rates.\nWe also observed hair shader illumination issues under cer-\ntain conditions. When used with the new real-time global\nillumination system (Lumen) in Unreal Engine 5 (UE5),\nsome hairstyles exhibit a strong hue shift. Also, the num-\nber of hair colors that we have is limited. When used in\nthe HDRI environments, with ray traced HDRI shadows en-\nabled, most hairstyles turn black. For this reason we do not\nuse ray traced HDRI shadows in the HDRI environment ren-\nders, though the 3D scenes do have cast shadows. Adding\nground contact shadows to the HDRI scenes would require\nthe use of a separate ground shadow caster render pass to\ncomposite the shadow into the image. We have not pursued\nthis because we plan to upgrade the hair assets to remove\nthese issues for future releases of the dataset.\nOther body models.\nBEDLAM is designed around\nSMPL-X but many methods in the field use SMPL [49].\nIn particular, most, if not all, current methods that process\nvideo sequences are based on SMPL and not SMPL-X. We\nwill provide the ground truth in SMPL format as well for\nbackward compatibility. We also plan to support other body\nmodels like GHUM [94] or SUPR [59] in the future.\nAdditional ground truth data: Depth maps and seman-\ntic segmenation.\nSince BEDLAM is rendered with UE5,\nwe can render out more than RGB images. In particular, we\nrender depth maps and segmentation masks as illustrated\nin Fig. 11. The segmentation information includes seman-\ntic labels for hair, clothing and skin. With these additional\nforms of ground truth, BEDLAM can be used to train and\nevaluate methods that regress depth from images, fit bodies\nto RGB-D data, perform semantic segmentation, etc.\nAssets.\nWe will make available the rendered images and\nthe SMPL-X ground truth. We also release the 3D clothing\nand clothing textures as well as the skin textures. We also\nwill make available the process to create more data. All\nassets used are described in Table 4. The table provides\na \u201cshopping list\u201d to recreate BEDLAM. The only asset that\npresents a problem for recreating BEDLAM is the hair since\nnew licenses of the the hair assets prohibit training of neural\nnetworks (we acquired the data under an older license). This\nmotivates us to develop new hair assets with an unrestricted\nlicense. More information about how to create new data is\nprovided on the project website.\nB. Comparison to other datasets\nTable 5 compares synthetic datasets mentioned in the re-\nlated work section of the main paper. Here we only sur-\nvey methods that provide images with 3D ground truth; this\nexcludes datasets focused solely on 3D clothing modeling.\nSome of the listed datasets are not public but we include\nthem anyway and some information is not provided in the\npublications (\u201cunk.\u201d in the table).\nMethods vary in terms of the number of subjects, from a\nhandful of bodies to over 1000 in the case of Ultrapose. Ul-\ntrapose, however, is not guaranteed to have realistic bodies\nand the dataset is biased towards mostly thin Asian bodies.\nThe released dataset also has blurred faces. The number\nof frames also varies significantly among datasets. To get a\nsense of the diversity of images, one must multiply the num-\nber of frames by the average number of subjects per image\n(Sub/image).\nThe methods vary in how images are generated. The\nmajority composite a rendered 3D body onto an image\nbackground. This has limited realism. Human3.6M has\nmixed reality data in which simple graphics characters\nare inserted into real scenes using structure from motion.\nMixed/composite methods capture images of real people\nwith a green screen in a multi-camera setup. They can then\nget pseudo-ground tuth and composite the original images\non new backgrounds. In the table, \u201crendered\u201d means that\nthe synthetic body is rendered in a scene (HDRI panorama\nor 3D model) with reasonable lighting. These are the most\nrealistic methods.\nClothing in previous datasets takes several forms. The\nsimplest is a texture map on the SMPL body surface (like\nin SURREAL [88]). Some methods capture real clothing or\nuse scans of real clothing. Another class of methods uses\ncommercial \u201crigged\u201d models with rigged clothing.\nThis\ntype of clothing lacks the realism of physics simulation.\nMost methods that do physics simulation use a very limited\nnumber of garments (often as few as 2) due to the complex-\nity and cost.\nIt is hard to get good, comparable, data about motion di-\nversity in these datasets. Here we list numbers of motions\ngleaned from the papers but these are quite approximate.\nSome of the low numbers describe classes of motions that\nmay be repeated with some unknown number of variations.\nAt the same time, some of the larger numbers may lack di-\nvesity. With BEDLAM, we are careful to sample a diverse\nset of motions.\nFigure 11. Additional ground truth: Depth maps and semantic segmentation masks. The segmentation maps are color coded for each\nindividual and each material type (hair, clothing, skin).\nAsset Type\nName\nSource\nBody Texture\nVarious\nMeshcapade GmbH, https://meshcapade.com\nClothing Texture\nVarious\nWowPatterns, https://www.wowpatterns.com/\nHair\nPrime Hairstyles\nReallusion, https://www.reallusion.com/ContentStore/Character-Creator/Pack/Prime-hairstyles/\nHair\nTrendy Hairstyles for Men Vol. 1\nReallusion, https://www.reallusion.com/ContentStore/Pack/universal-hairstyles-vol-1\nHair\nTrendy Hairstyles for Men Vol. 2\nReallusion, https://www.reallusion.com/ContentStore/Pack/universal-hairstyles-vol-2\nEnvironment - HDRI\nVarious free HDRIs\nPoly Haven, CC0 1.0 Universal Public Domain Dedication, https://polyhaven.com/hdris\nEnvironment - 3D\nArchViz User Interface 3\nhttps://www.unrealengine.com/marketplace/en-US/product/archviz-user-interface-3\nEnvironment - 3D\nBig Office\nhttps://www.unrealengine.com/marketplace/en-US/product/big-office\nEnvironment - 3D\nHigh School Basketball Gym\nhttps://www.unrealengine.com/marketplace/en-US/product/high-school-basketball-gym-day-night-afternoon-midnight-lighting\nEnvironment - 3D\nSports Stadium\nhttps://www.unrealengine.com/marketplace/en-US/product/sports-stadium\nEnvironment - 3D\nSuburb Neighborhood House Pack\nhttps://www.unrealengine.com/marketplace/en-US/product/suburb-neighborhood-house-pack-modular\nTable 4. Third-party assets used for rendering BEDLAM. All 3D environments are from the Unreal Marketplace.\nFor comparison with real-image datasets, 3DPW con-\ntains 60 sequences captured with a moving camera, with\nroughly 51K frames, and 7 subjects in a total of 18 cloth-\ning styles. With roughly 2 subjects per frame, this gives\naround 100K unique bounding boxes. Human3.6M train-\ning data has 1,464,216 frames captured by 4 static cameras\nat 50 fps, which means there are 366K unique articulated\nposes. If one reduces the frame rate to 30 fps, that gives\nroughly 220K bounding boxes of 5 subjects performing 15\ndifferent types of motions. We observe that the total num-\nber of frames is less important than the diversity of those\nframes in terms of scene, body, pose, lighting, and clothing.\nC. Implementation Details\nBEDLAM-CLIFF-X.\nSince most HPS methods output\nSMPL bodies, we focus on that in the main paper and de-\nscribe the SMPL-X methods here.\nSpecifically, we use\nBEDLAM hand poses to train a full body network called\nBEDLAM-CLIFF-X. For this, we train a separate hand net-\nwork on hand crops from BEDLAM with an HMR architec-\nture but replace SMPL with the MANO hand [72], which is\ncompatible with SMPL-X. We merge the body pose out-\nput \u03b8b \u2208 R22\u00d73 from BEDLAM-CLIFF (see Sec. 4.1 of\nthe main paper) and hand pose output \u03b8h \u2208 R16\u00d73 from\nthe hand network to get the full body pose with articulated\nhands \u03b8fb \u2208 R55\u00d73. The face parameters, \u03b8jaw, \u03b8leye and\n\u03b8reye are kept as neutral. Since both BEDLAM-CLIFF and\nthe hand network output different wrist poses, we cannot\nmerge them directly. Hence, we train a small regressor Rfb\nto combine them.\nSpecifically, we define the body pose \u03b8b = {\u02c6\u03b8b, \u03b8elbow,\n\u03b8b\nwrist } and and hand pose \u03b8h = {\u03b8h\nwrist \u03b8fingers}, where\n\u02c6\u03b8b \u2208 R20\u00d73 represents the first 20 pose parameters of\nSMPL-X. Rfb takes global average pooled features as well\nas \u03b8b and \u03b8h from the BEDLAM-CLIFF and hand networks,\nand outputs \u03b8fb = {\u02c6\u03b8b, \u03b8elbow + \u2206elbow, \u03b8b\nwrist+\u2206wrist,\n\u03b8fingers }. Basically, Rfb learns an update of the elbow\nand wrist pose from the body network using information\nfrom both the body and hand network. Since we learn only\nan update on the wrist pose generated by the body network,\nthis prevents the unnatural bending of the wrists. Similar\nto BEDLAM-CLIFF, to train BEDLAM-CLIFF-X, we use\na combination of MSE loss on model parameters, projected\nkeypoints, 3D joints, and an L1 loss on 3D vertices. All\nother details can be found the code (see project page).\nData augmentation.\nA lot of data augmentation is in-\ncluded during training, including random crops, scale, dif-\nferent kinds of blur and image compression, brightness and\ncontrast modification, noise addition, gamma, hue and satu-\nration modification, conversion to grayscale, and downscal-\ning using [15].\nD. Supplemental experiments\nD.1. Ablation of training data and backbones\nTable 6 expands on Table 3 from the main paper, pro-\nviding the full set of dataset ablation experiments. The key\nDataset\n#Sub\n#Frames\nImage\nSubj/image\nClothing\nMotion\nGround truth\n3D HUMANS-Train [26]\n19\n50K\ncomposite\n1\ncaptured\n>15\nSMPL\nSURREAL [88]\n145\n\u22486.5M\ncomposite\n1\ntexture\n> 2000\nSMPL\nHuman3.6M [31]\nfew\n7.5K\nmixed reality\n1\nrigged\nunk.\n3D joints\nMPI-INF-3DHP-Train [53]\n8\n>1.3M\nmixed/composite\n1\nreal\n8+\n3D joints\nMuCo-3DHP [54]\n8\n\u2248400K\nmixed/composite\n1-4\nreal\n8\n3D joints\nDan\u02c7e\u02c7cek et al. [21]\n10\nunk.\nrendered (simple)\n1\nphysics\n20 min\nunk.\nLiang and Lin [43]\n100\n128K\ncomposite\n1\nphysics\n5 seqs\nSMPL\nBCNet (a) [33]\n285\n13K\ncomposite\n1\nrigged\nunk.\nSMPL\nBCNet (b) [33]\n3048\n17K\ncomposite\n1\nstatic physics\n55\nSMPL\nLiu et al. [48]\nunk.\n3M\ncomposite\n1\nphysics\n5k\nSMPL\nUltrapose [97]\n>1000\n\u2248500K\ncomposite\n1\nphysics\nn/a\ndense points\n3DPeople [65]\n80\n\u22482.5M\ncomposite\n1\nrigged\n70\n3D joints\nHSPACE [10]\n100\n1M\nrendered\n5 avg.\nrigged (100)\n100\nGHUM\nGTA-Human [17]\n>600\n\u2248 1.4M\ngame\n1\nrigged\n20K\nSMPL\nAGORA [62]\n>350\n\u224818K\nrendered\n5-15\nscans\nn/a\nSMPL-X, SMPL\nBEDLAM (ours)\n217\n380K\nrendered\n1-10\nphysics (110)\n2311\nSMPL-X\nTable 5. Comparison of synthetic human datasets that provide images with 3D human pose annotations. See text.\nMethod\nDataset\nBackbone\nCrops %\nPA-MPJPE\nMPJPE\nPVE\nHMR\nB+A\nscratch\n100\n67.9\n108.8\n129.0\nHMR\nB+A\nImageNet\n100\n57.3\n91.7\n108.8\nHMR\nB+A\nCOCO\n100\n47.6\n79.0\n93.1\nCLIFF\nB+A\nscratch\n100\n61.7\n96.5\n115.0\nCLIFF\nB+A\nImageNet\n100\n51.8\n82.1\n96.9\nCLIFF\nB+A\nCOCO\n100\n47.4\n73.0\n86.6\nHMR\nB\nCOCO\n5\n55.8\n86.9\n104.3\nHMR\nB\nCOCO\n10\n55.5\n85.7\n102.9\nHMR\nB\nCOCO\n25\n53.9\n83.9\n100.4\nHMR\nB\nCOCO\n50\n53.8\n81.1\n97.3\nHMR\nB+A\nCOCO\n100\n47.6\n79.0\n93.1\nCLIFF\nB\nCOCO\n5\n54.0\n80.8\n96.8\nCLIFF\nB\nCOCO\n10\n53.8\n79.9\n95.7\nCLIFF\nB\nCOCO\n25\n52.2\n77.7\n93.6\nCLIFF\nB\nCOCO\n50\n51.0\n76.3\n91.1\nCLIFF\nB+A\nCOCO\n100\n47.4\n73.0\n86.6\nHMR\nA\nCOCO\n100\n58.3\n94.9\n109.0\nHMR\nB\nCOCO\n100\n51.2\n80.6\n96.1\nHMR\nB+A\nCOCO\n100\n47.6\n79.0\n93.1\nCLIFF\nA\nCOCO\n100\n54.0\n88.0\n101.8\nCLIFF\nB\nCOCO\n100\n50.5\n76.1\n90.6\nCLIFF\nB+A\nCOCO\n100\n47.4\n73.0\n86.6\nTable 6. Ablation experiments on 3DPW. B denotes BEDLAM\nand A denotes AGORA. Crops % only applies to BEDLAM.\ntakeaways are: (1) training with a backbone pretrained on\nthe 2D pose-estimation task on COCO produces the best\nresults, (2) training from scratch on BEDLAM does not\nwork as well as either pre-training on ImageNet or COCO,\n(3) training only on BEDLAM is better than training only\non AGORA, (4) training on BEDLAM+AGORA is consis-\ntently better than using either alone (note that both are syn-\nthetic), (5) one can get by with using a fraction of BEDLAM\n(50% or even 25% gives good performance), but training\nerror continues to decrease up to 100%. All of this suggest\nthat there is still room for improvement in the synthetic data\nin terms of variety.\nD.2. Ablation on losses\nTo understand which loss terms are important, we per-\nform an ablation study on standard losses used in training\nHPS methods including LSMPL, Lj3d, Lj2d, Lv3d, Lv2d. In-\ndividual losses are described here and the ablation on them\nis reported in Table 7.\nLSMPL = \u2225\u02c6\u03b8 \u2212 \u03b8\u2225 + \u2225\u02c6\u03b2 \u2212 \u03b2\u2225\nLj3d = \u2225 \u02c6\nJ \u2212 J \u2225\nLj2d = \u2225\u02c6j \u2212 j\u2225\nLv3d = \u2225\u02c6V \u2212 V\u2225\nLv2d = \u2225\u02c6v \u2212 v\u2225\n\u02c6x denotes the ground truth for the corresponding variable\nx and \u2225\u00b7\u2225 is the type of loss that can be L1 or L2. For\nshape we always use L1 norm. J , V, \u03b2 and \u03b8 denote the 3D\njoints, 3D vertices, shape and pose parameters of SMPL-X\nmodel respectively. j and v denote the 2D joints and ver-\ntices projected into the full image using the predicted cam-\nera parameters similar to [42]. \u03b8 is predicted in a 6D rota-\ntion representation form [108] and converted to a 3D axis-\nangle representation when passed to SMPL-X model. Since\nwe set the hand poses to neutral in BEDLAM-CLIFF, we\nuse only the first 22 pose parameters in the training loss.\nWe use a subset of BEDLAM training data for this ablation\nstudy. Note that, to compute Lv2d we use a downsampled\nmesh with 437 vertices, computed using the downsampling\nmethod in [68]. We find this optimal for training speed\nand performance. Since the downsampling module samples\nmore vertices in regions with high curvature, it helps pre-\nserve the body shape and we can store the sampled vertices\ndirectly in memory without the need to load them during\nLoss type\nSSP-3D\nHBW\nPVE-T-SC\nHeight\nChest\nWaist\nHips\nP2P20k\nL1\n15.1\n51\n73\n97\n64\n22\nMSE\n14.2\n51\n69\n88\n62\n22\nTable 8. Losses. The use of L2 or L1 losses are explored for shape\nestimation accuracy using BEDLAM-CLIFF: error on HBW [57]\nand SSP-3D [76] in mm.\nDataset attribute\nBackbone\nPAMPJPE\nMPJPE\nMVE\nSimulation + Hair\nImageNet\n65.6\n101.8\n120.8\nSimulation\nImageNet\n66.3\n104.5\n124.5\nTexture\nImageNet\n72.2\n116.1\n136.7\nSimulation + Hair\nCOCO\n51.6\n77.8\n92.4\nSimulation\nCOCO\n51.6\n78.7\n93.0\nTexture\nCOCO\n54.3\n80.8\n96.0\nTable 9.\nAblation of different dataset attributes.\nError on\n3DPW in mm. See text.\nMethod\nH3.6M\n3DPW\nPA-MPJPE\nMPJPE\nPA-MPJPE\nMPJPE\nPVE\nCLIFF [42]\n32.7\n47.1\n-\n-\n-\nCLIFF\u2020*\n39.4\n62.9\n43.6\n68.8\n82.1\nCLIFF\u2020* w/o H3.6M\n56.1\n89.6\n44.4\n68.9\n82.3\nBEDLAM-HMR\n51.7\n81.6\n47.6\n79.0\n93.1\nBEDLAM-CLIFF\n50.9\n70.9\n46.6\n72.0\n85.0\nTable 10.\nImpact of training without Human3.6M on Hu-\nman3.6M and 3DPW. CLIFF\u2020* is the same model as Table 1 in\nmain paper.\nLosses\nType\nPAMPJPE\nMPJPE\nMVE\nLj3d\nMSE\n59.1\n86.1\n105.1\nLv3d\nMSE\n56.2\n83.4\n96.7\nLSMPL\nMSE\n51.3\n83.8\n96.7\nLSMPL + Lj3d\nMSE\n48.5\n76.0\n89.6\nLSMPL + Lv3d\nMSE\n48.2\n74.7\n87.9\nLSMPL + Lv3d + Lj3d\nMSE\n47.6\n74.2\n87.2\nLSMPL + Lv3d + Lj3d + Lv2d\nMSE\n48.7\n74.4\n87.6\nLj3d\nL1\n59.4\n85.7\n114.6\nLv3d\nL1\n72.5\n97.4\n111.6\nLSMPL\nL1\n50.6\n83.6\n96.0\nLSMPL + Lj3d\nL1\n46.9\n74.7\n87.6\nLSMPL + Lv3d\nL1\n48.8\n76.2\n88.8\nLSMPL + Lv3d + Lj3d\nL1\n46.9\n73.0\n86.0\nLSMPL + Lv3d + Lj3d + Lv2d\nL1\n47.4\n73.5\n86.8\nTable 7. Ablation of different losses. Error on 3DPW in mm.\ntraining. We include a 2D joints loss in all cases as it is\nnecessary to obtain proper alignment with the image.\nAs shown in Table 7, Lj3d or Lv3d alone do not pro-\nvide enough supervision for training. Similar to [60] we\nfind that LSMPL provides stronger supervision reducing the\nloss by a large margin when used in combination with Lv3d\nand Lj3d. Surprisingly, we find that including Lv2d makes\nthe performance slightly worse. A plausible reason for this\ncould be that using Lv2d provides high weight on aligning\nthe predicted body to the image but the mismatch between\nthe ground truth and estimated camera used for projection\nduring inference makes the 3D pose worse, thus resulting in\nhigher 3D error. We suspect that Lv2d could provide strong\nsupervision in the presence of a better camera estimation\nmodel; this is future work.\nWe also experiment with two different types of losses,\nL1 and MSE and find that L1 loss yields lower error on\nthe 3DPW dataset as shown in Table 7. However, Table 8\nshows that the model using L1 loss performs worse when\nestimating body shape on the SSP and HBW datasets com-\npared to the model using MSE loss. This discrepancy may\nbe attributed to the L1 loss treating extreme body shapes as\noutliers, thereby learning only average body shapes. Since\nthe 3DPW dataset does not have extreme body shapes, it\nbenefits from the L1 loss. Consequently, we opted to use\nthe MSE loss for our final model and all results reported in\nthe main paper. Note that Lj3d or Lv3d alone is worse with\nL1 loss compared to MSE loss.\nD.3. Ablation of dataset attributes\nWe also perform an ablation study by varying different\ndataset attributes. We generated 3 different sets of around\n180K images by varying the use of different assets. Keep-\ning the scenes and the motion sequences exactly the same,\nwe experiment by ablating hair and then further replacing\nthe cloth simulation with simple cloth textures. We use a\nbackbone pretrained with either COCO [46] or ImageNet\nand study the performance on 3DPW [90]. When using\nthe ImageNet backbone, we find that training with cloth-\ning simulation leads to better accuracy than training with\nclothing texture mapped onto the body. Adding hair gives\na modest improvement in MPJPE and MVE. Surprisingly,\nwith the COCO backbone, the difference in the training data\nmakes less difference. Still, clothing simulation is consis-\ntently better than just using clothing textures. It is likely\nthat the backbone pretrained on a 2D pose estimation task\nusing COCO is already robust to clothing and hair. As men-\ntioned above, however, our hair models are not ideal and not\nas diverse as we would like. Future work, should explore\nwhether more diverse and complex hair has an impact.\nD.4. Experiment on Human3.6M\nWe also evaluate our method on the Human3.6M dataset\n[31] by calculating MPJPE and PA-MPJPE on 17 joints ob-\ntained using the Human3.6M regressor on vertices. Previ-\nous methods have used Human3.6M training images when\nevaluating on the test set. Specifically, CLIFF [42] and our\nre-implementation, CLIFF\u2020*, both use Human3.6M data for\nMethod\nMVE\nMPJPE\nFB\nB\nF\nLH/RH\nFB\nB\nF\nLH/RH\nSMPLify-X [63]\n236.5\n187.0\n48.9\n48.3/51.4\n231.8\n182.1\n52.9\n46.5/49.6\nExPose [20]\n217.3\n151.5\n51.1\n74.9/71.3\n215.9\n150.4\n55.2\n72.5/68.8\nFrankmocap [74]\n168.3\n54.7/55.7\n165.2\n52.3/53.1\nPIXIE [25]\n191.8\n142.2\n50.2\n49.5/49.0\n189.3\n140.3\n54.5\n46.4/46.0\nBEDLAM-CLIFF-X\n131.0\n96.5\n25.8\n38.8/39.0\n129.6\n95.9\n27.8\n36.6/36.7\nHand4Whole+ [55]\n135.5\n90.2\n41.6\n46.3/48.1\n132.6\n87.1\n46.1\n44.3/46.2\nPyMAF+ [105]\n125.7\n84.0\n35.0\n44.6/45.6\n124.6\n83.2\n37.9\n42.5/43.7\nBEDLAM-CLIFF-X+\n103.8\n74.5\n23.1\n31.7/33.2\n102.9\n74.3\n24.7\n29.9/31.3\nTable 11. SMPL-X methods on the AGORA test set. + denotes methods include AGROA training set. FB is full-body, B is body only, F\nis face, and LH/RH are the left and right hands respectively.\nMethod\nNMVE\nNMJE\nMVE\nMPJPE\nFB\nB\nFB\nB\nFB\nB\nF\nLH/RH\nFB\nB\nF\nLH/RH\nPyMAF-X [105]\n172.1\n123.6\n167.2\n120.1\n161.8\n117.4\n50.3\n40.5/42.6\n157.2\n114.1\n51.6\n38.2/39.7\nHand4Whole [55]\n178.8\n119.1\n176.2\n117.6\n168.1\n112.0\n59.7\n52.8/55.8\n165.7\n110.5\n63.7\n50.0/52.0\nPIXIE [25]\n160.0\n107.2\n154.8\n103.5\n150.4\n100.8\n51.4\n47.2/50.2\n145.6\n97.3\n55.4\n43.6/46.0\nBEDLAM-CLIFF-X\n101.7\n65.6\n99.0\n64.7\n95.6\n61.7\n29.9\n35.7/36.2\n93.1\n60.8\n30.5\n33.2/33.3\nBEDLAM-CLIFF-X+\n93.4\n61.2\n92.5\n60.4\n87.8\n56.8\n27.3\n31.9/33.9\n87.0\n57.5\n28.0\n29.5/31.1\nTable 12. SMPL-X methods on the BEDLAM test set. Comparison of SOTA methods on the BEDLAM test set. + denotes methods\ninclude AGROA training set.\ntraining and, consequently get low errors on Human3.6M\ntest data. Note that our implementation does not get as low\nan error as reported in [42] despite the fact that we match\ntheir performance on 3DPW and RICH (see main paper).\nTo ensure a fair comparison and to measure the gener-\nalization of the methods, we trained a version of CLIFF\n(CLIFF\u2020* w/o H3.6M) using 3D datasets MPI-INF-3DHP,\n3DPW and 2D datasets COCO and MPII but excluding\nHuman3.6M, following the same settings as BEDLAM-\nCLIFF. The results in Tab. 10 demonstrate that BEDLAM-\nCLIFF outperforms CLIFF when Human3.6M is not in-\ncluded in training. This is another confirmation of the re-\nsults in the main paper showing that BEDLAM-CLIFF has\nbetter generalization ability than CLIFF. Without using Hu-\nman3.6M in training, BEDLAM-HMR is also better than\nCLIFF on Human3.6M.\nNote that this experiment illustrates how training on Hu-\nman3.6M is crucial to getting low errors on that dataset.\nThe training and test sets are similar (same backgrounds\nand similar conditions) meaning that methods trained on\nthe dataset can effectively over-fit to it. This can be seen\nby comparing CLIFF\u2020* with CLIFF\u2020* w/o H3.6M. Training\non Human3.6M significantly reduces error on Human3.6M\nwithout reducing error on 3DPW.\nD.5. SMPL-X experiments on the AGORA dataset\nAGORA is interesting because it is one of the few\ndatasets with SMPL-X ground truth. Table 11 evaluates\nmethods that estimate SMPL-X bodies on the AGORA\ndataset.\nThe results are taken from the AGORA leader-\nboard. BEDLAM-CLIFF-X does particularly well on the\nface and hands. Since the BEDLAM training set contains\nbody shapes sampled from AGORA, it gives BEDLAM-\nCLIFF-X an advantage over methods that are not fine-tuned\non the AGORA training set (bottom section of Tab. 11).\nConsequently, we also compare a version of BEDLAM-\nCLIFF-X that is trained only on the BEDLAM training set.\nThis still outperforms all the methods that were not trained\nusing AGORA (top section of Tab. 11). Please see Figure 13\nfor qualitative results.\nD.6. SMPL-X experiments on BEDLAM\nFor completeness,\nTab. 12 shows that BEDLAM-\nCLIFF-X outperforms recent SOTA methods that estimate\nSMPL-X on the BEDLAM test set. Not surpisingly, our\nmethod is more accurate by a large margin. Note, how-\never, that the prior methods are not trained on the BED-\nLAM training data. We follow a similar evaluation protocol\nas [62]. Since the hands are occluded in a large number\nof frames, we use MediaPipe [50] to detect the hands and\nevaluate hand accuracy only if they are visible. To detect in-\ndividuals within an image during evaluation, we use the de-\ntector that is included in the respective method\u2019s demo code.\nIn cases where the detector is not provided, we use [69],\nthe same detector use by BEDLAM-CLIFF-X. Please see\nFig. 13 for qualitative results.\nE. Qualitative Comparison\nFigure 12 provides a qualitative comparison between\nPARE [37], CLIFF [42] (includes 3DPW training) and\nBEDLAM-CLIFF (only synthetic data). We show results\non both RICH (left two) and 3DPW (right two). We render\npredicted bodies overlaid on the image and in a side view.\nIn the side view, the pelvis of the predicted body is aligned\n(translation only) with the ground truth body. Note that,\nwhen projected into the image, all methods look reasonable\nand relatively well aligned with the image features. The side\nview, however, reveals that BEDLAM-CLIFF (bottom row)\npredicts a better aligned body pose with the ground truth\nbody in 3D despite variation in the cameras, camera angle,\nand frame occlusion. Also, please notice that BEDLAM-\nCLIFF produces more natural leg poses in the case of occlu-\nsion compared to the other methods as shown in columns 1,\n3 and 4 of Fig. 12\nWe also provide qualitative results of BEDLAM-CLIFF-\nX on 3DPW and the RICH dataset in Fig. 14. In this case,\nwe also estimate the SMPL-X hand poses. All multi-person\nresults are generated by running the method on individual\ncrops found by a multi-person detector [69].\nFigure 12. Qualitative results on RICH (left two columns) and 3DPW (right two columns). RGB images (row 1), PARE front (row 2),\nPARE side (row 3), CLIFF front (row 4), CLIFF side (row 5), BEDLAM-CLIFF front (row 6), BEDLAM-CLIFF side (row 7). Ground\ntruth body is in blue and predicted body is in pink. The BEDLAM-CLIFF predicted 3D body is better aligned with ground truth in both\nfront and side views despite wide camera variation or frame occlusion.\nFigure 13. BEDLAM-CLIFF-X results on the AGORA-test (top 4 rows) and the BEDLAM-test images (bottom 2 rows).\nFigure 14. BEDLAM-CLIFF-X results on 3DPW-test (top 2 rows) and RICH-test (bottom 2 rows) images. Note the hand poses and that\nthe body shapes are appropriately gendered.\n"
  },
  {
    "title": "An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs",
    "link": "https://arxiv.org/pdf/2306.16601.pdf",
    "upvote": "4",
    "text": "AN EFFICIENT SPARSE INFERENCE SOFTWARE ACCELERATOR FOR\nTRANSFORMER-BASED LANGUAGE MODELS ON CPUS\nHaihao Shen 1 Hengyu Meng 1 Bo Dong 1 Zhe Wang 1 Ofir Zafrir 2 Yi Ding 1 Yu Luo 1 Hanwen Chang 1\nQun Gao 1 Ziheng Wang 3 Guy Boudoukh 2 Moshe Wasserblat 2\n1Intel\n2Intel Labs\n3Stanford University\nABSTRACT\nIn recent years, Transformer-based language models have become the standard approach for natural language\nprocessing tasks. However, stringent throughput and latency requirements in industrial applications are limiting\ntheir adoption. To mitigate the gap, model compression techniques such as structured pruning are being used to\nimprove inference efficiency. However, most existing neural network inference runtimes lack adequate support\nfor structured sparsity. In this paper, we propose an efficient sparse deep learning inference software stack for\nTransformer-based language models where the weights are pruned with constant block size. Our sparse software\naccelerator leverages Intel\u00ae Deep Learning Boost to maximize the performance of sparse matrix - dense matrix\nmultiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel outperforms the existing sparse\nlibraries (oneMKL, TVM, and LIBXSMM) by an order of magnitude on a wide range of GEMM shapes under 5\nrepresentative sparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up to 5x speedup\nover dense GEMM kernel of oneDNN, a well-optimized dense library widely used in industry. We apply our sparse\naccelerator on widely-used Transformer-based language models including Bert-Mini, DistilBERT, Bert-Base,\nand BERT-Large. Our sparse inference software shows up to 1.5x speedup over Neural Magic\u2019s Deepsparse\nunder same configurations on Xeon on Amazon Web Services under proxy production latency constraints. We\nalso compare our solution with two framework-based inference solutions, ONNX Runtime and PyTorch, and\ndemonstrate up to 37x speedup over ONNX Runtime and 345x over PyTorch on Xeon under the latency constraints.\nAll the source code is publicly available on Github 4.\n1\nINTRODUCTION\nLarge Transformer-based Language Models (LMs) are\nevolving rapidly from millions of parameters, e.g., BERT-\nLarge (Devlin et al., 2018), to billions of parameters, e.g.,\nTuring-Megatron (Smith et al., 2022), and GPT3 (Brown\net al., 2020). Transformer-based LMs are currently used for\nsolving almost all natural language processing (NLP) tasks,\nand those large models have demonstrated promising state-\nof-the-art (SoTA) accuracy on a wide range of NLP tasks.\nHowever, it\u2019s still quite challenging to deploy these mod-\nels in production due to the demands of large computation\nresources and strict latency constraints in real applications.\nTo enable the deployment of Transformer models at scale,\nmodel compression and optimization are usually required\nto make model inference more efficient. Pruning (Sanh\net al., 2020) and quantization (Zafrir et al., 2019) are two\nwell-known approaches that have been widely used to com-\npress Transformer models. There are two kinds of pruning\nmethods: unstructured pruning (Han et al., 2015) (Gordon\net al., 2020) (Wang, 2020) and structured pruning (Pool\net al., 2021), where unstructured pruning does not require\nany special sparsity pattern while structured one requires\napplying the same sparsity pattern in the weights. In gen-\neral, it is difficult to demonstrate the performance gains\nfor an unstructured sparse model without high sparsity ra-\ntio. Even for a structured sparse model, speedup might be\ndifficult without special hardware support (e.g., NVIDIA\nAmpere architecture and beyond). On the other hand, there\nare some recent works (Yao et al., 2022) (Kim et al., 2021)\nthat have demonstrated the performance of 8-bit integer\n(INT8) quantization as 8-bit fused multiply-add (FMA) or\nmatrix multiplication instruction has been supported in ma-\njority modern hardwares. However, there is a lack of work\nthat demonstrate the efficient inference on modern CPUs\ncombining quantization and pruning.\nIn this paper, we propose an efficient sparse deep learning\ninference software stack for Transformer-based LMs that\nsupports efficient structured sparsity with quantization. We\ndefine a structured sparsity pattern based on constant block\nsize 4x1. We generate the sparse model based on the sparsity\npattern and further quantize the sparse model to make the\ninference more efficient while maintaining the acceptable\narXiv:2306.16601v1  [cs.LG]  28 Jun 2023\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\naccuracy. To demonstrate the performance of a quantized\nsparse model, we implement SpMM kernel based on Intel\u00ae\nDeep Learning Boost, as well as a sparse attention kernel.\nWe measure the performance of SpMM kernel on a wide\nrange of typical shapes (totally 90) under 5 representative\nsparsity ratios (70%, 75%, 80%, 85%, 90%), and compare\nwith (structured/unstructured) sparse GEMM kernel on pop-\nular libraries (oneMKL, TVM, and LIBXSMM) and dense\nGEMM kernel on oneDNN. Our SpMM kernel outperforms\nthe existing sparse libraries on the performance by an order\nof magnitude. In particular, we compare structured SpMM\nwith same block size (namely same number of block ele-\nments) e.g., 4x1 and 2x2), it shows the speedup up to 17x\nover oneMKL and up to 41x over TVM on single thread,\nand up to 20x over oneMKL and up to 62x over TVM on\nfour threads. We also compare with dense GEMM kernel of\noneDNN and demonstrate up to 5x performance. Moreover,\nour SpMM kernel shows almost linear scaling from single\nthread to four threads among all the configurations of dif-\nferent shapes and sparsity ratios. We apply the sparse accel-\nerator on popular widely-used Transformer-based language\nmodels including Bert-Mini, DistilBERT, Bert-Base, and\nBERT-Large. We also compare the workload performance\nwith Neural Magic\u2019s Deepsparse (a leading sparsity-aware\ninference engine)5 and demonstrate up to 1.5x speedup on\nsame CPU instance on Xeon and up to 4.9x on different CPU\ninstances (Xeon for Sparse Accelerator vs. Eypc for Neural\nMagic) respectively on Amazon Web Services (AWS) under\nthe proxy production latency constraints. We also compare\nour solution with ONNX Runtime and PyTorch (framework-\nbased inference solution) and demonstrate the speedup up\nto 37x over ONNX Runtime and 345x over PyTorch on\nsame Xeon instance, and 72x over ONNX Runtime and\n309x over PyTorch on Xeon vs. Eypc instances under the\nlatency constraint. In summary, our main contributions are:\n\u2022 Define a structured sparsity pattern with block size 4x1\nand generate 9 sparse Transformer models on various\ndownstream tasks with 80% - 90% sparsity ratio and\nprove the accuracy within 1% loss from baseline.\n\u2022 Develop efficient SpMM and sparse attention tech-\nniques based on Intel\u00ae Deep Learning Boost for\nTransformer-based LMs.\n\u2022 Outperform existing libraries (oneMKL, TVM, and\nLIBXSMM) on SpMM kernel performance by an or-\nder of magnitude: up to 17x over oneMKL and 41x\nover TVM on single thread; up to 20x over oneMKL\nand 62x over TVM on multi-threads. Show up to 5x\nperformance over dense GEMM kernel of oneDNN.\n\u2022 Demonstrate good end-to-end speedup: up to 1.5x\nto 4.9x over Neural Magic from same to different in-\n5https://github.com/neuralmagic/deepsparse\nstances (Xeon for sparse accelerator vs. Eypc for Neu-\nral Magic); up to 37x - 72x over ONNX Runtime and\n309x - 345x over PyTorch from Xeon to Eypc instance\nunder the latency constraint.\n2\nRELATED WORK\n2.1\nModel Compression\nTransformer-based LMs have demonstrated SoTA accuracy\non a variety range of NLP tasks while the model size is\ngrowing rapidly. However, those models are hard to deploy\nfor production due to the limited computation resources and\nstrict latency constraints. There has been a growing interest\nin the compression of Transformer-based LMs to improve\nthe inference efficiency.\nPruning has been proven to be an effective way of reducing\nmodel size while maintaining the similar model quality (Le-\nCun et al., 1989) (Sanh et al., 2020) (Wang, 2021). Struc-\ntured pruning is gaining popularity to prune the weights\nwith a pre-defined sparsity pattern such as block-wise prun-\ning (Lagunas et al., 2021) and fine-grained 2:4 (Pool & Yu,\n2021) or N:M structured sparsity (Zhou et al., 2021). Re-\ncent works (Zafrir et al., 2021; Kurtic et al., 2022) proposed\npruning Transformer models at pre-training to create sparse\npre-trained LMs and fine-tuning on downstream tasks.\nQuantization is another widely-used model compression\ntechnique that can improve the inference latency (Jacob\net al., 2018)(Zafrir et al., 2019) (Bhandare et al., 2019).\nThere are two typical quantization approaches: post-training\nquantization (PTQ) and quantization-aware training (QAT),\nwhere PTQ requires an offline calibration process on repre-\nsentative samples to collect the tensor statistics and generate\nthe scale and zero point used for quantization, and QAT\nrequires an additional fine-tuning phase simulating the quan-\ntization inference during training.\nKnowledge distillation is a popular compression tech-\nnique (Hinton et al., 2015) (Sanh et al., 2019) (Tang et al.,\n2019). It has been used to produce a much smaller BERT\nmodel (Jiao et al., 2019) (Sun et al., 2020) while achieving\nhigh accuracy. Typically, distillation can be incorporated\ninto pruning and quantization as a combined orchestrated\nmodel compression technique (Zafrir et al., 2021) (Yao et al.,\n2022) which can produce a compressed model with the best\ntrade-off among model size, performance, and accuracy.\n2.2\nSparse/Dense GEMM Libraries\nThere are several existing sparse and dense GEMM libraries\nthat support CPUs and/or GPUs.\noneAPI Math Kernel Library (oneMKL for short)6 has sup-\n6https://github.com/oneapi-src/oneMKL\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nported dense and sparse GEMM for a while. In particular\nfor sparse GEMM, oneMKL supports multiple sparse ma-\ntrix representations such as COO, CSR, BSR. However,\nsparse GEMM in oneMKL only supports 32-bit floating-\npoint (FP32) data type and square block size such as 2x2.\nLIBXSMM7 is an open-source high performance library for\nsmall matrix multiplications. It supports both dense and\nunstructured sparse GEMM and demonstrates the impres-\nsive performance while it may require additional tuning to\nachieve the high performance. One of the constraints for\nLIBXSMM is that each dimension (M, K, N) of GEMM\nrequires 32 dividable, which is limiting the usage for smaller\nshapes e.g., N = 16,\nApache TVM (TVM for short)8 is a widely-used compiler\nstack for deep learning systems which is designed to close\nthe gap between productivity on deep learning frameworks\nand performance/efficiency on hardware backends. TVM\nsupports two structured sparsity patterns (4x1 and 16x1),\ntwo sparse matrix representations (CSR and BSR), and two\ndata types (FP32 and INT8).\noneAPI Deep Neural Network Library (oneDNN for short) 9\nprovides the mature support of dense GEMM kernels on\nCPU. It has been used in mainstream deep learning frame-\nworks such as TensorFlow and PyTorch.\ncuSparse10 is a sparse GEMM libary for CUDA, supporting\nunstructured and structured sparsity 2:4 recently introduced\nin NVidia Ampere architecture and above. hipSPARSE11 is\na sparse library created by AMD for their GPUs.\n2.3\nInference Solutions\nModern deep learning frameworks such as TensorFlow, Py-\nTorch, ONNX Runtime provide both training and inference\nto allow researchers and developers easily develop, build,\nand deploy AI models. In addition to these frameworks,\nOpenVINO12 is an open-source toolkit for optimizing and\ndeploying AI inference, taking a model trained on frame-\nworks as input and converting to its own intermediate repre-\nsentation (IR) as a pre-condition for deployment. However,\nthese solutions do not support sparse models.\nBesides the general frameworks or inference toolkits that\nsupport different hardware backends (e.g., CPU, GPU),\nthere are some specialized inference solutions for dedicated\nhardwares. Neural Magic is a close-source sparsity-aware\ninference engine on CPU. It supports sparse GEMM kernel\nfor both unstructured and structured sparsity and accelerates\n7https://github.com/libxsmm/libxsmm\n8https://github.com/apache/tvm\n9https://github.com/oneapi-src/oneDNN\n10https://docs.nvidia.com/cuda/cusparse/index.html\n11https://github.com/ROCmSoftwarePlatform/hipSPARSE\n12https://github.com/openvinotoolkit/openvino\nthe performance on both Xeon and Eypc. To the best of\nour knowledge, this work is most relevant to ours which\nfocuses on sparse model inference acceleration on CPUs.\nTensorRT13 is a popular inference engine delivering the lat-\nest performance on NVidia hardwares, FasterTransformer14\nis an accelerator for Transformer-based LMs by leveraging\nNVidia\u2019s 2:4 structured sparsity.\nDespite the popularity of GPUs, to the best of our knowl-\nedge, most industry inference is still done on CPUs, so the\nbenchmark target in this paper is CPUs.\n3\nSPARSE SOFTWARE ACCELERATOR\nIn this section, we present our sparse deep learning inference\nsoftware accelerator for Transformer-based LMs, including\nstructured sparsity pattern, sparse GEMM and Transformer\nattention kernels, and end to end optimizations.\n3.1\nSparsity Pattern\nChoosing the right sparsity pattern is critical to achieving\ninference speedups while maintaining accuracy. There are\ntwo main factors that we need to consider: 1) structured vs.\nunstructured 2) sparsity pattern if structured. Note that we\nconcentrate on weight sparsity only in this paper.\nOne of the main purpose in this work is to showcase efficient\nsparse inference on CPUs, and our target hardware is Intel\u00ae\nXeon\u00ae Scalable Processors due to the support of advanced\nvector neural network instructions (VNNI) that can be used\nto maximize the performance of structured sparse kernels.\nWe thus choose structured sparsity to make the best use of\nour target hardware.\nFigure 1. A sample sparse weight with structured sparsity pattern\n4x1, where sparsity dimension is on N with highlighted colors\nThe next challenge is choosing a structured sparsity pattern.\nN:M such as 2:4 is out of our considerations, as there is\nlack of instruction support on our target hardware. We\nalso exclude the option of a square block size (e.g., 2x2,\n4x4) which leads to non-contiguous memory accesses. In\nthis paper, we focus on 4x1 which offers the best trade-\noff among the time to train a sparse model, accuracy, and\nperformance after extensive experiments. Figure 1 shows a\nsample sparse weight with sparsity pattern 4x1.\n13https://developer.nvidia.com/tensorrt\n14https://github.com/NVIDIA/FasterTransformer\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nFigure 2. INT8 sparse GEMM kernel (sparse weight x dense input) implementation based on VNNI. Non-zero weight elements are\nbroadcast to form a VNNI-format block (4x16), and input elements are re-structured per the mask of sparse weight to form another\nVNNI-format block. These two VNNI blocks are fed into VNNI to complete the sparse GEMM.\n3.2\nSparse GEMM Kernel\nTo demonstrate the performance of our defined sparsity pat-\ntern, we implement the sparse GEMM kernel by leveraging\njust-in-time (JIT) compilation to generate the machine code\ndirectly based on GEMM shapes, which gives the flexibility\nof bypassing the compiler to perform the loop unfolding\nmore efficiently. Our JIT-based kernel implementation is es-\npecially useful for sparse GEMM kernels, since each kernel\nis specialized for a sparse weight tensor and sparse indices.\nWe can just load the needed sparse indices before the kernel\nexecution to save the instruction routing cost during the\ninference.\nGiven a sparse weight block NxK (sparsity pattern 4x1) and\na dense input block, we first broadcast the non-zero weight\nblock to form a VNNI-format block A. Based on the mask\nin the sparse weight, we re-structure the corresponding in-\nput as another VNNI-format block B on the fly based on\nAVX512 permutation and shuffling instructions. Then the\nkernel uses VNNI to produce the intermediate output given\nA and B, and add bias C as the final output. Algorithm 1\ndescribes the code snippet of INT8 sparse GEMM kernel\nwith default optimization configurations. VNNI instructions\nare designed to multiply 16 groups of 4 adjacent pairs of un-\nsigned 8-bit integers in one matrix with signed or unsigned\n8-bit integers in the other matrix, produce 16 groups of 4\nintermediate signed 16-bit results, add up these 4 results\nin the same group with 32-bit integer in destination matrix,\nand store the packed 32-bit results back in destination. This\nalso explains why we use the constant block size 4 as our\nstructured sparsity pattern, since 4 is the maximum tiling\nsize to fully utilize the computation throughput in case no\nimplicit register reusing, thereby improving the GEMM per-\nformance greatly. In particular, we apply tiling along N\ndimensions with n tile = 64 while corresponds to 4 times of\nVNNI width as the default configuration. Note that the tiling\nsize is tunable offline to achieve even better performance\ngiven a GEMM shape with sparsity ratio.\nAlgorithm 1 Code snippet of INT8 sparse GEMM kernel\n// M, N, K as three dimensions of GEMM\n// m block = 4, n block = 64, k block = 4\n// weight ptr: weight tensor; src ptr: input tensor\nfor m = 0; m < M; m+ = m block do\nfor n = 0; n < N; n+ = n block do\nfor k = 0; k <= K; k+ = k block do\nvbroadcastss( m32i(weight ptr))\nvbroadcastss( m32i(weight ptr))\nvbroadcastss( m32i(weight ptr))\nvbroadcastss( m32i(weight ptr))\nfor i = 0; i < 4; + + i do\nvmovdqu8( m128i, src ptr)\nvmovdqu8( m128i, src ptr)\nvbroadcasti32x4( m512i, m128i)\nvbroadcasti32x4( m512i, m128i)\nvpermt2d( m512i, m512i, m512i)\nvpshufb( m512i, m512i, m512i)\nend\nvpdpbusd( m512i, m512i, m512i)\nvpdpbusd( m512i, m512i, m512i)\nvpdpbusd( m512i, m512i, m512i)\nvpdpbusd( m512i, m512i, m512i)\n// downconvert and post-operator fusion\nend\nend\nend\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\n(a)\n(b)\n(c)\n(d)\nFigure 3. Sparse attention and sparse Linear fusions. (a) Sparse attention vs. dense attention. All the Linear operators are converted from\ndense to sparse. Scaled dot-product attention is dense as there is no computation operators with the fixed weights. (b), (c), and (d) shows\nthe sparse Linear fusion with the post-operators such as BiasAdd, Reshape, GeLU, Sum, Quantize etc. Specially, Quantize operator is\nintroduced by INT8 quantization.\nNote that for the weight with 4 non-dividable sparsity di-\nmension or 4 non-dividable non-zero blocks, the additional\npadding is needed to satisfy the accumulation dimensions\nfor VNNI. For simplicity, we omit the special handling of\npadding in the sparse GEMM kernel implementation.\nTo scale the sparse GEMM kernel from single thread to\nmulti-threads, we implement two-dimensions blocking strat-\negy along M and N dimensions. Blocking on M dimension\nallows different kernels to compute with the corresponding\nsliced weight. However, blocking on N dimension intro-\nduces some redundant inter-core communication cost. To\navoid such overhead, we re-layout K x N matrix into 3-\ndimensional matrix NUM BN x K x BN, where BN means\nblock size along N dimensions, NUM BN means number\nof micro blocks in N dimension. Then we apply the thread\nparallelism along the first dimension NUM BN to balance\nthe task for each thread.\n3.3\nSparse Transformer Attention\nTransformer attention is a common block for Transformer-\nbased LMs. With the sparse GEMM kernel, we can easily\nreplace dense Linear operator with sparse Linear operator\nas shown in Figure 3(a) where the diagram is a simplified\nversion of Transformer attention (Vaswani et al., 2017) for\nbetter illustration of sparse attention. A more complete\nTransformer attention actually consists of other operators\nsuch as BiasAdd, Reshape, GeLU, Sum etc. Unfortunately,\nthese operators usually lead to the overhead of memory\ntraffic and slow down the model performance.\nOperator fusion is useful in deep learning for reducing the re-\ndundant memory and kernel launch overhead. There are two\nwidely-used approaches: 1) computation graph-based oper-\nator fusion (Jia et al., 2019) and graph compiler-based solu-\ntion (Rotem et al., 2018). We apply the computation graph-\nbased operator fusion given two considerations: 1) graph-\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nbased fusion solution is more mature and well adopted in\nindustry; 2) operator fusion patterns are very common in\nTransformer attention and there is no need to complicate the\nfusion by introducing graph compiler-based solution. To\nsupport operator fusion, we then categorize three main kinds\nof operators to be fused with Linear operator: element-wise\noperator, binary operator, and shape manipulation operator.\nTypically, element-wise and binary operators can be fused\ninto previous operator by reusing the data stored in SIMD\nregisters to avoid memory movement between register and\nmemory. Shape manipulation operators (e.g., Reshape) only\nmodify the properties of a tensor without any computation\nlogic, and therefore can be easily fused with other operators.\nWe implement an algorithm to fuse post-operators with\nsparse Linear operator. For each computation operator (e.g.,\nLinear) in the computation graph, we take it as the starting\noperator and apply depth first search to identify the operators\nto be fused based on pre-defined categories. The identified\noperators are added into post-operator chain.\nMoreover, we develop a lookup-table (LUT) based approach\nto accelerate element-wise operators in low precision. Ba-\nsically, LUT is a typical key-value table. Taking unsigned\n8-bit integer as an example, the key range is from 0 - 255;\nthe value is also INT8 quantized from FP32 value through\npre-calculation for the post-operator chain offline. With the\nprepared LUT, we can directly get an output of the post-\noperator chain through a simple lookup operation given an\ninput, without actual calculation during model inference.\nThe idea of LUT can also be applied to an individual op-\nerator where there are intensive element-wise operations.\nAlgorithm 2 gives the details on LUT generation.\nAlgorithm 2 LUT generation\nInput: bit width bit width, op chain op chain\nOutput: LUT\nLUT = init(bit width)\nindex \u2190 min index(bit width)\nwhile index \u2264 max index(bit width) do\nfor op in op chain do\nx \u2190 op(x)\nend\nLUT(index) \u2190 x\nindex = get next index()\nend\nreturn LUT\n3.4\nEnd-to-end Sparse Inference Framework\nWe develop an end-to-end sparse deep learning inference\nframework to demonstrate the performance for Transformer-\nbased LMs. Our framework has the similar architecture as\nother inference solutions which consist of three levels of\noptimizations: operator, graph, and runtime.\nOperator optimization requires the optimal kernels for\nsparse and dense operators. We describe sparse GEMM\nkernels in Section 3.2. For the remaining dense operators\nsuch as BatchMatmul, LayerNorm, Softmax, we also de-\nvelop JIT-based kernels with tunable configurations (e.g.,\ntiling size for better register utilization). We enable the\ncache mechanism to allow the first-jitted kernel to be reused\nduring inference if the operator shape is unchanged.\nGraph optimization includes three stages:\n1) pre-\noptimization\n(e.g.,\nconstant\nfolding,\ncommon\nsub-\nexpression elimination), 2) quantization (e.g., 16-bit or\n8-bit), and 3) back-end optimization.\nWe focus more\non quantization and back-end optimization, since most of\nTransformer-based LMs are well-designed and there is few\nopportunities in pre-optimization stage. On low precision\noptimization, we leverage Intel\u00ae Neural Compressor15 to\ngenerate INT8 models by using built-in accuracy-aware tun-\ning capability. Back-end optimizations have been mostly\ndescribed in Section 3.3.\nRuntime optimization requires an efficient memory allocator\nand thread scheduler. The default memory allocator usually\ncreates a new buffer each time when receiving a memory\nallocation request, and therefore the data is less likely to be\nreused. To reduce such memory allocation overhead, we\ndevelop a custom memory allocator to maximize the buffer\nreuse and make the data more cache friendly. To further\nshrink the memory usage, we implement weight sharing that\nallows a single copy of weight to be shared across multiple\ninstances running in parallel during inference. Moreover,\nwe create a thread management to schedule the thread us-\nage more effectively to support inter- and intra-instance\nexecution.\n4\nEXPERIMENTAL SETUP\nWe describe the experimental setup including hardware set-\ntings, sparse models and hyper-parameters, and kernel-level\nand model-level benchmark configurations.\n4.1\nHardware Settings\nWe select two popular x86 CPU instances (24 cores)\non AWS: c6i.12xlarge for Intel\u00ae Xeon\u00ae Ice Lake and\nc6a.12xlarge for AMD\u00ae Eypc for performance measure-\nment, since these two CPU types are well validated in pop-\nular GEMM libraries and deep learning inference solution.\nTurbo is enabled by default for both instances on AWS.\n15https://github.com/intel/neural-compressor\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nTable 1. Sparse models, sparsity ratio, approach, and accuracy (Acc). Delta is the difference between sparse and dense accuracy. Typically,\ndelta >= -1% is required, and higher is better. DistilBERT (Squad v1.1) shows better delta due to distillation used in pre-trained stage\nModel\nDataset\nSparsity Ratio\nApproach\nAcc (Dense)\nAcc (Sparse)\nAcc (Delta)\nBERT-Mini\nSquad v1.1\n80%\nDense + fine-tuned\n76.87%\n76.27%\n-0.78%\nBERT-Mini\nMRPC\n90%\nDense + fine-tuned\n87.52%\n87.21%\n-0.36%\nBERT-Mini\nSST-2\n90%\nDense + fine-tuned\n87.61%\n86.92%\n-0.79%\nDistilBert\nSquad v1.1\n80%\nSparse + pre-trained\n85.8%\n86.8%\n1.17%\nDistilBert\nMRPC\n90%\nDense + fine-tuned\n88.85%\n88.65%\n-0.23%\nBERT-Base\nSquad v1.1\n80%\nSparse + pre-trained\n88.59%\n88.67%\n0.09%\nBERT-Base\nSquad v1.1\n85%\nSparse + pre-trained\n88.59%\n88.03%\n-0.63%\nBERT-Base\nMRPC\n80%\nSparse + pre-trained\n90.5%\n90.43%\n-0.08%\nBERT-Base\nMRPC\n85%\nSparse + pre-trained\n90.5%\n89.63%\n-0.96%\n4.2\nSparse Models\nWe use two training approaches to generate the sparse mod-\nels: 1) initialize the weight from a dense model and prune\nthe model during fine-tuning for a downstream task (dense\n+ fine-tuned as short), and 2) initialize the weight from a\npre-trained sparse model, lock the sparsity pattern, and fine-\ntune the model for a downstream task (sparse + pre-trained\nas short). Table 2 shows the hyper-parameters for Distil-\nBERT (the others in Appendix A). All the sparse models use\n4x1 sparsity pattern, which demonstrates that this sparsity\npattern allows us to achieve high sparsity ratio while main-\ntaining less than 1% accuracy loss for our tested models.\nTable 2. Hyper-parameters for sparse DistilBERT\nHyper-parameter\nDistilBERT (Squad 80%)\nLearning rate\n1.8e-4\nBatch Size\n12\nWeight decay\n0.01\nEpochs\n8\nLearning rate decay\nLinear\nWarmup ratio\n0.05\nSequence length\n384\n\u03bbMLM\n0\n\u03bbkd\n1\nTemperature\n2\n4.3\nBenchmark Configurations\nWe benchmark our framework against commonly used so-\nlutions shown in Table 3. We show both kernel-level and\nend-to-end performance results.\nOn sparse kernel benchmark, we use single thread and four\nthreads to measure the performance on a set of GEMM\nshapes (totally 90) that are widely used in typical Trans-\nformer models. For oneMKL and TVM, we refer to the\ndocument and sample codes from the sparse libraries to\ndevelop our benchmark code.\nTable 3. Software version used for kernel or model benchmark\nSoftware\nVersion\nType\noneMKL\n2022.1.0\nKernel\nLIBXSMM\n1.17\nKernel\nTVM\n0.9.0\nKernel\nNeural Magic (Deep Sparse)\n1.1.0\nModel\nONNX Runtime\n1.11.1\nModel\nPyTorch\n1.11\nModel\nOn model benchmark, the goal is to achieve the maximum\nthroughput under proxy latency constraint for production\nper each model. You can see from Table 4 that the proxy\nlatency constraint per model almost aligns with the number\nof encoder layers and the weight shape. Note that the mea-\nsured best latency may exceed the latency constraint under\ncertain configurations, so we show the throughput based on\nthe best latency using all the possible cores.\nTable 4. Model name, proxy latency constraint for production,\nnumber of encoder layers, and weight shape\nModel\nLatency\nEncoder Layers\nWeight\nShape\nBERT-Mini\n1 ms\n4\n256x256\n256x1024\n1024x256\nDistilBERT\n10 ms\n6\n768x768\n768x3072\n3072x768\nBERT-Base\n20 ms\n12\n768x768\n768x3072\n3072x768\nBERT-Large\n50 ms\n24\n1024x1024\n1024x4096\n4096x1024\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n0.001\n0.01\n0.1\n1\n10\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n0.001\n0.01\n0.1\n1\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\nOurs/STRU/4x1(INT8)\noneMKL/STRU/2x2(FP32)\nTVM/STRU/4x1(FP32)\nTVM/STRU/4x1(INT8)\nN=32\nMxK\nMxK\nMxK\nMxK\nMxK\nThreads=1\nTime (ms)\nThreads=4\nTime (ms)\nSparsity 70%\nSparsity 75%\nSparsity 80%\nSparsity 85%\nSparsity 90%\n(a)\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n2\n5\n0.01\n2\n5\n0.1\n2\n5\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n2\n5\n0.01\n2\n5\n0.1\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\n256x256\n256x1024\n1024x256\n768x768\n1024x1024\n768x3072\n3072x768\n1024x4096\n4096x1024\nOurs/STRU(INT8)\noneDNN/Dense(INT8)\nN=32\nMxK\nMxK\nMxK\nMxK\nMxK\nThreads=1\nTime (ms)\nThreads=4\nTime (ms)\nSparsity 70%\nSparsity 75%\nSparsity 80%\nSparsity 85%\nSparsity 90%\n(b)\nFigure 4. Sparse GEMM kernel performance (N = 32). STRU and UNSTRU is structured and unstructured sparsity. (a) Comparing\nsparse GEMM on same block size across libraries, sparsity patterns (4x1, 2x2), sparsity ratios (70% - 90%), data types, and threads; (b)\nComparing sparse with dense GEMM. X-axis is GEMM shape MxK, and Y-axis is log10 based execution time on single or four threads\n5\nRESULTS\n5.1\nSparse Kernel Performance\nWe measure the sparse GEMM kernel performance on Xeon\nbased on benchmark configurations described in Section 4.3.\nTable 5 shows the results among the sparse libraries (ours,\noneMKL, TVM) where N is 32 as an example. Our sparse\nGEMM kernel outperforms the other sparse libraries in all\nthe benchmark configurations. There are three main results\nbased on the sample GEMM shapes:\n\u2022 Ours vs. other sparse libraries (Structured sparsity with\nsame pattern or same number of block elements e.g.,\n2x2): demonstrate 10x - 12x for geomean performance\nand 16x for maximum over oneMKL (Structured 2x2);\n14x - 19x for geomean and 22x - 64x for maximum\nover TVM\n\u2022 Our sparse GEMM kernel vs. dense GEMM kernel (of\noneDNN) shows 1.4x to 3.6x for geomean performance\nand 1.6x to 4.4x for minimum performance on single\nthread, and the similar performance on four threads\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nTable 5. Geomean and maximum (Max) performance for our sparse GEMM kernels\nSparse Library\nSparsity Pattern\nSparsity Ratio\nData Type\nThread 1\nGeomean / Max\nThread 4\nGeomean / Max\nOurs vs. oneMKL\nSTRU 4x1 vs. STRU 2x2\n70% - 90%\nINT8 vs. FP32\n12.7x / 16.4x\n10.9x / 16.2x\nOurs vs. TVM\nSTRU 4x1 vs. STRU 4x1\n70% - 90%\nINT8 vs. FP32\n19.2x / 65.6x\n14.1x / 22.6x\nOurs vs. TVM\nSTRU 4x1 vs. STRU 4x1\n70% - 90%\nINT8 vs. INT8\n16.5x / 31.5x\n18.7x / 47.0x\nOurs vs. oneDNN\nSTRU 4x1 vs. Dense\n70% for Ours\nINT8 vs. INT8\n1.4x / 1.6x\n1.3x / 1.5x\nOurs vs. oneDNN\nSTRU 4x1 vs. Dense\n75% for Ours\nINT8 vs. INT8\n1.6x / 1.8x\n1.5x / 1.8x\nOurs vs. oneDNN\nSTRU 4x1 vs. Dense\n80% for Ours\nINT8 vs. INT8\n1.9x / 2.2x\n1.8x / 2.2x\nOurs vs. oneDNN\nSTRU 4x1 vs. Dense\n85% for Ours\nINT8 vs. INT8\n2.5x / 2.9x\n2.3x / 2.9x\nOurs vs. oneDNN\nSTRU 4x1 vs. Dense\n90% for Ours\nINT8 vs. INT8\n3.6x / 4.4x\n3.1x / 4.1x\nTable 6. Geomean and maximum (Max) performance on sparse BERT-Mini (90% sparsity ratio), DistilBERT (80%), BERT-Base (80%),\nand BERT-Large (80%) on all the sequence lengths (16 - 384). ONNX RT is short for ONNX Runtime\nInference Solution\nCPU vs. CPU\nBERT-Mini 90%\nGeomean / Max\nDistilBERT 80%\nGeomean / Max\nBERT-Base 80%\nGeomean / Max\nBERT-Large 80%\nGeomean / Max\nOurs vs. Neural Magic\nXeon vs. Xeon\n-\n1.4x/1.5x\n1.3x/1.5x\n1.4x/1.8x\nXeon vs. Eypc\n-\n3.7x/5.0x\n3.2x/4.0x\n3.9x/7.3x\nOurs vs. ONNX RT\nXeon vs. Xeon\n16.8x/37.7x\n6.1x/10.7x\n7.3x/11.3x\n6.5x/10.0x\nXeon vs. Eypc\n76.3x/345.9x\n12.9x/17.9x\n14.9x/21.0x\n13.7x/20.4x\nOurs vs. PyTorch\nXeon vs. Xeon\n32.5x/72.7x\n16.9x/24.3x\n10.8x/13.4x\n8.1x/10.7x\nXeon vs. Eypc\n73.5x/309.1x\n22.5x/36.8x\n21.0x/29.1x\n18.6x/29.0x\n\u2022 Our sparse library shows the performance close to lin-\near linear 90%+ scaling from single to four threads\nDue to the space limitation, a more comprehensive perfor-\nmance comparison is described in Appendix B considering\nsparse (structured vs. unstructured) and dense, different\nsparse block sizes (4x1, 2x2, 4x4, 16x1), and 90 GEMM\nshapes (N from 16 to 384).\n5.2\nSparse Model Performance\nWe describe how to generate a FP32 sparse model in Sec-\ntion 4. To demonstrate our sparse GEMM kernel, we need to\ngenerate the INT8 sparse model. We leverage Intel\u00ae Neural\nCompressor, which is an open-source model compression\ntool offering accuracy-aware tuning capability, and produce\nthe INT8 sparse model that can meet the accuracy criteria\n(relative loss less than 1%). The sparse models generated\nfor our sparse accelerator are also used for ONNX Runtime\nand PyTorch. Note that how to quantize these sparse models\nis out of scope in this paper, but the quantization recipes\nand instructions will be published on Github along with the\nother source code.\nNeural Magic has its own sparse model zoo16 which pro-\nvides the quantized model using unstructured or structured\npruning on typical neural networks, including structured\nsparse model with block pattern 4x1 for DistilBERT (spar-\nsity ratio 80%) and BERT-Base (sparsity ratio 80%), and\nunstructured sparse model for BERT-Large (sparsity ratio\n80% and 90%), which are used for our benchmark. Note\nthat for BERT-Mini, we skip it in the performance compari-\nson as there is no published model in the sparse model zoo;\nfor BERT-Large, we generate a structured sparse model with\npattern 4x1 for our sparse accelerator based on the same\nsparsity ratio of their unstructured one.\nTo the best of our knowledge, our sparse accelerator is\nthe first one to demonstrate the performance on typical\nTransformer-based models across various downstream tasks.\nThe results are presented in Table 5. Our solution outper-\nforms Neural Magic by 1.3x - 1.4x (geomean) and 1.5x -\n1.8x (maximum), ONNX Runtime by 6x - 16x (geomean)\nand 10x - 37x (maximum), and PyTorch by 8x - 32x (ge-\nomean) and 10x - 72x (maximum) on same Xeon instance\nacross different models. Figure 5 shows the performance of\neach sparse model per difference sequence length. More in-\nterestingly, we also report the performance on Eypc instance\nwhich is also being used for inference.\n16https://sparsezoo.neuralmagic.com/\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\n16\n32\n48\n64\n80\n96\n112\n128\n2\n5\n1000\n2\n5\n10k\n2\n5\n16\n32\n48\n64\n80\n96\n112\n128\n5\n100\n2\n5\n1000\n2\n5\n10k\n16\n32\n48\n64\n80\n96\n112\n128\n5\n100\n2\n5\n1000\n2\n5\n16\n32\n48\n64\n80\n96\n112\n128\n384\n5\n10\n2\n5\n100\n2\n5\n1000\n2\nOurs/Xeon\nNeural Magic/Xeon\nNeural Magic/Epyc\nONNX Runtime/Xeon\nONNX Runtime/Epyc\nPyTorch/Xeon\nPyTorch/Epyc\nSparsity=80%\nSeq Length\nSeq Length\nThroughput (samples/sec)\nThroughput (samples/sec)\nBERT Mini\nDistilBERT\nBERT Base\nBERT Large\nFigure 5. Sparse model performance (maximum throughput under latency constraint (in Table 4)\n5.3\nDiscussions\nWe present the performance of sparse GEMM kernel and\nmodel performance in Section 5.1 and 5.2 and provide addi-\ntional observations as below.\nOn sparse GEMM libraries, oneMKL provides the best OOB\nexperience to run the sparse kernel performance without ad-\nditional tuning; TVM might be sub-optimal for AVX512\nand therefore it shows the overall worse performance than\nthe other two libraries; LIBXSMM provides sample code\nfor sparse kernel benchmark while it does not support struc-\ntured sparse GEMM kernels. On dense GEMM library,\noneDNN shows decent performance on a wide range of\nGEMM shapes and good scaling from single to four threads.\nOn end-to end sparse inference, we also measure the maxi-\nmum throughput without latency constraints and minimal\nlatency besides the default proxy production measurement.\nSimilar to maximum throughput under proxy latency con-\nstraint, our sparse accelerator outperforms the other infer-\nence solutions both in maximum throughput without latency\nconstraint and minimal latency in nearly all the configu-\nrations (shown in Figure 8 in Appendix). In addition, to\nunderstand the performance on sparse model more thor-\noughly, we generate the sparse models based on different\nsparsity ratio from 70% to 90% with 5% stride. Figure 7 in\nAppendix shows almost linear scaling in performance on\nDistilBERT, BERT-Base, and BERT-Large among all the\nsequence lengths, but some negative scaling on BERT-Mini\ndue to unbalanced thread/task scheduling on some corner\nGEMM shapes.\n6\nSUMMARY AND FUTURE WORK\nIn this paper, we developed an end-to-end solution for\nTransformer-based LMs inference with structured sparsity\nand quantization. Our SpMM kernel outperforms the exist-\ning sparse libraries (oneMKL, TVM, and LIBXSMM) by\nan order of magnitude on a wide range of shapes under rep-\nresentative sparsity ratios (70%, 75%, 80%, 85%, 90%). We\ndemonstrate large speedups on typical Transformer-based\nmodels (Bert-Mini, DistilBERT, Bert-Base, and BERT-\nLarge) on CPUs: up to 1.5x and 4.9x over Neural Magic\non same Xeon instance and different instances (Xeon vs.\nEypc), up to 37x - 72x over ONNX Runtime and 309x -\n345x over PyTorch from same to different CPU instance.\nAs future work, we plan to extend our software support\nto other CPU architectures (e.g., ARM) and contribute our\nopen-source solution to the Transformer ecosystem. More-\nover, we plan to extend the benchmark to provide Trans-\nformer users with the deployment choices for production\nwith respect to the performance per dollar on cloud.\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nREFERENCES\nBhandare, A., Sripathi, V., Karkada, D., Menon, V., Choi, S.,\nDatta, K., and Saletore, V. Efficient 8-bit quantization of\ntransformer neural machine language translation model.\narXiv preprint arXiv:1906.00532, 2019.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. ArXiv, abs/2005.14165,\n2020.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nGordon, M. A., Duh, K., and Andrews, N. Compressing\nbert: Studying the effects of weight pruning on transfer\nlearning. arXiv preprint arXiv:2002.08307, 2020.\nHan, S., Pool, J., Tran, J., and Dally, W. Learning both\nweights and connections for efficient neural network.\nAdvances in neural information processing systems, 28,\n2015.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the\nknowledge in a neural network. ArXiv, abs/1503.02531,\n2015.\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,\nA., Adam, H., and Kalenichenko, D.\nQuantization\nand training of neural networks for efficient integer-\narithmetic-only inference. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 2704\u20132713, 2018.\nJia, Z., Thomas, J., Warszawski, T., Gao, M., Zaharia, M.,\nand Aiken, A. Optimizing dnn computation with relaxed\ngraph substitutions. Proceedings of Machine Learning\nand Systems, 1:27\u201339, 2019.\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li,\nL., Wang, F., and Liu, Q.\nTinybert: Distilling bert\nfor natural language understanding.\narXiv preprint\narXiv:1909.10351, 2019.\nKim, S., Gholami, A., Yao, Z., Mahoney, M. W., and\nKeutzer, K. I-bert: Integer-only bert quantization. In\nInternational conference on machine learning, pp. 5506\u2013\n5518. PMLR, 2021.\nKurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M.,\nFineran, B., Goin, M., and Alistarh, D. The optimal bert\nsurgeon: Scalable and accurate second-order pruning for\nlarge language models. arXiv preprint arXiv:2203.07259,\n2022.\nLagunas, F., Charlaix, E., Sanh, V., and Rush, A. M.\nBlock pruning for faster transformers. arXiv preprint\narXiv:2109.04838, 2021.\nLeCun, Y., Denker, J., and Solla, S. Optimal brain damage.\nAdvances in neural information processing systems, 2,\n1989.\nPool, J. and Yu, C. Channel permutations for n: m sparsity.\nAdvances in Neural Information Processing Systems, 34:\n13316\u201313327, 2021.\nPool, J., Sawarkar, A., and Rodge, J. Accelerating inference\nwith sparsity using the nvidia ampere architecture and\nnvidia tensorrt. NVIDIA Developer Blog, 2021.\nRotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng,\nS., Dzhabarov, R., Gibson, N., Hegeman, J., Lele, M.,\nLevenstein, R., et al.\nGlow: Graph lowering com-\npiler techniques for neural networks.\narXiv preprint\narXiv:1805.00907, 2018.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert,\na distilled version of bert: smaller, faster, cheaper and\nlighter. arXiv preprint arXiv:1910.01108, 2019.\nSanh, V., Wolf, T., and Rush, A. Movement pruning: Adap-\ntive sparsity by fine-tuning. Advances in Neural Informa-\ntion Processing Systems, 33:20378\u201320389, 2020.\nSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhan-\ndari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G.,\nKorthikanti, V., et al. Using deepspeed and megatron to\ntrain megatron-turing nlg 530b, a large-scale generative\nlanguage model. arXiv preprint arXiv:2201.11990, 2022.\nSun, Z., Yu, H., Song, X., Liu, R., Yang, Y., and Zhou, D.\nMobilebert: a compact task-agnostic bert for resource-\nlimited devices. arXiv preprint arXiv:2004.02984, 2020.\nTang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin,\nJ. Distilling task-specific knowledge from bert into sim-\nple neural networks. arXiv preprint arXiv:1903.12136,\n2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWang, Z.\nSparsert: Accelerating unstructured sparsity\non gpus for deep learning inference.\narXiv preprint\narXiv:2008.11849, 2020.\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nWang, Z. Sparsednn: Fast sparse deep learning inference\non cpus. arXiv preprint arXiv:2101.07948, 2021.\nYao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and\nHe, Y. Zeroquant: Efficient and affordable post-training\nquantization for large-scale transformers. arXiv preprint\narXiv:2206.01861, 2022.\nZafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M.\nQ8bert: Quantized 8bit bert. In 2019 Fifth Workshop\non Energy Efficient Machine Learning and Cognitive\nComputing-NeurIPS Edition (EMC2-NIPS), pp. 36\u201339.\nIEEE, 2019.\nZafrir, O., Larey, A., Boudoukh, G., Shen, H., and\nWasserblat, M. Prune once for all: Sparse pre-trained lan-\nguage models. arXiv preprint arXiv:2111.05754, 2021.\nZhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K.,\nSun, W., and Li, H. Learning n: m fine-grained struc-\ntured sparse neural networks from scratch. arXiv preprint\narXiv:2102.04010, 2021.\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nA\nHYPER-PARAMETERS FOR MORE SPARSE MODEL\nTable 7. Hyper-parameters for sparse BERT-Mini\nHyper-parameter\nBERT-Mini\n(Squad 80%)\nBERT-Mini\n(MRPC 90%)\nBERT-Mini\n(SST-2 90%)\nLearning rate\n4.5e-4\n1e-3\n5e-5\nBatch Size\n16\n16\n16\nWeight decay\n1e-7\n1e-3\n5e-5\nEpochs\n10\n15\n15\nLearning rate decay\nLinear\nConstant\nConstant\nWarmup ratio\n0.018\n0\n0\nSequence length\n384\n128\n128\n\u03bbkd\n4.5\n2.0\n2.0\nTemperature\n2\n2\n2\nTable 8. Hyper-parameters for sparse BERT-Base\nHyper-parameter\nBERT-Base\nSquad 80%)\nBERT-Base\n(Squad 85%)\nBERT-Base\n(MRPC 80%)\nBERT-Base\n(MRPC 85%)\nLearning rate\n1e-4\n1.5e-4\n1e-4\n1.5e-4\nBatch Size\n12\n12\n32\n32\nWeight decay\n0\n0\n0\n0\nEpochs\n8\n8\n3\n3\nLearning rate decay\nLinear\nLinear\nLinear\nLinear\nWarmup ratio\n0\n0\n0\n0\nSequence length\n384\n384\n128\n128\n\u03bbkd\n1.0\n1.0\n1.0\n1.0\nTemperature\n2\n2\n2\n2\nB\nSPARSE GEMM KERNEL PERFORMANCE\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\noneMKL    \nTVM\nOurs\noneMKL    \nTVM\nOurs\n1\n4\n0.01\n0.1\n1\n10\n100\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\noneMKL    \nTVM\nOurs\n1\n4\n0.01\n0.1\n1\n10\n100\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\noneMKL    \nTVM\nOurs\n1\n4\n0.01\n0.1\n1\n10\n100\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\noneMKL    \nTVM\nOurs\n1\n4\n0.01\n0.1\n1\n10\n100\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\nTVM\noneMKL    \nOurs\nLIBXSMM\nTVM\noneMKL    \nOurs\nLIBXSMM\n1\n4\nTVM\noneMKL    \nOurs\nTVM\noneMKL    \nOurs\n1\n4\n0.001\n0.01\n0.1\n1\n10\noneMKL    \nTVM\nOurs\nLIBXSMM\noneMKL    \nTVM\nOurs\nLIBXSMM\n1\n4\nTVM\noneMKL    \nOurs\nLIBXSMM\nTVM\noneMKL    \nOurs\nLIBXSMM\n1\n4\nTVM\noneMKL    \nOurs\nLIBXSMM\nTVM\noneMKL    \nOurs\nLIBXSMM\n1\n4\nTVM\noneMKL    \nOurs\nLIBXSMM\nTVM\noneMKL    \nOurs\nLIBXSMM\n1\n4\nSparsity 70%\nSparsity 75%\nSparsity 80%\nSparsity 85%\nSparsity 90%\nTime (ms)\nTime (ms)\nTime (ms)\nTime (ms)\nTime (ms)\nN=16\n(STRU,UNSTRU(grey))\nN=32\n(STRU,UNSTRU(grey))\nN=64\n(STRU,UNSTRU(grey))\nN=128\n(STRU,UNSTRU(grey))\nN=384\n(STRU,UNSTRU(grey))\nFigure 6. Sparse GEMM kernel performance: a high-level overview. Besides structured sparse GEMM kernel with same block size, we\nalso compare: 1) structured SpMM with different or more aggressive block size: our kernel still shows competitive performance up to 12x\nover oneMKL (4x4) and 36x over TVM (16x1); and 2) unstructured SpMM: our kernel also shows up to 16x over oneMKL and 31x over\nLIBXSMM\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\nTable 9. Sparse GEMM kernel performance of all 90 shapes across libraries, sparsity patterns, data types, and threads\nLibrary\nSparsity\nData type\nThread 1\nGeomean / Maximum\nThread 4\nGeomean / Maximum\nOurs vs. oneMKL\nStru 4x1 vs. Unstru\nINT8 vs. FP32\n8.8x / 20.4x\n8.1x / 22.4x\nOurs vs. oneMKL\nStru 4x1 vs. Stru 2x2\nINT8 vs. FP32\n11.2x / 17.5x\n10.4x / 20.3x\nOurs vs. oneMKL\nStru 4x1 vs. Stru 4x4\nINT8 vs. FP32\n8.4x / 12.1x\n8.1x / 12.9x\nOurs vs. TVM\nStru 4x1 vs. Stru 4x1\nINT8 vs. FP32\n19.2x / 75.6x\n15.5x / 41.7x\nOurs vs. TVM\nStru 4x1 vs. Stru 16x1\nINT8 vs. FP32\n13.0x / 104.8x\n11.6x / 71.5x\nOurs vs. TVM\nStru 4x1 vs. Stru 4x1\nINT8 vs. INT8\n19.2x / 41.4x\n20.9x / 62.8x\nOurs vs. TVM\nStru 4x1 vs. Stru 16x1\nINT8 vs. INT8\n7.6x / 25.3x\n8.2x / 36.6x\nOurs vs. TVM\nStru 4x1 vs. Unstru\nINT8 vs. FP32\n7.7x / 25.3x\n8.4x / 31.1x\nOurs vs. oneDNN\nStru 4x1 vs. Dense\nINT8 vs. INT8\n2.1x / 4.8x\n2.0x / 5.1x\nC\nWORKLOAD PERFORMANCE\n70%\n75%\n80%\n85%\n90%\n10k\n100k\n70%\n75%\n80%\n85%\n90%\n1000\n10k\n70%\n75%\n80%\n85%\n90%\n1000\n10k\n70%\n75%\n80%\n85%\n90%\n100\n1000\nSeq=16\nSeq=32\nSeq=48\nSeq=64\nSeq=80\nSeq=96\nSeq=112\nSeq=128\nSparsity\nSparsity\nThroughput (samples/sec)\nThroughput (samples/sec)\nBERT Mini\nDistilBERT\nBERT Base\nBERT Large\nFigure 7. Sparse model performance scaling by sparsity ratio\nAn Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs\n16\n32\n48\n64\n80\n96\n112\n128\n1000\n10k\n100k\n16\n32\n48\n64\n80\n96\n112\n128\n100\n1000\n10k\n16\n32\n48\n64\n80\n96\n112\n128\n100\n1000\n10k\n16\n32\n48\n64\n80\n96\n112\n128\n384\n10\n100\n1000\nOurs/Xeon\nNeural Magic/Xeon\nNeural Magic/Epyc\nONNX Runtime/Xeon\nONNX Runtime/Epyc\nPyTorch/Xeon\nPyTorch/Epyc\nSparsity=80%\nSeq Length\nSeq Length\nThroughput (samples/sec)\nThroughput (samples/sec)\nBERT Mini\nDistilBERT\nBERT Base\nBERT Large\n(a)\n16\n32\n48\n64\n80\n96\n112\n128\n1\n5\n10\n16\n32\n48\n64\n80\n96\n112\n128\n1\n5\n10\n16\n32\n48\n64\n80\n96\n112\n128\n1\n5\n10\n16\n32\n48\n64\n80\n96\n112\n128\n384\n5\n10\n2\n5\n100\n2\nOurs/Xeon\nNeural Magic/Xeon\nNeural Magic/Epyc\nONNX Runtime/Xeon\nONNX Runtime/Epyc\nPyTorch/Xeon\nPyTorch/Epyc\nSparsity=80%\nSeq Length\nSeq Length\nMin Latency (ms)\nMin Latency (ms)\nBERT Mini\nDistilBERT\nBERT Base\nBERT Large\n(b)\nFigure 8. Sparse model performance on maximum throughput without latency constraint (a) and minimal latency (b). X-axis is sequence\nlength, and Y-axis is throughput with unit samples/sec (higher is better) in (a) and minimal latency with unit ms (lower is better) in (b),\nboth of which are log based number\n"
  },
  {
    "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
    "link": "https://arxiv.org/pdf/2306.16564.pdf",
    "upvote": "3",
    "text": "Automatic Calibration and Error Correction for\nGenerative Large Language Models via Pareto\nOptimal Self-Supervision\nTheodore Zhao\nMicrosoft\nMu Wei\nMicrosoft\nJ. Samuel Preston\nMicrosoft\nHoifung Poon\nMicrosoft\nAbstract\nGenerative Large language models (LLMs) have demonstrated remarkable capa-\nbilities for a wide range of applications, but reducing ungrounded or erroneous\nresponses remains a major growth area. Unlike task-specific models, there lack an\neffective method to calibrate the confidence level of LLM responses to indicate\npotential errors and facilitate human-in-the-loop verification. An important source\nof calibration stems from expert-stipulated programmatic supervision, which is\noften available at low cost but has its own limitations such as noise and coverage.\nIn this paper, we introduce a Pareto optimal self-supervision framework that can\nleverage available programmatic supervision to systematically calibrate LLM re-\nsponses by producing a risk score for every LLM response, without any additional\nmanual efforts. This is accomplished by learning a harmonizer model to align\nwith LLM output as well as other weak supervision sources. The model assigns\nhigher risk scores to more uncertain LLM responses and facilitate error correction.\nExperiments on standard relation extraction and classification tasks in biomedical\nand general domains demonstrate that the proposed risk score is highly correlated\nwith the actual LLM error rate. By using a dynamic prompting strategy based\non the risk score, we observed significant accuracy improvement for off-the-shelf\nLLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision\nmodel and GPT-4 results past SOTA supervised results on challenging evaluation\ndatasets.\n1\nIntroduction\nGenerative Large language models (LLMs) have evolved to be impressively powerful in recent\ndevelopment [41], with Generative Pretrained Transformer (GPT) models becoming increasingly\neffective in their emerging abilities. The evolution from GPT-3 [3] to GPT-4 [22], as well as the\nemergence of other LLMs such as PaLM [4] and LLaMA [30], showed a significant leap in terms of\nnatural language understanding and problem-solving abilities. The generative natural of the models\nmakes them widely adapted to numerous application fields. However, as shown in [13], the problem\nof hallucination or erroneous response is still a major challenge when LLMs are applied to fields with\nhigh standard for accuracy and reliability, such as biomedical and healthcare domains.\nUnfortunately, there lack systematic tools to efficiently identify hallucination, or estimate the con-\nfidence level of the output. As the outputs are free text, the intrinsic confidence score from the\ngenerative LLMs is often unavailable, or not well calibrated with respect to the desired target, espe-\ncially after applying reinforcement learning with human feedback (RLHF) [23] according to [22].\nPreprint. Under review.\narXiv:2306.16564v3  [cs.CL]  26 Oct 2023\nAs a compromise, recently researchers resorted to heuristic approaches such as querying the LLM\nin various ways and estimate the correctness of the answer (e.g. [19]). However, these types of\napproaches are computationally expensive (multiple LLM inferences), biased by the LLM itself\n(information from inside the model), and not quantitative.\nTo address these issues, we propose a novel approach to calibrate LLM outputs and automatically\nidentify error responses. As an early attempt to tackle the LLM error problem, we restrict ourselves to\nproblems where the expected output can be categorized, such as classification in the simplest setting.\nThe intuitions behind our method are:\n1. distilling LLM to smaller networks leads to calibration via implicit label smoothing, and\n2. incorporating independent noisy signals are guaranteed to enhance LLM performance.\nTheoretically analysis of these intuitions are provided in Section 3.2.\nThe proposed self-supervision framework is illustrated in Fig. 1. Given an input instance (top), the\nLLM is prompted to output its answer (left side). We leverage noisy weak supervision signals such\nas knowledge bases (KB) and regular expressions (RX) to produce multiple heuristic labels (right\nside). As the LLM and weak sources may disagree among themselves, the challenging goal is to train\na harmonizer network h(x) providing a probabilistic estimate of the answer\u2019s correctness. In this\npaper, we focus on how to obtain a well-calibrated network utilizing LLM response and the weak\nsources. Note that the entire process does not involve any human reviewing or labeling.\nFigure 1: Self-supervision framework to calibrate LLM output and automatically detect error.\nThere have been several prior works in programmatic weak supervision to combine multiple su-\npervision sources, but most of them produce a single label per instance by performing a weighted\nsum of labels from different sources [39, 32]. These approaches, although proven to be successful\nin previous applications, have significant limitations when applied to identify LLM errors due to\nthe weighting dilemma. As LLMs can be more robust compared to other weak supervisions like\nknowledge base and regular expression, if the weight on the LLM is low, the aggregated result will\nbe noisy. Conversely if the weight on the LLM is high, the output will be overwhelmed by the LLM\nand an error would be difficult to identify. A new approach is needed.\nIn this paper, we formulate the problem as multi-objective optimization. Leveraging the abundant\nresearch in Pareto optimization [24], we propose a flexible framework that combines information\nfrom both the LLM response and supervision sources using Pareto optimal learning. The harmonizer\nnetwork h(x) is optimized on LLM and weak sources simultaneously in Pareto optimal manner, thus\novercoming the weighting dilemma. The key contributions of our study are as follows:\n1. We are the first to propose adopting Pareto optimization in combining multiple supervision\nsources, an entirely new framework compared to previous weak supervision work.\n2. The Pareto optimal learning assessed risk (POLAR) score from our framework is shown to\nbe effective in estimating LLM error probability.\n3. A dynamic prompting strategy designed to automatically improve high-risk instances is\nshown to outperform SOTA supervised model without any manually labeled training data.\n2\n2\nRelated Work\nEarly works in model calibration date back to the seminal work of Platt scaling [25], where a\nLogistic calibration model is fitted on top of the original model output. Various techniques have\nbeen developed afterwards for model calibration, including isotonic regression [38], temperature\nscaling [9], and Bayesian binning [21]. A contextual calibration method for LLMs was proposed by\n[42], which adjust the class balance by taking an ensemble of LLM queries with content-free input.\nMost of these methods rely on labeled calibration data, and there isn\u2019t a systematic self-supervised\napproach to our knowledge.\nAs accurate calibration for LLM is challenging, heuristic methods have been proposed to detect\nhallucination or estimate the confidence level of the answer. [33] used self-consistency to infer the\nreliability of the answer. [19] proposed SelfCheckGPT as a black-box method to detect hallucination.\nThe chain-of-thought (CoT) by [34] has also been used to indicate potential errors. These methods\nare less quantitative and are vulnerable that the estimated confidence is biased by the model itself.\nThe quality of the results could be highly dependent on the prompting strategy, and there is no\nsystematical way to quantify this.\nOur work steams from the weak supervision domain that aggregates multiple supervision sources\nin a single label [39]. Following early works in distant supervision [11] and crowd-sourcing [29],\ndata programming was proposed by [28, 26] as one of the milestones. Numerous methods have been\nintroduced into the field [7, 31, 32, 16], with MeTaL (currently known as Snorkel) [27] as the most\npopular method as of today. Most of the existing works weight the sources across all examples,\nleading to the weighting dilemma. We address this problem with Pareto optimization to be adaptive\nto all sources simultaneously and show that this approach offers better LLM calibration ability.\n3\nMethodology\n3.1\nProblem setup\nDenote the LLM as a function LLM(x; prompt) parameterized by user-defined prompt for the\nspecific task. The LLM is required to give a correct response to the given input x \u2208 X. As evaluating\nLLM output in the general free text form is still challenging [13], in this study we restrict ourselves\nto tasks where the desired output space Y is finite. Fixing the prompt for the specific task and map\nLLM responses into the target space with operator P (e.g. taking the first token), define\n\u039b(x) := P(LLM(x; prompt)) \u2208 Y \u222a {0},\n(1)\nwhere 0 refers to the case where the LLM states \u201cunsure\u201d when it is not confident about the answer.\nThe error or hallucination in this setting is defined as: the model confidently states and answer\n(\u039b(x) \u0338= 0) that is actually wrong (\u039b(x) \u0338= y). The goal of LLM calibration and error detection is to\ndevelop an estimator for\nP(\u039b(x) \u0338= y|\u039b(x) \u0338= 0).\n(2)\nOur self-supervision framework utilizes the following ingredients:\n\u2022 Unlabelled input examples x1, \u00b7 \u00b7 \u00b7 , xn \u2208 X.\n\u2022 LLM function \u039b(x) := P(LLM(x; prompt)) \u2208 Y \u222a {0}\n\u2022 m Supervision Sunctions1 that can be triggered to output heuristic labels:\n\u03bbj(x) \u2208 Y \u222a {0}, j = 1, \u00b7 \u00b7 \u00b7 , m.\n(3)\nThe \u201ctriggering\u201d of such supervision functions can be: containing certain keywords, satisfying certain\nregular expressions, matching certain knowledge base record, etc. Again, 0 refers to an abstain output\nwhere the supervision function is not triggered. The supervision function \u03bbj\u2019s are typically required\nto be better than a random guess [28, 35], and we will show in Section 3.2 that even slightly better\nthan random guess signals can be helpful.\n1These are also referred to as labeling functions in some literature [39]. However, we use the name supervision\nfunction for more generalized settings.\n3\n3.2\nHow to Calibrate?\nIn Section 1 we described two intuitions behind how calibration can be achieved via self-supervision.\nHere we formally state the corresponding propositions, with detailed proof in Appendix A.\nProposition 1. Suppose the ground truth label y \u2208 Y is unseen, and the imperfect supervision model\n\u039b(x) has error rate \u03b1 with miss-classification evenly distributed:\nP(\u039b(x) = y) = 1 \u2212 \u03b1,\nP(\u039b(x) = y\u2032) =\n\u03b1\n|Y| \u2212 1, y\u2032 \u0338= y.\n(4)\nThen fitting a model h(x) to \u039b(x) is equivalent to training on ground truth labels with label smoothing\nas defined in Section 1.1 by [20].\nThe proof is straightforward using conditional expectation. Here we refer to the work by [20],\nwhich empirically showed that label smoothing improves or at least doesn\u2019t hurt model calibration.\nProposition 1 shows a path towards calibrating LLM via distilling a small network on the specific\ntask. However, this along can easily go into the pitfall where the distilled model is entirely bias by the\nLLM and never able to signal its error. This shows the needs for external signals that are independent\nto the LLM itself. While human provided supervision is undesired, our next claim shows that even the\nweakest noisy signals can always help the strongest LLM, relieving manual effort in a self-supervised\nform.\nProposition 2. Consider target space Y = {\u22121, 1} for simplicity. Suppose the LLM is arbitrarily\naccurate with P(\u039b(x) = y) = p < 1, and the weak independent signal ensemble is modeled by\nw(x) \u223c N(y \u00b7 \u00b5, \u03c32) with \u00b5 > 0, then there always exist a function \u03c8(\u039b, w) s.t.\nP(\u03c8(\u039b(x), w(x)) = y) > p.\nWe prove by constructing a parameterized function class in Appendix A. The implication of this\nproposition is: for any strong LLM with accuracy approaching 100%, and any weak supervision\nsignal ensemble (we approximate using central limit theorem (CLT) into a Guaussian distribution)\nthat just need to be slightly better than random guess, we can always combine them to further enhance\nthe LLM performance.\n3.3\nPareto optimal learning\nProposition 1 shows the calibration potential of fitting a small network to an imperfect source such\nas LLM outputs. Proposition 2 reveals that even the weakest supervision sources such as reg-ex\nor knowledge base can be used to assist LLM in someway. The limitations of the assumptions are\nobvious, though. In practice the weak sources are usually dependent, making the analysis of their\nbehavior difficult. The number of such supervision sources is another reason why CLT won\u2019t hold.\nTherefore, we propose our novel approach of fitting a single network to all imperfect supervision\nsources simultaneously with Pareto optimization. Mathematically, we define it as the harmonizer\nmodel h : X \u2192 Y and solve the following Pareto optimal learning problem:\nmin\nh\u2208H\nE[\u21130(h(x), \u039b(x))],\n{E[\u2113j(h(x), \u03bbj(x))]}m\nj=1,\n(5)\nwhere all the expectations are taken for x \u223c X.\nThe challenge is that the multiple objectives in this optimization problem can potentially conflict,\nmaking the problem ill-posed. Following multi-objective learning theory [12], we resort to finding a\nharmonizer h\u2217 \u2208 H that is Pareto optimal [24] as follows.\nDefinition 1 (Pareto optimal harmonizer). h\u2217 \u2208 H is a Pareto optimal harmonizer to \u039b and\n\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbm, if there does not exist any h \u2208 H that Pareto dominates h\u2217 in Problem 5. Mathe-\nmatically, if we denote \u03bb0 := \u039b, h\u2217 needs to satisfies the following:\n\u2204h \u2208 H,\ns.t.\n\u001a\n\u2200j = 0, 1, \u00b7 \u00b7 \u00b7 , m,\nE[\u2113j(h(x), \u03bbj(x))] \u2264 E[\u2113j(h\u2217(x), \u03bbj(x))],\n\u2203j = 0, 1, \u00b7 \u00b7 \u00b7 , m,\nE[\u2113j(h(x), \u03bbj(x))] < E[\u2113j(h\u2217(x), \u03bbj(x))].\nThe Pareto optimization framework deals with dependency between supervision sources nicely,\ne.g. the Pareto optimality of a harmonizer won\u2019t be affected by arbitrarily duplicating supervision\n4\nfunctions. However, finding Pareto optimal solutions is a challenging goal in the multi-objective\noptimization literature. One of the most popular approach is by scalarizing the multiple objectives\ninto a single one. For our specific problem, we propose to approximate the problem by minimize the\nfollowing Pareto loss scalarizer G : Rm+1\n+\n\u2192 R+:\nmin\nh\u2208H\nEx\u223cX [G (\u21130(h(x), \u039b(x)), \u21131(h(x), \u03bb1(x)), \u00b7 \u00b7 \u00b7 , \u2113m(h(x), \u03bbm(x)))].\n(6)\nWe require G : Rm+1\n+\n\u2192 R+ to satisfy the following conditions.\nDefinition 2 (Pareto scalarizer). G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) is a Pareto scalarizer, if it satisfies:\n\u2022 G(\u21130, \u00b7 \u00b7 \u00b7 , \u2113\u2032\nj, \u00b7 \u00b7 \u00b7 , \u2113m) < G(\u21130, \u00b7 \u00b7 \u00b7 , \u2113j, \u00b7 \u00b7 \u00b7 , \u2113m) if \u2113\u2032\nj < \u2113j, for \u2200j = 0, 1, \u00b7 \u00b7 \u00b7 , m;\n\u2022 G : Rm+1\n+\n\u2192 R+ is convex.\nIn this study, we explored four different types of scalarizers, namely:\n\u2022 Linear scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) = Pm\nj=0 \u2113j = \u2225\u20d7\u2113\u22251.\n\u2022 Quadratic scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) =\n\u0010Pm\nj=0 \u2113j\n\u00112\n= \u2225\u20d7\u2113\u22252\n1.\n\u2022 Euclidean norm scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) =\nqPm\nj=0 \u21132\nj = \u2225\u20d7\u2113\u22252.\n\u2022 Chebyshev scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) = maxm\nj=0 \u2113j = \u2225\u20d7\u2113\u2225\u221e.\nNote that the nonlinear function of \u20d7\u2113 is important in shaping the optimization in Eq. 6 different through\nJensen\u2019s inequality, e.g. the quadratic scalarizer emphasize more on the challenging examples. While\nthe first three scalarizers are Pareto scalarizer, the Chebyshev scalarizer does not satisfies the above\ndefinition, which serves as a comparison in our experiment.\nFor any Pareto scalarizer, the proposed approach in Eq. 6 is guaranteed by the following theorem.\nTheorem 1. Suppose G : Rm+1\n+\n\u2192 R+ is a Pareto scalarizer as in Definition 2. Solving the problem\nin Equation 6 approximate a Pareto optimal harmonizer by upperbounding an objective whose\noptimal solution is Pareto optimal as in Definition 1.\nWe refer to Appendix A for a detailed proof of the theorem. The outline of the proof is two-fold. In\nthe first step, Jensen\u2019s inequality is applied to upperbound an intermediate objective. We then use the\ndefinitions of Pareto optimal harmonizer and Pareto scalarizer to prove by contradiction.\nBased on the theoretical analysis above, we propose to find the harmonizer model h(x) for LLM\ncalibration by solving problem 6 using stochastic gradient-based algorithms such as Adam [14]. In\norder for the h(x) to deliver high-quality calibration, we recommend using cross-entropy loss for\n\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m. Once an optimal solution h\u2217 \u2208 H is found, for any input x and LLM response \u039b(x),\nthe Pareto optimal learning assessed risk (POLAR) score is defined as\n\u03b6(x, \u039b(x); h\u2217) = PY \u223ch\u2217(x)(\u039b(x) \u0338= Y |\u039b(x) \u0338= 0),\n(7)\nwhere PY \u223ch\u2217(x) is the probability distribution of Y estimated by h\u2217(x). The whole process is\nsummarized in Algorithm 1 below.\n3.4\nLLM error correction with POLAR-assisted dynamic prompting\nBeing able to identify LLM responses with high risk of error opens up the opportunity to automatically\nimprove its responses. Towards this goal we propose two dynamic prompting strategies, both assisted\nby the POLAR score, to illustrate the potential use of the POLAR score to correct LLM errors. The\ntwo strategies are as follows:\nDynamic self-examination\nIn this strategy, whenever the POLAR score \u03b6(x, \u039b(x); h\u2217) > \u03b4 for\nthreshold \u03b4, we ask the LLM to reflect on its answer. See appendix for detailed prompt.\n5\nAlgorithm 1 POLAR for LLM responses\n1: Input: LLM response \u039b as in Equation 1, supervision functions \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbm as in Equation 3,\nunlabeled training examples x1:n. Initialize harmonizer h \u2208 H.\n2: for i = 1 to n do\n3:\nlossLLM = (\u039b(xi) \u0338= 0) \u2217 \u2113(h(xi), \u039b(xi))\n4:\nfor j = 1 to m do\n5:\nlossj = (\u03bbj(xi) \u0338= 0) \u2217 \u2113(h(xi), \u03bbj(xi))\n6:\nend for\n7:\nUpdate h with SGD iteration of minh G(lossLLM, loss1, \u00b7 \u00b7 \u00b7 , lossm).\n8: end for\n9: Output: Harmonizer h\u2217. For any example x and LLM response \u039b(x), the POLAR score of the\nresponse is estimated as according to Equation 7.\nDynamic self-supervision\nIn this strategy, we utilize the supervision functions as sources to help\nthe LLM reflect on its initial response (e.g. stating the reg-ex of knowledge base findings).\nAlgorithm 2 outlines the POLAR-assisted dynamic prompting strategy. We provide detailed prompt-\ning design description in the Appendix D.\nAlgorithm 2 POLAR-assisted dynamic self-supervision for LLM\n1: Input: Example x, LLM response \u039b(x), supervision functions \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbm, harmonizer h\u2217.\n2: if \u03b6(x, \u039b(x); h\u2217) > \u03b4 then\n3:\nInitialize Reflection Prompt.\n4:\nfor j = 1 to m do\n5:\nif \u03bbj(x) \u0338= 0 then\n6:\nAdd evidence from supervision function j to the Reflection Prompt.\n7:\nend if\n8:\nend for\n9:\nRespond to the LLM with the Reflection Prompt and get new response \u039b\u2032(x).\n10:\nreturn \u039b\u2032(x)\n11: else\n12:\nreturn \u039b(x)\n13: end if\n4\nExperiments\nDataset\nSince one of the essential ingredients in our framework is the supervision functions, for\nreproducibility we only experiment on tasks that have publicly available supervision functions, which\nare mostly developed in the weak supervision literature. As such, we refer to the benchmark datasets\ncollected by [40], and experiment on four different NLP tasks, namely CDR [17], ChemProt [15],\nSemEval [10], and SMS [1]. The labels on the training sets are removed. Gold labels are available on\nthe test sets for evaluation. We select the datasets to broadly cover the following aspects:\n\u2022 Domain: General domain (SemEval, SMS), biomedical domain (CDR, ChemProt).\n\u2022 Task: Relation extraction (CDR, ChemProt, SemEval), classification (SMS).\n\u2022 Difficulty: The problem set includes tasks that are easily solvable by advanced LLMs like\nGPT-4 such as SMS (99% F-1), as well as tasks that still challenging for GPT-4, e.g. CDR\n(74% F-1), ChemProt (42% F-1), and SemEval (67% F-1).\nPrompt design\nIn order to leverage the maximal capability of the LLMs, we carefully design the\nprompt for each problem to clarify the problem setting, knowledge background, input and output\nstructure, and instruction to state \"unsure\". See appendix for detailed prompt information.\nSupervision functions\nThe supervision functions for the datasets are based on simple rules provided\nby human experts [26, 37, 43, 2]. The rules in the supervision functions include:\n6\n\u2022 Keywords and regular expression pattern checking.\n\u2022 Knowledge base (e.g. the Comparative Toxicogenomics Database [5])\n\u2022 Hierarchical combination of the above.\nHarmonizer training\nWe train harmonizer h(x) by Pareto optimal learning following Equation 6\nand Algorithm 1. Our main experiments used BERT [6] (PubMedBERT [8] for biomedical datasets\nCDR and ChemProt) as the harmonizer model, and quadratic scalarizer. Other choices are discussed\nin Section 5. Training details are described in Appendix C.\n4.1\nPOLAR calibration of LLM error\nIn this section we present our implementation results of Algorithm 1. The fitted harmonizer is applied\nto unseen test set to give a POLAR score for each LLM response. Gold labels on the test set are used\nto estimate the actual error rate in Equation 2, and to evaluate how well the proposed POLAR score\nis calibrated and correlated with the actual error rate.\nFig. 2 shows the POLAR score calibration results for GPT-4 on the CDR chemical-disease relation\nextraction task. The calibration curve in Figure 2a shows that the POLAR score is a good estimator\nof the true probability that the LLM is making a mistake, as defined in Equation 2. Figure 2b shows\nthat the POLAR score is highly correlated with the error rate. Lastly, Figure 2c shows that for all\nthree different LLMs, the responses with the highest POLAR score are the most likely to generate\nhallucination or errors, and the highest POLAR score indicate almost 100% error rate.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPOLAR risk score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nError rate on gold label\nECE:  0.043\nPOLAR Calibration Plot        GPT-4    CDR\n(a) Error calibration curve\n0.2\n0.4\n0.6\n0.8\nAverage POLAR risk score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nError rate on gold labels\nR-squared:  0.890\nPOLAR Score v.s. Error Rate    CDR  GPT-4\nObserved error rate\nLeast squares fit\n(b) Correlation with error rate\n0\n20\n40\n60\n80\n100\nPercentile of Top Risky Responses (%)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nResponse error rate\nError rate among top risky LLM responses    CDR\nGPT-4\nGPT-3.5-turbo\nText-davinci-003\n(c) LLM error detection\nFigure 2: LLM error calibration and hallucination detection using POLAR score. Figure 2a shows\nthe error rate on the test set for LLM responses ranked into ten POLAR score bins. The expected\ncalibration error (ECE) is the weighted average of the absolute deviation of POLAR score from the\nactual error rate in each bin. Figure 2b ranks the LLM responses into POLAR score bins each with\n100 examples, and plot the average PLOAR score and error rate for each bin. The R2 is calculated on\nthe scatter plot. Figure 2c shows the LLM error rate among the top POLAR score examples.\n7\nTabel 1 shows the performance of POLAR score for LLM error calibration. We provide two metrics,\nexpected calibration error (ECE) and R2 between the POLAR score and the actual error rate, both\ncalculated the same as in Figure 2. We report the results for four datasets, and three LLMs (GPT-4,\nGPT-3.5-turbo, and text-davinci-003). The following four methods serve as baseline comparison:\n\u2022 Snorkel [27]: One of the most recommended weak supervision method [40] combining\nmultiple supervision sources via matrix completion. Snorkel model fitted on training set is\nuse to give class probabilities for LLM error rate.\n\u2022 Majority vote: Another popular weak supervision method that estimates class probability\naccording to the voted ratios among the LLM and the supervision functions.\n\u2022 LLM distilled: Fit a BERT (PubMedBERT) model to the LLM output on the task directly.\nTake the class probabilities from the finetuned BERT model to estimate LLM error rate.\n\u2022 LLM ensemble: Query the LLM multiple times and estimate class probability from the\nresponse ensemble. As the approach is extremely expensive, we only implement for GPT-\n3.5-turbo on CDR with resulting ECE = 0.4466, far from comparable to other approaches.\nFrom Tabel 1 we can see that for all three LLMs across all four tasks, the proposed POLAR score\nconsistently outperform all other methods. Among the baseline methods, Snorkel and LLM distilled\nmodel can achieve top or close-to-top performance in some cases under specific metric, but lack the\nconsistency to deliver stable calibration for different LLMs on different tasks. In comparison, the\nproposed POLAR score is consistently well-calibrated to the true error rate.\nTable 1: LLM error calibration using POLAR score, compared with baseline methods. The best\nentries from each row in terms of low ECE and high R2 are highlighted in bold.\nPOLAR\nSnorkel\nMajority vote\nLLM Distilled\nLLM (CDR)\nECE\nR2\nECE\nR2\nECE\nR2\nECE\nR2\nGPT-4\n0.0431\n0.8898\n0.1669\n0.2990\n0.1450\n0.3482\n0.1643\n0.5918\nGPT-3.5-turbo\n0.0463\n0.9340\n0.1642\n0.3200\n0.1817\n0.5395\n0.2216\n0.0371\nText-davinci-003\n0.0550\n0.9071\n0.1536\n0.3705\n0.1492\n0.4499\n0.0892\n0.8964\nLLM (ChemProt)\nECE\nR2\nECE\nR2\nECE\nR2\nECE\nR2\nGPT-4\n0.0351\n0.9340\n0.1824\n0.5102\n0.2334\n0.2439\n0.2161\n0.7663\nGPT-3.5-turbo\n0.0481\n0.9436\n0.2283\n0.6253\n0.2822\n0.0307\n0.1590\n0.8447\nText-davinci-003\n0.0509\n0.9170\n0.2176\n0.6995\n0.2794\n0.3068\n0.1961\n0.8248\nLLM (SemEval)\nECE\nR2\nECE\nR2\nECE\nR2\nECE\nR2\nGPT-4\n0.0792\n0.9157\n0.0683\n0.7140\n0.1145\n0.3785\n0.0627\n0.9470\nGPT-3.5-turbo\n0.0473\n0.9631\n0.1498\n0.8212\n0.2773\n0.2081\n0.1078\n0.7571\nText-davinci-003\n0.0665\n0.9495\n0.1186\n0.7962\n0.2417\n0.3961\n0.0647\n0.9358\nLLM (SMS)\nECE\nR2\nECE\nR2\nECE\nR2\nECE\nR2\nGPT-4\n0.0144\n0.9802\n0.2435\n0.0887\n0.5882\n0.0914\n0.0127\n0.9768\nGPT-3.5-turbo\n0.0409\n0.9627\n0.0753\n0.2021\n0.1481\n0.0062\n0.1312\n0.7754\nText-davinci-003\n0.0229\n0.9427\n0.2006\n0.0528\n0.3250\n0.0909\n0.0328\n0.9558\n4.2\nLLM error correction with POLAR-assisted dynamic prompting\nIn this experiment we explore different dynamic prompting strategies in Section 3.4 to correct LLM\nerrors. As an extension experiment, we only focus on the CDR dataset for GPT-4 and GPT-3.5-turbo.\nWe sort the initial LLM responses by their POLAR score, and compute the error rate before and\nafter dynamic prompting. Figure 3a shows that the GPT-4 error rate decreases significantly for both\nstrategies, if and only if the POLAR score was high. Otherwise, re-prompting for self-examine or\nself-supervision can even increase error rate. Therefore, the best strategy is to only perform dynamic\nprompting when seeing a high POLAR score.\nFigure 3b shows the implementation result of dynamic self-examination and dynamic self-supervision\nas in Algorithm 2. We choose the POLAR score threshold as \u03b4 = 0.5. We can see that the two\n8\nstrategies increase the performance for both GPT-3.5-turbo and GPT-4 models. With the dynamic\nself-supervision, GPT-3.5-turbo surpasses the best result so far that was achieved without gold labeled\ntraining data [40], and GPT-4 outperforms the state of the art supervised method utilizing fully-labeled\ndata [36]. It is noteworthy that both strategies make use of zero gold-labeled examples.\n0.2\n0.4\n0.6\n0.8\nPOLAR risk score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nResponse error rate\nError rate before and after dynamic prompting\nInitial GPT-4 response\nGPT-4 response after self-examination\nGPT-4 response after self-supervision\n(a) POLAR score and LLM error rate change\nZero-shot\nSelf-examination\nSelf-supervision\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nF-1 score\nPOLAR-assisted dynamic prompting\nBest w/o gold label\nSupervised SOTA\nGPT-3.5-turbo\nGPT-4\n(b) Dynamic prompting performance\nFigure 3: (a) GPT-4 error rate before and after dynamic prompting, conditional on the initial POLAR\nscore. (b) shows the performance improvement using the two dynamic prompting strategies.\n5\nComponents for Calibration\nExternal sources of information The supervision functions serve as external sources of information\nin helping the harmonizer to be well-calibrated, and not biased by over-fitting to the LLM responses.\nTo compare with the scenario where external sources of information is absent, we refer to the LLM\ndistilled model columns in Table 1. We can see that without the supervision functions, the calibration\nability is not consistently good due to overfitting to the LLM.\nPareto loss scalarizer and harmonizer Table 2 shows the ECE and R2 measure for different loss\nscalarizer G, and different harmonizer model type, averaged over three LLMs on the four tasks. The\nnonlinear quadratic loss scalarizer paired with BERT finetuning gives the best calibration ability. For\nsimpler models like MLP and LR, simple linear scalarizer works best. Also note that the Chebyshev\nscalarizer has the worst performance in almost all cases, which experimentally supports Theorem 1\nthat a Pareto loss scalarizer (Def. 2) is essential to approximate Pareto optimal harmonizer (Def. 1).\nTable 2: Average calibration ability for different loss scalarizer G and harmonizer type.\nG function\nLinear\nQuadratic\nEuclidean norm\nChebyshev\nHarmonizer type\nECE\nR2\nECE\nR2\nECE\nR2\nECE\nR2\nBERT\n0.0625\n0.9273\n0.0458\n0.9366\n0.0549\n0.9003\n0.0711\n0.8260\nMLP\n0.0555\n0.9392\n0.0974\n0.9188\n0.0691\n0.9302\n0.0775\n0.8934\nLR\n0.0641\n0.9360\n0.1072\n0.9020\n0.0766\n0.9288\n0.0948\n0.8813\n6\nConclusion\nWe propose a novel framework for LLM calibration using Pareto optimal self-supervision. Our\ntheoretical results showed the calibration potential of distilled model from LLM, and the importance\nof incorporating independent weak supervision signals. The proposed Pareto optimal learning\nproblem was showed to approximate Pareto optimality. Experimentally, the proposed POLAR score\nis consistently well-calibrated to the probability of LLM error. We introduce POLAR-based dynamic\nprompting to automatically correct LLM errors, which boosts a GPT-4 baseline performance to\noutperform the SOTA supervised model, without using any human-labeled training data.\n9\nReproducibility Statement\nThe proof of the theoretical results are in Appendix A. Implementation of our experiments are\nillustrated in Algorithms 1 and 2. The training details of the harmonizer model are listed in Appendix\nC. The prompts for querying the LLMs are described in Appendix D to reproduce the response.\nAnonymized code is provided in the supplementary material.\nReferences\n[1] T. A. Almeida, J. M. G. Hidalgo, and A. Yamakami. Contributions to the study of sms spam\nfiltering: new collection and results. In Proceedings of the 11th ACM symposium on Document\nengineering, pages 259\u2013262, 2011.\n[2] A. Awasthi, S. Ghosh, R. Goyal, and S. Sarawagi. Learning from rules generalizing labeled\nexemplars. arXiv preprint arXiv:2004.06025, 2020.\n[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\n[4] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev,\nH. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,\nH. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai,\nT. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,\nX. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean,\nS. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022.\n[5] A. P. Davis, C. J. Grondin, R. J. Johnson, D. Sciaky, J. Wiegers, T. C. Wiegers, and C. J.\nMattingly. Comparative toxicogenomics database (ctd): update 2021. Nucleic acids research,\n49(D1):D1138\u2013D1143, 2021.\n[6] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. CoRR, abs/1810.04805, 2018.\n[7] D. Fu, M. Chen, F. Sala, S. Hooper, K. Fatahalian, and C. R\u00e9. Fast and three-rious: Speeding\nup weak supervision with triplet methods. In International Conference on Machine Learning,\npages 3280\u20133291. PMLR, 2020.\n[8] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon.\nDomain-specific language model pretraining for biomedical natural language processing, 2020.\n[9] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In\nInternational conference on machine learning, pages 1321\u20131330. PMLR, 2017.\n[10] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. O. S\u00e9aghdha, S. Pad\u00f3, M. Pennacchiotti,\nL. Romano, and S. Szpakowicz. Semeval-2010 task 8: Multi-way classification of semantic\nrelations between pairs of nominals. arXiv preprint arXiv:1911.10422, 2019.\n[11] R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and D. S. Weld. Knowledge-based weak\nsupervision for information extraction of overlapping relations. In Proceedings of the 49th\nannual meeting of the association for computational linguistics: human language technologies,\npages 541\u2013550, 2011.\n[12] C.-L. Hwang and A. S. M. Masud. Multiple objective decision making\u2014methods and applica-\ntions: a state-of-the-art survey, volume 164. Springer Science & Business Media, 2012.\n[13] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung.\nSurvey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338,\n2023.\n10\n[14] D. P. Kingma and J. Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[15] M. Krallinger, O. Rabal, S. A. Akhondi, M. P. P\u00e9rez, J. Santamar\u00eda, G. P. Rodr\u00edguez, G. Tsat-\nsaronis, A. Intxaurrondo, J. A. L\u00f3pez, U. Nandal, et al.\nOverview of the biocreative vi\nchemical-protein interaction track. In Proceedings of the sixth BioCreative challenge evaluation\nworkshop, volume 1, pages 141\u2013146, 2017.\n[16] H. Lang and H. Poon. Self-supervised self-supervision by combining deep learning and\nprobabilistic logic. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npages 4978\u20134986, 2021.\n[17] J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P. Davis, C. J. Mattingly, T. C.\nWiegers, and Z. Lu. Biocreative v cdr task corpus: a resource for chemical disease relation\nextraction. Database, 2016, 2016.\n[18] I. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[19] P. Manakul, A. Liusie, and M. J. Gales. Selfcheckgpt: Zero-resource black-box hallucination\ndetection for generative large language models. arXiv preprint arXiv:2303.08896, 2023.\n[20] R. M\u00fcller, S. Kornblith, and G. E. Hinton. When does label smoothing help? Advances in\nneural information processing systems, 32, 2019.\n[21] M. P. Naeini, G. Cooper, and M. Hauskrecht. Obtaining well calibrated probabilities using\nbayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29,\n2015.\n[22] OpenAI. Gpt-4 technical report, 2023.\n[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with\nhuman feedback, 2022.\n[24] V. Pareto. Cours d\u2019\u00e9conomie politique, volume 1. Librairie Droz, 1964.\n[25] J. Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized\nlikelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999.\n[26] A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. R\u00e9. Snorkel: Rapid training\ndata creation with weak supervision. In Proceedings of the VLDB Endowment. International\nConference on Very Large Data Bases, volume 11, page 269. NIH Public Access, 2017.\n[27] A. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R\u00e9. Training complex mod-\nels with multi-task weak supervision. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33, pages 4763\u20134771, 2019.\n[28] A. J. Ratner, C. M. De Sa, S. Wu, D. Selsam, and C. R\u00e9. Data programming: Creating large\ntraining sets, quickly. Advances in neural information processing systems, 29, 2016.\n[29] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning\nfrom crowds. Journal of machine learning research, 11(4), 2010.\n[30] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\n[31] P. Varma, F. Sala, S. Sagawa, J. Fries, D. Fu, S. Khattar, A. Ramamoorthy, K. Xiao, K. Fatahalian,\nJ. Priest, et al. Multi-resolution weak supervision for sequential data. Advances in Neural\nInformation Processing Systems, 32, 2019.\n[32] H. Wang and H. Poon. Deep probabilistic logic: A unifying framework for indirect supervision.\narXiv preprint arXiv:1808.08485, 2018.\n11\n[33] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[35] R. Wu, S.-E. Chen, J. Zhang, and X. Chu. Learning hyper label model for programmatic weak\nsupervision. In The Eleventh International Conference on Learning Representations.\n[36] Y. Xiao, Z. Zhang, Y. Mao, C. Yang, and J. Han. Sais: supervising and augmenting intermediate\nsteps for document-level relation extraction. arXiv preprint arXiv:2109.12093, 2021.\n[37] Y. Yu, S. Zuo, H. Jiang, W. Ren, T. Zhao, and C. Zhang. Fine-tuning pre-trained language\nmodel with weak supervision: A contrastive-regularized self-training approach. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 1063\u20131077, 2021.\n[38] B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability\nestimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pages 694\u2013699, 2002.\n[39] J. Zhang, C.-Y. Hsieh, Y. Yu, C. Zhang, and A. Ratner. A survey on programmatic weak\nsupervision. arXiv preprint arXiv:2202.05433, 2022.\n[40] J. Zhang, Y. Yu, Y. Li, Y. Wang, Y. Yang, M. Yang, and A. Ratner. Wrench: A comprehensive\nbenchmark for weak supervision. arXiv preprint arXiv:2109.11377, 2021.\n[41] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong,\net al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\n[42] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot\nperformance of language models. In International Conference on Machine Learning, pages\n12697\u201312706. PMLR, 2021.\n[43] W. Zhou, H. Lin, B. Y. Lin, Z. Wang, J. Du, L. Neves, and X. Ren. Nero: A neural rule grounding\nframework for label-efficient relation extraction. In Proceedings of The Web Conference 2020,\npages 2166\u20132176, 2020.\n12\nA\nProof of Theorems\nProposition A.1. Suppose the ground truth label y \u2208 Y is unseen, and the imperfect supervision\nmodel \u039b(x) has error rate \u03b1 with miss-classification evenly distributed:\nP(\u039b(x) = y) = 1 \u2212 \u03b1,\nP(\u039b(x) = y\u2032) =\n\u03b1\n|Y| \u2212 1, y\u2032 \u0338= y.\n(8)\nThen fitting a model h(x) to \u039b(x) is equivalent to training on ground truth labels with label smoothing\nas defined in Section 1.1 by [20].\nProof. Let\u2019s first vectorize y and \u039b(x). Denote yk = 1 if y = k, otherwise yk = 0. Denote \u039bk = 1\nif \u039b(x) = k, otherwise \u039bk = 0. Let pk be the probability of P(h(x) = k). Fitting a model h(x) to\n\u039b(x) is essentially minimizing\nE\n\uf8ee\n\uf8f0\n|Y|\nX\nk=1\n\u2212\u039bk \u00b7 log(pk)\n\uf8f9\n\uf8fb\n=E\n\uf8ee\n\uf8f0\n|Y|\nX\nk=1\n\u2212(1 \u2212 \u03b1)yk \u00b7 log(pk) \u2212\nX\nk\u2032\u0338=k\n\u03b1\n|Y| \u2212 1yk\u2032 \u00b7 log(pk)\n\uf8f9\n\uf8fb\n=E\n\uf8ee\n\uf8f0\n|Y|\nX\nk=1\n\u2212(yk(1 \u2212 \u03b2) + \u03b2/|Y|) \u00b7 log(pk)\n\uf8f9\n\uf8fb ,\nwhere \u03b2 =\n|Y|\n|Y|\u22121\u03b1 is the label smoothing parameter.\nProposition A.2. Consider target space Y = {\u22121, 1} for simplicity. Suppose the LLM is arbitrarily\naccurate with P(\u039b(x) = y) = p < 1, and the weak independent signal ensemble is modeled by\nw(x) \u223c N(y \u00b7 \u00b5, \u03c32) with \u00b5 > 0, then there always exist a function \u03c8(\u039b, w) s.t.\nP(\u03c8(\u039b(x), w(x)) = y) > p.\nProof. Let\u2019s prove by constructing the function class\n\u03c8(\u039b, w) = sign(\u03b1\u039b + w).\nSuppose y = 1 WLOG. Then the accuracy is\nP(\u03b1\u039b + w > 0) = p \u00b7 \u03a6\n\u0012\u00b5 + \u03b1\n\u03c3\n\u0013\n+ (1 \u2212 p) \u00b7 \u03a6\n\u0012\u00b5 \u2212 \u03b1\n\u03c3\n\u0013\n,\nwhere \u03a6(\u00b7) is the normal distribution CDF.\nAs \u03b1 \u2192 \u221e, P(\u03b1\u039b + w > 0) \u2192 p as it is equivalent to the LLM itself. We just need to show that the\n\u2202P(\u03b1\u039b+w>0)\n\u2202\u03b1\n< 0 as \u03b1 \u2192 \u221e. Let\u2019s write down\n\u2202P(\u03b1\u039b + w > 0)\n\u2202\u03b1\n= p\n\u03c3 \u00b7 \u03d5\n\u0012\u00b5 + \u03b1\n\u03c3\n\u0013\n\u2212 1 \u2212 p\n\u03c3\n\u00b7 \u03d5\n\u0012\u00b5 \u2212 \u03b1\n\u03c3\n\u0013\n.\nAs both terms are positive, the condition now is for\np\n1 \u2212 p \u00b7 \u03d5\n\u0012\u00b5 + \u03b1\n\u03c3\n\u0013\n/\u03d5\n\u0012\u00b5 \u2212 \u03b1\n\u03c3\n\u0013\n< 1\nWriting down the normal PDF \u03d5(\u00b7), we just need\nexp\n\u0012(\u00b5 + \u03b1)2 \u2212 (\u00b5 \u2212 \u03b1)2\n2\u03c32\n\u0013\n= exp\n\u00122\u00b5\u03b1\n\u03c32\n\u0013\n>\n\u221a\n2\u03c0\np\n1 \u2212 p.\nwhich is easily achieved by the exponential function as \u03b1 \u2192 \u221e.\n13\nDefinition A.1 (Pareto optimal harmonizer). h\u2217 \u2208 H is a Pareto optimal harmonizer to \u039b and\n\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbm, if there does not exist any h \u2208 H that Pareto dominates h\u2217 in Problem ??. Mathemati-\ncally, if we denote \u03bb0 := \u039b, h\u2217 needs to satisfies the following:\n\u2204h \u2208 H,\ns.t.\n\u001a\n\u2200j = 0, 1, \u00b7 \u00b7 \u00b7 , m,\nE[\u2113j(h(x), \u03bbj(x))] \u2264 E[\u2113j(h\u2217(x), \u03bbj(x))],\n\u2203j = 0, 1, \u00b7 \u00b7 \u00b7 , m,\nE[\u2113j(h(x), \u03bbj(x))] < E[\u2113j(h\u2217(x), \u03bbj(x))].\nFor our specific problem, we propose to approximate the problem by minimize the following Pareto\nloss function G : Rm+1\n+\n\u2192 R+:\nmin\nh\u2208H\nEx\u223cX [G (\u21130(h(x), \u039b(x)), \u21131(h(x), \u03bb1(x)), \u00b7 \u00b7 \u00b7 , \u2113m(h(x), \u03bbm(x)))].\n(9)\nWe require G : Rm+1\n+\n\u2192 R+ to satisfy the following conditions.\nDefinition A.2 (Pareto scalarizer). G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) is a Pareto scalarizer, if it satisfies:\n\u2022 G(\u21130, \u00b7 \u00b7 \u00b7 , \u2113\u2032\nj, \u00b7 \u00b7 \u00b7 , \u2113m) < G(\u21130, \u00b7 \u00b7 \u00b7 , \u2113j, \u00b7 \u00b7 \u00b7 , \u2113m) if \u2113\u2032\nj < \u2113j, for \u2200j = 0, 1, \u00b7 \u00b7 \u00b7 , m;\n\u2022 G : Rm+1\n+\n\u2192 R+ is convex.\nTheorem A.1. Suppose G : Rm+1\n+\n\u2192 R+ is a Pareto scalarizer as in Definition A.2. Solving the\nproblem in Equation 9 approximate a Pareto optimal harmonizer by upperbounding an objective\nwhose optimal solution is Pareto optimal as in Definition A.1.\nProof. For convenience, let\u2019s denote\nuj(h) := E[\u2113j(h(x), \u03bbj(x))],\nj = 0, 1, \u00b7 \u00b7 \u00b7 , m.\nWe first show that any h\u2217 minimizing G(u0, u1, \u00b7 \u00b7 \u00b7 , um) is Pareto optimal.\nProof by contradiction. Suppose h\u2217 is not Pareto optimal. Then there must exist some h\u2032 \u2208 H Pareto\ndominating h\u2217. Without loss of generality, let\u2019s assume uj(h\u2032) < uj(h\u2217), and uk(h\u2032) \u2264 uk(h\u2217),\n\u2200k \u0338= j. Then according to Definition A.2 of Pareto scalarizer,\nG(u0(h\u2032), \u00b7 \u00b7 \u00b7 , uj(h\u2032), \u00b7 \u00b7 \u00b7 , um(h\u2032))\n(10)\n\u2264G(u0(h\u2217), \u00b7 \u00b7 \u00b7 , uj(h\u2032), \u00b7 \u00b7 \u00b7 , um(h\u2217))\n(11)\n<G(u0(h\u2217), \u00b7 \u00b7 \u00b7 , uj(h\u2217), \u00b7 \u00b7 \u00b7 , um(h\u2217)),\n(12)\nwhich contradicts the assumption that h\u2217 is the minimizer for\nG(u0(h), \u00b7 \u00b7 \u00b7 , uj(h), \u00b7 \u00b7 \u00b7 , um(h)).\nTherefore, the original statement is true, and minimizing the objective\nmin\nh\u2208H\nG (E[\u21130(h(x), \u039b(x))], E[\u21131(h(x), \u03bb1(x))], \u00b7 \u00b7 \u00b7 , E[\u2113m(h(x), \u03bbm(x))])\n(13)\ngives a Pareto optimal harmonizer.\nNext, we use Jensen\u2019s inequality to upperbound this objective with the objective in problem 9. Using\nthe fact that G is convex, we apply Jensen\u2019s inequality and get\nG (E[\u21130(h(x), \u039b(x))], E[\u21131(h(x), \u03bb1(x))], \u00b7 \u00b7 \u00b7 , E[\u2113m(h(x), \u03bbm(x))])\n(14)\n\u2264\nEx\u223cX [G (\u21130(h(x), \u039b(x)), \u21131(h(x), \u03bb1(x)), \u00b7 \u00b7 \u00b7 , \u2113m(h(x), \u03bbm(x)))].\n(15)\nTherefore, solving the problem in Equation 9 approximates Pareto optimal harmonizer by upper-\nbounding Equation 13.\n14\nB\nWeights for Rebalancing the Sources\nIn our experiments, we explored four different types of scalarization functions, namely:\n\u2022 Linear scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) := Pm\nj=0 wj\u2113j.\n\u2022 Quadratic scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) :=\n\u0010Pm\nj=0 wj\u2113j\n\u00112\n.\n\u2022 Euclidean norm scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) := \u2225 (w0\u21130, w1\u21131, \u00b7 \u00b7 \u00b7 , wm\u2113m) \u2225.\n\u2022 Chebyshev scalarizer: G(\u21130, \u21131, \u00b7 \u00b7 \u00b7 , \u2113m) := maxm\nj=0 wj\u2113j.\nThe weights wj \u2208 R are parameters of G. In the main text of the paper, we fixed to equal weights\n\u20d7w = \u20d71. Here we introduce three approaches to determine the weighting if necessary.\nEqual Weight\nThe simplest weighting scheme of\nw0 = w1 = \u00b7 \u00b7 \u00b7 = wm =\n1\nm + 1\ngives nice performance in practice, and is the method we used for the results in the main body of\nthe paper. The nonlinear Pareto scalarizers have the ability to balance the sources even under equal\nweights. It is always recommended to start with equal weight.\nIn the case that the supervision sources are highly correlated, or when the quality of the sources\nvaries a lot, we propose the following two approaches utilizing the correlation in prediction residual.\nMaximal Eigenvalue of Residual Correlation\nSuppose we have a pilot harmonizer h0 \u2208 H, which\ncan usually be obtained from minimizing a Pareto scalarizer with equal weight, it gives predicted\ndistribution \u20d7p(x) \u2208 R|Y| for any input x \u2208 X, where\npc(x) := P(h0(x) = c).\nFor any source 0 \u2264 j \u2264 m, denote the one-hot vector \u20d7\u03bbj(x) \u2208 R|Y| as:\n\u03bbj,c(x) =\n\u001a\n1\nif \u03bbj(x) = c,\n0\notherwise.\nThe prediction redisual is defined as\n\u20d7rj(x) := \u20d7\u03bbj(x) \u2212 \u20d7p(x),\nwhich accounts for the supervision function label for x that is unexplained by the harmonizer h0(x).\nIn order to rebalance the sources, we consider the correlation matrix C between the prediction residual\n\u20d7rj\u2019s. Specifically, let the covariance be\n\u03a3ij := Ex\u223cX [\u20d7ri(x) \u00b7 \u20d7rj(x)] \u2212 Ex\u223cX [\u20d7ri(x)] \u00b7 Ex\u223cX [\u20d7rj(x)].\nThe correlation variance is denoted as\nCij =\n\u03a3ij\np\n\u03a3ii\u03a3jj\n.\nWe rebalance the sources according to the eigenvector \u20d7vmax \u2208 Rm+1 corresponding to the largest\neigenvalue of C. In order to get reasonable weights, we first normalize \u20d7vmax such that the sum of the\nentries equals to one. Then we project \u20d7vmax to the weights simplex with minimal values\n\u03f5\nm+1:\nwexcess,j =\n\u0012\n\u20d7vmax,j \u2212\n\u03f5\nm + 1\n\u0013+\n\u20d7wmax =\n\u03f5\nm + 1 +\nwexcess\n\u2225wexcess\u22251\n\u00b7 (1 \u2212 \u03f5).\n15\nThis projection ensures that the weight on each source is at least \u03f5 portion of the value from equal\nweight, with the minimal ratio threshold \u03f5 \u2208 [0, 1].\nMaximal eigenvalue method is recommended when the sources are relatively independent, and when\nthe quality of the sources differ a lot. Intuitively, suppose two sources tends to agree with each other\nwhen they are not fitted well by the harmonizer, because there isn\u2019t intrinsic dependency between\nthe sources, it is likely that the true label is given by the sources. Therefore, a maximal eigenvalue\nrebalancing scheme puts higher weights on the sources to encourage the harmonizer to fit to the\nunexplained examples.\nMinimal Variance of Residual Correlation\nThe same as in the maximal eigenvalue method, we\nconsider the correlation matrix C between the prediction residual \u20d7rj\u2019s. Instead of finding the maximal\neigenvalue of C, we consider solving the following minimal variance problem:\nmin\nv\nvT Cv,\ns.t. 1T v = 1.\nThis problem admits the closed form solution of\nvmin =\nC\u221211\n1T C\u221211.\nAgain, we project \u20d7vmin to the weights simplex with minimal values\n\u03f5\nm+1:\nwexcess,j =\n\u0012\n\u20d7vmin,j \u2212\n\u03f5\nm + 1\n\u0013+\n\u20d7wmin =\n\u03f5\nm + 1 +\nwexcess\n\u2225wexcess\u22251\n\u00b7 (1 \u2212 \u03f5),\nwhich ensures that the weight on each source is at least \u03f5 portion of the value from equal weight, with\nthe minimal ratio threshold \u03f5 \u2208 [0, 1].\nThe minimal variance method is a classical portfolio rebalancing strategy in financial mathematics.\nThe intuition behind the algorithm is minimizing the risk by diversification. This rebalancing\nscheme is useful when there are intrinsic dependency between the sources. Suppose two sources are\nduplicates and always tend to give the same label, their residuals should also be highly correlated.\nMinimal variance optimization automatically avoid putting too much weights on the duplicating\nsources.\nWhile the equal weight method typically delivers good results in the simplest way, the other two\nrebalancing schemes are designed to address the specific concern such as source dependency and\nquality. It is always recommended to check against the labels on a validation set if available.\n16\nC\nTraining details\nWe explored different configurations of Pareto optimal learning below:\n\u2022 Harmonizer model: we experiment 1. BERT [6] (PubMedBERT [8] for biomedical datasets\nCDR and ChemProt), 2. multi-layer perceptron (MLP), 3. Logistic regression (LR). The\nlast two are built on top of the last layer embedding of the corresponding BERT model.\n\u2022 Pareto loss scalarizer: we experiment all four loss scalarization functions as defined in\nSection 3.3, namely linear, quadratic, Euclidean norm, and Chebyshevy scalarization.\n\u2022 Optimizer: We use AdamW [18] optimizer with learning rate [10\u22124, 10\u22125, 10\u22126], weight\ndecay [10\u22124, 10\u22125], batch size 16. All hyperparameters are optimized on held out dev set.\n\u2022 Computation: We trained on Azure Standard NC12s v3 with 1 Nvidia V100 GPU.\nD\nLLM Prompting Details\nIn this section we will describe the details of the prompts used to query the LLMs.\nD.1\nOut-of-the-box prompt\n\u2022 Setting: describe the role of the LLM in the task, and the overall problem setting.\n\u2022 Background: necessary background knowledge for domain specific tasks, including infor-\nmation from annotation guidelines for human annotators.\n\u2022 Data structure: for relation extraction tasks, explain the definition of each entity.\n\u2022 Desired output: describe the list of the desired output. For each of the categories, provide\nexplanation and optionally some examples.\n\u2022 Chain of thought (CoT): instruction to encourage the LLM to think step-by-step, articulate\npoint-by-point, and give the response in the desired structure.\n\u2022 Confidence: ask the model to state \u201cunsure\u201d if it is not confident about the answer.\n\u2022 Example: state the example and ask the model to perform the task.\nEach prompt for out-of-the-box (zero-shot) prediction contains:\n\u2022 A problem setting part that depends on the specific dataset.\n\u2022 A response regularization part that encourages chain-of-thought (CoT) and confidence check,\nand specifies proper response format.\n\u2022 A task instance part that contains the input instance and restates the task to perform.\nProblem setting prompt\n\u2022 CDR: \u201cYou are an intelligent assistant to extract chemical-disease relations from academic\nliterature. Your job is to determine if in the given piece of text, the drug (entity 1) induces\nthe disease (entity 2) or not. Negative means the drug does NOT induce the disease.\nPositive means the drug induces the disease. Please use your judgement to the best of your\nknowledge. Your answer should be classified into the following categories: [Negative,\nPositive]. \u201d\n\u2022 ChemProt: \u201cYou are an intelligent assistant to extract chemical-protein interaction from\nacademic literature. Your task is to identify the chemical-protein interactions (CHEMPROT)\nbetween entity 2: Chemical Entities Mentions (CEMs) and entity 1: Gene and Protein\nRelated Objects (named as GPRO in the instruction below) in the given piece of text. In\nbrief, the chemical-protein interactions include direct interactions (when a physical contact\nexits between a CEM and a GPRO, in most cases this GPRO being a protein or protein\n17\nfamily and alters its function/activity) as well as indirect regulatory interactions between\nCEMs and GPROs (including genes, gene products (proteins, RNA), DNA/protein sequence\nelements and protein families, domains and complexes) that alter either the function or the\nquantity of the GPRO. The guidelines below provide curation rules to evaluate if the given\nsentence contains a description of a chemical-protein interaction; in particular, if sufficient\ndetail/evidence is provided for comentioned CEMs and GPROs. Additionally, it describes\ncuration rules and definitions to assign each identified chemical-protein interaction to any of\nthe 10 classes, with detailed description listed below:\n0. Part of: CEM that are structurally related to a GPRO: e.g. specific amino acid\nresidues of a protein.\n1. Regulator: CEM that clearly regulates a GPRO, but for which there is no further\ninformation on whether the regulation is direct or indirect.\n2. Upregulator: CEM that increments a GPRO signal, without any insight on the\nmechanism.\n3. Downregulator: CEM that decreases a GPRO signal, without any insight on the\nmechanism.\n4. Agonist: CEM that binds to a receptor and alters the receptor state resulting in a\nbiological response.\n5. Antagonist: CEM that reduces the action of another CEM, generally an agonist.\nMany antagonists act at the same receptor macromolecule as the agonist.\n6. Modulator: CEM that acts as allosteric modulator, compound that increases or\ndecreases the action of an (primary or orthosteric) agonist or antagonist by combining\nwith a distinct (allosteric or allotropic) site on the receptor macromolecule.\n7. Cofactor: CEM that is required for a protein\u2019s biological activity to happen.\n8. Substrate/Product: CEM that is both, substrate and product of enzymatic reaction.\n9. NOT: This class should be used to define the NEGATIVE occurrence of a chemical-\nprotein interaction, without providing any further information on the specific negative\nCHEMPROT class or class.\nPlease identity the CHEMPROT interaction to the best of your knowledge. Your answer\nshould be classified into the following categories: [Part of, Regulator, Upregulator,\nDownregulator, Agonist, Antagonist, Modulator, Cofactor, Substrate/Product, NOT]. \u201d\n\u2022 SemEval: \u201cYou are an intelligent assistant to help recognize semantic relations between\npairs of nomimals. For example, tea and ginseng are in an ENTITY-ORIGIN relation in\n\"The cup contained tea from dried ginseng.\". You will be given a piece of text, and Entity 1\nand Entity 2 in the text for you to classify their semantic relation. The semantic relations are\nin the format of \"entity1-entity2\". The complete semantic relation inventory is given below:\n0. Cause-Effect: An event or object (entity 1) leads to an effect (entity 2). Example:\nthose cancers (entity 2) were caused by radiation exposures (entity 1)\n1. Component-Whole: An object (entity 1) is a component of a larger whole (entity 2).\nExample: my apartment (entity 2) has a large kitchen (entity 1)\n2. Content-Container: An object (entity 1) is physically stored in a delineated area of\nspace (entity 2). Example: a bottle (entity 2) full of honey (entity 1) was weighed\n3. Entity-Destination: An entity (entity 1) is moving towards a destination (entity 2).\nExample: the boy (entity 1) went to bed (entity 2)\n4. Entity-Origin: An entity (entity 1) is coming or is derived from an origin (entity 2)\n(e.g., position or material). Example: letters (entity 1) from foreign countries (entity 2)\n5. Instrument-Agency: An agent (entity 2) uses an instrument (entity 1). Example:\nphone (entity 1) operator (entity 2)\n6. Member-Collection: A member (entity 1) forms a nonfunctional part of a collection\n(entity 2). Example: there are many trees (entity 1) in the forest (entity 2)\n7. Message-Topic: A message (entity 1), written or spoken, is about a topic (entity 2).\nExample: the lecture (entity 1) was about semantics (entity 2)\n8. Product-Producer: A producer (entity 2) causes a product (entity 1) to exist. Example:\na factory (entity 2) manufactures suits (entity 1)\n18\nPlease determine the semantic relation between entity 1 and entity 2 in the given text to\nthe best of your knowledge. Your answer should be classified into the following categories:\n[Cause-Effect, Component-Whole, Content-Container, Entity-Destination, Entity-Origin,\nInstrument-Agency, Member-Collection, Message-Topic, Product-Producer]. \u201d\n\u2022 SMS: \u201cYou are an intelligent assistant to determine if a text message is spam or not spam\n(ham). Your answer should be classified into the following categories: [ham, spam]. \u201d\nResponse regularization prompt\n\u201cYou may think step by step, articulate point by point, or make\nconclusion from multiple evidences, but please always state the most likely label as your answer at\nthe very begining of your response. You are encouraged to reflect on your response, but please keep\nin mind that a clear answer is always desired. Try to give a clear answer at your best guess even when\nyou are not very sure, in which case any of your conserns or explanations should go after the most\nlikely answer to the best of your knowledge. If you are very unsure about the answer and are not\nwilling to explicitly state any label, please say \u2019unsure\u2019 at the very begining of your response. \u201d\nTask instance prompt\n\u2022 Classification (for SMS):\n\u201cPlease classify the following example into the most likely category: [TEXT] \u201d\n\u2022 Relation extraction (for CDR, ChemProt, SemEval):\n\u201cPlease classify the following example into the most likely category: [TEXT] Entity 1\n[ENTITY 1] Entity 2: [ENTITY 2] \u201d\nThe complete prompt for querying the LLM is\nProblem setting prompt + Response regularization prompt + Task instance prompt\nD.2\nDynamic prompting\nIn dynamic prompting, we query another follow-up prompt after the LLM gives the initial out-of-the-\nbox response. As this is an extension to our main experiments, we only implemented for the CDR\nrelation extraction task. The follow-up prompts for the two dynamic prompting strategies are:\nDynamic self-examination\n\u201cAre you sure about your previous answer? If not, please give a new\nanswer. Otherwise, please restate your previous answer. \u201d\nDynamic self-supervision\n\u201cIt is possible that the answer could be something else. Here are some\nevidences to help you figure out the right answer.\nEvidencesFromSupervisionFunctions(x, \u20d7\u03bb(x))\nAre you sure about your previous answer? If not, please give a new answer. Otherwise, please restate\nyour previous answer. \u201d\nEvidencesFromSupervisionFunctions(x, \u20d7\u03bb(x)) contains evidences from all the supervision\nfunctions \u03bbj(x) \u0338= 0 that are triggered by the input instance x. Examples of evidence from the\nsupervision functions are shown below. Note that each evidence will be provided only when the\ncorresponding supervision function is triggered.\n\u2022 \u201cAccording to the Comparative Toxicogenomics Database, the relation between the given\nchemical-condition pair is listed, confirming the answer. \u201d\n\u2022 \u201cAccording to the Comparative Toxicogenomics Database, the given chemical-condition\npair \"[ENTITY 1]-[ENTITY 2]\" is listed that the chemical actually treats the condition, so\nthe answer that [ENTITY 1] does not induce [ENTITY 2] is confirmed. \u201d\n19\n\u2022 \u201cAccording to the Comparative Toxicogenomics Database, the given chemical-condition pair\n\"[ENTITY 1]-[ENTITY 2]\" is listed that the chemical is typically present with the condition,\nwhich may confirm the answer if [ENTITY 1] induces [ENTITY 2]. \u201d\n\u2022 \u201cBased on the expression [INDUCE PATTERN], it is likely that [ENTITY 1] induces\n[ENTITY 2]. \u201d\n\u2022 \u201cBased on the expression [NOT INDUCE PATTERN], it is not likely that [ENTITY 1]\ninduces [ENTITY 2]. \u201d\n\u2022 \u201cBased on the expression [C TREATS D PATTERN], [ENTITY 1] actually treats [ENTITY\n2]. , so it is not likely that [ENTITY 1] induces [ENTITY 2]. \u201d\n\u2022 \u201cBased on the expression [CLOSE MENTION PATTERN], [ENTITY 1] is closely mentioned\nwith [ENTITY 2], so they should be closely related. \u201d\n\u2022 \u201cBased on the expression [DISEASE IMPROVE PATTERN], the disease [ENTITY 2] is\nactually improved, so it is not likely that [ENTITY 1] induces [ENTITY 2]. \u201d\n\u2022 \u201cBased on the expression [INITIAL CONDITION PATTERN], [ENTITY 2] is the initial\ncondition of the patient(s), so it is not likely that [ENTITY 1] induces [ENTITY 2]. \u201d\n\u2022 \u201cBased on the expression [UNCERTAIN PATTERN], it is uncertain that [ENTITY 1] induces\n[ENTITY 2]. \u201d\n\u2022 \u201cBased on the expression [INDUCED BY OTHER PATTERN], [ENTITY 2] is induced by\nother factors, so it is not likely that [ENTITY 1] induces [ENTITY 2]. \u201d\n\u2022 \u201c[ENTITY 1] and [ENTITY 2] are not closely mentioned in the text, so it is not likely that\n[ENTITY 1] induces [ENTITY 2]. \u201d\n\u2022 \u201cAccording to phrases like [WEAK EXPRESSION], there is no strong signal that [ENTITY\n1] induces [ENTITY 2]. \u201d\n\u2022 \u201cAccording to the text, another chemical is mentioned closer to [ENTITY 2] than [ENTITY\n1], so it is not likely that [ENTITY 1] induces [ENTITY 2]. \u201d\n\u2022 \u201cAccording to the text, another disease is mentioned closer to [ENTITY 1] than [ENTITY\n2], so it is not likely that [ENTITY 1] induces [ENTITY 2]. \u201d\n20\n"
  }
]